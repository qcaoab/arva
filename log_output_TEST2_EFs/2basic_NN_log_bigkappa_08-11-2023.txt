/home/marcchen/Documents/constrain_factor/researchcode/exp_config_json_files/multi_portfolio_TEST2_EFs.json
Starting at: 
11-08-23_13:30

 numpy seed:  3  


 pytorch seed:  3  

Key parameters-------
paths: 256000
iterations: 20000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_nom_ret', 'VWD_nom_ret']
############# End: defined asset  basket #################
               CPI_nom_ret  B10_nom_ret  VWD_nom_ret
My identifier                                       
192607           -0.011299     0.005383     0.031411
192608           -0.005714     0.005363     0.028647
192609            0.005747     0.005343     0.005787
192610            0.005714     0.005323    -0.028996
192611            0.005682     0.005303     0.028554
               CPI_nom_ret  B10_nom_ret  VWD_nom_ret
My identifier                                       
202208           -0.000354    -0.043289    -0.036240
202209            0.002151    -0.050056    -0.091324
202210            0.004056    -0.014968     0.077403
202211           -0.001010     0.040789     0.052365
202212           -0.003070    -0.018566    -0.057116
Indices constructed with column names:
Index(['date_for_plt', 'CPI_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
B10_real_ret    0.001637
VWD_real_ret    0.006759
dtype: float64


timeseries_basket['data_df_stdev'] = 
B10_real_ret    0.019258
VWD_real_ret    0.053610
dtype: float64


timeseries_basket['data_df_corr'] = 
              B10_real_ret  VWD_real_ret
B10_real_ret      1.000000      0.090987
VWD_real_ret      0.090987      1.000000


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 192707
End: 199112
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 400, 'itbound_SGD_algorithms': 20000, 'nit_IterateAveragingStart': 18000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 8.9990, -0.3262],
        [ 9.8876, -0.4440],
        [ 9.4356, -0.5183],
        [ 9.7646, -4.6575],
        [ 8.9441, -0.4609],
        [ 0.3071, -7.9568],
        [ 8.7874, -0.5348],
        [-6.7191, -8.6281],
        [ 8.0896, -6.6798],
        [ 9.3436, -0.4676]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-8.3322, -7.9010, -7.8695, -5.6807, -8.1613, -6.3807, -8.2523, -6.3050,
        -6.0354, -7.9570], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.3758,  0.6926,  0.5690,  0.8044,  0.3903,  0.5214,  0.3247,  0.5116,
          0.7126,  0.5315],
        [ 2.7444,  5.4900,  3.4247,  4.6953,  2.8366,  4.3708,  2.5155,  5.5614,
          4.7626,  3.3188],
        [ 2.9186,  5.9124,  3.6715,  4.7250,  2.8336,  4.5040,  2.7602,  5.4868,
          5.0707,  3.3905],
        [ 2.9834,  6.0099,  3.6442,  5.2626,  2.9586,  5.5675,  2.7053,  6.6458,
          5.7662,  3.5071],
        [ 2.9057,  5.9133,  3.5257,  4.7828,  2.8734,  4.5245,  2.8143,  5.4756,
          5.0249,  3.5348],
        [ 3.0205,  5.9295,  3.6933,  5.0304,  3.0448,  4.9405,  2.7271,  6.2282,
          5.5398,  3.5624],
        [ 2.9056,  6.0650,  3.6122,  5.2804,  2.9497,  5.7353,  2.7194,  6.8885,
          5.8421,  3.4720],
        [ 0.3890,  0.7511,  0.6053,  0.8280,  0.4049,  0.5395,  0.3345,  0.6620,
          0.7597,  0.5626],
        [ 0.3877,  0.7486,  0.6033,  0.8252,  0.4036,  0.5374,  0.3334,  0.6595,
          0.7571,  0.5607],
        [-0.1744, -0.3843, -0.2829, -0.4794, -0.1638, -0.2872, -0.1176, -0.2960,
         -0.4880, -0.2616]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.9646, -9.3253, -9.2669, -8.5739, -9.2580, -8.8415, -8.4978,  4.4932,
         4.4795, -2.0682], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 7.4341, -5.2359, -5.8376, -7.5516, -5.9498, -6.8216, -7.5259,  5.0033,
          4.9051, -0.0246]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 8.4683e+00, -2.0167e-01],
        [ 1.1353e+01,  2.8206e+00],
        [ 3.2721e-01,  1.1209e+01],
        [-1.0832e+01, -4.6031e+00],
        [ 1.1318e+01,  2.8317e+00],
        [-1.1295e+01, -4.5147e+00],
        [ 9.0600e+00,  5.6569e+00],
        [-9.4756e+00,  4.3600e-03],
        [-1.0676e+01, -3.2558e+00],
        [ 5.9589e+00,  8.3174e+00]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-9.1974, -0.7285,  9.6826,  0.3683, -0.7407, -3.7073,  8.4888,  7.3284,
        -0.7428,  5.7239], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.8248e+00, -3.7009e+00, -1.6972e+01,  9.6545e+00, -3.2806e+00,
          1.4198e+00, -6.0537e+00,  6.0587e+00,  5.5822e+00, -1.2052e+01],
        [ 2.6859e-01,  1.5936e+00,  3.1093e+01, -1.0472e+01,  1.1667e+00,
          4.3199e-02,  2.5539e+00, -4.4719e+00, -8.1149e+00,  1.4017e+00],
        [-1.6968e+00, -2.7066e+00, -9.5378e-01, -5.5234e-01, -3.2704e+00,
         -2.8383e-03, -5.7126e-01, -2.6409e+00, -7.5603e-02,  4.3110e-02],
        [ 2.3349e-01, -2.3148e+00, -1.3560e+00,  2.1443e-01, -2.7326e+00,
         -2.1235e-02, -7.6739e-01, -2.4080e+00,  1.9538e-01, -7.5645e-01],
        [ 2.9510e-01, -2.1712e+00, -7.2625e-01, -1.7381e-01, -2.1573e+00,
         -5.5465e-02, -1.8791e+00, -1.8790e+00, -9.4138e-03, -1.0685e+00],
        [-4.7655e-01, -4.2180e+00,  8.1559e-01,  1.5024e+00, -4.1929e+00,
          3.4347e+00, -2.0415e+00,  2.6442e-01,  2.0524e+00, -1.3025e+00],
        [ 3.0924e-01, -1.4373e+00,  2.4077e+00,  4.6505e-02, -1.4234e+00,
         -2.1666e+00, -8.2834e-01,  1.1130e+00, -1.9177e+00,  1.1523e+00],
        [-6.1451e-01, -3.1931e+00,  1.0854e+00,  1.7979e+00, -3.1592e+00,
          3.8853e+00, -1.6575e+00,  6.4426e-01,  2.3141e+00, -1.0119e+00],
        [-2.3635e+00, -3.2829e+00, -2.1386e+00, -1.1463e+00, -3.3565e+00,
         -5.2687e-03,  2.0511e-01, -2.6414e+00, -8.6978e-02,  8.3263e-01],
        [-9.5220e+00, -1.4338e+01, -9.6207e+00,  1.4071e+01, -1.3813e+01,
          9.6557e+00, -1.9712e+01,  7.2483e+00,  1.2334e+01, -2.4767e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 0.1687, -2.3780, -3.8904, -3.1639, -2.9617, -3.8232, -2.7553, -3.5565,
        -4.0998, -2.6698], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -5.3770,   0.3848,  -0.2549,   0.4640,   0.2330,   1.2895,   1.1011,
           1.1877,  -0.4883,  19.8035],
        [  5.0610,  -0.4228,   0.2550,  -0.4232,  -0.2349,  -1.2884,  -1.0997,
          -1.1954,   0.6205, -19.8532]], device='cuda:0'))])
loaded xi:  -19.891674
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 806.3616944417624
W_T_median: 547.9700475945424
W_T_pctile_5: -287.9564157269737
W_T_CVAR_5_pct: -408.2820772432722
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  935.889145714144
Current xi:  [-19.592073]
objective value function right now is: 935.889145714144
4.0% of gradient descent iterations done. Method = Adam
new min fval:  923.9356786049873
Current xi:  [-17.476831]
objective value function right now is: 923.9356786049873
6.0% of gradient descent iterations done. Method = Adam
new min fval:  910.9864811686601
Current xi:  [-17.351707]
objective value function right now is: 910.9864811686601
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.448224]
objective value function right now is: 925.3868458296216
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.488214]
objective value function right now is: 946.5192293260172
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.507095]
objective value function right now is: 953.778844130368
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  903.4155735247244
Current xi:  [-14.208336]
objective value function right now is: 903.4155735247244
16.0% of gradient descent iterations done. Method = Adam
new min fval:  895.3457292943117
Current xi:  [-13.18479]
objective value function right now is: 895.3457292943117
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.756051]
objective value function right now is: 906.5873321087677
20.0% of gradient descent iterations done. Method = Adam
new min fval:  890.7710048997859
Current xi:  [-12.370561]
objective value function right now is: 890.7710048997859
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.791174]
objective value function right now is: 898.4015135098513
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.060513]
objective value function right now is: 904.4545093644169
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.588734]
objective value function right now is: 942.6547324480536
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-11.501392]
objective value function right now is: 908.6155257119162
30.0% of gradient descent iterations done. Method = Adam
new min fval:  883.4376274477949
Current xi:  [-11.447403]
objective value function right now is: 883.4376274477949
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.097973]
objective value function right now is: 908.5767758412654
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-11.924356]
objective value function right now is: 928.1690714403277
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-11.456757]
objective value function right now is: 924.0433373241251
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.561424]
objective value function right now is: 904.0586093581006
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-11.969475]
objective value function right now is: 938.043887879367
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.868357]
objective value function right now is: 941.1687568779225
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.764467]
objective value function right now is: 888.6406019793532
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.19002]
objective value function right now is: 883.5941618317385
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.153832]
objective value function right now is: 929.8536498677519
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.199087]
objective value function right now is: 893.859013439443
52.0% of gradient descent iterations done. Method = Adam
new min fval:  876.7189164444467
Current xi:  [-9.889546]
objective value function right now is: 876.7189164444467
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.34032]
objective value function right now is: 948.9148271818068
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-10.412814]
objective value function right now is: 910.3565933028651
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-11.008557]
objective value function right now is: 894.7753190769415
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.696875]
objective value function right now is: 940.1123358923795
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.592344]
objective value function right now is: 953.7803084346733
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.138774]
objective value function right now is: 905.5707210766027
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.470768]
objective value function right now is: 887.1517411378329
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.478623]
objective value function right now is: 894.0887031125201
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.922024]
objective value function right now is: 899.2929778907544
72.0% of gradient descent iterations done. Method = Adam
new min fval:  873.9627345418775
Current xi:  [-9.508164]
objective value function right now is: 873.9627345418775
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.061841]
objective value function right now is: 875.6323795931711
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.601561]
objective value function right now is: 876.930561174139
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.563156]
objective value function right now is: 881.2045580447906
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.291004]
objective value function right now is: 875.8994472520815
82.0% of gradient descent iterations done. Method = Adam
new min fval:  871.7085204953022
Current xi:  [-8.251661]
objective value function right now is: 871.7085204953022
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.677726]
objective value function right now is: 876.7330317239023
86.0% of gradient descent iterations done. Method = Adam
new min fval:  871.63215418125
Current xi:  [-7.0396852]
objective value function right now is: 871.63215418125
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.205282]
objective value function right now is: 873.3099169494442
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.235162]
objective value function right now is: 877.3405512072098
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.1864595]
objective value function right now is: 871.8678631462725
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.7765207]
objective value function right now is: 872.4285132026862
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.58724]
objective value function right now is: 873.9395818870004
new min fval from sgd:  869.4266528843367
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.511099]
objective value function right now is: 869.4266528843367
new min fval from sgd:  869.4023023431937
new min fval from sgd:  869.3665972421132
new min fval from sgd:  869.2950342104696
new min fval from sgd:  869.2497597873386
new min fval from sgd:  869.2004277689045
new min fval from sgd:  869.1692558172081
new min fval from sgd:  869.136840287134
new min fval from sgd:  869.103044773876
new min fval from sgd:  869.0697499787952
new min fval from sgd:  869.0357688729353
new min fval from sgd:  868.8372511192181
new min fval from sgd:  868.690249674042
new min fval from sgd:  868.5748221295229
new min fval from sgd:  868.4959778341226
new min fval from sgd:  868.4283533989354
new min fval from sgd:  868.378459163723
new min fval from sgd:  868.3417986637661
new min fval from sgd:  868.3071952904614
new min fval from sgd:  868.277758973219
new min fval from sgd:  868.2436775325987
new min fval from sgd:  868.2063679650951
new min fval from sgd:  868.1823910652708
new min fval from sgd:  868.171253428946
new min fval from sgd:  868.1629874350517
new min fval from sgd:  868.1263197873421
new min fval from sgd:  868.0661451268043
new min fval from sgd:  868.0122227319656
new min fval from sgd:  867.9597007603365
new min fval from sgd:  867.9236973105607
new min fval from sgd:  867.9120003100097
new min fval from sgd:  867.9004480708851
new min fval from sgd:  867.8971064740203
new min fval from sgd:  867.8929979081231
new min fval from sgd:  867.8909603547766
new min fval from sgd:  867.872404254762
new min fval from sgd:  867.8634330279723
new min fval from sgd:  867.850289138814
new min fval from sgd:  867.8353039653641
new min fval from sgd:  867.8290446386945
new min fval from sgd:  867.8118866429111
new min fval from sgd:  867.798273013086
new min fval from sgd:  867.7935326341043
new min fval from sgd:  867.780636065877
new min fval from sgd:  867.7740039162234
new min fval from sgd:  867.7704875251662
new min fval from sgd:  867.7693830315378
new min fval from sgd:  867.7625700782535
new min fval from sgd:  867.7488401334323
new min fval from sgd:  867.7445765261848
new min fval from sgd:  867.7314415866235
new min fval from sgd:  867.7230170017675
new min fval from sgd:  867.7197499254074
new min fval from sgd:  867.7105796285649
new min fval from sgd:  867.7011773458566
new min fval from sgd:  867.6966023386441
new min fval from sgd:  867.6842185409247
new min fval from sgd:  867.6722294587372
new min fval from sgd:  867.667094588322
new min fval from sgd:  867.6534583092996
new min fval from sgd:  867.6302775755997
new min fval from sgd:  867.596819701038
new min fval from sgd:  867.5453397072516
new min fval from sgd:  867.5128062224379
new min fval from sgd:  867.4809118986174
new min fval from sgd:  867.445754087419
new min fval from sgd:  867.440634231008
new min fval from sgd:  867.4341306342536
new min fval from sgd:  867.402450320392
new min fval from sgd:  867.3703820476386
new min fval from sgd:  867.32358260707
new min fval from sgd:  867.2782187051683
new min fval from sgd:  867.2638502214141
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.4428835]
objective value function right now is: 868.8368306148863
min fval:  867.2638502214141
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 11.6314,  -0.2897],
        [ 13.2417,  -0.9717],
        [ 12.3730,  -0.5825],
        [ 11.9187,  -4.5346],
        [ 10.4544,  -1.3441],
        [  3.2864,  -9.9393],
        [  7.8609,  -1.8931],
        [ -5.4921, -10.4610],
        [ 11.6343,  -6.4624],
        [ 12.2338,  -0.4644]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-10.9866,  -9.8536, -10.3358,  -7.4637, -11.0888,  -7.6733, -10.1109,
         -7.4684,  -6.9133, -10.4773], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 2.7863e-01,  1.0043e+00,  6.3554e-01,  7.1674e-01,  4.7085e-02,
          5.3504e-01,  4.4278e-03,  4.0172e-01,  9.6761e-01,  5.6390e-01],
        [ 7.6477e-01,  4.5708e+00,  2.2941e+00,  2.8896e+00, -1.2194e-02,
          3.9563e+00,  3.8295e-03,  4.7840e+00,  3.7086e+00,  2.0915e+00],
        [ 3.0067e+00,  7.4336e+00,  4.5036e+00,  4.7178e+00,  1.3336e+00,
          6.0289e+00,  1.3713e-01,  7.2708e+00,  5.7853e+00,  4.1883e+00],
        [ 3.6208e+00,  8.0840e+00,  4.9214e+00,  5.7345e+00,  2.5307e+00,
          7.3484e+00,  1.1371e+00,  7.5948e+00,  7.2115e+00,  4.7052e+00],
        [ 3.0697e+00,  7.4951e+00,  4.4403e+00,  4.8122e+00,  1.4361e+00,
          6.0753e+00,  1.7197e-01,  7.2914e+00,  5.7914e+00,  4.3590e+00],
        [ 3.8423e+00,  8.2761e+00,  5.2128e+00,  5.5633e+00,  2.6934e+00,
          6.0728e+00,  1.1233e+00,  7.5004e+00,  6.5408e+00,  4.9937e+00],
        [ 3.5561e+00,  8.1352e+00,  4.8944e+00,  5.7559e+00,  2.5161e+00,
          7.5124e+00,  1.1253e+00,  7.7566e+00,  7.2533e+00,  4.6754e+00],
        [ 2.5924e-01,  9.3430e-01,  5.8953e-01,  6.6339e-01,  4.4025e-02,
          4.8732e-01,  3.5796e-03,  3.6400e-01,  8.9722e-01,  5.2280e-01],
        [ 2.5821e-01,  9.3065e-01,  5.8713e-01,  6.6055e-01,  4.3864e-02,
          4.8485e-01,  3.5361e-03,  3.6209e-01,  8.9347e-01,  5.2066e-01],
        [-1.6834e-01, -5.2437e-01, -3.3995e-01, -4.8008e-01, -8.9842e-04,
         -2.3232e-01,  2.7835e-03, -8.1077e-02, -7.1713e-01, -3.1908e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([  4.7617,  -9.1319, -10.1226,  -9.4424, -10.1314, -10.0463,  -9.4261,
          4.4397,   4.4230,  -2.1670], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 6.5662, -3.0873, -5.7367, -9.0906, -5.8724, -8.0791, -9.0884,  4.2112,
          4.1157,  0.0174]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  9.9768,  -1.7313],
        [ 11.0757,   5.6272],
        [ -1.9518,  12.9735],
        [-13.2575,  -3.9810],
        [  1.5680,   3.2732],
        [-16.5229,  -5.5015],
        [ 11.3249,   3.5764],
        [-12.3339,  -0.2640],
        [-13.0272,  -4.1540],
        [  7.9576,   9.2124]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-13.1025,  -2.5910,  11.1286,  -0.3701, -10.6108,  -3.0541,   7.5880,
          9.9574,  -1.8873,   6.6914], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.0507e+00, -2.4776e+00, -2.4870e+01,  8.8998e+00, -6.3270e-03,
          5.8997e+00, -6.3410e+00,  5.4198e+00,  1.0631e+01, -5.6814e+00],
        [ 2.6771e+00, -1.6607e-01,  2.5331e+01, -1.4496e+01,  4.0303e-02,
          7.0570e+00,  3.0493e+00, -3.8980e+00, -8.5215e+00,  2.4753e+00],
        [-4.8715e-01, -9.8232e-01, -3.5298e-01,  1.4505e-01, -2.3684e-01,
          1.5661e-02, -3.8669e+00, -2.1466e+00,  3.3752e-02, -1.8114e+00],
        [-6.4101e-01, -6.8396e-01, -5.6381e-01, -1.5424e-01, -4.9646e-02,
         -3.6681e-02, -3.5989e+00, -1.0451e+00, -6.7667e-02, -1.4801e+00],
        [-4.8355e+00, -3.9095e+00, -7.1327e+00,  1.7625e+00, -3.5288e-03,
          1.3699e+00, -2.6370e+00,  2.2759e+00,  2.3090e+00, -1.1521e+00],
        [-4.7752e-01, -8.9377e-01, -5.1736e-01, -1.4831e-01, -4.5413e-01,
         -3.1337e-02, -3.6832e+00, -1.4922e+00, -6.1384e-02, -1.4150e+00],
        [ 2.9097e+00, -1.4325e+00,  2.7689e+00,  3.9229e-01, -2.8506e+00,
          6.0070e+00, -2.7235e+00,  9.3873e-01,  1.4047e+00,  3.8985e-01],
        [-4.7811e-01, -8.9367e-01, -5.1797e-01, -1.4849e-01, -4.5423e-01,
         -3.1364e-02, -3.6641e+00, -1.4913e+00, -6.1437e-02, -1.4149e+00],
        [-4.2229e+00, -1.0010e+00,  3.2034e+00,  2.5719e-01, -4.4662e+00,
          1.5013e+00, -1.4950e+00,  5.7361e-01,  4.6709e-01,  8.4347e-01],
        [-1.2768e+01, -2.0849e+00, -4.5121e+00,  1.4111e+01,  5.5664e-03,
          1.4339e+01, -2.0286e+01,  8.5591e+00,  1.1622e+01, -4.2144e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-0.1627, -1.8750, -3.8852, -4.0696, -2.9022, -3.6868, -4.6414, -3.7057,
        -4.3813, -3.6383], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -6.3696,   0.3957,   0.1471,  -0.0970,  -3.1987,  -0.0905,   0.6639,
          -0.0907,   1.8041,  20.7586],
        [  6.0596,  -0.4337,  -0.1471,   0.1065,   3.1982,   0.0905,  -0.6626,
           0.0903,  -1.7583, -20.7978]], device='cuda:0'))])
xi:  [-6.4342847]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 335.83603556048183
W_T_median: 91.5820744843654
W_T_pctile_5: -6.238912070330871
W_T_CVAR_5_pct: -156.70104317625666
Average q (qsum/M+1):  47.84696320564516
Optimal xi:  [-6.4342847]
Expected(across Rb) median(across samples) p_equity:  0.21682954244315625
obj fun:  tensor(867.2639, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 15.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 8.9990, -0.3262],
        [ 9.8876, -0.4440],
        [ 9.4356, -0.5183],
        [ 9.7646, -4.6575],
        [ 8.9441, -0.4609],
        [ 0.3071, -7.9568],
        [ 8.7874, -0.5348],
        [-6.7191, -8.6281],
        [ 8.0896, -6.6798],
        [ 9.3436, -0.4676]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-8.3322, -7.9010, -7.8695, -5.6807, -8.1613, -6.3807, -8.2523, -6.3050,
        -6.0354, -7.9570], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.3758,  0.6926,  0.5690,  0.8044,  0.3903,  0.5214,  0.3247,  0.5116,
          0.7126,  0.5315],
        [ 2.7444,  5.4900,  3.4247,  4.6953,  2.8366,  4.3708,  2.5155,  5.5614,
          4.7626,  3.3188],
        [ 2.9186,  5.9124,  3.6715,  4.7250,  2.8336,  4.5040,  2.7602,  5.4868,
          5.0707,  3.3905],
        [ 2.9834,  6.0099,  3.6442,  5.2626,  2.9586,  5.5675,  2.7053,  6.6458,
          5.7662,  3.5071],
        [ 2.9057,  5.9133,  3.5257,  4.7828,  2.8734,  4.5245,  2.8143,  5.4756,
          5.0249,  3.5348],
        [ 3.0205,  5.9295,  3.6933,  5.0304,  3.0448,  4.9405,  2.7271,  6.2282,
          5.5398,  3.5624],
        [ 2.9056,  6.0650,  3.6122,  5.2804,  2.9497,  5.7353,  2.7194,  6.8885,
          5.8421,  3.4720],
        [ 0.3890,  0.7511,  0.6053,  0.8280,  0.4049,  0.5395,  0.3345,  0.6620,
          0.7597,  0.5626],
        [ 0.3877,  0.7486,  0.6033,  0.8252,  0.4036,  0.5374,  0.3334,  0.6595,
          0.7571,  0.5607],
        [-0.1744, -0.3843, -0.2829, -0.4794, -0.1638, -0.2872, -0.1176, -0.2960,
         -0.4880, -0.2616]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.9646, -9.3253, -9.2669, -8.5739, -9.2580, -8.8415, -8.4978,  4.4932,
         4.4795, -2.0682], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 7.4341, -5.2359, -5.8376, -7.5516, -5.9498, -6.8216, -7.5259,  5.0033,
          4.9051, -0.0246]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 8.4683e+00, -2.0167e-01],
        [ 1.1353e+01,  2.8206e+00],
        [ 3.2721e-01,  1.1209e+01],
        [-1.0832e+01, -4.6031e+00],
        [ 1.1318e+01,  2.8317e+00],
        [-1.1295e+01, -4.5147e+00],
        [ 9.0600e+00,  5.6569e+00],
        [-9.4756e+00,  4.3600e-03],
        [-1.0676e+01, -3.2558e+00],
        [ 5.9589e+00,  8.3174e+00]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-9.1974, -0.7285,  9.6826,  0.3683, -0.7407, -3.7073,  8.4888,  7.3284,
        -0.7428,  5.7239], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.8248e+00, -3.7009e+00, -1.6972e+01,  9.6545e+00, -3.2806e+00,
          1.4198e+00, -6.0537e+00,  6.0587e+00,  5.5822e+00, -1.2052e+01],
        [ 2.6859e-01,  1.5936e+00,  3.1093e+01, -1.0472e+01,  1.1667e+00,
          4.3199e-02,  2.5539e+00, -4.4719e+00, -8.1149e+00,  1.4017e+00],
        [-1.6968e+00, -2.7066e+00, -9.5378e-01, -5.5234e-01, -3.2704e+00,
         -2.8383e-03, -5.7126e-01, -2.6409e+00, -7.5603e-02,  4.3110e-02],
        [ 2.3349e-01, -2.3148e+00, -1.3560e+00,  2.1443e-01, -2.7326e+00,
         -2.1235e-02, -7.6739e-01, -2.4080e+00,  1.9538e-01, -7.5645e-01],
        [ 2.9510e-01, -2.1712e+00, -7.2625e-01, -1.7381e-01, -2.1573e+00,
         -5.5465e-02, -1.8791e+00, -1.8790e+00, -9.4138e-03, -1.0685e+00],
        [-4.7655e-01, -4.2180e+00,  8.1559e-01,  1.5024e+00, -4.1929e+00,
          3.4347e+00, -2.0415e+00,  2.6442e-01,  2.0524e+00, -1.3025e+00],
        [ 3.0924e-01, -1.4373e+00,  2.4077e+00,  4.6505e-02, -1.4234e+00,
         -2.1666e+00, -8.2834e-01,  1.1130e+00, -1.9177e+00,  1.1523e+00],
        [-6.1451e-01, -3.1931e+00,  1.0854e+00,  1.7979e+00, -3.1592e+00,
          3.8853e+00, -1.6575e+00,  6.4426e-01,  2.3141e+00, -1.0119e+00],
        [-2.3635e+00, -3.2829e+00, -2.1386e+00, -1.1463e+00, -3.3565e+00,
         -5.2687e-03,  2.0511e-01, -2.6414e+00, -8.6978e-02,  8.3263e-01],
        [-9.5220e+00, -1.4338e+01, -9.6207e+00,  1.4071e+01, -1.3813e+01,
          9.6557e+00, -1.9712e+01,  7.2483e+00,  1.2334e+01, -2.4767e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 0.1687, -2.3780, -3.8904, -3.1639, -2.9617, -3.8232, -2.7553, -3.5565,
        -4.0998, -2.6698], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -5.3770,   0.3848,  -0.2549,   0.4640,   0.2330,   1.2895,   1.1011,
           1.1877,  -0.4883,  19.8035],
        [  5.0610,  -0.4228,   0.2550,  -0.4232,  -0.2349,  -1.2884,  -1.0997,
          -1.1954,   0.6205, -19.8532]], device='cuda:0'))])
loaded xi:  -19.891674
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 806.3616944417624
W_T_median: 547.9700475945424
W_T_pctile_5: -287.9564157269737
W_T_CVAR_5_pct: -408.2820772432722
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  2551.730085498762
Current xi:  [-17.807178]
objective value function right now is: 2551.730085498762
4.0% of gradient descent iterations done. Method = Adam
new min fval:  2492.1481761330347
Current xi:  [-18.35052]
objective value function right now is: 2492.1481761330347
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.06796]
objective value function right now is: 2646.910932504731
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.597156]
objective value function right now is: 2680.2810923063935
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.787382]
objective value function right now is: 2505.1135333129364
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.63992]
objective value function right now is: 2527.284948324366
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-11.978537]
objective value function right now is: 2507.254094507633
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.756263]
objective value function right now is: 3041.9493730492522
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-31.623539]
objective value function right now is: 3878.532952880157
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-39.87168]
objective value function right now is: 3285.751757771143
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-41.638176]
objective value function right now is: 3035.0466670090977
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-40.143795]
objective value function right now is: 2752.388553684928
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.25642]
objective value function right now is: 2643.3254526040664
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-31.401373]
objective value function right now is: 2592.7061287982146
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-27.892548]
objective value function right now is: 2572.0390891358365
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-24.630909]
objective value function right now is: 2713.964905758575
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.63345]
objective value function right now is: 2528.0608350928205
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-20.7778]
objective value function right now is: 2619.808641981599
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.013836]
objective value function right now is: 2526.2114413224135
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.926882]
objective value function right now is: 2542.8180181350313
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.029723]
objective value function right now is: 2548.289181229202
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.481981]
objective value function right now is: 2506.656764043929
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.2980795]
objective value function right now is: 2500.931938335448
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.077244]
objective value function right now is: 2497.8188636223153
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.307254]
objective value function right now is: 2503.9118736717014
52.0% of gradient descent iterations done. Method = Adam
new min fval:  2468.975794072395
Current xi:  [-10.4674225]
objective value function right now is: 2468.975794072395
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-11.018563]
objective value function right now is: 2589.153469349014
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-11.165267]
objective value function right now is: 2671.0690460920678
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-9.134834]
objective value function right now is: 2487.145263201888
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.816705]
objective value function right now is: 2501.14697497697
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.098031]
objective value function right now is: 2502.80851188269
64.0% of gradient descent iterations done. Method = Adam
new min fval:  2468.7332253793
Current xi:  [-9.914981]
objective value function right now is: 2468.7332253793
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.444094]
objective value function right now is: 2568.3140781613142
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.893954]
objective value function right now is: 2501.4073328363165
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.832136]
objective value function right now is: 2496.3888018224557
72.0% of gradient descent iterations done. Method = Adam
new min fval:  2455.903405703465
Current xi:  [-9.629929]
objective value function right now is: 2455.903405703465
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.31337]
objective value function right now is: 2472.263190381133
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.989102]
objective value function right now is: 2457.0195588470024
78.0% of gradient descent iterations done. Method = Adam
new min fval:  2441.248792856402
Current xi:  [-8.691377]
objective value function right now is: 2441.248792856402
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.562861]
objective value function right now is: 2453.1770555918692
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.157651]
objective value function right now is: 2442.4088375347965
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.9714503]
objective value function right now is: 2441.34844415237
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.632116]
objective value function right now is: 2452.5902045862917
88.0% of gradient descent iterations done. Method = Adam
new min fval:  2437.9569756907567
Current xi:  [-7.322913]
objective value function right now is: 2437.9569756907567
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.0552425]
objective value function right now is: 2438.9815517961283
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.2753882]
objective value function right now is: 2450.9580520258746
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.716471]
objective value function right now is: 2443.8403048994505
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.4556513]
objective value function right now is: 2440.161434045828
new min fval from sgd:  2433.9063192260624
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.406209]
objective value function right now is: 2433.9063192260624
new min fval from sgd:  2433.8154662561633
new min fval from sgd:  2433.7014527484152
new min fval from sgd:  2433.6678598410144
new min fval from sgd:  2433.434984608155
new min fval from sgd:  2433.2923228684144
new min fval from sgd:  2433.19079984921
new min fval from sgd:  2433.170771212748
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.3022366]
objective value function right now is: 2434.210839977735
min fval:  2433.170771212748
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 11.6026,  -0.0385],
        [ 13.4272,  -1.0897],
        [ 12.5133,  -0.5929],
        [ 12.2025,  -4.3618],
        [ 10.1553,  -1.3273],
        [  3.6277, -10.0658],
        [  7.2653,  -1.7152],
        [ -5.8051, -10.6109],
        [ 11.4751,  -6.2097],
        [ 12.3121,  -0.3863]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-11.3089,  -9.7025, -10.2737,  -7.2865, -11.1604,  -7.7172,  -9.8553,
         -7.6222,  -6.6806, -10.5459], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 2.1137e-01,  9.2848e-01,  5.6036e-01,  7.5487e-01,  3.1167e-02,
          6.7372e-01,  2.3643e-02,  4.0628e-01,  1.0279e+00,  4.7454e-01],
        [ 9.3053e-01,  5.2677e+00,  2.8029e+00,  3.6603e+00, -1.4245e-02,
          4.6772e+00, -2.1817e-02,  6.1526e+00,  3.9219e+00,  2.5258e+00],
        [ 2.7424e+00,  7.4035e+00,  4.4989e+00,  5.0721e+00,  8.8049e-01,
          5.9048e+00,  5.7481e-02,  7.7585e+00,  5.4242e+00,  4.1259e+00],
        [ 3.5025e+00,  8.1420e+00,  4.9719e+00,  6.2504e+00,  2.2551e+00,
          7.3875e+00,  1.0437e+00,  7.9642e+00,  6.9441e+00,  4.7005e+00],
        [ 2.8011e+00,  7.4560e+00,  4.4260e+00,  5.1559e+00,  9.6533e-01,
          5.9454e+00,  7.5976e-02,  7.7668e+00,  5.4213e+00,  4.2945e+00],
        [ 3.6990e+00,  8.3003e+00,  5.2444e+00,  6.0669e+00,  2.2993e+00,
          5.7757e+00,  8.9717e-01,  7.8949e+00,  6.1470e+00,  4.9736e+00],
        [ 3.4650e+00,  8.2199e+00,  4.9678e+00,  6.2766e+00,  2.2665e+00,
          7.5228e+00,  1.0575e+00,  8.1175e+00,  6.9922e+00,  4.6924e+00],
        [ 1.9704e-01,  8.5737e-01,  5.1587e-01,  6.9769e-01,  2.9553e-02,
          6.1612e-01,  2.3172e-02,  3.6527e-01,  9.5597e-01,  4.3690e-01],
        [ 1.9627e-01,  8.5365e-01,  5.1354e-01,  6.9461e-01,  2.9467e-02,
          6.1310e-01,  2.3145e-02,  3.6313e-01,  9.5209e-01,  4.3493e-01],
        [-8.1591e-02, -5.1523e-01, -2.8117e-01, -7.4286e-01, -1.5860e-02,
         -3.1740e-01, -4.2405e-03, -9.9228e-02, -9.0872e-01, -2.3219e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([  4.8939,  -9.7490, -10.3861,  -9.2830, -10.3885, -10.1868,  -9.2961,
          4.5637,   4.5465,  -2.4845], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 6.4632, -3.6368, -5.7664, -9.0061, -5.8887, -7.9789, -9.0702,  4.0934,
          3.9973, -0.0147]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  9.3064,  -2.3301],
        [  2.8814,   2.8181],
        [ -1.0525,  13.9248],
        [-13.8648,  -4.2382],
        [ 11.3725,   4.6418],
        [-18.5534,  -4.8556],
        [  6.8503,   3.3448],
        [-13.2014,  -0.8499],
        [-12.8850,  -4.4040],
        [  7.8974,   9.1457]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-12.9798,  -8.6654,  11.2610,  -0.7848,  -0.3500,  -4.0627,   7.6157,
          7.9000,  -2.4802,   6.7748], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.3512e+00, -9.2007e-02, -7.4504e-01,  1.9368e+00, -7.6229e-01,
         -1.4027e-01, -6.4734e+00,  1.5611e+00,  1.9378e+00, -2.0071e+00],
        [ 4.5286e+00,  1.8080e+00, -1.4686e+00, -1.6508e+00,  2.4050e+00,
         -2.5202e+00,  5.2349e+00, -1.6666e+00, -3.9126e+00,  2.2961e+00],
        [-1.6871e+01, -8.8356e-02, -1.3326e+01,  7.8785e+00, -1.2770e+01,
          2.6134e+01, -4.2604e+00,  1.7869e+00,  1.4167e+01, -3.1272e+01],
        [-3.0552e+00, -1.7739e+00, -2.2736e+01,  7.4459e+00, -3.7213e+00,
          3.7495e+00, -1.5877e+00,  5.1324e+00,  1.1143e+01, -1.0584e+01],
        [ 4.2410e+00,  1.9494e-01,  1.2508e+01, -1.4008e+00,  1.1575e+00,
          1.1401e+01,  3.3310e-02, -6.1971e-01, -1.0102e+01,  2.0127e+00],
        [-7.5433e-01, -6.8867e-02, -3.4704e-01, -8.9646e-02, -2.2054e+00,
          6.8109e-03, -3.4069e+00, -1.0035e+00, -3.3870e-02, -1.1832e+00],
        [-2.1416e+00, -2.5129e+00,  2.5380e+00, -1.5067e+00, -2.5470e+00,
         -1.0961e-01, -3.1477e+00,  4.9534e-01, -1.5689e+00, -4.7544e-01],
        [-8.0943e+00, -2.6080e+00,  3.1038e+00,  6.3377e-01, -8.9048e-01,
         -1.1298e+00, -2.2101e+00,  1.5198e+00,  4.5456e-01,  1.5590e+00],
        [ 1.8911e+00, -1.1370e-01, -2.5487e+00, -7.0253e+00, -1.8087e+00,
          8.6970e-03, -3.4781e+00, -9.2077e-01, -1.9457e+00,  1.3388e+00],
        [-4.4752e+00, -1.5415e-04, -1.9197e+00,  1.3039e+01, -3.6170e+00,
          4.9368e+00, -1.9765e+01,  6.4549e+00,  1.0456e+01, -1.7667e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.6073,  0.7836, -4.4060, -1.8561,  0.0145, -3.4142, -4.9917, -3.9965,
        -4.2194, -4.0646], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.0771,  -2.1022,  18.6155,  -7.4481,   2.5088,   0.0326,   0.9931,
           1.4001,  -1.7345,   8.6590],
        [ -0.1474,   2.0642, -18.6097,   7.4564,  -2.5093,  -0.0326,  -0.9919,
          -1.4066,   1.7486,  -8.6829]], device='cuda:0'))])
xi:  [-6.3948145]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 312.6585739028449
W_T_median: 84.73645755375928
W_T_pctile_5: -6.189715288728509
W_T_CVAR_5_pct: -156.22127779703314
Average q (qsum/M+1):  47.49597561743952
Optimal xi:  [-6.3948145]
Expected(across Rb) median(across samples) p_equity:  0.19382522826393445
obj fun:  tensor(2433.1708, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 25.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 8.9990, -0.3262],
        [ 9.8876, -0.4440],
        [ 9.4356, -0.5183],
        [ 9.7646, -4.6575],
        [ 8.9441, -0.4609],
        [ 0.3071, -7.9568],
        [ 8.7874, -0.5348],
        [-6.7191, -8.6281],
        [ 8.0896, -6.6798],
        [ 9.3436, -0.4676]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-8.3322, -7.9010, -7.8695, -5.6807, -8.1613, -6.3807, -8.2523, -6.3050,
        -6.0354, -7.9570], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.3758,  0.6926,  0.5690,  0.8044,  0.3903,  0.5214,  0.3247,  0.5116,
          0.7126,  0.5315],
        [ 2.7444,  5.4900,  3.4247,  4.6953,  2.8366,  4.3708,  2.5155,  5.5614,
          4.7626,  3.3188],
        [ 2.9186,  5.9124,  3.6715,  4.7250,  2.8336,  4.5040,  2.7602,  5.4868,
          5.0707,  3.3905],
        [ 2.9834,  6.0099,  3.6442,  5.2626,  2.9586,  5.5675,  2.7053,  6.6458,
          5.7662,  3.5071],
        [ 2.9057,  5.9133,  3.5257,  4.7828,  2.8734,  4.5245,  2.8143,  5.4756,
          5.0249,  3.5348],
        [ 3.0205,  5.9295,  3.6933,  5.0304,  3.0448,  4.9405,  2.7271,  6.2282,
          5.5398,  3.5624],
        [ 2.9056,  6.0650,  3.6122,  5.2804,  2.9497,  5.7353,  2.7194,  6.8885,
          5.8421,  3.4720],
        [ 0.3890,  0.7511,  0.6053,  0.8280,  0.4049,  0.5395,  0.3345,  0.6620,
          0.7597,  0.5626],
        [ 0.3877,  0.7486,  0.6033,  0.8252,  0.4036,  0.5374,  0.3334,  0.6595,
          0.7571,  0.5607],
        [-0.1744, -0.3843, -0.2829, -0.4794, -0.1638, -0.2872, -0.1176, -0.2960,
         -0.4880, -0.2616]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.9646, -9.3253, -9.2669, -8.5739, -9.2580, -8.8415, -8.4978,  4.4932,
         4.4795, -2.0682], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 7.4341, -5.2359, -5.8376, -7.5516, -5.9498, -6.8216, -7.5259,  5.0033,
          4.9051, -0.0246]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 8.4683e+00, -2.0167e-01],
        [ 1.1353e+01,  2.8206e+00],
        [ 3.2721e-01,  1.1209e+01],
        [-1.0832e+01, -4.6031e+00],
        [ 1.1318e+01,  2.8317e+00],
        [-1.1295e+01, -4.5147e+00],
        [ 9.0600e+00,  5.6569e+00],
        [-9.4756e+00,  4.3600e-03],
        [-1.0676e+01, -3.2558e+00],
        [ 5.9589e+00,  8.3174e+00]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-9.1974, -0.7285,  9.6826,  0.3683, -0.7407, -3.7073,  8.4888,  7.3284,
        -0.7428,  5.7239], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.8248e+00, -3.7009e+00, -1.6972e+01,  9.6545e+00, -3.2806e+00,
          1.4198e+00, -6.0537e+00,  6.0587e+00,  5.5822e+00, -1.2052e+01],
        [ 2.6859e-01,  1.5936e+00,  3.1093e+01, -1.0472e+01,  1.1667e+00,
          4.3199e-02,  2.5539e+00, -4.4719e+00, -8.1149e+00,  1.4017e+00],
        [-1.6968e+00, -2.7066e+00, -9.5378e-01, -5.5234e-01, -3.2704e+00,
         -2.8383e-03, -5.7126e-01, -2.6409e+00, -7.5603e-02,  4.3110e-02],
        [ 2.3349e-01, -2.3148e+00, -1.3560e+00,  2.1443e-01, -2.7326e+00,
         -2.1235e-02, -7.6739e-01, -2.4080e+00,  1.9538e-01, -7.5645e-01],
        [ 2.9510e-01, -2.1712e+00, -7.2625e-01, -1.7381e-01, -2.1573e+00,
         -5.5465e-02, -1.8791e+00, -1.8790e+00, -9.4138e-03, -1.0685e+00],
        [-4.7655e-01, -4.2180e+00,  8.1559e-01,  1.5024e+00, -4.1929e+00,
          3.4347e+00, -2.0415e+00,  2.6442e-01,  2.0524e+00, -1.3025e+00],
        [ 3.0924e-01, -1.4373e+00,  2.4077e+00,  4.6505e-02, -1.4234e+00,
         -2.1666e+00, -8.2834e-01,  1.1130e+00, -1.9177e+00,  1.1523e+00],
        [-6.1451e-01, -3.1931e+00,  1.0854e+00,  1.7979e+00, -3.1592e+00,
          3.8853e+00, -1.6575e+00,  6.4426e-01,  2.3141e+00, -1.0119e+00],
        [-2.3635e+00, -3.2829e+00, -2.1386e+00, -1.1463e+00, -3.3565e+00,
         -5.2687e-03,  2.0511e-01, -2.6414e+00, -8.6978e-02,  8.3263e-01],
        [-9.5220e+00, -1.4338e+01, -9.6207e+00,  1.4071e+01, -1.3813e+01,
          9.6557e+00, -1.9712e+01,  7.2483e+00,  1.2334e+01, -2.4767e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 0.1687, -2.3780, -3.8904, -3.1639, -2.9617, -3.8232, -2.7553, -3.5565,
        -4.0998, -2.6698], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -5.3770,   0.3848,  -0.2549,   0.4640,   0.2330,   1.2895,   1.1011,
           1.1877,  -0.4883,  19.8035],
        [  5.0610,  -0.4228,   0.2550,  -0.4232,  -0.2349,  -1.2884,  -1.0997,
          -1.1954,   0.6205, -19.8532]], device='cuda:0'))])
loaded xi:  -19.891674
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 806.3616944417624
W_T_median: 547.9700475945424
W_T_pctile_5: -287.9564157269737
W_T_CVAR_5_pct: -408.2820772432722
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  6518.507617788584
Current xi:  [-15.618099]
objective value function right now is: 6518.507617788584
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.44221]
objective value function right now is: 6545.765124248789
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.616329]
objective value function right now is: 6535.54049839908
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.188342]
objective value function right now is: 6556.165579726467
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.168278]
objective value function right now is: 6662.789798512144
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.293159]
objective value function right now is: 6589.826611119209
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  6446.650458989639
Current xi:  [-11.500219]
objective value function right now is: 6446.650458989639
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.140367]
objective value function right now is: 6674.851863355528
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.0312]
objective value function right now is: 6470.739369690803
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.642282]
objective value function right now is: 6507.107718003737
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.9437766]
objective value function right now is: 6454.364716662633
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.482151]
objective value function right now is: 6510.390570805295
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.877857]
objective value function right now is: 6482.865891433185
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-7.848912]
objective value function right now is: 6555.581162351437
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.3161936]
objective value function right now is: 6496.532408818403
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.236487]
objective value function right now is: 6524.919373914737
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.256306]
objective value function right now is: 6551.808395101827
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.5210705]
objective value function right now is: 6452.551707994699
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.737471]
objective value function right now is: 6491.333624384993
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.751668]
objective va57.99999999999999% of gradient descent iterat42.0% of gradient descent iterations done. Method = Adam
new min fval:  6417.187113969683
Current xi:  [-9.260.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.760017]
objective value function right now is: 6396.519849336648
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.746887]
objective value function right now is: 6426.395983277301
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-20.813385]
objective value function right now is: 10581.5791628058
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-21.487339]
objective value function right now is: 6786.209957595978
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.846193]
objective value function right now is: 6783.575203675424
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-29.57604]
objective value function right now is: 23434.97949111495
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.156498]
objective value function right now is: 9214.02526346418
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.319824]
objective value function right now is: 8203.64558487736
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.899605]
objective value function right now is: 7078.8638993757295
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.145077]
objective value function right now is: 6788.540124929523
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.4328]
objective value function right now is: 8726.4332878078
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.761753]
objective value function right now is: 6942.353487239188
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-33.510242]
objective value function right now is: 6789.945786533535
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-31.938187]
objective value function right now is: 6796.469895153325
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-30.570766]
objective value function right now is: 6634.51643511932
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-28.733486]
objective value function right now is: 6602.083806801241
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-29.175846]
objective value function right now is: 6672.715045903334
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-27.686495]
objective value function right now is: 6553.487828920948
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-26.770063]
objective value function right now is: 6552.251204583759
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-25.709455]
objective value function right now is: 6523.73866727471
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-25.482073]
objective value function right now is: 6533.21024879257
min fval:  6538.1628643356225
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[10.1703, -0.6919],
        [11.7950, -0.8178],
        [11.1236, -0.7274],
        [11.3864, -4.2267],
        [10.0321, -0.8573],
        [ 1.5889, -9.0412],
        [ 9.6642, -0.9789],
        [-5.8490, -9.4436],
        [ 9.8874, -6.2196],
        [10.9149, -0.6948]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-9.5169, -8.3277, -8.5402, -5.9236, -9.3483, -6.7225, -9.4858, -6.8376,
        -6.2586, -8.8294], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.2167,  1.0707,  0.8189,  1.2529,  0.1509,  1.1014,  0.0276,  1.0890,
          1.0783,  0.6494],
        [ 2.6090,  6.4673,  3.8245,  5.5039,  2.6190,  5.0319,  2.1464,  6.1843,
          5.2602,  3.5267],
        [ 2.8921,  6.9902,  4.1401,  5.7232,  2.7691,  5.3226,  2.5602,  6.2572,
          5.7025,  3.6783],
        [ 3.2336,  7.2969,  4.3304,  6.1395,  3.1554,  6.4178,  2.8141,  7.3689,
          6.3235,  4.0245],
        [ 2.9020,  7.0075,  4.0172,  5.7816,  2.8265,  5.3539,  2.6307,  6.2575,
          5.6594,  3.8332],
        [ 3.3115,  7.2925,  4.4497,  6.0962,  3.2934,  5.6244,  2.8941,  6.7867,
          6.1693,  4.1404],
        [ 3.1559,  7.3469,  4.2947,  6.1320,  3.1415,  6.5525,  2.8202,  7.5713,
          6.3430,  3.9861],
        [ 0.2013,  1.0089,  0.7718,  1.1851,  0.1376,  1.0390,  0.0188,  1.0327,
          1.0194,  0.6113],
        [ 0.2004,  1.0056,  0.7693,  1.1815,  0.1368,  1.0357,  0.0183,  1.0296,
          1.0162,  0.6093],
        [-0.0936, -0.9074, -0.6628, -0.9542, -0.0568, -0.6042, -0.0102, -0.5676,
         -0.6947, -0.5066]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 5.7943, -9.6470, -9.5425, -8.8108, -9.5283, -9.0830, -8.7883,  5.4800,
         5.4641, -3.5556], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 6.9392, -5.3816, -6.1695, -8.4996, -6.2841, -7.6682, -8.4854,  4.5243,
          4.4266, -0.0747]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  9.3692,  -1.2491],
        [ 11.5755,   3.2997],
        [ -1.0935,  11.9674],
        [-12.1360,  -3.7230],
        [ 11.4846,   3.3752],
        [-15.8250,  -4.7123],
        [ 11.8074,   3.2719],
        [-10.7991,  -0.1079],
        [-10.2603,  -4.9382],
        [  6.4387,   8.5625]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-11.0544,  -0.4119,   9.9214,  -0.3972,  -0.2144,  -2.9309,   8.9678,
          8.9285,   1.4826,   6.7179], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.6309e+00, -3.5605e+00, -2.0009e+01,  8.1301e+00, -3.2518e+00,
          5.8015e+00, -6.2862e+00,  6.6634e+00,  6.4151e+00, -8.6538e+00],
        [ 2.6004e+00,  1.9656e+00,  3.0128e+01, -1.2316e+01,  1.5148e+00,
          2.2720e+00,  2.4591e+00, -5.0692e+00, -3.1526e+00,  2.7656e+00],
        [-2.9023e-01, -2.7264e+00, -3.6212e+00, -1.3741e+00, -2.8995e+00,
          1.7004e-02, -2.6801e+00, -8.5527e-01, -5.6772e-01, -1.7168e+00],
        [-4.4656e-02, -2.7034e+00, -1.2204e+00,  2.6555e-01, -2.7292e+00,
          9.8434e-03, -3.1127e+00, -1.7017e+00, -1.1325e+00, -3.1114e+00],
        [ 2.3628e+00, -3.0085e+00, -6.0147e+00,  1.2690e+00, -3.0152e+00,
          2.2568e+00, -2.6832e+00, -2.3981e+00,  1.4328e+00, -2.3364e+00],
        [-7.7274e-01, -1.9101e+00, -3.2383e-01, -7.2087e-02, -2.0673e+00,
          4.1378e-01, -3.0192e+00, -1.1458e+00,  3.0003e-01, -1.3964e+00],
        [ 1.7120e+00, -1.8601e+00,  5.3189e+00, -2.2807e+00, -1.8637e+00,
          1.3884e+00, -1.7612e+00,  2.0729e+00,  1.5042e+00,  1.5789e+00],
        [ 1.2081e-01, -2.7101e+00,  4.6053e-02,  1.6638e-01, -2.7431e+00,
          1.1522e+00, -3.0704e+00, -9.9610e-01, -1.9320e-01, -1.1906e+00],
        [ 3.6825e+00, -1.9556e+00, -8.4146e+00, -3.7397e+00, -2.0403e+00,
         -6.9893e-01, -1.6118e-01, -3.3174e-01, -1.4173e+00,  7.1933e-01],
        [-1.4008e+01, -1.3115e+01, -5.2726e+00,  1.4471e+01, -1.2539e+01,
          1.4265e+01, -1.9617e+01,  8.2245e+00,  1.1828e+01, -3.4916e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-0.1405, -2.4718, -3.6764, -3.1737, -3.0779, -4.4154, -3.6882, -4.8053,
        -3.2280, -3.0240], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -4.3166,   0.3964,  -0.7270,   0.4002,  -1.0453,  -0.1606,   0.6100,
           0.1858,  -2.3683,  21.3716],
        [  4.0020,  -0.4345,   0.7271,  -0.3955,   1.0442,   0.1613,  -0.6087,
          -0.1926,   2.4648, -21.4196]], device='cuda:0'))])
xi:  [-28.733486]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 334.3231471624046
W_T_median: 121.71905501774407
W_T_pctile_5: -1.8094623566835208
W_T_CVAR_5_pct: -156.59850084010262
Average q (qsum/M+1):  47.031439012096776
Optimal xi:  [-28.733486]
Expected(across Rb) median(across samples) p_equity:  0.1998487651348114
obj fun:  tensor(6538.1629, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 8.9990, -0.3262],
        [ 9.8876, -0.4440],
        [ 9.4356, -0.5183],
        [ 9.7646, -4.6575],
        [ 8.9441, -0.4609],
        [ 0.3071, -7.9568],
        [ 8.7874, -0.5348],
        [-6.7191, -8.6281],
        [ 8.0896, -6.6798],
        [ 9.3436, -0.4676]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-8.3322, -7.9010, -7.8695, -5.6807, -8.1613, -6.3807, -8.2523, -6.3050,
        -6.0354, -7.9570], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.3758,  0.6926,  0.5690,  0.8044,  0.3903,  0.5214,  0.3247,  0.5116,
          0.7126,  0.5315],
        [ 2.7444,  5.4900,  3.4247,  4.6953,  2.8366,  4.3708,  2.5155,  5.5614,
          4.7626,  3.3188],
        [ 2.9186,  5.9124,  3.6715,  4.7250,  2.8336,  4.5040,  2.7602,  5.4868,
          5.0707,  3.3905],
        [ 2.9834,  6.0099,  3.6442,  5.2626,  2.9586,  5.5675,  2.7053,  6.6458,
          5.7662,  3.5071],
        [ 2.9057,  5.9133,  3.5257,  4.7828,  2.8734,  4.5245,  2.8143,  5.4756,
          5.0249,  3.5348],
        [ 3.0205,  5.9295,  3.6933,  5.0304,  3.0448,  4.9405,  2.7271,  6.2282,
          5.5398,  3.5624],
        [ 2.9056,  6.0650,  3.6122,  5.2804,  2.9497,  5.7353,  2.7194,  6.8885,
          5.8421,  3.4720],
        [ 0.3890,  0.7511,  0.6053,  0.8280,  0.4049,  0.5395,  0.3345,  0.6620,
          0.7597,  0.5626],
        [ 0.3877,  0.7486,  0.6033,  0.8252,  0.4036,  0.5374,  0.3334,  0.6595,
          0.7571,  0.5607],
        [-0.1744, -0.3843, -0.2829, -0.4794, -0.1638, -0.2872, -0.1176, -0.2960,
         -0.4880, -0.2616]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.9646, -9.3253, -9.2669, -8.5739, -9.2580, -8.8415, -8.4978,  4.4932,
         4.4795, -2.0682], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 7.4341, -5.2359, -5.8376, -7.5516, -5.9498, -6.8216, -7.5259,  5.0033,
          4.9051, -0.0246]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 8.4683e+00, -2.0167e-01],
        [ 1.1353e+01,  2.8206e+00],
        [ 3.2721e-01,  1.1209e+01],
        [-1.0832e+01, -4.6031e+00],
        [ 1.1318e+01,  2.8317e+00],
        [-1.1295e+01, -4.5147e+00],
        [ 9.0600e+00,  5.6569e+00],
        [-9.4756e+00,  4.3600e-03],
        [-1.0676e+01, -3.2558e+00],
        [ 5.9589e+00,  8.3174e+00]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-9.1974, -0.7285,  9.6826,  0.3683, -0.7407, -3.7073,  8.4888,  7.3284,
        -0.7428,  5.7239], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.8248e+00, -3.7009e+00, -1.6972e+01,  9.6545e+00, -3.2806e+00,
          1.4198e+00, -6.0537e+00,  6.0587e+00,  5.5822e+00, -1.2052e+01],
        [ 2.6859e-01,  1.5936e+00,  3.1093e+01, -1.0472e+01,  1.1667e+00,
          4.3199e-02,  2.5539e+00, -4.4719e+00, -8.1149e+00,  1.4017e+00],
        [-1.6968e+00, -2.7066e+00, -9.5378e-01, -5.5234e-01, -3.2704e+00,
         -2.8383e-03, -5.7126e-01, -2.6409e+00, -7.5603e-02,  4.3110e-02],
        [ 2.3349e-01, -2.3148e+00, -1.3560e+00,  2.1443e-01, -2.7326e+00,
         -2.1235e-02, -7.6739e-01, -2.4080e+00,  1.9538e-01, -7.5645e-01],
        [ 2.9510e-01, -2.1712e+00, -7.2625e-01, -1.7381e-01, -2.1573e+00,
         -5.5465e-02, -1.8791e+00, -1.8790e+00, -9.4138e-03, -1.0685e+00],
        [-4.7655e-01, -4.2180e+00,  8.1559e-01,  1.5024e+00, -4.1929e+00,
          3.4347e+00, -2.0415e+00,  2.6442e-01,  2.0524e+00, -1.3025e+00],
        [ 3.0924e-01, -1.4373e+00,  2.4077e+00,  4.6505e-02, -1.4234e+00,
         -2.1666e+00, -8.2834e-01,  1.1130e+00, -1.9177e+00,  1.1523e+00],
        [-6.1451e-01, -3.1931e+00,  1.0854e+00,  1.7979e+00, -3.1592e+00,
          3.8853e+00, -1.6575e+00,  6.4426e-01,  2.3141e+00, -1.0119e+00],
        [-2.3635e+00, -3.2829e+00, -2.1386e+00, -1.1463e+00, -3.3565e+00,
         -5.2687e-03,  2.0511e-01, -2.6414e+00, -8.6978e-02,  8.3263e-01],
        [-9.5220e+00, -1.4338e+01, -9.6207e+00,  1.4071e+01, -1.3813e+01,
          9.6557e+00, -1.9712e+01,  7.2483e+00,  1.2334e+01, -2.4767e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 0.1687, -2.3780, -3.8904, -3.1639, -2.9617, -3.8232, -2.7553, -3.5565,
        -4.0998, -2.6698], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -5.3770,   0.3848,  -0.2549,   0.4640,   0.2330,   1.2895,   1.1011,
           1.1877,  -0.4883,  19.8035],
        [  5.0610,  -0.4228,   0.2550,  -0.4232,  -0.2349,  -1.2884,  -1.0997,
          -1.1954,   0.6205, -19.8532]], device='cuda:0'))])
loaded xi:  -19.891674
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 806.3616944417624
W_T_median: 547.9700475945424
W_T_pctile_5: -287.9564157269737
W_T_CVAR_5_pct: -408.2820772432722
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  159.45246390812355
Current xi:  [-16.907404]
objective value function right now is: 159.45246390812355
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.789345]
objective value function right now is: 159.61597485849887
6.0% of gradient descent iterations done. Method = Adam
new min fval:  159.13885823023972
Current xi:  [-10.835787]
objective value function right now is: 159.13885823023972
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.503046]
objective value function right now is: 161.8866070802469
10.0% of gradient descent iterations done. Method = Adam
new min fval:  158.559006563371
Current xi:  [-10.524268]
objective value function right now is: 158.559006563371
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.981882]
objective value function right now is: 159.1986929486083
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  157.55476902666652
Current xi:  [-9.939548]
objective value function right now is: 157.55476902666652
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.064936]
objective value function right now is: 159.03252016798973
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.1513467]
objective value function right now is: 162.17639577827882
20.0% of gradient descent iterations done. Method = Adam
new min fval:  156.56184327508944
Current xi:  [-8.188091]
objective value function right now is: 156.56184327508944
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.630461]
objective value function right now is: 158.0494333084056
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.4100566]
objective value function right now is: 158.62427128056868
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.4580054]
objective value function right now is: 159.47567059935417
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-13.196458]
objective value function right now is: 162.32915362231702
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.135073]
objective value function right now is: 159.16032560483833
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-11.645851]
objective value function right now is: 160.24461794063436
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.294473]
objective value function right now is: 159.57677676192537
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.769154]
objective value function right now is: 158.25161787656847
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.0018435]
objective value function right now is: 160.67073072520674
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-32.098648]
objective value function right now is: 760.1295648919574
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-50.270325]
objective value function right now is: 372.60651739447144
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-51.643265]
objective value function right now is: 174.0426192578918
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-49.392338]
objective value function right now is: 170.89561390375806
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-46.12981]
objective value function right now is: 167.81602039910092
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-40.68613]
objective value function right now is: 162.98605490102537
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.43258]
objective value function right now is: 163.71259004879278
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-33.20175]
objective value function right now is: 168.171608835831
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-28.811867]
objective value function right now is: 164.61515701111134
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-24.483803]
objective value function right now is: 160.37863091581636
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-17.986282]
objective value function right now is: 158.60406510241842
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-16.601208]
objective value function right now is: 165.42701682049253
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.473844]
objective value function right now is: 167.8349374012608
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.485189]
objective value function right now is: 158.3620193807719
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.522484]
objective value function right now is: 159.06966683332104
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.742258]
objective value function right now is: 159.29358122577642
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.00317]
objective value function right now is: 158.12649568684498
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-17.057182]
objective value function right now is: 157.97351572613113
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.893586]
objective value function right now is: 156.66945339520962
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.807013]
objective value function right now is: 156.69093616946012
80.0% of gradient descent iterations done. Method = Adam
new min fval:  156.36325837572593
Current xi:  [-13.8175955]
objective value function right now is: 156.36325837572593
82.0% of gradient descent iterations done. Method = Adam
new min fval:  156.21638202792877
Current xi:  [-12.670285]
objective value function right now is: 156.21638202792877
84.0% of gradient descent iterations done. Method = Adam
new min fval:  156.02823787078728
Current xi:  [-11.373762]
objective value function right now is: 156.02823787078728
86.0% of gradient descent iterations done. Method = Adam
new min fval:  155.95915963232065
Current xi:  [-10.464234]
objective value function right now is: 155.95915963232065
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.65976]
objective value function right now is: 157.28719035696278
90.0% of gradient descent iterations done. Method = Adam
new min fval:  155.65443231664517
Current xi:  [-8.994102]
objective value function right now is: 155.65443231664517
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.374442]
objective value function right now is: 156.4772538733331
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.118098]
objective value function right now is: 155.77856082673975
96.0% of gradient descent iterations done. Method = Adam
new min fval:  155.65393600150773
Current xi:  [-7.096751]
objective value function right now is: 155.65393600150773
new min fval from sgd:  155.40642274938406
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.813122]
objective value function right now is: 155.40642274938406
new min fval from sgd:  155.40503980649362
new min fval from sgd:  155.39319967022175
new min fval from sgd:  155.38527767266086
new min fval from sgd:  155.38464301073353
new min fval from sgd:  155.38111879081248
new min fval from sgd:  155.37634887161101
new min fval from sgd:  155.3758390593189
new min fval from sgd:  155.37406218297298
new min fval from sgd:  155.37400599027083
new min fval from sgd:  155.3737463034456
new min fval from sgd:  155.37165881638433
new min fval from sgd:  155.37036027530903
new min fval from sgd:  155.36967737625537
new min fval from sgd:  155.36928553153732
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.7024693]
objective value function right now is: 155.38812289361067
min fval:  155.36928553153732
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.1282, -0.2037],
        [ 0.1282, -0.2037],
        [ 0.1282, -0.2037],
        [ 0.1282, -0.2037],
        [ 0.1282, -0.2037],
        [ 0.1282, -0.2037],
        [ 0.1282, -0.2037],
        [ 0.1282, -0.2037],
        [ 0.1282, -0.2037],
        [ 0.1282, -0.2037]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([0.2224, 0.2224, 0.2224, 0.2224, 0.2224, 0.2224, 0.2224, 0.2224, 0.2224,
        0.2224], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([0.3905, 0.3905, 0.3905, 0.3905, 0.3905, 0.3905, 0.3905, 0.3905, 0.3905,
        0.3905], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.7472, -1.7472, -1.7472, -1.7472, -1.7472, -1.7472, -1.7472, -1.7472,
         -1.7472, -1.7472]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  9.8174,  -1.6034],
        [ 12.0545,   2.5254],
        [ -1.1796,  12.3359],
        [-13.4835,  -4.3006],
        [  3.2181,   1.4356],
        [-15.9986,  -5.4463],
        [  4.4441,   1.1157],
        [-13.2292,   1.4708],
        [-13.8757,  -4.0713],
        [  7.3626,   9.0976]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-11.6182,  -4.7818,   9.8889,  -0.4936,  -8.3806,  -2.7343,   4.8146,
         10.7266,  -2.5440,   7.5393], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.6684e+00, -2.2871e+00, -2.0545e+01,  8.2722e+00, -1.6077e-02,
          2.4095e+00, -5.2193e+00,  5.7160e+00,  8.4159e+00, -8.2838e+00],
        [ 5.8126e+00,  9.2314e-01,  1.8644e+01, -1.7999e-01,  2.0884e-02,
         -7.1690e-01,  1.0493e+00, -6.3780e-01, -2.5574e+00,  3.4668e+00],
        [-5.8411e-01, -2.5672e+00, -3.3905e-01, -2.3352e-03, -1.5436e-02,
         -1.0897e-01, -2.6909e+00, -4.9261e-01, -5.3546e-02, -1.0929e+00],
        [ 1.2550e+00, -1.9938e+00, -4.2891e-01,  6.0963e-01, -2.4487e-02,
         -1.7663e-01, -2.6811e+00,  2.8978e-01, -2.9041e-02, -1.0602e+00],
        [-2.1175e+00, -4.7579e+00, -7.4954e+00,  5.9746e+00,  1.3040e-02,
         -1.7371e+00, -1.1539e-01,  1.9147e-01, -9.1106e-01, -4.4576e+00],
        [ 2.4966e+00, -6.5540e-01,  6.1113e-01,  1.0620e+00,  7.2426e-02,
          1.4587e-01,  2.4785e+00,  2.9927e+00,  8.2486e-02,  2.9396e+00],
        [ 2.9772e+00,  4.5625e+00,  8.0308e+00, -1.6699e+00,  7.3003e-02,
         -4.5381e+00,  5.8312e-01,  3.3303e+00,  2.9318e+00, -2.0307e-01],
        [-3.8217e-01, -1.2486e+00, -2.1570e-01, -1.1157e-01, -3.5568e-02,
         -5.8359e-03, -2.3491e+00, -9.1399e-01, -4.6665e-03, -1.0984e+00],
        [-3.6715e+00, -1.6564e+00,  3.3219e+00, -1.6191e-01, -2.7923e+00,
          1.4253e+00, -2.0535e+00,  7.8591e-01,  1.7219e+00,  3.4475e-01],
        [-2.1103e+00, -8.1808e+00, -9.7804e+00,  1.1960e+01,  7.0534e-03,
          1.4793e+01, -1.7303e+01,  8.9654e+00,  1.0312e+01, -2.3915e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 0.8384, -3.3416, -2.7293, -2.6861, -0.1292,  2.4806, -1.2031, -2.3529,
        -2.0582, -3.0457], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-5.1986e+00,  8.0444e-01, -5.0082e-01,  4.1430e-02, -3.3386e+00,
         -2.5710e+00,  2.1753e+00,  1.1993e-02,  1.2824e+00,  1.4855e+01],
        [ 4.9282e+00, -8.4239e-01,  5.0085e-01, -4.1315e-02,  3.3386e+00,
          2.5711e+00, -2.1739e+00, -1.1994e-02, -1.2824e+00, -1.4867e+01]],
       device='cuda:0'))])
xi:  [-6.7090735]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 775.1800624912019
W_T_median: 559.1541402249793
W_T_pctile_5: -5.036659543574745
W_T_CVAR_5_pct: -155.34841710653225
Average q (qsum/M+1):  35.0
Optimal xi:  [-6.7090735]
Expected(across Rb) median(across samples) p_equity:  0.17343460358679294
obj fun:  tensor(155.3693, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 999.0
-----------------------------------------------
89e-02, -2.4633e+00, -1.4028e+00, -4.8493e-02, -9.2111e-01],
        [-5.3850e-01, -1.1312e+00, -2.9587e-01, -1.1937e-01,  7.2552e-02,
         -3.1636e-02, -2.4710e+00, -1.4085e+00, -4.7581e-02, -9.1539e-01],
        [ 2.8874e+00, -3.0003e+00,  6.0043e+00, -7.3770e-01,  9.1971e-01,
          4.1934e+00, -2.9448e+00,  3.0577e+00,  2.7602e+00,  1.2433e+00],
        [-5.5083e-01, -1.1445e+00, -2.9874e-01, -1.2272e-01,  7.2207e-02,
         -3.2523e-02, -2.4595e+00, -1.4004e+00, -4.9050e-02, -9.2352e-01],
        [-5.3875e-01, -1.1315e+00, -2.9596e-01, -1.1945e-01,  7.2539e-02,
         -3.1652e-02, -2.4709e+00, -1.4080e+00, -4.7609e-02, -9.1556e-01],
        [-5.7069e+00, -7.1318e-01, -5.5678e+00,  1.3053e+01, -1.0203e-02,
          1.0547e+01, -1.6362e+01,  7.7176e+00,  1.0953e+01, -2.6905e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-0.8836, -2.1125, -2.4618, -2.4595, -2.4639, -2.4716, -4.8326, -2.4601,
        -2.4715, -2.8249], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -5.8658,   0.3860,  -0.0695,  -0.0699,  -0.0690,  -0.0683,   0.6403,
          -0.0696,  -0.0683,  12.2162],
        [  5.5842,  -0.4240,   0.0695,   0.0702,   0.0690,   0.0683,  -0.6390,
           0.0696,   0.0684, -12.2215]], device='cuda:0'))])
xi:  [-3.5765848]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 776.0808280474937
W_T_median: 579.1305196298968
W_T_pctile_5: -3.408453257737037
W_T_CVAR_5_pct: -155.4721746159828
Average q (qsum/M+1):  35.0
Optimal xi:  [-3.5765848]
Expected(across Rb) median(across samples) p_equity:  0.1793644712617
obj fun:  tensor(155.4715, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 999.0
-----------------------------------------------
