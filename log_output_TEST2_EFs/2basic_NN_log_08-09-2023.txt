/home/marcchen/Documents/constrain_factor/researchcode/exp_config_json_files/multi_portfolio_TEST2_EFs.json
Starting at: 
09-08-23_18:33

 numpy seed:  3  


 pytorch seed:  3  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_nom_ret', 'VWD_nom_ret']
############# End: defined asset  basket #################
               CPI_nom_ret  B10_nom_ret  VWD_nom_ret
My identifier                                       
192607           -0.011299     0.005383     0.031411
192608           -0.005714     0.005363     0.028647
192609            0.005747     0.005343     0.005787
192610            0.005714     0.005323    -0.028996
192611            0.005682     0.005303     0.028554
               CPI_nom_ret  B10_nom_ret  VWD_nom_ret
My identifier                                       
202208           -0.000354    -0.043289    -0.036240
202209            0.002151    -0.050056    -0.091324
202210            0.004056    -0.014968     0.077403
202211           -0.001010     0.040789     0.052365
202212           -0.003070    -0.018566    -0.057116
Indices constructed with column names:
Index(['date_for_plt', 'CPI_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
B10_real_ret    0.001637
VWD_real_ret    0.006759
dtype: float64


timeseries_basket['data_df_stdev'] = 
B10_real_ret    0.019258
VWD_real_ret    0.053610
dtype: float64


timeseries_basket['data_df_corr'] = 
              B10_real_ret  VWD_real_ret
B10_real_ret      1.000000      0.090987
VWD_real_ret      0.090987      1.000000


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 192707
End: 199112
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 1500, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 806.3616944417624
W_T_median: 547.9700475945424
W_T_pctile_5: -287.9564157269737
W_T_CVAR_5_pct: -408.2820772432722
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1635.086902630562
Current xi:  [79.01002]
objective value function right now is: -1635.086902630562
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1643.57931738985
Current xi:  [56.46451]
objective value function right now is: -1643.57931738985
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1649.9005004432315
Current xi:  [33.216232]
objective value function right now is: -1649.9005004432315
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1655.422395920591
Current xi:  [9.880102]
objective value function right now is: -1655.422395920591
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1660.7426031237358
Current xi:  [-11.538735]
objective value function right now is: -1660.7426031237358
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1665.6704005935537
Current xi:  [-33.291275]
objective value function right now is: -1665.6704005935537
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1669.5896535254435
Current xi:  [-54.08203]
objective value function right now is: -1669.5896535254435
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.8520020646733
Current xi:  [-75.45924]
objective value function right now is: -1673.8520020646733
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1677.2060705604913
Current xi:  [-96.858826]
objective value function right now is: -1677.2060705604913
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1681.003564060161
Current xi:  [-117.91548]
objective value function right now is: -1681.003564060161
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1684.289623492536
Current xi:  [-139.47525]
objective value function right now is: -1684.289623492536
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.978316785537
Current xi:  [-160.46246]
objective value function right now is: -1686.978316785537
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1688.9170924114362
Current xi:  [-181.46536]
objective value function right now is: -1688.9170924114362
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1692.6849988172219
Current xi:  [-202.55928]
objective value function right now is: -1692.6849988172219
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1694.5643371103868
Current xi:  [-223.32613]
objective value function right now is: -1694.5643371103868
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1697.1807062992636
Current xi:  [-244.44713]
objective value function right now is: -1697.1807062992636
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1699.392746988928
Current xi:  [-265.03088]
objective value function right now is: -1699.392746988928
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1701.3500965398523
Current xi:  [-285.78568]
objective value function right now is: -1701.3500965398523
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1703.3043278431808
Current xi:  [-306.00073]
objective value function right now is: -1703.3043278431808
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1705.0254734335267
Current xi:  [-326.28497]
objective value function right now is: -1705.0254734335267
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1706.273711443502
Current xi:  [-345.9548]
objective value function right now is: -1706.273711443502
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-365.7963]
objective value function right now is: -1703.794124368847
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1708.0810005472842
Current xi:  [-385.23886]
objective value function right now is: -1708.0810005472842
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1709.2289828690416
Current xi:  [-404.77338]
objective value function right now is: -1709.2289828690416
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1710.1899904470151
Current xi:  [-422.72226]
objective value function right now is: -1710.1899904470151
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1711.2423914698204
Current xi:  [-440.61453]
objective value function right now is: -1711.2423914698204
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1711.531110448861
Current xi:  [-458.21475]
objective value function right now is: -1711.531110448861
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-475.3402]
objective value function right now is: -1711.489833810286
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1712.679970958414
Current xi:  [-491.64658]
objective value function right now is: -1712.679970958414
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-507.13782]
objective value function right now is: -1712.6696052273246
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-521.3781]
objective value function right now is: -1711.6862995894812
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1713.0071077658035
Current xi:  [-534.30994]
objective value function right now is: -1713.0071077658035
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-545.9444]
objective value function right now is: -1712.8762352086037
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1713.5056901855703
Current xi:  [-555.8257]
objective value function right now is: -1713.5056901855703
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-562.7009]
objective value function right now is: -1713.236532311076
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1713.8641592574854
Current xi:  [-564.7681]
objective value function right now is: -1713.8641592574854
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-566.464]
objective value function right now is: -1713.739391328509
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-568.03503]
objective value function right now is: -1713.8506384534137
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-569.6522]
objective value function right now is: -1713.65069961373
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1714.005249381857
Current xi:  [-571.31903]
objective value function right now is: -1714.005249381857
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-572.73914]
objective value function right now is: -1713.9155120918272
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-573.55707]
objective value function right now is: -1713.56789665749
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1714.0248647846174
Current xi:  [-574.6801]
objective value function right now is: -1714.0248647846174
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1714.027879019967
Current xi:  [-576.33856]
objective value function right now is: -1714.027879019967
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-577.10815]
objective value function right now is: -1713.9745411903355
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-577.3818]
objective value function right now is: -1713.8035511647827
94.0% of gradient descent iterations done. Method = Adam
new min fval:  -1714.0895163423727
Current xi:  [-578.4089]
objective value function right now is: -1714.0895163423727
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-578.86127]
objective value function right now is: -1713.9504573395704
new min fval from sgd:  -1714.0901201088489
new min fval from sgd:  -1714.0919838936359
new min fval from sgd:  -1714.0934370334867
new min fval from sgd:  -1714.093464208399
new min fval from sgd:  -1714.0939869149076
new min fval from sgd:  -1714.0942573491404
new min fval from sgd:  -1714.0950481261207
new min fval from sgd:  -1714.0951324437458
new min fval from sgd:  -1714.0968423544662
new min fval from sgd:  -1714.0974506566647
new min fval from sgd:  -1714.097464955223
new min fval from sgd:  -1714.0984690202
new min fval from sgd:  -1714.1015486459019
new min fval from sgd:  -1714.1030916818859
new min fval from sgd:  -1714.104087413215
new min fval from sgd:  -1714.1053040019713
new min fval from sgd:  -1714.1055481177552
new min fval from sgd:  -1714.1057851494738
new min fval from sgd:  -1714.1060533803002
new min fval from sgd:  -1714.1077848741077
new min fval from sgd:  -1714.109187946046
new min fval from sgd:  -1714.1103128429966
new min fval from sgd:  -1714.111376668086
new min fval from sgd:  -1714.1115911635432
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-579.30804]
objective value function right now is: -1714.0925961520677
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-579.2948]
objective value function right now is: -1714.0785755873671
min fval:  -1714.1115911635432
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  8.3291,   2.5980],
        [-10.9143,   3.1158],
        [ -0.5507,   5.5431],
        [ -0.4369,   1.2990],
        [ -0.4368,   1.2989],
        [ 10.3673,   0.1053],
        [-11.5174,  -0.9996],
        [ -8.0726,   3.2617],
        [ -2.7845,   5.0630],
        [-11.6111,  -2.4033]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-4.6538, 11.1410,  9.6888, -0.6066, -0.6064, -9.0740,  7.4315,  9.2687,
         6.6586,  6.1055], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.9340e-03, -3.1754e-02, -4.9736e-02, -3.7896e-03, -3.7906e-03,
         -1.8412e-01, -2.5657e-01, -2.3966e-02, -5.8865e-03, -4.6493e-01],
        [-4.9340e-03, -3.1754e-02, -4.9736e-02, -3.7896e-03, -3.7906e-03,
         -1.8412e-01, -2.5657e-01, -2.3966e-02, -5.8865e-03, -4.6493e-01],
        [-2.2149e+00, -1.1376e+01, -4.9074e+00,  7.7220e-02,  8.0244e-02,
         -8.2611e+00,  5.1733e+00, -6.0865e+00, -1.5785e+00,  2.6575e+00],
        [-8.8113e-03,  3.6279e-02, -3.0679e-02, -8.3831e-04, -8.4062e-04,
         -2.0593e-01, -3.8407e-01,  2.8128e-02,  2.6723e-02, -6.5987e-01],
        [-4.9340e-03, -3.1754e-02, -4.9736e-02, -3.7896e-03, -3.7906e-03,
         -1.8412e-01, -2.5657e-01, -2.3966e-02, -5.8865e-03, -4.6493e-01],
        [-4.9340e-03, -3.1754e-02, -4.9736e-02, -3.7896e-03, -3.7906e-03,
         -1.8412e-01, -2.5657e-01, -2.3966e-02, -5.8865e-03, -4.6493e-01],
        [ 1.0884e+00,  1.2496e+00,  3.3881e+00,  2.4735e-01,  2.4744e-01,
          4.6153e+00, -3.3253e+00,  1.2051e+00,  1.1673e+00, -2.0030e+00],
        [ 1.0840e+00,  7.6239e+00,  2.9677e+00, -7.0166e-02, -6.6195e-02,
          4.9384e+00, -3.3243e+00,  3.7749e+00,  5.6802e-01, -1.5313e+00],
        [-4.9340e-03, -3.1754e-02, -4.9736e-02, -3.7896e-03, -3.7906e-03,
         -1.8412e-01, -2.5657e-01, -2.3966e-02, -5.8865e-03, -4.6493e-01],
        [-4.9340e-03, -3.1754e-02, -4.9736e-02, -3.7896e-03, -3.7906e-03,
         -1.8412e-01, -2.5657e-01, -2.3966e-02, -5.8865e-03, -4.6493e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5763, -0.5763,  0.4856, -0.7874, -0.5763, -0.5763,  0.0523, -0.3634,
        -0.5763, -0.5763], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0179,  -0.0179, -12.2065,   0.0803,  -0.0179,  -0.0179,   5.7268,
           9.0119,  -0.0179,  -0.0179]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  2.6510,   0.5979],
        [-19.9968,  -4.2400],
        [  6.4777,  -8.5174],
        [ -9.2885,   0.5769],
        [ -5.2933,  -9.2161],
        [  1.2250,   3.2564],
        [ -4.9727,  -8.2752],
        [-14.7304,  -8.7815],
        [ -7.6193,  -8.7379],
        [ 11.2951,   0.7806]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 3.9507,  1.7668, -6.7286,  6.6624, -6.2252,  9.5741, -1.7535, -2.9705,
        -9.3956, -8.6394], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.2539e+00, -5.0783e-01, -7.5870e-01, -6.3318e-01, -5.1906e-01,
         -1.2551e+00, -5.9917e-01, -5.1099e-01, -5.1013e-01, -6.5063e-01],
        [-1.2539e+00, -5.0783e-01, -7.5870e-01, -6.3318e-01, -5.1906e-01,
         -1.2551e+00, -5.9917e-01, -5.1099e-01, -5.1013e-01, -6.5063e-01],
        [ 1.0546e+00,  1.3900e-01, -1.8596e-02, -9.4333e+00,  2.5528e+00,
         -6.7410e-01,  6.6261e+00, -9.9898e-01, -1.0245e+00, -4.1148e-01],
        [-2.2891e+00,  1.9794e+00,  2.5814e+00, -2.0292e+00, -7.4826e-02,
         -5.5142e+00,  2.5230e+00, -3.2179e-01, -2.6244e+00, -3.4882e+00],
        [-1.2539e+00, -5.0783e-01, -7.5870e-01, -6.3318e-01, -5.1906e-01,
         -1.2551e+00, -5.9917e-01, -5.1099e-01, -5.1013e-01, -6.5063e-01],
        [-6.8317e+00,  8.9282e+00, -4.7597e+00,  2.2703e+00,  2.2955e-02,
          3.7282e+00, -4.4660e+00,  2.7073e+00,  9.5292e+00, -1.0563e+00],
        [-1.1642e+00, -9.0362e+00,  3.5402e+00, -2.7558e+00,  3.3685e+00,
          8.0518e-01,  1.6032e-01,  1.2301e+01, -1.1420e+01, -1.0720e+01],
        [-2.6715e+00,  2.6582e+00,  9.0334e-01, -1.5554e+00,  1.9235e-01,
         -3.8641e+00,  4.9046e-01, -4.9865e-02,  4.9874e-01, -2.0604e+00],
        [-2.6527e+00,  3.0702e+00,  1.0535e+00, -1.9015e+00,  2.4950e-01,
         -4.8953e+00,  8.0484e-01,  4.2303e-02,  2.2215e-01, -2.2828e+00],
        [-4.5574e+00,  2.2389e+00, -4.2595e+00,  4.2975e+00, -1.7121e+00,
          4.3587e-03,  1.8559e+00, -1.5239e+00,  8.4788e+00,  1.2389e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.8016, -1.8016,  1.6147,  2.1199, -1.8016, -4.4295, -4.0668,  0.6076,
         0.8080,  0.7418], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 4.7473e-03,  4.7481e-03, -2.4154e-01, -2.7771e+00,  4.7474e-03,
          4.4321e+00, -7.8538e+00, -7.6842e-01, -1.6523e+00,  1.0302e+00],
        [-4.7442e-03, -4.7434e-03,  3.7042e-01,  2.7719e+00, -4.7441e-03,
         -4.3528e+00,  7.9120e+00,  7.8944e-01,  1.9649e+00, -1.1766e+00]],
       device='cuda:0'))])
xi:  [-579.3133]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 219.66433090482124
W_T_median: 77.66515546040269
W_T_pctile_5: -577.5539875516268
W_T_CVAR_5_pct: -685.1076916878615
Average q (qsum/M+1):  56.398953345514116
Optimal xi:  [-579.3133]
Expected(across Rb) median(across samples) p_equity:  0.3456930866581388
obj fun:  tensor(-1714.1116, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 806.3616944417624
W_T_median: 547.9700475945424
W_T_pctile_5: -287.9564157269737
W_T_CVAR_5_pct: -408.2820772432722
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1511.6104782079587
Current xi:  [76.90953]
objective value function right now is: -1511.6104782079587
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1532.6087720056544
Current xi:  [54.890728]
objective value function right now is: -1532.6087720056544
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1545.8371054686859
Current xi:  [33.027607]
objective value function right now is: -1545.8371054686859
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1557.9046296284178
Current xi:  [11.580561]
objective value function right now is: -1557.9046296284178
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1566.2303199082282
Current xi:  [-6.621911]
objective value function right now is: -1566.2303199082282
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1573.607431909998
Current xi:  [-26.141624]
objective value function right now is: -1573.607431909998
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1580.1229783115723
Current xi:  [-44.871582]
objective value function right now is: -1580.1229783115723
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1585.992670273163
Current xi:  [-65.1844]
objective value function right now is: -1585.992670273163
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1590.6928484695181
Current xi:  [-83.45966]
objective value function right now is: -1590.6928484695181
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1597.463962706867
Current xi:  [-103.549286]
objective value function right now is: -1597.463962706867
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1601.2024096744494
Current xi:  [-122.41638]
objective value function right now is: -1601.2024096744494
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1606.7940831722597
Current xi:  [-141.91498]
objective value function right now is: -1606.7940831722597
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1610.3951976599158
Current xi:  [-161.62498]
objective value function right now is: -1610.3951976599158
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1614.107709700678
Current xi:  [-180.31158]
objective value function right now is: -1614.107709700678
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1615.8159996492425
Current xi:  [-199.42874]
objective value function right now is: -1615.8159996492425
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1618.276776396425
Current xi:  [-216.80573]
objective value function right now is: -1618.276776396425
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1621.6574690330629
Current xi:  [-235.29811]
objective value function right now is: -1621.6574690330629
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1623.6826224133358
Current xi:  [-252.53008]
objective value function right now is: -1623.6826224133358
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1624.170725562701
Current xi:  [-268.46048]
objective value function right now is: -1624.170725562701
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1625.4678850473672
Current xi:  [-285.53687]
objective value function right now is: -1625.4678850473672
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1627.1660925584658
Current xi:  [-299.6912]
objective value function right now is: -1627.1660925584658
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-312.66028]
objective value function right now is: -1626.9185333073815
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1628.348973639927
Current xi:  [-326.46536]
objective value function right now is: -1628.348973639927
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1628.5283806913644
Current xi:  [-338.309]
objective value function right now is: -1628.5283806913644
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1628.6645964463544
Current xi:  [-346.76703]
objective value function right now is: -1628.6645964463544
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1629.18657207286
Current xi:  [-353.25214]
objective value function right now is: -1629.18657207286
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-358.7371]
objective value function right now is: -1628.9051387554277
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-363.41833]
objective value function right now is: -1629.012711950492
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-366.69974]
objective value function right now is: -1628.5744977077577
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-371.3603]
objective value function right now is: -1629.045419270012
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-374.08838]
objective value function right now is: -1628.4300148512725
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-375.76114]
objective value function right now is: -1628.4091175035737
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-377.93332]
objective value function right now is: -1628.90734325945
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-377.95883]
objective value function right now is: -1628.08607886235
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-377.92624]
objective value function right now is: -1627.6919039733373
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1629.6941542047039
Current xi:  [-377.99545]
objective value function right now is: -1629.6941542047039
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-378.0611]
objective value function right now is: -1629.6379894690801
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1629.709067343188
Current xi:  [-378.27975]
objective value function right now is: -1629.709067343188
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-378.1834]
objective value function right now is: -1629.0686169089072
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-378.39648]
objective value function right now is: -1629.4103774338953
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-378.20142]
objective value function right now is: -1629.6947349736447
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-377.8032]
objective value function right now is: -1629.5873769799812
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-377.808]
objective value function right now is: -1629.5602455638661
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1629.7234676715252
Current xi:  [-377.84442]
objective value function right now is: -1629.7234676715252
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-377.68265]
objective value function right now is: -1629.6791952752365
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-378.16595]
objective value function right now is: -1629.649427655716
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-378.1475]
objective value function right now is: -1629.599629469377
new min fval from sgd:  -1629.7434998548297
new min fval from sgd:  -1629.7676210788386
new min fval from sgd:  -1629.7783426376154
new min fval from sgd:  -1629.7789620798126
new min fval from sgd:  -1629.7797699408563
new min fval from sgd:  -1629.783490585468
new min fval from sgd:  -1629.7899564791053
new min fval from sgd:  -1629.7941749110398
new min fval from sgd:  -1629.7967848526807
new min fval from sgd:  -1629.7970082268077
new min fval from sgd:  -1629.7985023583963
new min fval from sgd:  -1629.7990845197176
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-378.06415]
objective value function right now is: -1629.5416156736608
new min fval from sgd:  -1629.7992489208946
new min fval from sgd:  -1629.8016164497622
new min fval from sgd:  -1629.8035119606857
new min fval from sgd:  -1629.804772748127
new min fval from sgd:  -1629.8060352294488
new min fval from sgd:  -1629.8067449201674
new min fval from sgd:  -1629.8073049403974
new min fval from sgd:  -1629.8081275864415
new min fval from sgd:  -1629.8087559156631
new min fval from sgd:  -1629.8101192144027
new min fval from sgd:  -1629.811437027631
new min fval from sgd:  -1629.8128235534687
new min fval from sgd:  -1629.814022166422
new min fval from sgd:  -1629.8152033678796
new min fval from sgd:  -1629.8158397650313
new min fval from sgd:  -1629.8163047941225
new min fval from sgd:  -1629.816739278671
new min fval from sgd:  -1629.817966597178
new min fval from sgd:  -1629.8203748472342
new min fval from sgd:  -1629.8213876749417
new min fval from sgd:  -1629.8228054191761
new min fval from sgd:  -1629.8242681145518
new min fval from sgd:  -1629.8256418037747
new min fval from sgd:  -1629.826787014371
new min fval from sgd:  -1629.8277058211352
new min fval from sgd:  -1629.8279309995307
new min fval from sgd:  -1629.8283039285752
new min fval from sgd:  -1629.8286760037313
new min fval from sgd:  -1629.8288858139938
new min fval from sgd:  -1629.828967837596
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-378.33435]
objective value function right now is: -1629.8070492771276
new min fval from sgd:  -1629.829227530199
new min fval from sgd:  -1629.8300882136732
new min fval from sgd:  -1629.8307663148257
new min fval from sgd:  -1629.831393444392
new min fval from sgd:  -1629.8317717101543
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-378.33267]
objective value function right now is: -1629.8165911952592
min fval:  -1629.8317717101543
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.5906,  1.1402],
        [-0.4646,  1.3674],
        [-0.4645,  1.3675],
        [-4.7194,  6.5969],
        [ 9.3994,  5.8082],
        [-2.4023,  5.1090],
        [ 5.0249, -5.9394],
        [ 4.6763, -6.0849],
        [-0.4651,  1.3654],
        [-3.4212,  5.9183]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-1.0202, -0.3923, -0.3921,  9.0984,  0.2720,  4.5587, -8.5056, -8.4395,
        -0.4094,  6.9428], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.6370e-02, -2.2554e-02, -2.2556e-02, -4.6004e-02, -6.8282e-02,
         -5.0222e-03, -3.7125e-01, -3.8052e-01, -2.2286e-02, -1.6750e-02],
        [ 1.3788e-01,  1.7011e-01,  1.6990e-01, -4.1686e+00, -6.6768e+00,
         -5.0473e-01,  1.9242e+00,  2.0444e+00,  1.7045e-01, -1.7101e+00],
        [-1.6370e-02, -2.2554e-02, -2.2556e-02, -4.6004e-02, -6.8282e-02,
         -5.0222e-03, -3.7125e-01, -3.8052e-01, -2.2286e-02, -1.6750e-02],
        [-1.6370e-02, -2.2554e-02, -2.2556e-02, -4.6004e-02, -6.8282e-02,
         -5.0222e-03, -3.7125e-01, -3.8052e-01, -2.2286e-02, -1.6750e-02],
        [ 1.2943e-01,  1.7090e-01,  1.7089e-01, -2.9485e+00, -4.8113e+00,
         -1.9197e-01,  1.3001e+00,  1.5542e+00,  1.6990e-01, -1.0071e+00],
        [ 1.3772e-01,  1.8027e-01,  1.8026e-01, -3.1204e+00, -5.0727e+00,
         -2.1977e-01,  1.4245e+00,  1.5912e+00,  1.7944e-01, -1.0954e+00],
        [ 7.3878e-01,  1.1727e+00,  1.1727e+00,  7.6141e+00, -5.5191e-01,
          2.1442e+00, -3.0615e+00, -3.2007e+00,  1.1575e+00,  4.2224e+00],
        [ 1.3013e-01,  1.6123e-01,  1.6098e-01, -4.3167e+00, -6.7971e+00,
         -5.4510e-01,  1.7674e+00,  2.1538e+00,  1.6167e-01, -1.7903e+00],
        [-3.9700e-01, -5.9459e-01, -5.9498e-01,  7.8406e+00,  9.1614e+00,
          1.2183e+00, -4.0018e+00, -3.5711e+00, -5.8294e-01,  3.6100e+00],
        [-1.6370e-02, -2.2554e-02, -2.2556e-02, -4.6006e-02, -6.8283e-02,
         -5.0219e-03, -3.7127e-01, -3.8054e-01, -2.2286e-02, -1.6750e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5269, -1.0287, -0.5269, -0.5269, -1.1328, -1.0980,  2.1840, -0.9143,
         0.9753, -0.5269], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 7.6037e-03, -4.5540e+00,  7.6038e-03,  7.6038e-03, -2.9527e+00,
         -3.1414e+00,  5.8156e+00, -4.6906e+00,  9.0797e+00,  7.6043e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -9.2351,   1.2901],
        [ 11.4166,   7.0197],
        [  8.0824,  10.0747],
        [ -4.4740,  12.3592],
        [  9.4918,  -0.3888],
        [ 13.7282,  -1.1878],
        [-11.9599,  -1.2071],
        [ -1.5266,   8.1732],
        [ -3.2559,  -0.6451],
        [-13.6630,  -2.9513]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 6.0749,  5.9292, 10.1918,  9.4573, -9.8402, -6.9670,  6.9883,  7.2792,
        -4.2315,  0.7100], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[  4.0976, -11.3669,  -7.5908,  -2.7081,  -4.1743,  -5.3898,   4.1525,
          -5.0175,   4.6680,   9.1984],
        [  1.5140,  -0.4624,   4.6455, -14.2474,  -2.2942,  -5.1231,   1.1181,
          -5.3619,   6.9438,   6.0044],
        [  2.5183,  -0.1568,   0.6007,  -5.0887,  -3.1092,  -1.0994,  -0.4825,
          -0.4158,  -0.2532,  -2.9750],
        [  5.2276,  -5.8393,  -6.3071,   3.9687,  -4.6886,  -3.3679,   1.3050,
          -1.9598,  -3.4409,   4.1182],
        [ -0.0807,  -1.0119,  -0.5902,  -0.1541,  -0.6134,  -1.7325,  -0.6027,
          -0.1840,  -0.1409,  -0.1505],
        [  0.5422,  -4.9282, -14.0385,  -1.2102,  -5.7751,  -2.6010,   6.4240,
           2.0494,   1.1494,   9.2318],
        [ -9.6918,  -0.1644,  -0.2586,  -0.7706,  -0.3209,  -1.4650,   4.1419,
           2.1803,  -0.3032,   0.3917],
        [  1.8077,  -2.6123,  -0.7823,   9.1331,  -7.0373,  -3.6259,  -2.7457,
           0.1268,  -0.1077,   2.6287],
        [ -0.0807,  -1.0122,  -0.5901,  -0.1541,  -0.6134,  -1.7311,  -0.6026,
          -0.1840,  -0.1409,  -0.1506],
        [ -0.0807,  -1.0119,  -0.5902,  -0.1541,  -0.6134,  -1.7322,  -0.6026,
          -0.1840,  -0.1409,  -0.1505]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.2425, -1.7795, -1.5169, -1.1066, -1.7523, -0.6360, -0.1135, -3.3491,
        -1.7536, -1.7526], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 7.4527, -3.8996, -2.4345,  6.2850, -0.0236, -3.2394, -0.3515,  0.8180,
         -0.0235, -0.0235],
        [-7.3384,  3.8299,  2.4341, -6.2854,  0.0235,  3.4898,  0.3792, -1.0005,
          0.0236,  0.0236]], device='cuda:0'))])
xi:  [-378.32812]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 213.17206254425574
W_T_median: 62.74300891997442
W_T_pctile_5: -378.18300209267085
W_T_CVAR_5_pct: -468.8862228440009
Average q (qsum/M+1):  55.60029060609879
Optimal xi:  [-378.32812]
Expected(across Rb) median(across samples) p_equity:  0.35717100338079033
obj fun:  tensor(-1629.8318, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 806.3616944417624
W_T_median: 547.9700475945424
W_T_pctile_5: -287.9564157269737
W_T_CVAR_5_pct: -408.2820772432722
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1377.5161196274473
Current xi:  [77.4883]
objective value function right now is: -1377.5161196274473
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1438.378494353569
Current xi:  [54.877098]
objective value function right now is: -1438.378494353569
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1456.102496011885
Current xi:  [34.67059]
objective value function right now is: -1456.102496011885
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1467.0018006338855
Current xi:  [15.675986]
objective value function right now is: -1467.0018006338855
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1476.813915537712
Current xi:  [-1.8571911]
objective value function right now is: -1476.813915537712
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.224216]
objective value function right now is: -1471.1396776330998
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1490.7005425439143
Current xi:  [-33.06843]
objective value function right now is: -1490.7005425439143
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1493.9830837698546
Current xi:  [-46.532467]
objective value function right now is: -1493.9830837698546
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1498.942475924435
Current xi:  [-63.66731]
objective value function right now is: -1498.942475924435
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1503.0627907829678
Current xi:  [-78.3206]
objective value function right now is: -1503.0627907829678
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-92.53545]
objective value function right now is: -1483.5156221969241
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-115.27187]
objective value function right now is: -1493.4365306058298
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1507.5301726605637
Current xi:  [-128.83247]
objective value function right now is: -1507.5301726605637
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-143.20491]
objective value function right now is: -1506.847905343354
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1508.7888957483876
Current xi:  [-158.61725]
objective value function right now is: -1508.7888957483876
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1510.0936221788888
Current xi:  [-162.25528]
objective value function right now is: -1510.0936221788888
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1510.7966323020266
Current xi:  [-166.24849]
objective value function right now is: -1510.7966323020266
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-168.36942]
objective value function right now is: -1509.8815378974666
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1511.3845526295047
Current xi:  [-169.86882]
objective value function right now is: -1511.3845526295047
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1511.4436814976227
Current xi:  [-170.24588]
objective value function right now is: -1511.4436814976227
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-176.17036]
objective value function right now is: -1489.1746239951124
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1515.0852741782423
Current xi:  [-195.88002]
objective value function right now is: -1515.0852741782423
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-207.59534]
objective value function right now is: -1512.68536900896
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1517.458442589093
Current xi:  [-213.10475]
objective value function right now is: -1517.458442589093
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-215.26707]
objective value function right now is: -1516.8344762294294
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-216.56451]
objective value function right now is: -1517.3289869779003
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1517.5023494135298
Current xi:  [-216.15495]
objective value function right now is: -1517.5023494135298
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-215.79556]
objective value function right now is: -1517.2655355861675
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1518.1843764024225
Current xi:  [-216.3179]
objective value function right now is: -1518.1843764024225
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-216.09686]
objective value function right now is: -1517.1938278606376
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1518.212551097702
Current xi:  [-216.96178]
objective value function right now is: -1518.212551097702
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-216.89156]
objective value function right now is: -1518.0626764922824
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1518.3080363843815
Current xi:  [-217.76527]
objective value function right now is: -1518.3080363843815
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-217.13046]
objective value function right now is: -1517.4009367639037
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-216.76974]
objective value function right now is: -1516.9800864926856
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1518.7584830577312
Current xi:  [-216.48068]
objective value function right now is: -1518.7584830577312
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-215.91243]
objective value function right now is: -1518.6280642315382
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1518.8328015558238
Current xi:  [-215.43845]
objective value function right now is: -1518.8328015558238
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-215.31523]
objective value function right now is: -1518.8029845030173
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-214.80505]
objective value function right now is: -1518.6217282534974
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-214.715]
objective value function right now is: -1518.7374369143479
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1519.1419550852045
Current xi:  [-214.5016]
objective value function right now is: -1519.1419550852045
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-214.23781]
objective value function right now is: -1519.0816659821505
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-214.12216]
objective value function right now is: -1519.0949294170878
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-214.05882]
objective value function right now is: -1519.0458383797884
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-214.15056]
objective value function right now is: -1519.0198070942365
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-214.09901]
objective value function right now is: -1518.6752411076022
new min fval from sgd:  -1519.149530283846
new min fval from sgd:  -1519.1612809161848
new min fval from sgd:  -1519.1651959852286
new min fval from sgd:  -1519.177896495124
new min fval from sgd:  -1519.182593569841
new min fval from sgd:  -1519.1851830328685
new min fval from sgd:  -1519.1891379792892
new min fval from sgd:  -1519.1909877768214
new min fval from sgd:  -1519.192939172858
new min fval from sgd:  -1519.1935106642952
new min fval from sgd:  -1519.197685530132
new min fval from sgd:  -1519.204768434633
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-214.10117]
objective value function right now is: -1519.204768434633
new min fval from sgd:  -1519.2106869019904
new min fval from sgd:  -1519.2118572122429
new min fval from sgd:  -1519.2217134934128
new min fval from sgd:  -1519.2236814333364
new min fval from sgd:  -1519.2247494973788
new min fval from sgd:  -1519.2289395117364
new min fval from sgd:  -1519.2336985656534
new min fval from sgd:  -1519.2371629157622
new min fval from sgd:  -1519.2390819830732
new min fval from sgd:  -1519.2421620110665
new min fval from sgd:  -1519.2452392395503
new min fval from sgd:  -1519.2488830518403
new min fval from sgd:  -1519.2521624415415
new min fval from sgd:  -1519.254732442323
new min fval from sgd:  -1519.2564194450363
new min fval from sgd:  -1519.2569947901522
new min fval from sgd:  -1519.2575843061027
new min fval from sgd:  -1519.2580433058474
new min fval from sgd:  -1519.2582946137147
new min fval from sgd:  -1519.2589687955178
new min fval from sgd:  -1519.259195531977
new min fval from sgd:  -1519.2604478796145
new min fval from sgd:  -1519.2629475329277
new min fval from sgd:  -1519.2652633837695
new min fval from sgd:  -1519.2679133257243
new min fval from sgd:  -1519.2698800887222
new min fval from sgd:  -1519.2710873399524
new min fval from sgd:  -1519.272410681513
new min fval from sgd:  -1519.2731960785175
new min fval from sgd:  -1519.2744549332044
new min fval from sgd:  -1519.2759524568771
new min fval from sgd:  -1519.277466694967
new min fval from sgd:  -1519.2782152270554
new min fval from sgd:  -1519.278602934239
new min fval from sgd:  -1519.2792953398582
new min fval from sgd:  -1519.2796028961789
new min fval from sgd:  -1519.2798195200405
new min fval from sgd:  -1519.2799798021624
new min fval from sgd:  -1519.279993562493
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-213.98737]
objective value function right now is: -1519.2258474615278
new min fval from sgd:  -1519.2806073217928
new min fval from sgd:  -1519.2807236654446
new min fval from sgd:  -1519.2811965836217
new min fval from sgd:  -1519.2826857920595
new min fval from sgd:  -1519.2831647000178
new min fval from sgd:  -1519.28377754038
new min fval from sgd:  -1519.28509900301
new min fval from sgd:  -1519.2860369285886
new min fval from sgd:  -1519.286454491502
new min fval from sgd:  -1519.2866061014888
new min fval from sgd:  -1519.2871058829055
new min fval from sgd:  -1519.2883111654023
new min fval from sgd:  -1519.2895637094844
new min fval from sgd:  -1519.2913697322326
new min fval from sgd:  -1519.2921786873594
new min fval from sgd:  -1519.2928218718905
new min fval from sgd:  -1519.2934767264233
new min fval from sgd:  -1519.2940626031655
new min fval from sgd:  -1519.2952277639947
new min fval from sgd:  -1519.2960501709176
new min fval from sgd:  -1519.2963803728287
new min fval from sgd:  -1519.2963896874235
new min fval from sgd:  -1519.2989154528138
new min fval from sgd:  -1519.2998829884748
new min fval from sgd:  -1519.3026031037095
new min fval from sgd:  -1519.3031490628507
new min fval from sgd:  -1519.3047914280028
new min fval from sgd:  -1519.3055809061327
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-214.05185]
objective value function right now is: -1519.2166270707419
min fval:  -1519.3055809061327
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-2.1266, -9.6534],
        [-0.6908,  1.1105],
        [-0.6984,  1.1383],
        [ 5.4211, -6.2916],
        [ 9.2782, -1.2672],
        [ 0.2658, -6.6060],
        [-0.6907,  1.1101],
        [ 5.4829,  2.5801],
        [-0.6625,  1.0449],
        [-5.8168,  6.3408]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-9.0682, -1.3384, -1.3576, -7.7013, -9.6391, -7.3639, -1.3382, -4.7008,
        -1.2893,  7.5648], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-7.8969e-02,  1.5747e-02,  1.5136e-02, -6.5277e-01, -1.0118e-01,
         -8.4202e-02,  1.5755e-02, -1.9525e-02,  1.7284e-02, -1.3822e-01],
        [-7.8969e-02,  1.5747e-02,  1.5136e-02, -6.5277e-01, -1.0118e-01,
         -8.4202e-02,  1.5755e-02, -1.9525e-02,  1.7284e-02, -1.3822e-01],
        [-8.0828e+00, -3.3488e-02, -4.6688e-02, -5.3898e+00, -6.5066e+00,
         -3.0717e+00, -3.3305e-02, -2.9631e+00,  5.1241e-03,  8.8722e+00],
        [-7.8969e-02,  1.5747e-02,  1.5136e-02, -6.5277e-01, -1.0118e-01,
         -8.4202e-02,  1.5755e-02, -1.9525e-02,  1.7284e-02, -1.3822e-01],
        [-7.8969e-02,  1.5747e-02,  1.5136e-02, -6.5277e-01, -1.0118e-01,
         -8.4202e-02,  1.5755e-02, -1.9525e-02,  1.7284e-02, -1.3822e-01],
        [ 7.8920e+00, -7.9517e-02, -9.3921e-02,  3.8812e+00,  2.8668e+00,
          2.9368e+00, -7.9223e-02,  6.0597e-02, -4.4835e-02, -8.4863e+00],
        [ 7.3427e+00, -4.1262e-02, -4.8049e-02,  3.2629e+00,  2.8236e+00,
          2.7181e+00, -4.1268e-02,  1.4673e-01, -1.3906e-02, -7.8884e+00],
        [-7.8969e-02,  1.5747e-02,  1.5136e-02, -6.5277e-01, -1.0119e-01,
         -8.4202e-02,  1.5755e-02, -1.9526e-02,  1.7284e-02, -1.3822e-01],
        [-7.8969e-02,  1.5747e-02,  1.5136e-02, -6.5277e-01, -1.0118e-01,
         -8.4202e-02,  1.5755e-02, -1.9525e-02,  1.7284e-02, -1.3822e-01],
        [-7.8969e-02,  1.5747e-02,  1.5136e-02, -6.5277e-01, -1.0118e-01,
         -8.4202e-02,  1.5755e-02, -1.9525e-02,  1.7284e-02, -1.3822e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.9726, -0.9726,  3.1663, -0.9726, -0.9726, -4.1908, -3.8689, -0.9726,
        -0.9726, -0.9726], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 0.0381,  0.0381, 12.9832,  0.0381,  0.0381, -6.8815, -6.6144,  0.0381,
          0.0381,  0.0381]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.7300,  -0.7870],
        [  9.2801,   1.1326],
        [  9.0367,  10.1016],
        [ -3.1516,  10.0784],
        [  2.3247,   0.8208],
        [-11.7507,  -1.1677],
        [-13.0906,  -2.8926],
        [ -6.9221,   8.1108],
        [-13.5360,  -3.0443],
        [ -7.0228,   0.7083]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-6.0429, -1.4217,  8.9347,  6.9009,  5.5366,  5.3312, -0.2267, 10.6779,
        -0.0942,  3.5876], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-6.3549e-01, -1.5398e+00, -7.6998e-01, -6.0599e-02, -1.5907e+00,
         -8.3249e-01, -6.4769e-01, -9.9376e-02, -6.4889e-01, -5.7241e-02],
        [ 8.3788e+00, -4.8435e+00,  1.1244e+01, -1.8043e+01, -4.9603e+00,
         -6.7051e-01,  1.6170e+00,  3.2350e+00,  2.0860e+00,  1.7554e+00],
        [-2.4183e-01, -3.8635e+00, -1.4372e+01,  1.2802e+00,  1.8432e+00,
          9.7712e+00,  5.9837e+00, -1.3836e+01,  6.1682e+00,  1.0811e+00],
        [-9.1475e-01, -1.5798e+00, -8.3977e-01,  6.6396e-01, -1.6353e+00,
         -7.9985e-01, -9.3424e-01,  8.2127e-01, -9.3739e-01, -4.6987e-01],
        [-8.0602e+00, -2.7174e+00,  2.2651e+00,  3.0439e+00,  1.1964e+00,
         -4.2148e-02,  2.7522e+00, -4.9508e+00,  2.9658e+00,  1.5106e+00],
        [ 1.7055e-01, -1.9357e+00, -1.2558e+00, -5.9936e-01,  1.4333e+00,
         -1.0935e+01,  1.5985e+00, -1.0653e+01,  1.7374e+00, -4.3156e+00],
        [-2.6879e-02, -5.8407e+00, -2.5269e+01, -6.9401e+00, -5.1455e+00,
          1.0162e+01,  1.0790e+01,  7.6856e-01,  1.1653e+01, -3.3247e+00],
        [-4.0052e+00, -3.1503e+00, -4.1078e+00,  7.7954e-01,  1.2698e-01,
          9.0391e-02,  5.7280e-03,  1.0730e+00,  2.0299e-01,  1.7726e+00],
        [-3.1028e-02, -3.2811e+00, -9.0132e-02,  3.2048e+00, -3.4615e+00,
         -1.0825e+01, -7.3573e+00,  5.3745e+00, -8.0334e+00,  1.9339e+01],
        [-1.2546e+00, -1.1291e+00, -3.3490e-01,  1.3796e+00, -1.2748e+00,
         -5.1430e-01, -1.2560e+00,  1.5910e+00, -1.2583e+00, -2.2991e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.4704, -5.5677, -2.5239, -2.7611, -3.3938, -1.6316, -3.2730, -3.5661,
        -4.6048, -2.4416], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-0.0210, -4.2073, -4.8426, -0.2355,  2.6770,  3.8116,  8.8541,  1.1238,
          0.3888, -0.8380],
        [ 0.0098,  4.2597,  4.7261,  0.2345, -2.4912, -3.5284, -9.1335, -1.1255,
         -0.6015,  0.8376]], device='cuda:0'))])
xi:  [-214.05327]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 255.0572863384238
W_T_median: 47.739805567600165
W_T_pctile_5: -214.1485317687192
W_T_CVAR_5_pct: -303.3106454792431
Average q (qsum/M+1):  53.901961788054436
Optimal xi:  [-214.05327]
Expected(across Rb) median(across samples) p_equity:  0.34733749511651696
obj fun:  tensor(-1519.3056, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 806.3616944417624
W_T_median: 547.9700475945424
W_T_pctile_5: -287.9564157269737
W_T_CVAR_5_pct: -408.2820772432722
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -819.9782845287913
Current xi:  [84.41179]
objective value function right now is: -819.9782845287913
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1260.4647843108976
Current xi:  [62.560043]
objective value function right now is: -1260.4647843108976
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1341.8147836665767
Current xi:  [40.77805]
objective value function right now is: -1341.8147836665767
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1355.053914969714
Current xi:  [21.78129]
objective value function right now is: -1355.053914969714
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1366.549810908107
Current xi:  [5.0319395]
objective value function right now is: -1366.549810908107
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.70177]
objective value function right now is: -1307.9471023470041
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1379.4932281426886
Current xi:  [-21.316765]
objective value function right now is: -1379.4932281426886
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1382.7666937739968
Current xi:  [-34.374702]
objective value function right now is: -1382.7666937739968
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-42.457764]
objective value function right now is: -1377.1077728491794
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1387.4055196260551
Current xi:  [-51.99372]
objective value function right now is: -1387.4055196260551
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1390.238716350922
Current xi:  [-61.743107]
objective value function right now is: -1390.238716350922
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1392.3730313781132
Current xi:  [-71.64326]
objective value function right now is: -1392.3730313781132
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1392.5260592425016
Current xi:  [-77.77138]
objective value function right now is: -1392.5260592425016
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-79.79808]
objective value function right now is: -1392.3686634744072
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-80.779335]
objective value function right now is: -1391.2848015030609
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-82.69778]
objective value function right now is: -1390.6743738905773
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1395.0222439395004
Current xi:  [-83.25553]
objective value function right now is: -1395.0222439395004
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-83.373535]
objective value function right now is: -1390.9758100790953
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-84.013214]
objective value function right now is: -1394.7394764570747
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-88.16138]
objective value function right now is: -1393.329645318814
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1395.3082754451088
Current xi:  [-92.14921]
objective value function right now is: -1395.3082754451088
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-96.07322]
objective value function right now is: -1395.1938847919168
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1395.3363305463392
Current xi:  [-99.580505]
objective value function right now is: -1395.3363305463392
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1395.829653986461
Current xi:  [-102.85296]
objective value function right now is: -1395.829653986461
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-104.35769]
objective value function right now is: -1392.095470315041
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-108.82449]
objective value function right now is: -1389.023022561073
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1396.9880304763785
Current xi:  [-111.01523]
objective value function right now is: -1396.9880304763785
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-114.23006]
objective value function right now is: -1392.5868007852446
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-114.55674]
objective value function right now is: -1394.8510380928751
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-114.17373]
objective value function right now is: -1396.1953212384005
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1397.6480796461683
Current xi:  [-115.39841]
objective value function right now is: -1397.6480796461683
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-116.71843]
objective value function right now is: -1396.5803156686918
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-116.96428]
objective value function right now is: -1396.7304356544819
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-115.4724]
objective value function right now is: -1396.7657090772964
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-115.97573]
objective value function right now is: -1394.2245991057255
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1398.9127228107172
Current xi:  [-115.72332]
objective value function right now is: -1398.9127228107172
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-115.34942]
objective value function right now is: -1398.3513938226788
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1398.9917740091257
Current xi:  [-114.943405]
objective value function right now is: -1398.9917740091257
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-114.520966]
objective value function right now is: -1398.8981681526318
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-114.279236]
objective value function right now is: -1398.9797320740536
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-114.12503]
objective value function right now is: -1398.5673508827383
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1399.0588787224856
Current xi:  [-113.7301]
objective value function right now is: -1399.0588787224856
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-113.68379]
objective value function right now is: -1398.1336913073785
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1399.168439459903
Current xi:  [-113.22618]
objective value function right now is: -1399.168439459903
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-113.06883]
objective value function right now is: -1398.2983128575972
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -1399.1895266889105
Current xi:  [-113.00058]
objective value function right now is: -1399.1895266889105
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-113.07865]
objective value function right now is: -1398.7559945442802
new min fval from sgd:  -1399.2730398087012
new min fval from sgd:  -1399.3211028927842
new min fval from sgd:  -1399.3261692587268
new min fval from sgd:  -1399.3575320859622
new min fval from sgd:  -1399.3653577375546
new min fval from sgd:  -1399.380829686719
new min fval from sgd:  -1399.3873198877536
new min fval from sgd:  -1399.4019185898617
new min fval from sgd:  -1399.416880797947
new min fval from sgd:  -1399.4368042739384
new min fval from sgd:  -1399.4454449630953
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-112.907166]
objective value function right now is: -1398.8876747911047
new min fval from sgd:  -1399.4458290564173
new min fval from sgd:  -1399.4598014732883
new min fval from sgd:  -1399.4625789942068
new min fval from sgd:  -1399.4751901943885
new min fval from sgd:  -1399.483136991444
new min fval from sgd:  -1399.4855975592895
new min fval from sgd:  -1399.488553318289
new min fval from sgd:  -1399.49135425739
new min fval from sgd:  -1399.5016686844747
new min fval from sgd:  -1399.512833749521
new min fval from sgd:  -1399.5248388502798
new min fval from sgd:  -1399.5355339126072
new min fval from sgd:  -1399.541183537267
new min fval from sgd:  -1399.5445481098436
new min fval from sgd:  -1399.5481635896904
new min fval from sgd:  -1399.5496976494042
new min fval from sgd:  -1399.5502819628734
new min fval from sgd:  -1399.5513507806036
new min fval from sgd:  -1399.5535444121356
new min fval from sgd:  -1399.558747955408
new min fval from sgd:  -1399.559897548088
new min fval from sgd:  -1399.5604901571876
new min fval from sgd:  -1399.5608292689697
new min fval from sgd:  -1399.561250741408
new min fval from sgd:  -1399.5663070792557
new min fval from sgd:  -1399.570405489964
new min fval from sgd:  -1399.5723946970081
new min fval from sgd:  -1399.5741999587635
new min fval from sgd:  -1399.5793636370477
new min fval from sgd:  -1399.5828392575513
new min fval from sgd:  -1399.5862621084857
new min fval from sgd:  -1399.5910755927298
new min fval from sgd:  -1399.5932251749334
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-112.760155]
objective value function right now is: -1399.3896525419389
new min fval from sgd:  -1399.5947200782334
new min fval from sgd:  -1399.598125526863
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-112.706375]
objective value function right now is: -1399.5364747397598
min fval:  -1399.598125526863
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-11.2737,   1.7007],
        [  7.6959,   4.2054],
        [  3.8779,  -7.6092],
        [  6.0393,  -0.7980],
        [  8.1322,  -3.0392],
        [  6.6049,  -3.7852],
        [  2.1530,  -5.7798],
        [ -1.3644,  -9.8628],
        [  3.5980,  -8.0926],
        [ -1.3651,   0.6012]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 9.3869, -8.6201, -7.2168, -8.4064, -6.5788, -6.1712, -6.6115, -8.5746,
        -7.3207, -2.4487], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-7.3577e-01, -2.7393e-01, -2.3036e-01, -9.9731e-03, -4.7064e-01,
         -2.9026e-01, -8.3096e-02, -2.0880e-01, -2.6129e-01, -1.7089e-02],
        [-7.3577e-01, -2.7393e-01, -2.3036e-01, -9.9731e-03, -4.7064e-01,
         -2.9026e-01, -8.3096e-02, -2.0880e-01, -2.6130e-01, -1.7089e-02],
        [-7.3578e-01, -2.7394e-01, -2.3037e-01, -9.9731e-03, -4.7066e-01,
         -2.9027e-01, -8.3096e-02, -2.0880e-01, -2.6130e-01, -1.7087e-02],
        [-7.3577e-01, -2.7393e-01, -2.3036e-01, -9.9731e-03, -4.7064e-01,
         -2.9026e-01, -8.3096e-02, -2.0880e-01, -2.6129e-01, -1.7089e-02],
        [-1.0324e+01,  4.8241e+00,  4.3859e+00,  1.3515e+00,  3.1793e+00,
          2.1795e+00,  2.3569e+00,  7.9397e+00,  5.0193e+00, -1.1420e-01],
        [ 9.0389e+00, -4.7819e+00, -5.4399e+00, -2.2211e+00, -4.9436e+00,
         -3.1159e+00, -2.4649e+00, -9.1474e+00, -6.2830e+00, -1.0520e-01],
        [-7.3577e-01, -2.7393e-01, -2.3036e-01, -9.9731e-03, -4.7064e-01,
         -2.9026e-01, -8.3096e-02, -2.0880e-01, -2.6129e-01, -1.7089e-02],
        [-7.3577e-01, -2.7393e-01, -2.3036e-01, -9.9731e-03, -4.7064e-01,
         -2.9026e-01, -8.3096e-02, -2.0880e-01, -2.6129e-01, -1.7089e-02],
        [-9.6513e+00,  4.2907e+00,  4.1033e+00,  1.0119e+00,  2.9200e+00,
          2.0450e+00,  2.0683e+00,  7.4801e+00,  4.7041e+00, -1.2013e-01],
        [-8.7257e+00,  3.6455e+00,  3.7203e+00,  5.4290e-01,  2.6100e+00,
          1.7815e+00,  1.8002e+00,  6.6603e+00,  4.3647e+00, -1.2229e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.3169, -1.3169, -1.3171, -1.3169, -2.0545,  1.7691, -1.3169, -1.3169,
        -2.0951, -2.1880], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 0.0206,  0.0206,  0.0206,  0.0206, -7.7263, 13.2028,  0.0206,  0.0206,
         -6.5518, -5.1478]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  3.8228,  12.8336],
        [-13.3933,  -3.5186],
        [-10.3617,   0.2013],
        [ -6.2571,  15.3401],
        [ 12.7997,   4.0283],
        [ -7.8908,  12.0735],
        [ 11.7991,   2.8889],
        [  1.0779,  10.9936],
        [  8.1666,   8.7960],
        [-10.8444,  -1.8217]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([11.5308, -1.1118,  9.1239, 12.1809,  0.4775, 13.5917, -0.1118,  8.1238,
         7.6146,  1.7803], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 16.4445,  -1.4587, -13.9325,   0.7999,   2.2539,  12.7710,   4.5543,
           3.3750,   5.4250,  -9.0998],
        [ -0.6059,  -0.0900,  -1.2078,  -0.3471,  -1.6763,  -0.3871,  -1.7719,
          -0.4395,  -0.9491,  -0.3178],
        [  1.6752,   5.2328,  -1.8167, -16.0969,  -1.4204,   0.4261,  -1.9227,
          -2.6335,   0.9736,   1.1307],
        [  0.7216,  -0.1432,  -3.3983,  -0.9198,  -1.3730,  -0.9233,  -1.1609,
          -0.2923,  -0.9186,   0.0290],
        [  6.4699,   0.8734,  -1.8170,  -3.7577,  -3.0543,   8.5373,  -1.7796,
          -4.6219,  -0.3323,  -4.2928],
        [-14.7198,  10.3976,   6.2988,  -7.6778,  -2.8306,  -8.9073,  -4.8137,
          -7.0763,  -5.6585,   8.4990],
        [ -0.6066,  -0.0892,  -1.2012,  -0.3466,  -1.6712,  -0.3862,  -1.7563,
          -0.4385,  -0.9532,  -0.3182],
        [ -7.2569,  10.8954,   5.1379,  -1.6679,  -9.5597,   2.1443,  -9.0123,
          -2.5082, -23.5249,   8.5497],
        [  0.4047,   0.1861,  -3.2898,  -1.8702,  -1.6130,  -1.4351,  -1.8069,
           1.6671,   0.5666,   0.5384],
        [ -0.6085,  -0.0899,  -1.2019,  -0.3470,  -1.6747,  -0.3869,  -1.7694,
          -0.4384,  -0.9518,  -0.3184]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-0.1311, -1.9625, -2.0240, -2.6627, -4.3165,  0.2241, -1.9823, -1.5650,
        -2.4324, -1.9631], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.5137,  -0.1205,   2.5344,   0.5210,  -4.4510,  -5.6714,  -0.1072,
          11.0627,   1.2164,  -0.1196],
        [ -0.1021,   0.1215,  -2.5333,  -0.5880,   4.3878,   5.7438,   0.1351,
         -11.0546,  -1.2174,   0.1213]], device='cuda:0'))])
xi:  [-112.769135]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 466.31844192388985
W_T_median: 107.6441752249072
W_T_pctile_5: -112.30957322175772
W_T_CVAR_5_pct: -210.0789135347396
Average q (qsum/M+1):  51.92521027595766
Optimal xi:  [-112.769135]
Expected(across Rb) median(across samples) p_equity:  0.375542355577151
obj fun:  tensor(-1399.5981, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 806.3616944417624
W_T_median: 547.9700475945424
W_T_pctile_5: -287.9564157269737
W_T_CVAR_5_pct: -408.2820772432722
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -722.7232796877479
Current xi:  [82.67301]
objective value function right now is: -722.7232796877479
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -796.6003712217592
Current xi:  [67.37557]
objective value function right now is: -796.6003712217592
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -825.3395260459941
Current xi:  [54.49869]
objective value function right now is: -825.3395260459941
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1115.2444810265629
Current xi:  [27.002666]
objective value function right now is: -1115.2444810265629
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1265.5755790398994
Current xi:  [0.2399859]
objective value function right now is: -1265.5755790398994
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1277.9925740033443
Current xi:  [-8.14646]
objective value function right now is: -1277.9925740033443
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1282.7755156227695
Current xi:  [-17.889345]
objective value function right now is: -1282.7755156227695
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-38.042065]
objective value function right now is: -1190.8108636139034
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-46.957634]
objective value function right now is: -1279.0605828567516
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1288.0919840710085
Current xi:  [-51.100266]
objective value function right now is: -1288.0919840710085
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-55.177555]
objective value function right now is: -1274.9462695011266
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1294.012728642546
Current xi:  [-58.50045]
objective value function right now is: -1294.012728642546
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-62.821106]
objective value function right now is: -1292.4793772493792
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1294.4997183717478
Current xi:  [-66.745735]
objective value function right now is: -1294.4997183717478
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-72.48584]
objective value function right now is: -1293.926394550712
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.35774]
objective value function right now is: -1291.6615787267672
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-76.15236]
objective value function right now is: -1261.2951072326393
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1294.9966426331985
Current xi:  [-75.832535]
objective value function right now is: -1294.9966426331985
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1296.0885139448737
Current xi:  [-76.05993]
objective value function right now is: -1296.0885139448737
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-86.1324]
objective value function right now is: -1194.9159547105958
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-86.356674]
objective value function right now is: -1233.342127973693
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-86.647194]
objective value function right now is: -1287.5808750970273
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-78.07054]
objective value function right now is: -1285.3177227732199
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-83.05903]
objective value function right now is: -1290.7664800880366
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-88.7311]
objective value function right now is: -1268.3077694530398
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-89.69045]
objective value function right now is: -1258.8871054851027
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-83.4679]
objective value function right now is: -1289.8492910237549
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-77.56893]
objective value function right now is: -1287.3626667401606
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-75.93942]
objective value function right now is: -1294.8639493022654
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.75806]
objective value function right now is: -1294.974611117241
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.70531]
objective value function right now is: -1291.8504158454298
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.539536]
objective value function right now is: -1294.0970387227846
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.71121]
objective value function right now is: -1295.870780780001
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1296.138549272609
Current xi:  [-76.48709]
objective value function right now is: -1296.138549272609
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -1296.152708589986
Current xi:  [-75.63682]
objective value function right now is: -1296.152708589986
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1298.102718356418
Current xi:  [-75.63315]
objective value function right now is: -1298.102718356418
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.69715]
objective value function right now is: -1297.0569646045533
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1298.4668855731284
Current xi:  [-75.65844]
objective value function right now is: -1298.4668855731284
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1298.9537894995674
Current xi:  [-75.70389]
objective value function right now is: -1298.9537894995674
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.580696]
objective value function right now is: -1298.5338560252753
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.61714]
objective value function right now is: -1298.4979654021236
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.62461]
objective value function right now is: -1297.824857805967
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.46983]
objective value function right now is: -1298.38661101831
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1299.324554431708
Current xi:  [-75.26568]
objective value function right now is: -1299.324554431708
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.43136]
objective value function right now is: -1298.5352255321293
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -1299.4537815574226
Current xi:  [-75.34234]
objective value function right now is: -1299.4537815574226
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.35915]
objective value function right now is: -1298.6261599235468
new min fval from sgd:  -1299.6548036939078
new min fval from sgd:  -1299.664122579972
new min fval from sgd:  -1299.671233485782
new min fval from sgd:  -1299.7096093384453
new min fval from sgd:  -1299.7337351882757
new min fval from sgd:  -1299.8396733286604
new min fval from sgd:  -1299.9328345770818
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.203224]
objective value function right now is: -1298.6963834653004
new min fval from sgd:  -1299.956609212867
new min fval from sgd:  -1299.956719771426
new min fval from sgd:  -1299.9653679518215
new min fval from sgd:  -1299.9774722383886
new min fval from sgd:  -1299.9869468327233
new min fval from sgd:  -1299.9966893600651
new min fval from sgd:  -1300.0044452798047
new min fval from sgd:  -1300.0075367746829
new min fval from sgd:  -1300.0102056474834
new min fval from sgd:  -1300.012374300802
new min fval from sgd:  -1300.0125803479064
new min fval from sgd:  -1300.0199956022839
new min fval from sgd:  -1300.030702127269
new min fval from sgd:  -1300.0407192003688
new min fval from sgd:  -1300.0546149823447
new min fval from sgd:  -1300.078861091213
new min fval from sgd:  -1300.0986622561006
new min fval from sgd:  -1300.1143014722231
new min fval from sgd:  -1300.1275478381967
new min fval from sgd:  -1300.139806532763
new min fval from sgd:  -1300.1477272197212
new min fval from sgd:  -1300.1559127949092
new min fval from sgd:  -1300.162113729456
new min fval from sgd:  -1300.164464842665
new min fval from sgd:  -1300.1650193779776
new min fval from sgd:  -1300.1668578744213
new min fval from sgd:  -1300.1688834965983
new min fval from sgd:  -1300.172015710807
new min fval from sgd:  -1300.1763171266284
new min fval from sgd:  -1300.180860706347
new min fval from sgd:  -1300.1851473829379
new min fval from sgd:  -1300.1889363551543
new min fval from sgd:  -1300.1943326855762
new min fval from sgd:  -1300.200965842225
new min fval from sgd:  -1300.2088607504731
new min fval from sgd:  -1300.2170934074902
new min fval from sgd:  -1300.2233880583285
new min fval from sgd:  -1300.2280308579425
new min fval from sgd:  -1300.2299584825305
new min fval from sgd:  -1300.2343439113142
new min fval from sgd:  -1300.2386367220824
new min fval from sgd:  -1300.241074153997
new min fval from sgd:  -1300.2414284805293
new min fval from sgd:  -1300.2424959673835
new min fval from sgd:  -1300.243182738656
new min fval from sgd:  -1300.2452965531993
new min fval from sgd:  -1300.247110239922
new min fval from sgd:  -1300.2475528685457
new min fval from sgd:  -1300.247736450047
new min fval from sgd:  -1300.25248590373
new min fval from sgd:  -1300.2565022455235
new min fval from sgd:  -1300.258384041271
new min fval from sgd:  -1300.258396469972
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.236946]
objective value function right now is: -1300.1482179811965
new min fval from sgd:  -1300.2609868116904
new min fval from sgd:  -1300.2676883615386
new min fval from sgd:  -1300.2709015997275
new min fval from sgd:  -1300.2739244333557
new min fval from sgd:  -1300.2792102765338
new min fval from sgd:  -1300.283374620904
new min fval from sgd:  -1300.2870825446155
new min fval from sgd:  -1300.2905702456092
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.22202]
objective value function right now is: -1300.1528856379578
min fval:  -1300.2905702456092
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 2.0782, -3.3748],
        [ 3.6322, -5.2499],
        [-4.7881, -4.3868],
        [-3.7356, -1.3681],
        [-2.8116,  7.1122],
        [-3.9416, -7.7328],
        [ 5.7990, -6.7866],
        [-2.3129,  7.5458],
        [-7.0882, -0.1975],
        [ 9.3054, -0.8755]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-5.7839, -5.7502,  2.8614,  1.3385,  5.6225, -6.4654, -6.0063,  5.8090,
         5.4883, -7.3852], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-6.5834e-03, -4.0416e-02, -2.7958e-01, -1.9983e-01, -6.2054e-01,
         -3.7264e-02, -1.7267e-01, -6.2414e-01, -3.6743e-01, -3.0721e-01],
        [-1.0062e+00, -2.7133e+00,  6.8169e-01,  5.2598e-01,  5.5480e+00,
         -9.1629e+00, -1.2526e+01,  5.5823e+00,  4.3679e+00, -2.1864e+01],
        [-6.5835e-03, -4.0415e-02, -2.7958e-01, -1.9983e-01, -6.2053e-01,
         -3.7264e-02, -1.7266e-01, -6.2413e-01, -3.6743e-01, -3.0721e-01],
        [-6.5835e-03, -4.0415e-02, -2.7958e-01, -1.9983e-01, -6.2053e-01,
         -3.7264e-02, -1.7266e-01, -6.2413e-01, -3.6743e-01, -3.0721e-01],
        [-6.5835e-03, -4.0415e-02, -2.7958e-01, -1.9983e-01, -6.2053e-01,
         -3.7264e-02, -1.7266e-01, -6.2413e-01, -3.6743e-01, -3.0721e-01],
        [-6.5835e-03, -4.0415e-02, -2.7958e-01, -1.9983e-01, -6.2053e-01,
         -3.7264e-02, -1.7266e-01, -6.2413e-01, -3.6743e-01, -3.0721e-01],
        [-6.5835e-03, -4.0415e-02, -2.7958e-01, -1.9983e-01, -6.2053e-01,
         -3.7264e-02, -1.7266e-01, -6.2413e-01, -3.6743e-01, -3.0721e-01],
        [-6.5835e-03, -4.0415e-02, -2.7958e-01, -1.9983e-01, -6.2053e-01,
         -3.7264e-02, -1.7266e-01, -6.2413e-01, -3.6743e-01, -3.0721e-01],
        [ 9.2725e-01,  3.0141e+00, -8.0068e-01, -9.3097e-01, -6.5384e+00,
          4.4545e+00,  6.3406e+00, -6.6441e+00, -4.7694e-01,  2.7555e+00],
        [ 1.4154e+00,  2.1595e+00, -1.1626e+00,  3.6426e-01, -4.0944e+00,
          6.0315e+00,  5.2631e+00, -4.4536e+00, -3.9567e+00,  1.5376e+01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8259,  1.6491, -0.8259, -0.8259, -0.8259, -0.8259, -0.8259, -0.8259,
        -1.9749, -0.5597], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 5.4276e-03,  1.5268e+01,  5.4273e-03,  5.4273e-03,  5.4273e-03,
          5.4273e-03,  5.4272e-03,  5.4273e-03, -5.9489e+00, -1.1966e+01]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  3.6371,   6.1643],
        [-15.0258,   0.6421],
        [ -1.9023,  -2.7144],
        [-12.0596,  -3.9010],
        [  6.5253,  11.7922],
        [ 11.5826,   2.7289],
        [  0.2610,   0.2073],
        [  8.6589,   5.2815],
        [ 10.3234,   5.3365],
        [  8.9687,  -0.8116]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-1.7228,  1.8686, -6.9094,  3.5802,  8.8717, -1.4242,  5.1871, 12.5432,
         1.9315, -9.5336], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.1057e+00,  2.9148e-01, -2.3157e+00,  2.7817e+00,  1.6443e+00,
         -2.1026e+00, -1.8128e+00, -1.4526e+00, -2.3600e-01, -4.9287e+00],
        [ 1.1772e-01, -9.8050e-01,  7.0888e-01,  2.4365e-02, -1.5246e+00,
         -5.9696e-01, -1.1054e+00, -1.4821e+00, -9.6323e-01, -7.7558e-01],
        [ 7.9240e-01,  1.8398e+01,  4.4498e-01, -4.3744e+00,  1.1626e+01,
         -3.2706e-01, -3.6420e+00,  7.5307e+00, -5.7008e-01,  1.0184e+01],
        [ 5.1801e-01, -2.4722e+00,  2.3952e+00,  8.5212e-01, -2.2792e+00,
         -8.2271e-01, -1.1242e+00, -1.1672e+00, -1.3562e+00,  1.1681e+00],
        [-9.4008e-02,  6.0348e-02,  1.1153e-01, -1.1118e-01, -6.5956e-01,
         -8.1271e-01, -1.3531e+00, -2.3847e+00, -1.1465e+00, -5.6273e-01],
        [ 7.1072e-01, -2.3643e+00,  2.1314e+00,  1.5426e+00, -1.8355e+00,
          3.3729e-01, -3.1691e-01, -4.0449e+00, -6.8567e-01,  7.0388e-01],
        [-1.7492e-01,  6.5083e+00, -8.4441e-01,  1.8478e+01, -2.8964e+01,
         -1.0505e+01,  7.6249e+00, -2.0907e+01, -1.9410e+01, -1.0448e+01],
        [ 9.7469e-01,  3.6810e+00,  1.1398e-01,  1.7192e+00,  9.1586e+00,
         -3.9237e+00, -1.7650e+00, -1.9252e+00, -2.9691e+00, -1.0113e+00],
        [ 1.3169e-01, -9.2704e-01,  7.0163e-01,  4.4098e-02, -1.5116e+00,
         -5.8148e-01, -1.2595e+00, -1.3966e+00, -9.6711e-01, -7.4675e-01],
        [-2.7950e+00, -3.8686e+00,  1.7291e+00,  4.7953e+00, -1.7112e+01,
         -4.2371e+00,  1.9759e-01,  4.9316e-01, -4.9136e+00, -8.2053e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-0.6234, -1.4236, -4.4434, -1.1337, -1.4574, -0.3277, -0.0507, -1.7836,
        -1.4145,  0.1508], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.4256,  -0.6388,   0.5884,  -0.9272,  -0.1871,  -1.2894,  17.2040,
          -1.2620,  -0.6271,  -6.9976],
        [ -1.3793,   0.6439,  -0.1583,   0.9271,   0.1929,   1.2903, -17.2370,
           1.2550,   0.6399,   7.0230]], device='cuda:0'))])
xi:  [-75.23009]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 556.4913048150685
W_T_median: 133.564300999907
W_T_pctile_5: -74.98896038908676
W_T_CVAR_5_pct: -185.7466052962198
Average q (qsum/M+1):  50.932672316028224
Optimal xi:  [-75.23009]
Expected(across Rb) median(across samples) p_equity:  0.4125902185837428
obj fun:  tensor(-1300.2906, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 806.3616944417624
W_T_median: 547.9700475945424
W_T_pctile_5: -287.9564157269737
W_T_CVAR_5_pct: -408.2820772432722
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -321.74770612400295
Current xi:  [84.36731]
objective value function right now is: -321.74770612400295
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -416.49159056168367
Current xi:  [66.52116]
objective value function right now is: -416.49159056168367
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -564.6432497729643
Current xi:  [52.88503]
objective value function right now is: -564.6432497729643
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [41.553673]
objective value function right now is: -556.9091656529549
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -595.8822935932178
Current xi:  [28.91512]
objective value function right now is: -595.8822935932178
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -994.8642651864619
Current xi:  [13.237155]
objective value function right now is: -994.8642651864619
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1021.3278921504035
Current xi:  [-0.7736917]
objective value function right now is: -1021.3278921504035
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.681605]
objective value function right now is: -1019.9137771465921
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.38396]
objective value function right now is: -1018.8050000926927
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1030.799465034366
Current xi:  [-22.319193]
objective value function right now is: -1030.799465034366
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-27.225267]
objective value function right now is: -1012.8357823817516
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.045788]
objective value function right now is: -1030.6541959270974
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.94393]
objective value function right now is: -1025.2718905692484
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-35.5395]
objective value function right now is: -1009.1888541838274
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.28591]
objective value function right now is: -1016.7876238101481
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.288002]
objective value function right now is: -1030.7747115969491
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.200306]
objective value function right now is: -1025.5598638013328
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.941067]
objective value function right now is: -1016.6229542538468
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-48.497562]
objective value function right now is: 363.98863299747836
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-59.585667]
objective value function right now is: -337.62140123560727
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-59.85858]
objective value function right now is: -1015.1420594832545
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.364292]
objective value function right now is: -1016.9319020348432
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-54.38189]
objective value function right now is: -1008.6212435080355
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-52.818844]
objective value function right now is: -1003.1936116331847
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-50.294155]
objective value function right now is: -1029.3791430724923
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-46.78048]
objective value function right now is: -1028.1057069275619
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-42.50578]
objective value function right now is: -1027.642804614706
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-40.167046]
objective value function right now is: -980.192094213632
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-37.626762]
objective value function right now is: -1023.9982241188022
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.9933]
objective value function right now is: -1024.0398355044933
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.46016]
objective value function right now is: -1007.556607207357
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-36.00622]
objective value function right now is: -1001.8008212792271
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-42.680573]
objective value function right now is: -1026.5321032266725
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1031.52030860632
Current xi:  [-43.661606]
objective value function right now is: -1031.52030860632
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -1033.1556703165836
Current xi:  [-38.867134]
objective value function right now is: -1033.1556703165836
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1037.1669261358331
Current xi:  [-37.846416]
objective value function right now is: -1037.1669261358331
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-36.942295]
objective value function right now is: -1036.5764593277765
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1037.834921923121
Current xi:  [-35.6999]
objective value function right now is: -1037.834921923121
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.9872]
objective value function right now is: -1035.5320332436252
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1038.4802410649195
Current xi:  [-34.97867]
objective value function right now is: -1038.4802410649195
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1038.8362851834263
Current xi:  [-35.003254]
objective value function right now is: -1038.8362851834263
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.92139]
objective value function right now is: -1037.9771682615954
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.000015]
objective value function right now is: -1038.2609620853616
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.977116]
objective value function right now is: -1037.137212396741
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.988064]
objective value function right now is: -1036.6640887418807
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.98429]
objective value function right now is: -1038.4272533519857
94.0% of gradient descent iterations done. Method = Adam
new min fval:  -1039.1667517929964
Current xi:  [-34.97327]
objective value function right now is: -1039.1667517929964
new min fval from sgd:  -1039.3826344556849
new min fval from sgd:  -1039.5817634357095
new min fval from sgd:  -1039.8554196953935
new min fval from sgd:  -1040.1113781410945
new min fval from sgd:  -1040.3470109833136
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.933132]
objective value function right now is: -1038.538227326939
new min fval from sgd:  -1040.349146418425
new min fval from sgd:  -1040.376355772977
new min fval from sgd:  -1040.3948482209819
new min fval from sgd:  -1040.403295191552
new min fval from sgd:  -1040.4110332704472
new min fval from sgd:  -1040.418836631087
new min fval from sgd:  -1040.429621765018
new min fval from sgd:  -1040.4475407553236
new min fval from sgd:  -1040.463042735673
new min fval from sgd:  -1040.4805495771193
new min fval from sgd:  -1040.4809058298317
new min fval from sgd:  -1040.4870417428997
new min fval from sgd:  -1040.4924096054685
new min fval from sgd:  -1040.5049524502192
new min fval from sgd:  -1040.5104806865559
new min fval from sgd:  -1040.5117225397505
new min fval from sgd:  -1040.5121728995553
new min fval from sgd:  -1040.5145384694358
new min fval from sgd:  -1040.5146451655942
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.951515]
objective value function right now is: -1040.1281715511648
new min fval from sgd:  -1040.5168221510562
new min fval from sgd:  -1040.5435572318272
new min fval from sgd:  -1040.5569123033545
new min fval from sgd:  -1040.5576688838073
new min fval from sgd:  -1040.5760130677568
new min fval from sgd:  -1040.5915435492093
new min fval from sgd:  -1040.6105305173025
new min fval from sgd:  -1040.6192006607384
new min fval from sgd:  -1040.6275979687089
new min fval from sgd:  -1040.631895822887
new min fval from sgd:  -1040.6328812699508
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.95608]
objective value function right now is: -1040.390902765708
min fval:  -1040.6328812699508
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  9.9739,  -0.6840],
        [  7.6332,   1.8334],
        [ -1.5312,   0.0834],
        [-10.2613,   1.5863],
        [  8.3288,  -1.0078],
        [  0.1251, -10.2716],
        [  8.4885,  -3.6189],
        [-10.5911,   1.0354],
        [  6.8184,  -7.2119],
        [  4.2862,  -8.1014]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -8.1794, -10.1768,  -2.7669,   6.1060,  -8.7567,  -7.9621,  -6.1695,
          7.6760,  -5.7250,  -6.3193], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 1.1495e+00,  4.3123e-01,  7.3267e-02,  1.0564e+00,  2.9845e-01,
          1.2920e-01,  6.9860e-01,  1.6264e+00,  8.5625e-01,  2.7409e-01],
        [ 1.1878e-01,  8.0343e-02, -4.0176e-02, -9.9268e-01, -2.1860e-01,
         -2.9561e-02, -2.8495e-01, -2.0240e+00, -2.8839e-01, -7.0705e-02],
        [ 3.7927e+00,  3.5676e+00,  1.6558e-01, -4.8556e+00,  1.0427e+00,
          1.0049e+01,  1.0780e+00, -5.7267e+00,  2.9807e+00,  4.7166e+00],
        [ 3.8481e+00,  4.2869e+00,  1.6251e-01, -4.9011e+00,  1.7084e+00,
          1.0823e+01,  1.4823e+00, -6.2427e+00,  3.1302e+00,  4.6577e+00],
        [ 3.4630e+00,  3.6370e+00,  1.5290e-01, -4.2969e+00,  1.0108e+00,
          9.7981e+00,  1.0941e+00, -6.0029e+00,  3.0325e+00,  4.2357e+00],
        [ 9.3380e-01,  3.3076e-01,  6.2263e-02,  9.3091e-01,  2.5059e-01,
          1.3425e-01,  5.9738e-01,  1.4254e+00,  7.8932e-01,  2.5248e-01],
        [ 5.5567e+00,  5.7219e+00, -7.7481e-02, -6.0144e+00,  3.1917e+00,
          1.1453e+01,  3.8821e+00, -6.1218e+00,  5.6927e+00,  6.7704e+00],
        [-4.7471e-01, -1.3193e-01, -8.2143e-03, -2.9022e-01, -4.2027e-02,
         -2.5309e-02, -3.4232e-01, -5.4440e-01, -2.6131e-01, -1.1056e-01],
        [ 4.9954e+00,  5.0878e+00,  1.4877e-01, -5.5082e+00,  2.5853e+00,
          1.2077e+01,  1.7815e+00, -6.7869e+00,  3.3499e+00,  4.9632e+00],
        [ 5.1386e+00,  5.8048e+00, -1.8614e-01, -5.7305e+00,  3.0361e+00,
          1.1185e+01,  4.5993e+00, -6.6097e+00,  6.4961e+00,  6.8082e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 3.6659, -2.3623, -2.1325, -1.9746, -2.1401,  3.1165, -2.0361, -1.4115,
        -1.6959, -1.6132], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.0170e+01, -6.2214e-01, -4.5206e+00, -5.0383e+00, -4.2349e+00,
          4.0593e+00, -8.3537e+00,  1.0106e-02, -6.5558e+00, -8.3793e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  9.4065,   0.7940],
        [ -0.0918,  -3.3145],
        [ -2.9156,   9.3940],
        [ 13.0285,   3.6165],
        [ -1.1712,  -5.1052],
        [-13.3867,  -3.6611],
        [-16.0116,  -4.1431],
        [  8.4274,  -0.7132],
        [-10.5007,  -3.1510],
        [ -1.8095,   0.3203]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-6.4131, 10.8480,  8.3839,  0.0206, -5.8090, -0.5102, -2.9063, -9.6947,
        -3.6337, -4.4624], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.0473e+00,  6.8928e+00, -1.7798e+01, -4.8778e+00,  2.9915e+00,
          5.9888e+00,  4.5680e-01, -5.8322e+00, -1.5494e+00, -3.2052e-01],
        [-3.7386e+00, -7.1656e-01, -2.4054e+00, -1.7654e+00, -5.8176e-01,
          2.0614e+00, -3.9296e-02, -2.1327e+00,  3.7134e-01,  6.8371e-02],
        [-1.2159e+01,  4.2920e+00, -1.6968e+01, -1.6717e+01,  1.2420e+01,
          6.5912e+00,  1.7893e+01, -2.6485e+00,  5.2954e+00,  3.8287e-03],
        [ 1.8757e+00, -1.2342e+01,  6.1567e+00,  9.0796e-01,  1.9496e-01,
          8.8813e+00,  1.0095e+00,  1.5684e+00,  6.2401e-01,  6.0930e-01],
        [-3.7417e+00, -7.1285e-01, -2.4063e+00, -1.7621e+00, -5.7910e-01,
          2.0675e+00, -4.0370e-02, -2.1370e+00,  3.7114e-01,  6.8845e-02],
        [ 6.4618e+00,  1.1296e+00,  4.6795e+00,  7.3211e+00, -7.5796e+00,
         -5.2284e+00, -1.6269e+00,  1.6574e+00,  7.8046e-02, -1.8105e-01],
        [ 1.3290e+00, -1.3724e+01,  1.2443e+00,  5.0459e+00, -3.4319e+00,
         -1.2226e+00, -1.2751e-01,  5.6238e+00, -8.1158e-02,  2.2429e-01],
        [-3.7422e+00, -7.1221e-01, -2.4064e+00, -1.7615e+00, -5.7863e-01,
          2.0686e+00, -4.0570e-02, -2.1378e+00,  3.7110e-01,  6.8924e-02],
        [-1.4298e+00,  5.3030e+00, -1.5397e+01, -7.1645e+00,  2.0486e+00,
          3.1128e+00,  8.8480e-01, -3.5980e+00,  2.3103e+00,  4.3207e-01],
        [ 5.4904e+00, -9.9973e+00,  3.0348e+01,  1.0650e+01, -5.9009e+00,
         -1.1204e+01, -9.0735e+00,  5.2360e+00, -2.9501e+00,  1.4348e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -3.6152,  -1.7666, -11.3061,  -0.2304,  -1.7621,   3.5953,   1.3153,
         -1.7614,  -2.3325,   5.5942], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -1.0741,   1.6903,  14.4375,  -1.9674,   1.6923,  -2.0615,   1.9859,
           1.6942,  -4.7196,   2.4205],
        [  0.8599,  -1.6865, -14.4407,   1.7929,  -1.6933,   2.0234,  -1.6873,
          -1.6929,   4.6955,  -2.4882]], device='cuda:0'))])
xi:  [-34.958233]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 409.5608793253608
W_T_median: 109.81515792870853
W_T_pctile_5: -34.935968061112
W_T_CVAR_5_pct: -166.62941817879167
Average q (qsum/M+1):  49.694276871219756
Optimal xi:  [-34.958233]
Expected(across Rb) median(across samples) p_equity:  0.31598491171995796
obj fun:  tensor(-1040.6329, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 806.3616944417624
W_T_median: 547.9700475945424
W_T_pctile_5: -287.9564157269737
W_T_CVAR_5_pct: -408.2820772432722
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  204.58112632810642
Current xi:  [82.13419]
objective value function right now is: 204.58112632810642
4.0% of gradient descent iterations done. Method = Adam
new min fval:  52.67844503999001
Current xi:  [64.6535]
objective value function right now is: 52.67844503999001
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -45.19817579819866
Current xi:  [49.502243]
objective value function right now is: -45.19817579819866
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -179.35274969396056
Current xi:  [35.83815]
objective value function right now is: -179.35274969396056
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -250.774711253704
Current xi:  [25.001444]
objective value function right now is: -250.774711253704
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -262.70590052065745
Current xi:  [15.746857]
objective value function right now is: -262.70590052065745
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -272.6639513146598
Current xi:  [7.56472]
objective value function right now is: -272.6639513146598
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -277.4009464162488
Current xi:  [0.5846178]
objective value function right now is: -277.4009464162488
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.0909753]
objective value function right now is: -262.7315276945282
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -682.8236315960844
Current xi:  [-11.599419]
objective value function right now is: -682.8236315960844
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-17.845072]
objective value function right now is: -626.0580584595996
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.258986]
objective value function right now is: -682.7653045281015
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -682.9579545751012
Current xi:  [-34.691772]
objective value function right now is: -682.9579545751012
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -693.4071242928381
Current xi:  [-35.71197]
objective value function right now is: -693.4071242928381
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -696.0161650912715
Current xi:  [-34.77539]
objective value function right now is: -696.0161650912715
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.881325]
objective value function right now is: -694.3021719694955
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -701.3342804350125
Current xi:  [-35.11797]
objective value function right now is: -701.3342804350125
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.009624]
objective value function right now is: -693.6241791703296
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.996513]
objective value function right now is: -699.1968450466752
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.96889]
objective value function right now is: -700.217096737281
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -702.4785706837991
Current xi:  [-35.088516]
objective value function right now is: -702.4785706837991
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.341095]
objective value function right now is: -692.7408028505498
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.844288]
objective value function right now is: -701.4965867191067
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.904495]
objective value function right now is: -679.4532405430865
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.233788]
objective value function right now is: -673.2236973213713
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -703.1921228610097
Current xi:  [-34.86482]
objective value function right now is: -703.1921228610097
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.933582]
objective value function right now is: -694.4507332507553
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-34.989822]
objective value function right now is: -702.262606032815
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-35.038692]
objective value function right now is: -701.3873906182223
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.84488]
objective value function right now is: -647.5964441032642
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.752586]
objective value function right now is: -697.8953626518322
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.767345]
objective value function right now is: -700.2536126183858
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.07772]
objective value function right now is: -701.3275941715216
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.102257]
objective value function right now is: -695.2777573886273
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.85673]
objective value function right now is: -694.8107349414933
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.03002]
objective value function right now is: -632.9218963633077
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -706.8004056311933
Current xi:  [-34.93341]
objective value function right now is: -706.8004056311933
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -707.8946193488766
Current xi:  [-34.931335]
objective value function right now is: -707.8946193488766
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.8998]
objective value function right now is: -706.9257489292091
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.934536]
objective value function right now is: -707.7311363778764
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.885807]
objective value function right now is: -707.0764959920978
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.859066]
objective value function right now is: -707.0179744328985
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.918827]
objective value function right now is: -704.1206731926567
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -708.5227901368272
Current xi:  [-34.97563]
objective value function right now is: -708.5227901368272
90.0% of gradient descent iterations done. Method = Adam
new min fval:  -708.6822218502639
Current xi:  [-34.932343]
objective value function right now is: -708.6822218502639
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.92368]
objective value function right now is: -707.9888260965427
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.891132]
objective value function right now is: -708.4746605891975
new min fval from sgd:  -708.6943353316547
new min fval from sgd:  -708.6992311073838
new min fval from sgd:  -708.7697071509284
new min fval from sgd:  -708.8956724650888
new min fval from sgd:  -708.9280382151622
new min fval from sgd:  -708.9301508403004
new min fval from sgd:  -708.9932950657154
new min fval from sgd:  -709.0204441450422
new min fval from sgd:  -709.1279933652079
new min fval from sgd:  -709.2157721975761
new min fval from sgd:  -709.2669484949181
new min fval from sgd:  -709.2910674057131
new min fval from sgd:  -709.3501511426897
new min fval from sgd:  -709.4211930462101
new min fval from sgd:  -709.4393438386527
new min fval from sgd:  -709.4714884213402
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.900578]
objective value function right now is: -704.8813807334949
new min fval from sgd:  -709.4757408207851
new min fval from sgd:  -709.4901219945198
new min fval from sgd:  -709.4958408884339
new min fval from sgd:  -709.4982083875703
new min fval from sgd:  -709.5050270663814
new min fval from sgd:  -709.5158359354605
new min fval from sgd:  -709.5285430197825
new min fval from sgd:  -709.538228345573
new min fval from sgd:  -709.5475470670077
new min fval from sgd:  -709.558647966586
new min fval from sgd:  -709.5624005918952
new min fval from sgd:  -709.5632243738922
new min fval from sgd:  -709.5666243919579
new min fval from sgd:  -709.5788634113209
new min fval from sgd:  -709.586021743753
new min fval from sgd:  -709.5993485629075
new min fval from sgd:  -709.6129024378637
new min fval from sgd:  -709.6261721157939
new min fval from sgd:  -709.6479801161613
new min fval from sgd:  -709.6569895952315
new min fval from sgd:  -709.6655907681977
new min fval from sgd:  -709.6744330352545
new min fval from sgd:  -709.6783349965494
new min fval from sgd:  -709.6975581330937
new min fval from sgd:  -709.7368267479216
new min fval from sgd:  -709.7648706733138
new min fval from sgd:  -709.7859021191781
new min fval from sgd:  -709.8065657783782
new min fval from sgd:  -709.8222435555715
new min fval from sgd:  -709.8366885183609
new min fval from sgd:  -709.8547184854485
new min fval from sgd:  -709.8685542076216
new min fval from sgd:  -709.8767361547618
new min fval from sgd:  -709.8817888059247
new min fval from sgd:  -709.8850880935853
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.9166]
objective value function right now is: -709.7723569306437
new min fval from sgd:  -709.897809597879
new min fval from sgd:  -709.9077476862774
new min fval from sgd:  -709.918220900272
new min fval from sgd:  -709.9235212998395
new min fval from sgd:  -709.9353553440894
new min fval from sgd:  -709.9568480312889
new min fval from sgd:  -709.977392142309
new min fval from sgd:  -709.9957758498026
new min fval from sgd:  -710.0108381792875
new min fval from sgd:  -710.0230267063197
new min fval from sgd:  -710.0306259769524
new min fval from sgd:  -710.0351441870985
new min fval from sgd:  -710.0413091330215
new min fval from sgd:  -710.0435654417859
new min fval from sgd:  -710.0465712278423
new min fval from sgd:  -710.0584122454081
new min fval from sgd:  -710.0640567240608
new min fval from sgd:  -710.0723470935106
new min fval from sgd:  -710.0749765436576
new min fval from sgd:  -710.0769744858335
new min fval from sgd:  -710.0782014600617
new min fval from sgd:  -710.088562702446
new min fval from sgd:  -710.0970840610217
new min fval from sgd:  -710.1025991645085
new min fval from sgd:  -710.1093101259484
new min fval from sgd:  -710.1218095575789
new min fval from sgd:  -710.1282248189968
new min fval from sgd:  -710.1311847735335
new min fval from sgd:  -710.1314693168993
new min fval from sgd:  -710.1364448300747
new min fval from sgd:  -710.1380185876361
new min fval from sgd:  -710.1394106172719
new min fval from sgd:  -710.140834412084
new min fval from sgd:  -710.1435640338741
new min fval from sgd:  -710.1552264992872
new min fval from sgd:  -710.1703522719841
new min fval from sgd:  -710.1824320479325
new min fval from sgd:  -710.19249066123
new min fval from sgd:  -710.2008531896672
new min fval from sgd:  -710.2049128115355
new min fval from sgd:  -710.2104027427396
new min fval from sgd:  -710.2146302695675
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.930878]
objective value function right now is: -709.6075290969582
min fval:  -710.2146302695675
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  7.1171,  -6.2470],
        [  9.7375,  -0.5551],
        [  9.5808,  -0.5891],
        [-10.2315,  -9.4427],
        [  9.5257,  -0.5408],
        [  8.5288,  -0.9609],
        [  2.6896,  -7.1392],
        [  9.7577,  -0.3577],
        [  8.7013,  -5.5368],
        [  9.8662,  -0.2987]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-6.0426, -8.4579, -8.4735, -5.8819, -8.5133, -8.9230, -6.9849, -8.5341,
        -5.1190, -8.6227], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 3.3919,  1.8488,  1.7002,  4.0138,  1.5489,  0.3324,  2.4760,  2.3707,
          3.2828,  2.6322],
        [ 5.9646,  4.6685,  4.4607,  4.8664,  4.3350,  3.1079,  4.3287,  5.1171,
          5.4129,  5.3487],
        [ 3.9233,  2.9893,  2.5263,  4.4230,  2.4455,  1.0297,  3.2532,  3.5275,
          3.6319,  3.6590],
        [ 5.0965,  4.2391,  4.0265,  4.6722,  4.1028,  2.7455,  3.5836,  4.6510,
          5.1210,  4.9541],
        [ 5.8718,  4.7964,  4.5286,  4.9279,  4.5094,  2.9151,  4.5747,  5.2663,
          5.7028,  5.3951],
        [-0.4045, -0.2552, -0.2193, -0.0455, -0.2098, -0.0537, -0.0461, -0.2818,
         -0.7233, -0.2967],
        [ 4.5269,  3.7888,  3.3962,  4.7487,  3.3692,  2.0955,  3.4653,  4.3480,
          3.9144,  4.5362],
        [ 0.7707,  0.6629,  0.6046,  0.8230,  0.5843,  0.1985,  0.3206,  0.6923,
          0.9873,  0.7110],
        [-3.9130, -2.6887, -2.2337, -3.8437, -2.5164, -0.6514, -3.4346, -2.5705,
         -2.9521, -3.0575],
        [-0.4044, -0.2552, -0.2193, -0.0455, -0.2098, -0.0537, -0.0461, -0.2818,
         -0.7233, -0.2966]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-7.9994, -9.5456, -8.4064, -8.8620, -9.7348, -1.7152, -8.6683,  4.5188,
         7.8383, -1.7157], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-2.6988, -9.1561, -3.3649, -6.3513, -9.8898,  0.0213, -4.4942,  8.6385,
          7.9110,  0.0213]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.5854,   7.2608],
        [  0.7076,  -1.0305],
        [  2.8758,  10.6792],
        [-18.6513,  -5.7220],
        [ 13.5824,   2.5604],
        [ -1.2448,   1.1641],
        [  1.6120,   1.6824],
        [-21.0453,  -6.3902],
        [ 12.2835,   0.9170],
        [-18.0195,  -4.8001]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-1.0996, -0.3980,  8.8017, -4.3003, -5.8281, -1.3387, -2.3030, -4.4758,
        -8.2848,  1.0982], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.2930e+00, -8.4142e-03,  6.1673e-01,  1.7993e+00, -1.5231e+00,
          1.1490e+00,  1.0807e+00,  1.8580e+00, -9.3759e-01,  2.0284e+00],
        [-4.5242e+00,  7.6347e-01,  2.9730e+00,  4.3361e+00, -9.1730e-01,
          1.5210e-01,  1.0987e+00,  3.6840e+00,  3.1226e-01, -4.4937e-01],
        [ 3.8435e-02,  3.2701e+00, -3.2564e+01,  1.5839e+01, -6.5999e+00,
         -5.8983e+00, -6.2292e+00,  2.4507e+01, -5.2795e+00,  4.8598e+00],
        [ 1.1853e+00, -3.7293e-01,  5.0095e-01,  1.4985e+00, -1.5172e+00,
          1.0580e+00,  9.9023e-01,  1.5701e+00, -9.3970e-01,  1.7751e+00],
        [-5.1979e-02, -1.3091e+00,  1.4794e+01,  1.4774e+01,  4.3349e+00,
          1.4117e+00, -2.8486e-01,  1.8311e+01,  5.5943e+00, -2.2853e+00],
        [-3.2382e-01,  1.7080e-01, -3.6709e+00, -9.3917e-01, -2.1358e+00,
          3.4740e-01,  4.9739e-01, -1.0424e+00, -2.0369e+00,  3.9715e-01],
        [-1.2512e+00, -3.1285e-01, -1.2887e+00, -1.9809e-01, -3.5238e-02,
         -7.4804e-01, -8.4402e-01, -2.1178e-01, -3.5362e-01, -1.3510e+00],
        [ 1.4250e+00,  5.9852e-01,  7.4816e-01,  2.1304e+00, -1.6044e+00,
          1.2434e+00,  1.1789e+00,  2.1790e+00, -1.0019e+00,  2.3028e+00],
        [-5.5574e-01,  1.0228e+00, -2.3783e+01,  3.2294e+00, -5.8856e+00,
          4.4951e-01, -4.3320e-01,  3.9346e+00, -3.1731e+00,  1.0991e+01],
        [-4.1001e-03,  2.6434e+00, -4.4745e+01, -4.9229e+00, -2.6918e+00,
         -2.0215e+00, -2.8859e+00, -7.5993e+00, -2.9496e+00, -1.3407e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-6.0014, -6.6768, -7.9119, -6.0045, -4.5863, -5.1204, -5.6233, -5.9164,
        -2.4195, -2.3388], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.8346,   3.9146,  16.4650,   0.6661,   0.3692,  -1.6637,  -0.7407,
           1.0223,  -7.4476,  -3.3762],
        [ -0.7603,  -3.5179, -16.4890,  -0.7313,  -0.3979,   1.5341,   0.7988,
          -0.9278,   7.2532,   2.9301]], device='cuda:0'))])
xi:  [-34.92381]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 384.0966968547939
W_T_median: 84.60020293504596
W_T_pctile_5: -34.91357621909621
W_T_CVAR_5_pct: -162.61312121897467
Average q (qsum/M+1):  49.138124527469756
Optimal xi:  [-34.92381]
Expected(across Rb) median(across samples) p_equity:  0.2713421110063791
obj fun:  tensor(-710.2146, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 806.3616944417624
W_T_median: 547.9700475945424
W_T_pctile_5: -287.9564157269737
W_T_CVAR_5_pct: -408.2820772432722
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  1325.0252131124428
Current xi:  [83.01647]
objective value function right now is: 1325.0252131124428
4.0% of gradient descent iterations done. Method = Adam
new min fval:  1170.8439759430069
Current xi:  [66.52517]
objective value function right now is: 1170.8439759430069
6.0% of gradient descent iterations done. Method = Adam
new min fval:  1105.316640454739
Current xi:  [50.376385]
objective value function right now is: 1105.316640454739
8.0% of gradient descent iterations done. Method = Adam
new min fval:  1009.850314635708
Current xi:  [35.340775]
objective value function right now is: 1009.850314635708
10.0% of gradient descent iterations done. Method = Adam
new min fval:  558.7253233457716
Current xi:  [23.580061]
objective value function right now is: 558.7253233457716
12.0% of gradient descent iterations done. Method = Adam
new min fval:  521.7005208960956
Current xi:  [15.649006]
objective value function right now is: 521.7005208960956
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [8.501692]
objective value function right now is: 560.5610725569287
16.0% of gradient descent iterations done. Method = Adam
new min fval:  510.332898195519
Current xi:  [1.9476848]
objective value function right now is: 510.332898195519
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.5248866]
objective value function right now is: 592.3347939675895
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.430519]
objective value function right now is: 510.36448810038945
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.762848]
objective value function right now is: 537.0587042920286
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.819655]
objective value function right now is: 530.5143077859269
26.0% of gradient descent iterations done. Method = Adam
new min fval:  162.398000461654
Current xi:  [-8.134562]
objective value function right now is: 162.398000461654
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  151.54035258281732
Current xi:  [-13.449971]
objective value function right now is: 151.54035258281732
30.0% of gradient descent iterations done. Method = Adam
new min fval:  121.74858015279099
Current xi:  [-13.359146]
objective value function right now is: 121.74858015279099
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.816115]
objective value function right now is: 203.81037168116887
34.0% of gradient descent iterations done. Method = Adam
new min fval:  109.56657877517367
Current xi:  [-15.799866]
objective value function right now is: 109.56657877517367
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-17.333126]
objective value function right now is: 153.44551065047884
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-17.057653]
objective value function right now is: 111.43953034443807
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-16.412664]
objective value function right now is: 151.5093510146366
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-17.493214]
objective value function right now is: 127.67651860496822
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-17.374014]
objective value function right now is: 142.35924488482678
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-16.932762]
objective value function right now is: 120.3625536201289
48.0% of gradient descent iterations done. Method = Adam
new min fval:  108.99893506543008
Current xi:  [-15.92748]
objective value function right now is: 108.99893506543008
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-16.472637]
objective value function right now is: 116.2448138855788
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.020391]
objective value function right now is: 119.09012777483314
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.909438]
objective value function right now is: 124.2545819053062
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-15.362049]
objective value function right now is: 181.46018947845326
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-15.894165]
objective value function right now is: 119.42416175996969
60.0% of gradient descent iterations done. Method = Adam
new min fval:  105.66066624909581
Current xi:  [-17.573614]
objective value function right now is: 105.66066624909581
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.942974]
objective value function right now is: 110.52868820777597
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.862222]
objective value function right now is: 108.72548439698303
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.396359]
objective value function right now is: 119.84338295630238
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.872645]
objective value function right now is: 107.70690681728395
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-23.397598]
objective value function right now is: 170.21311070779373
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-23.546019]
objective value function right now is: 138.3689715299009
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.91267]
objective value function right now is: 134.67835764094175
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-21.550777]
objective value function right now is: 424.20867476898206
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-21.298733]
objective value function right now is: 127.26139947419978
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-21.187328]
objective value function right now is: 146.79643532926067
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-20.468618]
objective value function right now is: 809.9811913903148
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-20.620909]
objective value function right now is: 391.6232032887705
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-20.350857]
objective value function right now is: 234.1257950989203
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-20.064785]
objective value function right now is: 161.50888996382065
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-19.717878]
objective value function right now is: 153.5769475286246
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-19.563135]
objective value function right now is: 116.22080725832619
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-19.513155]
objective value function right now is: 111.08465929364708
new min fval from sgd:  105.63053144621313
new min fval from sgd:  105.59785262075248
new min fval from sgd:  105.5510989899359
new min fval from sgd:  105.50914129941245
new min fval from sgd:  105.47328985214568
new min fval from sgd:  105.45087713486501
new min fval from sgd:  104.91621426705976
new min fval from sgd:  104.68214310913004
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-19.96889]
objective value function right now is: 105.34570256380947
new min fval from sgd:  104.32371326117021
new min fval from sgd:  104.15349623349627
new min fval from sgd:  103.72999291604927
new min fval from sgd:  103.31567154804577
new min fval from sgd:  103.10599129842751
new min fval from sgd:  102.9830915974088
new min fval from sgd:  102.83468720872051
new min fval from sgd:  102.66269024379936
new min fval from sgd:  102.52480803843282
new min fval from sgd:  102.37407293440968
new min fval from sgd:  102.31486425274899
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-19.981304]
objective value function right now is: 102.62764531969097
new min fval from sgd:  102.01181124415729
new min fval from sgd:  101.91607667484725
new min fval from sgd:  101.74040772825427
new min fval from sgd:  101.62227962099757
new min fval from sgd:  101.58209065021153
new min fval from sgd:  101.52359543937003
new min fval from sgd:  101.51555474832311
new min fval from sgd:  101.51086816464873
new min fval from sgd:  101.49057975902639
new min fval from sgd:  101.46095715736443
new min fval from sgd:  101.44855590571018
new min fval from sgd:  101.36880387050023
new min fval from sgd:  101.36465620261652
new min fval from sgd:  101.35020939548212
new min fval from sgd:  101.32626179942562
new min fval from sgd:  101.29957843730108
new min fval from sgd:  101.28049478284296
new min fval from sgd:  101.27202290852047
new min fval from sgd:  101.2671363770538
new min fval from sgd:  101.2668397483083
new min fval from sgd:  101.25767648885638
new min fval from sgd:  101.24006062902129
new min fval from sgd:  101.21880063506542
new min fval from sgd:  101.18604338406897
new min fval from sgd:  101.17401091528689
new min fval from sgd:  101.16400489548512
new min fval from sgd:  101.14413424460594
new min fval from sgd:  101.1333740449267
new min fval from sgd:  101.13060258231697
new min fval from sgd:  101.12570896025436
new min fval from sgd:  101.11792330934458
new min fval from sgd:  101.11120512970453
new min fval from sgd:  101.11062410706687
new min fval from sgd:  101.07507011542481
new min fval from sgd:  101.05331364594292
new min fval from sgd:  101.03209727435632
new min fval from sgd:  101.00699268392013
new min fval from sgd:  100.97832399475938
new min fval from sgd:  100.97458744885247
new min fval from sgd:  100.95921258513125
new min fval from sgd:  100.94786562704554
new min fval from sgd:  100.9399234676026
new min fval from sgd:  100.93854601295354
new min fval from sgd:  100.9235290626568
new min fval from sgd:  100.8909124050152
new min fval from sgd:  100.86334747830776
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-19.87768]
objective value function right now is: 101.52678769123538
min fval:  100.86334747830776
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 8.9990, -0.3262],
        [ 9.8876, -0.4440],
        [ 9.4356, -0.5183],
        [ 9.7646, -4.6575],
        [ 8.9441, -0.4609],
        [ 0.3071, -7.9568],
        [ 8.7874, -0.5348],
        [-6.7191, -8.6281],
        [ 8.0896, -6.6798],
        [ 9.3436, -0.4676]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-8.3322, -7.9010, -7.8695, -5.6807, -8.1613, -6.3807, -8.2523, -6.3050,
        -6.0354, -7.9570], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.3758,  0.6926,  0.5690,  0.8044,  0.3903,  0.5214,  0.3247,  0.5116,
          0.7126,  0.5315],
        [ 2.7444,  5.4900,  3.4247,  4.6953,  2.8366,  4.3708,  2.5155,  5.5614,
          4.7626,  3.3188],
        [ 2.9186,  5.9124,  3.6715,  4.7250,  2.8336,  4.5040,  2.7602,  5.4868,
          5.0707,  3.3905],
        [ 2.9834,  6.0099,  3.6442,  5.2626,  2.9586,  5.5675,  2.7053,  6.6458,
          5.7662,  3.5071],
        [ 2.9057,  5.9133,  3.5257,  4.7828,  2.8734,  4.5245,  2.8143,  5.4756,
          5.0249,  3.5348],
        [ 3.0205,  5.9295,  3.6933,  5.0304,  3.0448,  4.9405,  2.7271,  6.2282,
          5.5398,  3.5624],
        [ 2.9056,  6.0650,  3.6122,  5.2804,  2.9497,  5.7353,  2.7194,  6.8885,
          5.8421,  3.4720],
        [ 0.3890,  0.7511,  0.6053,  0.8280,  0.4049,  0.5395,  0.3345,  0.6620,
          0.7597,  0.5626],
        [ 0.3877,  0.7486,  0.6033,  0.8252,  0.4036,  0.5374,  0.3334,  0.6595,
          0.7571,  0.5607],
        [-0.1744, -0.3843, -0.2829, -0.4794, -0.1638, -0.2872, -0.1176, -0.2960,
         -0.4880, -0.2616]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.9646, -9.3253, -9.2669, -8.5739, -9.2580, -8.8415, -8.4978,  4.4932,
         4.4795, -2.0682], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 7.4341, -5.2359, -5.8376, -7.5516, -5.9498, -6.8216, -7.5259,  5.0033,
          4.9051, -0.0246]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 8.4683e+00, -2.0167e-01],
        [ 1.1353e+01,  2.8206e+00],
        [ 3.2721e-01,  1.1209e+01],
        [-1.0832e+01, -4.6031e+00],
        [ 1.1318e+01,  2.8317e+00],
        [-1.1295e+01, -4.5147e+00],
        [ 9.0600e+00,  5.6569e+00],
        [-9.4756e+00,  4.3600e-03],
        [-1.0676e+01, -3.2558e+00],
        [ 5.9589e+00,  8.3174e+00]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-9.1974, -0.7285,  9.6826,  0.3683, -0.7407, -3.7073,  8.4888,  7.3284,
        -0.7428,  5.7239], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.8248e+00, -3.7009e+00, -1.6972e+01,  9.6545e+00, -3.2806e+00,
          1.4198e+00, -6.0537e+00,  6.0587e+00,  5.5822e+00, -1.2052e+01],
        [ 2.6859e-01,  1.5936e+00,  3.1093e+01, -1.0472e+01,  1.1667e+00,
          4.3199e-02,  2.5539e+00, -4.4719e+00, -8.1149e+00,  1.4017e+00],
        [-1.6968e+00, -2.7066e+00, -9.5378e-01, -5.5234e-01, -3.2704e+00,
         -2.8383e-03, -5.7126e-01, -2.6409e+00, -7.5603e-02,  4.3110e-02],
        [ 2.3349e-01, -2.3148e+00, -1.3560e+00,  2.1443e-01, -2.7326e+00,
         -2.1235e-02, -7.6739e-01, -2.4080e+00,  1.9538e-01, -7.5645e-01],
        [ 2.9510e-01, -2.1712e+00, -7.2625e-01, -1.7381e-01, -2.1573e+00,
         -5.5465e-02, -1.8791e+00, -1.8790e+00, -9.4138e-03, -1.0685e+00],
        [-4.7655e-01, -4.2180e+00,  8.1559e-01,  1.5024e+00, -4.1929e+00,
          3.4347e+00, -2.0415e+00,  2.6442e-01,  2.0524e+00, -1.3025e+00],
        [ 3.0924e-01, -1.4373e+00,  2.4077e+00,  4.6505e-02, -1.4234e+00,
         -2.1666e+00, -8.2834e-01,  1.1130e+00, -1.9177e+00,  1.1523e+00],
        [-6.1451e-01, -3.1931e+00,  1.0854e+00,  1.7979e+00, -3.1592e+00,
          3.8853e+00, -1.6575e+00,  6.4426e-01,  2.3141e+00, -1.0119e+00],
        [-2.3635e+00, -3.2829e+00, -2.1386e+00, -1.1463e+00, -3.3565e+00,
         -5.2687e-03,  2.0511e-01, -2.6414e+00, -8.6978e-02,  8.3263e-01],
        [-9.5220e+00, -1.4338e+01, -9.6207e+00,  1.4071e+01, -1.3813e+01,
          9.6557e+00, -1.9712e+01,  7.2483e+00,  1.2334e+01, -2.4767e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 0.1687, -2.3780, -3.8904, -3.1639, -2.9617, -3.8232, -2.7553, -3.5565,
        -4.0998, -2.6698], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -5.3770,   0.3848,  -0.2549,   0.4640,   0.2330,   1.2895,   1.1011,
           1.1877,  -0.4883,  19.8035],
        [  5.0610,  -0.4228,   0.2550,  -0.4232,  -0.2349,  -1.2884,  -1.0997,
          -1.1954,   0.6205, -19.8532]], device='cuda:0'))])
xi:  [-19.891674]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 338.9959714882085
W_T_median: 93.2669218487792
W_T_pctile_5: -19.498172272238303
W_T_CVAR_5_pct: -160.1308893328518
Average q (qsum/M+1):  48.403265183971776
Optimal xi:  [-19.891674]
Expected(across Rb) median(across samples) p_equity:  0.24138732080658276
obj fun:  tensor(100.8633, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 10.0
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 806.3616944417624
W_T_median: 547.9700475945424
W_T_pctile_5: -287.9564157269737
W_T_CVAR_5_pct: -408.2820772432722
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  935.889145714144
Current xi:  [-19.592073]
objective value function right now is: 935.889145714144
4.0% of gradient descent iterations done. Method = Adam
new min fval:  923.9356786049873
Current xi:  [-17.476831]
objective value function right now is: 923.9356786049873
6.0% of gradient descent iterations done. Method = Adam
new min fval:  910.9864811686601
Current xi:  [-17.351707]
objective value function right now is: 910.9864811686601
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.448224]
objective value function right now is: 925.3868458296216
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.488214]
objective value function right now is: 946.5192293260172
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.507095]
objective value function right now is: 953.778844130368
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  903.4155735247244
Current xi:  [-14.208336]
objective value function right now is: 903.4155735247244
16.0% of gradient descent iterations done. Method = Adam
new min fval:  895.3457292943117
Current xi:  [-13.18479]
objective value function right now is: 895.3457292943117
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.756051]
objective value function right now is: 906.5873321087677
20.0% of gradient descent iterations done. Method = Adam
new min fval:  890.7710048997859
Current xi:  [-12.370561]
objective value function right now is: 890.7710048997859
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.791174]
objective value function right now is: 898.4015135098513
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.060513]
objective value function right now is: 904.4545093644169
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.588734]
objective value function right now is: 942.6547324480536
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-11.501392]
objective value function right now is: 908.6155257119162
30.0% of gradient descent iterations done. Method = Adam
new min fval:  883.4376274477949
Current xi:  [-11.447403]
objective value function right now is: 883.4376274477949
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.097973]
objective value function right now is: 908.5767758412654
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-11.924356]
objective value function right now is: 928.1690714403277
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-11.456757]
objective value function right now is: 924.0433373241251
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.561424]
objective value function right now is: 904.0586093581006
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-11.969475]
objective value function right now is: 938.043887879367
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.868357]
objective value function right now is: 941.1687568779225
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.764467]
objective value function right now is: 888.6406019793532
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.19002]
objective value function right now is: 883.5941618317385
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.153832]
objective value function right now is: 929.8536498677519
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.199087]
objective value function right now is: 893.859013439443
52.0% of gradient descent iterations done. Method = Adam
new min fval:  876.7189164444467
Current xi:  [-9.889546]
objective value function right now is: 876.7189164444467
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.34032]
objective value function right now is: 948.9148271818068
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-10.412814]
objective value function right now is: 910.3565933028651
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-11.008557]
objective value function right now is: 894.7753190769415
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.696875]
objective value function right now is: 940.1123358923795
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.592344]
objective value function right now is: 953.7803084346733
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.138774]
objective value function right now is: 905.5707210766027
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.470768]
objective value function right now is: 887.1517411378329
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.478623]
objective value function right now is: 894.0887031125201
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.922024]
objective value function right now is: 899.2929778907544
72.0% of gradient descent iterations done. Method = Adam
new min fval:  873.9627345418775
Current xi:  [-9.508164]
objective value function right now is: 873.9627345418775
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.061841]
objective value function right now is: 875.6323795931711
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.601561]
objective value function right now is: 876.930561174139
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.563156]
objective value function right now is: 881.2045580447906
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.291004]
objective value function right now is: 875.8994472520815
82.0% of gradient descent iterations done. Method = Adam
new min fval:  871.7085204953022
Current xi:  [-8.251661]
objective value function right now is: 871.7085204953022
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.677726]
objective value function right now is: 876.7330317239023
86.0% of gradient descent iterations done. Method = Adam
new min fval:  871.63215418125
Current xi:  [-7.0396852]
objective value function right now is: 871.63215418125
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.205282]
objective value function right now is: 873.3099169494442
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.235162]
objective value function right now is: 877.3405512072098
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.1864595]
objective value function right now is: 871.8678631462725
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.7765207]
objective value function right now is: 872.4285132026862
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.58724]
objective value function right now is: 873.9395818870004
new min fval from sgd:  869.4266528843367
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.511099]
objective value function right now is: 869.4266528843367
new min fval from sgd:  869.4023023431937
new min fval from sgd:  869.3665972421132
new min fval from sgd:  869.2950342104696
new min fval from sgd:  869.2497597873386
new min fval from sgd:  869.2004277689045
new min fval from sgd:  869.1692558172081
new min fval from sgd:  869.136840287134
new min fval from sgd:  869.103044773876
new min fval from sgd:  869.0697499787952
new min fval from sgd:  869.0357688729353
new min fval from sgd:  868.8372511192181
new min fval from sgd:  868.690249674042
new min fval from sgd:  868.5748221295229
new min fval from sgd:  868.4959778341226
new min fval from sgd:  868.4283533989354
new min fval from sgd:  868.378459163723
new min fval from sgd:  868.3417986637661
new min fval from sgd:  868.3071952904614
new min fval from sgd:  868.277758973219
new min fval from sgd:  868.2436775325987
new min fval from sgd:  868.2063679650951
new min fval from sgd:  868.1823910652708
new min fval from sgd:  868.171253428946
new min fval from sgd:  868.1629874350517
new min fval from sgd:  868.1263197873421
new min fval from sgd:  868.0661451268043
new min fval from sgd:  868.0122227319656
new min fval from sgd:  867.9597007603365
new min fval from sgd:  867.9236973105607
new min fval from sgd:  867.9120003100097
new min fval from sgd:  867.9004480708851
new min fval from sgd:  867.8971064740203
new min fval from sgd:  867.8929979081231
new min fval from sgd:  867.8909603547766
new min fval from sgd:  867.872404254762
new min fval from sgd:  867.8634330279723
new min fval from sgd:  867.850289138814
new min fval from sgd:  867.8353039653641
new min fval from sgd:  867.8290446386945
new min fval from sgd:  867.8118866429111
new min fval from sgd:  867.798273013086
new min fval from sgd:  867.7935326341043
new min fval from sgd:  867.780636065877
new min fval from sgd:  867.7740039162234
new min fval from sgd:  867.7704875251662
new min fval from sgd:  867.7693830315378
new min fval from sgd:  867.7625700782535
new min fval from sgd:  867.7488401334323
new min fval from sgd:  867.7445765261848
new min fval from sgd:  867.7314415866235
new min fval from sgd:  867.7230170017675
new min fval from sgd:  867.7197499254074
new min fval from sgd:  867.7105796285649
new min fval from sgd:  867.7011773458566
new min fval from sgd:  867.6966023386441
new min fval from sgd:  867.6842185409247
new min fval from sgd:  867.6722294587372
new min fval from sgd:  867.667094588322
new min fval from sgd:  867.6534583092996
new min fval from sgd:  867.6302775755997
new min fval from sgd:  867.596819701038
new min fval from sgd:  867.5453397072516
new min fval from sgd:  867.5128062224379
new min fval from sgd:  867.4809118986174
new min fval from sgd:  867.445754087419
new min fval from sgd:  867.440634231008
new min fval from sgd:  867.4341306342536
new min fval from sgd:  867.402450320392
new min fval from sgd:  867.3703820476386
new min fval from sgd:  867.32358260707
new min fval from sgd:  867.2782187051683
new min fval from sgd:  867.2638502214141
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.4428835]
objective value function right now is: 868.8368306148863
min fval:  867.2638502214141
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 11.6314,  -0.2897],
        [ 13.2417,  -0.9717],
        [ 12.3730,  -0.5825],
        [ 11.9187,  -4.5346],
        [ 10.4544,  -1.3441],
        [  3.2864,  -9.9393],
        [  7.8609,  -1.8931],
        [ -5.4921, -10.4610],
        [ 11.6343,  -6.4624],
        [ 12.2338,  -0.4644]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-10.9866,  -9.8536, -10.3358,  -7.4637, -11.0888,  -7.6733, -10.1109,
         -7.4684,  -6.9133, -10.4773], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 2.7863e-01,  1.0043e+00,  6.3554e-01,  7.1674e-01,  4.7085e-02,
          5.3504e-01,  4.4278e-03,  4.0172e-01,  9.6761e-01,  5.6390e-01],
        [ 7.6477e-01,  4.5708e+00,  2.2941e+00,  2.8896e+00, -1.2194e-02,
          3.9563e+00,  3.8295e-03,  4.7840e+00,  3.7086e+00,  2.0915e+00],
        [ 3.0067e+00,  7.4336e+00,  4.5036e+00,  4.7178e+00,  1.3336e+00,
          6.0289e+00,  1.3713e-01,  7.2708e+00,  5.7853e+00,  4.1883e+00],
        [ 3.6208e+00,  8.0840e+00,  4.9214e+00,  5.7345e+00,  2.5307e+00,
          7.3484e+00,  1.1371e+00,  7.5948e+00,  7.2115e+00,  4.7052e+00],
        [ 3.0697e+00,  7.4951e+00,  4.4403e+00,  4.8122e+00,  1.4361e+00,
          6.0753e+00,  1.7197e-01,  7.2914e+00,  5.7914e+00,  4.3590e+00],
        [ 3.8423e+00,  8.2761e+00,  5.2128e+00,  5.5633e+00,  2.6934e+00,
          6.0728e+00,  1.1233e+00,  7.5004e+00,  6.5408e+00,  4.9937e+00],
        [ 3.5561e+00,  8.1352e+00,  4.8944e+00,  5.7559e+00,  2.5161e+00,
          7.5124e+00,  1.1253e+00,  7.7566e+00,  7.2533e+00,  4.6754e+00],
        [ 2.5924e-01,  9.3430e-01,  5.8953e-01,  6.6339e-01,  4.4025e-02,
          4.8732e-01,  3.5796e-03,  3.6400e-01,  8.9722e-01,  5.2280e-01],
        [ 2.5821e-01,  9.3065e-01,  5.8713e-01,  6.6055e-01,  4.3864e-02,
          4.8485e-01,  3.5361e-03,  3.6209e-01,  8.9347e-01,  5.2066e-01],
        [-1.6834e-01, -5.2437e-01, -3.3995e-01, -4.8008e-01, -8.9842e-04,
         -2.3232e-01,  2.7835e-03, -8.1077e-02, -7.1713e-01, -3.1908e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([  4.7617,  -9.1319, -10.1226,  -9.4424, -10.1314, -10.0463,  -9.4261,
          4.4397,   4.4230,  -2.1670], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 6.5662, -3.0873, -5.7367, -9.0906, -5.8724, -8.0791, -9.0884,  4.2112,
          4.1157,  0.0174]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  9.9768,  -1.7313],
        [ 11.0757,   5.6272],
        [ -1.9518,  12.9735],
        [-13.2575,  -3.9810],
        [  1.5680,   3.2732],
        [-16.5229,  -5.5015],
        [ 11.3249,   3.5764],
        [-12.3339,  -0.2640],
        [-13.0272,  -4.1540],
        [  7.9576,   9.2124]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-13.1025,  -2.5910,  11.1286,  -0.3701, -10.6108,  -3.0541,   7.5880,
          9.9574,  -1.8873,   6.6914], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.0507e+00, -2.4776e+00, -2.4870e+01,  8.8998e+00, -6.3270e-03,
          5.8997e+00, -6.3410e+00,  5.4198e+00,  1.0631e+01, -5.6814e+00],
        [ 2.6771e+00, -1.6607e-01,  2.5331e+01, -1.4496e+01,  4.0303e-02,
          7.0570e+00,  3.0493e+00, -3.8980e+00, -8.5215e+00,  2.4753e+00],
        [-4.8715e-01, -9.8232e-01, -3.5298e-01,  1.4505e-01, -2.3684e-01,
          1.5661e-02, -3.8669e+00, -2.1466e+00,  3.3752e-02, -1.8114e+00],
        [-6.4101e-01, -6.8396e-01, -5.6381e-01, -1.5424e-01, -4.9646e-02,
         -3.6681e-02, -3.5989e+00, -1.0451e+00, -6.7667e-02, -1.4801e+00],
        [-4.8355e+00, -3.9095e+00, -7.1327e+00,  1.7625e+00, -3.5288e-03,
          1.3699e+00, -2.6370e+00,  2.2759e+00,  2.3090e+00, -1.1521e+00],
        [-4.7752e-01, -8.9377e-01, -5.1736e-01, -1.4831e-01, -4.5413e-01,
         -3.1337e-02, -3.6832e+00, -1.4922e+00, -6.1384e-02, -1.4150e+00],
        [ 2.9097e+00, -1.4325e+00,  2.7689e+00,  3.9229e-01, -2.8506e+00,
          6.0070e+00, -2.7235e+00,  9.3873e-01,  1.4047e+00,  3.8985e-01],
        [-4.7811e-01, -8.9367e-01, -5.1797e-01, -1.4849e-01, -4.5423e-01,
         -3.1364e-02, -3.6641e+00, -1.4913e+00, -6.1437e-02, -1.4149e+00],
        [-4.2229e+00, -1.0010e+00,  3.2034e+00,  2.5719e-01, -4.4662e+00,
          1.5013e+00, -1.4950e+00,  5.7361e-01,  4.6709e-01,  8.4347e-01],
        [-1.2768e+01, -2.0849e+00, -4.5121e+00,  1.4111e+01,  5.5664e-03,
          1.4339e+01, -2.0286e+01,  8.5591e+00,  1.1622e+01, -4.2144e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-0.1627, -1.8750, -3.8852, -4.0696, -2.9022, -3.6868, -4.6414, -3.7057,
        -4.3813, -3.6383], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -6.3696,   0.3957,   0.1471,  -0.0970,  -3.1987,  -0.0905,   0.6639,
          -0.0907,   1.8041,  20.7586],
        [  6.0596,  -0.4337,  -0.1471,   0.1065,   3.1982,   0.0905,  -0.6626,
           0.0903,  -1.7583, -20.7978]], device='cuda:0'))])
xi:  [-6.4342847]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 335.83603556048183
W_T_median: 91.5820744843654
W_T_pctile_5: -6.238912070330871
W_T_CVAR_5_pct: -156.70104317625666
Average q (qsum/M+1):  47.84696320564516
Optimal xi:  [-6.4342847]
Expected(across Rb) median(across samples) p_equity:  0.21682954244315625
obj fun:  tensor(867.2639, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 15.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 8.9990, -0.3262],
        [ 9.8876, -0.4440],
        [ 9.4356, -0.5183],
        [ 9.7646, -4.6575],
        [ 8.9441, -0.4609],
        [ 0.3071, -7.9568],
        [ 8.7874, -0.5348],
        [-6.7191, -8.6281],
        [ 8.0896, -6.6798],
        [ 9.3436, -0.4676]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-8.3322, -7.9010, -7.8695, -5.6807, -8.1613, -6.3807, -8.2523, -6.3050,
        -6.0354, -7.9570], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.3758,  0.6926,  0.5690,  0.8044,  0.3903,  0.5214,  0.3247,  0.5116,
          0.7126,  0.5315],
        [ 2.7444,  5.4900,  3.4247,  4.6953,  2.8366,  4.3708,  2.5155,  5.5614,
          4.7626,  3.3188],
        [ 2.9186,  5.9124,  3.6715,  4.7250,  2.8336,  4.5040,  2.7602,  5.4868,
          5.0707,  3.3905],
        [ 2.9834,  6.0099,  3.6442,  5.2626,  2.9586,  5.5675,  2.7053,  6.6458,
          5.7662,  3.5071],
        [ 2.9057,  5.9133,  3.5257,  4.7828,  2.8734,  4.5245,  2.8143,  5.4756,
          5.0249,  3.5348],
        [ 3.0205,  5.9295,  3.6933,  5.0304,  3.0448,  4.9405,  2.7271,  6.2282,
          5.5398,  3.5624],
        [ 2.9056,  6.0650,  3.6122,  5.2804,  2.9497,  5.7353,  2.7194,  6.8885,
          5.8421,  3.4720],
        [ 0.3890,  0.7511,  0.6053,  0.8280,  0.4049,  0.5395,  0.3345,  0.6620,
          0.7597,  0.5626],
        [ 0.3877,  0.7486,  0.6033,  0.8252,  0.4036,  0.5374,  0.3334,  0.6595,
          0.7571,  0.5607],
        [-0.1744, -0.3843, -0.2829, -0.4794, -0.1638, -0.2872, -0.1176, -0.2960,
         -0.4880, -0.2616]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.9646, -9.3253, -9.2669, -8.5739, -9.2580, -8.8415, -8.4978,  4.4932,
         4.4795, -2.0682], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 7.4341, -5.2359, -5.8376, -7.5516, -5.9498, -6.8216, -7.5259,  5.0033,
          4.9051, -0.0246]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 8.4683e+00, -2.0167e-01],
        [ 1.1353e+01,  2.8206e+00],
        [ 3.2721e-01,  1.1209e+01],
        [-1.0832e+01, -4.6031e+00],
        [ 1.1318e+01,  2.8317e+00],
        [-1.1295e+01, -4.5147e+00],
        [ 9.0600e+00,  5.6569e+00],
        [-9.4756e+00,  4.3600e-03],
        [-1.0676e+01, -3.2558e+00],
        [ 5.9589e+00,  8.3174e+00]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-9.1974, -0.7285,  9.6826,  0.3683, -0.7407, -3.7073,  8.4888,  7.3284,
        -0.7428,  5.7239], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.8248e+00, -3.7009e+00, -1.6972e+01,  9.6545e+00, -3.2806e+00,
          1.4198e+00, -6.0537e+00,  6.0587e+00,  5.5822e+00, -1.2052e+01],
        [ 2.6859e-01,  1.5936e+00,  3.1093e+01, -1.0472e+01,  1.1667e+00,
          4.3199e-02,  2.5539e+00, -4.4719e+00, -8.1149e+00,  1.4017e+00],
        [-1.6968e+00, -2.7066e+00, -9.5378e-01, -5.5234e-01, -3.2704e+00,
         -2.8383e-03, -5.7126e-01, -2.6409e+00, -7.5603e-02,  4.3110e-02],
        [ 2.3349e-01, -2.3148e+00, -1.3560e+00,  2.1443e-01, -2.7326e+00,
         -2.1235e-02, -7.6739e-01, -2.4080e+00,  1.9538e-01, -7.5645e-01],
        [ 2.9510e-01, -2.1712e+00, -7.2625e-01, -1.7381e-01, -2.1573e+00,
         -5.5465e-02, -1.8791e+00, -1.8790e+00, -9.4138e-03, -1.0685e+00],
        [-4.7655e-01, -4.2180e+00,  8.1559e-01,  1.5024e+00, -4.1929e+00,
          3.4347e+00, -2.0415e+00,  2.6442e-01,  2.0524e+00, -1.3025e+00],
        [ 3.0924e-01, -1.4373e+00,  2.4077e+00,  4.6505e-02, -1.4234e+00,
         -2.1666e+00, -8.2834e-01,  1.1130e+00, -1.9177e+00,  1.1523e+00],
        [-6.1451e-01, -3.1931e+00,  1.0854e+00,  1.7979e+00, -3.1592e+00,
          3.8853e+00, -1.6575e+00,  6.4426e-01,  2.3141e+00, -1.0119e+00],
        [-2.3635e+00, -3.2829e+00, -2.1386e+00, -1.1463e+00, -3.3565e+00,
         -5.2687e-03,  2.0511e-01, -2.6414e+00, -8.6978e-02,  8.3263e-01],
        [-9.5220e+00, -1.4338e+01, -9.6207e+00,  1.4071e+01, -1.3813e+01,
          9.6557e+00, -1.9712e+01,  7.2483e+00,  1.2334e+01, -2.4767e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 0.1687, -2.3780, -3.8904, -3.1639, -2.9617, -3.8232, -2.7553, -3.5565,
        -4.0998, -2.6698], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -5.3770,   0.3848,  -0.2549,   0.4640,   0.2330,   1.2895,   1.1011,
           1.1877,  -0.4883,  19.8035],
        [  5.0610,  -0.4228,   0.2550,  -0.4232,  -0.2349,  -1.2884,  -1.0997,
          -1.1954,   0.6205, -19.8532]], device='cuda:0'))])
loaded xi:  -19.891674
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 806.3616944417624
W_T_median: 547.9700475945424
W_T_pctile_5: -287.9564157269737
W_T_CVAR_5_pct: -408.2820772432722
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  2551.730085498762
Current xi:  [-17.807178]
objective value function right now is: 2551.730085498762
4.0% of gradient descent iterations done. Method = Adam
new min fval:  2492.1481761330347
Current xi:  [-18.35052]
objective value function right now is: 2492.1481761330347
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.06796]
objective value function right now is: 2646.910932504731
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.597156]
objective value function right now is: 2680.2810923063935
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.787382]
objective value function right now is: 2505.1135333129364
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.63992]
objective value function right now is: 2527.284948324366
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-11.978537]
objective value function right now is: 2507.254094507633
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.756263]
objective value function right now is: 3041.9493730492522
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-31.623539]
objective value function right now is: 3878.532952880157
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-39.87168]
objective value function right now is: 3285.751757771143
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-41.638176]
objective value function right now is: 3035.0466670090977
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-40.143795]
objective value function right now is: 2752.388553684928
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.25642]
objective value function right now is: 2643.3254526040664
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-31.401373]
objective value function right now is: 2592.7061287982146
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-27.892548]
objective value function right now is: 2572.0390891358365
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-24.630909]
objective value function right now is: 2713.964905758575
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.63345]
objective value function right now is: 2528.0608350928205
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-20.7778]
objective value function right now is: 2619.808641981599
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.013836]
objective value function right now is: 2526.2114413224135
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.926882]
objective value function right now is: 2542.8180181350313
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.029723]
objective value function right now is: 2548.289181229202
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.481981]
objective value function right now is: 2506.656764043929
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.2980795]
objective value function right now is: 2500.931938335448
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.077244]
objective value function right now is: 2497.8188636223153
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.307254]
objective value function right now is: 2503.9118736717014
52.0% of gradient descent iterations done. Method = Adam
new min fval:  2468.975794072395
Current xi:  [-10.4674225]
objective value function right now is: 2468.975794072395
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-11.018563]
objective value function right now is: 2589.153469349014
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-11.165267]
objective value function right now is: 2671.0690460920678
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-9.134834]
objective value function right now is: 2487.145263201888
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.816705]
objective value function right now is: 2501.14697497697
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.098031]
objective value function right now is: 2502.80851188269
64.0% of gradient descent iterations done. Method = Adam
new min fval:  2468.7332253793
Current xi:  [-9.914981]
objective value function right now is: 2468.7332253793
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.444094]
objective value function right now is: 2568.3140781613142
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.893954]
objective value function right now is: 2501.4073328363165
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.832136]
objective value function right now is: 2496.3888018224557
72.0% of gradient descent iterations done. Method = Adam
new min fval:  2455.903405703465
Current xi:  [-9.629929]
objective value function right now is: 2455.903405703465
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.31337]
objective value function right now is: 2472.263190381133
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.989102]
objective value function right now is: 2457.0195588470024
78.0% of gradient descent iterations done. Method = Adam
new min fval:  2441.248792856402
Current xi:  [-8.691377]
objective value function right now is: 2441.248792856402
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.562861]
objective value function right now is: 2453.1770555918692
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.157651]
objective value function right now is: 2442.4088375347965
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.9714503]
objective value function right now is: 2441.34844415237
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.632116]
objective value function right now is: 2452.5902045862917
88.0% of gradient descent iterations done. Method = Adam
new min fval:  2437.9569756907567
Current xi:  [-7.322913]
objective value function right now is: 2437.9569756907567
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.0552425]
objective value function right now is: 2438.9815517961283
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.2753882]
objective value function right now is: 2450.9580520258746
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.716471]
objective value function right now is: 2443.8403048994505
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.4556513]
objective value function right now is: 2440.161434045828
new min fval from sgd:  2433.9063192260624
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.406209]
objective value function right now is: 2433.9063192260624
new min fval from sgd:  2433.8154662561633
new min fval from sgd:  2433.7014527484152
new min fval from sgd:  2433.6678598410144
new min fval from sgd:  2433.434984608155
new min fval from sgd:  2433.2923228684144
new min fval from sgd:  2433.19079984921
new min fval from sgd:  2433.170771212748
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.3022366]
objective value function right now is: 2434.210839977735
min fval:  2433.170771212748
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 11.6026,  -0.0385],
        [ 13.4272,  -1.0897],
        [ 12.5133,  -0.5929],
        [ 12.2025,  -4.3618],
        [ 10.1553,  -1.3273],
        [  3.6277, -10.0658],
        [  7.2653,  -1.7152],
        [ -5.8051, -10.6109],
        [ 11.4751,  -6.2097],
        [ 12.3121,  -0.3863]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-11.3089,  -9.7025, -10.2737,  -7.2865, -11.1604,  -7.7172,  -9.8553,
         -7.6222,  -6.6806, -10.5459], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 2.1137e-01,  9.2848e-01,  5.6036e-01,  7.5487e-01,  3.1167e-02,
          6.7372e-01,  2.3643e-02,  4.0628e-01,  1.0279e+00,  4.7454e-01],
        [ 9.3053e-01,  5.2677e+00,  2.8029e+00,  3.6603e+00, -1.4245e-02,
          4.6772e+00, -2.1817e-02,  6.1526e+00,  3.9219e+00,  2.5258e+00],
        [ 2.7424e+00,  7.4035e+00,  4.4989e+00,  5.0721e+00,  8.8049e-01,
          5.9048e+00,  5.7481e-02,  7.7585e+00,  5.4242e+00,  4.1259e+00],
        [ 3.5025e+00,  8.1420e+00,  4.9719e+00,  6.2504e+00,  2.2551e+00,
          7.3875e+00,  1.0437e+00,  7.9642e+00,  6.9441e+00,  4.7005e+00],
        [ 2.8011e+00,  7.4560e+00,  4.4260e+00,  5.1559e+00,  9.6533e-01,
          5.9454e+00,  7.5976e-02,  7.7668e+00,  5.4213e+00,  4.2945e+00],
        [ 3.6990e+00,  8.3003e+00,  5.2444e+00,  6.0669e+00,  2.2993e+00,
          5.7757e+00,  8.9717e-01,  7.8949e+00,  6.1470e+00,  4.9736e+00],
        [ 3.4650e+00,  8.2199e+00,  4.9678e+00,  6.2766e+00,  2.2665e+00,
          7.5228e+00,  1.0575e+00,  8.1175e+00,  6.9922e+00,  4.6924e+00],
        [ 1.9704e-01,  8.5737e-01,  5.1587e-01,  6.9769e-01,  2.9553e-02,
          6.1612e-01,  2.3172e-02,  3.6527e-01,  9.5597e-01,  4.3690e-01],
        [ 1.9627e-01,  8.5365e-01,  5.1354e-01,  6.9461e-01,  2.9467e-02,
          6.1310e-01,  2.3145e-02,  3.6313e-01,  9.5209e-01,  4.3493e-01],
        [-8.1591e-02, -5.1523e-01, -2.8117e-01, -7.4286e-01, -1.5860e-02,
         -3.1740e-01, -4.2405e-03, -9.9228e-02, -9.0872e-01, -2.3219e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([  4.8939,  -9.7490, -10.3861,  -9.2830, -10.3885, -10.1868,  -9.2961,
          4.5637,   4.5465,  -2.4845], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 6.4632, -3.6368, -5.7664, -9.0061, -5.8887, -7.9789, -9.0702,  4.0934,
          3.9973, -0.0147]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  9.3064,  -2.3301],
        [  2.8814,   2.8181],
        [ -1.0525,  13.9248],
        [-13.8648,  -4.2382],
        [ 11.3725,   4.6418],
        [-18.5534,  -4.8556],
        [  6.8503,   3.3448],
        [-13.2014,  -0.8499],
        [-12.8850,  -4.4040],
        [  7.8974,   9.1457]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-12.9798,  -8.6654,  11.2610,  -0.7848,  -0.3500,  -4.0627,   7.6157,
          7.9000,  -2.4802,   6.7748], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.3512e+00, -9.2007e-02, -7.4504e-01,  1.9368e+00, -7.6229e-01,
         -1.4027e-01, -6.4734e+00,  1.5611e+00,  1.9378e+00, -2.0071e+00],
        [ 4.5286e+00,  1.8080e+00, -1.4686e+00, -1.6508e+00,  2.4050e+00,
         -2.5202e+00,  5.2349e+00, -1.6666e+00, -3.9126e+00,  2.2961e+00],
        [-1.6871e+01, -8.8356e-02, -1.3326e+01,  7.8785e+00, -1.2770e+01,
          2.6134e+01, -4.2604e+00,  1.7869e+00,  1.4167e+01, -3.1272e+01],
        [-3.0552e+00, -1.7739e+00, -2.2736e+01,  7.4459e+00, -3.7213e+00,
          3.7495e+00, -1.5877e+00,  5.1324e+00,  1.1143e+01, -1.0584e+01],
        [ 4.2410e+00,  1.9494e-01,  1.2508e+01, -1.4008e+00,  1.1575e+00,
          1.1401e+01,  3.3310e-02, -6.1971e-01, -1.0102e+01,  2.0127e+00],
        [-7.5433e-01, -6.8867e-02, -3.4704e-01, -8.9646e-02, -2.2054e+00,
          6.8109e-03, -3.4069e+00, -1.0035e+00, -3.3870e-02, -1.1832e+00],
        [-2.1416e+00, -2.5129e+00,  2.5380e+00, -1.5067e+00, -2.5470e+00,
         -1.0961e-01, -3.1477e+00,  4.9534e-01, -1.5689e+00, -4.7544e-01],
        [-8.0943e+00, -2.6080e+00,  3.1038e+00,  6.3377e-01, -8.9048e-01,
         -1.1298e+00, -2.2101e+00,  1.5198e+00,  4.5456e-01,  1.5590e+00],
        [ 1.8911e+00, -1.1370e-01, -2.5487e+00, -7.0253e+00, -1.8087e+00,
          8.6970e-03, -3.4781e+00, -9.2077e-01, -1.9457e+00,  1.3388e+00],
        [-4.4752e+00, -1.5415e-04, -1.9197e+00,  1.3039e+01, -3.6170e+00,
          4.9368e+00, -1.9765e+01,  6.4549e+00,  1.0456e+01, -1.7667e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.6073,  0.7836, -4.4060, -1.8561,  0.0145, -3.4142, -4.9917, -3.9965,
        -4.2194, -4.0646], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.0771,  -2.1022,  18.6155,  -7.4481,   2.5088,   0.0326,   0.9931,
           1.4001,  -1.7345,   8.6590],
        [ -0.1474,   2.0642, -18.6097,   7.4564,  -2.5093,  -0.0326,  -0.9919,
          -1.4066,   1.7486,  -8.6829]], device='cuda:0'))])
xi:  [-6.3948145]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 312.6585739028449
W_T_median: 84.73645755375928
W_T_pctile_5: -6.189715288728509
W_T_CVAR_5_pct: -156.22127779703314
Average q (qsum/M+1):  47.49597561743952
Optimal xi:  [-6.3948145]
Expected(across Rb) median(across samples) p_equity:  0.19382522826393445
obj fun:  tensor(2433.1708, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 25.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 8.9990, -0.3262],
        [ 9.8876, -0.4440],
        [ 9.4356, -0.5183],
        [ 9.7646, -4.6575],
        [ 8.9441, -0.4609],
        [ 0.3071, -7.9568],
        [ 8.7874, -0.5348],
        [-6.7191, -8.6281],
        [ 8.0896, -6.6798],
        [ 9.3436, -0.4676]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-8.3322, -7.9010, -7.8695, -5.6807, -8.1613, -6.3807, -8.2523, -6.3050,
        -6.0354, -7.9570], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.3758,  0.6926,  0.5690,  0.8044,  0.3903,  0.5214,  0.3247,  0.5116,
          0.7126,  0.5315],
        [ 2.7444,  5.4900,  3.4247,  4.6953,  2.8366,  4.3708,  2.5155,  5.5614,
          4.7626,  3.3188],
        [ 2.9186,  5.9124,  3.6715,  4.7250,  2.8336,  4.5040,  2.7602,  5.4868,
          5.0707,  3.3905],
        [ 2.9834,  6.0099,  3.6442,  5.2626,  2.9586,  5.5675,  2.7053,  6.6458,
          5.7662,  3.5071],
        [ 2.9057,  5.9133,  3.5257,  4.7828,  2.8734,  4.5245,  2.8143,  5.4756,
          5.0249,  3.5348],
        [ 3.0205,  5.9295,  3.6933,  5.0304,  3.0448,  4.9405,  2.7271,  6.2282,
          5.5398,  3.5624],
        [ 2.9056,  6.0650,  3.6122,  5.2804,  2.9497,  5.7353,  2.7194,  6.8885,
          5.8421,  3.4720],
        [ 0.3890,  0.7511,  0.6053,  0.8280,  0.4049,  0.5395,  0.3345,  0.6620,
          0.7597,  0.5626],
        [ 0.3877,  0.7486,  0.6033,  0.8252,  0.4036,  0.5374,  0.3334,  0.6595,
          0.7571,  0.5607],
        [-0.1744, -0.3843, -0.2829, -0.4794, -0.1638, -0.2872, -0.1176, -0.2960,
         -0.4880, -0.2616]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.9646, -9.3253, -9.2669, -8.5739, -9.2580, -8.8415, -8.4978,  4.4932,
         4.4795, -2.0682], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 7.4341, -5.2359, -5.8376, -7.5516, -5.9498, -6.8216, -7.5259,  5.0033,
          4.9051, -0.0246]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 8.4683e+00, -2.0167e-01],
        [ 1.1353e+01,  2.8206e+00],
        [ 3.2721e-01,  1.1209e+01],
        [-1.0832e+01, -4.6031e+00],
        [ 1.1318e+01,  2.8317e+00],
        [-1.1295e+01, -4.5147e+00],
        [ 9.0600e+00,  5.6569e+00],
        [-9.4756e+00,  4.3600e-03],
        [-1.0676e+01, -3.2558e+00],
        [ 5.9589e+00,  8.3174e+00]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-9.1974, -0.7285,  9.6826,  0.3683, -0.7407, -3.7073,  8.4888,  7.3284,
        -0.7428,  5.7239], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.8248e+00, -3.7009e+00, -1.6972e+01,  9.6545e+00, -3.2806e+00,
          1.4198e+00, -6.0537e+00,  6.0587e+00,  5.5822e+00, -1.2052e+01],
        [ 2.6859e-01,  1.5936e+00,  3.1093e+01, -1.0472e+01,  1.1667e+00,
          4.3199e-02,  2.5539e+00, -4.4719e+00, -8.1149e+00,  1.4017e+00],
        [-1.6968e+00, -2.7066e+00, -9.5378e-01, -5.5234e-01, -3.2704e+00,
         -2.8383e-03, -5.7126e-01, -2.6409e+00, -7.5603e-02,  4.3110e-02],
        [ 2.3349e-01, -2.3148e+00, -1.3560e+00,  2.1443e-01, -2.7326e+00,
         -2.1235e-02, -7.6739e-01, -2.4080e+00,  1.9538e-01, -7.5645e-01],
        [ 2.9510e-01, -2.1712e+00, -7.2625e-01, -1.7381e-01, -2.1573e+00,
         -5.5465e-02, -1.8791e+00, -1.8790e+00, -9.4138e-03, -1.0685e+00],
        [-4.7655e-01, -4.2180e+00,  8.1559e-01,  1.5024e+00, -4.1929e+00,
          3.4347e+00, -2.0415e+00,  2.6442e-01,  2.0524e+00, -1.3025e+00],
        [ 3.0924e-01, -1.4373e+00,  2.4077e+00,  4.6505e-02, -1.4234e+00,
         -2.1666e+00, -8.2834e-01,  1.1130e+00, -1.9177e+00,  1.1523e+00],
        [-6.1451e-01, -3.1931e+00,  1.0854e+00,  1.7979e+00, -3.1592e+00,
          3.8853e+00, -1.6575e+00,  6.4426e-01,  2.3141e+00, -1.0119e+00],
        [-2.3635e+00, -3.2829e+00, -2.1386e+00, -1.1463e+00, -3.3565e+00,
         -5.2687e-03,  2.0511e-01, -2.6414e+00, -8.6978e-02,  8.3263e-01],
        [-9.5220e+00, -1.4338e+01, -9.6207e+00,  1.4071e+01, -1.3813e+01,
          9.6557e+00, -1.9712e+01,  7.2483e+00,  1.2334e+01, -2.4767e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 0.1687, -2.3780, -3.8904, -3.1639, -2.9617, -3.8232, -2.7553, -3.5565,
        -4.0998, -2.6698], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -5.3770,   0.3848,  -0.2549,   0.4640,   0.2330,   1.2895,   1.1011,
           1.1877,  -0.4883,  19.8035],
        [  5.0610,  -0.4228,   0.2550,  -0.4232,  -0.2349,  -1.2884,  -1.0997,
          -1.1954,   0.6205, -19.8532]], device='cuda:0'))])
loaded xi:  -19.891674
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 806.3616944417624
W_T_median: 547.9700475945424
W_T_pctile_5: -287.9564157269737
W_T_CVAR_5_pct: -408.2820772432722
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  6518.507617788584
Current xi:  [-15.618099]
objective value function right now is: 6518.507617788584
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.44221]
objective value function right now is: 6545.765124248789
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.616329]
objective value function right now is: 6535.54049839908
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.188342]
objective value function right now is: 6556.165579726467
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.168278]
objective value function right now is: 6662.789798512144
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.293159]
objective value function right now is: 6589.826611119209
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  6446.650458989639
Current xi:  [-11.500219]
objective value function right now is: 6446.650458989639
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.140367]
objective value function right now is: 6674.851863355528
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.0312]
objective value function right now is: 6470.739369690803
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.642282]
objective value function right now is: 6507.107718003737
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.9437766]
objective value function right now is: 6454.364716662633
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.482151]
objective value function right now is: 6510.390570805295
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.877857]
objective value function right now is: 6482.865891433185
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-7.848912]
objective value function right now is: 6555.581162351437
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.3161936]
objective value function right now is: 6496.532408818403
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.236487]
objective value function right now is: 6524.919373914737
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.256306]
objective value function right now is: 6551.808395101827
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.5210705]
objective value function right now is: 6452.551707994699
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.737471]
objective value function right now is: 6491.333624384993
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.751668]
objective va57.99999999999999% of gradient descent iterat42.0% of gradient descent iterations done. Method = Adam
new min fval:  6417.187113969683
Current xi:  [-9.260.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.760017]
objective value function right now is: 6396.519849336648
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.746887]
objective value function right now is: 6426.395983277301
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-20.813385]
objective value function right now is: 10581.5791628058
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-21.487339]
objective value function right now is: 6786.209957595978
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.846193]
objective value function right now is: 6783.575203675424
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-29.57604]
objective value function right now is: 23434.97949111495
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.156498]
objective value function right now is: 9214.02526346418
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.319824]
objective value function right now is: 8203.64558487736
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.899605]
objective value function right now is: 7078.8638993757295
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.145077]
objective value function right now is: 6788.540124929523
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.4328]
objective value function right now is: 8726.4332878078
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.761753]
objective value function right now is: 6942.353487239188
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-33.510242]
objective value function right now is: 6789.945786533535
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-31.938187]
objective value function right now is: 6796.469895153325
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-30.570766]
objective value function right now is: 6634.51643511932
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-28.733486]
objective value function right now is: 6602.083806801241
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-29.175846]
objective value function right now is: 6672.715045903334
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-27.686495]
objective value function right now is: 6553.487828920948
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-26.770063]
objective value function right now is: 6552.251204583759
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-25.709455]
objective value function right now is: 6523.73866727471
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-25.482073]
objective value function right now is: 6533.21024879257
min fval:  6538.1628643356225
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[10.1703, -0.6919],
        [11.7950, -0.8178],
        [11.1236, -0.7274],
        [11.3864, -4.2267],
        [10.0321, -0.8573],
        [ 1.5889, -9.0412],
        [ 9.6642, -0.9789],
        [-5.8490, -9.4436],
        [ 9.8874, -6.2196],
        [10.9149, -0.6948]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-9.5169, -8.3277, -8.5402, -5.9236, -9.3483, -6.7225, -9.4858, -6.8376,
        -6.2586, -8.8294], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.2167,  1.0707,  0.8189,  1.2529,  0.1509,  1.1014,  0.0276,  1.0890,
          1.0783,  0.6494],
        [ 2.6090,  6.4673,  3.8245,  5.5039,  2.6190,  5.0319,  2.1464,  6.1843,
          5.2602,  3.5267],
        [ 2.8921,  6.9902,  4.1401,  5.7232,  2.7691,  5.3226,  2.5602,  6.2572,
          5.7025,  3.6783],
        [ 3.2336,  7.2969,  4.3304,  6.1395,  3.1554,  6.4178,  2.8141,  7.3689,
          6.3235,  4.0245],
        [ 2.9020,  7.0075,  4.0172,  5.7816,  2.8265,  5.3539,  2.6307,  6.2575,
          5.6594,  3.8332],
        [ 3.3115,  7.2925,  4.4497,  6.0962,  3.2934,  5.6244,  2.8941,  6.7867,
          6.1693,  4.1404],
        [ 3.1559,  7.3469,  4.2947,  6.1320,  3.1415,  6.5525,  2.8202,  7.5713,
          6.3430,  3.9861],
        [ 0.2013,  1.0089,  0.7718,  1.1851,  0.1376,  1.0390,  0.0188,  1.0327,
          1.0194,  0.6113],
        [ 0.2004,  1.0056,  0.7693,  1.1815,  0.1368,  1.0357,  0.0183,  1.0296,
          1.0162,  0.6093],
        [-0.0936, -0.9074, -0.6628, -0.9542, -0.0568, -0.6042, -0.0102, -0.5676,
         -0.6947, -0.5066]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 5.7943, -9.6470, -9.5425, -8.8108, -9.5283, -9.0830, -8.7883,  5.4800,
         5.4641, -3.5556], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 6.9392, -5.3816, -6.1695, -8.4996, -6.2841, -7.6682, -8.4854,  4.5243,
          4.4266, -0.0747]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  9.3692,  -1.2491],
        [ 11.5755,   3.2997],
        [ -1.0935,  11.9674],
        [-12.1360,  -3.7230],
        [ 11.4846,   3.3752],
        [-15.8250,  -4.7123],
        [ 11.8074,   3.2719],
        [-10.7991,  -0.1079],
        [-10.2603,  -4.9382],
        [  6.4387,   8.5625]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-11.0544,  -0.4119,   9.9214,  -0.3972,  -0.2144,  -2.9309,   8.9678,
          8.9285,   1.4826,   6.7179], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.6309e+00, -3.5605e+00, -2.0009e+01,  8.1301e+00, -3.2518e+00,
          5.8015e+00, -6.2862e+00,  6.6634e+00,  6.4151e+00, -8.6538e+00],
        [ 2.6004e+00,  1.9656e+00,  3.0128e+01, -1.2316e+01,  1.5148e+00,
          2.2720e+00,  2.4591e+00, -5.0692e+00, -3.1526e+00,  2.7656e+00],
        [-2.9023e-01, -2.7264e+00, -3.6212e+00, -1.3741e+00, -2.8995e+00,
          1.7004e-02, -2.6801e+00, -8.5527e-01, -5.6772e-01, -1.7168e+00],
        [-4.4656e-02, -2.7034e+00, -1.2204e+00,  2.6555e-01, -2.7292e+00,
          9.8434e-03, -3.1127e+00, -1.7017e+00, -1.1325e+00, -3.1114e+00],
        [ 2.3628e+00, -3.0085e+00, -6.0147e+00,  1.2690e+00, -3.0152e+00,
          2.2568e+00, -2.6832e+00, -2.3981e+00,  1.4328e+00, -2.3364e+00],
        [-7.7274e-01, -1.9101e+00, -3.2383e-01, -7.2087e-02, -2.0673e+00,
          4.1378e-01, -3.0192e+00, -1.1458e+00,  3.0003e-01, -1.3964e+00],
        [ 1.7120e+00, -1.8601e+00,  5.3189e+00, -2.2807e+00, -1.8637e+00,
          1.3884e+00, -1.7612e+00,  2.0729e+00,  1.5042e+00,  1.5789e+00],
        [ 1.2081e-01, -2.7101e+00,  4.6053e-02,  1.6638e-01, -2.7431e+00,
          1.1522e+00, -3.0704e+00, -9.9610e-01, -1.9320e-01, -1.1906e+00],
        [ 3.6825e+00, -1.9556e+00, -8.4146e+00, -3.7397e+00, -2.0403e+00,
         -6.9893e-01, -1.6118e-01, -3.3174e-01, -1.4173e+00,  7.1933e-01],
        [-1.4008e+01, -1.3115e+01, -5.2726e+00,  1.4471e+01, -1.2539e+01,
          1.4265e+01, -1.9617e+01,  8.2245e+00,  1.1828e+01, -3.4916e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-0.1405, -2.4718, -3.6764, -3.1737, -3.0779, -4.4154, -3.6882, -4.8053,
        -3.2280, -3.0240], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -4.3166,   0.3964,  -0.7270,   0.4002,  -1.0453,  -0.1606,   0.6100,
           0.1858,  -2.3683,  21.3716],
        [  4.0020,  -0.4345,   0.7271,  -0.3955,   1.0442,   0.1613,  -0.6087,
          -0.1926,   2.4648, -21.4196]], device='cuda:0'))])
xi:  [-28.733486]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 334.3231471624046
W_T_median: 121.71905501774407
W_T_pctile_5: -1.8094623566835208
W_T_CVAR_5_pct: -156.59850084010262
Average q (qsum/M+1):  47.031439012096776
Optimal xi:  [-28.733486]
Expected(across Rb) median(across samples) p_equity:  0.1998487651348114
obj fun:  tensor(6538.1629, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 8.9990, -0.3262],
        [ 9.8876, -0.4440],
        [ 9.4356, -0.5183],
        [ 9.7646, -4.6575],
        [ 8.9441, -0.4609],
        [ 0.3071, -7.9568],
        [ 8.7874, -0.5348],
        [-6.7191, -8.6281],
        [ 8.0896, -6.6798],
        [ 9.3436, -0.4676]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-8.3322, -7.9010, -7.8695, -5.6807, -8.1613, -6.3807, -8.2523, -6.3050,
        -6.0354, -7.9570], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.3758,  0.6926,  0.5690,  0.8044,  0.3903,  0.5214,  0.3247,  0.5116,
          0.7126,  0.5315],
        [ 2.7444,  5.4900,  3.4247,  4.6953,  2.8366,  4.3708,  2.5155,  5.5614,
          4.7626,  3.3188],
        [ 2.9186,  5.9124,  3.6715,  4.7250,  2.8336,  4.5040,  2.7602,  5.4868,
          5.0707,  3.3905],
        [ 2.9834,  6.0099,  3.6442,  5.2626,  2.9586,  5.5675,  2.7053,  6.6458,
          5.7662,  3.5071],
        [ 2.9057,  5.9133,  3.5257,  4.7828,  2.8734,  4.5245,  2.8143,  5.4756,
          5.0249,  3.5348],
        [ 3.0205,  5.9295,  3.6933,  5.0304,  3.0448,  4.9405,  2.7271,  6.2282,
          5.5398,  3.5624],
        [ 2.9056,  6.0650,  3.6122,  5.2804,  2.9497,  5.7353,  2.7194,  6.8885,
          5.8421,  3.4720],
        [ 0.3890,  0.7511,  0.6053,  0.8280,  0.4049,  0.5395,  0.3345,  0.6620,
          0.7597,  0.5626],
        [ 0.3877,  0.7486,  0.6033,  0.8252,  0.4036,  0.5374,  0.3334,  0.6595,
          0.7571,  0.5607],
        [-0.1744, -0.3843, -0.2829, -0.4794, -0.1638, -0.2872, -0.1176, -0.2960,
         -0.4880, -0.2616]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.9646, -9.3253, -9.2669, -8.5739, -9.2580, -8.8415, -8.4978,  4.4932,
         4.4795, -2.0682], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 7.4341, -5.2359, -5.8376, -7.5516, -5.9498, -6.8216, -7.5259,  5.0033,
          4.9051, -0.0246]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 8.4683e+00, -2.0167e-01],
        [ 1.1353e+01,  2.8206e+00],
        [ 3.2721e-01,  1.1209e+01],
        [-1.0832e+01, -4.6031e+00],
        [ 1.1318e+01,  2.8317e+00],
        [-1.1295e+01, -4.5147e+00],
        [ 9.0600e+00,  5.6569e+00],
        [-9.4756e+00,  4.3600e-03],
        [-1.0676e+01, -3.2558e+00],
        [ 5.9589e+00,  8.3174e+00]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-9.1974, -0.7285,  9.6826,  0.3683, -0.7407, -3.7073,  8.4888,  7.3284,
        -0.7428,  5.7239], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.8248e+00, -3.7009e+00, -1.6972e+01,  9.6545e+00, -3.2806e+00,
          1.4198e+00, -6.0537e+00,  6.0587e+00,  5.5822e+00, -1.2052e+01],
        [ 2.6859e-01,  1.5936e+00,  3.1093e+01, -1.0472e+01,  1.1667e+00,
          4.3199e-02,  2.5539e+00, -4.4719e+00, -8.1149e+00,  1.4017e+00],
        [-1.6968e+00, -2.7066e+00, -9.5378e-01, -5.5234e-01, -3.2704e+00,
         -2.8383e-03, -5.7126e-01, -2.6409e+00, -7.5603e-02,  4.3110e-02],
        [ 2.3349e-01, -2.3148e+00, -1.3560e+00,  2.1443e-01, -2.7326e+00,
         -2.1235e-02, -7.6739e-01, -2.4080e+00,  1.9538e-01, -7.5645e-01],
        [ 2.9510e-01, -2.1712e+00, -7.2625e-01, -1.7381e-01, -2.1573e+00,
         -5.5465e-02, -1.8791e+00, -1.8790e+00, -9.4138e-03, -1.0685e+00],
        [-4.7655e-01, -4.2180e+00,  8.1559e-01,  1.5024e+00, -4.1929e+00,
          3.4347e+00, -2.0415e+00,  2.6442e-01,  2.0524e+00, -1.3025e+00],
        [ 3.0924e-01, -1.4373e+00,  2.4077e+00,  4.6505e-02, -1.4234e+00,
         -2.1666e+00, -8.2834e-01,  1.1130e+00, -1.9177e+00,  1.1523e+00],
        [-6.1451e-01, -3.1931e+00,  1.0854e+00,  1.7979e+00, -3.1592e+00,
          3.8853e+00, -1.6575e+00,  6.4426e-01,  2.3141e+00, -1.0119e+00],
        [-2.3635e+00, -3.2829e+00, -2.1386e+00, -1.1463e+00, -3.3565e+00,
         -5.2687e-03,  2.0511e-01, -2.6414e+00, -8.6978e-02,  8.3263e-01],
        [-9.5220e+00, -1.4338e+01, -9.6207e+00,  1.4071e+01, -1.3813e+01,
          9.6557e+00, -1.9712e+01,  7.2483e+00,  1.2334e+01, -2.4767e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 0.1687, -2.3780, -3.8904, -3.1639, -2.9617, -3.8232, -2.7553, -3.5565,
        -4.0998, -2.6698], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -5.3770,   0.3848,  -0.2549,   0.4640,   0.2330,   1.2895,   1.1011,
           1.1877,  -0.4883,  19.8035],
        [  5.0610,  -0.4228,   0.2550,  -0.4232,  -0.2349,  -1.2884,  -1.0997,
          -1.1954,   0.6205, -19.8532]], device='cuda:0'))])
loaded xi:  -19.891674
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 806.3616944417624
W_T_median: 547.9700475945424
W_T_pctile_5: -287.9564157269737
W_T_CVAR_5_pct: -408.2820772432722
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  159.45246390812355
Current xi:  [-16.907404]
objective value function right now is: 159.45246390812355
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.789345]
objective value function right now is: 159.61597485849887
6.0% of gradient descent iterations done. Method = Adam
new min fval:  159.13885823023972
Current xi:  [-10.835787]
objective value function right now is: 159.13885823023972
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.503046]
objective value function right now is: 161.8866070802469
10.0% of gradient descent iterations done. Method = Adam
new min fval:  158.559006563371
Current xi:  [-10.524268]
objective value function right now is: 158.559006563371
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.981882]
objective value function right now is: 159.1986929486083
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  157.55476902666652
Current xi:  [-9.939548]
objective value function right now is: 157.55476902666652
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.064936]
objective value function right now is: 159.03252016798973
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.1513467]
objective value function right now is: 162.17639577827882
20.0% of gradient descent iterations done. Method = Adam
new min fval:  156.56184327508944
Current xi:  [-8.188091]
objective value function right now is: 156.56184327508944
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.630461]
objective value function right now is: 158.0494333084056
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.4100566]
objective value function right now is: 158.62427128056868
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.4580054]
objective value function right now is: 159.47567059935417
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-13.196458]
objective value function right now is: 162.32915362231702
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.135073]
objective value function right now is: 159.16032560483833
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-11.645851]
objective value function right now is: 160.24461794063436
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.294473]
objective value function right now is: 159.57677676192537
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.769154]
objective value function right now is: 158.25161787656847
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.0018435]
objective value function right now is: 160.67073072520674
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-32.098648]
objective value function right now is: 760.1295648919574
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-50.270325]
objective value function right now is: 372.60651739447144
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-51.643265]
objective value function right now is: 174.0426192578918
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-49.392338]
objective value function right now is: 170.89561390375806
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-46.12981]
objective value function right now is: 167.81602039910092
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-40.68613]
objective value function right now is: 162.98605490102537
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.43258]
objective value function right now is: 163.71259004879278
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-33.20175]
objective value function right now is: 168.171608835831
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-28.811867]
objective value function right now is: 164.61515701111134
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-24.483803]
objective value function right now is: 160.37863091581636
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-17.986282]
objective value function right now is: 158.60406510241842
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-16.601208]
objective value function right now is: 165.42701682049253
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.473844]
objective value function right now is: 167.8349374012608
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.485189]
objective value function right now is: 158.3620193807719
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.522484]
objective value function right now is: 159.06966683332104
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.742258]
objective value function right now is: 159.29358122577642
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.00317]
objective value function right now is: 158.12649568684498
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-17.057182]
objective value function right now is: 157.97351572613113
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.893586]
objective value function right now is: 156.66945339520962
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.807013]
objective value function right now is: 156.69093616946012
80.0% of gradient descent iterations done. Method = Adam
new min fval:  156.36325837572593
Current xi:  [-13.8175955]
objective value function right now is: 156.36325837572593
82.0% of gradient descent iterations done. Method = Adam
new min fval:  156.21638202792877
Current xi:  [-12.670285]
objective value function right now is: 156.21638202792877
84.0% of gradient descent iterations done. Method = Adam
new min fval:  156.02823787078728
Current xi:  [-11.373762]
objective value function right now is: 156.02823787078728
86.0% of gradient descent iterations done. Method = Adam
new min fval:  155.95915963232065
Current xi:  [-10.464234]
objective value function right now is: 155.95915963232065
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.65976]
objective value function right now is: 157.28719035696278
90.0% of gradient descent iterations done. Method = Adam
new min fval:  155.65443231664517
Current xi:  [-8.994102]
objective value function right now is: 155.65443231664517
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.374442]
objective value function right now is: 156.4772538733331
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.118098]
objective value function right now is: 155.77856082673975
96.0% of gradient descent iterations done. Method = Adam
new min fval:  155.65393600150773
Current xi:  [-7.096751]
objective value function right now is: 155.65393600150773
new min fval from sgd:  155.40642274938406
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.813122]
objective value function right now is: 155.40642274938406
new min fval from sgd:  155.40503980649362
new min fval from sgd:  155.39319967022175
new min fval from sgd:  155.38527767266086
new min fval from sgd:  155.38464301073353
new min fval from sgd:  155.38111879081248
new min fval from sgd:  155.37634887161101
new min fval from sgd:  155.3758390593189
new min fval from sgd:  155.37406218297298
new min fval from sgd:  155.37400599027083
new min fval from sgd:  155.3737463034456
new min fval from sgd:  155.37165881638433
new min fval from sgd:  155.37036027530903
new min fval from sgd:  155.36967737625537
new min fval from sgd:  155.36928553153732
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.7024693]
objective value function right now is: 155.38812289361067
min fval:  155.36928553153732
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.1282, -0.2037],
        [ 0.1282, -0.2037],
        [ 0.1282, -0.2037],
        [ 0.1282, -0.2037],
        [ 0.1282, -0.2037],
        [ 0.1282, -0.2037],
        [ 0.1282, -0.2037],
        [ 0.1282, -0.2037],
        [ 0.1282, -0.2037],
        [ 0.1282, -0.2037]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([0.2224, 0.2224, 0.2224, 0.2224, 0.2224, 0.2224, 0.2224, 0.2224, 0.2224,
        0.2224], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411],
        [0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411, 0.2411,
         0.2411]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([0.3905, 0.3905, 0.3905, 0.3905, 0.3905, 0.3905, 0.3905, 0.3905, 0.3905,
        0.3905], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.7472, -1.7472, -1.7472, -1.7472, -1.7472, -1.7472, -1.7472, -1.7472,
         -1.7472, -1.7472]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  9.8174,  -1.6034],
        [ 12.0545,   2.5254],
        [ -1.1796,  12.3359],
        [-13.4835,  -4.3006],
        [  3.2181,   1.4356],
        [-15.9986,  -5.4463],
        [  4.4441,   1.1157],
        [-13.2292,   1.4708],
        [-13.8757,  -4.0713],
        [  7.3626,   9.0976]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-11.6182,  -4.7818,   9.8889,  -0.4936,  -8.3806,  -2.7343,   4.8146,
         10.7266,  -2.5440,   7.5393], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.6684e+00, -2.2871e+00, -2.0545e+01,  8.2722e+00, -1.6077e-02,
          2.4095e+00, -5.2193e+00,  5.7160e+00,  8.4159e+00, -8.2838e+00],
        [ 5.8126e+00,  9.2314e-01,  1.8644e+01, -1.7999e-01,  2.0884e-02,
         -7.1690e-01,  1.0493e+00, -6.3780e-01, -2.5574e+00,  3.4668e+00],
        [-5.8411e-01, -2.5672e+00, -3.3905e-01, -2.3352e-03, -1.5436e-02,
         -1.0897e-01, -2.6909e+00, -4.9261e-01, -5.3546e-02, -1.0929e+00],
        [ 1.2550e+00, -1.9938e+00, -4.2891e-01,  6.0963e-01, -2.4487e-02,
         -1.7663e-01, -2.6811e+00,  2.8978e-01, -2.9041e-02, -1.0602e+00],
        [-2.1175e+00, -4.7579e+00, -7.4954e+00,  5.9746e+00,  1.3040e-02,
         -1.7371e+00, -1.1539e-01,  1.9147e-01, -9.1106e-01, -4.4576e+00],
        [ 2.4966e+00, -6.5540e-01,  6.1113e-01,  1.0620e+00,  7.2426e-02,
          1.4587e-01,  2.4785e+00,  2.9927e+00,  8.2486e-02,  2.9396e+00],
        [ 2.9772e+00,  4.5625e+00,  8.0308e+00, -1.6699e+00,  7.3003e-02,
         -4.5381e+00,  5.8312e-01,  3.3303e+00,  2.9318e+00, -2.0307e-01],
        [-3.8217e-01, -1.2486e+00, -2.1570e-01, -1.1157e-01, -3.5568e-02,
         -5.8359e-03, -2.3491e+00, -9.1399e-01, -4.6665e-03, -1.0984e+00],
        [-3.6715e+00, -1.6564e+00,  3.3219e+00, -1.6191e-01, -2.7923e+00,
          1.4253e+00, -2.0535e+00,  7.8591e-01,  1.7219e+00,  3.4475e-01],
        [-2.1103e+00, -8.1808e+00, -9.7804e+00,  1.1960e+01,  7.0534e-03,
          1.4793e+01, -1.7303e+01,  8.9654e+00,  1.0312e+01, -2.3915e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 0.8384, -3.3416, -2.7293, -2.6861, -0.1292,  2.4806, -1.2031, -2.3529,
        -2.0582, -3.0457], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-5.1986e+00,  8.0444e-01, -5.0082e-01,  4.1430e-02, -3.3386e+00,
         -2.5710e+00,  2.1753e+00,  1.1993e-02,  1.2824e+00,  1.4855e+01],
        [ 4.9282e+00, -8.4239e-01,  5.0085e-01, -4.1315e-02,  3.3386e+00,
          2.5711e+00, -2.1739e+00, -1.1994e-02, -1.2824e+00, -1.4867e+01]],
       device='cuda:0'))])
xi:  [-6.7090735]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 775.1800624912019
W_T_median: 559.1541402249793
W_T_pctile_5: -5.036659543574745
W_T_CVAR_5_pct: -155.34841710653225
Average q (qsum/M+1):  35.0
Optimal xi:  [-6.7090735]
Expected(across Rb) median(across samples) p_equity:  0.17343460358679294
obj fun:  tensor(155.3693, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 999.0
