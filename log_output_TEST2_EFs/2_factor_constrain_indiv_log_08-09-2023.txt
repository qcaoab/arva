/home/marcchen/Documents/constrain_factor/researchcode/exp_config_json_files/multi_portfolio_TEST2_EFs.json
Starting at: 
09-08-23_18:33

 numpy seed:  3  


 pytorch seed:  3  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.005383     0.031411
192608                    0.0319              0.0561  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0355  ...     0.005323    -0.028996
192611                   -0.0038              0.0294  ...     0.005303     0.028554

[5 rows x 6 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.043289    -0.036240
202209                   -0.0955             -0.0871  ...    -0.050056    -0.091324
202210                    0.0883              0.1486  ...    -0.014968     0.077403
202211                   -0.0076              0.0462  ...     0.040789     0.052365
202212                   -0.0457             -0.0499  ...    -0.018566    -0.057116

[5 rows x 6 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'CPI_nom_ret_ind', 'T30_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Value_Hi30_real_ret
T30_real_ret             1.000000  ...             0.018239
B10_real_ret             0.351722  ...             0.031301
VWD_real_ret             0.068448  ...             0.909335
Size_Lo30_real_ret       0.014412  ...             0.908542
Value_Hi30_real_ret      0.018239  ...             1.000000

[5 rows x 5 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 192707
End: 199112
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       5       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       5              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 5)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 1500, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 940.130933411658
W_T_median: 581.3306640883857
W_T_pctile_5: -358.9604697855882
W_T_CVAR_5_pct: -487.0834540584932
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1698.3674115695521
Current xi:  [81.501625]
objective value function right now is: -1698.3674115695521
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1707.1437022870548
Current xi:  [60.98705]
objective value function right now is: -1707.1437022870548
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1713.6594167726046
Current xi:  [38.89037]
objective value function right now is: -1713.6594167726046
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1717.6628530567505
Current xi:  [16.293436]
objective value function right now is: -1717.6628530567505
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1721.2565192949671
Current xi:  [-5.3298945]
objective value function right now is: -1721.2565192949671
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1724.5068257128414
Current xi:  [-25.98768]
objective value function right now is: -1724.5068257128414
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1726.814561540036
Current xi:  [-46.4529]
objective value function right now is: -1726.814561540036
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1730.026765774683
Current xi:  [-67.85497]
objective value function right now is: -1730.026765774683
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1731.6968303746912
Current xi:  [-88.00166]
objective value function right now is: -1731.6968303746912
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1733.8511214241228
Current xi:  [-109.32622]
objective value function right now is: -1733.8511214241228
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1735.7580431117929
Current xi:  [-129.94193]
objective value function right now is: -1735.7580431117929
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1738.211685469383
Current xi:  [-150.80252]
objective value function right now is: -1738.211685469383
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1740.115697958247
Current xi:  [-172.19345]
objective value function right now is: -1740.115697958247
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1741.7811656439947
Current xi:  [-192.63805]
objective value function right now is: -1741.7811656439947
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1743.6718518270575
Current xi:  [-213.33511]
objective value function right now is: -1743.6718518270575
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1744.8763989069732
Current xi:  [-233.72789]
objective value function right now is: -1744.8763989069732
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1745.656351896639
Current xi:  [-253.76964]
objective value function right now is: -1745.656351896639
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1746.4636907868437
Current xi:  [-273.95416]
objective value function right now is: -1746.4636907868437
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1748.0000690559827
Current xi:  [-293.11975]
objective value function right now is: -1748.0000690559827
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1749.12457158273
Current xi:  [-312.4071]
objective value function right now is: -1749.12457158273
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1749.8495666327594
Current xi:  [-330.82385]
objective value function right now is: -1749.8495666327594
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1750.39240739361
Current xi:  [-348.72443]
objective value function right now is: -1750.39240739361
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1751.1802162057652
Current xi:  [-365.8834]
objective value function right now is: -1751.1802162057652
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1751.7955119349024
Current xi:  [-382.44986]
objective value function right now is: -1751.7955119349024
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1753.3002598092728
Current xi:  [-398.1609]
objective value function right now is: -1753.3002598092728
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1753.7947152891907
Current xi:  [-413.19147]
objective value function right now is: -1753.7947152891907
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1753.9071832382483
Current xi:  [-425.91525]
objective value function right now is: -1753.9071832382483
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-437.40186]
objective value function right now is: -1753.8823029141884
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1754.1347210301626
Current xi:  [-448.1907]
objective value function right now is: -1754.1347210301626
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1754.1751529252251
Current xi:  [-455.98193]
objective value function right now is: -1754.1751529252251
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1754.256082396247
Current xi:  [-461.3926]
objective value function right now is: -1754.256082396247
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1754.2745287987586
Current xi:  [-465.4604]
objective value function right now is: -1754.2745287987586
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-468.8163]
objective value function right now is: -1753.4598862047485
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-470.82263]
objective value function right now is: -1753.8399682201218
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-469.64142]
objective value function right now is: -1754.1146192669012
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1754.5229657129555
Current xi:  [-469.53043]
objective value function right now is: -1754.5229657129555
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1754.5968893099641
Current xi:  [-469.83255]
objective value function right now is: -1754.5968893099641
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-469.4805]
objective value function right now is: -1754.5575643671327
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-469.9461]
objective value function right now is: -1754.5054802344348
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-469.9084]
objective value function right now is: -1754.5849124784447
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-470.52045]
objective value function right now is: -1754.5607084554101
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-470.42996]
objective value function right now is: -1754.5267785550134
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-470.2761]
objective value function right now is: -1754.5781407596005
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-471.41455]
objective value function right now is: -1754.5811226103567
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-471.69962]
objective value function right now is: -1754.5608104589069
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -1754.5984308096795
Current xi:  [-471.4115]
objective value function right now is: -1754.5984308096795
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-471.89154]
objective value function right now is: -1754.573941449227
new min fval from sgd:  -1754.5987181930077
new min fval from sgd:  -1754.6017990563312
new min fval from sgd:  -1754.6024275237926
new min fval from sgd:  -1754.6048451383963
new min fval from sgd:  -1754.6082968077008
new min fval from sgd:  -1754.6083769276654
new min fval from sgd:  -1754.6104927751624
new min fval from sgd:  -1754.6123279960584
new min fval from sgd:  -1754.6164441950964
new min fval from sgd:  -1754.6201860614733
new min fval from sgd:  -1754.6205980956454
new min fval from sgd:  -1754.6208041087048
new min fval from sgd:  -1754.6235045451333
new min fval from sgd:  -1754.6259443835945
new min fval from sgd:  -1754.6266068747643
new min fval from sgd:  -1754.6275536116543
new min fval from sgd:  -1754.6322242272906
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-471.73462]
objective value function right now is: -1754.614883248915
new min fval from sgd:  -1754.6330073423214
new min fval from sgd:  -1754.6331126497375
new min fval from sgd:  -1754.6348919245017
new min fval from sgd:  -1754.6353061554782
new min fval from sgd:  -1754.635774587863
new min fval from sgd:  -1754.636630945774
new min fval from sgd:  -1754.6384277358916
new min fval from sgd:  -1754.6399667394041
new min fval from sgd:  -1754.6407446634983
new min fval from sgd:  -1754.6409484219769
new min fval from sgd:  -1754.6415656751162
new min fval from sgd:  -1754.642899560423
new min fval from sgd:  -1754.6443532291996
new min fval from sgd:  -1754.6452275035938
new min fval from sgd:  -1754.64635031004
new min fval from sgd:  -1754.6471760273687
new min fval from sgd:  -1754.647730272409
new min fval from sgd:  -1754.648636505285
new min fval from sgd:  -1754.6499321489794
new min fval from sgd:  -1754.6507260329179
new min fval from sgd:  -1754.6514658033727
new min fval from sgd:  -1754.651708810452
new min fval from sgd:  -1754.6521075774506
new min fval from sgd:  -1754.6525653263577
new min fval from sgd:  -1754.6532162673884
new min fval from sgd:  -1754.65368770985
new min fval from sgd:  -1754.654073489619
new min fval from sgd:  -1754.6545790355815
new min fval from sgd:  -1754.6550722077238
new min fval from sgd:  -1754.6556146113408
new min fval from sgd:  -1754.6558895137991
new min fval from sgd:  -1754.6559388377802
new min fval from sgd:  -1754.6566237733996
new min fval from sgd:  -1754.6572945346338
new min fval from sgd:  -1754.6576945560669
new min fval from sgd:  -1754.6583949052622
new min fval from sgd:  -1754.6587333249197
new min fval from sgd:  -1754.6589514649393
new min fval from sgd:  -1754.659052759112
new min fval from sgd:  -1754.6593601358938
new min fval from sgd:  -1754.6596635042647
new min fval from sgd:  -1754.659933660972
new min fval from sgd:  -1754.6600067919326
new min fval from sgd:  -1754.660724115355
new min fval from sgd:  -1754.66184476108
new min fval from sgd:  -1754.6636374990019
new min fval from sgd:  -1754.6649005134448
new min fval from sgd:  -1754.6655189477685
new min fval from sgd:  -1754.6659215515833
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-471.57886]
objective value function right now is: -1754.644395420937
new min fval from sgd:  -1754.6659311948724
new min fval from sgd:  -1754.6665178558749
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-471.7364]
objective value function right now is: -1754.6486315143673
min fval:  -1754.6665178558749
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-6.2506,  2.5071],
        [-0.5797,  1.2472],
        [-0.5797,  1.2472],
        [-0.5797,  1.2472],
        [ 8.3507, -4.8868],
        [-0.5786,  1.2471],
        [-0.2674,  1.6580],
        [-4.1403,  8.1916],
        [13.8968,  2.7439],
        [ 5.2711,  3.8861],
        [ 9.8531,  5.4387],
        [-0.5796,  1.2472],
        [-2.9209,  6.9192]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 5.3010, -0.9728, -0.9728, -0.9728, -9.9189, -0.9734, -0.9517, 11.4423,
        -7.3925, -2.6845, -2.2631, -0.9730,  6.9742], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.2998e-02, -9.7525e-03, -9.7524e-03, -9.7525e-03, -5.2619e-01,
         -9.7381e-03, -5.7660e-03, -7.0136e-02, -1.0754e-01,  1.1160e-03,
         -4.3273e-03, -9.7493e-03,  6.6830e-04],
        [-2.1972e+00, -1.6149e-02, -1.6149e-02, -1.6149e-02,  4.3529e+00,
          1.5208e-02,  5.1720e-02, -6.8459e+00, -9.1724e+00, -5.0593e-01,
         -2.3878e+00, -9.1701e-03, -1.1749e+00],
        [-3.2998e-02, -9.7525e-03, -9.7526e-03, -9.7526e-03, -5.2618e-01,
         -9.7382e-03, -5.7661e-03, -7.0133e-02, -1.0754e-01,  1.1158e-03,
         -4.3277e-03, -9.7493e-03,  6.6814e-04],
        [ 2.0006e+00,  9.6829e-02,  9.6829e-02,  9.6829e-02, -3.5824e+00,
          8.9145e-02, -2.2602e-02,  2.7715e+00, -2.5994e-01, -2.1789e-02,
         -3.1753e-01,  9.5874e-02,  4.2590e-01],
        [-3.2998e-02, -9.7525e-03, -9.7525e-03, -9.7525e-03, -5.2619e-01,
         -9.7381e-03, -5.7660e-03, -7.0136e-02, -1.0754e-01,  1.1160e-03,
         -4.3273e-03, -9.7493e-03,  6.6829e-04],
        [ 2.9407e+00,  2.3006e-03,  2.3007e-03,  2.3006e-03, -5.2603e+00,
          4.0613e-02,  1.3730e-01,  8.4186e+00,  1.0812e+01,  9.9893e-01,
          3.0300e+00,  1.0659e-02,  1.9889e+00],
        [-3.2998e-02, -9.7525e-03, -9.7525e-03, -9.7525e-03, -5.2619e-01,
         -9.7381e-03, -5.7660e-03, -7.0136e-02, -1.0754e-01,  1.1160e-03,
         -4.3273e-03, -9.7493e-03,  6.6829e-04],
        [-3.2998e-02, -9.7525e-03, -9.7525e-03, -9.7525e-03, -5.2619e-01,
         -9.7381e-03, -5.7660e-03, -7.0136e-02, -1.0754e-01,  1.1160e-03,
         -4.3274e-03, -9.7493e-03,  6.6829e-04],
        [-3.2998e-02, -9.7524e-03, -9.7525e-03, -9.7525e-03, -5.2619e-01,
         -9.7381e-03, -5.7660e-03, -7.0136e-02, -1.0754e-01,  1.1160e-03,
         -4.3273e-03, -9.7493e-03,  6.6830e-04],
        [-3.2998e-02, -9.7525e-03, -9.7525e-03, -9.7525e-03, -5.2619e-01,
         -9.7381e-03, -5.7660e-03, -7.0136e-02, -1.0754e-01,  1.1160e-03,
         -4.3273e-03, -9.7493e-03,  6.6830e-04],
        [-1.6024e+00, -1.2423e-02, -1.2423e-02, -1.2423e-02,  3.9235e+00,
          2.2214e-02,  1.0609e-01, -5.1073e+00, -7.5316e+00, -2.4816e-01,
         -1.8233e+00, -4.8780e-03, -5.7095e-01],
        [-3.2998e-02, -9.7524e-03, -9.7524e-03, -9.7524e-03, -5.2619e-01,
         -9.7381e-03, -5.7660e-03, -7.0136e-02, -1.0754e-01,  1.1160e-03,
         -4.3273e-03, -9.7493e-03,  6.6830e-04],
        [-3.2998e-02, -9.7525e-03, -9.7524e-03, -9.7525e-03, -5.2619e-01,
         -9.7381e-03, -5.7660e-03, -7.0136e-02, -1.0754e-01,  1.1160e-03,
         -4.3273e-03, -9.7493e-03,  6.6829e-04]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.6953,  0.8453, -0.6953, -0.0405, -0.6953, -1.1209, -0.6953, -0.6953,
        -0.6953, -0.6953,  0.1933, -0.6953, -0.6953], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 0.0299, -7.5291,  0.0299,  3.7610,  0.0299, 11.5565,  0.0299,  0.0299,
          0.0299,  0.0299, -5.3344,  0.0299,  0.0299]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.8300,   1.5077],
        [  1.9495,  10.1056],
        [ -7.2238,   0.4238],
        [-11.1789, -10.5442],
        [ 10.2840,   9.8062],
        [  4.8311,  -8.1338],
        [  9.0804,  -0.5139],
        [ -2.0066,   1.9311],
        [ 11.9814,   4.0576],
        [  8.9103,  -0.5410],
        [  9.8132,  -0.4348],
        [  1.1805,  13.6348],
        [ -1.5385,   1.0510]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-1.8887,  8.2124,  5.1185, -7.9034,  2.1704, -5.3768, -9.3425, -1.8126,
        -1.2425, -9.2958, -9.6873,  8.3569, -2.0888], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 4.2349e-03, -5.3785e-01, -2.7583e-01, -3.0736e-01, -5.5638e-01,
         -1.1879e+00, -4.8293e-01,  3.2810e-03, -9.9995e-01, -4.7186e-01,
         -5.1796e-01, -4.0185e-01,  3.3190e-03],
        [-2.2149e-01, -6.2096e+00,  2.7542e+00,  7.8382e+00, -1.8794e-01,
         -2.6448e-01, -3.5434e+00, -2.1397e-01, -6.7086e+00, -3.4773e+00,
         -3.7602e+00, -1.6899e+00, -1.7140e-01],
        [-3.8953e-01,  9.3821e-01,  1.8680e+00, -4.6885e-01, -2.6562e+00,
         -2.4643e+00,  1.7149e-01, -4.3298e-01, -2.4551e+00,  1.7435e-01,
          8.9414e-02,  2.0492e+00, -2.7660e-01],
        [ 3.9510e-01,  1.2811e+00, -2.4980e+00, -4.3437e+00, -5.0447e+00,
          3.0223e+00,  1.3390e+00,  4.4863e-01,  1.1329e+00,  1.4063e+00,
          1.5474e+00, -4.9715e+00,  2.2677e-01],
        [-2.0000e-01,  2.6155e-01, -5.8047e+00, -4.8727e+00,  4.2714e+00,
         -1.5290e+00,  5.2964e+00, -1.8066e-01, -3.0523e+00,  5.0035e+00,
          6.9406e+00, -5.3975e+00, -2.2586e-01],
        [ 1.2581e-02, -5.2651e-01, -2.9006e-01, -3.1629e-01, -5.5216e-01,
         -1.2064e+00, -4.7026e-01,  1.3766e-02, -1.0021e+00, -4.6007e-01,
         -5.0223e-01, -4.0766e-01,  9.2204e-03],
        [ 5.1915e-03, -5.3580e-01, -2.7703e-01, -3.0820e-01, -5.5525e-01,
         -1.1903e+00, -4.8154e-01,  4.4325e-03, -1.0005e+00, -4.7058e-01,
         -5.1620e-01, -4.0227e-01,  4.0147e-03],
        [-1.3865e-01,  2.6204e+00, -1.2903e+00, -3.5101e+00, -2.8499e-01,
         -2.3828e-01, -2.6940e-02, -2.0118e-01,  4.1733e+00, -2.1407e-01,
          5.5108e-01, -2.3241e+00, -7.2164e-02],
        [ 2.1785e-02,  3.9508e+00, -1.3481e+00,  1.9375e-01,  4.7086e+00,
         -4.4213e+00,  3.7715e+00,  3.4988e-02, -1.3511e+00,  3.5865e+00,
          4.7246e+00,  6.7888e+00,  2.1265e-02],
        [ 8.1797e-01, -2.4911e+00,  1.3529e+00,  8.6413e+00,  2.3739e+00,
         -2.9161e+00, -5.4953e+00,  1.1730e+00, -4.0685e+00, -5.1185e+00,
         -7.0111e+00,  7.9237e+00,  2.9928e-01],
        [ 8.2143e-03, -5.3065e-01, -2.8147e-01, -3.1126e-01, -5.5253e-01,
         -1.1981e+00, -4.7683e-01,  8.1278e-03, -1.0011e+00, -4.6618e-01,
         -5.1035e-01, -4.0417e-01,  6.1528e-03],
        [ 5.1549e-02,  3.8873e-02,  1.3112e+00,  1.2078e+00, -1.9396e+00,
         -2.2448e+00, -8.0858e-01, -1.0371e-02, -2.7024e+00, -8.1369e-01,
         -7.8556e-01,  7.3942e-01,  9.3851e-02],
        [ 9.6573e-02, -3.5548e+00,  2.7241e+00,  2.5573e+00, -8.6916e+00,
          4.9229e+00, -4.2862e+00,  3.5962e-02, -4.3408e+00, -3.9747e+00,
         -5.8638e+00, -1.2690e+01,  1.2462e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.3663, -2.9491, -1.8923,  0.5164, -5.0000, -1.3734, -1.3653,  0.8073,
        -1.3745, -1.3749, -1.3654, -0.6252,  0.7602], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 5.4145e-03,  9.6378e+00,  1.6917e+00, -1.0411e+01, -7.1773e-01,
          1.5312e-01,  1.6245e-02, -2.5960e+00,  4.4925e+00,  4.1745e+00,
          6.4245e-02,  8.1868e-01, -3.4587e+00],
        [-4.8228e-02, -1.1359e+00, -1.5402e+00,  3.4142e-01,  3.5408e+00,
          1.3969e-01, -3.2872e-02,  1.0204e+00,  1.4856e+00,  1.4067e+00,
          3.0190e-02, -1.6313e+00, -4.4155e+00],
        [-3.0115e-03, -8.4063e+00,  4.0957e-01,  5.1908e-01, -2.8890e+00,
          2.0643e-01,  1.4233e-02,  6.7106e-01, -7.9216e-01, -3.9931e+00,
          8.4554e-02,  8.4146e-01,  6.4355e+00],
        [ 1.6048e-02, -1.0310e+01,  1.3798e-02,  3.1272e+00, -1.6645e+00,
          1.1133e-02,  1.5200e-02,  8.3446e-01, -9.6271e-01, -6.8014e+00,
          1.2912e-02, -1.0164e+00,  7.6607e+00],
        [-1.4590e-01, -6.8268e+00, -2.6917e+00,  4.7215e+00,  2.0940e+00,
         -1.4376e-01, -1.4540e-01,  7.0301e+00,  1.9285e+00, -9.6105e+00,
         -1.4324e-01, -1.9482e+00,  4.7369e+00]], device='cuda:0'))])
xi:  [-471.66946]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 119.73824764369274
W_T_median: 86.82312713910993
W_T_pctile_5: -472.8995239779451
W_T_CVAR_5_pct: -543.6143930794857
Average q (qsum/M+1):  57.47896059097782
Optimal xi:  [-471.66946]
Expected(across Rb) median(across samples) p_equity:  0.166629250292317
obj fun:  tensor(-1754.6665, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 940.130933411658
W_T_median: 581.3306640883857
W_T_pctile_5: -358.9604697855882
W_T_CVAR_5_pct: -487.0834540584932
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1616.6236650702995
Current xi:  [77.443886]
objective value function right now is: -1616.6236650702995
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1643.3443699961924
Current xi:  [55.82946]
objective value function right now is: -1643.3443699961924
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1652.8973397515113
Current xi:  [36.372738]
objective value function right now is: -1652.8973397515113
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1659.2999521391719
Current xi:  [15.8322]
objective value function right now is: -1659.2999521391719
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1663.2432077007754
Current xi:  [-4.3427305]
objective value function right now is: -1663.2432077007754
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1666.7591911158734
Current xi:  [-17.016077]
objective value function right now is: -1666.7591911158734
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1669.8360713017992
Current xi:  [-35.223278]
objective value function right now is: -1669.8360713017992
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.4330689244239
Current xi:  [-52.106472]
objective value function right now is: -1673.4330689244239
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1675.737068834459
Current xi:  [-71.75931]
objective value function right now is: -1675.737068834459
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1677.1198746986302
Current xi:  [-85.54913]
objective value function right now is: -1677.1198746986302
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1679.2671186370837
Current xi:  [-103.51644]
objective value function right now is: -1679.2671186370837
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1681.1123283853283
Current xi:  [-118.01343]
objective value function right now is: -1681.1123283853283
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1682.1208855600405
Current xi:  [-133.46576]
objective value function right now is: -1682.1208855600405
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1683.655195163956
Current xi:  [-150.17413]
objective value function right now is: -1683.655195163956
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-162.19989]
objective value function right now is: -1683.6492495842422
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1685.3549376388694
Current xi:  [-179.41544]
objective value function right now is: -1685.3549376388694
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.830249585553
Current xi:  [-191.29224]
objective value function right now is: -1686.830249585553
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-202.28348]
objective value function right now is: -1686.665214945824
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1687.9261566782477
Current xi:  [-217.25381]
objective value function right now is: -1687.9261566782477
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1688.2010747984305
Current xi:  [-226.6646]
objective value function right now is: -1688.2010747984305
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-231.11952]
objective value function right now is: -1687.5509330624698
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1688.278690675517
Current xi:  [-234.80228]
objective value function right now is: -1688.278690675517
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1688.3483602189647
Current xi:  [-238.32204]
objective value function right now is: -1688.3483602189647
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1688.5156405507316
Current xi:  [-243.21794]
objective value function right now is: -1688.5156405507316
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1688.6079757021225
Current xi:  [-251.37941]
objective value function right now is: -1688.6079757021225
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-258.82462]
objective value function right now is: -1687.7954377604954
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1689.0292544706635
Current xi:  [-260.69223]
objective value function right now is: -1689.0292544706635
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-260.80292]
objective value function right now is: -1688.3322908372413
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-261.10672]
objective value function right now is: -1688.750613895209
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-261.51965]
objective value function right now is: -1688.6599214274338
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-261.3179]
objective value function right now is: -1688.1770336250563
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-261.97437]
objective value function right now is: -1688.0444470592042
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-262.04538]
objective value function right now is: -1687.893942997107
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-262.01328]
objective value function right now is: -1688.759229003284
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-262.24792]
objective value function right now is: -1688.5136142671802
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1689.3455095516258
Current xi:  [-261.93655]
objective value function right now is: -1689.3455095516258
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-261.8051]
objective value function right now is: -1689.3444387163888
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1689.357075992119
Current xi:  [-261.54587]
objective value function right now is: -1689.357075992119
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-261.26263]
objective value function right now is: -1689.345418942378
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1689.3817097795352
Current xi:  [-261.3864]
objective value function right now is: -1689.3817097795352
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-261.18677]
objective value function right now is: -1689.317512002767
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1689.3952067523962
Current xi:  [-261.28378]
objective value function right now is: -1689.3952067523962
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1689.4269238881498
Current xi:  [-261.19244]
objective value function right now is: -1689.4269238881498
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-261.4504]
objective value function right now is: -1689.322650361923
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-261.38782]
objective value function right now is: -1689.2438824418357
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-261.30786]
objective value function right now is: -1689.3877242102396
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-261.1925]
objective value function right now is: -1689.3672648356956
new min fval from sgd:  -1689.431251236856
new min fval from sgd:  -1689.445644468525
new min fval from sgd:  -1689.4579626660818
new min fval from sgd:  -1689.4680925388295
new min fval from sgd:  -1689.4735516588087
new min fval from sgd:  -1689.475046137945
new min fval from sgd:  -1689.4774041525156
new min fval from sgd:  -1689.4799088870986
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-261.1107]
objective value function right now is: -1689.3517288151584
new min fval from sgd:  -1689.4924918584381
new min fval from sgd:  -1689.5104684975338
new min fval from sgd:  -1689.5119166843897
new min fval from sgd:  -1689.5136154276654
new min fval from sgd:  -1689.5152470559506
new min fval from sgd:  -1689.516816462884
new min fval from sgd:  -1689.517699363908
new min fval from sgd:  -1689.5180497649042
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-261.42734]
objective value function right now is: -1689.5133550873322
new min fval from sgd:  -1689.5183131263627
new min fval from sgd:  -1689.5190551687683
new min fval from sgd:  -1689.5205663267193
new min fval from sgd:  -1689.5218840999548
new min fval from sgd:  -1689.5229306693584
new min fval from sgd:  -1689.524401479318
new min fval from sgd:  -1689.5253215376645
new min fval from sgd:  -1689.525913609705
new min fval from sgd:  -1689.5259449694083
new min fval from sgd:  -1689.5261138635083
new min fval from sgd:  -1689.5261466080049
new min fval from sgd:  -1689.5264114477118
new min fval from sgd:  -1689.526423188082
new min fval from sgd:  -1689.5267564139679
new min fval from sgd:  -1689.5270153605245
new min fval from sgd:  -1689.5274837620068
new min fval from sgd:  -1689.5278093221523
new min fval from sgd:  -1689.52825580511
new min fval from sgd:  -1689.5299809438377
new min fval from sgd:  -1689.5306966701569
new min fval from sgd:  -1689.5307044857323
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-261.3953]
objective value function right now is: -1689.522737003873
min fval:  -1689.5307044857323
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.9005,  0.3587],
        [-2.1322,  5.0395],
        [-9.0100,  4.9623],
        [ 3.9484,  6.3050],
        [-5.0695,  5.4144],
        [-2.3805,  5.2011],
        [-4.4784, -5.3749],
        [-4.6582, -5.5714],
        [-4.4085, -5.3113],
        [ 6.5022,  6.6204],
        [-5.1915, -6.1870],
        [-4.6716,  5.3826],
        [-4.3636, -5.2779]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-2.2698,  2.6561,  6.7849,  0.5081,  6.2593,  3.0967, -3.9665, -3.8785,
        -4.0467,  2.1299, -3.8653,  5.8486, -4.1274], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-7.7342e-03, -3.6271e-03, -4.5255e-02, -9.8674e-03, -7.7321e-02,
         -5.4762e-03, -9.6697e-02, -1.1699e-01, -8.6142e-02, -2.7918e-01,
         -1.5735e-01, -6.2106e-02, -7.7782e-02],
        [-1.1132e-01,  8.3696e-01,  6.9416e+00,  4.1263e-01,  4.9622e+00,
          1.0812e+00, -2.4227e+00, -2.8257e+00, -2.2942e+00,  3.3076e+00,
         -3.8489e+00,  4.2237e+00, -2.1562e+00],
        [-7.7342e-03, -3.6271e-03, -4.5255e-02, -9.8674e-03, -7.7321e-02,
         -5.4762e-03, -9.6697e-02, -1.1699e-01, -8.6142e-02, -2.7918e-01,
         -1.5735e-01, -6.2106e-02, -7.7782e-02],
        [-4.9791e-02,  6.8393e-01,  6.3630e+00,  2.8449e-01,  4.5807e+00,
          9.0171e-01, -2.1309e+00, -2.5234e+00, -2.0033e+00,  3.0338e+00,
         -3.5504e+00,  3.8376e+00, -2.0409e+00],
        [-7.7342e-03, -3.6271e-03, -4.5255e-02, -9.8674e-03, -7.7321e-02,
         -5.4762e-03, -9.6697e-02, -1.1699e-01, -8.6142e-02, -2.7918e-01,
         -1.5735e-01, -6.2106e-02, -7.7782e-02],
        [-7.7312e-03, -3.6235e-03, -4.5273e-02, -9.8616e-03, -7.7382e-02,
         -5.4746e-03, -9.6776e-02, -1.1708e-01, -8.6212e-02, -2.7936e-01,
         -1.5746e-01, -6.2149e-02, -7.7845e-02],
        [-7.7341e-03, -3.6270e-03, -4.5256e-02, -9.8672e-03, -7.7323e-02,
         -5.4761e-03, -9.6700e-02, -1.1699e-01, -8.6144e-02, -2.7919e-01,
         -1.5735e-01, -6.2107e-02, -7.7784e-02],
        [-5.2219e-03, -8.2977e-02, -1.1260e+00,  1.9762e-01, -1.0479e+00,
         -1.2316e-01,  2.5881e-01,  3.3209e-01,  2.2941e-01, -1.2939e+00,
          6.6655e-01, -8.3275e-01,  2.0024e-01],
        [-9.9749e-02,  8.1699e-01,  6.8672e+00,  4.8924e-01,  4.8760e+00,
          1.0535e+00, -2.3553e+00, -2.6354e+00, -2.2336e+00,  3.4271e+00,
         -3.6243e+00,  4.1441e+00, -2.1741e+00],
        [ 1.6724e-01, -8.4966e-01, -6.4098e+00, -3.4596e-01, -4.5224e+00,
         -1.0889e+00,  2.3253e+00,  2.5078e+00,  2.2354e+00, -3.3633e+00,
          3.5821e+00, -3.8685e+00,  2.0471e+00],
        [-2.7350e-02, -6.0193e-02, -5.7465e-01,  8.1683e-02, -6.2817e-01,
         -8.5415e-02, -5.1981e-02, -2.2565e-02, -6.0739e-02, -9.2259e-01,
          9.9990e-02, -5.1347e-01, -6.7604e-02],
        [-7.7342e-03, -3.6271e-03, -4.5255e-02, -9.8674e-03, -7.7321e-02,
         -5.4762e-03, -9.6697e-02, -1.1699e-01, -8.6142e-02, -2.7918e-01,
         -1.5735e-01, -6.2106e-02, -7.7782e-02],
        [ 9.7866e-02, -6.0123e-01, -5.4305e+00, -1.1223e-01, -3.8761e+00,
         -8.0055e-01,  1.9747e+00,  2.2436e+00,  1.8809e+00, -2.9746e+00,
          3.1304e+00, -3.2846e+00,  1.8222e+00]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.7247, -0.6797, -0.7247, -0.7275, -0.7247, -0.7249, -0.7247, -0.5540,
        -0.8670,  0.5635, -0.7918, -0.7247,  0.4474], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-0.0194,  6.2851, -0.0194,  5.4414, -0.0194, -0.0194, -0.0194, -1.4786,
          6.1540, -7.4730, -0.8577, -0.0194, -6.0218]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  5.7307,  10.3722],
        [ -0.9885,   1.9760],
        [-11.6621,  -4.9359],
        [ -0.9877,   1.9772],
        [ -7.9457,   1.6448],
        [ -6.7021,   7.9452],
        [-14.6239, -12.2138],
        [  8.4808,   8.1220],
        [ -0.9813,   2.3461],
        [ 11.6672,  -0.7682],
        [ -0.4220, -13.9865],
        [-12.3211,  -5.0035],
        [ -7.2393, -10.5970]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.9967,  -2.2665,  -2.5241,  -2.2661,   6.4737,   6.7377,  -8.2374,
          0.9597,  -2.3529, -11.0195,  -8.5659,  -0.9544,  -7.7858],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.1465e+00, -8.6688e-02, -2.5212e-01, -8.6753e-02, -7.5758e-01,
         -3.0069e-01, -2.5113e-01, -6.5574e-01, -9.9569e-02, -5.3055e-01,
         -1.1959e+00, -2.8654e-01, -4.4953e-01],
        [-5.7603e+00, -5.4615e-02,  4.0306e+00, -5.4631e-02,  2.7051e+00,
         -5.0121e+00,  7.6043e+00, -1.4978e-01, -5.0955e-02, -3.7244e+00,
         -2.7569e+00,  4.9557e+00,  6.8239e-01],
        [-7.0661e+00, -3.3227e-01, -3.9220e-01, -3.3247e-01, -6.8050e+00,
         -1.0002e+00,  3.0843e+00, -2.0889e+00, -2.9279e-01,  5.8266e+00,
          3.0369e+00, -1.8214e+00,  9.0234e+00],
        [-1.1465e+00, -8.6723e-02, -2.5216e-01, -8.6788e-02, -7.5772e-01,
         -3.0085e-01, -2.5117e-01, -6.5562e-01, -9.9612e-02, -5.3050e-01,
         -1.1961e+00, -2.8655e-01, -4.4955e-01],
        [ 6.6342e+00,  7.2646e-01, -2.8821e+00,  7.2794e-01,  1.6604e+00,
         -1.0495e+01, -1.0416e+01,  2.0723e+00,  9.3090e-01,  4.9612e+00,
          1.1103e+00, -2.8696e+00, -3.5001e+00],
        [-3.5710e+00,  5.5610e-01,  5.0150e+00,  5.5776e-01,  2.6052e+00,
          5.4622e+00,  1.6239e+00, -1.0157e+00,  9.3309e-01, -3.9194e+00,
         -3.0960e-01,  7.0095e+00,  3.3499e+00],
        [-7.5036e+00,  2.4748e-01, -2.0909e+00,  2.4821e-01, -5.2637e+00,
         -5.4581e+00,  1.3992e+00,  4.3196e+00,  3.3144e-01,  4.4181e+00,
          3.7120e+00, -2.2721e+00,  1.0554e+01],
        [-6.6824e+00,  3.7760e-03, -2.4192e+00,  3.8446e-03,  7.8441e-02,
         -3.3260e+00, -2.7348e+00, -2.6435e+00, -1.7173e-02, -2.2442e+00,
          7.1594e+00,  5.2992e-01,  1.0397e+01],
        [-1.1468e+00, -8.6962e-02, -2.5243e-01, -8.7027e-02, -7.5873e-01,
         -3.0196e-01, -2.5144e-01, -6.5476e-01, -9.9917e-02, -5.3009e-01,
         -1.1976e+00, -2.8673e-01, -4.4962e-01],
        [ 6.9020e-02, -3.5366e-01,  7.3727e-02, -3.5421e-01,  3.2556e+00,
          2.7568e+00,  8.7558e+00, -1.3310e+00, -8.0435e-01, -1.1421e+01,
         -1.2091e+01,  2.8771e+00,  4.9002e+00],
        [-1.1466e+00, -8.6766e-02, -2.5222e-01, -8.6831e-02, -7.5793e-01,
         -3.0104e-01, -2.5123e-01, -6.5545e-01, -9.9667e-02, -5.3040e-01,
         -1.1964e+00, -2.8661e-01, -4.4956e-01],
        [ 3.9977e+00,  1.0230e-01, -2.9502e+00,  1.0231e-01, -3.3997e+00,
          6.9213e+00, -6.0721e+00, -1.6721e-01,  9.6120e-02, -9.2112e-02,
          2.2754e+00, -4.1145e+00,  9.6277e-02],
        [-5.2019e+00, -1.5580e-01, -2.3832e-01, -1.5604e-01, -1.0077e+01,
         -1.3040e+00,  1.3934e+00, -2.4453e+00, -1.6216e-01,  1.1122e+01,
         -1.1955e-01, -6.7677e-01, -3.2258e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.7220, -5.4627, -0.3501, -1.7220,  2.0246, -1.9344,  1.5734,  1.2950,
        -1.7220, -0.2105, -1.7220,  4.7312, -2.3841], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 2.5384e-01,  1.4699e+01, -1.2527e-01,  2.5387e-01,  2.4631e+00,
         -2.4857e+00,  2.4552e+00, -1.1262e+01,  2.5800e-01,  4.5825e+00,
          2.5646e-01,  7.3007e-01, -1.4754e+00],
        [-5.0655e-02, -4.5045e+00, -1.0286e+00, -5.0923e-02,  4.6496e+00,
         -2.5918e+00,  5.1534e+00, -2.0349e+00, -4.8770e-02,  2.4714e+00,
         -4.8659e-02, -1.4004e+00,  8.4992e+00],
        [-1.4236e-01, -3.5107e+00,  3.2859e+00, -1.4270e-01, -6.4454e+00,
          3.2212e+00, -3.8894e+00,  5.7783e+00, -1.4123e-01, -9.1126e+00,
         -1.4065e-01,  1.5486e+00, -4.4243e+00],
        [ 1.4106e-02, -4.7033e+00, -2.5116e+00,  1.4095e-02, -5.8319e+00,
          3.6668e+00,  1.4787e+00,  4.2793e+00,  1.4049e-02, -1.2425e+01,
          1.4100e-02,  5.2121e+00, -2.0343e+00],
        [ 1.4794e-02, -5.5233e+00,  2.4940e-01,  1.4812e-02,  1.5072e+00,
         -2.0660e+00, -7.3843e-01,  7.7508e+00,  1.4977e-02, -9.5557e+00,
          1.4858e-02,  8.5768e+00,  8.4778e+00]], device='cuda:0'))])
xi:  [-261.40955]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 122.2152814332989
W_T_median: 80.16790460467709
W_T_pctile_5: -260.62127534931653
W_T_CVAR_5_pct: -339.49301957969374
Average q (qsum/M+1):  56.69131568170363
Optimal xi:  [-261.40955]
Expected(across Rb) median(across samples) p_equity:  0.21960790711455047
obj fun:  tensor(-1689.5307, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 940.130933411658
W_T_median: 581.3306640883857
W_T_pctile_5: -358.9604697855882
W_T_CVAR_5_pct: -487.0834540584932
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1540.678183439089
Current xi:  [77.14911]
objective value function right now is: -1540.678183439089
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1581.5685458410258
Current xi:  [57.923435]
objective value function right now is: -1581.5685458410258
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1590.5505866650788
Current xi:  [41.305176]
objective value function right now is: -1590.5505866650788
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [25.512716]
objective value function right now is: -1577.4402535224656
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.7780285]
objective value function right now is: -1573.8741871340887
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1598.229885652161
Current xi:  [-0.46908447]
objective value function right now is: -1598.229885652161
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1605.1069814023294
Current xi:  [-3.3693058]
objective value function right now is: -1605.1069814023294
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.1363807]
objective value function right now is: -1602.7024299556592
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1606.9729469689723
Current xi:  [-16.474127]
objective value function right now is: -1606.9729469689723
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-33.61585]
objective value function right now is: -1598.3917114977296
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-41.672512]
objective value function right now is: -1601.2748278355211
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-51.823853]
objective value function right now is: -1601.5155364290415
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-63.18314]
objective value function right now is: -1603.4990444979114
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-73.14463]
objective value function right now is: -1604.3886578905945
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.44991]
objective value function right now is: -1604.614280012298
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-76.17563]
objective value function right now is: -1605.196633469272
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-77.970436]
objective value function right now is: -1604.4533462238446
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-79.311646]
objective value function right now is: -1605.0744105552608
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-82.09692]
objective value function right now is: -1604.9373927692686
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-87.56003]
objective value function right now is: -1605.1055688999143
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-93.87736]
objective value function right now is: -1606.1705424240627
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-100.94524]
objective value function right now is: -1606.315168765768
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-108.34965]
objective value function right now is: -1606.213752939381
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-108.96498]
objective value function right now is: -1606.5920536336155
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-108.77113]
objective value function right now is: -1606.3490404373458
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1607.000306639848
Current xi:  [-108.96677]
objective value function right now is: -1607.000306639848
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-109.228165]
objective value function right now is: -1606.3148156561429
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-109.045586]
objective value function right now is: -1605.3235902424146
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-109.59706]
objective value function right now is: -1606.438959878271
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-109.36428]
objective value function right now is: -1606.6478027089286
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-109.09784]
objective value function right now is: -1606.8107981118098
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-109.188866]
objective value function right now is: -1606.3997352548763
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-109.50863]
objective value function right now is: -1606.8594896392228
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-109.25697]
objective value function right now is: -1605.7069778176124
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-109.067375]
objective value function right now is: -1606.6596498365939
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1607.5216975524747
Current xi:  [-109.125244]
objective value function right now is: -1607.5216975524747
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-108.96428]
objective value function right now is: -1607.2749788650926
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-108.92875]
objective value function right now is: -1607.404308768542
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-109.1098]
objective value function right now is: -1607.3606111961567
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-108.87514]
objective value function right now is: -1607.4107274816072
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-108.93492]
objective value function right now is: -1607.3450125899983
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-109.05748]
objective value function right now is: -1607.4578508639368
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-108.863884]
objective value function right now is: -1607.4375378688583
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-109.04291]
objective value function right now is: -1607.4899808487198
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-108.94272]
objective value function right now is: -1607.4744192726803
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-109.07578]
objective value function right now is: -1607.4031604916886
94.0% of gradient descent iterations done. Method = Adam
new min fval:  -1607.610908097521
Current xi:  [-108.96732]
objective value function right now is: -1607.610908097521
new min fval from sgd:  -1607.615029770869
new min fval from sgd:  -1607.6321926583473
new min fval from sgd:  -1607.640574187977
new min fval from sgd:  -1607.645571685556
new min fval from sgd:  -1607.6492043367514
new min fval from sgd:  -1607.676433201038
new min fval from sgd:  -1607.6873661724353
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-109.07545]
objective value function right now is: -1607.5244008004684
new min fval from sgd:  -1607.6919277624695
new min fval from sgd:  -1607.6959776412175
new min fval from sgd:  -1607.6989944401487
new min fval from sgd:  -1607.7009298787625
new min fval from sgd:  -1607.7025361037463
new min fval from sgd:  -1607.7042275436331
new min fval from sgd:  -1607.7053989038911
new min fval from sgd:  -1607.7057569155374
new min fval from sgd:  -1607.7081339684619
new min fval from sgd:  -1607.7098851083165
new min fval from sgd:  -1607.7110060968744
new min fval from sgd:  -1607.7114472742282
new min fval from sgd:  -1607.7122164209532
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-108.939644]
objective value function right now is: -1607.6946136065324
new min fval from sgd:  -1607.7147447521581
new min fval from sgd:  -1607.7179229587414
new min fval from sgd:  -1607.719526231483
new min fval from sgd:  -1607.7200573044647
new min fval from sgd:  -1607.7220855236574
new min fval from sgd:  -1607.7241873079327
new min fval from sgd:  -1607.725039393045
new min fval from sgd:  -1607.7254771906064
new min fval from sgd:  -1607.7256680675566
new min fval from sgd:  -1607.7258098984962
new min fval from sgd:  -1607.7258804173155
new min fval from sgd:  -1607.7258960365866
new min fval from sgd:  -1607.7264141025244
new min fval from sgd:  -1607.7264476695727
new min fval from sgd:  -1607.726718591061
new min fval from sgd:  -1607.72685613597
new min fval from sgd:  -1607.7286237557623
new min fval from sgd:  -1607.7305875076784
new min fval from sgd:  -1607.7308842263124
new min fval from sgd:  -1607.7316413961723
new min fval from sgd:  -1607.732675283011
new min fval from sgd:  -1607.733979685719
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-108.98688]
objective value function right now is: -1607.7191427376174
min fval:  -1607.733979685719
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.5713,  1.2729],
        [ 2.3250, -6.4015],
        [-2.8169,  8.1442],
        [-2.7203,  7.8024],
        [-7.8253, -8.5758],
        [-0.5702,  1.2723],
        [-2.7873,  7.8417],
        [-0.5709,  1.2727],
        [-0.5716,  1.2731],
        [-0.3501, -1.4500],
        [11.6331,  3.5442],
        [-0.5751,  1.2755],
        [-0.5714,  1.2730]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-1.7212, -5.4058,  6.2278,  5.7182, -3.3969, -1.7203,  5.8091, -1.7209,
        -1.7214, -4.3601, -8.4620, -1.7247, -1.7213], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.4851e-03, -4.9964e-01, -1.4390e-01, -1.1378e-01, -1.7949e-01,
         -4.4921e-03, -1.1659e-01, -4.4875e-03, -4.4829e-03, -1.7521e-03,
         -1.7752e-01, -4.4578e-03, -4.4839e-03],
        [-4.4851e-03, -4.9964e-01, -1.4390e-01, -1.1378e-01, -1.7949e-01,
         -4.4921e-03, -1.1659e-01, -4.4875e-03, -4.4829e-03, -1.7521e-03,
         -1.7752e-01, -4.4578e-03, -4.4839e-03],
        [-4.4851e-03, -4.9964e-01, -1.4390e-01, -1.1378e-01, -1.7949e-01,
         -4.4921e-03, -1.1659e-01, -4.4875e-03, -4.4829e-03, -1.7521e-03,
         -1.7752e-01, -4.4578e-03, -4.4839e-03],
        [-4.4851e-03, -4.9964e-01, -1.4390e-01, -1.1378e-01, -1.7949e-01,
         -4.4921e-03, -1.1659e-01, -4.4875e-03, -4.4829e-03, -1.7521e-03,
         -1.7752e-01, -4.4578e-03, -4.4839e-03],
        [-4.4851e-03, -4.9964e-01, -1.4390e-01, -1.1378e-01, -1.7949e-01,
         -4.4921e-03, -1.1659e-01, -4.4875e-03, -4.4829e-03, -1.7521e-03,
         -1.7752e-01, -4.4578e-03, -4.4839e-03],
        [-2.2526e-02,  2.0023e+00, -6.9460e+00, -5.4685e+00,  6.8378e+00,
         -2.2010e-02, -5.7274e+00, -2.2361e-02, -2.2871e-02, -1.4076e-01,
          1.0813e+01, -2.5056e-02, -2.2707e-02],
        [-2.4473e-02,  1.7619e+00, -6.8968e+00, -5.3612e+00,  6.7103e+00,
         -2.4078e-02, -5.7999e+00, -2.4326e-02, -2.4410e-02, -1.1239e-01,
          1.0670e+01, -2.5579e-02, -2.4448e-02],
        [-4.4851e-03, -4.9964e-01, -1.4390e-01, -1.1378e-01, -1.7949e-01,
         -4.4921e-03, -1.1659e-01, -4.4875e-03, -4.4829e-03, -1.7521e-03,
         -1.7752e-01, -4.4578e-03, -4.4839e-03],
        [-5.2841e-02, -2.4506e+00,  6.8078e+00,  5.9127e+00, -7.2846e+00,
         -5.0974e-02,  6.0945e+00, -5.2207e-02, -5.3383e-02, -4.8752e-01,
         -3.0808e+00, -5.9386e-02, -5.3135e-02],
        [ 1.1295e-02,  2.6583e+00, -7.5483e+00, -6.1150e+00,  6.8283e+00,
          1.2809e-02, -6.3198e+00,  1.1805e-02,  1.0889e-02,  3.0225e-01,
          2.6289e-01,  6.6924e-03,  1.1079e-02],
        [-4.4851e-03, -4.9964e-01, -1.4390e-01, -1.1378e-01, -1.7949e-01,
         -4.4921e-03, -1.1659e-01, -4.4875e-03, -4.4829e-03, -1.7521e-03,
         -1.7752e-01, -4.4578e-03, -4.4839e-03],
        [ 8.9097e-02,  1.4443e+00,  6.2162e-01,  5.4661e-01,  5.1785e-01,
          8.9179e-02,  5.5943e-01,  8.9126e-02,  8.9070e-02,  6.0379e-02,
          1.6127e-01,  8.8733e-02,  8.9082e-02],
        [-4.4851e-03, -4.9964e-01, -1.4390e-01, -1.1378e-01, -1.7949e-01,
         -4.4921e-03, -1.1659e-01, -4.4875e-03, -4.4829e-03, -1.7521e-03,
         -1.7752e-01, -4.4578e-03, -4.4839e-03]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8688, -0.8688, -0.8688, -0.8688, -0.8688, -1.2450, -1.0477, -0.8688,
         1.7433, -0.7770, -0.8688,  2.4669, -0.8688], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 0.0221,  0.0221,  0.0221,  0.0221,  0.0221, -7.8673, -7.7324,  0.0221,
         10.7441, -6.5662,  0.0221,  3.4631,  0.0221]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 10.7607,  -0.6892],
        [  2.8530,   5.4945],
        [  0.6582,  -2.0932],
        [  4.7800,   9.3446],
        [ -1.6907,  -0.6098],
        [ -1.7070,  -0.6118],
        [ -3.9362, -11.5825],
        [ 11.6810,   3.9118],
        [-16.7532,  -7.5815],
        [ -9.1129,   2.2862],
        [ 13.6967,   6.3443],
        [ -1.6879,  -0.6083],
        [ -8.9727, -10.4654]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-9.8629, -0.2543, -7.9773,  6.5920, -4.6829, -4.6899, -8.7039, -2.2732,
        -3.8257,  3.5524,  2.7543, -4.6719, -6.6673], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.3566e+01, -2.9209e-01, -2.7606e+00, -1.0026e+01, -3.6800e+00,
         -3.7841e+00,  1.1080e+01, -6.2733e+00,  6.2923e+00, -9.2189e+00,
         -1.4375e+00, -3.6248e+00,  5.9102e+00],
        [ 8.1199e+00, -4.5146e+00, -1.8152e+00, -2.3628e+00, -2.4113e+00,
         -2.4160e+00, -7.6018e-01,  7.2249e-02, -2.0301e+00, -3.7689e-03,
         -2.2052e+00, -2.4105e+00,  1.0265e+00],
        [-5.5700e+00, -8.2113e-01, -3.4719e-02,  2.2045e+00,  1.9314e-01,
          1.9500e-01, -1.0952e+01, -2.1646e+00, -2.2605e+00,  1.7630e+01,
          2.1669e+00,  1.9526e-01, -1.0963e+01],
        [ 6.1675e+00,  2.9613e+00, -4.4051e-01, -1.9608e+00, -3.8151e-01,
         -3.8029e-01, -7.4192e+00, -3.9407e-01, -3.4698e-01, -4.8132e+00,
         -7.4918e-01, -3.8221e-01, -7.0216e-01],
        [-1.4191e+00, -1.6943e+00, -4.0738e-01,  1.8887e+00, -3.6335e-01,
         -3.6325e-01, -5.0598e+00, -6.7483e-01,  1.9354e+00,  3.7081e+00,
         -3.9727e+00, -3.6300e-01,  9.4635e-02],
        [ 1.4396e+00, -5.1977e-02,  2.3866e+00, -2.2260e+00,  2.2625e+00,
          2.2618e+00, -7.4024e-01, -1.4376e+00,  5.3994e-01, -1.8103e+00,
         -3.0038e+00,  2.2620e+00, -8.1370e-01],
        [-1.0981e+00,  5.1446e-03, -2.1091e+00,  2.0302e+00, -2.0784e+00,
         -2.0781e+00,  5.0457e-01,  1.2557e+00, -5.4559e-01,  1.7053e+00,
          2.6926e+00, -2.0779e+00,  7.5861e-01],
        [-6.9963e+00, -4.1408e+00, -1.2216e-01, -4.2479e-01, -7.1063e-02,
         -7.0798e-02, -3.6014e+00, -4.0337e+00,  4.0753e+00, -4.9973e+00,
          2.1677e+00, -7.0709e-02, -3.1030e+00],
        [-9.4680e+00,  2.2553e+00, -3.2372e-01,  3.1290e+00,  5.0062e-02,
          5.8739e-02, -1.8032e+01,  3.8068e-01,  9.5819e+00,  5.2137e+00,
         -1.8223e-01,  5.2048e-02, -1.0116e+01],
        [ 1.3411e+00, -8.5190e-02,  2.1590e+00, -2.0898e+00,  2.0201e+00,
          2.0196e+00, -6.4489e-01, -1.3179e+00,  4.7331e-01, -1.6200e+00,
         -2.7907e+00,  2.0196e+00, -6.9436e-01],
        [ 3.8604e+00,  3.0108e-01,  5.1737e-01,  1.0420e+01,  7.2320e-03,
          6.8004e-03, -5.8441e+00,  2.8432e+00, -6.7696e+00,  1.3891e-01,
          9.6686e+00,  7.4077e-03, -6.5347e+00],
        [-1.1410e+00,  1.1031e-02, -2.1166e+00,  2.0318e+00, -2.0795e+00,
         -2.0791e+00,  5.6410e-01,  1.2588e+00, -5.5908e-01,  1.6592e+00,
          2.7400e+00, -2.0790e+00,  7.4866e-01],
        [-4.4711e+00, -3.4098e-01, -6.3773e-01, -1.1839e+01, -1.4768e-02,
         -1.4256e-02,  6.8421e+00, -3.5085e+00,  7.4889e+00, -6.1030e-01,
         -1.0863e+01, -1.5047e-02,  7.0659e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.1509, -3.0181, -1.0253, -2.7341, -0.9065, -2.7108,  2.3765,  0.8818,
        -2.6272, -2.5138,  4.2458,  2.4043, -4.4571], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-4.6373e+00, -4.5830e+00, -4.2941e-01,  1.5043e+00,  3.1829e-01,
         -3.5536e+00,  3.0639e+00, -3.3688e-01,  3.7320e+00, -2.6478e+00,
         -4.7796e+00,  3.2249e+00,  9.2400e+00],
        [ 4.5946e+00,  4.6284e+00,  2.4218e+00,  5.5182e-02, -3.1350e-01,
          5.5776e+00,  1.5152e+00,  4.1826e-01, -3.3090e+00,  4.3513e+00,
          1.0427e+00,  3.3140e-01, -5.1419e+00],
        [ 2.5491e-02, -9.1042e-02, -2.5906e-01, -7.6536e-02, -2.2815e-03,
         -7.5050e-01, -2.4290e+00, -1.1435e-01, -2.2610e-01, -7.5123e-01,
         -2.4109e+00, -2.4292e+00, -7.6838e-01],
        [ 8.5932e+00, -3.5540e-01,  4.7158e+00,  7.4295e-01,  1.6578e+00,
          6.8927e+00, -3.4241e+00, -2.7497e+00, -8.6476e+00,  3.1808e+00,
          6.3531e+00, -3.4798e+00, -1.0079e+01],
        [-3.1972e+00,  5.9996e-01,  5.5814e-01, -2.9457e+00, -4.9671e+00,
          7.6599e+00, -1.3200e-01,  3.7448e+00, -1.4993e+01,  7.6332e+00,
          1.3170e+01, -3.4877e-01, -6.7645e+00]], device='cuda:0'))])
xi:  [-108.997284]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 119.73232066091563
W_T_median: 70.2429391072381
W_T_pctile_5: -108.94546980799795
W_T_CVAR_5_pct: -196.48231508994309
Average q (qsum/M+1):  55.03145476310484
Optimal xi:  [-108.997284]
Expected(across Rb) median(across samples) p_equity:  0.23821278903633356
obj fun:  tensor(-1607.7340, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 940.130933411658
W_T_median: 581.3306640883857
W_T_pctile_5: -358.9604697855882
W_T_CVAR_5_pct: -487.0834540584932
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1056.8181908765328
Current xi:  [108.20268]
objective value function right now is: -1056.8181908765328
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1416.6291134054613
Current xi:  [98.006226]
objective value function right now is: -1416.6291134054613
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1436.8313221807248
Current xi:  [73.487595]
objective value function right now is: -1436.8313221807248
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1512.2454372967832
Current xi:  [67.84005]
objective value function right now is: -1512.2454372967832
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1514.7548828900588
Current xi:  [60.151493]
objective value function right now is: -1514.7548828900588
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1514.8387980621605
Current xi:  [53.525715]
objective value function right now is: -1514.8387980621605
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1517.7081188437912
Current xi:  [48.726048]
objective value function right now is: -1517.7081188437912
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.085995]
objective value function right now is: -1517.0061727411291
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1522.3821879904067
Current xi:  [42.118633]
objective value function right now is: -1522.3821879904067
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [39.548347]
objective value function right now is: -1521.591309380988
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [37.36354]
objective value function right now is: -1521.8322587273658
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [35.830032]
objective value function right now is: -1520.3490486584778
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1531.673932094428
Current xi:  [33.959465]
objective value function right now is: -1531.673932094428
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [32.48623]
objective value function right now is: -1521.3204330864503
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [31.276487]
objective value function right now is: -1529.5582741316275
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [30.02918]
objective value function right now is: -1528.269612744843
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1533.526350227664
Current xi:  [29.12367]
objective value function right now is: -1533.526350227664
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [28.16743]
objective value function right now is: -1530.5980374807586
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [27.681501]
objective value function right now is: -1533.5086533451024
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [26.361639]
objective value function right now is: -1532.755800674824
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [25.776821]
objective value function right now is: -1531.9520400827944
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1534.2194486189783
Current xi:  [25.049267]
objective value function right now is: -1534.2194486189783
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [24.496847]
objective value function right now is: -1532.7557644142137
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [24.093382]
objective value function right now is: -1530.1871139494592
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [24.107487]
objective value function right now is: -1532.9264025940672
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [23.333187]
objective value function right now is: -1528.8013239847585
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [23.175974]
objective value function right now is: -1533.87522664047
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [22.672674]
objective value function right now is: -1531.6680362488219
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [22.381779]
objective value function right now is: -1534.0704849301221
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [22.326738]
objective value function right now is: -1531.7032223924814
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [21.662567]
objective value function right now is: -1529.6352275075412
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [21.040031]
objective value function right now is: -1532.809452220673
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [20.770422]
objective value function right now is: -1534.1271882422411
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [20.710821]
objective value function right now is: -1531.887308889332
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [20.605282]
objective value function right now is: -1530.6228691116808
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1535.543241923007
Current xi:  [20.515913]
objective value function right now is: -1535.543241923007
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1536.2589612429963
Current xi:  [20.477804]
objective value function right now is: -1536.2589612429963
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1536.5694854192525
Current xi:  [20.437296]
objective value function right now is: -1536.5694854192525
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [20.36017]
objective value function right now is: -1536.2648791193296
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1536.752013802169
Current xi:  [20.273535]
objective value function right now is: -1536.752013802169
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [20.264812]
objective value function right now is: -1536.6891596548369
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [20.20054]
objective value function right now is: -1536.4291833240434
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [20.1066]
objective value function right now is: -1536.0061337770214
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1536.8233827109095
Current xi:  [20.085108]
objective value function right now is: -1536.8233827109095
90.0% of gradient descent iterations done. Method = Adam
new min fval:  -1536.9330820734635
Current xi:  [19.96984]
objective value function right now is: -1536.9330820734635
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.909939]
objective value function right now is: -1535.7613701483501
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.819609]
objective value function right now is: -1536.8764483139535
new min fval from sgd:  -1537.0039431978037
new min fval from sgd:  -1537.0482614550501
new min fval from sgd:  -1537.0565657074274
new min fval from sgd:  -1537.0650095991155
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.946127]
objective value function right now is: -1536.8432770686586
new min fval from sgd:  -1537.0651031672833
new min fval from sgd:  -1537.0684026802996
new min fval from sgd:  -1537.0690025455976
new min fval from sgd:  -1537.0780207891764
new min fval from sgd:  -1537.082697596656
new min fval from sgd:  -1537.0969062296338
new min fval from sgd:  -1537.1018385461152
new min fval from sgd:  -1537.1134142247013
new min fval from sgd:  -1537.117133185586
new min fval from sgd:  -1537.1202158977337
new min fval from sgd:  -1537.1225159968315
new min fval from sgd:  -1537.1232259144438
new min fval from sgd:  -1537.1265878526244
new min fval from sgd:  -1537.1270551130788
new min fval from sgd:  -1537.1276643349156
new min fval from sgd:  -1537.1279903213415
new min fval from sgd:  -1537.1289374816363
new min fval from sgd:  -1537.1316936541396
new min fval from sgd:  -1537.1426688920878
new min fval from sgd:  -1537.1448521932966
new min fval from sgd:  -1537.1473479069284
new min fval from sgd:  -1537.1477535499525
new min fval from sgd:  -1537.150533718602
new min fval from sgd:  -1537.1532772135363
new min fval from sgd:  -1537.1535123216597
new min fval from sgd:  -1537.155162633943
new min fval from sgd:  -1537.1568133060555
new min fval from sgd:  -1537.1574397690697
new min fval from sgd:  -1537.1575728558194
new min fval from sgd:  -1537.1591001310737
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.933197]
objective value function right now is: -1536.9494457626759
new min fval from sgd:  -1537.167100911995
new min fval from sgd:  -1537.168214576489
new min fval from sgd:  -1537.1704363361564
new min fval from sgd:  -1537.1757727718648
new min fval from sgd:  -1537.1768589302073
new min fval from sgd:  -1537.1784169174541
new min fval from sgd:  -1537.1888300164683
new min fval from sgd:  -1537.19461372056
new min fval from sgd:  -1537.1977505652587
new min fval from sgd:  -1537.1985036084604
new min fval from sgd:  -1537.199807716443
new min fval from sgd:  -1537.2016176736481
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.928333]
objective value function right now is: -1537.1322153278556
min fval:  -1537.2016176736481
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -7.2542,   7.3206],
        [ -6.7699, -10.5979],
        [ -5.2193,   8.6804],
        [  0.2241,   0.3177],
        [ -0.7246,   0.9976],
        [-52.2798,  -6.4820],
        [ -2.6707, -11.2124],
        [ -8.5838,   0.4254],
        [  6.5248,   8.9539],
        [ -0.7255,   0.9969],
        [ -0.7246,   0.9977],
        [  9.1326,  -0.8868],
        [  8.9232,  -1.4696]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 3.0600,  5.1975,  5.6464, -4.0477, -2.5143, -5.4062, -6.5253,  7.7716,
        -6.6490, -2.5147, -2.5143, -8.5705, -8.6983], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-4.2583e+00, -2.3487e+00, -9.1379e+00, -4.1893e-02, -5.7324e-02,
          1.1763e+01,  1.2797e+01, -2.3341e+00,  5.7757e+00, -5.9082e-02,
         -5.7144e-02,  6.3655e+00,  5.6082e+00],
        [-6.9528e+00, -2.6611e+00, -1.0702e+01, -1.3038e-01, -7.9669e-02,
          6.1040e+00,  1.1654e+01, -1.8717e+00,  3.0247e+00, -7.9543e-02,
         -7.9681e-02,  4.7476e+00,  4.0439e+00],
        [ 2.3317e-01,  1.6804e+00,  1.1144e+00,  2.7733e-02,  6.2818e-02,
          1.7155e-02,  4.5875e-01,  1.6943e+00,  4.4272e-01,  6.2998e-02,
          6.2799e-02,  4.2736e-01,  3.2275e-01],
        [-4.0942e+00, -2.3099e+00, -9.3733e+00,  1.0466e-02,  1.2226e-01,
          1.2791e+01,  1.3622e+01, -1.8156e+00,  6.1902e+00,  1.2472e-01,
          1.2201e-01,  6.4802e+00,  5.5247e+00],
        [ 2.2773e-01,  1.6243e+00,  1.0676e+00,  2.6215e-02,  6.0310e-02,
          1.6723e-02,  4.5316e-01,  1.6373e+00,  4.1183e-01,  6.0480e-02,
          6.0293e-02,  4.0009e-01,  3.0185e-01],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-5.3829e+00, -2.8298e+00, -9.4959e+00,  2.0391e-01, -4.5240e-02,
          9.0749e+00,  1.2055e+01, -2.8772e+00,  5.6057e+00, -4.5905e-02,
         -4.5172e-02,  8.2304e+00,  7.3503e+00],
        [-4.5205e+00, -2.6678e+00, -8.6396e+00, -4.4378e-02,  4.8922e-02,
          4.0271e+00,  1.0322e+01, -2.4150e+00,  2.2341e+00,  4.9162e-02,
          4.8897e-02,  3.9250e+00,  3.0669e+00],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-2.8199e-01, -2.2754e+00, -3.0344e+00, -4.0272e-03, -2.1728e-02,
          6.2891e-02,  3.0368e+00, -2.2460e+00,  5.0454e-01, -2.1810e-02,
         -2.1719e-02,  7.0139e-01,  4.5223e-01],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-3.0101e-02, -6.0257e-01, -3.4920e-01, -4.2911e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1737e-01, -6.6294e-02, -1.0337e-02,
         -1.0342e-02, -8.4352e-02, -6.9009e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8182, -0.0261,  0.0321,  2.4714,  0.0593,  2.3684, -0.8182, -0.3568,
        -0.2433, -0.8182, -0.8353, -0.8182, -0.8182], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.8009e-03, -6.9790e+00, -7.7505e+00,  7.8855e+00, -8.7395e+00,
          6.1127e+00,  1.8009e-03, -9.8697e+00, -4.9599e+00,  1.8009e-03,
         -1.0883e+00,  1.8009e-03,  1.8009e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.5627, -11.5538],
        [ -0.2054,  12.2057],
        [ -2.9373,   6.0430],
        [-10.6203,  -5.6749],
        [ 10.9489,   0.0949],
        [  5.0883,  10.4545],
        [ -7.3211,   0.0880],
        [-10.6611,  -5.9336],
        [ -1.5390,   0.8026],
        [ -5.0853,  12.2247],
        [ -7.5356,  14.9437],
        [  8.8835,   2.3574],
        [ -9.0614,  -4.6052]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-8.2035,  7.9696,  0.6017, -2.7780, -9.5798,  6.7590,  5.1073, -3.5817,
        -3.2062,  8.3475, 10.7078, -3.0235, -1.9507], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.5360e+00,  5.1902e+00, -8.8261e-01,  1.8825e-01, -1.1617e+01,
         -3.0109e+00,  7.3917e+00, -5.0429e-01,  2.7591e-02,  6.2055e+00,
          2.7854e+00, -1.0616e+00,  1.2582e+00],
        [ 1.0863e+01, -4.0755e+00, -3.4312e-01,  7.2742e-01, -1.0664e+01,
         -4.0139e+00,  9.5468e+00, -3.9590e+00,  1.2329e-02, -2.2796e+00,
         -1.5576e+01,  6.8790e-02,  3.1184e+00],
        [-1.4642e+01,  7.1294e+00, -4.1546e+00, -1.3205e-01,  1.2756e+01,
          3.2742e+00, -1.0937e+01, -8.2515e-02,  1.0508e-01, -7.1542e-01,
         -8.9964e+00, -3.1784e-01, -2.6675e-01],
        [-4.3054e+00,  1.3906e+01,  1.6532e-01, -8.5551e+00, -8.6290e+00,
          9.0489e+00, -1.2236e+00, -4.8719e+00, -1.1856e-01,  1.1045e+01,
          1.0371e+01,  1.6169e+00, -5.8449e+00],
        [-1.6620e+00,  2.2172e+00,  7.3730e-02, -7.4682e+00,  2.1070e+00,
          1.2570e+01,  2.5803e+00, -5.0797e+00,  4.1404e-02, -1.3499e-01,
         -1.4182e+00,  2.0066e+00, -3.8558e+00],
        [-2.7976e+00,  5.9804e+00,  2.5414e-01, -6.9438e+00,  5.2150e+00,
          8.2924e+00, -2.2936e+00, -2.8446e+00, -1.8938e-01,  7.2713e+00,
          8.7133e+00,  4.0492e+00, -3.9568e+00],
        [ 3.9558e+00, -4.9965e+00, -2.7392e-01,  9.7790e+00, -2.0234e+00,
         -4.3889e+00,  3.8306e+00,  6.0189e+00,  8.3592e-02, -1.1287e+01,
         -2.0694e+01, -4.3281e-01,  7.7430e+00],
        [-6.1261e-01,  7.4853e-01, -1.9733e+00,  1.8636e+00, -2.5340e-01,
         -8.6531e-01, -7.7204e-01,  1.0507e+00, -1.3834e+00,  1.6075e+00,
          1.6765e+00, -1.8126e+00,  2.0834e+00],
        [-1.7054e+00,  2.1658e+00,  7.0490e-02, -7.4993e+00,  2.1097e+00,
          1.2681e+01,  2.5081e+00, -4.9668e+00,  4.2617e-02, -2.1372e-01,
         -1.5385e+00,  1.9917e+00, -4.0639e+00],
        [-8.5966e-01, -5.4158e-01,  3.1900e-02, -1.0584e-01, -5.2516e-01,
         -1.5050e+00, -1.0865e+00, -7.5104e-02,  4.2592e-02, -2.2466e-01,
         -1.8496e-01, -1.6167e+00, -1.0209e-01],
        [ 2.8503e+00, -1.3851e+01, -3.2277e-01,  1.2349e+01, -6.6659e+00,
         -1.6797e+01,  2.0115e+00,  7.6731e+00,  4.9482e-02, -8.4937e+00,
         -8.1226e+00, -3.5145e+00,  7.0066e+00],
        [ 1.6826e+00,  5.2625e-01,  4.1788e-01,  1.3952e-01,  4.3639e-01,
          1.4718e+00,  1.7609e+00,  7.2000e-02,  4.9240e-02,  2.2221e-01,
          3.2291e-01,  1.5640e+00,  2.5923e-01],
        [-8.8093e-01, -4.3473e-01, -1.6270e-01, -3.4271e-02, -5.6408e-01,
         -1.4559e+00, -1.1035e+00, -1.5385e-01,  2.5814e-02, -3.4916e-02,
         -1.0794e-01, -1.7583e+00,  2.8486e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 1.3890,  2.7192, -2.4559, -2.6275,  4.8735,  0.2970,  2.1879, -1.9531,
         5.0658, -2.4521, -0.5826,  3.1194, -2.4773], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.6213e-01, -6.8777e-01, -6.5505e-01,  1.3206e+00,  7.1399e-01,
          1.2783e+00,  1.1355e+00,  4.2453e-01,  7.4292e-01, -1.6221e-01,
         -9.0400e+00, -3.4519e-01, -1.4551e-01],
        [ 5.6044e-01,  1.9149e+00,  1.3483e+00, -3.6948e-01,  2.6564e-01,
          1.6115e-01, -4.7250e-01, -4.1914e-01,  5.5808e-01,  1.6534e-01,
          8.9809e+00,  1.2504e+00,  1.4926e-01],
        [-8.2717e-01, -4.8577e-01, -6.9095e-01, -1.3295e+00, -1.5980e+00,
         -1.6062e+00, -3.5694e-01, -1.2789e-01, -1.5980e+00, -2.0943e-03,
         -4.4515e-02, -1.6309e+00, -1.6794e-03],
        [ 9.8592e-01,  3.9564e+00,  3.5376e+00, -2.5419e+00, -1.1885e+00,
         -2.7500e+00, -2.6475e+00,  5.9882e-01, -1.0693e+00,  1.4172e-01,
          7.7192e+00,  1.9260e+00,  2.3542e-01],
        [-2.3016e+00, -1.9959e+00,  5.9102e-07,  9.3182e-01,  1.2590e+01,
          1.4468e+00, -2.4106e+00, -1.9486e+00,  1.2788e+01, -7.4873e-02,
         -2.9963e+00, -1.8615e+00, -8.3155e-02]], device='cuda:0'))])
xi:  [19.928818]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 532.549095606086
W_T_median: 231.59572898939842
W_T_pctile_5: 19.911718093870476
W_T_CVAR_5_pct: -81.9701574866933
Average q (qsum/M+1):  52.23137443296371
Optimal xi:  [19.928818]
Expected(across Rb) median(across samples) p_equity:  0.22919555654128393
obj fun:  tensor(-1537.2016, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 940.130933411658
W_T_median: 581.3306640883857
W_T_pctile_5: -358.9604697855882
W_T_CVAR_5_pct: -487.0834540584932
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1517.7858340174932
Current xi:  [21.460419]
objective value function right now is: -1517.7858340174932
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1518.5415376184749
Current xi:  [23.478857]
objective value function right now is: -1518.5415376184749
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [25.094776]
objective value function right now is: -1512.738015837084
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [26.39586]
objective value function right now is: -1508.0360489620887
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [27.816078]
objective value function right now is: -1514.3111887062007
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [28.965624]
objective value function right now is: -1517.9306189376503
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [30.222895]
objective value function right now is: -1512.9801743949565
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [31.617825]
objective value function right now is: -1518.0644713025752
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [32.715946]
objective value function right now is: -1515.668898892751
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [33.909855]
objective value function right now is: -1516.7558898629532
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1518.9281816268915
Current xi:  [34.81112]
objective value function right now is: -1518.9281816268915
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [35.73416]
objective value function right now is: -1514.3135934473648
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [36.284412]
objective value function right now is: -1517.9229387733933
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [37.344505]
objective value function right now is: -1517.9481432161076
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [37.919456]
objective value function right now is: -1512.0716182608924
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1519.0673335354647
Current xi:  [38.59939]
objective value function right now is: -1519.0673335354647
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1519.539166719469
Current xi:  [39.372692]
objective value function right now is: -1519.539166719469
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [39.949635]
objective value function right now is: -1519.4740864697494
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [40.042564]
objective value function right now is: -1518.014421647562
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [40.44479]
objective value function right now is: -1512.8941595491162
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [41.000595]
objective value function right now is: -1516.5233528061322
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1520.1203749852627
Current xi:  [41.88247]
objective value function right now is: -1520.1203749852627
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1520.768625076755
Current xi:  [42.40033]
objective value function right now is: -1520.768625076755
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [42.44226]
objective value function right now is: -1513.1979248623536
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [42.664795]
objective value function right now is: -1518.7096952163713
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [42.946373]
objective value function right now is: -1519.9621760933624
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [43.336666]
objective value function right now is: -1513.5164972760895
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [43.68198]
objective value function right now is: -1518.0052346943419
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [43.722225]
objective value function right now is: -1518.1751969359557
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1521.4530439514451
Current xi:  [43.946896]
objective value function right now is: -1521.4530439514451
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [44.045753]
objective value function right now is: -1519.576312617639
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [44.11526]
objective value function right now is: -1516.2575381657136
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [44.41457]
objective value function right now is: -1516.2298226132136
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [44.5475]
objective value function right now is: -1517.2362518253483
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [44.79145]
objective value function right now is: -1516.7044109028172
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1522.1556532818552
Current xi:  [44.869152]
objective value function right now is: -1522.1556532818552
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1522.764974253494
Current xi:  [44.94367]
objective value function right now is: -1522.764974253494
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [44.99421]
objective value function right now is: -1522.4623401505146
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.024532]
objective value function right now is: -1521.1906923588797
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.063786]
objective value function right now is: -1522.6424652171845
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.04669]
objective value function right now is: -1522.4440139890667
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.148533]
objective value function right now is: -1522.5319420763467
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.232666]
objective value function right now is: -1522.6964458667971
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.148624]
objective value function right now is: -1521.9994086053207
90.0% of gradient descent iterations done. Method = Adam
new min fval:  -1522.768401475953
Current xi:  [45.164852]
objective value function right now is: -1522.768401475953
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.25969]
objective value function right now is: -1522.5937728983135
94.0% of gradient descent iterations done. Method = Adam
new min fval:  -1522.8738331898467
Current xi:  [45.250553]
objective value function right now is: -1522.8738331898467
96.0% of gradient descent iterations done. Method = Adam
new min fval:  -1523.0906597577343
Current xi:  [45.290264]
objective value function right now is: -1523.0906597577343
new min fval from sgd:  -1523.159914359997
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.261856]
objective value function right now is: -1523.159914359997
new min fval from sgd:  -1523.1601111766677
new min fval from sgd:  -1523.1702222798526
new min fval from sgd:  -1523.1780819550474
new min fval from sgd:  -1523.182490053968
new min fval from sgd:  -1523.1847314777394
new min fval from sgd:  -1523.1881121433155
new min fval from sgd:  -1523.1883587292477
new min fval from sgd:  -1523.1887905963786
new min fval from sgd:  -1523.1938568812764
new min fval from sgd:  -1523.1955463823947
new min fval from sgd:  -1523.1990768681974
new min fval from sgd:  -1523.201355071794
new min fval from sgd:  -1523.204033232312
new min fval from sgd:  -1523.2097857253575
new min fval from sgd:  -1523.2130619998472
new min fval from sgd:  -1523.2148151232825
new min fval from sgd:  -1523.2179924000625
new min fval from sgd:  -1523.2225819550217
new min fval from sgd:  -1523.2262219792847
new min fval from sgd:  -1523.2328599108635
new min fval from sgd:  -1523.239421569485
new min fval from sgd:  -1523.2408104965127
new min fval from sgd:  -1523.2441110937177
new min fval from sgd:  -1523.2473353447347
new min fval from sgd:  -1523.25300992535
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.276104]
objective value function right now is: -1523.2193611072348
min fval:  -1523.25300992535
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -9.4834,   8.0851],
        [ -8.5340, -14.6211],
        [ -4.6972,  10.1489],
        [ -0.6300,   0.9408],
        [ -0.6303,   0.9410],
        [-58.2058,  -7.7328],
        [ -1.6405, -12.6550],
        [-10.6069,  -0.0944],
        [  7.5598,   4.2895],
        [ -0.6303,   0.9410],
        [ -0.6303,   0.9410],
        [ 11.7310,  -1.2243],
        [ 11.7015,  -1.3131]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  3.3134,   4.8669,   6.2651,  -2.5859,  -2.5863,  -5.8082,  -7.4498,
          9.6547,  -8.4205,  -2.5863,  -2.5863, -10.5665, -10.5692],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.6865e-02, -6.6834e-01, -3.7564e-01, -5.1245e-03, -5.1212e-03,
         -6.2418e-02, -2.7318e-01, -7.0233e-01, -3.8567e-02, -5.1211e-03,
         -5.1212e-03, -8.8481e-02, -8.6919e-02],
        [-4.0587e+00, -2.3150e+00, -9.7409e+00,  7.6312e-02,  7.5662e-02,
          1.1431e+01,  1.4183e+01, -2.1588e+00,  3.6751e+00,  7.5649e-02,
          7.5664e-02,  8.0463e+00,  7.3823e+00],
        [-7.2997e+00, -2.8084e+00, -1.2527e+01, -8.0854e-02, -8.0781e-02,
          6.5414e+00,  1.3563e+01, -1.7409e+00, -1.5205e-01, -8.0780e-02,
         -8.0781e-02, -5.2366e+00, -5.3731e+00],
        [ 1.5474e-01,  1.4005e+00,  1.4549e+00,  6.7942e-02,  6.7936e-02,
          8.1157e-02,  6.2461e-01,  1.8155e+00,  3.3066e-01,  6.7935e-02,
          6.7935e-02,  4.4950e-01,  4.3914e-01],
        [-4.5302e+00, -2.4672e+00, -1.0332e+01,  3.3204e-02,  3.4138e-02,
          1.1604e+01,  1.5235e+01, -1.7673e+00,  4.2102e+00,  3.4157e-02,
          3.4136e-02,  8.6869e+00,  7.8526e+00],
        [ 1.5315e-01,  1.3597e+00,  1.4026e+00,  6.5993e-02,  6.5986e-02,
          7.9304e-02,  6.1258e-01,  1.7579e+00,  3.1070e-01,  6.5986e-02,
          6.5986e-02,  4.2553e-01,  4.1568e-01],
        [-2.6865e-02, -6.6834e-01, -3.7564e-01, -5.1245e-03, -5.1212e-03,
         -6.2418e-02, -2.7318e-01, -7.0233e-01, -3.8567e-02, -5.1211e-03,
         -5.1212e-03, -8.8481e-02, -8.6919e-02],
        [-5.1302e+00, -3.6371e+00, -1.0255e+01, -6.0150e-02, -6.0387e-02,
          1.1373e+01,  1.4443e+01, -3.4245e+00,  3.3741e+00, -6.0392e-02,
         -6.0386e-02,  1.0249e+01,  9.5148e+00],
        [-2.6865e-02, -6.6834e-01, -3.7564e-01, -5.1246e-03, -5.1213e-03,
         -6.2417e-02, -2.7318e-01, -7.0233e-01, -3.8567e-02, -5.1212e-03,
         -5.1213e-03, -8.8481e-02, -8.6919e-02],
        [-2.6865e-02, -6.6834e-01, -3.7564e-01, -5.1245e-03, -5.1211e-03,
         -6.2418e-02, -2.7318e-01, -7.0233e-01, -3.8567e-02, -5.1211e-03,
         -5.1212e-03, -8.8481e-02, -8.6919e-02],
        [-2.6865e-02, -6.6834e-01, -3.7564e-01, -5.1245e-03, -5.1212e-03,
         -6.2418e-02, -2.7318e-01, -7.0233e-01, -3.8567e-02, -5.1211e-03,
         -5.1212e-03, -8.8481e-02, -8.6919e-02],
        [-2.6866e-02, -6.6834e-01, -3.7564e-01, -5.1246e-03, -5.1212e-03,
         -6.2418e-02, -2.7318e-01, -7.0233e-01, -3.8567e-02, -5.1211e-03,
         -5.1212e-03, -8.8481e-02, -8.6919e-02],
        [-2.6865e-02, -6.6834e-01, -3.7564e-01, -5.1245e-03, -5.1212e-03,
         -6.2418e-02, -2.7318e-01, -7.0233e-01, -3.8567e-02, -5.1211e-03,
         -5.1212e-03, -8.8481e-02, -8.6919e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8506,  0.2622, -0.3138,  2.4918,  0.3104,  2.4014, -0.8506, -0.5537,
        -0.8505, -0.8506, -0.8506, -0.8506, -0.8506], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 7.1026e-03, -5.8455e+00, -8.2668e+00,  8.1395e+00, -8.1693e+00,
          6.4139e+00,  7.1026e-03, -1.2670e+01,  7.1026e-03,  7.1026e-03,
          7.1026e-03,  7.1026e-03,  7.1026e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -5.6926, -12.7840],
        [  1.3629,  14.8469],
        [ -4.0455,   7.7870],
        [-11.0120,  -6.7503],
        [ 13.5841,   0.1722],
        [  7.3511,  11.8893],
        [ -9.4735,   0.2618],
        [-11.4919,  -7.3467],
        [ -1.6557,   3.1049],
        [ -3.3640,  15.0303],
        [ -9.4125,  17.9788],
        [  0.0476,   1.5537],
        [ -9.6863,  -5.8428]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -8.2499,   9.3116,   1.4270,  -3.3478, -12.1305,   6.7960,   6.0509,
         -4.5290,  -3.1601,   9.9492,  12.8122,  -5.3948,  -2.9982],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 3.8778e+00,  2.5215e+00,  1.1464e+00, -2.6520e+00, -1.3085e+01,
         -3.4848e+00,  1.8004e+00, -7.9479e-01, -2.5316e+00,  6.7981e+00,
          4.3220e+00,  5.1508e-01, -2.2808e+00],
        [ 1.3532e+01, -3.7751e+00, -5.5203e-01, -4.1411e-01, -1.0739e+01,
         -4.0998e+00,  1.1091e+01, -3.8479e+00,  1.2291e-01, -3.1894e+00,
         -1.8400e+01, -5.3683e-02, -2.6267e-02],
        [-1.2438e+01,  9.2047e+00, -5.7010e+00,  5.0801e-03,  1.4297e+01,
          1.7312e+00, -1.1564e+01, -9.0698e-03, -1.5305e+00, -6.3841e-01,
         -7.2985e+00,  1.7644e+00, -7.0196e-03],
        [-2.9315e+00,  1.5321e+01,  2.8858e-01, -8.5966e+00, -8.3552e+00,
          7.2310e+00, -1.0934e+00, -6.2938e+00, -6.4670e-02,  1.1107e+01,
          1.0936e+01, -7.6383e-02, -5.8616e+00],
        [-3.4129e+00,  3.0014e+00,  2.7646e-02, -5.4902e+00,  2.8221e+00,
          1.0570e+01,  8.5448e-01, -5.1152e+00,  6.2745e-02,  5.8943e-01,
         -1.0131e+00,  3.2733e-02, -3.2543e+00],
        [-2.5160e+00,  8.5768e+00,  1.1089e-01, -6.8293e+00,  8.8484e+00,
          5.9169e+00, -2.6045e+00, -4.6556e+00,  1.3966e-03,  9.0015e+00,
          9.0569e+00, -2.0876e-03, -4.3059e+00],
        [ 6.2768e+00, -1.8927e+00, -2.6193e-01,  7.6497e+00,  2.4127e+00,
         -5.1141e+00,  2.6979e+00,  5.2002e+00, -2.0195e-02, -6.4893e+00,
         -1.8882e+01,  7.1611e-02,  5.8630e+00],
        [-1.2749e+00, -2.4490e+00, -5.7880e-02,  5.1793e+00, -3.8752e+00,
         -3.6994e+00,  2.5033e+00,  2.9414e+00,  9.5304e-02, -1.9957e+00,
         -6.2920e+00,  3.5083e-02,  5.1844e+00],
        [-3.0835e+00,  3.2279e+00,  4.5191e-02, -5.5065e+00,  3.7819e+00,
          1.1394e+01,  3.4910e-02, -4.9358e+00,  7.0883e-02,  5.6739e-01,
         -1.1409e+00,  3.4180e-02, -3.3551e+00],
        [-1.6956e+00, -1.4280e+00,  9.5913e-02, -1.9667e-01, -9.4286e-01,
         -1.3628e+00, -1.1112e+00, -8.3049e-02, -4.9183e-02, -5.7143e-01,
         -3.8740e-03, -2.7721e-02, -2.2258e-01],
        [ 2.7475e+00, -1.5380e+01, -5.1662e-01,  1.1534e+01, -1.0942e+01,
         -1.6518e+01,  3.8471e+00,  8.1822e+00,  5.8509e-02, -8.5999e+00,
         -1.0210e+01,  2.4041e-02,  7.6034e+00],
        [ 2.2102e+00, -4.8742e-02,  2.8087e+00,  8.0894e-01, -9.2259e-01,
          7.9825e-01,  1.1294e+00,  5.0930e-01,  1.5558e+00, -5.3879e-01,
         -1.0970e+00,  1.2731e+00,  7.6166e-01],
        [-1.6956e+00, -1.4280e+00,  9.5914e-02, -1.9667e-01, -9.4286e-01,
         -1.3628e+00, -1.1112e+00, -8.3049e-02, -4.9183e-02, -5.7143e-01,
         -3.8738e-03, -2.7721e-02, -2.2258e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 2.2918,  3.9664, -4.5516, -3.5345,  8.6043,  0.6481,  2.8650, -3.2822,
         9.0337, -3.0487, -2.7267,  3.0774, -3.0487], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 2.6292e-01, -5.9579e-01, -5.9712e-01,  1.3138e+00,  6.5545e-01,
          1.3861e+00,  1.2155e+00,  3.0388e+00,  6.8321e-01,  2.0424e-01,
         -1.0447e+01, -3.8773e-01,  2.0424e-01],
        [ 4.6367e-01,  1.8177e+00,  1.2930e+00, -3.5765e-01,  3.3337e-01,
          5.8264e-02, -5.5019e-01, -2.9606e+00,  6.2427e-01, -2.0250e-01,
          1.0553e+01,  1.3036e+00, -2.0250e-01],
        [-8.1844e-01, -4.9837e-01, -8.5108e-01, -1.3104e+00, -1.6483e+00,
         -1.6393e+00, -5.1775e-01, -7.5674e-02, -1.6483e+00, -7.8130e-03,
         -7.9686e-02, -1.6976e+00, -7.8130e-03],
        [ 1.0873e+00,  3.7309e+00,  3.4551e+00, -3.0277e+00, -1.6013e+00,
         -1.0681e+00, -3.0462e+00, -4.6440e-01, -1.4848e+00,  6.3544e-02,
          9.3193e+00,  1.4533e+00,  6.3544e-02],
        [-4.0953e+00, -9.1613e-01, -8.1814e-07,  9.4750e-04,  1.3348e+01,
          3.4403e-02, -1.0223e+00, -4.9738e+00,  1.4299e+01,  7.0637e-03,
         -1.2504e+00, -8.8539e-01,  7.0637e-03]], device='cuda:0'))])
xi:  [45.2737]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 553.3277039405424
W_T_median: 256.9321610173203
W_T_pctile_5: 45.29820883330802
W_T_CVAR_5_pct: -63.83728050733083
Average q (qsum/M+1):  51.60839056199597
Optimal xi:  [45.2737]
Expected(across Rb) median(across samples) p_equity:  0.23095121309161187
obj fun:  tensor(-1523.2530, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 1.2
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -7.2542,   7.3206],
        [ -6.7699, -10.5979],
        [ -5.2193,   8.6804],
        [  0.2241,   0.3177],
        [ -0.7246,   0.9976],
        [-52.2798,  -6.4820],
        [ -2.6707, -11.2124],
        [ -8.5838,   0.4254],
        [  6.5248,   8.9539],
        [ -0.7255,   0.9969],
        [ -0.7246,   0.9977],
        [  9.1326,  -0.8868],
        [  8.9232,  -1.4696]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 3.0600,  5.1975,  5.6464, -4.0477, -2.5143, -5.4062, -6.5253,  7.7716,
        -6.6490, -2.5147, -2.5143, -8.5705, -8.6983], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-4.2583e+00, -2.3487e+00, -9.1379e+00, -4.1893e-02, -5.7324e-02,
          1.1763e+01,  1.2797e+01, -2.3341e+00,  5.7757e+00, -5.9082e-02,
         -5.7144e-02,  6.3655e+00,  5.6082e+00],
        [-6.9528e+00, -2.6611e+00, -1.0702e+01, -1.3038e-01, -7.9669e-02,
          6.1040e+00,  1.1654e+01, -1.8717e+00,  3.0247e+00, -7.9543e-02,
         -7.9681e-02,  4.7476e+00,  4.0439e+00],
        [ 2.3317e-01,  1.6804e+00,  1.1144e+00,  2.7733e-02,  6.2818e-02,
          1.7155e-02,  4.5875e-01,  1.6943e+00,  4.4272e-01,  6.2998e-02,
          6.2799e-02,  4.2736e-01,  3.2275e-01],
        [-4.0942e+00, -2.3099e+00, -9.3733e+00,  1.0466e-02,  1.2226e-01,
          1.2791e+01,  1.3622e+01, -1.8156e+00,  6.1902e+00,  1.2472e-01,
          1.2201e-01,  6.4802e+00,  5.5247e+00],
        [ 2.2773e-01,  1.6243e+00,  1.0676e+00,  2.6215e-02,  6.0310e-02,
          1.6723e-02,  4.5316e-01,  1.6373e+00,  4.1183e-01,  6.0480e-02,
          6.0293e-02,  4.0009e-01,  3.0185e-01],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-5.3829e+00, -2.8298e+00, -9.4959e+00,  2.0391e-01, -4.5240e-02,
          9.0749e+00,  1.2055e+01, -2.8772e+00,  5.6057e+00, -4.5905e-02,
         -4.5172e-02,  8.2304e+00,  7.3503e+00],
        [-4.5205e+00, -2.6678e+00, -8.6396e+00, -4.4378e-02,  4.8922e-02,
          4.0271e+00,  1.0322e+01, -2.4150e+00,  2.2341e+00,  4.9162e-02,
          4.8897e-02,  3.9250e+00,  3.0669e+00],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-2.8199e-01, -2.2754e+00, -3.0344e+00, -4.0272e-03, -2.1728e-02,
          6.2891e-02,  3.0368e+00, -2.2460e+00,  5.0454e-01, -2.1810e-02,
         -2.1719e-02,  7.0139e-01,  4.5223e-01],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-3.0101e-02, -6.0257e-01, -3.4920e-01, -4.2911e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1737e-01, -6.6294e-02, -1.0337e-02,
         -1.0342e-02, -8.4352e-02, -6.9009e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8182, -0.0261,  0.0321,  2.4714,  0.0593,  2.3684, -0.8182, -0.3568,
        -0.2433, -0.8182, -0.8353, -0.8182, -0.8182], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.8009e-03, -6.9790e+00, -7.7505e+00,  7.8855e+00, -8.7395e+00,
          6.1127e+00,  1.8009e-03, -9.8697e+00, -4.9599e+00,  1.8009e-03,
         -1.0883e+00,  1.8009e-03,  1.8009e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.5627, -11.5538],
        [ -0.2054,  12.2057],
        [ -2.9373,   6.0430],
        [-10.6203,  -5.6749],
        [ 10.9489,   0.0949],
        [  5.0883,  10.4545],
        [ -7.3211,   0.0880],
        [-10.6611,  -5.9336],
        [ -1.5390,   0.8026],
        [ -5.0853,  12.2247],
        [ -7.5356,  14.9437],
        [  8.8835,   2.3574],
        [ -9.0614,  -4.6052]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-8.2035,  7.9696,  0.6017, -2.7780, -9.5798,  6.7590,  5.1073, -3.5817,
        -3.2062,  8.3475, 10.7078, -3.0235, -1.9507], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.5360e+00,  5.1902e+00, -8.8261e-01,  1.8825e-01, -1.1617e+01,
         -3.0109e+00,  7.3917e+00, -5.0429e-01,  2.7591e-02,  6.2055e+00,
          2.7854e+00, -1.0616e+00,  1.2582e+00],
        [ 1.0863e+01, -4.0755e+00, -3.4312e-01,  7.2742e-01, -1.0664e+01,
         -4.0139e+00,  9.5468e+00, -3.9590e+00,  1.2329e-02, -2.2796e+00,
         -1.5576e+01,  6.8790e-02,  3.1184e+00],
        [-1.4642e+01,  7.1294e+00, -4.1546e+00, -1.3205e-01,  1.2756e+01,
          3.2742e+00, -1.0937e+01, -8.2515e-02,  1.0508e-01, -7.1542e-01,
         -8.9964e+00, -3.1784e-01, -2.6675e-01],
        [-4.3054e+00,  1.3906e+01,  1.6532e-01, -8.5551e+00, -8.6290e+00,
          9.0489e+00, -1.2236e+00, -4.8719e+00, -1.1856e-01,  1.1045e+01,
          1.0371e+01,  1.6169e+00, -5.8449e+00],
        [-1.6620e+00,  2.2172e+00,  7.3730e-02, -7.4682e+00,  2.1070e+00,
          1.2570e+01,  2.5803e+00, -5.0797e+00,  4.1404e-02, -1.3499e-01,
         -1.4182e+00,  2.0066e+00, -3.8558e+00],
        [-2.7976e+00,  5.9804e+00,  2.5414e-01, -6.9438e+00,  5.2150e+00,
          8.2924e+00, -2.2936e+00, -2.8446e+00, -1.8938e-01,  7.2713e+00,
          8.7133e+00,  4.0492e+00, -3.9568e+00],
        [ 3.9558e+00, -4.9965e+00, -2.7392e-01,  9.7790e+00, -2.0234e+00,
         -4.3889e+00,  3.8306e+00,  6.0189e+00,  8.3592e-02, -1.1287e+01,
         -2.0694e+01, -4.3281e-01,  7.7430e+00],
        [-6.1261e-01,  7.4853e-01, -1.9733e+00,  1.8636e+00, -2.5340e-01,
         -8.6531e-01, -7.7204e-01,  1.0507e+00, -1.3834e+00,  1.6075e+00,
          1.6765e+00, -1.8126e+00,  2.0834e+00],
        [-1.7054e+00,  2.1658e+00,  7.0490e-02, -7.4993e+00,  2.1097e+00,
          1.2681e+01,  2.5081e+00, -4.9668e+00,  4.2617e-02, -2.1372e-01,
         -1.5385e+00,  1.9917e+00, -4.0639e+00],
        [-8.5966e-01, -5.4158e-01,  3.1900e-02, -1.0584e-01, -5.2516e-01,
         -1.5050e+00, -1.0865e+00, -7.5104e-02,  4.2592e-02, -2.2466e-01,
         -1.8496e-01, -1.6167e+00, -1.0209e-01],
        [ 2.8503e+00, -1.3851e+01, -3.2277e-01,  1.2349e+01, -6.6659e+00,
         -1.6797e+01,  2.0115e+00,  7.6731e+00,  4.9482e-02, -8.4937e+00,
         -8.1226e+00, -3.5145e+00,  7.0066e+00],
        [ 1.6826e+00,  5.2625e-01,  4.1788e-01,  1.3952e-01,  4.3639e-01,
          1.4718e+00,  1.7609e+00,  7.2000e-02,  4.9240e-02,  2.2221e-01,
          3.2291e-01,  1.5640e+00,  2.5923e-01],
        [-8.8093e-01, -4.3473e-01, -1.6270e-01, -3.4271e-02, -5.6408e-01,
         -1.4559e+00, -1.1035e+00, -1.5385e-01,  2.5814e-02, -3.4916e-02,
         -1.0794e-01, -1.7583e+00,  2.8486e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 1.3890,  2.7192, -2.4559, -2.6275,  4.8735,  0.2970,  2.1879, -1.9531,
         5.0658, -2.4521, -0.5826,  3.1194, -2.4773], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.6213e-01, -6.8777e-01, -6.5505e-01,  1.3206e+00,  7.1399e-01,
          1.2783e+00,  1.1355e+00,  4.2453e-01,  7.4292e-01, -1.6221e-01,
         -9.0400e+00, -3.4519e-01, -1.4551e-01],
        [ 5.6044e-01,  1.9149e+00,  1.3483e+00, -3.6948e-01,  2.6564e-01,
          1.6115e-01, -4.7250e-01, -4.1914e-01,  5.5808e-01,  1.6534e-01,
          8.9809e+00,  1.2504e+00,  1.4926e-01],
        [-8.2717e-01, -4.8577e-01, -6.9095e-01, -1.3295e+00, -1.5980e+00,
         -1.6062e+00, -3.5694e-01, -1.2789e-01, -1.5980e+00, -2.0943e-03,
         -4.4515e-02, -1.6309e+00, -1.6794e-03],
        [ 9.8592e-01,  3.9564e+00,  3.5376e+00, -2.5419e+00, -1.1885e+00,
         -2.7500e+00, -2.6475e+00,  5.9882e-01, -1.0693e+00,  1.4172e-01,
          7.7192e+00,  1.9260e+00,  2.3542e-01],
        [-2.3016e+00, -1.9959e+00,  5.9102e-07,  9.3182e-01,  1.2590e+01,
          1.4468e+00, -2.4106e+00, -1.9486e+00,  1.2788e+01, -7.4873e-02,
         -2.9963e+00, -1.8615e+00, -8.3155e-02]], device='cuda:0'))])
loaded xi:  19.928818
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 940.130933411658
W_T_median: 581.3306640883857
W_T_pctile_5: -358.9604697855882
W_T_CVAR_5_pct: -487.0834540584932
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1498.2406919296739
Current xi:  [23.679024]
objective value function right now is: -1498.2406919296739
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [27.753202]
objective value function right now is: -1496.5749784734674
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [31.58878]
objective value function right now is: -1498.0775538210244
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [34.87471]
objective value function right now is: -1492.092012684459
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [38.256027]
objective value function right now is: -1495.6493221491273
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1500.6008577702273
Current xi:  [41.203026]
objective value function right now is: -1500.6008577702273
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [44.005936]
objective value function right now is: -1498.7052386546995
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [46.449093]
objective value function right now is: -1498.8111120593903
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1502.091439687286
Current xi:  [48.892494]
objective value function right now is: -1502.091439687286
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [50.943687]
objective value function right now is: -1496.6045922660323
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.941032]
objective value function right now is: -1501.2469278980911
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.09199]
objective value function right now is: -1495.275040751444
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1502.9608642278442
Current xi:  [57.350636]
objective value function right now is: -1502.9608642278442
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1503.3624404272755
Current xi:  [59.007854]
objective value function right now is: -1503.3624404272755
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1503.4712555434562
Current xi:  [60.444645]
objective value function right now is: -1503.4712555434562
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [61.67378]
objective value function right now is: -1498.8692434448403
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [62.730125]
objective value function right now is: -1500.9931730550402
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [63.289364]
objective value function right now is: -1497.390885264162
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1503.4764059577906
Current xi:  [64.47463]
objective value function right now is: -1503.4764059577906
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1504.348843034615
Current xi:  [65.91577]
objective value function right now is: -1504.348843034615
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [66.50039]
objective value function right now is: -1497.1108459385757
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [66.9394]
objective value function right now is: -1502.5210932515247
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [67.65873]
objective value function right now is: -1499.9989031777416
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.33198]
objective value function right now is: -1502.9843525410047
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.62181]
objective value function right now is: -1500.3243118855494
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [69.69267]
objective value function right now is: -1499.5830148643913
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [69.62544]
objective value function right now is: -1498.9058633206055
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [70.008316]
objective value function right now is: -1503.5683265745934
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [70.12164]
objective value function right now is: -1501.552515679544
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.145226]
objective value function right now is: -1500.5414279812421
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.31481]
objective value function right now is: -1499.3439918863662
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.368866]
objective value function right now is: -1498.818204840637
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.05125]
objective value function right now is: -1494.2670843347823
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.15304]
objective value function right now is: -1501.6234205878998
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -1505.293693057096
Current xi:  [71.24937]
objective value function right now is: -1505.293693057096
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1506.0188620701679
Current xi:  [71.21881]
objective value function right now is: -1506.0188620701679
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.278435]
objective value function right now is: -1505.807987140855
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1506.5806582033672
Current xi:  [71.24892]
objective value function right now is: -1506.5806582033672
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1506.6467878452731
Current xi:  [71.29186]
objective value function right now is: -1506.6467878452731
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.24413]
objective value function right now is: -1506.057907029343
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.314476]
objective value function right now is: -1505.6922191355386
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.36907]
objective value function right now is: -1506.1513431228057
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.496056]
objective value function right now is: -1506.371500552488
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.560005]
objective value function right now is: -1506.4098075295258
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.5307]
objective value function right now is: -1506.1858861049911
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -1506.865540496247
Current xi:  [71.46959]
objective value function right now is: -1506.865540496247
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.636826]
objective value function right now is: -1506.425959218691
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.641205]
objective value function right now is: -1506.7416910559127
new min fval from sgd:  -1507.136020597648
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.55289]
objective value function right now is: -1507.136020597648
new min fval from sgd:  -1507.1365887407185
new min fval from sgd:  -1507.1618274720395
new min fval from sgd:  -1507.1769791020338
new min fval from sgd:  -1507.1860948450021
new min fval from sgd:  -1507.1951154774
new min fval from sgd:  -1507.200116072593
new min fval from sgd:  -1507.2114038074199
new min fval from sgd:  -1507.2275483101746
new min fval from sgd:  -1507.2411149666607
new min fval from sgd:  -1507.2576811828355
new min fval from sgd:  -1507.275650080838
new min fval from sgd:  -1507.2870870169295
new min fval from sgd:  -1507.288610688403
new min fval from sgd:  -1507.3033304246878
new min fval from sgd:  -1507.3156500884243
new min fval from sgd:  -1507.3160236721026
new min fval from sgd:  -1507.3226222698243
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.610535]
objective value function right now is: -1507.2026860473811
min fval:  -1507.3226222698243
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -9.4817,   7.9830],
        [ -8.1629, -16.0301],
        [ -4.9110,  10.3919],
        [ -0.7101,   0.9888],
        [ -0.7099,   0.9888],
        [-59.9122,  -7.8950],
        [ -1.6455, -13.0058],
        [-10.8498,   0.0750],
        [  7.2714,   1.6019],
        [ -0.7099,   0.9888],
        [ -0.7099,   0.9888],
        [ 11.9112,  -1.0453],
        [ 11.8750,  -1.0847]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  3.3485,   5.2007,   6.3170,  -2.7291,  -2.7292,  -5.7608,  -7.4352,
          9.5058,  -8.7228,  -2.7292,  -2.7292, -10.5444, -10.5349],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.0739e-01, -7.5869e-01, -4.7321e-01, -9.9977e-03, -9.9978e-03,
         -1.2751e-01, -3.3453e-01, -7.8950e-01, -2.2892e-02, -9.9978e-03,
         -9.9978e-03, -1.1202e-01, -1.1038e-01],
        [-4.2576e+00, -2.0546e+00, -9.8220e+00,  1.2130e-01,  1.2075e-01,
          1.1151e+01,  1.4489e+01, -1.9523e+00,  2.2885e+00,  1.2074e-01,
          1.2075e-01,  8.9311e+00,  8.2459e+00],
        [-7.7667e+00, -2.4750e+00, -1.3348e+01, -5.8500e-02, -5.8519e-02,
          7.5908e+00,  1.3467e+01, -1.3623e+00, -4.0798e-01, -5.8519e-02,
         -5.8519e-02, -5.5236e+00, -5.5493e+00],
        [ 2.0165e-01,  1.5585e+00,  1.4082e+00,  6.4879e-02,  6.4879e-02,
          7.1709e-02,  5.9895e-01,  1.7364e+00,  2.4912e-01,  6.4879e-02,
          6.4879e-02,  3.9495e-01,  3.8420e-01],
        [-4.8573e+00, -2.2501e+00, -1.0516e+01,  3.3985e-02,  3.4593e-02,
          1.1418e+01,  1.5487e+01, -1.6729e+00,  2.7473e+00,  3.4607e-02,
          3.4592e-02,  9.4779e+00,  8.6212e+00],
        [ 2.0031e-01,  1.5156e+00,  1.3537e+00,  6.2732e-02,  6.2731e-02,
          7.0256e-02,  5.8749e-01,  1.6822e+00,  2.3876e-01,  6.2731e-02,
          6.2731e-02,  3.6956e-01,  3.5935e-01],
        [-1.0739e-01, -7.5869e-01, -4.7321e-01, -9.9977e-03, -9.9978e-03,
         -1.2751e-01, -3.3453e-01, -7.8950e-01, -2.2892e-02, -9.9978e-03,
         -9.9978e-03, -1.1202e-01, -1.1038e-01],
        [-5.2225e+00, -3.4747e+00, -1.0427e+01, -8.4120e-02, -8.4253e-02,
          1.1953e+01,  1.4576e+01, -3.4692e+00,  2.0219e+00, -8.4257e-02,
         -8.4253e-02,  1.1099e+01,  1.0295e+01],
        [-1.0739e-01, -7.5869e-01, -4.7321e-01, -9.9977e-03, -9.9978e-03,
         -1.2751e-01, -3.3453e-01, -7.8950e-01, -2.2892e-02, -9.9978e-03,
         -9.9978e-03, -1.1202e-01, -1.1038e-01],
        [-1.0739e-01, -7.5869e-01, -4.7321e-01, -9.9977e-03, -9.9978e-03,
         -1.2751e-01, -3.3453e-01, -7.8950e-01, -2.2892e-02, -9.9978e-03,
         -9.9978e-03, -1.1202e-01, -1.1038e-01],
        [-1.0739e-01, -7.5869e-01, -4.7321e-01, -9.9977e-03, -9.9978e-03,
         -1.2751e-01, -3.3453e-01, -7.8950e-01, -2.2892e-02, -9.9978e-03,
         -9.9978e-03, -1.1202e-01, -1.1038e-01],
        [-1.0739e-01, -7.5869e-01, -4.7321e-01, -9.9977e-03, -9.9978e-03,
         -1.2751e-01, -3.3453e-01, -7.8950e-01, -2.2892e-02, -9.9978e-03,
         -9.9978e-03, -1.1202e-01, -1.1038e-01],
        [-1.0739e-01, -7.5869e-01, -4.7321e-01, -9.9977e-03, -9.9978e-03,
         -1.2751e-01, -3.3453e-01, -7.8950e-01, -2.2892e-02, -9.9978e-03,
         -9.9978e-03, -1.1202e-01, -1.1038e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.9714,  0.5562,  0.0584,  2.5137,  0.5334,  2.4214, -0.9714, -0.4106,
        -0.9714, -0.9714, -0.9714, -0.9714, -0.9714], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 4.7336e-03, -5.7945e+00, -8.5528e+00,  8.2324e+00, -8.1625e+00,
          6.5058e+00,  4.7336e-03, -1.3684e+01,  4.7336e-03,  4.7336e-03,
          4.7336e-03,  4.7336e-03,  4.7336e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -9.9171, -11.8083],
        [ -0.2404,  14.4484],
        [ -3.8844,   9.5175],
        [-11.1396,  -6.7256],
        [ 13.2995,   0.3983],
        [  6.3605,  12.5716],
        [ -9.3540,  -0.8998],
        [-12.0173,  -7.2657],
        [ -2.1217,   0.3035],
        [ -3.2698,  14.4902],
        [ -9.8026,  17.6141],
        [  2.8128,   2.8354],
        [-10.0513,  -5.9348]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -7.7265,   9.3339,   1.2811,  -3.2818, -12.0880,   6.4675,   6.5927,
         -4.3782,  -3.6060,   9.5988,  12.1938,  -5.7501,  -2.3246],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.6744e+00,  2.7529e+00,  2.8939e+00, -3.5293e+00, -1.2824e+01,
         -6.6641e+00,  6.3948e+00, -1.9499e+00,  4.9709e-02,  6.2259e+00,
          4.9528e+00, -2.3103e-01, -1.8192e+00],
        [ 6.9827e+00, -4.3791e+00, -3.6593e-01, -5.1469e-01, -1.0497e+01,
         -5.7339e+00,  1.1659e+01, -5.8649e+00, -9.0342e-03, -2.7291e+00,
         -1.6221e+01, -4.4569e-01,  3.2523e+00],
        [-8.9517e+00,  8.0527e+00, -6.6727e+00, -1.4096e-01,  1.3908e+01,
          3.6946e+00, -1.2110e+01, -5.5366e-03, -1.1711e-01,  1.3981e+00,
         -8.5100e+00, -1.9816e+00, -6.6350e-01],
        [-1.0067e+01,  1.5936e+01,  2.7731e-01, -1.1247e+01, -7.5591e+00,
          7.4575e+00,  3.6339e-02, -6.7630e+00, -6.4728e-03,  1.1844e+01,
          1.5014e+01, -3.7606e-01, -8.2398e+00],
        [-5.4103e+00,  5.2357e+00, -2.6159e-02, -5.9058e+00,  5.0265e+00,
          6.6531e+00,  1.6128e+00, -5.2592e+00,  4.5658e-01,  2.8449e+00,
         -2.1723e+00, -1.5851e-02, -3.5715e+00],
        [-7.9220e+00,  1.1390e+01, -1.5177e-02, -9.6248e+00,  4.9611e+00,
          1.0272e+01, -1.8893e+00, -5.8332e+00, -2.4476e-02,  1.0776e+01,
          1.0237e+01,  2.8466e-01, -7.2742e+00],
        [ 2.9036e+00, -3.0132e+00, -2.5692e-01,  7.7314e+00, -1.9472e+00,
         -4.5768e+00,  3.5385e+00,  3.7131e+00, -4.0552e-02, -4.8894e+00,
         -1.7025e+01,  4.1892e-02,  8.1277e+00],
        [-1.6056e+00, -2.4910e+00, -3.4564e-02,  4.4529e+00, -2.9973e+00,
         -2.7509e+00,  8.7040e-01,  3.0935e+00, -2.3385e-02, -1.8020e+00,
         -4.9181e+00,  1.3068e-01,  4.8433e+00],
        [-5.4334e+00,  5.3861e+00, -2.6693e-02, -5.9733e+00,  5.1320e+00,
          6.9408e+00,  1.5561e+00, -5.2387e+00,  4.7283e-01,  2.8863e+00,
         -2.2670e+00, -1.5693e-02, -3.6978e+00],
        [-2.6666e-01, -3.8728e-01, -6.6163e-03, -4.5080e-02, -7.8849e-01,
         -1.3184e+00, -1.7873e+00, -1.8888e-02, -5.5625e-03, -1.4414e-01,
          2.1688e-02, -1.4761e-01, -9.5913e-02],
        [ 1.7266e+00, -1.7106e+01,  2.6381e-02,  1.2847e+01, -1.2137e+01,
         -1.9331e+01,  1.4342e+00,  1.0972e+01,  4.7080e-02, -8.7313e+00,
         -5.7281e+00, -3.6511e-01,  7.5037e+00],
        [ 8.7235e-01,  1.3862e+00,  1.3125e-01,  1.4431e-01,  1.7718e+00,
          1.0883e+00,  1.1210e+00,  6.6250e-02,  5.1682e-03,  6.3299e-01,
         -2.6839e-01,  9.6383e-02,  2.4275e-01],
        [-2.6665e-01, -3.8728e-01, -6.6169e-03, -4.5080e-02, -7.8849e-01,
         -1.3184e+00, -1.7873e+00, -1.8888e-02, -5.5625e-03, -1.4415e-01,
          2.1685e-02, -1.4761e-01, -9.5913e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 3.2099,  3.5180, -3.9176, -2.9886, 10.3544,  1.0138,  1.5323, -3.5004,
        10.5336, -3.9809, -3.0428,  4.3658, -3.9809], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.7686e-01, -3.5331e-01, -5.1297e-01,  1.1328e+00,  5.6976e-01,
          1.9326e+00,  9.6221e-01,  2.9107e+00,  5.9903e-01, -5.9607e-02,
         -7.6688e+00, -4.6300e-01, -5.9608e-02],
        [ 5.4785e-01,  1.5745e+00,  1.2082e+00, -1.7868e-01,  4.1461e-01,
         -4.9186e-01, -3.0001e-01, -2.8277e+00,  7.0469e-01,  6.0958e-02,
          7.8565e+00,  1.3746e+00,  6.0958e-02],
        [-1.1986e+00, -5.8657e-01, -9.5647e-01, -1.3591e+00, -1.4180e+00,
         -1.6723e+00, -4.2607e-01, -4.8036e-01, -1.4179e+00, -5.1163e-03,
         -4.7817e-01, -1.9468e+00, -5.1163e-03],
        [ 2.3922e+00,  3.3723e+00,  3.1952e+00, -3.4782e+00, -1.9325e+00,
         -1.0904e+00, -3.3409e+00,  6.0678e-01, -1.8150e+00,  1.0153e-01,
          1.0400e+01,  1.1409e+00,  1.0153e-01],
        [-3.5103e+00, -1.1932e+00, -9.3114e-07, -1.8606e-05,  1.3760e+01,
          8.5696e-05, -1.3573e+00, -5.0326e+00,  1.4284e+01, -8.8980e-06,
         -1.6421e+00, -1.1438e+00, -8.8698e-06]], device='cuda:0'))])
xi:  [71.5617]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 558.3167984168591
W_T_median: 270.80630089217925
W_T_pctile_5: 71.541947121792
W_T_CVAR_5_pct: -45.776743793236655
Average q (qsum/M+1):  50.838359217489916
Optimal xi:  [71.5617]
Expected(across Rb) median(across samples) p_equity:  0.22163243393103282
obj fun:  tensor(-1507.3226, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -7.2542,   7.3206],
        [ -6.7699, -10.5979],
        [ -5.2193,   8.6804],
        [  0.2241,   0.3177],
        [ -0.7246,   0.9976],
        [-52.2798,  -6.4820],
        [ -2.6707, -11.2124],
        [ -8.5838,   0.4254],
        [  6.5248,   8.9539],
        [ -0.7255,   0.9969],
        [ -0.7246,   0.9977],
        [  9.1326,  -0.8868],
        [  8.9232,  -1.4696]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 3.0600,  5.1975,  5.6464, -4.0477, -2.5143, -5.4062, -6.5253,  7.7716,
        -6.6490, -2.5147, -2.5143, -8.5705, -8.6983], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-4.2583e+00, -2.3487e+00, -9.1379e+00, -4.1893e-02, -5.7324e-02,
          1.1763e+01,  1.2797e+01, -2.3341e+00,  5.7757e+00, -5.9082e-02,
         -5.7144e-02,  6.3655e+00,  5.6082e+00],
        [-6.9528e+00, -2.6611e+00, -1.0702e+01, -1.3038e-01, -7.9669e-02,
          6.1040e+00,  1.1654e+01, -1.8717e+00,  3.0247e+00, -7.9543e-02,
         -7.9681e-02,  4.7476e+00,  4.0439e+00],
        [ 2.3317e-01,  1.6804e+00,  1.1144e+00,  2.7733e-02,  6.2818e-02,
          1.7155e-02,  4.5875e-01,  1.6943e+00,  4.4272e-01,  6.2998e-02,
          6.2799e-02,  4.2736e-01,  3.2275e-01],
        [-4.0942e+00, -2.3099e+00, -9.3733e+00,  1.0466e-02,  1.2226e-01,
          1.2791e+01,  1.3622e+01, -1.8156e+00,  6.1902e+00,  1.2472e-01,
          1.2201e-01,  6.4802e+00,  5.5247e+00],
        [ 2.2773e-01,  1.6243e+00,  1.0676e+00,  2.6215e-02,  6.0310e-02,
          1.6723e-02,  4.5316e-01,  1.6373e+00,  4.1183e-01,  6.0480e-02,
          6.0293e-02,  4.0009e-01,  3.0185e-01],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-5.3829e+00, -2.8298e+00, -9.4959e+00,  2.0391e-01, -4.5240e-02,
          9.0749e+00,  1.2055e+01, -2.8772e+00,  5.6057e+00, -4.5905e-02,
         -4.5172e-02,  8.2304e+00,  7.3503e+00],
        [-4.5205e+00, -2.6678e+00, -8.6396e+00, -4.4378e-02,  4.8922e-02,
          4.0271e+00,  1.0322e+01, -2.4150e+00,  2.2341e+00,  4.9162e-02,
          4.8897e-02,  3.9250e+00,  3.0669e+00],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-2.8199e-01, -2.2754e+00, -3.0344e+00, -4.0272e-03, -2.1728e-02,
          6.2891e-02,  3.0368e+00, -2.2460e+00,  5.0454e-01, -2.1810e-02,
         -2.1719e-02,  7.0139e-01,  4.5223e-01],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-3.0101e-02, -6.0257e-01, -3.4920e-01, -4.2911e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1737e-01, -6.6294e-02, -1.0337e-02,
         -1.0342e-02, -8.4352e-02, -6.9009e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8182, -0.0261,  0.0321,  2.4714,  0.0593,  2.3684, -0.8182, -0.3568,
        -0.2433, -0.8182, -0.8353, -0.8182, -0.8182], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.8009e-03, -6.9790e+00, -7.7505e+00,  7.8855e+00, -8.7395e+00,
          6.1127e+00,  1.8009e-03, -9.8697e+00, -4.9599e+00,  1.8009e-03,
         -1.0883e+00,  1.8009e-03,  1.8009e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.5627, -11.5538],
        [ -0.2054,  12.2057],
        [ -2.9373,   6.0430],
        [-10.6203,  -5.6749],
        [ 10.9489,   0.0949],
        [  5.0883,  10.4545],
        [ -7.3211,   0.0880],
        [-10.6611,  -5.9336],
        [ -1.5390,   0.8026],
        [ -5.0853,  12.2247],
        [ -7.5356,  14.9437],
        [  8.8835,   2.3574],
        [ -9.0614,  -4.6052]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-8.2035,  7.9696,  0.6017, -2.7780, -9.5798,  6.7590,  5.1073, -3.5817,
        -3.2062,  8.3475, 10.7078, -3.0235, -1.9507], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.5360e+00,  5.1902e+00, -8.8261e-01,  1.8825e-01, -1.1617e+01,
         -3.0109e+00,  7.3917e+00, -5.0429e-01,  2.7591e-02,  6.2055e+00,
          2.7854e+00, -1.0616e+00,  1.2582e+00],
        [ 1.0863e+01, -4.0755e+00, -3.4312e-01,  7.2742e-01, -1.0664e+01,
         -4.0139e+00,  9.5468e+00, -3.9590e+00,  1.2329e-02, -2.2796e+00,
         -1.5576e+01,  6.8790e-02,  3.1184e+00],
        [-1.4642e+01,  7.1294e+00, -4.1546e+00, -1.3205e-01,  1.2756e+01,
          3.2742e+00, -1.0937e+01, -8.2515e-02,  1.0508e-01, -7.1542e-01,
         -8.9964e+00, -3.1784e-01, -2.6675e-01],
        [-4.3054e+00,  1.3906e+01,  1.6532e-01, -8.5551e+00, -8.6290e+00,
          9.0489e+00, -1.2236e+00, -4.8719e+00, -1.1856e-01,  1.1045e+01,
          1.0371e+01,  1.6169e+00, -5.8449e+00],
        [-1.6620e+00,  2.2172e+00,  7.3730e-02, -7.4682e+00,  2.1070e+00,
          1.2570e+01,  2.5803e+00, -5.0797e+00,  4.1404e-02, -1.3499e-01,
         -1.4182e+00,  2.0066e+00, -3.8558e+00],
        [-2.7976e+00,  5.9804e+00,  2.5414e-01, -6.9438e+00,  5.2150e+00,
          8.2924e+00, -2.2936e+00, -2.8446e+00, -1.8938e-01,  7.2713e+00,
          8.7133e+00,  4.0492e+00, -3.9568e+00],
        [ 3.9558e+00, -4.9965e+00, -2.7392e-01,  9.7790e+00, -2.0234e+00,
         -4.3889e+00,  3.8306e+00,  6.0189e+00,  8.3592e-02, -1.1287e+01,
         -2.0694e+01, -4.3281e-01,  7.7430e+00],
        [-6.1261e-01,  7.4853e-01, -1.9733e+00,  1.8636e+00, -2.5340e-01,
         -8.6531e-01, -7.7204e-01,  1.0507e+00, -1.3834e+00,  1.6075e+00,
          1.6765e+00, -1.8126e+00,  2.0834e+00],
        [-1.7054e+00,  2.1658e+00,  7.0490e-02, -7.4993e+00,  2.1097e+00,
          1.2681e+01,  2.5081e+00, -4.9668e+00,  4.2617e-02, -2.1372e-01,
         -1.5385e+00,  1.9917e+00, -4.0639e+00],
        [-8.5966e-01, -5.4158e-01,  3.1900e-02, -1.0584e-01, -5.2516e-01,
         -1.5050e+00, -1.0865e+00, -7.5104e-02,  4.2592e-02, -2.2466e-01,
         -1.8496e-01, -1.6167e+00, -1.0209e-01],
        [ 2.8503e+00, -1.3851e+01, -3.2277e-01,  1.2349e+01, -6.6659e+00,
         -1.6797e+01,  2.0115e+00,  7.6731e+00,  4.9482e-02, -8.4937e+00,
         -8.1226e+00, -3.5145e+00,  7.0066e+00],
        [ 1.6826e+00,  5.2625e-01,  4.1788e-01,  1.3952e-01,  4.3639e-01,
          1.4718e+00,  1.7609e+00,  7.2000e-02,  4.9240e-02,  2.2221e-01,
          3.2291e-01,  1.5640e+00,  2.5923e-01],
        [-8.8093e-01, -4.3473e-01, -1.6270e-01, -3.4271e-02, -5.6408e-01,
         -1.4559e+00, -1.1035e+00, -1.5385e-01,  2.5814e-02, -3.4916e-02,
         -1.0794e-01, -1.7583e+00,  2.8486e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 1.3890,  2.7192, -2.4559, -2.6275,  4.8735,  0.2970,  2.1879, -1.9531,
         5.0658, -2.4521, -0.5826,  3.1194, -2.4773], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.6213e-01, -6.8777e-01, -6.5505e-01,  1.3206e+00,  7.1399e-01,
          1.2783e+00,  1.1355e+00,  4.2453e-01,  7.4292e-01, -1.6221e-01,
         -9.0400e+00, -3.4519e-01, -1.4551e-01],
        [ 5.6044e-01,  1.9149e+00,  1.3483e+00, -3.6948e-01,  2.6564e-01,
          1.6115e-01, -4.7250e-01, -4.1914e-01,  5.5808e-01,  1.6534e-01,
          8.9809e+00,  1.2504e+00,  1.4926e-01],
        [-8.2717e-01, -4.8577e-01, -6.9095e-01, -1.3295e+00, -1.5980e+00,
         -1.6062e+00, -3.5694e-01, -1.2789e-01, -1.5980e+00, -2.0943e-03,
         -4.4515e-02, -1.6309e+00, -1.6794e-03],
        [ 9.8592e-01,  3.9564e+00,  3.5376e+00, -2.5419e+00, -1.1885e+00,
         -2.7500e+00, -2.6475e+00,  5.9882e-01, -1.0693e+00,  1.4172e-01,
          7.7192e+00,  1.9260e+00,  2.3542e-01],
        [-2.3016e+00, -1.9959e+00,  5.9102e-07,  9.3182e-01,  1.2590e+01,
          1.4468e+00, -2.4106e+00, -1.9486e+00,  1.2788e+01, -7.4873e-02,
         -2.9963e+00, -1.8615e+00, -8.3155e-02]], device='cuda:0'))])
loaded xi:  19.928818
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 940.130933411658
W_T_median: 581.3306640883857
W_T_pctile_5: -358.9604697855882
W_T_CVAR_5_pct: -487.0834540584932
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1411.1050916027832
Current xi:  [28.31305]
objective value function right now is: -1411.1050916027832
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1422.9259695864357
Current xi:  [37.02831]
objective value function right now is: -1422.9259695864357
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1423.8648060222272
Current xi:  [46.21046]
objective value function right now is: -1423.8648060222272
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1429.167985034795
Current xi:  [54.51729]
objective value function right now is: -1429.167985034795
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1438.0349239908453
Current xi:  [62.2239]
objective value function right now is: -1438.0349239908453
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1443.8422257069105
Current xi:  [69.81187]
objective value function right now is: -1443.8422257069105
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1444.4533387491113
Current xi:  [76.326355]
objective value function right now is: -1444.4533387491113
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [82.18511]
objective value function right now is: -1443.2532997154067
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1454.3383808171056
Current xi:  [88.8173]
objective value function right now is: -1454.3383808171056
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [93.69745]
objective value function right now is: -1448.6140409898987
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [98.457596]
objective value function right now is: -1451.4409520105214
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [102.87146]
objective value function right now is: -1451.6264111247679
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1456.2787449769291
Current xi:  [106.724976]
objective value function right now is: -1456.2787449769291
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1456.510996445664
Current xi:  [109.00731]
objective value function right now is: -1456.510996445664
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [111.592415]
objective value function right now is: -1446.2588049163335
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [114.02973]
objective value function right now is: -1453.306512352819
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.959435]
objective value function right now is: -1451.8529003264696
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.735794]
objective value function right now is: -1456.4859760592021
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [119.41261]
objective value function right now is: -1453.7547676941879
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1456.7928879673282
Current xi:  [120.64905]
objective value function right now is: -1456.7928879673282
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [120.62642]
objective value function right now is: -1454.0357969267545
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [121.66362]
objective value function right now is: -1452.5348447393912
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [122.20768]
objective value function right now is: -1455.4530984334447
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1459.643938586577
Current xi:  [122.2003]
objective value function right now is: -1459.643938586577
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [122.25055]
objective value function right now is: -1458.4624643333061
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [122.29045]
objective value function right now is: -1453.5014549086034
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1460.4599363189345
Current xi:  [122.67154]
objective value function right now is: -1460.4599363189345
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [122.42939]
objective value function right now is: -1453.424118172813
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1461.3052156522338
Current xi:  [122.18098]
objective value function right now is: -1461.3052156522338
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [121.57635]
objective value function right now is: -1456.5788186717834
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [121.431915]
objective value function right now is: -1447.5859982807856
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [121.60221]
objective value function right now is: -1459.580527627616
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [122.50547]
objective value function right now is: -1456.3174474389311
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [122.868286]
objective value function right now is: -1450.5025137973657
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [122.161606]
objective value function right now is: -1450.2222878780417
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1462.3979298099082
Current xi:  [122.32959]
objective value function right now is: -1462.3979298099082
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1462.4243789809955
Current xi:  [122.72149]
objective value function right now is: -1462.4243789809955
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [122.95037]
objective value function right now is: -1462.3440543326824
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1463.3179447309135
Current xi:  [123.09605]
objective value function right now is: -1463.3179447309135
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1464.0647879433282
Current xi:  [123.114845]
objective value function right now is: -1464.0647879433282
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [123.19594]
objective value function right now is: -1463.5102989369734
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1464.2543864360457
Current xi:  [123.21664]
objective value function right now is: -1464.2543864360457
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1464.405984529072
Current xi:  [123.2713]
objective value function right now is: -1464.405984529072
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1464.4231109525517
Current xi:  [123.19233]
objective value function right now is: -1464.4231109525517
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [123.13346]
objective value function right now is: -1464.2797388965532
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -1464.4280426532034
Current xi:  [123.55075]
objective value function right now is: -1464.4280426532034
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [123.58051]
objective value function right now is: -1462.6833205388143
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [123.47147]
objective value function right now is: -1464.054888341716
new min fval from sgd:  -1464.6619056542813
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [123.58335]
objective value function right now is: -1464.6619056542813
new min fval from sgd:  -1464.6866219493763
new min fval from sgd:  -1464.706660153796
new min fval from sgd:  -1464.71904556548
new min fval from sgd:  -1464.7272447658843
new min fval from sgd:  -1464.7310464770883
new min fval from sgd:  -1464.7319775991566
new min fval from sgd:  -1464.7618829306225
new min fval from sgd:  -1464.7942924987992
new min fval from sgd:  -1464.8152708665464
new min fval from sgd:  -1464.8436559421425
new min fval from sgd:  -1464.87043644887
new min fval from sgd:  -1464.8884695342592
new min fval from sgd:  -1464.9064723442182
new min fval from sgd:  -1464.9218333376566
new min fval from sgd:  -1464.927680492556
new min fval from sgd:  -1464.9283149512746
new min fval from sgd:  -1464.9522470678933
new min fval from sgd:  -1464.9987277025966
new min fval from sgd:  -1465.0049073574637
new min fval from sgd:  -1465.0109139980907
new min fval from sgd:  -1465.0137089430127
new min fval from sgd:  -1465.0201662603506
new min fval from sgd:  -1465.025563094301
new min fval from sgd:  -1465.0269027795757
new min fval from sgd:  -1465.0278694321137
new min fval from sgd:  -1465.0282115303687
new min fval from sgd:  -1465.0302525132754
new min fval from sgd:  -1465.033715712037
new min fval from sgd:  -1465.0399496275606
new min fval from sgd:  -1465.0441725192775
new min fval from sgd:  -1465.053889555114
new min fval from sgd:  -1465.0613722742926
new min fval from sgd:  -1465.0685803227923
new min fval from sgd:  -1465.0770455240095
new min fval from sgd:  -1465.0864617434881
new min fval from sgd:  -1465.0916224796927
new min fval from sgd:  -1465.0965439861034
new min fval from sgd:  -1465.1012667957777
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [123.61196]
objective value function right now is: -1464.9727065598643
min fval:  -1465.1012667957777
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -6.9316,   8.3495],
        [ -7.9217, -17.7145],
        [ -5.6247,  10.7852],
        [ -2.0879,   1.6814],
        [ -0.9426,   0.9293],
        [-59.2598,  -7.9476],
        [ -0.8204, -13.6983],
        [-11.2125,   0.4054],
        [  7.5052,   1.9873],
        [ -0.9569,   0.9403],
        [ -0.9424,   0.9291],
        [ 12.3948,  -1.0881],
        [ 12.3617,  -1.0908]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  3.6020,   5.6216,   6.2450,  -3.6876,  -2.9644,  -5.6838,  -7.2758,
          9.6719,  -9.1235,  -3.0223,  -2.9606, -10.6721, -10.6570],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.2322e-02, -7.3543e-01, -5.7866e-01,  5.3715e-03, -1.1871e-03,
         -7.8864e-02, -2.3987e-01, -7.8643e-01,  3.0090e-03, -6.0454e-04,
         -1.2222e-03, -2.0118e-01, -1.9954e-01],
        [-2.7742e+00, -2.0071e+00, -9.7332e+00,  1.3884e-02,  8.7856e-02,
          1.0216e+01,  1.4848e+01, -1.8802e+00,  3.1640e+00,  1.0975e-01,
          8.6266e-02,  1.0058e+01,  9.3791e+00],
        [-7.9083e+00, -2.0245e+00, -1.3770e+01, -1.5421e-01, -2.5856e-02,
          8.5594e+00,  1.3597e+01, -8.5725e-01, -5.1278e-01, -5.5579e-02,
         -2.3912e-02, -5.7731e+00, -5.7962e+00],
        [ 7.2282e-01,  1.5848e+00,  1.2226e+00,  7.1164e-03,  6.9638e-02,
          6.0658e-02,  4.9057e-01,  1.7969e+00,  2.4246e-01,  6.1464e-02,
          6.9931e-02,  3.3044e-01,  3.2624e-01],
        [-3.1689e+00, -2.2926e+00, -1.0376e+01, -4.0614e-01, -5.4951e-02,
          1.0632e+01,  1.5980e+01, -1.6894e+00,  3.6763e+00, -5.2559e-02,
         -5.4699e-02,  1.0769e+01,  9.8917e+00],
        [ 7.0314e-01,  1.5389e+00,  1.1792e+00,  7.4740e-03,  6.7913e-02,
          6.0255e-02,  4.8304e-01,  1.7391e+00,  2.2889e-01,  6.0055e-02,
          6.8199e-02,  3.1398e-01,  3.1004e-01],
        [-3.2322e-02, -7.3543e-01, -5.7866e-01,  5.3715e-03, -1.1871e-03,
         -7.8864e-02, -2.3987e-01, -7.8643e-01,  3.0090e-03, -6.0454e-04,
         -1.2222e-03, -2.0118e-01, -1.9954e-01],
        [-3.7600e+00, -3.4436e+00, -9.5925e+00, -6.8132e-01, -3.5773e-03,
          1.1821e+01,  1.5238e+01, -3.8973e+00,  2.7692e+00, -8.3743e-03,
         -3.6443e-03,  1.2878e+01,  1.2071e+01],
        [-3.2322e-02, -7.3543e-01, -5.7866e-01,  5.3716e-03, -1.1871e-03,
         -7.8864e-02, -2.3987e-01, -7.8643e-01,  3.0090e-03, -6.0454e-04,
         -1.2222e-03, -2.0118e-01, -1.9954e-01],
        [-3.2322e-02, -7.3543e-01, -5.7866e-01,  5.3715e-03, -1.1871e-03,
         -7.8864e-02, -2.3987e-01, -7.8643e-01,  3.0089e-03, -6.0454e-04,
         -1.2222e-03, -2.0118e-01, -1.9954e-01],
        [-3.2322e-02, -7.3543e-01, -5.7866e-01,  5.3715e-03, -1.1871e-03,
         -7.8864e-02, -2.3987e-01, -7.8643e-01,  3.0089e-03, -6.0454e-04,
         -1.2222e-03, -2.0118e-01, -1.9954e-01],
        [-3.2322e-02, -7.3543e-01, -5.7866e-01,  5.3715e-03, -1.1871e-03,
         -7.8864e-02, -2.3987e-01, -7.8643e-01,  3.0089e-03, -6.0454e-04,
         -1.2222e-03, -2.0118e-01, -1.9954e-01],
        [-3.2322e-02, -7.3543e-01, -5.7866e-01,  5.3715e-03, -1.1871e-03,
         -7.8864e-02, -2.3987e-01, -7.8643e-01,  3.0089e-03, -6.0454e-04,
         -1.2222e-03, -2.0118e-01, -1.9954e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.0270,  0.7421,  0.5415,  2.5179,  0.6993,  2.4280, -1.0270, -0.3955,
        -1.0270, -1.0270, -1.0270, -1.0270, -1.0270], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0302,  -5.5712,  -8.2775,   8.2582,  -8.1495,   6.5380,   0.0302,
         -14.3033,   0.0302,   0.0302,   0.0302,   0.0302,   0.0302]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -8.8111, -12.2973],
        [  2.6983,  15.3564],
        [ -1.2970,   2.4487],
        [-12.1491,  -6.8659],
        [ 13.1674,  -0.7135],
        [  7.3493,  12.8478],
        [-10.2900,  -0.4196],
        [-11.5927,  -7.6375],
        [ -1.2970,   2.4488],
        [ -0.2991,  15.2854],
        [ -5.8891,  17.9340],
        [  6.9755,   5.3546],
        [-10.6823,  -6.0099]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -8.9241,   9.1032,  -3.0626,  -2.0584, -13.0425,   6.5115,   4.6848,
         -4.5658,  -3.0627,   9.6243,  11.7477,  -7.3471,  -2.4997],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 4.4769e+00, -9.2468e+00,  2.2068e-02,  4.9700e+00, -1.2841e+01,
         -9.0159e+00,  2.8308e+00,  4.1177e+00,  2.2083e-02, -7.0473e+00,
         -9.6574e+00, -1.9974e+00,  4.3277e+00],
        [ 1.1400e+01, -1.0948e+01, -5.1513e-02,  2.3061e+00, -1.0959e+01,
         -9.0947e+00,  4.3698e+00,  3.1609e-01, -5.1658e-02,  1.6148e+00,
         -8.0369e+00, -1.0955e+00,  3.2446e+00],
        [ 2.8939e+00, -1.8183e+00,  2.6779e-01, -4.2944e-01,  8.7937e+00,
          3.6753e-01, -9.9306e+00, -3.6077e-01,  2.6782e-01, -6.2754e+00,
         -1.0437e+00, -1.1379e+00, -6.2301e-01],
        [-8.7152e+00,  1.3430e+01,  1.5257e-01, -9.6949e+00, -1.2375e+01,
          6.8981e+00, -3.6353e+00, -2.9102e+00,  1.5245e-01,  1.2480e+01,
          1.5323e+01, -9.9550e-01, -5.3156e+00],
        [-2.8990e+00,  5.8815e+00,  7.3864e-02, -6.9852e+00,  6.2761e+00,
          1.0412e+01,  2.6624e+00, -6.9662e+00,  7.3860e-02,  2.6987e+00,
         -8.5474e-01, -4.4907e-03, -4.5926e+00],
        [-4.3669e+00,  1.6473e+01, -2.3076e-02, -1.1920e+01,  7.4091e+00,
          9.1360e+00, -2.8565e+00, -7.0168e+00, -2.3028e-02,  1.2267e+01,
          1.3493e+01,  1.3060e+00, -6.9763e+00],
        [ 8.5483e+00, -3.9765e+00,  1.3051e-01,  3.5138e+00, -6.0262e+00,
         -7.0622e+00,  5.6698e+00, -1.5621e+00,  1.3018e-01, -1.7680e+00,
         -4.8862e+00, -1.4833e+00,  2.6109e+00],
        [-4.8746e-01,  6.7170e-01,  2.5923e-02,  6.6235e+00, -1.3620e+00,
          1.8816e-01,  1.0403e+00,  6.4742e+00,  2.5905e-02, -3.2652e-01,
         -8.0377e+00, -4.2371e+00,  5.2664e+00],
        [-2.8999e+00,  6.2305e+00,  7.5829e-02, -7.0705e+00,  6.6687e+00,
          1.1343e+01,  2.5998e+00, -6.9780e+00,  7.5825e-02,  2.7561e+00,
         -9.4419e-01, -4.5232e-03, -4.8053e+00],
        [-3.9130e-01, -6.0823e-01, -1.6969e-01, -2.2352e-01, -5.4615e-01,
         -1.7156e+00, -1.1796e+00, -7.4165e-02, -1.6969e-01, -8.4290e-01,
         -5.6448e-01, -2.7221e-01, -1.3856e-01],
        [ 2.4343e+00, -1.6656e+01, -1.8689e-01,  1.2703e+01, -1.1311e+01,
         -1.8535e+01,  1.1112e+00,  9.0520e+00, -1.8708e-01, -7.9475e+00,
         -7.4434e+00, -1.8468e+00,  7.2546e+00],
        [ 1.1230e+00, -2.1967e+00,  6.1690e-01,  3.2494e-01, -7.0125e-01,
         -6.4759e-01,  1.8289e+00,  7.4483e-02,  6.1682e-01,  1.2560e+00,
          4.4319e+00, -2.1066e-01,  1.6662e-01],
        [-3.9078e-01, -6.0790e-01, -1.6993e-01, -2.2383e-01, -5.4787e-01,
         -1.7145e+00, -1.1805e+00, -7.4132e-02, -1.6993e-01, -8.4471e-01,
         -5.6603e-01, -2.7256e-01, -1.3873e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.0893,  2.0313, -6.4534, -7.4908, 10.9128, -1.9056,  0.4108, -2.5550,
        11.1683, -5.0014, -1.9185,  4.3185, -5.0012], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-6.0238e+00,  3.4701e-01, -2.0570e+00,  6.8323e-01,  3.7578e-01,
          1.8894e+00,  3.7472e-01,  7.9904e-01,  4.0468e-01,  4.1787e-01,
         -5.4343e+00, -2.3907e-01,  4.1949e-01],
        [ 6.6938e+00,  9.1966e-01,  2.6988e+00,  2.8189e-01,  6.3831e-01,
         -4.1810e-01,  3.3340e-01, -7.5960e-01,  9.2980e-01, -4.1688e-01,
          5.4897e+00,  1.1796e+00, -4.1850e-01],
        [-2.4276e-02, -1.7056e-01,  5.3316e-02, -1.4494e+00, -2.4148e+00,
         -2.2859e+00, -1.7363e-01, -2.6631e-01, -2.4148e+00, -9.2319e-04,
         -1.8486e-02, -2.3885e+00, -9.2507e-04],
        [ 1.2542e+00,  1.9442e+00, -2.3099e+00, -7.4867e+00, -1.4421e+00,
         -7.8815e-01, -1.4002e+00,  9.1020e-01, -1.3225e+00,  3.8271e-01,
          9.4039e+00,  1.7058e+00,  3.8327e-01],
        [-2.7524e+00, -2.1169e+00,  7.3890e-01,  3.9524e-08,  1.6567e+01,
          7.3482e-07, -2.2538e+00, -2.9492e+00,  1.7663e+01, -1.3445e-02,
         -2.5767e+00, -2.0313e+00, -1.3449e-02]], device='cuda:0'))])
xi:  [123.60867]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 604.2411699615336
W_T_median: 321.44702691697023
W_T_pctile_5: 123.62650814277514
W_T_CVAR_5_pct: -18.447409822150757
Average q (qsum/M+1):  49.04658360635081
Optimal xi:  [123.60867]
Expected(across Rb) median(across samples) p_equity:  0.2809431478381157
obj fun:  tensor(-1465.1013, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -7.2542,   7.3206],
        [ -6.7699, -10.5979],
        [ -5.2193,   8.6804],
        [  0.2241,   0.3177],
        [ -0.7246,   0.9976],
        [-52.2798,  -6.4820],
        [ -2.6707, -11.2124],
        [ -8.5838,   0.4254],
        [  6.5248,   8.9539],
        [ -0.7255,   0.9969],
        [ -0.7246,   0.9977],
        [  9.1326,  -0.8868],
        [  8.9232,  -1.4696]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 3.0600,  5.1975,  5.6464, -4.0477, -2.5143, -5.4062, -6.5253,  7.7716,
        -6.6490, -2.5147, -2.5143, -8.5705, -8.6983], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-4.2583e+00, -2.3487e+00, -9.1379e+00, -4.1893e-02, -5.7324e-02,
          1.1763e+01,  1.2797e+01, -2.3341e+00,  5.7757e+00, -5.9082e-02,
         -5.7144e-02,  6.3655e+00,  5.6082e+00],
        [-6.9528e+00, -2.6611e+00, -1.0702e+01, -1.3038e-01, -7.9669e-02,
          6.1040e+00,  1.1654e+01, -1.8717e+00,  3.0247e+00, -7.9543e-02,
         -7.9681e-02,  4.7476e+00,  4.0439e+00],
        [ 2.3317e-01,  1.6804e+00,  1.1144e+00,  2.7733e-02,  6.2818e-02,
          1.7155e-02,  4.5875e-01,  1.6943e+00,  4.4272e-01,  6.2998e-02,
          6.2799e-02,  4.2736e-01,  3.2275e-01],
        [-4.0942e+00, -2.3099e+00, -9.3733e+00,  1.0466e-02,  1.2226e-01,
          1.2791e+01,  1.3622e+01, -1.8156e+00,  6.1902e+00,  1.2472e-01,
          1.2201e-01,  6.4802e+00,  5.5247e+00],
        [ 2.2773e-01,  1.6243e+00,  1.0676e+00,  2.6215e-02,  6.0310e-02,
          1.6723e-02,  4.5316e-01,  1.6373e+00,  4.1183e-01,  6.0480e-02,
          6.0293e-02,  4.0009e-01,  3.0185e-01],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-5.3829e+00, -2.8298e+00, -9.4959e+00,  2.0391e-01, -4.5240e-02,
          9.0749e+00,  1.2055e+01, -2.8772e+00,  5.6057e+00, -4.5905e-02,
         -4.5172e-02,  8.2304e+00,  7.3503e+00],
        [-4.5205e+00, -2.6678e+00, -8.6396e+00, -4.4378e-02,  4.8922e-02,
          4.0271e+00,  1.0322e+01, -2.4150e+00,  2.2341e+00,  4.9162e-02,
          4.8897e-02,  3.9250e+00,  3.0669e+00],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-2.8199e-01, -2.2754e+00, -3.0344e+00, -4.0272e-03, -2.1728e-02,
          6.2891e-02,  3.0368e+00, -2.2460e+00,  5.0454e-01, -2.1810e-02,
         -2.1719e-02,  7.0139e-01,  4.5223e-01],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-3.0101e-02, -6.0257e-01, -3.4920e-01, -4.2911e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1737e-01, -6.6294e-02, -1.0337e-02,
         -1.0342e-02, -8.4352e-02, -6.9009e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8182, -0.0261,  0.0321,  2.4714,  0.0593,  2.3684, -0.8182, -0.3568,
        -0.2433, -0.8182, -0.8353, -0.8182, -0.8182], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.8009e-03, -6.9790e+00, -7.7505e+00,  7.8855e+00, -8.7395e+00,
          6.1127e+00,  1.8009e-03, -9.8697e+00, -4.9599e+00,  1.8009e-03,
         -1.0883e+00,  1.8009e-03,  1.8009e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.5627, -11.5538],
        [ -0.2054,  12.2057],
        [ -2.9373,   6.0430],
        [-10.6203,  -5.6749],
        [ 10.9489,   0.0949],
        [  5.0883,  10.4545],
        [ -7.3211,   0.0880],
        [-10.6611,  -5.9336],
        [ -1.5390,   0.8026],
        [ -5.0853,  12.2247],
        [ -7.5356,  14.9437],
        [  8.8835,   2.3574],
        [ -9.0614,  -4.6052]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-8.2035,  7.9696,  0.6017, -2.7780, -9.5798,  6.7590,  5.1073, -3.5817,
        -3.2062,  8.3475, 10.7078, -3.0235, -1.9507], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.5360e+00,  5.1902e+00, -8.8261e-01,  1.8825e-01, -1.1617e+01,
         -3.0109e+00,  7.3917e+00, -5.0429e-01,  2.7591e-02,  6.2055e+00,
          2.7854e+00, -1.0616e+00,  1.2582e+00],
        [ 1.0863e+01, -4.0755e+00, -3.4312e-01,  7.2742e-01, -1.0664e+01,
         -4.0139e+00,  9.5468e+00, -3.9590e+00,  1.2329e-02, -2.2796e+00,
         -1.5576e+01,  6.8790e-02,  3.1184e+00],
        [-1.4642e+01,  7.1294e+00, -4.1546e+00, -1.3205e-01,  1.2756e+01,
          3.2742e+00, -1.0937e+01, -8.2515e-02,  1.0508e-01, -7.1542e-01,
         -8.9964e+00, -3.1784e-01, -2.6675e-01],
        [-4.3054e+00,  1.3906e+01,  1.6532e-01, -8.5551e+00, -8.6290e+00,
          9.0489e+00, -1.2236e+00, -4.8719e+00, -1.1856e-01,  1.1045e+01,
          1.0371e+01,  1.6169e+00, -5.8449e+00],
        [-1.6620e+00,  2.2172e+00,  7.3730e-02, -7.4682e+00,  2.1070e+00,
          1.2570e+01,  2.5803e+00, -5.0797e+00,  4.1404e-02, -1.3499e-01,
         -1.4182e+00,  2.0066e+00, -3.8558e+00],
        [-2.7976e+00,  5.9804e+00,  2.5414e-01, -6.9438e+00,  5.2150e+00,
          8.2924e+00, -2.2936e+00, -2.8446e+00, -1.8938e-01,  7.2713e+00,
          8.7133e+00,  4.0492e+00, -3.9568e+00],
        [ 3.9558e+00, -4.9965e+00, -2.7392e-01,  9.7790e+00, -2.0234e+00,
         -4.3889e+00,  3.8306e+00,  6.0189e+00,  8.3592e-02, -1.1287e+01,
         -2.0694e+01, -4.3281e-01,  7.7430e+00],
        [-6.1261e-01,  7.4853e-01, -1.9733e+00,  1.8636e+00, -2.5340e-01,
         -8.6531e-01, -7.7204e-01,  1.0507e+00, -1.3834e+00,  1.6075e+00,
          1.6765e+00, -1.8126e+00,  2.0834e+00],
        [-1.7054e+00,  2.1658e+00,  7.0490e-02, -7.4993e+00,  2.1097e+00,
          1.2681e+01,  2.5081e+00, -4.9668e+00,  4.2617e-02, -2.1372e-01,
         -1.5385e+00,  1.9917e+00, -4.0639e+00],
        [-8.5966e-01, -5.4158e-01,  3.1900e-02, -1.0584e-01, -5.2516e-01,
         -1.5050e+00, -1.0865e+00, -7.5104e-02,  4.2592e-02, -2.2466e-01,
         -1.8496e-01, -1.6167e+00, -1.0209e-01],
        [ 2.8503e+00, -1.3851e+01, -3.2277e-01,  1.2349e+01, -6.6659e+00,
         -1.6797e+01,  2.0115e+00,  7.6731e+00,  4.9482e-02, -8.4937e+00,
         -8.1226e+00, -3.5145e+00,  7.0066e+00],
        [ 1.6826e+00,  5.2625e-01,  4.1788e-01,  1.3952e-01,  4.3639e-01,
          1.4718e+00,  1.7609e+00,  7.2000e-02,  4.9240e-02,  2.2221e-01,
          3.2291e-01,  1.5640e+00,  2.5923e-01],
        [-8.8093e-01, -4.3473e-01, -1.6270e-01, -3.4271e-02, -5.6408e-01,
         -1.4559e+00, -1.1035e+00, -1.5385e-01,  2.5814e-02, -3.4916e-02,
         -1.0794e-01, -1.7583e+00,  2.8486e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 1.3890,  2.7192, -2.4559, -2.6275,  4.8735,  0.2970,  2.1879, -1.9531,
         5.0658, -2.4521, -0.5826,  3.1194, -2.4773], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.6213e-01, -6.8777e-01, -6.5505e-01,  1.3206e+00,  7.1399e-01,
          1.2783e+00,  1.1355e+00,  4.2453e-01,  7.4292e-01, -1.6221e-01,
         -9.0400e+00, -3.4519e-01, -1.4551e-01],
        [ 5.6044e-01,  1.9149e+00,  1.3483e+00, -3.6948e-01,  2.6564e-01,
          1.6115e-01, -4.7250e-01, -4.1914e-01,  5.5808e-01,  1.6534e-01,
          8.9809e+00,  1.2504e+00,  1.4926e-01],
        [-8.2717e-01, -4.8577e-01, -6.9095e-01, -1.3295e+00, -1.5980e+00,
         -1.6062e+00, -3.5694e-01, -1.2789e-01, -1.5980e+00, -2.0943e-03,
         -4.4515e-02, -1.6309e+00, -1.6794e-03],
        [ 9.8592e-01,  3.9564e+00,  3.5376e+00, -2.5419e+00, -1.1885e+00,
         -2.7500e+00, -2.6475e+00,  5.9882e-01, -1.0693e+00,  1.4172e-01,
          7.7192e+00,  1.9260e+00,  2.3542e-01],
        [-2.3016e+00, -1.9959e+00,  5.9102e-07,  9.3182e-01,  1.2590e+01,
          1.4468e+00, -2.4106e+00, -1.9486e+00,  1.2788e+01, -7.4873e-02,
         -2.9963e+00, -1.8615e+00, -8.3155e-02]], device='cuda:0'))])
loaded xi:  19.928818
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 940.130933411658
W_T_median: 581.3306640883857
W_T_pctile_5: -358.9604697855882
W_T_CVAR_5_pct: -487.0834540584932
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1316.7811573592971
Current xi:  [30.920048]
objective value function right now is: -1316.7811573592971
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1323.6955546017882
Current xi:  [42.618767]
objective value function right now is: -1323.6955546017882
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1358.8577554346793
Current xi:  [53.60671]
objective value function right now is: -1358.8577554346793
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [63.923088]
objective value function right now is: -1313.436154454279
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1376.8741698416718
Current xi:  [73.901184]
objective value function right now is: -1376.8741698416718
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1390.753168353063
Current xi:  [82.78211]
objective value function right now is: -1390.753168353063
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1399.6323103890252
Current xi:  [91.664055]
objective value function right now is: -1399.6323103890252
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1409.2172903879643
Current xi:  [98.96292]
objective value function right now is: -1409.2172903879643
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.41512]
objective value function right now is: -1404.2636775877058
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.91418]
objective value function right now is: -1407.9337894441205
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [119.22757]
objective value function right now is: -1401.8437831446547
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1416.3409405852624
Current xi:  [123.902016]
objective value function right now is: -1416.3409405852624
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1422.3664730196288
Current xi:  [127.77005]
objective value function right now is: -1422.3664730196288
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [130.67093]
objective value function right now is: -1418.5061844850661
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [134.0949]
objective value function right now is: -1412.2697017571006
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [136.00511]
objective value function right now is: -1415.3772051580484
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [136.8021]
objective value function right now is: -1417.302157573442
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1427.2267532624817
Current xi:  [137.3236]
objective value function right now is: -1427.2267532624817
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [138.67703]
objective value function right now is: -1420.4298420790078
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [140.93686]
objective value function right now is: -1406.42811046586
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [139.59802]
objective value function right now is: -1411.4779140132205
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [140.34521]
objective value function right now is: -1412.9565106959567
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [140.3453]
objective value function right now is: -1422.4179273661607
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [139.89635]
objective value function right now is: -1418.0521667513183
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [139.23244]
objective value function right now is: -1409.5983216152797
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [139.65372]
objective value function right now is: -1424.6261309597419
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [140.23836]
objective value function right now is: -1393.789445793298
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [140.6201]
objective value function right now is: -1418.9131986717184
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [140.87943]
objective value function right now is: -1415.3609720470606
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [140.17612]
objective value function right now is: -1420.6558179279446
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [140.35956]
objective value function right now is: -1416.4255595588982
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1428.2900100097013
Current xi:  [139.62907]
objective value function right now is: -1428.2900100097013
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [139.85599]
objective value function right now is: -1418.698872232834
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [140.62363]
objective value function right now is: -1422.141069329148
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [140.64769]
objective value function right now is: -1417.9378694708216
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1432.3200365543553
Current xi:  [140.83525]
objective value function right now is: -1432.3200365543553
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1432.3997501205322
Current xi:  [140.99709]
objective value function right now is: -1432.3997501205322
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [141.22754]
objective value function right now is: -1432.2503017829079
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1433.9747610068644
Current xi:  [141.2933]
objective value function right now is: -1433.9747610068644
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [141.49747]
objective value function right now is: -1432.0930949512817
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [141.60379]
objective value function right now is: -1433.8348930441998
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [141.7253]
objective value function right now is: -1432.2702348248574
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [141.8323]
objective value function right now is: -1433.6933018160853
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [141.83305]
objective value function right now is: -1433.6120154954772
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [141.91504]
objective value function right now is: -1433.0804684118018
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -1434.0119739878462
Current xi:  [141.99973]
objective value function right now is: -1434.0119739878462
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [141.9622]
objective value function right now is: -1429.4842809403226
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [142.14781]
objective value function right now is: -1429.8354038317177
new min fval from sgd:  -1435.0224554026024
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [142.14874]
objective value function right now is: -1435.0224554026024
new min fval from sgd:  -1435.0236311618146
new min fval from sgd:  -1435.0528305975172
new min fval from sgd:  -1435.0747072949066
new min fval from sgd:  -1435.090933850115
new min fval from sgd:  -1435.1001678213315
new min fval from sgd:  -1435.1115708961652
new min fval from sgd:  -1435.1220994878001
new min fval from sgd:  -1435.1313489596932
new min fval from sgd:  -1435.1458659710968
new min fval from sgd:  -1435.1523749307278
new min fval from sgd:  -1435.1555906033507
new min fval from sgd:  -1435.1620105527982
new min fval from sgd:  -1435.1757643174044
new min fval from sgd:  -1435.1832410985378
new min fval from sgd:  -1435.1906042996968
new min fval from sgd:  -1435.1965718854358
new min fval from sgd:  -1435.1981422323777
new min fval from sgd:  -1435.2051601783005
new min fval from sgd:  -1435.209571615835
new min fval from sgd:  -1435.2147405352855
new min fval from sgd:  -1435.2154649043412
new min fval from sgd:  -1435.229533625146
new min fval from sgd:  -1435.2448724474311
new min fval from sgd:  -1435.2600047425162
new min fval from sgd:  -1435.2664030104174
new min fval from sgd:  -1435.2705992675615
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [142.14857]
objective value function right now is: -1435.06007840519
min fval:  -1435.2705992675615
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-8.0672e+00,  8.1360e+00],
        [-6.6880e+00, -1.1801e+01],
        [-7.1438e+00,  1.0528e+01],
        [-1.0518e+00,  4.9085e-01],
        [-1.0518e+00,  4.9095e-01],
        [-5.5384e+01, -7.9777e+00],
        [-1.5975e-02, -1.3975e+01],
        [-1.1419e+01,  5.2075e-02],
        [ 7.6430e+00,  1.4521e+00],
        [-1.0518e+00,  4.9094e-01],
        [-1.0518e+00,  4.9095e-01],
        [ 1.2467e+01, -6.9242e-01],
        [ 1.2419e+01, -6.9495e-01]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  2.8374,   6.8159,   6.0309,  -3.1617,  -3.1619,  -5.6509,  -7.2315,
          9.6348,  -8.9851,  -3.1619,  -3.1619, -10.6384, -10.6191],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.3267e-01, -8.2524e-01, -5.5532e-01, -8.0077e-03, -8.0060e-03,
         -7.1010e-02, -2.6663e-01, -8.6329e-01, -5.3915e-02, -8.0060e-03,
         -8.0060e-03, -2.2396e-01, -2.2104e-01],
        [-2.4744e+00, -2.9521e+00, -9.9575e+00,  3.8607e-02,  3.8267e-02,
          1.0035e+01,  1.4123e+01, -3.2202e+00,  3.3797e+00,  3.8270e-02,
          3.8267e-02,  1.0690e+01,  9.9530e+00],
        [-5.1616e+00, -2.2860e+00, -1.2415e+01,  2.5188e-03,  2.3422e-03,
          7.9291e+00,  1.3178e+01, -1.2069e+00, -6.8196e-01,  2.3424e-03,
          2.3424e-03, -2.6989e+00, -2.7733e+00],
        [ 7.7428e-01,  1.5324e+00,  1.3015e+00,  2.9584e-02,  2.9587e-02,
          5.3442e-02,  3.7044e-01,  1.7795e+00,  2.5206e-01,  2.9587e-02,
          2.9587e-02,  5.0907e-01,  5.0300e-01],
        [-3.0976e+00, -2.3016e+00, -1.0721e+01, -4.2380e-02, -4.2163e-02,
          1.1295e+01,  1.6756e+01, -2.0388e+00,  3.4189e+00, -4.2164e-02,
         -4.2163e-02,  1.0243e+01,  9.3048e+00],
        [ 7.5151e-01,  1.4755e+00,  1.2492e+00,  2.8970e-02,  2.8972e-02,
          5.3092e-02,  3.6500e-01,  1.7108e+00,  2.3753e-01,  2.8972e-02,
          2.8972e-02,  4.8283e-01,  4.7714e-01],
        [-2.3267e-01, -8.2524e-01, -5.5532e-01, -8.0077e-03, -8.0060e-03,
         -7.1010e-02, -2.6663e-01, -8.6329e-01, -5.3915e-02, -8.0060e-03,
         -8.0060e-03, -2.2395e-01, -2.2104e-01],
        [-3.0292e+00, -2.9437e+00, -9.4668e+00,  5.1308e-02,  5.1330e-02,
          1.0283e+01,  1.5058e+01, -3.8905e+00,  2.3335e+00,  5.1329e-02,
          5.1330e-02,  1.2141e+01,  1.1259e+01],
        [-2.3267e-01, -8.2524e-01, -5.5532e-01, -8.0077e-03, -8.0060e-03,
         -7.1010e-02, -2.6663e-01, -8.6329e-01, -5.3915e-02, -8.0060e-03,
         -8.0060e-03, -2.2396e-01, -2.2104e-01],
        [-2.3267e-01, -8.2524e-01, -5.5532e-01, -8.0077e-03, -8.0060e-03,
         -7.1010e-02, -2.6663e-01, -8.6329e-01, -5.3915e-02, -8.0060e-03,
         -8.0060e-03, -2.2395e-01, -2.2104e-01],
        [-2.3267e-01, -8.2524e-01, -5.5532e-01, -8.0077e-03, -8.0060e-03,
         -7.1010e-02, -2.6663e-01, -8.6329e-01, -5.3915e-02, -8.0060e-03,
         -8.0060e-03, -2.2395e-01, -2.2104e-01],
        [-2.3267e-01, -8.2524e-01, -5.5532e-01, -8.0077e-03, -8.0060e-03,
         -7.1010e-02, -2.6663e-01, -8.6329e-01, -5.3915e-02, -8.0060e-03,
         -8.0060e-03, -2.2395e-01, -2.2104e-01],
        [-2.3267e-01, -8.2524e-01, -5.5532e-01, -8.0077e-03, -8.0060e-03,
         -7.1010e-02, -2.6663e-01, -8.6329e-01, -5.3915e-02, -8.0060e-03,
         -8.0060e-03, -2.2395e-01, -2.2104e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.1849, -0.3797,  0.2683,  2.6760,  0.3021,  2.5627, -1.1849, -0.5278,
        -1.1849, -1.1849, -1.1849, -1.1849, -1.1849], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.8515e-03, -7.4384e+00, -5.9478e+00,  7.0591e+00, -1.0568e+01,
          5.3403e+00,  1.8515e-03, -1.1846e+01,  1.8515e-03,  1.8515e-03,
          1.8515e-03,  1.8515e-03,  1.8515e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -9.5012, -11.8231],
        [  2.7017,  15.4618],
        [ -0.7762,   2.4078],
        [-11.5869,  -6.7025],
        [ 12.6334,  -1.0864],
        [  7.0666,  12.4337],
        [ -9.2041,  -1.1588],
        [-11.5604,  -7.2360],
        [ -0.7755,   2.4075],
        [ -5.8908,  15.6008],
        [ -1.7798,  18.0823],
        [  8.7596,   5.1763],
        [-10.3329,  -5.8550]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -8.5192,   8.8039,  -3.7281,  -2.2847, -13.4812,   6.5563,   3.2225,
         -4.2466,  -3.7286,   9.1158,  11.4530,  -5.8217,  -2.4213],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 8.1324e-01,  1.8955e+00, -5.1956e-01, -7.9039e-01, -1.5226e+00,
         -7.9528e+00,  1.3563e+00,  2.7744e+00, -5.1973e-01,  1.0357e+01,
          3.7804e+00, -9.7791e+00, -1.0363e+00],
        [ 4.7373e+00, -1.5229e+01, -1.4247e-01,  4.5015e+00, -1.4091e+01,
         -8.6517e+00,  3.9910e+00,  2.7582e+00, -1.4257e-01, -3.1471e+00,
         -1.2965e+01, -3.3099e+00,  5.6880e+00],
        [-1.0088e+01, -3.5404e+00,  8.6232e-01, -3.5634e+00,  8.6478e+00,
          1.2461e+00, -2.7788e+00, -3.6542e-01,  8.6335e-01, -1.0750e-01,
         -6.0657e+00,  2.4202e-01, -3.2821e+00],
        [-1.1516e+01,  1.1719e+01, -1.0689e-01, -9.3448e+00, -1.0162e+01,
          6.9062e+00, -1.7523e+00, -1.7786e+00, -1.0733e-01,  1.3064e+01,
          1.6452e+01, -1.4376e+00, -5.3131e+00],
        [-4.0114e+00,  4.7373e+00,  5.4297e-02, -6.7239e+00,  5.8864e+00,
          1.4653e+01,  1.0659e-01, -6.6896e+00,  5.4354e-02, -7.7756e-01,
          9.0578e-01, -6.0813e-03, -4.2399e+00],
        [-2.3356e+00,  1.5659e+01, -1.9282e-02, -1.4560e+01,  4.9492e+00,
          5.6383e+00,  1.0367e-01, -7.6986e+00, -1.9107e-02,  1.3664e+01,
          1.6150e+01, -1.3139e+00, -9.1125e+00],
        [ 9.9019e+00, -1.7572e+00, -3.7955e-01, -7.4926e-01, -3.1000e+00,
         -9.5116e+00,  5.7989e-01, -4.3245e+00, -3.7991e-01, -4.1497e-03,
         -7.6390e-01, -1.7285e+00, -8.2235e-01],
        [-1.9591e-01,  2.8167e-01, -3.3063e-02,  6.7791e+00, -5.8808e+00,
          3.6660e-01,  5.0816e-01,  7.2782e+00, -3.2975e-02, -6.5565e+00,
         -1.5841e+00, -3.5286e+00,  4.6157e+00],
        [-3.9423e+00,  5.2186e+00,  6.6103e-02, -6.8045e+00,  6.3576e+00,
          1.6038e+01, -5.7641e-02, -6.6517e+00,  6.6170e-02, -8.7008e-01,
          9.4909e-01, -6.1421e-03, -4.4657e+00],
        [-5.1463e-01, -7.0043e-01, -2.5382e-01, -2.5485e-01, -7.1419e-01,
         -2.4372e+00, -6.1399e-01, -5.9210e-02, -2.5383e-01, -4.6812e-01,
         -3.5917e-01,  5.4168e-03, -1.8664e-01],
        [ 1.3494e+00, -1.7959e+01,  1.9019e-01,  1.2997e+01, -1.1264e+01,
         -1.9635e+01,  1.7274e+00,  1.0690e+01,  1.9070e-01, -8.2734e+00,
         -1.0424e+01, -3.1650e+00,  7.7050e+00],
        [ 2.4709e+00,  1.0705e+00, -2.9406e-01,  3.8998e+00,  6.0434e+00,
         -2.3032e+00,  3.2258e+00,  1.3079e+00, -2.9450e-01,  4.0153e+00,
          2.1987e+00,  3.9520e+00,  2.9347e+00],
        [-5.1464e-01, -7.0043e-01, -2.5383e-01, -2.5485e-01, -7.1419e-01,
         -2.4372e+00, -6.1399e-01, -5.9211e-02, -2.5383e-01, -4.6812e-01,
         -3.5917e-01,  5.4173e-03, -1.8664e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-6.2544,  1.1337, -6.6370, -8.3639, 12.8030, -2.4604, -3.4370, -2.3849,
        13.0332, -5.7779, -1.3202,  3.8283, -5.7779], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.4454e+00, -2.4173e+00, -3.2372e+00,  6.5352e-01,  7.1182e-01,
          1.3749e+00, -2.7179e+00,  6.0148e-01,  7.4071e-01,  6.7879e-01,
         -1.2473e+01, -3.3244e-01,  6.7880e-01],
        [-8.6867e-01,  3.6331e+00,  3.8769e+00,  3.0002e-01,  2.7188e-01,
          6.6588e-02,  3.3552e+00, -5.7947e-01,  5.6374e-01, -6.7808e-01,
          1.2469e+01,  1.2419e+00, -6.7809e-01],
        [ 1.2148e-02,  1.8698e-01, -8.5112e-03, -1.7934e+00, -2.3930e+00,
         -2.9058e+00,  3.5602e-02,  4.8901e-01, -2.3930e+00, -1.1915e-02,
          4.8585e-02, -2.3288e+00, -1.1915e-02],
        [-1.1894e-01,  9.9371e-01,  3.2997e-01, -9.7582e+00, -1.2738e+00,
         -1.3140e+00, -1.8062e+00,  1.1090e+00, -1.1534e+00,  1.0366e-02,
          1.0896e+01,  1.7772e+00,  1.0366e-02],
        [-5.1413e-01, -3.2980e+00,  9.0231e-08, -3.8404e-08,  1.7667e+01,
         -5.9152e-08, -5.7872e+00, -4.2665e+00,  1.9169e+01, -1.4289e-02,
         -3.9634e+00, -3.2115e+00, -1.4289e-02]], device='cuda:0'))])
xi:  [142.14973]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 607.8717509459212
W_T_median: 348.33714171394445
W_T_pctile_5: 142.07389992580087
W_T_CVAR_5_pct: -13.132103123155042
Average q (qsum/M+1):  48.41712213331653
Optimal xi:  [142.14973]
Expected(across Rb) median(across samples) p_equity:  0.18824668688078722
obj fun:  tensor(-1435.2706, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -7.2542,   7.3206],
        [ -6.7699, -10.5979],
        [ -5.2193,   8.6804],
        [  0.2241,   0.3177],
        [ -0.7246,   0.9976],
        [-52.2798,  -6.4820],
        [ -2.6707, -11.2124],
        [ -8.5838,   0.4254],
        [  6.5248,   8.9539],
        [ -0.7255,   0.9969],
        [ -0.7246,   0.9977],
        [  9.1326,  -0.8868],
        [  8.9232,  -1.4696]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 3.0600,  5.1975,  5.6464, -4.0477, -2.5143, -5.4062, -6.5253,  7.7716,
        -6.6490, -2.5147, -2.5143, -8.5705, -8.6983], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-4.2583e+00, -2.3487e+00, -9.1379e+00, -4.1893e-02, -5.7324e-02,
          1.1763e+01,  1.2797e+01, -2.3341e+00,  5.7757e+00, -5.9082e-02,
         -5.7144e-02,  6.3655e+00,  5.6082e+00],
        [-6.9528e+00, -2.6611e+00, -1.0702e+01, -1.3038e-01, -7.9669e-02,
          6.1040e+00,  1.1654e+01, -1.8717e+00,  3.0247e+00, -7.9543e-02,
         -7.9681e-02,  4.7476e+00,  4.0439e+00],
        [ 2.3317e-01,  1.6804e+00,  1.1144e+00,  2.7733e-02,  6.2818e-02,
          1.7155e-02,  4.5875e-01,  1.6943e+00,  4.4272e-01,  6.2998e-02,
          6.2799e-02,  4.2736e-01,  3.2275e-01],
        [-4.0942e+00, -2.3099e+00, -9.3733e+00,  1.0466e-02,  1.2226e-01,
          1.2791e+01,  1.3622e+01, -1.8156e+00,  6.1902e+00,  1.2472e-01,
          1.2201e-01,  6.4802e+00,  5.5247e+00],
        [ 2.2773e-01,  1.6243e+00,  1.0676e+00,  2.6215e-02,  6.0310e-02,
          1.6723e-02,  4.5316e-01,  1.6373e+00,  4.1183e-01,  6.0480e-02,
          6.0293e-02,  4.0009e-01,  3.0185e-01],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-5.3829e+00, -2.8298e+00, -9.4959e+00,  2.0391e-01, -4.5240e-02,
          9.0749e+00,  1.2055e+01, -2.8772e+00,  5.6057e+00, -4.5905e-02,
         -4.5172e-02,  8.2304e+00,  7.3503e+00],
        [-4.5205e+00, -2.6678e+00, -8.6396e+00, -4.4378e-02,  4.8922e-02,
          4.0271e+00,  1.0322e+01, -2.4150e+00,  2.2341e+00,  4.9162e-02,
          4.8897e-02,  3.9250e+00,  3.0669e+00],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-2.8199e-01, -2.2754e+00, -3.0344e+00, -4.0272e-03, -2.1728e-02,
          6.2891e-02,  3.0368e+00, -2.2460e+00,  5.0454e-01, -2.1810e-02,
         -2.1719e-02,  7.0139e-01,  4.5223e-01],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-3.0101e-02, -6.0257e-01, -3.4920e-01, -4.2911e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1737e-01, -6.6294e-02, -1.0337e-02,
         -1.0342e-02, -8.4352e-02, -6.9009e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8182, -0.0261,  0.0321,  2.4714,  0.0593,  2.3684, -0.8182, -0.3568,
        -0.2433, -0.8182, -0.8353, -0.8182, -0.8182], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.8009e-03, -6.9790e+00, -7.7505e+00,  7.8855e+00, -8.7395e+00,
          6.1127e+00,  1.8009e-03, -9.8697e+00, -4.9599e+00,  1.8009e-03,
         -1.0883e+00,  1.8009e-03,  1.8009e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.5627, -11.5538],
        [ -0.2054,  12.2057],
        [ -2.9373,   6.0430],
        [-10.6203,  -5.6749],
        [ 10.9489,   0.0949],
        [  5.0883,  10.4545],
        [ -7.3211,   0.0880],
        [-10.6611,  -5.9336],
        [ -1.5390,   0.8026],
        [ -5.0853,  12.2247],
        [ -7.5356,  14.9437],
        [  8.8835,   2.3574],
        [ -9.0614,  -4.6052]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-8.2035,  7.9696,  0.6017, -2.7780, -9.5798,  6.7590,  5.1073, -3.5817,
        -3.2062,  8.3475, 10.7078, -3.0235, -1.9507], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.5360e+00,  5.1902e+00, -8.8261e-01,  1.8825e-01, -1.1617e+01,
         -3.0109e+00,  7.3917e+00, -5.0429e-01,  2.7591e-02,  6.2055e+00,
          2.7854e+00, -1.0616e+00,  1.2582e+00],
        [ 1.0863e+01, -4.0755e+00, -3.4312e-01,  7.2742e-01, -1.0664e+01,
         -4.0139e+00,  9.5468e+00, -3.9590e+00,  1.2329e-02, -2.2796e+00,
         -1.5576e+01,  6.8790e-02,  3.1184e+00],
        [-1.4642e+01,  7.1294e+00, -4.1546e+00, -1.3205e-01,  1.2756e+01,
          3.2742e+00, -1.0937e+01, -8.2515e-02,  1.0508e-01, -7.1542e-01,
         -8.9964e+00, -3.1784e-01, -2.6675e-01],
        [-4.3054e+00,  1.3906e+01,  1.6532e-01, -8.5551e+00, -8.6290e+00,
          9.0489e+00, -1.2236e+00, -4.8719e+00, -1.1856e-01,  1.1045e+01,
          1.0371e+01,  1.6169e+00, -5.8449e+00],
        [-1.6620e+00,  2.2172e+00,  7.3730e-02, -7.4682e+00,  2.1070e+00,
          1.2570e+01,  2.5803e+00, -5.0797e+00,  4.1404e-02, -1.3499e-01,
         -1.4182e+00,  2.0066e+00, -3.8558e+00],
        [-2.7976e+00,  5.9804e+00,  2.5414e-01, -6.9438e+00,  5.2150e+00,
          8.2924e+00, -2.2936e+00, -2.8446e+00, -1.8938e-01,  7.2713e+00,
          8.7133e+00,  4.0492e+00, -3.9568e+00],
        [ 3.9558e+00, -4.9965e+00, -2.7392e-01,  9.7790e+00, -2.0234e+00,
         -4.3889e+00,  3.8306e+00,  6.0189e+00,  8.3592e-02, -1.1287e+01,
         -2.0694e+01, -4.3281e-01,  7.7430e+00],
        [-6.1261e-01,  7.4853e-01, -1.9733e+00,  1.8636e+00, -2.5340e-01,
         -8.6531e-01, -7.7204e-01,  1.0507e+00, -1.3834e+00,  1.6075e+00,
          1.6765e+00, -1.8126e+00,  2.0834e+00],
        [-1.7054e+00,  2.1658e+00,  7.0490e-02, -7.4993e+00,  2.1097e+00,
          1.2681e+01,  2.5081e+00, -4.9668e+00,  4.2617e-02, -2.1372e-01,
         -1.5385e+00,  1.9917e+00, -4.0639e+00],
        [-8.5966e-01, -5.4158e-01,  3.1900e-02, -1.0584e-01, -5.2516e-01,
         -1.5050e+00, -1.0865e+00, -7.5104e-02,  4.2592e-02, -2.2466e-01,
         -1.8496e-01, -1.6167e+00, -1.0209e-01],
        [ 2.8503e+00, -1.3851e+01, -3.2277e-01,  1.2349e+01, -6.6659e+00,
         -1.6797e+01,  2.0115e+00,  7.6731e+00,  4.9482e-02, -8.4937e+00,
         -8.1226e+00, -3.5145e+00,  7.0066e+00],
        [ 1.6826e+00,  5.2625e-01,  4.1788e-01,  1.3952e-01,  4.3639e-01,
          1.4718e+00,  1.7609e+00,  7.2000e-02,  4.9240e-02,  2.2221e-01,
          3.2291e-01,  1.5640e+00,  2.5923e-01],
        [-8.8093e-01, -4.3473e-01, -1.6270e-01, -3.4271e-02, -5.6408e-01,
         -1.4559e+00, -1.1035e+00, -1.5385e-01,  2.5814e-02, -3.4916e-02,
         -1.0794e-01, -1.7583e+00,  2.8486e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 1.3890,  2.7192, -2.4559, -2.6275,  4.8735,  0.2970,  2.1879, -1.9531,
         5.0658, -2.4521, -0.5826,  3.1194, -2.4773], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.6213e-01, -6.8777e-01, -6.5505e-01,  1.3206e+00,  7.1399e-01,
          1.2783e+00,  1.1355e+00,  4.2453e-01,  7.4292e-01, -1.6221e-01,
         -9.0400e+00, -3.4519e-01, -1.4551e-01],
        [ 5.6044e-01,  1.9149e+00,  1.3483e+00, -3.6948e-01,  2.6564e-01,
          1.6115e-01, -4.7250e-01, -4.1914e-01,  5.5808e-01,  1.6534e-01,
          8.9809e+00,  1.2504e+00,  1.4926e-01],
        [-8.2717e-01, -4.8577e-01, -6.9095e-01, -1.3295e+00, -1.5980e+00,
         -1.6062e+00, -3.5694e-01, -1.2789e-01, -1.5980e+00, -2.0943e-03,
         -4.4515e-02, -1.6309e+00, -1.6794e-03],
        [ 9.8592e-01,  3.9564e+00,  3.5376e+00, -2.5419e+00, -1.1885e+00,
         -2.7500e+00, -2.6475e+00,  5.9882e-01, -1.0693e+00,  1.4172e-01,
          7.7192e+00,  1.9260e+00,  2.3542e-01],
        [-2.3016e+00, -1.9959e+00,  5.9102e-07,  9.3182e-01,  1.2590e+01,
          1.4468e+00, -2.4106e+00, -1.9486e+00,  1.2788e+01, -7.4873e-02,
         -2.9963e+00, -1.8615e+00, -8.3155e-02]], device='cuda:0'))])
loaded xi:  19.928818
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 940.130933411658
W_T_median: 581.3306640883857
W_T_pctile_5: -358.9604697855882
W_T_CVAR_5_pct: -487.0834540584932
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1076.7235135304993
Current xi:  [33.9054]
objective value function right now is: -1076.7235135304993
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1119.681780671249
Current xi:  [47.169212]
objective value function right now is: -1119.681780671249
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1197.4062984103575
Current xi:  [60.16257]
objective value function right now is: -1197.4062984103575
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1212.868270255871
Current xi:  [72.10531]
objective value function right now is: -1212.868270255871
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1264.2391528950548
Current xi:  [83.58]
objective value function right now is: -1264.2391528950548
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1286.8407838699716
Current xi:  [94.24872]
objective value function right now is: -1286.8407838699716
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1309.4393407342034
Current xi:  [104.111404]
objective value function right now is: -1309.4393407342034
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1320.805610554549
Current xi:  [112.37691]
objective value function right now is: -1320.805610554549
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1329.3793202263837
Current xi:  [119.962585]
objective value function right now is: -1329.3793202263837
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [127.62439]
objective value function right now is: -1326.0392473638465
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1350.147916761836
Current xi:  [134.50803]
objective value function right now is: -1350.147916761836
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1365.2426175849614
Current xi:  [139.06665]
objective value function right now is: -1365.2426175849614
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [143.02794]
objective value function right now is: -1351.506607784986
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [146.18988]
objective value function right now is: -1330.5522911371197
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [147.63797]
objective value function right now is: -1362.8995361076447
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [149.17583]
objective value function right now is: -1357.685406602531
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [150.36047]
objective value function right now is: -1325.3450712397937
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [151.60127]
objective value function right now is: -1360.9223967237576
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [151.79636]
objective value function right now is: -1349.471377090008
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [151.17279]
objective value function right now is: -1356.4127709117788
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [152.96536]
objective value function right now is: -1362.2052141151892
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [152.81062]
objective value function right now is: -1357.0211023304898
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [153.91348]
objective value function right now is: -1350.7056737497091
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [153.22102]
objective value function right now is: -1359.0105860151273
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [152.3519]
objective value function right now is: -1361.8090961498629
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1366.2528969268558
Current xi:  [151.72475]
objective value function right now is: -1366.2528969268558
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [153.11922]
objective value function right now is: -1332.9729745904492
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [152.35698]
objective value function right now is: -1359.6522954505494
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [152.45552]
objective value function right now is: -1355.006591885776
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1371.558699791045
Current xi:  [153.17107]
objective value function right now is: -1371.558699791045
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [153.26099]
objective value function right now is: -1334.2057329091033
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1373.810068320544
Current xi:  [153.2649]
objective value function right now is: -1373.810068320544
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [154.15218]
objective value function right now is: -1342.850586360493
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [154.16243]
objective value function right now is: -1346.7418043054522
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [154.16183]
objective value function right now is: -1356.4986210179136
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1377.3903064775818
Current xi:  [154.48473]
objective value function right now is: -1377.3903064775818
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [154.59586]
objective value function right now is: -1374.6055889466488
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [154.71465]
objective value function right now is: -1377.1170262390501
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1378.2125324229096
Current xi:  [154.80489]
objective value function right now is: -1378.2125324229096
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [154.93402]
objective value function right now is: -1372.5116089539997
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [155.06223]
objective value function right now is: -1375.761795126256
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1378.2359410780439
Current xi:  [155.56586]
objective value function right now is: -1378.2359410780439
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1378.4503816474319
Current xi:  [155.77734]
objective value function right now is: -1378.4503816474319
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [156.06104]
objective value function right now is: -1377.2736991886595
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [156.10759]
objective value function right now is: -1370.6905497047392
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [156.33269]
objective value function right now is: -1377.5046754632594
94.0% of gradient descent iterations done. Method = Adam
new min fval:  -1379.5485779578394
Current xi:  [156.21455]
objective value function right now is: -1379.5485779578394
96.0% of gradient descent iterations done. Method = Adam
new min fval:  -1381.1688161687111
Current xi:  [156.26666]
objective value function right now is: -1381.1688161687111
new min fval from sgd:  -1382.3038489159555
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [156.2838]
objective value function right now is: -1382.3038489159555
new min fval from sgd:  -1382.3511979757654
new min fval from sgd:  -1382.4318121487038
new min fval from sgd:  -1382.509098786159
new min fval from sgd:  -1382.5730918996105
new min fval from sgd:  -1382.626843054663
new min fval from sgd:  -1382.6710763397687
new min fval from sgd:  -1382.6989033305501
new min fval from sgd:  -1382.7182254877523
new min fval from sgd:  -1382.751986326925
new min fval from sgd:  -1382.7660804964323
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [156.31831]
objective value function right now is: -1382.278297896189
min fval:  -1382.7660804964323
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -8.1661,   8.5550],
        [ -7.4035, -13.2515],
        [ -7.3400,  10.6628],
        [ -1.1305,   0.6098],
        [ -1.1305,   0.6098],
        [-52.3497,  -7.9832],
        [  1.0019, -14.3454],
        [-11.7172,   0.5988],
        [  8.1322,   1.3983],
        [ -1.1305,   0.6098],
        [ -1.1305,   0.6098],
        [ 12.7435,  -0.8574],
        [ 12.6819,  -0.8533]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  2.6263,   6.7442,   6.0733,  -3.6159,  -3.6159,  -5.7709,  -7.1711,
          9.5338,  -8.9544,  -3.6159,  -3.6159, -10.5003, -10.4752],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.6565e-01, -1.0590e+00, -5.4357e-01, -6.1805e-03, -6.1808e-03,
         -2.1833e-02, -2.8342e-01, -1.0108e+00, -7.5300e-02, -6.1809e-03,
         -6.1808e-03, -2.6377e-01, -2.6002e-01],
        [-2.4708e+00, -2.9236e+00, -1.0003e+01,  4.4795e-02,  4.4654e-02,
          9.6962e+00,  1.4342e+01, -3.3306e+00,  2.9599e+00,  4.4652e-02,
          4.4654e-02,  1.1264e+01,  1.0451e+01],
        [-5.1635e+00, -2.2737e+00, -1.3053e+01, -7.0635e-02, -7.0828e-02,
          8.0214e+00,  1.3632e+01, -1.1701e+00, -7.1962e-01, -7.0831e-02,
         -7.0828e-02, -1.9232e+00, -2.1002e+00],
        [ 5.5218e-01,  1.4482e+00,  1.5190e+00,  4.3484e-02,  4.3488e-02,
          9.9163e-02,  4.0504e-01,  1.9386e+00,  2.7311e-01,  4.3488e-02,
          4.3488e-02,  5.9869e-01,  5.9080e-01],
        [-2.8013e+00, -2.5183e+00, -1.0098e+01,  7.4598e-02,  7.4906e-02,
          1.0641e+01,  1.7179e+01, -2.2738e+00,  2.9669e+00,  7.4910e-02,
          7.4905e-02,  1.0725e+01,  9.6939e+00],
        [ 5.3741e-01,  1.3951e+00,  1.4632e+00,  4.2089e-02,  4.2093e-02,
          9.8955e-02,  3.9957e-01,  1.8663e+00,  2.5958e-01,  4.2093e-02,
          4.2093e-02,  5.6825e-01,  5.6078e-01],
        [-2.6565e-01, -1.0590e+00, -5.4357e-01, -6.1805e-03, -6.1808e-03,
         -2.1833e-02, -2.8342e-01, -1.0108e+00, -7.5300e-02, -6.1808e-03,
         -6.1808e-03, -2.6377e-01, -2.6002e-01],
        [-3.6227e+00, -3.0464e+00, -9.6651e+00, -1.4425e-02, -1.4593e-02,
          9.3405e+00,  1.5358e+01, -3.8192e+00,  2.5977e+00, -1.4596e-02,
         -1.4592e-02,  1.2518e+01,  1.1511e+01],
        [-2.6565e-01, -1.0590e+00, -5.4357e-01, -6.1805e-03, -6.1808e-03,
         -2.1833e-02, -2.8342e-01, -1.0108e+00, -7.5300e-02, -6.1809e-03,
         -6.1808e-03, -2.6377e-01, -2.6002e-01],
        [-2.6565e-01, -1.0590e+00, -5.4357e-01, -6.1805e-03, -6.1809e-03,
         -2.1833e-02, -2.8342e-01, -1.0108e+00, -7.5300e-02, -6.1809e-03,
         -6.1809e-03, -2.6377e-01, -2.6002e-01],
        [-2.6565e-01, -1.0590e+00, -5.4357e-01, -6.1805e-03, -6.1808e-03,
         -2.1833e-02, -2.8342e-01, -1.0108e+00, -7.5300e-02, -6.1808e-03,
         -6.1808e-03, -2.6377e-01, -2.6002e-01],
        [-2.6565e-01, -1.0590e+00, -5.4357e-01, -6.1805e-03, -6.1808e-03,
         -2.1833e-02, -2.8342e-01, -1.0108e+00, -7.5300e-02, -6.1809e-03,
         -6.1808e-03, -2.6377e-01, -2.6002e-01],
        [-2.6565e-01, -1.0590e+00, -5.4357e-01, -6.1805e-03, -6.1808e-03,
         -2.1833e-02, -2.8342e-01, -1.0108e+00, -7.5300e-02, -6.1809e-03,
         -6.1809e-03, -2.6377e-01, -2.6002e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.3346, -0.2400,  0.2941,  2.8112,  0.2316,  2.6949, -1.3346, -0.2693,
        -1.3346, -1.3346, -1.3346, -1.3346, -1.3346], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-9.6639e-03, -7.6292e+00, -5.9919e+00,  7.0923e+00, -1.0242e+01,
          5.3708e+00, -9.6639e-03, -1.1769e+01, -9.6639e-03, -9.6639e-03,
         -9.6638e-03, -9.6638e-03, -9.6639e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.9311, -13.8605],
        [  0.6122,  15.6463],
        [ -0.8779,   5.6713],
        [-11.4378,  -6.5940],
        [ 13.2381,  -0.7696],
        [  8.1734,  11.9570],
        [-11.7079,  -1.8380],
        [-11.9199,  -7.8448],
        [ -1.3866,   3.2613],
        [ -2.1991,  16.0018],
        [  0.1228,  18.2868],
        [  6.2480,   4.9978],
        [-10.5878,  -5.8231]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -8.8011,   9.2222,  -2.7548,  -2.7115, -12.7155,   6.4676,   4.0090,
         -4.6538,  -3.9381,   9.0066,  10.7913,  -7.4814,  -1.6525],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 3.7417e+00, -8.1792e-01,  1.5110e-01,  4.6621e+00, -8.7428e+00,
         -1.3961e+01,  3.3791e+00,  5.4698e+00,  2.8468e-01,  1.5243e+00,
         -1.0903e+00,  3.0398e-02,  3.4267e+00],
        [ 1.6803e+01, -1.4071e+01, -3.8391e-01,  3.5728e+00,  6.0006e+00,
         -9.1235e+00,  2.6639e+00, -1.9602e+00, -3.2784e-01, -4.7123e+00,
         -1.2498e+01,  3.4117e+00,  5.1338e+00],
        [-4.6661e+00,  6.0444e+00, -2.2898e+00,  1.6909e+00,  8.5601e+00,
         -2.0209e+00,  3.7952e-01,  1.3037e+00, -1.0753e+00,  2.2470e+00,
         -4.1003e+00, -1.5593e+00,  1.4603e+00],
        [-1.5421e+00,  1.5256e+01, -8.8288e-02, -1.4181e+01, -1.1943e+01,
          7.9668e+00, -1.8061e+00, -5.2900e+00, -1.2738e-01,  1.6945e+01,
          1.4767e+01, -9.7473e-01, -1.0382e+01],
        [ 9.0725e-02,  2.0003e+00, -1.0013e-02, -8.1549e+00,  1.6740e+00,
          3.4857e+01, -4.3097e-03, -7.3389e+00,  1.1439e-02, -2.1834e+00,
         -1.9803e-01,  2.3608e-04, -4.3034e+00],
        [-1.8791e+00,  1.1969e+01,  1.6443e-01, -7.6498e+00,  9.3342e+00,
          2.1806e+00, -7.3932e+00,  2.9039e+00,  1.6936e-01,  1.4771e+01,
          1.3966e+01,  1.0484e+00, -6.2018e+00],
        [ 3.6885e+00, -5.2322e-01, -9.3599e-03,  7.7462e-01, -4.1230e+00,
         -1.2692e+01, -7.5768e-01, -4.1538e+00,  1.0939e-02, -1.4605e-01,
         -1.2353e+00, -1.2163e-02,  1.7902e+00],
        [-2.0316e+00,  3.3186e+00, -1.5631e+00,  1.0884e+01, -4.6520e+00,
         -1.9202e+00,  3.5741e+00,  9.2522e+00, -1.2346e+00, -8.7596e-01,
         -6.5252e-01, -2.7738e+00,  8.6934e+00],
        [ 1.0627e-01,  2.1573e+00, -1.3502e-02, -8.2034e+00,  1.8237e+00,
          3.5824e+01, -1.5809e-01, -7.3249e+00,  1.3147e-02, -2.2758e+00,
         -1.1992e-01,  4.1491e-05, -4.5106e+00],
        [-8.8750e-01, -2.4643e+00,  7.8404e-02,  1.0519e+00,  4.1353e+00,
         -4.8562e+00, -7.1344e-01,  1.4402e+00,  7.0094e-02, -8.6555e-01,
         -2.4969e+00, -2.9427e-02,  1.3612e+00],
        [ 2.5296e+00, -1.7400e+01,  9.1741e-02,  1.1690e+01, -9.4850e+00,
         -1.0305e+01,  2.6972e+00,  1.0392e+01,  2.0156e-01, -8.2137e+00,
         -1.1487e+01, -1.2871e+00,  5.5495e+00],
        [ 8.5110e-01, -1.7352e+00,  3.0046e+00,  9.5040e-01,  2.7709e+00,
          9.0514e-01,  7.0816e-01,  1.0862e-01,  3.1094e+00,  6.4015e-01,
         -9.9888e-01,  7.4531e-01,  1.5859e+00],
        [-8.8749e-01, -2.4643e+00,  7.8406e-02,  1.0519e+00,  4.1353e+00,
         -4.8562e+00, -7.1345e-01,  1.4402e+00,  7.0095e-02, -8.6554e-01,
         -2.4969e+00, -2.9425e-02,  1.3612e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.5334,  4.2086, -7.5887, -3.5253, 10.1338, -2.5347, -2.4729, -3.0512,
        10.4091, -3.4553, -1.2980,  3.3527, -3.4553], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.4553e+00,  7.6684e-01,  1.3962e+00,  1.3279e+00,  5.9645e-01,
          1.1176e+00, -1.9728e+00,  4.7112e-01,  6.2491e-01, -2.4782e+00,
         -3.3982e+01, -4.2490e-01, -2.4782e+00],
        [-8.0365e-01,  4.5998e-01, -7.3285e-01, -3.7584e-01,  3.8458e-01,
          3.2183e-01,  2.6214e+00, -4.6198e-01,  6.7718e-01,  2.4784e+00,
          3.3946e+01,  1.3316e+00,  2.4784e+00],
        [ 1.4186e-02, -8.3187e-01, -4.7997e-02, -2.2800e+00, -2.7503e+00,
         -1.9254e+00, -1.3796e-02, -2.2450e-01, -2.7503e+00, -8.0810e-03,
          1.4461e-02, -2.6948e+00, -8.0810e-03],
        [ 2.2158e+00,  4.7102e+00, -3.3477e+00, -1.7572e+00, -2.7763e+00,
         -1.5351e+00,  4.0878e-01,  4.8705e-01, -2.6540e+00, -1.4817e+00,
          7.5225e+00,  3.5447e-01, -1.4817e+00],
        [-4.7474e+00, -2.7993e+00,  3.0703e-01, -4.8988e-09,  2.1191e+01,
          1.1268e-01, -3.6208e+00, -3.3228e+00,  2.2425e+01,  4.8940e-02,
         -3.6195e+00, -2.6882e+00,  4.8941e-02]], device='cuda:0'))])
xi:  [156.28247]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 654.2447084930102
W_T_median: 392.5187698078705
W_T_pctile_5: 156.51690617506887
W_T_CVAR_5_pct: -9.354273098469282
Average q (qsum/M+1):  47.62296811995968
Optimal xi:  [156.28247]
Expected(across Rb) median(across samples) p_equity:  0.16209536530077456
obj fun:  tensor(-1382.7661, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 10.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -7.2542,   7.3206],
        [ -6.7699, -10.5979],
        [ -5.2193,   8.6804],
        [  0.2241,   0.3177],
        [ -0.7246,   0.9976],
        [-52.2798,  -6.4820],
        [ -2.6707, -11.2124],
        [ -8.5838,   0.4254],
        [  6.5248,   8.9539],
        [ -0.7255,   0.9969],
        [ -0.7246,   0.9977],
        [  9.1326,  -0.8868],
        [  8.9232,  -1.4696]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 3.0600,  5.1975,  5.6464, -4.0477, -2.5143, -5.4062, -6.5253,  7.7716,
        -6.6490, -2.5147, -2.5143, -8.5705, -8.6983], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-4.2583e+00, -2.3487e+00, -9.1379e+00, -4.1893e-02, -5.7324e-02,
          1.1763e+01,  1.2797e+01, -2.3341e+00,  5.7757e+00, -5.9082e-02,
         -5.7144e-02,  6.3655e+00,  5.6082e+00],
        [-6.9528e+00, -2.6611e+00, -1.0702e+01, -1.3038e-01, -7.9669e-02,
          6.1040e+00,  1.1654e+01, -1.8717e+00,  3.0247e+00, -7.9543e-02,
         -7.9681e-02,  4.7476e+00,  4.0439e+00],
        [ 2.3317e-01,  1.6804e+00,  1.1144e+00,  2.7733e-02,  6.2818e-02,
          1.7155e-02,  4.5875e-01,  1.6943e+00,  4.4272e-01,  6.2998e-02,
          6.2799e-02,  4.2736e-01,  3.2275e-01],
        [-4.0942e+00, -2.3099e+00, -9.3733e+00,  1.0466e-02,  1.2226e-01,
          1.2791e+01,  1.3622e+01, -1.8156e+00,  6.1902e+00,  1.2472e-01,
          1.2201e-01,  6.4802e+00,  5.5247e+00],
        [ 2.2773e-01,  1.6243e+00,  1.0676e+00,  2.6215e-02,  6.0310e-02,
          1.6723e-02,  4.5316e-01,  1.6373e+00,  4.1183e-01,  6.0480e-02,
          6.0293e-02,  4.0009e-01,  3.0185e-01],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-5.3829e+00, -2.8298e+00, -9.4959e+00,  2.0391e-01, -4.5240e-02,
          9.0749e+00,  1.2055e+01, -2.8772e+00,  5.6057e+00, -4.5905e-02,
         -4.5172e-02,  8.2304e+00,  7.3503e+00],
        [-4.5205e+00, -2.6678e+00, -8.6396e+00, -4.4378e-02,  4.8922e-02,
          4.0271e+00,  1.0322e+01, -2.4150e+00,  2.2341e+00,  4.9162e-02,
          4.8897e-02,  3.9250e+00,  3.0669e+00],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-2.8199e-01, -2.2754e+00, -3.0344e+00, -4.0272e-03, -2.1728e-02,
          6.2891e-02,  3.0368e+00, -2.2460e+00,  5.0454e-01, -2.1810e-02,
         -2.1719e-02,  7.0139e-01,  4.5223e-01],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-3.0101e-02, -6.0257e-01, -3.4920e-01, -4.2911e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1737e-01, -6.6294e-02, -1.0337e-02,
         -1.0342e-02, -8.4352e-02, -6.9009e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8182, -0.0261,  0.0321,  2.4714,  0.0593,  2.3684, -0.8182, -0.3568,
        -0.2433, -0.8182, -0.8353, -0.8182, -0.8182], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.8009e-03, -6.9790e+00, -7.7505e+00,  7.8855e+00, -8.7395e+00,
          6.1127e+00,  1.8009e-03, -9.8697e+00, -4.9599e+00,  1.8009e-03,
         -1.0883e+00,  1.8009e-03,  1.8009e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.5627, -11.5538],
        [ -0.2054,  12.2057],
        [ -2.9373,   6.0430],
        [-10.6203,  -5.6749],
        [ 10.9489,   0.0949],
        [  5.0883,  10.4545],
        [ -7.3211,   0.0880],
        [-10.6611,  -5.9336],
        [ -1.5390,   0.8026],
        [ -5.0853,  12.2247],
        [ -7.5356,  14.9437],
        [  8.8835,   2.3574],
        [ -9.0614,  -4.6052]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-8.2035,  7.9696,  0.6017, -2.7780, -9.5798,  6.7590,  5.1073, -3.5817,
        -3.2062,  8.3475, 10.7078, -3.0235, -1.9507], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.5360e+00,  5.1902e+00, -8.8261e-01,  1.8825e-01, -1.1617e+01,
         -3.0109e+00,  7.3917e+00, -5.0429e-01,  2.7591e-02,  6.2055e+00,
          2.7854e+00, -1.0616e+00,  1.2582e+00],
        [ 1.0863e+01, -4.0755e+00, -3.4312e-01,  7.2742e-01, -1.0664e+01,
         -4.0139e+00,  9.5468e+00, -3.9590e+00,  1.2329e-02, -2.2796e+00,
         -1.5576e+01,  6.8790e-02,  3.1184e+00],
        [-1.4642e+01,  7.1294e+00, -4.1546e+00, -1.3205e-01,  1.2756e+01,
          3.2742e+00, -1.0937e+01, -8.2515e-02,  1.0508e-01, -7.1542e-01,
         -8.9964e+00, -3.1784e-01, -2.6675e-01],
        [-4.3054e+00,  1.3906e+01,  1.6532e-01, -8.5551e+00, -8.6290e+00,
          9.0489e+00, -1.2236e+00, -4.8719e+00, -1.1856e-01,  1.1045e+01,
          1.0371e+01,  1.6169e+00, -5.8449e+00],
        [-1.6620e+00,  2.2172e+00,  7.3730e-02, -7.4682e+00,  2.1070e+00,
          1.2570e+01,  2.5803e+00, -5.0797e+00,  4.1404e-02, -1.3499e-01,
         -1.4182e+00,  2.0066e+00, -3.8558e+00],
        [-2.7976e+00,  5.9804e+00,  2.5414e-01, -6.9438e+00,  5.2150e+00,
          8.2924e+00, -2.2936e+00, -2.8446e+00, -1.8938e-01,  7.2713e+00,
          8.7133e+00,  4.0492e+00, -3.9568e+00],
        [ 3.9558e+00, -4.9965e+00, -2.7392e-01,  9.7790e+00, -2.0234e+00,
         -4.3889e+00,  3.8306e+00,  6.0189e+00,  8.3592e-02, -1.1287e+01,
         -2.0694e+01, -4.3281e-01,  7.7430e+00],
        [-6.1261e-01,  7.4853e-01, -1.9733e+00,  1.8636e+00, -2.5340e-01,
         -8.6531e-01, -7.7204e-01,  1.0507e+00, -1.3834e+00,  1.6075e+00,
          1.6765e+00, -1.8126e+00,  2.0834e+00],
        [-1.7054e+00,  2.1658e+00,  7.0490e-02, -7.4993e+00,  2.1097e+00,
          1.2681e+01,  2.5081e+00, -4.9668e+00,  4.2617e-02, -2.1372e-01,
         -1.5385e+00,  1.9917e+00, -4.0639e+00],
        [-8.5966e-01, -5.4158e-01,  3.1900e-02, -1.0584e-01, -5.2516e-01,
         -1.5050e+00, -1.0865e+00, -7.5104e-02,  4.2592e-02, -2.2466e-01,
         -1.8496e-01, -1.6167e+00, -1.0209e-01],
        [ 2.8503e+00, -1.3851e+01, -3.2277e-01,  1.2349e+01, -6.6659e+00,
         -1.6797e+01,  2.0115e+00,  7.6731e+00,  4.9482e-02, -8.4937e+00,
         -8.1226e+00, -3.5145e+00,  7.0066e+00],
        [ 1.6826e+00,  5.2625e-01,  4.1788e-01,  1.3952e-01,  4.3639e-01,
          1.4718e+00,  1.7609e+00,  7.2000e-02,  4.9240e-02,  2.2221e-01,
          3.2291e-01,  1.5640e+00,  2.5923e-01],
        [-8.8093e-01, -4.3473e-01, -1.6270e-01, -3.4271e-02, -5.6408e-01,
         -1.4559e+00, -1.1035e+00, -1.5385e-01,  2.5814e-02, -3.4916e-02,
         -1.0794e-01, -1.7583e+00,  2.8486e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 1.3890,  2.7192, -2.4559, -2.6275,  4.8735,  0.2970,  2.1879, -1.9531,
         5.0658, -2.4521, -0.5826,  3.1194, -2.4773], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.6213e-01, -6.8777e-01, -6.5505e-01,  1.3206e+00,  7.1399e-01,
          1.2783e+00,  1.1355e+00,  4.2453e-01,  7.4292e-01, -1.6221e-01,
         -9.0400e+00, -3.4519e-01, -1.4551e-01],
        [ 5.6044e-01,  1.9149e+00,  1.3483e+00, -3.6948e-01,  2.6564e-01,
          1.6115e-01, -4.7250e-01, -4.1914e-01,  5.5808e-01,  1.6534e-01,
          8.9809e+00,  1.2504e+00,  1.4926e-01],
        [-8.2717e-01, -4.8577e-01, -6.9095e-01, -1.3295e+00, -1.5980e+00,
         -1.6062e+00, -3.5694e-01, -1.2789e-01, -1.5980e+00, -2.0943e-03,
         -4.4515e-02, -1.6309e+00, -1.6794e-03],
        [ 9.8592e-01,  3.9564e+00,  3.5376e+00, -2.5419e+00, -1.1885e+00,
         -2.7500e+00, -2.6475e+00,  5.9882e-01, -1.0693e+00,  1.4172e-01,
          7.7192e+00,  1.9260e+00,  2.3542e-01],
        [-2.3016e+00, -1.9959e+00,  5.9102e-07,  9.3182e-01,  1.2590e+01,
          1.4468e+00, -2.4106e+00, -1.9486e+00,  1.2788e+01, -7.4873e-02,
         -2.9963e+00, -1.8615e+00, -8.3155e-02]], device='cuda:0'))])
loaded xi:  19.928818
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 940.130933411658
W_T_median: 581.3306640883857
W_T_pctile_5: -358.9604697855882
W_T_CVAR_5_pct: -487.0834540584932
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -436.36267108806385
Current xi:  [35.308292]
objective value function right now is: -436.36267108806385
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -558.4090696431531
Current xi:  [49.666256]
objective value function right now is: -558.4090696431531
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -753.9367099954826
Current xi:  [63.224506]
objective value function right now is: -753.9367099954826
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -850.9318915149157
Current xi:  [75.5491]
objective value function right now is: -850.9318915149157
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -891.259043579351
Current xi:  [87.28889]
objective value function right now is: -891.259043579351
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -908.3709922393543
Current xi:  [98.25513]
objective value function right now is: -908.3709922393543
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1035.0690701217507
Current xi:  [108.24942]
objective value function right now is: -1035.0690701217507
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1101.5315950398738
Current xi:  [118.252144]
objective value function right now is: -1101.5315950398738
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1163.259395943286
Current xi:  [127.23662]
objective value function right now is: -1163.259395943286
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1183.5955355104388
Current xi:  [134.71042]
objective value function right now is: -1183.5955355104388
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1194.747306258802
Current xi:  [141.67627]
objective value function right now is: -1194.747306258802
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1195.963548482684
Current xi:  [147.78627]
objective value function right now is: -1195.963548482684
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [151.4167]
objective value function right now is: -1173.4339113756344
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1200.659540994166
Current xi:  [154.44757]
objective value function right now is: -1200.659540994166
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1205.2229206475947
Current xi:  [157.8373]
objective value function right now is: -1205.2229206475947
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1227.3549009015408
Current xi:  [159.06358]
objective value function right now is: -1227.3549009015408
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [159.29065]
objective value function right now is: -1185.4142913691103
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [159.45157]
objective value function right now is: -1200.1799275156666
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [160.84477]
objective value function right now is: -1152.3135538440151
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.5874]
objective value function right now is: -1148.6305688153252
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.13489]
objective value function right now is: -1224.1429461312011
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [160.31473]
objective value function right now is: -1203.2883975076313
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [160.59785]
objective value function right now is: -1213.722172330692
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.30238]
objective value function right now is: -1224.7992864298592
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.55667]
objective value function right now is: -1186.850578940709
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.37512]
objective value function right now is: -1216.3097157779011
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.73763]
objective value function right now is: -1179.2371884213844
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [163.83928]
objective value function right now is: -1207.741566443245
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [162.59843]
objective value function right now is: -1192.555527613543
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.9687]
objective value function right now is: -1212.1191000420722
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.32701]
objective value function right now is: -1186.0865008934095
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.72208]
objective value function right now is: -1130.6552283563155
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.03572]
objective value function right now is: -1168.081620867284
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1230.7845441125548
Current xi:  [163.93884]
objective value function right now is: -1230.7845441125548
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.55121]
objective value function right now is: -1215.614538756309
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1249.035690053654
Current xi:  [164.29587]
objective value function right now is: -1249.035690053654
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1251.4421059199135
Current xi:  [164.27826]
objective value function right now is: -1251.4421059199135
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.45981]
objective value function right now is: -1231.5829331419025
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.20482]
objective value function right now is: -1227.1530482167955
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.55478]
objective value function right now is: -1246.37282109917
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.44691]
objective value function right now is: -1244.4354653772996
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.25221]
objective value function right now is: -1224.6804037411507
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.40858]
objective value function right now is: -1247.4529533966115
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1255.1705831830675
Current xi:  [164.268]
objective value function right now is: -1255.1705831830675
90.0% of gradient descent iterations done. Method = Adam
new min fval:  -1255.218168483643
Current xi:  [164.30894]
objective value function right now is: -1255.218168483643
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.50827]
objective value function right now is: -1238.6628222969787
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.7024]
objective value function right now is: -1248.563137829725
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.77615]
objective value function right now is: -1253.7227267770897
new min fval from sgd:  -1259.8558894280072
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.02795]
objective value function right now is: -1259.8558894280072
new min fval from sgd:  -1259.896644908208
new min fval from sgd:  -1259.9397970247962
new min fval from sgd:  -1260.0109796441843
new min fval from sgd:  -1260.0886693345292
new min fval from sgd:  -1260.1583404830164
new min fval from sgd:  -1260.2137469154177
new min fval from sgd:  -1260.2885994645503
new min fval from sgd:  -1260.3640835584013
new min fval from sgd:  -1260.4475850629756
new min fval from sgd:  -1260.513949210986
new min fval from sgd:  -1260.5685602250012
new min fval from sgd:  -1260.6121269682933
new min fval from sgd:  -1260.67262418565
new min fval from sgd:  -1260.7459749582817
new min fval from sgd:  -1260.7696228127988
new min fval from sgd:  -1260.8116769946157
new min fval from sgd:  -1260.853478952832
new min fval from sgd:  -1260.883013461886
new min fval from sgd:  -1260.9040809682808
new min fval from sgd:  -1260.9609174316968
new min fval from sgd:  -1261.1047487116227
new min fval from sgd:  -1261.1905142244793
new min fval from sgd:  -1261.2720754447687
new min fval from sgd:  -1261.3919148469327
new min fval from sgd:  -1261.4875816333833
new min fval from sgd:  -1261.549994891956
new min fval from sgd:  -1261.604110524687
new min fval from sgd:  -1261.6295386684506
new min fval from sgd:  -1261.6669087788864
new min fval from sgd:  -1261.687458435898
new min fval from sgd:  -1261.7014926692427
new min fval from sgd:  -1261.7056454532458
new min fval from sgd:  -1261.7272792421643
new min fval from sgd:  -1261.740232419239
new min fval from sgd:  -1261.7491274382364
new min fval from sgd:  -1261.764331197299
new min fval from sgd:  -1261.7687820687786
new min fval from sgd:  -1261.7885578952335
new min fval from sgd:  -1261.7923497224465
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.99928]
objective value function right now is: -1261.766404474657
min fval:  -1261.7923497224465
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -7.3538,   9.3916],
        [ -7.8570, -18.7216],
        [ -7.9832,  10.7893],
        [ -1.2091,   0.8181],
        [ -1.2091,   0.8180],
        [-46.3828,  -8.2445],
        [  2.4970, -14.5450],
        [-11.8831,   0.7183],
        [  8.2781,   0.4372],
        [ -1.2091,   0.8180],
        [ -1.2091,   0.8180],
        [ 12.8086,  -0.8078],
        [ 12.7429,  -0.8109]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  3.0282,   6.4977,   6.0090,  -3.9696,  -3.9697,  -5.8553,  -7.2857,
          9.1088,  -8.9783,  -3.9697,  -3.9697, -10.3163, -10.2773],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.8588e-01, -9.0952e-01, -8.9407e-01, -3.4565e-03, -3.4565e-03,
         -1.4422e-02, -3.0931e-01, -1.0680e+00, -1.0611e-01, -3.4565e-03,
         -3.4565e-03, -3.9994e-01, -3.9758e-01],
        [-5.0476e+00, -3.7833e+00, -8.7435e+00,  2.3930e-02,  2.3910e-02,
          9.9428e+00,  1.5307e+01, -3.4548e+00,  3.3953e+00,  2.3910e-02,
          2.3910e-02,  1.1665e+01,  1.0768e+01],
        [-7.6029e+00, -2.2020e+00, -1.4464e+01, -9.4023e-02, -9.4122e-02,
          8.9372e+00,  1.3906e+01, -8.8252e-01, -2.2456e+00, -9.4122e-02,
         -9.4122e-02, -3.3831e+00, -3.5977e+00],
        [ 7.8120e-01,  1.5812e+00,  1.6795e+00,  4.0457e-02,  4.0457e-02,
          1.1358e-01,  4.8080e-01,  1.9505e+00,  1.5727e-01,  4.0457e-02,
          4.0457e-02,  6.4679e-01,  6.4332e-01],
        [-4.9238e+00, -2.9609e+00, -1.0246e+01,  7.2360e-03,  7.4003e-03,
          8.0650e+00,  1.7517e+01, -1.4170e+00,  3.9083e+00,  7.4011e-03,
          7.4003e-03,  1.0973e+01,  9.8370e+00],
        [ 7.5845e-01,  1.5248e+00,  1.6166e+00,  3.8815e-02,  3.8815e-02,
          1.1373e-01,  4.7209e-01,  1.8799e+00,  1.5209e-01,  3.8815e-02,
          3.8815e-02,  6.1722e-01,  6.1394e-01],
        [-3.8588e-01, -9.0952e-01, -8.9407e-01, -3.4565e-03, -3.4565e-03,
         -1.4422e-02, -3.0931e-01, -1.0680e+00, -1.0611e-01, -3.4565e-03,
         -3.4565e-03, -3.9994e-01, -3.9758e-01],
        [-6.4968e+00, -4.2055e+00, -8.3658e+00,  2.8901e-02,  2.8801e-02,
          9.6427e+00,  1.6389e+01, -3.6686e+00,  3.1373e+00,  2.8800e-02,
          2.8801e-02,  1.2987e+01,  1.1842e+01],
        [-3.8584e-01, -9.0949e-01, -8.9399e-01, -3.4560e-03, -3.4560e-03,
         -1.4416e-02, -3.0931e-01, -1.0678e+00, -1.0609e-01, -3.4560e-03,
         -3.4560e-03, -3.9989e-01, -3.9754e-01],
        [-3.8588e-01, -9.0952e-01, -8.9407e-01, -3.4565e-03, -3.4565e-03,
         -1.4422e-02, -3.0931e-01, -1.0680e+00, -1.0611e-01, -3.4565e-03,
         -3.4565e-03, -3.9994e-01, -3.9758e-01],
        [-3.8588e-01, -9.0952e-01, -8.9407e-01, -3.4565e-03, -3.4565e-03,
         -1.4422e-02, -3.0931e-01, -1.0680e+00, -1.0611e-01, -3.4565e-03,
         -3.4565e-03, -3.9994e-01, -3.9758e-01],
        [-3.8588e-01, -9.0952e-01, -8.9407e-01, -3.4565e-03, -3.4565e-03,
         -1.4422e-02, -3.0931e-01, -1.0680e+00, -1.0611e-01, -3.4565e-03,
         -3.4565e-03, -3.9994e-01, -3.9758e-01],
        [-3.8588e-01, -9.0952e-01, -8.9407e-01, -3.4565e-03, -3.4565e-03,
         -1.4422e-02, -3.0931e-01, -1.0680e+00, -1.0611e-01, -3.4565e-03,
         -3.4565e-03, -3.9994e-01, -3.9758e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.5359, -0.0065,  0.3981,  2.9644,  1.0259,  2.8455, -1.5359,  0.0636,
        -1.5366, -1.5359, -1.5359, -1.5359, -1.5359], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0173,  -8.2796,  -7.4588,   7.0504, -10.0460,   5.3176,   0.0173,
         -13.2438,   0.0173,   0.0173,   0.0173,   0.0173,   0.0173]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -7.0557, -12.8231],
        [  3.0245,  15.4259],
        [  0.7493,  10.8097],
        [-12.2805,  -6.9224],
        [ 13.1425,  -0.8568],
        [  6.7963,  12.5275],
        [-11.3087,  -1.7568],
        [-11.6962,  -7.6344],
        [ -1.6685,   3.2559],
        [ -4.7587,  15.3751],
        [  2.3203,  18.9103],
        [  6.8891,   6.4289],
        [-10.7959,  -6.1252]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -8.6426,   8.6676,   1.3183,  -1.8896, -12.7462,   6.4398,   4.2215,
         -4.5194,  -4.3251,  10.0946,  10.6980,  -7.0746,  -2.6220],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-5.2021e-01,  4.6218e+00,  1.2528e-01,  1.1485e+01, -1.8470e+01,
         -6.0680e+00,  4.1943e+00,  1.4655e+01, -7.5461e-01,  1.2580e+00,
          2.7886e+00, -1.0604e+01,  1.0194e+01],
        [ 1.4685e+01, -1.2240e+01, -1.1647e+00,  2.6717e+00, -3.1499e+00,
         -6.8399e+00,  4.4125e+00, -1.5866e+00, -2.3238e-01, -2.9962e+00,
         -1.6017e+01,  1.0960e+00,  2.9401e+00],
        [-4.1633e+00,  1.7741e+00, -1.3602e-01,  4.0367e-01,  1.0677e+01,
         -1.1729e+00, -8.6425e+00,  7.0645e-01,  1.1184e-01, -1.5991e+00,
         -8.7394e+00, -5.2443e-01,  5.5563e-02],
        [-9.1085e+00,  1.1829e+01,  9.3989e-01, -1.0542e+01, -1.1456e+01,
          5.5569e+00, -5.3847e+00, -2.0343e+00, -2.0082e-01,  1.9402e+01,
          1.2677e+01,  5.8394e-02, -5.6783e+00],
        [-1.7662e+00,  7.8429e+00,  1.8339e-01, -7.7702e+00,  6.5321e+00,
          4.0865e+01,  2.2633e+00, -7.8426e+00,  1.6807e-01, -5.2720e+00,
          5.1167e+00,  9.1693e-03, -4.7572e+00],
        [-4.2409e+00,  1.6249e+01,  1.1115e+00, -1.3592e+01,  1.3316e+01,
          1.0039e+01, -1.4646e+00, -8.3358e+00,  1.0734e-01,  1.0667e+01,
          1.5377e+01,  1.4594e+00, -8.7248e+00],
        [ 7.0802e-01, -1.6962e-01, -5.0153e-02,  5.1200e+00,  5.3420e+00,
         -1.1856e+01, -1.3240e+01,  3.1928e+00, -3.7920e-02, -8.5465e-02,
         -6.6575e-02, -7.5451e-01,  2.0297e+00],
        [-4.8275e+00,  1.4020e+00,  9.4968e-01,  4.2056e+00, -2.4764e-01,
         -4.6318e+00, -3.5557e+00,  6.4193e+00, -4.3032e+00,  4.8252e+00,
          3.5903e+00, -1.8248e+00,  4.7580e+00],
        [-1.7180e+00,  8.0650e+00,  1.8801e-01, -7.7760e+00,  6.7091e+00,
          4.1449e+01,  2.0232e+00, -7.6901e+00,  1.7233e-01, -5.2139e+00,
          5.2511e+00,  9.3892e-03, -4.9471e+00],
        [-1.3089e+00, -5.5597e-02,  5.6393e-01, -5.0587e-02, -1.2073e+00,
         -2.3967e+00, -9.7506e-01,  4.8402e-02,  9.1337e-01,  2.6415e-01,
         -2.5198e-02,  5.0359e-01,  1.4141e-02],
        [ 2.4713e+00, -1.6842e+01, -1.4993e+00,  1.3333e+01, -9.8830e+00,
         -1.7508e+01,  3.0741e+00,  1.0324e+01,  2.6947e-02, -8.9754e+00,
         -1.3472e+01, -3.9708e+00,  7.6725e+00],
        [-1.3608e+00, -1.2019e+00, -7.8001e-01,  3.9159e+00,  1.3496e+01,
          3.5200e-01,  1.7806e+00,  2.2707e+00,  5.8475e+00,  2.0210e+00,
         -1.4972e+00,  3.8003e+00,  3.3548e+00],
        [-1.3089e+00, -5.5597e-02,  5.6393e-01, -5.0588e-02, -1.2073e+00,
         -2.3967e+00, -9.7506e-01,  4.8402e-02,  9.1336e-01,  2.6415e-01,
         -2.5199e-02,  5.0359e-01,  1.4141e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.7827,  5.1608, -6.5293, -7.8421, 11.2279, -2.6287, -4.7309, -6.2513,
        11.4301, -6.4994, -2.3509,  2.4216, -6.4994], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 6.6878e-01,  4.2208e-01, -1.5344e+00,  7.4554e-01,  3.7473e-01,
          2.2849e+00, -1.4768e+00,  1.1081e+00,  4.0358e-01, -2.9750e-01,
         -3.1006e+01, -5.6229e-01, -2.9750e-01],
        [ 5.2035e-02,  8.0257e-01,  2.2196e+00,  2.0012e-01,  6.0074e-01,
         -8.4922e-01,  2.1322e+00, -1.1029e+00,  8.9315e-01,  2.9685e-01,
          3.0836e+01,  1.4634e+00,  2.9685e-01],
        [ 2.4055e+00,  1.8592e+00,  1.1669e+00, -4.1737e+00, -1.0641e+00,
         -3.9185e+00,  1.7793e-02,  1.3884e+00, -1.0641e+00,  5.9773e-01,
          1.9826e+00, -9.1428e-01,  5.9773e-01],
        [ 7.6493e-01,  2.2939e+00,  3.2840e-01, -8.7703e+00, -1.8606e+00,
         -1.2107e+00, -2.2800e+00,  7.4310e-01, -1.7407e+00,  2.6639e-01,
          9.0588e+00,  1.2074e+00,  2.6639e-01],
        [-6.4722e+00, -3.6305e+00, -1.7506e-02,  1.1338e-09,  2.5637e+01,
          5.5043e-09, -2.6138e-01, -4.0842e+00,  2.6463e+01,  9.8416e-03,
         -4.5472e+00, -3.5013e+00,  9.8415e-03]], device='cuda:0'))])
xi:  [165.00023]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 682.4933716529026
W_T_median: 437.04206624622225
W_T_pctile_5: 165.22165678221353
W_T_CVAR_5_pct: -7.627882503502821
Average q (qsum/M+1):  46.85468907510081
Optimal xi:  [165.00023]
Expected(across Rb) median(across samples) p_equity:  0.14130072109401226
obj fun:  tensor(-1261.7923, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 25.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -7.2542,   7.3206],
        [ -6.7699, -10.5979],
        [ -5.2193,   8.6804],
        [  0.2241,   0.3177],
        [ -0.7246,   0.9976],
        [-52.2798,  -6.4820],
        [ -2.6707, -11.2124],
        [ -8.5838,   0.4254],
        [  6.5248,   8.9539],
        [ -0.7255,   0.9969],
        [ -0.7246,   0.9977],
        [  9.1326,  -0.8868],
        [  8.9232,  -1.4696]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 3.0600,  5.1975,  5.6464, -4.0477, -2.5143, -5.4062, -6.5253,  7.7716,
        -6.6490, -2.5147, -2.5143, -8.5705, -8.6983], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-4.2583e+00, -2.3487e+00, -9.1379e+00, -4.1893e-02, -5.7324e-02,
          1.1763e+01,  1.2797e+01, -2.3341e+00,  5.7757e+00, -5.9082e-02,
         -5.7144e-02,  6.3655e+00,  5.6082e+00],
        [-6.9528e+00, -2.6611e+00, -1.0702e+01, -1.3038e-01, -7.9669e-02,
          6.1040e+00,  1.1654e+01, -1.8717e+00,  3.0247e+00, -7.9543e-02,
         -7.9681e-02,  4.7476e+00,  4.0439e+00],
        [ 2.3317e-01,  1.6804e+00,  1.1144e+00,  2.7733e-02,  6.2818e-02,
          1.7155e-02,  4.5875e-01,  1.6943e+00,  4.4272e-01,  6.2998e-02,
          6.2799e-02,  4.2736e-01,  3.2275e-01],
        [-4.0942e+00, -2.3099e+00, -9.3733e+00,  1.0466e-02,  1.2226e-01,
          1.2791e+01,  1.3622e+01, -1.8156e+00,  6.1902e+00,  1.2472e-01,
          1.2201e-01,  6.4802e+00,  5.5247e+00],
        [ 2.2773e-01,  1.6243e+00,  1.0676e+00,  2.6215e-02,  6.0310e-02,
          1.6723e-02,  4.5316e-01,  1.6373e+00,  4.1183e-01,  6.0480e-02,
          6.0293e-02,  4.0009e-01,  3.0185e-01],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-5.3829e+00, -2.8298e+00, -9.4959e+00,  2.0391e-01, -4.5240e-02,
          9.0749e+00,  1.2055e+01, -2.8772e+00,  5.6057e+00, -4.5905e-02,
         -4.5172e-02,  8.2304e+00,  7.3503e+00],
        [-4.5205e+00, -2.6678e+00, -8.6396e+00, -4.4378e-02,  4.8922e-02,
          4.0271e+00,  1.0322e+01, -2.4150e+00,  2.2341e+00,  4.9162e-02,
          4.8897e-02,  3.9250e+00,  3.0669e+00],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-2.8199e-01, -2.2754e+00, -3.0344e+00, -4.0272e-03, -2.1728e-02,
          6.2891e-02,  3.0368e+00, -2.2460e+00,  5.0454e-01, -2.1810e-02,
         -2.1719e-02,  7.0139e-01,  4.5223e-01],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-3.0101e-02, -6.0257e-01, -3.4920e-01, -4.2911e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1737e-01, -6.6294e-02, -1.0337e-02,
         -1.0342e-02, -8.4352e-02, -6.9009e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8182, -0.0261,  0.0321,  2.4714,  0.0593,  2.3684, -0.8182, -0.3568,
        -0.2433, -0.8182, -0.8353, -0.8182, -0.8182], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.8009e-03, -6.9790e+00, -7.7505e+00,  7.8855e+00, -8.7395e+00,
          6.1127e+00,  1.8009e-03, -9.8697e+00, -4.9599e+00,  1.8009e-03,
         -1.0883e+00,  1.8009e-03,  1.8009e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.5627, -11.5538],
        [ -0.2054,  12.2057],
        [ -2.9373,   6.0430],
        [-10.6203,  -5.6749],
        [ 10.9489,   0.0949],
        [  5.0883,  10.4545],
        [ -7.3211,   0.0880],
        [-10.6611,  -5.9336],
        [ -1.5390,   0.8026],
        [ -5.0853,  12.2247],
        [ -7.5356,  14.9437],
        [  8.8835,   2.3574],
        [ -9.0614,  -4.6052]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-8.2035,  7.9696,  0.6017, -2.7780, -9.5798,  6.7590,  5.1073, -3.5817,
        -3.2062,  8.3475, 10.7078, -3.0235, -1.9507], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.5360e+00,  5.1902e+00, -8.8261e-01,  1.8825e-01, -1.1617e+01,
         -3.0109e+00,  7.3917e+00, -5.0429e-01,  2.7591e-02,  6.2055e+00,
          2.7854e+00, -1.0616e+00,  1.2582e+00],
        [ 1.0863e+01, -4.0755e+00, -3.4312e-01,  7.2742e-01, -1.0664e+01,
         -4.0139e+00,  9.5468e+00, -3.9590e+00,  1.2329e-02, -2.2796e+00,
         -1.5576e+01,  6.8790e-02,  3.1184e+00],
        [-1.4642e+01,  7.1294e+00, -4.1546e+00, -1.3205e-01,  1.2756e+01,
          3.2742e+00, -1.0937e+01, -8.2515e-02,  1.0508e-01, -7.1542e-01,
         -8.9964e+00, -3.1784e-01, -2.6675e-01],
        [-4.3054e+00,  1.3906e+01,  1.6532e-01, -8.5551e+00, -8.6290e+00,
          9.0489e+00, -1.2236e+00, -4.8719e+00, -1.1856e-01,  1.1045e+01,
          1.0371e+01,  1.6169e+00, -5.8449e+00],
        [-1.6620e+00,  2.2172e+00,  7.3730e-02, -7.4682e+00,  2.1070e+00,
          1.2570e+01,  2.5803e+00, -5.0797e+00,  4.1404e-02, -1.3499e-01,
         -1.4182e+00,  2.0066e+00, -3.8558e+00],
        [-2.7976e+00,  5.9804e+00,  2.5414e-01, -6.9438e+00,  5.2150e+00,
          8.2924e+00, -2.2936e+00, -2.8446e+00, -1.8938e-01,  7.2713e+00,
          8.7133e+00,  4.0492e+00, -3.9568e+00],
        [ 3.9558e+00, -4.9965e+00, -2.7392e-01,  9.7790e+00, -2.0234e+00,
         -4.3889e+00,  3.8306e+00,  6.0189e+00,  8.3592e-02, -1.1287e+01,
         -2.0694e+01, -4.3281e-01,  7.7430e+00],
        [-6.1261e-01,  7.4853e-01, -1.9733e+00,  1.8636e+00, -2.5340e-01,
         -8.6531e-01, -7.7204e-01,  1.0507e+00, -1.3834e+00,  1.6075e+00,
          1.6765e+00, -1.8126e+00,  2.0834e+00],
        [-1.7054e+00,  2.1658e+00,  7.0490e-02, -7.4993e+00,  2.1097e+00,
          1.2681e+01,  2.5081e+00, -4.9668e+00,  4.2617e-02, -2.1372e-01,
         -1.5385e+00,  1.9917e+00, -4.0639e+00],
        [-8.5966e-01, -5.4158e-01,  3.1900e-02, -1.0584e-01, -5.2516e-01,
         -1.5050e+00, -1.0865e+00, -7.5104e-02,  4.2592e-02, -2.2466e-01,
         -1.8496e-01, -1.6167e+00, -1.0209e-01],
        [ 2.8503e+00, -1.3851e+01, -3.2277e-01,  1.2349e+01, -6.6659e+00,
         -1.6797e+01,  2.0115e+00,  7.6731e+00,  4.9482e-02, -8.4937e+00,
         -8.1226e+00, -3.5145e+00,  7.0066e+00],
        [ 1.6826e+00,  5.2625e-01,  4.1788e-01,  1.3952e-01,  4.3639e-01,
          1.4718e+00,  1.7609e+00,  7.2000e-02,  4.9240e-02,  2.2221e-01,
          3.2291e-01,  1.5640e+00,  2.5923e-01],
        [-8.8093e-01, -4.3473e-01, -1.6270e-01, -3.4271e-02, -5.6408e-01,
         -1.4559e+00, -1.1035e+00, -1.5385e-01,  2.5814e-02, -3.4916e-02,
         -1.0794e-01, -1.7583e+00,  2.8486e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 1.3890,  2.7192, -2.4559, -2.6275,  4.8735,  0.2970,  2.1879, -1.9531,
         5.0658, -2.4521, -0.5826,  3.1194, -2.4773], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.6213e-01, -6.8777e-01, -6.5505e-01,  1.3206e+00,  7.1399e-01,
          1.2783e+00,  1.1355e+00,  4.2453e-01,  7.4292e-01, -1.6221e-01,
         -9.0400e+00, -3.4519e-01, -1.4551e-01],
        [ 5.6044e-01,  1.9149e+00,  1.3483e+00, -3.6948e-01,  2.6564e-01,
          1.6115e-01, -4.7250e-01, -4.1914e-01,  5.5808e-01,  1.6534e-01,
          8.9809e+00,  1.2504e+00,  1.4926e-01],
        [-8.2717e-01, -4.8577e-01, -6.9095e-01, -1.3295e+00, -1.5980e+00,
         -1.6062e+00, -3.5694e-01, -1.2789e-01, -1.5980e+00, -2.0943e-03,
         -4.4515e-02, -1.6309e+00, -1.6794e-03],
        [ 9.8592e-01,  3.9564e+00,  3.5376e+00, -2.5419e+00, -1.1885e+00,
         -2.7500e+00, -2.6475e+00,  5.9882e-01, -1.0693e+00,  1.4172e-01,
          7.7192e+00,  1.9260e+00,  2.3542e-01],
        [-2.3016e+00, -1.9959e+00,  5.9102e-07,  9.3182e-01,  1.2590e+01,
          1.4468e+00, -2.4106e+00, -1.9486e+00,  1.2788e+01, -7.4873e-02,
         -2.9963e+00, -1.8615e+00, -8.3155e-02]], device='cuda:0'))])
loaded xi:  19.928818
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 940.130933411658
W_T_median: 581.3306640883857
W_T_pctile_5: -358.9604697855882
W_T_CVAR_5_pct: -487.0834540584932
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  699.1469214339105
Current xi:  [34.841076]
objective value function right now is: 699.1469214339105
4.0% of gradient descent iterations done. Method = Adam
new min fval:  289.6040522569363
Current xi:  [49.800106]
objective value function right now is: 289.6040522569363
6.0% of gradient descent iterations done. Method = Adam
new min fval:  203.06650527304873
Current xi:  [63.1412]
objective value function right now is: 203.06650527304873
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -160.47610386148864
Current xi:  [76.26321]
objective value function right now is: -160.47610386148864
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -306.11609908084245
Current xi:  [88.44176]
objective value function right now is: -306.11609908084245
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -467.10062567280795
Current xi:  [100.07189]
objective value function right now is: -467.10062567280795
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -621.370717272304
Current xi:  [110.32978]
objective value function right now is: -621.370717272304
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -714.2583470703911
Current xi:  [119.111824]
objective value function right now is: -714.2583470703911
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -756.178775278762
Current xi:  [128.33258]
objective value function right now is: -756.178775278762
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -840.7874735307538
Current xi:  [135.99226]
objective value function right now is: -840.7874735307538
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [142.13104]
objective value function right now is: -799.4532920837331
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -935.2133254697561
Current xi:  [148.37698]
objective value function right now is: -935.2133254697561
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [152.42209]
objective value function right now is: -903.3190722881117
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -974.3305422134808
Current xi:  [156.96358]
objective value function right now is: -974.3305422134808
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [158.79448]
objective value function right now is: -934.7846316951708
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [159.7865]
objective value function right now is: -919.1857198601293
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1002.4140429974809
Current xi:  [161.71521]
objective value function right now is: -1002.4140429974809
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.6948]
objective value function right now is: -986.9971047520279
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.25012]
objective value function right now is: -942.6475031612881
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1009.5275836688153
Current xi:  [164.4735]
objective value function right now is: -1009.5275836688153
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.11092]
objective value function right now is: -905.5633688736672
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.38663]
objective value function right now is: -843.2353034814262
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.22438]
objective value function right now is: -960.7579030369984
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.69373]
objective value function right now is: -850.3035434373286
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.47156]
objective value function right now is: -1000.0801409651916
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1017.870651766215
Current xi:  [165.58205]
objective value function right now is: -1017.870651766215
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.68016]
objective value function right now is: -986.3499075061875
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [166.75735]
objective value function right now is: -947.5365305162611
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [167.25871]
objective value function right now is: -1015.4344077772367
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.162]
objective value function right now is: -1006.9457011801692
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.24086]
objective value function right now is: -986.3877453022602
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.36845]
objective value function right now is: -972.4331950308008
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.71638]
objective value function right now is: -922.5643598766185
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.55696]
objective value function right now is: -920.2433838980345
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.19571]
objective value function right now is: -866.382704451096
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1043.596691239388
Current xi:  [167.10843]
objective value function right now is: -1043.596691239388
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1051.155125153611
Current xi:  [167.3528]
objective value function right now is: -1051.155125153611
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.38408]
objective value function right now is: -1043.505302884689
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.24174]
objective value function right now is: -1049.5548324935794
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.16895]
objective value function right now is: -1040.7191813375562
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1052.93337561495
Current xi:  [167.20338]
objective value function right now is: -1052.93337561495
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1055.425845529858
Current xi:  [166.95416]
objective value function right now is: -1055.425845529858
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.1979]
objective value function right now is: -1054.307707269943
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.33907]
objective value function right now is: -1028.0896432469324
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.07645]
objective value function right now is: -1049.0183613363965
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -1058.0368186929934
Current xi:  [167.40909]
objective value function right now is: -1058.0368186929934
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.48366]
objective value function right now is: -1044.5261264752828
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.31802]
objective value function right now is: -1051.433046095759
new min fval from sgd:  -1063.4653698181412
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.43135]
objective value function right now is: -1063.4653698181412
new min fval from sgd:  -1063.5743288046071
new min fval from sgd:  -1063.7261703826878
new min fval from sgd:  -1063.8305367392697
new min fval from sgd:  -1064.0656706066125
new min fval from sgd:  -1064.1993653812372
new min fval from sgd:  -1064.356154100345
new min fval from sgd:  -1064.4437174064994
new min fval from sgd:  -1064.5654293492596
new min fval from sgd:  -1064.7714710622524
new min fval from sgd:  -1064.9092945579953
new min fval from sgd:  -1065.0284190185173
new min fval from sgd:  -1065.1118502992001
new min fval from sgd:  -1065.2233969707665
new min fval from sgd:  -1065.3008267701618
new min fval from sgd:  -1065.4115839580008
new min fval from sgd:  -1065.4992311350998
new min fval from sgd:  -1065.5524243030898
new min fval from sgd:  -1065.5580444744808
new min fval from sgd:  -1065.5882077490508
new min fval from sgd:  -1065.720247479396
new min fval from sgd:  -1065.8178376825176
new min fval from sgd:  -1065.911654086687
new min fval from sgd:  -1066.0302128814508
new min fval from sgd:  -1066.1649398086859
new min fval from sgd:  -1066.2743097497714
new min fval from sgd:  -1066.3974947724837
new min fval from sgd:  -1066.4701785589532
new min fval from sgd:  -1066.4810748943937
new min fval from sgd:  -1066.4813493974698
new min fval from sgd:  -1066.6044413086113
new min fval from sgd:  -1066.6816163725844
new min fval from sgd:  -1066.7574967959833
new min fval from sgd:  -1066.8149028661023
new min fval from sgd:  -1066.851751356719
new min fval from sgd:  -1066.8744459259283
new min fval from sgd:  -1066.937457621419
new min fval from sgd:  -1066.9864012557148
new min fval from sgd:  -1067.0501994515591
new min fval from sgd:  -1067.0555726734358
new min fval from sgd:  -1067.162347361293
new min fval from sgd:  -1067.30469572363
new min fval from sgd:  -1067.4936390979376
new min fval from sgd:  -1067.6155045062021
new min fval from sgd:  -1067.7170014388494
new min fval from sgd:  -1067.7728320600647
new min fval from sgd:  -1067.8369200623033
new min fval from sgd:  -1067.9315096788378
new min fval from sgd:  -1068.0115006871015
new min fval from sgd:  -1068.015206177691
new min fval from sgd:  -1068.1234392159258
new min fval from sgd:  -1068.1775198418507
new min fval from sgd:  -1068.2104086495692
new min fval from sgd:  -1068.217054355109
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.37666]
objective value function right now is: -1066.0335364636665
min fval:  -1068.217054355109
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -8.0609,   9.1308],
        [ -8.7770, -22.1906],
        [ -7.9832,  10.6538],
        [ -1.2186,   0.5411],
        [ -1.2088,   0.5738],
        [-45.4449,  -8.3937],
        [  3.8951, -14.4583],
        [-11.5652,   0.4154],
        [  6.5398,  -0.9509],
        [ -1.2088,   0.5738],
        [ -1.2088,   0.5738],
        [ 12.6335,  -0.6594],
        [ 12.5598,  -0.6966]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  2.9884,   5.7152,   6.0145,  -4.0994,  -4.1017,  -5.8468,  -7.2263,
          8.7702,  -9.7359,  -4.1017,  -4.1017, -10.0125,  -9.9436],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.3852e-01, -9.9698e-01, -8.8845e-01, -2.1567e-02, -2.1272e-02,
         -5.9870e-02, -3.8200e-01, -1.1611e+00, -1.0973e-02, -2.1272e-02,
         -2.1272e-02, -3.3766e-01, -3.3622e-01],
        [-5.3626e+00, -4.2633e+00, -9.1869e+00,  1.0861e-01,  9.5244e-02,
          1.0190e+01,  1.5485e+01, -3.0956e+00,  1.4625e+00,  9.5248e-02,
          9.5243e-02,  1.1346e+01,  1.0190e+01],
        [-8.9708e+00, -1.9695e+00, -1.4992e+01,  1.8076e-02,  5.7837e-02,
          1.2496e+01,  1.2896e+01, -1.1273e+00,  3.5043e-01,  5.7834e-02,
          5.7838e-02,  1.7160e+00,  1.7065e+00],
        [ 7.3957e-01,  1.2332e+00,  1.8460e+00,  6.4656e-03,  6.3811e-03,
          9.0850e-02,  3.9104e-01,  1.8693e+00, -6.1468e-03,  6.3810e-03,
          6.3812e-03,  6.2239e-01,  6.2293e-01],
        [-5.7141e+00, -3.0050e+00, -1.0138e+01,  2.6847e-05, -7.0725e-02,
          8.2180e+00,  1.6968e+01, -1.5787e+00,  2.6703e+00, -7.0731e-02,
         -7.0723e-02,  1.1735e+01,  1.0384e+01],
        [ 7.2131e-01,  1.1865e+00,  1.7726e+00,  6.2513e-03,  6.1585e-03,
          9.0370e-02,  3.8780e-01,  1.7963e+00, -6.1248e-03,  6.1584e-03,
          6.1585e-03,  5.8979e-01,  5.9004e-01],
        [-3.3852e-01, -9.9698e-01, -8.8845e-01, -2.1567e-02, -2.1272e-02,
         -5.9870e-02, -3.8200e-01, -1.1611e+00, -1.0973e-02, -2.1272e-02,
         -2.1272e-02, -3.3766e-01, -3.3622e-01],
        [-6.7513e+00, -4.6925e+00, -8.7968e+00, -5.0224e-02,  3.2212e-02,
          9.2381e+00,  1.6325e+01, -3.1334e+00,  2.0884e+00,  3.2218e-02,
          3.2210e-02,  1.2523e+01,  1.1067e+01],
        [-3.3794e-01, -9.9549e-01, -8.8729e-01, -2.1568e-02, -2.1272e-02,
         -5.9901e-02, -3.8202e-01, -1.1602e+00, -1.0969e-02, -2.1272e-02,
         -2.1272e-02, -3.3703e-01, -3.3559e-01],
        [-3.3852e-01, -9.9698e-01, -8.8845e-01, -2.1567e-02, -2.1272e-02,
         -5.9870e-02, -3.8200e-01, -1.1611e+00, -1.0973e-02, -2.1272e-02,
         -2.1272e-02, -3.3766e-01, -3.3622e-01],
        [-3.3822e-01, -9.9575e-01, -8.8788e-01, -2.1564e-02, -2.1270e-02,
         -5.9908e-02, -3.8209e-01, -1.1612e+00, -1.0968e-02, -2.1270e-02,
         -2.1270e-02, -3.3727e-01, -3.3583e-01],
        [-3.3852e-01, -9.9698e-01, -8.8845e-01, -2.1567e-02, -2.1272e-02,
         -5.9870e-02, -3.8200e-01, -1.1611e+00, -1.0973e-02, -2.1272e-02,
         -2.1272e-02, -3.3766e-01, -3.3622e-01],
        [-3.3852e-01, -9.9698e-01, -8.8845e-01, -2.1567e-02, -2.1272e-02,
         -5.9870e-02, -3.8200e-01, -1.1611e+00, -1.0973e-02, -2.1272e-02,
         -2.1272e-02, -3.3766e-01, -3.3622e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.6585,  0.5609,  0.6071,  3.0567,  1.4850,  2.9228, -1.6585,  0.6431,
        -1.6627, -1.6585, -1.6604, -1.6585, -1.6585], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0506,  -8.9523,  -7.3127,   6.6226, -10.4480,   4.8814,  -0.0506,
         -12.9322,  -0.0507,  -0.0506,  -0.0507,  -0.0506,  -0.0506]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -8.8227, -12.2629],
        [  2.2120,  16.5034],
        [  1.3171,  12.2131],
        [-11.9925,  -6.6548],
        [ 13.2287,  -0.0623],
        [  6.9632,  12.4065],
        [-11.1923,  -1.7265],
        [-11.9071,  -7.5915],
        [ -1.7492,   3.7527],
        [ -7.2983,  15.8703],
        [  0.8834,  19.0445],
        [  8.6054,   6.9365],
        [-10.7349,  -5.8651]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -8.8624,   8.4219,   2.5582,  -2.4934, -12.5838,   5.6734,   3.7376,
         -4.9146,  -4.7586,   9.6725,  11.0275,  -5.5810,  -3.0299],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 4.7382e+00,  5.9765e+00, -3.5937e+00,  1.1901e+01, -1.6761e+01,
         -5.9930e+00,  3.5983e+00,  1.5411e+01,  5.7603e-02, -2.8775e+00,
          6.9325e+00, -9.5058e+00,  9.9011e+00],
        [ 2.0346e+01, -1.4945e+01, -7.0664e+00,  3.0205e+00, -2.5941e+00,
         -5.7412e+00,  1.8319e+00, -1.7429e-02, -1.0328e-01, -1.0312e+00,
         -1.5481e+01,  2.8270e+00,  2.7415e+00],
        [-7.7698e+00, -1.8292e+00,  3.3877e+00, -3.1268e+00,  1.0599e+01,
         -2.2193e+00, -7.9381e+00, -4.9333e-01,  2.0072e+00,  2.3609e+00,
         -1.0366e+01,  1.6217e+00, -2.0401e+00],
        [-5.0985e+00,  1.4697e+01,  1.8117e+00, -1.4606e+01, -9.6109e+00,
          4.6180e+00, -4.4827e+00, -5.3838e+00, -3.5895e-01,  2.2553e+01,
          1.4454e+01,  5.9644e-01, -8.5212e+00],
        [-5.0333e+00,  1.2960e+01,  5.1016e-01, -8.0699e+00,  1.0560e+01,
          4.3073e+01,  1.1458e+00, -6.8296e+00,  1.0465e-01, -1.4290e+00,
          6.6730e+00, -5.7083e-03, -4.5360e+00],
        [ 6.8424e-01, -7.9833e-01, -4.2344e+00, -6.5594e+00,  7.2073e+00,
         -5.2161e+00, -7.4086e+00,  1.5555e+00,  1.6615e-01,  1.9998e+01,
          8.8559e+00, -1.8873e-01, -3.8167e+00],
        [ 5.7172e+00, -2.0648e+00,  1.3289e-01,  1.6636e+00, -1.9338e+00,
         -1.3661e+01,  2.4576e-02, -7.3805e-01, -8.1239e-02, -1.0793e-01,
         -4.1351e+00, -8.8614e-01, -1.4028e-01],
        [ 4.7215e-01,  2.3690e-01,  3.0304e+00,  6.8530e+00, -2.7324e+00,
         -4.3157e+00, -2.1532e+00,  1.0032e+01, -5.0099e+00,  3.8143e+00,
          3.5373e+00,  1.6647e+00,  6.2276e+00],
        [-4.8814e+00,  1.3208e+01,  5.3008e-01, -8.0688e+00,  1.0704e+01,
          4.3603e+01,  8.6177e-01, -6.6850e+00,  1.0770e-01, -1.3421e+00,
          6.7619e+00, -6.7114e-03, -4.7312e+00],
        [-2.8650e-01, -2.5694e-01, -1.2386e-01, -2.1084e-01, -1.1178e+00,
         -1.8281e+00, -1.0618e+00, -5.0527e-02, -3.6128e-01, -2.6949e-01,
         -2.5149e-01, -1.2407e-01, -1.1818e-01],
        [ 1.8037e+00, -1.8616e+01, -4.3712e+00,  1.1603e+01, -9.6205e+00,
         -1.3492e+01,  9.2393e-01,  9.9757e+00, -6.7192e-02, -8.1582e+00,
         -1.3475e+01, -4.1342e+00,  6.3481e+00],
        [-4.0064e+00, -4.2785e+00, -1.5538e+00,  1.8847e+00,  2.4442e+00,
         -4.2822e+00, -4.6220e+00,  9.9480e-01,  1.0015e+01, -1.4230e+00,
         -3.9139e+00,  1.6324e+00,  3.6498e-01],
        [-2.8650e-01, -2.5694e-01, -1.2386e-01, -2.1084e-01, -1.1178e+00,
         -1.8281e+00, -1.0618e+00, -5.0527e-02, -3.6128e-01, -2.6949e-01,
         -2.5149e-01, -1.2407e-01, -1.1818e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.0689,  4.8494, -6.5565, -3.7624, 11.5001, -4.3481, -3.6623, -6.2471,
        11.7372, -7.6316, -1.1431, -5.2088, -7.6316], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 7.6054e-01,  7.0853e-01, -4.8415e+00,  1.0261e+00,  5.5380e-01,
          6.6080e-01, -4.3421e+00,  5.3659e-01,  5.8276e-01,  1.6602e-01,
         -3.2379e+01,  3.6605e-01,  1.6602e-01],
        [-4.0119e-02,  5.1844e-01,  5.5218e+00, -7.8769e-02,  4.2326e-01,
          7.7493e-01,  4.9983e+00, -5.2987e-01,  7.1562e-01, -1.6583e-01,
          3.2322e+01,  5.2976e-01, -1.6582e-01],
        [-4.6658e-01, -1.5744e+00,  1.6725e-02, -3.3327e+00, -5.4794e+00,
         -5.8862e-01,  1.9475e-02, -8.6305e-02, -5.4794e+00,  1.1461e-03,
          5.0913e-03, -3.7012e-05,  1.1461e-03],
        [ 1.1260e+00,  4.1044e+00,  1.8594e+00, -4.4224e+00, -2.6367e+00,
         -3.3911e+00, -3.1257e-01,  8.8543e-01, -2.5168e+00,  1.3321e-01,
          1.0150e+01, -6.8465e-01,  1.3321e-01],
        [-5.4458e+00, -2.9409e+00,  2.8038e-07, -8.3197e-08,  2.8263e+01,
          1.4320e-01, -3.5244e+00, -5.0105e+00,  2.9103e+01, -1.0276e-02,
         -3.8399e+00, -6.5926e-01, -1.0276e-02]], device='cuda:0'))])
xi:  [167.39232]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 713.2587435911367
W_T_median: 469.14619712363265
W_T_pctile_5: 166.70051097099244
W_T_CVAR_5_pct: -7.369800203476443
Average q (qsum/M+1):  46.350034652217744
Optimal xi:  [167.39232]
Expected(across Rb) median(across samples) p_equity:  0.13828740666309994
obj fun:  tensor(-1068.2171, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -7.2542,   7.3206],
        [ -6.7699, -10.5979],
        [ -5.2193,   8.6804],
        [  0.2241,   0.3177],
        [ -0.7246,   0.9976],
        [-52.2798,  -6.4820],
        [ -2.6707, -11.2124],
        [ -8.5838,   0.4254],
        [  6.5248,   8.9539],
        [ -0.7255,   0.9969],
        [ -0.7246,   0.9977],
        [  9.1326,  -0.8868],
        [  8.9232,  -1.4696]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 3.0600,  5.1975,  5.6464, -4.0477, -2.5143, -5.4062, -6.5253,  7.7716,
        -6.6490, -2.5147, -2.5143, -8.5705, -8.6983], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-4.2583e+00, -2.3487e+00, -9.1379e+00, -4.1893e-02, -5.7324e-02,
          1.1763e+01,  1.2797e+01, -2.3341e+00,  5.7757e+00, -5.9082e-02,
         -5.7144e-02,  6.3655e+00,  5.6082e+00],
        [-6.9528e+00, -2.6611e+00, -1.0702e+01, -1.3038e-01, -7.9669e-02,
          6.1040e+00,  1.1654e+01, -1.8717e+00,  3.0247e+00, -7.9543e-02,
         -7.9681e-02,  4.7476e+00,  4.0439e+00],
        [ 2.3317e-01,  1.6804e+00,  1.1144e+00,  2.7733e-02,  6.2818e-02,
          1.7155e-02,  4.5875e-01,  1.6943e+00,  4.4272e-01,  6.2998e-02,
          6.2799e-02,  4.2736e-01,  3.2275e-01],
        [-4.0942e+00, -2.3099e+00, -9.3733e+00,  1.0466e-02,  1.2226e-01,
          1.2791e+01,  1.3622e+01, -1.8156e+00,  6.1902e+00,  1.2472e-01,
          1.2201e-01,  6.4802e+00,  5.5247e+00],
        [ 2.2773e-01,  1.6243e+00,  1.0676e+00,  2.6215e-02,  6.0310e-02,
          1.6723e-02,  4.5316e-01,  1.6373e+00,  4.1183e-01,  6.0480e-02,
          6.0293e-02,  4.0009e-01,  3.0185e-01],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-5.3829e+00, -2.8298e+00, -9.4959e+00,  2.0391e-01, -4.5240e-02,
          9.0749e+00,  1.2055e+01, -2.8772e+00,  5.6057e+00, -4.5905e-02,
         -4.5172e-02,  8.2304e+00,  7.3503e+00],
        [-4.5205e+00, -2.6678e+00, -8.6396e+00, -4.4378e-02,  4.8922e-02,
          4.0271e+00,  1.0322e+01, -2.4150e+00,  2.2341e+00,  4.9162e-02,
          4.8897e-02,  3.9250e+00,  3.0669e+00],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-2.8199e-01, -2.2754e+00, -3.0344e+00, -4.0272e-03, -2.1728e-02,
          6.2891e-02,  3.0368e+00, -2.2460e+00,  5.0454e-01, -2.1810e-02,
         -2.1719e-02,  7.0139e-01,  4.5223e-01],
        [-3.0101e-02, -6.0256e-01, -3.4920e-01, -4.2912e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1736e-01, -6.6291e-02, -1.0337e-02,
         -1.0342e-02, -8.4350e-02, -6.9009e-02],
        [-3.0101e-02, -6.0257e-01, -3.4920e-01, -4.2911e-03, -1.0341e-02,
         -3.4015e-02, -2.3276e-01, -6.1737e-01, -6.6294e-02, -1.0337e-02,
         -1.0342e-02, -8.4352e-02, -6.9009e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8182, -0.0261,  0.0321,  2.4714,  0.0593,  2.3684, -0.8182, -0.3568,
        -0.2433, -0.8182, -0.8353, -0.8182, -0.8182], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.8009e-03, -6.9790e+00, -7.7505e+00,  7.8855e+00, -8.7395e+00,
          6.1127e+00,  1.8009e-03, -9.8697e+00, -4.9599e+00,  1.8009e-03,
         -1.0883e+00,  1.8009e-03,  1.8009e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.5627, -11.5538],
        [ -0.2054,  12.2057],
        [ -2.9373,   6.0430],
        [-10.6203,  -5.6749],
        [ 10.9489,   0.0949],
        [  5.0883,  10.4545],
        [ -7.3211,   0.0880],
        [-10.6611,  -5.9336],
        [ -1.5390,   0.8026],
        [ -5.0853,  12.2247],
        [ -7.5356,  14.9437],
        [  8.8835,   2.3574],
        [ -9.0614,  -4.6052]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-8.2035,  7.9696,  0.6017, -2.7780, -9.5798,  6.7590,  5.1073, -3.5817,
        -3.2062,  8.3475, 10.7078, -3.0235, -1.9507], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.5360e+00,  5.1902e+00, -8.8261e-01,  1.8825e-01, -1.1617e+01,
         -3.0109e+00,  7.3917e+00, -5.0429e-01,  2.7591e-02,  6.2055e+00,
          2.7854e+00, -1.0616e+00,  1.2582e+00],
        [ 1.0863e+01, -4.0755e+00, -3.4312e-01,  7.2742e-01, -1.0664e+01,
         -4.0139e+00,  9.5468e+00, -3.9590e+00,  1.2329e-02, -2.2796e+00,
         -1.5576e+01,  6.8790e-02,  3.1184e+00],
        [-1.4642e+01,  7.1294e+00, -4.1546e+00, -1.3205e-01,  1.2756e+01,
          3.2742e+00, -1.0937e+01, -8.2515e-02,  1.0508e-01, -7.1542e-01,
         -8.9964e+00, -3.1784e-01, -2.6675e-01],
        [-4.3054e+00,  1.3906e+01,  1.6532e-01, -8.5551e+00, -8.6290e+00,
          9.0489e+00, -1.2236e+00, -4.8719e+00, -1.1856e-01,  1.1045e+01,
          1.0371e+01,  1.6169e+00, -5.8449e+00],
        [-1.6620e+00,  2.2172e+00,  7.3730e-02, -7.4682e+00,  2.1070e+00,
          1.2570e+01,  2.5803e+00, -5.0797e+00,  4.1404e-02, -1.3499e-01,
         -1.4182e+00,  2.0066e+00, -3.8558e+00],
        [-2.7976e+00,  5.9804e+00,  2.5414e-01, -6.9438e+00,  5.2150e+00,
          8.2924e+00, -2.2936e+00, -2.8446e+00, -1.8938e-01,  7.2713e+00,
          8.7133e+00,  4.0492e+00, -3.9568e+00],
        [ 3.9558e+00, -4.9965e+00, -2.7392e-01,  9.7790e+00, -2.0234e+00,
         -4.3889e+00,  3.8306e+00,  6.0189e+00,  8.3592e-02, -1.1287e+01,
         -2.0694e+01, -4.3281e-01,  7.7430e+00],
        [-6.1261e-01,  7.4853e-01, -1.9733e+00,  1.8636e+00, -2.5340e-01,
         -8.6531e-01, -7.7204e-01,  1.0507e+00, -1.3834e+00,  1.6075e+00,
          1.6765e+00, -1.8126e+00,  2.0834e+00],
        [-1.7054e+00,  2.1658e+00,  7.0490e-02, -7.4993e+00,  2.1097e+00,
          1.2681e+01,  2.5081e+00, -4.9668e+00,  4.2617e-02, -2.1372e-01,
         -1.5385e+00,  1.9917e+00, -4.0639e+00],
        [-8.5966e-01, -5.4158e-01,  3.1900e-02, -1.0584e-01, -5.2516e-01,
         -1.5050e+00, -1.0865e+00, -7.5104e-02,  4.2592e-02, -2.2466e-01,
         -1.8496e-01, -1.6167e+00, -1.0209e-01],
        [ 2.8503e+00, -1.3851e+01, -3.2277e-01,  1.2349e+01, -6.6659e+00,
         -1.6797e+01,  2.0115e+00,  7.6731e+00,  4.9482e-02, -8.4937e+00,
         -8.1226e+00, -3.5145e+00,  7.0066e+00],
        [ 1.6826e+00,  5.2625e-01,  4.1788e-01,  1.3952e-01,  4.3639e-01,
          1.4718e+00,  1.7609e+00,  7.2000e-02,  4.9240e-02,  2.2221e-01,
          3.2291e-01,  1.5640e+00,  2.5923e-01],
        [-8.8093e-01, -4.3473e-01, -1.6270e-01, -3.4271e-02, -5.6408e-01,
         -1.4559e+00, -1.1035e+00, -1.5385e-01,  2.5814e-02, -3.4916e-02,
         -1.0794e-01, -1.7583e+00,  2.8486e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 1.3890,  2.7192, -2.4559, -2.6275,  4.8735,  0.2970,  2.1879, -1.9531,
         5.0658, -2.4521, -0.5826,  3.1194, -2.4773], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.6213e-01, -6.8777e-01, -6.5505e-01,  1.3206e+00,  7.1399e-01,
          1.2783e+00,  1.1355e+00,  4.2453e-01,  7.4292e-01, -1.6221e-01,
         -9.0400e+00, -3.4519e-01, -1.4551e-01],
        [ 5.6044e-01,  1.9149e+00,  1.3483e+00, -3.6948e-01,  2.6564e-01,
          1.6115e-01, -4.7250e-01, -4.1914e-01,  5.5808e-01,  1.6534e-01,
          8.9809e+00,  1.2504e+00,  1.4926e-01],
        [-8.2717e-01, -4.8577e-01, -6.9095e-01, -1.3295e+00, -1.5980e+00,
         -1.6062e+00, -3.5694e-01, -1.2789e-01, -1.5980e+00, -2.0943e-03,
         -4.4515e-02, -1.6309e+00, -1.6794e-03],
        [ 9.8592e-01,  3.9564e+00,  3.5376e+00, -2.5419e+00, -1.1885e+00,
         -2.7500e+00, -2.6475e+00,  5.9882e-01, -1.0693e+00,  1.4172e-01,
          7.7192e+00,  1.9260e+00,  2.3542e-01],
        [-2.3016e+00, -1.9959e+00,  5.9102e-07,  9.3182e-01,  1.2590e+01,
          1.4468e+00, -2.4106e+00, -1.9486e+00,  1.2788e+01, -7.4873e-02,
         -2.9963e+00, -1.8615e+00, -8.3155e-02]], device='cuda:0'))])
loaded xi:  19.928818
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 940.130933411658
W_T_median: 581.3306640883857
W_T_pctile_5: -358.9604697855882
W_T_CVAR_5_pct: -487.0834540584932
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  43.7934270789913
Current xi:  [35.33025]
objective value function right now is: 43.7934270789913
4.0% of gradient descent iterations done. Method = Adam
new min fval:  34.79694586533834
Current xi:  [50.002895]
objective value function right now is: 34.79694586533834
6.0% of gradient descent iterations done. Method = Adam
new min fval:  32.99177533760736
Current xi:  [63.76112]
objective value function right now is: 32.99177533760736
8.0% of gradient descent iterations done. Method = Adam
new min fval:  27.931350510178078
Current xi:  [76.758766]
objective value function right now is: 27.931350510178078
10.0% of gradient descent iterations done. Method = Adam
new min fval:  21.79821942310433
Current xi:  [89.39404]
objective value function right now is: 21.79821942310433
12.0% of gradient descent iterations done. Method = Adam
new min fval:  21.280580732928176
Current xi:  [101.01129]
objective value function right now is: 21.280580732928176
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  15.741916145604762
Current xi:  [111.66841]
objective value function right now is: 15.741916145604762
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [120.485176]
objective value function right now is: 16.87007357067713
18.0% of gradient descent iterations done. Method = Adam
new min fval:  14.406727153578762
Current xi:  [129.50426]
objective value function right now is: 14.406727153578762
20.0% of gradient descent iterations done. Method = Adam
new min fval:  11.02236616763393
Current xi:  [137.48067]
objective value function right now is: 11.02236616763393
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [145.0305]
objective value function right now is: 12.634056031580913
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [149.4975]
objective value function right now is: 14.752994436235367
26.0% of gradient descent iterations done. Method = Adam
new min fval:  9.457102115193207
Current xi:  [154.75171]
objective value function right now is: 9.457102115193207
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [157.16826]
objective value function right now is: 9.591134989443848
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [160.75575]
objective value function right now is: 10.667476557819404
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.01434]
objective value function right now is: 11.981811467111196
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.0212]
objective value function right now is: 16.43198462256889
36.0% of gradient descent iterations done. Method = Adam
new min fval:  8.953497846254121
Current xi:  [167.54497]
objective value function right now is: 8.953497846254121
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.27505]
objective value function right now is: 9.522230430304642
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.51726]
objective value function right now is: 10.365318611536088
42.0% of gradient descent iterations done. Method = Adam
new min fval:  8.000870228687914
Current xi:  [167.83209]
objective value function right now is: 8.000870228687914
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.31154]
objective value function right now is: 10.607579808264303
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.27847]
objective value function right now is: 9.194562499186276
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.5927]
objective value function right now is: 8.348884443537209
50.0% of gradient descent iterations done. Method = Adam
new min fval:  7.928917471155655
Current xi:  [168.77147]
objective value function right now is: 7.928917471155655
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.6806]
objective value function right now is: 8.741161818625399
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.27661]
objective value function right now is: 9.622748916159978
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  7.8485563969839935
Current xi:  [166.89323]
objective value function right now is: 7.8485563969839935
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [167.15791]
objective value function right now is: 9.348754618205064
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.24675]
objective value function right now is: 10.388647999531722
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.819]
objective value function right now is: 16.174547121129887
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.17061]
objective value function right now is: 8.901919157782206
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.53354]
objective value function right now is: 13.592182985561701
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.76073]
objective value function right now is: 7.849159601889163
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.99995]
objective value function right now is: 9.286863856703949
72.0% of gradient descent iterations done. Method = Adam
new min fval:  7.664000905025041
Current xi:  [170.40102]
objective value function right now is: 7.664000905025041
74.0% of gradient descent iterations done. Method = Adam
new min fval:  7.314481028083629
Current xi:  [170.3438]
objective value function right now is: 7.314481028083629
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.52373]
objective value function right now is: 7.548351522943569
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.50732]
objective value function right now is: 7.4409698110367675
80.0% of gradient descent iterations done. Method = Adam
new min fval:  7.203140093973734
Current xi:  [170.20055]
objective value function right now is: 7.203140093973734
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.96362]
objective value function right now is: 7.336696289424698
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.8317]
objective value function right now is: 8.169448648183653
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.9162]
objective value function right now is: 7.320857688835635
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.24245]
objective value function right now is: 7.281321705728144
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.22581]
objective value function right now is: 7.557582249676089
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.1737]
objective value function right now is: 7.508330962083361
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.84123]
objective value function right now is: 7.732048083008643
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.79404]
objective value function right now is: 7.599441872108519
new min fval from sgd:  7.137676653686874
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.81898]
objective value function right now is: 7.137676653686874
new min fval from sgd:  7.1353343824040385
new min fval from sgd:  7.132895261501817
new min fval from sgd:  7.130425893045194
new min fval from sgd:  7.129109667035936
new min fval from sgd:  7.127408351794915
new min fval from sgd:  7.127365272605635
new min fval from sgd:  7.126845589117651
new min fval from sgd:  7.120052280999835
new min fval from sgd:  7.114740702586725
new min fval from sgd:  7.109944429054934
new min fval from sgd:  7.107778191294829
new min fval from sgd:  7.105187385509947
new min fval from sgd:  7.102807801314821
new min fval from sgd:  7.100417789573987
new min fval from sgd:  7.099247239650223
new min fval from sgd:  7.098115624145922
new min fval from sgd:  7.096731476228148
new min fval from sgd:  7.0960002660978825
new min fval from sgd:  7.095849031225589
new min fval from sgd:  7.091927220058817
new min fval from sgd:  7.0881524911295815
new min fval from sgd:  7.084992878668354
new min fval from sgd:  7.0828225566173515
new min fval from sgd:  7.080282726343535
new min fval from sgd:  7.077761226060829
new min fval from sgd:  7.075910977277772
new min fval from sgd:  7.0742559376654395
new min fval from sgd:  7.072817465861204
new min fval from sgd:  7.071827360218018
new min fval from sgd:  7.069418920667028
new min fval from sgd:  7.067934967971537
new min fval from sgd:  7.06724475514798
new min fval from sgd:  7.066661639861881
new min fval from sgd:  7.065219905696198
new min fval from sgd:  7.063528796480481
new min fval from sgd:  7.061511982722337
new min fval from sgd:  7.0604302709662266
new min fval from sgd:  7.059631105658431
new min fval from sgd:  7.059000618844018
new min fval from sgd:  7.0582954673452
new min fval from sgd:  7.05808712793421
new min fval from sgd:  7.057450295073855
new min fval from sgd:  7.057445715056705
new min fval from sgd:  7.0573501897499105
new min fval from sgd:  7.057343119362019
new min fval from sgd:  7.057057509974486
new min fval from sgd:  7.05412474764959
new min fval from sgd:  7.052355557598643
new min fval from sgd:  7.0507977568373885
new min fval from sgd:  7.049624488740953
new min fval from sgd:  7.048473970478065
new min fval from sgd:  7.048188333013819
new min fval from sgd:  7.047501201088457
new min fval from sgd:  7.046302146279511
new min fval from sgd:  7.045355499589918
new min fval from sgd:  7.044632884117257
new min fval from sgd:  7.043857361857665
new min fval from sgd:  7.043623135705587
new min fval from sgd:  7.0436186105206104
new min fval from sgd:  7.042337265888242
new min fval from sgd:  7.03951977765415
new min fval from sgd:  7.037788837038236
new min fval from sgd:  7.036942234561192
new min fval from sgd:  7.036351376992134
new min fval from sgd:  7.036289061472464
new min fval from sgd:  7.036282473706879
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.89029]
objective value function right now is: 7.038302819498082
min fval:  7.036282473706879
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.1013, -0.1368],
        [ 0.1013, -0.1368],
        [ 0.1013, -0.1368],
        [ 0.1013, -0.1368],
        [ 0.1013, -0.1368],
        [ 0.1013, -0.1368],
        [ 0.1013, -0.1368],
        [ 0.1013, -0.1368],
        [ 0.1013, -0.1368],
        [ 0.1013, -0.1368],
        [ 0.1013, -0.1368],
        [ 0.1013, -0.1368],
        [ 0.1013, -0.1368]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([0.1830, 0.1830, 0.1830, 0.1830, 0.1830, 0.1830, 0.1830, 0.1830, 0.1830,
        0.1830, 0.1830, 0.1830, 0.1830], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846,
         0.1846, 0.1846, 0.1846, 0.1846],
        [0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846,
         0.1846, 0.1846, 0.1846, 0.1846],
        [0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846,
         0.1846, 0.1846, 0.1846, 0.1846],
        [0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846,
         0.1846, 0.1846, 0.1846, 0.1846],
        [0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846,
         0.1846, 0.1846, 0.1846, 0.1846],
        [0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846,
         0.1846, 0.1846, 0.1846, 0.1846],
        [0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846,
         0.1846, 0.1846, 0.1846, 0.1846],
        [0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846,
         0.1846, 0.1846, 0.1846, 0.1846],
        [0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846,
         0.1846, 0.1846, 0.1846, 0.1846],
        [0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846,
         0.1846, 0.1846, 0.1846, 0.1846],
        [0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846,
         0.1846, 0.1846, 0.1846, 0.1846],
        [0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846,
         0.1846, 0.1846, 0.1846, 0.1846],
        [0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846, 0.1846,
         0.1846, 0.1846, 0.1846, 0.1846]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([0.3154, 0.3154, 0.3154, 0.3154, 0.3154, 0.3154, 0.3154, 0.3154, 0.3154,
        0.3154, 0.3154, 0.3154, 0.3154], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.4273, -1.4273, -1.4273, -1.4273, -1.4273, -1.4273, -1.4273, -1.4273,
         -1.4273, -1.4273, -1.4273, -1.4273, -1.4273]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -6.6179, -12.6728],
        [  2.2770,  15.3613],
        [ -1.4548,   1.4786],
        [-11.4334,  -6.3794],
        [ 13.0297,  -0.7908],
        [  8.0678,  12.1247],
        [-10.1482,  -1.7592],
        [-11.5147,  -7.8219],
        [ -1.4216,   1.4553],
        [ -1.2898,  15.0898],
        [ -0.7139,  18.3114],
        [  9.4234,   3.9637],
        [-11.9898,  -5.4220]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -8.4084,   8.8976,  -2.9742,  -2.3738, -12.4181,   6.3688,   4.7724,
         -5.2043,  -2.9758,   8.8430,  10.9166,  -6.1656,   0.1423],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[  3.3041,  -9.2853,   0.0999,   3.8149,  -4.4674,  -6.4771,   3.7649,
           3.1798,   0.0970,  -3.7717,  -6.8935,  -4.5349,   2.7701],
        [ 12.9089, -11.2475,  -0.0960,   2.6715,   7.1998,  -5.5050,   3.4387,
           3.0158,  -0.0997,  -4.4907, -10.3128,   2.9022,   3.2471],
        [ -0.8705,  -0.4621,  -0.3547,   0.0539,  -0.7264,  -1.9410,  -0.9225,
           0.0621,  -0.3550,  -0.2231,  -0.2442,  -0.3174,  -0.2113],
        [ -4.6169,   7.7156,  -0.0378,  -6.8868,  -8.6151,   9.5669,  -1.1334,
          -2.0347,  -0.0224,   8.0766,  11.7341,  -1.4760,  -7.4794],
        [ -3.6191,   2.8791,   0.4282,  -5.4396,   4.2455,   8.0637,   1.8629,
          -7.1089,   0.4050,   1.3538,   1.1895,  -0.0243,  -2.0040],
        [ -2.1692,   6.3523,  -0.0502,  -1.4648,   8.9927,   0.9126,  -4.9694,
           0.7276,  -0.0575,   5.2997,   9.2716,   0.9712,  -2.6292],
        [  7.2548,  -0.3814,  -0.8718,   0.1848,  -4.0931,  -4.1630,   7.0476,
           0.0898,  -0.6725,   6.1053,   4.6089,  -1.0602,   0.2222],
        [  0.3427,   1.6522,  -1.2742,   5.7964,  -3.4310,  -3.6414,   1.8038,
           1.2737,  -1.0350,   4.3194,   4.4085,  -7.0463,   7.3022],
        [ -3.6259,   3.0542,   0.4418,  -5.5795,   4.4548,   8.6951,   1.8664,
          -7.2577,   0.4177,   1.3832,   1.2347,  -0.0334,  -2.1723],
        [ -0.8709,  -0.4634,  -0.3545,   0.0538,  -0.7257,  -1.9406,  -0.9247,
           0.0620,  -0.3547,  -0.2239,  -0.2447,  -0.3194,  -0.2114],
        [  2.1695, -11.2616,   0.0995,  10.4728,  -9.6742, -13.2282,   2.5588,
           8.1730,   0.1007,  -5.0326,  -9.1462,  -1.3938,   6.5623],
        [  0.2884,   0.7967,   0.6503,   0.1421,   0.0393,   1.7322,   0.8450,
          -0.1689,   0.5353,   0.3277,   0.2140,   1.2140,   0.7630],
        [ -0.8709,  -0.4634,  -0.3545,   0.0538,  -0.7257,  -1.9406,  -0.9247,
           0.0620,  -0.3547,  -0.2239,  -0.2447,  -0.3193,  -0.2114]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.0062,  4.9011, -3.2126, -3.3611,  9.1196, -2.5971,  3.3892, -3.2736,
         9.3977, -3.2112, -2.0743,  2.9769, -3.2112], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-7.7856e+00,  3.4561e-01,  1.3910e-01,  1.1574e+00,  6.9400e-01,
          8.5588e-01,  6.5484e-01,  4.0309e-01,  7.2261e-01,  1.3007e-01,
         -4.1574e+00, -3.1661e-01,  1.3006e-01],
        [ 8.0772e+00,  8.7978e-01, -1.1846e-01, -1.9556e-01,  3.0168e-01,
          5.8566e-01,  2.2764e-02, -3.4877e-01,  5.9110e-01, -1.2804e-01,
          4.1682e+00,  1.2389e+00, -1.2804e-01],
        [-4.7497e-03, -2.3928e-01,  3.5098e-04, -1.9613e+00, -1.8706e+00,
         -1.4175e+00, -1.4621e+00, -1.9142e-01, -1.8706e+00,  3.5462e-04,
          3.2729e-03, -1.8682e+00,  3.5460e-04],
        [ 8.1588e-01,  5.1804e+00,  4.8745e-02, -1.4382e+00, -2.5401e+00,
         -2.0808e+00, -8.7870e-01,  6.3714e-01, -2.4218e+00,  4.8764e-02,
          8.8843e+00,  5.1218e-01,  4.8763e-02],
        [-3.1179e+00, -1.8244e+00, -1.8706e-02,  3.3077e-05,  1.3079e+01,
          5.1600e-02, -1.9458e+00, -2.2083e+00,  1.3870e+01, -1.8690e-02,
         -2.0953e+00, -1.7817e+00, -1.8690e-02]], device='cuda:0'))])
xi:  [169.88383]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1227.3721130417243
W_T_median: 957.9356734166329
W_T_pctile_5: 170.86426361550022
W_T_CVAR_5_pct: -7.033480599445207
Average q (qsum/M+1):  35.0
Optimal xi:  [169.88383]
Expected(across Rb) median(across samples) p_equity:  0.12121953355769316
obj fun:  tensor(7.0363, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 999.0
-----------------------------------------------
