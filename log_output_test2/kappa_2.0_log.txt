Starting at: 
2022-06-20 18:03:38
tracing parameter entered from terminal:  2.0


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = basic_T30_VWD
timeseries_basket['basket_desc'] = CRSP data: T30 and VWD
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
5.0% of MC simulations done.
10.0% of MC simulations done.
15.0% of MC simulations done.
20.0% of MC simulations done.
25.0% of MC simulations done.
30.0% of MC simulations done.
35.0% of MC simulations done.
40.0% of MC simulations done.
45.0% of MC simulations done.
50.0% of MC simulations done.
55.00000000000001% of MC simulations done.
60.0% of MC simulations done.
65.0% of MC simulations done.
70.0% of MC simulations done.
75.0% of MC simulations done.
80.0% of MC simulations done.
85.0% of MC simulations done.
90.0% of MC simulations done.
95.0% of MC simulations done.
100.0% of MC simulations done.
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  hidden_layer    None       None         None   
4        obj.layers[4]        4  hidden_layer    None       None         None   
5        obj.layers[5]        5  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  
4    False        None  
5    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  hidden_layer       4  logistic_sigmoid   
4        obj.layers[4]        4  hidden_layer       4  logistic_sigmoid   
5        obj.layers[5]        5  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 4)    False        None  
4       (4, 4)    False        None  
5       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
[ 0.10347998  0.18844875 -0.3666709   0.62803316 -0.89369922 -0.17398511
  0.93797695  0.78704989 -0.60791207 -0.16293226 -0.31685198  0.28094308
  0.0404241   0.17453202  0.0940472   0.01169373  0.2300954  -0.23784803
  0.78511084 -0.41859022  0.4809118  -0.01155852  0.07219357 -0.14482284
  0.4701344  -0.18827011 -0.8138637  -0.24629474  0.28361272  0.33354483
 -0.63777411 -0.25343004 -0.81551088 -0.26234589  0.20198453 -0.15135741
  0.25264285 -0.29817275  0.41496663 -0.58895503 -0.60305385 -0.63454123
  0.3979274   0.74701442  0.57452389 -0.67377327  0.6493153   0.07340044
 -0.29694484  0.52758424 -0.14347696  0.04319718 -0.28527617 -0.63077443
  0.13133703 -0.58713125  0.12759166  0.87507529 -0.19770888 -0.66759109
  0.25529887 -0.48501395  0.58877641 -0.72315372] (64,)
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1260.6521530687958
W_T_median: 1222.3920506735662
W_T_pctile_5: 834.1581876304141
W_T_CVAR_5_pct: 746.6575474940535
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.606102138868
gradient value of function right now is: [ 5.46008333e-03  3.80360206e-02  4.99257769e-02  5.02079674e-02
 -5.46008333e-03 -3.80360206e-02 -4.99257769e-02 -5.02079674e-02
  1.09522086e-03  6.42419577e-03  4.93000031e-04  6.40456485e-03
  3.58237883e-03  2.10247336e-02  1.61078302e-03  2.09609939e-02
  8.93973572e-04  5.24650906e-03  4.02069669e-04  5.23057142e-03
  5.10506079e-04  2.99357591e-03  2.29951026e-04  2.98438381e-03
 -3.46794395e-05 -3.39959266e-04 -2.13579520e-04 -3.83102270e-04
  2.37186459e-05  2.63658142e-04  1.61411911e-04  2.88660511e-04
 -6.75424377e-05 -6.85508487e-04 -4.27314416e-04 -7.65935761e-04
  3.09798386e-05  3.54284430e-04  2.15799188e-04  3.85626449e-04
 -2.64769234e-06  1.03093784e-04  4.05914892e-05  1.04892920e-04
  1.72821554e-05  2.08705592e-04  1.04992065e-04  2.12811830e-04
  2.55417834e-05  4.39835756e-04  1.92667542e-04  4.46832302e-04
 -2.32783581e-05  3.69450617e-05  3.00360947e-06  3.79993919e-05
 -4.12160361e-05 -1.62560256e-05  3.98943246e-05  8.95940035e-06
  1.01239040e-05  3.32294285e-05  3.11516041e-05  6.94489540e-06
 -3.24799215e+00]
supnorm grad right now is: 3.2479921510239853
Weights right now are: 
[-1.06328475 -1.08816297 -2.01646823 -1.0077086   0.27306551  1.10262661
  2.58777428  2.42279165  0.37912683  0.79576469  0.67494055  1.24693177
  0.4298037  -0.51382074  0.62806325 -0.68520935 -1.03750732 -1.69917276
 -0.44728318 -1.89196016 -1.09622282 -1.82681808 -1.43811123 -1.96014965
  1.41097878  0.76545316  0.14483763  0.73277023 -1.00327195 -1.29898537
 -2.07631371 -1.8603554   0.2049643   0.86638717  1.27304609  1.03480473
 -0.9019694  -1.67581527 -0.82810608 -2.02911967  0.36204382  0.27864019
  1.35598675  1.66816391  1.11530823 -1.46501279 -0.06716749 -0.83364787
  0.46753826  0.26524228 -0.31755364 -0.39020316 -0.92155894 -2.0685662
 -1.2421508  -2.04419877  1.09103624  1.56087775 -0.36465846 -1.76077123
 -0.25032365  0.26880511 -0.48521585 -1.92839744 24.7528963 ]
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.030336756176
gradient value of function right now is: [ 1.90403019e-03  1.37955297e-02  1.51568370e-02  1.52554079e-02
 -1.90403019e-03 -1.37955297e-02 -1.51568370e-02 -1.52554079e-02
  3.73928484e-04  2.25079240e-03  1.52664094e-04  2.24271314e-03
  6.09373920e-04  3.67587239e-03  2.47656620e-04  3.66295558e-03
  2.62398186e-04  1.58066275e-03  1.06967669e-04  1.57502777e-03
  1.41779804e-04  8.53049761e-04  5.79431748e-05  8.49972582e-04
  6.15537645e-06  6.20459051e-05  4.14577480e-05  6.88188247e-05
  5.40038119e-06  7.33432089e-05  4.66251797e-05  7.65546997e-05
 -8.88464347e-06 -1.02070455e-04 -6.66143874e-05 -1.10072942e-04
  6.84292475e-06  9.58002515e-05  6.06506883e-05  9.94636909e-05
 -3.63561634e-06  8.27274049e-06  1.67563624e-06  8.43178521e-06
 -3.03274044e-06  3.33097512e-05  1.45404709e-05  3.38587050e-05
 -9.82956145e-07  8.71022297e-05  3.76033572e-05  8.82568022e-05
 -8.55349768e-06  6.38700437e-06 -1.85409899e-06  6.58797399e-06
 -7.07686198e-06 -3.15791501e-06  7.68165103e-06  3.03985480e-06
  4.61256833e-06  1.82289200e-05  7.86005517e-06  2.86480871e-06
 -7.36435492e-01]
supnorm grad right now is: 0.7364354921751184
Weights right now are: 
[-1.07688399 -1.22325799 -2.15905052 -1.15166282  0.28666475  1.23772163
  2.73035656  2.56674586  0.36662649  0.71724734  0.66985171  1.16986237
  0.34717046 -1.02827626  0.59429411 -1.19059072 -1.04371477 -1.73520788
 -0.44982947 -1.92694613 -1.11291286 -1.90691591 -1.44516184 -2.03639533
  1.41122732  0.7687581   0.14665579  0.73509392 -1.0061535  -1.33258328
 -2.09854033 -1.87870429  0.20680818  0.88902454  1.28665053  1.05042464
 -0.90338291 -1.69492491 -0.8397202  -2.0408407   0.36436062  0.26484767
  1.35072206  1.65155776  1.11392786 -1.51860791 -0.09491756 -0.8888905
  0.46458928  0.18301958 -0.35678387 -0.48156778 -0.90977501 -2.07647777
 -1.24174217 -2.05243898  1.14721275  1.60852446 -0.43299787 -1.77087351
 -0.27795268  0.20730846 -0.51102401 -1.93644325 24.92425461]
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.5079149298326
gradient value of function right now is: [ 8.59283805e-04  5.63080591e-03  5.88797351e-03  5.92934710e-03
 -8.59283805e-04 -5.63080591e-03 -5.88797351e-03 -5.92934710e-03
  1.74140108e-04  1.01582942e-03  6.53516703e-05  1.01160780e-03
  1.64962531e-04  9.64398547e-04  6.15547174e-05  9.60471496e-04
  1.03347371e-04  6.03303947e-04  3.87141048e-05  6.00812131e-04
  5.29113628e-05  3.08600798e-04  1.98671891e-05  3.07315316e-04
  4.03689159e-06  4.27005125e-05  2.99205782e-05  4.72910656e-05
  1.67788923e-06  2.34140509e-05  1.57474015e-05  2.46283323e-05
 -2.41208629e-06 -2.81710014e-05 -1.94194983e-05 -3.05966133e-05
  2.14040546e-06  3.02484480e-05  2.03163428e-05  3.17506060e-05
 -1.15009120e-06 -1.02460279e-07 -6.06463201e-07 -9.28811499e-08
 -5.94708291e-07  8.26952232e-06  4.25934091e-06  8.36174575e-06
  1.51207754e-07  2.47723277e-05  1.25470021e-05  2.50299479e-05
 -2.44881400e-06  1.70907250e-06 -3.64043906e-07  1.74949727e-06
 -2.01560752e-06 -4.99440304e-07  2.34206694e-06  5.41304955e-07
  1.99112040e-06  7.94377027e-06  2.74830223e-06  6.66658016e-07
 -3.74832574e+00]
supnorm grad right now is: 3.7483257421740617
Weights right now are: 
[-1.09214733 -1.36901662 -2.29767423 -1.29171307  0.30192809  1.38348025
  2.86898027  2.70679611  0.35263953  0.62881772  0.6647915   1.08307961
  0.29870456 -1.33288654  0.57675132 -1.48977966 -1.04962963 -1.76979489
 -0.45198511 -1.96051989 -1.12800336 -1.97980738 -1.45084048 -2.10576616
  1.40984925  0.75068359  0.13517446  0.7224251  -1.00760944 -1.36008141
 -2.11677142 -1.89258556  0.20760221  0.90146872  1.29434131  1.05860781
 -0.90407562 -1.71031819 -0.84910203 -2.04956783  0.36893978  0.26237196
  1.35164882  1.64853165  1.12940117 -1.5483046  -0.10590632 -0.91950623
  0.47152053  0.12810098 -0.38165357 -0.5425297  -0.89309117 -2.0809746
 -1.23802038 -2.05718556  1.18032861  1.65200348 -0.47619839 -1.7861748
 -0.30517419  0.11098129 -0.53166717 -1.94930197 24.70684173]
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3585.6611462064056
gradient value of function right now is: [ 4.01221086e-04  2.25103958e-03  2.30449546e-03  2.32131526e-03
 -4.01221086e-04 -2.25103958e-03 -2.30449546e-03 -2.32131526e-03
  8.46741350e-05  4.71403444e-04  2.92834354e-05  4.69214327e-04
  5.06345405e-05  2.82340424e-04  1.74216903e-05  2.81049440e-04
  4.14514844e-05  2.30902280e-04  1.43096650e-05  2.29835394e-04
  2.01674472e-05  1.12283648e-04  6.97385297e-06  1.11762053e-04
  1.69310477e-06  1.83997769e-05  1.34612835e-05  2.05035521e-05
  5.79471877e-07  7.73976886e-06  5.49885631e-06  8.32571424e-06
 -9.22796573e-07 -1.05557760e-05 -7.65359074e-06 -1.16582258e-05
  7.24892640e-07  9.72127149e-06  6.90587102e-06  1.04496066e-05
 -3.48010649e-07 -4.58591135e-07 -4.15080333e-07 -4.61485410e-07
  1.01567908e-07  2.49860346e-06  1.60260928e-06  2.51133810e-06
  4.55697636e-07  8.09070244e-06  4.87231774e-06  8.15258016e-06
 -6.36767519e-07  5.42852250e-07  3.07055005e-08  5.49120372e-07
 -6.75526411e-07 -3.41712821e-08  8.29271255e-07  1.74873968e-08
  8.28037072e-07  3.19891209e-06  1.03625671e-06  1.00163934e-07
 -7.50043481e+00]
supnorm grad right now is: 7.500434813228267
Weights right now are: 
[-1.11067245e+00 -1.52470275e+00 -2.44078731e+00 -1.43635923e+00
  3.20453218e-01  1.53916639e+00  3.01209335e+00  2.85144228e+00
  3.34945158e-01  5.21800035e-01  6.58872974e-01  9.78124805e-01
  2.61653684e-01 -1.55536198e+00  5.64320772e-01 -1.70814029e+00
 -1.05588930e+00 -1.80479900e+00 -4.54097791e-01 -1.99447522e+00
 -1.14313751e+00 -2.04976953e+00 -1.45610879e+00 -2.17230486e+00
  1.40800303e+00  7.26100230e-01  1.18755886e-01  7.05036783e-01
 -1.00907196e+00 -1.38353548e+00 -2.13358857e+00 -1.90492494e+00
  2.08302274e-01  9.11873445e-01  1.30120923e+00  1.06561894e+00
 -9.04790328e-01 -1.72324881e+00 -8.57660336e-01 -2.05724894e+00
  3.71715456e-01  2.63807431e-01  1.35366866e+00  1.65021944e+00
  1.13525138e+00 -1.56877593e+00 -1.16832333e-01 -9.40481693e-01
  4.72930972e-01  8.46490643e-02 -4.06605505e-01 -5.90614265e-01
 -8.83198962e-01 -2.08427632e+00 -1.23672921e+00 -2.06063267e+00
  1.20506919e+00  1.67140460e+00 -5.10629166e-01 -1.79387749e+00
 -3.35394603e-01  1.74650088e-02 -5.50167875e-01 -1.95724241e+00
  2.43508204e+01]
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.943694966775
gradient value of function right now is: [ 1.65467471e-04  7.57689554e-04  7.67046245e-04  7.72671938e-04
 -1.65467471e-04 -7.57689554e-04 -7.67046245e-04 -7.72671938e-04
  3.55960293e-05  1.91157432e-04  1.07484236e-05  1.90286234e-04
  1.37911281e-05  7.42951092e-05  4.11048115e-06  7.39668248e-05
  1.39354486e-05  7.49111090e-05  4.19069471e-06  7.45729139e-05
  6.49009695e-06  3.48528613e-05  1.95977572e-06  3.46939279e-05
  4.55945738e-07  6.00218508e-06  4.48562836e-06  6.53402900e-06
  8.13063985e-08  2.21357872e-06  1.55811226e-06  2.21638533e-06
 -2.61634037e-07 -4.05083629e-06 -2.97193301e-06 -4.30774869e-06
  9.95111901e-08  2.64289714e-06  1.86422639e-06  2.65172714e-06
 -1.63861948e-07 -1.78525875e-07 -1.87696144e-07 -1.78535698e-07
 -1.90705271e-07  7.41318767e-07  3.99901625e-07  7.47828900e-07
 -1.40916919e-07  2.57329242e-06  1.56352081e-06  2.59545591e-06
 -3.59992946e-07  1.42424006e-07 -7.53391995e-08  1.46978940e-07
 -1.94626350e-07 -6.04588518e-08  2.39264822e-07  9.79652974e-08
  2.43877996e-07  1.02546263e-06  3.17118657e-07  1.08275317e-07
 -1.83340582e+00]
supnorm grad right now is: 1.8334058227924173
Weights right now are: 
[-1.13413647 -1.68981795 -2.59016355 -1.58736002  0.34391723  1.70428159
  3.1614696   3.00244307  0.3117679   0.38768784  0.65188915  0.84663335
  0.23068086 -1.73358588  0.55502246 -1.88303139 -1.06255153 -1.84047122
 -0.45611961 -2.02906997 -1.1584876  -2.11772798 -1.46091498 -2.23692198
  1.40618846  0.69958631  0.10042187  0.68637788 -1.01005469 -1.40433008
 -2.14892748 -1.91566259  0.20898234  0.92329937  1.30902291  1.07326539
 -0.90526121 -1.73429705 -0.86519423 -2.06370105  0.37468185  0.26644126
  1.3566011   1.65333796  1.14277846 -1.58618725 -0.12681309 -0.95828331
  0.47559196  0.04451654 -0.43243757 -0.63495369 -0.87281263 -2.08701901
 -1.23543171 -2.06349344  1.2285792   1.68268579 -0.5439088  -1.80035843
 -0.36738641 -0.08127792 -0.569632   -1.96418048 24.86306932]
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.5124633724854
gradient value of function right now is: [ 7.13559083e-05  2.60010201e-04  2.61659952e-04  2.63556606e-04
 -7.13559083e-05 -2.60010201e-04 -2.61659952e-04 -2.63556607e-04
  1.52833678e-05  7.95436892e-05  4.02442248e-06  7.91929975e-05
  4.05151496e-06  2.11849591e-05  1.04120711e-06  2.10960757e-05
  4.78188043e-06  2.49235480e-05  1.24988324e-06  2.48152833e-05
  2.14690890e-06  1.11737613e-05  5.65327413e-07  1.11244776e-05
  1.17053499e-07  1.87480072e-06  1.42482191e-06  1.99466386e-06
 -1.96079909e-09  6.93927373e-07  4.80350413e-07  6.38734586e-07
 -7.36349261e-08 -1.68206630e-06 -1.24372690e-06 -1.72061619e-06
  4.49367356e-10  7.78860504e-07  5.42359342e-07  7.22832948e-07
 -7.18769972e-08 -4.86687445e-08 -6.51460717e-08 -4.81386012e-08
 -1.39819439e-07  2.59898568e-07  1.13903436e-07  2.63730631e-07
 -1.49444354e-07  9.12644714e-07  5.41791658e-07  9.22371296e-07
 -1.77027306e-07  4.22488305e-08 -5.26918121e-08  4.49995561e-08
 -6.36212212e-08 -4.15578421e-08  7.73903551e-08  7.14848179e-08
  7.46930363e-08  3.59246378e-07  1.04512186e-07  6.82341730e-08
  4.08457183e+00]
supnorm grad right now is: 4.084571833292865
Weights right now are: 
[-1.16195025e+00 -1.84770102e+00 -2.73180662e+00 -1.73054358e+00
  3.71731016e-01  1.86216466e+00  3.30311266e+00  3.14562662e+00
  2.83826449e-01  2.32709459e-01  6.44299490e-01  6.94709310e-01
  2.05747886e-01 -1.87121007e+00  5.48330658e-01 -2.01806693e+00
 -1.06897453e+00 -1.87345434e+00 -4.57870927e-01 -2.06105214e+00
 -1.17266387e+00 -2.17792504e+00 -1.46490500e+00 -2.29415137e+00
  1.40472830e+00  6.76213843e-01  8.37968487e-02  6.70029173e-01
 -1.01062001e+00 -1.42201350e+00 -2.16223699e+00 -1.92461142e+00
  2.09671202e-01  9.36183529e-01  1.31809354e+00  1.08183837e+00
 -9.05521838e-01 -1.74316009e+00 -8.71368390e-01 -2.06877123e+00
  3.77532720e-01  2.68654532e-01  1.35930680e+00  1.65595077e+00
  1.15072760e+00 -1.60218486e+00 -1.36392465e-01 -9.74621932e-01
  4.78990583e-01  7.40511731e-03 -4.58181448e-01 -6.75888471e-01
 -8.62359791e-01 -2.08942719e+00 -1.23406698e+00 -2.06600845e+00
  1.24511854e+00  1.69020615e+00 -5.66246984e-01 -1.80673563e+00
 -3.87889322e-01 -1.51065015e-01 -5.86710316e-01 -1.97094301e+00
  2.52538621e+01]
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.0617158013097
gradient value of function right now is: [ 3.47920625e-05  9.87313788e-05  9.90176652e-05  9.97242101e-05
 -3.47920624e-05 -9.87313787e-05 -9.90176651e-05 -9.97242099e-05
  7.23811302e-06  3.62327642e-05  1.78094408e-06  3.60607008e-05
  1.38830684e-06  6.97467209e-06  3.34268385e-07  6.94283817e-06
  1.83748954e-06  9.20877890e-06  4.49030501e-07  9.16558290e-06
  7.95825944e-07  3.98422302e-06  1.95680605e-07  3.96531763e-06
  4.22089260e-08  6.40685066e-07  5.01835380e-07  6.87854082e-07
  3.02758816e-09  2.73186764e-07  1.98524954e-07  2.61385997e-07
 -3.94162319e-08 -7.62136517e-07 -5.84653180e-07 -7.95329307e-07
  4.40863696e-09  2.92934305e-07  2.14268371e-07  2.82785428e-07
 -2.49339789e-08 -9.40139732e-09 -1.72025611e-08 -9.22285008e-09
 -3.64247790e-08  1.15142973e-07  6.55250835e-08  1.16229832e-07
 -3.69151212e-08  3.69920597e-07  2.39550905e-07  3.73197023e-07
 -6.15801361e-08  2.00999359e-08 -1.27948969e-08  2.09116461e-08
 -2.96674849e-08 -1.13472957e-08  3.77761636e-08  2.01240374e-08
  3.25717851e-08  1.30933152e-07  4.73087265e-08  1.94438728e-08
 -2.18500886e-01]
supnorm grad right now is: 0.21850088595555314
Weights right now are: 
[-1.19661082 -2.00256864 -2.8700935  -1.87032022  0.40639159  2.01703228
  3.44139955  3.28540327  0.24953882  0.04892218  0.63594033  0.5145501
  0.18432309 -1.98545818  0.54318964 -2.13016642 -1.07530699 -1.90487532
 -0.45941934 -2.09151842 -1.18612844 -2.23318532 -1.468309   -2.34668844
  1.40355508  0.65572153  0.06888598  0.65578587 -1.01085143 -1.43846739
 -2.17472294 -1.93277731  0.21036447  0.95142166  1.32902719  1.09184444
 -0.90564214 -1.75103942 -0.87693056 -2.07316505  0.38046972  0.27004185
  1.36145351  1.65754768  1.15888302 -1.61776052 -0.14543147 -0.99053485
  0.48381201 -0.02915215 -0.48367974 -0.71589809 -0.85187037 -2.09164362
 -1.23244393 -2.06835263  1.25355409  1.694637   -0.57697742 -1.81319562
 -0.39816749 -0.18975446 -0.59784312 -1.97720971 24.96476455]
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.778400368654
gradient value of function right now is: [ 1.49458248e-05  3.28524101e-05  3.28736719e-05  3.31019103e-05
 -1.49458240e-05 -3.28524084e-05 -3.28736703e-05 -3.31019087e-05
  2.84714202e-06  1.38669092e-05  6.23098460e-07  1.38026151e-05
  4.19204895e-07  2.05023343e-06  8.90388906e-08  2.04118897e-06
  6.11926532e-07  2.98404468e-06  1.32760299e-07  2.97040527e-06
  2.57542061e-07  1.25437775e-06  5.63542518e-08  1.24856111e-06
  1.39470005e-08  2.32099940e-07  1.84130369e-07  2.46158793e-07
 -1.09662818e-09  9.97761315e-08  7.20300914e-08  9.06286047e-08
 -1.13894963e-08 -2.84789781e-07 -2.19749971e-07 -2.89790678e-07
 -2.31050813e-10  1.09672628e-07  8.02144811e-08  1.01721119e-07
 -1.12784056e-08 -2.89117130e-09 -6.91511805e-09 -2.77473207e-09
 -2.09913184e-08  4.32042392e-08  2.15299741e-08  4.37767170e-08
 -2.38208629e-08  1.36137831e-07  8.45326743e-08  1.37566998e-07
 -2.83559301e-08  6.98594538e-09 -7.53200478e-09  7.40633249e-09
 -1.07495671e-08 -6.14242737e-09  1.39055082e-08  1.12010667e-08
  1.13481823e-08  5.20506722e-08  1.73493162e-08  1.01474755e-08
  2.92332583e+00]
supnorm grad right now is: 2.9233258308746857
Weights right now are: 
[-1.24060154 -2.15490707 -3.00573763 -2.00740513  0.4503823   2.16937071
  3.57704368  3.42248817  0.20835334 -0.16435554  0.62669511  0.30551449
  0.16536364 -2.08304828  0.53901043 -2.22592396 -1.08158188 -1.93495576
 -0.46082997 -2.12068003 -1.19892468 -2.28409399 -1.4712843  -2.39510446
  1.40246388  0.63686498  0.05477058  0.64242257 -1.01106504 -1.45400045
 -2.18670151 -1.94115588  0.21116886  0.9686208   1.3417161   1.10330758
 -0.90578154 -1.75900421 -0.88273371 -2.0777253   0.38277582  0.27061187
  1.36275329  1.65816075  1.16277222 -1.6296974  -0.15288706 -1.00261457
  0.48711908 -0.05964142 -0.50511186 -0.74820554 -0.84492989 -2.09359358
 -1.23142361 -2.07038146  1.2577163   1.69565204 -0.58240357 -1.81522883
 -0.40269317 -0.20691527 -0.60411552 -1.97910486 25.16746523]
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.000596119588
gradient value of function right now is: [ 6.77341079e-06  1.19509872e-05  1.19436088e-05  1.20241067e-05
 -6.77341190e-06 -1.19509892e-05 -1.19436108e-05 -1.20241087e-05
  1.13477330e-06  5.33090135e-06  2.37017630e-07  5.30508498e-06
  1.44704495e-07  6.81798966e-07  2.95213868e-08  6.78618059e-07
  2.25288802e-07  1.05922433e-06  4.67544497e-08  1.05414571e-06
  9.24846206e-08  4.34457045e-07  1.93218229e-08  4.32351344e-07
  7.28033250e-09  1.09511027e-07  8.82973406e-08  1.17668759e-07
  4.15650299e-10  4.25527884e-08  3.18558652e-08  4.06561238e-08
 -5.26401085e-09 -1.05173973e-07 -8.28683779e-08 -1.09396015e-07
  9.32143813e-10  4.98587432e-08  3.77991528e-08  4.85775503e-08
 -4.05275812e-09 -2.15049443e-09 -3.20172519e-09 -2.13005140e-09
 -5.59236891e-09  1.74041492e-08  1.01575434e-08  1.75537012e-08
 -5.86981115e-09  5.31132479e-08  3.48650824e-08  5.35737717e-08
 -9.80338094e-09  3.10975153e-09 -1.95038892e-09  3.22947863e-09
 -4.64475104e-09 -1.45928886e-09  6.28150854e-09  2.92252281e-09
  5.23259117e-09  2.00907317e-08  7.70388583e-09  2.73853158e-09
 -1.37698913e+00]
supnorm grad right now is: 1.3769891258742444
Weights right now are: 
[-1.29391501 -2.29987616 -3.13460689 -2.13761744  0.50369578  2.3143398
  3.70591293  3.55270048  0.16382388 -0.38788266  0.61753135  0.08638879
  0.14941987 -2.16232091  0.53576667 -2.3038036  -1.08757311 -1.96280711
 -0.46206589 -2.14768307 -1.21015712 -2.328289   -1.4736879  -2.43727136
  1.40139476  0.61879673  0.04085501  0.62851065 -1.01128232 -1.46510421
 -2.19524141 -1.94870361  0.21190919  0.98453949  1.35370868  1.11444149
 -0.905956   -1.76682627 -0.88857562 -2.08261593  0.38380797  0.27105695
  1.36351876  1.65860819  1.16433064 -1.63530353 -0.15622318 -1.00828864
  0.48849922 -0.07635458 -0.51640283 -0.76533618 -0.84235166 -2.09451486
 -1.2309503  -2.07134696  1.25917437  1.69631922 -0.58432028 -1.81644699
 -0.40451783 -0.21265408 -0.60645191 -1.98026704 24.92581724]
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3586.953841935562
gradient value of function right now is: [ 2.54878053e-06  3.81475799e-06  3.80998457e-06  3.83443660e-06
 -2.54878215e-06 -3.81476043e-06 -3.80998702e-06 -3.83443906e-06
  3.59308316e-07  1.65653362e-06  6.64547398e-08  1.64930761e-06
  4.44079995e-08  2.05536871e-07  7.91845202e-09  2.04689073e-07
  7.16093270e-08  3.30410763e-07  1.31470256e-08  3.28985397e-07
  2.92693220e-08  1.34882472e-07  5.43515313e-09  1.34290295e-07
  2.71780656e-09  4.83028642e-08  3.86985522e-08  5.08104727e-08
 -4.91389245e-10  1.47696427e-08  1.05297156e-08  1.26876099e-08
 -8.17533419e-10 -3.11405817e-08 -2.39844061e-08 -3.07432766e-08
 -3.09171405e-10  1.87838450e-08  1.37097086e-08  1.68201027e-08
 -1.95645656e-09 -1.59524427e-09 -1.97319101e-09 -1.58192048e-09
 -4.31083329e-09  5.31888812e-09  1.90062312e-09  5.42031558e-09
 -4.95246686e-09  1.77737647e-08  1.02684908e-08  1.79971504e-08
 -4.91409787e-09  7.93778627e-10 -1.61802852e-09  8.71098691e-10
 -1.32687811e-09 -1.09272546e-09  1.77719805e-09  2.13190450e-09
  1.57831766e-09  8.25828179e-09  2.38704187e-09  1.90801068e-09
  5.72823514e+00]
supnorm grad right now is: 5.728235136040409
Weights right now are: 
[-1.35714317 -2.44077812 -3.25985525 -2.26413409  0.56692394  2.45524176
  3.8311613   3.67921714  0.12060722 -0.5984054   0.60948307 -0.12035692
  0.13791713 -2.21744479  0.53365837 -2.35824439 -1.09330133 -1.98879563
 -0.4631288  -2.17291401 -1.21821102 -2.36152425 -1.47524973 -2.46942662
  1.40059196  0.60492518  0.02990462  0.61579935 -1.01115867 -1.47054132
 -2.19917902 -1.95307509  0.2122416   0.99478045  1.36147517  1.1226425
 -0.90591714 -1.77251613 -0.89273874 -2.08679137  0.38453032  0.27146061
  1.36410736  1.65900888  1.16570793 -1.6375615  -0.15724962 -1.0105797
  0.49006191 -0.08334298 -0.52065622 -0.77241894 -0.84054709 -2.09488376
 -1.23044503 -2.07173992  1.25973858  1.69667809 -0.58507436 -1.81710663
 -0.40512346 -0.21588774 -0.60743917 -1.98086245 25.35916913]
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.1780453969195
gradient value of function right now is: [ 1.18215643e-06  1.61100567e-06  1.60909223e-06  1.61890495e-06
 -1.18215690e-06 -1.61100632e-06 -1.60909288e-06 -1.61890561e-06
  1.47940906e-07  6.64973766e-07  2.92606462e-08  6.61700675e-07
  1.94221323e-08  8.74632234e-08  3.77721293e-09  8.70437446e-08
  3.06287334e-08  1.37724690e-07  6.03754567e-09  1.37050145e-07
  1.26611396e-08  5.68984579e-08  2.50868751e-09  5.66174807e-08
  1.72325444e-09  2.39478377e-08  1.95234024e-08  2.60243946e-08
  1.90238193e-10  7.03323563e-09  5.40625231e-09  6.99534248e-09
 -7.00969904e-10 -1.20559445e-08 -9.63461136e-09 -1.27637396e-08
  3.26909167e-10  9.23504365e-09  7.18124434e-09  9.34480094e-09
 -6.12727966e-10 -8.68078795e-10 -8.58715694e-10 -8.70640617e-10
 -5.41731045e-10  2.36394600e-09  1.50821278e-09  2.37467168e-09
 -4.49106846e-10  7.48511458e-09  5.12517401e-09  7.53785487e-09
 -1.29045326e-09  4.70629535e-10 -1.90951764e-10  4.82517498e-10
 -6.92285593e-10 -7.63799787e-11  9.86384047e-10  2.28379054e-10
  9.26413928e-10  3.01016361e-09  1.25795248e-09  2.39815015e-10
 -4.70980512e+00]
supnorm grad right now is: 4.709805124918833
Weights right now are: 
[-1.42768384e+00 -2.57656845e+00 -3.38129831e+00 -2.38674620e+00
  6.37464634e-01  2.59103214e+00  3.95260441e+00  3.80182930e+00
  8.75048087e-02 -7.53571038e-01  6.03412241e-01 -2.73497474e-01
  1.31882217e-01 -2.24523447e+00  5.32552655e-01 -2.38584247e+00
 -1.09833504e+00 -2.01124944e+00 -4.64042813e-01 -2.19482306e+00
 -1.22226195e+00 -2.37930329e+00 -1.47602013e+00 -2.48695301e+00
  1.40012861e+00  5.97427047e-01  2.38704842e-02  6.08060762e-01
 -1.01114408e+00 -1.47283190e+00 -2.20087173e+00 -1.95514458e+00
  2.12407352e-01  9.99041379e-01  1.36479330e+00  1.12670528e+00
 -9.05935124e-01 -1.77539350e+00 -8.94898300e-01 -2.08936156e+00
  3.84781845e-01  2.71725601e-01  1.36440028e+00  1.65927322e+00
  1.16616313e+00 -1.63836282e+00 -1.57631472e-01 -1.01139290e+00
  4.90546442e-01 -8.59674022e-02 -5.22302679e-01 -7.75073084e-01
 -8.39947427e-01 -2.09501526e+00 -1.23028205e+00 -2.07188033e+00
  1.25995011e+00  1.69679224e+00 -5.85362323e-01 -1.81734677e+00
 -4.05400381e-01 -2.17029380e-01 -6.07816856e-01 -1.98108932e+00
  2.46200230e+01]
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.053749173582
gradient value of function right now is: [ 5.16843660e-07  6.76118512e-07  6.75370481e-07  6.79301858e-07
 -5.16843610e-07 -6.76118448e-07 -6.75370416e-07 -6.79301793e-07
  6.14540412e-08  2.75642177e-07  1.15487218e-08  2.74362819e-07
  8.30424961e-09  3.73374513e-08  1.52508920e-09  3.71700931e-08
  1.28527997e-08  5.76704245e-08  2.40712967e-09  5.74040848e-08
  5.42449114e-09  2.43194044e-08  1.02379525e-09  2.42057419e-08
  7.29860356e-10  1.11526873e-08  9.03705372e-09  1.19592637e-08
  4.36028934e-12  3.02520363e-09  2.25658027e-09  2.83595104e-09
 -2.13730955e-10 -4.90758624e-09 -3.86100129e-09 -5.03658630e-09
  5.62457125e-11  4.07678053e-09  3.09282001e-09  3.92861245e-09
 -3.23239191e-10 -4.41322919e-10 -4.45967363e-10 -4.41625558e-10
 -5.08423539e-10  9.66993268e-10  4.83298079e-10  9.77516176e-10
 -5.31640343e-10  3.22521975e-09  2.04732534e-09  3.25580711e-09
 -7.40114315e-10  1.74999554e-10 -1.91637446e-10  1.84271522e-10
 -2.72246854e-10 -1.02494049e-10  3.84707060e-10  2.28783900e-10
  3.68375609e-10  1.46247405e-09  5.13249532e-10  2.15926035e-10
 -5.37983804e-01]
supnorm grad right now is: 0.5379838040930838
Weights right now are: 
[-1.49613357e+00 -2.69128131e+00 -3.48698267e+00 -2.49332630e+00
  7.05914368e-01  2.70574501e+00  4.05828878e+00  3.90840941e+00
  7.03155579e-02 -8.31451740e-01  6.00131537e-01 -3.50807667e-01
  1.29389095e-01 -2.25647089e+00  5.32081074e-01 -2.39702206e+00
 -1.10155297e+00 -2.02557376e+00 -4.64650236e-01 -2.20895209e+00
 -1.22389549e+00 -2.38659305e+00 -1.47633653e+00 -2.49419546e+00
  1.39990588e+00  5.94187710e-01  2.12390097e-02  6.04581388e-01
 -1.01115887e+00 -1.47375003e+00 -2.20156780e+00 -1.95603071e+00
  2.12486255e-01  1.00056861e+00  1.36600393e+00  1.12828216e+00
 -9.05966602e-01 -1.77660909e+00 -8.95832359e-01 -2.09055227e+00
  3.84869165e-01  2.71849117e-01  1.36452338e+00  1.65939686e+00
  1.16627507e+00 -1.63866406e+00 -1.57803712e-01 -1.01169655e+00
  4.90653826e-01 -8.69457955e-02 -5.22947733e-01 -7.76059786e-01
 -8.39756091e-01 -2.09507190e+00 -1.23024143e+00 -2.07193936e+00
  1.26004063e+00  1.69681324e+00 -5.85490018e-01 -1.81740008e+00
 -4.05521376e-01 -2.17432797e-01 -6.07980709e-01 -1.98113975e+00
  2.49538725e+01]
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3585.893588670933
gradient value of function right now is: [ 2.46560615e-07  3.15965642e-07  3.15621559e-07  3.17399183e-07
 -2.46560930e-07 -3.15966047e-07 -3.15621964e-07 -3.17399590e-07
  2.88453284e-08  1.30075654e-07  4.92873214e-09  1.29542289e-07
  3.90590213e-09  1.76788194e-08  6.41751319e-10  1.76104853e-08
  5.99550350e-09  2.70469937e-08  1.02033499e-09  2.69367430e-08
  2.57380342e-09  1.15950356e-08  4.44212118e-10  1.15467559e-08
  2.95542608e-10  5.63633148e-09  4.50890042e-09  5.88186050e-09
 -7.50065739e-11  1.40109583e-09  9.75921238e-10  1.14044814e-09
 -3.21328588e-11 -2.29696875e-09 -1.74839256e-09 -2.20563330e-09
 -6.15732466e-11  1.93079570e-09  1.38631903e-09  1.66262895e-09
 -2.02907174e-10 -2.39408105e-10 -2.57270116e-10 -2.38500058e-10
 -5.05607678e-10  4.27487484e-10  8.09333644e-11  4.38648235e-10
 -5.79010653e-10  1.56353922e-09  8.37632346e-10  1.58628112e-09
 -5.24123642e-10  5.70769780e-11 -1.94663004e-10  6.57337720e-11
 -1.06133361e-10 -1.20721108e-10  1.43455925e-10  2.42724393e-10
  1.41005273e-10  8.38715896e-10  2.10614470e-10  2.17832803e-10
  8.23117840e+00]
supnorm grad right now is: 8.23117839928402
Weights right now are: 
[-1.55907788e+00 -2.78080443e+00 -3.57371915e+00 -2.58064617e+00
  7.68858688e-01  2.79526815e+00  4.14502527e+00  3.99572929e+00
  6.10448446e-02 -8.73146373e-01  5.98453132e-01 -3.92300790e-01
  1.28118495e-01 -2.26219656e+00  5.31858013e-01 -2.40272327e+00
 -1.10344988e+00 -2.03408038e+00 -4.64991650e-01 -2.21740563e+00
 -1.22472855e+00 -2.39032923e+00 -1.47648850e+00 -2.49791410e+00
  1.39979895e+00  5.92432082e-01  1.98221632e-02  6.02715068e-01
 -1.01114674e+00 -1.47421440e+00 -2.20190350e+00 -1.95644084e+00
  2.12509386e-01  1.00132034e+00  1.36658752e+00  1.12903393e+00
 -9.05961139e-01 -1.77723951e+00 -8.96298882e-01 -2.09113144e+00
  3.84931794e-01  2.71919150e-01  1.36459872e+00  1.65946690e+00
  1.16638863e+00 -1.63881519e+00 -1.57867991e-01 -1.01184925e+00
  4.90785459e-01 -8.74494164e-02 -5.23252009e-01 -7.76568098e-01
 -8.39603995e-01 -2.09509935e+00 -1.23019798e+00 -2.07196832e+00
  1.26008759e+00  1.69682842e+00 -5.85557087e-01 -1.81743551e+00
 -4.05574546e-01 -2.17700787e-01 -6.08068415e-01 -1.98117154e+00
  2.55449952e+01]
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.9683904595117
gradient value of function right now is: [ 1.76448921e-07  2.23829690e-07  2.23608357e-07  2.24852659e-07
 -1.76449132e-07 -2.23829958e-07 -2.23608624e-07 -2.24852928e-07
  2.08302767e-08  9.31275814e-08  3.79149716e-09  9.27100199e-08
  2.84846082e-09  1.27685990e-08  5.05046810e-10  1.27135683e-08
  4.30588263e-09  1.92566681e-08  7.81412116e-10  1.91706989e-08
  1.85862090e-09  8.30405868e-09  3.40447223e-10  8.26645476e-09
  2.49579787e-10  4.02119869e-09  3.24869472e-09  4.28220977e-09
 -1.35356361e-11  1.04935873e-09  7.69479344e-10  9.50153870e-10
 -5.83249629e-11 -1.64474043e-09 -1.28282666e-09 -1.65849255e-09
  2.73422108e-12  1.43539245e-09  1.07412059e-09  1.34516703e-09
 -1.23393442e-10 -1.65035274e-10 -1.68554920e-10 -1.64976030e-10
 -2.28906119e-10  3.27092643e-10  1.37006071e-10  3.31869458e-10
 -2.48664785e-10  1.11937129e-09  6.80216835e-10  1.13145803e-09
 -2.92654185e-10  5.56596011e-11 -8.74875446e-11  5.96944788e-11
 -9.00580332e-11 -4.86617582e-11  1.26560383e-10  1.04592651e-10
  1.22089458e-10  5.38205900e-10  1.73074945e-10  9.63653196e-11
  1.71766433e+00]
supnorm grad right now is: 1.7176643293859426
Weights right now are: 
[-1.60258336e+00 -2.83733572e+00 -3.62988346e+00 -2.63713669e+00
  8.12364167e-01  2.85179944e+00  4.20118958e+00  4.05221982e+00
  5.57020269e-02 -8.97012947e-01  5.97454909e-01 -4.16055394e-01
  1.27386346e-01 -2.26547388e+00  5.31724094e-01 -2.40598585e+00
 -1.10455524e+00 -2.03901729e+00 -4.65197566e-01 -2.22231847e+00
 -1.22520513e+00 -2.39245644e+00 -1.47657799e+00 -2.50003131e+00
  1.39973229e+00  5.91418520e-01  1.90008716e-02  6.01628326e-01
 -1.01114780e+00 -1.47448468e+00 -2.20210568e+00 -1.95669528e+00
  2.12528074e-01  1.00174309e+00  1.36692027e+00  1.12946792e+00
 -9.05966988e-01 -1.77760701e+00 -8.96578309e-01 -2.09148675e+00
  3.84959796e-01  2.71960371e-01  1.36463924e+00  1.65950816e+00
  1.16643323e+00 -1.63889943e+00 -1.57909577e-01 -1.01193451e+00
  4.90830742e-01 -8.77343075e-02 -5.23432043e-01 -7.76855809e-01
 -8.39540676e-01 -2.09511434e+00 -1.23018153e+00 -2.07198417e+00
  1.26011090e+00  1.69683842e+00 -5.85589835e-01 -1.81745731e+00
 -4.05607039e-01 -2.17830563e-01 -6.08112439e-01 -1.98119245e+00
  2.50887688e+01]
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3586.132941914931
gradient value of function right now is: [ 1.46412566e-07  1.84537891e-07  1.84373622e-07  1.85393805e-07
 -1.46412361e-07 -1.84537632e-07 -1.84373363e-07 -1.85393544e-07
  1.74741880e-08  7.73573736e-08  3.43661361e-09  7.69710924e-08
  2.41503083e-09  1.07062588e-08  4.68784370e-10  1.06538633e-08
  3.60036646e-09  1.59423990e-08  7.06575198e-10  1.58630303e-08
  1.55745429e-09  6.89316267e-09  3.06953248e-10  6.85860971e-09
  2.46578433e-10  3.28698626e-09  2.69094961e-09  3.59423063e-09
  3.45808883e-11  9.14391337e-10  7.12596658e-10  9.31464183e-10
 -8.71323141e-11 -1.36687632e-09 -1.10019481e-09 -1.46433100e-09
  5.56395317e-11  1.23747619e-09  9.74013842e-10  1.27855495e-09
 -7.48886636e-11 -1.28400308e-10 -1.20139976e-10 -1.28981594e-10
 -3.98917735e-11  2.90391043e-10  1.98918620e-10  2.90961364e-10
 -1.85917503e-11  9.22930747e-10  6.50629809e-10  9.28511932e-10
 -1.43193362e-10  6.09492282e-11 -1.29888866e-11  6.19576678e-11
 -8.84257773e-11 -6.57531743e-14  1.28401150e-10  1.28399206e-11
  1.25248876e-10  3.67472986e-10  1.64444477e-10  1.70089440e-11
 -6.77587554e+00]
supnorm grad right now is: 6.77587554093477
Weights right now are: 
[-1.63358785e+00 -2.87664737e+00 -3.66912561e+00 -2.67659737e+00
  8.43368662e-01  2.89111109e+00  4.24043173e+00  4.09168049e+00
  5.20177846e-02 -9.13463876e-01  5.96778113e-01 -4.32431494e-01
  1.26881223e-01 -2.26773468e+00  5.31633411e-01 -2.40823673e+00
 -1.10531553e+00 -2.04241273e+00 -4.65336907e-01 -2.22569842e+00
 -1.22553417e+00 -2.39392469e+00 -1.47663881e+00 -2.50149282e+00
  1.39968689e+00  5.90705863e-01  1.84247563e-02  6.00867589e-01
 -1.01114702e+00 -1.47467142e+00 -2.20224385e+00 -1.95686728e+00
  2.12539447e-01  1.00203355e+00  1.36714745e+00  1.12976257e+00
 -9.05969285e-01 -1.77786232e+00 -8.96770692e-01 -2.09172921e+00
  3.84979916e-01  2.71989987e-01  1.36466862e+00  1.65953773e+00
  1.16647137e+00 -1.63895718e+00 -1.57934675e-01 -1.01199330e+00
  4.90868954e-01 -8.79315316e-02 -5.23554206e-01 -7.77055415e-01
 -8.39494414e-01 -2.09512356e+00 -1.23016801e+00 -2.07199423e+00
  1.26012717e+00  1.69684894e+00 -5.85612651e-01 -1.81747951e+00
 -4.05630514e-01 -2.17919606e-01 -6.08144155e-01 -1.98121288e+00
  2.44705374e+01]
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.0577180052946
gradient value of function right now is: [ 1.06023880e-07  1.32783640e-07  1.32661689e-07  1.33385738e-07
 -1.06023885e-07 -1.32783647e-07 -1.32661696e-07 -1.33385745e-07
  1.25434287e-08  5.58613225e-08  2.30177218e-09  5.56069998e-08
  1.72693114e-09  7.70924572e-09  3.09479919e-10  7.67537901e-09
  2.57791160e-09  1.14836287e-08  4.71853842e-10  1.14315374e-08
  1.12333968e-09  4.99963393e-09  2.07366332e-10  4.97665767e-09
  1.57582205e-10  2.46188730e-09  1.99330000e-09  2.63229871e-09
 -3.30590913e-12  6.43546925e-10  4.76542385e-10  5.93941224e-10
 -3.89122031e-11 -9.83566698e-10 -7.70837001e-10 -1.00084532e-09
  7.49661968e-12  8.82596871e-10  6.65839061e-10  8.40184094e-10
 -7.23497546e-11 -1.01151766e-10 -1.01684178e-10 -1.01199988e-10
 -1.23059702e-10  1.99751317e-10  9.17077757e-11  2.02256521e-10
 -1.30936055e-10  6.77137821e-10  4.20996077e-10  6.83956173e-10
 -1.67507855e-10  3.53578443e-11 -4.69157056e-11  3.75457117e-11
 -5.60591761e-11 -2.48854636e-11  7.94056913e-11  5.51282853e-11
  7.69186787e-11  3.19189372e-10  1.07556337e-10  5.13601541e-11
  3.86931427e-01]
supnorm grad right now is: 0.386931427475767
Weights right now are: 
[-1.65896709e+00 -2.90855646e+00 -3.70100269e+00 -2.70864995e+00
  8.68747900e-01  2.92302019e+00  4.27230881e+00  4.12373307e+00
  4.89962411e-02 -9.26867648e-01  5.96199394e-01 -4.45770329e-01
  1.26464004e-01 -2.26958847e+00  5.31554698e-01 -2.41008175e+00
 -1.10593744e+00 -2.04517216e+00 -4.65455792e-01 -2.22844451e+00
 -1.22580406e+00 -2.39512147e+00 -1.47669067e+00 -2.50268377e+00
  1.39964635e+00  5.90126886e-01  1.79526141e-02  6.00240977e-01
 -1.01115104e+00 -1.47482769e+00 -2.20236405e+00 -1.95702176e+00
  2.12552476e-01  1.00226934e+00  1.36733571e+00  1.13001040e+00
 -9.05976600e-01 -1.77807519e+00 -8.96936400e-01 -2.09194356e+00
  3.84993314e-01  2.72013228e-01  1.36469039e+00  1.65956103e+00
  1.16648623e+00 -1.63900495e+00 -1.57963650e-01 -1.01204143e+00
  4.90881379e-01 -8.80913192e-02 -5.23662458e-01 -7.77216501e-01
 -8.39466964e-01 -2.09513265e+00 -1.23016330e+00 -2.07200365e+00
  1.26014179e+00  1.69685150e+00 -5.85633437e-01 -1.81748687e+00
 -4.05651738e-01 -2.17984335e-01 -6.08170660e-01 -1.98122058e+00
  2.50374461e+01]
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.8455497720242
gradient value of function right now is: [ 9.19261961e-08  1.14672520e-07  1.14571864e-07  1.15194059e-07
 -9.19261289e-08 -1.14672436e-07 -1.14571780e-07 -1.15193975e-07
  1.09088600e-08  4.84070369e-08  2.05186642e-09  4.81787931e-08
  1.50912125e-09  6.70996514e-09  2.78418672e-10  6.67923915e-09
  2.23892283e-09  9.93743606e-09  4.20159853e-10  9.89073233e-09
  9.77786158e-10  4.33675792e-09  1.84746991e-10  4.31615978e-09
  1.45269647e-10  2.13202124e-09  1.73361988e-09  2.29912826e-09
  6.33667640e-12  5.68043246e-10  4.29098761e-10  5.45106818e-10
 -4.14033014e-11 -8.51909663e-10 -6.74499190e-10 -8.84146307e-10
  1.73125440e-11  7.76926236e-10  5.95848087e-10  7.63705800e-10
 -5.75570217e-11 -8.63851812e-11 -8.45757336e-11 -8.65632197e-11
 -7.63816527e-11  1.77307892e-10  9.71245895e-11  1.78758909e-10
 -7.58441705e-11  5.86572964e-10  3.83041049e-10  5.91547590e-10
 -1.25851047e-10  3.38254912e-11 -2.86439537e-11  3.52274671e-11
 -5.16010803e-11 -1.30026948e-11  7.39571185e-11  3.21269277e-11
  7.16899268e-11  2.62251166e-10  9.78875244e-11  3.11866591e-11
 -2.40377859e+00]
supnorm grad right now is: 2.4037785858332925
Weights right now are: 
[-1.67797867e+00 -2.93231843e+00 -3.72474233e+00 -2.73251857e+00
  8.87759484e-01  2.94678216e+00  4.29604846e+00  4.14760171e+00
  4.67528372e-02 -9.36875752e-01  5.95795907e-01 -4.55734394e-01
  1.26155588e-01 -2.27096840e+00  5.31500881e-01 -2.41145588e+00
 -1.10639803e+00 -2.04722757e+00 -4.65538363e-01 -2.23049092e+00
 -1.22600522e+00 -2.39601821e+00 -1.47672712e+00 -2.50357652e+00
  1.39961877e+00  5.89682158e-01  1.75938961e-02  5.99767162e-01
 -1.01114819e+00 -1.47494310e+00 -2.20244740e+00 -1.95712390e+00
  2.12558001e-01  1.00244631e+00  1.36747287e+00  1.13018763e+00
 -9.05975413e-01 -1.77823371e+00 -8.97053580e-01 -2.09208955e+00
  3.85008862e-01  2.72031617e-01  1.36470980e+00  1.65957942e+00
  1.16651447e+00 -1.63904222e+00 -1.57978614e-01 -1.01207922e+00
  4.90912822e-01 -8.82145386e-02 -5.23735985e-01 -7.77341038e-01
 -8.39429778e-01 -2.09513919e+00 -1.23015223e+00 -2.07201063e+00
  1.26015220e+00  1.69685667e+00 -5.85648096e-01 -1.81749817e+00
 -4.05664493e-01 -2.18049795e-01 -6.08190801e-01 -1.98123094e+00
  2.48072887e+01]
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3586.5396835127767
gradient value of function right now is: [ 8.29619659e-08  1.03165113e-07  1.03079218e-07  1.03637216e-07
 -8.29619259e-08 -1.03165063e-07 -1.03079168e-07 -1.03637166e-07
  9.88620458e-09  4.36733770e-08  1.92363574e-09  4.34575179e-08
  1.37523817e-09  6.08418197e-09  2.63912937e-10  6.05474120e-09
  2.02721511e-09  8.95739066e-09  3.93668532e-10  8.91324136e-09
  8.86437228e-10  3.91479981e-09  1.72941066e-10  3.89536082e-09
  1.41510031e-10  1.91284003e-09  1.56467718e-09  2.08735285e-09
  1.76296928e-11  5.23904150e-10  4.06398009e-10  5.28907376e-10
 -4.71089845e-11 -7.68464504e-10 -6.17005702e-10 -8.19170864e-10
  2.95665976e-11  7.13360338e-10  5.59353220e-10  7.31552956e-10
 -4.48010556e-11 -7.59208925e-11 -7.13878458e-11 -7.62356987e-11
 -3.03713373e-11  1.64751507e-10  1.09295777e-10  1.65215699e-10
 -2.00160883e-11  5.27659215e-10  3.67582390e-10  5.31035622e-10
 -8.75145987e-11  3.42156263e-11 -1.05602659e-11  3.48999815e-11
 -4.99153826e-11 -1.70175844e-12  7.25051702e-11  1.05884120e-11
  7.09311220e-11  2.15295584e-10  9.34958544e-11  1.24394140e-11
 -6.15002474e+00]
supnorm grad right now is: 6.150024739974824
Weights right now are: 
[-1.69394512e+00 -2.95219936e+00 -3.74460543e+00 -2.75248905e+00
  9.03725941e-01  2.96666308e+00  4.31591156e+00  4.16757218e+00
  4.48657094e-02 -9.45272746e-01  5.95452218e-01 -4.64093398e-01
  1.25895026e-01 -2.27213063e+00  5.31454561e-01 -2.41261304e+00
 -1.10678516e+00 -2.04895059e+00 -4.65608697e-01 -2.23220616e+00
 -1.22617471e+00 -2.39677191e+00 -1.47675818e+00 -2.50432677e+00
  1.39959514e+00  5.89306639e-01  1.72899059e-02  5.99366484e-01
 -1.01114745e+00 -1.47504020e+00 -2.20251914e+00 -1.95721296e+00
  2.12563600e-01  1.00259367e+00  1.36758827e+00  1.13033707e+00
 -9.05976271e-01 -1.77836738e+00 -8.97154262e-01 -2.09221616e+00
  3.85019887e-01  2.72047102e-01  1.36472539e+00  1.65959492e+00
  1.16653367e+00 -1.63907193e+00 -1.57991810e-01 -1.01210928e+00
  4.90933826e-01 -8.83167852e-02 -5.23798892e-01 -7.77444281e-01
 -8.39403909e-01 -2.09514450e+00 -1.23014474e+00 -2.07201625e+00
  1.26016048e+00  1.69686025e+00 -5.85659914e-01 -1.81750599e+00
 -4.05675545e-01 -2.18099495e-01 -6.08206685e-01 -1.98123823e+00
  2.44805773e+01]
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.9999921728004
gradient value of function right now is: [ 6.60099080e-08  8.17714020e-08  8.16998954e-08  8.21376758e-08
 -6.60098756e-08 -8.17713617e-08 -8.16998552e-08 -8.21376354e-08
  7.78943178e-09  3.46431983e-08  1.40947601e-09  3.44878932e-08
  1.07856638e-09  4.80888048e-09  1.90324921e-10  4.78812143e-09
  1.59595840e-09  7.09966823e-09  2.88115845e-10  7.06794559e-09
  7.01728283e-10  3.11871674e-09  1.27853887e-10  3.10458441e-09
  9.80836214e-11  1.56906988e-09  1.26870923e-09  1.67251568e-09
 -4.66005044e-12  4.03144958e-10  2.96235106e-10  3.66289494e-10
 -2.18417628e-11 -6.07600687e-10 -4.74331687e-10 -6.13401123e-10
  1.76286433e-12  5.56205001e-10  4.17021272e-10  5.22867831e-10
 -4.72112093e-11 -6.54735762e-11 -6.60979477e-11 -6.54771598e-11
 -8.60865268e-11  1.23740086e-10  5.22369439e-11  1.25503674e-10
 -9.29784841e-11  4.24562215e-10  2.58830645e-10  4.29089084e-10
 -1.11089656e-10  2.12900198e-11 -3.29829147e-11  2.28009906e-11
 -3.44113247e-11 -1.78198461e-11  4.86094486e-11  3.88684576e-11
  4.72878504e-11  2.05315938e-10  6.66027485e-11  3.58807129e-11
  1.39888650e+00]
supnorm grad right now is: 1.398886500188608
Weights right now are: 
[-1.70878399e+00 -2.97061507e+00 -3.76300499e+00 -2.77098755e+00
  9.18564809e-01  2.98507880e+00  4.33431112e+00  4.18607069e+00
  4.31127873e-02 -9.53067583e-01  5.95132061e-01 -4.71852890e-01
  1.25652371e-01 -2.27321216e+00  5.31411234e-01 -2.41368983e+00
 -1.10714452e+00 -2.05054890e+00 -4.65674198e-01 -2.23379724e+00
 -1.22633245e+00 -2.39747284e+00 -1.47678718e+00 -2.50502449e+00
  1.39957279e+00  5.88955771e-01  1.70060355e-02  5.98991809e-01
 -1.01114712e+00 -1.47513087e+00 -2.20258630e+00 -1.95729666e+00
  2.12568988e-01  1.00273038e+00  1.36769531e+00  1.13047593e+00
 -9.05977457e-01 -1.77849223e+00 -8.97248446e-01 -2.09233498e+00
  3.85029700e-01  2.72061761e-01  1.36473994e+00  1.65960958e+00
  1.16655131e+00 -1.63909956e+00 -1.58003713e-01 -1.01213733e+00
  4.90951998e-01 -8.84118408e-02 -5.23857392e-01 -7.77540407e-01
 -8.39381232e-01 -2.09514919e+00 -1.23013793e+00 -2.07202129e+00
  1.26016757e+00  1.69686493e+00 -5.85669944e-01 -1.81751534e+00
 -4.05685801e-01 -2.18142597e-01 -6.08220952e-01 -1.98124699e+00
  2.51093243e+01]
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.9826951359587
gradient value of function right now is: [ 6.08961623e-08  7.52539961e-08  7.51907653e-08  7.55926160e-08
 -6.08963689e-08 -7.52542519e-08 -7.51910210e-08 -7.55928731e-08
  7.20682973e-09  3.19426855e-08  1.33943217e-09  3.17940407e-08
  1.00236703e-09  4.45205089e-09  1.82517816e-10  4.43196424e-09
  1.47592094e-09  6.54316263e-09  2.73726033e-10  6.51280554e-09
  6.49602382e-10  2.87763831e-09  1.21369519e-10  2.86413447e-09
  9.62905951e-11  1.44153025e-09  1.17071606e-09  1.55026016e-09
  2.13671511e-12  3.78220678e-10  2.83851348e-10  3.58296152e-10
 -2.54566596e-11 -5.60157500e-10 -4.41996620e-10 -5.77427703e-10
  9.18845625e-12  5.19994325e-10  3.96688132e-10  5.05787662e-10
 -3.98779175e-11 -5.92524968e-11 -5.82935925e-11 -5.93500777e-11
 -5.81257132e-11  1.16929396e-10  6.04044877e-11  1.18053520e-10
 -5.93486296e-11  3.90849353e-10  2.51000353e-10  3.94371126e-10
 -8.87778955e-11  2.18328353e-11 -2.20088861e-11  2.28791353e-11
 -3.37083479e-11 -1.05268154e-11  4.82239496e-11  2.49111670e-11
  4.68694623e-11  1.79040547e-10  6.44776142e-11  2.37865530e-11
 -1.54349053e+00]
supnorm grad right now is: 1.5434905299949813
Weights right now are: 
[-1.72137242e+00 -2.98619253e+00 -3.77856908e+00 -2.78663495e+00
  9.31153243e-01  3.00065626e+00  4.34987522e+00  4.20171809e+00
  4.16250289e-02 -9.59672923e-01  5.94859270e-01 -4.78428128e-01
  1.25445969e-01 -2.27413071e+00  5.31374268e-01 -2.41460433e+00
 -1.10744924e+00 -2.05190214e+00 -4.65729945e-01 -2.23514434e+00
 -1.22646648e+00 -2.39806756e+00 -1.47681191e+00 -2.50561648e+00
  1.39955346e+00  5.88657446e-01  1.67643315e-02  5.98672154e-01
 -1.01114673e+00 -1.47520856e+00 -2.20264382e+00 -1.95736848e+00
  2.12573633e-01  1.00284643e+00  1.36778627e+00  1.13059423e+00
 -9.05978392e-01 -1.77859915e+00 -8.97329110e-01 -2.09243694e+00
  3.85038625e-01  2.72074088e-01  1.36475238e+00  1.65962191e+00
  1.16656599e+00 -1.63912389e+00 -1.58015072e-01 -1.01216197e+00
  4.90967350e-01 -8.84929049e-02 -5.23908174e-01 -7.77622296e-01
 -8.39360828e-01 -2.09515352e+00 -1.23013230e+00 -2.07202589e+00
  1.26017442e+00  1.69686794e+00 -5.85679677e-01 -1.81752208e+00
 -4.05695044e-01 -2.18181237e-01 -6.08234182e-01 -1.98125326e+00
  2.48511899e+01]
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3586.0839894554106
gradient value of function right now is: [ 4.87398206e-08  6.00345719e-08  5.99806228e-08  6.02978790e-08
 -4.87397207e-08 -6.00344486e-08 -5.99804996e-08 -6.02977551e-08
  5.69803593e-09  2.54761817e-08  9.61788358e-10  2.53720871e-08
  7.87421179e-10  3.53265850e-09  1.28078887e-10  3.51900520e-09
  1.16653753e-09  5.21687715e-09  1.96429705e-10  5.19563530e-09
  5.16402151e-10  2.30630718e-09  8.81856758e-11  2.29671530e-09
  6.32420217e-11  1.19492100e-09  9.56984819e-10  1.24837120e-09
 -1.52337293e-11  2.89208909e-10  2.01813660e-10  2.36064203e-10
 -6.19354609e-12 -4.44777362e-10 -3.38716360e-10 -4.27128645e-10
 -1.24845438e-11  4.04379100e-10  2.90916446e-10  3.49246347e-10
 -4.17856218e-11 -5.20670604e-11 -5.48907408e-11 -5.19121947e-11
 -1.03106395e-10  8.59723059e-11  1.57908702e-11  8.82055134e-11
 -1.17514780e-10  3.16338699e-10  1.69512465e-10  3.20909626e-10
 -1.07516806e-10  1.16361160e-11 -3.98948496e-11  1.33902520e-11
 -2.16389746e-11 -2.40452339e-11  2.94964808e-11  4.89405738e-11
  2.93205416e-11  1.72366001e-10  4.35385721e-11  4.39398928e-11
  7.80119087e+00]
supnorm grad right now is: 7.801190868578369
Weights right now are: 
[-1.73327919e+00 -3.00089261e+00 -3.79325710e+00 -2.80140142e+00
  9.43060015e-01  3.01535635e+00  4.36456324e+00  4.21648456e+00
  4.02129455e-02 -9.65917957e-01  5.94592218e-01 -4.84643360e-01
  1.25249054e-01 -2.27500312e+00  5.31337670e-01 -2.41547270e+00
 -1.10773839e+00 -2.05318120e+00 -4.65784519e-01 -2.23641732e+00
 -1.22659377e+00 -2.39863023e+00 -1.47683608e+00 -2.50617645e+00
  1.39953394e+00  5.88376651e-01  1.65356052e-02  5.98368448e-01
 -1.01114800e+00 -1.47528321e+00 -2.20270069e+00 -1.95744120e+00
  2.12579289e-01  1.00295589e+00  1.36787330e+00  1.13070864e+00
 -9.05981191e-01 -1.77870155e+00 -8.97408201e-01 -2.09253886e+00
  3.85045842e-01  2.72085537e-01  1.36476333e+00  1.65963339e+00
  1.16657454e+00 -1.63914694e+00 -1.58028523e-01 -1.01218518e+00
  4.90975318e-01 -8.85690728e-02 -5.23958985e-01 -7.77699084e-01
 -8.39345530e-01 -2.09515799e+00 -1.23012934e+00 -2.07203053e+00
  1.26018130e+00  1.69686931e+00 -5.85689606e-01 -1.81752568e+00
 -4.05704600e-01 -2.18215085e-01 -6.08247158e-01 -1.98125690e+00
  2.55151546e+01]
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.0137447734883
gradient value of function right now is: [ 4.94237081e-08  6.07928700e-08  6.07429768e-08  6.10649896e-08
 -4.94236584e-08 -6.07928087e-08 -6.07429156e-08 -6.10649280e-08
  5.83879948e-09  2.58607405e-08  1.07960362e-09  2.57410423e-08
  8.14785213e-10  3.61642388e-09  1.47537663e-10  3.60020451e-09
  1.19509445e-09  5.29434335e-09  2.20527406e-10  5.26990803e-09
  5.28050984e-10  2.33744451e-09  9.81852384e-11  2.32652908e-09
  7.83255566e-11  1.18184041e-09  9.59376461e-10  1.26962204e-09
  1.07005199e-12  3.07809550e-10  2.30421241e-10  2.90102595e-10
 -1.99987569e-11 -4.52576633e-10 -3.56634099e-10 -4.65269449e-10
  6.72856640e-12  4.24266530e-10  3.22995826e-10  4.10962083e-10
 -3.29514942e-11 -4.89162662e-11 -4.81705404e-11 -4.89898945e-11
 -4.96453234e-11  9.46741174e-11  4.77127970e-11  9.56387888e-11
 -5.11343296e-11  3.17961082e-10  2.02844537e-10  3.20892707e-10
 -7.38305864e-11  1.75261449e-11 -1.88509083e-11  1.84140581e-11
 -2.72133269e-11 -9.14329582e-12  3.89140949e-11  2.13974209e-11
  3.79247834e-11  1.47121224e-10  5.22555736e-11  2.03265110e-11
 -1.21800143e+00]
supnorm grad right now is: 1.218001427860795
Weights right now are: 
[-1.74359684e+00 -3.01359808e+00 -3.80595203e+00 -2.81416376e+00
  9.53377669e-01  3.02806182e+00  4.37725817e+00  4.22924690e+00
  3.89945263e-02 -9.71319246e-01  5.94368163e-01 -4.90019789e-01
  1.25079233e-01 -2.27575752e+00  5.31307070e-01 -2.41622373e+00
 -1.10798785e+00 -2.05428726e+00 -4.65830315e-01 -2.23751830e+00
 -1.22670391e+00 -2.39911820e+00 -1.47685645e+00 -2.50666215e+00
  1.39951788e+00  5.88129746e-01  1.63353617e-02  5.98104000e-01
 -1.01114823e+00 -1.47534701e+00 -2.20274841e+00 -1.95750103e+00
  2.12583339e-01  1.00305028e+00  1.36794757e+00  1.13080522e+00
 -9.05982569e-01 -1.77878960e+00 -8.97475173e-01 -2.09262376e+00
  3.85052398e-01  2.72095858e-01  1.36477341e+00  1.65964371e+00
  1.16658561e+00 -1.63916619e+00 -1.58037630e-01 -1.01220471e+00
  4.90986402e-01 -8.86350978e-02 -5.24000734e-01 -7.77765804e-01
 -8.39330824e-01 -2.09516129e+00 -1.23012524e+00 -2.07203406e+00
  1.26018678e+00  1.69687187e+00 -5.85697347e-01 -1.81753166e+00
 -4.05713283e-01 -2.18243320e-01 -6.08257646e-01 -1.98126245e+00
  2.49241466e+01]
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3586.536653474285
gradient value of function right now is: [ 4.82316719e-08  5.92401458e-08  5.91942936e-08  5.95081219e-08
 -4.82317979e-08 -5.92403008e-08 -5.91944485e-08 -5.95082777e-08
  5.72954577e-09  2.52412571e-08  1.10962363e-09  2.51167626e-08
  8.04342907e-10  3.54858773e-09  1.53659368e-10  3.53144761e-09
  1.17259777e-09  5.16683479e-09  2.26686631e-10  5.14141436e-09
  5.17875472e-10  2.28076681e-09  1.00586128e-10  2.26946151e-09
  8.42269975e-11  1.13884343e-09  9.31737122e-10  1.24271435e-09
  1.02984074e-11  3.08279778e-10  2.39122217e-10  3.11077735e-10
 -2.69972647e-11 -4.41968790e-10 -3.54846252e-10 -4.70943519e-10
  1.74218057e-11  4.21958330e-10  3.30876062e-10  4.32593290e-10
 -2.65841770e-11 -4.57981040e-11 -4.28949027e-11 -4.59893136e-11
 -1.82677086e-11  9.59728896e-11  6.34678191e-11  9.62426591e-11
 -1.22057439e-11  3.08476330e-10  2.14723953e-10  3.10452210e-10
 -5.16947988e-11  1.99808363e-11 -6.40154373e-12  2.03846127e-11
 -2.91734317e-11 -9.62860945e-13  4.24739723e-11  6.24205144e-12
  4.17701672e-11  1.26842925e-10  5.49339992e-11  7.32982975e-12
 -6.15755985e+00]
supnorm grad right now is: 6.157559846870127
Weights right now are: 
[-1.75274906e+00 -3.02484433e+00 -3.81718892e+00 -2.82546006e+00
  9.62529884e-01  3.03930807e+00  4.38849506e+00  4.24054320e+00
  3.79162779e-02 -9.76103447e-01  5.94172783e-01 -4.94782402e-01
  1.24928918e-01 -2.27642605e+00  5.31280469e-01 -2.41688935e+00
 -1.10820855e+00 -2.05526670e+00 -4.65870225e-01 -2.23849333e+00
 -1.22680157e+00 -2.39955122e+00 -1.47687427e+00 -2.50709319e+00
  1.39950396e+00  5.87908970e-01  1.61566233e-02  5.97868462e-01
 -1.01114782e+00 -1.47540339e+00 -2.20279006e+00 -1.95755271e+00
  2.12586506e-01  1.00313382e+00  1.36801296e+00  1.13088985e+00
 -9.05983097e-01 -1.77886765e+00 -8.97533963e-01 -2.09269766e+00
  3.85058769e-01  2.72105096e-01  1.36478265e+00  1.65965296e+00
  1.16659691e+00 -1.63918322e+00 -1.58045070e-01 -1.01222195e+00
  4.90998687e-01 -8.86940013e-02 -5.24036925e-01 -7.77825298e-01
 -8.39315909e-01 -2.09516431e+00 -1.23012085e+00 -2.07203728e+00
  1.26019169e+00  1.69687404e+00 -5.85704356e-01 -1.81753641e+00
 -4.05719631e-01 -2.18271718e-01 -6.08267171e-01 -1.98126684e+00
  2.45038372e+01]
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.820293639685
gradient value of function right now is: [ 3.98842816e-08  4.88626521e-08  4.88220372e-08  4.90785551e-08
 -3.98844911e-08 -4.88629093e-08 -4.88222943e-08 -4.90788136e-08
  4.68374868e-09  2.08043483e-08  8.32873506e-10  2.07128968e-08
  6.53486753e-10  2.91029489e-09  1.13114132e-10  2.89800911e-09
  9.58508213e-10  4.25839843e-09  1.70098818e-10  4.23973311e-09
  4.25445061e-10  1.88821905e-09  7.62695931e-11  1.87981416e-09
  5.89157761e-11  9.72605789e-10  7.85060746e-10  1.03259632e-09
 -4.82769313e-12  2.44536805e-10  1.77927546e-10  2.17714075e-10
 -1.13795393e-11 -3.63011393e-10 -2.81990200e-10 -3.62772844e-10
 -1.22458870e-12  3.39726171e-10  2.52717261e-10  3.14229214e-10
 -3.00100240e-11 -4.13476975e-11 -4.19053714e-11 -4.13299985e-11
 -5.91433196e-11  7.39317821e-11  2.76143407e-11  7.51547616e-11
 -6.48300652e-11  2.58039789e-10  1.53342878e-10  2.60989689e-10
 -7.20365004e-11  1.22216818e-11 -2.27588709e-11  1.32478765e-11
 -2.02453680e-11 -1.25734107e-11  2.84778012e-11  2.69949720e-11
  2.79751991e-11  1.28739361e-10  3.96368186e-11  2.47218699e-11
  2.70925879e+00]
supnorm grad right now is: 2.709258793328768
Weights right now are: 
[-1.76172146e+00 -3.03585017e+00 -3.82818581e+00 -2.83651493e+00
  9.71502286e-01  3.05031392e+00  4.39949196e+00  4.25159808e+00
  3.68580128e-02 -9.80789858e-01  5.93979173e-01 -4.99447362e-01
  1.24781027e-01 -2.27708245e+00  5.31254019e-01 -2.41754284e+00
 -1.10842509e+00 -2.05622587e+00 -4.65909755e-01 -2.23944812e+00
 -1.22689750e+00 -2.39997576e+00 -1.47689192e+00 -2.50751577e+00
  1.39949001e+00  5.87692637e-01  1.59810638e-02  5.97636668e-01
 -1.01114766e+00 -1.47545912e+00 -2.20283154e+00 -1.95760460e+00
  2.12589851e-01  1.00321562e+00  1.36807730e+00  1.13097349e+00
 -9.05983911e-01 -1.77894472e+00 -8.97592375e-01 -2.09277158e+00
  3.85065027e-01  2.72114013e-01  1.36479159e+00  1.65966189e+00
  1.16660655e+00 -1.63920027e+00 -1.58053289e-01 -1.01223915e+00
  4.91009262e-01 -8.87518380e-02 -5.24073352e-01 -7.77883631e-01
 -8.39301488e-01 -2.09516754e+00 -1.23011695e+00 -2.07204065e+00
  1.26019627e+00  1.69687553e+00 -5.85710980e-01 -1.81753986e+00
 -4.05726084e-01 -2.18299886e-01 -6.08276202e-01 -1.98127011e+00
  2.51866464e+01]
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3586.547717370617
gradient value of function right now is: [ 4.12802433e-08  5.05285503e-08  5.04902606e-08  5.07563437e-08
 -4.12803037e-08 -5.05286242e-08 -5.04903345e-08 -5.07564180e-08
  4.89724572e-09  2.15577988e-08  9.46914283e-10  2.14515734e-08
  6.89549777e-10  3.03976131e-09  1.31519547e-10  3.02509254e-09
  1.00215539e-09  4.41234599e-09  1.93436639e-10  4.39065649e-09
  4.43853070e-10  1.95322861e-09  8.60777071e-11  1.94355492e-09
  7.25397224e-11  9.81425465e-10  8.02956376e-10  1.07084611e-09
  8.77732488e-12  2.64659478e-10  2.05242183e-10  2.66925515e-10
 -2.29624177e-11 -3.76886542e-10 -3.02557305e-10 -4.01469820e-10
  1.49105958e-11  3.62820053e-10  2.84457917e-10  3.71816549e-10
 -2.29149791e-11 -3.96313131e-11 -3.70880030e-11 -3.97964215e-11
 -1.59364533e-11  8.21405098e-11  5.42000553e-11  8.23743222e-11
 -1.07735108e-11  2.64361268e-10  1.83890828e-10  2.66058887e-10
 -4.45445236e-11  1.71044552e-11 -5.60369603e-12  1.74536791e-11
 -2.49945120e-11 -8.51417433e-13  3.64137888e-11  5.42933521e-12
  3.58648304e-11  1.09030275e-10  4.71480439e-11  6.35066101e-12
 -6.13503448e+00]
supnorm grad right now is: 6.135034484357166
Weights right now are: 
[-1.76965038e+00 -3.04555937e+00 -3.83788720e+00 -2.84626733e+00
  9.79431206e-01  3.06002312e+00  4.40919334e+00  4.26135047e+00
  3.59232783e-02 -9.84926700e-01  5.93808501e-01 -5.03565184e-01
  1.24650138e-01 -2.27766291e+00  5.31230601e-01 -2.41812071e+00
 -1.10861640e+00 -2.05707268e+00 -4.65944624e-01 -2.24029104e+00
 -1.22698236e+00 -2.40035113e+00 -1.47690751e+00 -2.50788940e+00
  1.39947781e+00  5.87500357e-01  1.58249299e-02  5.97431115e-01
 -1.01114775e+00 -1.47550811e+00 -2.20286828e+00 -1.95765057e+00
  2.12592838e-01  1.00328746e+00  1.36813394e+00  1.13104696e+00
 -9.05984877e-01 -1.77901265e+00 -8.97644193e-01 -2.09283713e+00
  3.85070022e-01  2.72122012e-01  1.36479934e+00  1.65966991e+00
  1.16661479e+00 -1.63921462e+00 -1.58060185e-01 -1.01225366e+00
  4.91017959e-01 -8.88024924e-02 -5.24105496e-01 -7.77934746e-01
 -8.39290064e-01 -2.09517012e+00 -1.23011388e+00 -2.07204336e+00
  1.26020017e+00  1.69687710e+00 -5.85716539e-01 -1.81754326e+00
 -4.05731401e-01 -2.18324528e-01 -6.08283633e-01 -1.98127343e+00
  2.44857582e+01]
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.9245561307694
gradient value of function right now is: [ 3.49941029e-08  4.27439283e-08  4.27092299e-08  4.29325237e-08
 -3.49940240e-08 -4.27438317e-08 -4.27091334e-08 -4.29324266e-08
  4.10749325e-09  1.82202362e-08  7.34065267e-10  1.81395069e-08
  5.75005627e-10  2.55707421e-09  1.00147560e-10  2.54617435e-09
  8.40594624e-10  3.72948369e-09  1.49939591e-10  3.71300369e-09
  3.73952525e-10  1.65750123e-09  6.73540173e-11  1.65006797e-09
  5.26918995e-11  8.56827555e-10  6.92282681e-10  9.11411813e-10
 -3.42701798e-12  2.15819271e-10  1.57789108e-10  1.94002896e-10
 -1.06015707e-11 -3.17522052e-10 -2.47228071e-10 -3.18752275e-10
 -1.10797087e-13  2.99933952e-10  2.23991942e-10  2.79582360e-10
 -2.59178126e-11 -3.64154143e-11 -3.66575186e-11 -3.64130599e-11
 -4.93538989e-11  6.51832753e-11  2.56477130e-11  6.61973542e-11
 -5.36605434e-11  2.26463506e-10  1.36103022e-10  2.28976139e-10
 -6.15660618e-11  1.09895118e-11 -1.89845483e-11  1.18492425e-11
 -1.79766814e-11 -1.03569624e-11  2.53740956e-11  2.24118051e-11
  2.49384563e-11  1.11851165e-10  3.51718218e-11  2.06182637e-11
  2.03719704e+00]
supnorm grad right now is: 2.0371970375056145
Weights right now are: 
[-1.77684838e+00 -3.05436054e+00 -3.84668136e+00 -2.85510763e+00
  9.86629206e-01  3.06882429e+00  4.41798751e+00  4.27019078e+00
  3.50755999e-02 -9.88679160e-01  5.93652423e-01 -5.07300379e-01
  1.24531251e-01 -2.27819029e+00  5.31209157e-01 -2.41864574e+00
 -1.10878990e+00 -2.05784086e+00 -4.65976517e-01 -2.24105570e+00
 -1.22705941e+00 -2.40069198e+00 -1.47692178e+00 -2.50822866e+00
  1.39946627e+00  5.87325981e-01  1.56836178e-02  5.97244067e-01
 -1.01114791e+00 -1.47555312e+00 -2.20290190e+00 -1.95769283e+00
  2.12595698e-01  1.00335298e+00  1.36818545e+00  1.13111409e+00
 -9.05985856e-01 -1.77907485e+00 -8.97691440e-01 -2.09289717e+00
  3.85074777e-01  2.72129388e-01  1.36480650e+00  1.65967729e+00
  1.16662249e+00 -1.63922828e+00 -1.58066716e-01 -1.01226749e+00
  4.91025670e-01 -8.88489380e-02 -5.24134680e-01 -7.77981676e-01
 -8.39279433e-01 -2.09517255e+00 -1.23011102e+00 -2.07204594e+00
  1.26020408e+00  1.69687882e+00 -5.85722090e-01 -1.81754708e+00
 -4.05737170e-01 -2.18345330e-01 -6.08291203e-01 -1.98127711e+00
  2.51544896e+01]
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.0633096758556
gradient value of function right now is: [ 3.34871010e-08  4.08494012e-08  4.08171545e-08  4.10302798e-08
 -3.34871243e-08 -4.08494297e-08 -4.08171829e-08 -4.10303084e-08
  3.93790044e-09  1.74266563e-08  7.17666428e-10  1.73473184e-08
  5.53029146e-10  2.45283063e-09  9.85511017e-11  2.44203424e-09
  8.05922142e-10  3.56718112e-09  1.46607260e-10  3.55098253e-09
  3.58678871e-10  1.58622758e-09  6.57964567e-11  1.57893165e-09
  5.27306505e-11  8.16965872e-10  6.62138895e-10  8.74520738e-10
 -7.29605048e-13  2.08941947e-10  1.55111684e-10  1.93636015e-10
 -1.22088347e-11 -3.03684372e-10 -2.38277594e-10 -3.09490585e-10
  2.90395062e-12  2.89601834e-10  2.18988646e-10  2.76711181e-10
 -2.33810928e-11 -3.43547331e-11 -3.40115746e-11 -3.43887472e-11
 -3.88080537e-11  6.34894101e-11  2.94161512e-11  6.42558340e-11
 -4.09468984e-11  2.16316714e-10  1.35050049e-10  2.18455787e-10
 -5.35287283e-11  1.13987636e-11 -1.48399777e-11  1.20828411e-11
 -1.80700909e-11 -7.50107041e-12  2.57631729e-11  1.70276381e-11
  2.52387548e-11  1.02839867e-10  3.50069511e-11  1.59367619e-11
  3.12346966e-02]
supnorm grad right now is: 0.031234696642583132
Weights right now are: 
[-1.78429781e+00 -3.06345689e+00 -3.85577065e+00 -2.86424452e+00
  9.94078638e-01  3.07792063e+00  4.42707680e+00  4.27932767e+00
  3.41967816e-02 -9.92560261e-01  5.93488648e-01 -5.11163273e-01
  1.24407681e-01 -2.27873706e+00  5.31186554e-01 -2.41919002e+00
 -1.10896975e+00 -2.05863528e+00 -4.66009978e-01 -2.24184640e+00
 -1.22713933e+00 -2.40104472e+00 -1.47693675e+00 -2.50857974e+00
  1.39945402e+00  5.87145869e-01  1.55370858e-02  5.97049910e-01
 -1.01114843e+00 -1.47560007e+00 -2.20293735e+00 -1.95773780e+00
  2.12598958e-01  1.00342068e+00  1.36823903e+00  1.13118423e+00
 -9.05987302e-01 -1.77913968e+00 -8.97741144e-01 -2.09296080e+00
  3.85079516e-01  2.72136849e-01  1.36481377e+00  1.65968476e+00
  1.16662905e+00 -1.63924269e+00 -1.58074383e-01 -1.01228204e+00
  4.91032093e-01 -8.88968439e-02 -5.24165829e-01 -7.78029999e-01
 -8.39269133e-01 -2.09517525e+00 -1.23010850e+00 -2.07204878e+00
  1.26020831e+00  1.69688011e+00 -5.85728139e-01 -1.81755022e+00
 -4.05743019e-01 -2.18366553e-01 -6.08299365e-01 -1.98127993e+00
  2.49730375e+01]
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.006702947127
gradient value of function right now is: [ 3.20220678e-08  3.90148254e-08  3.89846229e-08  3.91878615e-08
 -3.20222012e-08 -3.90149882e-08 -3.89847856e-08 -3.91880251e-08
  3.76886339e-09  1.66536624e-08  6.94521011e-10  1.65766647e-08
  5.30519301e-10  2.34908839e-09  9.57691441e-11  2.33855943e-09
  7.71379741e-10  3.40917092e-09  1.41894858e-10  3.39344797e-09
  3.43518732e-10  1.51701065e-09  6.36740701e-11  1.50993170e-09
  5.17753772e-11  7.80233044e-10  6.33527719e-10  8.38350926e-10
  7.18287063e-13  2.01250501e-10  1.50699967e-10  1.89737974e-10
 -1.28132389e-11 -2.90155296e-10 -2.28674472e-10 -2.98295969e-10
  4.45453198e-12  2.78559642e-10  2.12142618e-10  2.69931687e-10
 -2.15952399e-11 -3.26221624e-11 -3.19615836e-11 -3.26759464e-11
 -3.24009906e-11  6.13629577e-11  3.08648463e-11  6.19841312e-11
 -3.33081605e-11  2.06683878e-10  1.31869974e-10  2.08583893e-10
 -4.82275740e-11  1.14005519e-11 -1.23274172e-11  1.19767414e-11
 -1.77030922e-11 -5.86565867e-12  2.53731005e-11  1.38401048e-11
  2.48380969e-11  9.61921924e-11  3.41562644e-11  1.31651484e-11
 -1.29553940e+00]
supnorm grad right now is: 1.295539404198678
Weights right now are: 
[-1.79095312e+00 -3.07157217e+00 -3.86387969e+00 -2.87239593e+00
  1.00073394e+00  3.08603590e+00  4.43518583e+00  4.28747907e+00
  3.34121814e-02 -9.96024036e-01  5.93342760e-01 -5.14610707e-01
  1.24297135e-01 -2.27922588e+00  5.31166315e-01 -2.41967658e+00
 -1.10913039e+00 -2.05934455e+00 -4.66039817e-01 -2.24255234e+00
 -1.22721080e+00 -2.40136007e+00 -1.47695011e+00 -2.50889359e+00
  1.39944324e+00  5.86983944e-01  1.54053543e-02  5.96875947e-01
 -1.01114919e+00 -1.47564166e+00 -2.20296908e+00 -1.95777808e+00
  2.12601903e-01  1.00348070e+00  1.36828668e+00  1.13124642e+00
 -9.05988890e-01 -1.77919731e+00 -8.97785693e-01 -2.09301780e+00
  3.85083084e-01  2.72143679e-01  1.36482010e+00  1.65969160e+00
  1.16663463e+00 -1.63925467e+00 -1.58080717e-01 -1.01229420e+00
  4.91037053e-01 -8.89390458e-02 -5.24193653e-01 -7.78072640e-01
 -8.39261567e-01 -2.09517729e+00 -1.23010671e+00 -2.07205096e+00
  1.26021178e+00  1.69688164e+00 -5.85733107e-01 -1.81755376e+00
 -4.05748895e-01 -2.18383539e-01 -6.08306136e-01 -1.98128337e+00
  2.48998222e+01]
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.925554685684
gradient value of function right now is: [ 2.90796632e-08  3.53784001e-08  3.53503751e-08  3.55338791e-08
 -2.90793622e-08 -3.53780331e-08 -3.53500084e-08 -3.55335105e-08
  3.40696167e-09  1.50976318e-08  6.07903803e-10  1.50307805e-08
  4.78817869e-10  2.12713974e-09  8.32750765e-11  2.11807604e-09
  6.97411782e-10  3.09107795e-09  1.24218185e-10  3.07742495e-09
  3.11288820e-10  1.37835284e-09  5.59861375e-11  1.37217412e-09
  4.41514021e-11  7.17736460e-10  5.79956982e-10  7.63492961e-10
 -2.85859544e-12  1.79990282e-10  1.31604456e-10  1.61797141e-10
 -8.75242669e-12 -2.62572842e-10 -2.04445153e-10 -2.63561897e-10
 -8.73343239e-14  2.50600156e-10  1.87169308e-10  2.33610425e-10
 -2.16225549e-11 -3.06376766e-11 -3.07623327e-11 -3.06378088e-11
 -4.11716430e-11  5.41362804e-11  2.12326358e-11  5.49802115e-11
 -4.47280709e-11  1.88400450e-10  1.13203313e-10  1.90491769e-10
 -5.13256762e-11  9.13252047e-12 -1.58433716e-11  9.84927270e-12
 -1.49565381e-11 -8.61111159e-12  2.11303766e-11  1.86751559e-11
  2.08371664e-11  9.32515565e-11  2.93274701e-11  1.71847694e-11
  2.02145657e+00]
supnorm grad right now is: 2.0214565667804685
Weights right now are: 
[-1.79705154e+00 -3.07899668e+00 -3.87129833e+00 -2.87985313e+00
  1.00683236e+00  3.09346042e+00  4.44260446e+00  4.29493626e+00
  3.26969112e-02 -9.99192259e-01  5.93214420e-01 -5.17764863e-01
  1.24196683e-01 -2.27967200e+00  5.31148777e-01 -2.42012080e+00
 -1.10927676e+00 -2.05999300e+00 -4.66066018e-01 -2.24319791e+00
 -1.22727607e+00 -2.40164899e+00 -1.47696191e+00 -2.50918121e+00
  1.39943382e+00  5.86834183e-01  1.52842429e-02  5.96716044e-01
 -1.01114849e+00 -1.47567972e+00 -2.20299681e+00 -1.95781226e+00
  2.12603781e-01  1.00353604e+00  1.36832976e+00  1.13130217e+00
 -9.05988783e-01 -1.77925015e+00 -8.97825062e-01 -2.09306707e+00
  3.85088012e-01  2.72149968e-01  1.36482660e+00  1.65969789e+00
  1.16664306e+00 -1.63926660e+00 -1.58085763e-01 -1.01230626e+00
  4.91046507e-01 -8.89788889e-02 -5.24217732e-01 -7.78112872e-01
 -8.39249927e-01 -2.09517948e+00 -1.23010329e+00 -2.07205327e+00
  1.26021516e+00  1.69688298e+00 -5.85737929e-01 -1.81755690e+00
 -4.05753357e-01 -2.18403739e-01 -6.08312670e-01 -1.98128621e+00
  2.50839773e+01]
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3586.953341290793
gradient value of function right now is: [ 3.01032601e-08  3.66036812e-08  3.65768556e-08  3.67672531e-08
 -3.01032331e-08 -3.66036483e-08 -3.65768228e-08 -3.67672201e-08
  3.55649784e-09  1.56446398e-08  6.80144821e-10  1.55685194e-08
  5.03552421e-10  2.21847606e-09  9.48815925e-11  2.20792287e-09
  7.28041840e-10  3.20314636e-09  1.39000038e-10  3.18759666e-09
  3.24334649e-10  1.42616735e-09  6.22494318e-11  1.41918636e-09
  5.27449407e-11  7.27338684e-10  5.94298466e-10  7.91398177e-10
  5.32576620e-12  1.93336169e-10  1.48960385e-10  1.92600048e-10
 -1.57306949e-11 -2.72577022e-10 -2.18066739e-10 -2.88434074e-10
  9.67242899e-12  2.66203484e-10  2.07587621e-10  2.70014078e-10
 -1.75977801e-11 -2.97577686e-11 -2.80744571e-11 -2.98679422e-11
 -1.53026140e-11  5.95547621e-11  3.75446270e-11  5.97946752e-11
 -1.24404184e-11  1.93463688e-10  1.32404270e-10  1.94797968e-10
 -3.52538816e-11  1.22025437e-11 -5.61442325e-12  1.25103723e-11
 -1.79762242e-11 -1.47209494e-12  2.61567446e-11  5.56865899e-12
  2.56899631e-11  8.21903278e-11  3.41545773e-11  5.98029766e-12
 -5.21581621e+00]
supnorm grad right now is: 5.215816212123595
Weights right now are: 
[-1.80285539e+00 -3.08605705e+00 -3.87835340e+00 -2.88694491e+00
  1.01263621e+00  3.10052079e+00  4.44965953e+00  4.30202805e+00
  3.20126352e-02 -1.00220835e+00  5.93086051e-01 -5.20766638e-01
  1.24100030e-01 -2.28009879e+00  5.31130956e-01 -2.42054562e+00
 -1.10941683e+00 -2.06061049e+00 -4.66092255e-01 -2.24381248e+00
 -1.22733849e+00 -2.40192397e+00 -1.47697368e+00 -2.50945488e+00
  1.39942406e+00  5.86693148e-01  1.51692927e-02  5.96563623e-01
 -1.01114912e+00 -1.47571658e+00 -2.20302483e+00 -1.95784798e+00
  2.12606474e-01  1.00358854e+00  1.36837147e+00  1.13135690e+00
 -9.05990170e-01 -1.77930108e+00 -8.97864330e-01 -2.09311754e+00
  3.85091605e-01  2.72155783e-01  1.36483224e+00  1.65970371e+00
  1.16664752e+00 -1.63927789e+00 -1.58092113e-01 -1.01231762e+00
  4.91050800e-01 -8.90161869e-02 -5.24242364e-01 -7.78150466e-01
 -8.39242311e-01 -2.09518166e+00 -1.23010155e+00 -2.07205555e+00
  1.26021866e+00  1.69688362e+00 -5.85742995e-01 -1.81755889e+00
 -4.05758367e-01 -2.18419156e-01 -6.08319277e-01 -1.98128798e+00
  2.46202586e+01]
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.046008352231
gradient value of function right now is: [ 2.65575957e-08  3.22404407e-08  3.22155574e-08  3.23822407e-08
 -2.65574484e-08 -3.22402615e-08 -3.22153783e-08 -3.23820607e-08
  3.11264199e-09  1.37686136e-08  5.61813216e-10  1.37066391e-08
  4.38954837e-10  1.94619999e-09  7.73853974e-11  1.93774422e-09
  6.37318977e-10  2.81964358e-09  1.14836643e-10  2.80698205e-09
  2.84866156e-10  1.25917579e-09  5.17881039e-11  1.25344416e-09
  4.15760385e-11  6.55833449e-10  5.30961966e-10  7.00375609e-10
 -1.38327993e-12  1.65766950e-10  1.22338574e-10  1.51826383e-10
 -8.96765626e-12 -2.39291515e-10 -1.87190533e-10 -2.42416408e-10
  1.37844006e-12  2.30560241e-10  1.73515158e-10  2.18212903e-10
 -1.91110777e-11 -2.78579624e-11 -2.76827979e-11 -2.78759501e-11
 -3.35747576e-11  5.00061651e-11  2.17462632e-11  5.06774047e-11
 -3.58596269e-11  1.71944353e-10  1.05746809e-10  1.73724948e-10
 -4.43624478e-11  8.78091087e-12 -1.28927972e-11  9.36910693e-12
 -1.40899600e-11 -6.69929909e-12  2.00400589e-11  1.49191849e-11
  1.96908269e-11  8.33119184e-11  2.74933559e-11  1.38491382e-11
  7.35391043e-01]
supnorm grad right now is: 0.735391042943542
Weights right now are: 
[-1.80867811e+00 -3.09313058e+00 -3.88542149e+00 -2.89404963e+00
  1.01845893e+00  3.10759431e+00  4.45672762e+00  4.30913277e+00
  3.13290852e-02 -1.00522929e+00  5.92961575e-01 -5.23773867e-01
  1.24003659e-01 -2.28052571e+00  5.31113822e-01 -2.42097067e+00
 -1.10955674e+00 -2.06122895e+00 -4.66117681e-01 -2.24442814e+00
 -1.22740097e+00 -2.40219991e+00 -1.47698513e+00 -2.50972955e+00
  1.39941475e+00  5.86550173e-01  1.50533717e-02  5.96410246e-01
 -1.01114885e+00 -1.47575323e+00 -2.20305190e+00 -1.95788178e+00
  2.12608552e-01  1.00364122e+00  1.36841275e+00  1.13141061e+00
 -9.05990528e-01 -1.77935191e+00 -8.97902615e-01 -2.09316595e+00
  3.85096003e-01  2.72161753e-01  1.36483830e+00  1.65970969e+00
  1.16665433e+00 -1.63928935e+00 -1.58097460e-01 -1.01232920e+00
  4.91058152e-01 -8.90541081e-02 -5.24265978e-01 -7.78188742e-01
 -8.39232225e-01 -2.09518381e+00 -1.23009874e+00 -2.07205781e+00
  1.26022173e+00  1.69688484e+00 -5.85747410e-01 -1.81756159e+00
 -4.05762360e-01 -2.18438102e-01 -6.08325335e-01 -1.98129050e+00
  2.50333152e+01]
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.9015234062385
gradient value of function right now is: [ 2.62346176e-08  3.18239220e-08  3.18001718e-08  3.19647119e-08
 -2.62348509e-08 -3.18242055e-08 -3.18004552e-08 -3.19649968e-08
  3.08377354e-09  1.36002427e-08  5.71657627e-10  1.35367431e-08
  4.36388216e-10  1.92833055e-09  7.93492257e-11  1.91958542e-09
  6.31466445e-10  2.78541913e-09  1.16863318e-10  2.77244400e-09
  2.82174119e-10  1.24375792e-09  5.25955996e-11  1.23789997e-09
  4.34800406e-11  6.43322042e-10  5.23068649e-10  6.93001453e-10
  1.38577058e-12  1.66188910e-10  1.25180162e-10  1.58469630e-10
 -1.10670110e-11 -2.36460417e-10 -1.86927416e-10 -2.44501145e-10
  4.61512547e-12  2.30214206e-10  1.76186022e-10  2.25187594e-10
 -1.73289145e-11 -2.69023561e-11 -2.61151327e-11 -2.69595875e-11
 -2.40134005e-11  5.05760831e-11  2.67485575e-11  5.10192863e-11
 -2.41412651e-11  1.69369185e-10  1.09621194e-10  1.70842117e-10
 -3.79572519e-11  9.61942945e-12 -9.10520049e-12  1.00485708e-11
 -1.47882098e-11 -4.02523433e-12  2.12860838e-11  9.94725171e-12
  2.08649584e-11  7.77683026e-11  2.84693591e-11  9.59241735e-12
 -2.11124368e+00]
supnorm grad right now is: 2.1112436800134597
Weights right now are: 
[-1.81381782e+00 -3.09936752e+00 -3.89165365e+00 -2.90031402e+00
  1.02359865e+00  3.11383126e+00  4.46295979e+00  4.31539716e+00
  3.07266111e-02 -1.00789326e+00  5.92852448e-01 -5.26425783e-01
  1.23918629e-01 -2.28090254e+00  5.31098770e-01 -2.42134585e+00
 -1.10968010e+00 -2.06177454e+00 -4.66139989e-01 -2.24497127e+00
 -1.22745613e+00 -2.40244362e+00 -1.47699519e+00 -2.50997214e+00
  1.39940662e+00  5.86423287e-01  1.49506107e-02  5.96274608e-01
 -1.01114865e+00 -1.47578539e+00 -2.20307570e+00 -1.95791138e+00
  2.12610343e-01  1.00368753e+00  1.36844903e+00  1.13145763e+00
 -9.05990880e-01 -1.77939662e+00 -8.97936345e-01 -2.09320843e+00
  3.85099673e-01  2.72167148e-01  1.36484362e+00  1.65971509e+00
  1.16666074e+00 -1.63929901e+00 -1.58101857e-01 -1.01233899e+00
  4.91064915e-01 -8.90873284e-02 -5.24286686e-01 -7.78222304e-01
 -8.39223787e-01 -2.09518552e+00 -1.23009642e+00 -2.07205963e+00
  1.26022474e+00  1.69688598e+00 -5.85751727e-01 -1.81756425e+00
 -4.05766524e-01 -2.18453998e-01 -6.08331120e-01 -1.98129303e+00
  2.48257382e+01]
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.802356074836
gradient value of function right now is: [ 2.36000619e-08  2.85895743e-08  2.85673695e-08  2.87144521e-08
 -2.35997636e-08 -2.85892121e-08 -2.85670076e-08 -2.87140883e-08
  2.75630316e-09  1.22096641e-08  4.87303016e-10  1.21561826e-08
  3.88861998e-10  1.72699419e-09  6.69356526e-11  1.71972668e-09
  5.64544559e-10  2.50119721e-09  9.96446756e-11  2.49026652e-09
  2.52956169e-10  1.11957374e-09  4.51081412e-11  1.11460426e-09
  3.55951765e-11  5.88887319e-10  4.75376878e-10  6.25053637e-10
 -3.00365495e-12  1.46077796e-10  1.06205394e-10  1.29796188e-10
 -6.51559918e-12 -2.11815701e-10 -1.64465022e-10 -2.11411235e-10
 -8.60415621e-13  2.04039609e-10  1.51706482e-10  1.88457069e-10
 -1.80261125e-11 -2.53574235e-11 -2.55406838e-11 -2.53506620e-11
 -3.57355602e-11  4.36310145e-11  1.59467319e-11  4.43633693e-11
 -3.91640438e-11  1.53189594e-10  9.07532436e-11  1.54950681e-10
 -4.32855006e-11  7.20339784e-12 -1.37800218e-11  7.82047971e-12
 -1.19955199e-11 -7.52435955e-12  1.69099673e-11  1.62289697e-11
  1.67244619e-11  7.71477918e-11  2.36428399e-11  1.48595887e-11
  2.78469298e+00]
supnorm grad right now is: 2.7846929806263088
Weights right now are: 
[-1.81887687e+00 -3.10549975e+00 -3.89778114e+00 -2.90647309e+00
  1.02865770e+00  3.11996350e+00  4.46908728e+00  4.32155623e+00
  3.01346944e-02 -1.01051233e+00  5.92747019e-01 -5.29033294e-01
  1.23835159e-01 -2.28127286e+00  5.31084325e-01 -2.42171461e+00
 -1.10980127e+00 -2.06231081e+00 -4.66161516e-01 -2.24550518e+00
 -1.22751038e+00 -2.40268348e+00 -1.47700493e+00 -2.51021093e+00
  1.39939885e+00  5.86297788e-01  1.48491023e-02  5.96140739e-01
 -1.01114789e+00 -1.47581701e+00 -2.20309868e+00 -1.95793958e+00
  2.12611778e-01  1.00373313e+00  1.36848452e+00  1.13150347e+00
 -9.05990596e-01 -1.77944065e+00 -8.97969091e-01 -2.09324929e+00
  3.85103942e-01  2.72172415e-01  1.36484908e+00  1.65972037e+00
  1.16666792e+00 -1.63930885e+00 -1.58106086e-01 -1.01234892e+00
  4.91073254e-01 -8.91204473e-02 -5.24306755e-01 -7.78255715e-01
 -8.39213562e-01 -2.09518739e+00 -1.23009355e+00 -2.07206159e+00
  1.26022754e+00  1.69688684e+00 -5.85755758e-01 -1.81756634e+00
 -4.05770251e-01 -2.18472206e-01 -6.08336557e-01 -1.98129497e+00
  2.51752369e+01]
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3586.058031282347
gradient value of function right now is: [ 2.54206047e-08  3.07909997e-08  3.07694220e-08  3.09286072e-08
 -2.54207503e-08 -3.07911763e-08 -3.07695985e-08 -3.09287847e-08
  3.00296254e-09  1.31747728e-08  5.82286322e-10  1.31093461e-08
  4.27620392e-10  1.87852150e-09  8.18902765e-11  1.86936909e-09
  6.15074735e-10  2.69894330e-09  1.19081674e-10  2.68556829e-09
  2.74752219e-10  1.20505212e-09  5.34234038e-11  1.19903887e-09
  4.62584650e-11  6.15872714e-10  5.04586105e-10  6.73607934e-10
  6.21469893e-12  1.65106694e-10  1.28733829e-10  1.68158183e-10
 -1.45290147e-11 -2.29121019e-10 -1.84468513e-10 -2.45329522e-10
  1.02152804e-11  2.27192472e-10  1.78947798e-10  2.34771233e-10
 -1.38532072e-11 -2.50850529e-11 -2.31763966e-11 -2.52009683e-11
 -7.56414448e-12  5.07833823e-11  3.46189247e-11  5.08738065e-11
 -3.70251530e-12  1.63091985e-10  1.14884275e-10  1.64071738e-10
 -2.59702358e-11  1.07692773e-11 -2.54903438e-12  1.09487566e-11
 -1.56333444e-11  1.23734572e-13  2.28789275e-11  2.19615870e-12
  2.26912749e-11  6.63935515e-11  2.95710886e-11  2.96002873e-12
 -6.88355499e+00]
supnorm grad right now is: 6.88355499401066
Weights right now are: 
[-1.82358311e+00 -3.11119982e+00 -3.90347689e+00 -2.91219820e+00
  1.03336393e+00  3.12566356e+00  4.47478303e+00  4.32728134e+00
  2.95836719e-02 -1.01294782e+00  5.92647242e-01 -5.31457716e-01
  1.23757191e-01 -2.28161819e+00  5.31070497e-01 -2.42205841e+00
 -1.10991417e+00 -2.06280987e+00 -4.66181933e-01 -2.24600198e+00
 -1.22756093e+00 -2.40290677e+00 -1.47701415e+00 -2.51043320e+00
  1.39939142e+00  5.86180949e-01  1.47544742e-02  5.96016059e-01
 -1.01114786e+00 -1.47584635e+00 -2.20312054e+00 -1.95796681e+00
  2.12613462e-01  1.00377528e+00  1.36851760e+00  1.13154629e+00
 -9.05991085e-01 -1.77948154e+00 -8.98000099e-01 -2.09328839e+00
  3.85106994e-01  2.72177440e-01  1.36485390e+00  1.65972540e+00
  1.16667352e+00 -1.63931734e+00 -1.58109871e-01 -1.01235754e+00
  4.91079024e-01 -8.91506671e-02 -5.24325596e-01 -7.78286266e-01
 -8.39206564e-01 -2.09518884e+00 -1.23009157e+00 -2.07206313e+00
  1.26023008e+00  1.69688805e+00 -5.85759369e-01 -1.81756903e+00
 -4.05774065e-01 -2.18486021e-01 -6.08341500e-01 -1.98129757e+00
  2.44543746e+01]
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.3210910182415
gradient value of function right now is: [ 2.11743169e-08  2.56021084e-08  2.55820759e-08  2.57131874e-08
 -2.11744651e-08 -2.56022881e-08 -2.55822555e-08 -2.57133679e-08
  2.46445457e-09  1.09328929e-08  4.26626027e-10  1.08863213e-08
  3.47779538e-10  1.54721566e-09  5.84181233e-11  1.54091569e-09
  5.04957045e-10  2.24046281e-09  8.72761812e-11  2.23094005e-09
  2.26774654e-10  1.00504184e-09  3.96563881e-11  1.00069391e-09
  3.07104191e-11  5.33647913e-10  4.29539957e-10  5.63002154e-10
 -4.27961109e-12  1.29933553e-10  9.30195233e-11  1.11840725e-10
 -4.55574509e-12 -1.89337034e-10 -1.45907645e-10 -1.86140256e-10
 -2.63884211e-12  1.82253465e-10  1.33839221e-10  1.64137935e-10
 -1.70895937e-11 -2.32847408e-11 -2.37482179e-11 -2.32582448e-11
 -3.73483219e-11  3.84142668e-11  1.12646166e-11  3.91951594e-11
 -4.16539580e-11  1.37780100e-10  7.85246813e-11  1.39523135e-10
 -4.22618492e-11  5.91834273e-12 -1.44434475e-11  6.55753473e-12
 -1.02810249e-11 -8.18646836e-12  1.43490637e-11  1.72610241e-11
  1.43047813e-11  7.19762874e-11  2.04937236e-11  1.56581610e-11
  4.77154002e+00]
supnorm grad right now is: 4.771540021963873
Weights right now are: 
[-1.82810366e+00 -3.11667057e+00 -3.90894355e+00 -2.91769301e+00
  1.03788448e+00  3.13113430e+00  4.48024969e+00  4.33277615e+00
  2.90540217e-02 -1.01528608e+00  5.92550456e-01 -5.33785245e-01
  1.23682134e-01 -2.28195023e+00  5.31057062e-01 -2.42238897e+00
 -1.11002268e+00 -2.06328902e+00 -4.66201732e-01 -2.24647893e+00
 -1.22760954e+00 -2.40312124e+00 -1.47702309e+00 -2.51064667e+00
  1.39938410e+00  5.86068995e-01  1.46636614e-02  5.95896094e-01
 -1.01114791e+00 -1.47587479e+00 -2.20314180e+00 -1.95799346e+00
  2.12615187e-01  1.00381582e+00  1.36854950e+00  1.13158778e+00
 -9.05991653e-01 -1.77952108e+00 -8.98030170e-01 -2.09332651e+00
  3.85110026e-01  2.72182204e-01  1.36485851e+00  1.65973017e+00
  1.16667842e+00 -1.63932579e+00 -1.58113972e-01 -1.01236607e+00
  4.91084146e-01 -8.91798335e-02 -5.24344017e-01 -7.78315711e-01
 -8.39199704e-01 -2.09519038e+00 -1.23008977e+00 -2.07206476e+00
  1.26023263e+00  1.69688890e+00 -5.85763064e-01 -1.81757108e+00
 -4.05777846e-01 -2.18499586e-01 -6.08346471e-01 -1.98129954e+00
  2.53334210e+01]
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.7312584595516
gradient value of function right now is: [ 2.07593831e-08  2.50831334e-08  2.50639074e-08  2.51923057e-08
 -2.07595064e-08 -2.50832828e-08 -2.50640567e-08 -2.51924558e-08
  2.41980952e-09  1.07155461e-08  4.25761485e-10  1.06688648e-08
  3.42274882e-10  1.51964185e-09  5.86040354e-11  1.51328647e-09
  4.95875827e-10  2.19621447e-09  8.71128864e-11  2.18666756e-09
  2.22707883e-10  9.85340561e-10  3.95379121e-11  9.80988469e-10
  3.12374891e-11  5.21365006e-10  4.20668089e-10  5.52777912e-10
 -2.93649670e-12  1.28537263e-10  9.31955539e-11  1.13556931e-10
 -5.47183830e-12 -1.85598962e-10 -1.43910148e-10 -1.84718555e-10
 -1.09731515e-12  1.79873572e-10  1.33445964e-10  1.65382226e-10
 -1.60549773e-11 -2.25641190e-11 -2.27432454e-11 -2.25553899e-11
 -3.24604231e-11  3.82196032e-11  1.34382194e-11  3.88870036e-11
 -3.56867889e-11  1.34869499e-10  7.93386351e-11  1.36449263e-10
 -3.87609488e-11  6.23411525e-12 -1.25197921e-11  6.79330177e-12
 -1.04731450e-11 -6.87785370e-12  1.47468139e-11  1.47855880e-11
  1.46390861e-11  6.85010250e-11  2.07123936e-11  1.35144451e-11
  3.15740996e+00]
supnorm grad right now is: 3.1574099564331592
Weights right now are: 
[-1.83242618e+00 -3.12189595e+00 -3.91416494e+00 -2.92294117e+00
  1.04220701e+00  3.13635969e+00  4.48547108e+00  4.33802431e+00
  2.85495711e-02 -1.01751860e+00  5.92461042e-01 -5.36008009e-01
  1.23610830e-01 -2.28226668e+00  5.31044797e-01 -2.42270411e+00
 -1.11012600e+00 -2.06374636e+00 -4.66220002e-01 -2.24693428e+00
 -1.22765591e+00 -2.40332628e+00 -1.47703138e+00 -2.51085080e+00
  1.39937741e+00  5.85961120e-01  1.45765573e-02  5.95781109e-01
 -1.01114716e+00 -1.47590190e+00 -2.20316134e+00 -1.95801734e+00
  2.12616349e-01  1.00385474e+00  1.36857965e+00  1.13162669e+00
 -9.05991295e-01 -1.77955887e+00 -8.98058076e-01 -2.09336122e+00
  3.85113820e-01  2.72186789e-01  1.36486330e+00  1.65973476e+00
  1.16668512e+00 -1.63933434e+00 -1.58117409e-01 -1.01237473e+00
  4.91091764e-01 -8.92082837e-02 -5.24360930e-01 -7.78344441e-01
 -8.39190634e-01 -2.09519195e+00 -1.23008710e+00 -2.07206642e+00
  1.26023516e+00  1.69688985e+00 -5.85766663e-01 -1.81757333e+00
 -4.05780878e-01 -2.18515336e-01 -6.08351311e-01 -1.98130158e+00
  2.51935038e+01]
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.861879407232
gradient value of function right now is: [ 2.01409473e-08  2.43177934e-08  2.42993736e-08  2.44237262e-08
 -2.01407063e-08 -2.43175018e-08 -2.42990822e-08 -2.44234332e-08
  2.34853114e-09  1.03907704e-08  4.15957630e-10  1.03450824e-08
  3.32695815e-10  1.47566950e-09  5.74103890e-11  1.46942939e-09
  4.81346582e-10  2.12998783e-09  8.51229780e-11  2.12064215e-09
  2.16271290e-10  9.56059800e-10  3.86319082e-11  9.51800533e-10
  3.07998048e-11  5.05557994e-10  4.08335014e-10  5.37142781e-10
 -2.32375276e-12  1.25223530e-10  9.12758328e-11  1.11826671e-10
 -5.71969660e-12 -1.79932280e-10 -1.39879137e-10 -1.80001757e-10
 -4.43071199e-13  1.75103915e-10  1.30465324e-10  1.62389973e-10
 -1.52836027e-11 -2.18159514e-11 -2.18680995e-11 -2.18150700e-11
 -2.97765757e-11  3.72961262e-11  1.39922989e-11  3.79035613e-11
 -3.24916697e-11  1.30776170e-10  7.79191851e-11  1.32257074e-10
 -3.64913878e-11  6.22345568e-12 -1.14864701e-11  6.73827662e-12
 -1.02920642e-11 -6.21692058e-12  1.45393420e-11  1.34790521e-11
  1.44114076e-11  6.56094912e-11  2.02991515e-11  1.23756119e-11
  2.45210378e+00]
supnorm grad right now is: 2.452103779134818
Weights right now are: 
[-1.83667135e+00 -3.12702412e+00 -3.91928925e+00 -2.92809174e+00
  1.04645217e+00  3.14148786e+00  4.49059539e+00  4.34317488e+00
  2.80540239e-02 -1.01971009e+00  5.92372365e-01 -5.38189760e-01
  1.23540608e-01 -2.28257797e+00  5.31032531e-01 -2.42301406e+00
 -1.11022756e+00 -2.06419557e+00 -4.66238152e-01 -2.24738150e+00
 -1.22770151e+00 -2.40352777e+00 -1.47703961e+00 -2.51105139e+00
  1.39937077e+00  5.85854975e-01  1.44907832e-02  5.95668017e-01
 -1.01114685e+00 -1.47592842e+00 -2.20318079e+00 -1.95804137e+00
  2.12617689e-01  1.00389273e+00  1.36860926e+00  1.13166495e+00
 -9.05991408e-01 -1.77959586e+00 -8.98085766e-01 -2.09339591e+00
  3.85116912e-01  2.72191372e-01  1.36486785e+00  1.65973935e+00
  1.16669093e+00 -1.63934230e+00 -1.58120492e-01 -1.01238282e+00
  4.91097879e-01 -8.92358591e-02 -5.24377474e-01 -7.78372340e-01
 -8.39183402e-01 -2.09519327e+00 -1.23008482e+00 -2.07206786e+00
  1.26023718e+00  1.69689130e+00 -5.85769511e-01 -1.81757628e+00
 -4.05783666e-01 -2.18528631e-01 -6.08355398e-01 -1.98130434e+00
  2.51387444e+01]
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.439242578458
gradient value of function right now is: [ 1.89586745e-08  2.28692966e-08  2.28517449e-08  2.29683709e-08
 -1.89589395e-08 -2.28696170e-08 -2.28520651e-08 -2.29686927e-08
  2.20444734e-09  9.76925132e-09  3.82807660e-10  9.72741715e-09
  3.12061721e-10  1.38676596e-09  5.26255038e-11  1.38108241e-09
  4.51920391e-10  2.00303616e-09  7.83603072e-11  1.99447648e-09
  2.03339245e-10  9.00255337e-10  3.56621778e-11  8.96341345e-10
  2.78827751e-11  4.79552841e-10  3.86252612e-10  5.06568602e-10
 -3.55744531e-12  1.16814266e-10  8.38984788e-11  1.01208788e-10
 -4.28524858e-12 -1.68976112e-10 -1.30417660e-10 -1.66620202e-10
 -2.03321965e-12  1.63931289e-10  1.20700510e-10  1.48410834e-10
 -1.51751177e-11 -2.09341161e-11 -2.12544698e-11 -2.09154069e-11
 -3.25661508e-11  3.44947780e-11  1.05995013e-11  3.51713879e-11
 -3.61926368e-11  1.23360764e-10  7.08743289e-11  1.24891992e-10
 -3.73068982e-11  5.39608157e-12 -1.25790977e-11  5.95315692e-12
 -9.31507102e-12 -7.05547165e-12  1.30413831e-11  1.49651462e-11
  1.30012544e-11  6.41443634e-11  1.85535031e-11  1.36034447e-11
  4.34408857e+00]
supnorm grad right now is: 4.344088572272357
Weights right now are: 
[-1.84080322e+00 -3.13201244e+00 -3.92427392e+00 -2.93310196e+00
  1.05058404e+00  3.14647618e+00  4.49558006e+00  4.34818510e+00
  2.75703984e-02 -1.02184297e+00  5.92283774e-01 -5.40312853e-01
  1.23471885e-01 -2.28288170e+00  5.31020208e-01 -2.42331645e+00
 -1.11032668e+00 -2.06463277e+00 -4.66256280e-01 -2.24781670e+00
 -1.22774600e+00 -2.40372384e+00 -1.47704781e+00 -2.51124655e+00
  1.39936395e+00  5.85752394e-01  1.44075259e-02  5.95557814e-01
 -1.01114687e+00 -1.47595462e+00 -2.20320031e+00 -1.95806588e+00
  2.12619284e-01  1.00392978e+00  1.36863839e+00  1.13170291e+00
 -9.05991912e-01 -1.77963229e+00 -8.98113394e-01 -2.09343099e+00
  3.85119838e-01  2.72195714e-01  1.36487214e+00  1.65974369e+00
  1.16669547e+00 -1.63935032e+00 -1.58124417e-01 -1.01239093e+00
  4.91102574e-01 -8.92626628e-02 -5.24394416e-01 -7.78399394e-01
 -8.39176856e-01 -2.09519476e+00 -1.23008307e+00 -2.07206942e+00
  1.26023965e+00  1.69689208e+00 -5.85773052e-01 -1.81757819e+00
 -4.05786824e-01 -2.18541379e-01 -6.08360125e-01 -1.98130609e+00
  2.53048886e+01]
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.9454629853535
gradient value of function right now is: [ 1.88240682e-08  2.26942266e-08  2.26772880e-08  2.27930447e-08
 -1.88240290e-08 -2.26941793e-08 -2.26772407e-08 -2.27929972e-08
  2.19413011e-09  9.69923609e-09  3.90236424e-10  9.65632428e-09
  3.11508451e-10  1.38039663e-09  5.40245571e-11  1.37451585e-09
  4.49864061e-10  1.98894233e-09  7.98922980e-11  1.98016071e-09
  2.02352050e-10  8.93774260e-10  3.62869452e-11  8.89769358e-10
  2.91756783e-11  4.73263465e-10  3.82529595e-10  5.03584127e-10
 -1.83725746e-12  1.17475641e-10  8.59368277e-11  1.05675530e-10
 -5.60047740e-12 -1.67841318e-10 -1.30709723e-10 -1.68496858e-10
 -1.43428812e-14  1.64257317e-10  1.22741731e-10  1.53228879e-10
 -1.41181091e-11 -2.04065458e-11 -2.03653809e-11 -2.04107578e-11
 -2.67648580e-11  3.50020534e-11  1.36913757e-11  3.55449714e-11
 -2.90182394e-11  1.22201250e-10  7.34591227e-11  1.23552957e-10
 -3.34267022e-11  5.93087970e-12 -1.03164397e-11  6.39502678e-12
 -9.72717213e-12 -5.52198389e-12  1.37798066e-11  1.20571423e-11
  1.36559113e-11  6.08172751e-11  1.91711864e-11  1.11046317e-11
  1.91862944e+00]
supnorm grad right now is: 1.9186294426335517
Weights right now are: 
[-1.84479285e+00 -3.13682635e+00 -3.92908442e+00 -2.93793713e+00
  1.05457367e+00  3.15129009e+00  4.50039055e+00  4.35302027e+00
  2.71023611e-02 -1.02390229e+00  5.92196051e-01 -5.42362353e-01
  1.23405128e-01 -2.28317587e+00  5.31007872e-01 -2.42360924e+00
 -1.11042266e+00 -2.06505513e+00 -4.66274255e-01 -2.24823704e+00
 -1.22778907e+00 -2.40391324e+00 -1.47705592e+00 -2.51143504e+00
  1.39935712e+00  5.85653657e-01  1.43271260e-02  5.95451284e-01
 -1.01114749e+00 -1.47598002e+00 -2.20321977e+00 -1.95809076e+00
  2.12621170e-01  1.00396536e+00  1.36866670e+00  1.13174003e+00
 -9.05993069e-01 -1.77966754e+00 -8.98140742e-01 -2.09346621e+00
  3.85121986e-01  2.72199930e-01  1.36487600e+00  1.65974791e+00
  1.16669844e+00 -1.63935771e+00 -1.58128518e-01 -1.01239841e+00
  4.91105008e-01 -8.92881191e-02 -5.24411333e-01 -7.78425108e-01
 -8.39172495e-01 -2.09519606e+00 -1.23008212e+00 -2.07207081e+00
  1.26024190e+00  1.69689288e+00 -5.85776288e-01 -1.81758018e+00
 -4.05790680e-01 -2.18550945e-01 -6.08364472e-01 -1.98130803e+00
  2.50640905e+01]
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.7308776585833
gradient value of function right now is: [ 1.92930963e-08  2.32522155e-08  2.32357677e-08  2.33546030e-08
 -1.92929427e-08 -2.32520300e-08 -2.32355823e-08 -2.33544166e-08
  2.26125612e-09  9.94713935e-09  4.21504481e-10  9.90022474e-09
  3.22631450e-10  1.42179530e-09  5.90759415e-11  1.41526771e-09
  4.63656304e-10  2.03991946e-09  8.62975677e-11  2.03031802e-09
  2.08280884e-10  9.15729777e-10  3.90238828e-11  9.11375180e-10
  3.29527417e-11  4.78211163e-10  3.89392409e-10  5.16533761e-10
  1.67454880e-12  1.23409653e-10  9.35512690e-11  1.19109920e-10
 -8.55647385e-12 -1.72338388e-10 -1.36683524e-10 -1.79288067e-10
  4.19505380e-12  1.71265229e-10  1.31770344e-10  1.69215388e-10
 -1.24449128e-11 -2.00636505e-11 -1.92504601e-11 -2.01159636e-11
 -1.56859838e-11  3.73733695e-11  2.07421619e-11  3.76541025e-11
 -1.52230494e-11  1.24574021e-10  8.18445630e-11  1.25599234e-10
 -2.66077978e-11  7.26749445e-12 -5.91837085e-12  7.55320876e-12
 -1.10495339e-11 -2.42312594e-12  1.59801047e-11  6.35302719e-12
  1.57702020e-11  5.62888878e-11  2.12867355e-11  6.23982578e-12
 -2.93375691e+00]
supnorm grad right now is: 2.9337569141060755
Weights right now are: 
[-1.84855330e+00 -3.14135908e+00 -3.93361382e+00 -2.94248966e+00
  1.05833412e+00  3.15582281e+00  4.50491996e+00  4.35757280e+00
  2.66631065e-02 -1.02584013e+00  5.92116753e-01 -5.44291431e-01
  1.23342657e-01 -2.28345208e+00  5.30996850e-01 -2.42388424e+00
 -1.11051272e+00 -2.06545248e+00 -4.66290487e-01 -2.24863260e+00
 -1.22782956e+00 -2.40409175e+00 -1.47706328e+00 -2.51161273e+00
  1.39935112e+00  5.85559550e-01  1.42508153e-02  5.95350623e-01
 -1.01114732e+00 -1.47600367e+00 -2.20323730e+00 -1.95811256e+00
  2.12622452e-01  1.00399889e+00  1.36869299e+00  1.13177412e+00
 -9.05993304e-01 -1.77970054e+00 -8.98165667e-01 -2.09349761e+00
  3.85124703e-01  2.72203930e-01  1.36487996e+00  1.65975191e+00
  1.16670303e+00 -1.63936481e+00 -1.58131771e-01 -1.01240560e+00
  4.91109907e-01 -8.93124522e-02 -5.24426557e-01 -7.78449678e-01
 -8.39166232e-01 -2.09519733e+00 -1.23008039e+00 -2.07207216e+00
  1.26024396e+00  1.69689371e+00 -5.85779231e-01 -1.81758215e+00
 -4.05793636e-01 -2.18562928e-01 -6.08368495e-01 -1.98130981e+00
  2.47780938e+01]
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.483843252123
gradient value of function right now is: [ 1.88652843e-08  2.27228395e-08  2.27070047e-08  2.28230609e-08
 -1.88649868e-08 -2.27224805e-08 -2.27066459e-08 -2.28227002e-08
  2.21273610e-09  9.72283862e-09  4.16126681e-10  9.67642159e-09
  3.16224905e-10  1.39183633e-09  5.84957362e-11  1.38535558e-09
  4.53778708e-10  1.99423241e-09  8.52110859e-11  1.98473106e-09
  2.03885868e-10  8.95453493e-10  3.85189998e-11  8.91146648e-10
  3.28612440e-11  4.66785366e-10  3.80659165e-10  5.05727241e-10
  2.33697028e-12  1.21321215e-10  9.25882328e-11  1.18632747e-10
 -8.91006874e-12 -1.68444992e-10 -1.34072364e-10 -1.76453643e-10
  4.93197076e-12  1.68163190e-10  1.30107652e-10  1.67960439e-10
 -1.17816128e-11 -1.94885589e-11 -1.85414829e-11 -1.95483543e-11
 -1.30641344e-11  3.68467590e-11  2.15758125e-11  3.70694414e-11
 -1.20549254e-11  1.21696400e-10  8.13012120e-11  1.22631143e-10
 -2.45269645e-11  7.33596083e-12 -4.89796977e-12  7.58080550e-12
 -1.10135234e-11 -1.76130733e-12  1.59869261e-11  5.10100376e-12
  1.57936202e-11  5.37640480e-11  2.11453068e-11  5.13854595e-12
 -3.85903862e+00]
supnorm grad right now is: 3.859038620227361
Weights right now are: 
[-1.85223451e+00 -3.14579243e+00 -3.93804387e+00 -2.94694227e+00
  1.06201533e+00  3.16025617e+00  4.50935000e+00  4.36202540e+00
  2.62346012e-02 -1.02773481e+00  5.92041100e-01 -5.46177821e-01
  1.23281797e-01 -2.28372188e+00  5.30986399e-01 -2.42415291e+00
 -1.11060058e+00 -2.06584105e+00 -4.66305972e-01 -2.24901948e+00
 -1.22786913e+00 -2.40426655e+00 -1.47707033e+00 -2.51178676e+00
  1.39934546e+00  5.85466720e-01  1.41758693e-02  5.95251912e-01
 -1.01114676e+00 -1.47602666e+00 -2.20325397e+00 -1.95813294e+00
  2.12623432e-01  1.00403173e+00  1.36871846e+00  1.13180691e+00
 -9.05993083e-01 -1.77973271e+00 -8.98189537e-01 -2.09352728e+00
  3.85127693e-01  2.72207938e-01  1.36488402e+00  1.65975593e+00
  1.16670870e+00 -1.63937177e+00 -1.58134435e-01 -1.01241266e+00
  4.91116137e-01 -8.93364758e-02 -5.24440914e-01 -7.78473971e-01
 -8.39159085e-01 -2.09519852e+00 -1.23007825e+00 -2.07207343e+00
  1.26024590e+00  1.69689479e+00 -5.85781975e-01 -1.81758451e+00
 -4.05796389e-01 -2.18575564e-01 -6.08372315e-01 -1.98131202e+00
  2.47119325e+01]
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.0450579619646
gradient value of function right now is: [ 1.75794719e-08  2.11546914e-08  2.11395247e-08  2.12472005e-08
 -1.75794530e-08 -2.11546686e-08 -2.11395019e-08 -2.12471775e-08
  2.05288546e-09  9.04678440e-09  3.74345580e-10  9.00534817e-09
  2.92830217e-10  1.29315446e-09  5.22853025e-11  1.28741605e-09
  4.21110278e-10  1.85605423e-09  7.66788671e-11  1.84756991e-09
  1.89557488e-10  8.34798211e-10  3.47934831e-11  8.30935114e-10
  2.88546730e-11  4.40128598e-10  3.57167554e-10  4.72135234e-10
  2.28066901e-14  1.11353511e-10  8.30598069e-11  1.04134143e-10
 -6.59306600e-12 -1.56507110e-10 -1.23089612e-10 -1.60186282e-10
  2.05079926e-12  1.55185665e-10  1.17821392e-10  1.49402794e-10
 -1.22336995e-11 -1.87400535e-11 -1.83125154e-11 -1.87688408e-11
 -1.92718431e-11  3.34293048e-11  1.60699124e-11  3.37988740e-11
 -2.00272348e-11  1.13840522e-10  7.18363174e-11  1.14924538e-10
 -2.75485374e-11  6.13499954e-12 -7.37903527e-12  6.47394325e-12
 -9.63192248e-12 -3.54468974e-12  1.38157785e-11  8.27156955e-12
  1.36402820e-11  5.39765818e-11  1.87653551e-11  7.81619192e-12
 -7.48093811e-01]
supnorm grad right now is: 0.7480938114976481
Weights right now are: 
[-1.85587080e+00 -3.15016908e+00 -3.94241729e+00 -2.95133794e+00
  1.06565162e+00  3.16463281e+00  4.51372342e+00  4.36642108e+00
  2.58110583e-02 -1.02960566e+00  5.91965581e-01 -5.48040389e-01
  1.23221545e-01 -2.28398869e+00  5.30975932e-01 -2.42441859e+00
 -1.11068745e+00 -2.06622480e+00 -4.66321435e-01 -2.24940153e+00
 -1.22790825e+00 -2.40443922e+00 -1.47707736e+00 -2.51195865e+00
  1.39933974e+00  5.85375190e-01  1.41018482e-02  5.95154287e-01
 -1.01114638e+00 -1.47604951e+00 -2.20327064e+00 -1.95815348e+00
  2.12624529e-01  1.00406415e+00  1.36874370e+00  1.13183953e+00
 -9.05993055e-01 -1.77976463e+00 -8.98213339e-01 -2.09355706e+00
  3.85130549e-01  2.72211855e-01  1.36488801e+00  1.65975984e+00
  1.16671382e+00 -1.63937877e+00 -1.58137219e-01 -1.01241976e+00
  4.91121741e-01 -8.93602145e-02 -5.24455170e-01 -7.78497956e-01
 -8.39152341e-01 -2.09519976e+00 -1.23007615e+00 -2.07207476e+00
  1.26024776e+00  1.69689584e+00 -5.85784616e-01 -1.81758671e+00
 -4.05798750e-01 -2.18587819e-01 -6.08376073e-01 -1.98131399e+00
  2.49362254e+01]
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.053867196763
gradient value of function right now is: [ 1.69945137e-08  2.04375130e-08  2.04228927e-08  2.05267773e-08
 -1.69945415e-08 -2.04375465e-08 -2.04229262e-08 -2.05268110e-08
  1.98317465e-09  8.73993010e-09  3.60757853e-10  8.70001942e-09
  2.83056535e-10  1.25007360e-09  5.04029259e-11  1.24454537e-09
  4.06889304e-10  1.79344227e-09  7.39117309e-11  1.78526846e-09
  1.83272784e-10  8.07141430e-10  3.35638455e-11  8.03416657e-10
  2.78097390e-11  4.26355927e-10  3.45878860e-10  4.57045430e-10
 -1.17837944e-13  1.07581569e-10  8.01208978e-11  1.00291127e-10
 -6.25349658e-12 -1.51125797e-10 -1.18759791e-10 -1.54425090e-10
  1.81857839e-12  1.50031146e-10  1.13762360e-10  1.44070259e-10
 -1.19074882e-11 -1.81932729e-11 -1.77980359e-11 -1.82193991e-11
 -1.91121829e-11  3.22399293e-11  1.52473815e-11  3.26092325e-11
 -1.99422954e-11  1.10084194e-10  6.91845120e-11  1.11147811e-10
 -2.69319139e-11  5.87785464e-12 -7.32167522e-12  6.21360578e-12
 -9.25937597e-12 -3.56717698e-12  1.32696359e-11  8.25561568e-12
  1.31215876e-11  5.24424110e-11  1.80709307e-11  7.78254313e-12
 -5.37988508e-01]
supnorm grad right now is: 0.5379885079121803
Weights right now are: 
[-1.85931561e+00 -3.15431223e+00 -3.94655737e+00 -2.95549906e+00
  1.06909642e+00  3.16877595e+00  4.51786350e+00  4.37058218e+00
  2.54105883e-02 -1.03137636e+00  5.91894967e-01 -5.49803343e-01
  1.23164562e-01 -2.28424128e+00  5.30966146e-01 -2.42467011e+00
 -1.11076961e+00 -2.06658816e+00 -4.66335902e-01 -2.24976330e+00
 -1.22794530e+00 -2.40460288e+00 -1.47708395e+00 -2.51212158e+00
  1.39933446e+00  5.85287927e-01  1.40314317e-02  5.95061678e-01
 -1.01114594e+00 -1.47607096e+00 -2.20328625e+00 -1.95817255e+00
  2.12625467e-01  1.00409474e+00  1.36876745e+00  1.13187005e+00
 -9.05992935e-01 -1.77979469e+00 -8.98235711e-01 -2.09358485e+00
  3.85133199e-01  2.72215659e-01  1.36489179e+00  1.65976365e+00
  1.16671907e+00 -1.63938507e+00 -1.58139541e-01 -1.01242616e+00
  4.91127477e-01 -8.93825998e-02 -5.24468475e-01 -7.78520602e-01
 -8.39145999e-01 -2.09520080e+00 -1.23007420e+00 -2.07207589e+00
  1.26024956e+00  1.69689686e+00 -5.85787163e-01 -1.81758891e+00
 -4.05801145e-01 -2.18599491e-01 -6.08379579e-01 -1.98131614e+00
  2.49611633e+01]
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.8445848790147
gradient value of function right now is: [ 1.68730166e-08  2.02820941e-08  2.02679245e-08  2.03710306e-08
 -1.68731815e-08 -2.02822928e-08 -2.02681231e-08 -2.03712302e-08
  1.97274240e-09  8.67665520e-09  3.65302593e-10  8.63605359e-09
  2.82225897e-10  1.24362091e-09  5.12978981e-11  1.23796234e-09
  4.04803462e-10  1.78070265e-09  7.48520095e-11  1.77238625e-09
  1.82294014e-10  8.01321647e-10  3.39442178e-11  7.97538686e-10
  2.86639218e-11  4.21283348e-10  3.42755943e-10  4.54242039e-10
  1.08498203e-12  1.07857060e-10  8.14275785e-11  1.03264008e-10
 -7.15085280e-12 -1.50063516e-10 -1.18762107e-10 -1.55454953e-10
  3.23168993e-12  1.50011286e-10  1.15031076e-10  1.47238924e-10
 -1.11453131e-11 -1.77931531e-11 -1.71324003e-11 -1.78354186e-11
 -1.49948265e-11  3.25069877e-11  1.74227505e-11  3.27790442e-11
 -1.48896411e-11  1.09068885e-10  7.09307668e-11  1.10001006e-10
 -2.41607004e-11  6.23892690e-12 -5.67924395e-12  6.50726633e-12
 -9.56847341e-12 -2.42402990e-12  1.38188562e-11  6.13737372e-12
  1.36538580e-11  5.00609365e-11  1.85049487e-11  5.96134180e-12
 -2.41149371e+00]
supnorm grad right now is: 2.411493709119363
Weights right now are: 
[-1.86260730e+00 -3.15827102e+00 -3.95051346e+00 -2.95947532e+00
  1.07238811e+00  3.17273475e+00  4.52181958e+00  4.37455845e+00
  2.50247597e-02 -1.03307061e+00  5.91821920e-01 -5.51489400e-01
  1.23109255e-01 -2.28448448e+00  5.30955809e-01 -2.42491216e+00
 -1.11084880e+00 -2.06693594e+00 -4.66350882e-01 -2.25010940e+00
 -1.22798092e+00 -2.40475922e+00 -1.47709072e+00 -2.51227717e+00
  1.39932864e+00  5.85206234e-01  1.39647881e-02  5.94973187e-01
 -1.01114659e+00 -1.47609212e+00 -2.20330258e+00 -1.95819359e+00
  2.12627137e-01  1.00412399e+00  1.36879082e+00  1.13190082e+00
 -9.05994063e-01 -1.77982404e+00 -8.98258622e-01 -2.09361454e+00
  3.85134930e-01  2.72219129e-01  1.36489495e+00  1.65976713e+00
  1.16672106e+00 -1.63939125e+00 -1.58143220e-01 -1.01243240e+00
  4.91128949e-01 -8.94036162e-02 -5.24482705e-01 -7.78541806e-01
 -8.39142619e-01 -2.09520196e+00 -1.23007358e+00 -2.07207710e+00
  1.26025153e+00  1.69689732e+00 -5.85790012e-01 -1.81759011e+00
 -4.05804034e-01 -2.18607744e-01 -6.08383351e-01 -1.98131736e+00
  2.48296048e+01]
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3586.5768219732727
gradient value of function right now is: [ 1.46812445e-08  1.76250559e-08  1.76116136e-08  1.77005594e-08
 -1.46815029e-08 -1.76253670e-08 -1.76119245e-08 -1.77008719e-08
  1.69642021e-09  7.52690941e-09  2.86250029e-10  7.49585030e-09
  2.40992348e-10  1.07256724e-09  3.93172536e-11  1.06835907e-09
  3.48263546e-10  1.54541471e-09  5.86927580e-11  1.53904860e-09
  1.57485510e-10  6.97958954e-10  2.68973279e-11  6.95025505e-10
  2.05907617e-11  3.78009727e-10  3.03366907e-10  3.96225682e-10
 -4.15814745e-12  8.95936379e-11  6.30624237e-11  7.44006876e-11
 -2.17211757e-12 -1.29677804e-10 -9.91204087e-11 -1.25365278e-10
 -3.23357133e-12  1.26578160e-10  9.17172838e-11  1.10845294e-10
 -1.25411061e-11 -1.68253109e-11 -1.72909642e-11 -1.67942880e-11
 -2.99400976e-11  2.59828963e-11  5.40967475e-12  2.66185727e-11
 -3.38131807e-11  9.58193917e-11  5.22629123e-11  9.71531330e-11
 -3.19021201e-11  3.67647761e-12 -1.16101206e-11  4.18449917e-12
 -6.73013976e-12 -6.77881646e-12  9.29268357e-12  1.40297409e-11
  9.39248468e-12  5.21703097e-11  1.36711732e-11  1.26573064e-11
  6.71149951e+00]
supnorm grad right now is: 6.711499507852522
Weights right now are: 
[-1.86573146e+00 -3.16202479e+00 -3.95426455e+00 -2.96324545e+00
  1.07551227e+00  3.17648852e+00  4.52557068e+00  4.37832858e+00
  2.46606521e-02 -1.03467580e+00  5.91755904e-01 -5.53087311e-01
  1.23057251e-01 -2.28471424e+00  5.30946587e-01 -2.42514091e+00
 -1.11092353e+00 -2.06726541e+00 -4.66364412e-01 -2.25043738e+00
 -1.22801460e+00 -2.40490763e+00 -1.47709688e+00 -2.51242489e+00
  1.39932352e+00  5.85127677e-01  1.39011368e-02  5.94889017e-01
 -1.01114650e+00 -1.47611192e+00 -2.20331724e+00 -1.95821190e+00
  2.12628245e-01  1.00415178e+00  1.36881259e+00  1.13192912e+00
 -9.05994324e-01 -1.77985166e+00 -8.98279465e-01 -2.09364089e+00
  3.85137188e-01  2.72222509e-01  1.36489827e+00  1.65977051e+00
  1.16672485e+00 -1.63939724e+00 -1.58145941e-01 -1.01243848e+00
  4.91132834e-01 -8.94239183e-02 -5.24495328e-01 -7.78562324e-01
 -8.39137504e-01 -2.09520302e+00 -1.23007215e+00 -2.07207823e+00
  1.26025325e+00  1.69689815e+00 -5.85792465e-01 -1.81759196e+00
 -4.05806452e-01 -2.18617365e-01 -6.08386724e-01 -1.98131907e+00
  2.54429145e+01]
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.9163992712693
gradient value of function right now is: [ 1.61055209e-08  1.93426131e-08  1.93291295e-08  1.94272747e-08
 -1.61056503e-08 -1.93427688e-08 -1.93292850e-08 -1.94274311e-08
  1.88101168e-09  8.27422831e-09  3.46949090e-10  8.23570410e-09
  2.69304406e-10  1.18688558e-09  4.87307863e-11  1.18151652e-09
  3.86086372e-10  1.69857453e-09  7.11122529e-11  1.69068127e-09
  1.74019344e-10  7.65024816e-10  3.22845907e-11  7.61429976e-10
  2.72184413e-11  4.03312529e-10  3.27954566e-10  4.34377064e-10
  7.97790904e-13  1.02841565e-10  7.74271870e-11  9.79371891e-11
 -6.63172556e-12 -1.43017593e-10 -1.13025265e-10 -1.47750610e-10
  2.80728456e-12  1.43178508e-10  1.09543244e-10  1.39918084e-10
 -1.07983068e-11 -1.70870944e-11 -1.65034001e-11 -1.71248396e-11
 -1.50941844e-11  3.09414136e-11  1.62009883e-11  3.12171095e-11
 -1.51858118e-11  1.04176881e-10  6.72919978e-11  1.05088160e-10
 -2.36178988e-11  5.88789587e-12 -5.73752023e-12  6.15565565e-12
 -9.07904801e-12 -2.50291401e-12  1.30967884e-11  6.21708894e-12
  1.29279291e-11  4.82576219e-11  1.75903175e-11  5.99469589e-12
 -2.04973774e+00]
supnorm grad right now is: 2.049737736403332
Weights right now are: 
[-1.86879928e+00 -3.16570846e+00 -3.95794557e+00 -2.96694513e+00
  1.07858010e+00  3.18017219e+00  4.52925170e+00  4.38202826e+00
  2.43036814e-02 -1.03625066e+00  5.91691990e-01 -5.54655122e-01
  1.23006279e-01 -2.28493965e+00  5.30937677e-01 -2.42536534e+00
 -1.11099680e+00 -2.06758873e+00 -4.66377513e-01 -2.25075925e+00
 -1.22804767e+00 -2.40505339e+00 -1.47710284e+00 -2.51257000e+00
  1.39931863e+00  5.85050096e-01  1.38383179e-02  5.94806256e-01
 -1.01114631e+00 -1.47613122e+00 -2.20333145e+00 -1.95822947e+00
  2.12629238e-01  1.00417896e+00  1.36883384e+00  1.13195658e+00
 -9.05994447e-01 -1.77987866e+00 -8.98299755e-01 -2.09366633e+00
  3.85139459e-01  2.72225835e-01  1.36490159e+00  1.65977384e+00
  1.16672888e+00 -1.63940298e+00 -1.58148379e-01 -1.01244428e+00
  4.91137266e-01 -8.94438226e-02 -5.24507484e-01 -7.78582422e-01
 -8.39132184e-01 -2.09520405e+00 -1.23007056e+00 -2.07207933e+00
  1.26025498e+00  1.69689880e+00 -5.85794960e-01 -1.81759348e+00
 -4.05808777e-01 -2.18627159e-01 -6.08390098e-01 -1.98132047e+00
  2.50348312e+01]
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3587.980519563494
gradient value of function right now is: [ 1.57750593e-08  1.89394188e-08  1.89261741e-08  1.90221861e-08
 -1.57753228e-08 -1.89397359e-08 -1.89264910e-08 -1.90225046e-08
  1.84086705e-09  8.10099412e-09  3.37853502e-10  8.06352736e-09
  2.63544710e-10  1.16205502e-09  4.74175531e-11  1.15683867e-09
  3.77890024e-10  1.66319959e-09  6.92573737e-11  1.65552191e-09
  1.70403225e-10  7.49414686e-10  3.14666080e-11  7.45914962e-10
  2.64190105e-11  3.95937487e-10  3.21705091e-10  4.25761638e-10
  4.79462626e-13  1.00501998e-10  7.53908722e-11  9.50303020e-11
 -6.25255502e-12 -1.39973232e-10 -1.10407492e-10 -1.44069641e-10
  2.39224104e-12  1.40055106e-10  1.06832158e-10  1.36066247e-10
 -1.07470331e-11 -1.68333213e-11 -1.63213228e-11 -1.68662764e-11
 -1.58001481e-11  3.01711364e-11  1.52731359e-11  3.04664561e-11
 -1.61111134e-11  1.02100561e-10  6.53416653e-11  1.03025905e-10
 -2.37782656e-11  5.66080160e-12 -6.02517928e-12  5.94003000e-12
 -8.78247442e-12 -2.75312978e-12  1.26438592e-11  6.63460682e-12
  1.24940059e-11  4.77888101e-11  1.70745055e-11  6.34258531e-12
 -1.56284188e+00]
supnorm grad right now is: 1.5628418791735457
Weights right now are: 
[-1.87209018e+00 -3.16965754e+00 -3.96189180e+00 -2.97091133e+00
  1.08187100e+00  3.18412127e+00  4.53319793e+00  4.38599446e+00
  2.39209677e-02 -1.03793877e+00  5.91623978e-01 -5.56335730e-01
  1.22951612e-01 -2.28518135e+00  5.30928199e-01 -2.42560601e+00
 -1.11107537e+00 -2.06793533e+00 -4.66391454e-01 -2.25110432e+00
 -1.22808315e+00 -2.40520977e+00 -1.47710920e+00 -2.51272567e+00
  1.39931351e+00  5.84966581e-01  1.37706924e-02  5.94717318e-01
 -1.01114600e+00 -1.47615182e+00 -2.20334659e+00 -1.95824812e+00
  2.12630216e-01  1.00420804e+00  1.36885655e+00  1.13198586e+00
 -9.05994463e-01 -1.77990755e+00 -8.98321432e-01 -2.09369343e+00
  3.85141919e-01  2.72229409e-01  1.36490516e+00  1.65977742e+00
  1.16673332e+00 -1.63940904e+00 -1.58150881e-01 -1.01245042e+00
  4.91142170e-01 -8.94651365e-02 -5.24520518e-01 -7.78603947e-01
 -8.39126351e-01 -2.09520513e+00 -1.23006883e+00 -2.07208047e+00
  1.26025659e+00  1.69689961e+00 -5.85797272e-01 -1.81759520e+00
 -4.05810954e-01 -2.18638433e-01 -6.08393329e-01 -1.98132210e+00
  2.50677324e+01]
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.063346359652
gradient value of function right now is: [ 1.52550152e-08  1.83077650e-08  1.82947920e-08  1.83874583e-08
 -1.52548438e-08 -1.83075590e-08 -1.82945860e-08 -1.83872513e-08
  1.77655779e-09  7.82851806e-09  3.21373839e-10  7.79300725e-09
  2.54116612e-10  1.12220256e-09  4.49681500e-11  1.11727760e-09
  3.64738531e-10  1.60746989e-09  6.58906510e-11  1.60019173e-09
  1.64609758e-10  7.24845931e-10  2.99895615e-11  7.21521365e-10
  2.48342625e-11  3.84946441e-10  3.12080689e-10  4.12052044e-10
 -3.85876881e-13  9.64964585e-11  7.16154973e-11  8.93283034e-11
 -5.37139958e-12 -1.35168244e-10 -1.06031163e-10 -1.37624852e-10
  1.30667361e-12  1.34816025e-10  1.01937092e-10  1.28725267e-10
 -1.08709634e-11 -1.65118602e-11 -1.61938231e-11 -1.65320784e-11
 -1.81088002e-11  2.87973740e-11  1.31385622e-11  2.91491229e-11
 -1.90751821e-11  9.88910279e-11  6.16010995e-11  9.98725734e-11
 -2.48174845e-11  5.18359560e-12 -6.95230129e-12  5.50007888e-12
 -8.24317540e-12 -3.43607694e-12  1.17968368e-11  7.86934352e-12
  1.16763134e-11  4.76211434e-11  1.61336302e-11  7.37301384e-12
  7.80802401e-03]
supnorm grad right now is: 0.007808024010968609
Weights right now are: 
[-1.87507693e+00 -3.17324011e+00 -3.96547184e+00 -2.97450948e+00
  1.08485775e+00  3.18770384e+00  4.53677797e+00  4.38959262e+00
  2.35733880e-02 -1.03947072e+00  5.91560984e-01 -5.57860716e-01
  1.22901844e-01 -2.28540114e+00  5.30919366e-01 -2.42582484e+00
 -1.11114676e+00 -2.06825001e+00 -4.66404376e-01 -2.25141757e+00
 -1.22811538e+00 -2.40535173e+00 -1.47711508e+00 -2.51286698e+00
  1.39930862e+00  5.84891102e-01  1.37095673e-02  5.94636568e-01
 -1.01114597e+00 -1.47617070e+00 -2.20336065e+00 -1.95826569e+00
  2.12631290e-01  1.00423446e+00  1.36887729e+00  1.13201279e+00
 -9.05994776e-01 -1.77993393e+00 -8.98341423e-01 -2.09371872e+00
  3.85143963e-01  2.72232689e-01  1.36490829e+00  1.65978071e+00
  1.16673679e+00 -1.63941456e+00 -1.58153388e-01 -1.01245602e+00
  4.91145730e-01 -8.94844740e-02 -5.24532560e-01 -7.78623488e-01
 -8.39121710e-01 -2.09520611e+00 -1.23006757e+00 -2.07208151e+00
  1.26025813e+00  1.69690034e+00 -5.85799460e-01 -1.81759671e+00
 -4.05813176e-01 -2.18647964e-01 -6.08396376e-01 -1.98132365e+00
  2.49806692e+01]
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.0633303320133
gradient value of function right now is: [ 1.50537364e-08  1.80613379e-08  1.80485703e-08  1.81399438e-08
 -1.50535292e-08 -1.80610887e-08 -1.80483213e-08 -1.81396934e-08
  1.75286181e-09  7.72317231e-09  3.17185870e-10  7.68812005e-09
  2.50825217e-10  1.10752580e-09  4.44032471e-11  1.10266196e-09
  3.59906655e-10  1.58598304e-09  6.50385349e-11  1.57879811e-09
  1.62465145e-10  7.15317812e-10  2.96073951e-11  7.12035244e-10
  2.45415552e-11  3.80034534e-10  3.08121611e-10  4.06847720e-10
 -3.53811860e-13  9.52628912e-11  7.07250262e-11  8.82459032e-11
 -5.31798115e-12 -1.33326178e-10 -1.04604484e-10 -1.35792907e-10
  1.32186519e-12  1.33102246e-10  1.00670602e-10  1.27158726e-10
 -1.07123555e-11 -1.63030049e-11 -1.59793319e-11 -1.63233568e-11
 -1.77873687e-11  2.84210636e-11  1.30054571e-11  2.87663682e-11
 -1.87187567e-11  9.75819768e-11  6.08331831e-11  9.85482109e-11
 -2.44316808e-11  5.12195485e-12 -6.82876953e-12  5.43305442e-12
 -8.13796336e-12 -3.37011842e-12  1.16485554e-11  7.72758322e-12
  1.15342549e-11  4.69521806e-11  1.59276090e-11  7.24378103e-12
 -3.12290767e-02]
supnorm grad right now is: 0.031229076713867057
Weights right now are: 
[-1.87792198e+00 -3.17665060e+00 -3.96887991e+00 -2.97793477e+00
  1.08770279e+00  3.19111433e+00  4.54018604e+00  4.39301790e+00
  2.32426667e-02 -1.04092880e+00  5.91501736e-01 -5.59312264e-01
  1.22854520e-01 -2.28561028e+00  5.30911094e-01 -2.42603307e+00
 -1.11121467e+00 -2.06854949e+00 -4.66416525e-01 -2.25171571e+00
 -1.22814607e+00 -2.40548694e+00 -1.47712062e+00 -2.51300157e+00
  1.39930405e+00  5.84819001e-01  1.36511765e-02  5.94559540e-01
 -1.01114575e+00 -1.47618867e+00 -2.20337386e+00 -1.95828204e+00
  2.12632198e-01  1.00425965e+00  1.36889697e+00  1.13203826e+00
 -9.05994849e-01 -1.77995908e+00 -8.98360298e-01 -2.09374240e+00
  3.85146152e-01  2.72235786e-01  1.36491139e+00  1.65978381e+00
  1.16674058e+00 -1.63941998e+00 -1.58155774e-01 -1.01246150e+00
  4.91149878e-01 -8.95029933e-02 -5.24543934e-01 -7.78642188e-01
 -8.39116606e-01 -2.09520709e+00 -1.23006613e+00 -2.07208255e+00
  1.26025981e+00  1.69690093e+00 -5.85801880e-01 -1.81759819e+00
 -4.05815532e-01 -2.18657089e-01 -6.08399614e-01 -1.98132497e+00
  2.53231696e+01]
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.0590054402
gradient value of function right now is: [ 1.49159522e-08  1.78916559e-08  1.78790687e-08  1.79695545e-08
 -1.49158620e-08 -1.78915474e-08 -1.78789604e-08 -1.79694456e-08
  1.73707415e-09  7.65100932e-09  3.15163256e-10  7.61615761e-09
  2.48710875e-10  1.09777478e-09  4.41652622e-11  1.09293300e-09
  3.56695861e-10  1.57129928e-09  6.46297492e-11  1.56415477e-09
  1.61035460e-10  7.08793315e-10  2.94195728e-11  7.05529623e-10
  2.44690181e-11  3.76427577e-10  3.05323176e-10  4.03336617e-10
 -1.89217235e-13  9.45443443e-11  7.03364595e-11  8.79415902e-11
 -5.39299704e-12 -1.32072034e-10 -1.03728078e-10 -1.34792983e-10
  1.50142593e-12  1.32054097e-10  1.00046125e-10  1.26580797e-10
 -1.05241661e-11 -1.61298113e-11 -1.57715338e-11 -1.61523647e-11
 -1.71028416e-11  2.82261968e-11  1.31748723e-11  2.85569312e-11
 -1.78917722e-11  9.66639902e-11  6.05640697e-11  9.76065856e-11
 -2.38620025e-11  5.12699848e-12 -6.56257142e-12  5.42686811e-12
 -8.09805287e-12 -3.20979708e-12  1.16056298e-11  7.40215405e-12
  1.14925992e-11  4.62628720e-11  1.58438897e-11  6.96535288e-12
 -3.70525658e-01]
supnorm grad right now is: 0.3705256579056663
Weights right now are: 
[-1.88072560e+00 -3.18000988e+00 -3.97223683e+00 -2.98130863e+00
  1.09050642e+00  3.19447360e+00  4.54354296e+00  4.39639176e+00
  2.29165699e-02 -1.04236512e+00  5.91443008e-01 -5.60742116e-01
  1.22807810e-01 -2.28581650e+00  5.30902888e-01 -2.42623840e+00
 -1.11128164e+00 -2.06884450e+00 -4.66428562e-01 -2.25200940e+00
 -1.22817634e+00 -2.40562016e+00 -1.47712611e+00 -2.51313419e+00
  1.39929949e+00  5.84748049e-01  1.35936563e-02  5.94483532e-01
 -1.01114553e+00 -1.47620648e+00 -2.20338698e+00 -1.95829835e+00
  2.12633119e-01  1.00428447e+00  1.36891641e+00  1.13206349e+00
 -9.05994934e-01 -1.77998396e+00 -8.98379009e-01 -2.09376599e+00
  3.85148372e-01  2.72238803e-01  1.36491440e+00  1.65978683e+00
  1.16674399e+00 -1.63942543e+00 -1.58158318e-01 -1.01246699e+00
  4.91153713e-01 -8.95213378e-02 -5.24555271e-01 -7.78660692e-01
 -8.39111456e-01 -2.09520814e+00 -1.23006475e+00 -2.07208365e+00
  1.26026131e+00  1.69690137e+00 -5.85804037e-01 -1.81759928e+00
 -4.05817503e-01 -2.18666845e-01 -6.08402490e-01 -1.98132602e+00
  2.44880842e+01]
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1550.7177017322722
W_T_median: 1404.9332849160564
W_T_pctile_5: 624.2405425066908
W_T_CVAR_5_pct: 486.6279485662865
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.0
F value: -3588.0590054402
-----------------------------------------------
