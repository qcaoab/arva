Starting at: 
2022-06-20 18:03:38
tracing parameter entered from terminal:  1.4


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = basic_T30_VWD
timeseries_basket['basket_desc'] = CRSP data: T30 and VWD
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
5.0% of MC simulations done.
10.0% of MC simulations done.
15.0% of MC simulations done.
20.0% of MC simulations done.
25.0% of MC simulations done.
30.0% of MC simulations done.
35.0% of MC simulations done.
40.0% of MC simulations done.
45.0% of MC simulations done.
50.0% of MC simulations done.
55.00000000000001% of MC simulations done.
60.0% of MC simulations done.
65.0% of MC simulations done.
70.0% of MC simulations done.
75.0% of MC simulations done.
80.0% of MC simulations done.
85.0% of MC simulations done.
90.0% of MC simulations done.
95.0% of MC simulations done.
100.0% of MC simulations done.
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  hidden_layer    None       None         None   
4        obj.layers[4]        4  hidden_layer    None       None         None   
5        obj.layers[5]        5  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  
4    False        None  
5    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  hidden_layer       4  logistic_sigmoid   
4        obj.layers[4]        4  hidden_layer       4  logistic_sigmoid   
5        obj.layers[5]        5  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 4)    False        None  
4       (4, 4)    False        None  
5       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
[ 0.10347998  0.18844875 -0.3666709   0.62803316 -0.89369922 -0.17398511
  0.93797695  0.78704989 -0.60791207 -0.16293226 -0.31685198  0.28094308
  0.0404241   0.17453202  0.0940472   0.01169373  0.2300954  -0.23784803
  0.78511084 -0.41859022  0.4809118  -0.01155852  0.07219357 -0.14482284
  0.4701344  -0.18827011 -0.8138637  -0.24629474  0.28361272  0.33354483
 -0.63777411 -0.25343004 -0.81551088 -0.26234589  0.20198453 -0.15135741
  0.25264285 -0.29817275  0.41496663 -0.58895503 -0.60305385 -0.63454123
  0.3979274   0.74701442  0.57452389 -0.67377327  0.6493153   0.07340044
 -0.29694484  0.52758424 -0.14347696  0.04319718 -0.28527617 -0.63077443
  0.13133703 -0.58713125  0.12759166  0.87507529 -0.19770888 -0.66759109
  0.25529887 -0.48501395  0.58877641 -0.72315372] (64,)
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1260.6521530687958
W_T_median: 1222.3920506735662
W_T_pctile_5: 834.1581876304141
W_T_CVAR_5_pct: 746.6575474940535
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.312537072015
gradient value of function right now is: [ 5.07446744e-03  3.24224352e-02  3.75236559e-02  3.77715486e-02
 -5.07446744e-03 -3.24224352e-02 -3.75236559e-02 -3.77715486e-02
  4.74663032e-04  5.18503733e-03 -3.76143864e-04  5.14995366e-03
  9.30993575e-04  1.27335613e-02 -1.29260670e-03  1.26521911e-02
 -1.30037991e-04  4.77176151e-03 -1.23472191e-03  4.75247756e-03
 -8.33509522e-06  2.13930935e-03 -4.75155846e-04  2.12957376e-03
 -1.13077806e-04 -2.27184585e-04 -1.95160821e-04 -2.87440577e-04
 -1.00157360e-03  3.68406379e-04 -1.31172536e-04  3.63201579e-05
  2.64953608e-04 -8.46074684e-04 -4.67366815e-04 -8.22724270e-04
 -1.11892524e-03  6.02152973e-04 -1.84889635e-05  2.46584780e-04
 -2.74514914e-04  3.35323371e-05 -3.02003196e-05  3.85097688e-05
 -5.99968803e-04  2.57972518e-04  1.06010373e-04  2.89461736e-04
 -9.81562707e-04  4.94552111e-04  2.18485044e-04  5.28758743e-04
 -1.98552512e-03  8.86503241e-05 -3.44700443e-04  1.21731154e-04
 -2.42959878e-04 -5.55457020e-05  1.28518387e-04  1.55710681e-04
 -7.76748197e-06  1.82532358e-04  1.46228309e-04  1.14753930e-04
 -2.75769962e+00]
supnorm grad right now is: 2.757699622388893
Weights right now are: 
[-9.04922940e-01 -9.63882153e-01 -1.92267492e+00 -9.12120204e-01
  1.14703704e-01  9.78345791e-01  2.49398097e+00  2.32720325e+00
  2.81397739e-01  6.15075248e-01  5.89461087e-01  1.06925193e+00
  5.27140753e-01 -6.46956967e-01  8.24816938e-01 -8.27817319e-01
 -8.28265762e-01 -1.49731852e+00 -2.05609660e-01 -1.71260869e+00
 -8.55141598e-01 -1.76065537e+00 -1.07706930e+00 -1.95224848e+00
  1.37886423e+00  6.97598880e-01  1.16429640e-01  7.06935122e-01
 -9.54598022e-02 -1.28206290e+00 -1.45132882e+00 -1.66614429e+00
  1.96147858e-02  8.40004373e-01  1.15806416e+00  9.61682172e-01
 -3.72999167e-01 -1.53074419e+00 -4.15193807e-01 -1.76583980e+00
  3.69812880e-01  2.52672527e-01  1.34407072e+00  1.66129001e+00
  2.60315930e+00 -1.64788283e+00  1.27858311e-02 -1.08260418e+00
  1.62089026e+00  1.48030410e-01 -1.85808133e-01 -6.08602568e-01
  1.52282333e+00 -1.98517948e+00 -3.65667892e-01 -2.08029710e+00
  1.32695450e+00  3.08943648e+00  4.09477113e-01 -2.32694241e+00
 -1.07813550e-02 -6.97241358e-01 -1.50451448e-01 -2.59316244e+00
  2.47933990e+01]
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.618480135104
gradient value of function right now is: [ 1.12729648e-03  9.29372660e-03  8.80132758e-03  8.70948323e-03
 -1.12729648e-03 -9.29372660e-03 -8.80132758e-03 -8.70948323e-03
 -5.04323168e-05  1.62271365e-03 -4.32467613e-04  1.57164848e-03
 -6.89952023e-04  1.46575297e-03 -1.39446032e-03  1.35251753e-03
 -6.42054919e-04  6.98562211e-04 -1.15250307e-03  6.12251060e-04
 -2.59389488e-04  2.83571942e-04 -4.65901212e-04  2.48691189e-04
 -7.94161422e-05  8.58657614e-06 -1.21165767e-05 -1.95279862e-06
 -8.45272638e-04  6.30111810e-05 -1.50730119e-04 -4.13965055e-05
  2.01245841e-04 -1.80680011e-04 -8.73936170e-05 -1.67448639e-04
 -9.01177708e-04  1.20973044e-04 -1.20520600e-04  1.26034731e-05
  1.04442889e-04  8.94465983e-07  1.56608067e-05  3.77488075e-07
 -3.31409577e-04  5.52986708e-05  5.16609435e-06  6.18071672e-05
 -5.12972634e-04  1.04488868e-04  2.21912701e-05  1.11770426e-04
 -7.91822293e-04  1.95933766e-05 -9.37752171e-05  2.76720485e-05
 -7.23201276e-05 -6.20242458e-05  3.20333957e-05  6.58076065e-05
 -9.10767345e-06  9.18032789e-05  3.77335843e-05  6.06898997e-05
 -4.09474850e-01]
supnorm grad right now is: 0.4094748497492372
Weights right now are: 
[-9.21720624e-01 -1.12710209e+00 -2.08634955e+00 -1.07505461e+00
  1.31501387e-01  1.14156573e+00  2.65765559e+00  2.49013766e+00
  2.77895821e-01  5.20663942e-01  6.02195295e-01  9.77729123e-01
  5.50349284e-01 -1.05926903e+00  9.39583777e-01 -1.22802480e+00
 -8.17540003e-01 -1.54168211e+00 -1.81080462e-01 -1.75488277e+00
 -8.31550427e-01 -1.85753304e+00 -1.01860181e+00 -2.04311892e+00
  1.39309320e+00  7.03906795e-01  1.25248359e-01  7.13869926e-01
  2.92342539e-01 -1.35729128e+00 -1.37475961e+00 -1.65885806e+00
 -5.39542609e-03  8.88693626e-01  1.18265298e+00  9.91149108e-01
 -2.25514022e-01 -1.57826339e+00 -3.98455268e-01 -1.77393215e+00
  4.32087213e-01  2.48379308e-01  1.34979388e+00  1.65550739e+00
  3.13816622e+00 -1.76760702e+00 -2.04605655e-02 -1.20991401e+00
  2.14232407e+00 -1.57897509e-03 -2.40978134e-01 -7.80135309e-01
  2.40026860e+00 -2.03246976e+00 -1.67399011e-01 -2.14001277e+00
  2.04682349e+00  3.16941282e+00  7.04574457e-02 -2.48841209e+00
  5.74551967e-03 -1.02665488e+00 -3.56916402e-01 -2.72428442e+00
  2.49415367e+01]
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2667.568698394415
gradient value of function right now is: [-1.10130843e+00  1.39635674e-01 -9.65788350e-01 -7.83235489e-01
  1.10130843e+00 -1.39635674e-01  9.65788350e-01  7.83235489e-01
 -6.22535460e-03  6.13404514e-03 -6.46769731e-02  4.86930869e-03
 -3.61055383e-02  1.22755454e-01 -4.89298260e-01  1.23227673e-01
 -3.25502633e-01  1.27549655e-01 -3.22166053e+00  7.26246260e-02
 -1.86514235e-01  1.07105511e-01 -1.88403225e+00  7.67333262e-02
  1.87460589e-01 -3.02228614e-02 -2.46742058e-02 -3.68094491e-02
  2.03549359e-02  4.34089512e-02  3.27664510e-02  6.55618713e-02
  1.26761100e+00 -1.06288885e-01 -7.03266970e-02 -1.64061187e-01
 -1.32533375e-01  4.55491210e-02  3.37850388e-02  6.97700689e-02
 -2.43060346e-01 -1.42761263e-01 -1.16473522e-01 -1.34374521e-01
  1.28331439e-01  6.77832750e-02  5.50335840e-02  6.38846334e-02
  5.07734223e-02  4.30634773e-02  3.46175147e-02  4.07402935e-02
  1.86887174e-01  9.73214083e-02  7.94999425e-02  9.12837488e-02
 -5.56219571e-01  7.21842091e-01  2.23357002e-01 -2.91877374e-01
  1.29476675e-01 -1.71060382e-01  2.84722797e-01 -3.77094471e-01
 -1.06995350e+01]
supnorm grad right now is: 10.699534998555333
Weights right now are: 
[-0.4412396  -0.9292194  -1.42820654 -0.4015335  -0.34897964  0.94368304
  1.99951258  1.81661654  0.47652442  0.41682054  1.37905116  1.02883542
  1.60797007 -1.32498906  3.07699837 -0.67305242 -0.13593206 -1.46863482
  1.22030801 -1.21486224  0.80650742 -1.64200452  1.3291696  -0.70505812
  2.25433199  0.86012967  0.29158322  0.8361314   4.85318288 -1.7682479
 -1.29932158 -1.92022471 -1.59517116  1.1358255   1.2259117   1.2186012
  3.30154419 -1.77880291 -0.24400247 -1.9217013  -3.65828967  0.96852151
  1.96654532  2.43299022  4.58776804 -2.61774958 -0.8138441  -2.06761773
  3.97576424 -0.67478137 -0.88049023 -1.49812987  4.1417936  -2.88269418
 -0.85698942 -3.00766548  3.51003825  3.92032651 -1.10426901 -2.46703964
 -0.22044005 -2.26168392 -1.39556735 -2.82856727 24.74270019]
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2654.626179045276
gradient value of function right now is: [ 3.32764615e-06  3.01101364e-04  2.98493405e-04  2.99258638e-04
 -3.32764613e-06 -3.01101362e-04 -2.98493403e-04 -2.99258636e-04
 -7.16523249e-10  5.88436414e-07 -3.10240035e-08  5.89110010e-07
 -1.22736987e-07  4.45833111e-06 -4.40806445e-06  4.59448113e-06
 -2.65951906e-07  6.21634260e-06 -7.61626852e-06  5.98059870e-06
 -1.62644574e-07  4.42682476e-06 -4.87917678e-06  4.34031459e-06
  1.55589642e-07 -1.57199133e-08 -9.88026491e-09 -2.32240165e-08
  5.34535409e-06  2.06079482e-06  1.78772980e-06  3.03528460e-06
  6.32942681e-06 -8.31947092e-07 -5.55235524e-07 -1.23766799e-06
  1.17410202e-06  1.37547069e-06  1.14722308e-06  2.02954001e-06
 -1.62545313e-05 -2.50494539e-06 -2.49377613e-06 -9.14382405e-07
  1.34966221e-05  1.97489820e-06  1.96810476e-06  7.32815889e-07
  8.19231353e-06  1.42392807e-06  1.41862684e-06  5.17591151e-07
  1.86091363e-05  2.77244595e-06  2.76195325e-06  1.02077444e-06
 -1.31565482e-07  2.11389408e-06  1.34996192e-05 -1.79340539e-04
  1.11084091e-05 -1.42002403e-04  5.77585905e-06 -7.54699521e-05
 -8.27044779e+00]
supnorm grad right now is: 8.27044778854738
Weights right now are: 
[-0.47994059 -2.7888969  -2.20175777 -1.14582069 -0.31027865  2.80336053
  2.77306382  2.56090374  1.7357541   1.78205279  4.00630234  2.67670508
  0.91577585 -3.89083063  2.73540118 -2.68028663 -0.41323227 -3.25677495
  1.30833555 -2.44928353  0.79724117 -3.95612664  1.68356733 -1.87179804
  3.47939426  3.25744249  2.36643397  3.10646918  3.62263241 -3.98638571
 -3.32169998 -4.1158961  -1.34156816  3.35849499  3.00850654  3.41795034
  2.81784958 -3.78364501 -1.90852685 -3.89999633 -1.84467875  3.31111108
  4.15810921  4.88911096  2.77477614 -4.8785102  -2.90425717 -4.45055363
  2.64696623 -2.83582982 -2.87435938 -3.80370162  2.36563153 -5.09733946
 -2.91078097 -5.34210953  5.36730441  1.66539861 -3.29857151 -0.72684156
 -2.52842514 -0.66841794 -3.65606505 -0.95353829 24.27387619]
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.543510236505
gradient value of function right now is: [ 2.66838191e-06  2.28542414e-04  2.26870765e-04  2.27316980e-04
 -2.66838193e-06 -2.28542415e-04 -2.26870766e-04 -2.27316982e-04
  4.77710959e-11  4.55252600e-07 -1.25633412e-08  4.57947923e-07
  1.53431498e-08  4.37329485e-06 -1.23180562e-06  4.92562716e-06
 -1.31168701e-07  5.42173745e-06 -4.46113959e-06  5.55048092e-06
 -6.34481248e-08  3.91775751e-06 -2.56446936e-06  4.10876090e-06
  3.09677657e-08 -2.63839434e-08 -2.11704584e-08 -3.88638744e-08
  1.03805773e-05  2.40448071e-06  2.19996615e-06  3.53390768e-06
  2.14290888e-07 -1.37508616e-06 -1.12957754e-06 -2.03108044e-06
  5.85459750e-06  1.76030590e-06  1.57482457e-06  2.59019935e-06
 -2.20895660e-05 -2.67294198e-06 -2.38385352e-06 -7.47478763e-07
  1.81237450e-05  2.11308849e-06  1.89144226e-06  6.02200060e-07
  1.19996386e-05  1.57795949e-06  1.40671927e-06  4.41205965e-07
  2.54060984e-05  2.99036219e-06  2.67155661e-06  8.43637214e-07
 -9.32224766e-08  1.31372996e-06  2.36913390e-05 -2.42726571e-04
  1.63553740e-05 -1.67879520e-04  7.20622685e-06 -7.73001825e-05
 -1.60924183e+00]
supnorm grad right now is: 1.6092418282180094
Weights right now are: 
[-0.48116082 -2.82271114 -2.21706336 -1.16243372 -0.30905842  2.83717478
  2.78836941  2.57751677  1.73570342  1.78154391  4.0060484   2.67613522
  0.90685488 -3.92542411  2.7106692  -2.72094026 -0.41562394 -3.27374144
  1.30159633 -2.46606038  0.79396872 -3.98202262  1.67490301 -1.89446748
  3.48209657  3.25915     2.36862641  3.10808265  3.57826608 -4.00514446
 -3.3489715  -4.13391112 -1.32309668  3.37157483  3.02613978  3.43024079
  2.77592182 -3.80604516 -1.93959085 -3.92118931 -1.82402129  3.31222789
  4.15922533  4.88937151  2.74832106 -4.8800155  -2.90574695 -4.45092005
  2.61015444 -2.83799076 -2.87647649 -3.80420819  2.3389909  -5.09885211
 -2.91227762 -5.34247229  5.36730881  1.66535172 -3.30223081 -0.71586158
 -2.53213079 -0.65897867 -3.65671124 -0.9513123  24.88034861]
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.0441876583864
gradient value of function right now is: [ 4.16995705e-06  1.67029094e-04  1.74984511e-04  1.71763782e-04
 -4.16995707e-06 -1.67029096e-04 -1.74984513e-04 -1.71763784e-04
  1.04546915e-08  3.50666870e-07  2.31999300e-07  3.80244033e-07
  1.79930774e-06  1.08573871e-05  4.04031693e-05  1.60418629e-05
  1.62593599e-06  1.10584915e-05  3.65676536e-05  1.57115460e-05
  1.23676273e-06  8.17228794e-06  2.77970559e-05  1.17194682e-05
 -1.55222914e-06 -1.36119293e-07 -1.51759028e-07 -1.98781343e-07
  6.85482915e-05  6.24223217e-06  6.89145748e-06  9.11855167e-06
 -7.52947935e-05 -6.61318733e-06 -7.36824622e-06 -9.66130740e-06
  6.32117704e-05  5.67352831e-06  6.28663756e-06  8.28805738e-06
 -8.24094790e-05 -1.84629198e-06 -1.58912778e-06 -3.31993731e-07
  6.45976210e-05  1.48726374e-06  1.28357770e-06  2.74320172e-07
  5.40715877e-05  1.18426343e-06  1.01964286e-06  2.13085988e-07
  9.30139787e-05  2.11693458e-06  1.82441801e-06  3.85385120e-07
 -2.81910103e-08  3.86949662e-07  2.15592807e-05 -2.21529033e-04
  1.45040783e-05 -1.49767602e-04  4.15896129e-06 -4.33304373e-05
  4.21631992e+00]
supnorm grad right now is: 4.216319923711429
Weights right now are: 
[-0.48388856 -2.89649175 -2.25068017 -1.19886012 -0.30633068  2.91095539
  2.82198621  2.61394317  1.73558894  1.78042221  4.00547583  2.6748711
  0.88634095 -4.00163795  2.65402354 -2.81164561 -0.42103005 -3.31151105
  1.28641398 -2.50375662  0.78652804 -4.03948193  1.6552591  -1.94537983
  3.48829579  3.26306763  2.37365628  3.1117887   3.47895543 -4.04568616
 -3.40847804 -4.17287312 -1.28087308  3.40143135  3.06638842  3.4583238
  2.6811709  -3.85564872 -2.00871159 -3.96816822 -1.77760351  3.31378883
  4.16108847  4.88970519  2.68941605 -4.88181918 -2.90799183 -4.45130928
  2.52811081 -2.84103265 -2.88001489 -3.80489236  2.27950398 -5.10073848
 -2.9145934  -5.34287518  5.36728704  1.66538318 -3.30787614 -0.69716089
 -2.53893993 -0.64093702 -3.65758454 -0.94739701 25.26210835]
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.632597993311
gradient value of function right now is: [ 2.10157948e-06  1.10312258e-04  1.12733431e-04  1.11678706e-04
 -2.10157947e-06 -1.10312257e-04 -1.12733429e-04 -1.11678705e-04
  4.06899319e-09  2.41373578e-07  8.72631818e-08  2.53886543e-07
  6.88778826e-07  4.81396842e-06  1.51225833e-05  6.98330203e-06
  5.82146195e-07  4.99254061e-06  1.27828910e-05  6.81841503e-06
  4.53953490e-07  3.68076932e-06  9.96412317e-06  5.10679237e-06
 -5.66782216e-07 -5.03306005e-08 -5.74783769e-08 -7.35024499e-08
  2.80082119e-05  2.60157398e-06  2.93683334e-06  3.80205656e-06
 -2.78105612e-05 -2.47014076e-06 -2.81959908e-06 -3.60972002e-06
  2.57888156e-05  2.35018019e-06  2.66559118e-06  3.43455300e-06
 -3.24252894e-05 -5.18020661e-07 -5.57863955e-07 -1.89997765e-07
  2.60952384e-05  4.34097196e-07  4.67744803e-07  1.62525035e-07
  2.23080907e-05  3.47242779e-07  3.74222922e-07  1.27165572e-07
  3.75383012e-05  6.12795708e-07  6.60076041e-07  2.27171477e-07
 -2.41551889e-08  3.28879774e-07  5.20272358e-06 -5.47561977e-05
  4.37012958e-06 -4.61605872e-05  2.02181453e-06 -2.16275303e-05
 -2.34221337e-02]
supnorm grad right now is: 0.02342213369673516
Weights right now are: 
[-0.48864698 -3.01237033 -2.30653028 -1.258596   -0.30157226  3.02683397
  2.87783633  2.67367905  1.73538173  1.77871186  4.00440188  2.67299077
  0.8547446  -4.05344437  2.5653438  -2.88166555 -0.43130306 -3.35356528
  1.25656545 -2.55001365  0.77308368 -4.09673977  1.61852433 -2.00223064
  3.49861188  3.26773461  2.37967885  3.11621696  3.39286366 -4.06785738
 -3.43828725 -4.19440372 -1.21557642  3.43417189  3.11032499  3.48922112
  2.58481788 -3.88840937 -2.05141931 -3.99951677 -1.73496624  3.30779788
  4.1570843   4.88967161  2.64453279 -4.87383527 -2.90278061 -4.4514397
  2.46943893 -2.82932781 -2.87229933 -3.80496259  2.233353   -5.0925847
 -2.90923523 -5.34295033  5.36731968  1.66513552 -3.27963761 -0.76685135
 -2.51855117 -0.6838791  -3.6559991  -0.95086525 24.97681466]
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.3341356541973
gradient value of function right now is: [ 1.03596975e-07  7.88697347e-06  7.90352727e-06  7.89669542e-06
 -1.03597026e-07 -7.88697778e-06 -7.90353159e-06 -7.89669974e-06
  9.37104413e-11  3.51513508e-08  1.57221691e-09  3.54920639e-08
  5.33116806e-09  1.00045407e-07  9.81920404e-08  1.21676512e-07
  4.02455473e-09  9.26417067e-08  7.40256711e-08  1.08957966e-07
  3.41374316e-09  7.50734187e-08  6.28025746e-08  8.89168098e-08
 -1.36170981e-09 -1.29470864e-10 -1.35472567e-10 -1.85435589e-10
  2.22157438e-07  1.97051681e-08  2.07261512e-08  2.87931814e-08
 -1.61899804e-07 -1.40353574e-08 -1.48371739e-08 -2.05102633e-08
  2.21814504e-07  1.94820655e-08  2.05357027e-08  2.84617924e-08
 -2.54543794e-07 -5.12904044e-11 -4.53388979e-11 -6.81643692e-11
  1.95706181e-07  1.28819765e-10  1.72049840e-10  1.32588283e-10
  1.49403524e-07  5.36795099e-11  6.66248172e-11  5.94657761e-11
  2.83157153e-07  1.21477089e-10  1.55154432e-10  1.32717292e-10
 -1.75146935e-10  2.14120260e-09 -1.71509326e-09  1.46459966e-08
 -2.80116944e-09 -1.17388121e-08 -1.43160104e-09  1.65131555e-08
  2.97502035e+00]
supnorm grad right now is: 2.9750203523201364
Weights right now are: 
[-0.58438946 -3.41971963 -2.73400005 -1.6640817  -0.20582977  3.43418327
  3.3053061   3.07916475  1.73076319  1.76817299  3.9814589   2.65698758
  0.44280285 -3.93670778  2.05941107 -2.75897589 -0.6392916  -3.65242352
  0.75822885 -2.90203093  0.5147704  -4.27538528  1.07516952 -2.26638957
  3.68430002  3.37963839  2.49566025  3.22625327  3.42632783 -4.10967538
 -3.35890296 -4.24620594 -0.67735904  3.81730216  3.45785964  3.87573226
  2.51951584 -3.9416555  -1.98903197 -4.06816294 -1.51834316  3.52011924
  4.3897657   5.12969063  2.55581818 -5.08260115 -3.1267105  -4.69484095
  2.4974409  -3.04935551 -3.10523948 -4.06987527  2.1388792  -5.30802601
 -3.13979416 -5.59342535  5.47339869  1.25109905 -3.41652135 -0.70899188
 -2.78207676 -0.56591326 -3.84437145 -0.76288432 25.17131108]
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.5960072873145
gradient value of function right now is: [ 1.02817758e-07  7.77086156e-06  7.78764663e-06  7.78069725e-06
 -1.02817755e-07 -7.77086132e-06 -7.78764638e-06 -7.78069701e-06
  9.61658436e-11  3.48164301e-08  1.61664472e-09  3.51771094e-08
  5.53774483e-09  1.01573866e-07  1.00882007e-07  1.24370737e-07
  4.13139220e-09  9.31160890e-08  7.51987432e-08  1.10104712e-07
  3.52192710e-09  7.56940774e-08  6.41084525e-08  9.01819286e-08
 -1.41770430e-09 -1.36761914e-10 -1.43034463e-10 -1.96265889e-10
  2.35434830e-07  2.13689890e-08  2.24357278e-08  3.12371366e-08
 -1.66476925e-07 -1.48036461e-08 -1.56075294e-08 -2.16403278e-08
  2.32734022e-07  2.09472855e-08  2.20310820e-08  3.06151132e-08
 -2.73504737e-07 -3.51486062e-10 -9.60191191e-10 -3.02467954e-10
  2.10027784e-07  3.41122592e-10  8.44473581e-10  2.96248442e-10
  1.59867405e-07  2.20264075e-10  5.81275753e-10  1.89150607e-10
  3.03811633e-07  4.40056499e-10  1.14352035e-09  3.80027122e-10
 -1.67161994e-10  2.06924481e-09  2.91976035e-09 -4.18972177e-08
  7.94430005e-09 -1.39478006e-07  2.78740922e-09 -3.56609307e-08
 -1.06282827e+00]
supnorm grad right now is: 1.0628282704219738
Weights right now are: 
[-0.58484137 -3.44877098 -2.7468212  -1.67906643 -0.20537786  3.46323461
  3.31812724  3.09414948  1.73074647  1.76682629  3.981397    2.65572136
  0.44195368 -3.94034584  2.0584318  -2.76229976 -0.63949036 -3.65463877
  0.75786476 -2.90396758  0.51445578 -4.27827406  1.07465859 -2.26886644
  3.68439459  3.37971376  2.49574788  3.2263282   3.42385108 -4.11201337
 -3.36163834 -4.24848097 -0.67656158  3.81812392  3.45881418  3.8765249
  2.51706636 -3.94407576 -1.99182021 -4.07051353 -1.51665159  3.52027704
  4.39013138  5.12984468  2.5537486  -5.08280944 -3.1271823  -4.69504455
  2.49514043 -3.04960287 -3.1057943  -4.07011574  2.13684953 -5.30823285
 -3.14026206 -5.59362764  5.47341154  1.25104934 -3.41717812 -0.7079413
 -2.78367006 -0.56368117 -3.84498072 -0.76190277 24.94835301]
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2656.4134626370887
gradient value of function right now is: [ 7.12197974e-08  5.08262307e-06  5.09944793e-06  5.09267549e-06
 -7.12198363e-08 -5.08262633e-06 -5.09945119e-06 -5.09267874e-06
  9.51683440e-11  2.30447951e-08  1.60584879e-09  2.34138190e-08
  5.57986935e-09  8.10897249e-08  1.00550930e-07  1.04313089e-07
  4.10067631e-09  7.10748047e-08  7.38866730e-08  8.81188642e-08
  3.52042354e-09  5.85189687e-08  6.34153238e-08  7.31590780e-08
 -1.44130357e-09 -1.39807331e-10 -1.45828022e-10 -2.01987931e-10
  2.41331852e-07  2.26374298e-08  2.36598281e-08  3.31127057e-08
 -1.65821750e-07 -1.51234355e-08 -1.58877176e-08 -2.21179731e-08
  2.35867050e-07  2.18737164e-08  2.29093063e-08  3.19892590e-08
 -2.85285734e-07 -8.72158513e-10 -2.20265608e-09 -7.51354236e-10
  2.18917315e-07  7.31843713e-10  1.78354763e-09  6.33144852e-10
  1.65874501e-07  5.06923926e-10  1.26938440e-09  4.36112837e-10
  3.16462756e-07  1.00628787e-09  2.49998395e-09  8.68210831e-10
 -2.67107498e-10  3.02807141e-09  1.04612901e-08 -1.22848880e-07
  2.18583219e-08 -2.87767130e-07  1.02625966e-08 -1.15972231e-07
  6.00221918e+00]
supnorm grad right now is: 6.002219181402664
Weights right now are: 
[-0.58547011 -3.5140339  -2.77531595 -1.71251747 -0.20474912  3.52849754
  3.34662199  3.12760051  1.73074561  1.76418374  3.98139457  2.6533293
  0.4419383  -3.94228098  2.05839416 -2.76355256 -0.63949755 -3.65615276
  0.75784801 -2.90492365  0.51444635 -4.28011646  1.07463642 -2.27001033
  3.68439704  3.37971645  2.49575108  3.22632993  3.42380402 -4.11205513
 -3.36167722 -4.24852692 -0.67653267  3.81813815  3.45883097  3.87653924
  2.51701102 -3.94410677 -1.99184916 -4.07054984 -1.51662591  3.52028787
  4.3901225   5.12985963  2.55371107 -5.08282703 -3.12717605 -4.6950675
  2.49509985 -3.04961796 -3.10577774 -4.0701369   2.1368141  -5.30824858
 -3.14025286 -5.59364882  5.47341656  1.25103064 -3.41719571 -0.70794902
 -2.78356973 -0.5638628  -3.84501426 -0.76188675 25.37429608]
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2656.669289167371
gradient value of function right now is: [ 4.61270917e-08  3.94828571e-06  3.94584039e-06  3.94649682e-06
 -4.61270506e-08 -3.94828229e-06 -3.94583697e-06 -3.94649340e-06
 -8.18807012e-12  1.79608198e-08 -1.80809752e-10  1.79015406e-08
 -6.90205963e-10  2.83321284e-08 -1.05778501e-08  2.49451179e-08
 -4.48548422e-10  3.02228044e-08 -6.80795283e-09  2.79850518e-08
 -4.05284551e-10  2.39378959e-08 -6.18358765e-09  2.19291982e-08
  1.84242144e-10  1.73995578e-11  1.80571758e-11  2.75268463e-11
 -3.75798067e-08 -4.29992885e-09 -4.42792462e-09 -6.31652336e-09
  1.80220806e-08  2.28763558e-09  2.32557788e-09  3.36268671e-09
 -3.29464293e-08 -3.88774932e-09 -3.98668030e-09 -5.71490189e-09
  5.12083104e-08  6.38864289e-10  1.98433129e-09  5.00627198e-10
 -3.83965947e-08 -4.55504221e-10 -1.46435905e-09 -3.53442150e-10
 -2.82432092e-08 -3.55427176e-10 -1.11880146e-09 -2.77773127e-10
 -5.54952000e-08 -6.80424596e-10 -2.14831232e-09 -5.30827190e-10
  5.30254884e-12  4.32238197e-12 -9.98254231e-09  1.20601691e-07
 -2.35242748e-08  2.77124173e-07 -9.11229679e-09  1.11156077e-07
 -4.89511542e+00]
supnorm grad right now is: 4.895115418321019
Weights right now are: 
[-0.58693788 -3.64030404 -2.83100763 -1.77776581 -0.20328136  3.65476767
  3.40231367  3.19284885  1.73072814  1.76057468  3.98130636  2.64983346
  0.44099871 -3.94958381  2.05693505 -2.76989657 -0.63977456 -3.6610808
  0.7573216  -2.90889336  0.51403956 -4.28605222  1.07388755 -2.27490685
  3.68452538  3.37975386  2.49579041  3.22638049  3.42011908 -4.11508575
 -3.36512171 -4.25168708 -0.6753532   3.81929419  3.46016273  3.87768364
  2.51335934 -3.94717621 -1.99528824 -4.07378039 -1.51412419  3.52048227
  4.3905984   5.13004282  2.55064085 -5.08306003 -3.12773662 -4.69528193
  2.49168493 -3.04984161 -3.10632631 -4.07033569  2.13380276 -5.30850024
 -3.14085417 -5.59388482  5.47343076  1.25097189 -3.41805914 -0.70651189
 -2.78574864 -0.56067455 -3.84580566 -0.7605627  24.60176379]
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.6265524246874
gradient value of function right now is: [ 2.69461444e-08  1.89545038e-06  1.90214076e-06  1.89947770e-06
 -2.69461342e-08 -1.89544953e-06 -1.90213992e-06 -1.89947686e-06
  3.69454712e-11  8.82157505e-09  6.38804799e-10  8.96545296e-09
  2.36721985e-09  3.25834817e-08  4.31527093e-08  4.23750488e-08
  1.68157173e-09  2.75952933e-08  3.06314689e-08  3.45473788e-08
  1.46941232e-09  2.31281037e-08  2.67691247e-08  2.92041058e-08
 -6.28334711e-10 -5.85870944e-11 -6.15290702e-11 -8.46966969e-11
  9.91606685e-08  8.92818573e-09  9.39669092e-09  1.30507579e-08
 -7.02259840e-08 -6.23906833e-09 -6.58511976e-09 -9.11990343e-09
  9.80377905e-08  8.77696586e-09  9.24870278e-09  1.28281198e-08
 -1.14745448e-07 -8.14720617e-11 -2.99790312e-10 -6.52431601e-11
  8.82157413e-08  8.24017175e-11  2.61204191e-10  6.78756920e-11
  6.72686819e-08  5.17960389e-11  1.82736003e-10  4.14409253e-11
  1.27662643e-07  1.04706235e-10  3.56752003e-10  8.49740890e-11
 -4.59041735e-11  5.42040542e-10  5.81270101e-10 -1.11256716e-08
  2.60406244e-09 -5.21065832e-08  4.63509311e-10 -7.79089629e-09
 -4.13352042e-01]
supnorm grad right now is: 0.4133520417851028
Weights right now are: 
[-0.5885272  -3.80366527 -2.90603527 -1.86542648 -0.20169204  3.81812893
  3.47734133  3.28050954  1.73073096  1.75830851  3.98131776  2.64761401
  0.44118681 -3.95190482  2.05691773 -2.77161537 -0.63973645 -3.66350117
  0.75729809 -2.91065494  0.51411954 -4.28821723  1.07386525 -2.27654863
  3.68450159  3.37974777  2.49578398  3.22637149  3.42061148 -4.1141644
 -3.36408175 -4.25073758 -0.67534071  3.81904669  3.45986624  3.87749335
  2.51371266 -3.94628765 -1.99430553 -4.07284473 -1.51453149  3.52041287
  4.3902833   5.12998816  2.5511563  -5.08299519 -3.12742114 -4.6952352
  2.49226928 -3.04978191 -3.10604611 -4.07029298  2.13429397 -5.30841775
 -3.14047353 -5.59382184  5.47343548  1.25095204 -3.41764024 -0.70722817
 -2.78400361 -0.56322454 -3.84545579 -0.76117812 24.96107615]
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2655.543367675812
gradient value of function right now is: [ 1.19613796e-08  7.13882079e-07  7.19464899e-07  7.17317815e-07
 -1.19614176e-08 -7.13885219e-07 -7.19468039e-07 -7.17320955e-07
  2.98037101e-11  3.38443772e-09  5.21986910e-10  3.50361320e-09
  2.00918668e-09  2.00629339e-08  3.64130960e-08  2.83647917e-08
  1.40580242e-09  1.56808059e-08  2.54861933e-08  2.14839633e-08
  1.24443314e-09  1.35151785e-08  2.25532335e-08  1.86544365e-08
 -5.43305192e-10 -5.04245085e-11 -5.27303631e-11 -7.34532813e-11
  8.41470810e-08  7.76709198e-09  8.11812446e-09  1.13740008e-08
 -5.91552037e-08 -5.33386005e-09 -5.59863933e-09 -7.80869928e-09
  8.25567981e-08  7.54785200e-09  7.90258644e-09  1.10510550e-08
 -9.91937495e-08 -2.72996530e-10 -6.47591474e-10 -2.35674007e-10
  7.58889567e-08  2.21191273e-10  5.13819414e-10  1.91462321e-10
  5.75106418e-08  1.54110362e-10  3.66709786e-10  1.32643207e-10
  1.09843979e-07  3.09141112e-10  7.27077194e-10  2.67055362e-10
 -8.58628621e-11  9.16857731e-10  3.38586167e-09 -3.93766520e-08
  6.55450135e-09 -8.54380307e-08  3.33390941e-09 -3.72599988e-08
  8.05259556e+00]
supnorm grad right now is: 8.052595564451979
Weights right now are: 
[-0.58960075 -3.98185302 -3.00606577 -1.97955666 -0.20061848  3.9963168
  3.57737189  3.39463979  1.73074969  1.7571343   3.98156001  2.64652652
  0.44238772 -3.94587103  2.06150372 -2.76234313 -0.63905377 -3.66037752
  0.75874979 -2.90585865  0.51481686 -4.28487705  1.0760952  -2.27102195
  3.68421459  3.37971458  2.49574951  3.22632287  3.43282337 -4.10861265
 -3.35824123 -4.24321637 -0.67870077  3.81611797  3.45671678  3.87395311
  2.52569375 -3.94104439 -1.98880884 -4.06565752 -1.5223323   3.51994247
  4.38897363  5.12958961  2.56128132 -5.08260567 -3.12632571 -4.69491199
  2.50411203 -3.04948357 -3.10520109 -4.07004757  2.14378366 -5.30788123
 -3.13897869 -5.5933713   5.47339558  1.25107191 -3.41421761 -0.71193821
 -2.77476401 -0.57570767 -3.84236071 -0.76540817 25.53841967]
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.4806343324035
gradient value of function right now is: [ 5.97143298e-09  4.11002095e-07  4.12595585e-07  4.11952879e-07
 -5.97141513e-09 -4.11000622e-07 -4.12594112e-07 -4.11951406e-07
  9.17722438e-12  1.96387909e-09  1.57523289e-10  2.00085955e-09
  6.25914989e-10  8.21211605e-09  1.11967122e-08  1.08744781e-08
  4.30836306e-10  6.72067372e-09  7.69392698e-09  8.55385393e-09
  3.87060983e-10  5.79082364e-09  6.91653563e-09  7.43730123e-09
 -1.71093299e-10 -1.61752659e-11 -1.69505929e-11 -2.35261188e-11
  2.69846118e-08  2.44401521e-09  2.57114753e-09  3.58791202e-09
 -1.81957001e-08 -1.66258093e-09 -1.74793107e-09 -2.44125235e-09
  2.63766235e-08  2.39686407e-09  2.52100294e-09  3.51859907e-09
 -3.11597444e-08  2.39552584e-11 -4.95610678e-11  2.66860597e-11
  2.39843078e-08 -1.44166715e-11  4.55262317e-11 -1.69782439e-11
  1.82205948e-08 -1.06662810e-11  3.72603463e-11 -1.27823370e-11
  3.48604092e-08 -2.27918029e-11  6.43363744e-11 -2.63950829e-11
  3.59230708e-12 -3.00309961e-11 -3.83877707e-10  1.48250805e-09
  6.02367015e-10 -2.00605536e-08 -4.85899416e-10  3.34990188e-09
  2.09661253e+00]
supnorm grad right now is: 2.0966125347241915
Weights right now are: 
[-0.59278931 -4.11183179 -3.10985142 -2.08979456 -0.19742993  4.12629559
  3.68115756  3.50487771  1.73073253  1.75642661  3.98127248  2.64574544
  0.44122833 -3.95558396  2.04649878 -2.7768099  -0.63983099 -3.66727181
  0.75177263 -2.91586414  0.51411024 -4.29104605  1.06768357 -2.28012743
  3.68452691  3.37974593  2.49578204  3.22636876  3.39474626 -4.11376085
 -3.36358752 -4.25071375 -0.6623058   3.81931887  3.46005425  3.87854648
  2.48888866 -3.94591765 -1.99387136 -4.0727621  -1.48861985  3.52033651
  4.39002248  5.1299133   2.52747793 -5.08290942 -3.1271334  -4.69516119
  2.47445207 -3.04970842 -3.1058018  -4.07023164  2.10441033 -5.30831716
 -3.14013878 -5.59372914  5.47344504  1.25063787 -3.41922879 -0.69047193
 -2.78570139 -0.52669576 -3.84703657 -0.745895   25.13003906]
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2655.4799129537987
gradient value of function right now is: [ 3.94090652e-09  3.37021110e-07  3.36801402e-07  3.36864870e-07
 -3.94088951e-09 -3.37019706e-07 -3.36799999e-07 -3.36863467e-07
 -1.02999547e-12  1.59616666e-09 -1.92999789e-11  1.58948312e-09
 -7.96921506e-11  2.52572348e-09 -1.14802428e-09  2.12322552e-09
 -4.93543907e-11  2.69467901e-09 -7.01409960e-10  2.44165248e-09
 -4.67423712e-11  2.20398588e-09 -6.69207955e-10  1.96588943e-09
  2.19935056e-11  2.47053873e-12  2.51243727e-12  3.82505083e-12
 -4.60990268e-09 -5.73446942e-10 -5.83438959e-10 -8.44782026e-10
  1.95848558e-09  2.72026974e-10  2.73522267e-10  4.01047526e-10
 -3.89009779e-09 -4.99236258e-10 -5.06104050e-10 -7.35917838e-10
  6.48792254e-09  9.77518932e-11  3.07803208e-10  7.73503609e-11
 -4.89099526e-09 -7.22683553e-11 -2.31029578e-10 -5.69609799e-11
 -3.55643673e-09 -5.46561342e-11 -1.72945581e-10 -4.32201109e-11
 -7.06094267e-09 -1.06199982e-10 -3.36687910e-10 -8.38958711e-11
  3.87214949e-12 -4.45045867e-11 -1.44163333e-09  1.73717197e-08
 -3.50573657e-09  4.12659897e-08 -1.31842961e-09  1.60073825e-08
 -7.12938278e+00]
supnorm grad right now is: 7.129382776335893
Weights right now are: 
[-0.59337561 -4.17886823 -3.17329032 -2.15439039 -0.19684362  4.19333212
  3.74459654  3.56947363  1.73073453  1.75611062  3.98130672  2.64543874
  0.44136554 -3.95517334  2.04857202 -2.77579268 -0.63973974 -3.66719836
  0.75285585 -2.91538794  0.51419358 -4.29092839  1.06888917 -2.27963975
  3.68448948  3.37974215  2.49577811  3.22636316  3.40055462 -4.11308701
 -3.36288745 -4.24972349 -0.66490952  3.81893108  3.4596509   3.87797888
  2.4943219  -3.94529439 -1.99322371 -4.07184586 -1.49442434  3.52030797
  4.38983158  5.12989393  2.53282251 -5.08288788 -3.12698774 -4.69514662
  2.47877281 -3.04969227 -3.10569283 -4.07022074  2.11113086 -5.30828568
 -3.1399275  -5.5937078   5.47344491  1.25063286 -3.41882639 -0.69224445
 -2.78359596 -0.53996981 -3.84672308 -0.74704014 24.43849408]
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.62150904577
gradient value of function right now is: [ 4.57299668e-09  2.39828629e-07  2.42655121e-07  2.41582741e-07
 -4.57296302e-09 -2.39825852e-07 -2.42652344e-07 -2.41579964e-07
  1.48700421e-11  1.16693155e-09  2.65386383e-10  1.22714588e-09
  1.05653361e-09  9.35326069e-09  1.93607215e-08  1.37360670e-08
  7.43592788e-10  7.13917664e-09  1.36411689e-08  1.02176777e-08
  6.67903387e-10  6.29115763e-09  1.22460141e-08  9.05844755e-09
 -2.88467065e-10 -2.66729838e-11 -2.79712450e-11 -3.85309901e-11
  4.46232010e-08  4.15020518e-09  4.34223559e-09  6.01341258e-09
 -3.15867596e-08 -2.86128211e-09 -3.00859345e-09 -4.14426850e-09
  4.38720404e-08  4.03867051e-09  4.23387215e-09  5.85090758e-09
 -5.25781091e-08 -2.66427125e-10 -3.43494733e-10 -2.44689778e-10
  4.05560564e-08  2.13656232e-10  2.74769849e-10  1.96189722e-10
  3.07771166e-08  1.52458887e-10  1.96117989e-10  1.40098523e-10
  5.81279305e-08  3.00417504e-10  3.86793515e-10  2.75877901e-10
 -1.70147164e-10  1.83413598e-09  3.03307429e-09 -3.12174842e-08
  3.10603780e-09 -3.24379150e-08  3.20739414e-09 -3.28528944e-08
  5.90440524e-01]
supnorm grad right now is: 0.5904405237904522
Weights right now are: 
[-0.59235654 -4.21828603 -3.21021617 -2.19227705 -0.19786269  4.23274997
  3.78152243  3.60736033  1.73074773  1.75596079  3.9815293   2.64534875
  0.4422628  -3.9483467   2.06360447 -2.76491993 -0.63913806 -3.66277932
  0.76282701 -2.90825606  0.51474496 -4.28683701  1.07811123 -2.27306155
  3.68424314  3.37971598  2.4957512   3.22632477  3.44370494 -4.10846031
 -3.35813239 -4.24294282 -0.68951101  3.8162686   3.4569134   3.87407734
  2.53412281 -3.94103258 -1.98884295 -4.06559978 -1.54865489  3.51972901
  4.3886471   5.1293976   2.57476372 -5.08244368 -3.12607961 -4.69476577
  2.50995408 -3.04936417 -3.10502049 -4.06993958  2.17124836 -5.30764551
 -3.13861756 -5.59315902  5.47331565  1.25199174 -3.4106652  -0.7797813
 -2.77036764 -0.68507067 -3.83868991 -0.83262839 25.04878034]
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.434828416725
gradient value of function right now is: [ 1.53300032e-09  1.82929362e-07  1.81601425e-07  1.82075387e-07
 -1.53295514e-09 -1.82925637e-07 -1.81597701e-07 -1.82071663e-07
 -5.90892875e-12  8.61723238e-10 -1.15174870e-10  8.36311251e-10
 -4.73215632e-10 -9.09910121e-10 -8.79979412e-09 -2.85418085e-09
 -3.33901813e-10 -1.64273310e-10 -6.20224639e-09 -1.53960059e-09
 -3.00149634e-10 -2.50102141e-10 -5.57871239e-09 -1.48522507e-09
  1.29570074e-10  1.08294085e-11  1.15587483e-11  1.57812880e-11
 -1.92677925e-08 -1.60353323e-09 -1.71770474e-09 -2.32275203e-09
  1.43189660e-08  1.24003684e-09  1.31739422e-09  1.79710051e-09
 -1.94026752e-08 -1.64192139e-09 -1.75253754e-09 -2.37891129e-09
  2.17919584e-08 -4.94647723e-11 -2.52787088e-11 -5.62103768e-11
 -1.66615104e-08  4.42792751e-11  2.70129977e-11  4.89112940e-11
 -1.28456782e-08  2.75100412e-11  1.25527895e-11  3.16672710e-11
 -2.40085055e-08  5.92973058e-11  3.35377256e-11  6.63406066e-11
 -7.02093139e-11  7.78577981e-10  1.55324553e-10  1.05636817e-09
 -2.92051834e-10  6.62994288e-09  3.48981000e-10 -1.19649402e-09
 -2.28049595e+00]
supnorm grad right now is: 2.2804959473774686
Weights right now are: 
[-0.59310337 -4.25923126 -3.2515803  -2.2334848  -0.19711587  4.27369517
  3.82288653  3.64856805  1.73074549  1.75576267  3.98148819  2.645142
  0.44209769 -3.94973308  2.0604886  -2.76696744 -0.63925584 -3.66387085
  0.76061365 -2.90982023  0.51463977 -4.28778963  1.07612958 -2.27443583
  3.68428822  3.3797199   2.49575536  3.22633038  3.43715803 -4.10901886
 -3.35872646 -4.24374309 -0.68445139  3.81670514  3.45737734  3.87470431
  2.5274796  -3.94159996 -1.98944657 -4.06641335 -1.54119051  3.51973913
  4.38865653  5.12940591  2.56900902 -5.08245315 -3.1260888  -4.69477376
  2.50551482 -3.04937142 -3.1050278  -4.06994565  2.16305932 -5.3076576
 -3.13862885 -5.5931691   5.47332893  1.25184452 -3.41078827 -0.77830957
 -2.77045843 -0.68391364 -3.83880388 -0.83126629 24.81580759]
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2656.183451210218
gradient value of function right now is: [ 1.80513808e-09  1.58448120e-07  1.58216992e-07  1.58291877e-07
 -1.80519349e-09 -1.58452686e-07 -1.58221557e-07 -1.58296442e-07
 -9.18867666e-13  7.57989814e-10 -1.91155707e-11  7.53085305e-10
 -8.13893736e-11  1.07222242e-09 -1.42520656e-09  7.10593786e-10
 -5.50317000e-11  1.19035421e-09 -9.61624353e-10  9.44224124e-10
 -5.04502585e-11  9.78458823e-10 -8.82871710e-10  7.53399111e-10
  2.25204603e-11  1.91213853e-12  2.02470671e-12  2.89160511e-12
 -3.67297012e-09 -3.29303963e-10 -3.49258757e-10 -4.83407399e-10
  2.34649134e-09  2.27094646e-10  2.37349129e-10  3.33641180e-10
 -3.54888681e-09 -3.27679146e-10 -3.45469064e-10 -4.81287195e-10
  4.32277816e-09 -1.53521356e-13  2.08317802e-11 -5.56363637e-12
 -3.28567454e-09  2.64845157e-12 -1.28240468e-11  6.56791011e-12
 -2.49058373e-09 -2.62702520e-13 -1.27214435e-11  2.91307587e-12
 -4.78776692e-09  1.98396728e-12 -2.09670502e-11  7.84654485e-12
 -1.54030414e-11  1.69048530e-10 -1.45493856e-10  2.92042034e-09
 -4.06577921e-10  6.09425827e-09 -6.06116974e-11  1.97730881e-09
 -6.00367374e+00]
supnorm grad right now is: 6.003673740342826
Weights right now are: 
[-0.59485898 -4.30022678 -3.29461706 -2.27574076 -0.19536026  4.31469059
  3.8659232   3.69082391  1.73073433  1.7555417   3.98129072  2.64487315
  0.4412979  -3.95574833  2.04625168 -2.77641814 -0.63980998 -3.6681074
  0.75074434 -2.91643456  0.51413665 -4.29162238  1.06716986 -2.28042852
  3.68450762  3.37974096  2.49577731  3.22636107  3.40162446 -4.11245796
 -3.36230448 -4.24875782 -0.66108775  3.81893529  3.45970331  3.87795477
  2.4932643  -3.9448942  -1.99287702 -4.07121614 -1.49851835  3.52001623
  4.38905652  5.12964988  2.53613532 -5.08266789 -3.12639843 -4.69496291
  2.48077877 -3.04952932 -3.10525607 -4.07008457  2.11573162 -5.30796573
 -3.13907348 -5.59344044  5.47344757  1.2506233  -3.41453478 -0.73774869
 -2.77477685 -0.63645077 -3.84258591 -0.79060067 24.49289762]
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.557869295819
gradient value of function right now is: [ 2.04961379e-09  1.21674227e-07  1.22600281e-07  1.22238522e-07
 -2.04959377e-09 -1.21672577e-07 -1.22598631e-07 -1.22236873e-07
  5.23505409e-12  5.95098038e-10  9.09380835e-11  6.16476751e-10
  3.67546772e-10  3.75295792e-09  6.62134685e-09  5.30635538e-09
  2.57918244e-10  2.93998962e-09  4.65125501e-09  4.02776231e-09
  2.33981667e-10  2.59867727e-09  4.21702240e-09  3.58636096e-09
 -1.00783061e-10 -9.46842274e-12 -9.89541739e-12 -1.37759504e-11
  1.59811777e-08  1.50528617e-09  1.56968683e-09  2.20031854e-09
 -1.08218186e-08 -9.88286317e-10 -1.03657315e-09 -1.44392726e-09
  1.55731927e-08  1.44943360e-09  1.51487617e-09  2.11823242e-09
 -1.87237177e-08 -7.21732925e-11 -8.76249647e-11 -6.86618919e-11
  1.44736433e-08  5.92853275e-11  7.20142595e-11  5.62624671e-11
  1.09378698e-08  4.08134133e-11  4.93207057e-11  3.88691466e-11
  2.09012311e-08  8.30572804e-11  1.00868842e-10  7.89107150e-11
 -4.94014335e-11  5.19667648e-10  7.85855633e-10 -7.52680587e-09
  7.35303649e-10 -7.26376475e-09  8.70132197e-10 -8.27419550e-09
  1.53259525e+00]
supnorm grad right now is: 1.5325952499710753
Weights right now are: 
[-0.59471868 -4.32348563 -3.31725779 -2.29862526 -0.19550056  4.33794945
  3.88856395  3.71370843  1.73073807  1.75544056  3.98135438  2.64478919
  0.44155792 -3.9539231   2.05063825 -2.77341207 -0.63963471 -3.66695813
  0.75370034 -2.91448892  0.51429815 -4.29054255  1.069894   -2.27861501
  3.68443595  3.37973353  2.49576966  3.2263501   3.41420113 -4.11114744
 -3.36095629 -4.24682703 -0.66835567  3.81817661  3.45892304  3.87683676
  2.50493583 -3.94367759 -1.99162551 -4.06942357 -1.51413434  3.51987436
  4.38882914  5.1295285   2.54815963 -5.08255913 -3.12622393 -4.69486988
  2.48969323 -3.04944864 -3.10512651 -4.07001559  2.13317643 -5.30780775
 -3.13882012 -5.5933053   5.47341728  1.25094536 -3.41248446 -0.76077355
 -2.77217015 -0.66588473 -3.84057067 -0.81316812 25.11680206]
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.52717844932
gradient value of function right now is: [ 1.11132687e-09  1.04238768e-07  1.03973680e-07  1.04068052e-07
 -1.11134342e-09 -1.04240131e-07 -1.03975043e-07 -1.04069414e-07
 -1.28392122e-12  4.99627801e-10 -2.43488148e-11  4.93211307e-10
 -1.02771560e-10  3.28136570e-10 -1.76757495e-09 -1.34513919e-10
 -6.94813180e-11  5.36438872e-10 -1.19379646e-09  2.22492649e-10
 -6.40516043e-11  4.16562749e-10 -1.10140292e-09  1.27571131e-10
  2.84263447e-11  2.64207227e-12  2.76253446e-12  3.95067786e-12
 -4.80087986e-09 -4.58630938e-10 -4.79896040e-10 -6.75769895e-10
  2.91964752e-09  2.91120960e-10  3.02200857e-10  4.29105443e-10
 -4.55746880e-09 -4.42465496e-10 -4.61527059e-10 -6.52116332e-10
  5.75961883e-09  1.95203314e-11  4.67472348e-11  1.23013896e-11
 -4.40364665e-09 -1.28985520e-11 -3.33253809e-11 -7.53731215e-12
 -3.30482961e-09 -1.13134886e-11 -2.71618093e-11 -7.13726873e-12
 -6.41809308e-09 -2.02999810e-11 -5.03909405e-11 -1.23636827e-11
 -8.95106029e-12  9.33645407e-11 -3.92424222e-10  5.41622847e-09
 -6.60072058e-10  8.57895074e-09 -3.18433851e-10  4.59784895e-09
 -1.72905056e+00]
supnorm grad right now is: 1.7290505577822721
Weights right now are: 
[-0.59581764 -4.35231946 -3.34730067 -2.32820112 -0.19440159  4.36678331
  3.91860685  3.74328432  1.73073135  1.75528719  3.98123616  2.64460685
  0.44107164 -3.95762499  2.04204737 -2.779223   -0.63997072 -3.66957807
  0.74776419 -2.9185659   0.5139913  -4.29292021  1.064473   -2.28232334
  3.6845698   3.37974629  2.49578297  3.22636879  3.39242183 -4.11321725
 -3.36311672 -4.24986005 -0.65424622  3.81951776  3.46032283  3.87880209
  2.48394662 -3.94567272 -1.9937079  -4.07234719 -1.4883027   3.51997859
  4.38899837  5.12961447  2.52827747 -5.08263962 -3.12635451 -4.69493628
  2.47471691 -3.04950926 -3.10522481 -4.07006562  2.10438023 -5.30792408
 -3.13900893 -5.59340124  5.47343526  1.25071914 -3.41396598 -0.74294549
 -2.77409299 -0.64271473 -3.84197322 -0.79629192 24.83790278]
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2655.4497424669144
gradient value of function right now is: [ 1.56541961e-09  7.91537908e-08  8.00799984e-08  7.97136103e-08
 -1.56545041e-09 -7.91563264e-08 -8.00825336e-08 -7.97161455e-08
  5.51596857e-12  3.95580024e-10  9.40725950e-11  4.19104590e-10
  3.83841873e-10  3.62042706e-09  6.72424174e-09  5.29384881e-09
  2.65819598e-10  2.70803254e-09  4.66177644e-09  3.86490181e-09
  2.43431243e-10  2.43385280e-09  4.26639418e-09  3.49412216e-09
 -1.05613249e-10 -1.03393458e-11 -1.07272990e-11 -1.51608602e-11
  1.75280388e-08  1.72713714e-09  1.78840907e-09  2.54017581e-09
 -1.10508333e-08 -1.05987269e-09 -1.10275365e-09 -1.55808267e-09
  1.67233290e-08  1.63118830e-09  1.69211922e-09  2.39859088e-09
 -2.09528280e-08 -1.17814316e-10 -1.64854709e-10 -1.06186077e-10
  1.61945302e-08  9.36794817e-11  1.30665329e-10  8.44662132e-11
  1.21291396e-08  6.66174262e-11  9.31716136e-11  6.00310808e-11
  2.34858618e-08  1.33807877e-10  1.86937783e-10  1.20621301e-10
 -5.36384931e-11  5.55945923e-10  1.52580337e-09 -1.58199404e-08
  1.69770027e-09 -1.80617172e-08  1.57766339e-09 -1.61979873e-08
  8.24764871e+00]
supnorm grad right now is: 8.2476487085344
Weights right now are: 
[-0.59588701 -4.37147429 -3.36621075 -2.34720599 -0.19433223  4.38593813
  3.93751692  3.76228918  1.7307328   1.75519894  3.98126104  2.6445252
  0.44117293 -3.95703724  2.04378259 -2.77818329 -0.63990142 -3.66923859
  0.74895267 -2.91791731  0.51405508 -4.29259129  1.06556601 -2.28170983
  3.6845419   3.37974349  2.49578009  3.22636466  3.39723411 -4.1127204
 -3.36260586 -4.24912743 -0.65711163  3.81922971  3.46002524  3.87837759
  2.48845033 -3.94521269 -1.993234   -4.071669   -1.49419638  3.51992994
  4.38892623  5.12957144  2.53283177 -5.08260175 -3.12629842 -4.69490275
  2.47810119 -3.049482   -3.10518423 -4.07004155  2.11098935 -5.30786935
 -3.13892784 -5.59335282  5.47341613  1.25090343 -3.41328061 -0.75017599
 -2.77328616 -0.6514114  -3.84127933 -0.80354081 25.54175464]
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.569318721693
gradient value of function right now is: [ 8.72345985e-10  8.20829101e-08  8.18648337e-08  8.19423996e-08
 -8.72390772e-10 -8.20865968e-08 -8.18685196e-08 -8.19460855e-08
 -1.04200852e-12  3.94695956e-10 -1.99150531e-11  3.89531116e-10
 -8.48041462e-11  2.51928272e-10 -1.46894164e-09 -1.27400631e-10
 -5.75989715e-11  4.19300577e-10 -9.96531660e-10  1.60678072e-10
 -5.31302107e-11  3.26112009e-10 -9.20060019e-10  8.79200154e-11
  2.34791272e-11  2.14376811e-12  2.24892578e-12  3.20410465e-12
 -3.91412205e-09 -3.65237029e-10 -3.83763069e-10 -5.38065460e-10
  2.42403264e-09  2.37504278e-10  2.47253419e-10  3.50063000e-10
 -3.74302936e-09 -3.55897233e-10 -3.72563643e-10 -5.24468139e-10
  4.64282040e-09  8.60039689e-12  2.77813757e-11  3.33348563e-12
 -3.54717504e-09 -4.80728960e-12 -1.91234109e-11 -9.17903903e-13
 -2.67140724e-09 -5.14202266e-12 -1.63752669e-11 -2.08049937e-12
 -5.17433606e-09 -8.31491917e-12 -2.94708874e-11 -2.53791140e-12
 -1.06437192e-11  1.09720294e-10 -2.21069573e-10  3.45013826e-09
 -4.26838413e-10  5.89895228e-09 -1.55751500e-10  2.73202297e-09
 -1.38474184e+00]
supnorm grad right now is: 1.3847418396003879
Weights right now are: 
[-0.59633444 -4.39081659 -3.38588801 -2.36675243 -0.19388479  4.40528042
  3.95719417  3.78183561  1.73073087  1.75510097  3.9812276   2.64441898
  0.44103597 -3.95821227  2.04136313 -2.77995083 -0.63999667 -3.67010241
  0.74726825 -2.91919269  0.51396793 -4.29337125  1.06402592 -2.28286656
  3.68457958  3.3797471   2.49578384  3.22636994  3.39106438 -4.11331689
 -3.36322546 -4.25000382 -0.65313999  3.81960431  3.46041612  3.87892765
  2.48252177 -3.94578039 -1.99382475 -4.07250292 -1.48688639  3.51996351
  4.38897145  5.12960219  2.52718397 -5.08262844 -3.12633427 -4.6949272
  2.47385752 -3.04950093 -3.10520975 -4.07005888  2.10280108 -5.30790754
 -3.13897922 -5.5933878   5.47343455  1.25072322 -3.41373416 -0.7456137
 -2.77377185 -0.64638508 -3.84175519 -0.79880225 24.91069596]
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2656.1518737365595
gradient value of function right now is: [ 9.05887836e-10  8.25407730e-08  8.23717420e-08  8.24305751e-08
 -9.05906040e-10 -8.25422711e-08 -8.23732399e-08 -8.24320730e-08
 -8.04324298e-13  3.98137480e-10 -1.53388032e-11  3.94079409e-10
 -6.52978537e-11  3.82627479e-10 -1.12087957e-09  8.81165261e-11
 -4.42177492e-11  5.12060816e-10 -7.58343364e-10  3.11890672e-10
 -4.09069776e-11  4.10848131e-10 -7.02129322e-10  2.25937915e-10
  1.80949496e-11  1.65717097e-12  1.73366758e-12  2.49385748e-12
 -3.06262655e-09 -2.91198200e-10 -3.04854370e-10 -4.30176535e-10
  1.85183368e-09  1.83801245e-10  1.90899605e-10  2.71630140e-10
 -2.90724502e-09 -2.80901772e-10 -2.93160007e-10 -4.15092238e-10
  3.65306589e-09  8.57594565e-12  2.55168352e-11  4.28760079e-12
 -2.79369407e-09 -5.33581811e-12 -1.80325643e-11 -2.15860006e-12
 -2.09725945e-09 -5.03633600e-12 -1.49249305e-11 -2.54977121e-12
 -4.08200619e-09 -8.72040192e-12 -2.74909765e-11 -3.99720432e-12
 -6.08391264e-12  6.50227047e-11 -1.89422742e-10  2.86884406e-09
 -3.68281190e-10  5.05017766e-09 -1.38517014e-10  2.28839299e-09
 -6.07517272e+00]
supnorm grad right now is: 6.075172715892729
Weights right now are: 
[-0.59680517 -4.40812908 -3.40358621 -2.38429667 -0.19341406  4.42259291
  3.97489237  3.79937985  1.73072851  1.75501115  3.98118761  2.64431869
  0.44087002 -3.95966245  2.03852951 -2.78214569 -0.64010977 -3.6711351
  0.74533722 -2.92073278  0.51386346 -4.29431383  1.06224218 -2.28427786
  3.68462541  3.37975175  2.49578865  3.22637679  3.38318879 -4.11411089
 -3.36404629 -4.2511747  -0.64845298  3.82007739  3.46090508  3.87962533
  2.47512516 -3.94652658 -1.99459607 -4.07360331 -1.47730218  3.52002656
  4.38907565  5.12965428  2.51980134 -5.08267716 -3.12641473 -4.69496746
  2.46835697 -3.04953727 -3.10526973 -4.07008893  2.09206824 -5.30797826
 -3.13909605 -5.59344623  5.47344412  1.25060055 -3.41461233 -0.73514694
 -2.7749323  -0.63261456 -3.84258665 -0.78886547 24.51077216]
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.3579401719107
gradient value of function right now is: [ 9.11209768e-10  6.08782881e-08  6.11335889e-08  6.10280169e-08
 -9.11180685e-10 -6.08758946e-08 -6.11311959e-08 -6.10256239e-08
  1.62590147e-12  3.00351656e-10  2.70143072e-11  3.07080694e-10
  1.11281076e-10  1.43120081e-09  1.94459551e-09  1.91747152e-09
  7.71646463e-11  1.16413716e-09  1.34804457e-09  1.50097995e-09
  7.09464620e-11  1.02986983e-09  1.23922998e-09  1.33970158e-09
 -3.07068237e-11 -2.96836761e-12 -3.08654813e-12 -4.37302073e-12
  5.06549238e-09  4.81114662e-10  5.00387589e-10  7.14389775e-10
 -3.18470235e-09 -2.96376601e-10 -3.09500903e-10 -4.40017160e-10
  4.86689809e-09  4.58483029e-10  4.77631427e-10  6.80687317e-10
 -5.86542271e-09 -4.63822742e-12 -5.01294441e-12 -4.87745235e-12
  4.54480797e-09  4.79317674e-12  5.42894862e-12  4.86174264e-12
  3.41613635e-09  2.76155377e-12  2.98988694e-12  2.87978178e-12
  6.65863137e-09  6.12341101e-12  6.81166429e-12  6.30576828e-12
 -5.62815642e-12  5.95916700e-11  2.10770022e-11 -1.39098643e-10
  4.88123283e-12 -2.10241457e-10  3.34934315e-11 -1.93426802e-10
  2.87964782e+00]
supnorm grad right now is: 2.879647824967749
Weights right now are: 
[-0.59777731 -4.42924998 -3.42589146 -2.40614628 -0.19244192  4.44371374
  3.99719754  3.82122939  1.73072203  1.75489518  3.98107235  2.64417496
  0.44039115 -3.96312289  2.0299859  -2.78766102 -0.64044398 -3.67359178
  0.73937481 -2.92462369  0.51355768 -4.29655235  1.05678689 -2.28782865
  3.68475741  3.37976382  2.49580131  3.22639455  3.36209929 -4.11604184
 -3.36607169 -4.25401635 -0.63446981  3.82135331  3.46224434  3.88150286
  2.45456782 -3.94840603 -1.99656806 -4.07636904 -1.45298388  3.52003666
  4.38909868  5.1296614   2.50105007 -5.08268555 -3.12643326 -4.6949735
  2.45415613 -3.0495431  -3.10528317 -4.07009299  2.0647816  -5.30799011
 -3.13912257 -5.59345468  5.4734485   1.25056151 -3.41474125 -0.73299743
 -2.77517949 -0.62833536 -3.84268597 -0.78728046 25.19783386]
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2656.188405633937
gradient value of function right now is: [ 7.93773566e-10  6.86871981e-08  6.86083280e-08  6.86331539e-08
 -7.93814120e-10 -6.86905348e-08 -6.86116641e-08 -6.86364898e-08
 -3.12563420e-13  3.33656047e-10 -6.58520840e-12  3.31845874e-10
 -2.86350933e-11  5.06694982e-10 -4.87175527e-10  3.75127606e-10
 -1.92144643e-11  5.56647546e-10 -3.26615552e-10  4.67792575e-10
 -1.78755027e-11  4.62977712e-10 -3.04212110e-10  3.80522474e-10
  7.96174193e-12  6.83933997e-13  7.20121273e-13  1.05288034e-12
 -1.36385826e-09 -1.27068030e-10 -1.33748536e-10 -1.88521471e-10
  8.06188753e-10  8.07264720e-11  8.38133931e-11  1.19820525e-10
 -1.29156123e-09 -1.23667660e-10 -1.29452822e-10 -1.83559811e-10
  1.61665877e-09  6.76464492e-13  8.41290420e-12 -1.37519079e-12
 -1.23107202e-09  5.51266520e-13 -5.13522288e-12  2.02951354e-12
 -9.25025020e-10 -3.73953044e-13 -4.89312723e-12  8.11104373e-13
 -1.80884655e-09  1.42494254e-14 -8.51138411e-12  2.25226726e-12
 -5.42662775e-12  6.01801053e-11 -6.36965486e-11  1.16570649e-09
 -1.57173717e-10  2.29528275e-09 -3.26974434e-11  8.18550969e-10
 -5.99621296e+00]
supnorm grad right now is: 5.996212963970083
Weights right now are: 
[-0.59764491 -4.43890847 -3.43518254 -2.41558397 -0.19257432  4.45337225
  4.00648865  3.8306671   1.73072428  1.75485393  3.98111076  2.64414407
  0.44055217 -3.96196635  2.03269406 -2.78576912 -0.64033505 -3.67284537
  0.74120612 -2.92337956  0.5136589  -4.29585025  1.05848915 -2.28666419
  3.68471282  3.37975933  2.49579667  3.22638786  3.36989495 -4.11526198
 -3.36526429 -4.25285705 -0.63895005  3.82089798  3.46177428  3.88082578
  2.46184969 -3.94767297 -1.99580999 -4.07527917 -1.46238089  3.51999734
  4.38901964  5.12963192  2.50828541 -5.0826559  -3.12637322 -4.69495139
  2.45954182 -3.04952019 -3.10523735 -4.07007575  2.07538525 -5.30794618
 -3.13903396 -5.59342184  5.4734512   1.25055085 -3.41414011 -0.74088915
 -2.77423192 -0.64042392 -3.84216256 -0.79428037 24.49744936]
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.467636532849
gradient value of function right now is: [ 8.62915300e-10  5.55413553e-08  5.58270566e-08  5.57109151e-08
 -8.62894782e-10 -5.55396666e-08 -5.58253682e-08 -5.57092267e-08
  1.76666891e-12  2.74549807e-10  2.97066374e-11  2.81860867e-10
  1.22658302e-10  1.45177013e-09  2.15786261e-09  1.98414042e-09
  8.54465063e-11  1.16559594e-09  1.50347548e-09  1.53594167e-09
  7.85027027e-11  1.03492316e-09  1.38087298e-09  1.37537047e-09
 -3.38333706e-11 -3.24330731e-12 -3.37710460e-12 -4.76589022e-12
  5.53675177e-09  5.25365111e-10  5.46714124e-10  7.76975167e-10
 -3.53601510e-09 -3.27794009e-10 -3.42659944e-10 -4.84678512e-10
  5.33682748e-09  5.01777101e-10  5.23106255e-10  7.41982428e-10
 -6.43028006e-09 -1.05618039e-11 -1.19918498e-11 -1.04636861e-11
  4.97972910e-09  9.48201831e-12  1.09092249e-11  9.28630205e-12
  3.74717887e-09  6.08395555e-12  6.86500557e-12  6.02406949e-12
  7.26872141e-09  1.28753404e-11  1.47290457e-11  1.26745617e-11
 -9.09723809e-12  9.73931764e-11  8.70972714e-11 -7.64898248e-10
  6.63277838e-11 -7.04551454e-10  1.05445510e-10 -8.96051906e-10
  2.17959319e+00]
supnorm grad right now is: 2.1795931897499328
Weights right now are: 
[-0.59761203 -4.44862812 -3.44468838 -2.42517566 -0.19260721  4.4630919
  4.01599449  3.84025878  1.73072564  1.75481041  3.98113358  2.6441069
  0.4406474  -3.96129026  2.03427363 -2.78465283 -0.64027091 -3.67242487
  0.74226993 -2.92266239  0.51371869 -4.29544934  1.05948094 -2.28598681
  3.68468646  3.37975658  2.49579385  3.22638378  3.37462736 -4.11476903
 -3.36475733 -4.25212447 -0.64157397  3.82062221  3.46149113  3.88041591
  2.46621006 -3.9472172  -1.99534156 -4.0746018  -1.46820296  3.51995589
  4.38894899  5.12959806  2.5127738  -5.08262434 -3.1263192  -4.69492565
  2.46286212 -3.04949654 -3.10519701 -4.07005643  2.0819502  -5.30789975
 -3.13895465 -5.59338394  5.47344676  1.25060905 -3.41353634 -0.74807869
 -2.77341615 -0.65007944 -3.84159512 -0.80105977 25.16703286]
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.627075617619
gradient value of function right now is: [ 8.75964054e-10  5.49194328e-08  5.52307803e-08  5.51037296e-08
 -8.75914063e-10 -5.49153199e-08 -5.52266681e-08 -5.50996175e-08
  1.94942370e-12  2.72469115e-10  3.27194378e-11  2.80711601e-10
  1.35580009e-10  1.56328780e-09  2.36097306e-09  2.15843634e-09
  9.39581347e-11  1.23776587e-09  1.63663696e-09  1.64969255e-09
  8.66007637e-11  1.10419565e-09  1.50799413e-09  1.48407091e-09
 -3.74335218e-11 -3.64404270e-12 -3.78551583e-12 -5.36768165e-12
  6.23230338e-09  6.01832222e-10  6.24731401e-10  8.91517388e-10
 -3.87791071e-09 -3.66996206e-10 -3.82405444e-10 -5.43524250e-10
  5.95772690e-09  5.70856556e-10  5.93455265e-10  8.45514649e-10
 -7.30838846e-09 -1.85125373e-11 -2.58042684e-11 -1.68982203e-11
  5.65620688e-09  1.54345712e-11  2.13555772e-11  1.40838279e-11
  4.24193796e-09  1.05879058e-11  1.47232311e-11  9.66136219e-12
  8.26439536e-09  2.17144425e-11  3.01569845e-11  1.98152158e-11
 -9.49992610e-12  1.01965993e-10  2.12974820e-10 -2.24978933e-09
  2.35847795e-10 -2.70216565e-09  2.23705060e-10 -2.29275981e-09
 -4.09473346e-01]
supnorm grad right now is: 0.409473346468727
Weights right now are: 
[-0.59788512 -4.46178037 -3.4580004  -2.43842228 -0.19233411  4.47624416
  4.02930652  3.85350541  1.7307246   1.75474352  3.98111636  2.64403536
  0.4405757  -3.96199829  2.03306886 -2.78568779 -0.64031959 -3.67294251
  0.74145194 -2.92340188  0.51367344 -4.29592094  1.05872069 -2.28666463
  3.6847063   3.37975864  2.49579597  3.22638682  3.37113312 -4.11512837
 -3.36512737 -4.25265722 -0.63957674  3.82082753  3.46170257  3.88072033
  2.46296232 -3.94755126 -1.99568556 -4.07509708 -1.46393665  3.51998488
  4.38899503  5.12962242  2.50948068 -5.08264681 -3.12635486 -4.69494454
  2.46041945 -3.04951319 -3.10522341 -4.07007043  2.07714296 -5.30793248
 -3.13900661 -5.59341145  5.473452    1.25054433 -3.41394326 -0.74335608
 -2.77393196 -0.64410896 -3.84198718 -0.79650148 24.94448472]
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.586550265911
gradient value of function right now is: [ 8.27324058e-10  5.22827154e-08  5.25700888e-08  5.24527285e-08
 -8.27295573e-10 -5.22803719e-08 -5.25677458e-08 -5.24503855e-08
  1.79631425e-12  2.59418560e-10  3.01936074e-11  2.67006697e-10
  1.25459501e-10  1.45857715e-09  2.18683091e-09  2.00882700e-09
  8.69994778e-11  1.15924571e-09  1.51670333e-09  1.54038741e-09
  8.02215217e-11  1.03384794e-09  1.39816853e-09  1.38546360e-09
 -3.46514005e-11 -3.36087593e-12 -3.49370083e-12 -4.95155943e-12
  5.75708337e-09  5.52954058e-10  5.74497941e-10  8.19422722e-10
 -3.59134252e-09 -3.38669766e-10 -3.53091477e-10 -5.01777262e-10
  5.51071208e-09  5.25540130e-10  5.46763859e-10  7.78695479e-10
 -6.73164596e-09 -1.39347384e-11 -1.98422750e-11 -1.26944107e-11
  5.20863719e-09  1.17574619e-11  1.65786758e-11  1.07076918e-11
  3.90905193e-09  7.98830843e-12  1.13497298e-11  7.27199399e-12
  7.61497360e-09  1.64532700e-11  2.33157800e-11  1.49840360e-11
 -7.22201627e-12  7.79378365e-11  1.55003405e-10 -1.67275028e-09
  1.76204145e-10 -2.11750251e-09  1.62472340e-10 -1.68158164e-09
 -1.17922307e+00]
supnorm grad right now is: 1.1792230696926378
Weights right now are: 
[-0.59806656 -4.47277446 -3.4690503  -2.44944759 -0.19215267  4.48723829
  4.04035647  3.86453077  1.73072416  1.75468834  3.98110955  2.64397814
  0.44054803 -3.96237214  2.03262987 -2.78619362 -0.64033803 -3.67322542
  0.74115925 -2.9237727   0.5136561  -4.29617759  1.05844558 -2.28700396
  3.68471394  3.37975954  2.49579688  3.22638816  3.36965197 -4.11530076
 -3.36530142 -4.25291367 -0.63883931  3.8209127   3.46178866  3.88084701
  2.46165033 -3.94770358 -1.99583941 -4.07532366 -1.46201789  3.5200126
  4.38903543  5.12964698  2.50799216 -5.08266848 -3.1263864  -4.69496375
  2.45933675 -3.04952886 -3.10524624 -4.07008432  2.07497475 -5.30796392
 -3.1390524  -5.59343932  5.47346065  1.25045303 -3.4143361  -0.73923032
 -2.77438619 -0.6393321  -3.84238639 -0.79231227 24.90824775]
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.4775501422832
gradient value of function right now is: [ 7.25388192e-10  4.60837236e-08  4.63389446e-08  4.62358711e-08
 -7.25380637e-10 -4.60831019e-08 -4.63383230e-08 -4.62352495e-08
  1.55248961e-12  2.28328183e-10  2.63200717e-11  2.34749501e-10
  1.09425234e-10  1.25103931e-09  1.93356625e-09  1.72390962e-09
  7.64627544e-11  1.00258403e-09  1.35115863e-09  1.33264202e-09
  7.03006379e-11  8.92040943e-10  1.24198421e-09  1.19564243e-09
 -3.02026615e-11 -2.86119008e-12 -2.98489451e-12 -4.20384282e-12
  4.90745980e-09  4.60053870e-10  4.79747119e-10  6.80060645e-10
 -3.16823520e-09 -2.91012938e-10 -3.04698774e-10 -4.30096446e-10
  4.74869269e-09  4.41567612e-10  4.61217769e-10  6.52642278e-10
 -5.66909323e-09 -5.18182251e-12 -5.11483498e-12 -5.47815482e-12
  4.38725299e-09  5.05394376e-12  5.26295130e-12  5.18809550e-12
  3.30772887e-09  2.98552099e-12  2.92761030e-12  3.14871652e-12
  6.40435274e-09  6.61019108e-12  6.73302785e-12  6.87208620e-12
 -6.23147584e-12  6.60192871e-11  2.44061643e-11 -1.40567317e-10
  3.59374575e-12 -8.89566624e-11  3.82579708e-11 -2.23237722e-10
  2.10066154e+00]
supnorm grad right now is: 2.1006615364510624
Weights right now are: 
[-0.59797139 -4.48046853 -3.47645068 -2.45696287 -0.19224784  4.49493235
  4.04775682  3.87204602  1.73072587  1.75465482  3.98113929  2.64395229
  0.44067319 -3.96153499  2.03479061 -2.78479957 -0.64025203 -3.67267867
  0.74264404 -2.92284318  0.51373568 -4.29566482  1.05981946 -2.28613707
  3.68467931  3.37975622  2.49579342  3.22638322  3.37548133 -4.11474066
 -3.36471826 -4.25208354 -0.64239844  3.82056912  3.46143127  3.88033779
  2.46719921 -3.94716939 -1.99528345 -4.07453194 -1.4688898   3.51999765
  4.38900448  5.12963616  2.51328729 -5.08265724 -3.12636288 -4.69495567
  2.46330708 -3.0495202  -3.10522833 -4.07007805  2.08272279 -5.30794726
 -3.13901773 -5.59342729  5.47346171  1.25044715 -3.41411552 -0.74234135
 -2.77402483 -0.64426302 -3.84220223 -0.79497413 25.09237789]
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2656.5538181965717
gradient value of function right now is: [ 5.21227150e-10  4.95873277e-08  4.94535281e-08  4.95020529e-08
 -5.21179033e-10 -4.95833701e-08 -4.94495713e-08 -4.94980961e-08
 -6.96705292e-13  2.40189968e-10 -1.27931957e-11  2.36737616e-10
 -5.47174094e-11  1.09345982e-10 -9.29038343e-10 -1.39785901e-10
 -3.70838643e-11  2.30871686e-10 -6.29193643e-10  6.15313080e-11
 -3.45164242e-11  1.75277944e-10 -5.85992873e-10  1.78436993e-11
  1.51839019e-11  1.44975244e-12  1.50742176e-12  2.17361866e-12
 -2.62465686e-09 -2.57381787e-10 -2.67765715e-10 -3.80937973e-10
  1.54033930e-09  1.55824377e-10  1.61195668e-10  2.30684549e-10
 -2.46736105e-09 -2.44722866e-10 -2.54043661e-10 -3.62272262e-10
  3.16236437e-09  1.30855324e-11  2.66372116e-11  9.37792165e-12
 -2.42521192e-09 -9.21780913e-12 -1.94527143e-11 -6.44027769e-12
 -1.81107190e-09 -7.52869367e-12 -1.53734360e-11 -5.39264431e-12
 -3.54448732e-09 -1.40795557e-11 -2.91659155e-11 -9.96957143e-12
 -2.69149293e-12  2.81405455e-11 -2.35505764e-10  3.11229904e-09
 -3.56782463e-10  4.54438004e-09 -2.03825747e-10  2.76093369e-09
 -5.15562205e+00]
supnorm grad right now is: 5.155622046370063
Weights right now are: 
[-0.59797455 -4.4886598  -3.4844831  -2.46505629 -0.19224468  4.50312361
  4.05578924  3.88013946  1.73072674  1.75461662  3.98115484  2.64391794
  0.44073953 -3.96116217  2.03595206 -2.78413484 -0.6402063  -3.67244753
  0.74344386 -2.92241059  0.51377791 -4.29544561  1.06055868 -2.28573191
  3.68466093  3.37975452  2.49579164  3.22638071  3.37848416 -4.11446781
 -3.36443107 -4.25168012 -0.6443075   3.82039     3.46124388  3.88007277
  2.47010425 -3.94690195 -1.99500265 -4.07413638 -1.47235564  3.52000154
  4.38900256  5.12964176  2.51594825 -5.08266064 -3.12636193 -4.69496036
  2.46531916 -3.04952206 -3.1052267  -4.07008092  2.08661908 -5.30795191
 -3.13901592 -5.59343385  5.47346922  1.25037731 -3.41415908 -0.74257838
 -2.77398668 -0.64555691 -3.84228157 -0.79481487 24.62640298]
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.609682976413
gradient value of function right now is: [ 6.45666560e-10  4.18549165e-08  4.20662191e-08  4.19804221e-08
 -6.45689725e-10 -4.18568209e-08 -4.20681231e-08 -4.19823261e-08
  1.29128124e-12  2.07502221e-10  2.18869135e-11  2.12841685e-10
  9.14821107e-11  1.07866827e-09  1.61583361e-09  1.47432172e-09
  6.39155982e-11  8.72171878e-10  1.12851963e-09  1.14841314e-09
  5.88450441e-11  7.75423114e-10  1.03893521e-09  1.02982044e-09
 -2.52748849e-11 -2.38285942e-12 -2.48817186e-12 -3.50532055e-12
  4.10158746e-09  3.80721813e-10  3.97627436e-10  5.63797901e-10
 -2.64700889e-09 -2.41934117e-10 -2.53497317e-10 -3.58235467e-10
  3.97491198e-09  3.66617651e-10  3.83412747e-10  5.42847401e-10
 -4.70829496e-09  8.55013696e-13  2.50544642e-12  1.96385585e-13
  3.64293890e-09  1.62262108e-13 -8.99381847e-13  5.97409904e-13
  2.74931871e-09 -4.20357908e-13 -1.33980215e-12 -5.99627506e-14
  5.32916640e-09 -3.56694191e-13 -2.05187894e-12  3.29442429e-13
 -3.14652140e-12  3.28014714e-11 -4.64152202e-11  5.43684023e-10
 -6.62222500e-11  5.64216393e-10 -3.88547546e-11  5.28720909e-10
  8.45164002e-01]
supnorm grad right now is: 0.8451640019698357
Weights right now are: 
[-0.59842056 -4.50098838 -3.49724752 -2.47764635 -0.19179867  4.51545219
  4.06855366  3.8927295   1.73072404  1.75454935  3.98110909  2.64383857
  0.44054793 -3.9627681   2.03268714 -2.78660256 -0.64033753 -3.67357487
  0.74120672 -2.92412787  0.51365608 -4.29648564  1.05848225 -2.28731976
  3.68471395  3.37975984  2.49579714  3.22638859  3.36932185 -4.11539004
 -3.36538347 -4.25304634 -0.63890702  3.82092952  3.46180175  3.88087193
  2.46150618 -3.94776503 -1.99589439 -4.07541491 -1.46130623  3.52006225
  4.389098    5.12969366  2.5074192  -5.08270774 -3.12643589 -4.69500063
  2.45896896 -3.04955664 -3.1052811  -4.07011046  2.07416969 -5.30802044
 -3.13912359 -5.59349243  5.47348254  1.25023013 -3.41501623 -0.73276024
 -2.77505939 -0.63311583 -3.8431214  -0.78524915 25.04062226]
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.489336142921
gradient value of function right now is: [ 4.25149104e-10  4.10903414e-08  4.09735033e-08  4.10166334e-08
 -4.25192257e-10 -4.10938895e-08 -4.09770507e-08 -4.10201807e-08
 -6.37608187e-13  1.99177381e-10 -1.15031410e-11  1.95983299e-10
 -4.92146892e-11  4.61156111e-11 -8.22695477e-10 -1.81205650e-10
 -3.31529117e-11  1.63835596e-10 -5.53794192e-10  1.02648581e-11
 -3.09941556e-11  1.18883114e-10 -5.18072677e-10 -2.45218473e-11
  1.36671127e-11  1.34716699e-12  1.39417749e-12  2.02236497e-12
 -2.42248537e-09 -2.44596077e-10 -2.53224088e-10 -3.62840577e-10
  1.36851916e-09  1.42494414e-10  1.46719496e-10  2.11437783e-10
 -2.24980042e-09 -2.29690487e-10 -2.37305410e-10 -3.40797781e-10
  2.95714471e-09  1.65129972e-11  3.14082188e-11  1.25443447e-11
 -2.26954339e-09 -1.19594806e-11 -2.32539525e-11 -8.96854800e-12
 -1.68620053e-09 -9.45359783e-12 -1.80298388e-11 -7.17853950e-12
 -3.32023342e-09 -1.80350246e-11 -3.46730704e-11 -1.36160808e-11
 -1.47608192e-12  1.33430718e-11 -2.73682829e-10  3.50121717e-09
 -3.99408895e-10  4.96984358e-09 -2.44641479e-10  3.18440648e-09
 -2.03046960e+00]
supnorm grad right now is: 2.030469597936265
Weights right now are: 
[-0.59838225 -4.50766985 -3.50375192 -2.48422087 -0.19183698  4.52213371
  4.07505811  3.89930408  1.73072512  1.75451953  3.98112756  2.6438137
  0.44062576 -3.9622344   2.03400489 -2.78571619 -0.64028446 -3.67323531
  0.74210541 -2.92354777  0.51370547 -4.29616347  1.05931861 -2.28677374
  3.68469239  3.37975769  2.49579491  3.22638538  3.37307551 -4.11501281
 -3.36499359 -4.25248638 -0.64108868  3.8207098   3.46157477  3.88054577
  2.46501892 -3.94741174 -1.99552931 -4.07489047 -1.46583229  3.5200401
  4.38905961  5.12967551  2.51090881 -5.08269079 -3.12640645 -4.69498676
  2.46156595 -3.04954397 -3.10525911 -4.07010009  2.07927445 -5.30799557
 -3.13908044 -5.59347207  5.47348009  1.25025894 -3.41468872 -0.73673521
 -2.77461008 -0.63859431 -3.84281277 -0.78898647 24.83457419]
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.3572019542175
gradient value of function right now is: [ 5.98118203e-10  3.69764810e-08  3.72045815e-08  3.71126258e-08
 -5.98088956e-10 -3.69740757e-08 -3.72021767e-08 -3.71102210e-08
  1.39355504e-12  1.84047435e-10  2.36138091e-11  1.89858783e-10
  9.85933320e-11  1.09010296e-09  1.73678314e-09  1.51755659e-09
  6.89091829e-11  8.65178261e-10  1.21421488e-09  1.16355881e-09
  6.35059004e-11  7.73378892e-10  1.11862167e-09  1.04851742e-09
 -2.72329640e-11 -2.59537389e-12 -2.70421502e-12 -3.81616032e-12
  4.45737933e-09  4.22410117e-10  4.39688091e-10  6.24505299e-10
 -2.85026345e-09 -2.63878463e-10 -2.75910780e-10 -3.90009327e-10
  4.29766464e-09  4.03551364e-10  4.20806566e-10  5.96521077e-10
 -5.17666016e-09 -8.58204770e-12 -9.88411022e-12 -8.53170878e-12
  4.00646382e-09  7.54964028e-12  8.78063267e-12  7.42982501e-12
  3.01534896e-09  4.84992019e-12  5.55485941e-12  4.82138779e-12
  5.84655014e-09  1.03432621e-11  1.19807996e-11  1.02251551e-11
 -7.18083488e-12  7.46102626e-11  7.89339566e-11 -6.89890372e-10
  6.44312346e-11 -6.77381236e-10  9.42918189e-11 -7.94074233e-10
  2.88361109e+00]
supnorm grad right now is: 2.883611094269543
Weights right now are: 
[-0.59837546 -4.51472934 -3.51068777 -2.49120702 -0.19184377  4.52919319
  4.08199395  3.90629021  1.73072596  1.75448765  3.98114126  2.64378578
  0.44068297 -3.96181309  2.0349349  -2.7850259  -0.64024611 -3.67297855
  0.74272855 -2.92311058  0.51374148 -4.29591556  1.05990375 -2.28635651
  3.68467653  3.37975595  2.49579314  3.2263828   3.37602685 -4.11468981
 -3.3646641  -4.25200636 -0.64264438  3.82053851  3.46140019  3.88029115
  2.4676896  -3.94711887 -1.99523065 -4.0744552  -1.46955533  3.5200008
  4.38899872  5.12964173  2.51378355 -5.08266055 -3.12635957 -4.69496078
  2.46367596 -3.04952167 -3.10522454 -4.07008093  2.08347068 -5.30795137
 -3.13901193 -5.5934341   5.47347234  1.25034608 -3.41412713 -0.74306214
 -2.77391907 -0.64637498 -3.84225884 -0.79522981 25.18031652]
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2655.684648097131
gradient value of function right now is: [ 4.40317286e-10  4.20087475e-08  4.18872954e-08  4.19309753e-08
 -4.40294169e-10 -4.20068470e-08 -4.18853953e-08 -4.19290752e-08
 -6.08441104e-13  2.04248249e-10 -1.13563309e-11  2.01314756e-10
 -4.89534265e-11  9.75609791e-11 -8.48211516e-10 -1.20983803e-10
 -3.35501572e-11  1.97665497e-10 -5.80940011e-10  4.74959633e-11
 -3.11386568e-11  1.51630242e-10 -5.39519626e-10  1.24048308e-11
  1.35895881e-11  1.23474302e-12  1.29413499e-12  1.85132969e-12
 -2.26695850e-09 -2.10452743e-10 -2.20898730e-10 -3.11483689e-10
  1.39938710e-09  1.34632382e-10  1.40355987e-10  1.99344463e-10
 -2.17173810e-09 -2.04404565e-10 -2.13966079e-10 -3.02612142e-10
  2.65201436e-09  9.17335281e-13  7.94655059e-12 -1.04757061e-12
 -2.03277307e-09  1.41156642e-14 -5.22262833e-12  1.46122150e-12
 -1.53107941e-09 -6.34193800e-13 -4.76403990e-12  5.10737499e-13
 -2.97730766e-09 -5.12203352e-13 -8.29990470e-12  1.65161354e-12
 -4.93396602e-12  5.23922454e-11 -5.45708956e-11  1.09142599e-09
 -1.37104058e-10  2.11493550e-09 -2.55232322e-11  7.60998646e-10
 -6.81334566e+00]
supnorm grad right now is: 6.813345664707056
Weights right now are: 
[-0.59853903 -4.52258114 -3.5186464  -2.49912334 -0.1916802   4.53704494
  4.08995253  3.91420649  1.73072533  1.75444784  3.9811304   2.64374326
  0.44063708 -3.96222059  2.03413121 -2.78563429 -0.64027794 -3.67328321
  0.74217138 -2.92355461  0.51371207 -4.29619172  1.0593889  -2.28676139
  3.68468923  3.37975716  2.4957944   3.22638457  3.37394016 -4.11488528
 -3.36486835 -4.25229529 -0.64132288  3.82066255  3.46152978  3.88047455
  2.46567922 -3.94730728 -1.99542753 -4.07473372 -1.46712859  3.52000302
  4.38900386  5.12964314  2.51191215 -5.08266242 -3.12636371 -4.694962
  2.46226607 -3.04952308 -3.10522768 -4.07008185  2.08073684 -5.30795399
 -3.13901785 -5.59343578  5.47347214  1.25034583 -3.41415314 -0.74256539
 -2.77397223 -0.64549006 -3.84227574 -0.79485159 24.46273639]
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.0356608956454
gradient value of function right now is: [ 5.78343794e-10  3.29888031e-08  3.32528847e-08  3.31460371e-08
 -5.78304125e-10 -3.29855415e-08 -3.32496237e-08 -3.31427761e-08
  1.65265995e-12  1.65730580e-10  2.77890591e-11  1.72813730e-10
  1.16274329e-10  1.20990899e-09  2.01643706e-09  1.72249561e-09
  8.06648580e-11  9.31837826e-10  1.39958736e-09  1.28700623e-09
  7.46729217e-11  8.40734559e-10  1.29507112e-09  1.16970890e-09
 -3.21544459e-11 -3.14599559e-12 -3.26324944e-12 -4.64224952e-12
  5.40610108e-09  5.28721573e-10  5.47695598e-10  7.83390604e-10
 -3.32078633e-09 -3.17525513e-10 -3.30290966e-10 -4.70311301e-10
  5.14417185e-09  4.98743691e-10  5.17475263e-10  7.38849614e-10
 -6.38056032e-09 -2.16041939e-11 -3.02039149e-11 -1.96620580e-11
  4.93760476e-09  1.75888928e-11  2.44577459e-11  1.60117029e-11
  3.69524077e-09  1.22374875e-11  1.71016583e-11  1.11305683e-11
  7.21208549e-09  2.50257755e-11  3.48958684e-11  2.27771154e-11
 -9.98432347e-12  1.04113516e-10  2.68985669e-10 -2.83071056e-09
  2.99061556e-10 -3.33053646e-09  2.80776593e-10 -2.89064861e-09
  4.25219761e+00]
supnorm grad right now is: 4.252197607885988
Weights right now are: 
[-0.59870811 -4.53033041 -3.52651038 -2.50694164 -0.19151112  4.54479421
  4.09781651  3.92202479  1.73072464  1.75440826  3.98111858  2.64370069
  0.4405872  -3.96266539  2.03326485 -2.78629907 -0.64031246 -3.67361234
  0.74157203 -2.92403596  0.51368011 -4.29649132  1.05883397 -2.28720194
  3.68470304  3.37975849  2.49579578  3.22638654  3.37163521 -4.1151066
 -3.36509862 -4.25262282 -0.63989579  3.8207988   3.46167172  3.88067617
  2.46347696 -3.94751821 -1.9956471  -4.07504585 -1.46441885  3.52000937
  4.38901531  5.12964812  2.50982087 -5.0826675  -3.12637276 -4.69496602
  2.46069661 -3.04952678 -3.10523435 -4.07008476  2.07768202 -5.30796129
 -3.13903092 -5.59344153  5.47347346  1.25033124 -3.41423552 -0.74143732
 -2.77409406 -0.64381814 -3.84234844 -0.79385838 25.30124308]
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.305620660563
gradient value of function right now is: [ 5.14670369e-10  3.20521737e-08  3.22428055e-08  3.21656239e-08
 -5.14623179e-10 -3.20482938e-08 -3.22389264e-08 -3.21617448e-08
  1.17577919e-12  1.59838121e-10  1.98634896e-11  1.64750864e-10
  8.32679649e-11  9.32799583e-10  1.46301305e-09  1.29487310e-09
  5.81611499e-11  7.42261204e-10  1.02201446e-09  9.94882097e-10
  5.36981418e-11  6.64148197e-10  9.43323637e-10  8.97499790e-10
 -2.30203542e-11 -2.19667263e-12 -2.28814368e-12 -3.23444682e-12
  3.78097841e-09  3.58122447e-10  3.72760129e-10  5.30289005e-10
 -2.40193653e-09 -2.22626016e-10 -2.32705734e-10 -3.29567500e-10
  3.64156592e-09  3.41966944e-10  3.56543431e-10  5.06285119e-10
 -4.38358368e-09 -5.55706766e-12 -6.22802567e-12 -5.63205911e-12
  3.39307266e-09  5.06316481e-12  5.77441297e-12  5.05291605e-12
  2.55281355e-09  3.15171559e-12  3.51337601e-12  3.19137571e-12
  4.95889377e-09  6.83407112e-12  7.73813083e-12  6.86677713e-12
 -5.26635857e-12  5.46066999e-11  4.49190473e-11 -3.69978962e-10
  3.27085201e-11 -3.73703025e-10  5.63868578e-11 -4.39169560e-10
  3.13359128e+00]
supnorm grad right now is: 3.1335912841816733
Weights right now are: 
[-0.59881384 -4.53722314 -3.53343045 -2.51384992 -0.19140539  4.55168699
  4.10473663  3.92893311  1.73072443  1.7543738   3.98111536  2.64366532
  0.44057417 -3.96285189  2.03304855 -2.78654503 -0.64032141 -3.67376016
  0.74142276 -2.92422449  0.51367176 -4.29662431  1.05869504 -2.28737299
  3.68470663  3.37975888  2.49579618  3.22638713  3.37097453 -4.11518118
 -3.36517401 -4.25273353 -0.63953584  3.82083687  3.4617106   3.88073263
  2.4628776  -3.94758439 -1.99571423 -4.07514404 -1.46358605  3.52002026
  4.38902905  5.12965833  2.5091717  -5.08267615 -3.1263837  -4.69497412
  2.46022139 -3.04953283 -3.10524196 -4.07009043  2.07673854 -5.30797373
 -3.13904663 -5.59345319  5.4734788   1.25027618 -3.41437333 -0.74010617
 -2.77423007 -0.64251413 -3.84249727 -0.7924163  25.19517856]
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.47341146091
gradient value of function right now is: [ 5.05660566e-10  3.16293989e-08  3.18159685e-08  3.17406293e-08
 -5.05652180e-10 -3.16287094e-08 -3.18152791e-08 -3.17399400e-08
  1.13958147e-12  1.57647554e-10  1.93325851e-11  1.62388027e-10
  8.12208145e-11  9.07351316e-10  1.43225506e-09  1.25927807e-09
  5.68290381e-11  7.24492447e-10  1.00212855e-09  9.70483512e-10
  5.24437436e-11  6.47747062e-10  9.24606918e-10  8.74853107e-10
 -2.24565790e-11 -2.12593061e-12 -2.21773020e-12 -3.12808703e-12
  3.66341563e-09  3.43498219e-10  3.58181309e-10  5.08291615e-10
 -2.35023268e-09 -2.16225829e-10 -2.26321655e-10 -3.19899829e-10
  3.54089745e-09  3.29503692e-10  3.44111304e-10  4.87517634e-10
 -4.22859648e-09 -3.01060316e-12 -2.72849997e-12 -3.29667893e-12
  3.27195684e-09  3.05834496e-12  3.01835394e-12  3.21473698e-12
  2.46585448e-09  1.73163610e-12  1.56020811e-12  1.88981517e-12
  4.78101586e-09  3.93517907e-12  3.75334645e-12  4.20846232e-12
 -4.26837426e-12  4.48436219e-11  8.44981946e-12 -6.18456922e-12
 -6.67763535e-12  2.72093221e-11  1.80052468e-11 -5.84740406e-11
  2.13223934e+00]
supnorm grad right now is: 2.1322393418063634
Weights right now are: 
[-0.59884654 -4.5434194  -3.53956395 -2.52000815 -0.1913727   4.55788325
  4.11087013  3.93509135  1.73072482  1.75434444  3.98112192  2.64363777
  0.44060224 -3.96269533  2.03351682 -2.78625907 -0.64030247 -3.67367512
  0.74173858 -2.92405194  0.5136895  -4.29653952  1.05899091 -2.28720633
  3.68469884  3.37975809  2.49579537  3.22638594  3.37235585 -4.11504152
 -3.36502955 -4.25252611 -0.64031471  3.82075596  3.46162731  3.88061241
  2.46416082 -3.94745333 -1.99557891 -4.07494937 -1.46526731  3.52001143
  4.38901218  5.12965169  2.51046423 -5.08266955 -3.12637095 -4.6949692
  2.46118144 -3.04952766 -3.10523216 -4.07008653  2.07863106 -5.30796391
 -3.13902779 -5.59344584  5.47348018  1.25026726 -3.4142376  -0.74191219
 -2.77402711 -0.64509596 -3.84237856 -0.79404461 25.12185477]
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.113557529921
gradient value of function right now is: [ 5.21212671e-10  2.95439733e-08  2.97853887e-08  2.96877465e-08
 -5.21258205e-10 -2.95477165e-08 -2.97891313e-08 -2.96914890e-08
  1.51095157e-12  1.48689369e-10  2.54246480e-11  1.55172362e-10
  1.06729986e-10  1.10388048e-09  1.85076491e-09  1.57450522e-09
  7.40952013e-11  8.49492453e-10  1.28548885e-09  1.17582157e-09
  6.86612682e-11  7.67515483e-10  1.19071298e-09  1.07008305e-09
 -2.95292169e-11 -2.88698549e-12 -2.99486467e-12 -4.26098619e-12
  4.96697326e-09  4.85505941e-10  5.02971237e-10  7.19451247e-10
 -3.04921386e-09 -2.91509261e-10 -3.03233387e-10 -4.31831712e-10
  4.72631901e-09  4.58047318e-10  4.75277347e-10  6.78648348e-10
 -5.86001497e-09 -1.95045292e-11 -2.71679532e-11 -1.77784120e-11
  4.53444329e-09  1.58717272e-11  2.19916240e-11  1.44701230e-11
  3.39355686e-09  1.10420427e-11  1.53765317e-11  1.00581759e-11
  6.62419392e-09  2.25918953e-11  3.13881670e-11  2.05930656e-11
 -9.01693028e-12  9.39565587e-11  2.42932666e-10 -2.55846575e-09
  2.69117589e-10 -3.00107502e-09  2.53960622e-10 -2.61564413e-09
  3.94127651e+00]
supnorm grad right now is: 3.9412765127300706
Weights right now are: 
[-0.59896595 -4.55018625 -3.54638748 -2.52680884 -0.19125328  4.56465008
  4.11769364  3.94189201  1.73072447  1.75431049  3.98111603  2.64360233
  0.44057722 -3.96294816  2.03308276 -2.78662248 -0.64031975 -3.67386956
  0.74143874 -2.92432278  0.51367347 -4.29671493  1.0587129  -2.28745257
  3.68470577  3.37975876  2.49579607  3.22638693  3.37120018 -4.11515147
 -3.36514419 -4.25268878 -0.63959924  3.82082441  3.4616986   3.8807137
  2.46305533 -3.94755877 -1.9956888  -4.07510538 -1.46391153  3.52001391
  4.38901709  5.12965349  2.50941924 -5.08267152 -3.12637479 -4.69497064
  2.4603965  -3.04952917 -3.10523509 -4.07008764  2.0771038  -5.30796674
 -3.13903336 -5.5934479   5.47348005  1.25026672 -3.41427393 -0.74138777
 -2.7740844  -0.64429508 -3.84240892 -0.79359538 25.27544016]
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.5656674638303
gradient value of function right now is: [ 4.68006786e-10  2.95647396e-08  2.97313602e-08  2.96638391e-08
 -4.68039164e-10 -2.95674000e-08 -2.97340201e-08 -2.96664989e-08
  1.02319818e-12  1.47459110e-10  1.73399996e-11  1.51724521e-10
  7.30619564e-11  8.29292215e-10  1.28654505e-09  1.14635475e-09
  5.10958816e-11  6.64603219e-10  8.99580330e-10  8.86169546e-10
  4.72056176e-11  5.94268195e-10  8.30976500e-10  7.99034162e-10
 -2.02128252e-11 -1.91300340e-12 -1.99570113e-12 -2.81755711e-12
  3.30307300e-09  3.09148114e-10  3.22420954e-10  4.58032890e-10
 -2.11143259e-09 -1.94208223e-10 -2.03264110e-10 -2.87689199e-10
  3.19122759e-09  2.96569789e-10  3.09751155e-10  4.39337211e-10
 -3.80542993e-09 -1.14050326e-12 -5.51019143e-13 -1.50196719e-12
  2.94444012e-09  1.51694693e-12  1.22281394e-12  1.74080650e-12
  2.21892252e-09  6.67586122e-13  3.35551759e-13  8.65268950e-13
  4.30782948e-09  1.75751368e-12  1.21784083e-12  2.12273973e-12
 -3.15219099e-12  3.23515426e-11 -1.04907475e-11  1.70385652e-10
 -2.32679881e-11  1.65869207e-10 -3.37210509e-12  1.45953693e-10
  1.43038355e+00]
supnorm grad right now is: 1.4303835451276203
Weights right now are: 
[-0.59914027 -4.55729927 -3.55361038 -2.53398532 -0.19107897  4.57176309
  4.12491653  3.94906849  1.73072367  1.75427289  3.98110323  2.64356097
  0.4405241  -3.96350159  2.03222709 -2.7874267  -0.64035546 -3.67426123
  0.74086345 -2.92488293  0.51363985 -4.29707852  1.0581713  -2.28797486
  3.68472048  3.37976043  2.49579775  3.2263894   3.36840776 -4.11546716
 -3.36546429 -4.25315833 -0.63816408  3.82098523  3.46186191  3.88095286
  2.46055389 -3.94784069 -1.99597481 -4.07552467 -1.46034641  3.52005882
  4.38908281  5.12969316  2.50665789 -5.08270643 -3.12642585 -4.69500148
  2.45837931 -3.04955451 -3.10527218 -4.07011001  2.07307728 -5.30801756
 -3.1391077  -5.59349278  5.47349241  1.25013467 -3.41490365 -0.73459514
 -2.77481586 -0.63638462 -3.84304665 -0.78672555 25.0340067 ]
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.2155327071096
gradient value of function right now is: [ 3.20294378e-10  3.03841100e-08  3.03079534e-08  3.03359648e-08
 -3.20268991e-10 -3.03820230e-08 -3.03058668e-08 -3.03338783e-08
 -4.22142669e-13  1.48031984e-10 -7.57570315e-12  1.45884868e-10
 -3.26079096e-11  6.00006607e-11 -5.39687230e-10 -9.20055432e-11
 -2.19244978e-11  1.40663450e-10 -3.62579785e-10  3.81438302e-11
 -2.05981217e-11  1.05813659e-10 -3.40879753e-10  9.61572469e-12
  9.06852925e-12  9.06032731e-13  9.34944033e-13  1.36421679e-12
 -1.63482200e-09 -1.68053472e-10 -1.73445005e-10 -2.49740444e-10
  9.00368693e-10  9.53798118e-11  9.79306996e-11  1.41777080e-10
 -1.50658136e-09 -1.56506378e-10 -1.61220276e-10 -2.32626962e-10
  2.01015314e-09  1.28692710e-11  2.36805977e-11  1.00153458e-11
 -1.54373375e-09 -9.41082542e-12 -1.76224228e-11 -7.25676822e-12
 -1.14329231e-09 -7.33978421e-12 -1.35493572e-11 -5.70721758e-12
 -2.26044601e-09 -1.41407817e-11 -2.62430343e-11 -1.09565118e-11
 -4.66616481e-13  3.92397125e-12 -2.10005107e-10  2.63190308e-09
 -2.98358856e-10  3.65998418e-09 -1.91133174e-10  2.42568344e-09
 -3.29394091e+00]
supnorm grad right now is: 3.2939409126564083
Weights right now are: 
[-0.59920478 -4.56296081 -3.55926748 -2.5396441  -0.19101445  4.57742469
  4.13057369  3.95472732  1.7307237   1.75424526  3.98110378  2.64353356
  0.44052656 -3.96353069  2.03225754 -2.78744146 -0.64035403 -3.6743009
  0.74087988 -2.92491387  0.51364129 -4.29711027  1.05818856 -2.28799799
  3.68471978  3.37976033  2.49579766  3.22638925  3.36857862 -4.11544495
 -3.36544201 -4.25312506 -0.63821919  3.82097562  3.46185258  3.88093839
  2.4606913  -3.94782146 -1.99595569 -4.0754958  -1.46058984  3.52005392
  4.38907436  5.12968919  2.50684375 -5.0827028  -3.12641952 -4.69499855
  2.45851127 -3.04955169 -3.10526734 -4.07010773  2.07335003 -5.30801213
 -3.1390983  -5.5934884   5.47349262  1.25013432 -3.41482541 -0.73552778
 -2.77471057 -0.63759938 -3.84297303 -0.78761905 24.75124418]
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2656.981260381826
gradient value of function right now is: [ 3.12603432e-10  3.00665105e-08  2.99813954e-08  3.00126241e-08
 -3.12575424e-10 -3.00642085e-08 -2.99790938e-08 -3.00103226e-08
 -4.58142080e-13  1.46474054e-10 -8.31865729e-12  1.44196047e-10
 -3.59296567e-11  4.40759577e-11 -6.05455725e-10 -1.20687759e-10
 -2.43594094e-11  1.27576847e-10 -4.10207469e-10  1.55577825e-11
 -2.27989037e-11  9.44447891e-11 -3.84157685e-10 -1.02812847e-11
  9.98940621e-12  9.67174616e-13  1.00337284e-12  1.45198537e-12
 -1.74977119e-09 -1.73701167e-10 -1.80273141e-10 -2.57665416e-10
  1.00632862e-09  1.02845915e-10  1.06180998e-10  1.52594315e-10
 -1.63584586e-09 -1.64102029e-10 -1.69974992e-10 -2.43469942e-10
  2.11472101e-09  9.47595286e-12  1.85036932e-11  6.98262857e-12
 -1.62318107e-09 -6.76240764e-12 -1.35936093e-11 -4.88989397e-12
 -1.20916025e-09 -5.43572008e-12 -1.06492360e-11 -4.00225413e-12
 -2.37596043e-09 -1.02811883e-11 -2.03603903e-11 -7.50840162e-12
 -1.44668251e-12  1.49087899e-11 -1.65886023e-10  2.16436482e-09
 -2.44091534e-10  3.08475941e-09 -1.45944143e-10  1.94399122e-09
 -4.04900328e+00]
supnorm grad right now is: 4.049003279866849
Weights right now are: 
[-0.59915991 -4.5675635  -3.56373865 -2.54416942 -0.19105932  4.58202737
  4.13504485  3.95925263  1.73072461  1.7542256   3.98111864  2.64351826
  0.44058913 -3.96302873  2.03326702 -2.78664349 -0.64031212 -3.67398266
  0.74155528 -2.92439719  0.51368081 -4.29680543  1.05882576 -2.28750611
  3.68470242  3.37975841  2.4957957   3.22638639  3.37184607 -4.11508423
 -3.36507452 -4.25258839 -0.63991214  3.82078695  3.46166063  3.88065763
  2.46363461 -3.94749557 -1.99562383 -4.07501092 -1.46472689  3.52000896
  4.3890044   5.12965071  2.51003895 -5.08266815 -3.12636559 -4.6949689
  2.46085263 -3.04952613 -3.10522754 -4.07008585  2.07801734 -5.30796147
 -3.13901944 -5.59344504  5.473484    1.2502305  -3.41417872 -0.74288559
 -2.77391173 -0.64668101 -3.84233771 -0.79484779 24.69358742]
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.6243477372304
gradient value of function right now is: [ 4.63666896e-10  2.81463410e-08  2.83291164e-08  2.82548828e-08
 -4.63665950e-10 -2.81462633e-08 -2.83290387e-08 -2.82548050e-08
  1.13830349e-12  1.41061883e-10  1.92246795e-11  1.45902614e-10
  8.12362126e-11  8.89827352e-10  1.41573460e-09  1.24639308e-09
  5.65452115e-11  7.00466669e-10  9.85582828e-10  9.48423365e-10
  5.24119446e-11  6.30017317e-10  9.13315198e-10  8.59945363e-10
 -2.24923319e-11 -2.17013029e-12 -2.25699822e-12 -3.20194113e-12
  3.74470718e-09  3.58724350e-10  3.72849103e-10  5.31910409e-10
 -2.32993901e-09 -2.19463142e-10 -2.28839955e-10 -3.25347687e-10
  3.58448055e-09  3.41125008e-10  3.55006911e-10  5.05747435e-10
 -4.37066070e-09 -7.78118396e-12 -1.10569474e-11 -7.13283749e-12
  3.38031175e-09  6.57266707e-12  9.24510678e-12  6.02225758e-12
  2.53713072e-09  4.43556026e-12  6.30086766e-12  4.06002013e-12
  4.94536473e-09  9.19964536e-12  1.30081964e-11  8.42943491e-12
 -4.13343669e-12  4.38594768e-11  8.64941601e-11 -9.42079791e-10
  9.83957360e-11 -1.21595471e-09  9.11541135e-11 -9.42708078e-10
 -5.02944599e-01]
supnorm grad right now is: 0.5029445991225409
Weights right now are: 
[-0.59937229 -4.57450342 -3.57085372 -2.55121266 -0.19084694  4.58896731
  4.14215993  3.96629589  1.73072344  1.75418807  3.98109933  2.64347535
  0.44050767 -3.96377724  2.03191634 -2.78776783 -0.64036742 -3.67450848
  0.74063829 -2.92517805  0.51362897 -4.29729377  1.05796624 -2.28823357
  3.68472501  3.3797608   2.49579816  3.22638995  3.36776191 -4.11551712
 -3.36551802 -4.25323129 -0.63766104  3.82102456  3.46190423  3.88101049
  2.45988406 -3.94789253 -1.99603062 -4.07560044 -1.45967291  3.5200519
  4.3890703   5.12968764  2.50613467 -5.08270144 -3.12641664 -4.69499753
  2.45797144 -3.04955056 -3.10526505 -4.07010685  2.07231634 -5.30800998
 -3.13909387 -5.59348676  5.4734934   1.25012545 -3.4147818  -0.7360109
 -2.77464824 -0.63824312 -3.8429325  -0.78808618 24.95557928]
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.6124194231925
gradient value of function right now is: [ 4.49857268e-10  2.73901635e-08  2.75659484e-08  2.74945062e-08
 -4.49876603e-10 -2.73917518e-08 -2.75675363e-08 -2.74960942e-08
  1.09557073e-12  1.37300382e-10  1.85027754e-11  1.41959348e-10
  7.82758443e-11  8.60231786e-10  1.36397545e-09  1.20389391e-09
  5.44871224e-11  6.77962678e-10  9.49560913e-10  9.16963771e-10
  5.05209342e-11  6.09803512e-10  8.80242277e-10  8.31496352e-10
 -2.16769492e-11 -2.09028143e-12 -2.17420363e-12 -3.08466672e-12
  3.60883788e-09  3.45273871e-10  3.58944170e-10  5.12074706e-10
 -2.24485562e-09 -2.11348814e-10 -2.20394834e-10 -3.13390405e-10
  3.45502879e-09  3.28492685e-10  3.41914060e-10  4.87125254e-10
 -4.20879055e-09 -6.93921507e-12 -9.89567996e-12 -6.35718485e-12
  3.25500401e-09  5.89444928e-12  8.31203059e-12  5.39778118e-12
  2.44335716e-09  3.96452949e-12  5.65124505e-12  3.62622501e-12
  4.76329526e-09  8.23109865e-12  1.16738831e-11  7.53746669e-12
 -3.73934645e-12  3.98957078e-11  7.55023904e-11 -8.32336408e-10
  8.64664414e-11 -1.09099089e-09  7.95082264e-11 -8.28140994e-10
 -7.83078434e-01]
supnorm grad right now is: 0.7830784343525572
Weights right now are: 
[-0.59946383 -4.58002324 -3.57641121 -2.55675511 -0.1907554   4.59448713
  4.14771743  3.97183834  1.73072321  1.75416036  3.98109543  2.64344669
  0.44049153 -3.96394991  2.03162821 -2.78800926 -0.64037888 -3.67464568
  0.74043353 -2.925364    0.51361842 -4.29741697  1.05777796 -2.28840169
  3.68472946  3.37976122  2.49579859  3.22639056  3.36703957 -4.11558641
 -3.36558973 -4.2533338  -0.63718928  3.82106688  3.4619486   3.88107303
  2.45918501 -3.94795777 -1.99609849 -4.0756969  -1.45884006  3.52005401
  4.38907063  5.12969034  2.50548622 -5.08270332 -3.12641721 -4.69499984
  2.45748381 -3.0495516  -3.10526501 -4.07010824  2.07137125 -5.30801252
 -3.13909445 -5.59348996  5.473497    1.2500913  -3.41479697 -0.7360694
 -2.77463426 -0.63860673 -3.84296106 -0.78801855 24.94480211]
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.470566286459
gradient value of function right now is: [ 2.63147931e-10  2.59780972e-08  2.58956979e-08  2.59264133e-08
 -2.63128601e-10 -2.59765084e-08 -2.58941095e-08 -2.59248249e-08
 -4.58890855e-13  1.26534465e-10 -8.22067078e-12  1.24256950e-10
 -3.55402103e-11 -1.59131883e-12 -5.94435925e-10 -1.65655011e-10
 -2.40354258e-11  8.44380867e-11 -4.01728946e-10 -2.68066411e-11
 -2.25497813e-11  5.71626797e-11 -3.77131436e-10 -4.70945657e-11
  9.88538693e-12  9.76309178e-13  1.01014363e-12  1.46473816e-12
 -1.75273494e-09 -1.76582055e-10 -1.82809064e-10 -2.62150289e-10
  9.89878300e-10  1.02664524e-10  1.05737205e-10  1.52455092e-10
 -1.62893638e-09 -1.65842311e-10 -1.71356099e-10 -2.46252984e-10
  2.13297132e-09  1.13044248e-11  2.10958494e-11  8.61997134e-12
 -1.63786540e-09 -8.20085073e-12 -1.56290197e-11 -6.17576260e-12
 -1.21697739e-09 -6.48214345e-12 -1.21210624e-11 -4.94271147e-12
 -2.39790143e-09 -1.23666074e-11 -2.33160282e-11 -9.37347133e-12
 -1.10101400e-12  9.76820588e-12 -1.87856106e-10  2.41302805e-09
 -2.69073971e-10  3.36005227e-09 -1.68600282e-10  2.20464641e-09
 -2.11511111e+00]
supnorm grad right now is: 2.115111109962365
Weights right now are: 
[-0.59941846 -4.58412144 -3.58036921 -2.56076885 -0.19080078  4.5985853
  4.1516754   3.97585206  1.73072408  1.75414248  3.98111019  2.64343278
  0.44055453 -3.96350283  2.03269263 -2.78727591 -0.64033584 -3.67435608
  0.74116087 -2.92487875  0.51365866 -4.29714228  1.05845787 -2.28794413
  3.68471197  3.37975946  2.49579677  3.22638795  3.37009677 -4.11527883
 -3.36527187 -4.25287704 -0.63895526  3.82088843  3.46176434  3.88080802
  2.4620427  -3.94766977 -1.99580096 -4.0752692  -1.46252756  3.5200358
  4.38903949  5.12967556  2.50832897 -5.0826894  -3.12639336 -4.69498856
  2.45959838 -3.04954115 -3.10524715 -4.07009976  2.07553133 -5.30799207
 -3.13905944 -5.59347337  5.47349555  1.250112   -3.41453677 -0.73932369
 -2.77428012 -0.64300127 -3.8427188  -0.79106422 24.85450949]
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2656.1766549171343
gradient value of function right now is: [ 4.25346545e-10  2.22694495e-08  2.24987553e-08  2.24062991e-08
 -4.25293530e-10 -2.22650933e-08 -2.24944000e-08 -2.24019439e-08
  1.43868700e-12  1.13198018e-10  2.42177210e-11  1.19428812e-10
  1.02244994e-10  1.00346301e-09  1.76649601e-09  1.45609236e-09
  7.09593371e-11  7.58515234e-10  1.22673019e-09  1.07226946e-09
  6.59335559e-11  6.90114658e-10  1.13931134e-09  9.81815375e-10
 -2.83151555e-11 -2.78516318e-12 -2.88566171e-12 -4.11450471e-12
  4.79940463e-09  4.73463285e-10  4.89801729e-10  7.01707369e-10
 -2.91576407e-09 -2.81317593e-10 -2.92201918e-10 -4.16777444e-10
  4.55025527e-09  4.45088775e-10  4.61160340e-10  6.59539773e-10
 -5.69175601e-09 -2.25692037e-11 -3.15647059e-11 -2.05052240e-11
  4.40329510e-09  1.81269932e-11  2.52443220e-11  1.64768985e-11
  3.29013000e-09  1.27486818e-11  1.78372031e-11  1.15742769e-11
  6.43132816e-09  2.59639967e-11  3.62380727e-11  2.35937048e-11
 -9.26942331e-12  9.63521402e-11  2.92102890e-10 -3.10499813e-09
  3.25150879e-10 -3.61057435e-09  3.04045025e-10 -3.17527492e-09
  6.64275499e+00]
supnorm grad right now is: 6.642754986980578
Weights right now are: 
[-0.59941935 -4.58844598 -3.5846106  -2.56504241 -0.19079988  4.60290979
  4.15591674  3.98012556  1.73072455  1.75412225  3.9811185   2.64341464
  0.44059041 -3.96329504  2.03331596 -2.78690923 -0.64031108 -3.67422678
  0.74159037 -2.92463967  0.51368167 -4.29701889  1.05885741 -2.28771876
  3.68470201  3.37975853  2.4957958   3.22638656  3.37174754 -4.11512616
 -3.36511173 -4.25265078 -0.63998283  3.82079094  3.46166255  3.88066347
  2.46362984 -3.94752156 -1.99564577 -4.07504951 -1.46444263  3.52003691
  4.38903639  5.12967791  2.50980083 -5.08269039 -3.12639114 -4.69499049
  2.46070812 -3.04954157 -3.10524507 -4.07010091  2.07768911 -5.30799338
 -3.13905602 -5.59347607  5.47349928  1.25007648 -3.41454839 -0.73963409
 -2.77423912 -0.64398584 -3.84275284 -0.79112262 25.43941835]
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.468143472983
gradient value of function right now is: [ 2.52254452e-10  2.49439059e-08  2.48645419e-08  2.48941697e-08
 -2.52301874e-10 -2.49478020e-08 -2.48684373e-08 -2.48980651e-08
 -4.44646509e-13  1.21499044e-10 -7.94453270e-12  1.19291569e-10
 -3.43717733e-11 -4.43877069e-12 -5.73863759e-10 -1.63359526e-10
 -2.32305216e-11  7.93949615e-11 -3.87548889e-10 -2.82997594e-11
 -2.18104195e-11  5.32297281e-11 -3.64095776e-10 -4.77709429e-11
  9.56242961e-12  9.49428648e-13  9.81524139e-13  1.42404118e-12
 -1.70081194e-09 -1.72152976e-10 -1.78089222e-10 -2.55532801e-10
  9.56366017e-10  9.96769666e-11  1.02582402e-10  1.47997729e-10
 -1.57812041e-09 -1.61423642e-10 -1.66666531e-10 -2.39654202e-10
  2.07542746e-09  1.17438646e-11  2.14781564e-11  9.08201182e-12
 -1.59373986e-09 -8.56424654e-12 -1.59547473e-11 -6.55388356e-12
 -1.18331175e-09 -6.73661691e-12 -1.23399642e-11 -5.21078373e-12
 -2.33254816e-09 -1.28757843e-11 -2.37620211e-11 -9.90707084e-12
 -9.17375695e-13  7.71598444e-12 -1.92786951e-10  2.46225104e-09
 -2.71835158e-10  3.38107426e-09 -1.74857908e-10  2.27024836e-09
 -2.12665608e+00]
supnorm grad right now is: 2.126656082213934
Weights right now are: 
[-0.59960793 -4.5942927  -3.59061283 -2.57098051 -0.1906113   4.60875649
  4.16191895  3.98606364  1.73072347  1.75409005  3.981101    2.64337748
  0.44051731 -3.96398307  2.03211116 -2.78793535 -0.64036082 -3.67470832
  0.74076982 -2.925351    0.51363498 -4.29746785  1.05808768 -2.28838357
  3.68472226  3.37976072  2.49579803  3.22638981  3.36802425 -4.11553398
 -3.36552689 -4.25325676 -0.63797131  3.82100625  3.46188258  3.88098325
  2.46024203 -3.94788952 -1.99602087 -4.07559616 -1.45977819  3.5200867
  4.38910708  5.12972264  2.50618664 -5.0827293  -3.12644631 -4.69502547
  2.45805348 -3.04956957 -3.10528487 -4.07012605  2.07241912 -5.30804987
 -3.13913616 -5.59352683  5.47351528  1.24991249 -3.41524425 -0.73222134
 -2.77502315 -0.63554168 -3.84346926 -0.78352997 25.04630425]
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.5462374244007
gradient value of function right now is: [ 2.62734225e-10  2.43302294e-08  2.42775711e-08  2.42966284e-08
 -2.62690034e-10 -2.43265988e-08 -2.42739412e-08 -2.42929985e-08
 -2.81252233e-13  1.18974446e-10 -5.15314657e-12  1.17509156e-10
 -2.24861272e-11  8.02995831e-11 -3.71539366e-10 -2.50330756e-11
 -1.50733235e-11  1.35120070e-10 -2.48697799e-10  6.42133772e-11
 -1.42000575e-11  1.05892630e-10 -2.34555391e-10  3.92114383e-11
  6.26536249e-12  6.20360729e-13  6.41267063e-13  9.36105446e-13
 -1.12979040e-09 -1.14880406e-10 -1.18895517e-10 -1.70666832e-10
  6.20924104e-10  6.61166577e-11  6.78798464e-11  9.82734865e-11
 -1.04218543e-09 -1.07734281e-10 -1.11164589e-10 -1.60101873e-10
  1.38759809e-09  8.11043228e-12  1.57754061e-11  6.01134771e-12
 -1.06316472e-09 -5.76956573e-12 -1.15636052e-11 -4.19195027e-12
 -7.88198237e-10 -4.66390377e-12 -9.07984575e-12 -3.46136045e-12
 -1.55773293e-09 -8.79018702e-12 -1.73470599e-11 -6.45387126e-12
 -1.52592019e-12  1.43182829e-11 -1.38846570e-10  1.83230160e-09
 -2.04920466e-10  2.59382220e-09 -1.22462023e-10  1.66125630e-09
 -1.58601925e+00]
supnorm grad right now is: 1.5860192476137938
Weights right now are: 
[-0.59966936 -4.59950649 -3.59581608 -2.5761868  -0.19054988  4.61397024
  4.16712216  3.99126989  1.73072349  1.75406421  3.98110161  2.64335168
  0.44052038 -3.96403994  2.03217961 -2.78798203 -0.6403585  -3.67476148
  0.74082005 -2.9253961   0.51363703 -4.29751457  1.05813282 -2.28842334
  3.6847214   3.37976071  2.49579801  3.22638979  3.36808741 -4.11554106
 -3.36553185 -4.2532676  -0.63807842  3.82100166  3.46187675  3.88097657
  2.46034392 -3.9478909  -1.99602026 -4.07559848 -1.45977752  3.52009728
  4.38912088  5.12973247  2.50618156 -5.08273758 -3.12645712 -4.69503316
  2.45806413 -3.04957547 -3.10529254 -4.07013154  2.0724169  -5.30806188
 -3.13915183 -5.59353799  5.47351888  1.24987424 -3.41538684 -0.73080328
 -2.77516948 -0.63412134 -3.84362157 -0.78200259 25.09306624]
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.6327282819543
gradient value of function right now is: [ 4.08670731e-10  2.44210158e-08  2.45911833e-08  2.45224403e-08
 -4.08629567e-10 -2.44176340e-08 -2.45878021e-08 -2.45190592e-08
  1.04701552e-12  1.22643724e-10  1.77795785e-11  1.27087178e-10
  7.54745123e-11  8.03385515e-10  1.32008008e-09  1.13348843e-09
  5.26560409e-11  6.30913006e-10  9.21014656e-10  8.61034245e-10
  4.88215561e-11  5.68331852e-10  8.53785683e-10  7.81769270e-10
 -2.09063100e-11 -1.99999630e-12 -2.08286748e-12 -2.94996317e-12
  3.46007894e-09  3.28567452e-10  3.42017893e-10  4.86897410e-10
 -2.17227382e-09 -2.03207134e-10 -2.12146371e-10 -3.01072653e-10
  3.32240241e-09  3.13628687e-10  3.26849812e-10  4.64702873e-10
 -4.02331596e-09 -5.25090773e-12 -7.53391403e-12 -4.83827745e-12
  3.11033082e-09  4.54911479e-12  6.43227174e-12  4.18895129e-12
  2.33789171e-09  3.00301361e-12  4.31313809e-12  2.75951792e-12
  4.54949132e-09  6.29283143e-12  8.96380036e-12  5.79478097e-12
 -3.14248686e-12  3.31873666e-11  5.35306770e-11 -6.06636445e-10
  6.22722535e-11 -8.41684588e-10  5.66982092e-11 -5.92929216e-10
  7.80790484e-03]
supnorm grad right now is: 0.0078079048446373775
Weights right now are: 
[-0.59974992 -4.60427426 -3.60061965 -2.58097609 -0.19046931  4.61873802
  4.17192574  3.9960592   1.73072328  1.75404029  3.98109796  2.64332687
  0.44050476 -3.96419824  2.03190304 -2.78820786 -0.64036944 -3.67488626
  0.74062646 -2.92556819  0.51362691 -4.29762676  1.05795365 -2.2885793
  3.68472573  3.37976111  2.49579843  3.22639038  3.36738852 -4.11560499
 -3.36559887 -4.253362   -0.637624    3.82104322  3.46192034  3.88103798
  2.45966416 -3.94795305 -1.99608544 -4.07569028 -1.45897633  3.52009684
  4.38912045  5.12973194  2.50556333 -5.08273733 -3.1264569  -4.69503283
  2.45759652 -3.04957527 -3.10529236 -4.07013127  2.07151381 -5.30806144
 -3.13915142 -5.59353745  5.47351882  1.24987355 -3.41536955 -0.73090909
 -2.77515244 -0.6341946  -3.84360157 -0.78214322 24.96285827]
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.6325808927886
gradient value of function right now is: [ 4.05088079e-10  2.41435637e-08  2.43128527e-08  2.42444150e-08
 -4.05147769e-10 -2.41484671e-08 -2.43177552e-08 -2.42493174e-08
  1.04475926e-12  1.21296856e-10  1.77222713e-11  1.25738486e-10
  7.52285857e-11  8.00923850e-10  1.31423187e-09  1.13033352e-09
  5.24644449e-11  6.28218868e-10  9.16629996e-10  8.57759622e-10
  4.86614418e-11  5.66212450e-10  8.50005633e-10  7.79192989e-10
 -2.08393367e-11 -1.99877327e-12 -2.08064610e-12 -2.94865467e-12
  3.45688567e-09  3.29374897e-10  3.42660107e-10  4.88157364e-10
 -2.16330028e-09 -2.02896872e-10 -2.11726281e-10 -3.00646071e-10
  3.31546784e-09  3.13943988e-10  3.27004086e-10  4.65227452e-10
 -4.02598170e-09 -6.09367623e-12 -8.69300653e-12 -5.60649705e-12
  3.11266004e-09  5.20157263e-12  7.32962071e-12  4.78402422e-12
  2.33834641e-09  3.47432204e-12  4.96105284e-12  3.18931285e-12
  4.55279065e-09  7.24539576e-12  1.02737053e-11  6.66339761e-12
 -3.40086010e-12  3.57926969e-11  6.57270111e-11 -7.31348047e-10
  7.55413579e-11 -9.75569147e-10  6.94458266e-11 -7.23830267e-10
 -7.80649924e-02]
supnorm grad right now is: 0.07806499242131364
Weights right now are: 
[-0.59973371 -4.60795493 -3.60422297 -2.58461147 -0.19048553  4.62241866
  4.17552902  3.99969455  1.73072383  1.75402402  3.98110687  2.64331326
  0.44054236 -3.96389671  2.03250295 -2.78772675 -0.64034432 -3.67469889
  0.74102697 -2.92526073  0.5136507  -4.29744553  1.05833311 -2.28828439
  3.68471528  3.37975993  2.49579723  3.22638862  3.3693915  -4.11537922
 -3.36536963 -4.25302579 -0.63863366  3.82092815  3.46180369  3.88086657
  2.46145209 -3.94775079 -1.99588018 -4.07538904 -1.46153858  3.52006573
  4.3890732   5.12970488  2.50754348 -5.08271336 -3.12642048 -4.69501199
  2.45904227 -3.04955763 -3.10526558 -4.07011593  2.07440636 -5.30802637
 -3.13909814 -5.59350695  5.47351321  1.24993704 -3.41492322 -0.73591625
 -2.77461461 -0.64020678 -3.84315601 -0.78715073 25.35020683]
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2657.6268888304057
gradient value of function right now is: [ 4.02262472e-10  2.40055370e-08  2.41727769e-08  2.41051221e-08
 -4.02312246e-10 -2.40096253e-08 -2.41768645e-08 -2.41092096e-08
  1.03395963e-12  1.20620520e-10  1.75294250e-11  1.25019446e-10
  7.44210858e-11  7.94770970e-10  1.29939297e-09  1.12083735e-09
  5.18951312e-11  6.23497667e-10  9.06211768e-10  8.50671863e-10
  4.81441820e-11  5.62036059e-10  8.40516879e-10  7.72872431e-10
 -2.06163656e-11 -1.98017260e-12 -2.06081163e-12 -2.92131111e-12
  3.42391995e-09  3.26775570e-10  3.39861004e-10  4.84330486e-10
 -2.13924191e-09 -2.00898143e-10 -2.09592442e-10 -2.97696710e-10
  3.28193288e-09  3.11250591e-10  3.24112110e-10  4.61258854e-10
 -3.99074394e-09 -6.47830395e-12 -9.17820242e-12 -5.96533352e-12
  3.08552027e-09  5.48988931e-12  7.69418182e-12  5.05267800e-12
  2.31731031e-09  3.68576210e-12  5.22596316e-12  3.38715590e-12
  4.51300462e-09  7.67225089e-12  1.08126333e-11  7.06141156e-12
 -3.50601604e-12  3.69671832e-11  7.12579415e-11 -7.84493211e-10
  8.10640205e-11 -1.02446060e-09  7.53807405e-11 -7.82380552e-10
 -4.09465861e-01]
supnorm grad right now is: 0.4094658611909029
Weights right now are: 
[-0.59981483 -4.6124789  -3.60878349 -2.58915681 -0.19040441  4.62694262
  4.18008954  4.00423988  1.73072359  1.75400112  3.98110286  2.64328928
  0.44052505 -3.96408302  2.03221416 -2.78799282 -0.64035605 -3.6748394
  0.74083153 -2.92545532  0.51363968 -4.29757363  1.05814943 -2.28846324
  3.6847201   3.37976044  2.49579775  3.22638937  3.36853736 -4.11546593
 -3.36545924 -4.25315439 -0.6381523   3.82097835  3.46185533  3.88094107
  2.46065892 -3.94783218 -1.99596414 -4.07550979 -1.4604965   3.52007183
  4.38908395  5.12970964  2.50674198 -5.08271801 -3.1264287  -4.6950156
  2.45844733 -3.04956123 -3.10527187 -4.07011876  2.07323433 -5.3080332
 -3.13911019 -5.59351226  5.47351262  1.24993828 -3.41501273 -0.73473774
 -2.77474008 -0.63862084 -3.84323674 -0.78605922 24.5067333 ]
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1550.7177017085312
W_T_median: 1404.9332849038447
W_T_pctile_5: 624.2405425187701
W_T_CVAR_5_pct: 486.6279485839219
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.4
F value: -2657.6268888304057
-----------------------------------------------
