Starting at: 
2022-06-20 18:03:38
tracing parameter entered from terminal:  0.1


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = basic_T30_VWD
timeseries_basket['basket_desc'] = CRSP data: T30 and VWD
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
5.0% of MC simulations done.
10.0% of MC simulations done.
15.0% of MC simulations done.
20.0% of MC simulations done.
25.0% of MC simulations done.
30.0% of MC simulations done.
35.0% of MC simulations done.
40.0% of MC simulations done.
45.0% of MC simulations done.
50.0% of MC simulations done.
55.00000000000001% of MC simulations done.
60.0% of MC simulations done.
65.0% of MC simulations done.
70.0% of MC simulations done.
75.0% of MC simulations done.
80.0% of MC simulations done.
85.0% of MC simulations done.
90.0% of MC simulations done.
95.0% of MC simulations done.
100.0% of MC simulations done.
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  hidden_layer    None       None         None   
4        obj.layers[4]        4  hidden_layer    None       None         None   
5        obj.layers[5]        5  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  
4    False        None  
5    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  hidden_layer       4  logistic_sigmoid   
4        obj.layers[4]        4  hidden_layer       4  logistic_sigmoid   
5        obj.layers[5]        5  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 4)    False        None  
4       (4, 4)    False        None  
5       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
[ 0.10347998  0.18844875 -0.3666709   0.62803316 -0.89369922 -0.17398511
  0.93797695  0.78704989 -0.60791207 -0.16293226 -0.31685198  0.28094308
  0.0404241   0.17453202  0.0940472   0.01169373  0.2300954  -0.23784803
  0.78511084 -0.41859022  0.4809118  -0.01155852  0.07219357 -0.14482284
  0.4701344  -0.18827011 -0.8138637  -0.24629474  0.28361272  0.33354483
 -0.63777411 -0.25343004 -0.81551088 -0.26234589  0.20198453 -0.15135741
  0.25264285 -0.29817275  0.41496663 -0.58895503 -0.60305385 -0.63454123
  0.3979274   0.74701442  0.57452389 -0.67377327  0.6493153   0.07340044
 -0.29694484  0.52758424 -0.14347696  0.04319718 -0.28527617 -0.63077443
  0.13133703 -0.58713125  0.12759166  0.87507529 -0.19770888 -0.66759109
  0.25529887 -0.48501395  0.58877641 -0.72315372] (64,)
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1260.6521530687958
W_T_median: 1222.3920506735662
W_T_pctile_5: 834.1581876304141
W_T_CVAR_5_pct: 746.6575474940535
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.31671847349
gradient value of function right now is: [ 3.67398893e+00  9.79701271e-01  1.75097674e-01  1.28578526e-01
 -3.67398893e+00 -9.79701271e-01 -1.75097674e-01 -1.28578526e-01
 -3.56821224e-01 -5.74522571e-02 -3.53515992e-01 -1.72240692e-02
 -6.41288106e-01 -1.03246759e-01 -6.35347301e-01 -3.09518102e-02
 -6.62432720e-02 -1.06667888e-02 -6.56297480e-02 -3.19796390e-03
 -8.93678829e-02 -1.43881587e-02 -8.85399654e-02 -4.31342630e-03
 -9.60161658e-05 -7.24785354e-05 -9.24236274e-05 -4.28561477e-05
  1.12379608e-02  8.64276462e-03  1.08824002e-02  5.01377143e-03
  2.80017366e-03  2.15401145e-03  2.71162906e-03  1.24993534e-03
 -2.54406763e-03 -1.95622491e-03 -2.46316098e-03 -1.13624706e-03
 -3.01253636e-04 -1.17330217e-04 -7.94355977e-05 -6.31088427e-05
 -1.46150458e-03 -5.52483362e-04 -3.67168645e-04 -2.89774157e-04
  7.09330996e-04  2.72571349e-04  1.82669695e-04  1.43588621e-04
 -9.10342933e-04 -3.50866426e-04 -2.35952830e-04 -1.86775499e-04
  1.62919220e-04 -2.19382352e-04 -2.79728594e-04  4.55722839e-04
  6.68774840e-05 -8.38478002e-05 -5.26553169e-05  8.43754883e-05
  5.08070040e+00]
supnorm grad right now is: 5.080700400090108
Weights right now are: 
[ 8.43215654e-01  4.69164843e-01  4.92498982e-01  1.08586321e+00
 -1.63343489e+00 -4.54701205e-01  7.88070634e-02  3.29219837e-01
 -1.70114193e+00 -9.00231465e-01 -1.45469408e+00 -4.80043416e-01
  5.47001205e-01  4.39883944e-01  5.14173853e-01  6.81668340e-03
  1.30603438e+00  3.89849234e-01  1.84324949e+00  2.05500392e-01
  1.89549641e+00  1.31609588e+00  1.40525156e+00  9.68729919e-01
 -5.45397188e-01 -1.07518113e+00 -1.80323366e+00 -9.11219562e-01
  9.34782272e-01  1.06365430e+00  5.29217474e-02  6.40471051e-01
 -1.65044378e+00 -9.85468306e-01 -6.11488284e-01 -7.28977976e-01
  1.40149194e+00  8.30178613e-01  1.58970880e+00  2.88791261e-01
 -1.52189594e+00 -1.33134135e+00 -3.78424756e-01  5.73624478e-02
  1.94785971e-02 -1.30423521e+00 -3.83342817e-01 -7.95316231e-01
 -1.27525289e+00 -3.14497861e-01 -1.20957921e+00 -8.64562486e-01
  6.64443188e-01  7.67379098e-02  4.38735915e-01 -1.37196192e-01
  7.70581487e-02  1.17711229e+00  6.79097279e-01 -7.67700829e-01
  9.48617203e-01 -1.26050969e+00  1.33903065e+00 -1.37918676e+00
  3.09930600e+01]
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.009444001454
gradient value of function right now is: [ 4.93635986e+00  1.43921230e-01  5.62148618e-02  2.09627561e-02
 -4.93635986e+00 -1.43921230e-01 -5.62148618e-02 -2.09627561e-02
 -9.53078057e-02 -8.99650693e-02 -9.51325714e-02 -3.48234744e-03
 -6.62350198e-02 -6.25220544e-02 -6.61132399e-02 -2.42007131e-03
 -1.64911192e-02 -1.55666744e-02 -1.64607989e-02 -6.02541283e-04
 -1.21818348e-02 -1.14989497e-02 -1.21594373e-02 -4.45097213e-04
 -5.13697549e-04 -4.82676317e-04 -5.24321802e-04 -3.42511454e-04
 -3.12353325e-04 -2.93473035e-04 -3.18811441e-04 -2.08242713e-04
 -3.61219180e-04 -3.39443110e-04 -3.68706516e-04 -2.40896956e-04
 -5.97853688e-04 -5.61814017e-04 -6.10255147e-04 -3.98716544e-04
 -3.03175320e-05 -1.87291455e-05 -1.58855641e-05 -1.41613912e-05
 -9.74205595e-05 -5.86584435e-05 -4.96503482e-05 -4.45586561e-05
 -4.75997331e-05 -2.94093548e-05 -2.48820572e-05 -2.21006220e-05
 -2.07598246e-04 -1.24596084e-04 -1.05596741e-04 -9.50529898e-05
 -1.21299379e-05  8.98517023e-06 -5.36298502e-05  7.60531425e-05
 -3.57549910e-05  4.91698490e-05 -5.45080536e-05  8.04821853e-05
  8.13214107e+00]
supnorm grad right now is: 8.132141066329297
Weights right now are: 
[ 0.95694405  0.24596473  0.43466077  1.00185426 -1.74716329 -0.2315011
  0.13664527  0.41322879 -1.96455273 -1.35049067 -1.7151275  -0.52317699
  1.12669763  1.40789305  1.08746899  0.22795836  1.71985356  0.59499959
  2.22994551  0.22790807  2.1932522   1.69242246  1.69449548  1.04172295
 -0.81093285 -1.39042054 -2.10401396 -1.11310786 -0.43611201 -0.51266457
 -1.49069639 -0.6539978  -1.96406449 -1.34332099 -0.9636782  -0.91186804
  1.27903761  0.68336455  1.44886449  0.22244659 -1.78260306 -1.51831537
 -0.55429629 -0.11524203 -0.26587631 -1.67377972 -0.82664384 -1.25862864
 -1.65674349 -0.64252509 -1.56814494 -1.23982123  0.22604817 -0.4414581
 -0.24394842 -0.88328196 -0.46928046  2.02111001 -0.11920278 -0.07470167
  0.26573048 -0.21005454  0.71486827 -0.13793237 31.02365227]
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.5556605200481
gradient value of function right now is: [-6.46825614e-02 -6.36801171e-04 -2.48945888e-04 -1.06883103e-04
  6.46825614e-02  6.36801171e-04  2.48945888e-04  1.06883103e-04
  5.65435501e-04  5.59681907e-04  5.64300395e-04  4.78619488e-05
  1.74098596e-04  1.72329764e-04  1.73749681e-04  1.47198521e-05
  3.49603606e-05  3.46060582e-05  3.48904884e-05  2.95028677e-06
  4.76898276e-05  4.72015204e-05  4.75934353e-05  4.05572791e-06
  7.82989553e-06  7.63637067e-06  8.16231714e-06  6.18202806e-06
  8.08370040e-06  7.87849122e-06  8.44950731e-06  6.30335740e-06
  7.72242315e-06  7.52566509e-06  8.05085748e-06  6.08031419e-06
  1.25759294e-05  1.21602751e-05  1.33780597e-05  8.88765854e-06
  9.16800316e-07  6.17783200e-07  5.48762758e-07  4.69996914e-07
  2.36523937e-06  1.85477523e-06  1.68840399e-06  1.28574316e-06
  1.36886000e-06  9.53735280e-07  8.47773116e-07  7.17330650e-07
  3.79233370e-06  3.53100685e-06  3.27942404e-06  1.94105102e-06
  1.27127726e-06 -1.43757336e-06  2.66759431e-06  3.26525570e-06
  2.29524887e-06  3.27001520e-06  3.57895680e-06  5.11371422e-06
  5.99301957e+00]
supnorm grad right now is: 5.993019572253178
Weights right now are: 
[ 0.88023563  0.14848668  0.3589839   0.9346796  -1.67045487 -0.13402305
  0.21232215  0.48040345 -2.13719005 -1.6798519  -1.88628164 -0.54630252
  1.30288089  2.09353911  1.26299305  0.3003869   2.05919785  1.00423475
  2.54998809  0.24673532  2.34070302  2.22773338  1.83908313  1.08116502
 -0.88094639 -1.48581869 -2.18550418 -1.19652789 -0.83040007 -1.01575556
 -1.93631829 -1.22968975 -2.0507756  -1.45783053 -1.06400731 -0.9917474
  1.14392097  0.51106407  1.2929747   0.11783997 -1.87890333 -1.60653661
 -0.64532724 -0.21395432 -0.41631473 -1.88375616 -1.08624478 -1.55333875
 -1.72751143 -0.7255588  -1.66912791 -1.353032   -0.07935893 -0.86529026
 -0.82891154 -1.5612132  -0.67643059  2.20338648 -0.57855302  0.29652673
 -0.17543809  0.19373517  0.22835655  0.46990609 30.97540567]
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.0445842715815
gradient value of function right now is: [ 5.10226350e+00  2.46055565e-02  8.71954772e-03  4.51081006e-03
 -5.10226350e+00 -2.46055565e-02 -8.71954772e-03 -4.51081006e-03
 -2.16375645e-02 -2.16231696e-02 -2.16256877e-02 -1.82219730e-03
 -5.40967093e-03 -5.40607201e-03 -5.40670157e-03 -4.55573368e-04
 -1.32477240e-03 -1.32389104e-03 -1.32404521e-03 -1.11565998e-04
 -1.69585216e-03 -1.69472400e-03 -1.69492136e-03 -1.42813131e-04
 -1.07332709e-04 -1.07982068e-04 -1.09088044e-04 -1.03983408e-04
 -8.67708763e-05 -8.72957190e-05 -8.81898354e-05 -8.40631581e-05
 -1.08445540e-04 -1.09102497e-04 -1.10219963e-04 -1.05061554e-04
 -5.58532074e-04 -5.61841388e-04 -5.67602230e-04 -5.41114949e-04
  4.36236911e-06  3.42143626e-06  3.12550414e-06  3.18394264e-06
 -6.48147771e-06 -5.12996281e-06 -4.69128580e-06 -4.75670170e-06
  1.42575767e-06  1.11020143e-06  1.01290114e-06  1.03486925e-06
 -2.08325457e-05 -1.64012997e-05 -1.49863629e-05 -1.52353886e-05
  2.83744387e-07 -2.46717112e-07 -3.45662268e-06  4.64873260e-06
 -3.53698114e-06  4.68302737e-06 -6.58803158e-06  8.37274890e-06
  4.74053811e+00]
supnorm grad right now is: 5.102263496448853
Weights right now are: 
[ 9.48918191e-01  1.13426320e-01  3.56505571e-01  8.92964272e-01
 -1.73913743e+00 -9.89626815e-02  2.14800475e-01  5.22118776e-01
 -2.33184432e+00 -1.99736441e+00 -2.07966513e+00 -5.47521190e-01
  1.42230596e+00  2.52197579e+00  1.38204353e+00  3.97007186e-01
  2.30026531e+00  1.29880417e+00  2.77764872e+00  2.70413163e-01
  2.45028928e+00  2.57837168e+00  1.94665058e+00  1.14604041e+00
 -9.25961481e-01 -1.54889821e+00 -2.23762159e+00 -1.26027259e+00
 -9.79248475e-01 -1.21026831e+00 -2.10339439e+00 -1.48538500e+00
 -2.10538682e+00 -1.53198392e+00 -1.12688583e+00 -1.05143265e+00
  1.01086242e+00  3.39344991e-01  1.14099077e+00 -2.79191860e-03
 -1.90852829e+00 -1.63769917e+00 -6.77481593e-01 -2.49923352e-01
 -4.65275023e-01 -1.96292781e+00 -1.18266756e+00 -1.66046431e+00
 -1.74655970e+00 -7.51216650e-01 -1.70029597e+00 -1.38856949e+00
 -1.87245916e-01 -1.03973417e+00 -1.06175346e+00 -1.81793438e+00
 -7.67933125e-01  2.25628176e+00 -8.27730503e-01  3.91542978e-01
 -4.49312070e-01  3.07754329e-01 -1.07877910e-01  6.28062084e-01
  3.10407871e+01]
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.635400186943
gradient value of function right now is: [-1.75370848e+00 -4.18290961e-03 -1.43609380e-03 -8.15543219e-04
  1.75370848e+00  4.18290961e-03  1.43609380e-03  8.15543219e-04
  4.38898214e-03  4.38987845e-03  4.38715378e-03  3.91089723e-04
  4.53711989e-04  4.53804648e-04  4.53522969e-04  4.04297102e-05
  8.59707229e-05  8.59882792e-05  8.59349122e-05  7.66044424e-06
  2.09313522e-04  2.09356273e-04  2.09226300e-04  1.86527985e-05
  2.40206191e-05  2.41798051e-05  2.44164059e-05  2.34476344e-05
  1.77585265e-05  1.78757658e-05  1.80506440e-05  1.73351519e-05
  2.47270239e-05  2.48900739e-05  2.51338135e-05  2.41368872e-05
  1.16107552e-04  1.16924630e-04  1.18068247e-04  1.13322143e-04
 -6.85524769e-07 -5.60802448e-07 -5.16154683e-07 -4.97969578e-07
  1.69578061e-06  1.30343608e-06  1.19901853e-06  1.20402673e-06
 -1.75771421e-07 -1.59752649e-07 -1.47712078e-07 -1.35928535e-07
  4.45616487e-06  3.49004979e-06  3.20976526e-06  3.18381318e-06
 -5.38736841e-09 -4.10716212e-08  9.36261653e-07 -9.53019767e-07
  9.49578715e-07 -8.58661805e-07  1.74075644e-06 -1.49750107e-06
  7.30622560e-01]
supnorm grad right now is: 1.7537084751222485
Weights right now are: 
[ 8.23144357e-01  5.78458033e-02  3.11233856e-01  8.33329905e-01
 -1.61336359e+00 -4.33821653e-02  2.60072190e-01  5.81753143e-01
 -2.45919688e+00 -2.17346403e+00 -2.20641280e+00 -4.54279988e-01
  1.54835279e+00  2.95077740e+00  1.50771673e+00  5.28988609e-01
  2.52836654e+00  1.57723135e+00  2.99324388e+00  3.00636022e-01
  2.57777595e+00  2.94062595e+00  2.07186658e+00  1.24916395e+00
 -9.58310748e-01 -1.59415529e+00 -2.27490115e+00 -1.30810420e+00
 -1.06500821e+00 -1.32217017e+00 -2.19919937e+00 -1.63807984e+00
 -2.14448310e+00 -1.58498679e+00 -1.17169312e+00 -1.09605474e+00
  9.93647179e-01  3.22350266e-01  1.12187467e+00 -1.11045606e-02
 -1.92745719e+00 -1.65890302e+00 -6.99974909e-01 -2.77026073e-01
 -4.92521034e-01 -1.99979815e+00 -1.22837287e+00 -1.71623592e+00
 -1.75626546e+00 -7.64140313e-01 -1.71647522e+00 -1.40882973e+00
 -2.42997866e-01 -1.11415626e+00 -1.16263954e+00 -1.93909733e+00
 -7.97181582e-01  2.28568195e+00 -9.08361603e-01  4.80807492e-01
 -5.33014548e-01  4.08768910e-01 -2.06874886e-01  7.78571469e-01
  3.09961535e+01]
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1036.3319906900526
gradient value of function right now is: [ 4.57748649e+00  3.91645897e-03  1.28884214e-03  8.84876414e-04
 -4.57748649e+00 -3.91645897e-03 -1.28884214e-03 -8.84876414e-04
 -4.33993670e-03 -4.34252255e-03 -4.33892640e-03 -5.49518796e-04
 -5.04409016e-04 -5.04709546e-04 -5.04291601e-04 -6.38670829e-05
 -1.34401934e-04 -1.34482015e-04 -1.34370647e-04 -1.70178585e-05
 -1.99638495e-04 -1.99757436e-04 -1.99592027e-04 -2.52774238e-05
 -1.82369876e-05 -1.84198317e-05 -1.84840144e-05 -1.81412258e-05
 -1.01068801e-05 -1.02084070e-05 -1.02439454e-05 -1.00538392e-05
 -1.85050374e-05 -1.86907340e-05 -1.87558491e-05 -1.84078997e-05
 -8.69808854e-05 -8.78409612e-05 -8.81486950e-05 -8.65225285e-05
  1.18421955e-07  9.43236745e-08  8.74219218e-08  9.27116098e-08
 -9.75505176e-07 -8.20713746e-07 -7.61238413e-07 -7.80320490e-07
 -4.11835582e-08 -3.91325341e-08 -3.62417735e-08 -3.46224112e-08
 -2.15813329e-06 -1.81158444e-06 -1.67925786e-06 -1.72379538e-06
 -1.01113188e-07  6.19583099e-08 -5.61413765e-07  6.41065546e-07
 -5.63282782e-07  6.30191821e-07 -8.63360099e-07  9.09285924e-07
 -1.16003760e+01]
supnorm grad right now is: 11.600375971058936
Weights right now are: 
[ 0.97699565  0.06681071  0.33251058  0.81635561 -1.76721489 -0.05234707
  0.23879546  0.59872743 -2.79107852 -2.58320428 -2.53723578 -0.34626556
  1.73082099  3.55061977  1.68967248  0.77165864  2.85636465  1.97692307
  3.3034564   0.35769877  2.75162965  3.39002746  2.24273428  1.43999348
 -1.02023484 -1.68143543 -2.34624873 -1.4023964  -1.1871001  -1.48160677
 -2.3353355  -1.85651445 -2.21741024 -1.68450888 -1.25523918 -1.18184608
  0.8941241   0.20536123  1.01258653 -0.1056159  -1.95094338 -1.68101796
 -0.72378414 -0.30750262 -0.52885134 -2.04773095 -1.28361641 -1.77938307
 -1.76809529 -0.77834084 -1.73418301 -1.43184786 -0.30978516 -1.20541345
 -1.27573105 -2.06549233 -0.80824098  2.31696498 -0.9737922   0.56685462
 -0.6021842   0.49021869 -0.30819804  0.88571703 30.86030896]
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -1034.6643690031765
gradient value of function right now is: [-1.61585030e+01 -4.15453343e-03 -1.36326831e-03 -9.63229223e-04
  1.61585030e+01  4.15453343e-03  1.36326831e-03  9.63229223e-04
  7.18505874e-03  7.18954095e-03  7.18394022e-03  8.69723351e-04
 -2.41348018e-05 -2.41498588e-05 -2.41310442e-05 -2.92147413e-06
 -4.71951983e-05 -4.72246403e-05 -4.71878510e-05 -5.71281821e-06
  5.00603590e-05  5.00915912e-05  5.00525645e-05  6.05976363e-06
  2.99808007e-05  3.02693593e-05  3.03659172e-05  2.98654224e-05
  1.59726448e-05  1.61260420e-05  1.61776413e-05  1.59109865e-05
  3.04446609e-05  3.07374787e-05  3.08356042e-05  3.03273734e-05
  7.92627291e-05  8.00341457e-05  8.02858577e-05  7.89622796e-05
  1.20183182e-06  9.93583512e-07  9.18615665e-07  9.40073251e-07
  1.86334722e-06  1.53913828e-06  1.42547326e-06  1.46065926e-06
  9.59209709e-07  7.92604886e-07  7.31964202e-07  7.49405385e-07
  3.28146058e-06  2.72514741e-06  2.52292540e-06  2.57699404e-06
  6.16856767e-07 -5.10869082e-07  1.36378415e-06 -1.49749862e-06
  1.32684744e-06 -1.38008391e-06  1.74151733e-06 -1.69695360e-06
  1.25828747e+01]
supnorm grad right now is: 16.158502950136622
Weights right now are: 
[ 6.63287915e-01  8.82176679e-04  2.64687935e-01  7.30331546e-01
 -1.45350715e+00  1.35814613e-02  3.06618111e-01  6.84751502e-01
 -2.96296472e+00 -2.78044525e+00 -2.70873139e+00 -1.04496225e-01
  1.95381616e+00  4.27174207e+00  1.91206957e+00  1.10303931e+00
  3.23512266e+00  2.43772386e+00  3.66185882e+00  4.35683821e-01
  3.00963130e+00  4.02524225e+00  2.49640655e+00  1.70770529e+00
 -1.06630175e+00 -1.74627666e+00 -2.39929135e+00 -1.47280443e+00
 -1.26687008e+00 -1.58466368e+00 -2.42391290e+00 -1.99448433e+00
 -2.27128719e+00 -1.75789106e+00 -1.31689655e+00 -1.24569917e+00
  9.01546978e-01  2.13614151e-01  1.01898697e+00 -9.48886598e-02
 -1.97390044e+00 -1.70313796e+00 -7.45533285e-01 -3.30969635e-01
 -5.54758847e-01 -2.07508758e+00 -1.31170277e+00 -1.81020444e+00
 -1.78034686e+00 -7.92979412e-01 -1.75025484e+00 -1.44979060e+00
 -3.50842773e-01 -1.24998151e+00 -1.32359535e+00 -2.11797061e+00
 -8.25141352e-01  2.33468321e+00 -1.00014296e+00  6.08727986e-01
 -6.27742604e-01  5.27148152e-01 -3.39852489e-01  9.35457812e-01
  3.10284049e+01]
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.292979652247
gradient value of function right now is: [ 3.99286375e+00  4.69884059e-04  1.56629007e-04  1.12970048e-04
 -3.99286375e+00 -4.69884059e-04 -1.56629007e-04 -1.12970048e-04
 -7.43189081e-04 -7.43550825e-04 -7.43113088e-04 -1.27781579e-04
 -5.53031551e-05 -5.53300704e-05 -5.52975014e-05 -9.50841117e-06
 -1.47090730e-05 -1.47162324e-05 -1.47075690e-05 -2.52901956e-06
 -1.75852178e-05 -1.75937759e-05 -1.75834203e-05 -3.02343822e-06
 -2.37294606e-06 -2.39538191e-06 -2.39953763e-06 -2.37127786e-06
 -1.15543283e-06 -1.16639837e-06 -1.16840994e-06 -1.15463572e-06
 -2.38277966e-06 -2.40534059e-06 -2.40950748e-06 -2.38111633e-06
 -1.00495490e-05 -1.01425060e-05 -1.01606248e-05 -1.00417698e-05
 -3.66462239e-08 -3.24201397e-08 -3.00839575e-08 -3.05029493e-08
 -1.22700523e-07 -1.05477178e-07 -9.79417152e-08 -1.01007404e-07
 -3.34068366e-08 -2.97777937e-08 -2.76095304e-08 -2.78575532e-08
 -2.34959598e-07 -2.01142590e-07 -1.86672114e-07 -1.92984728e-07
 -2.64062573e-08  1.37442540e-08 -7.97317297e-08  9.34846736e-08
 -7.89521468e-08  9.09645581e-08 -1.06074135e-07  1.13203122e-07
  4.94970089e+00]
supnorm grad right now is: 4.949700894110655
Weights right now are: 
[ 0.98550599  0.07561821  0.34267978  0.79498466 -1.77572523 -0.06115457
  0.22862627  0.62009839 -3.32912749 -3.1817926  -3.07437074 -0.26808819
  2.08290085  4.68352266  2.04081818  1.292992    3.46193491  2.71274212
  3.87665131  0.48247211  3.14930293  4.35469063  2.63378423  1.83396477
 -1.12116685 -1.82068493 -2.46159254 -1.55267694 -1.3299332  -1.66007846
 -2.49208579 -2.0833929  -2.33291651 -1.83868028 -1.38636084 -1.31767243
  0.79214777  0.09176902  0.90295437 -0.20533703 -1.98608339 -1.71294751
 -0.75485216 -0.34092002 -0.57117315 -2.08961885 -1.32540769 -1.8241456
 -1.78844739 -0.8003076  -1.7574661  -1.45758159 -0.37724076 -1.27428583
 -1.34667783 -2.14104846 -0.83123282  2.34305976 -1.01378418  0.62212545
 -0.64088528  0.5386529  -0.3583979   0.95082577 30.99626168]
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.270825847122
gradient value of function right now is: [-3.27752350e+00 -1.15691817e-04 -3.93086498e-05 -2.95417206e-05
  3.27752350e+00  1.15691817e-04  3.93086498e-05  2.95417206e-05
  2.57678603e-04  2.57781498e-04  2.57658079e-04  5.16044472e-05
  1.13660302e-05  1.13705703e-05  1.13651244e-05  2.27635194e-06
  2.48319357e-06  2.48418522e-06  2.48299577e-06  4.97306383e-07
  3.75196169e-06  3.75346049e-06  3.75166265e-06  7.51438477e-07
  7.83472622e-07  7.90890287e-07  7.93431306e-07  7.80926762e-07
  4.08803618e-07  4.12649458e-07  4.13983521e-07  4.07465530e-07
  7.87870666e-07  7.95311688e-07  7.97871673e-07  7.85302261e-07
  4.38638231e-06  4.42955500e-06  4.44335315e-06  4.37267767e-06
  1.04641227e-10 -6.04189670e-10 -5.91492379e-10 -2.23077138e-10
  5.80104355e-08  4.70038323e-08  4.34512159e-08  4.44955506e-08
  4.19610727e-09  2.78912896e-09  2.53817351e-09  2.92506925e-09
  1.16032516e-07  9.50477246e-08  8.78398129e-08  8.93400778e-08
  8.14665001e-09 -8.92124657e-09  3.43393535e-08 -3.79177553e-08
  3.40086279e-08 -3.45185654e-08  5.33542977e-08 -5.02215655e-08
  8.48796533e+00]
supnorm grad right now is: 8.487965331024311
Weights right now are: 
[ 0.83595323  0.05970631  0.32083513  0.77454766 -1.62617246 -0.04524267
  0.25047092  0.64053539 -3.57028438 -3.43577923 -3.3153261  -0.22818936
  2.29994017  5.35956911  2.25730113  1.67128251  3.83272729  3.15748328
  4.22865502  0.5792643   3.40935231  4.91043651  2.88977881  2.03974321
 -1.16256552 -1.8720644  -2.507186   -1.60625706 -1.36859127 -1.70180342
 -2.53234011 -2.1270215  -2.37837113 -1.89331211 -1.43596967 -1.36824719
  0.74838234  0.04530538  0.8571622  -0.24843522 -1.99061288 -1.71638501
 -0.75805313 -0.34439226 -0.57720083 -2.09380866 -1.32935819 -1.82851894
 -1.79183949 -0.80285874 -1.75985392 -1.46023097 -0.38701691 -1.28113764
 -1.35316076 -2.14812298 -0.83252319  2.34591406 -1.01607908  0.63025495
 -0.64313593  0.54586556 -0.36160144  0.95858022 31.00099805]
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.4298465837771
gradient value of function right now is: [ 1.96833536e+00  2.78248562e-05  1.05715809e-05  8.23680707e-06
 -1.96833536e+00 -2.78248562e-05 -1.05715809e-05 -8.23680707e-06
 -6.76063510e-05 -6.76290664e-05 -6.76018477e-05 -1.77324279e-05
 -3.42226288e-06 -3.42341234e-06 -3.42203508e-06 -8.97587404e-07
 -8.64678935e-07 -8.64969447e-07 -8.64621346e-07 -2.26794567e-07
 -1.36200208e-06 -1.36245953e-06 -1.36191143e-06 -3.57222882e-07
 -1.91256004e-07 -1.93032000e-07 -1.93173391e-07 -1.91505626e-07
 -1.01656041e-07 -1.02607763e-07 -1.02679964e-07 -1.01792698e-07
 -1.92792372e-07 -1.94588877e-07 -1.94729785e-07 -1.93046980e-07
 -2.57495838e-06 -2.59749941e-06 -2.59987273e-06 -2.57762530e-06
  1.44773357e-08  1.24284090e-08  1.15718926e-08  1.23290240e-08
 -1.01659273e-08 -9.14069601e-09 -8.51557326e-09 -8.86474177e-09
  8.06540200e-09  7.00395207e-09  6.51340869e-09  6.90174662e-09
 -2.73248401e-08 -2.40955444e-08 -2.24324131e-08 -2.35906953e-08
  1.39358144e-09 -5.65287241e-10 -3.16885808e-09  3.44463634e-09
 -3.48311091e-09  4.51578098e-09 -7.17514514e-09  9.41336930e-09
  7.34682230e+00]
supnorm grad right now is: 7.346822304081305
Weights right now are: 
[ 0.88621378  0.06601399  0.32340417  0.78708427 -1.67643302 -0.05155035
  0.24790188  0.62799878 -3.83544423 -3.71174052 -3.58032315 -0.33934295
  2.45901057  5.80306408  2.41597838  1.89756812  4.07673886  3.4318624
  4.46404787  0.65119869  3.59102783  5.17721294  3.06954285  2.12278814
 -1.1823594  -1.89307801 -2.5276867  -1.62718934 -1.38198018 -1.71541907
 -2.54594349 -2.14053642 -2.39884701 -1.9147997  -1.45708485 -1.38912223
  0.66359144 -0.04099299  0.77080599 -0.33236339 -1.99047386 -1.71617162
 -0.75787564 -0.3443048  -0.5789836  -2.09516452 -1.33057723 -1.82972936
 -1.79185982 -0.80279178 -1.75980251 -1.46024141 -0.39072247 -1.28407182
 -1.35579613 -2.15066888 -0.83251737  2.34625044 -1.01734885  0.63134386
 -0.64442859  0.54679272 -0.36393019  0.96011046 31.03365436]
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1036.695544078993
gradient value of function right now is: [ 3.62735498e+00  2.26466886e-05  1.00122834e-05  8.15075810e-06
 -3.62735498e+00 -2.26466886e-05 -1.00122834e-05 -8.15075810e-06
 -5.38151879e-05 -5.38326223e-05 -5.38116416e-05 -1.67543607e-05
 -4.03887876e-06 -4.04018704e-06 -4.03861268e-06 -1.25741472e-06
 -1.39481526e-06 -1.39526713e-06 -1.39472335e-06 -4.34249181e-07
 -1.98583424e-06 -1.98647749e-06 -1.98570341e-06 -6.18244738e-07
 -1.53054110e-07 -1.54598349e-07 -1.54838463e-07 -1.53061208e-07
 -8.02905846e-08 -8.11032162e-08 -8.12284813e-08 -8.02952561e-08
 -1.54750455e-07 -1.56313935e-07 -1.56556346e-07 -1.54758426e-07
 -2.47237275e-06 -2.49672044e-06 -2.50073219e-06 -2.47228261e-06
  1.67790470e-08  1.43495795e-08  1.33222540e-08  1.37407436e-08
 -1.28805475e-08 -1.11072547e-08 -1.03199901e-08 -1.05852467e-08
  1.04885256e-08  9.02904530e-09  8.37510426e-09  8.59686487e-09
 -3.37500078e-08 -2.90075366e-08 -2.69342518e-08 -2.76853742e-08
  2.75942415e-09 -1.75446966e-09 -4.14591830e-09  4.37315660e-09
 -4.38397285e-09  4.76890087e-09 -1.01686210e-08  1.02646050e-08
 -1.14820905e+01]
supnorm grad right now is: 11.48209047768755
Weights right now are: 
[ 0.96763289  0.09791807  0.3570047   0.83099008 -1.75785213 -0.08345443
  0.21430135  0.58409297 -4.11796522 -4.00213949 -3.86272081 -0.4744242
  2.61853677  6.15858008  2.57514552  2.04024238  4.27875669  3.64410926
  4.66276989  0.7109275   3.74882969  5.35810154  3.22658749  2.17921287
 -1.19315422 -1.9041191  -2.5387223  -1.63804195 -1.38903269 -1.72257152
 -2.55312043 -2.14757036 -2.40992204 -1.92610074 -1.46839528 -1.40018522
  0.60068348 -0.10511576  0.70620549 -0.39455701 -1.98950431 -1.7153713
 -0.75713803 -0.34361078 -0.58019792 -2.09613495 -1.33146663 -1.8306156
 -1.79122853 -0.80226616 -1.75931756 -1.45978075 -0.39351234 -1.286336
 -1.35787654 -2.15270712 -0.83223     2.34601894 -1.01791651  0.63189534
 -0.64498199  0.54721302 -0.36515642  0.96087447 30.85571361]
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1035.9690053850986
gradient value of function right now is: [ 3.15986286e+00  9.70826753e-06  5.76989508e-06  4.48174040e-06
 -3.15986286e+00 -9.70826753e-06 -5.76989508e-06 -4.48174040e-06
 -2.22144618e-05 -2.22215379e-05 -2.22130122e-05 -9.08322948e-06
 -1.85478936e-06 -1.85538013e-06 -1.85466836e-06 -7.58398643e-07
 -8.54566607e-07 -8.54838817e-07 -8.54510846e-07 -3.49422039e-07
 -1.17589886e-06 -1.17627339e-06 -1.17582214e-06 -4.80809342e-07
 -6.35077614e-08 -6.41805603e-08 -6.42730510e-08 -6.35345174e-08
 -3.30568732e-08 -3.34082308e-08 -3.34560491e-08 -3.30712480e-08
 -6.43523712e-08 -6.50351179e-08 -6.51286637e-08 -6.43798694e-08
 -1.43446546e-06 -1.44925002e-06 -1.45143183e-06 -1.43492330e-06
  9.02243125e-09  7.76613590e-09  7.21276877e-09  7.41225577e-09
 -8.07049166e-09 -6.99220082e-09 -6.49937784e-09 -6.64889285e-09
  6.01356610e-09  5.21418934e-09  4.83848918e-09  4.94619366e-09
 -2.07575550e-08 -1.79356481e-08 -1.66603548e-08 -1.70743167e-08
  1.46985690e-09 -8.72984849e-10 -2.92522674e-09  3.03850466e-09
 -2.98086381e-09  3.16712690e-09 -6.49794764e-09  6.29583560e-09
 -1.69649793e+01]
supnorm grad right now is: 16.964979290198983
Weights right now are: 
[ 0.94550743  0.10046918  0.35744943  0.83689295 -1.73572666 -0.08600555
  0.21385662  0.57819009 -4.32527722 -4.21303765 -4.06997416 -0.59251229
  2.76132925  6.35686271  2.71774772  2.11686993  4.38865017  3.75494162
  4.77233798  0.75032684  3.82787642  5.43898719  3.30555864  2.20969307
 -1.19673095 -1.90774064 -2.54236748 -1.64158762 -1.39147855 -1.72504469
 -2.55560833 -2.14999728 -2.41360801 -1.92983143 -1.47215065 -1.40383709
  0.49448407 -0.21260014  0.59818653 -0.50008465 -1.98788294 -1.71404335
 -0.75591632 -0.34240281 -0.5810066  -2.09675858 -1.33203957 -1.83119124
 -1.79008587 -0.80133151 -1.75845802 -1.45892387 -0.39589154 -1.28822349
 -1.35961263 -2.15443279 -0.83170625  2.34558334 -1.01804283  0.63203155
 -0.64512416  0.54729512 -0.36588285  0.96140355 30.78355248]
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.1428227139436
gradient value of function right now is: [-6.87452707e-01 -1.34640757e-06 -9.83813357e-07 -7.39520059e-07
  6.87452707e-01  1.34640757e-06  9.83813357e-07  7.39520059e-07
  3.00724583e-06  3.00819371e-06  3.00705971e-06  1.43819768e-06
  1.95639164e-07  1.95700849e-07  1.95627047e-07  9.35641916e-08
  1.01681077e-07  1.01713127e-07  1.01674783e-07  4.86284077e-08
  1.73468397e-07  1.73523090e-07  1.73457654e-07  8.29610516e-08
  8.91947872e-09  8.95669086e-09  9.01147532e-09  8.85109923e-09
  4.74914392e-09  4.76780076e-09  4.79739954e-09  4.71215678e-09
  9.04495522e-09  9.08159716e-09  9.13749382e-09  8.97497677e-09
  2.60942527e-07  2.62534950e-07  2.63988922e-07  2.59168680e-07
 -1.92656035e-09 -1.38898725e-09 -1.28171414e-09 -1.33870148e-09
  2.26641161e-09  1.57027107e-09  1.45024014e-09  1.56849397e-09
 -1.40627293e-09 -1.00692106e-09 -9.28479340e-10 -9.80321681e-10
  5.81835869e-09  4.15541249e-09  3.83978837e-09  4.07909541e-09
 -2.08981634e-10  4.35270448e-10  7.00433238e-10 -1.54188393e-09
  7.33483373e-10 -1.33113841e-09  1.64030778e-09 -2.71400791e-09
 -1.10456085e+01]
supnorm grad right now is: 11.045608491089592
Weights right now are: 
[ 0.80899819  0.07082886  0.32972104  0.81904417 -1.59921743 -0.05636522
  0.24158501  0.59603888 -4.42003338 -4.30879251 -4.16471296 -0.63977404
  2.85355247  6.45604419  2.80993138  2.16244269  4.44853746  3.81491963
  4.83219563  0.77736709  3.8668632   5.47810551  3.34453815  2.22824267
 -1.19785426 -1.90886608 -2.54350676 -1.64269106 -1.39230546 -1.72587413
 -2.5564461  -2.15081295 -2.41477557 -1.93100113 -1.47333454 -1.40498422
  0.42344807 -0.28430246  0.5261265  -0.57066212 -1.98701999 -1.7133415
 -0.75526947 -0.34176458 -0.58154052 -2.09714114 -1.33238665 -1.83155661
 -1.78946053 -0.80081824 -1.75798444 -1.45845758 -0.39742577 -1.28939316
 -1.3606816  -2.15551799 -0.83147295  2.34537627 -1.01815545  0.63241055
 -0.64525267  0.54762667 -0.36640663  0.96191089 30.84808015]
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -1036.4874507967643
gradient value of function right now is: [ 6.61227956e+00  8.39899869e-06  7.43748264e-06  5.41236042e-06
 -6.61227956e+00 -8.39899869e-06 -7.43748264e-06 -5.41236042e-06
 -1.45532727e-05 -1.45578059e-05 -1.45523523e-05 -8.23953516e-06
 -2.02341966e-06 -2.02404994e-06 -2.02329169e-06 -1.14558636e-06
 -1.37724299e-06 -1.37767198e-06 -1.37715589e-06 -7.79745025e-07
 -1.62315580e-06 -1.62366140e-06 -1.62305314e-06 -9.18971544e-07
 -3.73050605e-08 -3.76711811e-08 -3.77324611e-08 -3.73103218e-08
 -1.74978524e-08 -1.76698416e-08 -1.76985182e-08 -1.75004086e-08
 -3.76552218e-08 -3.80250532e-08 -3.80868662e-08 -3.76606268e-08
 -5.23659136e-07 -5.28723037e-07 -5.29598665e-07 -5.23709948e-07
  2.63010877e-10  2.11330508e-10  1.96347646e-10  2.09750765e-10
 -4.82855947e-09 -4.08253755e-09 -3.78954525e-09 -3.91604284e-09
  3.41728779e-10  2.82186870e-10  2.61711091e-10  2.74431130e-10
 -1.03652939e-08 -8.76218334e-09 -8.12924245e-09 -8.40216979e-09
 -5.10619248e-10  3.57160566e-10 -2.75102062e-09  3.33073027e-09
 -2.68943636e-09  3.17000493e-09 -4.09581472e-09  4.44859668e-09
  3.07767292e+00]
supnorm grad right now is: 6.612279559379721
Weights right now are: 
[ 1.0051216   0.12296326  0.37398016  0.85440744 -1.79534084 -0.10849962
  0.19732589  0.5606756  -4.65893527 -4.54855305 -4.40359058 -0.77656544
  2.92660827  6.52981171  2.88297907  2.20174151  4.50069826  3.86710347
  4.88435069  0.8050206   3.88973253  5.50098893  3.36740597  2.24120473
 -1.19933898 -1.91036414 -2.54501571 -1.64416645 -1.39329495 -1.72687252
 -2.55745065 -2.15179737 -2.41631664 -1.93255592 -1.47490049 -1.40651569
  0.33349071 -0.37516095  0.43488273 -0.66036093 -1.98625241 -1.71273938
 -0.75470502 -0.34117345 -0.58226885 -2.0976829  -1.33289774 -1.83210396
 -1.78890334 -0.80038163 -1.75757554 -1.45802859 -0.39927716 -1.2907918
 -1.36199997 -2.15691881 -0.83135734  2.34525097 -1.01828694  0.63279409
 -0.64537178  0.54795608 -0.36673394  0.96263256 30.95040722]
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.099832412206
gradient value of function right now is: [ 2.80377996e+00  2.66144396e-06  2.58513485e-06  1.88544540e-06
 -2.80377996e+00 -2.66144396e-06 -2.58513485e-06 -1.88544540e-06
 -4.91178140e-06 -4.91331049e-06 -4.91146921e-06 -3.00832624e-06
 -5.45872839e-07 -5.46042788e-07 -5.45838138e-07 -3.34330900e-07
 -3.93915189e-07 -3.94037819e-07 -3.93890152e-07 -2.41261825e-07
 -5.20521098e-07 -5.20683155e-07 -5.20488009e-07 -3.18803680e-07
 -1.32949506e-08 -1.34367155e-08 -1.34516254e-08 -1.33097386e-08
 -6.55345385e-09 -6.62360746e-09 -6.63087247e-09 -6.56085977e-09
 -1.34810306e-08 -1.36250536e-08 -1.36401179e-08 -1.34961400e-08
 -2.78034790e-07 -2.80891884e-07 -2.81230656e-07 -2.78301521e-07
  4.13470410e-10  3.49314508e-10  3.24200173e-10  3.39287776e-10
 -2.25569349e-09 -1.97097727e-09 -1.83125072e-09 -1.87683760e-09
  3.90681750e-10  3.34712223e-10  3.10358665e-10  3.21670950e-10
 -5.00527967e-09 -4.35750893e-09 -4.04601995e-09 -4.15628511e-09
 -1.74091856e-10  8.15377831e-11 -1.28388839e-09  1.37913973e-09
 -1.24812348e-09  1.34644780e-09 -1.95186897e-09  1.89977924e-09
 -9.33772399e+00]
supnorm grad right now is: 9.337723986525514
Weights right now are: 
[ 0.90726465  0.10612545  0.35825297  0.84300752 -1.69748389 -0.09166181
  0.21305307  0.57207553 -4.69888333 -4.58852904 -4.44353491 -0.7945047
  2.98324554  6.5865471   2.93961249  2.23542129  4.54908303  3.91550389
  4.93273217  0.83365099  3.91738414  5.52865007  3.39505594  2.2579639
 -1.19985852 -1.91088923 -2.54554349 -1.64468165 -1.39369895 -1.72728062
 -2.55786038 -2.15219909 -2.4168678  -1.93311285 -1.47546023 -1.40706239
  0.28437614 -0.42476792  0.38513739 -0.70935935 -1.98593466 -1.71247492
 -0.75446108 -0.34092553 -0.58265625 -2.09799834 -1.33318767 -1.83239488
 -1.78865967 -0.80017965 -1.75738944 -1.45783896 -0.40025494 -1.29159657
 -1.36273974 -2.15765994 -0.83132075  2.34522274 -1.01848666  0.6329131
 -0.64557179  0.54805745 -0.36713968  0.96292431 30.88818405]
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1035.4527152471198
gradient value of function right now is: [ 4.50963917e+00  3.70249403e-06  3.71777645e-06  2.75051456e-06
 -4.50963917e+00 -3.70249403e-06 -3.71777645e-06 -2.75051456e-06
 -7.16119416e-06 -7.16342589e-06 -7.16073890e-06 -4.55734852e-06
 -8.70320086e-07 -8.70591329e-07 -8.70264751e-07 -5.53866468e-07
 -6.76327451e-07 -6.76538224e-07 -6.76284455e-07 -4.30411429e-07
 -8.19209295e-07 -8.19464608e-07 -8.19157209e-07 -5.21339839e-07
 -1.89052880e-08 -1.91007708e-08 -1.91297988e-08 -1.89136609e-08
 -9.13569757e-09 -9.23033750e-09 -9.24431933e-09 -9.13980819e-09
 -1.91289671e-08 -1.93269511e-08 -1.93562931e-08 -1.91375090e-08
 -3.36245441e-07 -3.39657583e-07 -3.40187182e-07 -3.36372692e-07
  9.01518166e-11  6.97204838e-11  6.47153732e-11  7.08437142e-11
 -3.19467330e-09 -2.72612830e-09 -2.53140418e-09 -2.60169716e-09
  2.29642351e-10  1.91507972e-10  1.77541030e-10  1.85012788e-10
 -6.85111437e-09 -5.84366720e-09 -5.42311436e-09 -5.57582071e-09
 -3.69698909e-10  2.46457564e-10 -1.88030262e-09  2.10145171e-09
 -1.81980426e-09  1.99014602e-09 -2.77058298e-09  2.79274316e-09
 -1.71973675e+01]
supnorm grad right now is: 17.197367505695443
Weights right now are: 
[ 0.97809728  0.12115097  0.3729617   0.853814   -1.76831652 -0.10668733
  0.19834435  0.56126905 -4.73897468 -4.62863904 -4.48362283 -0.81667096
  3.01030698  6.61362171  2.9666723   2.2526235   4.57626487  3.94269397
  4.95991233  0.8508134   3.93026482  5.54153441  3.40793592  2.26636352
 -1.20020566 -1.91123932 -2.54589663 -1.64502473 -1.39394795 -1.72753172
 -2.55811323 -2.15244596 -2.41723472 -1.93348282 -1.47583335 -1.40742513
  0.25822086 -0.45119816  0.35860531 -0.73543337 -1.98579029 -1.71235652
 -0.75435027 -0.34081167 -0.58292882 -2.09820921 -1.33338308 -1.83259617
 -1.78854216 -0.80008296 -1.75729912 -1.45774664 -0.40089332 -1.29210075
 -1.3632073  -2.15813718 -0.83132377  2.34522756 -1.01861134  0.63305774
 -0.64569437  0.54817704 -0.36737836  0.96315472 30.80801713]
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1036.377625833822
gradient value of function right now is: [-1.01345888e+01 -6.97686314e-06 -7.20874656e-06 -5.43156565e-06
  1.01345888e+01  6.97686314e-06  7.20874656e-06  5.43156565e-06
  1.45406258e-05  1.45451800e-05  1.45396987e-05  9.54407913e-06
  1.28564167e-06  1.28604431e-06  1.28555970e-06  8.43862120e-07
  9.33163503e-07  9.33455772e-07  9.33104006e-07  6.12503704e-07
  1.40132816e-06  1.40176702e-06  1.40123882e-06  9.19795806e-07
  4.14858502e-08  4.18894496e-08  4.19846861e-08  4.14507259e-08
  2.12435664e-08  2.14496580e-08  2.14986979e-08  2.12252352e-08
  4.21812061e-08  4.25910729e-08  4.26880810e-08  4.21451868e-08
  1.00212385e-06  1.01208864e-06  1.01430331e-06  1.00139825e-06
 -1.25613861e-09 -1.06500986e-09 -9.87970892e-10 -1.00196372e-09
  9.93140390e-09  8.18026179e-09  7.58139860e-09  7.81548939e-09
 -1.48964371e-09 -1.26033257e-09 -1.16766846e-09 -1.18556293e-09
  2.18152075e-08  1.80589535e-08  1.67303207e-08  1.71963008e-08
  8.22161691e-10 -8.03596760e-10  5.55658472e-09 -6.46662732e-09
  5.37582517e-09 -5.92238836e-09  8.66646356e-09 -8.99869454e-09
  1.33026657e+00]
supnorm grad right now is: 10.134588817095816
Weights right now are: 
[ 0.76050531  0.10275659  0.35385584  0.83922873 -1.55072455 -0.08829295
  0.2174502   0.57585432 -4.71312533 -4.60278443 -4.45777442 -0.79553025
  3.04712066  6.6504475   3.00348372  2.2768102   4.61328634  3.97972675
  4.99693151  0.87501485  3.95327545  5.56455188  3.43094517  2.28168197
 -1.20037808 -1.91141288 -2.54607213 -1.64519374 -1.39411668 -1.7277018
 -2.55828445 -2.15261296 -2.41742509 -1.93367445 -1.47602701 -1.40761202
  0.2347583  -0.47490347  0.33482749 -0.75883089 -1.98566761 -1.71225095
 -0.75425269 -0.34071334 -0.58315317 -2.09838898 -1.33354793 -1.83276151
 -1.78844063 -0.79999603 -1.75721888 -1.4576659  -0.4014305  -1.29253843
 -1.36360908 -2.1585385  -0.83131539  2.34523827 -1.01874317  0.63313839
 -0.6458212   0.54824581 -0.36761332  0.96332913 30.97281633]
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1036.767979823832
gradient value of function right now is: [ 4.37104722e+00  2.64808371e-06  2.78549970e-06  2.13684873e-06
 -4.37104722e+00 -2.64808371e-06 -2.78549970e-06 -2.13684873e-06
 -6.11269247e-06 -6.11456569e-06 -6.11231508e-06 -4.14162803e-06
 -6.05415360e-07 -6.05600905e-07 -6.05377977e-07 -4.10195431e-07
 -4.85461530e-07 -4.85610299e-07 -4.85431558e-07 -3.28922289e-07
 -6.24543541e-07 -6.24734948e-07 -6.24504977e-07 -4.23155590e-07
 -1.67157474e-08 -1.68772160e-08 -1.68987043e-08 -1.67296352e-08
 -8.41055607e-09 -8.49203494e-09 -8.50276858e-09 -8.41764819e-09
 -1.69504134e-08 -1.71143909e-08 -1.71361249e-08 -1.69646029e-08
 -3.80575080e-07 -3.84150807e-07 -3.84668998e-07 -3.80848818e-07
  1.38204345e-10  1.07045740e-10  9.95365704e-11  1.09769128e-10
 -3.18616568e-09 -2.71332262e-09 -2.52101543e-09 -2.62514912e-09
  2.95287907e-10  2.44941189e-10  2.27276768e-10  2.40224131e-10
 -6.99098429e-09 -5.93841734e-09 -5.51508252e-09 -5.75085400e-09
 -3.06348284e-10  1.83344474e-10 -1.77242382e-09  2.41869899e-09
 -1.73387712e-09  2.33151412e-09 -2.58991941e-09  3.14392787e-09
  1.39403746e+01]
supnorm grad right now is: 13.940374599885061
Weights right now are: 
[ 0.93053349  0.12054147  0.37171716  0.85290913 -1.72075272 -0.10607783
  0.19958889  0.56217392 -4.7579781  -4.6476556  -4.50262291 -0.82162917
  3.07103231  6.67436608  3.02739403  2.29314857  4.64041018  4.00685856
  5.02405378  0.89338917  3.96676766  5.57804769  3.44443672  2.29111584
 -1.2007357  -1.91177287 -2.54643679 -1.64554657 -1.39436674 -1.72795363
 -2.55853883 -2.15286053 -2.41780219 -1.934054   -1.47641135 -1.4079842
  0.21216286 -0.49770481  0.31188676 -0.78131734 -1.9855764  -1.71217869
 -0.75418486 -0.34064179 -0.5834607  -2.09860263 -1.33375132 -1.83298415
 -1.78835833 -0.79993308 -1.75715964 -1.45760254 -0.40210412 -1.29301136
 -1.36405914 -2.15902845 -0.83130758  2.34527496 -1.0188047   0.63336337
 -0.64586911  0.54843145 -0.3677002   0.96368134 31.0295254 ]
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1036.2459679914766
gradient value of function right now is: [ 6.80522575e+00  4.02894586e-06  4.23813386e-06  3.27818967e-06
 -6.80522575e+00 -4.02894586e-06 -4.23813386e-06 -3.27818967e-06
 -9.77439847e-06 -9.77742013e-06 -9.77378586e-06 -6.65325413e-06
 -9.98535622e-07 -9.98844324e-07 -9.98473033e-07 -6.79683658e-07
 -8.18533557e-07 -8.18786599e-07 -8.18482255e-07 -5.57160760e-07
 -1.00893017e-06 -1.00924209e-06 -1.00886693e-06 -6.86759006e-07
 -2.68203014e-08 -2.70866983e-08 -2.71294702e-08 -2.68293551e-08
 -1.34540711e-08 -1.35878810e-08 -1.36092943e-08 -1.34586711e-08
 -2.71881818e-08 -2.74584164e-08 -2.75017477e-08 -2.71974228e-08
 -5.90714478e-07 -5.96504280e-07 -5.97462416e-07 -5.90889775e-07
  1.11867442e-10  8.71725373e-11  8.11004886e-11  8.83214679e-11
 -5.52284750e-09 -4.66490810e-09 -4.32961672e-09 -4.47346352e-09
  4.33152704e-10  3.62929644e-10  3.36458911e-10  3.49343589e-10
 -1.19474861e-08 -1.00944960e-08 -9.36417168e-09 -9.67420978e-09
 -6.06909987e-10  4.43653465e-10 -3.20193529e-09  3.86634839e-09
 -3.10814183e-09  3.64953980e-09 -4.72913980e-09  5.10983270e-09
 -3.57749133e-01]
supnorm grad right now is: 6.805225754999804
Weights right now are: 
[ 1.04253069  0.13067827  0.38170168  0.86108402 -1.83274993 -0.11621463
  0.18960437  0.55399903 -4.77320625 -4.662896   -4.51784755 -0.83017519
  3.07543999  6.67877446  3.03180166  2.29629998  4.64610644  4.01255607
  5.02974985  0.89737271  3.96879149  5.58007141  3.44646066  2.29266722
 -1.20088632 -1.91192801 -2.54659334 -1.64569422 -1.39446115 -1.72805047
 -2.55863645 -2.15295339 -2.41796074 -1.93421715 -1.47657595 -1.40813971
  0.2080755  -0.50194062  0.30762912 -0.78535072 -1.98555974 -1.71215989
 -0.75416768 -0.3406276  -0.58362801 -2.09875283 -1.33388542 -1.83310424
 -1.78833175 -0.79990556 -1.75713468 -1.45758104 -0.4024537  -1.29333477
 -1.36434783 -2.15928164 -0.83135478  2.34529986 -1.01899622  0.63335434
 -0.64605095  0.54840236 -0.36802656  0.96368759 30.9800811 ]
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.2059120929678
gradient value of function right now is: [ 4.49576074e+00  2.53989938e-06  2.67721256e-06  2.08888999e-06
 -4.49576074e+00 -2.53989938e-06 -2.67721256e-06 -2.08888999e-06
 -6.51466022e-06 -6.51666560e-06 -6.51425447e-06 -4.45663065e-06
 -5.94877455e-07 -5.95060591e-07 -5.94840399e-07 -4.06950028e-07
 -4.75842416e-07 -4.75988894e-07 -4.75812780e-07 -3.25520195e-07
 -6.19129774e-07 -6.19320375e-07 -6.19091206e-07 -4.23540792e-07
 -1.82399607e-08 -1.84219822e-08 -1.84463253e-08 -1.82541570e-08
 -9.31996912e-09 -9.41321261e-09 -9.42557939e-09 -9.32732070e-09
 -1.85147615e-08 -1.86997650e-08 -1.87244261e-08 -1.85292708e-08
 -4.39206799e-07 -4.43483882e-07 -4.44097362e-07 -4.39507402e-07
  1.84693409e-10  1.47801739e-10  1.37264183e-10  1.47878513e-10
 -3.81767515e-09 -3.25986702e-09 -3.02724025e-09 -3.13287908e-09
  3.77367599e-10  3.16853074e-10  2.93803960e-10  3.07001080e-10
 -8.34650165e-09 -7.11539483e-09 -6.60430313e-09 -6.84146009e-09
 -3.86433779e-10  2.39880761e-10 -2.18462198e-09  2.71825478e-09
 -2.12650336e-09  2.60350994e-09 -3.21966826e-09  3.57259198e-09
  3.92178350e+00]
supnorm grad right now is: 4.4957607403989615
Weights right now are: 
[ 0.94726275  0.12344294  0.37355632  0.85504047 -1.73748199 -0.1089793
  0.19774972  0.56004258 -4.7505182  -4.64020487 -4.49515992 -0.81238055
  3.08677028  6.69010793  3.04313133  2.30421208  4.65821624  4.02466937
  5.04185896  0.90576977  3.97702277  5.58830488  3.45469151  2.29848367
 -1.20091182 -1.91195303 -2.54662066 -1.64571586 -1.39449733 -1.72808658
 -2.55867379 -2.15298755 -2.41799108 -1.93424698 -1.47660815 -1.40816609
  0.2044079  -0.50564832  0.30386369 -0.78893359 -1.9855353  -1.71213882
 -0.7541481  -0.34060835 -0.58375131 -2.09883699 -1.33396299 -1.83318336
 -1.78830569 -0.7998845  -1.75711518 -1.45756156 -0.40271863 -1.29352086
 -1.36451933 -2.15945343 -0.83136165  2.34532715 -1.01906104  0.63340518
 -0.64611122  0.54843284 -0.36813867  0.96379493 31.00358008]
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.0346942467877
gradient value of function right now is: [ 3.43244218e+00  1.89326746e-06  1.99467397e-06  1.56682366e-06
 -3.43244218e+00 -1.89326746e-06 -1.99467397e-06 -1.56682366e-06
 -5.13090283e-06 -5.13248964e-06 -5.13058006e-06 -3.51647157e-06
 -4.38320624e-07 -4.38456197e-07 -4.38293044e-07 -3.00402484e-07
 -3.46829065e-07 -3.46936328e-07 -3.46807247e-07 -2.37699721e-07
 -4.60645622e-07 -4.60788100e-07 -4.60616636e-07 -3.15702878e-07
 -1.46296369e-08 -1.47825438e-08 -1.48007860e-08 -1.46441470e-08
 -7.55739687e-09 -7.63662627e-09 -7.64597812e-09 -7.56499283e-09
 -1.48644100e-08 -1.50200085e-08 -1.50384974e-08 -1.48792518e-08
 -3.66847169e-07 -3.70572235e-07 -3.71056736e-07 -3.67168256e-07
  1.94050202e-10  1.57931938e-10  1.46561446e-10  1.56355913e-10
 -3.22578885e-09 -2.78211122e-09 -2.58473278e-09 -2.65968416e-09
  3.49129169e-10  2.95745291e-10  2.74253608e-10  2.85134247e-10
 -7.03605418e-09 -6.05589831e-09 -5.62290944e-09 -5.79352676e-09
 -3.33884857e-10  1.93872711e-10 -1.87730875e-09  2.14965060e-09
 -1.81703359e-09  2.06116870e-09 -2.76844373e-09  2.84304538e-09
 -8.53322575e+00]
supnorm grad right now is: 8.53322574872979
Weights right now are: 
[ 0.96148447  0.12391304  0.37353993  0.85534746 -1.7517037  -0.1094494
  0.19776612  0.55973559 -4.74455648 -4.63424556 -4.48919742 -0.80649796
  3.09239353  6.69573256  3.04875433  2.30820503  4.66457776  4.03103258
  5.04822016  0.9102335   3.98083126  5.59211417  3.45849987  2.30124684
 -1.20096308 -1.91200482 -2.54667428 -1.64576333 -1.39453721 -1.72812677
 -2.558715   -2.15302542 -2.41804607 -1.93430245 -1.47666551 -1.40821718
  0.20245647 -0.50765411  0.30182537 -0.79080778 -1.9855221  -1.71212381
 -0.75413442 -0.3405973  -0.58386395 -2.0989274  -1.33404262 -1.83325485
 -1.7882864  -0.79986495 -1.7570975  -1.45754666 -0.40296131 -1.29372466
 -1.36469915 -2.15961062 -0.83138791  2.34535358 -1.01918662  0.63340837
 -0.6462343   0.54842096 -0.36836401  0.96382533 30.88009982]
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.0073939066385
gradient value of function right now is: [ 5.19628274e+00  2.70177155e-06  2.85355826e-06  2.26907035e-06
 -5.19628274e+00 -2.70177155e-06 -2.85355826e-06 -2.26907035e-06
 -7.68346389e-06 -7.68583168e-06 -7.68298440e-06 -5.31630798e-06
 -6.54601014e-07 -6.54802756e-07 -6.54560157e-07 -4.52927345e-07
 -5.23783602e-07 -5.23945016e-07 -5.23750914e-07 -3.62413925e-07
 -6.86177486e-07 -6.86388960e-07 -6.86134658e-07 -4.74775524e-07
 -2.18832637e-08 -2.21017786e-08 -2.21335828e-08 -2.18968939e-08
 -1.13128517e-08 -1.14260416e-08 -1.14424208e-08 -1.13199848e-08
 -2.22274146e-08 -2.24495926e-08 -2.24818563e-08 -2.22413475e-08
 -5.47231790e-07 -5.52591954e-07 -5.53411707e-07 -5.47534933e-07
  2.04824214e-10  1.64942690e-10  1.53192692e-10  1.63856808e-10
 -4.90143822e-09 -4.16057605e-09 -3.86387789e-09 -3.99738567e-09
  4.73765315e-10  3.98036636e-10  3.69127279e-10  3.84157107e-10
 -1.06815374e-08 -9.06085406e-09 -8.41035544e-09 -8.70495843e-09
 -4.99048433e-10  3.45318359e-10 -2.79855428e-09  3.45561616e-09
 -2.71668646e-09  3.28594970e-09 -4.12860795e-09  4.56363788e-09
  1.93476216e-01]
supnorm grad right now is: 5.196282735286852
Weights right now are: 
[ 0.99742293  0.12921895  0.3782949   0.85949208 -1.78764217 -0.11475531
  0.19301114  0.55559097 -4.75424894 -4.6439459  -4.49888779 -0.80967905
  3.10500658  6.70834907  3.06136672  2.31720389  4.67912827  4.04558725
  5.06276985  0.92049458  3.98920727  5.6004923   3.4668755   2.30735183
 -1.20111966 -1.91216239 -2.54683533 -1.64591529 -1.39465129 -1.72824162
 -2.55883173 -2.15313707 -2.41821182 -1.93446921 -1.47683584 -1.40837821
  0.19473068 -0.51547074  0.29392292 -0.79843205 -1.98549047 -1.71209585
 -0.75410842 -0.34057196 -0.58406175 -2.09906524 -1.33417183 -1.8333893
 -1.78825026 -0.7998354  -1.75706997 -1.45751922 -0.40337711 -1.29402119
 -1.36497666 -2.15989448 -0.83139575  2.34539585 -1.01926232  0.63351196
 -0.64630587  0.54849408 -0.36850426  0.96399748 30.94635933]
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1036.9250753690094
gradient value of function right now is: [ 2.59076994e+00  1.30289469e-06  1.37397958e-06  1.10273428e-06
 -2.59076994e+00 -1.30289469e-06 -1.37397958e-06 -1.10273428e-06
 -3.95334025e-06 -3.95453305e-06 -3.95310226e-06 -2.73957604e-06
 -3.07448342e-07 -3.07541119e-07 -3.07429828e-07 -2.13053995e-07
 -2.40922732e-07 -2.40995424e-07 -2.40908228e-07 -1.66953978e-07
 -3.27250464e-07 -3.27349216e-07 -3.27230757e-07 -2.26776357e-07
 -1.11849554e-08 -1.12895268e-08 -1.12983382e-08 -1.12037268e-08
 -5.83704537e-09 -5.89191476e-09 -5.89639636e-09 -5.84700068e-09
 -1.13586056e-08 -1.14650910e-08 -1.14739588e-08 -1.13778182e-08
 -2.96496075e-07 -2.99135528e-07 -2.99415791e-07 -2.96923835e-07
  1.38365431e-10  1.06073807e-10  9.86517486e-11  1.11338074e-10
 -2.09584996e-09 -1.81483671e-09 -1.68921211e-09 -1.78064173e-09
  2.30580177e-10  1.90524706e-10  1.77012653e-10  1.91398253e-10
 -4.69545727e-09 -4.03492790e-09 -3.75396628e-09 -3.97138141e-09
 -1.59181545e-10  4.87346559e-11 -1.09454747e-09  1.80010672e-09
 -1.08668529e-09  1.78963630e-09 -1.55685623e-09  2.27787780e-09
  1.41513514e+01]
supnorm grad right now is: 14.15135141270705
Weights right now are: 
[ 0.9145299   0.12559009  0.3736938   0.85627469 -1.70474913 -0.11112646
  0.19761225  0.55880836 -4.7279743  -4.61767011 -4.47261269 -0.78850913
  3.1138934   6.71723812  3.07025314  2.32357685  4.68886626  4.05532785
  5.07250735  0.92740281  3.99586612  5.60715263  3.4735341   2.31220306
 -1.20114079 -1.91218591 -2.54685995 -1.645933   -1.3946806  -1.72827233
 -2.55886305 -2.15316458 -2.41823765 -1.93449746 -1.47686522 -1.40840054
  0.19458249 -0.51569535  0.29367253 -0.79849795 -1.98547865 -1.71208421
 -0.75409796 -0.34056305 -0.58419222 -2.09918023 -1.33427316 -1.83347445
 -1.78823019 -0.79981704 -1.75705362 -1.45750485 -0.40365269 -1.29427087
 -1.36519629 -2.16007505 -0.83143649  2.3454252  -1.01943763  0.63345068
 -0.64647756  0.54841896 -0.36881258  0.96397243 31.05529742]
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1036.2597381662792
gradient value of function right now is: [ 9.60207753e-01  4.71207503e-07  4.96328807e-07  4.01489665e-07
 -9.60207753e-01 -4.71207503e-07 -4.96328807e-07 -4.01489665e-07
 -1.40705193e-06 -1.40749584e-06 -1.40695849e-06 -9.77364994e-07
 -1.14279873e-07 -1.14315946e-07 -1.14272277e-07 -7.93796466e-08
 -8.98267967e-08 -8.98551379e-08 -8.98208309e-08 -6.23952897e-08
 -1.21246288e-07 -1.21284560e-07 -1.21238228e-07 -8.42185580e-08
 -4.11329527e-09 -4.17255308e-09 -4.17013687e-09 -4.13142134e-09
 -2.14515318e-09 -2.17633423e-09 -2.17498206e-09 -2.15473000e-09
 -4.18969374e-09 -4.25029866e-09 -4.24779147e-09 -4.20825593e-09
 -9.90965336e-08 -1.00407035e-07 -1.00380090e-07 -9.94842998e-08
  1.29027741e-12 -7.29341715e-12 -7.12069557e-12 -1.83859672e-12
 -8.33978436e-10 -8.21840019e-10 -7.66603792e-10 -7.57271781e-10
  6.00480991e-11  5.14281519e-11  4.76194586e-11  5.13901472e-11
 -1.75148418e-09 -1.70852900e-09 -1.59203524e-09 -1.58421104e-09
 -1.44280980e-10 -1.88262785e-12 -5.99264480e-10  3.06724204e-10
 -5.62276900e-10  3.33488640e-10 -8.19091741e-10  3.77402111e-10
 -1.73974532e+01]
supnorm grad right now is: 17.397453174570828
Weights right now are: 
[ 0.83424558  0.12457707  0.37199748  0.85523724 -1.62446482 -0.11011343
  0.19930856  0.55984581 -4.72124278 -4.61094054 -4.4658805  -0.78144005
  3.12090065  6.72424719  3.07726003  2.32863147  4.69665897  4.06312271
  5.08029965  0.93295521  4.00104978  5.61233754  3.47871753  2.31601008
 -1.20119849 -1.91224363 -2.54692019 -1.64598643 -1.39472755 -1.72831942
 -2.5589115  -2.15320928 -2.41829918 -1.93455896 -1.47692931 -1.40845768
  0.19249197 -0.51781533  0.2914909  -0.80048804 -1.98546374 -1.71207077
 -0.75408574 -0.34055135 -0.58432657 -2.09927291 -1.33435811 -1.83355871
 -1.78820981 -0.79980087 -1.757039   -1.45749028 -0.40393559 -1.29447029
 -1.36537881 -2.16025324 -0.83144676  2.34546184 -1.01952228  0.63348555
 -0.64655433  0.54843181 -0.36895024  0.96407511 30.84460758]
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.233307574463
gradient value of function right now is: [ 3.29338215e+00  1.55207468e-06  1.63806996e-06  1.33410750e-06
 -3.29338215e+00 -1.55207468e-06 -1.63806996e-06 -1.33410750e-06
 -4.71793618e-06 -4.71937396e-06 -4.71764684e-06 -3.30040819e-06
 -3.91571543e-07 -3.91690890e-07 -3.91547522e-07 -2.73920718e-07
 -3.10967317e-07 -3.11062085e-07 -3.10948246e-07 -2.17535505e-07
 -4.13404207e-07 -4.13530209e-07 -4.13378847e-07 -2.89193574e-07
 -1.33457001e-08 -1.34767354e-08 -1.34908286e-08 -1.33627661e-08
 -6.91679320e-09 -6.98496750e-09 -6.99218212e-09 -6.92576059e-09
 -1.35532827e-08 -1.36866153e-08 -1.37008665e-08 -1.35707333e-08
 -3.28789674e-07 -3.31904327e-07 -3.32285343e-07 -3.29159017e-07
  3.76688710e-11  2.14650641e-11  1.99246643e-11  2.69981059e-11
 -2.69447166e-09 -2.31932396e-09 -2.15571688e-09 -2.24426607e-09
  2.10110008e-10  1.73220488e-10  1.60708718e-10  1.71503507e-10
 -5.88829340e-09 -5.04795814e-09 -4.68963398e-09 -4.89302669e-09
 -2.75539245e-10  1.40511073e-10 -1.52195449e-09  2.07405023e-09
 -1.48764887e-09  2.01616587e-09 -2.18458987e-09  2.64871945e-09
  8.74953109e+00]
supnorm grad right now is: 8.749531087289277
Weights right now are: 
[ 0.94583794  0.13477462  0.38201888  0.86371652 -1.73605718 -0.12031098
  0.18928716  0.55136653 -4.75238671 -4.64209968 -4.49702067 -0.8002571
  3.12784119  6.73118939  3.08420029  2.33372408  4.70529092  4.07175695
  5.08893116  0.93916521  4.00537587  5.61666443  3.48304352  2.3193031
 -1.2014026  -1.91245151 -2.54712984 -1.64618729 -1.39485881 -1.72845284
 -2.55904591 -2.15333882 -2.41851184 -1.93477545 -1.47714762 -1.40866701
  0.18605008 -0.52439432  0.28487523 -0.80686517 -1.98544682 -1.71205357
 -0.7540698  -0.34053671 -0.58450514 -2.09942443 -1.3344944  -1.83368505
 -1.7881838  -0.79977606 -1.75701635 -1.45746948 -0.40431243 -1.2947978
 -1.36567346 -2.1605224  -0.83149091  2.34549343 -1.01970079  0.63349888
 -0.64672366  0.54842529 -0.36924667  0.9641277  31.00296284]
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1036.2969522273897
gradient value of function right now is: [ 6.32204402e+00  2.93349607e-06  3.09179004e-06  2.53461974e-06
 -6.32204402e+00 -2.93349607e-06 -3.09179004e-06 -2.53461974e-06
 -9.20774350e-06 -9.21058131e-06 -9.20716822e-06 -6.45815785e-06
 -7.90339858e-07 -7.90583454e-07 -7.90290473e-07 -5.54329890e-07
 -6.39693490e-07 -6.39890643e-07 -6.39653522e-07 -4.48670251e-07
 -8.22190878e-07 -8.22444291e-07 -8.22139502e-07 -5.76669607e-07
 -2.62542575e-08 -2.65178338e-08 -2.65581509e-08 -2.62677325e-08
 -1.35771434e-08 -1.37136407e-08 -1.37344433e-08 -1.35841778e-08
 -2.66643620e-08 -2.69322503e-08 -2.69731676e-08 -2.66781165e-08
 -6.32105899e-07 -6.38364161e-07 -6.39353338e-07 -6.32401893e-07
  3.92670249e-12 -4.39816113e-12 -3.86455217e-12  7.25299470e-13
 -5.96545392e-09 -5.04586301e-09 -4.68382328e-09 -4.83789354e-09
  4.17382144e-10  3.49794795e-10  3.24306944e-10  3.36853844e-10
 -1.28473106e-08 -1.08696043e-08 -1.00844452e-08 -1.04153482e-08
 -6.86219716e-10  4.97793985e-10 -3.48435559e-09  4.18189238e-09
 -3.37471143e-09  3.94176921e-09 -5.10951695e-09  5.48703789e-09
 -3.92228264e+00]
supnorm grad right now is: 6.322044024026634
Weights right now are: 
[ 1.06271332  0.14349641  0.39071382  0.87111399 -1.85293255 -0.12903277
  0.18059223  0.54396905 -4.77185104 -4.66157387 -4.51648265 -0.81199135
  3.13099317  6.73434202  3.08735217  2.33609131  4.7095869   4.07605401
  5.09322695  0.94229471  4.00705515  5.61834387  3.48472278  2.32065337
 -1.20152372 -1.91257377 -2.54725434 -1.64630495 -1.39493464 -1.72852936
 -2.55912363 -2.15341283 -2.41863746 -1.9349022  -1.47727667 -1.4087891
  0.18316447 -0.52732502  0.28189211 -0.80967173 -1.98543943 -1.71204633
 -0.75406302 -0.34053056 -0.58463905 -2.099519   -1.33458177 -1.83377318
 -1.78816939 -0.79976419 -1.75700537 -1.45745897 -0.40458857 -1.29499731
 -1.36585743 -2.16070412 -0.8315065   2.34552578 -1.01977473  0.63355446
 -0.64679647  0.5484575  -0.36938068  0.96423414 30.94588227]
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1036.6740317456192
gradient value of function right now is: [-7.45350275e+00 -3.33273129e-06 -3.50335531e-06 -2.89580034e-06
  7.45350275e+00  3.33273129e-06  3.50335531e-06  2.89580034e-06
  1.05232945e-05  1.05265580e-05  1.05226339e-05  7.36334241e-06
  7.93337863e-07  7.93583864e-07  7.93288072e-07  5.55114704e-07
  6.04924510e-07  6.05112106e-07  6.04886537e-07  4.23276960e-07
  8.59427811e-07  8.59694305e-07  8.59373873e-07  6.01359209e-07
  3.07036794e-08  3.09936627e-08  3.10638398e-08  3.06795479e-08
  1.61264620e-08  1.62782495e-08  1.63153423e-08  1.61134812e-08
  3.12145260e-08  3.15088749e-08  3.15803710e-08  3.11897145e-08
  7.46149933e-07  7.53390746e-07  7.55022787e-07  7.45673172e-07
 -8.70997233e-11 -9.19746235e-11 -8.68647472e-11 -7.91986478e-11
  7.82241332e-09  6.39875816e-09  5.92647015e-09  6.13109964e-09
 -6.02193253e-10 -5.16564813e-10 -4.79131440e-10 -4.82811363e-10
  1.68186650e-08  1.38345628e-08  1.28091434e-08  1.32085684e-08
  8.74330326e-10 -8.71697325e-10  4.53010929e-09 -5.48621656e-09
  4.38392254e-09 -5.00738253e-09  6.84166956e-09 -7.37540616e-09
 -5.85218989e+00]
supnorm grad right now is: 7.4535027459217265
Weights right now are: 
[ 0.76951462  0.12740866  0.37317736  0.85697242 -1.55973386 -0.11294502
  0.19812868  0.55811063 -4.71616911 -4.60588041 -4.46080254 -0.76983421
  3.14285742  6.74620953  3.09921579  2.34464542  4.7215388   4.08800928
  5.10517819  0.95085969  4.01728789  5.62857934  3.49495502  2.32807846
 -1.20144669 -1.91249705 -2.54717895 -1.64622476 -1.39491347 -1.72850852
 -2.55910351 -2.15339    -2.41856297 -1.93482802 -1.47720385 -1.40871138
  0.18472972 -0.52579103  0.28339718 -0.80803828 -1.98543077 -1.7120373
 -0.75405458 -0.34052292 -0.58473048 -2.09959089 -1.33464585 -1.83382942
 -1.78815618 -0.799752   -1.75699419 -1.45744897 -0.404775   -1.29514947
 -1.36599287 -2.1608191  -0.83153079  2.34555342 -1.01987316  0.6335302
 -0.6468914   0.54841877 -0.36955402  0.96424027 30.90960222]
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -1036.4704374910182
gradient value of function right now is: [ 6.70299272e+00  2.92216612e-06  3.07450955e-06  2.55878598e-06
 -6.70299272e+00 -2.92216612e-06 -3.07450955e-06 -2.55878598e-06
 -9.67455159e-06 -9.67752555e-06 -9.67394980e-06 -6.82374754e-06
 -8.04125499e-07 -8.04372703e-07 -8.04075473e-07 -5.67172124e-07
 -6.47561022e-07 -6.47760083e-07 -6.47520741e-07 -4.56743831e-07
 -8.41152674e-07 -8.41411261e-07 -8.41100345e-07 -5.93288422e-07
 -2.77043947e-08 -2.79802248e-08 -2.80214991e-08 -2.77201836e-08
 -1.43923413e-08 -1.45358462e-08 -1.45572323e-08 -1.44006194e-08
 -2.81428628e-08 -2.84232725e-08 -2.84651637e-08 -2.81589802e-08
 -6.70268739e-07 -6.76845615e-07 -6.77866242e-07 -6.70618157e-07
 -5.16086955e-11 -5.25450995e-11 -4.84436635e-11 -4.47201932e-11
 -6.26828004e-09 -5.30462141e-09 -4.92248797e-09 -5.08919916e-09
  4.04691578e-10  3.38355522e-10  3.13645972e-10  3.26634332e-10
 -1.34992927e-08 -1.14248317e-08 -1.05965359e-08 -1.09554313e-08
 -7.32937547e-10  5.18986012e-10 -3.67822418e-09  4.47077072e-09
 -3.56911964e-09  4.22244617e-09 -5.38238698e-09  5.83447389e-09
  5.69919085e+00]
supnorm grad right now is: 6.702992718680942
Weights right now are: 
[ 0.9800462   0.14207624  0.38805036  0.86953735 -1.77026544 -0.1276126
  0.18325569  0.5455457  -4.7572477  -4.646973   -4.50187824 -0.79608249
  3.14723846  6.75059176  3.10359659  2.34796235  4.72768851  4.09416076
  5.11132754  0.95536644  4.01954974  5.63084172  3.49721677  2.32992287
 -1.20162522 -1.91267418 -2.54736063 -1.6463984  -1.39502427 -1.72861867
 -2.55921612 -2.1534982  -2.41874692 -1.93501052 -1.47739099 -1.40889033
  0.17944093 -0.53108297  0.27800125 -0.81321978 -1.98541867 -1.71202473
 -0.75404273 -0.34051234 -0.58489259 -2.09967495 -1.33472901 -1.83393334
 -1.78813858 -0.79973866 -1.75698124 -1.45743586 -0.40510274 -1.29531958
 -1.36616207 -2.16102849 -0.83150768  2.3456023  -1.01984037  0.63373263
 -0.64684895  0.54858583 -0.36948931  0.96451202 30.97396223]
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.1183493919996
gradient value of function right now is: [-8.29831809e-01 -3.50916243e-07 -3.68179772e-07 -3.08750775e-07
  8.29831809e-01  3.50916243e-07  3.68179772e-07  3.08750775e-07
  1.18056511e-06  1.18092898e-06  1.18049353e-06  8.31035472e-07
  8.94439055e-08  8.94714529e-08  8.94384901e-08  6.29638591e-08
  6.93330469e-08  6.93544144e-08  6.93288440e-08  4.88057910e-08
  9.58863101e-08  9.59158413e-08  9.58805046e-08  6.74989806e-08
  3.44875074e-09  3.46661653e-09  3.48497563e-09  3.42850757e-09
  1.81163596e-09  1.82066721e-09  1.83044660e-09  1.80081775e-09
  3.49986327e-09  3.51763032e-09  3.53637539e-09  3.47910718e-09
  8.46289564e-08  8.52093893e-08  8.56150458e-08  8.42012737e-08
 -1.73181567e-11 -2.83627787e-11 -2.69380026e-11 -2.04633469e-11
  1.11920981e-09  7.95008412e-10  7.35183606e-10  7.90859928e-10
 -8.85010236e-11 -7.71285605e-11 -7.18052250e-11 -6.94113605e-11
  2.41490396e-09  1.75494663e-09  1.62315536e-09  1.72255051e-09
  7.39748242e-11 -2.16596901e-10  5.11881841e-10 -9.89748348e-10
  5.04787485e-10 -8.50706949e-10  8.38995449e-10 -1.36571528e-09
 -1.12138254e+01]
supnorm grad right now is: 11.21382544895916
Weights right now are: 
[ 0.85681817  0.13541964  0.38056171  0.8635114  -1.64703741 -0.120956
  0.19074434  0.55157165 -4.73561865 -4.62533902 -4.48025018 -0.77807996
  3.15596766  6.75932355  3.11232526  2.3543265   4.73687189  4.1033469
  5.12051037  0.96200445  4.02683767  5.63813177  3.50450426  2.33528554
 -1.20161257 -1.91265792 -2.54734876 -1.64638085 -1.39503379 -1.72862637
 -2.55922619 -2.1535051  -2.41873623 -1.9349961  -1.47738106 -1.40887462
  0.17873424 -0.53174415  0.27724205 -0.81382222 -1.98540726 -1.71201188
 -0.75403062 -0.3405018  -0.58500764 -2.09971626 -1.33477356 -1.83399985
 -1.7881248  -0.79972789 -1.7569706  -1.45742529 -0.40532646 -1.29539746
 -1.3662477  -2.16115581 -0.83147801  2.34564596 -1.01975838  0.63391899
 -0.64676497  0.54873483 -0.36936614  0.96475521 30.83712772]
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.3483450481428
gradient value of function right now is: [-3.43734670e+00 -1.42468285e-06 -1.49291023e-06 -1.25825820e-06
  3.43734670e+00  1.42468285e-06  1.49291023e-06  1.25825820e-06
  4.95165143e-06  4.95318985e-06  4.95134039e-06  3.48793921e-06
  3.60402827e-07  3.60514776e-07  3.60380198e-07  2.53869085e-07
  2.76921779e-07  2.77007812e-07  2.76904385e-07  1.95063654e-07
  3.88209416e-07  3.88330001e-07  3.88185040e-07  2.73456155e-07
  1.46087440e-08  1.47415387e-08  1.47808575e-08  1.45871528e-08
  7.71638542e-09  7.78611278e-09  7.80704801e-09  7.70475961e-09
  1.48534293e-08  1.49880623e-08  1.50281559e-08  1.48312598e-08
  3.58778713e-07  3.62202937e-07  3.63114444e-07  3.58330090e-07
 -1.71989580e-11 -3.04269312e-11 -2.91897758e-11 -2.15835670e-11
  3.99013126e-09  3.20719588e-09  2.96783604e-09  3.07463076e-09
 -2.92256975e-10 -2.52249370e-10 -2.33918459e-10 -2.33065562e-10
  8.55510193e-09  6.93116638e-09  6.41189422e-09  6.61135577e-09
  4.45383657e-10 -5.11501936e-10  2.29801807e-09 -2.81243398e-09
  2.22478848e-09 -2.52666668e-09  3.50937505e-09 -3.82600593e-09
 -5.48234991e+00]
supnorm grad right now is: 5.482349907708683
Weights right now are: 
[ 0.80581197  0.13365733  0.37830777  0.86178674 -1.59603121 -0.11919369
  0.19299827  0.55329631 -4.72091131 -4.61062924 -4.46554348 -0.76548877
  3.16159677  6.76495425  3.11795404  2.35846931  4.74300525  4.10948204
  5.12664336  0.96646568  4.03139815  5.6426935   3.50906447  2.3386859
 -1.20160911 -1.91265174 -2.54734564 -1.64637255 -1.39504252 -1.72863373
 -2.55923519 -2.15351127 -2.41873335 -1.9349904  -1.4773785  -1.40886678
  0.17899528 -0.53144244  0.27747792 -0.81345568 -1.98540167 -1.71200407
 -0.75402347 -0.34049596 -0.58509656 -2.09975932 -1.33481382 -1.83404567
 -1.78811539 -0.79971998 -1.75696329 -1.45741846 -0.40551116 -1.29549084
 -1.366335   -2.1612519  -0.83146576  2.345689   -1.01978135  0.63397327
 -0.64678175  0.54877354 -0.36939381  0.96488283 30.94428763]
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.2228096432864
gradient value of function right now is: [-5.42530181e+00 -2.21130602e-06 -2.31327450e-06 -1.95941491e-06
  5.42530181e+00  2.21130602e-06  2.31327450e-06  1.95941491e-06
  7.88155091e-06  7.88400316e-06  7.88105324e-06  5.55181430e-06
  5.56766385e-07  5.56939590e-07  5.56731239e-07  3.92191458e-07
  4.24248805e-07  4.24380802e-07  4.24222017e-07  2.98843738e-07
  6.01884505e-07  6.02071745e-07  6.01846510e-07  4.23973091e-07
  2.33694935e-08  2.35933627e-08  2.36494536e-08  2.33472272e-08
  1.23866668e-08  1.25048469e-08  1.25347795e-08  1.23746008e-08
  2.37704478e-08  2.39977287e-08  2.40549179e-08  2.37475524e-08
  5.75839218e-07  5.81539376e-07  5.82856701e-07  5.75386514e-07
 -2.30420514e-11 -3.76345160e-11 -3.61350889e-11 -2.74978430e-11
  6.28069287e-09  5.12908333e-09  4.74843717e-09  4.89782054e-09
 -4.58817158e-10 -3.95706467e-10 -3.66847547e-10 -3.67170201e-10
  1.34420659e-08  1.10460144e-08  1.02224120e-08  1.05050801e-08
  7.37130925e-10 -7.47541830e-10  3.69662084e-09 -4.26835561e-09
  3.56819057e-09 -3.86188032e-09  5.59659458e-09 -5.78364140e-09
 -1.35382082e+00]
supnorm grad right now is: 5.425301805937529
Weights right now are: 
[ 0.77958317  0.13307014  0.37726758  0.86111573 -1.5698024  -0.1186065
  0.19403847  0.55396732 -4.71263913 -4.60235637 -4.45727166 -0.75745056
  3.16635865  6.76971746  3.12271563  2.36199405  4.74826441  4.11474271
  5.13190219  0.97030437  4.03521687  5.64651326  3.51288297  2.3415569
 -1.20162106 -1.91266047 -2.54735796 -1.64637889 -1.3950584  -1.72864791
 -2.55925132 -2.15352414 -2.41874578 -1.93499945 -1.47739124 -1.40887345
  0.1790075  -0.53138696  0.27745922 -0.81332527 -1.98539563 -1.71199474
 -0.75401483 -0.34048917 -0.58519762 -2.09980845 -1.33485949 -1.83409777
 -1.7881046  -0.79971003 -1.75695404 -1.45741024 -0.40572112 -1.2956
 -1.36643643 -2.16136229 -0.83146006  2.3457363  -1.01980578  0.63405242
 -0.64680406  0.54883026 -0.36944403  0.96502364 30.92646696]
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1036.6125320357
gradient value of function right now is: [-8.05011818e+00 -3.17526184e-06 -3.31674206e-06 -2.82993039e-06
  8.05011818e+00  3.17526184e-06  3.31674206e-06  2.82993039e-06
  1.15621877e-05  1.15658019e-05  1.15614496e-05  8.15720832e-06
  7.94578354e-07  7.94826700e-07  7.94527642e-07  5.60582731e-07
  5.98124218e-07  5.98311180e-07  5.98086037e-07  4.21981255e-07
  8.63708202e-07  8.63978154e-07  8.63653078e-07  6.09354516e-07
  3.45990145e-08  3.49507148e-08  3.50284392e-08  3.45783028e-08
  1.84059831e-08  1.85925525e-08  1.86341300e-08  1.83946754e-08
  3.52153029e-08  3.55728194e-08  3.56520745e-08  3.51939658e-08
  8.50811900e-07  8.59658472e-07  8.61498518e-07  8.50404154e-07
  1.38849099e-11 -8.52693103e-12 -9.40889131e-12  9.82792925e-13
  9.42739422e-09  7.79112573e-09  7.21936223e-09  7.40974594e-09
 -6.58515285e-10 -5.68621914e-10 -5.27312351e-10 -5.28060786e-10
  2.00537254e-08  1.66580814e-08  1.54284040e-08  1.57863068e-08
  1.17355157e-09 -1.09188490e-09  5.63640469e-09 -6.07314111e-09
  5.40829074e-09 -5.49188999e-09  8.46108601e-09 -8.22728619e-09
  9.05450698e+00]
supnorm grad right now is: 9.05450697947513
Weights right now are: 
[ 0.7652543   0.13294638  0.37641198  0.86070696 -1.55547354 -0.11848274
  0.19489407  0.55437609 -4.7127347  -4.60245824 -4.45736518 -0.75354727
  3.17476996  6.77813088  3.13112656  2.3682411   4.75759193  4.12407274
  5.14122925  0.97713066  4.04196248  5.65326044  3.51962832  2.34664813
 -1.20171746 -1.91275979 -2.54745897 -1.6464725  -1.3951309  -1.72872214
 -2.55932648 -2.15359516 -2.41884798 -1.93510463 -1.47749815 -1.40897281
  0.17739064 -0.53308528  0.27572324 -0.81487902 -1.98539014 -1.71198904
 -0.75400943 -0.34048418 -0.58534077 -2.09992332 -1.33496408 -1.83419633
 -1.78809075 -0.7996977  -1.75694269 -1.45739968 -0.40600972 -1.29583507
 -1.3666504  -2.16156052 -0.83149092  2.34576913 -1.01993053  0.63406008
 -0.64691579  0.54881906 -0.3696348   0.96507519 31.07480127]
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.642862522293
gradient value of function right now is: [-4.33534350e-01 -1.69917255e-07 -1.76487678e-07 -1.52075747e-07
  4.33534350e-01  1.69917255e-07  1.76487678e-07  1.52075747e-07
  5.95860444e-07  5.96064636e-07  5.95817191e-07  4.19580536e-07
  4.55531662e-08  4.55687427e-08  4.55498712e-08  3.20804197e-08
  3.49624029e-08  3.49743802e-08  3.49598663e-08  2.46194921e-08
  4.87828030e-08  4.87994839e-08  4.87792744e-08  3.43548434e-08
  1.93854665e-09  1.95684950e-09  1.97124405e-09  1.92048675e-09
  1.03017494e-09  1.03959433e-09  1.04729598e-09  1.02052820e-09
  1.97635001e-09  1.99471418e-09  2.00943991e-09  1.95783595e-09
  4.10890757e-08  4.16004058e-08  4.18990025e-08  4.07111289e-08
  2.25231679e-11  5.22011411e-12  4.22879207e-12  1.05114741e-11
  9.71225569e-10  7.31728266e-10  6.73150247e-10  6.79811847e-10
 -5.27962737e-11 -5.10985613e-11 -4.73946272e-11 -4.18644687e-11
  1.99752037e-09  1.53443362e-09  1.41086545e-09  1.40353400e-09
  1.47875470e-10 -2.24099387e-10  6.15435980e-10 -4.97259562e-10
  5.81927039e-10 -3.68730917e-10  9.94651586e-10 -7.69278682e-10
  3.44734971e+00]
supnorm grad right now is: 3.4473497134210636
Weights right now are: 
[ 0.843451    0.14031671  0.38366384  0.86710708 -1.63367024 -0.12585307
  0.18764221  0.54797596 -4.74571782 -4.63545473 -4.49034516 -0.77485727
  3.17820214  6.78156384  3.13455862  2.37087054  4.762064    4.12854597
  5.14570111  0.98045256  4.0442226   5.65552093  3.52188839  2.3484608
 -1.20188566 -1.91293028 -2.5476313  -1.64663815 -1.3952352  -1.72882776
 -2.55943312 -2.15369809 -2.4190218  -1.93528074 -1.47767616 -1.40914399
  0.17317187 -0.53738366  0.27138607 -0.81904706 -1.9853848  -1.71198219
 -0.754003   -0.34047855 -0.5854748  -2.10002641 -1.33505894 -1.83428994
 -1.78807743 -0.79968499 -1.75693095 -1.45738883 -0.40628256 -1.29605102
 -1.36684885 -2.16175241 -0.8315184   2.34579769 -1.0200174   0.63411196
 -0.64700038  0.54884834 -0.36979155  0.96515402 30.96282937]
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.4675572518445
gradient value of function right now is: [ 1.66338434e+00  6.31672762e-07  6.61336563e-07  5.67140094e-07
 -1.66338434e+00 -6.31672762e-07 -6.61336563e-07 -5.67140094e-07
 -2.30247589e-06 -2.30316345e-06 -2.30233885e-06 -1.63879895e-06
 -1.73840143e-07 -1.73892070e-07 -1.73829791e-07 -1.23730790e-07
 -1.35476157e-07 -1.35516614e-07 -1.35468093e-07 -9.64257503e-08
 -1.85389083e-07 -1.85444459e-07 -1.85378043e-07 -1.31950749e-07
 -6.51518293e-09 -6.57860575e-09 -6.57981131e-09 -6.53346582e-09
 -3.41547378e-09 -3.44900782e-09 -3.44952355e-09 -3.42522044e-09
 -6.61869202e-09 -6.68340098e-09 -6.68454695e-09 -6.63741346e-09
 -1.63370624e-07 -1.64842377e-07 -1.64916119e-07 -1.63762117e-07
 -1.86336763e-11 -2.83344035e-11 -2.66136593e-11 -2.16669750e-11
 -1.06095741e-09 -9.54710154e-10 -8.91219372e-10 -9.42378022e-10
  6.24740118e-11  4.63867613e-11  4.30944270e-11  5.10049544e-11
 -2.35269783e-09 -2.08474724e-09 -1.94502866e-09 -2.07137990e-09
 -8.86772469e-11 -1.81335241e-11 -5.56321565e-10  9.79573482e-10
 -5.51712662e-10  1.00231462e-09 -7.27589685e-10  1.16950437e-09
  6.97314905e+00]
supnorm grad right now is: 6.973149050145137
Weights right now are: 
[ 0.91386776  0.14591799  0.38918143  0.87198624 -1.704087   -0.13145435
  0.18212462  0.54309681 -4.75672345 -4.64646561 -4.50134959 -0.78083653
  3.18104399  6.78440639  3.13740034  2.37303932  4.76572306  4.13220602
  5.14935997  0.98316641  4.04614673  5.65744548  3.52381245  2.34998947
 -1.20195373 -1.91299778 -2.54770125 -1.64670308 -1.39527962 -1.72887192
 -2.5594786  -2.15374084 -2.41909226 -1.9353506  -1.47774851 -1.40921124
  0.17216371 -0.53839117  0.27032594 -0.81998749 -1.98538306 -1.71197856
 -0.7539995  -0.3404761  -0.58556826 -2.10007891 -1.33510905 -1.83434772
 -1.78807075 -0.79967918 -1.75692535 -1.45738378 -0.40646795 -1.29615665
 -1.36695006 -2.16186666 -0.83151846  2.34582789 -1.02002685  0.63420568
 -0.6470047   0.54891937 -0.36980683  0.96528024 30.97789308]
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.3862213592547
gradient value of function right now is: [ 3.74405404e+00  1.40916738e-06  1.47047262e-06  1.26867398e-06
 -3.74405404e+00 -1.40916738e-06 -1.47047262e-06 -1.26867398e-06
 -5.29194337e-06 -5.29356080e-06 -5.29161645e-06 -3.76248917e-06
 -3.96869074e-07 -3.96990389e-07 -3.96844550e-07 -2.82166432e-07
 -3.09737934e-07 -3.09832604e-07 -3.09718799e-07 -2.20218694e-07
 -4.21720432e-07 -4.21849343e-07 -4.21694372e-07 -2.99835282e-07
 -1.53759347e-08 -1.55322481e-08 -1.55494981e-08 -1.53946616e-08
 -8.09441166e-09 -8.17695846e-09 -8.18595702e-09 -8.10438370e-09
 -1.56339459e-08 -1.57931356e-08 -1.58106198e-08 -1.56530981e-08
 -3.78719731e-07 -3.82458205e-07 -3.82913772e-07 -3.79134174e-07
 -6.52230490e-11 -6.63386355e-11 -6.15544337e-11 -5.77468363e-11
 -3.34483702e-09 -2.88041728e-09 -2.67435804e-09 -2.76419120e-09
  1.93470942e-10  1.59425699e-10  1.47752342e-10  1.56716971e-10
 -7.20281922e-09 -6.18723151e-09 -5.74165680e-09 -5.94318073e-09
 -4.03712930e-10  2.24349052e-10 -1.98693702e-09  2.41656061e-09
 -1.93000454e-09  2.32185221e-09 -2.85840996e-09  3.09473340e-09
  1.70350712e+00]
supnorm grad right now is: 3.744054041908011
Weights right now are: 
[ 0.92779596  0.14739228  0.39025766  0.87318648 -1.71801519 -0.13292864
  0.18104838  0.54189657 -4.7551853  -4.64493254 -4.49980963 -0.77750151
  3.18462764  6.78799073  3.14098391  2.37575659  4.7698511   4.13633501
  5.15348786  0.9862311   4.04897815  5.66027732  3.52664384  2.35218457
 -1.20201483 -1.91306118 -2.54776639 -1.64676172 -1.39532212 -1.72891568
 -2.55952331 -2.15378202 -2.41915743 -1.93541809 -1.47781779 -1.4092739
  0.17228649 -0.53832919  0.27035411 -0.81981446 -1.9853856  -1.71197887
 -0.75399959 -0.34047702 -0.58568996 -2.100172   -1.33519465 -1.83443146
 -1.78806427 -0.79967228 -1.7569189  -1.45737847 -0.40670303 -1.2963406
 -1.36711927 -2.16202807 -0.83155089  2.34585548 -1.02011053  0.63424999
 -0.64707873  0.54893707 -0.36994349  0.96533983 30.98926147]
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1034.9821996348867
gradient value of function right now is: [-1.04134064e+00 -3.81179574e-07 -3.96355452e-07 -3.44739952e-07
  1.04134064e+00  3.81179574e-07  3.96355452e-07  3.44739952e-07
  1.46703950e-06  1.46747393e-06  1.46695687e-06  1.04137215e-06
  1.05286289e-07  1.05317444e-07  1.05280367e-07  7.47385475e-08
  8.09165941e-08  8.09405526e-08  8.09120374e-08  5.74385102e-08
  1.12723469e-07  1.12756825e-07  1.12717129e-07  8.00179058e-08
  4.16533425e-09  4.18088488e-09  4.19811156e-09  4.14611910e-09
  2.19418527e-09  2.20194513e-09  2.21118557e-09  2.18381588e-09
  4.22033089e-09  4.23563999e-09  4.25322447e-09  4.20059688e-09
  1.04995762e-07  1.05559437e-07  1.05937188e-07  1.04607036e-07
 -1.68791845e-11 -3.04696260e-11 -2.93705440e-11 -2.24516653e-11
  1.04823890e-09  7.36858324e-10  6.74726499e-10  7.42085705e-10
 -8.37226588e-11 -7.54710109e-11 -7.01795422e-11 -6.79239492e-11
  2.34674974e-09  1.70019583e-09  1.55969682e-09  1.68729268e-09
  5.72320507e-11 -1.81885179e-10  5.04806517e-10 -1.14125054e-09
  5.20930029e-10 -1.02380404e-09  8.42153311e-10 -1.51273063e-09
 -2.27277842e+01]
supnorm grad right now is: 22.727784233693924
Weights right now are: 
[ 0.82268895  0.1449805   0.38732815  0.87073805 -1.61290819 -0.13051686
  0.1839779   0.54434499 -4.73327191 -4.62301344 -4.47789772 -0.75865047
  3.19309059  6.7964562   3.14944631  2.38202473  4.77889819  4.14538482
  5.16253438  0.9928564   4.05612319  5.66742448  3.53378843  2.35753359
 -1.20198537 -1.91302828 -2.54773632 -1.64672762 -1.39532106 -1.72891287
 -2.55952206 -2.15377847 -2.41912773 -1.93538484 -1.47778743 -1.40923944
  0.17361767 -0.536938    0.27168889 -0.81838589 -1.98538035 -1.71197082
 -0.75399204 -0.34047098 -0.58574917 -2.10019175 -1.33521268 -1.83445519
 -1.78805782 -0.79966596 -1.75691291 -1.45737338 -0.40682894 -1.29638731
 -1.36716242 -2.1620807  -0.83153324  2.34589356 -1.02012204  0.63429947
 -0.64708354  0.54897489 -0.36994859  0.96545482 30.77933264]
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1036.1524185885035
gradient value of function right now is: [ 6.10570541e+00  2.21017528e-06  2.29438633e-06  2.00680706e-06
 -6.10570541e+00 -2.21017528e-06 -2.29438633e-06 -2.00680706e-06
 -8.77850864e-06 -8.78120567e-06 -8.77796196e-06 -6.24837112e-06
 -6.62417955e-07 -6.62621484e-07 -6.62376698e-07 -4.71494912e-07
 -5.22517742e-07 -5.22678278e-07 -5.22485202e-07 -3.71917789e-07
 -6.98252639e-07 -6.98467178e-07 -6.98209150e-07 -4.97001280e-07
 -2.57097130e-08 -2.59698768e-08 -2.60079380e-08 -2.57267334e-08
 -1.35466408e-08 -1.36839171e-08 -1.37039243e-08 -1.35556769e-08
 -2.61457795e-08 -2.64105505e-08 -2.64492275e-08 -2.61631588e-08
 -6.31892360e-07 -6.38198431e-07 -6.39152557e-07 -6.32281482e-07
 -1.25973140e-10 -1.14454455e-10 -1.05956734e-10 -1.04674116e-10
 -6.03281481e-09 -5.10922474e-09 -4.74188198e-09 -4.89470898e-09
  3.44223343e-10  2.87854711e-10  2.66850848e-10  2.77584917e-10
 -1.29090259e-08 -1.09357656e-08 -1.01441348e-08 -1.04701043e-08
 -7.34831604e-10  5.29060411e-10 -3.56736934e-09  4.22494276e-09
 -3.45159526e-09  3.97883691e-09 -5.19811609e-09  5.51122367e-09
 -6.35243966e+00]
supnorm grad right now is: 6.352439659184205
Weights right now are: 
[ 1.04922945  0.15750023  0.39991965  0.88191656 -1.83944868 -0.14303659
  0.17138639  0.53316649 -4.77609689 -4.66585259 -4.52072001 -0.78670092
  3.19483685  6.79820288  3.15119248  2.3834626   4.78194672  4.14843419
  5.16558274  0.99517329  4.05674214  5.66804348  3.53440735  2.35818478
 -1.20214575 -1.91318824 -2.54789867 -1.64688405 -1.39541664 -1.72900826
 -2.55961875 -2.15387193 -2.41929095 -1.93554756 -1.4779526  -1.40939861
  0.17060944 -0.53995535  0.26862376 -0.82131149 -1.98537739 -1.71196538
 -0.75398686 -0.340467   -0.58585128 -2.10025814 -1.33527242 -1.83451631
 -1.78804911 -0.79965734 -1.75690496 -1.45736649 -0.40704685 -1.29653641
 -1.3672968  -2.16221413 -0.83154595  2.34592687 -1.02018957  0.63436037
 -0.64715131  0.54901773 -0.37007165  0.96556006 30.87950847]
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1035.4934933550771
gradient value of function right now is: [-1.37655332e+01 -4.90817013e-06 -5.07188909e-06 -4.45899935e-06
  1.37655332e+01  4.90817013e-06  5.07188909e-06  4.45899935e-06
  1.95214218e-05  1.95274940e-05  1.95201842e-05  1.37909190e-05
  1.30546903e-06  1.30587507e-06  1.30538628e-06  9.22251188e-07
  9.73931098e-07  9.74234040e-07  9.73869358e-07  6.88034424e-07
  1.41385669e-06  1.41429643e-06  1.41376707e-06  9.98821872e-07
  5.82905628e-08  5.88822434e-08  5.90030333e-08  5.82743242e-08
  3.10933291e-08  3.14082429e-08  3.14730223e-08  3.10842355e-08
  5.93210702e-08  5.99226610e-08  6.00457911e-08  5.93041917e-08
  1.38421689e-06  1.39850512e-06  1.40127212e-06  1.38397297e-06
  2.73972394e-10  2.02497345e-10  1.84873884e-10  2.02852923e-10
  1.49877135e-08  1.24406509e-08  1.15305575e-08  1.18537529e-08
 -8.55926174e-10 -7.44084189e-10 -6.90818639e-10 -6.92113809e-10
  3.17723573e-08  2.64898269e-08  2.45411916e-08  2.51633243e-08
  1.92655354e-09 -1.70683955e-09  8.96639968e-09 -9.92372642e-09
  8.61628378e-09 -9.04784935e-09  1.33352417e-08 -1.32367905e-08
  8.01707821e+00]
supnorm grad right now is: 13.765533161248888
Weights right now are: 
[ 0.68812912  0.13974453  0.38114389  0.86567825 -1.47834835 -0.12528089
  0.19016216  0.54940479 -4.70012486 -4.5898623  -4.4447513  -0.73022362
  3.20303009  6.80639834  3.15938529  2.38942468  4.78928881  4.15577831
  5.17292443  1.00049701  4.06475781  5.6760613   3.54242261  2.36403108
 -1.20196527 -1.91300639 -2.54771813 -1.6467004  -1.39532879 -1.72891972
 -2.55953094 -2.15378238 -2.41910876 -1.93536394 -1.47777032 -1.40921316
  0.1767139  -0.53382279  0.27473346 -0.81513917 -1.98537834 -1.71196397
 -0.75398552 -0.34046651 -0.58589367 -2.10028548 -1.33529565 -1.83453321
 -1.78804675 -0.79965402 -1.75690207 -1.45736477 -0.40712465 -1.29659065
 -1.36734237 -2.16224253 -0.83156689  2.34595201 -1.02024723  0.63432849
 -0.6472074   0.54896995 -0.37018363  0.96554498 30.99452349]
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1036.0350654031154
gradient value of function right now is: [-8.37967340e+00 -2.94166470e-06 -3.03391349e-06 -2.68158059e-06
  8.37967340e+00  2.94166470e-06  3.03391349e-06  2.68158059e-06
  1.18433522e-05  1.18470616e-05  1.18425918e-05  8.37441074e-06
  8.13167538e-07  8.13422193e-07  8.13115343e-07  5.74991188e-07
  6.12459303e-07  6.12651123e-07  6.12419984e-07  4.33069033e-07
  8.74667523e-07  8.74941437e-07  8.74611380e-07  6.18477857e-07
  3.55752582e-08  3.59498517e-08  3.60269340e-08  3.55600747e-08
  1.89524238e-08  1.91514744e-08  1.91927552e-08  1.89440670e-08
  3.62150963e-08  3.65959983e-08  3.66746030e-08  3.61994044e-08
  8.35452881e-07  8.44434757e-07  8.46181407e-07  8.35185321e-07
  2.25487644e-10  1.69915306e-10  1.55790284e-10  1.68703376e-10
  9.55459012e-09  7.94971838e-09  7.36380882e-09  7.52403808e-09
 -5.13173411e-10 -4.49622575e-10 -4.17035331e-10 -4.13853103e-10
  2.01319234e-08  1.68344996e-08  1.55856690e-08  1.58757182e-08
  1.31775953e-09 -1.15905106e-09  5.89258809e-09 -5.90848484e-09
  5.64163448e-09 -5.32377617e-09  8.79601845e-09 -7.99692026e-09
  1.61083212e+01]
supnorm grad right now is: 16.108321202738132
Weights right now are: 
[ 0.769912    0.14609421  0.3872247   0.87124909 -1.56013123 -0.13163057
  0.18408135  0.54383396 -4.72148245 -4.61122944 -4.46610667 -0.74250563
  3.20671994  6.81008906  3.16307499  2.39227291  4.79387836  4.16036907
  5.17751376  1.00392943  4.0674507   5.67875474  3.54511541  2.36618794
 -1.20208187 -1.91312205 -2.54783805 -1.64681202 -1.39540316 -1.72899362
 -2.55960715 -2.15385406 -2.41922899 -1.93548315 -1.47789389 -1.40932829
  0.17525873 -0.53527737  0.27319121 -0.81649116 -1.98537735 -1.71195872
 -0.7539806  -0.34046367 -0.58604407 -2.10036759 -1.3353755  -1.83462749
 -1.78803738 -0.79964516 -1.75689354 -1.45735728 -0.40741862 -1.29675327
 -1.36750081 -2.16242537 -0.83156394  2.34600184 -1.02023826  0.6344949
 -0.64719632  0.54909432 -0.37019106  0.96575359 31.05923846]
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.0763410457969
gradient value of function right now is: [-3.70293903e+00 -1.28904949e-06 -1.32730171e-06 -1.17772455e-06
  3.70293903e+00  1.28904949e-06  1.32730171e-06  1.17772455e-06
  5.16551302e-06  5.16714100e-06  5.16517838e-06  3.65491550e-06
  3.69311280e-07  3.69427646e-07  3.69287365e-07  2.61312379e-07
  2.80567827e-07  2.80656248e-07  2.80549652e-07  1.98519130e-07
  3.94661559e-07  3.94785912e-07  3.94636002e-07  2.79249384e-07
  1.55607613e-08  1.57239132e-08  1.57635186e-08  1.55437873e-08
  8.26374768e-09  8.34999433e-09  8.37116917e-09  8.25455432e-09
  1.58408182e-08  1.60065504e-08  1.60469633e-08  1.58233653e-08
  3.57428605e-07  3.61326525e-07  3.62194503e-07  3.57098745e-07
  1.34085726e-10  9.64666393e-11  8.82700054e-11  9.74634580e-11
  4.41724948e-09  3.63426381e-09  3.36192934e-09  3.42595091e-09
 -2.15478926e-10 -1.92887103e-10 -1.78890353e-10 -1.73853898e-10
  9.26062659e-09  7.67198150e-09  7.09357627e-09  7.19664392e-09
  6.33618993e-10 -6.10793002e-10  2.77307727e-09 -2.64235665e-09
  2.64840541e-09 -2.33373000e-09  4.17252715e-09 -3.64508545e-09
  1.09514252e+01]
supnorm grad right now is: 10.95142518951885
Weights right now are: 
[ 0.81608533  0.15072212  0.39156244  0.87534352 -1.60630457 -0.13625848
  0.17974361  0.53973953 -4.73685969 -4.626616   -4.48148145 -0.75123113
  3.20890537  6.8122748   3.1652604   2.39400229  4.79669741  4.16318869
  5.18033272  1.00606687  4.06900326  5.68030738  3.546668    2.36748001
 -1.20218854 -1.91322912 -2.54794893 -1.64691411 -1.39546925 -1.72905991
 -2.55967552 -2.15391767 -2.41933969 -1.93559419 -1.47800884 -1.40943429
  0.17436268 -0.53620025  0.2721919  -0.81729385 -1.98538009 -1.71195677
 -0.75397869 -0.34046373 -0.58620325 -2.1004675  -1.33547008 -1.83473101
 -1.78802937 -0.79963698 -1.75688569 -1.45735068 -0.40772783 -1.29694855
 -1.36768632 -2.16262503 -0.83157151  2.34605238 -1.02027898  0.63462233
 -0.64722251  0.54918568 -0.3702457   0.9659273  31.06422403]
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1034.2566725590686
gradient value of function right now is: [ 2.78749137e+00  9.62368286e-07  9.90251473e-07  8.81336174e-07
 -2.78749137e+00 -9.62368286e-07 -9.90251473e-07 -8.81336174e-07
 -3.99500008e-06 -3.99624046e-06 -3.99474511e-06 -2.83309303e-06
 -2.90490900e-07 -2.90581107e-07 -2.90472354e-07 -2.06003245e-07
 -2.24161761e-07 -2.24231361e-07 -2.24147453e-07 -1.58966334e-07
 -3.07512144e-07 -3.07607636e-07 -3.07492511e-07 -2.18073962e-07
 -1.18647533e-08 -1.20000775e-08 -1.20138635e-08 -1.18817894e-08
 -6.28563991e-09 -6.35755256e-09 -6.36478936e-09 -6.29476059e-09
 -1.20790700e-08 -1.22170522e-08 -1.22310466e-08 -1.20965057e-08
 -2.82342916e-07 -2.85470559e-07 -2.85821408e-07 -2.82710866e-07
 -8.62893815e-11 -8.29471275e-11 -7.72643345e-11 -7.43885412e-11
 -2.80150479e-09 -2.43622068e-09 -2.26607255e-09 -2.31288727e-09
  1.37944221e-10  1.13992148e-10  1.05711997e-10  1.11199966e-10
 -5.89852921e-09 -5.12130393e-09 -4.76037231e-09 -4.86425225e-09
 -3.87771327e-10  2.25910253e-10 -1.71080829e-09  1.71296813e-09
 -1.62960899e-09  1.61877601e-09 -2.44156797e-09  2.22596088e-09
 -2.42396186e+01]
supnorm grad right now is: 24.239618607113837
Weights right now are: 
[ 0.93148754  0.15745402  0.39808341  0.88138442 -1.72170678 -0.14299039
  0.17322263  0.53369863 -4.75550779 -4.64527414 -4.50012697 -0.76226475
  3.21037568  6.81374522  3.16673073  2.39521994  4.79893244  4.16542415
  5.1825677   1.00778624  4.06982421  5.68112823  3.54748902  2.36824818
 -1.20229538 -1.91333643 -2.54805981 -1.64701691 -1.39553385 -1.72912476
 -2.55974231 -2.1539801  -2.41945035 -1.93570527 -1.47812358 -1.4095408
  0.17384719 -0.536739    0.27158299 -0.81772944 -1.98538432 -1.71195618
 -0.75397812 -0.34046499 -0.58634614 -2.10055531 -1.33555413 -1.83482448
 -1.78802355 -0.79963063 -1.75687961 -1.45734566 -0.40800026 -1.29711713
 -1.36784828 -2.16280203 -0.83158097  2.34609767 -1.0202995   0.63474846
 -0.64723303  0.54927504 -0.37028003  0.96608472 30.6845059 ]
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1036.3374204055497
gradient value of function right now is: [ 7.01460834e+00  2.38240152e-06  2.44721847e-06  2.18975911e-06
 -7.01460834e+00 -2.38240152e-06 -2.44721847e-06 -2.18975911e-06
 -9.80318137e-06 -9.80618179e-06 -9.80257436e-06 -6.96440431e-06
 -7.55829331e-07 -7.56060680e-07 -7.55782525e-07 -5.36957108e-07
 -5.90026180e-07 -5.90206769e-07 -5.89989645e-07 -4.19167932e-07
 -7.94154933e-07 -7.94398013e-07 -7.94105753e-07 -5.64184433e-07
 -2.85493543e-08 -2.88351491e-08 -2.88765777e-08 -2.85697768e-08
 -1.50055943e-08 -1.51560156e-08 -1.51777367e-08 -1.50164023e-08
 -2.90290893e-08 -2.93198940e-08 -2.93619840e-08 -2.90499312e-08
 -6.72751616e-07 -6.79395284e-07 -6.80392259e-07 -6.73202090e-07
 -2.14329049e-10 -1.90065778e-10 -1.75944520e-10 -1.76606457e-10
 -6.39849387e-09 -5.41067198e-09 -5.02067072e-09 -5.19157674e-09
  3.00405688e-10  2.49386185e-10  2.31227051e-10  2.41814247e-10
 -1.36472280e-08 -1.15431120e-08 -1.07057336e-08 -1.10690878e-08
 -7.94558711e-10  5.68771672e-10 -3.78035127e-09  4.58699390e-09
 -3.66492338e-09  4.32263339e-09 -5.49050435e-09  5.94114602e-09
  5.94604110e+00]
supnorm grad right now is: 7.014608336316332
Weights right now are: 
[ 1.00956367  0.16475957  0.40504502  0.8878933  -1.79978291 -0.15029593
  0.16626102  0.52718975 -4.77894422 -4.6687237  -4.52355993 -0.7756677
  3.21419462  6.81756484  3.17054958  2.39819018  4.80372093  4.17021375
  5.18735601  1.01137916  4.07261782  5.68392218  3.55028262  2.37051008
 -1.20244673 -1.91349106 -2.54821659 -1.64716483 -1.39562908 -1.72922181
 -2.55984054 -2.15407348 -2.41960729 -1.93586551 -1.47828602 -1.40969421
  0.17282289 -0.53784282  0.27044156 -0.81868611 -1.98538966 -1.71195795
 -0.75397947 -0.34046749 -0.5864989  -2.10067736 -1.33566427 -1.83492873
 -1.78801716 -0.79962291 -1.75687243 -1.45734019 -0.40830376 -1.29736508
 -1.36807213 -2.16300924 -0.83162474  2.34613551 -1.02044182  0.63478333
 -0.6473603   0.54928399 -0.37050146  0.96614684 31.01378046]
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1036.9240953496371
gradient value of function right now is: [-6.23211811e+00 -2.07905538e-06 -2.12808577e-06 -1.91287343e-06
  6.23211811e+00  2.07905538e-06  2.12808577e-06  1.91287343e-06
  8.72026103e-06  8.72295239e-06  8.71971664e-06  6.15801835e-06
  6.04232415e-07  6.04418873e-07  6.04194704e-07  4.26694642e-07
  4.53490242e-07  4.53630200e-07  4.53461933e-07  3.20243090e-07
  6.46386832e-07  6.46586299e-07  6.46346491e-07  4.56463093e-07
  2.58375740e-08  2.60814525e-08  2.61406700e-08  2.58195706e-08
  1.37378236e-08  1.38669932e-08  1.38986998e-08  1.37279609e-08
  2.62801725e-08  2.65277824e-08  2.65881611e-08  2.62615935e-08
  6.00287303e-07  6.06132249e-07  6.07441429e-07  5.99968547e-07
  1.53937179e-10  1.06811963e-10  9.72935429e-11  1.10141907e-10
  6.47738180e-09  5.27966365e-09  4.88935123e-09  5.06267586e-09
 -3.37507635e-10 -2.96444964e-10 -2.75402450e-10 -2.73846538e-10
  1.37554978e-08  1.12800324e-08  1.04426285e-08  1.07746977e-08
  7.84261094e-10 -8.04104631e-10  3.77495452e-09 -4.60556951e-09
  3.65100575e-09 -4.18852663e-09  5.65189967e-09 -6.13508847e-09
 -5.67437093e+00]
supnorm grad right now is: 6.2321181095966
Weights right now are: 
[ 0.76881587  0.15270103  0.39233209  0.87666895 -1.55903511 -0.13823739
  0.17897396  0.53841409 -4.72657212 -4.61633965 -4.47119004 -0.73572619
  3.22185096  6.82522328  3.17820552  2.40379922  4.81080615  4.17730094
  5.19444084  1.01654087  4.07992465  5.69123098  3.55758907  2.37588236
 -1.20233309 -1.91337576 -2.54810313 -1.64704751 -1.39557846 -1.72917034
 -2.55979009 -2.15402088 -2.41949263 -1.93574912 -1.47817153 -1.40957579
  0.17704899 -0.53358897  0.27466343 -0.81438611 -1.98539013 -1.71195523
 -0.75397684 -0.3404661  -0.58655396 -2.10070913 -1.3356922  -1.83495397
 -1.78801412 -0.79961847 -1.75686837 -1.45733754 -0.40841004 -1.29743142
 -1.36813019 -2.16305714 -0.83164103  2.34616405 -1.02048735  0.63478744
 -0.6474074   0.54927004 -0.37059946  0.96617307 30.94112121]
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1036.0175142776263
gradient value of function right now is: [ 6.59324879e+00  2.19431456e-06  2.24460210e-06  2.02490509e-06
 -6.59324879e+00 -2.19431456e-06 -2.24460210e-06 -2.02490509e-06
 -9.24960890e-06 -9.25244286e-06 -9.24903471e-06 -6.56215086e-06
 -7.13091010e-07 -7.13309506e-07 -7.13046739e-07 -5.05902393e-07
 -5.55625837e-07 -5.55796076e-07 -5.55591345e-07 -3.94189546e-07
 -7.47828305e-07 -7.48057444e-07 -7.47781877e-07 -5.30546776e-07
 -2.70059052e-08 -2.72788923e-08 -2.73185240e-08 -2.70251515e-08
 -1.42023739e-08 -1.43461214e-08 -1.43669192e-08 -1.42125586e-08
 -2.74638840e-08 -2.77416861e-08 -2.77819620e-08 -2.74835224e-08
 -6.33353708e-07 -6.39674797e-07 -6.40621348e-07 -6.33778698e-07
 -2.00933684e-10 -1.77656824e-10 -1.64490278e-10 -1.65187169e-10
 -6.09601168e-09 -5.15714530e-09 -4.78542671e-09 -4.94114844e-09
  2.86234896e-10  2.38325644e-10  2.20949014e-10  2.30399403e-10
 -1.29729450e-08 -1.09797445e-08 -1.01830501e-08 -1.05123829e-08
 -7.66182280e-10  5.54897813e-10 -3.61893879e-09  4.29219233e-09
 -3.50302130e-09  4.03731850e-09 -5.25918760e-09  5.57848958e-09
 -4.46208427e+00]
supnorm grad right now is: 6.593248787337324
Weights right now are: 
[ 1.0705786   0.17039932  0.41008781  0.89285532 -1.86079783 -0.15593568
  0.16121823  0.52222773 -4.79154388 -4.68133419 -4.53615694 -0.77998642
  3.22005484  6.82342633  3.17640959  2.40269798  4.81053761  4.1770321
  5.19417238  1.0164784   4.07729778  5.688603    3.55496245  2.37420499
 -1.20257401 -1.91361944 -2.5483485  -1.64728493 -1.39571399 -1.72930733
 -2.55992802 -2.15415451 -2.41973857 -1.9359978  -1.47842194 -1.40981812
  0.17345595 -0.53725528  0.27097077 -0.8179104  -1.98539477 -1.71195635
 -0.75397743 -0.34046805 -0.58668792 -2.10082105 -1.33579044 -1.83504294
 -1.78800786 -0.79961024 -1.75686075 -1.45733217 -0.40869213 -1.29767484
 -1.36834442 -2.163247   -0.83168834  2.34619671 -1.02065205  0.63480868
 -0.64756799  0.54927558 -0.37087879  0.96621567 30.90147345]
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1034.6868225712744
gradient value of function right now is: [ 8.96200056e+00  2.96871639e-06  3.02940601e-06  2.74346404e-06
 -8.96200056e+00 -2.96871639e-06 -3.02940601e-06 -2.74346404e-06
 -1.31258848e-05 -1.31299032e-05 -1.31250716e-05 -9.30206581e-06
 -9.81081228e-07 -9.81381594e-07 -9.81020446e-07 -6.95272407e-07
 -7.65007543e-07 -7.65241747e-07 -7.64960151e-07 -5.42146191e-07
 -1.02669241e-06 -1.02700674e-06 -1.02662880e-06 -7.27596137e-07
 -3.85909598e-08 -3.89761190e-08 -3.90348953e-08 -3.86134956e-08
 -2.03839373e-08 -2.05875707e-08 -2.06185719e-08 -2.03959003e-08
 -3.92522131e-08 -3.96441635e-08 -3.97039184e-08 -3.92751983e-08
 -9.36590215e-07 -9.45849734e-07 -9.47295220e-07 -9.37110666e-07
 -1.51120617e-10 -1.35823436e-10 -1.25351626e-10 -1.24287078e-10
 -8.95676778e-09 -7.54782829e-09 -6.99987016e-09 -7.23236156e-09
  5.21441669e-10  4.36848377e-10  4.04824503e-10  4.19810497e-10
 -1.91483723e-08 -1.61502543e-08 -1.49705053e-08 -1.54603280e-08
 -1.08249293e-09  8.10992064e-10 -5.29994894e-09  6.32303958e-09
 -5.13791346e-09  5.93575102e-09 -7.76472733e-09  8.28082084e-09
  7.85162917e+00]
supnorm grad right now is: 8.962000560649743
Weights right now are: 
[ 1.15936646  0.17350542  0.41294306  0.895643   -1.9495857  -0.15904179
  0.15836298  0.51944005 -4.79650901 -4.68630418 -4.5411207  -0.78151336
  3.22177191  6.82514368  3.17812662  2.40405414  4.81267491  4.17916987
  5.1963096   1.01809732  4.07859036  5.68989572  3.55625502  2.37526989
 -1.20262189 -1.91366724 -2.54839891 -1.64732979 -1.39574496 -1.72933824
 -2.55996036 -2.15418385 -2.4197885  -1.93604759 -1.47847441 -1.40986494
  0.17426376 -0.53645221  0.27172552 -0.8170459  -1.98539907 -1.71195668
 -0.75397767 -0.3404697  -0.58677992 -2.10087416 -1.33584176 -1.83510127
 -1.78800551 -0.79960687 -1.75685746 -1.45732976 -0.40886278 -1.29777405
 -1.3684408  -2.1633542  -0.83169659  2.34622781 -1.02065578  0.63489663
 -0.64756741  0.54933561 -0.37089341  0.96632224 30.95491396]
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.4834520102738
gradient value of function right now is: [ 2.84990048e+00  9.34302308e-07  9.51324270e-07  8.63888469e-07
 -2.84990048e+00 -9.34302308e-07 -9.51324270e-07 -8.63888469e-07
 -4.12846232e-06 -4.12972097e-06 -4.12820744e-06 -2.91483185e-06
 -2.94540250e-07 -2.94630063e-07 -2.94522059e-07 -2.07953986e-07
 -2.25039029e-07 -2.25107640e-07 -2.25025135e-07 -1.58884874e-07
 -3.11013574e-07 -3.11108410e-07 -3.10994365e-07 -2.19584639e-07
 -1.20808187e-08 -1.22075136e-08 -1.22183246e-08 -1.21011025e-08
 -6.40319270e-09 -6.47062058e-09 -6.47625992e-09 -6.41407038e-09
 -1.22884738e-08 -1.24176128e-08 -1.24285503e-08 -1.23092268e-08
 -2.87314538e-07 -2.90214416e-07 -2.90503362e-07 -2.87747274e-07
 -5.63308402e-11 -5.95001870e-11 -5.53656993e-11 -5.11161703e-11
 -2.48541316e-09 -2.16798188e-09 -2.01407594e-09 -2.07666911e-09
  1.32277260e-10  1.07295595e-10  9.93967805e-11  1.07007911e-10
 -5.32291880e-09 -4.62440618e-09 -4.29369444e-09 -4.43762448e-09
 -3.11319561e-10  1.45894516e-10 -1.49321167e-09  1.75236974e-09
 -1.44715383e-09  1.69965210e-09 -2.12438916e-09  2.22680530e-09
 -2.50925505e+00]
supnorm grad right now is: 2.8499004848705867
Weights right now are: 
[ 0.86209383  0.16409242  0.40288872  0.88677531 -1.65231306 -0.14962879
  0.16841733  0.52830773 -4.74083874 -4.63062384 -4.48545141 -0.73862636
  3.2297909   6.83316465  3.18614529  2.40996695  4.82028652  4.18678345
  5.20392087  1.02366189  4.0861008   5.69740795  3.56376518  2.38083857
 -1.20252235 -1.91357008 -2.54830245 -1.64722888 -1.39570332 -1.72929793
 -2.55992045 -2.15414146 -2.41969043 -1.93595194 -1.47837949 -1.40976547
  0.18041289 -0.53033457  0.27783699 -0.81086963 -1.98540878 -1.71196379
 -0.75398404 -0.34047672 -0.58684216 -2.10092911 -1.33589136 -1.8351428
 -1.7880086  -0.79960778 -1.75685827 -1.45733197 -0.40896424 -1.2978697
 -1.36852678 -2.16341973 -0.83173516  2.34624504 -1.02073119  0.63486655
 -0.6476336   0.54928668 -0.37101793  0.96628184 30.84502475]
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.519380703049
gradient value of function right now is: [ 2.69360054e+00  8.78599406e-07  8.93363251e-07  8.13238530e-07
 -2.69360054e+00 -8.78599406e-07 -8.93363251e-07 -8.13238530e-07
 -3.87652815e-06 -3.87770809e-06 -3.87628939e-06 -2.73510095e-06
 -2.79222361e-07 -2.79307367e-07 -2.79205156e-07 -1.97005305e-07
 -2.13293004e-07 -2.13357929e-07 -2.13279866e-07 -1.50489615e-07
 -2.94581363e-07 -2.94671045e-07 -2.94563212e-07 -2.07841854e-07
 -1.13075523e-08 -1.14257917e-08 -1.14354320e-08 -1.13276301e-08
 -5.98585937e-09 -6.04872881e-09 -6.05373935e-09 -5.99661748e-09
 -1.15007536e-08 -1.16212814e-08 -1.16310256e-08 -1.15212974e-08
 -2.66612450e-07 -2.69287581e-07 -2.69547154e-07 -2.67035402e-07
 -5.73948711e-11 -6.05445947e-11 -5.63824486e-11 -5.22388216e-11
 -2.27528333e-09 -1.98645541e-09 -1.84662541e-09 -1.90767082e-09
  1.16201043e-10  9.31237671e-11  8.63014779e-11  9.38205302e-11
 -4.87260482e-09 -4.23443980e-09 -3.93413591e-09 -4.07513145e-09
 -2.79477369e-10  1.26106653e-10 -1.34838605e-09  1.63398933e-09
 -1.30717076e-09  1.58950090e-09 -1.90608723e-09  2.06478999e-09
 -1.74568876e+00]
supnorm grad right now is: 2.69360054490315
Weights right now are: 
[ 0.71969957  0.1593679   0.3978047   0.8822795  -1.5099188  -0.14490426
  0.17350135  0.53280355 -4.72148678 -4.61127013 -4.4660995  -0.72273337
  3.23459178  6.83796669  3.19094596  2.41351986  4.82489165  4.19138975
  5.20852578  1.02703712  4.09055976  5.70186795  3.56822396  2.38415951
 -1.20251343 -1.91356284 -2.54829502 -1.6472177  -1.39570779 -1.72930331
 -2.55992578 -2.1541447  -2.41968233 -1.93594549 -1.47837287 -1.40975504
  0.18229995 -0.52849363  0.27969306 -0.80894233 -1.98541106 -1.7119639
 -0.7539837  -0.34047708 -0.5868933  -2.10099172 -1.33594115 -1.83517252
 -1.78800551 -0.79960161 -1.7568528  -1.45732887 -0.40907844 -1.29801393
 -1.3686432  -2.16349009 -0.8317939   2.34625742 -1.02090854  0.63477526
 -0.6478106   0.54919247 -0.37132695  0.96618556 31.02200723]
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.515251673416
gradient value of function right now is: [ 2.58934077e+00  8.41066893e-07  8.54189061e-07  7.79172865e-07
 -2.58934077e+00 -8.41066893e-07 -8.54189061e-07 -7.79172865e-07
 -3.71138920e-06 -3.71251942e-06 -3.71116031e-06 -2.61695166e-06
 -2.69006993e-07 -2.69088930e-07 -2.68990396e-07 -1.89679312e-07
 -2.05439527e-07 -2.05502092e-07 -2.05426856e-07 -1.44858068e-07
 -2.83618309e-07 -2.83704696e-07 -2.83600810e-07 -1.99981892e-07
 -1.08197456e-08 -1.09341111e-08 -1.09426419e-08 -1.08398524e-08
 -5.72399140e-09 -5.78477185e-09 -5.78919176e-09 -5.73475833e-09
 -1.10047635e-08 -1.11213528e-08 -1.11299684e-08 -1.10253371e-08
 -2.53140447e-07 -2.55704034e-07 -2.55935829e-07 -2.53560664e-07
 -5.97934270e-11 -6.28560550e-11 -5.85121643e-11 -5.43348764e-11
 -2.15888852e-09 -1.89500511e-09 -1.76075627e-09 -1.81439062e-09
  1.06418995e-10  8.50454245e-11  7.87519789e-11  8.58283076e-11
 -4.61973292e-09 -4.03526511e-09 -3.74722640e-09 -3.87236397e-09
 -2.76156150e-10  1.15072192e-10 -1.30604977e-09  1.52111125e-09
 -1.26619184e-09  1.48333601e-09 -1.84816857e-09  1.92078341e-09
 -2.60590049e+00]
supnorm grad right now is: 2.605900492096615
Weights right now are: 
[ 1.09069387  0.17891924  0.41728519  0.90032328 -1.8809131  -0.1644556
  0.15402086  0.51475977 -4.7967333  -4.68654366 -4.54133996 -0.77409458
  3.23136934  6.83474288  3.18772384  2.41142195  4.82332914  4.18982647
  5.20696346  1.02606685  4.08660563  5.69791221  3.5642702   2.38155592
 -1.20278946 -1.91384306 -2.54857701 -1.64749135 -1.39586116 -1.72945895
 -2.56008236 -2.1542968  -2.41996471 -1.93623213 -1.4786613  -1.41003498
  0.17855518 -0.53232036  0.27583116 -0.81263591 -1.98541925 -1.71196998
 -0.75398916 -0.3404829  -0.5870377  -2.1011066  -1.33604604 -1.83527454
 -1.78800208 -0.79959841 -1.75684983 -1.45732643 -0.40936749 -1.29824648
 -1.3688555  -2.16369385 -0.83182695  2.34628935 -1.02102504  0.63481832
 -0.64791682  0.54921628 -0.37150323  0.96626772 30.80184447]
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.566520101201
gradient value of function right now is: [ 2.02193807e+00  6.53827144e-07  6.63394131e-07  6.06181728e-07
 -2.02193807e+00 -6.53827144e-07 -6.63394131e-07 -6.06181728e-07
 -2.88266567e-06 -2.88354144e-06 -2.88248823e-06 -2.03151976e-06
 -2.09842991e-07 -2.09906759e-07 -2.09830067e-07 -1.47882824e-07
 -1.60073106e-07 -1.60121740e-07 -1.60063252e-07 -1.12809192e-07
 -2.21203651e-07 -2.21270872e-07 -2.21190028e-07 -1.55889037e-07
 -8.37251233e-09 -8.46282978e-09 -8.46715633e-09 -8.39227108e-09
 -4.42565842e-09 -4.47368630e-09 -4.47587388e-09 -4.43624169e-09
 -8.51565705e-09 -8.60779340e-09 -8.61212988e-09 -8.53588196e-09
 -1.94676737e-07 -1.96662643e-07 -1.96797203e-07 -1.95082920e-07
 -5.02438345e-11 -5.52537338e-11 -5.15758520e-11 -4.71265789e-11
 -1.58156448e-09 -1.41098977e-09 -1.31286929e-09 -1.35311032e-09
  7.31810974e-11  5.64393485e-11  5.22479968e-11  5.88325389e-11
 -3.38559446e-09 -2.99787292e-09 -2.78764123e-09 -2.88549123e-09
 -2.03259454e-10  5.69080721e-11 -9.52189078e-10  1.12303906e-09
 -9.22850901e-10  1.11238982e-09 -1.32370124e-09  1.39257216e-09
 -3.04081255e+00]
supnorm grad right now is: 3.040812545433331
Weights right now are: 
[ 0.89676021  0.17131613  0.40929596  0.89316674 -1.68697945 -0.1568525
  0.16201009  0.52191631 -4.75816512 -4.6479683  -4.50277257 -0.74460111
  3.2368592   6.8402341   3.19321347  2.4154463   4.82836555  4.19486418
  5.21199964  1.02973226  4.09184904  5.70315689  3.5695134   2.38541623
 -1.20271993 -1.91377296 -2.54850963 -1.64741889 -1.39583176 -1.72942926
 -2.56005415 -2.15426584 -2.4198961  -1.93616291 -1.47859486 -1.40996338
  0.18223746 -0.52862498  0.27947686 -0.80889729 -1.98542475 -1.71197152
 -0.75399053 -0.3404859  -0.58711921 -2.10114718 -1.33608712 -1.83532466
 -1.78800197 -0.79959712 -1.75684834 -1.45732605 -0.40950754 -1.29831163
 -1.3689232  -2.16377616 -0.83182919  2.34631996 -1.02099956  0.63491535
 -0.6478858   0.54928121 -0.37147248  0.96637149 30.98778091]
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.5437908952883
gradient value of function right now is: [ 2.18098096e+00  7.02952747e-07  7.12464962e-07  6.52200876e-07
 -2.18098096e+00 -7.02952747e-07 -7.12464962e-07 -6.52200876e-07
 -3.10431519e-06 -3.10525923e-06 -3.10412387e-06 -2.18642212e-06
 -2.27026192e-07 -2.27095249e-07 -2.27012193e-07 -1.59897211e-07
 -1.73167822e-07 -1.73220486e-07 -1.73157148e-07 -1.21964877e-07
 -2.39152165e-07 -2.39224910e-07 -2.39137418e-07 -1.68437682e-07
 -9.02123465e-09 -9.11811416e-09 -9.12364758e-09 -9.04103773e-09
 -4.76688447e-09 -4.81835846e-09 -4.82118551e-09 -4.77748374e-09
 -9.17539034e-09 -9.27419673e-09 -9.27976191e-09 -9.19565785e-09
 -2.08561999e-07 -2.10689502e-07 -2.10850416e-07 -2.08968179e-07
 -5.58060640e-11 -5.98453976e-11 -5.58085760e-11 -5.15385147e-11
 -1.73080095e-09 -1.53496238e-09 -1.42769461e-09 -1.47127100e-09
  7.91487545e-11  6.15017796e-11  5.69416018e-11  6.35672183e-11
 -3.69973784e-09 -3.25972945e-09 -3.03006834e-09 -3.13439677e-09
 -2.23310931e-10  7.39917721e-11 -1.04263660e-09  1.22360662e-09
 -1.00992384e-09  1.20435903e-09 -1.45715359e-09  1.52601520e-09
 -3.31609790e+00]
supnorm grad right now is: 3.316097903665307
Weights right now are: 
[ 0.79121195  0.16751752  0.4052943   0.88955243 -1.58143119 -0.15305388
  0.16601174  0.52553062 -4.73771416 -4.62751263 -4.48232257 -0.72858425
  3.24054504  6.84392097  3.1968991   2.41815575  4.83183592  4.19833554
  5.21546981  1.03226179  4.09530185  5.70661064  3.57296602  2.3879681
 -1.20267661 -1.91372766 -2.54846631 -1.64737251 -1.39581376 -1.72941021
 -2.56003617 -2.1542462  -2.41985197 -1.93611673 -1.4785507  -1.40991612
  0.18440621 -0.52641828  0.2816463  -0.80666819 -1.98542553 -1.7119698
 -0.75398884 -0.34048523 -0.58716269 -2.10116258 -1.33610183 -1.83534444
 -1.78800048 -0.79959522 -1.75684647 -1.45732477 -0.40959137 -1.29834133
 -1.36895191 -2.16381318 -0.83182578  2.34634368 -1.02099776  0.63497014
 -0.64787963  0.54931938 -0.37146987  0.96644911 30.90477468]
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1052.5807648908083
W_T_median: 1051.1097794420066
W_T_pctile_5: 958.6687214501627
W_T_CVAR_5_pct: 932.3280240299817
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.1
F value: -1037.5437908952883
-----------------------------------------------
