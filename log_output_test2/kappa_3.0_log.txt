Starting at: 
2022-06-20 18:03:38
tracing parameter entered from terminal:  3.0


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = basic_T30_VWD
timeseries_basket['basket_desc'] = CRSP data: T30 and VWD
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
5.0% of MC simulations done.
10.0% of MC simulations done.
15.0% of MC simulations done.
20.0% of MC simulations done.
25.0% of MC simulations done.
30.0% of MC simulations done.
35.0% of MC simulations done.
40.0% of MC simulations done.
45.0% of MC simulations done.
50.0% of MC simulations done.
55.00000000000001% of MC simulations done.
60.0% of MC simulations done.
65.0% of MC simulations done.
70.0% of MC simulations done.
75.0% of MC simulations done.
80.0% of MC simulations done.
85.0% of MC simulations done.
90.0% of MC simulations done.
95.0% of MC simulations done.
100.0% of MC simulations done.
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  hidden_layer    None       None         None   
4        obj.layers[4]        4  hidden_layer    None       None         None   
5        obj.layers[5]        5  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  
4    False        None  
5    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  hidden_layer       4  logistic_sigmoid   
4        obj.layers[4]        4  hidden_layer       4  logistic_sigmoid   
5        obj.layers[5]        5  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 4)    False        None  
4       (4, 4)    False        None  
5       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
[ 0.10347998  0.18844875 -0.3666709   0.62803316 -0.89369922 -0.17398511
  0.93797695  0.78704989 -0.60791207 -0.16293226 -0.31685198  0.28094308
  0.0404241   0.17453202  0.0940472   0.01169373  0.2300954  -0.23784803
  0.78511084 -0.41859022  0.4809118  -0.01155852  0.07219357 -0.14482284
  0.4701344  -0.18827011 -0.8138637  -0.24629474  0.28361272  0.33354483
 -0.63777411 -0.25343004 -0.81551088 -0.26234589  0.20198453 -0.15135741
  0.25264285 -0.29817275  0.41496663 -0.58895503 -0.60305385 -0.63454123
  0.3979274   0.74701442  0.57452389 -0.67377327  0.6493153   0.07340044
 -0.29694484  0.52758424 -0.14347696  0.04319718 -0.28527617 -0.63077443
  0.13133703 -0.58713125  0.12759166  0.87507529 -0.19770888 -0.66759109
  0.25529887 -0.48501395  0.58877641 -0.72315372] (64,)
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1260.6521530687958
W_T_median: 1222.3920506735662
W_T_pctile_5: 834.1581876304141
W_T_CVAR_5_pct: 746.6575474940535
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.438393176161
gradient value of function right now is: [ 6.61476192e-03  5.20572472e-02  6.91045662e-02  6.92365718e-02
 -6.61476192e-03 -5.20572472e-02 -6.91045662e-02 -6.92365718e-02
  1.48818828e-03  8.78280479e-03  6.21159258e-04  8.76863878e-03
  5.31128997e-03  3.13369613e-02  2.21760796e-03  3.12861951e-02
  1.07683843e-03  6.35528034e-03  4.49472849e-04  6.34503181e-03
  7.32689224e-04  4.32530328e-03  3.05722592e-04  4.31835800e-03
 -5.39106744e-05 -4.45811357e-04 -3.02918658e-04 -5.27041668e-04
  3.87210997e-05  3.11480031e-04  2.13299856e-04  3.70926886e-04
 -9.76295827e-05 -7.95524000e-04 -5.42576646e-04 -9.43879848e-04
  4.88096223e-05  3.85185785e-04  2.65525676e-04  4.61407867e-04
  4.42454394e-05  1.29882357e-04  7.54470971e-05  1.29385791e-04
  1.01874701e-04  2.40732142e-04  1.59969140e-04  2.39378857e-04
  1.35056841e-04  5.07759959e-04  2.64204922e-04  5.07188082e-04
  2.11673281e-05  3.95397102e-05  2.99082180e-05  3.91252907e-05
 -1.49101093e-05 -3.99271496e-06  4.08427147e-05 -9.48162517e-06
  1.61824971e-05  2.69787695e-05  2.83370273e-05 -1.86037378e-06
 -2.69612074e+00]
supnorm grad right now is: 2.6961207435629415
Weights right now are: 
[-1.13810652 -1.15885791 -2.05882524 -1.05034906  0.34788728  1.17332155
  2.63013129  2.4654321   0.4286559   0.86458174  0.72417962  1.31462264
  0.38501444 -0.48913287  0.57764472 -0.66303097 -1.11714727 -1.77071997
 -0.53093778 -1.95843742 -1.13352315 -1.81641506 -1.48857017 -1.94404901
  1.43951485  0.77393135  0.16052092  0.74243866 -1.14421055 -1.30977485
 -2.17703197 -1.86111401  0.27688745  0.90500708  1.33341287  1.08117626
 -1.04070464 -1.75259705 -0.95493427 -2.09297518  0.37518857  0.31507838
  1.37539596  1.70247706  0.50861076 -1.44118287 -0.09149089 -0.78384206
  0.07320927  0.2415886  -0.39658499 -0.37928898 -1.52599957 -2.06471362
 -1.27666553 -2.02558622  0.9416977   1.30771663 -0.65958106 -1.64652503
 -0.12902644  0.33055584 -0.62177987 -1.75095697 24.79693567]
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.752238103191
gradient value of function right now is: [ 2.55623871e-03  2.12064759e-02  2.35437240e-02  2.36118663e-02
 -2.55623871e-03 -2.12064759e-02 -2.35437240e-02 -2.36118663e-02
  5.64973852e-04  3.40035804e-03  2.21153399e-04  3.39317032e-03
  1.04215304e-03  6.26982107e-03  4.08175339e-04  6.25650863e-03
  3.60142266e-04  2.16758191e-03  1.40977761e-04  2.16300034e-03
  2.34054759e-04  1.40902411e-03  9.15883664e-05  1.40605364e-03
  1.00264548e-05  8.63987098e-05  6.05440590e-05  9.99668482e-05
  1.26950139e-05  1.03670091e-04  7.37073047e-05  1.21696141e-04
 -1.61167465e-05 -1.33171964e-04 -9.43777047e-05 -1.55825281e-04
  1.50723918e-05  1.21352913e-04  8.66626256e-05  1.43062217e-04
  5.01039687e-06  1.25521762e-05  8.15544478e-06  1.24378297e-05
  1.89853774e-05  4.30293881e-05  2.99007449e-05  4.26017079e-05
  3.04223484e-05  1.15130908e-04  6.16907745e-05  1.14765157e-04
  4.21289313e-06  7.89778305e-06  6.09560544e-06  7.77772315e-06
 -1.92091819e-06 -5.13001570e-07  8.65339886e-06 -2.15302581e-06
  7.94215828e-06  1.48460100e-05  8.72661577e-06 -1.00296660e-07
 -3.90007626e-01]
supnorm grad right now is: 0.39000762573144393
Weights right now are: 
[-1.14877283 -1.27913754 -2.17974719 -1.1727986   0.35855359  1.29360118
  2.75105324  2.58788165  0.41741684  0.79476338  0.71986935  1.24591068
  0.30262139 -0.9860795   0.54581186 -1.15141134 -1.12194424 -1.79740358
 -0.53279588 -1.98437279 -1.14728127 -1.87879628 -1.49404448 -2.00370307
  1.43985447  0.77712794  0.16252546  0.74488511 -1.1471863  -1.33325572
 -2.19403128 -1.87494453  0.27873232  0.92243233  1.34475264  1.09389468
 -1.04223532 -1.76582503 -0.9639563  -2.10173569  0.36980135  0.30310631
  1.36735995  1.68830355  0.4773614  -1.47953518 -0.12005469 -0.8235642
  0.0497703   0.18047403 -0.4321767  -0.44686062 -1.53074645 -2.06963788
 -1.28075382 -2.03068985  0.9536216   1.31547288 -0.70096764 -1.63490925
 -0.15502796  0.2933882  -0.63772437 -1.7488631  24.9435824 ]
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.2641793331
gradient value of function right now is: [ 1.18326286e-03  8.96486164e-03  9.42025201e-03  9.45489634e-03
 -1.18326286e-03 -8.96486164e-03 -9.42025201e-03 -9.45489634e-03
  2.67221960e-04  1.57227691e-03  9.72632321e-05  1.56829758e-03
  2.80920176e-04  1.65210860e-03  1.02341565e-04  1.64790817e-03
  1.46098193e-04  8.59601315e-04  5.31802237e-05  8.57425355e-04
  9.05604283e-05  5.32954814e-04  3.29488989e-05  5.31608783e-04
  7.47870102e-06  6.48247547e-05  4.71162887e-05  7.49001068e-05
  4.20966915e-06  3.43996017e-05  2.53426166e-05  4.03333971e-05
 -4.32947441e-06 -3.59290658e-05 -2.63737115e-05 -4.19618643e-05
  4.86147101e-06  3.92004192e-05  2.89828335e-05  4.61331047e-05
  4.42555465e-07 -2.48234039e-07  2.84048591e-07 -2.69991287e-07
  5.19117770e-06  1.02379891e-05  7.80087043e-06  1.00906368e-05
  9.47937544e-06  3.33074745e-05  1.91640007e-05  3.31427115e-05
  1.26848211e-06  2.03780968e-06  1.73918780e-06  1.99529380e-06
 -2.85767549e-07 -6.69450449e-08  2.43647243e-06 -8.44045786e-07
  3.34432477e-06  6.58270286e-06  3.10149841e-06 -1.42884271e-07
 -3.62261841e+00]
supnorm grad right now is: 3.6226184097918908
Weights right now are: 
[-1.16157901 -1.41974601 -2.30664752 -1.30141577  0.37135977  1.43420965
  2.87795357  2.71649882  0.40391686  0.71087322  0.71507143  1.16337961
  0.24996309 -1.30422996  0.52693894 -1.46396224 -1.12695589 -1.82530149
 -0.53459577 -2.01147855 -1.16100575 -1.94107071 -1.4991089  -2.06323309
  1.43826163  0.76047924  0.15151463  0.73264401 -1.14976912 -1.35489836
 -2.21007448 -1.88743806  0.27974237  0.93246708  1.351474    1.1010884
 -1.043503   -1.7775619  -0.97212778 -2.10933272  0.36845039  0.30062466
  1.36541693  1.6853886   0.46104652 -1.50159441 -0.13629524 -0.84635373
  0.03430079  0.13647312 -0.45811931 -0.49545553 -1.53305687 -2.07272627
 -1.28309005 -2.03388614  0.95836955  1.31866116 -0.7284377  -1.62982702
 -0.18614046  0.2373616  -0.65228648 -1.74981036 24.71817917]
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5136.142844947429
gradient value of function right now is: [ 5.58013503e-04  3.62356945e-03  3.71910701e-03  3.73496174e-03
 -5.58013503e-04 -3.62356945e-03 -3.71910701e-03 -3.73496174e-03
  1.30121222e-04  7.36745519e-04  4.37925728e-05  7.34628403e-04
  8.41273830e-05  4.76094714e-04  2.83501139e-05  4.74720083e-04
  5.90923816e-05  3.34568205e-04  1.98904882e-05  3.33606382e-04
  3.49407090e-05  1.97871534e-04  1.17537958e-05  1.97303945e-04
  3.19216682e-06  2.80951722e-05  2.10514902e-05  3.24493562e-05
  1.39580402e-06  1.13978431e-05  8.66879551e-06  1.33948076e-05
 -1.62863349e-06 -1.36131755e-05 -1.03042282e-05 -1.59099446e-05
  1.54662135e-06  1.24661001e-05  9.50974237e-06  1.46990361e-05
 -6.06044852e-08 -7.84460546e-07 -3.44664279e-07 -7.87550788e-07
  1.72979091e-06  2.98344932e-06  2.47763891e-06  2.92841842e-06
  3.39219372e-06  1.09684695e-05  6.81082696e-06  1.08979432e-05
  4.52759741e-07  6.14538090e-07  5.81973596e-07  5.98197566e-07
 -3.62901952e-08 -1.44151321e-08  8.07872310e-07 -3.53749473e-07
  1.34436614e-06  2.75726421e-06  1.15617363e-06 -1.12674103e-07
 -7.82202724e+00]
supnorm grad right now is: 7.822027243479116
Weights right now are: 
[-1.17763570e+00 -1.57580739e+00 -2.44226153e+00 -1.43895985e+00
  3.87416466e-01  1.59027102e+00  3.01356757e+00  2.85404290e+00
  3.86480108e-01  6.06042647e-01  7.09319783e-01  1.06029160e+00
  2.09850564e-01 -1.53872389e+00  5.13583791e-01 -1.69422765e+00
 -1.13242658e+00 -1.85477072e+00 -5.36420556e-01 -2.04009859e+00
 -1.17528999e+00 -2.00379646e+00 -1.50400363e+00 -2.12316864e+00
  1.43597621e+00  7.36556389e-01  1.35117414e-01  7.15009599e-01
 -1.15210647e+00 -1.37408540e+00 -2.22489231e+00 -1.89859241e+00
  2.80598279e-01  9.40913982e-01  1.35735887e+00  1.10716736e+00
 -1.04462163e+00 -1.78766207e+00 -9.79457545e-01 -2.11592190e+00
  3.68245370e-01  3.02114874e-01  1.36587646e+00  1.68718313e+00
  4.46588627e-01 -1.51663650e+00 -1.49119536e-01 -8.61819943e-01
  1.97022279e-02  1.00564499e-01 -4.81567465e-01 -5.35049677e-01
 -1.53554446e+00 -2.07497205e+00 -1.28518961e+00 -2.03619471e+00
  9.60114147e-01  1.31963225e+00 -7.50037756e-01 -1.62263820e+00
 -2.19992380e-01  1.75938559e-01 -6.66100678e-01 -1.74914333e+00
  2.43202675e+01]
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.687201641095
gradient value of function right now is: [ 2.42715323e-04  1.28086527e-03  1.29891114e-03  1.30495808e-03
 -2.42715323e-04 -1.28086527e-03 -1.29891114e-03 -1.30495808e-03
  5.79057949e-05  3.15148282e-04  1.77211891e-05  3.14181236e-04
  2.38663851e-05  1.29845164e-04  7.31318310e-06  1.29445337e-04
  2.11026904e-05  1.14845375e-04  6.45925266e-06  1.14492820e-04
  1.19322441e-05  6.49480672e-05  3.65020491e-06  6.47489982e-05
  9.62756972e-07  9.12394111e-06  6.97788819e-06  1.04299627e-05
  3.91642431e-07  3.45192570e-06  2.67145036e-06  4.00624250e-06
 -6.34064702e-07 -5.73178658e-06 -4.41689496e-06 -6.61622984e-06
  4.01887463e-07  3.51215019e-06  2.72254821e-06  4.08418878e-06
 -3.46158566e-08 -2.70902025e-07 -1.41371035e-07 -2.71417743e-07
  4.94495108e-07  9.74215634e-07  7.84762951e-07  9.57367265e-07
  1.04217638e-06  3.69776550e-06  2.36339182e-06  3.67544314e-06
  1.20613543e-07  1.90220224e-07  1.71419326e-07  1.85446487e-07
 -5.94363981e-09 -3.12617623e-09  2.66658945e-07 -8.12692014e-08
  4.67825860e-07  1.15326404e-06  3.89559318e-07 -6.47785926e-09
 -1.63242088e+00]
supnorm grad right now is: 1.6324208800355133
Weights right now are: 
[-1.19825000e+00 -1.74335686e+00 -2.58533078e+00 -1.58413680e+00
  4.08030763e-01  1.75782050e+00  3.15663682e+00  2.99921984e+00
  3.63440946e-01  4.72883477e-01  7.02346770e-01  9.29380650e-01
  1.76720178e-01 -1.72497871e+00  5.03470018e-01 -1.87707717e+00
 -1.13833508e+00 -1.88537765e+00 -5.38229128e-01 -2.06981570e+00
 -1.19002135e+00 -2.06600477e+00 -1.50863553e+00 -2.18259415e+00
  1.43363239e+00  7.11035227e-01  1.17190826e-01  6.96294160e-01
 -1.15412574e+00 -1.39129697e+00 -2.23849912e+00 -1.90853900e+00
  2.81531387e-01  9.50530191e-01  1.36421750e+00  1.11404079e+00
 -1.04553419e+00 -1.79624333e+00 -9.85825091e-01 -2.12148092e+00
  3.68627821e-01  3.04775952e-01  1.36731972e+00  1.69035809e+00
  4.34370617e-01 -1.52960200e+00 -1.60412937e-01 -8.75116953e-01
  6.24402185e-03  6.67012961e-02 -5.04664171e-01 -5.72349505e-01
 -1.53764910e+00 -2.07690462e+00 -1.28702956e+00 -2.03817362e+00
  9.60719825e-01  1.32027492e+00 -7.70591903e-01 -1.61587644e+00
 -2.55914285e-01  1.03272721e-01 -6.80298893e-01 -1.74825317e+00
  2.48781625e+01]
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.049862056015
gradient value of function right now is: [ 1.09825668e-04  4.54844666e-04  4.58301369e-04  4.60538767e-04
 -1.09825668e-04 -4.54844666e-04 -4.58301368e-04 -4.60538767e-04
  2.61923478e-05  1.37769170e-04  7.30239912e-06  1.37323014e-04
  7.22095624e-06  3.79724308e-05  2.01554894e-06  3.78491529e-05
  7.57087650e-06  3.98206026e-05  2.11114082e-06  3.96915978e-05
  4.11703198e-06  2.16566761e-05  1.14743340e-06  2.15865931e-05
  2.60537192e-07  2.65914421e-06  2.07548684e-06  3.01151941e-06
  1.18805643e-07  1.13082499e-06  8.90708869e-07  1.29752691e-06
 -2.66836546e-07 -2.60493940e-06 -2.04475427e-06 -2.97439950e-06
  1.10569539e-07  1.04757879e-06  8.25760567e-07  1.20317325e-06
  6.28481917e-10 -4.40753095e-08 -2.06512847e-08 -4.45210394e-08
  1.69466262e-07  3.88526917e-07  3.04844471e-07  3.82664287e-07
  3.58441942e-07  1.38569115e-06  9.18720730e-07  1.37816079e-06
  3.77200359e-08  6.96727329e-08  6.02322493e-08  6.81275126e-08
 -2.98473902e-09 -9.36205953e-10  1.03183495e-07 -1.63951012e-08
  1.68043654e-07  4.80367537e-07  1.40890194e-07  8.54692002e-09
  4.74341328e+00]
supnorm grad right now is: 4.7434132812001755
Weights right now are: 
[-1.22337582e+00 -1.90666877e+00 -2.72351714e+00 -1.72440308e+00
  4.33156583e-01  1.92113241e+00  3.29482319e+00  3.13948613e+00
  3.34925907e-01  3.14474591e-01  6.94434934e-01  7.73685103e-01
  1.49971189e-01 -1.86952079e+00  4.95994465e-01 -2.01894279e+00
 -1.14416827e+00 -1.91442533e+00 -5.39865213e-01 -2.09801196e+00
 -1.20395794e+00 -2.12257854e+00 -1.51264985e+00 -2.23662405e+00
  1.43173873e+00  6.89453824e-01  1.01647523e-01  6.80528970e-01
 -1.15581215e+00 -1.40614991e+00 -2.25053959e+00 -1.91710066e+00
  2.82622815e-01  9.62176771e-01  1.37274009e+00  1.12234216e+00
 -1.04623410e+00 -1.80307006e+00 -9.91013522e-01 -2.12588509e+00
  3.68777568e-01  3.06623970e-01  1.36835624e+00  1.69256689e+00
  4.22488119e-01 -1.54251363e+00 -1.71881125e-01 -8.88346520e-01
 -6.98043604e-03  3.41252359e-02 -5.28443015e-01 -6.08193151e-01
 -1.53969670e+00 -2.07871930e+00 -1.28881256e+00 -2.04002779e+00
  9.61121446e-01  1.32074997e+00 -7.88763337e-01 -1.61049055e+00
 -2.86362486e-01  3.58791650e-02 -6.93949531e-01 -1.74775057e+00
  2.52929851e+01]
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.780799649002
gradient value of function right now is: [ 5.34467363e-05  1.68011969e-04  1.68639489e-04  1.69484276e-04
 -5.34467364e-05 -1.68011969e-04 -1.68639490e-04 -1.69484276e-04
  1.22538472e-05  6.27102927e-05  3.16024415e-06  6.24924647e-05
  2.34779861e-06  1.20123355e-05  6.06413570e-07  1.19704964e-05
  2.81572837e-06  1.44091635e-05  7.26374458e-07  1.43590877e-05
  1.47801791e-06  7.56435011e-06  3.81026457e-07  7.53809333e-06
  7.34519477e-08  7.54231603e-07  6.03975939e-07  8.54010590e-07
  4.57058198e-08  4.28829176e-07  3.46966050e-07  4.93329440e-07
 -1.25443670e-07 -1.21687569e-06 -9.80651303e-07 -1.39147393e-06
  3.94285739e-08  3.68828308e-07  2.98561034e-07  4.24554485e-07
  9.94925345e-09  1.04116125e-08  1.17732466e-08  9.96087481e-09
  8.39484976e-08  1.77445552e-07  1.45864147e-07  1.74482471e-07
  1.60455190e-07  5.66450744e-07  3.98849421e-07  5.62752163e-07
  1.91776162e-08  3.07468346e-08  2.83650230e-08  2.99555212e-08
 -1.59223193e-09 -7.81295974e-10  4.87123527e-08 -1.11967014e-08
  6.73683391e-08  1.78988639e-07  5.95779932e-08 -1.04747606e-09
 -6.24543316e-02]
supnorm grad right now is: 0.06245433159969254
Weights right now are: 
[-1.25548244e+00 -2.06781330e+00 -2.85917334e+00 -1.86212551e+00
  4.65263202e-01  2.08227694e+00  3.43047939e+00  3.27720856e+00
  2.99103415e-01  1.21571316e-01  6.85315659e-01  5.84123253e-01
  1.27133122e-01 -1.98909125e+00  4.90135648e-01 -2.13627626e+00
 -1.14997637e+00 -1.94245619e+00 -5.41360364e-01 -2.12521565e+00
 -1.21732198e+00 -2.17515786e+00 -1.51618275e+00 -2.28682963e+00
  1.43032394e+00  6.72653691e-01  8.92616087e-02  6.68301075e-01
 -1.15735872e+00 -1.42022206e+00 -2.26220613e+00 -1.92520887e+00
  2.83946232e-01  9.76864079e-01  1.38372910e+00  1.13276265e+00
 -1.04682145e+00 -1.80899576e+00 -9.95620082e-01 -2.12969384e+00
  3.68333578e-01  3.06902408e-01  1.36824642e+00  1.69292203e+00
  4.10498908e-01 -1.55701196e+00 -1.84785398e-01 -9.03182513e-01
 -2.06547754e-02  8.93108428e-06 -5.54470245e-01 -6.45602969e-01
 -1.54180377e+00 -2.08065380e+00 -1.29073740e+00 -2.04200366e+00
  9.61466763e-01  1.32102168e+00 -8.01118060e-01 -1.60772204e+00
 -3.05431252e-01 -8.70397658e-03 -7.05136404e-01 -1.74795958e+00
  2.49749361e+01]
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.481728234694
gradient value of function right now is: [ 2.40060586e-05  5.66117641e-05  5.66847235e-05  5.69714209e-05
 -2.40060578e-05 -5.66117623e-05 -5.66847217e-05 -5.69714191e-05
  4.98254874e-06  2.50117801e-05  1.18206362e-06  2.49214665e-05
  7.09651731e-07  3.56176482e-06  1.68593750e-07  3.54887595e-06
  9.49578478e-07  4.76663767e-06  2.25330633e-07  4.74942013e-06
  4.83378719e-07  2.42662084e-06  1.14629599e-07  2.41786427e-06
  2.27267419e-08  2.40286766e-07  1.95819070e-07  2.70368180e-07
  1.69779689e-08  1.64265904e-07  1.35057125e-07  1.87590049e-07
 -4.81715289e-08 -4.81017930e-07 -3.94180192e-07 -5.46339806e-07
  1.48372279e-08  1.43689141e-07  1.18141948e-07  1.64072405e-07
  5.16820091e-09  8.99153884e-09  8.04983249e-09  8.78635337e-09
  3.21357004e-08  7.17939218e-08  5.85352157e-08  7.06692664e-08
  5.97322734e-08  2.16868764e-07  1.54412894e-07  2.15526592e-07
  7.09770920e-09  1.22042472e-08  1.10173124e-08  1.19131758e-08
 -5.35724720e-10 -2.24095120e-10  2.00800001e-08 -3.34322700e-09
  2.58419796e-08  6.61535823e-08  2.32904666e-08  2.83545132e-10
  2.98291785e+00]
supnorm grad right now is: 2.982917845405135
Weights right now are: 
[-1.2969427  -2.22448678 -2.99065262 -1.99561917  0.50672347  2.23895042
  3.56195867  3.41070222  0.25582069 -0.10619308  0.67513234  0.3603437
  0.10747899 -2.08962593  0.4854775  -2.23491917 -1.15564375 -1.96918348
 -0.5427081  -2.15114943 -1.2299055  -2.22358507 -1.51925582 -2.33306718
  1.42922784  0.65951687  0.07934146  0.65870107 -1.15891873 -1.43446444
 -2.27424467 -1.93361653  0.28551258  0.99430161  1.39707248  1.14515585
 -1.04740633 -1.81492342 -1.00033071 -2.13351535  0.36739183  0.30582168
  1.36707634  1.69173596  0.40102763 -1.57117174 -0.19730646 -0.91746023
 -0.03331232 -0.03331299 -0.58029501 -0.68135084 -1.54366504 -2.08265317
 -1.29271722 -2.04401878  0.96166088  1.32113213 -0.80788055 -1.6057686
 -0.31430504 -0.03076248 -0.71234561 -1.74733503 25.17154425]
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.744622146615
gradient value of function right now is: [ 1.08228373e-05  1.98223457e-05  1.98179627e-05  1.99180667e-05
 -1.08228365e-05 -1.98223442e-05 -1.98179613e-05 -1.99180652e-05
  1.92123658e-06  9.43657742e-06  4.26137370e-07  9.40130386e-06
  2.29527886e-07  1.12720045e-06  5.09885884e-08  1.12297753e-06
  3.33778810e-07  1.63940094e-06  7.40462344e-08  1.63327144e-06
  1.65449605e-07  8.12689684e-07  3.66760830e-08  8.09654435e-07
  1.15011488e-08  1.19744237e-07  9.89910956e-08  1.34880265e-07
  7.27219888e-09  6.83037243e-08  5.70418804e-08  7.82747628e-08
 -1.79173812e-08 -1.74325559e-07 -1.45050838e-07 -1.98559782e-07
  6.98305745e-09  6.58061701e-08  5.49441618e-08  7.53720527e-08
  1.80371178e-09  1.87789279e-09  2.11556344e-09  1.79687643e-09
  1.37306972e-08  2.76178003e-08  2.33811579e-08  2.71243559e-08
  2.46647523e-08  8.23828284e-08  6.01725500e-08  8.17882775e-08
  3.18232992e-09  4.81672736e-09  4.57319033e-09  4.68483222e-09
 -4.20908682e-11 -8.89459566e-11  8.32536553e-09 -2.06562268e-09
  1.07477910e-08  2.51842136e-08  9.70435002e-09 -5.18154687e-10
 -1.06284048e+00]
supnorm grad right now is: 1.0628404827231852
Weights right now are: 
[-1.34839477 -2.37221174 -3.11438526 -2.12124986  0.55817553  2.38667537
  3.68569131  3.53633291  0.20857285 -0.34990994  0.66482978  0.12090486
  0.09094894 -2.17249551  0.48184141 -2.31626205 -1.16099308 -1.99391994
 -0.54388629 -2.17514906 -1.24125159 -2.26671526 -1.52182394 -2.37427579
  1.42812469  0.6463937   0.06916801  0.64871043 -1.16033861 -1.44743171
 -2.28529241 -1.94231053  0.2870973   1.01182648  1.41077362  1.15789249
 -1.04805746 -1.82147358 -1.0056422  -2.13788906  0.3666486   0.30496066
  1.3661606   1.69084802  0.39607728 -1.5797347  -0.20481464 -0.92595683
 -0.04119739 -0.05634535 -0.59777756 -0.70505387 -1.54485947 -2.08400796
 -1.29410617 -2.04535831  0.96170674  1.32118237 -0.81064096 -1.60510842
 -0.31796568 -0.03983979 -0.71545768 -1.74723461 24.94863787]
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5137.56004680237
gradient value of function right now is: [ 4.31658743e-06  6.54304543e-06  6.53598375e-06  6.56848805e-06
 -4.31658883e-06 -6.54304756e-06 -6.53598587e-06 -6.56849018e-06
  6.30255578e-07  3.02287911e-06  1.30353106e-07  3.01173762e-06
  7.21923261e-08  3.46222463e-07  1.49475536e-08  3.44944495e-07
  1.10800171e-07  5.31427683e-07  2.29168652e-08  5.29468952e-07
  5.38979594e-08  2.58521888e-07  1.11410741e-08  2.57569788e-07
  5.74545988e-09  6.22423421e-08  5.15436747e-08  6.96515511e-08
  2.57365327e-09  2.55820623e-08  2.13462450e-08  2.90257563e-08
 -5.14684070e-09 -5.26036665e-08 -4.37801340e-08 -5.94100058e-08
  2.72401788e-09  2.72049961e-08  2.26931260e-08  3.08442861e-08
  5.93835966e-11 -9.30800148e-10 -4.75527220e-10 -9.43180191e-10
  3.89376755e-09  8.77156374e-09  7.18281881e-09  8.63231572e-09
  7.60549204e-09  2.81339762e-08  2.01482654e-08  2.79654747e-08
  8.63917557e-10  1.54227235e-09  1.37967615e-09  1.50669481e-09
  5.79278198e-11  2.19295187e-11  2.71922903e-09 -3.32279341e-10
  3.97329654e-09  9.82238464e-09  3.42975453e-09  1.64496991e-10
  6.00625474e+00]
supnorm grad right now is: 6.006254736147957
Weights right now are: 
[-1.41018331 -2.51365479 -3.23277189 -2.2414447   0.61996408  2.52811844
  3.80407794  3.65652775  0.161907   -0.58511339  0.65539945 -0.11030606
  0.07780222 -2.23690435  0.47914374 -2.37967473 -1.16610292 -2.01706325
 -0.54492684 -2.19761138 -1.25076014 -2.30364865 -1.5238222  -2.40974944
  1.42694938  0.63271271  0.05822756  0.63665671 -1.16115443 -1.45558423
 -2.29211664 -1.94946623  0.28826853  1.02506365  1.42131734  1.16853763
 -1.04863896 -1.82763589 -1.01067717 -2.14263268  0.36654438  0.30492094
  1.36607322  1.690814    0.39459169 -1.58326738 -0.20765381 -0.92944114
 -0.04405353 -0.06689381 -0.60534504 -0.71561584 -1.54515483 -2.0846097
 -1.2946198  -2.04594852  0.96170675  1.3211819  -0.81170572 -1.60498393
 -0.31940379 -0.04350817 -0.71675549 -1.74729227 25.37458555]
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5137.819043351506
gradient value of function right now is: [ 1.89741299e-06  2.55875137e-06  2.55535472e-06  2.56762916e-06
 -1.89741359e-06 -2.55875217e-06 -2.55535552e-06 -2.56762997e-06
  2.31214341e-07  1.08717564e-06  4.69316990e-08  1.08309443e-06
  2.81336029e-08  1.32269493e-07  5.71889399e-09  1.31771936e-07
  4.35550733e-08  2.04798997e-07  8.83984153e-09  2.04030313e-07
  2.11316050e-08  9.93690064e-08  4.28509492e-09  9.89964884e-08
  3.17917839e-09  3.25054702e-08  2.71306105e-08  3.67008373e-08
  1.19427680e-09  1.08524610e-08  9.16555716e-09  1.25006300e-08
 -1.89227930e-09 -1.78528796e-08 -1.50179491e-08 -2.04288154e-08
  1.35101581e-09  1.23397934e-08  1.04174242e-08  1.42014639e-08
 -1.31149765e-10 -9.78150723e-10 -6.18648203e-10 -9.80006192e-10
  1.87238240e-09  3.19051925e-09  2.87075730e-09  3.12027307e-09
  3.51400573e-09  1.06912549e-08  8.01330275e-09  1.06007895e-08
  4.72891664e-10  6.02139978e-10  6.15527118e-10  5.82317308e-10
  6.16938213e-11 -8.62916292e-12  1.11715002e-09 -4.00332044e-10
  1.73546599e-09  3.73986733e-09  1.48627433e-09 -1.68629773e-10
 -4.89132815e+00]
supnorm grad right now is: 4.891328150480665
Weights right now are: 
[-1.48065046 -2.65090772 -3.34799336 -2.35839237  0.69043124  2.6653714
  3.91929945  3.77347545  0.12257505 -0.77748767  0.64774132 -0.29993438
  0.06967509 -2.27568133  0.4775159  -2.41809814 -1.17089773 -2.0384226
 -0.54586116 -2.21838693 -1.25681481 -2.32929448 -1.52504719 -2.43477088
  1.4260899   0.62327661  0.05046164  0.62683108 -1.16153082 -1.45926283
 -2.29519988 -1.95340896  0.28887381  1.03138174  1.42651777  1.17475597
 -1.04901461 -1.83139533 -1.01380666 -2.14631942  0.36655757  0.30513932
  1.3662019   1.69103414  0.39401712 -1.58447768 -0.20866857 -0.93063087
 -0.04517146 -0.07084481 -0.60821324 -0.71954562 -1.54528667 -2.08482608
 -1.2948202  -2.04615939  0.96169263  1.32118127 -0.8120955  -1.6049115
 -0.31999276 -0.04502508 -0.71725578 -1.74729989 24.60205597]
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.7749967315995
gradient value of function right now is: [ 8.03359728e-07  1.02554651e-06  1.02424197e-06  1.02893408e-06
 -8.03359550e-07 -1.02554628e-06 -1.02424174e-06 -1.02893385e-06
  8.88277904e-08  4.14151217e-07  1.76258171e-08  4.12620474e-07
  1.15076326e-08  5.36486353e-08  2.28608574e-09  5.34500278e-08
  1.75020682e-08  8.16028823e-08  3.47227906e-09  8.13013451e-08
  8.61392208e-09  4.01643411e-08  1.70764548e-09  4.00160778e-08
  1.43526254e-09  1.50644091e-08  1.25499022e-08  1.69384587e-08
  4.77571503e-10  4.51179788e-09  3.79628395e-09  5.16202853e-09
 -6.70620292e-10 -6.54609066e-09 -5.48999887e-09 -7.44774420e-09
  5.58010078e-10  5.29979627e-09  4.45751149e-09  6.05829211e-09
 -1.05954212e-10 -5.50777934e-10 -3.67662777e-10 -5.50109843e-10
  6.53533360e-10  1.20022105e-09  1.05253506e-09  1.17554459e-09
  1.30770853e-09  4.28462328e-09  3.15665003e-09  4.25242989e-09
  1.61301401e-10  2.28517730e-10  2.23595992e-10  2.21697472e-10
  3.09226855e-11  2.96191678e-12  4.27197321e-10 -1.16729905e-10
  7.18236897e-10  1.63252543e-09  6.00296241e-10 -2.62248515e-11
 -4.09457291e-01]
supnorm grad right now is: 0.4094572913050658
Weights right now are: 
[-1.55322523 -2.77604155 -3.45533906 -2.46720617  0.76300602  2.79050524
  4.02664515  3.88228926  0.09899358 -0.88933767  0.643087   -0.41083544
  0.06611303 -2.29237363  0.47679925 -2.43470834 -1.17456478 -2.05489931
 -0.54657944 -2.23454595 -1.25948293 -2.34145005 -1.52558403 -2.44681353
  1.42565251  0.618796    0.04672536  0.62183417 -1.16168998 -1.46070143
 -2.29641633 -1.95505266  0.28910743  1.03358876  1.4283715   1.17721448
 -1.04919474 -1.83303731 -1.01519302 -2.14815786  0.36658055  0.30528837
  1.36629793  1.69118327  0.39377186 -1.58488418 -0.20903801 -0.93102831
 -0.04563489 -0.0722356  -0.60925808 -0.72092491 -1.5453503  -2.08490337
 -1.29490077 -2.04623415  0.96168341  1.32118187 -0.81224241 -1.60486136
 -0.32022587 -0.04550033 -0.71745338 -1.74727964 24.96136854]
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5136.689333161178
gradient value of function right now is: [ 3.59041819e-07  4.47038016e-07  4.46506932e-07  4.48478109e-07
 -3.59041748e-07 -4.47037928e-07 -4.46506844e-07 -4.48478021e-07
  3.86363309e-08  1.79479730e-07  7.51454101e-09  1.78830118e-07
  5.14405358e-09  2.38946737e-08  1.00129467e-09  2.38080982e-08
  7.68580343e-09  3.57037679e-08  1.49460571e-09  3.55745695e-08
  3.83943300e-09  1.78364266e-08  7.46210540e-10  1.77719294e-08
  6.45466291e-10  7.08614816e-09  5.88017537e-09  7.91333417e-09
  1.99895493e-10  2.02245179e-09  1.68983884e-09  2.28791536e-09
 -2.69104549e-10 -2.79087093e-09 -2.32676126e-09 -3.14446111e-09
  2.36711723e-10  2.40754640e-09  2.01088810e-09  2.72134221e-09
 -5.56466674e-11 -2.76873597e-10 -1.86484900e-10 -2.76342260e-10
  2.31395110e-10  5.12078074e-10  4.22789927e-10  5.03533546e-10
  5.06787024e-10  1.91613599e-09  1.36847235e-09  1.90512907e-09
  5.29116202e-11  9.57499621e-11  8.54533870e-11  9.35600884e-11
  1.38518615e-11  5.70385423e-12  1.77445058e-10 -1.69339278e-11
  3.17316896e-10  7.89259187e-10  2.58531410e-10  2.16182456e-11
  8.05667443e+00]
supnorm grad right now is: 8.056674432767219
Weights right now are: 
[-1.62654015 -2.8846989  -3.55493034 -2.56777645  0.83632094  2.89916259
  4.12623643  3.98285954  0.08659641 -0.94721394  0.6406693  -0.46844465
  0.06443123 -2.30020491  0.47646978 -2.44250985 -1.17689192 -2.06559853
 -0.54703158 -2.2451532  -1.26074054 -2.34728187 -1.52583017 -2.45261916
  1.42544291  0.61652731  0.04484105  0.61929919 -1.16175595 -1.46136488
 -2.2969705  -1.95580294  0.28919883  1.03452868  1.42915502  1.17827091
 -1.04927211 -1.83382032 -1.0158467  -2.14904011  0.3666      0.30537386
  1.36635682  1.69126858  0.39369451 -1.58505946 -0.20918308 -0.93119995
 -0.04580455 -0.07286812 -0.60971406 -0.72155257 -1.5453658  -2.08493672
 -1.29492939 -2.04626653  0.96167866  1.32118076 -0.81230748 -1.6048462
 -0.32033119 -0.04577341 -0.71754527 -1.74727648 25.53871238]
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.6283418071425
gradient value of function right now is: [ 2.23263230e-07  2.74697111e-07  2.74387265e-07  2.75574652e-07
 -2.23262874e-07 -2.74696672e-07 -2.74386826e-07 -2.75574212e-07
  2.39795647e-08  1.11052484e-07  4.68842687e-09  1.10645679e-07
  3.22368113e-09  1.49282333e-08  6.30921656e-10  1.48734741e-08
  4.76322634e-09  2.20594794e-08  9.31093404e-10  2.19786961e-08
  2.39968459e-09  1.11139620e-08  4.68749409e-10  1.10732998e-08
  4.22096212e-10  4.49046520e-09  3.73803531e-09  5.03863541e-09
  1.32764616e-10  1.27937272e-09  1.07469343e-09  1.45888907e-09
 -1.73165360e-10 -1.71989348e-09 -1.44051643e-09 -1.95124871e-09
  1.57957547e-10  1.53035145e-09  1.28500876e-09  1.74356520e-09
 -3.63699276e-11 -1.76390334e-10 -1.19275381e-10 -1.76031346e-10
  1.68812774e-10  3.22555079e-10  2.79187066e-10  3.16184120e-10
  3.49691915e-10  1.19034589e-09  8.69612892e-10  1.18199434e-09
  4.11777897e-11  6.17303273e-11  5.90734454e-11  5.99950886e-11
  9.46320351e-12  1.85033792e-12  1.15901029e-10 -2.61959377e-11
  2.02948863e-10  4.73328418e-10  1.67397189e-10 -1.14104862e-12
  2.10056328e+00]
supnorm grad right now is: 2.1005632788028126
Weights right now are: 
[-1.68084503 -2.95498382 -3.62359783 -2.63684456  0.89062582  2.96944751
  4.19490392  4.05192765  0.08016348 -0.97705214  0.63940429 -0.49816877
  0.06356655 -2.30421387  0.47629945 -2.44650392 -1.17816074 -2.07146992
 -0.54728085 -2.25099819 -1.26138451 -2.35026634 -1.52595676 -2.45559231
  1.4253294   0.61533722  0.04384952  0.61796143 -1.16179237 -1.46170768
 -2.29725911 -1.95619525  0.28924669  1.03499445  1.42954581  1.17880073
 -1.04931531 -1.83422899 -1.01619062 -2.14950723  0.36660927  0.30542015
  1.36638784  1.6913148   0.39364591 -1.58514657 -0.20925987 -0.93128532
 -0.04590272 -0.07318689 -0.60994844 -0.72186909 -1.54537818 -2.08495345
 -1.29494601 -2.04628276  0.96167614  1.32118049 -0.81233852 -1.60483817
 -0.32038514 -0.04588875 -0.71758985 -1.74727567 25.13033218]
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5136.630327046124
gradient value of function right now is: [ 1.67990701e-07  2.05210799e-07  2.04988337e-07  2.05864654e-07
 -1.67990358e-07 -2.05210379e-07 -2.04987918e-07 -2.05864234e-07
  1.80584078e-08  8.34078405e-08  3.57542265e-09  8.30956258e-08
  2.44273130e-09  1.12813174e-08  4.84300807e-10  1.12390078e-08
  3.58204197e-09  1.65450517e-08  7.09002047e-10  1.64831467e-08
  1.81384038e-09  8.37848979e-09  3.58678906e-10  8.34718180e-09
  3.36559872e-10  3.40215077e-09  2.84697702e-09  3.84829693e-09
  1.09772423e-10  9.78578524e-10  8.29488417e-10  1.13102246e-09
 -1.39212483e-10 -1.28994354e-09 -1.08883688e-09 -1.48064598e-09
  1.30942741e-10  1.17359920e-09  9.94352285e-10  1.35516073e-09
 -2.77880668e-11 -1.32539723e-10 -8.98131080e-11 -1.32266508e-10
  1.59653809e-10  2.47800133e-10  2.31187874e-10  2.41715546e-10
  3.06611855e-10  8.89690498e-10  6.75222027e-10  8.81594107e-10
  4.22408055e-11  4.89674182e-11  5.22373480e-11  4.71974374e-11
  7.96034783e-12 -9.81273171e-13  9.36193491e-11 -3.92715890e-11
  1.58422628e-10  3.28568521e-10  1.32655035e-10 -1.90121706e-11
 -7.12565385e+00]
supnorm grad right now is: 7.12565385345238
Weights right now are: 
[-1.7184884  -3.00148028 -3.66988028 -2.68333706  0.92826919  3.01594397
  4.24118637  4.09842016  0.07606053 -0.99603871  0.63860089 -0.51708518
  0.06301325 -2.30677393  0.47619098 -2.44905454 -1.17897436 -2.07523379
 -0.54744011 -2.25474781 -1.2617961  -2.35217091 -1.52603728 -2.45748985
  1.42525609  0.61456401  0.04320547  0.61709275 -1.16181557 -1.46192785
 -2.29744438 -1.95644697  0.28927665  1.03528834  1.42979228  1.17913482
 -1.04934295 -1.83449265 -1.01641242 -2.14980841  0.36661537  0.30545073
  1.36640841  1.6913453   0.39361571 -1.58520181 -0.20930834 -0.9313395
 -0.04596472 -0.07339064 -0.61009832 -0.72207142 -1.545386   -2.08496395
 -1.29495648 -2.04629298  0.96167449  1.32118019 -0.81235875 -1.60483454
 -0.32042086 -0.04596529 -0.71761939 -1.74727631 24.43878744]
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.769657354723
gradient value of function right now is: [ 1.22110071e-07  1.48334934e-07  1.48176862e-07  1.48803263e-07
 -1.22109688e-07 -1.48334468e-07 -1.48176396e-07 -1.48802796e-07
  1.30975615e-08  6.04779727e-08  2.55542308e-09  6.02560316e-08
  1.77828656e-09  8.21064688e-09  3.47314408e-10  8.18047331e-09
  2.59721473e-09  1.19928550e-08  5.06608776e-10  1.19488590e-08
  1.32151498e-09  6.10250822e-09  2.57582395e-10  6.08014312e-09
  2.38842590e-10  2.52211566e-09  2.10143541e-09  2.83325560e-09
  7.46325586e-11  7.11022663e-10  5.98130426e-10  8.12346718e-10
 -9.47451781e-11 -9.31371765e-10 -7.81080161e-10 -1.05844254e-09
  8.92765311e-11  8.55165486e-10  7.19092474e-10  9.76163460e-10
 -2.11644295e-11 -1.00561066e-10 -6.82531500e-11 -1.00336555e-10
  9.60185059e-11  1.77025145e-10  1.55133878e-10  1.73375859e-10
  1.97200696e-10  6.54757560e-10  4.81108481e-10  6.49944206e-10
  2.37688748e-11  3.41759726e-11  3.32579398e-11  3.31693354e-11
  5.47197867e-12  8.15254725e-13  6.45438115e-11 -1.66252917e-11
  1.13197374e-10  2.59364865e-10  9.35132450e-11 -2.65013470e-12
  5.98267953e-01]
supnorm grad right now is: 0.5982679528420374
Weights right now are: 
[-1.74782418 -3.03726035 -3.70560524 -2.71921511  0.95760496  3.05172404
  4.27691132  4.13429819  0.07290344 -1.01061722  0.63797982 -0.53160946
  0.06258545 -2.3087492   0.47610671 -2.45102245 -1.17960039 -2.07812455
 -0.54756323 -2.25762778 -1.26211386 -2.35363832 -1.52609971 -2.45895182
  1.42519789  0.61396331  0.04270362  0.61641549 -1.16183433 -1.46209892
 -2.2975891  -1.95664392  0.28930038  1.03551323  1.42998174  1.17939198
 -1.04936538 -1.83469813 -1.01658618 -2.15004479  0.36662004  0.30547439
  1.36642428  1.69136892  0.39358932 -1.58524438 -0.20934752 -0.93138109
 -0.04601598 -0.07354696 -0.61021553 -0.72222646 -1.5453931  -2.08497225
 -1.29496531 -2.046301    0.96167311  1.32118025 -0.81237472 -1.60482889
 -0.32044846 -0.04602578 -0.71764185 -1.74727426 25.04907418]
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.583821981554
gradient value of function right now is: [ 1.02033109e-07  1.23459737e-07  1.23330889e-07  1.23848480e-07
 -1.02032977e-07 -1.23459578e-07 -1.23330730e-07 -1.23848321e-07
  1.09384566e-08  5.04454660e-08  2.14084903e-09  5.02591821e-08
  1.49088803e-09  6.87506261e-09  2.92123202e-10  6.84963511e-09
  2.16896570e-09  1.00029415e-08  4.24386668e-10  9.96601741e-09
  1.10693473e-09  5.10529704e-09  2.16410942e-10  5.08647233e-09
  2.03976554e-10  2.12085135e-09  1.76992966e-09  2.38825180e-09
  6.42888118e-11  5.98209721e-10  5.04594301e-10  6.86211080e-10
 -8.05843656e-11 -7.75645970e-10 -6.52010742e-10 -8.84561223e-10
  7.70280091e-11  7.20675510e-10  6.07634444e-10  8.25938640e-10
 -1.80380593e-11 -8.46133444e-11 -5.75443500e-11 -8.44201550e-11
  8.61422236e-11  1.48806142e-10  1.33457713e-10  1.45505933e-10
  1.72809511e-10  5.46388182e-10  4.06118224e-10  5.42014929e-10
  2.18585008e-11  2.90464732e-11  2.92087312e-11  2.81147871e-11
  4.76784753e-12  2.36318558e-13  5.51987177e-11 -1.76935051e-11
  9.58354809e-11  2.12322286e-10  7.95475036e-11 -5.73627514e-12
 -2.27664453e+00]
supnorm grad right now is: 2.276644528246631
Weights right now are: 
[-1.76963565 -3.06371219 -3.73202748 -2.74574877  0.97941644  3.07817587
  4.30333356  4.16083185  0.07056493 -1.02141345  0.63752418 -0.54236624
  0.06226747 -2.31021712  0.4760447  -2.45248501 -1.1800641  -2.0802654
 -0.54765355 -2.2597608  -1.26235015 -2.35472929 -1.52614571 -2.46003881
  1.42515502  0.61351084  0.0423269   0.61590724 -1.16184751 -1.46222637
 -2.29769602 -1.95678912  0.28931714  1.03567956  1.43012095  1.17958068
 -1.04938113 -1.83485146 -1.01671475 -2.15021931  0.36662424  0.30549249
  1.36643677  1.69138698  0.39357317 -1.58527656 -0.20937503 -0.93141261
 -0.04605056 -0.0736644  -0.61030137 -0.72234302 -1.54539674 -2.08497848
 -1.29497101 -2.04630705  0.96167211  1.32118008 -0.81238657 -1.60482604
 -0.32046868 -0.04607216 -0.71765911 -1.7472739  24.8161017 ]
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5137.333542414058
gradient value of function right now is: [ 8.87435316e-08  1.07038984e-07  1.06929381e-07  1.07375541e-07
 -8.87430989e-08 -1.07038462e-07 -1.06928860e-07 -1.07375017e-07
  9.51037069e-09  4.38052352e-08  1.87123992e-09  4.36419544e-08
  1.30084394e-09  5.99120096e-09  2.56279803e-10  5.96882910e-09
  1.88607716e-09  8.68757225e-09  3.70981561e-10  8.65520480e-09
  9.64835322e-10  4.44446882e-09  1.89604247e-10  4.42793085e-09
  1.81980724e-10  1.85133613e-09  1.54853124e-09  2.09198968e-09
  5.82139057e-11  5.23884567e-10  4.43655775e-10  6.04482054e-10
 -7.20085174e-11 -6.72781012e-10 -5.67467407e-10 -7.71147033e-10
  6.98425270e-11  6.31957628e-10  5.34939552e-10  7.28501504e-10
 -1.58411492e-11 -7.36978650e-11 -5.01816147e-11 -7.35246890e-11
  8.24646143e-11  1.30388643e-10  1.20808759e-10  1.27239780e-10
  1.60354215e-10  4.73587595e-10  3.57876316e-10  4.69388945e-10
  2.16816703e-11  2.57919712e-11  2.72006608e-11  2.48809942e-11
  4.34180250e-12 -3.32246453e-13  4.93546186e-11 -1.96661819e-11
  8.46943805e-11  1.77965915e-10  7.06144073e-11 -8.98874426e-12
 -6.00374586e+00]
supnorm grad right now is: 6.003745857641124
Weights right now are: 
[-1.78769925 -3.08553753 -3.75382998 -2.76764251  0.99748003  3.10000121
  4.32513606  4.18272559  0.06862966 -1.03033689  0.63714649 -0.55125684
  0.06200336 -2.31143484  0.4759931  -2.45369824 -1.18044789 -2.08203506
 -0.54772844 -2.26152396 -1.26254627 -2.35563363 -1.52618395 -2.46093982
  1.42511905  0.61313354  0.04201225  0.61548297 -1.16185878 -1.46233228
 -2.29778527 -1.9569104   0.28933125  1.03581657  1.43023602  1.17973668
 -1.04939465 -1.83497916 -1.01682232 -2.15036541  0.3666274   0.30550761
  1.36644701  1.69140206  0.3935583  -1.5853027  -0.20939829 -0.93143819
 -0.04608046 -0.07376137 -0.61037294 -0.72243924 -1.5454005  -2.08498358
 -1.29497608 -2.04631198  0.96167129  1.32117999 -0.81239617 -1.60482303
 -0.32048557 -0.0461101  -0.7176729  -1.74727295 24.49319216]
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.70573865005
gradient value of function right now is: [ 7.29016928e-08  8.76721368e-08  8.75829414e-08  8.79459870e-08
 -7.29020731e-08 -8.76725944e-08 -8.75833985e-08 -8.79464459e-08
  7.79454824e-09  3.59092605e-08  1.51320439e-09  3.57779070e-08
  1.06860036e-09  4.92268334e-09  2.07657188e-10  4.90465274e-09
  1.54663175e-09  7.12542089e-09  3.00180057e-10  7.09936595e-09
  7.93409589e-10  3.65545589e-09  1.53879510e-10  3.64210196e-09
  1.45331501e-10  1.54213580e-09  1.28451802e-09  1.73109086e-09
  4.46867163e-11  4.28888226e-10  3.60554393e-10  4.89399289e-10
 -5.55607932e-11 -5.49712984e-10 -4.60764917e-10 -6.24054741e-10
  5.37091334e-11  5.18298413e-10  4.35542076e-10  5.90900830e-10
 -1.33456203e-11 -6.25648598e-11 -4.25790002e-11 -6.24143406e-11
  5.61135164e-11  1.05196239e-10  9.16795167e-11  1.03061153e-10
  1.16662850e-10  3.92990566e-10  2.87851813e-10  3.90170321e-10
  1.38037592e-11  2.03278936e-11  1.95985749e-11  1.97424020e-11
  3.36156707e-12  6.10413607e-13  3.84539816e-11 -9.22618997e-12
  6.82244101e-11  1.57791451e-10  5.61636185e-11 -8.53516070e-13
  1.54045274e+00]
supnorm grad right now is: 1.5404527377844304
Weights right now are: 
[-1.80391798 -3.10507278 -3.77334516 -2.78723883  1.01369876  3.11953646
  4.34465124  4.20232191  0.06689484 -1.03833322  0.63681017 -0.55922399
  0.06176594 -2.31252909  0.47594703 -2.45478849 -1.18079205 -2.08362145
 -0.54779514 -2.26310455 -1.26272261 -2.35644648 -1.5262181  -2.46174971
  1.42508702  0.61279131  0.04172738  0.6150992  -1.1618686  -1.46242741
 -2.29786517 -1.9570188   0.28934352  1.03593905  1.43033859  1.17987553
 -1.04940645 -1.83509405 -1.01691878 -2.15049621  0.36663031  0.3055215
  1.36645644  1.69141592  0.39354613 -1.585326   -0.20941837 -0.93146105
 -0.04610604 -0.0738488  -0.61043669 -0.72252612 -1.54540351 -2.08498804
 -1.29498033 -2.04631633  0.96167059  1.32117979 -0.81240432 -1.60482157
 -0.32050051 -0.04614525 -0.71768508 -1.74727337 25.11709723]
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.676010938042
gradient value of function right now is: [ 6.57170106e-08  7.88459696e-08  7.87669234e-08  7.90919822e-08
 -6.57174566e-08 -7.88465048e-08 -7.87674580e-08 -7.90925191e-08
  7.02275138e-09  3.23228694e-08  1.36901094e-09  3.22037866e-08
  9.65614760e-10  4.44399717e-09  1.88441794e-10  4.42760029e-09
  1.39401938e-09  6.41623996e-09  2.71670516e-10  6.39261110e-09
  7.16390096e-10  3.29749821e-09  1.39500839e-10  3.28536764e-09
  1.33643061e-10  1.39393257e-09  1.16309323e-09  1.56891523e-09
  4.15735401e-11  3.88628168e-10  3.27685074e-10  4.45446437e-10
 -5.11537543e-11 -4.94354767e-10 -4.15431718e-10 -5.63393636e-10
  5.00151817e-11  4.70111060e-10  3.96220757e-10  5.38351121e-10
 -1.21929919e-11 -5.64521715e-11 -3.84936559e-11 -5.63136951e-11
  5.47762924e-11  9.54327371e-11  8.53370262e-11  9.33329328e-11
  1.10776826e-10  3.53294872e-10  2.62081256e-10  3.50506797e-10
  1.38679203e-11  1.86566072e-11  1.86651231e-11  1.80654361e-11
  3.14947097e-12  2.25721154e-13  3.55038481e-11 -1.09994666e-11
  6.22390926e-11  1.38696550e-10  5.15206992e-11 -3.27799064e-12
 -1.72130000e+00]
supnorm grad right now is: 1.7212999996058065
Weights right now are: 
[-1.81771575 -3.12164888 -3.78990453 -2.80386671  1.02749652  3.13611256
  4.3612106   4.21894979  0.06541963 -1.04512561  0.63652264 -0.56599139
  0.06156341 -2.31346154  0.47590751 -2.45571751 -1.18108481 -2.08496942
 -0.54785218 -2.26444757 -1.26287289 -2.35713851 -1.52624736 -2.4624392
  1.42505909  0.6124995   0.04148397  0.61477081 -1.16187726 -1.4625089
 -2.29793382 -1.95711211  0.28935423  1.03604306  1.43042593  1.17999397
 -1.04941686 -1.83519255 -1.01700171 -2.15060888  0.36663292  0.3055333
  1.36646453  1.69142769  0.3935349  -1.58534615 -0.20943621 -0.93148077
 -0.04612912 -0.07392299 -0.61049167 -0.72259973 -1.54540629 -2.08499196
 -1.29498419 -2.04632013  0.96166993  1.32117972 -0.81241183 -1.6048195
 -0.32051349 -0.04617578 -0.71769599 -1.74727291 24.83819829]
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5136.595624285168
gradient value of function right now is: [ 5.49572124e-08  6.57853583e-08  6.57195310e-08  6.59892425e-08
 -5.49569536e-08 -6.57850484e-08 -6.57192215e-08 -6.59889317e-08
  5.85801875e-09  2.69756973e-08  1.12377218e-09  2.68785320e-08
  8.06832705e-10  3.71522090e-09  1.54893101e-10  3.70182600e-09
  1.16364570e-09  5.35857151e-09  2.23181456e-10  5.33927555e-09
  5.99425931e-10  2.76044341e-09  1.14902634e-10  2.75051022e-09
  1.07506669e-10  1.18216353e-09  9.81455559e-10  1.31986551e-09
  3.18696210e-11  3.23060414e-10  2.70044281e-10  3.65357901e-10
 -3.95828414e-11 -4.11109843e-10 -3.42907932e-10 -4.63097891e-10
  3.84122979e-11  3.91459810e-10  3.27101802e-10  4.42348400e-10
 -1.02231304e-11 -4.88857781e-11 -3.31816838e-11 -4.87687076e-11
  3.54121277e-11  7.77889155e-11  6.44128365e-11  7.64707786e-11
  7.89268563e-11  2.98736957e-10  2.13389161e-10  2.97021774e-10
  8.15434970e-12  1.47289952e-11  1.31619046e-11  1.43905294e-11
  2.42811935e-12  9.99040947e-13  2.75796861e-11 -2.59169383e-12
  5.07953234e-11  1.26160100e-10  4.10726468e-11  3.56680726e-12
  8.25572083e+00]
supnorm grad right now is: 8.255720826758013
Weights right now are: 
[-1.83044532 -3.13690716 -3.80514762 -2.81917261  1.0402261   3.15137084
  4.37645369  4.23425568  0.06405932 -1.051383    0.63625653 -0.57222558
  0.06137612 -2.31432301  0.47587083 -2.45657578 -1.18135487 -2.08621175
 -0.547905   -2.26568529 -1.26301177 -2.3577774  -1.5262745  -2.46307572
  1.42503287  0.61222946  0.04125833  0.61446625 -1.16188551 -1.46258439
 -2.29799764 -1.95719897  0.28936431  1.03613872  1.43050651  1.18010335
 -1.04942679 -1.83528391 -1.01707892 -2.15071391  0.36663525  0.30554421
  1.36647195  1.69143858  0.39352357 -1.58536466 -0.20945317 -0.93149886
 -0.0461515  -0.07399121 -0.61054283 -0.72266739 -1.54540926 -2.0849956
 -1.29498798 -2.04632365  0.9616693   1.32117973 -0.81241882 -1.60481704
 -0.32052554 -0.04620279 -0.71770603 -1.74727201 25.54205042]
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.718051828662
gradient value of function right now is: [ 5.28486264e-08  6.31493508e-08  6.30872977e-08  6.33454876e-08
 -5.28483583e-08 -6.31490304e-08 -6.30869776e-08 -6.33451662e-08
  5.63595601e-09  2.59143264e-08  1.09634057e-09  2.58189845e-08
  7.78629966e-10  3.57990620e-09  1.51625679e-10  3.56671610e-09
  1.11989778e-09  5.14943941e-09  2.17785168e-10  5.13050184e-09
  5.77473382e-10  2.65543885e-09  1.12211951e-10  2.64568355e-09
  1.08339431e-10  1.13228720e-09  9.44654051e-10  1.27402645e-09
  3.34608555e-11  3.13733371e-10  2.64462991e-10  3.59417165e-10
 -4.07857739e-11 -3.95197785e-10 -3.32032254e-10 -4.50191983e-10
  4.03377394e-11  3.80294086e-10  3.20434124e-10  4.35273414e-10
 -1.00446291e-11 -4.62150491e-11 -3.15525351e-11 -4.60980989e-11
  4.36493000e-11  7.65134523e-11  6.82731220e-11  7.48403762e-11
  8.87182457e-11  2.84569106e-10  2.10819337e-10  2.82346172e-10
  1.10295262e-11  1.49666227e-11  1.49204300e-11  1.44964801e-11
  2.56340116e-12  2.19250159e-13  2.84937616e-11 -8.62059244e-12
  5.02265096e-11  1.12368330e-10  4.15077298e-11 -2.40185283e-12
 -1.38475832e+00]
supnorm grad right now is: 1.3847583209483556
Weights right now are: 
[-1.84156629 -3.1502094  -3.81843671 -2.83251619  1.05134708  3.16467308
  4.38974279  4.24759928  0.06287279 -1.05684033  0.63602602 -0.57766286
  0.06121241 -2.31507591  0.47583899 -2.45732591 -1.18159058 -2.08729588
 -0.54795078 -2.26676544 -1.26313321 -2.35833601 -1.52629806 -2.46363228
  1.42501027  0.61199152  0.0410599   0.61419879 -1.1618925  -1.46265023
 -2.29805315 -1.95727436  0.28937285  1.03622194  1.43057641  1.18019807
 -1.04943522 -1.8353637  -1.01714616 -2.15080519  0.36663725  0.30555394
  1.36647853  1.69144829  0.39351446 -1.58538059 -0.20946741 -0.93151446
 -0.04616997 -0.07405107 -0.61058708 -0.72272681 -1.54541164 -2.08499868
 -1.29499112 -2.04632664  0.96166877  1.32117967 -0.8124247  -1.60481558
 -0.32053625 -0.04622628 -0.7177146  -1.74727184 24.91099225]
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5137.301997909416
gradient value of function right now is: [ 4.98451733e-08  5.94653845e-08  5.94077320e-08  5.96501745e-08
 -4.98452422e-08 -5.94654667e-08 -5.94078141e-08 -5.96502570e-08
  5.31518344e-09  2.44147696e-08  1.04158405e-09  2.43238782e-08
  7.36160684e-10  3.38118194e-09  1.44442173e-10  3.36857236e-09
  1.05659026e-09  4.85346613e-09  2.06981937e-10  4.83540654e-09
  5.45417818e-10  2.50554434e-09  1.06746977e-10  2.49623308e-09
  1.04912975e-10  1.06721378e-09  8.92841970e-10  1.20596450e-09
  3.30854876e-11  2.97634525e-10  2.52112802e-10  3.43449068e-10
 -3.98592161e-11 -3.72233340e-10 -3.14041955e-10 -4.26693827e-10
  3.99094205e-11  3.60988286e-10  3.05639599e-10  4.16163969e-10
 -9.52307231e-12 -4.33024263e-11 -2.96146476e-11 -4.31896977e-11
  4.63787121e-11  7.28647526e-11  6.76907949e-11  7.10897042e-11
  9.05467833e-11  2.66931503e-10  2.01828689e-10  2.64557176e-10
  1.22355598e-11  1.44833308e-11  1.53121244e-11  1.39686121e-11
  2.54095514e-12 -1.82490796e-13  2.78113368e-11 -1.11644477e-11
  4.81898559e-11  1.01073865e-10  4.00759674e-11 -5.12468742e-12
 -6.06758517e+00]
supnorm grad right now is: 6.067585170641836
Weights right now are: 
[-1.85147343 -3.16203838 -3.83025408 -2.84438183  1.06125423  3.17650208
  4.40156018  4.25946493  0.06181712 -1.06169481  0.63582159 -0.58249958
  0.06106647 -2.315747    0.4758107  -2.45799454 -1.18180041 -2.08826079
 -0.5479914  -2.26772682 -1.2632415  -2.35883398 -1.52631901 -2.46412843
  1.42499017  0.61177833  0.04088228  0.61395948 -1.16189864 -1.46270895
 -2.29810253 -1.95734138  0.28938034  1.03629587  1.43063841  1.18028203
 -1.04944263 -1.83543491 -1.01720604 -2.15088641  0.36663912  0.30556269
  1.3664845   1.69145701  0.39350676 -1.5853948  -0.20947985 -0.93152837
 -0.04618589 -0.07410458 -0.61062632 -0.72277991 -1.54541356 -2.08500145
 -1.2949938  -2.04632933  0.9616683   1.32117958 -0.81243    -1.60481419
 -0.32054558 -0.04624791 -0.71772238 -1.74727156 24.51106894]
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.505406279174
gradient value of function right now is: [ 4.31429329e-08  5.13829114e-08  5.13330657e-08  5.15416396e-08
 -4.31426219e-08 -5.13825409e-08 -5.13326956e-08 -5.15412680e-08
  4.58959706e-09  2.10945134e-08  8.86011141e-10  2.10176565e-08
  6.36449883e-10  2.92504498e-09  1.22978126e-10  2.91437457e-09
  9.13031497e-10  4.19651560e-09  1.76212095e-10  4.18123135e-09
  4.72197596e-10  2.17043191e-09  9.10696252e-11  2.16253409e-09
  8.75264511e-11  9.35078734e-10  7.78511237e-10  1.04856936e-09
  2.64164855e-11  2.56060531e-10  2.15072869e-10  2.91708549e-10
 -3.21249759e-11 -3.20651439e-10 -2.68570361e-10 -3.63498423e-10
  3.19080913e-11  3.10980732e-10  2.61097805e-10  3.53963947e-10
 -8.32020003e-12 -3.86633412e-11 -2.63639806e-11 -3.85644464e-11
  3.21785709e-11  6.17038088e-11  5.33780771e-11  6.04815900e-11
  6.79437003e-11  2.33511309e-10  1.70291664e-10  2.31896318e-10
  7.86066850e-12  1.19258296e-11  1.13670260e-11  1.15935387e-11
  2.04300045e-12  4.56832174e-13  2.25841912e-11 -4.85933297e-12
  4.06955284e-11  9.53206864e-11  3.33013169e-11  1.09399301e-13
  2.88361580e+00]
supnorm grad right now is: 2.8836157991483296
Weights right now are: 
[-1.86096308 -3.17335044 -3.84155513 -2.85572889  1.07074388  3.18781414
  4.41286122  4.27081199  0.06080684 -1.06633839  0.63562623 -0.5871262
  0.06092652 -2.31639022  0.47578361 -2.45863541 -1.18200132 -2.08918425
 -0.54803024 -2.26864692 -1.26334532 -2.35931122 -1.52633907 -2.46460393
  1.42497088  0.61157328  0.04071141  0.61372934 -1.16190449 -1.46276526
 -2.29814988 -1.95740562  0.28938747  1.03636651  1.43069763  1.18036221
 -1.04944969 -1.83550329 -1.01726349 -2.15096433  0.36664097  0.3055711
  1.36649027  1.69146539  0.39349948 -1.58540845 -0.20949175 -0.93154172
 -0.04620099 -0.07415587 -0.61066391 -0.72283082 -1.54541529 -2.08500412
 -1.29499633 -2.04633191  0.96166785  1.32117952 -0.81243493 -1.60481267
 -0.3205545  -0.04626993 -0.71772964 -1.7472711  25.19813098]
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5137.338511043338
gradient value of function right now is: [ 4.23528374e-08  5.03768638e-08  5.03287761e-08  5.05328870e-08
 -4.23528906e-08 -5.03769271e-08 -5.03288393e-08 -5.05329505e-08
  4.50819338e-09  2.06917787e-08  8.82334560e-10  2.06147876e-08
  6.26803957e-10  2.87666365e-09  1.22829669e-10  2.86594137e-09
  8.97134777e-10  4.11778953e-09  1.75523593e-10  4.10247551e-09
  4.64270565e-10  2.13110476e-09  9.07506498e-11  2.12318920e-09
  8.97675172e-11  9.13442769e-10  7.64211421e-10  1.03214978e-09
  2.81809571e-11  2.53625614e-10  2.14834193e-10  2.92642974e-10
 -3.36930531e-11 -3.14759238e-10 -2.65555657e-10 -3.60789391e-10
  3.40454774e-11  3.08087395e-10  2.60848744e-10  3.55148917e-10
 -8.25058302e-12 -3.72725603e-11 -2.55238457e-11 -3.71726587e-11
  3.93429660e-11  6.17834058e-11  5.74065299e-11  6.02768746e-11
  7.69483718e-11  2.26971720e-10  1.71593814e-10  2.24954348e-10
  1.03832960e-11  1.22961868e-11  1.29967424e-11  1.18593350e-11
  2.18166829e-12 -1.47073271e-13  2.36267179e-11 -9.47054914e-12
  4.10698188e-11  8.61881574e-11  3.41304021e-11 -4.33829870e-12
 -5.99245419e+00]
supnorm grad right now is: 5.992454186354057
Weights right now are: 
[-1.86945857 -3.18346254 -3.85165749 -2.86587225  1.07923937  3.19792624
  4.42296359  4.28095536  0.05990281 -1.07049055  0.63545099 -0.59126308
  0.06080101 -2.31696661  0.47575925 -2.45920967 -1.18218119 -2.09001041
 -0.54806509 -2.26947004 -1.26343838 -2.35973866 -1.52635708 -2.4650298
  1.42495346  0.61138926  0.04055787  0.61352259 -1.16190989 -1.46281574
 -2.29819249 -1.95746347  0.28939396  1.03642954  1.43075063  1.18043398
 -1.04945621 -1.83556466 -1.01731528 -2.1510346   0.36664247  0.30557865
  1.36649536  1.69147293  0.39349234 -1.58542044 -0.20950261 -0.93155345
 -0.04621511 -0.0742016  -0.61069774 -0.7228762  -1.54541721 -2.08500647
 -1.29499876 -2.04633419  0.96166745  1.32117951 -0.81243927 -1.60481115
 -0.32056238 -0.04628979 -0.71773591 -1.74727061 24.49774672]
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.615310118848
gradient value of function right now is: [ 3.74642232e-08  4.45019357e-08  4.44593895e-08  4.46390376e-08
 -3.74642778e-08 -4.45020005e-08 -4.44594543e-08 -4.46391026e-08
  3.97929161e-09  1.82751489e-08  7.68171308e-10  1.82084864e-08
  5.53801399e-10  2.54320899e-09  1.07007378e-10  2.53392038e-09
  7.92435105e-10  3.63937912e-09  1.52931408e-10  3.62610876e-09
  4.10733044e-10  1.88644018e-09  7.92108856e-11  1.87956803e-09
  7.67243806e-11  8.16805918e-10  6.80305723e-10  9.16435449e-10
  2.31387406e-11  2.23049662e-10  1.87468010e-10  2.54336310e-10
 -2.79146337e-11 -2.77257605e-10 -2.32356183e-10 -3.14557724e-10
  2.79862659e-11  2.71255154e-10  2.27891550e-10  3.09030225e-10
 -7.33983145e-12 -3.39002342e-11 -2.31412679e-11 -3.38118480e-11
  2.84163743e-11  5.35557268e-11  4.65959005e-11  5.24761865e-11
  5.96967245e-11  2.02733422e-10  1.48234686e-10  2.01304498e-10
  6.99591672e-12  1.03861353e-11  9.98177529e-12  1.00907725e-11
  1.80120598e-12  3.64935725e-13  1.96872368e-11 -4.51625294e-12
  3.54849821e-11  8.25302946e-11  2.90511854e-11 -1.74851158e-13
  2.17961902e+00]
supnorm grad right now is: 2.1796190199730443
Weights right now are: 
[-1.87711631 -3.19256566 -3.86075191 -2.8750035   1.08689711  3.20702935
  4.432058    4.2900866   0.05908852 -1.07422931  0.63529242 -0.59498805
  0.06068774 -2.31748665  0.47573717 -2.45972778 -1.18234329 -2.09075469
 -0.54809665 -2.27021157 -1.26352232 -2.36012411 -1.52637341 -2.46541383
  1.42493742  0.61122337  0.04041939  0.61333567 -1.16191485 -1.46286151
 -2.29823114 -1.95751604  0.2893999   1.03648636  1.43079844  1.18049885
 -1.04946221 -1.83562026 -1.01736221 -2.15109842  0.36664395  0.3055855
  1.36650002  1.69147977  0.39348573 -1.58543151 -0.20951262 -0.93156427
 -0.04622842 -0.07424282 -0.61072842 -0.72291711 -1.54541894 -2.08500863
 -1.295001   -2.04633628  0.96166707  1.32117948 -0.81244344 -1.60480996
 -0.32056982 -0.04630411 -0.717742   -1.74727039 25.16733072]
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.77551959817
gradient value of function right now is: [ 3.55177660e-08  4.21377765e-08  4.20978570e-08  4.22675524e-08
 -3.55180185e-08 -4.21380762e-08 -4.20981564e-08 -4.22678531e-08
  3.77081975e-09  1.73071541e-08  7.30381382e-10  1.72436748e-08
  5.25813297e-10  2.41319029e-09  1.01950297e-10  2.40432690e-09
  7.51292748e-10  3.44832620e-09  1.45476490e-10  3.43568371e-09
  3.89770667e-10  1.78908131e-09  7.54152178e-11  1.78252878e-09
  7.37790034e-11  7.75037438e-10  6.46365830e-10  8.71365651e-10
  2.24586187e-11  2.12160990e-10  1.78715485e-10  2.42744397e-10
 -2.69054173e-11 -2.62444113e-10 -2.20367573e-10 -2.98635721e-10
  2.71766207e-11  2.58145669e-10  2.17359492e-10  2.95090460e-10
 -7.03906119e-12 -3.21062595e-11 -2.19614734e-11 -3.20204271e-11
  2.86460801e-11  5.10406105e-11  4.52833216e-11  4.99439928e-11
  5.88967616e-11  1.91730571e-10  1.41557284e-10  1.90271534e-10
  7.20490583e-12  9.98660540e-12  9.86815963e-12  9.68032101e-12
  1.75603439e-12  2.10307955e-13  1.90279075e-11 -5.39018373e-12
  3.39366650e-11  7.66406193e-11  2.79386526e-11 -1.21586890e-12
 -4.05578447e-01]
supnorm grad right now is: 0.4055784474955385
Weights right now are: 
[-1.88490459 -3.20181212 -3.86998958 -2.88427847  1.09468538  3.21627581
  4.44129567  4.29936156  0.05826113 -1.07802707  0.63513188 -0.59877183
  0.06057247 -2.31801571  0.47571478 -2.4602549  -1.18250808 -2.09151112
 -0.54812862 -2.27096522 -1.26360776 -2.36051631 -1.52638997 -2.46580459
  1.4249212   0.61105385  0.04027793  0.61314494 -1.16191982 -1.46290803
 -2.29827037 -1.95756935  0.28940586  1.03654398  1.43084687  1.1805645
 -1.04946822 -1.83567684 -1.01740991 -2.1511632   0.36664546  0.3055925
  1.3665048   1.69148674  0.39347925 -1.5854427  -0.20952265 -0.93157522
 -0.04624154 -0.0742848  -0.61075954 -0.72295875 -1.54542061 -2.08501083
 -1.2950032  -2.04633841  0.96166669  1.32117944 -0.81244762 -1.60480872
 -0.32057724 -0.04632115 -0.71774815 -1.74726999 24.94478301]
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.735225224184
gradient value of function right now is: [ 3.35137636e-08  3.97158183e-08  3.96784506e-08  3.98380197e-08
 -3.35137149e-08 -3.97157606e-08 -3.96783929e-08 -3.98379618e-08
  3.55570794e-09  1.63133750e-08  6.89242883e-10  1.62534417e-08
  4.96637700e-10  2.27838302e-09  9.63695018e-11  2.27000060e-09
  7.08789193e-10  3.25195395e-09  1.37350034e-10  3.24001185e-09
  3.68057618e-10  1.68875058e-09  7.12664732e-11  1.68255556e-09
  7.00528362e-11  7.32827296e-10  6.11429021e-10  8.24446950e-10
  2.13632697e-11  2.00533578e-10  1.69045030e-10  2.29690228e-10
 -2.54858801e-11 -2.47184992e-10 -2.07687094e-10 -2.81538914e-10
  2.58652825e-11  2.44133855e-10  2.05711309e-10  2.79374733e-10
 -6.69404675e-12 -3.03815268e-11 -2.07992808e-11 -3.02993006e-11
  2.75419752e-11  4.82057883e-11  4.30405803e-11  4.71498411e-11
  5.62776106e-11  1.80786421e-10  1.33892914e-10  1.79379497e-10
  6.97452068e-12  9.46129553e-12  9.43299085e-12  9.16459063e-12
  1.67590975e-12  1.59026285e-13  1.80551874e-11 -5.41473045e-12
  3.21323442e-11  7.19019212e-11  2.64803140e-11 -1.45750525e-12
 -1.17923721e+00]
supnorm grad right now is: 1.1792372077681703
Weights right now are: 
[-1.89191225 -3.21012191 -3.87829155 -2.89261387  1.10169304  3.2245856
  4.44959764  4.30769696  0.05751697 -1.08144043  0.6349874  -0.60217257
  0.0604686  -2.31849209  0.47569459 -2.46072951 -1.18265637 -2.09219133
 -0.54815739 -2.27164291 -1.26368472 -2.36086934 -1.52640489 -2.46615632
  1.42490658  0.6109009   0.04015017  0.61297281 -1.16192438 -1.46294987
 -2.29830579 -1.95761748  0.28941125  1.03659563  1.43089039  1.18062349
 -1.04947374 -1.8357278  -1.01745303 -2.15122177  0.36664667  0.30559883
  1.36650905  1.69149306  0.39347303 -1.58545252 -0.20953177 -0.93158484
 -0.0462537  -0.07432239 -0.61078769 -0.72299607 -1.54542238 -2.08501274
 -1.29500531 -2.04634026  0.96166633  1.32117944 -0.8124513  -1.60480767
 -0.32058412 -0.04633652 -0.71775354 -1.74726983 24.90854638]
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.625244073769
gradient value of function right now is: [ 3.09548542e-08  3.66441570e-08  3.66097660e-08  3.67566087e-08
 -3.09545938e-08 -3.66438487e-08 -3.66094580e-08 -3.67562994e-08
  3.28014006e-09  1.50499713e-08  6.32364319e-10  1.49950939e-08
  4.58704101e-10  2.10450072e-09  8.85140628e-11  2.09681735e-09
  6.54231802e-10  3.00181214e-09  1.26091108e-10  2.99087074e-09
  3.40094067e-10  1.56052467e-09  6.55003253e-11  1.55484199e-09
  6.39435653e-11  6.80605603e-10  5.66913372e-10  7.63647123e-10
  1.91874759e-11  1.84897801e-10  1.55417337e-10  2.10845126e-10
 -2.29320364e-11 -2.27689497e-10 -1.90834106e-10 -2.58336098e-10
  2.32493550e-11  2.25268420e-10  1.89274536e-10  2.56653665e-10
 -6.19170703e-12 -2.84281089e-11 -1.94282102e-11 -2.83520929e-11
  2.34778377e-11  4.41282905e-11  3.84313281e-11  4.32353489e-11
  4.94009662e-11  1.67585549e-10  1.22569101e-10  1.66401889e-10
  5.78821501e-12  8.57398208e-12  8.24788440e-12  8.32942733e-12
  1.50951512e-12  3.04589036e-13  1.62732011e-11 -3.75693635e-12
  2.94320871e-11  6.83974312e-11  2.40783659e-11 -1.59722433e-13
  2.10461305e+00]
supnorm grad right now is: 2.1046130467594195
Weights right now are: 
[-1.89836588 -3.21776609 -3.88592853 -2.90028153  1.10814667  3.23222978
  4.45723462  4.31536462  0.0568328  -1.08458001  0.63485533 -0.60530071
  0.060373   -2.31893076  0.47567612 -2.46116658 -1.18279279 -2.09281735
 -0.54818372 -2.27226665 -1.2637556  -2.3611946  -1.52641856 -2.4664804
  1.4248932   0.61075935  0.04003224  0.61281384 -1.16192837 -1.46298846
 -2.2983382  -1.95766147  0.28941605  1.03664321  1.43093025  1.18067748
 -1.04947858 -1.83577479 -1.01749248 -2.15127528  0.36664805  0.30560472
  1.36651312  1.69149893  0.3934682  -1.58546188 -0.20953985 -0.931594
 -0.04626402 -0.07435736 -0.6108133  -0.72303078 -1.54542348 -2.08501457
 -1.295007   -2.04634204  0.96166602  1.32117937 -0.81245477 -1.60480678
 -0.32059026 -0.04635086 -0.71775864 -1.74726965 25.09267687]
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5137.70368225768
gradient value of function right now is: [ 3.07224201e-08  3.63368466e-08  3.63031758e-08  3.64486104e-08
 -3.07222956e-08 -3.63366994e-08 -3.63030287e-08 -3.64484627e-08
  3.25703484e-09  1.49276571e-08  6.35096108e-10  1.48722775e-08
  4.56407702e-10  2.09163751e-09  8.91021748e-11  2.08386495e-09
  6.49833191e-10  2.97839468e-09  1.26667273e-10  2.96735080e-09
  3.37938281e-10  1.54897291e-09  6.58128730e-11  1.54323635e-09
  6.57708818e-11  6.72493663e-10  5.62402035e-10  7.59302415e-10
  2.03855914e-11  1.84829456e-10  1.56435846e-10  2.12983053e-10
 -2.40356539e-11 -2.25996412e-10 -1.90544513e-10 -2.58757530e-10
  2.47021594e-11  2.25204355e-10  1.90522654e-10  2.59263771e-10
 -6.22405617e-12 -2.77836507e-11 -1.90720638e-11 -2.77052511e-11
  2.79101131e-11  4.45414493e-11  4.11354774e-11  4.34694519e-11
  5.51344520e-11  1.64967513e-10  1.24278831e-10  1.63530141e-10
  7.31925787e-12  8.86789686e-12  9.27836170e-12  8.55844901e-12
  1.60451069e-12 -5.60672036e-14  1.70410284e-11 -6.56460460e-12
  2.98722105e-11  6.34797662e-11  2.47830733e-11 -2.86142955e-12
 -5.14800120e+00]
supnorm grad right now is: 5.1480012013058625
Weights right now are: 
[-1.90448794 -3.22501077 -3.89316648 -2.9075485   1.11426872  3.23947446
  4.46447256  4.32263159  0.05618353 -1.08755619  0.63472884 -0.60826585
  0.06028209 -2.31934743  0.47565839 -2.4615817  -1.1829223  -2.093411
 -0.54820894 -2.2728581  -1.26382292 -2.36150321 -1.52643166 -2.46678787
  1.42488017  0.61062543  0.03992026  0.61266272 -1.16193241 -1.46302524
 -2.29836933 -1.95770383  0.28942082  1.03668826  1.43096823  1.18072904
 -1.04948348 -1.8358196  -1.01753038 -2.15132684  0.36664926  0.30561025
  1.36651691  1.69150445  0.39346267 -1.58547072 -0.20954802 -0.93160263
 -0.04627491 -0.07439023 -0.61083804 -0.72306336 -1.54542494 -2.08501632
 -1.29500885 -2.04634373  0.9616657   1.32117936 -0.81245819 -1.60480555
 -0.32059626 -0.04636382 -0.7177636  -1.74726909 24.62670219]
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.7577525197485
gradient value of function right now is: [ 2.79781555e-08  3.30571527e-08  3.30264928e-08  3.31584247e-08
 -2.79782203e-08 -3.30572293e-08 -3.30265693e-08 -3.31585015e-08
  2.96102685e-09  1.35766554e-08  5.71492046e-10  1.35270238e-08
  4.15296340e-10  1.90405962e-09  8.02318598e-11  1.89708990e-09
  5.91157921e-10  2.71058480e-09  1.14062435e-10  2.70067991e-09
  3.07795167e-10  1.41137343e-09  5.93443751e-11  1.40622111e-09
  5.83964179e-11  6.17433961e-10  5.14643281e-10  6.93480167e-10
  1.75678485e-11  1.67591040e-10  1.41029749e-10  1.91433469e-10
 -2.08462753e-11 -2.05115650e-10 -1.72083942e-10 -2.33068754e-10
  2.13069114e-11  2.04380133e-10  1.71917012e-10  2.33246929e-10
 -5.67705355e-12 -2.58314357e-11 -1.76817464e-11 -2.57603379e-11
  2.18744815e-11  3.99426652e-11  3.51367187e-11  3.91068050e-11
  4.55633165e-11  1.51302556e-10  1.11203482e-10  1.50190126e-10
  5.45349813e-12  7.80063331e-12  7.61006026e-12  7.56941948e-12
  1.39079821e-12  2.24999033e-13  1.48565378e-11 -3.82912739e-12
  2.67599472e-11  6.12771377e-11  2.19531695e-11 -5.61749319e-13
  8.49086958e-01]
supnorm grad right now is: 0.8490869578451379
Weights right now are: 
[-1.91051699 -3.23213802 -3.9002871  -2.9146976   1.12029778  3.24660171
  4.47159319  4.32978069  0.05554524 -1.09048341  0.63460565 -0.6111824
  0.06019265 -2.31975762  0.47564111 -2.46199039 -1.18304969 -2.09399525
 -0.54823352 -2.27344022 -1.26388922 -2.36180729 -1.52644445 -2.46709083
  1.42486758  0.61049259  0.03980955  0.61251348 -1.16193616 -1.46306137
 -2.29839968 -1.95774504  0.2894253   1.03673255  1.43100535  1.18077932
 -1.04948802 -1.83586364 -1.01756736 -2.15137702  0.36665057  0.30561579
  1.36652075  1.69150997  0.39345811 -1.58547946 -0.20955557 -0.93161119
 -0.04628465 -0.07442288 -0.61086199 -0.72309577 -1.54542599 -2.08501804
 -1.29501043 -2.0463454   0.96166541  1.3211793  -0.81246135 -1.60480472
 -0.32060187 -0.04637795 -0.7177683  -1.74726894 25.0409219 ]
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.638267933382
gradient value of function right now is: [ 2.71561918e-08  3.20596882e-08  3.20301773e-08  3.21579315e-08
 -2.71559108e-08 -3.20593563e-08 -3.20298457e-08 -3.21575986e-08
  2.87339732e-09  1.31676176e-08  5.56842793e-10  1.31191722e-08
  4.03617661e-10  1.84948368e-09  7.83013066e-11  1.84266930e-09
  5.73892480e-10  2.62997771e-09  1.11179819e-10  2.62030610e-09
  2.98976021e-10  1.37019020e-09  5.78735912e-11  1.36515686e-09
  5.75204715e-11  5.99024327e-10  5.00052465e-10  6.74391948e-10
  1.75028653e-11  1.63180583e-10  1.37672651e-10  1.87125085e-10
 -2.06350188e-11 -1.98905270e-10 -1.67246121e-10 -2.26783168e-10
  2.12336688e-11  1.99060971e-10  1.67871539e-10  2.28060050e-10
 -5.56518713e-12 -2.49805610e-11 -1.71364704e-11 -2.49105302e-11
  2.27430416e-11  3.90082219e-11  3.50911370e-11  3.81317753e-11
  4.62364244e-11  1.46417194e-10  1.08817960e-10  1.45246509e-10
  5.80131937e-12  7.69436648e-12  7.74698312e-12  7.44625626e-12
  1.39127873e-12  9.34424294e-14  1.47348553e-11 -4.71523706e-12
  2.62155222e-11  5.80943931e-11  2.16239018e-11 -1.48560221e-12
 -2.03049411e+00]
supnorm grad right now is: 2.030494107173778
Weights right now are: 
[-1.91594385 -3.23854765 -3.9066908  -2.92112686  1.12572464  3.25301134
  4.47799689  4.33620996  0.05497102 -1.09311587  0.63449472 -0.61380522
  0.06011206 -2.32012703  0.47562553 -2.46235846 -1.18316435 -2.09452092
 -0.54825567 -2.27396397 -1.26394893 -2.36208108 -1.52645597 -2.46736363
  1.42485621  0.61037279  0.03970967  0.61237885 -1.16193959 -1.46309392
 -2.29842708 -1.95778224  0.28942936  1.03677232  1.43103872  1.18082454
 -1.04949218 -1.83590333 -1.01760076 -2.15142235  0.36665168  0.30562081
  1.36652419  1.69151497  0.39345383 -1.58548723 -0.20956244 -0.93161879
 -0.04629356 -0.07445222 -0.61088362 -0.72312488 -1.54542706 -2.08501956
 -1.29501193 -2.04634687  0.96166513  1.32117926 -0.81246436 -1.60480392
 -0.32060718 -0.04638966 -0.71777272 -1.7472688  24.83487411]
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.5046582928735
gradient value of function right now is: [ 2.50833502e-08  2.95865949e-08  2.95593585e-08  2.96769754e-08
 -2.50832777e-08 -2.95865094e-08 -2.95592731e-08 -2.96768896e-08
  2.65036655e-09  1.21489096e-08  5.09609379e-10  1.21047056e-08
  3.72610756e-10  1.70789489e-09  7.17095603e-11  1.70167321e-09
  5.29653877e-10  2.42790669e-09  1.01812643e-10  2.41907614e-09
  2.76206468e-10  1.26617199e-09  5.30569894e-11  1.26157097e-09
  5.21147571e-11  5.56876928e-10  4.63711923e-10  6.24449931e-10
  1.54967305e-11  1.50230433e-10  1.26204110e-10  1.71142718e-10
 -1.83537355e-11 -1.83196796e-10 -1.53469089e-10 -2.07677556e-10
  1.88151284e-11  1.83400284e-10  1.54007397e-10  2.08745904e-10
 -5.12355686e-12 -2.34500661e-11 -1.60383080e-11 -2.33858200e-11
  1.86420299e-11  3.55721727e-11  3.08290811e-11  3.48622620e-11
  3.95860884e-11  1.35949032e-10  9.91699242e-11  1.35007269e-10
  4.56866774e-12  6.90895522e-12  6.59466734e-12  6.71562210e-12
  1.23535988e-12  2.78189949e-13  1.31290394e-11 -2.84134831e-12
  2.39127509e-11  5.59642627e-11  1.95156314e-11  6.95926197e-14
  2.89151353e+00]
supnorm grad right now is: 2.891513532882447
Weights right now are: 
[-1.92121429 -3.24476709 -3.9129045  -2.9273653   1.13099509  3.25923079
  4.48421059  4.3424484   0.054414   -1.09566976  0.63438767 -0.61634985
  0.06003381 -2.32048578  0.47561048 -2.46271591 -1.18327564 -2.09503117
 -0.54827705 -2.27447237 -1.26400695 -2.36234708 -1.52646711 -2.46762866
  1.42484527  0.61025592  0.03961238  0.61224779 -1.1619428  -1.4631255
 -2.29845355 -1.95781814  0.28943319  1.03681088  1.43107099  1.1808682
 -1.04949607 -1.83594187 -1.01763305 -2.1514661   0.36665286  0.3056257
  1.36652759  1.69151985  0.39345011 -1.58549484 -0.20956892 -0.93162623
 -0.04630167 -0.07448087 -0.61090445 -0.72315331 -1.54542785 -2.08502106
 -1.29501327 -2.04634832  0.96166486  1.32117922 -0.81246719 -1.60480307
 -0.32061221 -0.04640208 -0.71777689 -1.74726855 25.18061665]
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5136.835018354934
gradient value of function right now is: [ 2.55725340e-08  3.01438214e-08  3.01164703e-08  3.02362424e-08
 -2.55722373e-08 -3.01434716e-08 -3.01161208e-08 -3.02358916e-08
  2.70465701e-09  1.23813994e-08  5.28137192e-10  1.23352871e-08
  3.81017058e-10  1.74407062e-09  7.44944895e-11  1.73756373e-09
  5.40619913e-10  2.47492228e-09  1.05526151e-10  2.46570992e-09
  2.81944398e-10  1.29080524e-09  5.49820201e-11  1.28600676e-09
  5.56454599e-11  5.63638335e-10  4.71863100e-10  6.37357626e-10
  1.72986205e-11  1.54543899e-10  1.31044793e-10  1.78552668e-10
 -2.01447941e-11 -1.86943675e-10 -1.57868223e-10 -2.14528912e-10
  2.09995592e-11  1.88639289e-10  1.59882946e-10  2.17738306e-10
 -5.28366509e-12 -2.33745850e-11 -1.60715246e-11 -2.33065187e-11
  2.41879638e-11  3.70727613e-11  3.47751442e-11  3.61454517e-11
  4.71938881e-11  1.37052122e-10  1.04028680e-10  1.35805684e-10
  6.45251727e-12  7.43244086e-12  7.95556082e-12  7.16152889e-12
  1.37596762e-12 -1.23683459e-13  1.43500006e-11 -6.02933565e-12
  2.51097703e-11  5.20487798e-11  2.08380044e-11 -2.89501932e-12
 -6.80961027e+00]
supnorm grad right now is: 6.809610266840565
Weights right now are: 
[-1.92617561 -3.25061728 -3.91874932 -2.93323338  1.1359564   3.26508098
  4.49005542  4.34831648  0.05388969 -1.09807214  0.63428646 -0.61874342
  0.05996004 -2.32082377  0.47559622 -2.46305265 -1.18338044 -2.09551135
 -0.54829727 -2.27495079 -1.2640616  -2.36259754 -1.52647765 -2.4678782
  1.42483485  0.6101459   0.03952064  0.6121242  -1.16194596 -1.46315527
 -2.29847864 -1.9578522   0.2894369   1.03684709  1.4311014   1.18090939
 -1.04949991 -1.83597821 -1.01766368 -2.15150767  0.36665381  0.30563033
  1.36653073  1.69152447  0.39344607 -1.58550182 -0.20957519 -0.93163306
 -0.04630989 -0.07450765 -0.61092421 -0.7231799  -1.54542893 -2.08502242
 -1.29501467 -2.04634964  0.9616646   1.3211792  -0.81246985 -1.60480235
 -0.32061704 -0.04641265 -0.71778082 -1.74726846 24.46303698]
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.1827024091
gradient value of function right now is: [ 2.27972567e-08  2.68494030e-08  2.68248648e-08  2.69312272e-08
 -2.27971610e-08 -2.68492903e-08 -2.68247522e-08 -2.69311141e-08
  2.40537219e-09  1.10225715e-08  4.61268688e-10  1.09825929e-08
  3.38922565e-10  1.55302029e-09  6.50491249e-11  1.54738115e-09
  4.81136984e-10  2.20484212e-09  9.22407703e-11  2.19684817e-09
  2.51254569e-10  1.15143825e-09  4.81372457e-11  1.14726714e-09
  4.72673940e-11  5.08644093e-10  4.23277373e-10  5.69754663e-10
  1.39360796e-11  1.36571945e-10  1.14600423e-10  1.55305536e-10
 -1.64658857e-11 -1.65940095e-10 -1.38878392e-10 -1.87824139e-10
  1.69361588e-11  1.66878613e-10  1.39976647e-10  1.89604603e-10
 -4.68728025e-12 -2.15281303e-11 -1.47173982e-11 -2.14689834e-11
  1.63548674e-11  3.21725478e-11  2.76061697e-11  3.15509696e-11
  3.52196033e-11  1.23737961e-10  8.98185758e-11  1.22914462e-10
  3.95907728e-12  6.22667221e-12  5.85598015e-12  6.05938743e-12
  1.11830088e-12  2.99152657e-13  1.18245372e-11 -2.22466875e-12
  2.17073244e-11  5.14982325e-11  1.76547712e-11  4.09072198e-13
  4.25619654e+00]
supnorm grad right now is: 4.256196543889612
Weights right now are: 
[-1.93088881 -3.25617077 -3.92429774 -2.93880383  1.1406696   3.27063446
  4.49560384  4.35388693  0.0533919  -1.10035254  0.63419028 -0.62101546
  0.05988992 -2.32114498  0.47558266 -2.46337269 -1.18347997 -2.09596735
 -0.5483165  -2.27540512 -1.26411355 -2.36283553 -1.52648767 -2.46811532
  1.42482488  0.61004127  0.03943339  0.61200657 -1.16194897 -1.4631836
 -2.29850252 -1.95788464  0.28944043  1.03688147  1.43113028  1.18094852
 -1.04950356 -1.83601281 -1.01769283 -2.15154724  0.36665477  0.30563473
  1.36653373  1.69152886  0.39344225 -1.58550853 -0.20958119 -0.93163962
 -0.04631773 -0.07453312 -0.61094301 -0.72320516 -1.54542991 -2.08502374
 -1.29501599 -2.04635092  0.96166435  1.32117917 -0.81247244 -1.60480157
 -0.32062169 -0.04642217 -0.71778463 -1.74726826 25.30154436]
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.45299739397
gradient value of function right now is: [ 2.20461575e-08  2.59470077e-08  2.59234138e-08  2.60260535e-08
 -2.20458637e-08 -2.59466617e-08 -2.59230681e-08 -2.60257064e-08
  2.32512960e-09  1.06514721e-08  4.46499925e-10  1.06127483e-08
  3.28007306e-10  1.50252210e-09  6.30437966e-11  1.49705316e-09
  4.65275274e-10  2.13147837e-09  8.93226337e-11  2.12373231e-09
  2.43104815e-10  1.11373864e-09  4.66387213e-11  1.10969482e-09
  4.60139475e-11  4.92275506e-10  4.09889408e-10  5.51910319e-10
  1.36191392e-11  1.32273762e-10  1.11101743e-10  1.50640709e-10
 -1.60327010e-11 -1.60287708e-10 -1.34260674e-10 -1.81660410e-10
  1.65558723e-11  1.61678365e-10  1.35745555e-10  1.83966400e-10
 -4.56351576e-12 -2.08268754e-11 -1.42529195e-11 -2.07689004e-11
  1.62712417e-11  3.11768468e-11  2.69855973e-11  3.05567540e-11
  3.46777432e-11  1.19528025e-10  8.71263834e-11  1.18704555e-10
  3.98115050e-12  6.05831984e-12  5.76972050e-12  5.88972311e-12
  1.09434197e-12  2.54080883e-13  1.15228866e-11 -2.44362952e-12
  2.10612598e-11  4.93951952e-11  1.71690039e-11  1.14664069e-13
  3.14150224e+00]
supnorm grad right now is: 3.141502238528488
Weights right now are: 
[-1.93542021 -3.26150608 -3.92962821 -2.94415542  1.14520101  3.27596978
  4.5009343   4.35923852  0.05291381 -1.10254298  0.63409822 -0.62319794
  0.05982251 -2.3214538   0.47556967 -2.46368038 -1.18357562 -2.09640556
 -0.54833491 -2.27584174 -1.2641635  -2.3630644  -1.52649728 -2.46834336
  1.42481533  0.60994038  0.03934935  0.61189326 -1.16195178 -1.46321087
 -2.2985254  -1.95791568  0.28944376  1.03691451  1.43115793  1.18098599
 -1.04950697 -1.83604611 -1.01772075 -2.15158511  0.36665581  0.30563898
  1.36653669  1.6915331   0.39343896 -1.58551512 -0.20958683 -0.93164606
 -0.04632491 -0.07455773 -0.61096098 -0.72322958 -1.54543062 -2.08502504
 -1.29501716 -2.04635218  0.96166412  1.32117913 -0.81247497 -1.60480088
 -0.32062604 -0.04643225 -0.71778834 -1.74726809 25.19548019]
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.621090020598
gradient value of function right now is: [ 2.13266003e-08  2.50834009e-08  2.50607014e-08  2.51597841e-08
 -2.13264485e-08 -2.50832224e-08 -2.50605231e-08 -2.51596051e-08
  2.24824860e-09  1.02962454e-08  4.32219968e-10  1.02587390e-08
  3.17529930e-10  1.45409614e-09  6.11002971e-11  1.44879273e-09
  4.50075639e-10  2.06124146e-09  8.65005479e-11  2.05373594e-09
  2.35290935e-10  1.07762600e-09  4.51887092e-11  1.07370575e-09
  4.47690565e-11  4.76645165e-10  3.97064546e-10  5.34781832e-10
  1.32927173e-11  1.28128665e-10  1.07709862e-10  1.46103418e-10
 -1.55954603e-11 -1.54872575e-10 -1.29817763e-10 -1.75714096e-10
  1.61640750e-11  1.56662609e-10  1.31643146e-10  1.78481747e-10
 -4.43818713e-12 -2.01634383e-11 -1.38090025e-11 -2.01069392e-11
  1.61164626e-11  3.02012935e-11  2.63333144e-11  2.95871473e-11
  3.40558082e-11  1.15516339e-10  8.44959977e-11  1.14699920e-10
  3.98161683e-12  5.88812730e-12  5.66878092e-12  5.71974622e-12
  1.06822285e-12  2.17528782e-13  1.12036649e-11 -2.59229131e-12
  2.04208748e-11  4.74362989e-11  1.66732901e-11 -1.02301316e-13
  2.13619178e+00]
supnorm grad right now is: 2.1361917793616474
Weights right now are: 
[-1.93983555 -3.26670113 -3.93481854 -2.94936629  1.14961635  3.28116484
  4.50612464  4.3644494   0.05244824 -1.10467555  0.63400874 -0.62532275
  0.0597568  -2.32175479  0.47555703 -2.46398027 -1.1836688  -2.09683239
 -0.54835281 -2.27626702 -1.2642122  -2.36328748 -1.52650664 -2.46856563
  1.42480608  0.60984177  0.03926723  0.61178265 -1.16195452 -1.46323739
 -2.29854767 -1.9579459   0.28944698  1.0369466   1.43118482  1.18102238
 -1.0495103  -1.83607853 -1.01774796 -2.15162202  0.36665674  0.30564316
  1.36653955  1.69153726  0.39343567 -1.58552138 -0.20959224 -0.93165221
 -0.04633196 -0.07458165 -0.61097844 -0.72325334 -1.54543143 -2.08502625
 -1.29501831 -2.04635336  0.9616639   1.32117907 -0.81247721 -1.60480046
 -0.32063016 -0.04644208 -0.71779172 -1.7472682  25.12215667]
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.260687415007
gradient value of function right now is: [ 2.02839843e-08  2.38410865e-08  2.38195580e-08  2.39135751e-08
 -2.02841667e-08 -2.38413009e-08 -2.38197723e-08 -2.39137903e-08
  2.13658978e-09  9.78464523e-09  4.09576768e-10  9.74913882e-09
  3.02040582e-10  1.38313628e-09  5.79499506e-11  1.37811141e-09
  4.27922585e-10  1.95973550e-09  8.20084740e-11  1.95262675e-09
  2.23861288e-10  1.02524922e-09  4.28727399e-11  1.02153348e-09
  4.23350636e-11  4.54785136e-10  3.78538373e-10  5.09559878e-10
  1.24612486e-11  1.21763783e-10  1.02211964e-10  1.38533243e-10
 -1.46268919e-11 -1.47031953e-10 -1.23092724e-10 -1.66491411e-10
  1.51608148e-11  1.48951714e-10  1.24985150e-10  1.69317923e-10
 -4.22026763e-12 -1.93160667e-11 -1.32139465e-11 -1.92622287e-11
  1.46763996e-11  2.85778610e-11  2.46042324e-11  2.80202925e-11
  3.15260707e-11  1.10060358e-10  8.00013541e-11  1.09320769e-10
  3.57246269e-12  5.54284580e-12  5.23946002e-12  5.39225584e-12
  1.00594210e-12  2.58684589e-13  1.05371609e-11 -2.05791938e-12
  1.93748208e-11  4.57588649e-11  1.57496329e-11  2.93246414e-13
  3.94921425e+00]
supnorm grad right now is: 3.949214246940949
Weights right now are: 
[-1.9441253  -3.27174527 -3.93985814 -2.95442581  1.1539061   3.28620897
  4.51116424  4.36950892  0.05199592 -1.10674626  0.63392128 -0.62738586
  0.05969287 -2.32204745  0.47554465 -2.46427185 -1.18375936 -2.09724698
 -0.54837032 -2.27668009 -1.26425955 -2.36350424 -1.52651578 -2.4687816
  1.4247969   0.60974616  0.03918744  0.61167503 -1.16195728 -1.46326326
 -2.29856948 -1.95797554  0.2894502   1.03697778  1.43121101  1.1810579
 -1.04951366 -1.83611015 -1.01777461 -2.15165821  0.36665766  0.30564718
  1.36654233  1.69154127  0.39343217 -1.58552755 -0.20959773 -0.93165824
 -0.04633919 -0.07460482 -0.6109956  -0.72327632 -1.54543231 -2.08502747
 -1.29501952 -2.04635454  0.96166368  1.32117905 -0.8124796  -1.60479976
 -0.32063431 -0.04645119 -0.71779523 -1.74726799 25.27574251]
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.713557100375
gradient value of function right now is: [ 1.98649471e-08  2.33344588e-08  2.33135130e-08  2.34054259e-08
 -1.98649140e-08 -2.33344199e-08 -2.33134741e-08 -2.34053869e-08
  2.09200376e-09  9.57637994e-09  4.02378175e-10  9.54145110e-09
  2.96097734e-10  1.35533742e-09  5.70048862e-11  1.35038777e-09
  4.19145547e-10  1.91872532e-09  8.05946337e-11  1.91172988e-09
  2.19361882e-10  1.00422036e-09  4.21490410e-11  1.00056260e-09
  4.19654682e-11  4.45165052e-10  3.70975857e-10  4.99742687e-10
  1.24683048e-11  1.19552142e-10  1.00557982e-10  1.36443269e-10
 -1.45595590e-11 -1.43893373e-10 -1.20676428e-10 -1.63384960e-10
  1.51714246e-11  1.46274317e-10  1.22983957e-10  1.66791452e-10
 -4.18127735e-12 -1.88594543e-11 -1.29328203e-11 -1.88054227e-11
  1.52440867e-11  2.81506610e-11  2.46726091e-11  2.75667322e-11
  3.20607323e-11  1.07529470e-10  7.88608252e-11  1.06750855e-10
  3.78329241e-12  5.50529806e-12  5.33550503e-12  5.34433073e-12
  1.00747745e-12  1.83439943e-13  1.05041865e-11 -2.58788217e-12
  1.90926486e-11  4.39899274e-11  1.56241498e-11 -2.65120976e-13
  1.43431971e+00]
supnorm grad right now is: 1.434319714247563
Weights right now are: 
[-1.94827083 -3.27661682 -3.94472533 -2.95931222  1.15805164  3.29108052
  4.51603143  4.37439532  0.05155883 -1.10874604  0.63383657 -0.62937824
  0.05963101 -2.32233043  0.47553265 -2.46455379 -1.1838469  -2.09764752
 -0.54838728 -2.27707915 -1.26430534 -2.36371375 -1.52652464 -2.46899034
  1.42478799  0.60965371  0.0391102   0.61157087 -1.16196001 -1.46328827
 -2.29859065 -1.95800432  0.28945336  1.03700783  1.43123635  1.18109225
 -1.04951699 -1.83614074 -1.01780049 -2.15169339  0.36665846  0.30565108
  1.36654497  1.69154516  0.39342849 -1.58553339 -0.20960314 -0.93166395
 -0.0463465  -0.07462709 -0.61101231 -0.72329842 -1.54543334 -2.08502861
 -1.29502076 -2.04635565  0.96166345  1.32117904 -0.81248186 -1.60479912
 -0.3206385  -0.04645932 -0.71779856 -1.74726788 25.03430938]
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.36485284838
gradient value of function right now is: [ 1.97783345e-08  2.32201995e-08  2.31995312e-08  2.32909239e-08
 -1.97781648e-08 -2.32200002e-08 -2.31993321e-08 -2.32907241e-08
  2.08335935e-09  9.53009631e-09  4.03676828e-10  9.49494421e-09
  2.95281738e-10  1.35063308e-09  5.72776317e-11  1.34564366e-09
  4.17533039e-10  1.91000600e-09  8.08736962e-11  1.90296437e-09
  2.18569338e-10  9.99901408e-10  4.22997293e-11  9.96219287e-10
  4.27549355e-11  4.41935526e-10  3.69237538e-10  4.98127044e-10
  1.29763840e-11  1.19571819e-10  1.01027938e-10  1.37396233e-10
 -1.50181404e-11 -1.43255378e-10 -1.20606921e-10 -1.63622421e-10
  1.57901399e-11  1.46307247e-10  1.23562537e-10  1.67960380e-10
 -4.20331989e-12 -1.85946990e-11 -1.27906442e-11 -1.85396222e-11
  1.71036827e-11  2.83272329e-11  2.58087845e-11  2.76686533e-11
  3.44641932e-11  1.06485846e-10  7.96058757e-11  1.05602510e-10
  4.42847575e-12  5.62880865e-12  5.76970285e-12  5.44054611e-12
  1.04774447e-12  2.94565550e-14  1.08172593e-11 -3.76838150e-12
  1.92770920e-11  4.19746900e-11  1.59056040e-11 -1.39648275e-12
 -3.29011507e+00]
supnorm grad right now is: 3.2901150739304366
Weights right now are: 
[-1.95219473 -3.2812249  -3.94932928 -2.96393432  1.16197553  3.2956886
  4.52063538  4.37901742  0.05114555 -1.11063719  0.63375691 -0.63126246
  0.05957248 -2.32259823  0.47552136 -2.46482061 -1.18392971 -2.09802648
 -0.54840324 -2.27745672 -1.26434868 -2.36391212 -1.52653299 -2.46918798
  1.42477965  0.60956584  0.03903689  0.6114721  -1.16196251 -1.46331191
 -2.29861057 -1.95803136  0.28945627  1.03703624  1.43126021  1.18112458
 -1.04952003 -1.83616968 -1.01782486 -2.15172646  0.36665929  0.30565479
  1.36654752  1.69154886  0.39342535 -1.58553896 -0.20960811 -0.9316694
 -0.04635298 -0.07464827 -0.61102799 -0.72331944 -1.54543414 -2.08502971
 -1.29502185 -2.04635671  0.96166325  1.32117902 -0.81248398 -1.60479851
 -0.32064234 -0.04646829 -0.7178017  -1.7472677  24.75154718]
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.130811295882
gradient value of function right now is: [ 1.92086335e-08  2.25385471e-08  2.25185708e-08  2.26071720e-08
 -1.92082631e-08 -2.25381124e-08 -2.25181365e-08 -2.26067360e-08
  2.02253330e-09  9.24942926e-09  3.92289724e-10  9.21525179e-09
  2.86955542e-10  1.31220216e-09  5.57205351e-11  1.30734592e-09
  4.05496111e-10  1.85445811e-09  7.86213918e-11  1.84760921e-09
  2.12366891e-10  9.71274138e-10  4.11400198e-11  9.67691234e-10
  4.17336572e-11  4.29518224e-10  3.59022338e-10  4.84462271e-10
  1.26981975e-11  1.16264484e-10  9.83061896e-11  1.33744544e-10
 -1.46545350e-11 -1.38983040e-10 -1.17085568e-10 -1.58897872e-10
  1.54557994e-11  1.42300146e-10  1.20266635e-10  1.63540908e-10
 -4.10573451e-12 -1.80685512e-11 -1.24400408e-11 -1.80143061e-11
  1.69138820e-11  2.75502772e-11  2.52586687e-11  2.68974649e-11
  3.38817000e-11  1.03326407e-10  7.74927874e-11  1.02449459e-10
  4.40306485e-12  5.49072476e-12  5.67571983e-12  5.30318227e-12
  1.02760810e-12  4.18868227e-15  1.05725254e-11 -3.85961917e-12
  1.87833252e-11  4.04794703e-11  1.55201541e-11 -1.55024691e-12
 -4.04905304e+00]
supnorm grad right now is: 4.0490530416654
Weights right now are: 
[-1.95602414 -3.28571918 -3.95381954 -2.96844224  1.16580494  3.30018288
  4.52512564  4.38352534  0.05074273 -1.11248114  0.63367974 -0.63309973
  0.0595154  -2.32285952  0.47551041 -2.46508095 -1.18401047 -2.09839616
 -0.5484187  -2.27782506 -1.26439099 -2.36410578 -1.52654109 -2.46938093
  1.42477161  0.60947968  0.03896519  0.61137553 -1.16196485 -1.46333495
 -2.29862989 -1.95805755  0.28945901  1.03706393  1.43128338  1.18115592
 -1.04952288 -1.83619788 -1.0178485  -2.15175848  0.36666014  0.30565846
  1.36655005  1.69155251  0.39342265 -1.58554441 -0.20961276 -0.93167474
 -0.0463589  -0.07466907 -0.61104311 -0.72334009 -1.54543475 -2.08503077
 -1.29502283 -2.04635774  0.96166306  1.32117898 -0.812486   -1.60479808
 -0.32064602 -0.04647724 -0.71780472 -1.74726774 24.69389073]
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.772821908959
gradient value of function right now is: [ 1.81433194e-08  2.12756082e-08  2.12567411e-08  2.13402299e-08
 -1.81432270e-08 -2.12754997e-08 -2.12566327e-08 -2.13401211e-08
  1.90821966e-09  8.72875690e-09  3.67822172e-10  8.69678989e-09
  2.70910591e-10  1.23914307e-09  5.22718006e-11  1.23459887e-09
  3.82766834e-10  1.75092723e-09  7.37569921e-11  1.74451776e-09
  2.00610474e-10  9.17717130e-10  3.86266484e-11  9.14361178e-10
  3.88192781e-11  4.07711906e-10  3.40108308e-10  4.58403782e-10
  1.15997565e-11  1.09522578e-10  9.22829392e-11  1.25322432e-10
 -1.34367065e-11 -1.30985404e-10 -1.10016768e-10 -1.49064931e-10
  1.41267184e-11  1.34120935e-10  1.12961420e-10  1.53329572e-10
 -3.87157394e-12 -1.72809897e-11 -1.18713112e-11 -1.72304045e-11
  1.45845352e-11  2.57713699e-11  2.29373427e-11  2.52110942e-11
  3.01779194e-11  9.80075245e-11  7.24098309e-11  9.72582909e-11
  3.68523923e-12  5.07590222e-12  5.02997764e-12  4.91894334e-12
  9.41836626e-13  1.14126628e-13  9.71536888e-12 -2.78801561e-12
  1.75588741e-11  3.95776363e-11  1.44091924e-11 -6.39457817e-13
 -4.91254192e-01]
supnorm grad right now is: 0.49125419150673294
Weights right now are: 
[-1.95979305 -3.29013999 -3.95823641 -2.97287646  1.16957384  3.30460368
  4.5295425   4.38795956  0.05034643 -1.11429478  0.6336037  -0.63490679
  0.05945918 -2.32311679  0.47549962 -2.46533728 -1.18408995 -2.09875991
 -0.54843395 -2.27818749 -1.26443264 -2.36429641 -1.52654907 -2.46957087
  1.42476365  0.60939484  0.03889453  0.61128036 -1.16196719 -1.46335765
 -2.29864895 -1.9580834   0.28946174  1.03709116  1.43130617  1.18118677
 -1.04952572 -1.83622568 -1.01787182 -2.1517901   0.36666097  0.30566206
  1.36655254  1.69155611  0.3934199  -1.58554977 -0.20961737 -0.93168
 -0.04636484 -0.07468952 -0.61105801 -0.72336039 -1.54543539 -2.08503182
 -1.29502379 -2.04635876  0.96166287  1.32117893 -0.81248796 -1.60479764
 -0.32064956 -0.04648594 -0.71780768 -1.7472677  24.95588294]
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.760979313235
gradient value of function right now is: [ 1.76051777e-08  2.06336138e-08  2.06153802e-08  2.06962530e-08
 -1.76049095e-08 -2.06332993e-08 -2.06150660e-08 -2.06959375e-08
  1.85077733e-09  8.46432765e-09  3.56841187e-10  8.43330895e-09
  2.63000389e-10  1.20272360e-09  5.07590082e-11  1.19831003e-09
  3.71383835e-10  1.69852205e-09  7.15815724e-11  1.69230042e-09
  1.94733764e-10  8.90658319e-10  3.75042612e-11  8.87399264e-10
  3.77720629e-11  3.96047932e-10  3.30437032e-10  4.45406360e-10
  1.12925490e-11  1.06349692e-10  8.96361161e-11  1.21744953e-10
 -1.30535920e-11 -1.26958763e-10 -1.06662369e-10 -1.44537688e-10
  1.37564610e-11  1.30272911e-10  1.09752823e-10  1.48995008e-10
 -3.77226172e-12 -1.67959950e-11 -1.15432452e-11 -1.67465116e-11
  1.42575322e-11  2.50104516e-11  2.23187215e-11  2.44620863e-11
  2.94289731e-11  9.50662065e-11  7.03254099e-11  9.43324924e-11
  3.61226306e-12  4.93286770e-12  4.90586793e-12  4.77886242e-12
  9.18619708e-13  1.02468199e-13  9.45012473e-12 -2.77782855e-12
  1.70645300e-11  3.83163594e-11  1.40096385e-11 -6.89862509e-13
 -7.79192020e-01]
supnorm grad right now is: 0.7791920199328757
Weights right now are: 
[-1.96336453 -3.29432686 -3.96241956 -2.97707602  1.17314533  3.30879056
  4.53372566  4.39215912  0.04997113 -1.11601216  0.63353184 -0.63661794
  0.0594059  -2.32336061  0.47548941 -2.46558022 -1.18416525 -2.0991045
 -0.54844837 -2.27853083 -1.26447212 -2.3644771  -1.52655662 -2.46975091
  1.42475615  0.60931423  0.03882743  0.61119004 -1.16196938 -1.46337914
 -2.29866698 -1.95810784  0.2894643   1.0371169   1.43132772  1.18121592
 -1.0495284  -1.836252   -1.01789391 -2.15182002  0.36666173  0.30566551
  1.3665549   1.69155954  0.39341733 -1.58555479 -0.20962169 -0.93168492
 -0.04637038 -0.0747089  -0.6110721  -0.72337964 -1.54543602 -2.08503279
 -1.29502472 -2.04635971  0.96166269  1.3211789  -0.81248984 -1.60479723
 -0.32065296 -0.04649411 -0.71781047 -1.74726775 24.94510606]
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.619532414172
gradient value of function right now is: [ 1.72276024e-08  2.01811120e-08  2.01633565e-08  2.02423750e-08
 -1.72279191e-08 -2.01814831e-08 -2.01637272e-08 -2.02427472e-08
  1.81060104e-09  8.27812906e-09  3.49733176e-10  8.24770329e-09
  2.57544165e-10  1.17741860e-09  4.97989394e-11  1.17308488e-09
  3.63447956e-10  1.66173599e-09  7.01790577e-11  1.65563130e-09
  1.90644984e-10  8.71701726e-10  3.67822286e-11  8.68502858e-10
  3.72267583e-11  3.87606387e-10  3.23622253e-10  4.36390263e-10
  1.11849392e-11  1.04229650e-10  8.79544841e-11  1.19533990e-10
 -1.28849604e-11 -1.24136167e-10 -1.04399409e-10 -1.41547779e-10
  1.36280625e-11  1.27703690e-10  1.07716167e-10  1.46319199e-10
 -3.71462615e-12 -1.64187278e-11 -1.12977982e-11 -1.63696717e-11
  1.43948510e-11  2.45394770e-11  2.21270547e-11  2.39835825e-11
  2.93887020e-11  9.28947170e-11  6.90749073e-11  9.21496036e-11
  3.68364362e-12  4.86293831e-12  4.90632333e-12  4.70527136e-12
  9.11645577e-13  6.32065672e-14  9.33923073e-12 -3.01455768e-12
  1.67702537e-11  3.71113865e-11  1.38022302e-11 -9.58651120e-13
 -2.11125605e+00]
supnorm grad right now is: 2.1112560499966917
Weights right now are: 
[-1.96677631 -3.29832493 -3.96641413 -2.98108626  1.17655711  3.31278863
  4.53772023  4.39616937  0.04961215 -1.11765261  0.63346189 -0.63825227
  0.05935483 -2.32359393  0.47547944 -2.46581267 -1.18423729 -2.0994337
 -0.5484624  -2.2788588  -1.26450989 -2.36464969 -1.52656397 -2.46992286
  1.42474861  0.60923787  0.0387635   0.61110372 -1.16197173 -1.46339985
 -2.29868457 -1.9581318   0.28946697  1.03714151  1.43134853  1.18124419
 -1.04953125 -1.83627736 -1.01791545 -2.15184934  0.3666624   0.30566873
  1.36655709  1.69156275  0.39341402 -1.58555963 -0.2096263  -0.93168964
 -0.04637678 -0.07472715 -0.61108599 -0.72339773 -1.54543697 -2.08503376
 -1.29502581 -2.04636064  0.9616625   1.32117891 -0.81249176 -1.60479653
 -0.32065634 -0.04650075 -0.71781328 -1.74726749 24.85481394]
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5137.3229526735495
gradient value of function right now is: [ 1.58176771e-08  1.85188040e-08  1.85024165e-08  1.85747698e-08
 -1.58176115e-08 -1.85187272e-08 -1.85023397e-08 -1.85746927e-08
  1.65942807e-09  7.59268092e-09  3.16177556e-10  7.56531954e-09
  2.36073709e-10  1.08009912e-09  4.50141273e-11  1.07620296e-09
  3.33290326e-10  1.52498969e-09  6.34869475e-11  1.51949605e-09
  1.74990831e-10  8.00710081e-10  3.33131377e-11  7.97827814e-10
  3.29268186e-11  3.59114178e-10  2.98502157e-10  4.01445169e-10
  9.50161646e-12  9.50431031e-11  7.95897637e-11  1.07719213e-10
 -1.10681847e-11 -1.13589861e-10 -9.49002188e-11 -1.28197644e-10
  1.15882893e-11  1.16543938e-10  9.75582587e-11  1.31975892e-10
 -3.34691857e-12 -1.54411441e-11 -1.05519366e-11 -1.53983867e-11
  1.05695804e-11  2.20182128e-11  1.85443864e-11  2.16199429e-11
  2.34592804e-11  8.60695354e-11  6.18948136e-11  8.55441616e-11
  2.50059276e-12  4.24031127e-12  3.87683287e-12  4.13568310e-12
  7.77218369e-13  2.73556733e-13  8.02562946e-12 -1.06955570e-12
  1.50424984e-11  3.66495832e-11  1.21316536e-11  7.54972616e-13
  6.65078058e+00]
supnorm grad right now is: 6.650780575274675
Weights right now are: 
[-1.97001927 -3.30212304 -3.9702089  -2.9848959   1.17980007  3.31658675
  4.541515    4.399979    0.04927142 -1.11921044  0.6333961  -0.63980439
  0.05930635 -2.32381558  0.47547007 -2.4660335  -1.1843057  -2.09974647
 -0.5484756  -2.27917042 -1.26454578 -2.3648138  -1.52657089 -2.47008636
  1.4247416   0.60916486  0.03870256  0.61102152 -1.16197382 -1.46341948
 -2.29870112 -1.9581543   0.28946938  1.03716487  1.43136816  1.18127081
 -1.04953381 -1.83630141 -1.01793572 -2.15187688  0.36666311  0.30567183
  1.36655922  1.69156585  0.39341136 -1.58556426 -0.20963044 -0.93169417
 -0.04638231 -0.07474464 -0.61109898 -0.7234151  -1.54543765 -2.08503467
 -1.29502672 -2.04636152  0.96166233  1.32117889 -0.81249352 -1.60479607
 -0.32065949 -0.04650764 -0.7178159  -1.74726742 25.43972309]
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.61711477789
gradient value of function right now is: [ 1.64986990e-08  1.93133542e-08  1.92964361e-08  1.93719305e-08
 -1.64986038e-08 -1.93132428e-08 -1.92963247e-08 -1.93718187e-08
  1.73277649e-09  7.92054783e-09  3.34598735e-10  7.89143874e-09
  2.46780435e-10  1.12795877e-09  4.77030471e-11  1.12380745e-09
  3.48011621e-10  1.59080454e-09  6.71778251e-11  1.58496093e-09
  1.82665560e-10  8.35030730e-10  3.52318938e-11  8.31966696e-10
  3.57190280e-11  3.71889978e-10  3.10506347e-10  4.18699185e-10
  1.07191135e-11  9.98793873e-11  8.42856143e-11  1.14546857e-10
 -1.23206329e-11 -1.18688308e-10 -9.98201705e-11 -1.35337960e-10
  1.30658675e-11  1.22424309e-10  1.03265656e-10  1.40272504e-10
 -3.57438564e-12 -1.57762895e-11 -1.08588653e-11 -1.57288706e-11
  1.37837124e-11  2.34805118e-11  2.11781293e-11  2.29481091e-11
  2.81513507e-11  8.89591578e-11  6.61534032e-11  8.82452490e-11
  3.52866522e-12  4.65525844e-12  4.69816021e-12  4.50422090e-12
  8.75784602e-13  6.06803165e-14  8.94331063e-12 -2.89001304e-12
  1.60733142e-11  3.55613805e-11  1.32261828e-11 -9.20497585e-13
 -2.12668219e+00]
supnorm grad right now is: 2.12668219258015
Weights right now are: 
[-1.97323385 -3.30588596 -3.9739685  -2.98867021  1.18301465  3.32034966
  4.5452746   4.40375331  0.04893401 -1.12075343  0.63333132 -0.64134175
  0.05925832 -2.32403524  0.47546084 -2.46625236 -1.18437347 -2.10005639
 -0.54848861 -2.27947922 -1.26458135 -2.36497651 -1.52657772 -2.47024849
  1.42473476  0.60909216  0.03864199  0.61093993 -1.16197584 -1.46343888
 -2.29871744 -1.95817643  0.28947172  1.03718798  1.43138754  1.18129704
 -1.04953627 -1.8363252  -1.0179557  -2.15190398  0.36666381  0.30567493
  1.36656136  1.69156894  0.39340893 -1.58556881 -0.20963442 -0.93169862
 -0.04638744 -0.07476206 -0.61111173 -0.72343238 -1.54543824 -2.08503556
 -1.29502757 -2.04636239  0.96166216  1.32117886 -0.81249527 -1.60479559
 -0.32066262 -0.04651477 -0.7178185  -1.7472673  25.04660931]
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.695043225787
gradient value of function right now is: [ 1.61955338e-08  1.89536789e-08  1.89370913e-08  1.90111321e-08
 -1.61951110e-08 -1.89531840e-08 -1.89365968e-08 -1.90106357e-08
  1.70036667e-09  7.77226802e-09  3.28021309e-10  7.74374109e-09
  2.42257310e-10  1.10726746e-09  4.67823366e-11  1.10319772e-09
  3.41569828e-10  1.56133074e-09  6.58705954e-11  1.55560282e-09
  1.79329890e-10  8.19766842e-10  3.45555302e-11  8.16762652e-10
  3.49992606e-11  3.65470161e-10  3.05058265e-10  4.11281864e-10
  1.04730671e-11  9.80178543e-11  8.26740278e-11  1.12326914e-10
 -1.20378339e-11 -1.16423941e-10 -9.78740795e-11 -1.32668198e-10
  1.27682763e-11  1.20163467e-10  1.01309164e-10  1.37578577e-10
 -3.50973861e-12 -1.55265577e-11 -1.06831738e-11 -1.54800198e-11
  1.33523409e-11  2.30095715e-11  2.06657077e-11  2.24948903e-11
  2.74025126e-11  8.73743087e-11  6.48354789e-11  8.66844237e-11
  3.40526439e-12  4.55397264e-12  4.56918680e-12  4.40851839e-12
  8.56108977e-13  7.42308378e-14  8.73886992e-12 -2.71871381e-12
  1.57550516e-11  3.50830710e-11  1.29486495e-11 -7.89192498e-13
 -1.58215135e+00]
supnorm grad right now is: 1.582151346737424
Weights right now are: 
[-1.97659121 -3.30981394 -3.97789303 -2.99261006  1.186372    3.32427764
  4.54919912  4.40769315  0.04858194 -1.12236363  0.63326417 -0.64294614
  0.05920816 -2.3242646   0.47545127 -2.46648089 -1.18444422 -2.10037996
 -0.5485021  -2.27980161 -1.26461852 -2.3651465  -1.5265848  -2.47041786
  1.42472773  0.60901587  0.03857851  0.61085457 -1.16197788 -1.46345908
 -2.29873437 -1.95819935  0.28947408  1.03721206  1.43140767  1.18132424
 -1.04953875 -1.83634998 -1.01797647 -2.15193208  0.36666454  0.30567819
  1.3665636   1.69157219  0.39340661 -1.5855735  -0.20963841 -0.93170321
 -0.04639247 -0.0747803  -0.61112493 -0.72345049 -1.54543878 -2.08503648
 -1.29502841 -2.04636328  0.96166199  1.32117882 -0.81249697 -1.60479518
 -0.32066575 -0.04652343 -0.71782105 -1.74726728 25.09337167]
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.781048701062
gradient value of function right now is: [ 1.57964769e-08  1.84819354e-08  1.84657595e-08  1.85379040e-08
 -1.57962146e-08 -1.84816284e-08 -1.84654527e-08 -1.85375960e-08
  1.65770130e-09  7.57787871e-09  3.19011259e-10  7.55016096e-09
  2.36249383e-10  1.07990160e-09  4.55087022e-11  1.07594639e-09
  3.33072742e-10  1.52261530e-09  6.40763619e-11  1.51704851e-09
  1.74922197e-10  7.99681547e-10  3.36257511e-11  7.96760822e-10
  3.39329439e-11  3.57174296e-10  2.97902316e-10  4.01447208e-10
  1.00824588e-11  9.55018001e-11  8.04447546e-11  1.09220068e-10
 -1.16045535e-11 -1.13447905e-10 -9.52626226e-11 -1.29046441e-10
  1.22951208e-11  1.17106024e-10  9.86011857e-11  1.33805909e-10
 -3.41612557e-12 -1.52190777e-11 -1.04599041e-11 -1.51738413e-11
  1.25593693e-11  2.23506754e-11  1.98448800e-11  2.18680160e-11
  2.61131271e-11  8.53571310e-11  6.29720051e-11  8.47113685e-11
  3.16670961e-12  4.40253930e-12  4.34635952e-12  4.26770194e-12
  8.25236475e-13  1.11352986e-13  8.43301064e-12 -2.35383773e-12
  1.53088817e-11  3.46586582e-11  1.25432600e-11 -4.86874552e-13
  7.80800024e-03]
supnorm grad right now is: 0.007808000239378742
Weights right now are: 
[-1.9796798  -3.31342598 -3.98150192 -2.99623303  1.18946059  3.32788967
  4.552808    4.41131612  0.04825801 -1.12384444  0.63320182 -0.64442153
  0.05916196 -2.32447579  0.47544237 -2.46669131 -1.18450933 -2.10067761
 -0.54851463 -2.28009818 -1.26465273 -2.36530289 -1.52659138 -2.47057368
  1.42472108  0.60894595  0.03852022  0.61077599 -1.16197985 -1.46347777
 -2.29875011 -1.95822072  0.28947635  1.03723422  1.43142628  1.18134945
 -1.04954116 -1.83637289 -1.01799576 -2.15195827  0.3666652   0.30568118
  1.36656565  1.69157518  0.39340415 -1.58557786 -0.20964228 -0.93170747
 -0.0463976  -0.074797   -0.61113721 -0.72346707 -1.54543941 -2.08503734
 -1.29502927 -2.04636411  0.96166183  1.32117881 -0.8124986  -1.6047947
 -0.32066874 -0.0465297  -0.7178235  -1.74726723 24.96316411]
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.780927541485
gradient value of function right now is: [ 1.55871390e-08  1.82328287e-08  1.82168940e-08  1.82880283e-08
 -1.55873210e-08 -1.82330416e-08 -1.82171067e-08 -1.82882418e-08
  1.63539270e-09  7.47532347e-09  3.14711846e-10  7.44797859e-09
  2.33165874e-10  1.06572471e-09  4.49138743e-11  1.06182110e-09
  3.28649558e-10  1.50227949e-09  6.32241192e-11  1.49678660e-09
  1.72634803e-10  7.89163728e-10  3.31853138e-11  7.86281189e-10
  3.35118959e-11  3.52639938e-10  2.94129745e-10  3.96368554e-10
  9.95552308e-12  9.42598457e-11  7.94024623e-11  1.07807262e-10
 -1.14493441e-11 -1.11887440e-10 -9.39564035e-11 -1.27279411e-10
  1.21418745e-11  1.15598120e-10  9.73362248e-11  1.32092119e-10
 -3.37696631e-12 -1.50317842e-11 -1.03328008e-11 -1.49870131e-11
  1.24067448e-11  2.20518280e-11  1.95880416e-11  2.15748999e-11
  2.57888691e-11  8.42211987e-11  6.21470395e-11  8.35829506e-11
  3.12943061e-12  4.34512863e-12  4.29196468e-12  4.21181176e-12
  8.15507702e-13  1.08388606e-13  8.32342237e-12 -2.33447417e-12
  1.51116248e-11  3.41907376e-11  1.23823260e-11 -4.90988312e-13
 -7.80659470e-02]
supnorm grad right now is: 0.07806594699391722
Weights right now are: 
[-1.98262923 -3.31687372 -3.98494666 -2.9996912   1.19241003  3.33133742
  4.55625275  4.41477429  0.04794879 -1.12525766  0.63314237 -0.64582959
  0.05911782 -2.3246775   0.47543387 -2.46689228 -1.1845715  -2.10096177
 -0.54852658 -2.2803813  -1.26468541 -2.36545225 -1.52659765 -2.4707225
  1.42471473  0.60887909  0.03846447  0.61070086 -1.16198173 -1.46349561
 -2.29876513 -1.95824111  0.2894785   1.03725536  1.43144402  1.18137349
 -1.04954345 -1.83639479 -1.01801418 -2.15198326  0.36666586  0.30568404
  1.36656762  1.69157802  0.39340186 -1.58558204 -0.20964598 -0.93171157
 -0.04640241 -0.07481296 -0.61114897 -0.7234829  -1.54543997 -2.08503816
 -1.29503007 -2.04636491  0.96166167  1.32117879 -0.81250024 -1.60479425
 -0.32067168 -0.04653608 -0.71782592 -1.7472671  25.3505131 ]
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5138.775336795193
gradient value of function right now is: [ 1.54099456e-08  1.80215795e-08  1.80058554e-08  1.80761309e-08
 -1.54103039e-08 -1.80219986e-08 -1.80062741e-08 -1.80765512e-08
  1.61651912e-09  7.38832285e-09  3.11184903e-10  7.36127989e-09
  2.30571369e-10  1.05376124e-09  4.44295035e-11  1.04989906e-09
  3.24911472e-10  1.48504604e-09  6.25259106e-11  1.47961291e-09
  1.70703032e-10  7.80256518e-10  3.28247236e-11  7.77404836e-10
  3.31917383e-11  3.48747712e-10  2.90926455e-10  3.92084810e-10
  9.86897088e-12  9.32280131e-11  7.85532917e-11  1.06668266e-10
 -1.13372078e-11 -1.10566363e-10 -9.28674765e-11 -1.25818872e-10
  1.20376380e-11  1.14345419e-10  9.63058131e-11  1.30710822e-10
 -3.34551545e-12 -1.48664644e-11 -1.02220622e-11 -1.48220444e-11
  1.23489846e-11  2.18101026e-11  1.94158559e-11  2.13355510e-11
  2.56106327e-11  8.32376621e-11  6.14881958e-11  8.26022822e-11
  3.12273685e-12  4.30174369e-12  4.26256557e-12  4.16880834e-12
  8.08555256e-13  1.00783051e-13  8.24121453e-12 -2.35693455e-12
  1.49528373e-11  3.37183835e-11  1.22576543e-11 -5.30912488e-13
 -4.09470876e-01]
supnorm grad right now is: 0.4094708760876667
Weights right now are: 
[-1.98553543 -3.32026951 -3.9883395  -3.00309726  1.19531622  3.33473321
  4.55964559  4.41818035  0.04764417 -1.12664948  0.63308368 -0.64721631
  0.05907431 -2.32487632  0.47542548 -2.46709037 -1.18463277 -2.10124171
 -0.54853838 -2.28066021 -1.26471762 -2.36559944 -1.52660385 -2.47086915
  1.42470843  0.60881323  0.03840951  0.61062676 -1.16198359 -1.46351323
 -2.29877997 -1.95826128  0.28948065  1.03727619  1.43146152  1.1813972
 -1.04954572 -1.83641641 -1.01803238 -2.15200797  0.36666652  0.30568684
  1.36656956  1.69158082  0.39339953 -1.5855862  -0.20964967 -0.93171563
 -0.04640727 -0.07482867 -0.61116057 -0.72349849 -1.54544052 -2.08503899
 -1.29503087 -2.04636571  0.96166152  1.32117877 -0.81250179 -1.60479369
 -0.32067443 -0.04654258 -0.71782819 -1.7472669  24.50703997]
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1550.717701739314
W_T_median: 1404.9332849199334
W_T_pctile_5: 624.2405425026941
W_T_CVAR_5_pct: 486.6279485603609
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
F value: -5138.775336795193
-----------------------------------------------
