Starting at: 
2022-06-20 18:03:38
tracing parameter entered from terminal:  0.4


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = basic_T30_VWD
timeseries_basket['basket_desc'] = CRSP data: T30 and VWD
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
5.0% of MC simulations done.
10.0% of MC simulations done.
15.0% of MC simulations done.
20.0% of MC simulations done.
25.0% of MC simulations done.
30.0% of MC simulations done.
35.0% of MC simulations done.
40.0% of MC simulations done.
45.0% of MC simulations done.
50.0% of MC simulations done.
55.00000000000001% of MC simulations done.
60.0% of MC simulations done.
65.0% of MC simulations done.
70.0% of MC simulations done.
75.0% of MC simulations done.
80.0% of MC simulations done.
85.0% of MC simulations done.
90.0% of MC simulations done.
95.0% of MC simulations done.
100.0% of MC simulations done.
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  hidden_layer    None       None         None   
4        obj.layers[4]        4  hidden_layer    None       None         None   
5        obj.layers[5]        5  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  
4    False        None  
5    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  hidden_layer       4  logistic_sigmoid   
4        obj.layers[4]        4  hidden_layer       4  logistic_sigmoid   
5        obj.layers[5]        5  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 4)    False        None  
4       (4, 4)    False        None  
5       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
[ 0.10347998  0.18844875 -0.3666709   0.62803316 -0.89369922 -0.17398511
  0.93797695  0.78704989 -0.60791207 -0.16293226 -0.31685198  0.28094308
  0.0404241   0.17453202  0.0940472   0.01169373  0.2300954  -0.23784803
  0.78511084 -0.41859022  0.4809118  -0.01155852  0.07219357 -0.14482284
  0.4701344  -0.18827011 -0.8138637  -0.24629474  0.28361272  0.33354483
 -0.63777411 -0.25343004 -0.81551088 -0.26234589  0.20198453 -0.15135741
  0.25264285 -0.29817275  0.41496663 -0.58895503 -0.60305385 -0.63454123
  0.3979274   0.74701442  0.57452389 -0.67377327  0.6493153   0.07340044
 -0.29694484  0.52758424 -0.14347696  0.04319718 -0.28527617 -0.63077443
  0.13133703 -0.58713125  0.12759166  0.87507529 -0.19770888 -0.66759109
  0.25529887 -0.48501395  0.58877641 -0.72315372] (64,)
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1260.6521530687958
W_T_median: 1222.3920506735662
W_T_pctile_5: 834.1581876304141
W_T_CVAR_5_pct: 746.6575474940535
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1355.636332560002
gradient value of function right now is: [ 4.52142008e+00  1.40970190e+00  2.59561961e-01  2.12555228e-01
 -4.52142008e+00 -1.40970190e+00 -2.59561961e-01 -2.12555228e-01
 -7.06960050e-01 -8.45283993e-02 -7.00124308e-01 -8.84792483e-02
 -6.47783393e-01 -4.76932134e-02 -6.41032531e-01 -5.08792112e-02
  4.43651883e-02  5.10061721e-03  4.39289453e-02  5.34473134e-03
 -5.07620563e-02 -4.25224618e-03 -5.02263228e-02 -4.51044812e-03
 -3.27038676e-02 -2.89397793e-02 -2.92295329e-02 -2.41377777e-02
  2.22239979e-02  1.95779593e-02  2.03685450e-02  1.66084300e-02
 -3.29185554e-02 -2.91485969e-02 -2.95575377e-02 -2.44064688e-02
  2.78525994e-02  2.46047176e-02  2.52295737e-02  2.07122927e-02
 -6.84641509e-02 -4.47986205e-02 -1.70817189e-02 -1.69479295e-02
 -5.63985021e-02 -3.66771089e-02 -1.42488552e-02 -1.40786493e-02
 -5.92938456e-02 -4.01663771e-02 -1.27816910e-02 -1.29542512e-02
 -3.45958627e-02 -2.28747451e-02 -7.88245605e-03 -7.86246373e-03
 -1.22870227e-02  2.29217926e-02 -5.53773976e-03  4.97546704e-03
  9.43314914e-04 -4.62137814e-02 -7.52359849e-04 -5.10753577e-02
  6.61154826e+00]
supnorm grad right now is: 6.611548260859411
Weights right now are: 
[ 0.76066994  0.36114007  0.17124718  0.84385975 -1.55088918 -0.34667643
  0.40005887  0.5712233  -1.52783244  0.18187659 -1.31550139  0.66043735
  0.46450031  1.07052506  0.40999475  0.75220706  1.33530463  0.27128554
  1.90935853  0.03404118  1.86161685  1.61872807  1.39288698  1.04661576
 -0.77468169 -1.22288654 -1.77662169 -0.94169358  1.74438679  1.87682348
  0.95174306  1.34262278 -1.90048966 -1.11415423 -0.61173454 -0.68854625
  1.88460645  1.21804497  1.93156868  0.48356224 -1.59632739 -0.25990115
  3.31936649  4.21950708 -0.93970471 -0.87081277  3.85269377  3.89264488
 -1.54031884  0.51787571  2.70409087  3.49472417 -0.68600113 -0.33788706
  3.61290358  3.25896131  0.76660964  3.94546182  1.75035425  1.30473079
  0.28434486 -4.23046506  0.39586782 -4.24598685 30.99961074]
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1356.0019884194407
gradient value of function right now is: [ 8.05749529e+00  7.20317768e-01  1.64274294e-01  1.55680323e-01
 -8.05749529e+00 -7.20317768e-01 -1.64274294e-01 -1.55680323e-01
 -1.97166115e+00 -4.74567981e-01 -1.95085752e+00 -4.58968169e-01
 -1.37017493e-01 -1.81655955e-02 -1.35641009e-01 -1.72574890e-02
  1.74349441e-02  4.07629647e-03  1.72540534e-02  3.94347752e-03
 -3.79643283e-03 -6.09163068e-04 -3.75726828e-03 -5.82215121e-04
 -1.02119102e-01 -8.42570434e-02 -9.89365389e-02 -6.85713884e-02
  7.11868401e-02  5.85710167e-02  6.93959022e-02  4.76519755e-02
 -9.42948932e-02 -7.78184045e-02 -9.13427440e-02 -6.34054114e-02
  9.00035181e-02  7.40514713e-02  8.78189232e-02  6.02065653e-02
 -3.51156917e-01 -1.12013613e-01 -8.74877962e-02 -8.66791534e-02
 -2.90563327e-01 -9.14277261e-02 -7.01054363e-02 -6.94854529e-02
 -2.99499247e-01 -9.52367044e-02 -7.69969586e-02 -7.62069727e-02
 -1.77033715e-01 -5.40902388e-02 -4.26104840e-02 -4.21778033e-02
 -1.09259322e-01  6.39736252e-02  7.76048310e-02 -7.92757410e-02
  3.71500018e-01 -2.50478572e-01  4.14907531e-01 -2.85215599e-01
  4.83693653e-02]
supnorm grad right now is: 8.057495285827223
Weights right now are: 
[ 0.9140122   0.12342476  0.21565911  0.71918909 -1.70423144 -0.10896112
  0.35564693  0.69589396 -1.53851192  1.38914558 -1.33353953  1.92428049
  1.3343002   1.41941978  1.2684751   1.1585476   2.10299604  0.74111202
  2.62915822  0.49668607  2.45296788  1.83249175  1.96663446  1.35253624
 -1.12438458 -1.68873396 -2.01513759 -1.30672615  2.27944808  2.45152946
  1.47633588  1.9282526  -2.24072215 -1.56926474 -0.85705182 -1.03730428
  2.53008456  1.91634812  2.53454229  1.17018357 -1.07178406  1.02997531
  4.51717825  5.59882828 -0.57108936  0.13650949  5.28948854  5.52042154
 -0.92565848  2.0106934   3.76810366  4.73284192 -0.17552275  0.82957589
  5.00135784  4.83443185 -0.21264655  2.65229226  3.02049325 -0.7354882
  1.94222151 -3.16817946  2.01476124 -3.10867708 30.95621206]
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1360.058730775333
gradient value of function right now is: [ 1.57974595e+00  8.75566166e-02  3.05228993e-02  1.92762316e-02
 -1.57974595e+00 -8.75566166e-02 -3.05228993e-02 -1.92762316e-02
 -1.11878634e+00 -6.24031725e-01 -1.10424899e+00 -6.10267682e-01
 -1.22853162e-02 -7.97726020e-03 -1.21523351e-02 -7.81550492e-03
 -7.23002836e-03 -4.62075051e-03 -7.14871239e-03 -4.53746498e-03
 -2.16564537e-03 -1.40382989e-03 -2.14000586e-03 -1.37459182e-03
 -4.73805671e-02 -3.97282413e-02 -4.96159355e-02 -3.31239525e-02
  6.59351849e-02  5.54323717e-02  6.78998760e-02  4.56061633e-02
 -4.10273845e-02 -3.44354256e-02 -4.30823183e-02 -2.88381270e-02
  8.30811070e-02  6.98959167e-02  8.53978534e-02  5.74536434e-02
 -1.70542825e-01 -7.69458072e-02 -8.37588518e-02 -8.34253691e-02
 -1.35001821e-01 -6.17512459e-02 -6.79957736e-02 -6.76993923e-02
 -1.53922661e-01 -6.83474438e-02 -7.29343967e-02 -7.26937847e-02
 -8.31109854e-02 -3.79794894e-02 -4.17992526e-02 -4.16183120e-02
 -2.13119355e-02  2.27761655e-02  7.82520892e-02 -1.29309897e-01
  2.98863358e-01 -5.20648775e-01  3.30750698e-01 -5.80125897e-01
  7.64100534e+00]
supnorm grad right now is: 7.64100533615606
Weights right now are: 
[ 0.8790808   0.14206733  0.4990865   0.81358195 -1.66930003 -0.12760369
  0.07221954  0.6015011  -1.56122498  1.96621087 -1.35923262  2.54153351
  1.97503199  1.73145756  1.90105465  1.51193773  2.76036898  1.50933843
  3.25683992  1.27522812  3.13897102  2.32995555  2.63276927  2.06542537
 -1.28115718 -1.89531292 -2.09273539 -1.50390739  2.60004446  2.78944761
  1.83828342  2.30886137 -2.37964116 -1.75847201 -0.91327123 -1.2195821
  2.86719078  2.27312629  2.90921111  1.57826346 -0.6030937   1.58905812
  5.73051934  6.86918717 -0.11905129  0.66568799  6.66560353  6.95176257
 -0.34820275  2.71470751  4.87892074  5.90410898  0.50980811  1.58340542
  6.44017174  6.33400454  1.78573003 -0.45769045  3.00841192 -2.85063473
  2.19571138 -3.07051712  2.22267261 -3.06751829 30.93567582]
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1356.4741401646993
gradient value of function right now is: [ 1.03879684e+01  2.22138972e-01  8.76489807e-02  4.67537018e-02
 -1.03879684e+01 -2.22138972e-01 -8.76489807e-02 -4.67537018e-02
 -3.48582452e+00 -1.58828619e+00 -3.43687170e+00 -1.58066625e+00
 -1.02116651e-01 -2.80729639e-02 -1.00464397e-01 -2.79539741e-02
 -5.52856479e-02 -2.23355485e-02 -5.44815881e-02 -2.22366843e-02
 -1.79404254e-02 -5.45782590e-03 -1.76531151e-02 -5.43317588e-03
 -1.53388450e-01 -1.41109978e-01 -1.67509492e-01 -1.35336675e-01
  1.25003642e-01  1.13801369e-01  1.37736140e-01  1.08214411e-01
 -1.41384055e-01 -1.30299043e-01 -1.54157898e-01 -1.25141949e-01
  1.62943962e-01  1.48349069e-01  1.79525623e-01  1.41058331e-01
 -8.36007801e-02 -8.42638011e-02 -1.23263564e-01 -1.22822900e-01
 -7.28802945e-02 -7.34631149e-02 -1.08458891e-01 -1.08046982e-01
 -7.80287219e-02 -7.86450306e-02 -1.14000479e-01 -1.13622134e-01
 -5.14951394e-02 -5.19117321e-02 -7.71833837e-02 -7.68806324e-02
  3.58485647e-02 -5.91762900e-02  2.12660514e-01 -3.58780668e-01
  8.24559221e-01 -1.27885483e+00  8.96257506e-01 -1.39534129e+00
  8.16636550e-01]
supnorm grad right now is: 10.38796839972763
Weights right now are: 
[ 1.10881676  0.31838996  0.75215134  0.96973882 -1.899036   -0.30392632
 -0.18084529  0.44534423 -1.76862549  1.75485949 -1.56552135  2.33089099
  2.2901996   1.95871916  2.21197314  1.77029079  2.84479663  1.47817886
  3.3332144   1.23996321  3.4387377   2.47710043  2.92223793  2.2779877
 -1.17573632 -1.86678074 -1.87681475 -1.47507265  2.58908157  2.82160258
  1.79536563  2.36301105 -2.29231428 -1.74840191 -0.70773509 -1.20240458
  2.79508801  2.24326838  2.80664514  1.57315332  0.03600851  2.73393157
  6.84895281  7.99683523  0.50934697  1.85825195  7.88427982  8.17769017
  0.38467916  3.86779186  5.90851652  6.94604166  1.24137169  2.88442635
  7.73582448  7.63771795  2.55376744 -3.2143981   2.55319372 -3.20384807
  1.76014316 -2.80874902  1.78093493 -2.80404801 30.95081576]
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1359.94001692157
gradient value of function right now is: [-2.09282681e+00  1.16647323e-03  6.87506083e-03  1.38300934e-03
  2.09282681e+00 -1.16647323e-03 -6.87506083e-03 -1.38300934e-03
 -7.89970334e-01 -7.36437773e-01 -7.76076172e-01 -7.26585781e-01
  2.54688133e-03 -2.99681826e-03  2.53718790e-03 -2.94485668e-03
 -1.47327055e-03 -2.28498698e-03 -1.44466700e-03 -2.25512817e-03
  1.49247251e-04 -6.76757793e-04  1.52753451e-04 -6.65525152e-04
 -2.63342825e-02 -1.93164537e-02 -3.73502184e-02 -1.71134850e-02
  4.04501201e-02  3.21435797e-02  5.34305623e-02  2.95849377e-02
 -2.10963904e-02 -1.49594852e-02 -3.07428719e-02 -1.30196205e-02
  5.04043847e-02  4.00191441e-02  6.66374249e-02  3.68126382e-02
 -4.09547033e-02 -4.78835479e-02 -7.43781436e-02 -7.36972100e-02
 -3.24208553e-02 -3.78409098e-02 -5.90877908e-02 -5.85261539e-02
 -4.71193028e-02 -5.51744346e-02 -8.51461887e-02 -8.44001813e-02
 -2.45538704e-02 -2.86544300e-02 -4.49454196e-02 -4.45090618e-02
  1.14754465e-02 -7.37966217e-03  1.40765057e-01 -9.55045505e-02
  5.36245916e-01 -2.92258921e-01  5.88508109e-01 -3.24303178e-01
 -6.01924714e+00]
supnorm grad right now is: 6.019247138216392
Weights right now are: 
[ 7.22159577e-01  2.18351797e-01  5.69470450e-01  9.08352869e-01
 -1.51237881e+00 -2.03888159e-01  1.83559587e-03  5.06730179e-01
 -1.72348057e+00  2.11545641e+00 -1.50135946e+00  2.70059127e+00
  2.89589678e+00  2.70661386e+00  2.82261549e+00  2.58094512e+00
  3.45166504e+00  2.26769000e+00  3.93459192e+00  2.03988843e+00
  4.02561935e+00  3.17201957e+00  3.50892043e+00  3.14579005e+00
 -1.73872805e+00 -2.43599143e+00 -2.46734659e+00 -2.06173967e+00
  3.28372675e+00  3.49397356e+00  2.56774501e+00  3.05705611e+00
 -2.82537628e+00 -2.28833301e+00 -1.26462821e+00 -1.75715424e+00
  3.49503695e+00  2.92140350e+00  3.58360445e+00  2.27388771e+00
 -1.69672658e-01  2.51049075e+00  6.38046281e+00  7.53968272e+00
  3.96189729e-01  1.75043258e+00  7.58842136e+00  7.88961439e+00
  2.59241803e-02  3.47467191e+00  5.19954249e+00  6.25282594e+00
  1.14993500e+00  2.80575088e+00  7.48411411e+00  7.39305257e+00
  3.89248332e+00 -3.77587543e+00  3.69732424e+00 -3.34151561e+00
  2.65450485e+00 -2.73469092e+00  2.68386984e+00 -2.73522138e+00
  3.08493629e+01]
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1357.5326168933852
gradient value of function right now is: [ 5.23722433e+00  1.60582957e-02  7.25317776e-03  3.35977433e-03
 -5.23722433e+00 -1.60582957e-02 -7.25317776e-03 -3.35977433e-03
 -2.30449469e+00 -1.29899227e+00 -2.27302976e+00 -1.28616040e+00
 -5.31708516e-03 -2.19048400e-03 -5.23165526e-03 -2.15966438e-03
 -2.16384356e-03 -1.16130656e-03 -2.13330105e-03 -1.14948372e-03
 -8.84798093e-04 -3.86382193e-04 -8.70460777e-04 -3.81133513e-04
 -7.92902833e-02 -6.59282900e-02 -1.03012977e-01 -6.25258092e-02
  7.77502002e-02  6.42791350e-02  1.01615075e-01  6.05847289e-02
 -7.14373311e-02 -5.94628627e-02 -9.27029227e-02 -5.64480067e-02
  9.76712102e-02  8.07377283e-02  1.27664474e-01  7.60771153e-02
 -2.74514482e-02 -5.65773578e-02 -9.63913238e-02 -9.62804517e-02
 -2.21931371e-02 -4.64239204e-02 -8.14880751e-02 -8.12902101e-02
 -3.16337408e-02 -6.37541443e-02 -1.03173047e-01 -1.03302682e-01
 -1.66246037e-02 -3.51525471e-02 -6.23098135e-02 -6.21478590e-02
  1.23146396e-02 -3.04815948e-02  1.27858107e-01 -2.99667644e-01
  5.10958203e-01 -1.13630485e+00  5.54148307e-01 -1.24214690e+00
 -2.39450651e+01]
supnorm grad right now is: 23.945065121800326
Weights right now are: 
[ 0.84270196  0.24221924  0.51951989  0.89816352 -1.63292119 -0.2277556
  0.05178616  0.51691953 -1.71404823  2.04769532 -1.49420461  2.62420695
  3.38292415  3.26552984  3.30898873  3.15236485  3.99845701  2.74660141
  4.46985644  2.51492229  4.56425559  3.71478719  4.04167408  3.75740589
 -1.40789216 -2.25201858 -1.97218079 -1.92997852  3.02611934  3.27163453
  2.2976661   2.84037696 -2.53629532 -2.15515824 -0.79643478 -1.67937731
  3.20793362  2.67256857  3.28077205  2.03254036  0.21588029  3.03938567
  7.04448831  8.1985817   0.89816844  2.41753783  8.38362681  8.67818359
  0.26713515  3.81717703  5.66673926  6.71746437  1.76151185  3.5866416
  8.38123063  8.28346201  3.97578623 -4.5833922   3.06848624 -3.40079837
  1.69006923 -2.83812493  1.72703989 -2.8244127  30.60733663]
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -1358.8332857867654
gradient value of function right now is: [-8.86284172e+00 -1.56105587e-02 -6.57934645e-03 -3.24889991e-03
  8.86284172e+00  1.56105587e-02  6.57934645e-03  3.24889991e-03
  4.77774582e+00  2.07421910e+00  4.65164411e+00  2.00787707e+00
  3.49108823e-03  1.01655656e-03  3.39621664e-03  9.80572619e-04
  1.33019624e-03  5.48375121e-04  1.29584426e-03  5.30860172e-04
  5.36773705e-04  1.65310987e-04  5.22372451e-04  1.59518014e-04
  1.64529144e-01  1.32453409e-01  2.43681950e-01  1.28537782e-01
 -1.63723423e-01 -1.23631574e-01 -2.71679813e-01 -1.18626234e-01
  1.43275554e-01  1.15918553e-01  2.10080402e-01  1.12591732e-01
 -1.99896926e-01 -1.50403975e-01 -3.33705962e-01 -1.44233461e-01
  3.42363159e-02  8.12938283e-02  1.97250926e-01  1.92115410e-01
  2.25097366e-02  5.64639480e-02  1.36567684e-01  1.33427368e-01
  6.29763380e-02  1.38464654e-01  3.38647454e-01  3.28393572e-01
  1.67950477e-02  4.27335028e-02  1.04639322e-01  1.02201857e-01
 -2.53048419e-02  2.68279071e-02 -3.21860659e-01  3.28416908e-01
 -1.69664021e+00  1.31761137e+00 -1.85314487e+00  1.45002598e+00
  1.08045946e+01]
supnorm grad right now is: 10.804594555342195
Weights right now are: 
[ 0.80615771  0.14939733  0.4117182   0.80868637 -1.59637695 -0.13493369
  0.15958785  0.60639667 -1.20244807  2.27979022 -0.96843618  2.86951724
  3.91905505  3.81040437  3.84134745  3.70066239  4.47288415  3.35996722
  4.9326489   3.12206583  5.10263715  4.27157312  4.57092013  4.36494517
 -1.12083712 -2.02903864 -1.66960418 -1.72217323  3.0418431   3.14952334
  2.62740216  2.70095391 -2.24877121 -1.95090453 -0.45512117 -1.4920165
  3.25411458  2.58442727  3.637248    1.93095117  0.36362901  3.07379787
  6.86515741  8.020092    1.14300133  2.55190475  8.504321    8.785603
  0.25498216  3.64072145  4.90468952  5.98090133  2.01320852  3.72869477
  8.55836654  8.44477946  5.14380844 -5.14153866  3.86687105 -3.65442861
  1.92507423 -2.80902574  2.00920771 -2.80085738 30.83193295]
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1357.2972933703427
gradient value of function right now is: [-6.46841112e+00 -3.37494696e-03 -9.09613214e-04 -6.16908004e-04
  6.46841112e+00  3.37494696e-03  9.09613214e-04  6.16908004e-04
  1.36827046e+00  1.50586640e-01  1.31491031e+00  1.25403159e-01
  1.94425953e-03  1.87911289e-04  1.88640493e-03  1.69191480e-04
  5.16999194e-04  6.09658434e-05  4.99716425e-04  5.35221160e-05
  3.08700086e-04  3.24147926e-05  2.99442768e-04  2.91592010e-05
  6.99222954e-02  5.57404008e-02  1.18116677e-01  5.56689661e-02
 -3.65677404e-02 -2.75523639e-02 -7.25909920e-02 -2.78975295e-02
  6.68330562e-02  5.35956434e-02  1.10762210e-01  5.34545108e-02
 -4.56962303e-02 -3.42525049e-02 -9.20626523e-02 -3.46990330e-02
 -2.05198134e-03  1.76593910e-03  5.40296250e-02  4.81059859e-02
 -7.94405512e-04  2.72191957e-03  4.00696421e-02  3.60768378e-02
 -8.24485751e-03 -1.10827868e-03  1.17987237e-01  1.03894727e-01
 -1.84112812e-04  3.10322370e-03  3.44195976e-02  3.11329601e-02
 -6.01674418e-03 -1.35639301e-03 -1.62503079e-01 -5.68694705e-02
 -1.43253903e+00 -7.00802239e-02 -1.51823883e+00 -1.00613601e-01
  2.93927414e+01]
supnorm grad right now is: 29.392741411437758
Weights right now are: 
[ 0.79950324  0.35735648  0.63706096  1.01327193 -1.58972248 -0.34289284
 -0.06575492  0.40181112 -1.61790931  1.67492557 -1.37473426  2.26346016
  4.31449511  4.22791951  4.23454024  4.08122317  4.92212967  3.6913156
  5.36781786  3.42429451  5.52173429  4.65842858  4.98084112  4.73673106
 -1.50193522 -2.58063187 -1.92201946 -2.30945269  3.20417941  3.34868798
  3.02399523  2.91532706 -2.64277265 -2.52068038 -0.68323534 -2.09740193
  3.40489479  2.76208001  4.04997881  2.12346131  0.51805628  3.19198836
  6.93967764  8.08365879  1.71005681  2.91852772  8.83394032  9.09186339
 -0.46715083  2.94440255  4.05577096  5.14039912  2.7340022   4.19276676
  8.95204384  8.81537397  4.93000494 -5.4563125   3.77951683 -4.1893409
  1.73726415 -3.22029584  1.85615521 -3.21888682 31.08746822]
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1357.1324558609917
gradient value of function right now is: [-3.54441377e+00  1.84633513e-04  3.24705523e-04  8.81066054e-05
  3.54441377e+00 -1.84633513e-04 -3.24705523e-04 -8.81066054e-05
 -5.34441053e-01 -6.15127616e-01 -4.88964177e-01 -5.82651443e-01
  3.72864585e-05 -1.05623134e-04  4.96893639e-05 -9.65585429e-05
 -1.05147180e-04 -9.20212271e-05 -9.46959906e-05 -8.54118254e-05
 -1.16259774e-05 -2.29392467e-05 -8.68136912e-06 -2.10051049e-05
 -7.56551537e-03  9.89266799e-04 -6.97560698e-02  2.32077557e-03
  1.31291023e-02  6.80522681e-03  5.73407152e-02  5.86104063e-03
 -3.84197254e-03  3.65598314e-03 -5.91276953e-02  4.83848097e-03
  1.66862904e-02  8.51838307e-03  7.48852345e-02  7.29335901e-03
 -1.14879876e-02 -1.84342133e-02 -3.66004250e-02 -3.50793057e-02
 -6.65925872e-03 -1.06182570e-02 -2.12531628e-02 -2.02877403e-02
 -5.55584254e-02 -7.98032408e-02 -1.43473346e-01 -1.38494706e-01
 -5.03890641e-03 -7.98687452e-03 -1.61120831e-02 -1.53484461e-02
  1.75935239e-03 -3.00309362e-03  7.50861316e-02 -6.97592244e-02
  2.32337547e-01 -1.47511113e-01  2.64167641e-01 -1.78034720e-01
  1.49195585e+01]
supnorm grad right now is: 14.919558470725562
Weights right now are: 
[ 0.73218484  0.26723958  0.63481926  0.94745794 -1.52240408 -0.25277594
 -0.06351322  0.4676251  -1.85108875  1.46135356 -1.5955093   2.05109291
  4.92116209  4.76133095  4.8352925   4.57340106  5.40525071  4.0695984
  5.83181405  3.77179041  6.0937006   5.13946687  5.53967823  5.19931759
 -1.78753555 -3.01140903 -2.10280957 -2.76909019  3.75505979  3.95212685
  3.7925275   3.54170243 -2.89525653 -2.92767346 -0.78789788 -2.53199542
  3.91202375  3.32108526  4.77874655  2.70918187  0.87717536  3.49839639
  6.85515071  8.02168364  2.30438896  3.18119441  8.88618809  9.12772632
 -0.82471373  2.92813546  3.15018429  4.34434933  3.35578026  4.41636606
  8.99259495  8.83336776  5.394185   -5.48706036  4.91542347 -3.89101573
  2.69392594 -2.96658191  2.88464154 -2.95559612 31.0327185 ]
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1361.181016304069
gradient value of function right now is: [ 1.82170449e+00  2.91339192e-04  8.89235759e-05  5.57853118e-05
 -1.82170449e+00 -2.91339192e-04 -8.89235759e-05 -5.57853118e-05
 -8.12690659e-01 -9.81326995e-02 -7.49882423e-01 -7.26812114e-02
 -4.25591702e-05 -8.17934416e-06 -3.89179780e-05 -6.47721754e-06
 -3.86899249e-06 -4.63353419e-07 -3.54696371e-06 -3.25916213e-07
 -5.49433249e-07 -9.59586220e-08 -5.03198288e-07 -7.47353639e-08
 -3.35044158e-02 -2.08775813e-02 -9.04251833e-02 -1.97955899e-02
  4.18163279e-02  2.75200828e-02  9.42008092e-02  2.61639837e-02
 -2.94047809e-02 -1.80567742e-02 -8.21814868e-02 -1.71051125e-02
  4.49313867e-02  2.96293841e-02  9.53499929e-02  2.81650634e-02
 -8.54308230e-03 -1.42007263e-02 -5.42760792e-02 -4.47499939e-02
 -7.83503744e-03 -1.16479107e-02 -4.71637453e-02 -3.80035501e-02
  2.49670496e-02 -1.83763337e-03  2.60066766e-03 -7.13486011e-03
 -6.66874282e-03 -9.75411428e-03 -4.00343425e-02 -3.21100834e-02
  3.68703082e-02 -3.22934233e-02  3.57987147e-02 -1.33110622e-01
  5.86916215e-01 -5.02166456e-01  4.57715872e-01 -5.14870956e-01
 -6.46868508e+00]
supnorm grad right now is: 6.468685081473072
Weights right now are: 
[ 8.81452546e-01  1.13996238e-01  3.19963091e-01  7.18978051e-01
 -1.67167178e+00 -9.95326004e-02  2.51342955e-01  6.96104997e-01
 -1.39669699e+00  2.34459607e+00 -1.14747949e+00  2.95671012e+00
  5.53947083e+00  5.61838530e+00  5.41453917e+00  5.37128051e+00
  6.27256713e+00  4.80544784e+00  6.65061610e+00  4.45852500e+00
  6.90292780e+00  5.85542849e+00  6.30737087e+00  5.88168747e+00
 -1.70425362e+00 -3.49204736e+00 -1.18099793e+00 -3.29554912e+00
  3.16648258e+00  3.62714411e+00  3.53015848e+00  3.24650428e+00
 -2.92649169e+00 -3.54479985e+00  1.66604487e-01 -3.19400818e+00
  3.34248453e+00  2.98971477e+00  4.58659171e+00  2.40838400e+00
  2.32358849e+00  4.53286421e+00  8.06100711e+00  9.18005194e+00
  4.03751697e+00  3.86913577e+00  1.02632092e+01  1.03018009e+01
 -2.95527511e-02  4.34860685e+00  2.72992049e+00  4.28409910e+00
  5.12777043e+00  5.05080661e+00  1.03862061e+01  1.00049558e+01
  4.27721789e+00 -5.21184758e+00  4.35002914e+00 -3.85917254e+00
  1.85783555e+00 -2.97074682e+00  2.30962755e+00 -2.95005384e+00
  3.08103351e+01]
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1362.0992857682868
gradient value of function right now is: [ 2.19418223e+00 -8.11810243e-05 -2.95113378e-05 -2.01097638e-05
 -2.19418223e+00  8.11810243e-05  2.95113378e-05  2.01097638e-05
 -1.25913987e+00 -3.73076812e-01 -1.18315707e+00 -3.07687714e-01
  5.15118348e-05  2.62044381e-06  2.91838085e-05 -1.64375902e-06
  2.05595179e-05 -3.31442091e-07  1.12806854e-05 -1.84042523e-06
  1.41115071e-05  6.28439236e-07  8.36435053e-06 -4.51504005e-07
 -2.48400427e-02 -1.70665998e-02 -9.47431771e-02 -1.58034212e-02
  3.33878019e-02  2.28738804e-02  1.88981193e-01  2.12279010e-02
 -2.25357893e-02 -1.54508274e-02 -8.33393271e-02 -1.42987544e-02
  3.23048730e-02  2.18007060e-02  2.11554513e-01  2.01680406e-02
 -1.10276123e-02 -1.22051866e-02 -3.95812303e-02 -3.38971851e-02
 -1.53381838e-02 -1.14429564e-02 -3.84881805e-02 -3.18506762e-02
 -1.87291775e-01 -6.38483279e-02 -2.54503032e-01 -2.02481818e-01
 -1.40885288e-02 -9.87009901e-03 -3.33709096e-02 -2.74367065e-02
  5.66414900e-02 -8.87576951e-02  1.94388941e-01 -1.55001357e-01
  4.14104002e-01 -5.41793355e-01  4.90814579e-01 -5.55660303e-01
 -3.00313432e+00]
supnorm grad right now is: 3.0031343197334444
Weights right now are: 
[ 0.95455     0.50766459  0.8139737   1.24782975 -1.74476923 -0.49320095
 -0.24266766  0.1672533  -1.30367354  2.86590194 -1.02016426  3.57843613
  7.16234906  7.35055266  6.80341877  6.75395915  7.93968573  6.13349812
  8.10114027  5.48907061  8.71433285  7.03060393  7.87004674  6.75946713
 -2.17565179 -4.48976795 -0.98055331 -4.36763929  3.08973034  4.03983527
  3.62019816  3.74661829 -3.51504627 -4.68237     0.41834427 -4.40690238
  3.24662718  3.3496847   4.80170537  2.8509354   4.5263502   5.21616512
  9.23873872 10.13965082  6.85492423  4.14400465 11.25694135 10.93401415
 -1.81493107  5.2089739   1.58674726  3.60957852  8.06384931  5.25721622
 11.3386579  10.57536941  0.16894213 -6.55183732  4.8129646  -4.1489241
  1.37137042 -3.48510647  2.45581767 -3.19891924 30.83569538]
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1361.253801294463
gradient value of function right now is: [ 5.05168413e+00  7.39561989e-04  2.69319990e-04  2.00420866e-04
 -5.05168413e+00 -7.39561989e-04 -2.69319990e-04 -2.00420866e-04
 -2.84026538e+00 -4.18156480e-01 -2.38837271e+00 -2.91614271e-01
 -1.82450979e-03 -3.69425665e-04 -1.45200159e-03 -2.70380565e-04
 -6.22832646e-04 -1.37559129e-04 -4.85224871e-04 -1.00124414e-04
 -5.21868648e-04 -1.13313638e-04 -4.07902612e-04 -8.26482670e-05
 -3.55654680e-02 -2.45654873e-02 -3.88822559e-01 -2.29416705e-02
  4.53975892e-02  3.12903934e-02  5.23682045e-01  2.92690635e-02
 -2.86602393e-02 -1.98954735e-02 -2.90658690e-01 -1.85907601e-02
  4.51103034e-02  3.12032303e-02  5.02891117e-01  2.92270112e-02
 -3.11246294e-02 -1.23683492e-02 -4.74407690e-02 -3.91599210e-02
 -2.77554049e-02 -1.30460098e-02 -4.79173804e-02 -4.03463936e-02
 -8.39171259e-01 -7.18268793e-02 -4.96894335e-01 -3.36104972e-01
 -2.38059325e-02 -1.18121950e-02 -4.27128988e-02 -3.62052315e-02
 -2.87778057e-01 -1.23612732e-01  3.89728519e-01 -2.15516275e-01
  9.90509884e-01 -8.06321799e-01  1.15636543e+00 -8.37570277e-01
 -1.45810920e+01]
supnorm grad right now is: 14.581091964152476
Weights right now are: 
[ 0.99560489  1.79820017  2.01017535  2.63963492 -1.78582412 -1.78373653
 -1.43886931 -1.22455188 -1.32173963  3.10401942 -0.87692773  3.8484394
  5.55210538  8.35803574  5.70898799  8.63481188  6.4301839   7.15181916
  7.16369247  6.96278175  6.92283124  7.9616632   6.63300531  8.16332366
 -2.33800975 -5.04409991 -1.27638295 -5.02705455  3.01417974  4.35354693
  3.95567913  4.17556376 -3.68366806 -5.22732631  0.27371557 -5.04901137
  3.24165125  3.75748901  4.87829131  3.37632742  5.47619238  6.4284212
 10.22721898 11.15992528  8.30650179  5.14269609 12.09288408 11.67122198
 -1.97596207  5.38080321  1.76182995  3.90425684  9.62917316  6.19254979
 12.11229021 11.22010342 -0.46524837 -6.56298492  4.12727059 -4.32662143
  1.42254551 -3.35245792  2.22677033 -3.1480168  30.71138534]
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1356.9353523638883
gradient value of function right now is: [ 6.04223619e+00  6.85898454e-04  2.75507808e-04  2.17798880e-04
 -6.04223619e+00 -6.85898454e-04 -2.75507808e-04 -2.17798880e-04
 -3.26584976e+00 -7.94858041e-01 -2.63916729e+00 -6.40003095e-01
 -1.74893914e-03 -4.21490612e-04 -1.31639728e-03 -3.09533278e-04
 -6.54092014e-04 -1.77613993e-04 -4.79192014e-04 -1.30662523e-04
 -5.88383224e-04 -1.55252284e-04 -4.33971458e-04 -1.13982268e-04
 -4.59952098e-02 -3.14160578e-02 -4.64560040e-01 -2.86392685e-02
  4.66081647e-02  3.13838277e-02  5.79723632e-01  2.85379386e-02
 -4.21310201e-02 -2.93240556e-02 -3.65694331e-01 -2.68626147e-02
  4.44685362e-02  3.01267897e-02  5.56658180e-01  2.74530482e-02
 -4.96584650e-02 -2.09706326e-02 -6.02087737e-02 -5.26755061e-02
 -4.14478407e-02 -2.15115167e-02 -5.81617240e-02 -5.24151618e-02
 -8.27738713e-01 -1.65437374e-01 -5.30203745e-01 -4.12762144e-01
 -3.48663583e-02 -1.94311633e-02 -5.15123729e-02 -4.68703453e-02
  6.16909765e-04 -2.37220448e-01  4.50349621e-01 -3.31681493e-01
  1.01965961e+00 -9.04332317e-01  1.15784047e+00 -9.69000739e-01
 -2.96884924e+01]
supnorm grad right now is: 29.688492392836185
Weights right now are: 
[ 0.92426433  2.00901846  2.22243325  2.89403792 -1.71448356 -1.99455482
 -1.6511272  -1.47895487 -1.34110841  2.9961367  -0.96793374  3.64097782
  5.98623435  9.07172619  6.29343188  9.65744407  6.91647702  8.01694268
  7.81725907  8.05666954  7.32784036  8.7538612   7.20547556  9.23131397
 -2.9889075  -5.86248412 -0.64693145 -5.91905167  3.81171185  5.33224321
  3.83499398  5.22111234 -4.14341209 -5.91725087  1.1626528  -5.83431073
  4.05172994  4.78600755  4.78970447  4.47812131  5.49830172  6.86703653
 10.50881311 11.42869423  8.9790449   5.4137365  12.35715514 11.786823
 -1.837383    5.2013128   1.71002457  3.79480704 10.56387498  6.43206295
 12.37564989 11.30191757 -0.19790761 -6.5983227   4.37960613 -4.1709249
  1.73904885 -3.63972886  2.47698257 -3.33658056 30.52126767]
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -1360.4100380600198
gradient value of function right now is: [ 3.03354507e+00 -4.81067292e-04 -1.58767107e-04 -1.57984427e-04
 -3.03354507e+00  4.81067292e-04  1.58767107e-04  1.57984427e-04
 -1.97805726e+00 -1.27139090e+00 -1.72724189e+00 -1.18128377e+00
  1.85138391e-03  7.39618204e-05  9.11434290e-04 -2.68599755e-05
  6.05776581e-04 -1.54481069e-05  2.59216073e-04 -4.91179475e-05
  6.49685765e-04  1.77585941e-05  3.05656774e-04 -1.85195226e-05
 -8.82394297e-03 -4.60440702e-03 -2.20649583e-01 -3.66290786e-03
  7.95105078e-03  4.54669550e-03  2.12336561e-01  3.83029762e-03
 -4.99869673e-03 -1.73889845e-03 -1.68195818e-01 -9.99639930e-04
  9.13089760e-03  5.15296917e-03  2.64137875e-01  4.32933518e-03
 -8.87213923e-03 -9.10147525e-03 -1.68698653e-02 -1.64817707e-02
 -9.12917499e-03 -7.85082086e-03 -1.47687916e-02 -1.41706493e-02
 -3.73841325e-01 -2.13414573e-01 -3.93510888e-01 -3.52430291e-01
 -7.91589267e-03 -6.64211280e-03 -1.24822537e-02 -1.19392154e-02
 -5.43994868e-02 -1.69204388e-02  2.96361975e-01 -2.99235566e-01
  2.15701874e-01 -3.36135886e-01  3.73079283e-01 -4.66757344e-01
 -8.21037243e+00]
supnorm grad right now is: 8.210372434826853
Weights right now are: 
[ 8.68722607e-01  2.98387113e+00  3.21040778e+00  3.90074938e+00
 -1.65894184e+00 -2.96940749e+00 -2.63910174e+00 -2.48566634e+00
 -1.74396214e+00  1.87565867e+00 -1.29035364e+00  2.47846810e+00
  5.22473973e+00  8.52388778e+00  5.53831150e+00  9.23672302e+00
  6.16712118e+00  7.48950205e+00  7.08341065e+00  7.66019356e+00
  6.53258437e+00  8.16633602e+00  6.41611688e+00  8.77183546e+00
 -2.87767844e+00 -5.77321671e+00 -9.69819660e-01 -5.91703636e+00
  3.76077988e+00  5.22632341e+00  4.06589895e+00  5.15731482e+00
 -3.79607913e+00 -5.75491269e+00  1.05172208e+00 -5.80871619e+00
  4.01764853e+00  4.74974009e+00  4.89561247e+00  4.49440832e+00
  5.37514261e+00  7.79092312e+00  1.10055694e+01  1.20515653e+01
  9.73456587e+00  6.20766068e+00  1.30654160e+01  1.23914915e+01
 -1.56240205e+00  5.48388919e+00  1.73204656e+00  3.92882887e+00
  1.16334837e+01  7.18343047e+00  1.31151138e+01  1.18819572e+01
 -9.23253517e-03 -5.18405978e+00  3.93817030e+00 -3.86662488e+00
  1.43852554e+00 -3.42976544e+00  2.27091526e+00 -3.15762222e+00
  3.07923614e+01]
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1360.364463232021
gradient value of function right now is: [ 6.78594655e+00  1.19545106e-02  5.41709687e-03  4.87109558e-03
 -6.78594655e+00 -1.19545106e-02 -5.41709687e-03 -4.87109558e-03
 -2.98073458e+00 -1.06171133e+00 -2.37502584e+00 -9.72667295e-01
 -7.83944709e-02 -1.32594961e-02 -5.59510336e-02 -1.08978737e-02
 -3.48070968e-02 -6.90333573e-03 -2.40865317e-02 -5.72628630e-03
 -3.36468549e-02 -6.28593543e-03 -2.34407759e-02 -5.18681543e-03
 -2.43720808e-02 -1.27687550e-02 -3.86902748e-01 -1.06478847e-02
  2.24616375e-02  1.15169986e-02  4.20065869e-01  9.59076237e-03
 -1.65037235e-02 -8.93804542e-03 -2.25219314e-01 -7.48096649e-03
  2.42155101e-02  1.25527763e-02  4.59914047e-01  1.04773884e-02
 -4.06482146e-02 -1.94521765e-02 -3.25504272e-02 -3.00850920e-02
 -2.51663606e-02 -1.62196550e-02 -2.49346806e-02 -2.41522757e-02
 -8.13745129e-01 -2.65300166e-01 -4.75888438e-01 -4.05807434e-01
 -2.02465945e-02 -1.40426424e-02 -2.11606190e-02 -2.07228620e-02
 -2.03886994e-01 -8.64519432e-02  5.98697548e-01 -6.01736812e-01
  5.83830173e-01 -6.19273913e-01  8.28421593e-01 -8.34485128e-01
 -1.04546045e+01]
supnorm grad right now is: 10.45460452680497
Weights right now are: 
[ 0.91867087  4.69671196  5.05645461  5.77098768 -1.7088901  -4.68224833
 -4.48514856 -4.35590463 -1.79172542  2.32482344 -1.12357047  2.94893939
  3.63683397  7.0496393   4.22331788  7.92168017  4.31154832  6.19034584
  5.406318    6.56142987  4.68017992  6.73172644  4.7464899   7.51578488
 -3.30190012 -6.51966651 -1.29207277 -6.8508434   4.15193242  5.74981796
  4.40672584  5.78817925 -4.17826655 -6.69569893  0.97296931 -6.98691631
  4.40671125  5.33011788  5.02801704  5.19747861  4.82231373  8.735221
 11.1327929  12.40612048 10.29648532  6.48491307 13.4046418  12.51685549
 -1.76236174  6.23713159  1.88955181  4.44112536 12.46494403  7.24227354
 13.41070974 11.86879386 -0.64810671 -5.17759816  3.36320311 -3.69155798
  1.63779804 -3.63112988  2.29772983 -3.34668432 30.84160317]
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1360.872164485749
gradient value of function right now is: [ 5.06758751e+00  2.25033145e-02  1.15106294e-02  1.03851833e-02
 -5.06758751e+00 -2.25033145e-02 -1.15106294e-02 -1.03851833e-02
 -2.56754179e+00 -5.12643503e-01 -1.70641993e+00 -4.33578674e-01
 -1.39801106e-01 -2.47289165e-02 -7.86113257e-02 -1.99702855e-02
 -7.02199059e-02 -1.33303218e-02 -3.76335527e-02 -1.07591147e-02
 -6.80187366e-02 -1.26174120e-02 -3.68934223e-02 -1.01787378e-02
 -2.42060664e-02 -1.43315559e-02 -3.24633049e-01 -1.27152710e-02
  2.78952599e-02  1.53446436e-02  5.87680338e-01  1.35154968e-02
 -1.40831209e-02 -9.24399976e-03 -9.04228524e-02 -8.32999380e-03
  2.88698141e-02  1.61200924e-02  5.91487557e-01  1.42436574e-02
 -4.45110626e-02 -1.84887813e-02 -3.14168221e-02 -2.86642583e-02
 -3.16993136e-02 -1.97519631e-02 -2.89112802e-02 -2.85708946e-02
 -1.08689246e+00 -1.38143937e-01 -4.14412158e-01 -2.85103610e-01
 -2.80683597e-02 -1.91038736e-02 -2.71055299e-02 -2.72276048e-02
 -3.63912269e-01 -2.33729742e-01  6.37867152e-01 -4.58912131e-01
  7.74054340e-01 -6.69770678e-01  1.02939367e+00 -7.96555443e-01
 -1.81355900e+01]
supnorm grad right now is: 18.135589969965572
Weights right now are: 
[ 0.94713627  5.02881769  5.33928455  6.08797548 -1.73735551 -5.01435405
 -4.7679785  -4.67289243 -1.57390593  2.75119933 -0.90925692  3.35042545
  3.54446317  7.60849127  4.28008991  8.72688041  4.25634814  6.84052611
  5.56506463  7.44869654  4.58509254  7.32895075  4.84850196  8.35921266
 -3.54569365 -7.57004217 -0.70418257 -8.23427059  4.15185998  6.5792114
  4.2104375   6.96159942 -4.76147891 -8.05555119  1.90297642 -8.61934667
  4.43417984  6.26632846  4.75479546  6.47457886  5.06570235  9.09832524
 11.94521232 13.05517807 11.56951784  5.90979301 13.71998722 12.32816945
 -1.8209841   6.2375652   1.69928026  4.35271874 13.88356128  6.4071962
 13.44764309 11.38578289 -0.80286755 -5.18495997  3.44402427 -3.48586497
  1.52407949 -3.75828679  2.3112913  -3.33214798 30.69167017]
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1359.6716657326065
gradient value of function right now is: [-6.22059696e+00 -6.06604527e-03 -2.17288405e-03 -2.16958536e-03
  6.22059696e+00  6.06604527e-03  2.17288405e-03  2.16958536e-03
  5.27673479e+00  2.02049170e+00  3.73589593e+00  1.83940623e+00
  3.59645618e-02  5.35798596e-03  2.24411240e-02  4.41653002e-03
  1.23992556e-02  2.01514484e-03  7.82898646e-03  1.68310692e-03
  1.35510802e-02  2.10218374e-03  8.50924471e-03  1.74323545e-03
  7.43645246e-02  4.37700409e-02  7.39428904e-01  4.02373439e-02
 -8.06463580e-02 -4.63761311e-02 -9.95277520e-01 -4.25592651e-02
  5.61937661e-02  3.65119361e-02  4.99565275e-01  3.40365115e-02
 -8.29189459e-02 -4.87918173e-02 -1.09044223e+00 -4.49207431e-02
  1.11692500e-01  5.61601943e-02  7.02534650e-02  7.56768438e-02
  8.59081687e-02  7.04646616e-02  7.90169854e-02  9.13517481e-02
  5.62904630e-01  6.38345832e-01  6.83922350e-01  7.65480807e-01
  8.06029490e-02  7.22027028e-02  7.94843680e-02  9.29979445e-02
 -5.90017966e-01  1.10082050e+00 -6.57483269e-01  2.72206056e+00
 -6.68545681e-01  2.45846757e+00 -8.79909166e-01  3.54870514e+00
  1.51125802e+01]
supnorm grad right now is: 15.11258024568879
Weights right now are: 
[ 0.9742338   4.27369773  4.4221243   5.21344905 -1.76445303 -4.2592341
 -3.85081825 -3.798366   -1.46527088  2.83832807 -0.91908756  3.43110199
  4.31964106  8.81103118  4.90980157 10.0308745   5.17728206  8.17057154
  6.29057824  8.86463806  5.46136147  8.63013761  5.54067363  9.75325329
 -2.76756068 -7.46029208 -0.76342946 -8.39659528  4.12674943  6.61056946
  3.89798543  7.13228893 -3.37437914 -8.00227099  1.98880762 -8.94094687
  4.37379347  6.29770703  4.47104782  6.64129407  5.65378713 10.36005933
 12.68715775 13.97444079 13.43633268  6.76968096 14.61998708 13.08895735
 -1.78008966  6.09833706  1.42346215  4.17529334 15.83637218  7.14179388
 14.25906565 12.0302646  -1.30403326 -4.1605169   3.54640252 -3.05761003
  1.83817699 -3.27789785  2.62118299 -2.86345544 30.80688921]
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1361.0344274161805
gradient value of function right now is: [-7.21161686e+00 -1.07012817e-02 -3.49042255e-03 -3.71840679e-03
  7.21161686e+00  1.07012817e-02  3.49042255e-03  3.71840679e-03
  3.97033227e+00  7.31630238e-01  2.85250767e+00  6.51621456e-01
  7.25171316e-02  6.32372751e-03  4.40652050e-02  5.14096791e-03
  2.25706808e-02  2.11724460e-03  1.30018984e-02  1.73288433e-03
  2.63179140e-02  2.39520774e-03  1.54201651e-02  1.95183604e-03
  6.43182735e-02  3.24726388e-02  4.06014441e-01  2.90165464e-02
 -6.44375650e-02 -2.89252200e-02 -5.08324281e-01 -2.55932520e-02
  3.91949301e-02  2.43413563e-02  2.05184468e-01  2.22831440e-02
 -6.51566825e-02 -3.00967999e-02 -5.44053994e-01 -2.67456596e-02
  1.01613705e-01  3.29662946e-02  5.32435978e-02  5.15412298e-02
  5.49929898e-02  2.89959919e-02  4.29790631e-02  4.41556605e-02
  1.43146446e-01  2.57333948e-01  2.95710699e-01  3.23259806e-01
  5.02241776e-02  2.90087797e-02  4.23478625e-02  4.39605078e-02
 -5.81174153e-01  8.12318723e-01 -2.22950606e-01  1.15786519e+00
 -4.47546362e-01  1.45208456e+00 -4.41065335e-01  1.80690502e+00
  3.37466580e+00]
supnorm grad right now is: 7.21161685627886
Weights right now are: 
[ 0.88543196  4.55457878  4.69880001  5.50940766 -1.6756512  -4.54011515
 -4.12749397 -4.09432461 -1.34989802  2.77149033 -0.65186912  3.35620594
  4.1410957   8.65768268  4.63404393  9.92662582  4.99568532  8.01326945
  6.00846612  8.75069406  5.25568314  8.45645732  5.22987634  9.62641854
 -3.67509074 -7.61471569 -0.71192892 -8.56900669  4.63455068  6.44467298
  4.05045956  6.98671698 -4.04600458 -8.3748363   2.15783804 -9.43632047
  4.72122545  6.0995052   4.5849123   6.48432132  4.48376889 11.49775477
 13.08675516 14.7226533  13.70654912  7.54874148 15.4702821  13.83482589
 -1.76072173  5.44254925  1.246515    3.80379203 16.53796568  7.80295954
 15.08961658 12.69077753 -0.92967998 -4.10559226  3.38676866 -3.56292539
  2.02461938 -3.34047802  2.60954837 -3.13129001 30.71852496]
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1362.1384052927378
gradient value of function right now is: [ 4.61755216e-01 -1.73845969e-03 -3.30823262e-04 -5.21605952e-04
 -4.61755216e-01  1.73845969e-03  3.30823262e-04  5.21605952e-04
 -3.31481818e-01 -2.71070429e-02 -1.51809168e-01 -5.80078089e-03
  1.47520107e-02 -2.84614379e-03  5.22897385e-03 -2.66849480e-03
  4.38094614e-03 -2.08566845e-03  1.00223135e-03 -1.89517754e-03
  5.76051597e-03 -1.88631785e-03  1.68719469e-03 -1.73134493e-03
  4.82595352e-03 -1.07841971e-03 -1.17591171e-01 -1.44707515e-03
 -3.99682866e-03  2.17303870e-03  1.43014868e-01  2.25189471e-03
 -8.66465623e-04 -4.00404268e-04 -6.41558098e-02 -6.24097803e-04
 -2.53201820e-03  2.44145776e-03  1.54949366e-01  2.46694075e-03
  1.91487008e-02 -6.64202951e-03 -6.29833279e-03 -7.36055172e-03
 -1.50060112e-03 -5.74884616e-03 -6.04994765e-03 -6.74738003e-03
 -1.63192100e-01 -1.48181750e-02 -1.06747504e-02 -2.45385249e-02
 -3.61376352e-03 -5.92928140e-03 -6.31692697e-03 -7.00848859e-03
 -2.78252290e-01  3.96030070e-02 -8.84957325e-03 -3.18614694e-01
 -1.81113666e-04 -2.16491632e-01  7.13053037e-03 -3.46864226e-01
  3.05891063e+00]
supnorm grad right now is: 3.058910632575722
Weights right now are: 
[  0.93594703   4.78479867   5.06511014   5.84597533  -1.72616627
  -4.77033503  -4.49380409  -4.43089228  -1.41894542   2.69162028
  -0.72070575   3.26918661   4.25207116   8.26274639   4.98826261
   9.60041139   4.93615319   7.60218681   6.09570383   8.40970371
   5.22672741   8.0246429    5.37571002   9.26416242  -3.54429397
  -8.87389526  -0.56831486 -10.22850687   4.4984236    7.23847236
   4.38666191   8.10028616  -4.98235205 -10.31688889   2.44865617
 -11.67696094   4.66488721   6.97441462   4.85501167   7.66872644
   4.34121085  11.92500395  13.05343957  14.84172412  14.44066943
   7.42074454  15.20970803  13.50594965  -1.53301611   5.59860883
   1.54002782   4.00304027  17.02591077   7.42709439  14.58279395
  12.11108483  -1.3223514   -4.45880844   3.15452646  -4.26741216
   2.11955816  -4.27879597   2.55566903  -4.02463716  30.88084633]
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1358.3560135928194
gradient value of function right now is: [ 8.13450739e+00  2.07948740e-03  7.53587988e-04  7.93686936e-04
 -8.13450739e+00 -2.07948740e-03 -7.53587988e-04 -7.93686936e-04
 -2.82089053e+00 -6.40098382e-01 -2.26214785e+00 -5.87851908e-01
 -1.29997700e-02 -2.76658841e-03 -1.10588606e-02 -2.42168040e-03
 -3.96283359e-03 -1.25193508e-03 -3.12679005e-03 -1.09528067e-03
 -4.78872890e-03 -1.33399463e-03 -3.89659219e-03 -1.16714029e-03
 -2.28831197e-02 -1.08977345e-02 -3.19359311e-01 -9.72625657e-03
  1.33592203e-02  6.89570177e-03  2.98243886e-01  6.25437820e-03
 -2.31972714e-02 -1.07678941e-02 -2.37530926e-01 -9.59485021e-03
  1.42449526e-02  7.31378990e-03  3.29901434e-01  6.65513626e-03
 -3.31655042e-02 -1.93199990e-02 -2.15453037e-02 -2.31497623e-02
 -2.74347048e-02 -1.69700732e-02 -1.78550979e-02 -1.97691315e-02
 -4.05399998e-01 -2.21837033e-01 -2.55533638e-01 -2.54714328e-01
 -2.60323677e-02 -1.77025053e-02 -1.82474345e-02 -2.04069745e-02
  2.00301383e-01 -2.76487956e-01  3.76255617e-01 -3.80203649e-01
  2.76740191e-01 -3.72521364e-01  4.24048737e-01 -4.84574840e-01
  4.08505467e+00]
supnorm grad right now is: 8.134507387991462
Weights right now are: 
[  0.9979097    4.49375005   4.63610627   5.45715624  -1.78812893
  -4.47928641  -4.06480022  -4.04207319  -1.55235297   2.43817937
  -1.09096831   2.99808877   4.55861826   8.9092984    5.11891689
  10.3427654    5.40150368   8.38649413   6.47063264   9.29105549
   5.64042097   8.76320413   5.66711779  10.10046741  -4.86475216
  -9.65654784  -0.53663411 -10.96228762   5.16680791   7.95217775
   4.24709933   8.88616452  -6.51858045 -10.94912178   2.16940525
 -12.2111592    5.2521313    7.68677245   4.72827932   8.47192625
   3.37498284  12.96734389  13.45925888  15.50065376  14.1865344
   7.88307726  15.7229411   13.8726601   -1.58673814   4.84562044
   0.82511248   3.29520588  17.22618743   7.75836728  15.03172298
  12.36812114   0.4745242   -5.26101935   3.8067536   -3.92162123
   2.96915971  -4.29386703   3.22935529  -3.88814747  30.92433573]
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1361.1785532151382
gradient value of function right now is: [-6.39836642e+00 -1.52521903e-02 -6.29336974e-03 -6.39196006e-03
  6.39836642e+00  1.52521903e-02  6.29336974e-03  6.39196006e-03
  2.33133218e+00 -3.40290382e-02  1.71411773e+00 -3.90763439e-02
  8.96892198e-02  3.72618713e-03  3.50186160e-02  2.73014406e-03
  3.55234652e-02  1.13033049e-03  1.26013081e-02  7.58611175e-04
  3.94655297e-02  1.41515684e-03  1.44456093e-02  9.89736952e-04
  4.81077394e-02  1.37896027e-02  5.71889145e-02  1.11847255e-02
 -4.49874530e-02 -7.21600182e-03 -8.56806770e-02 -5.58392371e-03
  3.11061141e-02  1.37232839e-02  2.63824477e-02  1.16316427e-02
 -4.41727817e-02 -7.35462854e-03 -7.51266141e-02 -5.73125442e-03
  9.28906153e-02  7.79218990e-03  1.64457077e-02  1.28278973e-02
  3.36358274e-02  4.67786867e-03  8.81698737e-03  7.33705425e-03
 -8.80870001e-02 -1.01341041e-02 -2.44228633e-02 -1.94961622e-02
  2.98191086e-02  4.61003640e-03  8.50784853e-03  7.16538103e-03
 -6.70766325e-01  7.07067693e-01 -5.95411299e-02  3.25468273e-02
 -2.22030566e-01  2.92929425e-01 -1.54071808e-01  1.92320023e-01
 -4.57989745e+00]
supnorm grad right now is: 6.398366420891976
Weights right now are: 
[  0.81658497   4.41055936   4.53949252   5.34962871  -1.6068042
  -4.39609572  -3.96818647  -3.93454566  -1.25489218   2.66949669
  -0.72124366   3.24854438   4.74207477   9.25175559   5.29277452
  10.7987059    5.6158212    8.64547684   6.62898431   9.64680684
   5.8488286    9.04630588   5.83206663  10.48588413  -3.39494922
  -8.35823251  -0.25368392 -10.07668242   4.01346017   7.23834891
   4.44049816   8.57645747  -4.74431752  -9.68209419   3.1544028
 -11.37129877   4.04496087   7.09947179   4.85999984   8.27859501
   4.03071755  14.5137276   14.95622328  17.03634423  16.57965762
   8.23431393  16.52479783  14.39455233  -1.69943943   5.38368852
   1.36969277   3.77684986  19.57877994   7.83751725  15.49868258
  12.58564413  -1.14033474  -4.08885103   3.24779083  -4.29826437
   2.20074669  -3.93185591   2.67632841  -3.93609621  30.69738345]
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1361.4960948390176
gradient value of function right now is: [ 5.22906800e+00  3.40671329e-03  1.20083880e-03  1.23206252e-03
 -5.22906800e+00 -3.40671329e-03 -1.20083880e-03 -1.23206252e-03
 -2.62069592e+00 -4.10547078e-01 -1.46803091e+00 -3.63813597e-01
 -1.85089710e-02 -2.67678334e-03 -8.34574295e-03 -2.27042950e-03
 -5.74072516e-03 -9.53898975e-04 -2.33908170e-03 -8.09059164e-04
 -6.71819647e-03 -1.06780862e-03 -2.82404450e-03 -9.05633402e-04
 -3.04267936e-02 -8.32030571e-03 -3.47347884e-01 -7.64025366e-03
  2.52016665e-02  7.16382210e-03  3.67077971e-01  6.70689317e-03
 -2.93410305e-02 -8.82747415e-03 -2.09901674e-01 -7.99992582e-03
  2.58625143e-02  7.45744362e-03  3.83663445e-01  7.00441377e-03
 -7.52012127e-02 -2.38020649e-02 -2.57924094e-02 -2.84781565e-02
 -2.60959875e-02 -1.36947683e-02 -1.40406225e-02 -1.59635064e-02
 -5.50518817e-01 -1.53822474e-01 -1.73680352e-01 -1.80910276e-01
 -2.59767529e-02 -1.54043647e-02 -1.56495776e-02 -1.78554689e-02
  1.36255849e-01 -3.45249125e-01  5.25117131e-01 -3.66728129e-01
  3.68986510e-01 -4.02447555e-01  5.74006595e-01 -4.94146613e-01
 -2.97809052e+00]
supnorm grad right now is: 5.229068001825763
Weights right now are: 
[ 9.48227077e-01  3.91015872e+00  3.86545701e+00  4.73302860e+00
 -1.73844631e+00 -3.89569509e+00 -3.29415097e+00 -3.31794555e+00
 -1.48221351e+00  2.95469697e+00 -8.35156593e-01  3.53578159e+00
  5.18942575e+00  9.98010993e+00  5.03464220e+00  1.15754692e+01
  6.27436391e+00  9.38071607e+00  6.64304024e+00  1.04170269e+01
  6.42988442e+00  9.75875607e+00  5.75339008e+00  1.12359770e+01
 -5.13080454e+00 -9.47932383e+00 -1.11814606e+00 -1.12823860e+01
  5.47417491e+00  7.84488391e+00  4.31551165e+00  9.12905233e+00
 -5.09596806e+00 -1.16392608e+01  3.04601140e+00 -1.36054084e+01
  5.45850320e+00  7.68425691e+00  4.74691076e+00  8.82681533e+00
  1.98283187e+00  1.52391779e+01  1.50109732e+01  1.73387958e+01
  1.65079250e+01  8.77566361e+00  1.69085196e+01  1.47968108e+01
 -1.61614638e+00  5.04450630e+00  1.30046182e+00  3.56781308e+00
  1.96553804e+01  8.24200507e+00  1.57907091e+01  1.28701491e+01
  6.96133679e-03 -4.94125629e+00  3.72829211e+00 -3.77573729e+00
  3.40401986e+00 -3.89758803e+00  3.39286880e+00 -3.63488925e+00
  3.08804295e+01]
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1361.8367260651094
gradient value of function right now is: [ 1.10113754e+00 -1.36156773e-04  1.32510786e-04  9.20036483e-05
 -1.10113754e+00  1.36156773e-04 -1.32510786e-04 -9.20036483e-05
 -4.31707962e-01 -1.39839136e-01 -2.77520809e-01 -1.29167752e-01
  2.40932893e-03 -1.05015480e-03 -3.32341780e-04 -9.09405951e-04
  1.30057333e-04 -4.90974910e-04 -2.83208717e-04 -4.20553012e-04
  3.15934752e-04 -5.22139649e-04 -2.81652374e-04 -4.48069105e-04
 -3.46827112e-06  2.30905656e-03 -6.56588937e-02  2.99124344e-03
  6.37446156e-04 -1.71499796e-03  7.68634556e-02 -2.23273487e-03
 -2.24263191e-03  3.83931918e-03 -2.07667532e-02  4.70873371e-03
  9.27588720e-04 -1.83215240e-03  7.70700912e-02 -2.36445179e-03
  1.30451991e-02 -2.13833382e-03 -2.56525825e-03 -2.64773628e-03
 -2.07407319e-03  4.01693689e-03  4.19305694e-03  4.67695087e-03
 -2.16384993e-01 -3.17457666e-02 -4.00304782e-02 -4.02460677e-02
 -1.52965997e-04  5.93339911e-03  6.27132928e-03  6.96149775e-03
  2.24791828e-03  5.59931096e-02  1.50912745e-01  1.18910519e-01
  6.16582254e-02  1.46117482e-01  1.36785726e-01  1.62985384e-01
  1.39910102e+01]
supnorm grad right now is: 13.991010217363815
Weights right now are: 
[  0.95467987   3.65633879   3.55884861   4.43396233  -1.7448991
  -3.64187516  -2.98754257  -3.01887928  -1.45012854   2.92775367
  -1.0097955    3.49966023   5.48153579  10.45084422   5.24952456
  12.08268165   6.62898081   9.79577523   6.91637816  10.85862141
   6.76638828  10.19269996   6.01377834  11.69967448  -5.47617846
  -9.25024239  -0.42605863 -11.23270505   5.61013508   7.81412511
   4.34089075   9.34925755  -5.06310737 -12.18102859   3.74539417
 -14.30444223   5.51700521   7.7017053    4.81823493   9.09035644
   0.64228089  16.57584805  15.88561747  18.35093775  17.89653598
   9.43510974  17.52025347  15.42001707  -1.33947385   4.43741265
   0.91361835   3.06371301  20.94730953   8.63751381  16.14131867
  13.22983667   0.14647139  -4.45120105   3.73710407  -3.79052323
   3.42870613  -3.71695752   3.37391869  -3.57177166  30.9568807 ]
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1359.0095212911021
gradient value of function right now is: [ 4.81124869e+00  1.73411605e-02  7.44207258e-03  7.26958932e-03
 -4.81124869e+00 -1.73411605e-02 -7.44207258e-03 -7.26958932e-03
 -2.94302495e+00 -8.20808673e-01 -1.05433760e+00 -7.34817287e-01
 -8.10554230e-02 -1.16882420e-02 -2.17164391e-02 -9.62282151e-03
 -3.05655557e-02 -4.76997165e-03 -7.93564976e-03 -3.93038645e-03
 -3.38338366e-02 -5.17824984e-03 -8.85444663e-03 -4.26543365e-03
 -3.19682228e-02 -5.39989899e-03 -5.93464849e-01 -5.41733632e-03
  2.38277881e-02  3.27810326e-03  6.34319573e-01  3.30607467e-03
 -1.59314162e-02 -4.14037342e-03 -2.19765686e-01 -4.20504410e-03
  2.36643489e-02  3.32354315e-03  6.62511963e-01  3.36010230e-03
 -9.25014877e-02 -2.31107059e-02 -2.49211725e-02 -2.76819277e-02
 -1.41368336e-02 -7.53362669e-03 -6.91292395e-03 -8.23108103e-03
 -1.17696992e+00 -4.56596265e-01 -5.11029681e-01 -5.20325181e-01
 -1.67195141e-02 -9.55122568e-03 -8.67162387e-03 -1.03573918e-02
 -1.06859375e-01 -2.02743581e-01  5.56219130e-01 -4.51763250e-01
  2.54120362e-01 -3.21280352e-01  5.41081221e-01 -4.95858987e-01
 -2.27854087e+01]
supnorm grad right now is: 22.785408666675913
Weights right now are: 
[  0.84648621   3.86191743   3.75172748   4.61513814  -1.63670545
  -3.84745379  -3.18042143  -3.2000551   -1.61359205   2.81329248
  -1.01362252   3.40779539   5.41228567  10.44301321   5.17830085
  12.14531211   6.6228977    9.82701699   6.87279915  10.97016814
   6.74330749  10.18994873   5.96316173  11.77396692  -5.50027328
  -8.87769846  -0.26779662 -11.19076777   5.98542992   8.0129266
   4.55007712   9.76291955  -4.4987112  -12.21587762   4.23755263
 -14.77489821   5.88228144   7.91862674   5.03494492   9.51911049
   0.74438413  17.30964337  16.30608021  18.86354274  19.15005948
   9.4582777   17.48411293  15.39271827  -1.08523162   3.6672862
   0.49533699   2.47292112  22.02698119   8.36268266  15.772948
  12.88807108  -0.55765024  -4.27158341   3.51551346  -3.94168621
   2.86175687  -4.29921688   2.96318167  -3.944374    30.72541451]
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1362.1938859741185
gradient value of function right now is: [ 4.17297605e+00  6.17046189e-03  2.04164026e-03  2.13127617e-03
 -4.17297605e+00 -6.17046189e-03 -2.04164026e-03 -2.13127617e-03
 -1.34925560e+00  2.13056027e-01 -7.45511231e-01  2.08248608e-01
 -2.89283324e-02 -3.24250481e-03 -1.03507808e-02 -2.61402769e-03
 -7.65529669e-03 -1.03901112e-03 -2.42428251e-03 -8.35512816e-04
 -9.31008793e-03 -1.20819676e-03 -3.04675737e-03 -9.72528453e-04
 -5.54676023e-02 -9.34810320e-03 -1.59875279e-01 -8.59926563e-03
  3.86391428e-02  5.38143616e-03  1.85423340e-02  4.57567407e-03
 -3.61420874e-02 -8.17613314e-03 -8.10967957e-02 -7.68660029e-03
  3.71292348e-02  5.12661375e-03 -1.59969057e-02  4.29122993e-03
 -1.32756036e-01 -2.66213540e-02 -3.19170520e-02 -3.63579717e-02
 -2.39976832e-02 -6.76298410e-03 -7.70560406e-03 -9.34088592e-03
 -3.13903012e-03  2.35381056e-01  2.12835342e-01  2.39508122e-01
 -2.55632070e-02 -6.23641190e-03 -7.54035947e-03 -9.13018002e-03
  4.24637207e-01 -4.19666495e-01  3.94176745e-01  5.40247872e-02
  5.10261184e-01 -2.16069136e-01  6.40145717e-01 -1.42533418e-01
 -6.48440623e+00]
supnorm grad right now is: 6.484406234815867
Weights right now are: 
[ 9.53732176e-01  3.69020242e+00  3.42562417e+00  4.30551397e+00
 -1.74395141e+00 -3.67573878e+00 -2.85431812e+00 -2.89043092e+00
 -1.53343904e+00  3.74160929e+00 -7.68504578e-01  4.37592036e+00
  5.48108904e+00  1.13556056e+01  5.32414388e+00  1.30853519e+01
  6.86810091e+00  1.06391482e+01  7.18066186e+00  1.17958095e+01
  6.95722102e+00  1.10319994e+01  6.24435464e+00  1.26328360e+01
 -6.14948947e+00 -8.87665569e+00  1.94740167e-01 -1.10643985e+01
  5.99339495e+00  7.49338439e+00  4.20746377e+00  9.28583883e+00
 -5.33450744e+00 -1.31345507e+01  4.25374248e+00 -1.55755138e+01
  5.82916854e+00  7.41871452e+00  4.71641899e+00  9.06876999e+00
  1.83150065e-02  1.83433268e+01  1.68023657e+01  1.96135749e+01
  1.92872250e+01  1.02153784e+01  1.80995870e+01  1.60626151e+01
 -1.05545284e+00  4.07256090e+00  8.41847131e-01  2.80012615e+00
  2.22920777e+01  9.00638060e+00  1.62963019e+01  1.34599159e+01
  4.54650770e-01 -4.85474740e+00  3.63069557e+00 -3.46468180e+00
  3.17290552e+00 -3.83036791e+00  3.17724467e+00 -3.35632596e+00
  3.07547663e+01]
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1359.8200689172206
gradient value of function right now is: [ 7.50261972e+00  1.30582773e-01  5.80420832e-02  5.92940195e-02
 -7.50261972e+00 -1.30582773e-01 -5.80420832e-02 -5.92940195e-02
 -3.11707844e+00 -3.99750846e-01 -1.17896773e+00 -3.33414224e-01
 -5.28334049e-01 -4.38140640e-02 -1.06005635e-01 -3.38021974e-02
 -1.85254886e-01 -1.86737971e-02 -3.54507104e-02 -1.43951905e-02
 -2.19043327e-01 -2.12294793e-02 -4.23126406e-02 -1.63641650e-02
  1.62136998e-01 -7.22796456e-03  4.33562004e-01 -7.61231242e-03
  1.52576185e-01  6.50599143e-03  9.03951016e-01  6.58479476e-03
  7.14803832e-02 -5.76240531e-03  1.96191600e-01 -5.98116396e-03
  1.37049805e-01  6.05556388e-03  8.29415586e-01  6.12979417e-03
 -1.26704343e-01 -3.66835094e-02 -3.91450905e-02 -4.63818899e-02
 -1.68202664e-02 -6.95777344e-03 -7.28349828e-03 -8.94850903e-03
 -1.96173896e+00 -2.94688397e-01 -3.17081051e-01 -3.58874346e-01
 -2.10416758e-02 -9.35693674e-03 -9.77957714e-03 -1.20126252e-02
 -3.36532109e-01 -2.60648515e-01  8.28182523e-01 -5.02844116e-01
  4.38465929e-01 -4.43843477e-01  8.59335147e-01 -6.44353030e-01
 -1.66230157e+01]
supnorm grad right now is: 16.623015663205486
Weights right now are: 
[  1.01347452   4.17270799   3.69075387   4.61956129  -1.80369376
  -4.15824435  -3.11944782  -3.20447824  -1.50151554   3.41419188
  -1.06651158   4.03754718   4.97565618  12.53439591   5.34304121
  14.31212216   6.4836254   12.02202856   7.13491745  13.20163946
   6.51878713  12.3734249    6.19546512  14.00363948  -4.46063812
  -8.85052476   1.66318586 -11.36911144   6.90859128   7.00918198
   4.26734938   9.18655069  -3.81977007 -14.35249904   5.83009252
 -17.00435961   6.61136624   7.03069404   4.89153145   9.05568385
  -1.02586407  19.37219649  17.16448203  20.26177628  20.93289783
  10.36814264  18.26675173  16.19710489  -0.81925016   3.64803299
   0.66459411   2.42175115  23.7442406    8.84325907  16.09214584
  13.25134638  -0.48142608  -4.60071479   3.63904182  -3.89329512
   3.50633875  -3.90432446   3.31781966  -3.62107873  30.77306607]
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1361.9997231678933
gradient value of function right now is: [ 3.74944432e+00  5.67572770e-03  1.98627104e-03  2.02607220e-03
 -3.74944432e+00 -5.67572770e-03 -1.98627104e-03 -2.02607220e-03
 -1.40639768e+00 -2.74697627e-01 -6.69270229e-01 -2.23777032e-01
 -1.38024441e-02 -2.38818660e-03 -3.46378453e-03 -1.74685994e-03
 -2.71388073e-03 -5.16676208e-04 -6.22981437e-04 -3.78965834e-04
 -3.73880303e-03 -6.95610424e-04 -8.68850812e-04 -5.09849349e-04
 -2.70109116e-02 -6.03281244e-03 -2.88679672e-01 -6.91200972e-03
  1.81012126e-02  2.82002699e-03  2.75023796e-01  3.27774861e-03
 -1.72011547e-02 -5.65144230e-03 -1.17827210e-01 -6.34094295e-03
  1.61025722e-02  2.53096595e-03  2.58518642e-01  2.94464947e-03
 -4.76259886e-02 -1.48342350e-02 -1.58937441e-02 -1.85875531e-02
 -1.25322690e-02 -6.57875653e-03 -6.57027403e-03 -7.97515553e-03
 -3.44452103e-01 -1.19566229e-01 -1.25319876e-01 -1.36382932e-01
 -1.70351640e-02 -9.64500214e-03 -9.58809114e-03 -1.16408830e-02
  8.30519953e-02 -2.93645530e-01  2.13082394e-01 -2.74763814e-01
  1.61071182e-01 -2.67000929e-01  2.44280463e-01 -3.52443426e-01
  8.14154315e+00]
supnorm grad right now is: 8.141543148946027
Weights right now are: 
[  0.97646749   2.50115939   1.73126294   2.6438241   -1.76668673
  -2.48669576  -1.1599569   -1.22874105  -1.59372325   3.44198806
  -1.18412014   4.18505533   6.250577    14.38642343   6.41124884
  16.13229464   8.32508924  13.30194435   8.67065837  14.43774079
   8.29422861  13.77394888   7.68095401  15.36441148  -5.37839024
  -9.23883063   0.88535559 -11.99445437   5.81590227   7.60824144
   4.59463084   9.97281655  -3.50064552 -14.66030493   5.38839441
 -17.48687912   5.47393487   7.64981663   5.39689163   9.8683926
  -0.84360927  20.46603767  17.77158156  21.04838958  21.82024398
  10.06271606  17.81807063  15.76257829  -0.79930605   3.47263256
   0.68173383   2.24507683  24.31880284   8.30094959  15.37493325
  12.57193371  -0.41969741  -4.9131982    4.03926744  -4.25231799
   3.69672072  -4.37105104   3.59597114  -4.04651699  30.94474241]
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -1359.0509603756107
gradient value of function right now is: [ 5.55055497e+00  1.99823895e-02  9.49922183e-03  9.73286189e-03
 -5.55055497e+00 -1.99823895e-02 -9.49922183e-03 -9.73286189e-03
 -2.46227166e+00 -6.03416050e-01 -8.63416151e-01 -4.83813588e-01
 -3.32605676e-02 -5.94967576e-03 -6.49074162e-03 -4.05650170e-03
 -1.02111316e-02 -1.79875446e-03 -1.83158391e-03 -1.22360912e-03
 -1.39149430e-02 -2.44465450e-03 -2.52190283e-03 -1.66311295e-03
 -3.03735217e-02 -9.23911397e-03 -6.11359030e-01 -1.05695193e-02
  1.38732915e-02  3.05480693e-03  5.70090211e-01  3.68816457e-03
 -2.99346733e-02 -9.89369274e-03 -2.65835211e-01 -1.08237737e-02
  1.23182292e-02  2.72607357e-03  5.36100775e-01  3.30163552e-03
 -2.06758768e-02 -1.19218711e-02 -1.55035043e-02 -1.64797983e-02
 -1.91552429e-02 -7.86935162e-03 -9.63738491e-03 -1.06439582e-02
 -9.50024288e-01 -4.06036176e-01 -4.82480559e-01 -4.72708813e-01
 -2.61657030e-02 -1.20326704e-02 -1.44924799e-02 -1.60687027e-02
  1.21585115e-01 -4.63499331e-01  3.87581239e-01 -2.94781397e-01
  2.18024236e-01 -3.57685643e-01  3.78392653e-01 -4.21374160e-01
 -2.17188627e+01]
supnorm grad right now is: 21.718862680962097
Weights right now are: 
[  0.87606203   2.0883839    1.64773329   2.50554489  -1.66628127
  -2.07392026  -1.07642725  -1.09046185  -1.55783523   3.1256651
  -1.3638153    3.94723247   7.06471637  14.75836448   7.20682878
  16.52438726   8.92826641  13.62253122   9.28870549  14.76946581
   8.8793712   14.01224773   8.27861945  15.62203198  -4.71345029
  -8.96575792   0.89115764 -11.67273143   5.23868079   7.6191073
   5.16390599  10.14568085  -3.51930622 -14.05996233   5.53263499
 -16.83757896   4.85866982   7.69583929   6.02299897  10.09123394
   0.26728617  21.71253931  18.67428867  22.07550769  22.27775931
  10.41602092  18.24557026  16.14630554  -1.23058464   3.32210684
   0.50460707   1.99172165  24.84835912   8.39436838  15.54789503
  12.71561036  -0.12012632  -4.58287181   4.25472009  -4.21172292
   3.41759382  -4.14419437   3.59904737  -3.89569857  30.69765602]
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -1361.496672268765
gradient value of function right now is: [-6.63047297e+00 -1.80049864e-02 -4.56523797e-03 -5.15350081e-03
  6.63047297e+00  1.80049864e-02  4.56523797e-03  5.15350081e-03
  3.28142976e+00  5.79103396e-01  8.91889825e-01  4.61368683e-01
  4.16950683e-02  1.88514377e-03  3.82381562e-03  1.39051991e-03
  5.98572041e-03  2.29341150e-04  5.37783664e-04  1.79946298e-04
  9.21509338e-03  3.71494595e-04  8.34915271e-04  2.87061000e-04
  1.16498880e-01  1.91833421e-02  5.87854048e-01  2.36861769e-02
 -5.25216393e-02 -1.09481936e-02 -6.07148796e-01 -1.49399412e-02
  6.51838961e-02  2.43980592e-02  2.87967359e-01  2.82117981e-02
 -4.56935119e-02 -1.00221449e-02 -6.00650934e-01 -1.37545449e-02
  2.21999565e-01  2.26208533e-02  2.19230719e-02  2.73699644e-02
  3.83273483e-02  2.73595470e-02  2.06015758e-02  2.81826968e-02
  2.32821539e-01  3.27145046e-01  2.81013439e-01  3.31553204e-01
  5.79947790e-02  4.77622381e-02  3.58236918e-02  4.87796823e-02
 -5.90955162e-01  1.22496768e+00 -3.66629122e-01  1.10412621e+00
 -3.36601739e-01  8.58489668e-01 -4.62628708e-01  1.22490736e+00
  6.28430238e+00]
supnorm grad right now is: 6.630472969210546
Weights right now are: 
[ 8.70468735e-01  1.99855366e+00  1.38885437e+00  2.21911559e+00
 -1.66068797e+00 -1.98409003e+00 -8.17548322e-01 -8.04032540e-01
 -1.33050629e+00  3.65407023e+00 -1.25535005e+00  4.44207818e+00
  7.18479482e+00  1.49920421e+01  7.58632349e+00  1.67519455e+01
  9.41716175e+00  1.38789332e+01  9.72720867e+00  1.49790629e+01
  9.27119601e+00  1.42144533e+01  8.68787618e+00  1.57876345e+01
 -5.04145869e+00 -8.96573199e+00  1.47702141e+00 -1.18521970e+01
  4.84571194e+00  8.64530820e+00  4.67876407e+00  1.13091532e+01
 -2.87109810e+00 -1.32448198e+01  6.92147529e+00 -1.63518471e+01
  4.38817947e+00  8.74078834e+00  5.52353381e+00  1.12569485e+01
 -1.51682647e-02  2.28319491e+01  1.93738107e+01  2.29809634e+01
  2.31801749e+01  9.89988475e+00  1.78813310e+01  1.56791197e+01
 -1.01355889e+00  4.03429792e+00  1.07167959e+00  2.60068614e+00
  2.55331992e+01  7.74651111e+00  1.50044420e+01  1.20845828e+01
 -6.75983902e-01 -4.49143230e+00  4.40413899e+00 -3.39321144e+00
  3.78932755e+00 -3.97466246e+00  3.79823539e+00 -3.47796911e+00
  3.07755355e+01]
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1360.8842939640092
gradient value of function right now is: [ 4.35910172e+00  2.82723833e-02  1.26941819e-02  1.36683549e-02
 -4.35910172e+00 -2.82723833e-02 -1.26941819e-02 -1.36683549e-02
 -1.62961369e+00 -7.67667721e-02 -5.12683192e-01 -4.52202740e-02
 -4.34383439e-02 -7.38457897e-03 -4.78865853e-03 -4.75486033e-03
 -1.02268875e-02 -2.11635287e-03 -1.03339497e-03 -1.36014331e-03
 -1.48970240e-02 -2.99443710e-03 -1.52410724e-03 -1.92455912e-03
 -8.67685324e-02 -1.17924133e-02 -2.53777425e-01 -1.18184959e-02
  4.74406418e-02  3.09568984e-03  2.77048282e-01  2.33445587e-03
 -4.84821567e-02 -1.23249888e-02 -9.61170581e-02 -1.25869267e-02
  3.76112791e-02  2.40194359e-03  1.92696147e-01  1.67545005e-03
 -1.61340168e-01 -1.78922641e-02 -2.47908548e-02 -2.60565897e-02
 -2.34789049e-02 -4.19341119e-03 -5.71598053e-03 -6.40180204e-03
 -5.84828447e-01  1.60470104e-02 -1.25854434e-02  5.00900713e-03
 -2.78034898e-02 -3.51851013e-03 -5.43084251e-03 -6.05644468e-03
  4.39410979e-01 -6.05042849e-01  4.83859473e-01  4.65710325e-03
  4.74427377e-01 -2.00785233e-01  6.22456026e-01 -1.56755520e-01
 -1.93526217e+01]
supnorm grad right now is: 19.352621688156937
Weights right now are: 
[  0.97244915   2.08699614   1.49932794   2.31627624  -1.76266838
  -2.0725325   -0.9280219   -0.90119319  -1.46768171   4.13495212
  -1.09899689   4.93193079   7.29896706  15.13847709   7.60007041
  16.93448401   9.73040154  14.00534399   9.89025853  15.11878228
   9.47164386  14.28336303   8.78731179  15.8877433   -5.90434893
  -8.37476723   1.98898288 -11.15747596   5.16937133   8.59644833
   4.49747661  11.40714501  -3.91228428 -13.1907405    7.14901163
 -16.04603395   4.73850774   8.74744208   5.45160912  11.40288682
  -0.09701133  23.67498525  19.93326755  23.59250414  23.35478763
  10.60104559  18.6371517   16.38818684  -0.95802501   3.82178344
   1.04404435   2.46353459  25.81867145   8.24061204  15.5061053
  12.57335884   0.19207754  -4.67875999   4.22569929  -3.78637884
   3.73927157  -3.74709868   3.77698238  -3.4719625   30.69212026]
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1361.8305734178705
gradient value of function right now is: [-2.97301150e+00 -2.46485727e-02 -8.58564238e-03 -1.04221051e-02
  2.97301150e+00  2.46485727e-02  8.58564238e-03  1.04221051e-02
  1.89072190e+00  7.38501468e-01  3.28813187e-01  5.23409292e-01
  2.14302990e-02  4.69745771e-03  2.78704871e-03  2.80983879e-03
  1.13560291e-03  2.96218138e-04  1.45721940e-04  1.76669843e-04
  3.78729417e-03  9.57325072e-04  4.88236038e-04  5.71412484e-04
  7.60991093e-02  1.17477542e-02  6.28583053e-01  1.44136790e-02
 -4.71944625e-02 -6.75385659e-03 -9.45854982e-01 -9.24811636e-03
  3.55668792e-02  1.03768904e-02  2.01320633e-01  1.16972991e-02
 -3.48482108e-02 -5.16651926e-03 -7.99445175e-01 -7.11228549e-03
  2.22947861e-01  2.06693430e-02  2.79007700e-02  2.90116968e-02
  2.25017514e-02  9.02456050e-03  1.16643412e-02  1.24549700e-02
  1.33899440e+00  4.54803018e-01  5.23966140e-01  5.28570512e-01
  3.64596656e-02  1.63677137e-02  2.09496390e-02  2.23410507e-02
 -2.08034250e-01  7.89793340e-01 -6.75232091e-01  4.31095155e-01
 -4.50308132e-01  5.60133220e-01 -6.70510768e-01  6.34168381e-01
  8.21981424e+00]
supnorm grad right now is: 8.219814235680618
Weights right now are: 
[  0.91832541   1.32333648   0.50452989   1.29526822  -1.70854464
  -1.30887284   0.06677615   0.11981482  -1.41085504   4.31676501
  -1.25514971   5.19215652   8.21512232  16.91380363   8.86949108
  18.72745582  11.50667341  16.33157021  11.7106342   17.34823224
  10.91729786  16.43685052  10.42212196  17.99934538  -5.73547274
  -8.047411     2.50333012 -11.01969501   5.19966477   8.50233742
   4.42923238  11.7442477   -3.63625806 -13.43503275   7.35123858
 -16.49870506   4.64718957   8.76496103   5.65798415  11.84791161
  -0.27602804  24.90517209  20.77348545  24.52975405  24.71323246
  10.42932924  18.53492269  16.20644573  -0.56798077   3.54890172
   0.96591015   2.2477509   26.94955448   7.81670431  15.08660245
  12.11009883  -0.59902286  -4.1140911    4.23693469  -4.23895
   3.78395331  -3.90037046   3.78889642  -3.79556337  30.7867915 ]
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1362.6763637313043
gradient value of function right now is: [-2.90464210e-01 -1.02729555e-03 -3.52578833e-04 -4.19090150e-04
  2.90464210e-01  1.02729555e-03  3.52578833e-04  4.19090150e-04
 -1.48525523e-01 -6.09957628e-02  2.04976808e-02 -4.31508069e-02
  1.36262659e-03 -8.03903660e-05 -5.05409983e-04 -7.52514282e-05
  6.54269929e-08  6.05301535e-09 -8.88313707e-09  1.17967276e-09
  1.10032940e-04  5.40065141e-06 -2.47453540e-05 -8.80240874e-07
  4.11390673e-03 -1.04846441e-02 -9.73419649e-02 -1.29414087e-02
 -3.99528385e-04  2.03435641e-03  5.89559705e-02  2.58808597e-03
 -1.11544320e-02 -6.01124195e-03 -4.58140782e-02 -7.09311617e-03
 -2.06984228e-04  1.36701932e-03  4.37546083e-02  1.73604223e-03
  7.40881189e-02 -3.01235417e-03 -4.35761521e-03 -4.32807833e-03
 -1.28384581e-02 -4.18795315e-03 -6.19242467e-03 -6.12440602e-03
  1.57208928e-01  1.87868023e-02  1.31815902e-02  1.31637508e-02
 -1.96439276e-02 -6.90688417e-03 -1.01770892e-02 -1.00670130e-02
  2.66374444e-01 -3.62331463e-01 -1.18812117e-01 -2.38948767e-01
 -8.42577182e-03 -2.52280718e-01 -6.50619216e-02 -2.87995724e-01
  2.71678764e+00]
supnorm grad right now is: 2.716787644039908
Weights right now are: 
[  0.81393412   1.56865837   0.2886813    1.167845    -1.60415335
  -1.55419474   0.28262475   0.24723805  -1.75947402   3.86760449
  -0.96472426   4.83509909   7.69237907  17.80658516   8.48158621
  19.62935952  11.71327285  17.41889122  11.903245    18.39288697
  10.81881276  17.44775289  10.35489253  19.01137038  -5.65044913
  -7.82299217   2.40925151 -10.76469274   5.90356431   9.81393752
   4.85179247  13.32299913  -3.95963407 -13.37424938   7.21049892
 -16.23805078   5.3472178   10.13373804   6.31681656  13.46751544
  -0.30432939  25.39398857  20.88855677  24.72084993  24.38389832
  10.37187436  18.44635191  16.11373853  -0.3324217    3.1897973
   0.65154215   1.85919878  26.57517171   7.22885703  14.53356481
  11.5472444    0.48837132  -4.86343814   4.44305689  -4.99849219
   3.99657456  -4.41163861   4.01268105  -4.43270601  30.92353503]
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1360.2251377448504
gradient value of function right now is: [-2.32754249e+00  1.67588026e-01  7.46332366e-02  9.69438996e-02
  2.32754249e+00 -1.67588026e-01 -7.46332366e-02 -9.69438996e-02
  6.55552108e-01 -3.49150523e-01  5.03108387e-01 -2.14578198e-01
 -5.29313890e-02 -2.15781781e-02 -4.23757557e-03 -9.14016638e-03
 -1.99580835e-04 -5.32155502e-05 -1.53970252e-05 -2.19521158e-05
 -9.91528916e-03 -2.79873808e-03 -7.76962962e-04 -1.16405477e-03
  1.55051503e-01  1.94420053e-02  2.45770960e-01  2.49264855e-02
 -1.89684141e-03 -1.12614399e-03  5.93852169e-01 -1.78665893e-03
  2.45948541e-02  9.30594104e-03  5.43846301e-02  1.14989796e-02
 -1.77563619e-03 -6.83004059e-04  3.81610075e-01 -1.08080156e-03
  1.67823881e-01  3.96828884e-03  1.02393238e-02  8.51982675e-03
  1.01789151e-02  4.15163484e-03  6.79469063e-03  6.69490947e-03
 -1.50678867e+00 -5.20300241e-01 -6.59851316e-01 -6.20514826e-01
  1.59124874e-02  7.88134399e-03  1.26095286e-02  1.24957386e-02
 -4.88589964e-01  4.58879298e-01  2.05648531e-01  1.27984743e-01
 -1.70235020e-01  2.95990318e-01 -1.40166273e-03  2.60515991e-01
 -1.17502458e+01]
supnorm grad right now is: 11.750245801670774
Weights right now are: 
[ 8.46908686e-01  1.39264307e+00  3.16359354e-01  1.06939569e+00
 -1.63712792e+00 -1.37817943e+00  2.54946692e-01  3.45687361e-01
 -1.66536671e+00  3.50720732e+00 -8.09040431e-01  4.76716613e+00
  8.05643522e+00  1.84881467e+01  8.93978545e+00  2.02277495e+01
  1.29631585e+01  1.95867623e+01  1.36011435e+01  2.04562422e+01
  1.15859075e+01  1.90260349e+01  1.13984516e+01  2.04687541e+01
 -5.59169392e+00 -6.52600995e+00  2.94298552e+00 -9.66690726e+00
  5.71989250e+00  1.04466410e+01  5.46760771e+00  1.45269255e+01
 -3.79764274e+00 -1.30977703e+01  7.90947628e+00 -1.57352841e+01
  5.21450548e+00  1.08010983e+01  7.16236541e+00  1.46981897e+01
 -6.71842643e-01  2.68829937e+01  2.21313765e+01  2.60187544e+01
  2.54965119e+01  1.06246351e+01  1.87380363e+01  1.63788303e+01
 -1.15059774e+00  2.78841128e+00  2.39771053e-01  1.37854915e+00
  2.74839889e+01  6.78129688e+00  1.40990264e+01  1.11015916e+01
 -1.51649420e-03 -4.60475668e+00  5.07700990e+00 -4.25966177e+00
  3.93684530e+00 -3.78531944e+00  4.22135219e+00 -3.75078294e+00
  3.06932975e+01]
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1361.622405211327
gradient value of function right now is: [ 2.44210214e+00  4.26515561e-03  1.25742625e-04  5.46080999e-04
 -2.44210214e+00 -4.26515561e-03 -1.25742625e-04 -5.46080999e-04
  1.55842676e-01  1.76995631e-01 -3.63014267e-01  1.19149515e-01
 -7.95013759e-04 -1.92046786e-04 -7.33253753e-05 -8.00099688e-05
  6.71032925e-04  2.18535108e-04  8.34487467e-05  1.07829839e-04
  8.04829170e-04  2.24772559e-04  8.58491726e-05  1.05978949e-04
  8.28841238e-02 -2.65064307e-03  1.09924787e-01 -3.48237977e-03
 -1.19497662e-03 -5.86527075e-04 -1.42334954e-01 -2.02156591e-03
  1.66799542e-02 -5.16197931e-03  2.68246127e-02 -6.60906336e-03
 -9.78887387e-04 -4.78769049e-04 -1.39299252e-01 -1.51749584e-03
  5.80803827e-02 -2.61419416e-03 -3.28437467e-03 -3.49046990e-03
 -1.12002274e-03 -5.53094362e-04 -9.22853705e-04 -1.02475018e-03
  1.25811160e-02  1.50250443e-01  1.58711070e-01  1.66098475e-01
  8.04427003e-04  2.51983648e-03  2.26394771e-03  2.38235216e-03
 -1.73990423e-02 -1.18686803e-01 -1.68024300e-03  1.06722431e-01
  1.75402381e-02  9.29314137e-03  1.60430468e-02  5.14894765e-02
  1.85754738e+01]
supnorm grad right now is: 18.57547378869789
Weights right now are: 
[  0.97189585   0.21830764  -1.01329873  -0.27897721  -1.76211508
  -0.203844     1.58460477   1.69406026  -1.45066199   4.26382582
  -1.58448345   5.59276297   8.93889212  19.75112393   9.07573623
  21.31194613  13.32643051  20.59155006  14.31017692  21.3659454
  12.40144947  20.41726944  12.12706343  21.60823311  -5.12537723
  -7.28487317   3.09969241 -10.74855431   4.66809116  11.09239214
   4.86507772  15.26097875  -3.07902471 -13.32335775   8.42079505
 -16.12371263   4.2235224   11.3969262    6.86317449  15.34600407
  -1.19782783  27.48598435  22.35178748  26.38745173  26.07717437
   9.88918662  17.9603895   15.54403014  -0.84200852   3.71655752
   1.28754989   2.33947349  27.81337317   6.05602511  13.20143614
  10.17906545  -0.29787927  -5.06490988   5.50728837  -4.27274281
   4.64979941  -4.2025392    4.79476289  -4.00500439  30.93002607]
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1362.1135096677342
gradient value of function right now is: [ 1.82914958e+00  1.27331302e-02  2.67326959e-03  3.67451291e-03
 -1.82914958e+00 -1.27331302e-02 -2.67326959e-03 -3.67451291e-03
 -7.68823837e-01 -3.44887690e-01 -2.46228574e-01 -1.94812092e-01
 -5.68743546e-03 -1.34437915e-03 -7.30470400e-04 -4.87439429e-04
  3.30990948e-04  8.66071296e-05  3.95688842e-05  3.15544077e-05
 -7.08302598e-05 -1.81428620e-05 -8.54920760e-06 -6.60537097e-06
  2.72848232e-02 -1.04419693e-02 -3.32394518e-01 -1.63559180e-02
  1.87536926e-03  1.63058610e-03  3.22774488e-01  2.94815938e-03
 -2.59041685e-03 -9.34185533e-03 -1.04824621e-01 -1.30437111e-02
  9.18771030e-04  7.77225169e-04  2.15602229e-01  1.43269394e-03
  5.90993986e-02 -4.03232942e-03 -5.47673441e-03 -5.60395159e-03
 -1.16119722e-02 -5.10856920e-03 -7.39408843e-03 -7.50449210e-03
 -5.25838185e-01 -2.13104878e-01 -2.42345262e-01 -2.41211921e-01
 -2.39911869e-02 -1.12395225e-02 -1.60083375e-02 -1.62506477e-02
  1.89057587e-01 -4.63960014e-01  1.24493838e-01 -1.54988445e-01
  8.00624524e-02 -2.50355143e-01  1.10102382e-01 -2.45136289e-01
 -8.53353172e+00]
supnorm grad right now is: 8.5335317169242
Weights right now are: 
[  0.83885489   0.75012906   0.06312228   0.73248259  -1.62907412
  -0.73566543   0.50818377   0.68260046  -1.65942955   3.65498315
  -1.50615736   5.25674316   9.87177783  21.28555995  10.2814974
  22.6619744   14.49379038  21.69217242  15.40695299  22.25118378
  13.63322507  21.60594315  13.35417993  22.58989941  -5.39742496
  -7.26440958   2.57891994 -10.76712451   3.71983357  10.85077001
   5.36952835  15.55383691  -2.64510532 -13.63883356   8.14100676
 -16.59806273   3.29829253  11.25778629   7.85509311  15.83028711
  -1.34149055  29.5974719   23.80693635  27.98006833  26.41080161
   9.88879908  17.66526089  15.32992065  -0.35229979   2.86819951
   0.64298402   1.58849164  27.83785907   5.44148781  12.3178279
   9.39200358  -0.07512688  -5.28642783   4.85795902  -5.15563826
   4.24935354  -4.59736577   4.28645691  -4.57386733  30.81602013]
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1362.0026572448864
gradient value of function right now is: [ 6.04098285e-02  1.15884570e-03  7.94911558e-05  1.22895947e-04
 -6.04098285e-02 -1.15884570e-03 -7.94911558e-05 -1.22895947e-04
 -4.62914488e-01 -1.79110462e-02 -1.13033060e-02 -2.64639143e-02
 -1.20464565e-03 -8.91814436e-05 -1.43235564e-04 -3.01115716e-05
 -2.34418673e-05 -1.17398067e-06 -2.28528613e-06 -3.77430556e-07
 -6.36065379e-05 -3.60445211e-06 -6.61312630e-06 -1.17031893e-06
 -7.97850298e-02 -7.22157533e-03 -1.41346251e-01 -1.06961761e-02
  2.98948378e-02  9.88384129e-04  4.55295382e-03  1.11558471e-03
 -2.65970101e-02 -7.29805753e-03 -5.70747797e-02 -1.06264765e-02
  1.48352556e-02  3.83647138e-04 -1.94478720e-02  3.51668099e-04
 -1.86760714e-01 -9.01196973e-03 -1.01582600e-02 -1.12002421e-02
 -7.99317876e-03 -4.56289129e-03 -3.91580284e-03 -4.85228359e-03
  1.85501819e-01  6.45981429e-02  7.17758112e-02  7.20910175e-02
 -1.36087581e-02 -7.92681182e-03 -6.72538181e-03 -8.42944198e-03
  2.51006810e-02 -3.30109902e-01  1.29524990e-01 -1.15551611e-01
  1.65074474e-01 -1.52603937e-01  1.87909073e-01 -1.67004873e-01
 -1.41820298e+01]
supnorm grad right now is: 14.182029848570076
Weights right now are: 
[ 7.72103746e-01  1.07851314e+00  5.69990683e-01  1.22050564e+00
 -1.56232298e+00 -1.06404951e+00  1.31536252e-03  1.94577412e-01
 -1.54574127e+00  3.96277344e+00 -1.72644056e+00  5.20978759e+00
  1.07039238e+01  2.23745834e+01  1.14500844e+01  2.39277499e+01
  1.55288764e+01  2.27795709e+01  1.70200867e+01  2.32223095e+01
  1.48312528e+01  2.29378748e+01  1.52924471e+01  2.37518605e+01
 -5.88792211e+00 -7.06924067e+00  3.15363400e+00 -1.07574549e+01
  3.62742537e+00  1.06468025e+01  4.70543913e+00  1.55123344e+01
 -2.70457991e+00 -1.38538422e+01  8.72833086e+00 -1.69030727e+01
  2.97334810e+00  1.09226842e+01  7.35437390e+00  1.57003065e+01
 -1.60204813e+00  3.11843491e+01  2.45648945e+01  2.89305052e+01
  2.72736496e+01  1.02875011e+01  1.77857943e+01  1.54318389e+01
  9.46012490e-02  3.13690578e+00  1.03294116e+00  2.00863464e+00
  2.87040284e+01  5.60344165e+00  1.22326143e+01  9.32065235e+00
 -3.26096992e-01 -4.98337582e+00  4.74739153e+00 -4.00294322e+00
  4.43058081e+00 -4.35805309e+00  4.39089548e+00 -4.01922005e+00
  3.06947691e+01]
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1362.2417125583809
gradient value of function right now is: [ 5.59067293e+00  3.72824777e-03  5.40743126e-04  6.72083701e-04
 -5.59067293e+00 -3.72824777e-03 -5.40743126e-04 -6.72083701e-04
 -1.29262715e+00 -2.62911867e-02 -4.02038646e-01  6.22415713e-03
 -1.87554745e-03 -3.19927105e-04 -2.17878880e-04 -1.05621429e-04
  4.26180182e-05  1.03656299e-05  4.78305013e-06  3.45251485e-06
 -3.84425479e-05 -8.88044269e-06 -4.36157509e-06 -2.95225303e-06
 -1.38674316e-01 -1.33884301e-02 -3.08273792e-01 -1.91628146e-02
  4.84373579e-02  2.31537709e-03  1.05567934e-01  3.27293053e-03
 -5.45904853e-02 -1.49803245e-02 -1.19676812e-01 -1.99791025e-02
  2.28848929e-02  9.61550211e-04  2.85379980e-02  1.30396516e-03
 -2.51291225e-01 -1.55974909e-02 -1.65167228e-02 -1.81744030e-02
 -1.93011343e-02 -8.68582711e-03 -7.11483566e-03 -8.73071406e-03
  1.23083426e-01  6.87696888e-02  6.83241744e-02  7.12764847e-02
 -3.27034849e-02 -1.65836097e-02 -1.34368571e-02 -1.65645940e-02
  2.72442435e-01 -6.94607788e-01  2.50765117e-01 -1.86399490e-01
  2.61375720e-01 -2.46647132e-01  3.09063691e-01 -2.63058988e-01
  2.78651755e+00]
supnorm grad right now is: 5.590672928606374
Weights right now are: 
[  0.94985204   0.70806425   0.16336773   0.80296861  -1.74007127
  -0.69360061   0.40793831   0.61211444  -1.6196111    3.99479685
  -1.86866017   5.34619563  10.94025301  23.29195131  10.93925737
  24.82215089  16.01618743  23.65782235  17.12915129  24.08545254
  15.38511411  24.06349672  15.18120858  24.7948171   -6.02133557
  -7.57618111   3.35648976 -11.28187883   3.60168902  10.87201152
   4.87358134  15.73686936  -2.34246314 -14.08687499   8.78315266
 -17.28024792   2.83460619  11.00848374   7.7127858   15.79089989
  -1.58769447  31.53954638  24.73637869  29.16771513  27.49847265
   9.82030265  17.41996598  14.96390958   0.06334286   3.36003702
   1.24395904   2.19640615  28.90596792   5.21627606  11.88094883
   8.89958434  -0.14721717  -5.15941126   5.33104695  -4.09074452
   4.99631516  -4.56063876   4.97810943  -4.20197456  30.88806423]
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1360.521733998528
gradient value of function right now is: [-9.21567741e+00 -7.11292960e-03 -5.52906372e-04 -8.40993503e-04
  9.21567741e+00  7.11292960e-03  5.52906372e-04  8.40993503e-04
  2.89026164e+00  4.50042173e-01  8.25968420e-01  2.41605994e-01
  5.21723721e-03  8.19303142e-04  5.56631436e-04  2.89247653e-04
  1.19234865e-04  2.41219152e-05  1.15746745e-05  8.59419428e-06
  2.27674082e-04  4.40105351e-05  2.24355650e-05  1.56412371e-05
  9.35973141e-02  4.77864806e-02  6.19915270e-01  7.02216047e-02
 -1.35525446e-02 -7.41446309e-03 -4.19237423e-01 -1.38645636e-02
  9.25465018e-02  5.17271456e-02  2.64491431e-01  6.50927136e-02
 -6.42626878e-03 -3.66423188e-03 -2.99910945e-01 -7.04706459e-03
 -4.54833393e-02  1.38034187e-02  1.24160263e-02  1.50304858e-02
  4.67382464e-02  2.12712480e-02  1.96367205e-02  2.33191838e-02
  3.75202355e-01  2.39181419e-01  2.46020050e-01  2.53254327e-01
  9.21362735e-02  4.91117087e-02  4.52478864e-02  5.33631433e-02
 -1.00557959e+00  2.55945124e+00 -2.50999191e-01  8.55124470e-01
 -2.21809964e-01  9.60311823e-01 -2.66919517e-01  1.06632665e+00
  2.48121160e+00]
supnorm grad right now is: 9.215677409344652
Weights right now are: 
[  0.74033661   1.03199894   0.61780757   1.11819438  -1.53055585
  -1.0175353   -0.04650152   0.29688867  -1.61179996   4.02319918
  -1.21691896   5.62392867  11.25873582  24.21219535  11.95328193
  25.83182947  17.82117798  25.44827933  18.97881968  25.91184839
  16.65424925  25.74910623  16.83025671  26.51034576  -6.4407533
  -6.30087321   2.9909312   -9.894584     4.06165293  10.59830029
   5.21265571  15.67087921  -1.99846658 -12.10944933   9.75750563
 -15.01787301   3.11338214  10.7933528    7.95856967  15.78456898
  -0.95878864  32.42336418  25.39176995  29.85033714  28.38663809
  11.39796773  18.841676    16.43249799   0.04981877   3.90606903
   1.79181469   2.72327883  29.71494083   6.32379277  12.82380619
   9.91354934  -0.35915348  -4.19146065   5.02408592  -3.74316421
   4.60359212  -3.95586348   4.64253527  -3.6996265   30.74957162]
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1359.3822038986775
gradient value of function right now is: [-1.66777941e+01 -7.59070634e-02 -1.62509837e-02 -1.95539830e-02
  1.66777941e+01  7.59070634e-02  1.62509837e-02  1.95539830e-02
  2.69969590e+00  3.75023531e-01  9.63425963e-01  7.01345217e-02
  1.69709007e-02  5.46970397e-03  1.42690865e-03  1.03246136e-03
  4.99340497e-04  2.06929719e-04  4.06832236e-05  3.80148076e-05
  9.82876400e-04  3.94244171e-04  8.04743473e-05  7.26601722e-05
  3.01819647e-01  2.85244170e-02  1.14321182e+00  4.27411760e-02
 -8.57109165e-02 -4.82718336e-03 -7.30853491e-01 -9.31488162e-03
  2.92317210e-02  1.21811545e-02  1.14822498e-01  1.66957319e-02
 -2.35041774e-02 -1.50821339e-03 -2.36589381e-01 -3.03602761e-03
  5.36731787e-01  2.93290901e-02  3.87293809e-02  3.84891406e-02
  2.24649405e-02  1.10950505e-02  1.09169417e-02  1.22007984e-02
  3.12178948e-01  9.90271342e-02  1.23194854e-01  1.15118325e-01
  4.71224576e-02  2.81911084e-02  2.74722900e-02  3.06361496e-02
 -5.22977487e-01  8.46027238e-01 -8.69815733e-01  2.57036391e-01
 -9.08903955e-01  3.96575913e-01 -1.04072663e+00  3.86004906e-01
  2.16837880e+01]
supnorm grad right now is: 21.683788046264006
Weights right now are: 
[  0.66778401   0.56762767   0.36545208   0.84371721  -1.45800325
  -0.55316404   0.20585396   0.57136584  -2.08091519   3.99277935
  -1.12146457   5.55292117  12.14338202  25.99628354  13.25468056
  27.44313813  19.4994146   27.66105885  20.92791655  27.91611666
  18.33209867  28.0895003   18.86517607  28.6174815   -7.04135873
  -6.94690492   3.8425229  -10.14083584   3.94155132  10.9635825
   4.80896208  16.17219279  -2.9351459  -13.39070212   8.90384796
 -16.38804245   3.10991473  11.33753823   8.3531417   16.40089471
  -0.38947574  33.80233317  26.57801952  31.04552794  27.74432906
  10.98482251  18.41608665  15.967863    -0.5975209    2.92851905
   1.0104497    1.86551612  29.23047586   6.0297432   12.53040163
   9.58733395   0.62060273  -5.11559834   4.7060529   -4.23216725
   4.27202373  -4.41527613   4.3264834   -4.18999949  30.89160968]
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1362.4746713717946
gradient value of function right now is: [ 4.33670510e+00  8.62015028e-03  7.70005859e-04  1.03779108e-03
 -4.33670510e+00 -8.62015028e-03 -7.70005859e-04 -1.03779108e-03
 -1.28343185e+00 -2.72287201e-01 -2.20450679e-01 -1.00771963e-01
 -4.67091931e-03 -1.07641970e-03 -4.59070046e-04 -2.30949053e-04
 -9.45068492e-05 -3.16594647e-05 -9.24549970e-06 -6.82608261e-06
 -1.66415495e-04 -5.27565358e-05 -1.63381652e-05 -1.13426595e-05
 -1.45146925e-01 -1.13964417e-02 -5.19613908e-01 -1.98751252e-02
  2.91528298e-02  1.31332026e-03  3.27156868e-01  2.66345218e-03
 -9.85483308e-03 -3.05063451e-03 -5.48579815e-02 -5.55709707e-03
  9.58115185e-03  3.70921415e-04  1.52259760e-01  7.70868001e-04
 -2.87477133e-01 -1.37647601e-02 -1.49917928e-02 -1.61902246e-02
 -7.32613373e-03 -4.13281095e-03 -3.55323029e-03 -4.22541903e-03
 -2.83119649e-01 -1.52898871e-01 -1.56107630e-01 -1.59789612e-01
 -1.89145243e-02 -1.10662071e-02 -9.53211166e-03 -1.12837715e-02
  1.63045748e-03 -3.55644397e-01  3.16441915e-01 -2.34344320e-01
  2.64285886e-01 -2.23050706e-01  3.33019355e-01 -2.61357877e-01
 -5.02785031e+00]
supnorm grad right now is: 5.027850309991502
Weights right now are: 
[ 8.86032077e-01  9.93127351e-01  5.51043992e-01  1.04688557e+00
 -1.67625131e+00 -9.78663713e-01  2.02620536e-02  3.68197480e-01
 -2.13923025e+00  4.09247635e+00 -1.13846590e+00  6.11989156e+00
  1.20670505e+01  2.63075784e+01  1.27672696e+01  2.77718716e+01
  2.03417469e+01  2.85559822e+01  2.16266250e+01  2.87248235e+01
  1.90891219e+01  2.89289353e+01  1.94355707e+01  2.93802032e+01
 -7.32098508e+00 -5.92683616e+00  3.67160401e+00 -9.50153968e+00
  4.32940375e+00  1.14534977e+01  5.10893157e+00  1.77916827e+01
 -2.37054953e+00 -1.39544542e+01  9.42536208e+00 -1.68049226e+01
  3.34524260e+00  1.20791007e+01  8.92032497e+00  1.82300732e+01
 -1.06038122e+00  3.45357663e+01  2.69222092e+01  3.15117334e+01
  2.89971878e+01  1.08993629e+01  1.83565505e+01  1.58727784e+01
  2.54421287e-03  2.53175052e+00  7.35199391e-01  1.53004473e+00
  2.97141015e+01  4.78408220e+00  1.13332812e+01  8.36890553e+00
 -5.09282120e-01 -5.07120578e+00  4.88778376e+00 -4.38962381e+00
  4.52110350e+00 -4.75204005e+00  4.54511121e+00 -4.44039325e+00
  3.08798331e+01]
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1358.56061415731
gradient value of function right now is: [ 5.89824670e+00  1.39222606e-02  7.59926984e-04  1.06023579e-03
 -5.89824670e+00 -1.39222606e-02 -7.59926984e-04 -1.06023579e-03
 -2.14709924e+00 -2.31351017e-01 -1.39676057e-02 -5.68355524e-02
 -7.42729557e-03 -1.29573884e-03 -4.41928675e-04 -2.56880253e-04
  1.25547744e-04  3.30697876e-05  7.78554438e-06  6.58581343e-06
  1.35729517e-04  3.37750897e-05  8.44537870e-06  6.70128391e-06
 -2.72063837e-01 -1.70466485e-02 -6.90669498e-01 -2.86508004e-02
  6.95647254e-02  2.22878760e-03  3.73616271e-01  4.49337780e-03
 -1.14775018e-02 -2.94091454e-03 -3.74137769e-02 -5.16419322e-03
  2.61991936e-02  6.14599517e-04  1.44342570e-01  1.24494144e-03
 -5.42249327e-01 -2.72331583e-02 -3.12581224e-02 -3.21910292e-02
 -1.06383269e-02 -4.46193613e-03 -4.96275892e-03 -5.14978244e-03
 -1.94272125e-01 -4.71362710e-02 -5.09491464e-02 -5.19665613e-02
 -2.51477646e-02 -1.17051185e-02 -1.29560475e-02 -1.34295478e-02
 -1.80250198e-02 -4.30960320e-01  5.13886524e-01 -3.33916436e-01
  4.68732956e-01 -3.63676905e-01  5.55495306e-01 -3.90543609e-01
 -2.65480796e+01]
supnorm grad right now is: 26.548079576885616
Weights right now are: 
[  0.84763073   0.87560542  -0.04080687   0.45878479  -1.63784996
  -0.86114178   0.61211291   0.95629826  -1.99101095   4.46251315
  -0.85558714   6.92665886  11.54226063  27.34270166  12.60493308
  28.52975939  21.01925881  29.46359512  21.54589587  29.47312623
  19.70067882  29.99870499  19.21083769  30.24734526  -7.33496078
  -5.84294273   3.69086633  -8.62050059   4.81172649  12.28536561
   5.12337422  18.50626836  -1.01396486 -13.41639934  10.83152867
 -15.80931154   3.44285907  13.12519064   9.2691768   19.1329673
  -1.45500068  34.44303288  26.59571638  31.2386234   29.36106705
  12.0250711   19.46308363  16.94407618   0.28422334   2.95744279
   1.30226081   1.99273004  29.75280053   5.86300445  12.44524523
   9.43880773  -0.78022914  -5.15062367   5.65568065  -4.303095
   5.43358136  -4.19665945   5.40213254  -4.13766208  30.59045951]
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1362.2485840898576
gradient value of function right now is: [ 6.24142935e+00  1.05198305e-02  8.15793159e-04  1.08428495e-03
 -6.24142935e+00 -1.05198305e-02 -8.15793159e-04 -1.08428495e-03
 -1.45988834e+00 -8.12288684e-02 -2.95166126e-01  1.73118827e-03
 -2.20923686e-03 -4.97172970e-04 -1.76422479e-04 -8.50710952e-05
  8.96527348e-05  3.02149672e-05  6.98171898e-06  5.22295480e-06
  1.19122515e-04  3.80070976e-05  9.33405387e-06  6.54994906e-06
 -1.86553085e-01 -1.31016028e-02 -4.44888371e-01 -2.35618226e-02
  4.09642356e-02  1.26872425e-03  1.89163589e-01  2.31064137e-03
 -2.51843396e-02 -8.74187490e-03 -6.98027138e-02 -1.41593377e-02
  1.29697026e-02  3.04004856e-04  4.45540846e-02  4.86678306e-04
 -3.50341788e-01 -1.40973153e-02 -1.55941844e-02 -1.64173354e-02
 -1.11559910e-02 -5.50876251e-03 -4.73870407e-03 -5.53380826e-03
  2.69776348e-03  4.83502221e-02  4.30654670e-02  4.63031803e-02
 -2.25409053e-02 -1.31079639e-02 -1.11512353e-02 -1.30851778e-02
  1.80078729e-01 -4.81324259e-01  3.11783404e-01 -1.61728430e-01
  2.99651047e-01 -2.09808815e-01  3.49632201e-01 -2.16716328e-01
  1.11039934e+00]
supnorm grad right now is: 6.241429349523385
Weights right now are: 
[ 9.34039028e-01  4.08068337e-01  2.25051334e-02  4.49973285e-01
 -1.72425826e+00 -3.93604700e-01  5.48800912e-01  9.65109763e-01
 -1.88444616e+00  4.22540781e+00 -1.61298190e+00  6.84172425e+00
  1.23912153e+01  2.79215649e+01  1.32248331e+01  2.91430009e+01
  2.18684690e+01  3.07115166e+01  2.22146342e+01  3.03884012e+01
  2.05383222e+01  3.11859211e+01  1.98705849e+01  3.10915124e+01
 -7.19042540e+00 -6.41331994e+00  4.07486354e+00 -9.23926302e+00
  4.07043524e+00  1.13289372e+01  5.25967387e+00  1.84848342e+01
 -1.59052169e+00 -1.41154704e+01  1.01758048e+01 -1.64925288e+01
  2.57816319e+00  1.26082722e+01  9.67437003e+00  1.95541529e+01
 -9.60131191e-01  3.57911789e+01  2.77614820e+01  3.24261741e+01
  2.96042838e+01  1.21212621e+01  1.94284198e+01  1.69270252e+01
 -4.06930987e-02  3.12807566e+00  1.55619181e+00  2.19974495e+00
  3.01463184e+01  5.21405092e+00  1.17180734e+01  8.72080375e+00
 -3.83068319e-01 -5.47569079e+00  5.59239292e+00 -3.77429860e+00
  5.24447412e+00 -4.16374604e+00  5.26839857e+00 -3.88521121e+00
  3.08934853e+01]
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1362.839988623446
gradient value of function right now is: [ 8.17460839e-01 -1.40535687e-03 -1.71693962e-04 -2.35457129e-04
 -8.17460839e-01  1.40535687e-03  1.71693962e-04  2.35457129e-04
 -3.08865936e-01  1.09191485e-01  9.05992436e-02  6.04117181e-02
  1.67674157e-04  2.47204973e-04  4.19137140e-04  2.56602243e-05
  7.13888867e-05  2.39454382e-05  1.97142180e-05  3.03707932e-06
  9.77739405e-05  3.19125930e-05  2.85100786e-05  4.02154150e-06
 -1.02048540e-01  5.05906109e-03 -8.09781907e-02  8.87412548e-03
  2.94705557e-02 -1.18359122e-03 -4.68767247e-02 -3.76490672e-03
 -8.06537345e-03  2.00503221e-03  9.77757435e-03  2.80111042e-03
  9.65140795e-03 -2.63392156e-04 -4.76246280e-02 -9.19583914e-04
 -2.88884956e-01 -8.20429876e-03 -9.32724506e-03 -9.60296668e-03
  3.17480456e-03  1.73858306e-03  1.80018185e-03  1.99002896e-03
  2.22813859e-01  1.19713964e-01  1.22701486e-01  1.24697372e-01
  9.65028400e-03  8.12620481e-03  8.38958537e-03  9.16647442e-03
 -2.67938898e-01  2.20527201e-01  1.55729246e-01  4.45693598e-02
  1.44399835e-01  7.10433868e-02  1.75492530e-01  6.50138963e-02
  6.31982528e+00]
supnorm grad right now is: 6.319825279724991
Weights right now are: 
[  0.83858482   1.68984779   1.52541542   1.94453522  -1.62880406
  -1.67538416  -0.95410938  -0.52945217  -1.87641871   4.26088141
  -1.4912487    6.86838564  12.20735661  28.53419491  14.48095802
  30.0336654   22.71725672  31.9142667   22.76787714  31.60273659
  21.34039502  32.40444938  20.29418375  32.35877513  -7.22900297
  -6.26528203   4.54623327  -9.0393407    4.42585998   9.70533822
   5.16972793  17.17980961  -2.25753026 -15.48054837   9.86460839
 -17.70192049   2.74581354  11.36950694  10.07555991  18.73295983
  -1.40987203  36.22041975  28.02840391  32.75463688  30.18425334
  12.25433195  19.48983384  16.97278303   0.26239972   3.05474105
   1.64449447   2.18884852  31.11810756   5.61095255  12.00381593
   9.03497592  -0.49483751  -5.24888147   5.51597729  -4.11461227
   5.33107068  -4.06330025   5.30809061  -3.90515282  30.93201505]
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1358.4675713499273
gradient value of function right now is: [ 1.09868320e+01  2.75573623e-02  2.72509516e-03  3.43340529e-03
 -1.09868320e+01 -2.75573623e-02 -2.72509516e-03 -3.43340529e-03
 -2.22745277e+00 -3.09762880e-01 -5.43837543e-01 -6.90965915e-02
 -1.94976902e-02 -5.09876343e-03 -1.69678675e-03 -6.73013428e-04
 -1.41660787e-03 -5.22255601e-04 -1.12630567e-04 -6.83189686e-05
 -1.77460106e-03 -6.26830269e-04 -1.43225944e-04 -8.19336034e-05
 -2.70782648e-01 -3.51580820e-02 -7.99188878e-01 -5.65732094e-02
  4.81809246e-02  4.52834391e-03  4.89403273e-01  9.62189046e-03
 -5.47407838e-02 -2.06271921e-02 -1.56178901e-01 -2.91426970e-02
  1.13641901e-02  9.11597820e-04  1.50598274e-01  2.01649891e-03
 -3.01882148e-01 -1.90496813e-02 -2.20030141e-02 -2.27684228e-02
 -3.01922478e-02 -1.12775186e-02 -1.22445017e-02 -1.30961691e-02
 -2.28497401e-01 -8.66532605e-02 -9.22274258e-02 -9.22776321e-02
 -5.73112409e-02 -2.89753027e-02 -3.11172333e-02 -3.32772234e-02
  9.82291281e-01 -1.15730674e+00  5.77204742e-01 -3.81770507e-01
  5.90313757e-01 -5.18578650e-01  6.53877349e-01 -5.09333498e-01
 -1.27947632e+01]
supnorm grad right now is: 12.794763183674496
Weights right now are: 
[  1.05185621   1.6658829    1.74116462   2.12409545  -1.84207545
  -1.65141927  -1.16985857  -0.7090124   -2.19512347   4.13208772
  -1.69611685   6.90780633  12.73437071  28.97567445  15.1587036
  30.79673186  23.10665283  32.04492372  23.32920093  32.02817381
  21.78201256  32.57870233  20.89655753  32.83520953  -7.43768842
  -7.10442726   4.43147273  -9.39727243   3.94811641   9.61221209
   5.38162077  16.86820958  -2.63705324 -14.49756039  10.48392876
 -16.54039839   1.98964239  11.29769915  10.31156193  18.5916386
  -0.86867848  37.12853557  28.75100766  33.49502872  30.06862537
  13.29227982  20.2782158   17.81043067  -0.32185288   3.26547381
   1.9376305    2.44384905  31.28033607   6.55095088  12.7228034
   9.80496294   0.7153008   -5.1799923    5.3482659   -3.63635702
   5.11206591  -3.59858474   5.1104327   -3.47733118  30.78856696]
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1362.3334782249576
gradient value of function right now is: [-4.87974255e+00 -5.39966169e-02 -8.41271004e-03 -9.88656319e-03
  4.87974255e+00  5.39966169e-02  8.41271004e-03  9.88656319e-03
  1.28945489e+00  2.28652739e-01  9.33441543e-02 -2.42450628e-02
 -3.64739864e-03 -1.48528721e-03 -3.06931909e-04 -8.34596830e-05
 -1.68709860e-03 -9.37279766e-04 -1.19870272e-04 -4.81534733e-05
 -2.10047084e-03 -1.12691183e-03 -1.53008803e-04 -5.84804672e-05
  2.43409551e-01  1.83353107e-02  7.74063012e-01  3.59466290e-02
 -7.69897780e-02 -6.11351584e-03 -4.32553114e-01 -1.44395159e-02
  8.08876648e-03  6.13166720e-03  1.19967716e-01  1.59790365e-02
 -8.74272757e-03 -7.15590750e-04 -4.62513128e-02 -1.82705662e-03
  4.16458365e-01  1.75904390e-02  2.06155348e-02  2.10352497e-02
  1.27820314e-02  1.26628710e-02  1.09968034e-02  1.24132554e-02
 -3.94748905e-01 -4.68386183e-02 -5.34269294e-02 -5.30320401e-02
  3.93369506e-02  3.88313764e-02  3.39709802e-02  3.81286385e-02
 -2.06962872e-01  8.22233009e-01 -4.10068099e-01  3.66468626e-01
 -4.44228906e-01  4.70664665e-01 -4.91413957e-01  4.57439789e-01
  1.05530097e+01]
supnorm grad right now is: 10.553009721114911
Weights right now are: 
[  0.80421666  -0.19014968  -0.50213937  -0.10536319  -1.5944359
   0.20461332   1.07344542   1.52044623  -1.72800445   4.05513985
  -2.2117341    6.5404283   13.76640406  30.35977142  16.19840787
  31.91868047  24.9045816   33.56195791  24.86316376  33.19441091
  23.51361059  34.06921514  22.39328207  33.99644737  -7.10489569
  -7.53305028   4.74279759  -9.94595199   3.41896448   8.75373631
   4.62283041  15.98630069  -2.43195797 -15.10644803  10.07913524
 -17.32400608   1.56761732  10.40473036  10.82823139  17.6992137
  -0.5983188   37.85646671  29.38442716  34.13258812  30.66230215
  12.95480012  19.86237927  17.37598219  -0.50052035   2.89719198
   1.62338564   2.11837226  32.16163484   6.47761718  12.57529982
   9.63430841   0.62572059  -4.96149052   5.32126426  -3.88507334
   5.01553276  -4.16024572   5.02774674  -3.964761    30.801459  ]
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1362.947789181832
gradient value of function right now is: [ 3.33672248e+00  2.53392925e-03  1.36495656e-04  1.76127517e-04
 -3.33672248e+00 -2.53392925e-03 -1.36495656e-04 -1.76127517e-04
 -8.50438350e-01 -2.29722829e-01 -1.87265929e-01 -7.22814810e-02
 -4.25552730e-04 -1.10493614e-04 -3.62980671e-05 -9.80400123e-06
  1.49630179e-05  6.95524563e-06  1.27422602e-06  6.23305079e-07
  2.18197694e-05  9.43028193e-06  1.86888377e-06  8.40559493e-07
 -9.69333589e-02 -1.06113114e-02 -3.06588250e-01 -1.71478714e-02
  2.31253865e-02  9.69693739e-04  2.10172576e-01  1.57350538e-03
 -2.11809810e-02 -9.64421935e-03 -9.14432380e-02 -1.46170191e-02
  3.97596842e-03  1.08821392e-04  6.75575492e-02  1.67086697e-04
 -1.63323373e-01 -9.03911278e-03 -8.78010247e-03 -9.55790532e-03
 -1.04659306e-02 -5.18838191e-03 -4.18486361e-03 -4.86811175e-03
 -1.32561517e-01 -6.67177700e-02 -6.91300494e-02 -6.93569450e-02
 -1.94994557e-02 -1.06129149e-02 -8.53160543e-03 -9.92813629e-03
  1.48538366e-01 -4.91109373e-01  2.23113118e-01 -1.74855884e-01
  1.83531134e-01 -1.82971880e-01  2.20227510e-01 -1.95247950e-01
 -3.49460572e+00]
supnorm grad right now is: 3.494605718799812
Weights right now are: 
[ 7.20321182e-01  6.56940272e-01  1.28959371e-01  5.44293271e-01
 -1.51054042e+00 -6.42476634e-01  4.42346674e-01  8.70789777e-01
 -1.99217169e+00  4.60331312e+00 -1.72339710e+00  7.13491518e+00
  1.40798731e+01  3.07402971e+01  1.66762561e+01  3.21443399e+01
  2.54659205e+01  3.42862771e+01  2.56375312e+01  3.35652878e+01
  2.41240701e+01  3.48374510e+01  2.33000607e+01  3.44003707e+01
 -7.96420369e+00 -7.59766088e+00  4.31658663e+00 -1.07087681e+01
  4.06833948e+00  1.09617596e+01  4.49551166e+00  1.85780011e+01
 -1.73008649e+00 -1.47514778e+01  1.04271657e+01 -1.74053547e+01
  1.71584094e+00  1.22439416e+01  1.03984962e+01  1.98253704e+01
 -1.60933139e+00  3.80560048e+01  2.93810297e+01  3.41862348e+01
  3.12245292e+01  1.16459410e+01  1.87372346e+01  1.61827647e+01
  3.87688324e-01  3.27791741e+00  1.96592241e+00  2.50036418e+00
  3.15466481e+01  4.46367404e+00  1.07167141e+01  7.71785426e+00
 -4.92676513e-03 -5.68419349e+00  5.52817935e+00 -4.15689071e+00
  5.55184807e+00 -4.83751953e+00  5.45242248e+00 -4.48012253e+00
  3.08817928e+01]
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1363.2172860471724
gradient value of function right now is: [ 2.52765800e+00  1.39693830e-03  7.06882657e-05  8.83165496e-05
 -2.52765800e+00 -1.39693830e-03 -7.06882657e-05 -8.83165496e-05
 -7.11167281e-01 -1.25749750e-01 -1.08428945e-01 -2.81560922e-02
 -5.49755634e-04 -1.42064180e-04 -4.87261533e-05 -1.36198435e-05
 -2.42374056e-06 -1.39801948e-06 -2.07435471e-07 -1.35688592e-07
 -2.33325021e-06 -1.23795652e-06 -2.04467653e-07 -1.19588973e-07
 -1.00228133e-01 -8.95438691e-03 -2.54829609e-01 -1.57164524e-02
  2.36414839e-02  5.72610081e-04  1.46503436e-01  8.12821386e-04
 -1.49395105e-02 -6.83856967e-03 -6.05586292e-02 -1.15695731e-02
  4.54346034e-03  6.12727813e-05  3.92981879e-02  5.35143890e-05
 -1.83910947e-01 -8.38012563e-03 -7.13262677e-03 -8.25297911e-03
 -7.37789325e-03 -4.63563899e-03 -3.07460198e-03 -3.89698523e-03
 -2.23584143e-02 -1.31516488e-02 -1.13920392e-02 -1.29316205e-02
 -1.44751877e-02 -1.00589286e-02 -6.59003951e-03 -8.39979178e-03
  7.23269370e-02 -3.72398854e-01  2.09574497e-01 -1.54275785e-01
  1.52565545e-01 -1.43503706e-01  1.95565483e-01 -1.61812113e-01
 -3.30010612e+00]
supnorm grad right now is: 3.300106122080112
Weights right now are: 
[  0.63619918   1.53201462   1.15849309   1.56430747  -1.42641842
  -1.51755099  -0.58718704  -0.14922442  -1.80242212   3.73708472
  -1.53933521   7.01981465  14.31085541  30.21085769  16.39279143
  31.89572436  25.74997351  34.68684168  25.37253618  33.70634137
  24.43350185  35.27059696  23.03498291  34.56644202  -7.46706527
  -5.7344473    4.64205957  -9.28061484   3.51375601  10.23839774
   5.44026655  19.15768112  -2.27212467 -14.62059869  10.58555915
 -17.09239938   1.26815282  11.7882752   11.27562134  20.25708128
  -0.83943009  39.71432795  30.71856911  35.65394235  32.3423945
  12.51005056  19.61482217  17.05563006  -0.0545892    2.63337832
   1.64444674   2.02828286  32.28609467   3.99117907  10.36973958
   7.29157231   0.43436172  -4.89255326   4.74737846  -3.84764196
   4.90137525  -4.49279328   4.71354329  -4.11167291  31.09147037]
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1363.2319671095572
gradient value of function right now is: [ 2.35275125e+00  2.24641160e-03  1.42394135e-04  1.73148951e-04
 -2.35275125e+00 -2.24641160e-03 -1.42394135e-04 -1.73148951e-04
 -6.60497212e-01 -1.76587616e-01 -1.14235377e-01 -4.89663958e-02
 -1.32023226e-03 -4.41395696e-04 -9.09983181e-05 -4.04001839e-05
 -1.83493277e-05 -1.13774943e-05 -1.34807146e-06 -1.02386474e-06
 -2.32298672e-05 -1.37021469e-05 -1.70234930e-06 -1.23076503e-06
 -8.69352681e-02 -6.83143990e-03 -2.44089405e-01 -1.22353256e-02
  1.97878697e-02  3.29790163e-04  1.82624549e-01  4.13243769e-04
 -1.15070251e-02 -5.45044726e-03 -5.41863835e-02 -9.46389238e-03
  3.60342092e-03  3.60036112e-05  5.36375578e-02  2.17068689e-05
 -1.57653696e-01 -7.17874265e-03 -5.90166735e-03 -6.97013601e-03
 -5.59138549e-03 -3.44991476e-03 -2.27599298e-03 -2.89313169e-03
 -1.27745970e-01 -4.89887560e-02 -4.78977531e-02 -4.99260957e-02
 -1.10717729e-02 -7.62195420e-03 -4.96319747e-03 -6.34527043e-03
  5.61725222e-02 -2.89187069e-01  2.07935313e-01 -1.19678856e-01
  1.41817023e-01 -1.09001876e-01  1.86357979e-01 -1.24324896e-01
 -4.18528144e+00]
supnorm grad right now is: 4.185281439171336
Weights right now are: 
[ 1.05455041e+00  2.66115365e+00  8.24808532e-01  1.39678439e+00
 -1.84476965e+00 -2.64669001e+00 -2.53502487e-01  1.82986545e-02
 -2.17816704e+00  4.03956945e+00 -2.46664727e+00  6.28460862e+00
  1.24903287e+01  3.09756758e+01  1.44502681e+01  3.30001623e+01
  2.60337493e+01  3.64554288e+01  2.56006754e+01  3.53887222e+01
  2.44908064e+01  3.69782163e+01  2.30358427e+01  3.62482396e+01
 -7.62511346e+00 -5.86779755e+00  4.78472628e+00 -9.58987818e+00
  5.04835822e+00  1.26749940e+01  4.29540564e+00  2.06837228e+01
 -1.48521832e+00 -1.44973604e+01  1.08889146e+01 -1.72031050e+01
  1.95151423e+00  1.35170128e+01  1.15383016e+01  2.07202872e+01
 -8.83085682e-01  3.90826808e+01  3.02193385e+01  3.50766489e+01
  3.25933692e+01  1.16424547e+01  1.87459900e+01  1.61824256e+01
 -3.10852526e-01  2.42120440e+00  1.24299237e+00  1.73588023e+00
  3.21382156e+01  3.50612156e+00  9.96694712e+00  6.81987597e+00
  1.02397490e-01 -5.19479747e+00  5.65124728e+00 -4.02873916e+00
  5.96900013e+00 -4.72889156e+00  5.70041668e+00 -4.37400975e+00
  3.09969716e+01]
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1363.301448712881
gradient value of function right now is: [ 1.93228888e+00  9.11371855e-04 -9.30412192e-07 -1.89217175e-06
 -1.93228888e+00 -9.11371855e-04  9.30412192e-07  1.89217175e-06
 -5.52380089e-01 -1.75297375e-01 -1.11103920e-01 -5.67493247e-02
 -1.06141501e-03 -2.17911540e-04 -7.16095380e-05 -2.20184996e-05
 -6.09290282e-08 -3.08939827e-06 -2.65333993e-07 -3.69226582e-07
 -7.51376147e-07 -3.64389214e-06 -3.77639118e-07 -4.48521783e-07
 -7.28181346e-02 -6.48415002e-03 -2.08089655e-01 -1.16856189e-02
  1.45930385e-02  2.00876631e-04  1.55936180e-01  3.40030189e-05
 -9.86546200e-03 -5.14191319e-03 -4.89589334e-02 -8.99707130e-03
  2.27773430e-03  1.61032394e-05  4.86315864e-02 -3.26805117e-05
 -1.25957412e-01 -5.58630184e-03 -4.34884070e-03 -5.28712100e-03
 -5.03342500e-03 -3.13522295e-03 -2.05258984e-03 -2.62027775e-03
 -1.33850794e-01 -6.10034998e-02 -6.03388191e-02 -6.22342518e-02
 -9.46706245e-03 -6.58502396e-03 -4.20890599e-03 -5.43904697e-03
  6.46772926e-02 -2.63910139e-01  1.72881194e-01 -9.84874279e-02
  1.12447936e-01 -9.20152847e-02  1.50838092e-01 -1.04381523e-01
 -3.67467751e+00]
supnorm grad right now is: 3.6746775059193038
Weights right now are: 
[  0.84786764   2.12855541   0.66804132   1.17346005  -1.63808687
  -2.11409177  -0.09673527   0.241623    -2.36755621   4.10458667
  -1.74626159   6.63851915  13.28316981  31.27541608  15.40525141
  33.55881185  27.12415853  37.34219804  26.30887555  36.37183384
  25.5343948   37.74787722  23.71204905  37.20674911  -8.20495829
  -6.36297814   4.54390048 -10.23331428   4.58189123  12.21340459
   4.18409507  20.46822324  -0.47081249 -13.82130013  11.31099801
 -16.84994801   1.64262455  13.04896606  11.86710951  20.69236381
  -0.73885459  40.35009359  31.57856845  36.36587906  33.32663097
  11.30181955  18.40513202  15.84184328  -0.60072754   2.54037201
   1.44774948   1.89720886  32.19247873   3.06496139   9.47436419
   6.343721    -0.18539511  -5.1757531    5.10365328  -3.89132393
   5.50596949  -4.69204867   5.19449204  -4.2672863   30.82729378]
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1363.2665730994704
gradient value of function right now is: [ 2.68516448e+00  2.06908300e-03 -2.81216249e-05 -2.15934372e-05
 -2.68516448e+00 -2.06908300e-03  2.81216249e-05  2.15934372e-05
 -6.57294716e-01 -1.73132670e-01 -1.28535412e-01 -4.36230656e-02
 -2.25575414e-03 -4.73252281e-04 -2.06542058e-04 -4.16149849e-05
 -1.42038539e-06 -2.78939362e-06 -9.99343676e-07 -4.80648497e-07
 -5.01362354e-06 -3.94304506e-06 -1.61911321e-06 -6.46723409e-07
 -8.67937396e-02 -9.11154930e-03 -2.52570878e-01 -1.63081965e-02
  1.58897662e-02  5.64696860e-04  1.81212490e-01  8.95367114e-04
 -1.21345656e-02 -6.20796178e-03 -5.17358662e-02 -1.04333770e-02
  2.15890684e-03  5.23279378e-05  4.57983717e-02  5.25642880e-05
 -1.25644642e-01 -6.36679010e-03 -4.79198746e-03 -5.91018557e-03
 -6.97981149e-03 -3.82817237e-03 -2.55350889e-03 -3.23241484e-03
 -1.15838966e-01 -4.98073231e-02 -4.85435057e-02 -5.06460878e-02
 -1.40413013e-02 -8.89971107e-03 -5.86859062e-03 -7.46459083e-03
  1.51790080e-01 -3.66708747e-01  1.89792270e-01 -1.26211008e-01
  1.23593109e-01 -1.16735352e-01  1.65457594e-01 -1.32349977e-01
 -4.31470620e+00]
supnorm grad right now is: 4.31470620385725
Weights right now are: 
[  0.8461794    2.09061348   0.14012809   0.66868539  -1.63639863
  -2.07614985   0.43117795   0.74639766  -2.37855458   4.51453347
  -1.1964133    7.302071    12.94354932  32.0316636   16.27418285
  35.37387243  28.82439676  39.85382396  28.26781727  38.4937494
  26.94709289  40.01664915  25.58466211  39.50880476  -8.45480637
  -6.70492581   4.49853833 -10.21809507   5.34613292  11.34938514
   3.88131629  19.37726485  -0.53044534 -14.68635347  11.10055077
 -17.66421472   2.57046892  10.78693622  11.64487823  18.1482979
  -0.95641766  40.1933465   31.55105516  36.23637519  33.54189297
  11.59754293  18.64140359  16.06386757  -0.18340453   3.77277297
   2.67167314   3.12636017  32.95763012   4.3073268   10.63992083
   7.48881823   0.2854226   -5.20829401   5.42374918  -4.30355711
   5.70779251  -4.83819264   5.49034102  -4.56887365  30.72110795]
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1094.0386418274145
W_T_median: 1057.2357272641216
W_T_pctile_5: 951.901603625499
W_T_CVAR_5_pct: 925.7265391888309
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.4
F value: -1363.2665730994704
-----------------------------------------------
