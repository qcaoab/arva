Starting at: 
08-06-23_17:42

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.005383     0.031411
192608                    0.0319              0.0561  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0355  ...     0.005323    -0.028996
192611                   -0.0038              0.0294  ...     0.005303     0.028554

[5 rows x 6 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.043289    -0.036240
202209                   -0.0955             -0.0871  ...    -0.050056    -0.091324
202210                    0.0883              0.1486  ...    -0.014968     0.077403
202211                   -0.0076              0.0462  ...     0.040789     0.052365
202212                   -0.0457             -0.0499  ...    -0.018566    -0.057116

[5 rows x 6 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'CPI_nom_ret_ind', 'T30_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Value_Hi30_real_ret
T30_real_ret             1.000000  ...             0.018239
B10_real_ret             0.351722  ...             0.031301
VWD_real_ret             0.068448  ...             0.909335
Size_Lo30_real_ret       0.014412  ...             0.908542
Value_Hi30_real_ret      0.018239  ...             1.000000

[5 rows x 5 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       5       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       5           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 5)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 946.50330641104
W_T_median: 707.5512584605915
W_T_pctile_5: -238.22847581189643
W_T_CVAR_5_pct: -367.3344080824147
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1747.4023950822375
Current xi:  [77.737946]
objective value function right now is: -1747.4023950822375
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1751.1555962808798
Current xi:  [55.0291]
objective value function right now is: -1751.1555962808798
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1793.7518952156074
Current xi:  [36.327606]
objective value function right now is: -1793.7518952156074
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1795.4456567194206
Current xi:  [23.285051]
objective value function right now is: -1795.4456567194206
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1796.5860372080888
Current xi:  [7.5865912]
objective value function right now is: -1796.5860372080888
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1796.9857961538987
Current xi:  [-4.313299]
objective value function right now is: -1796.9857961538987
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1797.4010448974293
Current xi:  [-17.494143]
objective value function right now is: -1797.4010448974293
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1798.6053752822975
Current xi:  [-33.568504]
objective value function right now is: -1798.6053752822975
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-45.500305]
objective value function right now is: -1798.5018104742492
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1798.6377696958334
Current xi:  [-62.39432]
objective value function right now is: -1798.6377696958334
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1799.6590834549254
Current xi:  [-75.81731]
objective value function right now is: -1799.6590834549254
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-88.86552]
objective value function right now is: -1799.319755896377
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-104.96002]
objective value function right now is: -1799.278891123278
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1800.2094301481284
Current xi:  [-114.91552]
objective value function right now is: -1800.2094301481284
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-126.93786]
objective value function right now is: -1800.1401558594098
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1800.717929512618
Current xi:  [-139.85976]
objective value function right now is: -1800.717929512618
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-147.55667]
objective value function right now is: -1800.5867167501544
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-153.42302]
objective value function right now is: -1799.345778936064
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1800.828889076267
Current xi:  [-159.73436]
objective value function right now is: -1800.828889076267
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-166.59125]
objective value function right now is: -1800.418592470121
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-171.54587]
objective value function right now is: -1800.6334195007717
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-174.53967]
objective value function right now is: -1800.2246474779597
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-177.33192]
objective value function right now is: -1799.991424737391
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-178.3575]
objective value function right now is: -1800.2993540319915
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-178.27702]
objective value function right now is: -1800.359489780617
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-178.98744]
objective value function right now is: -1800.7590301138066
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-177.89313]
objective value function right now is: -1800.5827139102391
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-178.60516]
objective value function right now is: -1799.8814873537083
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-178.69673]
objective value function right now is: -1800.7048909239825
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-177.55704]
objective value function right now is: -1799.9133504977315
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-177.69835]
objective value function right now is: -1800.2823588899212
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-178.8006]
objective value function right now is: -1800.4542142743455
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-179.7164]
objective value function right now is: -1800.6477435691868
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-178.32272]
objective value function right now is: -1800.0733513640278
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-177.64915]
objective value function right now is: -1800.6004676126754
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1800.9788676535866
Current xi:  [-177.07726]
objective value function right now is: -1800.9788676535866
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-176.41548]
objective value function right now is: -1800.9591367535754
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-175.77872]
objective value function right now is: -1800.9670831308897
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1800.987722498889
Current xi:  [-175.81886]
objective value function right now is: -1800.987722498889
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1800.995303149676
Current xi:  [-175.61023]
objective value function right now is: -1800.995303149676
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-175.13246]
objective value function right now is: -1800.9434143831527
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1801.0230947339824
Current xi:  [-174.88335]
objective value function right now is: -1801.0230947339824
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-174.58476]
objective value function right now is: -1800.9138431252811
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-174.63066]
objective value function right now is: -1800.7655731105274
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-174.99126]
objective value function right now is: -1800.9427787473326
new min fval from sgd:  -1801.0266492562173
new min fval from sgd:  -1801.0325981645376
new min fval from sgd:  -1801.040824295313
new min fval from sgd:  -1801.041323928778
new min fval from sgd:  -1801.041880315835
new min fval from sgd:  -1801.0422375689175
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-175.18277]
objective value function right now is: -1800.9611235227555
new min fval from sgd:  -1801.0492753690457
new min fval from sgd:  -1801.0549989327255
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-175.26497]
objective value function right now is: -1800.7821776783364
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-175.14795]
objective value function right now is: -1801.0536318076488
new min fval from sgd:  -1801.0585152643152
new min fval from sgd:  -1801.0634080475866
new min fval from sgd:  -1801.0650350373985
new min fval from sgd:  -1801.0650731075648
new min fval from sgd:  -1801.0675813699916
new min fval from sgd:  -1801.0697717157047
new min fval from sgd:  -1801.0721858657955
new min fval from sgd:  -1801.073856943805
new min fval from sgd:  -1801.073880901395
new min fval from sgd:  -1801.078250747195
new min fval from sgd:  -1801.0801330653878
new min fval from sgd:  -1801.0817443064168
new min fval from sgd:  -1801.0824265820097
new min fval from sgd:  -1801.0829043248584
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-175.12878]
objective value function right now is: -1801.0649730912335
new min fval from sgd:  -1801.0832137748682
new min fval from sgd:  -1801.0833493099612
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-175.1423]
objective value function right now is: -1801.0831495276457
min fval:  -1801.0833493099612
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 10.7196,   2.8558],
        [ -0.2882,   1.0825],
        [ -0.2882,   1.0825],
        [  4.7975,   8.4395],
        [ -0.2882,   1.0825],
        [ 11.2281,   0.0580],
        [  5.3768,   8.4047],
        [  9.3956,   3.0515],
        [ -0.2882,   1.0825],
        [ -0.1638,   2.5734],
        [  9.0277,   0.0214],
        [-12.8872,   4.9779],
        [ -0.2882,   1.0825]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.5987,  -0.8569,  -0.8569,   7.8126,  -0.8569, -10.2629,   7.3860,
         -3.6247,  -0.8569,   0.2644,  -8.3833,   8.0716,  -0.8569],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-6.3520e-02, -1.1504e-02, -1.1503e-02, -7.6244e-02, -1.1504e-02,
         -2.6598e-02, -7.1911e-02, -1.6430e-02, -1.1504e-02, -7.0962e-03,
         -2.7375e-02, -1.6573e-02, -1.1504e-02],
        [-6.4619e-02, -1.1487e-02, -1.1486e-02, -7.7233e-02, -1.1487e-02,
         -2.7163e-02, -7.2744e-02, -1.6636e-02, -1.1487e-02, -7.1030e-03,
         -2.7915e-02, -1.6616e-02, -1.1487e-02],
        [-6.3544e-02, -1.1504e-02, -1.1503e-02, -7.6266e-02, -1.1504e-02,
         -2.6610e-02, -7.1929e-02, -1.6434e-02, -1.1504e-02, -7.0966e-03,
         -2.7386e-02, -1.6574e-02, -1.1504e-02],
        [ 2.6356e+00,  2.1685e-02,  2.1675e-02,  5.3790e+00,  2.1685e-02,
          6.7906e+00,  5.1289e+00,  1.8170e+00,  2.1685e-02,  2.0917e-01,
          3.3423e+00,  6.5518e+00,  2.1685e-02],
        [-1.4412e+00, -5.9751e-04, -5.4762e-04, -2.5399e+00, -5.9743e-04,
         -2.7097e+00, -2.4945e+00, -8.2038e-01, -5.9755e-04,  5.1195e-03,
         -1.2092e+00, -2.6458e+00, -5.9672e-04],
        [-6.3520e-02, -1.1504e-02, -1.1503e-02, -7.6244e-02, -1.1504e-02,
         -2.6598e-02, -7.1911e-02, -1.6430e-02, -1.1504e-02, -7.0962e-03,
         -2.7375e-02, -1.6573e-02, -1.1504e-02],
        [ 2.0586e+00,  9.0823e-02,  9.0708e-02,  4.2519e+00,  9.0823e-02,
          5.3477e+00,  4.0536e+00,  1.3694e+00,  9.0823e-02,  9.9237e-02,
          2.6627e+00,  5.2409e+00,  9.0823e-02],
        [-6.3555e-02, -1.1503e-02, -1.1503e-02, -7.6276e-02, -1.1503e-02,
         -2.6616e-02, -7.1938e-02, -1.6436e-02, -1.1503e-02, -7.0966e-03,
         -2.7391e-02, -1.6574e-02, -1.1503e-02],
        [-6.3520e-02, -1.1504e-02, -1.1503e-02, -7.6244e-02, -1.1504e-02,
         -2.6598e-02, -7.1911e-02, -1.6430e-02, -1.1504e-02, -7.0962e-03,
         -2.7375e-02, -1.6573e-02, -1.1504e-02],
        [-6.3520e-02, -1.1504e-02, -1.1503e-02, -7.6244e-02, -1.1504e-02,
         -2.6598e-02, -7.1911e-02, -1.6430e-02, -1.1504e-02, -7.0962e-03,
         -2.7375e-02, -1.6573e-02, -1.1504e-02],
        [-6.3520e-02, -1.1504e-02, -1.1503e-02, -7.6244e-02, -1.1504e-02,
         -2.6598e-02, -7.1911e-02, -1.6430e-02, -1.1504e-02, -7.0962e-03,
         -2.7375e-02, -1.6573e-02, -1.1504e-02],
        [-2.4150e+00,  5.3122e-02,  5.3015e-02, -4.9504e+00,  5.3123e-02,
         -6.4078e+00, -4.9575e+00, -1.7153e+00,  5.3123e-02, -1.5553e-01,
         -3.0327e+00, -6.1391e+00,  5.3122e-02],
        [-6.3520e-02, -1.1504e-02, -1.1503e-02, -7.6244e-02, -1.1504e-02,
         -2.6598e-02, -7.1911e-02, -1.6430e-02, -1.1504e-02, -7.0962e-03,
         -2.7375e-02, -1.6573e-02, -1.1504e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.3687, -0.3756, -0.3689, -6.6470,  2.9529, -0.3687, -5.3763, -0.3690,
        -0.3687, -0.3687, -0.3687,  6.1493, -0.3687], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0110,  -0.0111,  -0.0110,   9.6549,  -3.7413,  -0.0110,   6.8410,
          -0.0110,  -0.0110,  -0.0110,  -0.0110, -10.1844,  -0.0110]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-3.1285, -7.9441],
        [-1.2254,  0.5274],
        [-8.5915,  0.5648],
        [ 9.2327,  1.9794],
        [-3.2298,  8.5577],
        [ 9.6194,  3.6357],
        [-6.0394, -2.3959],
        [ 6.9933,  0.2281],
        [10.2114, -0.3822],
        [-2.0428, -0.6292],
        [-2.7105,  4.3547],
        [-0.1385,  7.5572],
        [-6.8603, -7.1168]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -6.9321,  -2.0628,   4.7019,   0.3603,   7.6305,   1.0871,  -2.2703,
         -5.2578, -10.0900,  -3.6180,   1.5964,   6.5494,  -7.4174],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-5.7379e-01, -8.5190e-02, -3.5501e+00, -1.3251e+00, -5.3031e-01,
         -3.9543e+00,  2.6651e+00, -8.6375e-01,  1.9316e+00,  2.6431e+00,
          1.9367e-01, -8.0188e-01,  1.6453e+00],
        [-5.6885e-01,  1.1710e-03, -1.0904e-01, -1.1113e+00, -2.6943e-01,
         -1.1064e+00, -4.2543e-01, -5.0329e-01, -7.3531e-01, -5.1157e-01,
         -5.8813e-02, -5.2507e-01, -4.8927e-01],
        [-5.6887e-01,  1.1641e-03, -1.0914e-01, -1.1112e+00, -2.6942e-01,
         -1.1063e+00, -4.2549e-01, -5.0321e-01, -7.3524e-01, -5.1154e-01,
         -5.8784e-02, -5.2507e-01, -4.8927e-01],
        [ 4.2762e+00, -9.6613e-02, -2.1746e+00,  4.7903e+00, -7.0360e+00,
          3.8633e+00, -1.1555e+00,  4.0208e+00,  5.3254e+00, -8.2587e-03,
         -2.3623e+00, -3.5618e+00,  5.5669e-01],
        [ 2.2806e+00,  1.2103e-02,  3.2729e+00,  1.7674e+00, -1.0194e+01,
         -9.0913e-01, -4.3525e-02, -1.8136e-01, -3.7824e+00,  1.6725e+00,
         -1.6301e+00, -3.4352e+00,  2.1457e+00],
        [ 2.9485e+00, -2.8926e-02,  8.3778e-01, -6.0107e+00, -2.5441e+00,
         -4.8074e+00,  1.5450e+00, -3.9370e+00, -4.1647e+00,  1.8918e+00,
         -3.9756e-01, -2.7660e+00,  7.4920e+00],
        [ 3.2989e+00,  2.1809e-01, -8.6005e-01,  2.7221e+00, -2.5305e+00,
          1.2597e-01, -2.2631e-01,  2.8621e+00,  9.0401e+00, -6.6165e-02,
         -3.5981e+00, -3.2477e+00,  3.2119e-01],
        [ 3.0452e+00, -2.9803e-02,  8.6332e-01, -6.2064e+00, -2.6383e+00,
         -4.9445e+00,  1.5116e+00, -4.0308e+00, -4.4117e+00,  1.9455e+00,
         -4.1100e-01, -2.8586e+00,  7.7337e+00],
        [-1.1295e+00, -7.6275e-02, -8.3622e+00,  1.5930e+00, -3.5555e+00,
         -4.0487e-01, -7.9186e-01,  2.5592e+00,  8.2205e+00, -2.5433e-01,
         -1.8896e+00, -1.9969e+00, -2.1837e-01],
        [ 5.9281e-01,  7.4098e-02,  2.4234e+00,  1.8990e+00,  8.1634e-01,
          2.2880e+00, -2.2000e+00,  9.8332e-01, -1.2057e+00, -2.7198e+00,
         -4.5810e-02,  9.1519e-01, -1.0437e+00],
        [ 5.9119e-01,  7.5660e-02,  2.4447e+00,  1.8999e+00,  8.3039e-01,
          2.3015e+00, -2.2049e+00,  9.7747e-01, -1.2173e+00, -2.7203e+00,
         -3.9351e-02,  9.2300e-01, -1.0575e+00],
        [-1.2704e+00, -7.8245e-02, -1.7976e-01,  4.5493e+00, -2.0493e+00,
          5.0397e+00, -2.7566e+00,  1.5759e+00,  1.6898e+00, -5.4521e-01,
         -3.5777e-01, -3.1622e-01, -5.1379e+00],
        [ 5.9892e+00, -6.8512e-02,  2.6744e+00, -1.1179e+00, -4.8904e+00,
         -3.5286e+00,  3.5229e+00, -1.0913e+00, -8.1689e+00,  3.1463e+00,
         -6.2623e-01, -6.4138e+00,  1.5521e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.4046, -1.5180, -1.5180,  2.9696,  0.8042, -2.4816,  0.3795, -2.5388,
        -0.2798,  1.0663,  1.0826,  0.7472,  0.0092], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.0900,   0.4200,   0.4201,  -2.8094, -10.2487,  -1.1021,  -1.1905,
          -1.0947,  -6.3652,   2.3829,   2.5899,   0.6040,   3.7491],
        [ -2.7172,  -0.0946,  -0.0941,   0.5842,   0.3283,   4.5038,   0.9887,
           4.6524,   0.3957,   1.2994,   1.2731,  -1.6273,  -4.7537],
        [ -0.4500,  -0.0298,  -0.0298,  -2.0711,  -0.6088,  -0.4528,  -1.3074,
          -0.4526,  -0.7360,  -1.7552,  -1.7552,  -1.7108,  -0.4957],
        [ -0.4339,  -0.0299,  -0.0299,  -2.0019,  -0.5897,  -0.4387,  -1.2422,
          -0.4385,  -0.6967,  -1.7223,  -1.7224,  -1.6624,  -0.4906],
        [  3.3983,  -0.0655,  -0.0650,   1.0302,   0.9548,  -3.6597,   0.4849,
          -3.9342,   0.8328,  -0.6382,  -0.6906,   2.3219,   5.3500]],
       device='cuda:0'))])
xi:  [-175.1534]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 90.69701109636219
W_T_median: 86.04750205878126
W_T_pctile_5: -175.26912764063474
W_T_CVAR_5_pct: -303.79645564367195
Average q (qsum/M+1):  58.589457850302416
Optimal xi:  [-175.1534]
Expected(across Rb) median(across samples) p_equity:  0.14313838680585225
obj fun:  tensor(-1801.0833, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 946.50330641104
W_T_median: 707.5512584605915
W_T_pctile_5: -238.22847581189643
W_T_CVAR_5_pct: -367.3344080824147
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1755.8224064767944
Current xi:  [88.611755]
objective value function right now is: -1755.8224064767944
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1756.2414734319998
Current xi:  [79.24135]
objective value function right now is: -1756.2414734319998
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1759.6841913978362
Current xi:  [70.309135]
objective value function right now is: -1759.6841913978362
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1760.5824426761167
Current xi:  [62.642826]
objective value function right now is: -1760.5824426761167
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.37103]
objective value function right now is: -1760.552035127485
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [49.108727]
objective value function right now is: -1760.5636796871397
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [43.531944]
objective value function right now is: -1760.4517578851783
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [37.83546]
objective value function right now is: -1760.3406751117577
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1761.0707712574356
Current xi:  [32.776115]
objective value function right now is: -1761.0707712574356
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1761.164294102491
Current xi:  [27.993258]
objective value function right now is: -1761.164294102491
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [23.090569]
objective value function right now is: -1761.1085992264532
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.678856]
objective value function right now is: -1760.535133845545
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1761.406404798063
Current xi:  [8.153036]
objective value function right now is: -1761.406404798063
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1761.794262918895
Current xi:  [-0.0855931]
objective value function right now is: -1761.794262918895
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1762.5606033162082
Current xi:  [-0.04418065]
objective value function right now is: -1762.5606033162082
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0179831]
objective value function right now is: -1762.2656362950527
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02518791]
objective value function right now is: -1762.4235431900488
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0335366]
objective value function right now is: -1762.5512295765611
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1762.5824484391815
Current xi:  [-0.02676764]
objective value function right now is: -1762.5824484391815
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00802626]
objective value function right now is: -1762.5615067935041
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1762.6669705983056
Current xi:  [0.02303109]
objective value function right now is: -1762.6669705983056
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0172136]
objective value function right now is: -1762.2422323020203
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1762.7592216129844
Current xi:  [0.00810512]
objective value function right now is: -1762.7592216129844
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01402927]
objective value function right now is: -1762.4246906110586
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1762.9359053268952
Current xi:  [-0.08226342]
objective value function right now is: -1762.9359053268952
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01784397]
objective value function right now is: -1762.7246761389008
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02236846]
objective value function right now is: -1761.455854732378
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01568559]
objective value function right now is: -1762.9183229837106
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [0.01466564]
objective value function right now is: -1761.5314539838591
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02572471]
objective value function right now is: -1762.6150853250706
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02194148]
objective value function right now is: -1762.647395544276
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02145429]
objective value function right now is: -1761.8240003952203
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01783862]
objective value function right now is: -1762.5397296921165
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00603153]
objective value function right now is: -1761.8404737726287
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00265503]
objective value function right now is: -1762.892653003396
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1763.0557969869785
Current xi:  [0.00241992]
objective value function right now is: -1763.0557969869785
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1763.098646660956
Current xi:  [0.00067682]
objective value function right now is: -1763.098646660956
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1763.2593703323384
Current xi:  [0.00200537]
objective value function right now is: -1763.2593703323384
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.7309604e-05]
objective value function right now is: -1763.205305586253
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1763.2640276425775
Current xi:  [0.00186001]
objective value function right now is: -1763.2640276425775
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1763.3105019449142
Current xi:  [0.00207915]
objective value function right now is: -1763.3105019449142
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02509574]
objective value function right now is: -1763.0621746947354
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00399308]
objective value function right now is: -1762.925484020256
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00433342]
objective value function right now is: -1763.2173577883016
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00239577]
objective value function right now is: -1762.8478271402169
new min fval from sgd:  -1763.311551598066
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.000298]
objective value function right now is: -1763.1699112050908
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00254538]
objective value function right now is: -1762.8610889847625
new min fval from sgd:  -1763.324175640348
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00228008]
objective value function right now is: -1763.1889362972568
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00050259]
objective value function right now is: -1763.2993182464045
new min fval from sgd:  -1763.3250931911923
new min fval from sgd:  -1763.3255228814735
new min fval from sgd:  -1763.3260794869227
new min fval from sgd:  -1763.3279759605873
new min fval from sgd:  -1763.3292748377332
new min fval from sgd:  -1763.3302985424439
new min fval from sgd:  -1763.3306059710285
new min fval from sgd:  -1763.330636078868
new min fval from sgd:  -1763.3319493774777
new min fval from sgd:  -1763.3331978345523
new min fval from sgd:  -1763.3340795375957
new min fval from sgd:  -1763.3354231637409
new min fval from sgd:  -1763.3364011587437
new min fval from sgd:  -1763.3370765635746
new min fval from sgd:  -1763.3378626857564
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00064942]
objective value function right now is: -1763.3256730727376
min fval:  -1763.3378626857564
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -0.2043,   1.3195],
        [  1.1881,   5.4379],
        [  1.2158,   5.6482],
        [  1.3929,   5.8282],
        [ -0.3240,  -5.9316],
        [-10.3321,   6.1151],
        [ -0.1892,   1.3413],
        [  4.2102,   6.4222],
        [  1.2701,   5.6091],
        [  1.5158,   5.8937],
        [  2.9639,   6.2711],
        [  3.8663,   5.9827],
        [ -3.6223,  -6.6383]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-1.4412,  2.8128,  3.2258,  3.4800, -5.3534,  5.9145, -1.4651,  4.2725,
         3.1031,  3.5387,  4.1008,  3.7575, -5.2566], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.0120, -0.0281, -0.0374, -0.0461, -0.3594, -0.0425, -0.0116, -0.2574,
         -0.0346, -0.0494, -0.1489, -0.2171, -0.2218],
        [ 0.0833,  2.3835,  2.8378,  3.0442, -5.1634,  6.2125,  0.1358,  3.0202,
          2.6529,  3.0819,  3.1794,  2.2165, -4.8605],
        [-0.0120, -0.0281, -0.0374, -0.0461, -0.3594, -0.0425, -0.0116, -0.2574,
         -0.0346, -0.0494, -0.1489, -0.2171, -0.2218],
        [ 0.0781, -0.5178, -0.6094, -0.8477,  1.6085, -2.8616,  0.0918, -3.7490,
         -0.6330, -0.9867, -2.4153, -2.8651,  4.0450],
        [ 0.1249,  1.1085,  1.3641,  1.5656, -3.6802,  4.4588,  0.1146,  1.8563,
          1.2866,  1.5969,  1.8499,  1.2321, -3.2356],
        [ 0.0896, -0.6664, -0.7441, -1.0539,  1.8724, -3.2682,  0.1129, -4.2833,
         -0.7981, -1.2236, -2.8431, -3.3079,  4.5935],
        [-0.0120, -0.0281, -0.0374, -0.0461, -0.3594, -0.0425, -0.0116, -0.2574,
         -0.0346, -0.0494, -0.1489, -0.2171, -0.2218],
        [-0.0114, -0.0373, -0.0506, -0.0629, -0.3864, -0.0614, -0.0112, -0.3150,
         -0.0468, -0.0675, -0.1849, -0.2608, -0.2516],
        [ 0.1290, -0.8857, -0.9708, -1.3381,  2.1539, -3.9227,  0.1480, -4.9374,
         -1.0441, -1.5665, -3.3627, -3.9117,  5.5125],
        [-0.0120, -0.0281, -0.0374, -0.0461, -0.3594, -0.0425, -0.0116, -0.2574,
         -0.0346, -0.0494, -0.1489, -0.2171, -0.2218],
        [ 0.1266,  1.1600,  1.4254,  1.6312, -3.7379,  4.5528,  0.1154,  1.8917,
          1.3436,  1.6767,  1.8995,  1.2712, -3.3358],
        [-0.0120, -0.0281, -0.0374, -0.0461, -0.3594, -0.0425, -0.0116, -0.2574,
         -0.0346, -0.0494, -0.1489, -0.2171, -0.2218],
        [-0.0120, -0.0281, -0.0374, -0.0461, -0.3594, -0.0425, -0.0116, -0.2574,
         -0.0346, -0.0494, -0.1489, -0.2171, -0.2218]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.6353, -1.8032, -0.6353,  0.4461, -1.1128,  0.7588, -0.6353, -0.7194,
         1.0969, -0.6354, -1.1181, -0.6353, -0.6353], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 6.1051e-03,  8.7331e+00,  6.1051e-03, -3.9109e+00,  4.1626e+00,
         -4.8908e+00,  6.1051e-03, -5.0038e-03, -6.5176e+00,  6.1052e-03,
          4.3287e+00,  6.1051e-03,  6.1051e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  1.2147,  -9.4601],
        [ -1.4028,   1.4975],
        [ -1.4515,   0.9543],
        [ 12.2204,   5.8840],
        [ -1.4238,   1.3862],
        [ -1.4808,   0.9405],
        [-11.1153,  -6.8939],
        [ 11.2397,  -0.3825],
        [ -8.2059,   5.6873],
        [  9.9230,  -0.4456],
        [ -1.4322,   1.3140],
        [  2.3461,   7.8532],
        [  6.8978,   8.5181]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -8.7281,  -2.4962,  -2.5784,   0.8069,  -2.5046,  -2.5492,  -4.8159,
         -9.8274,   5.8385, -10.3276,  -2.5137,  -5.1456,   6.4722],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.0302e+01, -1.2012e-01,  5.5094e-02, -1.0809e+00, -8.8371e-02,
          5.4346e-02,  6.0228e+00, -6.9158e+00,  2.0180e+00, -3.2897e+00,
         -7.5269e-02, -9.0121e-01,  1.0215e+00],
        [-1.5921e+00, -2.6632e-02, -3.4144e-02, -1.1295e+00, -2.8335e-02,
         -3.5204e-02, -9.9010e-01, -8.5936e-01, -3.5568e-01, -9.3861e-01,
         -2.9289e-02, -1.9824e-01, -1.4662e+00],
        [-1.5203e+00, -2.3585e-02, -4.7607e-02, -1.0823e+00, -2.9482e-02,
         -4.9174e-02, -1.4310e+00, -7.0416e-01, -5.3398e-01, -1.2219e+00,
         -3.2829e-02, -2.9266e-01, -1.7285e+00],
        [-2.1029e+00,  5.6541e-01,  2.6579e-01,  6.3848e-01,  4.9740e-01,
          2.7289e-01,  1.0058e+00, -1.3030e+00,  1.1701e+00, -2.5190e+00,
          4.5519e-01, -7.8350e-01,  2.0753e+00],
        [ 3.4983e+00,  1.9931e-01,  2.0084e-01, -7.4877e+00,  2.0801e-01,
          1.9548e-01,  4.5580e+00, -1.0900e+01, -2.5882e+00, -8.6612e+00,
          2.0819e-01,  1.4316e-03, -1.2917e+01],
        [ 2.3398e+00,  5.2274e-02,  5.1037e-02, -2.2567e+00,  6.6451e-02,
          5.3674e-02, -1.7702e-01, -1.1210e+00,  2.8221e+00, -3.3459e+00,
          7.5625e-02,  3.8142e-01,  2.0693e+00],
        [-1.5916e+00, -2.6645e-02, -3.4153e-02, -1.1285e+00, -2.8347e-02,
         -3.5214e-02, -9.9037e-01, -8.5906e-01, -3.5537e-01, -9.3846e-01,
         -2.9300e-02, -1.9820e-01, -1.4666e+00],
        [-1.5916e+00, -2.6660e-02, -3.4184e-02, -1.1286e+00, -2.8365e-02,
         -3.5245e-02, -9.9073e-01, -8.5900e-01, -3.5554e-01, -9.3878e-01,
         -2.9320e-02, -1.9828e-01, -1.4667e+00],
        [-1.6104e+00, -1.8677e-02, -2.0875e-02, -1.1619e+00, -1.9262e-02,
         -2.1522e-02, -8.2743e-01, -8.7656e-01, -2.9805e-01, -7.9699e-01,
         -1.9537e-02, -1.6673e-01, -1.4094e+00],
        [-1.5916e+00, -2.6632e-02, -3.4128e-02, -1.1285e+00, -2.8331e-02,
         -3.5187e-02, -9.9007e-01, -8.5912e-01, -3.5523e-01, -9.3819e-01,
         -2.9282e-02, -1.9813e-01, -1.4665e+00],
        [-1.5916e+00, -2.6599e-02, -3.4066e-02, -1.1285e+00, -2.8292e-02,
         -3.5124e-02, -9.8933e-01, -8.5925e-01, -3.5492e-01, -9.3755e-01,
         -2.9239e-02, -1.9799e-01, -1.4662e+00],
        [-1.5916e+00, -2.6650e-02, -3.4163e-02, -1.1285e+00, -2.8353e-02,
         -3.5224e-02, -9.9049e-01, -8.5903e-01, -3.5543e-01, -9.3856e-01,
         -2.9306e-02, -1.9823e-01, -1.4666e+00],
        [ 6.5257e+00,  4.5493e-01,  1.5229e-01, -1.7622e+00,  3.7636e-01,
          1.3225e-01,  2.8885e+00,  5.9301e+00, -7.5823e+00,  3.9270e+00,
          3.2754e-01,  4.5331e+00, -2.9094e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.2396, -2.5246, -2.4361, -1.3233, -1.0883, -2.6763, -2.5256, -2.5255,
        -2.5030, -2.5256, -2.5257, -2.5256,  0.2516], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 6.4396e+00, -1.7232e-01, -2.5496e-01,  1.2671e+00, -4.5570e-01,
          3.1982e+00, -1.7220e-01, -1.7229e-01, -1.4435e-01, -1.7213e-01,
         -1.7196e-01, -1.7223e-01, -1.2546e+01],
        [-2.9217e+00, -7.4123e-02, -4.6781e-01,  1.5677e+00, -6.6653e+00,
         -2.7035e-01, -7.4701e-02, -7.4824e-02,  8.7071e-03, -7.4576e-02,
         -7.4198e-02, -7.4707e-02,  6.6117e-01],
        [-2.7051e-01, -5.3000e-02, -4.7134e-02, -9.6146e+00, -2.9605e-02,
         -1.9862e+00, -5.2985e-02, -5.2979e-02, -5.7235e-02, -5.2991e-02,
         -5.3006e-02, -5.2983e-02, -8.7326e+00],
        [-2.7413e-01, -5.5631e-02, -4.9200e-02, -9.5220e+00, -1.3950e-02,
         -2.1090e+00, -5.5615e-02, -5.5608e-02, -6.0212e-02, -5.5621e-02,
         -5.5637e-02, -5.5613e-02, -8.4396e+00],
        [-1.6208e+00,  2.1054e-01,  6.6962e-01,  1.6163e+00,  6.2292e+00,
          1.3815e+00,  2.1015e-01,  2.1047e-01,  1.1475e-01,  2.0989e-01,
          2.0935e-01,  2.1029e-01,  1.0646e+00]], device='cuda:0'))])
xi:  [0.0007728]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 90.6392179860722
W_T_median: 75.85965975799384
W_T_pctile_5: 0.00078339558524263
W_T_CVAR_5_pct: -130.96198224133127
Average q (qsum/M+1):  57.726786951864916
Optimal xi:  [0.0007728]
Expected(across Rb) median(across samples) p_equity:  0.16055743420862806
obj fun:  tensor(-1763.3379, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 946.50330641104
W_T_median: 707.5512584605915
W_T_pctile_5: -238.22847581189643
W_T_CVAR_5_pct: -367.3344080824147
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1732.9472690558816
Current xi:  [113.01274]
objective value function right now is: -1732.9472690558816
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1746.251134840852
Current xi:  [131.32811]
objective value function right now is: -1746.251134840852
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1751.55009443727
Current xi:  [148.11812]
objective value function right now is: -1751.55009443727
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1752.8460029997304
Current xi:  [163.87247]
objective value function right now is: -1752.8460029997304
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1754.0417119524159
Current xi:  [178.28857]
objective value function right now is: -1754.0417119524159
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.74065]
objective value function right now is: -1754.0161340639931
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1755.723325780691
Current xi:  [202.49948]
objective value function right now is: -1755.723325780691
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1757.4746952602356
Current xi:  [213.78577]
objective value function right now is: -1757.4746952602356
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [224.27399]
objective value function right now is: -1755.519453556173
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1759.0247305132261
Current xi:  [234.18774]
objective value function right now is: -1759.0247305132261
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [242.3394]
objective value function right now is: -1756.9099486788862
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [250.0297]
objective value function right now is: -1757.7197472412972
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [257.15225]
objective value function right now is: -1757.4972151760494
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [264.19058]
objective value function right now is: -1758.8479640644055
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1759.232594916569
Current xi:  [268.93964]
objective value function right now is: -1759.232594916569
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1759.961718300277
Current xi:  [272.8856]
objective value function right now is: -1759.961718300277
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [277.3111]
objective value function right now is: -1758.4888997272435
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1761.864957243466
Current xi:  [281.44803]
objective value function right now is: -1761.864957243466
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [286.934]
objective value function right now is: -1761.4776203169204
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1763.7993573042277
Current xi:  [293.28207]
objective value function right now is: -1763.7993573042277
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1763.9314953796995
Current xi:  [297.75742]
objective value function right now is: -1763.9314953796995
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [302.07336]
objective value function right now is: -1758.7954081764426
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [305.75076]
objective value function right now is: -1762.0498033722272
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1766.6489500144019
Current xi:  [311.3628]
objective value function right now is: -1766.6489500144019
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [318.26758]
objective value function right now is: -1764.770889990327
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [322.10513]
objective value function right now is: -1762.3171149571058
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [323.3731]
objective value function right now is: -1763.2793078828581
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [325.6007]
objective value function right now is: -1754.812852165268
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [326.10294]
objective value function right now is: -1764.3692602048577
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [327.75525]
objective value function right now is: -1764.3447093676823
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [328.55164]
objective value function right now is: -1766.0994166989667
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [328.92902]
objective value function right now is: -1763.83332239322
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [329.86832]
objective value function right now is: -1763.8446098425213
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [331.11002]
objective value function right now is: -1765.0227400906597
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [332.6017]
objective value function right now is: -1766.5924183574148
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1769.812926326047
Current xi:  [334.04858]
objective value function right now is: -1769.812926326047
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [335.6563]
objective value function right now is: -1769.5615666067908
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1770.0686057663736
Current xi:  [337.26035]
objective value function right now is: -1770.0686057663736
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1770.422204269236
Current xi:  [339.19598]
objective value function right now is: -1770.422204269236
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [341.0537]
objective value function right now is: -1770.3792021167749
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1770.670926877491
Current xi:  [342.99176]
objective value function right now is: -1770.670926877491
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1770.7137054637535
Current xi:  [344.9]
objective value function right now is: -1770.7137054637535
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1770.911353743181
Current xi:  [346.55756]
objective value function right now is: -1770.911353743181
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [348.30048]
objective value function right now is: -1770.753347354108
new min fval from sgd:  -1771.0693209832316
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [350.17307]
objective value function right now is: -1771.0693209832316
new min fval from sgd:  -1771.1504138530959
new min fval from sgd:  -1771.15606088258
new min fval from sgd:  -1771.1958512885487
new min fval from sgd:  -1771.2353555611992
new min fval from sgd:  -1771.24392164047
new min fval from sgd:  -1771.2532007654174
new min fval from sgd:  -1771.2566389334263
new min fval from sgd:  -1771.2672106799018
new min fval from sgd:  -1771.270026052854
new min fval from sgd:  -1771.276716282867
new min fval from sgd:  -1771.2847764265762
new min fval from sgd:  -1771.2863377903225
new min fval from sgd:  -1771.2918651918503
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.8125]
objective value function right now is: -1770.1489577830444
new min fval from sgd:  -1771.3261558809468
new min fval from sgd:  -1771.3294872098475
new min fval from sgd:  -1771.3522698323923
new min fval from sgd:  -1771.3589289797192
new min fval from sgd:  -1771.4017156158286
new min fval from sgd:  -1771.4029540560828
new min fval from sgd:  -1771.406893422139
new min fval from sgd:  -1771.4267906065372
new min fval from sgd:  -1771.4326483236466
new min fval from sgd:  -1771.4463414336267
new min fval from sgd:  -1771.4539767602478
new min fval from sgd:  -1771.4637864638182
new min fval from sgd:  -1771.4642819063467
new min fval from sgd:  -1771.4806367740491
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.52695]
objective value function right now is: -1771.0673027098603
new min fval from sgd:  -1771.5145335816653
new min fval from sgd:  -1771.5239731126505
new min fval from sgd:  -1771.5562039359374
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.01752]
objective value function right now is: -1771.1391531689046
new min fval from sgd:  -1771.5586861230558
new min fval from sgd:  -1771.5595027021325
new min fval from sgd:  -1771.563948901772
new min fval from sgd:  -1771.5675685302108
new min fval from sgd:  -1771.5697113440353
new min fval from sgd:  -1771.572613540336
new min fval from sgd:  -1771.5736897551244
new min fval from sgd:  -1771.5746393096251
new min fval from sgd:  -1771.5746959138717
new min fval from sgd:  -1771.5747744341663
new min fval from sgd:  -1771.5753304503085
new min fval from sgd:  -1771.5802575127716
new min fval from sgd:  -1771.5839656751677
new min fval from sgd:  -1771.5857075178446
new min fval from sgd:  -1771.5885376074652
new min fval from sgd:  -1771.5888691372006
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.98517]
objective value function right now is: -1771.5586462156716
new min fval from sgd:  -1771.5913711463909
new min fval from sgd:  -1771.6008325124142
new min fval from sgd:  -1771.6140728797693
new min fval from sgd:  -1771.6153190925163
new min fval from sgd:  -1771.6167413045782
new min fval from sgd:  -1771.6180922243436
new min fval from sgd:  -1771.6186481846642
new min fval from sgd:  -1771.6197746499317
new min fval from sgd:  -1771.6211167658284
new min fval from sgd:  -1771.621207615736
new min fval from sgd:  -1771.6213160766397
new min fval from sgd:  -1771.6269465784633
new min fval from sgd:  -1771.6306601321585
new min fval from sgd:  -1771.6349579312937
new min fval from sgd:  -1771.635041455815
new min fval from sgd:  -1771.6362159362234
new min fval from sgd:  -1771.6374601226119
new min fval from sgd:  -1771.6382945121636
new min fval from sgd:  -1771.6436546965015
new min fval from sgd:  -1771.6465746968872
new min fval from sgd:  -1771.6509220321188
new min fval from sgd:  -1771.6531235392038
new min fval from sgd:  -1771.6546402300094
new min fval from sgd:  -1771.6554566004124
new min fval from sgd:  -1771.6565091287205
new min fval from sgd:  -1771.6569519545744
new min fval from sgd:  -1771.657339691945
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [356.31006]
objective value function right now is: -1771.6485835823405
min fval:  -1771.657339691945
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -3.8012,  -7.2977],
        [ -2.3606,   7.1722],
        [ -2.9715,   7.3044],
        [ -2.5110,   7.1897],
        [ -2.7309,   7.2732],
        [-27.7578,  -6.9315],
        [ -8.5151,  -8.7285],
        [ -3.9910,  -7.7905],
        [ -2.7764,   7.2643],
        [ -2.4423,   7.0697],
        [ -2.6632,   7.2479],
        [  9.9536,  -2.2661],
        [ -3.5169,   6.8521]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-4.4832,  2.3032,  3.2074,  2.5252,  2.8515, -3.5104, -2.4217, -4.3266,
         2.9098,  2.2426,  2.7443, -9.9490,  1.8093], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 7.5801e-02, -6.4772e-01, -8.0810e-02, -4.6010e-01, -2.4080e-01,
          2.5387e-02, -3.0291e+00,  4.0222e-01, -1.9180e-01, -7.8091e-01,
         -3.0749e-01,  1.4308e+01, -1.3990e+00],
        [-1.0113e-01, -2.4640e-01, -3.1662e-01, -2.6250e-01, -2.8670e-01,
         -7.5784e-03, -2.7760e-01, -1.4589e-01, -2.9140e-01, -2.3086e-01,
         -2.7791e-01, -3.8427e-01, -7.4045e-02],
        [ 6.2360e+00, -2.8041e+00, -3.8196e+00, -3.0420e+00, -3.4352e+00,
          2.3579e+00,  6.7569e+00,  7.0156e+00, -3.4753e+00, -2.6567e+00,
         -3.2459e+00, -8.4580e+00, -2.3457e+00],
        [-9.8337e-02, -2.4458e-01, -3.1573e-01, -2.6098e-01, -2.8540e-01,
         -1.9077e-03, -2.7413e-01, -1.4272e-01, -2.9017e-01, -2.2894e-01,
         -2.7654e-01, -3.8476e-01, -7.2949e-02],
        [ 1.6133e+00,  2.6069e-01,  1.6502e-01,  3.7050e-01,  3.5685e-01,
          4.0706e+00,  1.0975e+00,  2.0352e+00,  3.1684e-01,  1.7694e-01,
          3.7103e-01,  1.1870e+01, -6.7980e-01],
        [-2.1975e-01,  1.1158e+00,  7.8148e-01,  1.0132e+00,  8.8449e-01,
         -1.6639e-01, -8.1528e-01, -1.7772e-01,  8.5046e-01,  1.0560e+00,
          9.2445e-01,  9.0798e+00,  3.9837e-02],
        [-1.0146e-01, -2.4669e-01, -3.1694e-01, -2.6279e-01, -2.8701e-01,
         -7.9548e-03, -2.7820e-01, -1.4631e-01, -2.9171e-01, -2.3116e-01,
         -2.7821e-01, -3.8465e-01, -7.4190e-02],
        [-1.0145e-01, -2.4663e-01, -3.1682e-01, -2.6272e-01, -2.8692e-01,
         -8.1208e-03, -2.7801e-01, -1.4625e-01, -2.9161e-01, -2.3111e-01,
         -2.7813e-01, -3.8434e-01, -7.4179e-02],
        [-9.8336e-02, -2.4458e-01, -3.1573e-01, -2.6098e-01, -2.8540e-01,
         -1.9079e-03, -2.7413e-01, -1.4272e-01, -2.9016e-01, -2.2894e-01,
         -2.7653e-01, -3.8475e-01, -7.2949e-02],
        [ 1.6685e+00, -3.7421e+00, -5.1642e+00, -4.2081e+00, -4.6808e+00,
         -1.6754e+01,  9.4470e+00,  3.1556e+00, -4.8025e+00, -3.6632e+00,
         -4.5689e+00,  1.9280e+00, -3.0268e+00],
        [-3.9653e+00,  2.7169e+00,  4.1809e+00,  3.2389e+00,  3.6695e+00,
         -6.6062e+00, -2.7695e+00, -4.2528e+00,  3.8484e+00,  2.7621e+00,
          3.4973e+00, -8.3817e+00,  2.3212e+00],
        [-4.1327e+00,  2.4269e+00,  3.9084e+00,  2.7154e+00,  3.2451e+00,
         -6.2220e+00, -3.5354e+00, -4.5450e+00,  3.2397e+00,  2.3259e+00,
          3.0112e+00, -8.2172e+00,  1.9028e+00],
        [-9.8340e-02, -2.4459e-01, -3.1574e-01, -2.6098e-01, -2.8541e-01,
         -1.9067e-03, -2.7413e-01, -1.4273e-01, -2.9017e-01, -2.2894e-01,
         -2.7654e-01, -3.8477e-01, -7.2951e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-3.7068, -1.5200, -3.1802, -1.4812, -5.5818, -6.5312, -1.5205, -1.5227,
        -1.4812,  0.1562,  3.7291,  3.6441, -1.4811], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -9.6047,   0.0607,  -8.6562,   0.0529,  -8.8901,  -6.6815,   0.0613,
           0.0615,   0.0529, -11.2933,   8.0044,   7.2756,   0.0529]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 10.0432,  -0.9437],
        [ 11.2609,   7.6379],
        [ -0.7457, -10.2490],
        [ -1.5563,   0.2215],
        [ 11.2578,  -0.2174],
        [ -1.7291,   0.4906],
        [ -1.3229,   0.1528],
        [ -1.7099,   0.7568],
        [-14.0470,   4.2873],
        [  5.4521,  -4.7341],
        [-15.0569,  -6.3296],
        [ -3.6665,   4.9236],
        [  7.9759,  -1.0845]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-10.5766,   3.6322,  -5.6268,  -3.6363, -10.2439,  -3.8375,  -3.8259,
         -3.9205,   3.8688,  -4.0697,  -3.0514,  -8.1073, -10.6503],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-7.1338e+00, -8.5923e+00,  4.1995e+00,  1.3799e-02, -1.0064e+01,
         -1.0517e-02,  7.2728e-02,  1.1527e-02, -4.4243e+00,  1.0864e+00,
          7.7307e+00,  2.3029e-03, -4.3782e+00],
        [-2.9423e-01, -2.6411e+00, -6.7814e-01,  2.9420e-02, -8.8143e-01,
          1.9314e-02,  2.2492e-02, -3.2795e-02,  1.7235e-01, -1.3911e+00,
          6.5741e-01, -5.4632e-01, -7.7763e-02],
        [ 8.7985e+00, -3.5457e+00,  8.4087e+00,  1.8921e-02,  8.2591e+00,
          2.3808e-01, -6.6634e-02,  3.8177e-01, -6.3381e+00,  3.6986e+00,
         -2.0422e+00,  8.6297e+00,  2.0541e+00],
        [-2.4272e+00, -3.9002e+00,  4.6572e-01,  5.2757e-02, -2.9213e+00,
          6.4887e-02, -6.1464e-02,  1.1367e-01, -5.1407e+00,  6.6853e-01,
         -8.1930e-01,  1.2590e-01, -2.2533e+00],
        [-2.9471e-01, -2.6395e+00, -6.8032e-01,  2.8990e-02, -8.8011e-01,
          1.9127e-02,  2.2126e-02, -3.2470e-02,  1.6774e-01, -1.3906e+00,
          6.5146e-01, -5.4591e-01, -7.8060e-02],
        [-2.9197e-01, -2.6465e+00, -6.6946e-01,  3.0913e-02, -8.8675e-01,
          1.9907e-02,  2.3773e-02, -3.4133e-02,  1.8815e-01, -1.3932e+00,
          6.7833e-01, -5.4558e-01, -7.6330e-02],
        [ 5.9211e-01, -1.9734e+00, -1.4785e+00, -1.5010e-01,  1.1821e+00,
         -1.2000e-01, -2.0192e-01, -2.5620e-01, -1.8941e+00, -1.0321e+00,
          2.8047e+00, -1.2745e+00,  1.6979e+00],
        [-2.9477e-01, -2.6394e+00, -6.8055e-01,  2.8948e-02, -8.7997e-01,
          1.9109e-02,  2.2090e-02, -3.2438e-02,  1.6729e-01, -1.3906e+00,
          6.5087e-01, -5.4589e-01, -7.8094e-02],
        [-2.9620e+00,  5.4077e-01, -2.9025e+00,  3.7583e-01, -4.6810e-02,
         -4.7426e-02,  7.4659e-01, -5.6114e-01,  2.0409e+00,  2.9139e-01,
         -2.2882e+00,  1.9479e-01, -2.5800e+00],
        [-5.3727e+00,  1.5252e+00, -8.5859e+00, -1.3493e-03, -1.2403e+01,
         -2.8376e-01,  1.2649e-01, -6.2409e-01,  4.8773e+00, -5.0682e+00,
          5.0606e+00, -4.2617e+00, -2.0551e-01],
        [-2.9229e-01, -2.6458e+00, -6.7063e-01,  3.0725e-02, -8.8602e-01,
          1.9837e-02,  2.3612e-02, -3.3947e-02,  1.8619e-01, -1.3929e+00,
          6.7569e-01, -5.4586e-01, -7.6544e-02],
        [-1.7474e+00, -3.3964e+00, -5.6811e-01, -1.0474e-01, -1.9273e+00,
         -6.2015e-02, -6.9977e-02, -1.2429e-01, -2.2470e+00,  4.4275e-01,
         -2.0472e+00, -4.9741e-01, -1.0861e+00],
        [-2.9453e-01, -2.6401e+00, -6.7948e-01,  2.9156e-02, -8.8062e-01,
          1.9200e-02,  2.2268e-02, -3.2595e-02,  1.6953e-01, -1.3908e+00,
          6.5377e-01, -5.4608e-01, -7.7946e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.9617, -2.7104, -2.7405, -1.2034, -2.7111, -2.7083, -3.6940, -2.7112,
         0.7858, -0.4300, -2.7086, -2.3907, -2.7108], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -7.2783,   0.7754, -12.2472,  -0.6336,   0.7726,   0.7874,   0.5808,
           0.7723,   1.2798,   4.3665,   0.7857,   0.2665,   0.7737],
        [ -4.2357,   0.0126,   0.8560,  -3.2470,   0.0126,   0.0173,  -0.5473,
           0.0125,   1.5742,  -0.0485,   0.0163,  -1.9197,   0.0126],
        [ -0.8943,  -0.1459,  -8.8254,  -0.1510,  -0.1451,  -0.1486,  -0.0516,
          -0.1450, -10.2176,  -2.2413,  -0.1483,  -0.0460,  -0.1454],
        [ -0.9116,  -0.1677,  -8.4980,  -0.1605,  -0.1667,  -0.1712,  -0.0493,
          -0.1666, -10.2022,  -2.4705,  -0.1708,  -0.0478,  -0.1671],
        [  7.7182,  -0.1148,   1.1363,   5.0414,  -0.1135,  -0.1145,   1.3520,
          -0.1135,   1.7542,   0.7629,  -0.1150,   3.2940,  -0.1140]],
       device='cuda:0'))])
xi:  [356.2053]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 519.9779452737943
W_T_median: 494.6918842216224
W_T_pctile_5: 356.3045928193685
W_T_CVAR_5_pct: 151.85958538439755
Average q (qsum/M+1):  54.700943485383064
Optimal xi:  [356.2053]
Expected(across Rb) median(across samples) p_equity:  0.20394850376372536
obj fun:  tensor(-1771.6573, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 946.50330641104
W_T_median: 707.5512584605915
W_T_pctile_5: -238.22847581189643
W_T_CVAR_5_pct: -367.3344080824147
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1738.99676825437
Current xi:  [118.63456]
objective value function right now is: -1738.99676825437
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1763.4287738273315
Current xi:  [140.10677]
objective value function right now is: -1763.4287738273315
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1778.493858474476
Current xi:  [161.53506]
objective value function right now is: -1778.493858474476
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1796.7511389711754
Current xi:  [182.90065]
objective value function right now is: -1796.7511389711754
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1807.3658660833541
Current xi:  [204.69708]
objective value function right now is: -1807.3658660833541
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1822.9991241422645
Current xi:  [229.72435]
objective value function right now is: -1822.9991241422645
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1828.6193955467088
Current xi:  [251.28743]
objective value function right now is: -1828.6193955467088
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1838.3514448863734
Current xi:  [272.03323]
objective value function right now is: -1838.3514448863734
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1845.9913866552795
Current xi:  [292.0479]
objective value function right now is: -1845.9913866552795
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1855.1228755595316
Current xi:  [311.7086]
objective value function right now is: -1855.1228755595316
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1861.4456451222652
Current xi:  [331.3318]
objective value function right now is: -1861.4456451222652
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1865.252022859762
Current xi:  [350.17572]
objective value function right now is: -1865.252022859762
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1872.028788162663
Current xi:  [367.9684]
objective value function right now is: -1872.028788162663
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1878.6282270551326
Current xi:  [385.40762]
objective value function right now is: -1878.6282270551326
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [403.04996]
objective value function right now is: -1874.8024809286908
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [419.3523]
objective value function right now is: -1872.1128639584247
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1890.0332014488015
Current xi:  [435.01392]
objective value function right now is: -1890.0332014488015
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [449.89978]
objective value function right now is: -1879.9175188005863
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1890.5701460445666
Current xi:  [464.73633]
objective value function right now is: -1890.5701460445666
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1899.1957952926534
Current xi:  [478.4974]
objective value function right now is: -1899.1957952926534
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [491.99478]
objective value function right now is: -1888.1310328674583
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1903.525374454843
Current xi:  [504.0895]
objective value function right now is: -1903.525374454843
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [515.7385]
objective value function right now is: -1891.9316974018172
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1904.1005797333844
Current xi:  [525.61536]
objective value function right now is: -1904.1005797333844
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1909.7480374227287
Current xi:  [535.60364]
objective value function right now is: -1909.7480374227287
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [543.7909]
objective value function right now is: -1906.0546351871706
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [551.9368]
objective value function right now is: -1905.334460631481
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [558.1506]
objective value function right now is: -1901.7525222636368
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [563.19977]
objective value function right now is: -1908.6600917016476
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1913.8384375717733
Current xi:  [569.5731]
objective value function right now is: -1913.8384375717733
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [573.91943]
objective value function right now is: -1908.259424378229
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [577.851]
objective value function right now is: -1905.7742947820896
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [582.2483]
objective value function right now is: -1910.5485602827303
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [586.14056]
objective value function right now is: -1905.9301128493578
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [589.1233]
objective value function right now is: -1907.16823147455
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1915.5612657907393
Current xi:  [589.8524]
objective value function right now is: -1915.5612657907393
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1916.0595498691096
Current xi:  [590.6635]
objective value function right now is: -1916.0595498691096
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [591.18243]
objective value function right now is: -1915.4853166461264
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [592.1058]
objective value function right now is: -1916.0040651434006
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1916.1064420530824
Current xi:  [593.12164]
objective value function right now is: -1916.1064420530824
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [593.8131]
objective value function right now is: -1915.3311370430151
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [595.061]
objective value function right now is: -1915.8243361387701
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [596.11017]
objective value function right now is: -1914.6180522429715
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [596.7322]
objective value function right now is: -1915.90330330419
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [597.50146]
objective value function right now is: -1911.5148290667228
new min fval from sgd:  -1916.1769072473442
new min fval from sgd:  -1916.3164302171963
new min fval from sgd:  -1916.3174312889334
new min fval from sgd:  -1916.322150857894
new min fval from sgd:  -1916.4067161661237
new min fval from sgd:  -1916.4480489928987
new min fval from sgd:  -1916.4955260215334
new min fval from sgd:  -1916.5455572934243
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [598.1326]
objective value function right now is: -1915.8867022079537
new min fval from sgd:  -1916.571039617167
new min fval from sgd:  -1916.585775584539
new min fval from sgd:  -1916.594025634945
new min fval from sgd:  -1916.6408613422202
new min fval from sgd:  -1916.707487460392
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [598.4332]
objective value function right now is: -1914.4359674604484
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [599.4921]
objective value function right now is: -1915.5940131414711
new min fval from sgd:  -1916.709375406276
new min fval from sgd:  -1916.715018067102
new min fval from sgd:  -1916.7222102534122
new min fval from sgd:  -1916.7268407223896
new min fval from sgd:  -1916.7304019896696
new min fval from sgd:  -1916.7325516432645
new min fval from sgd:  -1916.7344337780382
new min fval from sgd:  -1916.7471174539187
new min fval from sgd:  -1916.7595413689783
new min fval from sgd:  -1916.7661639581142
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [599.8204]
objective value function right now is: -1916.613678188132
new min fval from sgd:  -1916.7690911735729
new min fval from sgd:  -1916.770990600937
new min fval from sgd:  -1916.7724816224165
new min fval from sgd:  -1916.7774739867332
new min fval from sgd:  -1916.778559068101
new min fval from sgd:  -1916.7801763572334
new min fval from sgd:  -1916.7820509336254
new min fval from sgd:  -1916.7829629393889
new min fval from sgd:  -1916.7849084613952
new min fval from sgd:  -1916.79075550333
new min fval from sgd:  -1916.7911213570992
new min fval from sgd:  -1916.8038472942576
new min fval from sgd:  -1916.809323376676
new min fval from sgd:  -1916.8102586363434
new min fval from sgd:  -1916.8116108901452
new min fval from sgd:  -1916.8123399151084
new min fval from sgd:  -1916.8129148588882
new min fval from sgd:  -1916.8136206416618
new min fval from sgd:  -1916.825964344989
new min fval from sgd:  -1916.8264399921422
new min fval from sgd:  -1916.83282606901
new min fval from sgd:  -1916.8415624356312
new min fval from sgd:  -1916.8472567913777
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [600.00244]
objective value function right now is: -1916.6926335919313
min fval:  -1916.8472567913777
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 12.2183,  -1.0046],
        [-23.8274, -12.0814],
        [ -1.1918,   0.0547],
        [ -1.1909,   0.0542],
        [ -1.1918,   0.0547],
        [ -5.9744, -12.5412],
        [ -3.7587,  11.1208],
        [ -1.1918,   0.0547],
        [-19.9120,   6.7577],
        [ -1.1918,   0.0547],
        [-11.2001,   4.6390],
        [ -1.1918,   0.0547],
        [ -1.1918,   0.0547]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-10.6712,  -3.8268,  -3.0881,  -3.0869,  -3.0881,  -1.8783,   2.3098,
         -3.0881,   3.9609,  -3.0881,   0.8609,  -3.0881,  -3.0882],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-7.4769e+00,  4.2819e-03, -8.7627e-02, -8.6414e-02, -8.7631e-02,
         -1.1745e+01, -1.1518e-01, -8.7622e-02, -5.1411e-01, -8.7629e-02,
         -1.9461e+00, -8.7626e-02, -8.7681e-02],
        [ 1.4854e+01,  8.2277e+00, -1.3007e-01, -1.2437e-01, -1.3009e-01,
          6.8216e+00, -4.5209e+00, -1.3004e-01, -5.4447e+00, -1.3009e-01,
         -1.6158e+00, -1.3006e-01, -1.3035e-01],
        [-2.5211e-01, -1.7279e-02, -6.6487e-03, -6.6653e-03, -6.6487e-03,
         -3.0614e-01, -6.1513e-01, -6.6487e-03, -8.1939e-02, -6.6487e-03,
         -3.8596e-02, -6.6487e-03, -6.6482e-03],
        [-2.5211e-01, -1.7278e-02, -6.6487e-03, -6.6653e-03, -6.6487e-03,
         -3.0614e-01, -6.1513e-01, -6.6487e-03, -8.1939e-02, -6.6487e-03,
         -3.8596e-02, -6.6487e-03, -6.6482e-03],
        [ 1.3854e+01,  7.5629e+00, -7.6041e-02, -7.5505e-02, -7.6035e-02,
          6.8077e+00, -4.4027e+00, -7.6047e-02, -5.4098e+00, -7.6037e-02,
         -1.4601e+00, -7.6042e-02, -7.5990e-02],
        [-1.3458e+01, -5.6117e+00, -3.3803e-02, -3.2063e-02, -3.3807e-02,
         -1.0801e+01,  4.3827e+00, -3.3798e-02,  6.0944e+00, -3.3806e-02,
          1.6878e+00, -3.3802e-02, -3.3858e-02],
        [-2.5211e-01, -1.7278e-02, -6.6487e-03, -6.6653e-03, -6.6487e-03,
         -3.0614e-01, -6.1513e-01, -6.6487e-03, -8.1939e-02, -6.6487e-03,
         -3.8596e-02, -6.6487e-03, -6.6482e-03],
        [-2.5211e-01, -1.7278e-02, -6.6487e-03, -6.6653e-03, -6.6487e-03,
         -3.0614e-01, -6.1513e-01, -6.6487e-03, -8.1939e-02, -6.6487e-03,
         -3.8596e-02, -6.6487e-03, -6.6482e-03],
        [-2.5211e-01, -1.7278e-02, -6.6487e-03, -6.6652e-03, -6.6486e-03,
         -3.0614e-01, -6.1513e-01, -6.6487e-03, -8.1939e-02, -6.6487e-03,
         -3.8596e-02, -6.6487e-03, -6.6482e-03],
        [-2.5211e-01, -1.7279e-02, -6.6487e-03, -6.6653e-03, -6.6487e-03,
         -3.0614e-01, -6.1513e-01, -6.6488e-03, -8.1939e-02, -6.6487e-03,
         -3.8596e-02, -6.6487e-03, -6.6482e-03],
        [ 4.4568e-01, -1.5292e+01,  9.4492e-02,  9.7662e-02,  9.4484e-02,
          6.9497e+00, -7.7324e-01,  9.4500e-02, -1.1082e+01,  9.4486e-02,
         -5.5618e+00,  9.4493e-02,  9.4390e-02],
        [ 1.2822e+01,  7.2135e+00, -3.7742e-02, -3.8192e-02, -3.7736e-02,
          6.8476e+00, -4.2788e+00, -3.7748e-02, -5.4557e+00, -3.7738e-02,
         -1.4904e+00, -3.7743e-02, -3.7679e-02],
        [-2.5211e-01, -1.7278e-02, -6.6487e-03, -6.6653e-03, -6.6487e-03,
         -3.0614e-01, -6.1513e-01, -6.6487e-03, -8.1939e-02, -6.6487e-03,
         -3.8596e-02, -6.6487e-03, -6.6482e-03]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 2.6231, -3.6689, -1.7101, -1.7101, -3.5868,  2.4147, -1.7101, -1.7101,
        -1.7101, -1.7101,  6.4571, -3.5231, -1.7101], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 7.3683, -7.6111,  0.0479,  0.0479, -6.5409, 17.5487,  0.0479,  0.0479,
          0.0479,  0.0479, -7.5788, -5.9340,  0.0479]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  6.9060, -13.1393],
        [  6.5569,   7.4352],
        [ 10.8370,   3.1342],
        [  4.6287,   8.2918],
        [-12.3089,  -3.7201],
        [-12.9486,  -3.9281],
        [ -1.3979,   1.5832],
        [ -9.6740,  -3.1890],
        [  2.3974,   1.3759],
        [ 13.7493,   0.2184],
        [ -9.8710,   6.8881],
        [ -1.4980,   1.5649],
        [ 11.5294,   3.5855]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -6.0787,   4.8552,  -2.7106,   0.7299,  -2.0636,  -1.9967,  -4.7390,
         -3.9057,  -7.9278, -11.7126,   3.1082,  -4.3530,  -1.7238],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.5835e+00,  1.8219e+00,  1.5848e+00,  6.5152e-01,  4.2584e-02,
          4.0530e-02, -1.0365e+00, -4.3691e-02, -1.1745e+00,  2.7027e-01,
          3.7579e-01, -6.9279e-01,  1.6918e+00],
        [-9.0503e-02, -2.4694e+00, -2.1141e+00, -4.5862e-01,  8.6515e-01,
          9.2735e-01, -2.2800e-01,  2.1796e-01, -2.0569e-01, -2.4456e+00,
          1.4580e+00, -2.3988e-01, -2.2155e+00],
        [-3.1614e+00, -1.3319e+00, -5.5159e-01,  5.9841e-01,  1.0496e+00,
          1.1100e+00, -1.9071e+00,  1.9319e-01, -2.0276e+00,  5.7285e-01,
          1.2198e-01, -1.4772e+00, -9.7850e-01],
        [-7.4473e+00,  1.2093e+01,  4.5157e+00,  5.9305e+00, -5.7380e+00,
         -5.0542e+00,  2.9444e-01, -2.3661e+00,  4.3936e-03,  1.3828e+01,
         -3.2799e+00,  3.0955e-01,  4.6879e+00],
        [ 2.0831e+00, -2.0666e+00, -9.4523e-01, -1.2460e+01,  6.9661e+00,
          8.8951e+00,  5.2429e-02,  3.8686e+00, -1.8138e-01, -2.6600e+00,
         -7.9105e+00,  6.5349e-02, -1.1281e+00],
        [ 8.9788e+00,  2.9741e+00,  5.4133e+00,  3.8404e+00, -5.7712e+00,
         -6.5655e+00,  1.0683e-01, -1.5875e+00,  2.6670e-02,  1.4100e-03,
         -4.1878e+00,  1.3283e-01,  7.2853e+00],
        [-7.2816e+00,  1.3473e+00, -2.5587e-01,  1.5604e+00,  4.3643e+00,
          4.6484e+00, -2.3821e-01,  1.4273e+00, -3.1800e-01, -1.5107e+01,
          3.8307e+00, -2.7448e-01, -1.9318e+00],
        [-3.1601e+00, -1.3327e+00, -5.5237e-01,  5.9749e-01,  1.0485e+00,
          1.1088e+00, -1.9059e+00,  1.9267e-01, -2.0260e+00,  5.7191e-01,
          1.2111e-01, -1.4767e+00, -9.7938e-01],
        [ 6.8244e-01, -3.3595e+01, -5.1693e+00, -2.8374e-02,  2.1202e+00,
          2.4812e+00, -1.6544e-01,  2.8516e+00, -1.0808e-02, -7.9209e+00,
          2.0815e-01, -2.6191e-01, -7.0965e+00],
        [-3.1504e+00, -1.3389e+00, -5.5817e-01,  5.9085e-01,  1.0403e+00,
          1.1007e+00, -1.8977e+00,  1.8895e-01, -2.0143e+00,  5.6513e-01,
          1.1501e-01, -1.4729e+00, -9.8584e-01],
        [ 3.4269e+00, -2.8349e+00, -1.9392e-01,  1.2434e+00,  5.0546e+00,
          7.2169e+00,  1.3041e-02,  3.5124e+00,  9.7944e-01,  6.8438e+00,
         -3.8644e+00, -2.7882e-01,  2.9933e-01],
        [ 1.6145e+00,  1.8551e+00,  1.6008e+00,  6.6210e-01,  5.5826e-02,
          5.4792e-02, -1.0251e+00, -3.7516e-02, -1.1552e+00,  2.7404e-01,
          3.9900e-01, -6.8915e-01,  1.7099e+00],
        [-3.1684e+00, -1.3277e+00, -5.4732e-01,  6.0331e-01,  1.0544e+00,
          1.1146e+00, -1.9127e+00,  1.9553e-01, -2.0353e+00,  5.7762e-01,
          1.2516e-01, -1.4798e+00, -9.7366e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 3.2017, -1.6666, -2.5139, -1.4171, -0.5566,  1.8048, -2.7823, -2.5146,
        -1.0899, -2.5195, -2.7297,  3.2397, -2.5103], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-6.6453e-01,  2.1610e+00,  1.3901e+00,  5.5678e+00, -1.7378e+01,
         -2.0654e+00,  2.8692e+00,  1.3897e+00,  1.1524e-09,  1.3875e+00,
         -3.8119e+01, -7.9954e-01,  1.3923e+00],
        [-3.4266e-01, -1.5739e+00, -1.3518e+00,  3.1535e+00, -1.5064e+00,
         -5.5120e-01, -1.6127e-01, -1.3511e+00,  1.5022e+01, -1.3447e+00,
          5.5519e-01, -5.1008e-01, -1.3542e+00],
        [-2.8194e+00, -7.8775e-03, -4.9443e-03, -2.7107e+00, -2.4081e-01,
         -2.8180e+00, -5.3132e-02, -4.9275e-03, -1.7333e-03, -4.8085e-03,
         -9.1884e-01, -2.8195e+00, -5.0330e-03],
        [-2.7510e+00, -4.3007e-03, -7.0306e-03, -2.6887e+00, -2.0580e-01,
         -2.7481e+00, -7.1591e-02, -7.0050e-03, -3.9829e-03, -6.8183e-03,
         -8.1437e-01, -2.7511e+00, -7.1631e-03],
        [ 1.6098e+00,  4.3462e-01,  4.9823e-01, -3.0751e+00,  1.6901e+00,
          1.8956e+00,  2.4249e-01,  4.9735e-01, -1.5052e+01,  4.9191e-01,
          5.2908e-01,  1.5787e+00,  5.0376e-01]], device='cuda:0'))])
xi:  [599.9581]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 840.8741458467044
W_T_median: 792.4569038962094
W_T_pctile_5: 599.9437552448857
W_T_CVAR_5_pct: 304.69243769552884
Average q (qsum/M+1):  52.00497338079637
Optimal xi:  [599.9581]
Expected(across Rb) median(across samples) p_equity:  0.22543967130283515
obj fun:  tensor(-1916.8473, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 946.50330641104
W_T_median: 707.5512584605915
W_T_pctile_5: -238.22847581189643
W_T_CVAR_5_pct: -367.3344080824147
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1750.5591198681893
Current xi:  [118.354836]
objective value function right now is: -1750.5591198681893
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1779.0092045320907
Current xi:  [139.41104]
objective value function right now is: -1779.0092045320907
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1799.6576992282762
Current xi:  [161.67355]
objective value function right now is: -1799.6576992282762
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1847.6736648052856
Current xi:  [185.94606]
objective value function right now is: -1847.6736648052856
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1867.1532787458605
Current xi:  [210.10732]
objective value function right now is: -1867.1532787458605
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1896.8380804525734
Current xi:  [232.54482]
objective value function right now is: -1896.8380804525734
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1914.103920690748
Current xi:  [254.03429]
objective value function right now is: -1914.103920690748
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1929.6350107962205
Current xi:  [275.10098]
objective value function right now is: -1929.6350107962205
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1942.372899093512
Current xi:  [295.605]
objective value function right now is: -1942.372899093512
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1950.0910640088644
Current xi:  [315.28842]
objective value function right now is: -1950.0910640088644
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1964.4729174899749
Current xi:  [334.39792]
objective value function right now is: -1964.4729174899749
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1974.187482925066
Current xi:  [353.90057]
objective value function right now is: -1974.187482925066
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1986.899119811498
Current xi:  [372.9677]
objective value function right now is: -1986.899119811498
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1995.516676207209
Current xi:  [392.16388]
objective value function right now is: -1995.516676207209
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -2003.87273111863
Current xi:  [411.22678]
objective value function right now is: -2003.87273111863
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -2013.9226044431382
Current xi:  [429.65082]
objective value function right now is: -2013.9226044431382
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -2024.2007381712065
Current xi:  [446.80237]
objective value function right now is: -2024.2007381712065
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -2026.9595581094818
Current xi:  [464.40405]
objective value function right now is: -2026.9595581094818
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [481.44495]
objective value function right now is: -2019.1571790366515
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2043.8989848076637
Current xi:  [497.86655]
objective value function right now is: -2043.8989848076637
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -2045.9989100309674
Current xi:  [513.1271]
objective value function right now is: -2045.9989100309674
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -2052.2880226857105
Current xi:  [528.9457]
objective value function right now is: -2052.2880226857105
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2053.5762177914294
Current xi:  [543.36957]
objective value function right now is: -2053.5762177914294
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -2064.024068122951
Current xi:  [556.9546]
objective value function right now is: -2064.024068122951
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [570.5761]
objective value function right now is: -2060.171721286971
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [583.0612]
objective value function right now is: -2063.8847070197066
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [593.89325]
objective value function right now is: -2061.163633338429
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -2072.256465263462
Current xi:  [604.3626]
objective value function right now is: -2072.256465263462
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [613.9756]
objective value function right now is: -2066.4665186861666
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [622.3454]
objective value function right now is: -2069.4543784004654
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [629.44476]
objective value function right now is: -2070.9383127690135
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [636.4127]
objective value function right now is: -2070.770942881755
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [641.37463]
objective value function right now is: -2067.6224251166695
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [644.87866]
objective value function right now is: -2064.990565460318
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -2074.998184373638
Current xi:  [649.3131]
objective value function right now is: -2074.998184373638
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2077.900456471638
Current xi:  [650.8212]
objective value function right now is: -2077.900456471638
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [651.88]
objective value function right now is: -2077.4259750086544
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -2079.668685157136
Current xi:  [653.0873]
objective value function right now is: -2079.668685157136
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2080.5054614922196
Current xi:  [653.86206]
objective value function right now is: -2080.5054614922196
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [654.8595]
objective value function right now is: -2078.383237863253
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -2081.125063791602
Current xi:  [656.13214]
objective value function right now is: -2081.125063791602
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [657.1665]
objective value function right now is: -2080.0732805719636
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [658.18054]
objective value function right now is: -2078.5285887508307
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [659.26294]
objective value function right now is: -2079.6432838553355
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [660.667]
objective value function right now is: -2080.690787438424
new min fval from sgd:  -2081.129721402779
new min fval from sgd:  -2081.251849549479
new min fval from sgd:  -2081.358436806494
new min fval from sgd:  -2081.5018101015676
new min fval from sgd:  -2081.546621856721
new min fval from sgd:  -2081.5592033708404
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [661.1813]
objective value function right now is: -2078.9978992106417
new min fval from sgd:  -2081.561284467331
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [662.3616]
objective value function right now is: -2075.050198453692
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [663.043]
objective value function right now is: -2080.2477309153132
new min fval from sgd:  -2081.59253607318
new min fval from sgd:  -2081.624629469893
new min fval from sgd:  -2081.6346319943214
new min fval from sgd:  -2081.6577065487636
new min fval from sgd:  -2081.6774918477995
new min fval from sgd:  -2081.6981758283396
new min fval from sgd:  -2081.7169321416964
new min fval from sgd:  -2081.730442560565
new min fval from sgd:  -2081.73933394698
new min fval from sgd:  -2081.747603290998
new min fval from sgd:  -2081.7490224604853
new min fval from sgd:  -2081.768250209508
new min fval from sgd:  -2081.7814648245135
new min fval from sgd:  -2081.7895096660586
new min fval from sgd:  -2081.794478586487
new min fval from sgd:  -2081.796181192907
new min fval from sgd:  -2081.7973937140646
new min fval from sgd:  -2081.798782907613
new min fval from sgd:  -2081.8033878530478
new min fval from sgd:  -2081.8075735176667
new min fval from sgd:  -2081.808496595716
new min fval from sgd:  -2081.8121128037965
new min fval from sgd:  -2081.812360141441
new min fval from sgd:  -2081.8178872291496
new min fval from sgd:  -2081.82888347215
new min fval from sgd:  -2081.8409226156223
new min fval from sgd:  -2081.857075856675
new min fval from sgd:  -2081.865496049917
new min fval from sgd:  -2081.8783581003813
new min fval from sgd:  -2081.88679658616
new min fval from sgd:  -2081.894949677206
new min fval from sgd:  -2081.9023897039206
new min fval from sgd:  -2081.907640412266
new min fval from sgd:  -2081.9129261834887
new min fval from sgd:  -2081.9161035860548
new min fval from sgd:  -2081.9467040746417
new min fval from sgd:  -2081.9855266614472
new min fval from sgd:  -2082.0099477938097
new min fval from sgd:  -2082.030812962982
new min fval from sgd:  -2082.047764585723
new min fval from sgd:  -2082.0603568757324
new min fval from sgd:  -2082.0606432695577
new min fval from sgd:  -2082.0618595566734
new min fval from sgd:  -2082.0679439517994
new min fval from sgd:  -2082.0736071385472
new min fval from sgd:  -2082.0840205991867
new min fval from sgd:  -2082.092225281175
new min fval from sgd:  -2082.0967371433185
new min fval from sgd:  -2082.101416533011
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [663.6225]
objective value function right now is: -2081.9821882195315
new min fval from sgd:  -2082.104113262399
new min fval from sgd:  -2082.1223267225173
new min fval from sgd:  -2082.1424298860597
new min fval from sgd:  -2082.1568038967457
new min fval from sgd:  -2082.1599166588303
new min fval from sgd:  -2082.1620219502415
new min fval from sgd:  -2082.168932186927
new min fval from sgd:  -2082.175393521384
new min fval from sgd:  -2082.175555469588
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [663.83905]
objective value function right now is: -2081.911808067355
min fval:  -2082.175555469588
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 7.3953e+00,  4.1353e+00],
        [-2.9976e+00,  1.3829e+00],
        [ 1.1483e+01, -9.6327e-01],
        [-4.8801e+00,  1.3247e+01],
        [-9.9088e-01,  2.0142e-02],
        [-9.9090e-01,  2.0271e-02],
        [-1.1170e+01,  9.1732e+00],
        [-1.2874e+01,  5.6341e+00],
        [-3.2782e+01, -1.1330e+01],
        [-8.7446e+00, -1.7429e+01],
        [-9.9098e-01,  2.0830e-02],
        [ 1.1134e+01, -5.4983e-01],
        [-9.9078e-01,  1.9482e-02]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-10.5779,  -3.4339,  -9.7010,   2.2796,  -3.0824,  -3.0822,   3.0877,
          1.9992,  -3.8152,   0.3916,  -3.0813,  -9.7323,  -3.0835],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 5.0727e-03, -2.1287e-03, -2.7211e-01, -6.2713e-01, -1.7203e-02,
         -1.7206e-02, -2.2033e-01, -1.3938e-01, -8.0510e-03, -3.2979e-01,
         -1.7219e-02, -2.2864e-01, -1.7187e-02],
        [-6.9736e+00,  4.8277e-01, -9.3636e+00,  4.4629e+00, -1.0079e-01,
         -1.0039e-01,  6.8170e+00,  5.0489e+00, -1.0620e+01, -1.0940e+01,
         -9.8640e-02, -8.9433e+00, -1.0285e-01],
        [ 5.5682e+00,  2.8608e-01,  7.8038e+00, -4.7005e+00,  1.4631e-02,
          1.4703e-02, -5.6259e+00, -3.8637e+00,  8.0212e+00,  8.7850e+00,
          1.5027e-02,  7.9387e+00,  1.4222e-02],
        [ 5.0727e-03, -2.1287e-03, -2.7211e-01, -6.2713e-01, -1.7203e-02,
         -1.7206e-02, -2.2033e-01, -1.3938e-01, -8.0510e-03, -3.2979e-01,
         -1.7219e-02, -2.2864e-01, -1.7187e-02],
        [ 5.0726e-03, -2.1287e-03, -2.7211e-01, -6.2713e-01, -1.7203e-02,
         -1.7206e-02, -2.2033e-01, -1.3938e-01, -8.0510e-03, -3.2979e-01,
         -1.7219e-02, -2.2864e-01, -1.7187e-02],
        [ 5.0726e-03, -2.1287e-03, -2.7211e-01, -6.2713e-01, -1.7203e-02,
         -1.7206e-02, -2.2033e-01, -1.3938e-01, -8.0510e-03, -3.2979e-01,
         -1.7219e-02, -2.2864e-01, -1.7187e-02],
        [ 8.0745e+00, -3.5305e-02,  9.9640e+00, -5.0073e+00, -1.5269e-01,
         -1.5215e-01, -8.0007e+00, -5.4633e+00,  1.0842e+01,  1.0792e+01,
         -1.4980e-01,  1.0041e+01, -1.5547e-01],
        [ 5.0594e-03, -2.1286e-03, -2.7205e-01, -6.2695e-01, -1.7199e-02,
         -1.7202e-02, -2.2034e-01, -1.3938e-01, -8.0471e-03, -3.2967e-01,
         -1.7215e-02, -2.2861e-01, -1.7184e-02],
        [ 5.0726e-03, -2.1287e-03, -2.7211e-01, -6.2713e-01, -1.7203e-02,
         -1.7206e-02, -2.2033e-01, -1.3938e-01, -8.0510e-03, -3.2979e-01,
         -1.7219e-02, -2.2864e-01, -1.7187e-02],
        [ 5.0729e-03, -2.1287e-03, -2.7211e-01, -6.2714e-01, -1.7203e-02,
         -1.7206e-02, -2.2033e-01, -1.3939e-01, -8.0511e-03, -3.2979e-01,
         -1.7219e-02, -2.2865e-01, -1.7187e-02],
        [ 5.0728e-03, -2.1287e-03, -2.7211e-01, -6.2713e-01, -1.7203e-02,
         -1.7206e-02, -2.2034e-01, -1.3938e-01, -8.0511e-03, -3.2979e-01,
         -1.7219e-02, -2.2865e-01, -1.7187e-02],
        [ 5.0726e-03, -2.1287e-03, -2.7211e-01, -6.2713e-01, -1.7203e-02,
         -1.7206e-02, -2.2033e-01, -1.3938e-01, -8.0510e-03, -3.2979e-01,
         -1.7219e-02, -2.2864e-01, -1.7187e-02],
        [ 5.0727e-03, -2.1287e-03, -2.7211e-01, -6.2713e-01, -1.7203e-02,
         -1.7206e-02, -2.2033e-01, -1.3938e-01, -8.0510e-03, -3.2979e-01,
         -1.7219e-02, -2.2864e-01, -1.7187e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.8820,  1.6754, -3.2347, -1.8820, -1.8820, -1.8820, -2.9182, -1.8824,
        -1.8820, -1.8820, -1.8820, -1.8820, -1.8820], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0276,  15.6696,  -5.8825,   0.0276,   0.0276,   0.0276, -10.9054,
           0.0276,   0.0276,   0.0276,   0.0276,   0.0276,   0.0276]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 11.6850,   3.0182],
        [-10.3859,  -0.3161],
        [-10.8541,  -0.1712],
        [-10.5124,  -2.1740],
        [-10.4389,  -0.3445],
        [-12.7490,  11.0590],
        [  7.5333,   7.6612],
        [ -3.9081,  12.3034],
        [  3.1088,   8.4645],
        [ 11.9256,   2.9767],
        [-14.2080,  -3.0025],
        [-13.0026,  -3.7855],
        [  6.8909,   7.7295]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-2.1610,  8.2753,  9.2104, -2.4919,  8.2905,  3.3114,  4.0520,  4.2538,
         3.8836, -1.6675, -1.4382,  0.1945,  5.5173], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-5.2405e+00,  5.0464e+00,  6.2235e+00, -6.0621e-01,  4.4169e+00,
          7.1543e+00, -1.0208e+00,  9.9418e+00,  9.3101e-01, -4.9726e+00,
         -4.6473e+00, -1.4236e+01, -2.1281e+00],
        [-3.3474e+00,  1.3999e+00,  2.0367e+00,  4.1433e-02,  1.0696e+00,
          9.1106e+00, -4.8166e+00,  1.1474e+01, -4.2998e+00, -3.1875e+00,
          3.8696e+00,  1.9055e+00, -5.0537e+00],
        [-1.4562e+00, -2.0215e+00, -2.1716e+00, -7.3399e-03, -2.0134e+00,
         -7.1614e-01, -1.5251e-01, -1.9478e+00, -1.5068e-01, -1.4910e+00,
         -3.2853e-02, -4.4982e-02, -3.0290e-01],
        [-1.0931e+00, -2.0743e+00, -2.1950e+00, -4.7455e-02, -2.0749e+00,
         -1.1090e+00,  6.6977e-02, -2.4767e+00,  1.0335e-01, -1.1321e+00,
         -1.0325e-01, -1.8612e-01, -1.2808e-01],
        [-1.8688e+00, -1.1735e+00, -1.3033e+00, -1.2579e-02, -1.1680e+00,
         -2.5290e-01, -8.2576e-01, -8.1557e-01, -4.2287e-01, -1.8978e+00,
         -1.8801e-02, -2.6651e-02, -9.7565e-01],
        [-1.7270e+00, -1.6856e+00, -1.8066e+00, -8.9312e-03, -1.6767e+00,
         -5.2392e-01, -5.2031e-01, -1.5048e+00, -2.3703e-01, -1.7553e+00,
         -2.4124e-02,  1.9811e-02, -6.7951e-01],
        [-4.6698e+00,  3.4112e+00,  1.9812e+00,  2.6217e+00,  3.2587e+00,
          1.3934e-02, -1.7025e+01,  1.4030e-02, -6.8973e-01, -4.6550e+00,
          4.7077e+00,  6.7917e-01, -2.9924e+01],
        [-5.4254e+00,  4.2898e+00,  6.3613e+00,  5.5895e+00,  4.2581e+00,
          1.0533e+00, -6.3053e+00, -1.0534e+01, -1.1192e+01, -5.5766e+00,
          1.1369e+01,  4.1655e+00, -5.6157e+00],
        [-1.7018e+00, -1.6716e+00, -1.8506e+00, -1.1309e-02, -1.6643e+00,
         -4.9397e-01, -4.0407e-01, -1.5146e+00, -3.3545e-01, -1.7433e+00,
         -2.4950e-02, -9.5173e-03, -5.2861e-01],
        [-4.4675e+00,  3.9546e+00,  4.5437e+00, -3.1186e+00,  4.0264e+00,
          5.4238e+00, -3.2602e+00,  1.2064e+01, -2.0622e+00, -4.7740e+00,
         -4.3493e+00, -3.5201e+00, -5.2272e+00],
        [-2.4486e+00,  1.5445e+00,  1.3183e+00,  3.6774e-01,  1.5456e+00,
         -1.8273e+00, -1.0485e+00, -2.4674e+00, -2.2309e+00, -2.2365e+00,
          2.0958e-01,  6.6745e-01, -6.2854e-01],
        [ 3.1429e-01,  2.2174e+00,  2.5344e+00, -4.6231e-01,  2.1987e+00,
          5.0563e-01, -6.8335e-01,  2.1973e+00, -9.1488e-02,  4.0442e-01,
         -1.0279e+00, -1.7999e+00, -5.5141e-01],
        [ 1.6850e+00, -1.2462e+00, -2.4812e+00,  2.0421e-01, -1.1867e+00,
          1.5057e+00,  2.0483e+00,  1.5551e+01,  5.8968e+00,  1.5780e+00,
         -2.3906e+00, -8.3526e+00,  1.3229e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-7.9809, -4.9504, -2.0280, -1.7009, -2.3103, -2.2158, -7.0515, -1.1380,
        -2.3362, -6.4273, -1.9501,  1.1340, -3.0643], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 5.8340e+00,  2.0301e+00, -2.3750e-01, -1.4899e+00, -1.1883e-02,
         -6.2252e-02, -8.3849e-03, -9.5267e-01, -7.1676e-02,  2.4158e+00,
          8.7668e-02, -8.3416e+00,  3.2552e+00],
        [-5.5810e-02, -8.5058e-01,  8.8796e-01,  1.1852e+00,  1.6137e-01,
          5.3122e-01,  1.5344e+01, -2.3988e+00,  5.2791e-01,  3.1483e-01,
         -1.7117e+00,  4.5489e-01,  6.1146e-01],
        [-3.1992e-01, -3.4497e-03, -2.1721e-03, -8.3309e-03, -9.0144e-04,
         -6.8251e-04, -1.5944e-04, -1.6421e-01, -6.4047e-04, -3.2851e-02,
         -3.8871e-01, -9.9678e+00, -3.2128e+00],
        [-3.5697e-01, -4.2236e-03, -2.6761e-03, -7.5152e-03, -6.6981e-04,
         -5.2022e-04, -1.2456e-03, -1.6066e-01, -4.1444e-04, -2.6292e-02,
         -3.9257e-01, -9.6769e+00, -3.1337e+00],
        [-6.5072e-02,  2.8600e-01, -8.2008e-01, -1.2451e+00, -1.5851e-01,
         -4.8576e-01, -1.4375e+01,  6.5333e+00, -3.7385e-01,  5.7091e-01,
          1.9789e+00,  1.2715e+00, -1.3183e-02]], device='cuda:0'))])
xi:  [663.7855]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 920.9660381229537
W_T_median: 855.6552192367049
W_T_pctile_5: 664.1408554642554
W_T_CVAR_5_pct: 340.8565208003977
Average q (qsum/M+1):  50.67477318548387
Optimal xi:  [663.7855]
Expected(across Rb) median(across samples) p_equity:  0.24359237137250603
obj fun:  tensor(-2082.1756, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 946.50330641104
W_T_median: 707.5512584605915
W_T_pctile_5: -238.22847581189643
W_T_CVAR_5_pct: -367.3344080824147
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1382.1029842631403
Current xi:  [123.55912]
objective value function right now is: -1382.1029842631403
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1885.7678114434432
Current xi:  [143.61658]
objective value function right now is: -1885.7678114434432
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1993.6273454066243
Current xi:  [165.81311]
objective value function right now is: -1993.6273454066243
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -2049.698387572902
Current xi:  [189.40965]
objective value function right now is: -2049.698387572902
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -2094.9879221264446
Current xi:  [212.69073]
objective value function right now is: -2094.9879221264446
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -2123.4833892861484
Current xi:  [235.48013]
objective value function right now is: -2123.4833892861484
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -2144.42495933155
Current xi:  [257.87943]
objective value function right now is: -2144.42495933155
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -2217.5016994835737
Current xi:  [279.73373]
objective value function right now is: -2217.5016994835737
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -2249.217631591885
Current xi:  [301.16666]
objective value function right now is: -2249.217631591885
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -2283.6058650987993
Current xi:  [322.8057]
objective value function right now is: -2283.6058650987993
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -2313.6125468665136
Current xi:  [343.8808]
objective value function right now is: -2313.6125468665136
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -2326.182169802793
Current xi:  [364.96585]
objective value function right now is: -2326.182169802793
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -2376.287259984061
Current xi:  [385.80704]
objective value function right now is: -2376.287259984061
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -2393.0997318318905
Current xi:  [405.8352]
objective value function right now is: -2393.0997318318905
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -2408.003006332501
Current xi:  [425.95035]
objective value function right now is: -2408.003006332501
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -2445.48021974504
Current xi:  [445.57788]
objective value function right now is: -2445.48021974504
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -2463.5430631992226
Current xi:  [465.25903]
objective value function right now is: -2463.5430631992226
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -2493.0303686960137
Current xi:  [484.47668]
objective value function right now is: -2493.0303686960137
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -2510.0836813408714
Current xi:  [503.41718]
objective value function right now is: -2510.0836813408714
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2523.6745365489946
Current xi:  [522.3455]
objective value function right now is: -2523.6745365489946
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -2533.9821646114133
Current xi:  [540.7661]
objective value function right now is: -2533.9821646114133
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -2551.2803077583476
Current xi:  [558.7837]
objective value function right now is: -2551.2803077583476
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2554.315155706316
Current xi:  [576.00964]
objective value function right now is: -2554.315155706316
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -2571.0906892704816
Current xi:  [589.0376]
objective value function right now is: -2571.0906892704816
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -2582.388489104539
Current xi:  [605.1777]
objective value function right now is: -2582.388489104539
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [620.3015]
objective value function right now is: -2581.559644334021
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -2591.106701584464
Current xi:  [634.4062]
objective value function right now is: -2591.106701584464
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -2604.1581742464414
Current xi:  [645.951]
objective value function right now is: -2604.1581742464414
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -2608.072014968224
Current xi:  [659.3207]
objective value function right now is: -2608.072014968224
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -2611.694365298246
Current xi:  [670.11346]
objective value function right now is: -2611.694365298246
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [681.5706]
objective value function right now is: -2588.041519452872
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -2612.3479419488185
Current xi:  [690.3198]
objective value function right now is: -2612.3479419488185
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [697.2896]
objective value function right now is: -2606.3961944532384
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [702.8378]
objective value function right now is: -2611.87186013195
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -2615.133178601253
Current xi:  [707.8969]
objective value function right now is: -2615.133178601253
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2622.6220198805668
Current xi:  [708.99976]
objective value function right now is: -2622.6220198805668
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [710.2164]
objective value function right now is: -2619.882115378283
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [711.4945]
objective value function right now is: -2620.6330206908974
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2623.478512695249
Current xi:  [712.48267]
objective value function right now is: -2623.478512695249
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [713.5307]
objective value function right now is: -2619.201787989705
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [714.50555]
objective value function right now is: -2623.419454329571
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [715.49756]
objective value function right now is: -2620.2226678987467
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -2624.2253477442164
Current xi:  [716.43774]
objective value function right now is: -2624.2253477442164
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [717.1083]
objective value function right now is: -2623.1313812166773
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [718.3115]
objective value function right now is: -2622.2584718634675
new min fval from sgd:  -2624.241156299147
new min fval from sgd:  -2624.5797620782096
new min fval from sgd:  -2624.854801396072
new min fval from sgd:  -2625.1486265126127
new min fval from sgd:  -2625.4851183807114
new min fval from sgd:  -2625.71187835027
new min fval from sgd:  -2625.879079170822
new min fval from sgd:  -2626.0096689626293
new min fval from sgd:  -2626.1087536917294
new min fval from sgd:  -2626.169157852374
new min fval from sgd:  -2626.1893144389896
new min fval from sgd:  -2626.1913586400933
new min fval from sgd:  -2626.2449438888643
new min fval from sgd:  -2626.5755548588572
new min fval from sgd:  -2626.7562538917837
new min fval from sgd:  -2626.809943254755
new min fval from sgd:  -2626.835230081168
new min fval from sgd:  -2626.859241235021
new min fval from sgd:  -2626.896969274868
new min fval from sgd:  -2626.9473049812673
new min fval from sgd:  -2626.9483730000734
new min fval from sgd:  -2626.9725411703853
new min fval from sgd:  -2627.0455948309436
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [719.1995]
objective value function right now is: -2627.0455948309436
new min fval from sgd:  -2627.0705182464812
new min fval from sgd:  -2627.1175834264
new min fval from sgd:  -2627.2965687719893
new min fval from sgd:  -2627.308064442775
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [719.80927]
objective value function right now is: -2623.25690750556
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [720.5615]
objective value function right now is: -2622.0865055229033
new min fval from sgd:  -2627.3169465775563
new min fval from sgd:  -2627.3240889050503
new min fval from sgd:  -2627.325546498038
new min fval from sgd:  -2627.330213960516
new min fval from sgd:  -2627.3370283587624
new min fval from sgd:  -2627.3595812624308
new min fval from sgd:  -2627.383982228859
new min fval from sgd:  -2627.416289670727
new min fval from sgd:  -2627.4456737144587
new min fval from sgd:  -2627.482904366582
new min fval from sgd:  -2627.5260662902574
new min fval from sgd:  -2627.557625768895
new min fval from sgd:  -2627.5865416845786
new min fval from sgd:  -2627.6105672327394
new min fval from sgd:  -2627.629693431699
new min fval from sgd:  -2627.6421368175484
new min fval from sgd:  -2627.659542021094
new min fval from sgd:  -2627.6767592894744
new min fval from sgd:  -2627.6919084248284
new min fval from sgd:  -2627.7067580459743
new min fval from sgd:  -2627.711349108612
new min fval from sgd:  -2627.718129224428
new min fval from sgd:  -2627.725292729856
new min fval from sgd:  -2627.7436700381045
new min fval from sgd:  -2627.762156887832
new min fval from sgd:  -2627.7757959107435
new min fval from sgd:  -2627.779706699381
new min fval from sgd:  -2627.7862629365345
new min fval from sgd:  -2627.790122473833
new min fval from sgd:  -2627.794110106414
new min fval from sgd:  -2627.7951808611956
new min fval from sgd:  -2627.804325336154
new min fval from sgd:  -2627.809685573705
new min fval from sgd:  -2627.8136464443514
new min fval from sgd:  -2627.8154862337765
new min fval from sgd:  -2627.8286134439436
new min fval from sgd:  -2627.8393482983224
new min fval from sgd:  -2627.843306338531
new min fval from sgd:  -2627.851074928209
new min fval from sgd:  -2627.8538470798317
new min fval from sgd:  -2627.858109838061
new min fval from sgd:  -2627.8601694965514
new min fval from sgd:  -2627.8793402685737
new min fval from sgd:  -2627.9007116873604
new min fval from sgd:  -2627.9162718839034
new min fval from sgd:  -2627.9258719547224
new min fval from sgd:  -2627.929765015606
new min fval from sgd:  -2627.9346069169956
new min fval from sgd:  -2627.940823354433
new min fval from sgd:  -2627.9432094845884
new min fval from sgd:  -2627.948691755352
new min fval from sgd:  -2627.9563888157068
new min fval from sgd:  -2627.961580295149
new min fval from sgd:  -2627.9652906021297
new min fval from sgd:  -2627.97123297473
new min fval from sgd:  -2627.9742879431387
new min fval from sgd:  -2627.9795867168523
new min fval from sgd:  -2627.985777329036
new min fval from sgd:  -2627.989942036769
new min fval from sgd:  -2627.9929976968915
new min fval from sgd:  -2628.0026999391966
new min fval from sgd:  -2628.0154772864776
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [721.2173]
objective value function right now is: -2627.9857291583958
new min fval from sgd:  -2628.0220906050613
new min fval from sgd:  -2628.0477684668554
new min fval from sgd:  -2628.0679989355936
new min fval from sgd:  -2628.08515488688
new min fval from sgd:  -2628.096425487371
new min fval from sgd:  -2628.108619502766
new min fval from sgd:  -2628.11314159671
new min fval from sgd:  -2628.1149090557324
new min fval from sgd:  -2628.1172490276035
new min fval from sgd:  -2628.123005987796
new min fval from sgd:  -2628.137736072761
new min fval from sgd:  -2628.14840147502
new min fval from sgd:  -2628.1597741670366
new min fval from sgd:  -2628.1692103915643
new min fval from sgd:  -2628.1728321827854
new min fval from sgd:  -2628.1773704312686
new min fval from sgd:  -2628.180044520961
new min fval from sgd:  -2628.183811369761
new min fval from sgd:  -2628.1846942989723
new min fval from sgd:  -2628.1876530910436
new min fval from sgd:  -2628.1901809496376
new min fval from sgd:  -2628.190523379567
new min fval from sgd:  -2628.1912933997046
new min fval from sgd:  -2628.198856621245
new min fval from sgd:  -2628.2092756953966
new min fval from sgd:  -2628.220202469144
new min fval from sgd:  -2628.2277016747025
new min fval from sgd:  -2628.2409452697148
new min fval from sgd:  -2628.252778881147
new min fval from sgd:  -2628.2605453766423
new min fval from sgd:  -2628.2618994573977
new min fval from sgd:  -2628.261924630645
new min fval from sgd:  -2628.263861547714
new min fval from sgd:  -2628.266213298068
new min fval from sgd:  -2628.2663479825046
new min fval from sgd:  -2628.2690335999323
new min fval from sgd:  -2628.273007120145
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [721.3914]
objective value function right now is: -2628.203310510654
min fval:  -2628.273007120145
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 10.5694,  -0.6563],
        [ -0.7195,  -0.2480],
        [ 12.4641,  17.4819],
        [ 11.5922,  -1.0196],
        [ -6.7640,   2.0710],
        [ 41.1166,   8.4581],
        [-14.4744,   3.6657],
        [ 11.4812,  -0.9913],
        [  2.3684, -14.4039],
        [  5.4955,   2.9040],
        [ -0.6901,  -0.2841],
        [ -0.7527,  -0.2093],
        [  9.2415,  -9.3809]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -8.8335,  -3.2970,  -6.5875,  -9.0396,  -2.5799,   2.4361,   0.8117,
         -8.9967,  -2.2967, -10.6819,  -3.2455,  -3.3423,  -2.3990],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 2.6248e-01,  6.1165e-02,  6.5093e-01,  2.9226e-01,  8.2915e-03,
          2.3802e+00,  4.3209e-01,  2.9073e-01,  3.6728e-01,  5.1290e-03,
          6.0570e-02,  5.9672e-02,  5.2334e-01],
        [ 7.7034e+00,  7.7120e-02, -9.4870e+00,  1.0502e+01, -2.6354e-01,
         -5.0175e+00, -6.5006e+00,  9.9891e+00,  1.2516e+01,  4.2327e+00,
          1.3375e-02,  1.3992e-01,  8.3902e+00],
        [-1.2273e-01, -1.6737e-02, -2.5581e-01, -1.2719e-01, -1.6429e-02,
         -1.0999e+00, -2.4385e-01, -1.2664e-01, -3.2706e-01, -7.6783e-02,
         -1.7385e-02, -1.6132e-02, -2.4682e-01],
        [ 2.5707e-01,  5.9708e-02,  6.3938e-01,  2.8653e-01,  9.8093e-03,
          2.3150e+00,  4.2354e-01,  2.8502e-01,  3.5928e-01,  4.0506e-03,
          5.9201e-02,  5.8245e-02,  5.0711e-01],
        [ 2.5268e-01,  5.8509e-02,  6.2998e-01,  2.8184e-01,  1.0899e-02,
          2.2612e+00,  4.1662e-01,  2.8036e-01,  3.5251e-01,  3.0565e-03,
          5.8067e-02,  5.7071e-02,  4.9344e-01],
        [-1.2273e-01, -1.6737e-02, -2.5581e-01, -1.2719e-01, -1.6429e-02,
         -1.0999e+00, -2.4385e-01, -1.2664e-01, -3.2706e-01, -7.6783e-02,
         -1.7385e-02, -1.6132e-02, -2.4682e-01],
        [-1.2273e-01, -1.6737e-02, -2.5581e-01, -1.2719e-01, -1.6429e-02,
         -1.0999e+00, -2.4385e-01, -1.2664e-01, -3.2706e-01, -7.6784e-02,
         -1.7385e-02, -1.6132e-02, -2.4682e-01],
        [-1.2273e-01, -1.6737e-02, -2.5581e-01, -1.2719e-01, -1.6429e-02,
         -1.0999e+00, -2.4385e-01, -1.2664e-01, -3.2706e-01, -7.6783e-02,
         -1.7385e-02, -1.6132e-02, -2.4682e-01],
        [ 2.5655e-01,  5.9565e-02,  6.3828e-01,  2.8597e-01,  9.9421e-03,
          2.3087e+00,  4.2274e-01,  2.8447e-01,  3.5848e-01,  3.9358e-03,
          5.9066e-02,  5.8104e-02,  5.0549e-01],
        [ 5.4568e+00,  1.4230e-01, -6.9626e+00,  7.6402e+00, -8.7420e-01,
         -5.0859e+00, -5.9343e+00,  7.2040e+00,  9.0340e+00,  9.3319e-01,
          8.5587e-02,  1.3246e-01,  5.1828e+00],
        [ 2.5983e-01,  6.0458e-02,  6.4531e-01,  2.8946e-01,  9.0527e-03,
          2.3486e+00,  4.2794e-01,  2.8794e-01,  3.6341e-01,  4.6163e-03,
          5.9907e-02,  5.8979e-02,  5.1550e-01],
        [ 8.5847e+00, -1.7348e-01, -8.3536e+00,  1.1300e+01, -1.7697e+00,
         -5.9920e+00, -7.0329e+00,  1.0840e+01,  1.1111e+01,  4.0346e+00,
         -1.0478e-01, -2.3101e-01,  7.1146e+00],
        [-1.2273e-01, -1.6737e-02, -2.5581e-01, -1.2719e-01, -1.6429e-02,
         -1.0999e+00, -2.4385e-01, -1.2664e-01, -3.2706e-01, -7.6783e-02,
         -1.7385e-02, -1.6132e-02, -2.4682e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 2.3647,  0.0148, -1.2140,  2.2981,  2.2439, -1.2140, -1.2140, -1.2140,
         2.2914, -0.8100,  2.3319, -1.0714, -1.2140], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  3.3646, -12.8941,   0.0208,   2.8477,   2.4874,   0.0208,   0.0208,
           0.0208,   2.8017,  -6.0827,   3.1010, -13.2007,   0.0208]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-1.3843e+01, -3.3027e+00],
        [ 3.4349e+00,  1.1001e+01],
        [-5.5413e+00,  1.0417e+01],
        [ 4.9488e+00,  8.0020e+00],
        [-1.6111e+01,  1.1603e+01],
        [-1.2757e+00,  1.5323e+00],
        [-1.0907e+01,  8.2295e-03],
        [ 1.2212e+01,  3.5468e+00],
        [ 9.9752e+00,  2.1985e+00],
        [-1.2112e+01, -3.5647e+00],
        [-1.7957e+00,  1.1535e+00],
        [ 1.0903e+01,  2.9532e+00],
        [ 3.4743e+00,  1.1006e+01]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-2.3442, -0.5780,  2.4419,  6.0913,  3.0113, -4.5459,  9.2058, -2.0756,
        -9.7847, -0.2371, -4.9717, -3.3528,  4.3934], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.6355e-02, -4.7203e-01, -2.2414e-01, -1.2930e+00, -9.7963e-02,
         -8.9425e-02, -2.5861e+00, -2.3741e+00,  1.6912e-01,  1.4224e-01,
         -1.8616e-01, -1.6146e+00, -3.0752e-01],
        [ 9.4379e+00, -7.2012e-04, -2.8763e-02, -2.8295e+01, -1.7439e-02,
         -4.1387e-01,  2.6457e+00, -5.6731e+00,  1.4961e-02,  2.6755e+00,
         -3.3600e-01, -3.5999e+00, -6.6378e-02],
        [ 6.7634e+00, -1.3554e+00, -1.2655e+01, -1.5746e+00, -4.2819e+00,
         -1.3759e-01,  2.1706e+00, -2.5130e+00, -5.6303e+00,  6.5255e+00,
         -2.1359e-01, -3.7939e-01, -3.7780e+00],
        [-2.5292e+00, -5.0515e-01, -8.4915e-01,  4.9504e+00, -5.9242e+00,
          8.3704e-02, -7.1340e+00, -9.7224e-02,  5.8917e+00, -7.3664e+00,
         -5.9515e-03,  9.5787e-01,  2.5381e+00],
        [ 2.5504e-02, -4.7458e-01, -2.2197e-01, -1.2961e+00, -9.0961e-02,
         -8.9074e-02, -2.5750e+00, -2.3729e+00,  1.6415e-01,  1.4310e-01,
         -1.8259e-01, -1.6237e+00, -3.0919e-01],
        [-2.2873e+00, -2.7441e+00,  4.2746e+00, -3.8141e+00,  7.2052e+00,
          2.5999e-01,  3.8499e+00, -2.0788e+00, -1.0618e+01, -1.5316e+00,
          3.2362e-01, -1.9326e+00, -8.2316e-01],
        [-9.9645e+00, -4.6834e+00,  4.3551e+00,  5.1502e+00,  9.5067e+00,
         -1.7102e+00,  7.9429e-01, -7.9561e-01, -1.2556e+01, -1.1889e+01,
         -1.2672e+00, -6.6184e-01,  3.2272e+00],
        [ 1.4009e+01,  5.1276e-01, -1.0523e+01, -9.7465e+00, -7.5477e+00,
          2.5440e-01,  7.9842e+00, -6.9826e+00, -4.2503e+00,  1.4034e+01,
          4.1232e-01, -4.0212e+00, -8.3833e+00],
        [ 8.0221e+00, -3.7171e+00,  1.6547e+00, -5.7710e+00,  2.5336e+00,
          4.6761e-01,  3.6684e+00, -7.0854e+00, -2.5274e-02,  1.8505e+00,
          4.0934e-01, -3.3734e+00,  2.6350e+00],
        [ 2.7571e-02, -4.7257e-01, -2.2574e-01, -1.2870e+00, -1.0231e-01,
         -8.9907e-02, -2.6062e+00, -2.3723e+00,  1.7389e-01,  1.4924e-01,
         -1.9162e-01, -1.6095e+00, -3.0218e-01],
        [ 2.5591e-02, -4.7429e-01, -2.2230e-01, -1.2952e+00, -9.1580e-02,
         -8.9114e-02, -2.5767e+00, -2.3738e+00,  1.6471e-01,  1.4234e-01,
         -1.8288e-01, -1.6230e+00, -3.0945e-01],
        [ 3.5224e+00, -1.5636e-03,  2.7983e-03, -3.6976e+01,  1.6361e-03,
         -6.2412e-01,  6.0239e+00, -5.3542e+00,  3.2895e-02,  7.0334e-01,
         -5.2304e-01, -3.2760e+00, -2.7621e-02],
        [ 2.8014e-02, -4.7512e-01, -2.2670e-01, -1.2808e+00, -1.0393e-01,
         -9.0538e-02, -2.6158e+00, -2.3714e+00,  1.7410e-01,  1.5191e-01,
         -1.9387e-01, -1.6050e+00, -2.9856e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.0770, -4.4564,  1.4003, -1.5550, -3.0728, -2.4462, -7.3930, -0.1988,
        -5.9333, -3.0772, -3.0704, -5.9760, -3.0752], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 2.6406e-01,  2.8950e-08, -5.4246e+01, -1.7433e+00,  2.6904e-01,
          2.3095e+00,  1.7215e+00, -2.7275e-01,  3.5224e+00,  2.6019e-01,
          2.6994e-01,  3.6273e-08,  2.6102e-01],
        [ 3.4794e-01,  1.0555e+01,  3.0083e-01,  1.0342e+00,  3.2757e-01,
         -6.5404e-01,  2.1922e-01, -6.4964e+00,  3.6963e+00,  3.6247e-01,
          3.3283e-01,  1.2928e+01,  3.6868e-01],
        [-7.1975e-02,  2.2891e-06, -2.5811e+01, -1.8582e+01, -7.2639e-02,
         -8.6930e+00, -1.9884e+01, -1.1779e-01, -5.8403e-01, -7.1544e-02,
         -7.2691e-02, -7.9108e-06, -7.1685e-02],
        [-6.1638e-02,  1.3731e-04, -2.4808e+01, -1.8877e+01, -6.2116e-02,
         -8.8532e+00, -1.8843e+01, -1.4986e-01, -7.3460e-01, -6.1342e-02,
         -6.2152e-02,  1.0494e-04, -6.1447e-02],
        [-5.8250e-01, -9.6942e+00,  2.0598e+00,  1.1006e+00, -5.8363e-01,
          1.0372e+00,  7.1349e-01,  8.9899e+00, -4.5926e+00, -5.9979e-01,
         -5.7792e-01, -1.2217e+01, -6.0202e-01]], device='cuda:0'))])
xi:  [721.3563]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1013.1590027520233
W_T_median: 919.9357491545371
W_T_pctile_5: 721.4216569238694
W_T_CVAR_5_pct: 373.0615552219261
Average q (qsum/M+1):  48.680317540322584
Optimal xi:  [721.3563]
Expected(across Rb) median(across samples) p_equity:  0.24949616921755174
obj fun:  tensor(-2628.2730, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 946.50330641104
W_T_median: 707.5512584605915
W_T_pctile_5: -238.22847581189643
W_T_CVAR_5_pct: -367.3344080824147
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1603.3160273761746
Current xi:  [121.94449]
objective value function right now is: -1603.3160273761746
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1856.9103712746441
Current xi:  [145.35085]
objective value function right now is: -1856.9103712746441
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -2016.804821675104
Current xi:  [168.87646]
objective value function right now is: -2016.804821675104
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -2100.4632621087726
Current xi:  [192.12323]
objective value function right now is: -2100.4632621087726
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -2179.0811233805994
Current xi:  [215.15562]
objective value function right now is: -2179.0811233805994
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -2267.8693877201563
Current xi:  [237.65909]
objective value function right now is: -2267.8693877201563
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -2526.9279790119854
Current xi:  [259.3116]
objective value function right now is: -2526.9279790119854
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -2598.0266669486905
Current xi:  [281.1325]
objective value function right now is: -2598.0266669486905
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -2664.200532493658
Current xi:  [302.51675]
objective value function right now is: -2664.200532493658
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -2718.767947575384
Current xi:  [323.90704]
objective value function right now is: -2718.767947575384
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -2752.8540563776023
Current xi:  [345.69873]
objective value function right now is: -2752.8540563776023
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -2834.168587321325
Current xi:  [367.43588]
objective value function right now is: -2834.168587321325
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -2875.6587911321712
Current xi:  [388.92004]
objective value function right now is: -2875.6587911321712
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -2927.932552752215
Current xi:  [410.1343]
objective value function right now is: -2927.932552752215
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -2948.6316682281877
Current xi:  [431.07364]
objective value function right now is: -2948.6316682281877
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -3018.411619596544
Current xi:  [451.01096]
objective value function right now is: -3018.411619596544
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -3053.5373725120094
Current xi:  [470.72098]
objective value function right now is: -3053.5373725120094
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -3059.3220067495004
Current xi:  [490.76593]
objective value function right now is: -3059.3220067495004
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -3108.019832330729
Current xi:  [509.8424]
objective value function right now is: -3108.019832330729
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -3143.4412682918664
Current xi:  [528.91425]
objective value function right now is: -3143.4412682918664
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -3160.028889690878
Current xi:  [546.67084]
objective value function right now is: -3160.028889690878
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -3174.525088125166
Current xi:  [564.24493]
objective value function right now is: -3174.525088125166
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -3193.109277263688
Current xi:  [582.552]
objective value function right now is: -3193.109277263688
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -3212.222171267717
Current xi:  [600.0461]
objective value function right now is: -3212.222171267717
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -3214.1919540585905
Current xi:  [615.8561]
objective value function right now is: -3214.1919540585905
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -3216.155167138188
Current xi:  [629.93756]
objective value function right now is: -3216.155167138188
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -3242.9535466675425
Current xi:  [644.2397]
objective value function right now is: -3242.9535466675425
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -3258.727919583007
Current xi:  [656.777]
objective value function right now is: -3258.727919583007
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -3273.7040031027423
Current xi:  [668.24304]
objective value function right now is: -3273.7040031027423
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [679.7216]
objective value function right now is: -3263.2772097601382
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [687.69574]
objective value function right now is: -3271.215220279696
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -3278.7842335955984
Current xi:  [695.39954]
objective value function right now is: -3278.7842335955984
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -3284.413551191966
Current xi:  [701.40546]
objective value function right now is: -3284.413551191966
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [707.3315]
objective value function right now is: -3272.4386501981144
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -3289.569767215046
Current xi:  [711.9559]
objective value function right now is: -3289.569767215046
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -3300.7856706918897
Current xi:  [712.7489]
objective value function right now is: -3300.7856706918897
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [713.6018]
objective value function right now is: -3298.878111403331
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [714.70526]
objective value function right now is: -3299.040730987911
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [715.69806]
objective value function right now is: -3299.8726393837323
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [716.8949]
objective value function right now is: -3299.2778079391387
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -3301.896087490725
Current xi:  [717.7405]
objective value function right now is: -3301.896087490725
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -3302.141261893498
Current xi:  [718.5371]
objective value function right now is: -3302.141261893498
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [719.2776]
objective value function right now is: -3301.7120240803965
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -3302.781559910589
Current xi:  [719.6829]
objective value function right now is: -3302.781559910589
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [720.2635]
objective value function right now is: -3297.0309957303907
new min fval from sgd:  -3302.824248423154
new min fval from sgd:  -3302.8700139124435
new min fval from sgd:  -3302.9146412443565
new min fval from sgd:  -3302.9178151372766
new min fval from sgd:  -3302.9341453628035
new min fval from sgd:  -3302.969627542773
new min fval from sgd:  -3303.0783969386034
new min fval from sgd:  -3303.23996632787
new min fval from sgd:  -3303.2417412521595
new min fval from sgd:  -3303.3487097747116
new min fval from sgd:  -3303.4330886970856
new min fval from sgd:  -3303.566524154503
new min fval from sgd:  -3303.6452563267617
new min fval from sgd:  -3303.694211880998
new min fval from sgd:  -3303.7481534295403
new min fval from sgd:  -3303.7909219530643
new min fval from sgd:  -3303.8161217041984
new min fval from sgd:  -3303.860647924415
new min fval from sgd:  -3303.877536014843
new min fval from sgd:  -3303.8921391985373
new min fval from sgd:  -3303.9109623367053
new min fval from sgd:  -3303.9179620271148
new min fval from sgd:  -3304.0037039575586
new min fval from sgd:  -3304.069176239443
new min fval from sgd:  -3304.1300323666137
new min fval from sgd:  -3304.198217911209
new min fval from sgd:  -3304.209611328574
new min fval from sgd:  -3304.2150255622714
new min fval from sgd:  -3304.2628516171976
new min fval from sgd:  -3304.4668654105894
new min fval from sgd:  -3304.6191802864137
new min fval from sgd:  -3304.729674908003
new min fval from sgd:  -3304.7904925104203
new min fval from sgd:  -3304.8523395061634
new min fval from sgd:  -3304.8776593061143
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [721.0669]
objective value function right now is: -3302.854794826724
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [721.4757]
objective value function right now is: -3299.290817609458
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [722.57104]
objective value function right now is: -3301.3926230917536
new min fval from sgd:  -3304.897469336813
new min fval from sgd:  -3304.909986910321
new min fval from sgd:  -3304.9256443692548
new min fval from sgd:  -3304.943795165422
new min fval from sgd:  -3304.959553907064
new min fval from sgd:  -3305.00439679989
new min fval from sgd:  -3305.050073469287
new min fval from sgd:  -3305.0685415479775
new min fval from sgd:  -3305.0721783524605
new min fval from sgd:  -3305.073009016538
new min fval from sgd:  -3305.088042115175
new min fval from sgd:  -3305.0900957966146
new min fval from sgd:  -3305.0908034665817
new min fval from sgd:  -3305.0924704812674
new min fval from sgd:  -3305.0965851216943
new min fval from sgd:  -3305.0973872306254
new min fval from sgd:  -3305.112888620788
new min fval from sgd:  -3305.1284255559676
new min fval from sgd:  -3305.1476072068085
new min fval from sgd:  -3305.1705411861717
new min fval from sgd:  -3305.1887301210177
new min fval from sgd:  -3305.211898118293
new min fval from sgd:  -3305.236768350079
new min fval from sgd:  -3305.2517072331348
new min fval from sgd:  -3305.269591838758
new min fval from sgd:  -3305.2811272942577
new min fval from sgd:  -3305.286649034962
new min fval from sgd:  -3305.2899319513763
new min fval from sgd:  -3305.2963307450746
new min fval from sgd:  -3305.3012868080514
new min fval from sgd:  -3305.3074060219815
new min fval from sgd:  -3305.3083523228643
new min fval from sgd:  -3305.3124837655387
new min fval from sgd:  -3305.3218434155347
new min fval from sgd:  -3305.328001965777
new min fval from sgd:  -3305.3340261696203
new min fval from sgd:  -3305.3455414840314
new min fval from sgd:  -3305.35758886359
new min fval from sgd:  -3305.370113114293
new min fval from sgd:  -3305.380568745632
new min fval from sgd:  -3305.3918587210087
new min fval from sgd:  -3305.406197244881
new min fval from sgd:  -3305.434139092856
new min fval from sgd:  -3305.4579134714277
new min fval from sgd:  -3305.46912724781
new min fval from sgd:  -3305.4748106340085
new min fval from sgd:  -3305.4760036804973
new min fval from sgd:  -3305.47863765728
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [723.2784]
objective value function right now is: -3305.1461853537708
new min fval from sgd:  -3305.4886857178067
new min fval from sgd:  -3305.5080601925256
new min fval from sgd:  -3305.534136726918
new min fval from sgd:  -3305.5394096622795
new min fval from sgd:  -3305.5505609085267
new min fval from sgd:  -3305.564843008256
new min fval from sgd:  -3305.5785450307962
new min fval from sgd:  -3305.584656307146
new min fval from sgd:  -3305.588219429635
new min fval from sgd:  -3305.5988210702476
new min fval from sgd:  -3305.6032500125516
new min fval from sgd:  -3305.6078613224113
new min fval from sgd:  -3305.6121938898896
new min fval from sgd:  -3305.615071225965
new min fval from sgd:  -3305.6234283716603
new min fval from sgd:  -3305.6389326669023
new min fval from sgd:  -3305.66396723283
new min fval from sgd:  -3305.673605818911
new min fval from sgd:  -3305.678105897152
new min fval from sgd:  -3305.6838603653405
new min fval from sgd:  -3305.692720805387
new min fval from sgd:  -3305.701590554147
new min fval from sgd:  -3305.711628185818
new min fval from sgd:  -3305.720567435208
new min fval from sgd:  -3305.7267423325106
new min fval from sgd:  -3305.736257580382
new min fval from sgd:  -3305.7365231367144
new min fval from sgd:  -3305.7402399281877
new min fval from sgd:  -3305.750938943159
new min fval from sgd:  -3305.765757842905
new min fval from sgd:  -3305.780063844269
new min fval from sgd:  -3305.7932764733523
new min fval from sgd:  -3305.8039254739047
new min fval from sgd:  -3305.8111988082624
new min fval from sgd:  -3305.817783192382
new min fval from sgd:  -3305.818754822491
new min fval from sgd:  -3305.8254933635444
new min fval from sgd:  -3305.826019391925
new min fval from sgd:  -3305.827108883977
new min fval from sgd:  -3305.8311363858766
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [723.4676]
objective value function right now is: -3305.3084654385652
min fval:  -3305.8311363858766
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 10.9117,  -0.6514],
        [ -0.3834,  20.1370],
        [ 11.3923,  -0.9216],
        [ -0.6887,  26.9583],
        [ -6.2041,  24.4507],
        [ 11.4444,  -0.9815],
        [ -6.1027,  14.9481],
        [ 10.9703,  -0.9251],
        [-22.3509,   2.1035],
        [ 11.3854,  -9.6060],
        [ 10.6664,  -0.9586],
        [ -8.2865,   6.3911],
        [-12.0121,   3.8041]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-8.5433, -2.4206, -8.3745, -2.0585,  3.3735, -8.3216, -0.8727, -8.0290,
         0.0310, -2.5547, -7.8904,  6.8840, -0.6699], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  8.2100,  -4.2397,   9.3502,  -7.5712, -11.8732,   9.8912,  -3.6658,
           8.1483,   0.1744,  13.7242,   5.7938,  -5.8338,  -2.3075],
        [  1.2745,   1.6559,   1.7128,   2.0029,   0.5438,   1.7984,   0.2776,
           1.7929,  -0.4097,   0.2865,   1.7246,  -1.3212,  -0.4912],
        [  1.0856,  -0.2930,   1.5871,  -0.0566, -17.7694,   1.6841,  -5.1776,
           1.6309,   5.8904,  -6.2454,   1.6108,   4.9208,   2.9346],
        [  8.0194,  -1.8521,   8.8841,  -4.6526,  -9.5454,   9.3831,  -1.8356,
           8.2586,   0.0920,   7.5151,   5.7435, -12.5455,   0.4126],
        [ 11.4346,  -2.2710,  13.2726,  -5.8099, -11.9605,  13.7172,  -2.4175,
          12.3961,   0.1424,  10.6052,   9.4388, -13.8858,  -1.5333],
        [  1.5139,   1.8799,   1.9447,   2.2570,   0.3533,   2.0283,   0.4616,
           2.0190,  -0.5255,   0.8566,   1.9552,  -1.6079,  -0.4942],
        [  1.4675,   1.8567,   1.8979,   2.2342,   0.3593,   1.9818,   0.4459,
           1.9735,  -0.5246,   0.8566,   1.9104,  -1.5962,  -0.4849],
        [  7.2424,   2.5450,   7.1612,   3.8208,   3.7556,   7.0052,   1.8920,
           6.5635,   9.4450,  -7.6330,   6.1465,  -0.9556,   9.1221],
        [ -0.0582,  -0.1635,  -0.0712,  -0.1717,  -0.6489,  -0.0733,  -0.1569,
          -0.0721,  -0.1127,  -0.3753,  -0.0660,  -1.0928,  -0.0775],
        [ -0.0581,  -0.1636,  -0.0712,  -0.1718,  -0.6498,  -0.0733,  -0.1569,
          -0.0721,  -0.1128,  -0.3753,  -0.0660,  -1.0933,  -0.0776],
        [  1.2869,   1.7493,   1.7001,   2.1131,   0.5561,   1.7817,   0.3624,
           1.7798,  -0.4608,   0.6414,   1.7201,  -1.4992,  -0.4524],
        [ -0.0586,  -0.1636,  -0.0716,  -0.1710,  -0.6326,  -0.0737,  -0.1566,
          -0.0725,  -0.1107,  -0.3739,  -0.0664,  -1.0899,  -0.0766],
        [ -0.0582,  -0.1635,  -0.0712,  -0.1718,  -0.6483,  -0.0733,  -0.1568,
          -0.0721,  -0.1126,  -0.3751,  -0.0660,  -1.0929,  -0.0775]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 8.2586, -1.2367, -0.6147,  4.1939,  4.5852, -1.5062, -1.4950, -0.1974,
        -1.0962, -1.0952, -1.4113, -1.0986, -1.0971], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -8.9613,   0.6633, -14.3299,  -6.9710, -12.5618,   0.8945,   0.8552,
          11.4157,  -0.0190,  -0.0191,   0.7198,  -0.0182,  -0.0190]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-1.4415e+01,  7.9253e+00],
        [-9.6175e+00,  6.2276e+00],
        [-1.3025e+01,  7.7043e+00],
        [ 5.9706e+00,  6.7075e+00],
        [-1.0683e+00,  7.1891e+00],
        [-1.1680e+01, -3.4854e+00],
        [-2.3563e+00,  9.7525e-03],
        [-2.6702e-02,  3.9497e+00],
        [ 5.6923e+00,  5.0236e+00],
        [-2.2282e+00,  7.9527e-02],
        [-1.1286e+01, -3.5860e+00],
        [ 9.0040e+00,  1.7628e-01],
        [-1.1280e+01, -3.4237e+00]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  0.9269,  -2.6019,   3.2309,   1.7024,   2.1258,  -0.9592,  -5.3962,
        -11.1483,  -9.6505,  -5.8044,  -2.2029,  -8.0575,   0.7905],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 5.7705e+00,  1.0121e+00,  5.5684e+00,  3.4439e+00,  1.3270e-01,
         -3.3941e+00, -9.0476e-02, -7.6369e+00, -5.3356e+00,  4.8262e-02,
         -5.4173e+00, -4.6624e+00, -4.6233e-01],
        [-4.8983e-02,  3.4660e-03, -1.2035e-01, -1.5816e+00,  1.4957e-01,
         -2.2551e-01, -1.3940e-02, -1.4315e-03,  2.7701e-01, -1.2422e-02,
         -6.1709e-02, -1.4597e+00, -5.0557e-01],
        [-4.9220e-02,  3.4818e-03, -1.1955e-01, -1.5782e+00,  1.5621e-01,
         -2.2781e-01, -1.3986e-02, -1.5790e-03,  2.8025e-01, -1.2479e-02,
         -6.2757e-02, -1.4583e+00, -5.0621e-01],
        [ 4.8714e+00,  2.2360e+00,  3.8434e+00,  1.0144e+00,  3.7274e+00,
          1.8107e+00, -1.7551e-02, -2.9499e+00, -2.6172e+00, -5.7415e-03,
         -5.0762e-01, -4.8925e+00,  3.0191e+00],
        [-5.1427e-02,  2.8579e-03, -1.2056e-01, -1.5788e+00,  1.7022e-01,
         -2.3695e-01, -1.4015e-02, -2.6894e-03,  2.8842e-01, -1.2512e-02,
         -6.6313e-02, -1.4600e+00, -5.1447e-01],
        [-4.9908e-02,  3.6314e-03, -1.1950e-01, -1.5813e+00,  1.6407e-01,
         -2.3298e-01, -1.4024e-02, -1.5186e-03,  2.8585e-01, -1.2543e-02,
         -6.4847e-02, -1.4643e+00, -5.1070e-01],
        [-4.0835e+00, -1.3715e+00, -1.3921e+01,  2.3100e+00,  1.7804e+00,
         -7.6909e+00, -1.4198e+00,  6.5942e-02, -9.0193e+00, -9.9247e-01,
         -1.5739e+00,  2.4156e-01, -1.4104e+01],
        [-4.7121e-02,  4.8725e-03, -1.1887e-01, -1.5887e+00,  1.4417e-01,
         -2.2345e-01, -1.3981e-02,  4.9026e-04,  2.7695e-01, -1.2511e-02,
         -6.1107e-02, -1.4722e+00, -5.0351e-01],
        [-6.5291e+00, -6.3938e+00, -6.2993e+00, -2.1523e+00, -6.1910e+00,
          4.5688e+00,  5.8668e-01, -1.8999e-03,  1.7718e+00,  6.9103e-01,
          1.9945e+00, -2.7930e+00,  6.2296e+00],
        [-5.2933e-02,  2.2325e-03, -1.2029e-01, -1.5732e+00,  1.8315e-01,
         -2.4258e-01, -1.4067e-02, -3.7425e-03,  2.9422e-01, -1.2560e-02,
         -6.8701e-02, -1.4553e+00, -5.1805e-01],
        [-7.6247e+00, -2.4762e+00, -4.1805e+00, -1.6125e+01, -1.1919e+01,
          1.1738e+01, -1.0722e-01, -1.1261e-02, -4.4359e-02, -7.5279e-02,
          1.0357e+01, -8.1802e+00,  1.0921e+01],
        [ 1.9379e+00, -1.9457e-01, -2.2239e-01,  7.2896e-01, -1.5576e-01,
         -8.1326e+00, -1.0860e-01, -2.1450e+00,  6.7878e-01, -9.8974e-02,
          4.5376e-01,  1.4789e+00, -1.1163e+01],
        [-4.7722e-02,  4.4779e-03, -1.1773e-01, -1.5785e+00,  1.5436e-01,
         -2.2595e-01, -1.4036e-02, -2.8540e-04,  2.8053e-01, -1.2562e-02,
         -6.2285e-02, -1.4631e+00, -5.0309e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -4.3525,  -7.1661,  -7.1692, -10.2801,  -7.1664,  -7.1663,  -7.2048,
         -7.1624,  -0.2445,  -7.1699,  -3.4081,  -2.5947,  -7.1710],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 2.6144e+00, -3.9547e-01, -4.0323e-01,  2.8256e+00, -3.8595e-01,
         -3.9577e-01, -6.7796e+00, -3.9597e-01, -4.4903e+01, -3.9210e-01,
         -2.3116e-01, -3.3121e+00, -4.1415e-01],
        [ 2.8302e-01,  8.4128e-03, -9.0648e-04, -1.4184e+00,  4.7068e-02,
          1.6795e-02,  4.1782e-01, -1.2262e-02,  8.3343e-01,  5.2768e-02,
         -6.7282e+00,  1.9566e+00, -3.2731e-02],
        [-3.0960e+01, -6.6821e-02, -6.6835e-02, -2.2164e-01, -6.7049e-02,
         -6.6881e-02, -1.3443e+00, -6.6617e-02, -2.8651e+01, -6.7197e-02,
         -1.6690e-02, -3.9169e+01, -6.6667e-02],
        [-2.9968e+01, -6.0747e-02, -6.0751e-02, -2.8414e-01, -6.0945e-02,
         -6.0796e-02, -1.1620e+00, -6.0572e-02, -2.7395e+01, -6.1068e-02,
         -2.0806e-02, -3.8628e+01, -6.0599e-02],
        [ 9.0305e-01,  1.7834e-01,  1.7194e-01,  7.9865e-02,  2.2549e-01,
          1.9420e-01,  3.6432e+00,  1.5920e-01,  2.7216e+00,  2.3557e-01,
          8.4949e+00,  1.9170e+00,  1.4175e-01]], device='cuda:0'))])
xi:  [723.4612]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 929.5084118438796
W_T_median: 870.2551250009337
W_T_pctile_5: 723.9018035682567
W_T_CVAR_5_pct: 367.76094376131397
Average q (qsum/M+1):  47.32430931829637
Optimal xi:  [723.4612]
Expected(across Rb) median(across samples) p_equity:  0.23160961434865993
obj fun:  tensor(-3305.8311, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 946.50330641104
W_T_median: 707.5512584605915
W_T_pctile_5: -238.22847581189643
W_T_CVAR_5_pct: -367.3344080824147
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -5990.720759463228
Current xi:  [122.2712]
objective value function right now is: -5990.720759463228
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -7018.436707894972
Current xi:  [146.17929]
objective value function right now is: -7018.436707894972
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -8038.148414666287
Current xi:  [169.84981]
objective value function right now is: -8038.148414666287
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -8763.111345123716
Current xi:  [193.14679]
objective value function right now is: -8763.111345123716
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -9843.112209307134
Current xi:  [216.09564]
objective value function right now is: -9843.112209307134
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -10654.072125012073
Current xi:  [238.96777]
objective value function right now is: -10654.072125012073
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -11412.257311337093
Current xi:  [261.28284]
objective value function right now is: -11412.257311337093
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -11974.019697495625
Current xi:  [283.70496]
objective value function right now is: -11974.019697495625
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -12830.936706063054
Current xi:  [305.52472]
objective value function right now is: -12830.936706063054
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -13555.059091926129
Current xi:  [327.55978]
objective value function right now is: -13555.059091926129
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -14157.822997942772
Current xi:  [349.54504]
objective value function right now is: -14157.822997942772
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -14722.105749121567
Current xi:  [370.94516]
objective value function right now is: -14722.105749121567
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -15331.934635868569
Current xi:  [392.5069]
objective value function right now is: -15331.934635868569
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -15818.870659038887
Current xi:  [413.81085]
objective value function right now is: -15818.870659038887
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -16087.748453654021
Current xi:  [434.6659]
objective value function right now is: -16087.748453654021
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -16643.50409442165
Current xi:  [455.47318]
objective value function right now is: -16643.50409442165
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -17230.786393220547
Current xi:  [476.07202]
objective value function right now is: -17230.786393220547
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -17592.14060786385
Current xi:  [496.0045]
objective value function right now is: -17592.14060786385
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -17899.564892809427
Current xi:  [515.6406]
objective value function right now is: -17899.564892809427
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -18247.283766998204
Current xi:  [535.414]
objective value function right now is: -18247.283766998204
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -18494.626725593724
Current xi:  [554.6513]
objective value function right now is: -18494.626725593724
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -18562.437540100946
Current xi:  [574.0087]
objective value function right now is: -18562.437540100946
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -18951.622730102703
Current xi:  [591.42706]
objective value function right now is: -18951.622730102703
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [609.09265]
objective value function right now is: -18894.498025259243
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -19369.41114831295
Current xi:  [625.4195]
objective value function right now is: -19369.41114831295
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -19439.65132541804
Current xi:  [641.4398]
objective value function right now is: -19439.65132541804
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [655.941]
objective value function right now is: -19338.51076840544
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -19602.415081694297
Current xi:  [669.8752]
objective value function right now is: -19602.415081694297
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -19701.41222531642
Current xi:  [680.93427]
objective value function right now is: -19701.41222531642
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -19813.003319112035
Current xi:  [690.88]
objective value function right now is: -19813.003319112035
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [702.30066]
objective value function right now is: -19750.07618432666
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -19935.58466981832
Current xi:  [710.1115]
objective value function right now is: -19935.58466981832
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [718.0165]
objective value function right now is: -19826.31462532803
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -19973.367734640404
Current xi:  [723.9589]
objective value function right now is: -19973.367734640404
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -20002.162070632625
Current xi:  [728.7267]
objective value function right now is: -20002.162070632625
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -20011.041239598184
Current xi:  [730.02747]
objective value function right now is: -20011.041239598184
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -20076.644326316997
Current xi:  [730.7284]
objective value function right now is: -20076.644326316997
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -20077.412553398746
Current xi:  [731.7722]
objective value function right now is: -20077.412553398746
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -20094.241013918563
Current xi:  [733.0318]
objective value function right now is: -20094.241013918563
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [734.2167]
objective value function right now is: -20088.049282532476
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [735.19885]
objective value function right now is: -20090.560190430435
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [736.1829]
objective value function right now is: -20078.78139992548
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [736.93115]
objective value function right now is: -20091.951261440034
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -20110.600126452977
Current xi:  [737.8701]
objective value function right now is: -20110.600126452977
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [738.82666]
objective value function right now is: -20102.349999962145
new min fval from sgd:  -20111.180619072828
new min fval from sgd:  -20112.822875300142
new min fval from sgd:  -20114.604335902048
new min fval from sgd:  -20115.55166254014
new min fval from sgd:  -20116.353952501737
new min fval from sgd:  -20117.059153528877
new min fval from sgd:  -20118.02736262141
new min fval from sgd:  -20118.8693844803
new min fval from sgd:  -20119.776280243612
new min fval from sgd:  -20120.345772322686
new min fval from sgd:  -20120.64852563263
new min fval from sgd:  -20120.670012130704
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [739.39026]
objective value function right now is: -20103.107042876236
new min fval from sgd:  -20120.77836065958
new min fval from sgd:  -20120.82047332595
new min fval from sgd:  -20120.962399438078
new min fval from sgd:  -20121.73536966748
new min fval from sgd:  -20121.904011484094
new min fval from sgd:  -20121.940623857
new min fval from sgd:  -20122.29625605528
new min fval from sgd:  -20122.520959059435
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [740.26324]
objective value function right now is: -20047.304211338454
new min fval from sgd:  -20123.60442279693
new min fval from sgd:  -20124.16281457644
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [740.942]
objective value function right now is: -20121.130791362553
new min fval from sgd:  -20124.227726459434
new min fval from sgd:  -20124.62794750214
new min fval from sgd:  -20124.864170001427
new min fval from sgd:  -20125.56731216244
new min fval from sgd:  -20125.91970935774
new min fval from sgd:  -20126.338827522086
new min fval from sgd:  -20126.676116707706
new min fval from sgd:  -20126.842337270085
new min fval from sgd:  -20127.073975076593
new min fval from sgd:  -20127.23286806889
new min fval from sgd:  -20127.476370320604
new min fval from sgd:  -20127.776859827467
new min fval from sgd:  -20128.143564929393
new min fval from sgd:  -20128.550543421916
new min fval from sgd:  -20128.93717575914
new min fval from sgd:  -20129.243541109132
new min fval from sgd:  -20129.562911163055
new min fval from sgd:  -20129.770936227105
new min fval from sgd:  -20129.91255854667
new min fval from sgd:  -20129.955743600003
new min fval from sgd:  -20129.966338140362
new min fval from sgd:  -20129.983848461663
new min fval from sgd:  -20129.994285040368
new min fval from sgd:  -20130.060448171484
new min fval from sgd:  -20130.102991833486
new min fval from sgd:  -20130.235078007743
new min fval from sgd:  -20130.390772940224
new min fval from sgd:  -20130.452159197164
new min fval from sgd:  -20130.476834800178
new min fval from sgd:  -20130.548583581574
new min fval from sgd:  -20130.637836767844
new min fval from sgd:  -20130.768840202585
new min fval from sgd:  -20130.948750892527
new min fval from sgd:  -20131.129048283798
new min fval from sgd:  -20131.307819416365
new min fval from sgd:  -20131.467214672288
new min fval from sgd:  -20131.510364044123
new min fval from sgd:  -20131.66035346861
new min fval from sgd:  -20131.83567636265
new min fval from sgd:  -20131.932166645012
new min fval from sgd:  -20131.965106923944
new min fval from sgd:  -20131.969736355924
new min fval from sgd:  -20132.042206796967
new min fval from sgd:  -20132.094986128403
new min fval from sgd:  -20132.211139345476
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [741.20215]
objective value function right now is: -20131.9870880823
new min fval from sgd:  -20132.382971308427
new min fval from sgd:  -20132.516698299685
new min fval from sgd:  -20132.65382904963
new min fval from sgd:  -20132.770353579745
new min fval from sgd:  -20132.87737452034
new min fval from sgd:  -20132.893789790265
new min fval from sgd:  -20132.90429026355
new min fval from sgd:  -20132.93715810345
new min fval from sgd:  -20133.017781147442
new min fval from sgd:  -20133.123585164034
new min fval from sgd:  -20133.1849074999
new min fval from sgd:  -20133.200167050043
new min fval from sgd:  -20133.247705279402
new min fval from sgd:  -20133.362479496034
new min fval from sgd:  -20133.49189877506
new min fval from sgd:  -20133.523061092223
new min fval from sgd:  -20133.57548623861
new min fval from sgd:  -20133.71107174793
new min fval from sgd:  -20133.803140212247
new min fval from sgd:  -20133.88822243068
new min fval from sgd:  -20134.01422555342
new min fval from sgd:  -20134.1786591576
new min fval from sgd:  -20134.26675068996
new min fval from sgd:  -20134.342829583296
new min fval from sgd:  -20134.354420602762
new min fval from sgd:  -20134.379076656027
new min fval from sgd:  -20134.420931045726
new min fval from sgd:  -20134.48284369591
new min fval from sgd:  -20134.544276338984
new min fval from sgd:  -20134.55052575709
new min fval from sgd:  -20134.60511248457
new min fval from sgd:  -20134.802083355225
new min fval from sgd:  -20134.979327956167
new min fval from sgd:  -20135.127208849983
new min fval from sgd:  -20135.238257296343
new min fval from sgd:  -20135.33247163057
new min fval from sgd:  -20135.430268817992
new min fval from sgd:  -20135.59071965766
new min fval from sgd:  -20135.767248257947
new min fval from sgd:  -20135.974110117793
new min fval from sgd:  -20136.213201897794
new min fval from sgd:  -20136.408668782748
new min fval from sgd:  -20136.530456056873
new min fval from sgd:  -20136.64768133004
new min fval from sgd:  -20136.731231893307
new min fval from sgd:  -20136.766973295755
new min fval from sgd:  -20136.7826903254
new min fval from sgd:  -20136.795916741376
new min fval from sgd:  -20136.83692602687
new min fval from sgd:  -20136.846843811414
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [741.39124]
objective value function right now is: -20134.54562970812
min fval:  -20136.846843811414
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.1285, -0.1590],
        [ 0.1285, -0.1590],
        [ 0.1285, -0.1590],
        [ 0.1285, -0.1590],
        [ 0.1285, -0.1590],
        [ 0.1285, -0.1590],
        [ 0.1285, -0.1590],
        [ 0.1285, -0.1590],
        [ 0.1285, -0.1590],
        [ 0.1285, -0.1590],
        [ 0.1285, -0.1590],
        [ 0.1285, -0.1590],
        [ 0.1285, -0.1590]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([0.2281, 0.2281, 0.2281, 0.2281, 0.2281, 0.2281, 0.2281, 0.2281, 0.2281,
        0.2281, 0.2281, 0.2281, 0.2281], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102,
         0.2102, 0.2102, 0.2102, 0.2102],
        [0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102,
         0.2102, 0.2102, 0.2102, 0.2102],
        [0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102,
         0.2102, 0.2102, 0.2102, 0.2102],
        [0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102,
         0.2102, 0.2102, 0.2102, 0.2102],
        [0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102,
         0.2102, 0.2102, 0.2102, 0.2102],
        [0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102,
         0.2102, 0.2102, 0.2102, 0.2102],
        [0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102,
         0.2102, 0.2102, 0.2102, 0.2102],
        [0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102,
         0.2102, 0.2102, 0.2102, 0.2102],
        [0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102,
         0.2102, 0.2102, 0.2102, 0.2102],
        [0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102,
         0.2102, 0.2102, 0.2102, 0.2102],
        [0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102,
         0.2102, 0.2102, 0.2102, 0.2102],
        [0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102,
         0.2102, 0.2102, 0.2102, 0.2102],
        [0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102, 0.2102,
         0.2102, 0.2102, 0.2102, 0.2102]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([0.3492, 0.3492, 0.3492, 0.3492, 0.3492, 0.3492, 0.3492, 0.3492, 0.3492,
        0.3492, 0.3492, 0.3492, 0.3492], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.7160, -1.7160, -1.7160, -1.7160, -1.7160, -1.7160, -1.7160, -1.7160,
         -1.7160, -1.7160, -1.7160, -1.7160, -1.7160]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -5.6305,   4.4407],
        [ -9.3305,  -2.5562],
        [-11.3981,  13.6544],
        [-10.4516,   6.3928],
        [  0.2270,   7.8833],
        [-11.9213,  -3.8221],
        [ -7.2805,   6.3111],
        [  3.9887,   6.5798],
        [ -7.5578,   5.8690],
        [ 11.5927,   4.6443],
        [ -7.4142,   5.6830],
        [  9.9202,  -0.6988],
        [-22.2315,   4.1079]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  2.7161,  -0.2380,   3.3461,  -3.8907,  -3.1763,  -1.7433,  -6.6245,
          1.8740, -11.5902,  -1.7644, -10.9331, -10.7187,   0.3230],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-8.3563e-01, -1.2375e+00, -6.6125e-01, -1.5435e-01,  6.7757e-01,
         -2.4488e-01, -1.4084e-01,  3.3326e-02, -1.9128e-01, -3.9119e+00,
         -1.9113e-01, -1.5069e+00, -4.4987e-01],
        [ 1.5365e+00, -6.3572e+00,  3.2637e+00,  3.3043e+00, -3.3491e+00,
         -5.1906e+00,  2.7856e+00,  2.2577e+00, -2.3768e+00, -2.7673e-01,
         -1.4446e+00,  3.0482e+00, -4.9266e-01],
        [-8.4069e-01, -6.2750e-01, -3.1252e-01,  3.0696e-03, -2.0365e-03,
         -1.3049e-01,  5.8606e-03, -1.6958e+00,  3.9869e-03, -3.7901e+00,
          3.9822e-03, -3.3422e-01, -9.7911e-02],
        [-1.2653e+01, -5.7466e+00, -2.7815e-01, -2.6602e-01,  5.0156e+00,
          1.7608e+00, -2.3660e-01,  3.8225e+00, -3.4948e-01, -3.4843e+00,
         -3.4909e-01, -1.5093e+00, -7.4307e-02],
        [-4.2516e+00,  6.2674e+00, -1.2469e+01, -7.3971e+00,  3.9651e+00,
          5.4620e+00, -2.1669e+00, -2.8252e+00, -2.7217e-03, -3.0149e+00,
         -5.1831e-03, -2.7546e+00,  3.7107e+00],
        [-4.0454e-01,  7.7939e+00, -1.2680e+01, -3.9599e+00, -1.0246e+01,
          1.2060e+01, -2.3770e-01, -1.2480e+01,  3.1414e-02, -9.2247e+00,
          3.5871e-02, -1.4357e+01, -8.6102e-01],
        [-7.2161e-01, -5.4939e+00, -5.4590e+00, -1.5898e-01,  1.3452e+00,
         -6.0588e-01, -1.6587e-01,  2.2766e+00, -2.2360e-01, -5.4373e+00,
         -2.2362e-01,  2.0936e-01,  2.1354e+00],
        [-2.0465e+00, -2.2096e+00, -6.3564e-01, -1.3502e-02, -7.7173e-02,
          3.4449e-01, -3.9408e-03, -2.3009e+00, -2.8328e-03, -6.4254e+00,
         -2.8411e-03,  1.5300e+00, -1.2932e-01],
        [-1.3126e+00, -1.5300e+00, -5.6460e+00,  2.8360e-02,  1.3901e+00,
          1.3635e+00,  4.3940e-03,  1.8952e+00, -1.8740e-02, -4.5781e+00,
         -1.8663e-02, -5.8391e-01,  9.1371e-01],
        [-2.0940e-01, -8.3606e+00,  6.1559e+00,  1.1556e+00, -5.8793e+00,
         -4.5156e+00,  7.5488e-01,  5.2130e+00, -7.2138e+00, -1.7503e+00,
         -5.4522e+00, -1.4678e+01,  1.2988e+01],
        [-1.9430e+01,  4.2527e+00, -2.1062e-03,  1.6753e-03, -2.4350e-02,
          9.4857e+00, -1.9702e-03, -3.4836e+01,  3.0213e-05, -6.4887e+00,
         -1.2031e-06, -1.2592e+01,  5.2337e-03],
        [-3.0683e+01, -1.2947e+01, -7.3239e+00, -1.4169e-02, -4.9136e+00,
         -1.3462e+01, -1.2616e-02,  8.7995e-01, -1.7264e-03, -4.1544e+00,
         -2.5797e-03,  7.4834e-01, -2.9800e-02],
        [-1.1974e+01, -2.3568e+00, -1.0006e+00, -3.0155e-01,  4.7436e+00,
          6.6608e-01, -2.7108e-01,  3.7643e+00, -3.8289e-01, -4.3198e+00,
         -3.8249e-01, -2.1347e+00, -6.5525e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-7.1076, -3.4611, -7.4979, -5.2613,  2.2424,  0.1528, -4.9056, -6.2340,
        -6.7562, -3.6629,  0.1675,  0.1356, -3.9343], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-8.9291e-01, -1.9790e-01, -5.3933e-01, -5.6503e+00, -3.1188e+01,
         -1.1970e+01, -9.8106e+00, -2.3707e-01, -7.6266e+00,  3.8605e+00,
         -3.1032e-02, -2.0413e+01, -3.3718e+00],
        [ 1.0931e+00,  1.1389e+00,  1.1992e-01, -2.5392e+00,  9.8825e-01,
         -7.0529e+00,  1.7899e+00,  2.7713e+00,  1.7237e+00, -6.8909e-01,
         -2.0885e+00,  3.4471e+00, -3.7604e+00],
        [-1.6186e-02, -6.2944e+01, -2.9532e-03, -2.8577e-01, -4.5133e+01,
         -6.9304e-02, -7.8848e-02, -2.9464e-03, -2.3949e-02, -2.9110e+01,
          1.0638e-04, -6.1896e+00, -4.3348e-01],
        [-9.3315e-03, -6.3726e+01, -3.8543e-03, -2.7395e-01, -4.4359e+01,
         -5.9363e-02, -6.2928e-02, -5.0007e-03, -1.7969e-02, -2.9786e+01,
         -5.8847e-04, -5.4517e+00, -3.9162e-01],
        [ 1.4787e+00,  1.1951e+00, -1.1578e-01,  4.8175e+00,  2.6342e+00,
          8.7048e+00,  2.4178e+00,  9.1120e-01,  2.3638e+00,  5.3917e-01,
          5.0897e+00,  1.2389e+00,  4.4147e+00]], device='cuda:0'))])
xi:  [741.37415]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1122.8408872266496
W_T_median: 1055.875833095798
W_T_pctile_5: 743.4071158832477
W_T_CVAR_5_pct: 381.06029330664256
Average q (qsum/M+1):  35.0
Optimal xi:  [741.37415]
Expected(across Rb) median(across samples) p_equity:  0.15993515650431314
obj fun:  tensor(-20136.8468, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
