Starting at: 
19-06-23_14:41

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.005383     0.031411
192608                    0.0319              0.0561  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0355  ...     0.005323    -0.028996
192611                   -0.0038              0.0294  ...     0.005303     0.028554

[5 rows x 6 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.043289    -0.036240
202209                   -0.0955             -0.0871  ...    -0.050056    -0.091324
202210                    0.0883              0.1486  ...    -0.014968     0.077403
202211                   -0.0076              0.0462  ...     0.040789     0.052365
202212                   -0.0457             -0.0499  ...    -0.018566    -0.057116

[5 rows x 6 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'CPI_nom_ret_ind', 'T30_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Value_Hi30_real_ret
T30_real_ret             1.000000  ...             0.018239
B10_real_ret             0.351722  ...             0.031301
VWD_real_ret             0.068448  ...             0.909335
Size_Lo30_real_ret       0.014412  ...             0.908542
Value_Hi30_real_ret      0.018239  ...             1.000000

[5 rows x 5 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 199201
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       5       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       5           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 5)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 833.9454981193105
W_T_median: 579.7709853217282
W_T_pctile_5: -323.51989722579935
W_T_CVAR_5_pct: -452.446956541955
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1783.3091419163338
Current xi:  [77.48318]
objective value function right now is: -1783.3091419163338
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1787.4701727948445
Current xi:  [54.87011]
objective value function right now is: -1787.4701727948445
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1790.4507204274812
Current xi:  [32.544678]
objective value function right now is: -1790.4507204274812
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1791.3440283529685
Current xi:  [9.965424]
objective value function right now is: -1791.3440283529685
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1794.005721906826
Current xi:  [-11.062469]
objective value function right now is: -1794.005721906826
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1795.728908631275
Current xi:  [-31.51677]
objective value function right now is: -1795.728908631275
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1796.3343784152592
Current xi:  [-51.61153]
objective value function right now is: -1796.3343784152592
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1797.8545562222432
Current xi:  [-72.2115]
objective value function right now is: -1797.8545562222432
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1798.695673485023
Current xi:  [-92.63186]
objective value function right now is: -1798.695673485023
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1800.0849488817416
Current xi:  [-113.08635]
objective value function right now is: -1800.0849488817416
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-133.20253]
objective value function right now is: -1799.8555801666962
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1801.8977039406363
Current xi:  [-153.26334]
objective value function right now is: -1801.8977039406363
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.815829681113
Current xi:  [-173.16798]
objective value function right now is: -1802.815829681113
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1802.9706616358421
Current xi:  [-192.62738]
objective value function right now is: -1802.9706616358421
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1803.2891279390026
Current xi:  [-211.42177]
objective value function right now is: -1803.2891279390026
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1804.43776439172
Current xi:  [-229.83704]
objective value function right now is: -1804.43776439172
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1805.1266783997753
Current xi:  [-248.27313]
objective value function right now is: -1805.1266783997753
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1805.3152305954304
Current xi:  [-264.98856]
objective value function right now is: -1805.3152305954304
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1805.9066333188725
Current xi:  [-281.02515]
objective value function right now is: -1805.9066333188725
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-295.96454]
objective value function right now is: -1805.7504905930894
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1805.9270711788447
Current xi:  [-309.81064]
objective value function right now is: -1805.9270711788447
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-322.45767]
objective value function right now is: -1805.8850537928092
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1805.9313802615818
Current xi:  [-332.1581]
objective value function right now is: -1805.9313802615818
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1806.1502371298732
Current xi:  [-340.55103]
objective value function right now is: -1806.1502371298732
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-347.35233]
objective value function right now is: -1805.8917417964024
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-351.3964]
objective value function right now is: -1805.8101227758912
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-354.44986]
objective value function right now is: -1805.7087426643714
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-356.39346]
objective value function right now is: -1806.0108829870298
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-356.8267]
objective value function right now is: -1805.5278540354443
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-358.46747]
objective value function right now is: -1806.023026403695
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1806.2306163369308
Current xi:  [-358.69522]
objective value function right now is: -1806.2306163369308
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1806.4469287682264
Current xi:  [-357.32898]
objective value function right now is: -1806.4469287682264
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-356.98975]
objective value function right now is: -1806.0404320958653
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-356.85187]
objective value function right now is: -1806.069032943261
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-357.3472]
objective value function right now is: -1806.0308081155383
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1806.4763349036918
Current xi:  [-356.94028]
objective value function right now is: -1806.4763349036918
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1806.516606643073
Current xi:  [-356.72052]
objective value function right now is: -1806.516606643073
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-356.77884]
objective value function right now is: -1806.4803124872321
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-356.55594]
objective value function right now is: -1806.2825967085296
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1806.52244790488
Current xi:  [-356.48978]
objective value function right now is: -1806.52244790488
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-356.114]
objective value function right now is: -1806.167530448173
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-356.23776]
objective value function right now is: -1806.4096544627726
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1806.5768786680899
Current xi:  [-356.16116]
objective value function right now is: -1806.5768786680899
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-355.85886]
objective value function right now is: -1806.5521660235818
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-355.82968]
objective value function right now is: -1806.4925170784877
new min fval from sgd:  -1806.57775375037
new min fval from sgd:  -1806.5810355108363
new min fval from sgd:  -1806.5829335106475
new min fval from sgd:  -1806.5867952177762
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-355.41324]
objective value function right now is: -1806.4583798068595
new min fval from sgd:  -1806.5891782070012
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-355.30518]
objective value function right now is: -1806.5243860086434
new min fval from sgd:  -1806.5895417533848
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-354.86658]
objective value function right now is: -1806.5193494615055
new min fval from sgd:  -1806.5899416733905
new min fval from sgd:  -1806.589994778767
new min fval from sgd:  -1806.5903599165774
new min fval from sgd:  -1806.5918067119792
new min fval from sgd:  -1806.5922062170876
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-354.8018]
objective value function right now is: -1806.5716082307797
new min fval from sgd:  -1806.5939617780498
new min fval from sgd:  -1806.594344889981
new min fval from sgd:  -1806.5947802303597
new min fval from sgd:  -1806.5948987116224
new min fval from sgd:  -1806.5951599910632
new min fval from sgd:  -1806.5957692037427
new min fval from sgd:  -1806.59577182398
new min fval from sgd:  -1806.5960434647072
new min fval from sgd:  -1806.5963158048944
new min fval from sgd:  -1806.5964602205047
new min fval from sgd:  -1806.5970293573355
new min fval from sgd:  -1806.5974930425532
new min fval from sgd:  -1806.597623996808
new min fval from sgd:  -1806.5983905469352
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-354.72614]
objective value function right now is: -1806.585002769559
min fval:  -1806.5983905469352
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.0861, -0.0648],
        [ 0.0861, -0.0648],
        [ 0.0861, -0.0648],
        [ 0.0861, -0.0648],
        [ 0.0861, -0.0648],
        [ 0.0861, -0.0648],
        [ 0.0861, -0.0648],
        [ 0.0861, -0.0648],
        [ 0.0861, -0.0648],
        [ 0.0861, -0.0648],
        [ 0.0861, -0.0648],
        [ 0.0861, -0.0648],
        [ 0.0861, -0.0648]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([0.1818, 0.1818, 0.1818, 0.1818, 0.1818, 0.1818, 0.1818, 0.1818, 0.1818,
        0.1818, 0.1818, 0.1818, 0.1818], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783,
         0.1783, 0.1783, 0.1783, 0.1783],
        [0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783,
         0.1783, 0.1783, 0.1783, 0.1783],
        [0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783,
         0.1783, 0.1783, 0.1783, 0.1783],
        [0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783,
         0.1783, 0.1783, 0.1783, 0.1783],
        [0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783,
         0.1783, 0.1783, 0.1783, 0.1783],
        [0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783,
         0.1783, 0.1783, 0.1783, 0.1783],
        [0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783,
         0.1783, 0.1783, 0.1783, 0.1783],
        [0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783,
         0.1783, 0.1783, 0.1783, 0.1783],
        [0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783,
         0.1783, 0.1783, 0.1783, 0.1783],
        [0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783,
         0.1783, 0.1783, 0.1783, 0.1783],
        [0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783,
         0.1783, 0.1783, 0.1783, 0.1783],
        [0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783,
         0.1783, 0.1783, 0.1783, 0.1783],
        [0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783,
         0.1783, 0.1783, 0.1783, 0.1783]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([0.3184, 0.3184, 0.3184, 0.3184, 0.3184, 0.3184, 0.3184, 0.3184, 0.3184,
        0.3184, 0.3184, 0.3184, 0.3184], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[1.3872, 1.3872, 1.3872, 1.3872, 1.3872, 1.3872, 1.3872, 1.3872, 1.3872,
         1.3872, 1.3872, 1.3872, 1.3872]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-5.8051, -5.5290],
        [-0.9669, -0.5352],
        [-1.0835,  0.8631],
        [11.2842,  2.4801],
        [ 0.6604, -0.5605],
        [ 5.9546, -9.8136],
        [-5.8430, -5.5879],
        [10.0792, -0.2151],
        [ 9.6758,  0.4479],
        [-5.2361, -4.7337],
        [-1.3349,  4.3631],
        [ 1.8320,  9.3373],
        [-2.8228, -8.6787]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-5.2256, -3.7424, -1.9111, -1.3514,  2.8704, -7.6673, -5.2730, -9.6621,
        -4.9817, -4.6484, -0.8200,  5.7440, -7.2237], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 3.9425e-01, -2.4971e-02,  1.9282e-02,  9.4432e-01, -7.3067e-01,
          2.6579e+00,  3.9278e-01,  3.2589e+00, -5.1815e-01,  3.8782e-01,
          1.0706e-01, -4.4950e+00,  2.0072e+00],
        [-2.2729e-01, -2.2886e-01, -3.9291e-02, -6.9741e-01, -1.0095e+00,
         -7.4083e-01, -2.2714e-01, -3.7570e-01, -5.4092e-01, -2.2912e-01,
         -6.4754e-02, -5.0675e-01, -2.8481e-01],
        [-2.2729e-01, -2.2886e-01, -3.9291e-02, -6.9741e-01, -1.0095e+00,
         -7.4083e-01, -2.2714e-01, -3.7570e-01, -5.4092e-01, -2.2912e-01,
         -6.4754e-02, -5.0675e-01, -2.8481e-01],
        [ 1.3034e+00,  4.5776e+00, -1.6300e-01, -2.0859e+00, -2.1064e+00,
         -1.2449e+00,  1.2882e+00,  3.5045e+00, -1.4807e+00,  1.6110e+00,
          9.4057e-02, -8.2298e-01, -4.9044e-01],
        [-3.7856e+00, -4.9756e-01, -1.0184e-01,  6.4274e+00, -1.4505e-01,
          5.8163e+00, -3.8158e+00,  8.3906e-01,  3.7380e+00, -3.2225e+00,
         -2.1610e-01, -1.6508e+00, -2.4006e+00],
        [ 4.9615e+00,  8.3920e-02, -4.2082e-01, -8.0514e+00, -3.9228e+00,
          7.9422e-01,  5.0203e+00, -2.3230e+00, -6.4369e+00,  3.9243e+00,
         -1.2107e-01, -1.4761e+00,  3.7839e+00],
        [ 7.6761e-01,  6.8521e-01,  1.1138e-01,  5.6658e-01, -6.7672e-02,
          2.4849e+00,  7.4781e-01, -4.2275e+00, -3.3994e+00,  9.7136e-01,
         -9.2669e-02, -4.0398e+00,  2.3752e+00],
        [-3.1745e+00, -5.9056e-01,  1.3651e-01,  6.2541e-01,  2.5977e+00,
         -4.4827e+00, -3.2055e+00,  8.8777e+00,  5.1686e+00, -2.5912e+00,
          3.7054e-01,  6.9538e+00, -6.8730e+00],
        [ 1.1330e+00,  4.3929e+00, -1.4160e-01, -2.1607e+00, -1.8644e+00,
         -1.0039e+00,  1.1173e+00,  3.2464e+00, -1.6615e+00,  1.4429e+00,
          8.3608e-02, -7.7558e-01, -4.8037e-01],
        [-2.2731e-01, -2.2888e-01, -3.9287e-02, -6.9751e-01, -1.0097e+00,
         -7.4091e-01, -2.2717e-01, -3.7572e-01, -5.4097e-01, -2.2915e-01,
         -6.4744e-02, -5.0682e-01, -2.8485e-01],
        [-9.2685e-01, -3.5876e+00,  8.0351e-02,  2.0656e+00,  1.3321e+00,
          4.6931e-01, -9.1067e-01, -2.3884e+00,  1.7303e+00, -1.2248e+00,
         -5.7504e-02,  7.0513e-01,  4.7445e-01],
        [-3.9585e+00, -1.6952e+00,  8.7466e-04,  2.1575e+00,  1.0952e-01,
          6.8454e+00, -3.9992e+00,  1.8780e+00,  5.7343e+00, -3.3706e+00,
          2.8811e-01, -4.8997e+00, -1.6079e+00],
        [ 7.5162e-01,  1.4043e-01, -6.6235e-02,  3.5772e+00, -6.9149e-01,
          1.9838e+00,  7.4984e-01,  9.0726e+00,  9.6454e-01,  7.3318e-01,
         -1.3545e+00, -4.9005e+00,  4.2727e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-0.8372, -1.0335, -1.0335, -2.1752, -0.5135, -3.8616,  0.1205,  1.9045,
        -1.9454, -1.0337,  1.4073, -0.4644, -0.8390], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 0.2248,  0.0251,  0.0251, -2.4215, -1.8390,  7.2289,  0.2028,  5.2823,
         -1.6921,  0.0233,  1.4132, -1.7804, -0.2889],
        [-1.0789, -0.0408, -0.0408, -0.8208, -1.6113, -0.8339, -1.0147, -1.5879,
         -0.8217, -0.0407, -1.6564, -1.5528, -1.8763],
        [-1.2233, -0.0507, -0.0507, -0.9411, -1.7202, -0.9681, -1.1367, -1.7161,
         -0.9422, -0.0507, -1.7813, -1.6566, -2.0900],
        [-1.2212, -0.0519, -0.0519, -0.9235, -1.6975, -0.9705, -1.1342, -1.6987,
         -0.9252, -0.0518, -1.7735, -1.6442, -2.1116],
        [ 0.6813,  0.0099,  0.0099,  3.3566,  2.8894, -5.5511,  0.9932, -3.7057,
          2.6515,  0.0081, -0.3373,  2.6118,  1.4429]], device='cuda:0'))])
xi:  [-354.7536]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 74.45561045547473
W_T_median: 81.08350331891498
W_T_pctile_5: -353.54351061707115
W_T_CVAR_5_pct: -515.555648048173
Average q (qsum/M+1):  59.108914283014116
Optimal xi:  [-354.7536]
Expected(across Rb) median(across samples) p_equity:  3.050812699711969e-05
obj fun:  tensor(-1806.5984, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 833.9454981193105
W_T_median: 579.7709853217282
W_T_pctile_5: -323.51989722579935
W_T_CVAR_5_pct: -452.446956541955
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1797.9635088044142
Current xi:  [108.87635]
objective value function right now is: -1797.9635088044142
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [123.591095]
objective value function right now is: -1796.6514287741502
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1800.298661007868
Current xi:  [137.73415]
objective value function right now is: -1800.298661007868
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1801.2536566915821
Current xi:  [150.49988]
objective value function right now is: -1801.2536566915821
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1801.256913916748
Current xi:  [162.39882]
objective value function right now is: -1801.256913916748
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1801.6935573848723
Current xi:  [174.21568]
objective value function right now is: -1801.6935573848723
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1801.7708607978284
Current xi:  [185.93538]
objective value function right now is: -1801.7708607978284
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1801.8675292626938
Current xi:  [196.01285]
objective value function right now is: -1801.8675292626938
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.44429]
objective value function right now is: -1801.5652154703293
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.24467]
objective value function right now is: -1801.2471372917475
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.767531701382
Current xi:  [220.11714]
objective value function right now is: -1802.767531701382
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [226.76443]
objective value function right now is: -1802.5488792558124
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.8541421589075
Current xi:  [233.60184]
objective value function right now is: -1802.8541421589075
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [239.64145]
objective value function right now is: -1800.8897166081456
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [245.31186]
objective value function right now is: -1802.5920432896144
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1803.7026076210122
Current xi:  [250.02754]
objective value function right now is: -1803.7026076210122
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [254.90106]
objective value function right now is: -1803.5670521738882
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [259.1958]
objective value function right now is: -1803.5956898471138
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [262.86435]
objective value function right now is: -1803.3354022843569
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1803.805213394623
Current xi:  [266.52237]
objective value function right now is: -1803.805213394623
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [270.60825]
objective value function right now is: -1803.1786896970214
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1804.0269374714517
Current xi:  [274.31058]
objective value function right now is: -1804.0269374714517
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [277.46182]
objective value function right now is: -1803.8347256331529
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [280.4883]
objective value function right now is: -1803.7967599827589
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [283.33557]
objective value function right now is: -1803.8937662091903
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [285.22937]
objective value function right now is: -1803.5303877573265
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [287.34912]
objective value function right now is: -1803.6538744751567
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [288.64972]
objective value function right now is: -1802.8509336642885
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1804.2372462848896
Current xi:  [289.6732]
objective value function right now is: -1804.2372462848896
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [292.2648]
objective value function right now is: -1801.782683907789
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [294.0344]
objective value function right now is: -1803.7521934966378
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [294.94604]
objective value function right now is: -1803.940799181935
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [295.91202]
objective value function right now is: -1803.6723932649368
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [297.20963]
objective value function right now is: -1803.7289744899808
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [298.63358]
objective value function right now is: -1802.4976491194932
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [298.92633]
objective value function right now is: -1804.0076241338975
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [299.0848]
objective value function right now is: -1804.04667831528
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1804.247578984566
Current xi:  [299.3452]
objective value function right now is: -1804.247578984566
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [299.68826]
objective value function right now is: -1804.1607749725515
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1804.4052830738867
Current xi:  [299.96143]
objective value function right now is: -1804.4052830738867
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [300.04324]
objective value function right now is: -1804.39841952703
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [300.48764]
objective value function right now is: -1804.2656850113096
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [300.7302]
objective value function right now is: -1804.242294768325
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [300.76947]
objective value function right now is: -1804.3290360857256
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [300.7781]
objective value function right now is: -1804.190950339372
new min fval from sgd:  -1804.4173494563215
new min fval from sgd:  -1804.4249162722956
new min fval from sgd:  -1804.4270126776803
new min fval from sgd:  -1804.4384100106238
new min fval from sgd:  -1804.4457090647388
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [301.15906]
objective value function right now is: -1804.2784762180206
new min fval from sgd:  -1804.4549247747068
new min fval from sgd:  -1804.4577401046488
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [301.4112]
objective value function right now is: -1804.413207381221
new min fval from sgd:  -1804.4632197203064
new min fval from sgd:  -1804.4654165092293
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [301.67953]
objective value function right now is: -1804.0642431373506
new min fval from sgd:  -1804.465654470134
new min fval from sgd:  -1804.4678130083173
new min fval from sgd:  -1804.469186004
new min fval from sgd:  -1804.4696729970026
new min fval from sgd:  -1804.4698808866597
new min fval from sgd:  -1804.4708988007492
new min fval from sgd:  -1804.4720252819366
new min fval from sgd:  -1804.4723475407675
new min fval from sgd:  -1804.4734826214685
new min fval from sgd:  -1804.4751563792527
new min fval from sgd:  -1804.476401546368
new min fval from sgd:  -1804.4766965151596
new min fval from sgd:  -1804.4770919947675
new min fval from sgd:  -1804.4774723704784
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [301.9467]
objective value function right now is: -1804.4774723704784
new min fval from sgd:  -1804.4776699855508
new min fval from sgd:  -1804.477898267346
new min fval from sgd:  -1804.4780811040687
new min fval from sgd:  -1804.4786633876613
new min fval from sgd:  -1804.4787748541282
new min fval from sgd:  -1804.4788810902255
new min fval from sgd:  -1804.4790598907298
new min fval from sgd:  -1804.4797440357559
new min fval from sgd:  -1804.4803220240087
new min fval from sgd:  -1804.4814462209074
new min fval from sgd:  -1804.4817131592094
new min fval from sgd:  -1804.482367099931
new min fval from sgd:  -1804.4839229645497
new min fval from sgd:  -1804.48401517925
new min fval from sgd:  -1804.4850811368526
new min fval from sgd:  -1804.4851718953557
new min fval from sgd:  -1804.4858549125534
new min fval from sgd:  -1804.4868170730438
new min fval from sgd:  -1804.487047441318
new min fval from sgd:  -1804.487064678716
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [301.96664]
objective value function right now is: -1804.4744952104013
min fval:  -1804.487064678716
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -6.4632,  -6.2175],
        [ -0.0279,   0.8484],
        [  5.3447,   8.3389],
        [  2.3407,   4.7271],
        [ -2.9678,  -7.5227],
        [-17.1092,   6.9248],
        [  0.0286,   0.6708],
        [  5.9816,   7.3934],
        [ -0.0413,   0.8848],
        [  3.8432,   6.5897],
        [  4.9603,   7.5291],
        [  4.8174,   7.3694],
        [ -6.9129,  -8.0925]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-2.9009, -1.9007,  2.6539, -0.3876, -4.2591,  6.4593, -1.7607,  2.5132,
        -1.9395,  0.9113,  2.0607,  1.8900, -3.4479], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.1756, -0.0119, -0.1691, -0.0154, -0.1232, -0.2549, -0.0150, -0.2263,
         -0.0113, -0.0493, -0.1369, -0.1238, -0.2229],
        [-4.1972,  0.0414,  3.9748,  0.9609, -4.5384,  5.2507, -0.0818,  3.0463,
          0.1181,  1.9313,  2.7023,  2.4372, -6.3250],
        [-0.1756, -0.0119, -0.1691, -0.0154, -0.1232, -0.2549, -0.0150, -0.2263,
         -0.0113, -0.0493, -0.1369, -0.1238, -0.2229],
        [ 3.0831,  0.0132, -2.4938, -0.1008,  0.5044, -4.0414, -0.0340, -3.0670,
          0.0414, -0.7363, -2.1695, -1.9935,  4.6688],
        [-3.3782,  0.0687,  3.1798,  0.5923, -3.6630,  4.6413,  0.0346,  2.5148,
          0.0261,  1.3988,  2.1962,  1.9208, -5.3384],
        [-0.1756, -0.0119, -0.1691, -0.0154, -0.1232, -0.2549, -0.0150, -0.2263,
         -0.0113, -0.0493, -0.1369, -0.1238, -0.2229],
        [-0.1756, -0.0119, -0.1691, -0.0154, -0.1232, -0.2549, -0.0150, -0.2263,
         -0.0113, -0.0493, -0.1369, -0.1238, -0.2229],
        [ 4.2720,  0.1843, -3.0741, -0.2496,  1.0245, -4.2169,  0.0721, -3.6903,
          0.1642, -1.1215, -2.6749, -2.6051,  6.1395],
        [-0.1756, -0.0119, -0.1691, -0.0154, -0.1232, -0.2549, -0.0150, -0.2263,
         -0.0113, -0.0493, -0.1369, -0.1238, -0.2229],
        [ 3.3766,  0.0344, -2.6770, -0.1368,  0.5362, -4.1723, -0.0350, -3.2421,
          0.0593, -0.8481, -2.3271, -2.1525,  4.9630],
        [-2.3178,  0.1413,  1.9649,  0.2118, -2.4552,  3.3963,  0.1479,  1.6212,
          0.0519,  0.6774,  1.3301,  1.1699, -3.7681],
        [-0.1754, -0.0119, -0.1704, -0.0155, -0.1258, -0.2535, -0.0151, -0.2289,
         -0.0114, -0.0497, -0.1381, -0.1249, -0.2231],
        [-0.1735, -0.0121, -0.1726, -0.0157, -0.1316, -0.2477, -0.0154, -0.2341,
         -0.0115, -0.0502, -0.1402, -0.1268, -0.2221]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.6407, -2.4388, -0.6407,  0.9716, -2.0385, -0.6407, -0.6407,  1.5383,
        -0.6407,  1.1813, -1.5314, -0.6460, -0.6551], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-0.0184,  7.8501, -0.0184, -3.9445,  5.4284, -0.0184, -0.0184, -5.2504,
         -0.0184, -4.2067,  2.9502, -0.0181, -0.0171]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.7048, -10.5117],
        [ -1.5987,   0.7075],
        [ -1.5986,   0.7071],
        [ -1.5960,   0.6971],
        [ -1.5984,   0.7066],
        [ -1.5985,   0.7066],
        [-10.3743,  -5.3612],
        [ -2.6760,  -0.6250],
        [ -8.2801,   0.6574],
        [ -1.5981,   0.7051],
        [ -9.2819,  -7.5201],
        [ 10.6661,  -0.6408],
        [ -7.6389,  11.3759]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-5.5241, -2.9472, -2.9472, -2.9453, -2.9471, -2.9471, -1.7931, -4.8324,
         3.6325, -2.9468, -4.1677, -9.6573,  6.1681], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-8.4851e-01, -1.0832e-02, -1.0830e-02, -1.0787e-02, -1.0828e-02,
         -1.0828e-02, -1.2455e-01, -1.0794e-01, -4.5583e-01, -1.0821e-02,
         -1.3280e-01, -7.6754e-01, -7.7347e-01],
        [ 3.3713e+00, -1.4097e-02, -1.4139e-02, -1.4904e-02, -1.4188e-02,
         -1.4187e-02,  7.2165e+00, -5.0892e+00,  4.3284e-01, -1.4362e-02,
          5.8072e+00, -1.1305e+01, -1.1635e+01],
        [ 9.1093e+00, -2.0385e-01, -2.0382e-01, -2.0248e-01, -2.0377e-01,
         -2.0377e-01, -1.9783e+00, -3.5925e-02, -3.2498e+00, -2.0365e-01,
         -8.2136e-01,  3.9220e+00, -4.2406e+00],
        [-1.8512e+00,  1.1502e-02,  1.1442e-02,  1.0041e-02,  1.1361e-02,
          1.1363e-02, -1.8448e+00,  4.1366e+00, -2.4718e+00,  1.1127e-02,
          4.8797e-01,  3.2023e+00,  7.2475e+00],
        [-8.4868e-01, -1.0860e-02, -1.0858e-02, -1.0815e-02, -1.0856e-02,
         -1.0856e-02, -1.2469e-01, -1.0787e-01, -4.5630e-01, -1.0849e-02,
         -1.3284e-01, -7.6742e-01, -7.7365e-01],
        [-8.4865e-01, -1.0854e-02, -1.0853e-02, -1.0809e-02, -1.0850e-02,
         -1.0850e-02, -1.2467e-01, -1.0788e-01, -4.5620e-01, -1.0843e-02,
         -1.3283e-01, -7.6745e-01, -7.7362e-01],
        [-8.4868e-01, -1.0859e-02, -1.0857e-02, -1.0814e-02, -1.0855e-02,
         -1.0855e-02, -1.2469e-01, -1.0787e-01, -4.5628e-01, -1.0848e-02,
         -1.3284e-01, -7.6743e-01, -7.7365e-01],
        [-8.4866e-01, -1.0856e-02, -1.0854e-02, -1.0811e-02, -1.0852e-02,
         -1.0852e-02, -1.2467e-01, -1.0788e-01, -4.5623e-01, -1.0845e-02,
         -1.3283e-01, -7.6744e-01, -7.7363e-01],
        [-8.9565e-01,  1.1088e-01,  1.1054e-01,  1.0172e-01,  1.1006e-01,
          1.1007e-01,  2.5014e+00,  8.3029e-02, -1.0595e+01,  1.0873e-01,
          5.0839e+00,  7.7977e+00, -8.2297e-01],
        [-8.4862e-01, -1.0850e-02, -1.0848e-02, -1.0805e-02, -1.0846e-02,
         -1.0846e-02, -1.2464e-01, -1.0789e-01, -4.5613e-01, -1.0839e-02,
         -1.3282e-01, -7.6747e-01, -7.7359e-01],
        [-8.4859e-01, -1.0844e-02, -1.0842e-02, -1.0799e-02, -1.0840e-02,
         -1.0840e-02, -1.2461e-01, -1.0791e-01, -4.5603e-01, -1.0833e-02,
         -1.3282e-01, -7.6749e-01, -7.7355e-01],
        [ 1.3079e+01, -1.7128e-01, -1.7105e-01, -1.6523e-01, -1.7073e-01,
         -1.7074e-01, -8.1357e+00, -1.0932e-01, -3.4859e+00, -1.6984e-01,
         -1.7845e+00,  6.6813e+00, -6.7276e+00],
        [-8.4864e-01, -1.0853e-02, -1.0851e-02, -1.0808e-02, -1.0849e-02,
         -1.0849e-02, -1.2466e-01, -1.0789e-01, -4.5618e-01, -1.0842e-02,
         -1.3283e-01, -7.6745e-01, -7.7361e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.9414, -6.5162, -2.4883,  1.3611, -3.9416, -3.9416, -3.9416, -3.9416,
        -1.4394, -3.9416, -3.9415,  7.0975, -3.9416], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 6.0270e-02, -5.3333e+00,  4.9742e-01,  2.6182e+00,  5.2090e-02,
          5.3706e-02,  5.2403e-02,  5.3194e-02, -8.6936e-02,  5.4982e-02,
          5.6774e-02, -5.3330e-01,  5.4132e-02],
        [-5.2460e-02, -1.8670e-01, -1.5119e+00, -5.0272e+00, -5.2426e-02,
         -5.2433e-02, -5.2428e-02, -5.2431e-02, -3.4116e-01, -5.2438e-02,
         -5.2445e-02, -5.2242e+00, -5.2434e-02],
        [-5.7937e-02, -1.9739e-01, -1.6762e+00, -5.5772e+00, -5.7903e-02,
         -5.7910e-02, -5.7904e-02, -5.7908e-02, -3.1969e-01, -5.7915e-02,
         -5.7922e-02, -5.7572e+00, -5.7911e-02],
        [-5.7441e-02, -1.8703e-01, -1.6776e+00, -5.5233e+00, -5.7404e-02,
         -5.7412e-02, -5.7406e-02, -5.7409e-02, -3.4043e-01, -5.7417e-02,
         -5.7426e-02, -5.7835e+00, -5.7413e-02],
        [ 1.2295e-02,  6.1439e+00,  1.0335e+00, -6.6764e-01,  4.1270e-03,
          5.7410e-03,  4.4399e-03,  5.2301e-03,  1.1388e+00,  7.0152e-03,
          8.8052e-03,  2.1159e+00,  6.1671e-03]], device='cuda:0'))])
xi:  [301.9866]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 479.0427173871805
W_T_median: 444.9693366802909
W_T_pctile_5: 301.9604152220994
W_T_CVAR_5_pct: 64.38468537868552
Average q (qsum/M+1):  57.79386261970766
Optimal xi:  [301.9866]
Expected(across Rb) median(across samples) p_equity:  0.00010778455345861933
obj fun:  tensor(-1804.4871, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 833.9454981193105
W_T_median: 579.7709853217282
W_T_pctile_5: -323.51989722579935
W_T_CVAR_5_pct: -452.446956541955
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1798.0692879083842
Current xi:  [121.9566]
objective value function right now is: -1798.0692879083842
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1806.7722601568546
Current xi:  [145.22557]
objective value function right now is: -1806.7722601568546
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1812.7141202643738
Current xi:  [167.1202]
objective value function right now is: -1812.7141202643738
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1814.3698167563357
Current xi:  [188.5942]
objective value function right now is: -1814.3698167563357
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1818.5616183633563
Current xi:  [209.34337]
objective value function right now is: -1818.5616183633563
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1821.1848799728812
Current xi:  [230.00456]
objective value function right now is: -1821.1848799728812
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1828.3131124984106
Current xi:  [249.58102]
objective value function right now is: -1828.3131124984106
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1832.223268615584
Current xi:  [269.6244]
objective value function right now is: -1832.223268615584
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1836.244933749124
Current xi:  [288.87222]
objective value function right now is: -1836.244933749124
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1839.3392894687895
Current xi:  [308.57852]
objective value function right now is: -1839.3392894687895
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1843.1483363674106
Current xi:  [327.4061]
objective value function right now is: -1843.1483363674106
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1852.3755489167422
Current xi:  [345.99792]
objective value function right now is: -1852.3755489167422
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1857.3504756608272
Current xi:  [363.91302]
objective value function right now is: -1857.3504756608272
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1860.7439098122422
Current xi:  [381.68292]
objective value function right now is: -1860.7439098122422
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1861.3104911625203
Current xi:  [398.32178]
objective value function right now is: -1861.3104911625203
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1864.1676953963292
Current xi:  [414.50134]
objective value function right now is: -1864.1676953963292
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1866.341979474406
Current xi:  [430.64383]
objective value function right now is: -1866.341979474406
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1868.5904005890043
Current xi:  [446.74094]
objective value function right now is: -1868.5904005890043
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1870.3226341368804
Current xi:  [461.94626]
objective value function right now is: -1870.3226341368804
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1871.9475176870212
Current xi:  [476.7504]
objective value function right now is: -1871.9475176870212
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1873.4053432014728
Current xi:  [490.9227]
objective value function right now is: -1873.4053432014728
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1873.5564779697913
Current xi:  [504.2828]
objective value function right now is: -1873.5564779697913
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1876.388684902262
Current xi:  [518.5798]
objective value function right now is: -1876.388684902262
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1876.5421601976784
Current xi:  [531.1927]
objective value function right now is: -1876.5421601976784
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1877.3153601322533
Current xi:  [543.73145]
objective value function right now is: -1877.3153601322533
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1879.0862315910852
Current xi:  [555.6813]
objective value function right now is: -1879.0862315910852
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1879.640414809836
Current xi:  [567.6166]
objective value function right now is: -1879.640414809836
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1880.1741552383314
Current xi:  [578.21674]
objective value function right now is: -1880.1741552383314
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1885.9724824916925
Current xi:  [592.5956]
objective value function right now is: -1885.9724824916925
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [609.2784]
objective value function right now is: -1885.579285191222
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1887.8716698554576
Current xi:  [623.8749]
objective value function right now is: -1887.8716698554576
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [636.0278]
objective value function right now is: -1881.3066381896738
