Starting at: 
09-06-23_17:41

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.005383     0.031411
192608                    0.0319              0.0561  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0355  ...     0.005323    -0.028996
192611                   -0.0038              0.0294  ...     0.005303     0.028554

[5 rows x 6 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.043289    -0.036240
202209                   -0.0955             -0.0871  ...    -0.050056    -0.091324
202210                    0.0883              0.1486  ...    -0.014968     0.077403
202211                   -0.0076              0.0462  ...     0.040789     0.052365
202212                   -0.0457             -0.0499  ...    -0.018566    -0.057116

[5 rows x 6 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'CPI_nom_ret_ind', 'T30_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Value_Hi30_real_ret
T30_real_ret             1.000000  ...             0.018239
B10_real_ret             0.351722  ...             0.031301
VWD_real_ret             0.068448  ...             0.909335
Size_Lo30_real_ret       0.014412  ...             0.908542
Value_Hi30_real_ret      0.018239  ...             1.000000

[5 rows x 5 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       5       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       5           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 5)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -0.5448,   4.9815],
        [-11.2190,   5.0415],
        [ -0.1984,   1.1038],
        [  0.1972,   4.9215],
        [ -0.1984,   1.1038],
        [ -0.1984,   1.1038],
        [ 10.8644,   0.5850],
        [  5.8310,   9.0238],
        [ -0.1984,   1.1038],
        [ -0.1984,   1.1038],
        [ 10.0973,   3.5341],
        [ 11.1774,   0.3604],
        [ -0.1984,   1.1038]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 3.7413,  6.7544, -0.9277,  3.2615, -0.9277, -0.9277, -8.3267,  8.8024,
        -0.9277, -0.9277, -3.1127, -9.2957, -0.9277], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.0147, -0.0299, -0.0201, -0.0128, -0.0201, -0.0201, -0.0418, -0.0975,
         -0.0201, -0.0201, -0.0222, -0.0383, -0.0201],
        [-0.8823, -5.1312,  0.0223, -0.7225,  0.0228,  0.0223, -3.4577, -7.0258,
          0.0223,  0.0223, -1.7175, -4.3522,  0.0239],
        [ 0.5244,  3.3293,  0.0442,  0.3916,  0.0438,  0.0442,  1.9752,  4.4816,
          0.0442,  0.0442,  0.7923,  2.4487,  0.0430],
        [ 0.5656,  3.5762,  0.0348,  0.4268,  0.0344,  0.0348,  2.2155,  4.7905,
          0.0348,  0.0348,  0.9048,  2.7600,  0.0335],
        [-0.0147, -0.0299, -0.0201, -0.0128, -0.0201, -0.0201, -0.0418, -0.0975,
         -0.0201, -0.0201, -0.0222, -0.0383, -0.0201],
        [ 1.1758,  6.0665, -0.0167,  0.9964, -0.0163, -0.0167,  3.9857,  7.9798,
         -0.0167, -0.0167,  1.9411,  5.0626, -0.0154],
        [-0.0147, -0.0299, -0.0201, -0.0128, -0.0201, -0.0201, -0.0418, -0.0975,
         -0.0201, -0.0201, -0.0222, -0.0383, -0.0201],
        [-0.0147, -0.0299, -0.0201, -0.0128, -0.0201, -0.0201, -0.0418, -0.0975,
         -0.0201, -0.0201, -0.0222, -0.0383, -0.0201],
        [-0.0147, -0.0299, -0.0201, -0.0128, -0.0201, -0.0201, -0.0418, -0.0975,
         -0.0201, -0.0201, -0.0222, -0.0383, -0.0201],
        [-0.0147, -0.0299, -0.0201, -0.0128, -0.0201, -0.0201, -0.0418, -0.0975,
         -0.0201, -0.0201, -0.0222, -0.0383, -0.0201],
        [-1.1430, -6.0410,  0.0291, -0.9817,  0.0287,  0.0291, -4.0280, -8.1659,
          0.0291,  0.0291, -2.0099, -5.1625,  0.0280],
        [-0.0147, -0.0299, -0.0201, -0.0128, -0.0201, -0.0201, -0.0418, -0.0975,
         -0.0201, -0.0201, -0.0222, -0.0383, -0.0201],
        [-0.0119, -0.0709, -0.0121, -0.0083, -0.0121, -0.0121, -0.1168, -0.2328,
         -0.0121, -0.0121, -0.0485, -0.1051, -0.0121]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.3521,  4.6399, -3.0141, -3.2108, -0.3521, -5.4275, -0.3521, -0.3521,
        -0.3521, -0.3521,  5.4711, -0.3521, -0.5395], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-5.5984e-03, -8.1214e+00,  4.3754e+00,  4.8556e+00, -5.5983e-03,
          1.0210e+01, -5.5983e-03, -5.5983e-03, -5.5983e-03, -5.5983e-03,
         -1.0122e+01, -5.5984e-03, -6.4945e-02]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-1.5585, -0.5557],
        [ 2.3244,  5.7005],
        [-9.0479,  0.3702],
        [-2.5081, -0.5453],
        [ 7.0797,  2.7386],
        [-3.3911, -6.9716],
        [ 9.6950, -0.3889],
        [ 6.7624,  2.4309],
        [ 7.7130,  3.7511],
        [-4.1180, -5.9741],
        [ 0.1364,  9.0038],
        [-3.5287,  7.4713],
        [-1.5590, -0.5555]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-3.8862,  3.4174,  7.4650, -3.2162,  1.1102, -7.0996, -9.8191,  0.7106,
         2.7241, -6.3967,  8.4295,  6.8838, -3.8842], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.4230, -2.0390,  4.4893, -2.0396, -2.9295,  4.6458, -7.0249, -2.6802,
         -2.1431,  2.2411, -8.1740, -5.9432, -2.4170],
        [ 2.0134,  1.0286, -3.2436,  1.7844,  0.9220, -3.3341,  4.3194,  0.7270,
          0.0414, -1.7689,  4.9092,  2.2053,  2.0134],
        [-1.5496,  0.2222,  5.2814, -0.8335, -4.9875,  2.3637, -5.4553, -4.6167,
         -5.7510,  4.0941,  0.2084,  1.0956, -1.5489],
        [-3.0952,  1.0696, -7.2767, -2.5803,  1.4821, -0.2697,  4.4460,  1.1652,
          1.7369, -0.5340, -0.7383, -4.2227, -3.0854],
        [-0.4033, -0.3434, -0.3804, -0.4047, -0.7349, -0.5067, -0.5866, -0.7230,
         -0.7689, -0.4590, -0.3873, -0.2127, -0.4033],
        [ 0.0640, -0.4337, -7.4795,  0.0672, -0.1682,  0.3304,  7.0196, -0.1720,
         -0.3947, -0.5505, -1.8728, -1.0197,  0.0638],
        [-0.4033, -0.3434, -0.3804, -0.4047, -0.7349, -0.5067, -0.5866, -0.7230,
         -0.7689, -0.4590, -0.3873, -0.2127, -0.4033],
        [ 0.3769, -0.9851, -5.5795,  0.3140,  1.1839,  1.8124,  1.2725,  1.4159,
          1.2871,  0.2946, -4.9656, -1.1548,  0.3770],
        [-0.5148,  0.5716, -0.4047, -0.7070,  3.5430, -2.5209,  0.3070,  3.2351,
          4.0543, -3.5664,  0.7451,  0.3850, -0.5149],
        [-1.0174,  0.6242, -1.0090, -1.1405,  2.6147, -2.1563,  0.9753,  2.4713,
          3.2338, -2.2026,  2.4579,  2.0370, -1.0176],
        [-0.2349,  0.3080,  3.4844, -0.2292, -3.2914, -2.7196, -3.4517, -2.9003,
         -3.4806, -0.9499,  5.0554,  2.5789, -0.2351],
        [-0.4033, -0.3434, -0.3804, -0.4047, -0.7349, -0.5067, -0.5866, -0.7230,
         -0.7689, -0.4590, -0.3873, -0.2127, -0.4033],
        [ 1.7084, -0.3602, -0.8062,  1.6567, -1.7059,  0.0130,  0.6819, -1.6875,
         -1.7966,  0.2635, -0.3659, -0.4405,  1.7083]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 0.3641, -1.0828, -4.2378,  0.9596, -1.3045, -0.9579, -1.3045,  0.3335,
         0.8127,  0.7756, -2.3363, -1.3045, -0.6180], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.9686,  -1.5961,  10.6004,  -0.9046,  -0.0119,  -1.6195,  -0.0119,
          -2.3301,  -6.7529,  -6.7677,  -2.7633,  -0.0119,  -0.9185],
        [ -6.7605,   0.9032,   0.8470,   0.0524,   0.0164,   1.2380,   0.0166,
           0.3538,  -0.2095,   1.0015,   1.9648,   0.0166,  -0.5257],
        [ -0.9154,  -2.4808,  -0.5196,  -0.6984,  -0.1407,  -1.4504,  -0.1407,
          -1.8519,  -3.5169,  -3.4975,  -0.2657,  -0.1407,  -1.5086],
        [ -0.8492,  -2.3344,  -0.5620,  -0.6656,  -0.1234,  -1.3559,  -0.1234,
          -1.7402,  -3.3198,  -3.2989,  -0.2234,  -0.1234,  -1.4184],
        [  6.9672,   0.0654, -10.1260,   0.6217,   0.0623,   0.2123,   0.0624,
           0.9620,   1.6335,  -0.2842,  -1.1921,   0.0624,   1.1050]],
       device='cuda:0'))])
loaded xi:  -105.932106
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 946.50330641104
W_T_median: 707.5512584605915
W_T_pctile_5: -238.22847581189643
W_T_CVAR_5_pct: -367.3344080824147
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1800.3695081949584
Current xi:  [-120.42449]
objective value function right now is: -1800.3695081949584
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [-140.6419]
objective value function right now is: -1800.08680717689
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1800.7107213799595
Current xi:  [-147.60362]
objective value function right now is: -1800.7107213799595
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1800.8774217030593
Current xi:  [-157.37369]
objective value function right now is: -1800.8774217030593
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-173.0582]
objective value function right now is: -1800.5986388286663
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-178.63588]
objective value function right now is: -1800.6518779409435
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1800.941190724732
Current xi:  [-179.55545]
objective value function right now is: -1800.941190724732
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-179.47699]
objective value function right now is: -1800.139094958394
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-179.90346]
objective value function right now is: -1800.5205964483223
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1801.0709011684223
Current xi:  [-180.305]
objective value function right now is: -1801.0709011684223
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-181.13907]
objective value function right now is: -1800.8495620046094
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-181.50557]
objective value function right now is: -1800.294282712552
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.78278]
objective value function right now is: -1800.9096876448616
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1801.170526675224
Current xi:  [-180.12486]
objective value function right now is: -1801.170526675224
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.50256]
objective value function right now is: -1800.9632463525525
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-181.1281]
objective value function right now is: -1801.10027383462
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-181.13982]
objective value function right now is: -1801.083511275621
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.40329]
objective value function right now is: -1800.9543059198722
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.48953]
objective value function right now is: -1800.9256738725671
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.01724]
objective value function right now is: -1800.7504044931816
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.07025]
objective value function right now is: -1801.1175398355301
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.23752]
objective value function right now is: -1800.9204229907064
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-181.09329]
objective value function right now is: -1801.0883974808046
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-181.22449]
objective value function right now is: -1800.9027555802634
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.90773]
objective value function right now is: -1801.1523436443206
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-181.33781]
objective value function right now is: -1800.922290641163
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-179.73718]
objective value function right now is: -1800.8821858773345
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-180.56712]
objective value function right now is: -1800.9463661398831
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-179.72392]
objective value function right now is: -1800.250252236593
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-179.49748]
objective value function right now is: -1801.1363358727617
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.33238]
objective value function right now is: -1800.7102937089978
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.03918]
objective value function right now is: -1800.9841792768473
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.52809]
objective value function right now is: -1800.9876729380796
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-179.84262]
objective value function right now is: -1800.83441863609
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-179.99062]
objective value function right now is: -1800.8758193178498
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1801.275852459561
Current xi:  [-179.95432]
objective value function right now is: -1801.275852459561
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.28804]
objective value function right now is: -1801.2346770269085
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.03737]
objective value function right now is: -1801.1896987176417
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-179.82738]
objective value function right now is: -1801.2403477993403
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.23811]
objective value function right now is: -1801.1660829349914
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.58755]
objective value function right now is: -1801.2178331185128
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.47598]
objective value function right now is: -1801.1698783012582
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.28233]
objective value function right now is: -1801.2308227623846
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.53503]
objective value function right now is: -1801.2373235300302
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.83902]
objective value function right now is: -1801.2304515643516
new min fval from sgd:  -1801.275894721999
new min fval from sgd:  -1801.2768285952798
new min fval from sgd:  -1801.279402486351
new min fval from sgd:  -1801.280896049274
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-181.12692]
objective value function right now is: -1801.1985650201677
new min fval from sgd:  -1801.2847399988057
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.9608]
objective value function right now is: -1800.9920586148567
new min fval from sgd:  -1801.2859600057918
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-181.19771]
objective value function right now is: -1801.1941542516756
new min fval from sgd:  -1801.2876029370027
new min fval from sgd:  -1801.2883270000714
new min fval from sgd:  -1801.2886331704335
new min fval from sgd:  -1801.2899067326407
new min fval from sgd:  -1801.2904977214932
new min fval from sgd:  -1801.2919193653436
new min fval from sgd:  -1801.2935845028148
new min fval from sgd:  -1801.2944285310307
new min fval from sgd:  -1801.2944346900993
new min fval from sgd:  -1801.2947228440853
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-181.12671]
objective value function right now is: -1801.2656506894575
new min fval from sgd:  -1801.2947250521731
new min fval from sgd:  -1801.2953825404509
new min fval from sgd:  -1801.29604443579
new min fval from sgd:  -1801.2963435601537
new min fval from sgd:  -1801.296681082938
new min fval from sgd:  -1801.2969870843167
new min fval from sgd:  -1801.2970770730064
new min fval from sgd:  -1801.297493714485
new min fval from sgd:  -1801.2977813651817
new min fval from sgd:  -1801.2982900244035
new min fval from sgd:  -1801.2984030924729
new min fval from sgd:  -1801.2985235487051
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-181.25246]
objective value function right now is: -1801.290991244012
min fval:  -1801.2985235487051
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -0.1979,   0.9765],
        [-11.3124,   6.5908],
        [ -0.1979,   0.9765],
        [ -0.1979,   0.9765],
        [ -0.1979,   0.9765],
        [ -0.1979,   0.9765],
        [ 12.0988,   0.9894],
        [  8.2641,  10.5602],
        [ -0.1979,   0.9765],
        [ -0.1979,   0.9765],
        [  8.3044,   3.4489],
        [ 13.3137,   0.8075],
        [ -0.1979,   0.9765]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -0.7651,  10.0077,  -0.7651,  -0.7651,  -0.7651,  -0.7651,  -8.8459,
          8.9300,  -0.7651,  -0.7651,  -1.8028, -10.4919,  -0.7651],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.1067e-03, -1.0426e-02, -5.1067e-03, -5.1067e-03, -5.1067e-03,
         -5.1067e-03, -1.2094e-02, -6.0873e-02, -5.1067e-03, -5.1067e-03,
         -1.5448e-02, -8.0253e-03, -5.1067e-03],
        [-1.9239e-02, -3.5187e+00, -1.9239e-02, -1.9239e-02, -1.9239e-02,
         -1.9239e-02, -2.0642e+00, -4.3092e+00, -1.9239e-02, -1.9239e-02,
         -1.0615e+00, -3.0432e+00, -1.9239e-02],
        [-5.1068e-03, -1.0426e-02, -5.1068e-03, -5.1068e-03, -5.1068e-03,
         -5.1068e-03, -1.2094e-02, -6.0873e-02, -5.1068e-03, -5.1068e-03,
         -1.5448e-02, -8.0253e-03, -5.1068e-03],
        [-5.1068e-03, -1.0426e-02, -5.1068e-03, -5.1068e-03, -5.1068e-03,
         -5.1068e-03, -1.2094e-02, -6.0873e-02, -5.1068e-03, -5.1068e-03,
         -1.5448e-02, -8.0255e-03, -5.1068e-03],
        [-5.1067e-03, -1.0426e-02, -5.1067e-03, -5.1067e-03, -5.1067e-03,
         -5.1067e-03, -1.2094e-02, -6.0873e-02, -5.1067e-03, -5.1067e-03,
         -1.5448e-02, -8.0253e-03, -5.1067e-03],
        [-5.0994e-02,  7.5755e+00, -5.0993e-02, -5.0994e-02, -5.0994e-02,
         -5.0994e-02,  4.4445e+00,  8.2652e+00, -5.0993e-02, -5.0993e-02,
          2.0252e+00,  6.6432e+00, -5.0994e-02],
        [-5.1067e-03, -1.0426e-02, -5.1067e-03, -5.1068e-03, -5.1068e-03,
         -5.1068e-03, -1.2094e-02, -6.0873e-02, -5.1068e-03, -5.1068e-03,
         -1.5448e-02, -8.0253e-03, -5.1068e-03],
        [-5.1068e-03, -1.0426e-02, -5.1068e-03, -5.1068e-03, -5.1068e-03,
         -5.1068e-03, -1.2094e-02, -6.0873e-02, -5.1068e-03, -5.1068e-03,
         -1.5448e-02, -8.0253e-03, -5.1068e-03],
        [-5.1068e-03, -1.0426e-02, -5.1068e-03, -5.1068e-03, -5.1068e-03,
         -5.1068e-03, -1.2094e-02, -6.0873e-02, -5.1068e-03, -5.1068e-03,
         -1.5448e-02, -8.0253e-03, -5.1068e-03],
        [-5.1068e-03, -1.0426e-02, -5.1067e-03, -5.1067e-03, -5.1067e-03,
         -5.1067e-03, -1.2094e-02, -6.0873e-02, -5.1068e-03, -5.1067e-03,
         -1.5448e-02, -8.0253e-03, -5.1067e-03],
        [ 8.2408e-03, -6.0189e+00,  8.2411e-03,  8.2409e-03,  8.2409e-03,
          8.2408e-03, -3.6017e+00, -6.8635e+00,  8.2409e-03,  8.2410e-03,
         -1.7055e+00, -5.3093e+00,  8.2408e-03],
        [-5.1068e-03, -1.0426e-02, -5.1068e-03, -5.1068e-03, -5.1068e-03,
         -5.1068e-03, -1.2094e-02, -6.0873e-02, -5.1068e-03, -5.1068e-03,
         -1.5448e-02, -8.0253e-03, -5.1068e-03],
        [-5.1067e-03, -1.0426e-02, -5.1067e-03, -5.1067e-03, -5.1068e-03,
         -5.1067e-03, -1.2094e-02, -6.0873e-02, -5.1067e-03, -5.1067e-03,
         -1.5448e-02, -8.0253e-03, -5.1067e-03]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.3637,  2.7373, -0.3637, -0.3637, -0.3637, -5.7692, -0.3637, -0.3637,
        -0.3637, -0.3637,  4.6769, -0.3637, -0.3637], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.1368e-02, -4.9089e+00, -1.1368e-02, -1.1368e-02, -1.1368e-02,
          1.4432e+01, -1.1368e-02, -1.1368e-02, -1.1368e-02, -1.1368e-02,
         -9.4813e+00, -1.1368e-02, -1.1368e-02]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-1.5971, -0.5279],
        [-1.1267,  0.6744],
        [-9.5246,  0.4777],
        [-1.5970, -0.5279],
        [ 7.5713,  3.0762],
        [-3.7273, -8.5685],
        [12.6894, -0.4068],
        [ 6.5909,  1.8207],
        [ 7.7798,  4.7579],
        [-7.7583, -5.8473],
        [-0.6126, 10.0873],
        [-7.3910,  9.0133],
        [-1.5971, -0.5279]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-4.0142e+00, -1.9104e+00,  6.5162e+00, -4.0137e+00,  1.0248e+00,
        -7.8381e+00, -1.2477e+01, -4.5727e-03,  3.1424e+00, -5.1589e+00,
         7.8906e+00,  7.8415e+00, -4.0142e+00], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.2116e+00,  3.5784e-02,  1.1203e+00, -2.2103e+00, -2.3012e+00,
          5.1758e+00, -8.0717e+00, -1.5652e+00, -2.5231e+00,  1.2017e+00,
         -4.8447e+00, -6.5096e+00, -2.2116e+00],
        [ 7.6862e-01,  1.2997e-02, -9.8960e-01,  7.6853e-01,  3.5470e-02,
         -2.2871e+00,  4.9523e+00,  8.8693e-02, -6.9247e-01,  1.3671e+00,
          4.4875e+00,  4.3801e+00,  7.6862e-01],
        [-1.7592e+00,  4.8688e-02,  4.5078e+00, -1.7592e+00, -4.5605e+00,
          3.0327e+00, -5.3991e+00, -3.9549e+00, -7.2003e+00,  5.6561e+00,
         -1.0069e+00, -1.7534e+00, -1.7592e+00],
        [-3.6580e+00, -6.1906e-02, -5.8204e+00, -3.6577e+00,  7.0356e-01,
          4.3024e+00,  6.3546e+00,  8.2733e-01,  5.6026e-01, -2.9854e+00,
         -1.5805e-01, -2.0638e+00, -3.6580e+00],
        [-4.0052e-01, -1.3884e-02, -2.9634e-01, -4.0053e-01, -8.5334e-01,
         -4.8818e-01, -5.1736e-01, -8.3258e-01, -8.7694e-01, -4.1749e-01,
         -3.9283e-01, -6.2557e-02, -4.0052e-01],
        [-4.0055e-01, -1.3891e-02, -2.9634e-01, -4.0056e-01, -8.5335e-01,
         -4.8816e-01, -5.1740e-01, -8.3259e-01, -8.7697e-01, -4.1748e-01,
         -3.9284e-01, -6.2600e-02, -4.0055e-01],
        [-4.0052e-01, -1.3884e-02, -2.9633e-01, -4.0053e-01, -8.5334e-01,
         -4.8818e-01, -5.1736e-01, -8.3258e-01, -8.7694e-01, -4.1749e-01,
         -3.9283e-01, -6.2557e-02, -4.0052e-01],
        [-4.0055e-01, -1.3891e-02, -2.9634e-01, -4.0056e-01, -8.5336e-01,
         -4.8816e-01, -5.1740e-01, -8.3259e-01, -8.7697e-01, -4.1748e-01,
         -3.9284e-01, -6.2600e-02, -4.0055e-01],
        [-5.9330e-01, -1.5383e-01, -3.6358e-01, -5.9331e-01,  3.1955e+00,
         -1.0303e+00,  9.2553e-04,  2.6015e+00,  3.8707e+00, -4.4622e+00,
         -1.1209e+00, -1.6399e+00, -5.9330e-01],
        [-1.2407e+00,  3.6830e-02, -9.1997e-02, -1.2407e+00,  1.5084e+00,
         -1.2886e+00,  6.3409e-01,  1.5303e+00,  1.6214e+00, -2.1138e+00,
          1.0728e+00,  1.9728e+00, -1.2407e+00],
        [-1.3553e+00, -2.3315e-03,  2.3949e+00, -1.3553e+00, -1.8915e+00,
          4.3979e-01, -2.5396e+00, -1.6821e+00, -2.8029e+00,  2.3579e+00,
          2.3354e+00,  1.7429e+00, -1.3553e+00],
        [-4.0052e-01, -1.3884e-02, -2.9634e-01, -4.0053e-01, -8.5334e-01,
         -4.8818e-01, -5.1736e-01, -8.3258e-01, -8.7694e-01, -4.1749e-01,
         -3.9283e-01, -6.2557e-02, -4.0052e-01],
        [ 1.2911e+00, -3.3140e-02, -1.8813e+00,  1.2911e+00, -1.7584e+00,
         -4.3644e-03,  9.8844e-01, -1.5624e+00, -1.5103e+00,  1.3780e+00,
         -4.4539e-01,  1.1651e-02,  1.2911e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 1.4402, -1.3549, -5.2616, -0.7992, -1.4024, -1.4025, -1.4024, -1.4025,
         0.7103,  0.7117, -2.7773, -1.4024, -0.6595], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 0.8498, -4.0109, 10.4039, -1.9124, -0.0359, -0.0359, -0.0359, -0.0359,
         -5.2831, -4.0486,  2.0438, -0.0359, -2.5664],
        [-6.4432,  1.0197, -0.4109,  0.0312,  0.0114,  0.0105,  0.0114,  0.0105,
          0.1509,  1.1402,  2.1139,  0.0114,  0.0772],
        [-0.6490, -3.9453, -0.2527, -1.1864, -0.1334, -0.1334, -0.1334, -0.1334,
         -3.4499, -3.4535, -0.2321, -0.1334, -3.3840],
        [-0.6385, -3.8176, -0.3294, -1.0868, -0.1357, -0.1357, -0.1357, -0.1357,
         -3.3365, -3.3483, -0.2500, -0.1357, -3.2574],
        [ 7.2764,  0.3068, -9.2931,  0.8435,  0.0642,  0.0634,  0.0642,  0.0634,
          1.6449, -0.0977, -2.4039,  0.0642,  1.2173]], device='cuda:0'))])
xi:  [-181.25606]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 988.6072926542458
W_T_median: 571.4073872894905
W_T_pctile_5: -180.2227997856325
W_T_CVAR_5_pct: -286.38161738828455
Average q (qsum/M+1):  58.56830818422379
Optimal xi:  [-181.25606]
Expected(across Rb) median(across samples) p_equity:  0.60500458329916
obj fun:  tensor(-1801.2985, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-27.4193,  -3.0640],
        [  1.6440,   6.0462],
        [  1.4182,   5.7601],
        [  1.8946,   6.1474],
        [-17.4430,   4.6301],
        [  1.7809,   6.0646],
        [ -3.2937,  -4.5061],
        [ -3.6052,  -5.0399],
        [ -0.5695,  -5.1054],
        [  2.1892,   6.2793],
        [ -3.8324,  -5.5411],
        [  1.3981,   5.6450],
        [ -0.1237,   1.3278]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-3.4665,  3.7394,  3.1007,  3.9473,  5.5996,  3.7783, -4.3590, -4.4339,
        -4.3720,  4.2351, -4.7003,  2.8424, -1.5628], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 2.5044, -1.3001, -0.7147, -1.6094, -2.1733, -1.4138,  1.3561,  1.6474,
          0.1426, -2.0168,  2.2099, -0.5779, -0.0211],
        [-0.0067, -0.1102, -0.0688, -0.1326, -0.0915, -0.1176, -0.1751, -0.2230,
         -0.4347, -0.1671, -0.2449, -0.0570, -0.0200],
        [ 4.7079, -2.5648, -1.7793, -2.6966, -4.2775, -2.4625,  1.3332,  1.9557,
          2.6529, -2.9678,  2.7232, -1.4970, -0.1105],
        [-0.0067, -0.1102, -0.0688, -0.1326, -0.0915, -0.1176, -0.1751, -0.2230,
         -0.4347, -0.1671, -0.2449, -0.0570, -0.0200],
        [ 0.0739, -0.1803, -0.1230, -0.2122, -0.5341, -0.1883, -0.1463, -0.1153,
         -0.5070, -0.2727, -0.0743, -0.1066, -0.0445],
        [-5.9091,  3.0526,  2.2903,  3.0700,  4.4306,  2.9685, -1.4868, -2.2511,
         -2.9180,  3.3480, -2.9176,  1.9641,  0.0702],
        [ 6.1990, -3.2345, -2.3015, -3.8470, -2.6985, -3.5986,  3.1311,  3.4695,
          0.9508, -4.4587,  4.2591, -2.1659,  0.2486],
        [-4.9855,  2.2080,  1.6042,  2.3838,  3.8049,  2.2497, -0.9931, -1.8327,
         -2.4111,  2.6483, -2.3305,  1.3703,  0.1692],
        [-0.0067, -0.1102, -0.0688, -0.1326, -0.0915, -0.1176, -0.1751, -0.2230,
         -0.4347, -0.1671, -0.2449, -0.0570, -0.0200],
        [-5.5805,  3.0666,  2.2448,  3.4206,  3.0735,  3.0767, -2.3909, -3.1885,
         -1.3093,  3.8087, -3.5443,  2.0288, -0.0622],
        [-4.9964,  2.2928,  1.6664,  2.4302,  3.8388,  2.2836, -1.1445, -1.7916,
         -2.5440,  2.6905, -2.5982,  1.3958,  0.1640],
        [ 3.3654, -1.5484, -0.8862, -1.8808, -2.2697, -1.6668,  1.6377,  1.9685,
          0.4640, -2.3061,  2.5025, -0.7339,  0.0117],
        [-0.0067, -0.1102, -0.0688, -0.1326, -0.0915, -0.1176, -0.1751, -0.2230,
         -0.4347, -0.1671, -0.2449, -0.0570, -0.0200]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.0891, -0.7195,  0.3982, -0.7195, -1.2426, -0.7655, -0.8213, -0.6956,
        -0.7195,  0.3284, -0.5331, -1.1321, -0.7195], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-2.1546e+00, -2.8415e-03, -4.1425e+00, -2.8415e-03, -1.8998e-01,
          6.2780e+00, -6.5454e+00,  4.3297e+00, -2.8415e-03,  5.7933e+00,
          4.4656e+00, -2.5867e+00, -2.8415e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-1.8395,  2.7129],
        [-1.6234,  0.5981],
        [-7.7627, -0.3968],
        [14.0214,  5.8924],
        [-2.6932, 10.9161],
        [-1.6164,  0.5930],
        [-1.6246,  0.5991],
        [ 8.2469,  7.8832],
        [-6.1322, -7.2559],
        [ 9.9933, -0.7365],
        [ 3.3069, -0.8839],
        [10.3110, -0.5446],
        [ 6.2503, -5.8150]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -2.9289,  -2.5406,   5.0678,   0.3627,   9.2281,  -2.5414,  -2.5405,
          6.3350,  -6.9545, -10.2317,  -7.4052, -10.1698,  -5.1490],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.1319e-01,  2.9096e-01,  3.0937e-01, -5.1651e+00, -1.6734e+01,
          2.7858e-01,  2.9316e-01,  2.3746e+00, -5.6274e-01, -3.8493e+00,
         -9.5915e-01, -3.6169e+00,  1.8319e+00],
        [ 7.0438e-03, -9.8765e-04, -8.0580e-01, -1.4444e+00, -5.0061e-01,
         -1.0580e-03, -9.7425e-04, -1.2157e+00, -5.8459e-01, -5.1133e-01,
         -1.4502e-01, -5.6229e-01, -1.6381e+00],
        [-5.4782e-02,  2.0757e-01,  2.3828e+00, -9.0628e+00, -1.1470e+01,
          2.0313e-01,  2.0834e-01, -1.2172e+01,  3.7069e+00, -7.7470e+00,
         -8.4761e-01, -8.6579e+00,  1.8675e+00],
        [ 7.1802e-03, -9.5000e-04, -8.0509e-01, -1.4444e+00, -5.0041e-01,
         -1.0203e-03, -9.3662e-04, -1.2149e+00, -5.8485e-01, -5.1154e-01,
         -1.4542e-01, -5.6251e-01, -1.6383e+00],
        [ 7.1071e-03, -9.7022e-04, -8.0548e-01, -1.4444e+00, -5.0052e-01,
         -1.0405e-03, -9.5684e-04, -1.2153e+00, -5.8471e-01, -5.1143e-01,
         -1.4520e-01, -5.6239e-01, -1.6382e+00],
        [ 1.7355e+00,  1.1367e-01, -1.7311e+00, -8.5874e-01, -3.5069e+00,
          1.0206e-01,  1.1587e-01, -1.2835e-01, -3.4636e+00,  4.7155e+00,
          8.5108e-01,  5.0353e+00,  1.3769e+00],
        [ 7.1802e-03, -9.4997e-04, -8.0509e-01, -1.4444e+00, -5.0041e-01,
         -1.0203e-03, -9.3660e-04, -1.2149e+00, -5.8485e-01, -5.1154e-01,
         -1.4542e-01, -5.6251e-01, -1.6383e+00],
        [ 1.4727e+00,  1.4110e-01,  3.9059e+00, -3.3137e+00,  6.9012e+00,
          1.2964e-01,  1.4314e-01, -2.2599e+00, -1.2410e+00, -4.0612e+00,
         -7.8249e-02, -5.7457e+00, -3.2716e+00],
        [ 7.1542e-03, -9.5720e-04, -8.0523e-01, -1.4444e+00, -5.0045e-01,
         -1.0275e-03, -9.4382e-04, -1.2151e+00, -5.8480e-01, -5.1150e-01,
         -1.4534e-01, -5.6247e-01, -1.6383e+00],
        [ 7.1846e-03, -9.4876e-04, -8.0507e-01, -1.4444e+00, -5.0041e-01,
         -1.0190e-03, -9.3537e-04, -1.2149e+00, -5.8486e-01, -5.1155e-01,
         -1.4544e-01, -5.6252e-01, -1.6383e+00],
        [ 8.7391e-01, -3.0530e-02,  2.6762e+00, -9.3027e-01,  3.7926e+00,
         -3.5531e-02, -2.9590e-02, -1.4421e-01,  2.0954e+00, -2.2998e+00,
          1.0554e+00, -3.4052e+00, -1.3197e+00],
        [ 1.3594e+00,  2.0036e-01, -1.0095e+00, -8.0021e-01,  8.7529e-01,
          1.9979e-01,  2.0054e-01, -3.6440e-01,  1.1917e-01,  1.5775e-01,
          5.1862e-01,  2.1414e-01, -4.6089e+00],
        [ 2.7299e-02,  1.3029e-03, -1.0905e+00, -1.8835e+00, -1.9927e-01,
          1.1547e-03,  1.3347e-03, -1.4960e+00, -1.0489e+00,  3.3693e-01,
         -7.7090e-01,  7.6276e-01, -1.5161e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 1.6894, -1.9467,  0.0116, -1.9467, -1.9467,  0.4407, -1.9467, -2.6117,
        -1.9467, -1.9467, -0.9375, -1.8611, -2.6310], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.1086e+01,  7.5686e-02, -1.9976e+00,  7.5069e-02,  7.5385e-02,
         -1.1610e+01,  7.5068e-02,  3.8522e+00,  7.5175e-02,  7.5050e-02,
          1.9220e+00,  1.6149e+00,  1.0886e-01],
        [ 4.7196e-01, -3.9595e-02, -6.2809e+00, -4.2239e-02, -4.0830e-02,
          5.1823e-01, -4.2241e-02, -6.2344e-01, -4.1741e-02, -4.2324e-02,
          1.3394e+00, -1.1914e+00,  3.3863e-01],
        [-2.9999e+00, -2.8762e-02, -1.1846e-02, -2.8781e-02, -2.8770e-02,
         -1.0962e+01, -2.8781e-02, -1.1811e-01, -2.8777e-02, -2.8781e-02,
         -1.0319e+01, -1.0338e-01, -1.2737e-02],
        [-3.0144e+00, -3.2550e-02, -1.3117e-02, -3.2573e-02, -3.2560e-02,
         -1.0357e+01, -3.2573e-02, -1.5191e-01, -3.2568e-02, -3.2573e-02,
         -9.5572e+00, -1.4187e-01, -1.3336e-02],
        [ 1.4280e+00,  2.1419e-03,  6.7391e+00, -5.3612e-04,  8.9219e-04,
          9.6695e-01, -5.3831e-04, -7.4492e-01, -3.1595e-05, -6.2347e-04,
          1.4097e+00,  6.2159e-01,  7.1333e-01]], device='cuda:0'))])
loaded xi:  109.16449
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 946.50330641104
W_T_median: 707.5512584605915
W_T_pctile_5: -238.22847581189643
W_T_CVAR_5_pct: -367.3344080824147
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1760.325175109599
Current xi:  [103.93299]
objective value function right now is: -1760.325175109599
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1760.4487731075794
Current xi:  [96.762276]
objective value function right now is: -1760.4487731075794
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1761.1801599125772
Current xi:  [88.991554]
objective value function right now is: -1761.1801599125772
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1761.291204614258
Current xi:  [81.56442]
objective value function right now is: -1761.291204614258
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.457375]
objective value function right now is: -1760.0183936672152
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1761.9535735193124
Current xi:  [67.91696]
objective value function right now is: -1761.9535735193124
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [61.684795]
objective value function right now is: -1759.9118216942522
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [56.156834]
objective value function right now is: -1761.4013286395134
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [50.48205]
objective value function right now is: -1761.8869687214456
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.258545]
objective value function right now is: -1760.9931342567634
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [40.06755]
objective value function right now is: -1761.894612458751
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [36.012638]
objective value function right now is: -1761.6545635735533
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1762.3529251054106
Current xi:  [31.957336]
objective value function right now is: -1762.3529251054106
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1762.4500637076835
Current xi:  [27.88022]
objective value function right now is: -1762.4500637076835
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1762.45078993065
Current xi:  [24.748081]
objective value function right now is: -1762.45078993065
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [21.31387]
objective value function right now is: -1762.2553609521426
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [18.637516]
objective value function right now is: -1762.4364681870882
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.136126]
objective value function right now is: -1762.3275797666424
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.821798]
objective value function right now is: -1762.2253479226638
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1762.519475150106
Current xi:  [10.045501]
objective value function right now is: -1762.519475150106
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1762.9458461474767
Current xi:  [5.9821086]
objective value function right now is: -1762.9458461474767
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04669326]
objective value function right now is: -1762.1000048703524
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02713453]
objective value function right now is: -1762.8731199291376
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01252974]
objective value function right now is: -1762.008445058837
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02172256]
objective value function right now is: -1762.562638491837
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0093816]
objective value function right now is: -1762.5324548433675
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00616875]
objective value function right now is: -1761.6709243440164
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03179019]
objective value function right now is: -1762.883267226689
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05055138]
objective value function right now is: -1761.4870986983271
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03354837]
objective value function right now is: -1762.664786005863
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03621722]
objective value function right now is: -1762.5094919990688
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01550621]
objective value function right now is: -1762.089944292413
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1762.9463164907727
Current xi:  [-0.00681928]
objective value function right now is: -1762.9463164907727
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0007321]
objective value function right now is: -1762.8919891494666
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -1763.130864320888
Current xi:  [-0.04218996]
objective value function right now is: -1763.130864320888
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1763.1786767571643
Current xi:  [0.00327348]
objective value function right now is: -1763.1786767571643
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1763.2260230703978
Current xi:  [0.00177029]
objective value function right now is: -1763.2260230703978
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1763.273772010768
Current xi:  [0.00031056]
objective value function right now is: -1763.273772010768
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00080954]
objective value function right now is: -1763.1410518186851
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1763.3021066897436
Current xi:  [0.00143234]
objective value function right now is: -1763.3021066897436
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00636875]
objective value function right now is: -1763.2809862950799
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00123929]
objective value function right now is: -1763.0062607609057
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00077566]
objective value function right now is: -1763.1835303206608
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00383326]
objective value function right now is: -1763.0917290887414
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00028015]
objective value function right now is: -1763.1273764115367
new min fval from sgd:  -1763.303784944263
new min fval from sgd:  -1763.3051461118296
new min fval from sgd:  -1763.3056689795098
new min fval from sgd:  -1763.3075728467386
new min fval from sgd:  -1763.313378831078
new min fval from sgd:  -1763.32763552697
new min fval from sgd:  -1763.3336990646505
new min fval from sgd:  -1763.3356423365133
new min fval from sgd:  -1763.3385757574597
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00133908]
objective value function right now is: -1763.2075715793198
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00226353]
objective value function right now is: -1763.0462328914255
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0043662]
objective value function right now is: -1763.1965572331408
new min fval from sgd:  -1763.3388472209665
new min fval from sgd:  -1763.3388497614376
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00076609]
objective value function right now is: -1763.3215336334583
new min fval from sgd:  -1763.341362644317
new min fval from sgd:  -1763.3441460929696
new min fval from sgd:  -1763.3461019847468
new min fval from sgd:  -1763.3484575291318
new min fval from sgd:  -1763.3508140132276
new min fval from sgd:  -1763.3515025802042
new min fval from sgd:  -1763.3524367323043
new min fval from sgd:  -1763.354080419764
new min fval from sgd:  -1763.3554081852185
new min fval from sgd:  -1763.3557686873596
new min fval from sgd:  -1763.3563871295578
new min fval from sgd:  -1763.3575905859254
new min fval from sgd:  -1763.3581418601448
new min fval from sgd:  -1763.3592061168902
new min fval from sgd:  -1763.359758578782
new min fval from sgd:  -1763.3604541963557
new min fval from sgd:  -1763.360510177101
new min fval from sgd:  -1763.3609341864465
new min fval from sgd:  -1763.3613788736425
new min fval from sgd:  -1763.361468350809
new min fval from sgd:  -1763.3615420351848
new min fval from sgd:  -1763.3621464251066
new min fval from sgd:  -1763.3629110166128
new min fval from sgd:  -1763.3633030270266
new min fval from sgd:  -1763.3635285000605
new min fval from sgd:  -1763.3637043157505
new min fval from sgd:  -1763.3642558734296
new min fval from sgd:  -1763.3644434157375
new min fval from sgd:  -1763.3644451246435
new min fval from sgd:  -1763.3645174403396
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00109778]
objective value function right now is: -1763.3533414405817
min fval:  -1763.3645174403396
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -0.2983,   1.2232],
        [  1.4885,   7.3105],
        [ -0.1730,   2.5427],
        [  1.7046,   7.6320],
        [-10.5439,   6.9667],
        [  1.5508,   7.3922],
        [ -0.2960,   1.2205],
        [ -4.6282,  -6.7657],
        [ -2.3224,  -4.6895],
        [  2.0539,   7.9514],
        [ -5.0262,  -7.4017],
        [ -0.3014,   1.2265],
        [ -0.3073,   1.2316]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-1.3021,  4.7301, -0.4991,  5.1805,  6.6607,  4.8397, -1.3004, -5.5396,
        -5.8139,  5.5953, -5.9064, -1.3042, -1.3064], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.0455e-02, -4.1599e-02, -6.0430e-03, -5.9653e-02, -3.4693e-02,
         -4.5602e-02, -1.0501e-02, -1.6310e-01, -3.5131e-02, -8.7422e-02,
         -1.7556e-01, -1.0402e-02, -1.0339e-02],
        [-1.0455e-02, -4.1599e-02, -6.0430e-03, -5.9653e-02, -3.4693e-02,
         -4.5602e-02, -1.0501e-02, -1.6310e-01, -3.5131e-02, -8.7422e-02,
         -1.7556e-01, -1.0402e-02, -1.0339e-02],
        [-5.1467e-02, -2.2752e+00, -1.4380e-01, -2.7671e+00, -5.3820e+00,
         -2.3800e+00, -5.2231e-02,  2.6957e+00,  1.4757e+00, -3.2653e+00,
          3.6091e+00, -4.3362e-02, -5.3924e-02],
        [-1.0455e-02, -4.1599e-02, -6.0430e-03, -5.9653e-02, -3.4693e-02,
         -4.5602e-02, -1.0501e-02, -1.6310e-01, -3.5131e-02, -8.7422e-02,
         -1.7556e-01, -1.0402e-02, -1.0339e-02],
        [-1.0455e-02, -4.1599e-02, -6.0430e-03, -5.9653e-02, -3.4693e-02,
         -4.5602e-02, -1.0501e-02, -1.6310e-01, -3.5131e-02, -8.7422e-02,
         -1.7556e-01, -1.0402e-02, -1.0339e-02],
        [ 1.0690e-01,  2.9977e+00,  3.4424e-01,  3.4839e+00,  6.1337e+00,
          3.0942e+00,  1.0092e-01, -2.9320e+00, -2.0421e+00,  3.9727e+00,
         -3.9662e+00,  1.1766e-01,  1.2485e-01],
        [ 8.6222e-02, -2.6361e+00,  5.4270e-02, -3.4371e+00, -4.2213e+00,
         -2.8686e+00,  8.6841e-02,  4.5503e+00, -8.2629e-02, -4.3946e+00,
          5.8457e+00,  9.0292e-02,  1.0083e-01],
        [-1.0455e-02, -4.1599e-02, -6.0430e-03, -5.9653e-02, -3.4693e-02,
         -4.5602e-02, -1.0501e-02, -1.6310e-01, -3.5131e-02, -8.7422e-02,
         -1.7556e-01, -1.0402e-02, -1.0339e-02],
        [-1.0455e-02, -4.1599e-02, -6.0430e-03, -5.9653e-02, -3.4693e-02,
         -4.5602e-02, -1.0501e-02, -1.6310e-01, -3.5131e-02, -8.7422e-02,
         -1.7556e-01, -1.0402e-02, -1.0338e-02],
        [-3.7611e-02,  2.3187e+00,  5.3259e-02,  2.8940e+00,  4.6918e+00,
          2.4360e+00, -3.2263e-02, -3.9687e+00,  6.3898e-02,  3.5501e+00,
         -4.9659e+00, -3.7910e-02, -4.0946e-02],
        [-1.0456e-02, -4.1610e-02, -6.0435e-03, -5.9669e-02, -3.4696e-02,
         -4.5614e-02, -1.0502e-02, -1.6313e-01, -3.5140e-02, -8.7446e-02,
         -1.7559e-01, -1.0403e-02, -1.0339e-02],
        [-1.0455e-02, -4.1599e-02, -6.0430e-03, -5.9653e-02, -3.4693e-02,
         -4.5602e-02, -1.0501e-02, -1.6310e-01, -3.5131e-02, -8.7422e-02,
         -1.7556e-01, -1.0402e-02, -1.0339e-02],
        [-1.0455e-02, -4.1599e-02, -6.0430e-03, -5.9653e-02, -3.4693e-02,
         -4.5602e-02, -1.0501e-02, -1.6310e-01, -3.5131e-02, -8.7422e-02,
         -1.7556e-01, -1.0402e-02, -1.0339e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8553, -0.8553,  0.8665, -0.8553, -0.8553, -1.4089, -1.5469, -0.8554,
        -0.8553,  0.7762, -0.8557, -0.8553, -0.8553], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 0.0145,  0.0145, -5.2392,  0.0145,  0.0145,  8.1702, -7.9447,  0.0145,
          0.0145,  7.0580,  0.0145,  0.0145,  0.0145]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.5887,   1.3276],
        [ -1.5121,   1.5745],
        [-11.0223,  -0.4246],
        [ 16.0624,   5.6457],
        [ -2.7422,  14.6356],
        [ -1.5121,   1.5745],
        [ -1.5121,   1.5745],
        [  8.8073,  10.3509],
        [ -1.5122,   1.5740],
        [  2.1387,  -0.7199],
        [ -1.5121,   1.5745],
        [ 13.1560,  -0.7991],
        [  7.1282,  -6.6886]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -2.1971,  -2.1582,   7.8540,  -0.6342,  12.1675,  -2.1582,  -2.1582,
          7.9584,  -2.1583,  -8.5101,  -2.1582, -12.8281,  -6.5518],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[  0.4825,   0.4922,   1.2069,  -4.8865, -18.0849,   0.4922,   0.4922,
           1.0015,   0.4922,  -4.8844,   0.4922,  -4.3220,   1.6888],
        [  0.0463,   0.0407,  -0.6312,  -1.5238,  -0.3069,   0.0407,   0.0407,
          -1.0355,   0.0407,  -0.6527,   0.0407,  -1.0964,  -1.8530],
        [ -0.1018,  -0.1110,   1.9032,  -7.1088,  -4.2744,  -0.1110,  -0.1110,
         -16.6847,  -0.1109,  -2.6684,  -0.1110, -10.2303,   2.5578],
        [  0.0463,   0.0407,  -0.6324,  -1.5228,  -0.3071,   0.0407,   0.0407,
          -1.0356,   0.0407,  -0.6522,   0.0407,  -1.0959,  -1.8528],
        [  0.0463,   0.0407,  -0.6319,  -1.5232,  -0.3070,   0.0407,   0.0407,
          -1.0355,   0.0407,  -0.6524,   0.0407,  -1.0961,  -1.8529],
        [  0.3055,   0.3789,  -2.5289,  -1.0337,  -3.2770,   0.3789,   0.3789,
           1.7964,   0.3787,  -2.8014,   0.3789,   5.2796,   1.6880],
        [  0.0463,   0.0407,  -0.6322,  -1.5230,  -0.3071,   0.0407,   0.0407,
          -1.0356,   0.0407,  -0.6522,   0.0407,  -1.0960,  -1.8528],
        [  0.5647,   0.6895,   4.0083,  -3.1819,   6.5143,   0.6895,   0.6895,
          -2.9133,   0.6892,  -0.8447,   0.6895,  -3.1500,  -3.9949],
        [  0.0463,   0.0407,  -0.6322,  -1.5230,  -0.3071,   0.0407,   0.0407,
          -1.0356,   0.0407,  -0.6522,   0.0407,  -1.0960,  -1.8528],
        [  0.0463,   0.0407,  -0.6324,  -1.5228,  -0.3071,   0.0407,   0.0407,
          -1.0356,   0.0407,  -0.6522,   0.0407,  -1.0959,  -1.8528],
        [  0.9239,   1.1270,   2.5283,  -2.1890,   3.1461,   1.1270,   1.1270,
           1.7711,   1.1266,  -0.2339,   1.1270,  -5.4631,  -1.8782],
        [  0.0454,   0.0408,  -0.6711,  -1.4877,  -0.3157,   0.0408,   0.0408,
          -1.0407,   0.0408,  -0.6344,   0.0408,  -1.0777,  -1.8488],
        [  0.0454,   0.0408,  -0.6736,  -1.4850,  -0.3163,   0.0408,   0.0408,
          -1.0412,   0.0408,  -0.6332,   0.0408,  -1.0764,  -1.8488]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 1.7735, -1.7969,  0.7536, -1.7977, -1.7974, -0.5977, -1.7976, -2.3522,
        -1.7976, -1.7977, -1.1204, -1.8263, -1.8284], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-10.9062,   0.6831,  -0.7180,   0.6819,   0.6824,  -8.6811,   0.6821,
           3.3400,   0.6821,   0.6820,   2.5991,   0.6416,   0.6388],
        [  0.7561,  -0.0996,  -6.0674,  -0.0992,  -0.0994,   0.7151,  -0.0993,
          -1.3219,  -0.0993,  -0.0992,   1.7691,  -0.0875,  -0.0870],
        [ -5.1927,  -0.0740,  -0.2089,  -0.0740,  -0.0740, -11.9703,  -0.0740,
          -0.2862,  -0.0740,  -0.0740,  -9.0754,  -0.0727,  -0.0726],
        [ -4.4445,  -0.0740,  -0.2495,  -0.0739,  -0.0740, -11.5833,  -0.0740,
          -0.2913,  -0.0740,  -0.0739,  -8.9171,  -0.0726,  -0.0725],
        [  1.7245,  -0.1273,   6.5018,  -0.1270,  -0.1271,   1.1596,  -0.1270,
          -0.1672,  -0.1270,  -0.1270,   2.0466,  -0.1154,  -0.1149]],
       device='cuda:0'))])
xi:  [0.00063455]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 100.08567425597322
W_T_median: 78.7185952121931
W_T_pctile_5: 0.0009962966614477154
W_T_CVAR_5_pct: -131.88857336664722
Average q (qsum/M+1):  57.733622889364916
Optimal xi:  [0.00063455]
Expected(across Rb) median(across samples) p_equity:  0.15820904881693423
obj fun:  tensor(-1763.3645, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-13.9193,  -9.4814],
        [ -4.0635, -10.0859],
        [ -3.7680,   6.3815],
        [ -4.1112,   6.5912],
        [ -2.6124,   2.8879],
        [ -4.6375,   6.1590],
        [ -3.9421,   6.5035],
        [  0.2053,   2.6008],
        [  7.1332,  -0.7121],
        [  8.4325,  -1.7700],
        [ -4.1473,   5.1382],
        [  8.0891,   1.3692],
        [  8.4909,   2.4764]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-0.5965, -5.4013,  4.2614,  3.9439, -1.4830,  0.6895,  4.1248, -1.6968,
        -8.3507, -8.5455, -0.2842, -8.2528, -8.4217], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.0851e-01,  1.0838e-01, -1.7656e+00, -1.2170e+00,  4.5265e-03,
         -4.2701e-02, -1.4878e+00,  2.6715e-02,  5.6006e-01,  1.6241e+00,
         -1.6665e-02,  1.1955e+00,  1.4190e+00],
        [ 4.4284e+00,  4.4435e+00, -3.0776e+00, -2.6918e+00, -4.9765e-03,
         -1.9605e-01, -2.8261e+00,  1.4640e-02,  7.8317e-01,  2.4084e+00,
         -1.2010e-01,  1.1927e+00,  1.4787e+00],
        [-2.7029e-01, -3.8280e-01,  3.8048e-01,  4.2380e-01, -1.7282e-01,
         -2.4522e-01,  4.1204e-01,  2.3622e-01,  4.3298e-01,  4.3465e-01,
         -2.5213e-01,  6.7016e-01,  7.5427e-01],
        [ 5.9008e+00,  5.8063e+00, -3.7487e+00, -3.3579e+00, -6.2299e-02,
         -6.3839e-01, -3.4991e+00,  8.9013e-02,  1.0563e+00,  2.7656e+00,
         -4.2648e-01,  1.3942e+00,  1.5627e+00],
        [-2.7011e-01, -3.8274e-01,  3.8010e-01,  4.2347e-01, -1.7272e-01,
         -2.4510e-01,  4.1169e-01,  2.3584e-01,  4.3247e-01,  4.3428e-01,
         -2.5198e-01,  6.6947e-01,  7.5358e-01],
        [-2.7011e-01, -3.8274e-01,  3.8010e-01,  4.2347e-01, -1.7272e-01,
         -2.4510e-01,  4.1169e-01,  2.3584e-01,  4.3247e-01,  4.3428e-01,
         -2.5198e-01,  6.6947e-01,  7.5358e-01],
        [ 6.8954e+00,  8.9894e+00, -5.0100e+00, -4.5978e+00, -8.6199e-01,
         -1.8107e+00, -4.6836e+00,  7.8542e-01,  3.8400e+00,  6.8900e+00,
         -1.4918e+00,  5.7511e+00,  7.0410e+00],
        [-2.7080e-01, -3.8306e-01,  3.8145e-01,  4.2453e-01, -1.7319e-01,
         -2.4565e-01,  4.1291e-01,  2.3744e-01,  4.3361e-01,  4.3595e-01,
         -2.5268e-01,  6.7153e-01,  7.5579e-01],
        [-6.5181e+00, -7.7325e+00,  4.2892e+00,  3.8151e+00,  4.9481e-02,
          8.7090e-01,  4.0676e+00,  4.0157e-01, -3.8879e+00, -6.7266e+00,
          4.6984e-01, -6.0587e+00, -7.2564e+00],
        [ 4.1094e+00,  3.9593e+00, -2.6601e+00, -2.3351e+00,  2.7441e-02,
         -1.0458e-01, -2.4297e+00,  1.3929e-01,  1.0831e+00,  2.8707e+00,
         -8.4861e-02,  1.6582e+00,  2.0296e+00],
        [ 9.1538e-02,  1.1433e-02, -4.5969e-01, -5.5030e-01,  7.9296e-02,
          1.2314e-01, -5.2011e-01, -7.7216e-01, -8.9524e-01, -8.0913e-01,
          2.6480e-01, -1.0249e+00, -1.0745e+00],
        [-2.7011e-01, -3.8274e-01,  3.8009e-01,  4.2346e-01, -1.7272e-01,
         -2.4510e-01,  4.1168e-01,  2.3583e-01,  4.3246e-01,  4.3427e-01,
         -2.5197e-01,  6.6945e-01,  7.5356e-01],
        [-5.9444e+00, -8.5876e+00,  4.1212e+00,  3.7874e+00,  1.9993e-01,
          9.4620e-01,  4.0082e+00,  7.1781e-01, -3.9988e+00, -6.4882e+00,
          5.1646e-01, -5.8764e+00, -7.4145e+00]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-3.9881, -1.9749, -1.4296, -2.2109, -1.4277, -1.4277, -1.8161, -1.4351,
         1.6336, -2.2885,  2.4262, -1.4277,  1.8564], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-2.3734, -2.3577, -0.4406, -3.5981, -0.4402, -0.4402, -9.3949, -0.4418,
          7.2232, -2.8069,  1.0345, -0.4402,  7.6960]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 11.9181,   4.0088],
        [ -1.2643,   1.3101],
        [ -3.5113,   2.7685],
        [ -0.4595,  10.8757],
        [ -8.2363,   3.6134],
        [ -1.3933,  -0.8612],
        [-12.7377,   0.7511],
        [  7.2150,   8.7777],
        [ -9.7717,  -3.5038],
        [-10.9268,   0.5853],
        [  9.8567,   7.7442],
        [ -1.8700,  -1.1236],
        [ 10.0067,  -0.0468]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  0.1735,  -3.6938,  -7.6988,   5.9521,   5.2625,  -5.0771,  -0.6058,
          4.7066,  -3.2193,   9.4473,   6.2407,  -5.5687, -10.3012],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.6602e+00,  6.2425e-01,  7.1238e-01,  2.4862e-01,  1.1649e-01,
          1.6227e-02, -8.4174e-02, -9.4818e-01, -1.3434e-01, -1.1821e+00,
         -1.6839e+00, -1.3421e-02,  2.8699e-02],
        [-4.2847e+00, -1.7835e-01, -5.4477e+00,  7.9615e+00,  2.0292e+00,
         -1.6190e+00,  3.1775e+00,  1.3874e+00,  2.5172e-01,  3.2368e+00,
         -3.6006e+00, -9.9907e-01, -1.0370e+01],
        [-1.6602e+00,  6.2424e-01,  7.1238e-01,  2.4862e-01,  1.1649e-01,
          1.6227e-02, -8.4174e-02, -9.4818e-01, -1.3434e-01, -1.1821e+00,
         -1.6839e+00, -1.3421e-02,  2.8703e-02],
        [-4.2248e+00, -1.6574e+00, -2.5166e+00,  7.7978e+00,  1.4286e+00,
          1.8750e+00,  3.5323e+00, -1.3036e+00,  5.3291e+00,  2.5482e-01,
         -6.1487e+00,  1.8649e+00, -3.8582e+00],
        [ 1.2138e+00,  1.2596e+00,  1.9248e+00,  6.3574e-01, -4.3035e-01,
          1.3083e-01,  1.0481e+00, -1.3703e+00,  1.4158e+00, -5.4719e-01,
          9.5239e-01,  7.7539e-02,  2.7585e+00],
        [-1.6602e+00,  6.2424e-01,  7.1238e-01,  2.4862e-01,  1.1649e-01,
          1.6226e-02, -8.4173e-02, -9.4818e-01, -1.3434e-01, -1.1821e+00,
         -1.6839e+00, -1.3421e-02,  2.8699e-02],
        [-1.6602e+00,  6.2424e-01,  7.1238e-01,  2.4862e-01,  1.1649e-01,
          1.6227e-02, -8.4174e-02, -9.4818e-01, -1.3434e-01, -1.1821e+00,
         -1.6839e+00, -1.3421e-02,  2.8702e-02],
        [ 8.7695e-01, -1.2088e-01, -3.4276e-03,  1.0753e+01,  2.7210e+00,
          1.2052e+00, -6.9686e-02, -1.3815e+00,  1.3082e+00,  4.2485e+00,
         -2.2177e+00,  1.4302e+00, -4.5515e-01],
        [-1.6602e+00,  6.2424e-01,  7.1238e-01,  2.4862e-01,  1.1649e-01,
          1.6227e-02, -8.4174e-02, -9.4818e-01, -1.3434e-01, -1.1821e+00,
         -1.6839e+00, -1.3420e-02,  2.8703e-02],
        [ 3.9884e+00,  2.1640e-01,  4.8553e-03,  1.1544e+01, -2.8488e-01,
         -2.9657e+00, -2.2956e+00,  6.9070e+00, -4.4489e+00, -7.0601e+00,
          5.8253e+00, -3.1000e+00,  3.5295e+00],
        [-2.2533e+00,  1.1872e-01, -3.4843e-03, -1.0215e+01,  4.1189e-01,
         -2.3052e+00,  3.2778e+00, -4.7182e+00,  2.9881e+00,  3.9972e+00,
         -3.0825e+00, -3.0815e+00, -3.0683e+00],
        [-8.9891e+00, -1.2579e-01,  1.4932e-03, -4.7517e-01, -2.2527e-01,
         -9.0926e-01, -6.2097e+00, -1.2974e+00,  1.7524e+00,  5.7490e+00,
         -1.8143e+01, -3.4321e-01, -3.8105e+00],
        [-1.6602e+00,  6.2424e-01,  7.1238e-01,  2.4861e-01,  1.1649e-01,
          1.6223e-02, -8.4172e-02, -9.4817e-01, -1.3435e-01, -1.1821e+00,
         -1.6839e+00, -1.3422e-02,  2.8678e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.5250, -4.5330, -2.5250, -2.6387,  3.4759, -2.5250, -2.5250,  3.0636,
        -2.5250,  0.0245, -0.8955, -2.3337, -2.5250], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.0179e-02,  1.7568e-01, -1.0179e-02, -3.8203e-03, -3.5104e+00,
         -1.0179e-02, -1.0179e-02, -3.4491e+00, -1.0179e-02, -3.1488e+00,
         -3.0845e-01, -4.6061e-03, -1.0179e-02],
        [-3.1109e-01,  1.2230e+00, -3.1153e-01,  3.2400e+00, -1.4943e+00,
         -3.1047e-01, -3.1151e-01, -1.9758e-01, -3.1154e-01,  3.6840e+00,
         -4.2899e+00,  1.2964e+01, -3.1092e-01],
        [-7.7125e-03, -1.8817e-01, -7.7125e-03, -1.2127e-02, -3.5807e+00,
         -7.7125e-03, -7.7125e-03, -3.5273e+00, -7.7125e-03, -3.4574e+00,
         -1.4078e-01, -1.8215e-02, -7.7124e-03],
        [-4.7418e-03, -2.0762e-01, -4.7417e-03, -1.7918e-02, -3.3317e+00,
         -4.7418e-03, -4.7417e-03, -3.3015e+00, -4.7417e-03, -3.2399e+00,
         -1.2576e-01, -2.7883e-02, -4.7421e-03],
        [ 3.1653e-01,  2.8862e-02,  3.1610e-01, -2.1932e+00,  2.9705e+00,
          3.1715e-01,  3.1611e-01,  1.6275e+00,  3.1608e-01, -2.0671e+00,
          4.9758e+00, -1.2923e+01,  3.1670e-01]], device='cuda:0'))])
loaded xi:  485.26944
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 946.50330641104
W_T_median: 707.5512584605915
W_T_pctile_5: -238.22847581189643
W_T_CVAR_5_pct: -367.3344080824147
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1771.4181634189404
Current xi:  [477.22922]
objective value function right now is: -1771.4181634189404
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [468.19278]
objective value function right now is: -1771.254211483633
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [458.214]
objective value function right now is: -1769.7310302848493
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [450.86615]
objective value function right now is: -1770.898821102251
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1773.315877693584
Current xi:  [444.1244]
objective value function right now is: -1773.315877693584
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [427.2867]
objective value function right now is: -1763.030918800279
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1774.0063840157372
Current xi:  [417.77438]
objective value function right now is: -1774.0063840157372
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [414.79697]
objective value function right now is: -1773.9542713740314
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [414.74173]
objective value function right now is: -1772.6627154597538
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [411.53214]
objective value function right now is: -1772.434168806931
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.71262]
objective value function right now is: -1766.5045651137025
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [409.19043]
objective value function right now is: -1757.82841545154
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [407.86963]
objective value function right now is: -1770.7666402059835
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [408.05164]
objective value function right now is: -1769.7912897194801
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1774.187256505223
Current xi:  [406.52036]
objective value function right now is: -1774.187256505223
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [404.9877]
objective value function right now is: -1771.8560961683695
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [403.92032]
objective value function right now is: -1771.5192830712253
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [402.07666]
objective value function right now is: -1773.942631564008
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [390.9882]
objective value function right now is: -1766.5749501014343
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [382.62338]
objective value function right now is: -1770.3629213165805
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [373.91226]
objective value function right now is: -1772.9646832700912
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [369.53162]
objective value function right now is: -1770.856708869017
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [363.03616]
objective value function right now is: -1770.0429579195118
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.71432]
objective value function right now is: -1769.50600940684
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [361.16315]
objective value function right now is: -1772.9312133871924
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [367.49738]
objective value function right now is: -1766.295719369627
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [370.27667]
objective value function right now is: -1773.50366904897
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [373.27594]
objective value function right now is: -1773.900268304661
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [375.0858]
objective value function right now is: -1773.4942805007854
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [380.154]
objective value function right now is: -1773.2464401948685
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [381.53763]
objective value function right now is: -1773.142897373904
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [384.10925]
objective value function right now is: -1770.2906803620795
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [386.4457]
objective value function right now is: -1773.4861093163977
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [374.5915]
objective value function right now is: -1769.6976290449231
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [366.32217]
objective value function right now is: -1770.7072300482891
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [365.00897]
objective value function right now is: -1770.9573840294654
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [363.71692]
objective value function right now is: -1770.983763655073
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [362.39478]
objective value function right now is: -1771.2685257905243
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [361.3224]
objective value function right now is: -1771.21258188786
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [360.02423]
objective value function right now is: -1771.4855907310957
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [359.13068]
objective value function right now is: -1768.874305793928
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [358.27332]
objective value function right now is: -1771.399356851218
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [357.1743]
objective value function right now is: -1770.9394578533772
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [356.26263]
objective value function right now is: -1771.603377349337
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.39526]
objective value function right now is: -1771.710619556154
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.54776]
objective value function right now is: -1770.8689883489487
new min fval from sgd:  -1774.1897587455765
new min fval from sgd:  -1774.2059197201922
new min fval from sgd:  -1774.2220780139628
new min fval from sgd:  -1774.2575911377396
new min fval from sgd:  -1774.2752736354414
new min fval from sgd:  -1774.2894450892943
new min fval from sgd:  -1774.305355919086
new min fval from sgd:  -1774.3146087454807
new min fval from sgd:  -1774.328008729009
new min fval from sgd:  -1774.3317206235515
new min fval from sgd:  -1774.3363271095982
new min fval from sgd:  -1774.3430755256875
new min fval from sgd:  -1774.3550180999378
new min fval from sgd:  -1774.3565043551057
new min fval from sgd:  -1774.3625118853947
new min fval from sgd:  -1774.3832419517869
new min fval from sgd:  -1774.3967224969506
new min fval from sgd:  -1774.4035790775383
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [356.5973]
objective value function right now is: -1773.59808965398
new min fval from sgd:  -1774.435270822386
new min fval from sgd:  -1774.4390689171153
new min fval from sgd:  -1774.439129112145
new min fval from sgd:  -1774.4429177406323
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [358.14645]
objective value function right now is: -1774.3955919474965
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [357.4542]
objective value function right now is: -1771.7286134662952
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [357.19345]
objective value function right now is: -1771.8526786964676
min fval:  -1774.4429177406323
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-15.2630, -11.7276],
        [ -5.2034, -13.2408],
        [ -6.6340,  12.1517],
        [ -4.2869,   7.1336],
        [ -1.2494,   0.9016],
        [ -1.2446,   0.9052],
        [ -7.0035,   8.8893],
        [ -1.2494,   0.9016],
        [ -1.2427,   0.8970],
        [ 11.4522,  -5.0557],
        [ -1.2495,   0.9040],
        [  9.8702,   4.5146],
        [  8.3925,   1.5642]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  1.2204,  -7.1250,   3.1421,   0.4528,  -2.5394,  -2.5391,   4.7455,
         -2.5394,  -2.5384, -11.9840,  -2.5400,  -9.5348, -10.1413],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -0.7661,  -0.2750,  -0.0658,  -0.0214,  -0.0321,  -0.0316,  -0.2502,
          -0.0321,  -0.0322,  -1.0032,  -0.0319,  -0.6814,  -0.1518],
        [ -1.1997,  -0.3476,  -0.1809,  -0.0727,  -0.0357,  -0.0352,  -0.3653,
          -0.0357,  -0.0358,  -1.4847,  -0.0355,  -0.8513,  -0.2739],
        [ -0.7661,  -0.2750,  -0.0658,  -0.0214,  -0.0321,  -0.0316,  -0.2502,
          -0.0321,  -0.0322,  -1.0032,  -0.0319,  -0.6814,  -0.1518],
        [  3.8795,   8.2111,  -1.3457,   0.0571,   0.2394,   0.2161,  -2.3082,
           0.2394,   0.2400,   9.2238,   0.2295,   4.6933,   2.0899],
        [ -0.7661,  -0.2750,  -0.0658,  -0.0214,  -0.0321,  -0.0316,  -0.2502,
          -0.0321,  -0.0322,  -1.0032,  -0.0319,  -0.6814,  -0.1518],
        [ -0.7661,  -0.2750,  -0.0658,  -0.0214,  -0.0321,  -0.0316,  -0.2502,
          -0.0321,  -0.0322,  -1.0032,  -0.0319,  -0.6814,  -0.1518],
        [  4.4521,   7.5200,  -2.8701,  -0.6129,   0.0128,   0.0150,  -5.4453,
           0.0128,   0.0107,   9.6855,   0.0168,   3.9931,   2.2236],
        [ -0.7661,  -0.2750,  -0.0658,  -0.0214,  -0.0321,  -0.0316,  -0.2502,
          -0.0321,  -0.0322,  -1.0032,  -0.0319,  -0.6814,  -0.1518],
        [ -4.9853,  -6.2496,   1.7640,   0.2188,   0.1031,   0.0908,   3.5455,
           0.1031,   0.0884,  -9.9547,   0.1070,  -3.5143,  -1.9399],
        [ -1.0534,  -0.3378,  -0.1463,  -0.0545,  -0.0339,  -0.0333,  -0.3353,
          -0.0339,  -0.0340,  -1.3156,  -0.0337,  -0.8095,  -0.2430],
        [ -0.7661,  -0.2750,  -0.0658,  -0.0214,  -0.0321,  -0.0316,  -0.2502,
          -0.0321,  -0.0322,  -1.0032,  -0.0319,  -0.6814,  -0.1518],
        [ -0.7661,  -0.2750,  -0.0658,  -0.0214,  -0.0321,  -0.0316,  -0.2502,
          -0.0321,  -0.0322,  -1.0032,  -0.0319,  -0.6814,  -0.1518],
        [ -3.1270,  -9.8734,   3.0344,   1.0024,   0.3262,   0.2887,   4.7351,
           0.3262,   0.3257, -10.4293,   0.3108,  -3.5267,  -2.1562]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.6227, -1.5046, -1.6227, -5.7559, -1.6227, -1.6227, -5.1152, -1.6227,
         4.1036, -1.5713, -1.6227, -1.6227,  4.1710], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 0.1967,  0.4735,  0.1967, -7.4744,  0.1967,  0.1967, -7.3995,  0.1967,
          8.4229,  0.3943,  0.1967,  0.1967,  9.8449]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 13.4521,   4.4869],
        [ -1.7280,   0.8860],
        [ -1.6849,   2.7601],
        [  0.1637,  13.2650],
        [ -8.1379,   8.7985],
        [ -1.7276,   0.8861],
        [-16.6545,   2.4409],
        [ 11.8978,   9.7272],
        [-12.6794,  -5.6896],
        [-13.5872,   0.4379],
        [ 10.4994,   8.8611],
        [ -2.1339,  -0.3729],
        [ 12.4936,  -0.8336]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -0.9385,  -3.4395,  -5.7811,   7.1640,   4.7816,  -3.4398,   1.1642,
          4.5565,  -3.2865,  11.6766,   6.4006,  -5.5456, -13.1008],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.0718e+00, -3.0744e-01, -3.1258e-01, -6.2808e-01, -4.4448e-01,
         -3.0749e-01, -1.0421e-01, -1.1589e+00, -3.5976e-01, -8.8749e-01,
         -1.1780e+00, -4.0043e-01, -8.0099e-01],
        [-1.9177e+00, -1.7765e-01, -3.4092e+00,  6.1774e+00,  2.1374e+00,
         -1.7770e-01,  7.5890e+00,  9.9981e-01,  7.5211e-01,  4.8755e+00,
         -4.0884e+00,  3.8675e-02, -8.7776e+00],
        [-1.0718e+00, -3.0744e-01, -3.1258e-01, -6.2808e-01, -4.4448e-01,
         -3.0749e-01, -1.0421e-01, -1.1589e+00, -3.5976e-01, -8.8749e-01,
         -1.1780e+00, -4.0043e-01, -8.0099e-01],
        [-4.6702e+00, -1.7595e-01, -4.9631e-01,  4.9861e+00,  2.9890e+00,
         -1.7577e-01,  4.4255e+00, -2.1811e+00,  2.3763e+00,  1.4820e+00,
         -2.3595e+00, -2.5642e-01, -2.0382e+00],
        [ 8.6723e-01, -2.1441e-01,  2.7060e-01,  2.2152e+00,  1.9462e+00,
         -2.1422e-01, -1.6314e-01, -1.5864e+00,  1.1157e+00,  3.4732e+00,
         -4.7883e-01,  2.3216e-01,  6.0543e-02],
        [-1.0718e+00, -3.0744e-01, -3.1258e-01, -6.2808e-01, -4.4448e-01,
         -3.0749e-01, -1.0421e-01, -1.1589e+00, -3.5976e-01, -8.8749e-01,
         -1.1780e+00, -4.0043e-01, -8.0099e-01],
        [-1.0718e+00, -3.0744e-01, -3.1258e-01, -6.2808e-01, -4.4448e-01,
         -3.0749e-01, -1.0421e-01, -1.1589e+00, -3.5976e-01, -8.8749e-01,
         -1.1780e+00, -4.0043e-01, -8.0099e-01],
        [-2.0482e+00, -2.6677e-01, -2.9557e-01, -5.3566e-01, -4.0400e-01,
         -2.6683e-01, -1.6189e-01, -1.4298e+00, -5.8919e-01, -4.3237e-01,
         -2.3333e+00, -1.5144e-01, -2.4601e+00],
        [-1.0718e+00, -3.0744e-01, -3.1258e-01, -6.2808e-01, -4.4448e-01,
         -3.0749e-01, -1.0421e-01, -1.1589e+00, -3.5976e-01, -8.8749e-01,
         -1.1780e+00, -4.0043e-01, -8.0099e-01],
        [ 4.7583e+00,  4.8411e-01,  9.3912e-02,  8.8842e+00,  3.0695e+00,
          4.8307e-01, -3.5466e-01,  1.3066e+01, -3.0784e+00, -1.1021e+01,
          6.2467e+00,  2.9213e+00,  6.5611e+00],
        [-1.4822e+00,  2.7935e-01,  8.0669e-02, -1.1444e+01, -2.2899e+00,
          2.7860e-01,  4.3607e+00, -3.1811e+00,  2.0715e+00,  4.1886e+00,
         -1.9377e+00, -3.2391e+00, -2.0790e+00],
        [-1.0359e+01, -1.3410e-01,  6.4402e-04, -1.7548e-01, -6.1433e-01,
         -1.3404e-01, -2.6307e+00, -1.9553e+00,  5.3179e+00,  6.3843e+00,
         -1.6833e+01, -4.1910e-01, -5.1631e+00],
        [-1.0718e+00, -3.0744e-01, -3.1258e-01, -6.2808e-01, -4.4448e-01,
         -3.0749e-01, -1.0421e-01, -1.1589e+00, -3.5976e-01, -8.8749e-01,
         -1.1780e+00, -4.0043e-01, -8.0099e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.6445, -5.5752, -2.6445, -4.6217,  3.8919, -2.6445, -2.6445, -0.8994,
        -2.6445,  1.0167, -2.1829, -7.7664, -2.6445], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-5.1414e-02,  2.6288e-01, -5.1414e-02,  4.3568e-03, -6.0505e+00,
         -5.1414e-02, -5.1414e-02, -8.4133e-02, -5.1414e-02, -3.8431e+00,
         -1.1984e+00, -2.7140e-02, -5.1414e-02],
        [-3.1886e-01,  1.1451e+00, -3.1886e-01,  9.8182e-01, -1.8943e+00,
         -3.1886e-01, -3.1886e-01,  5.1279e-01, -3.1886e-01,  3.2085e+00,
         -3.4437e+00,  1.3100e+01, -3.1886e-01],
        [-3.3009e-02, -3.9726e-01, -3.3009e-02, -6.2208e-02, -5.9585e+00,
         -3.3009e-02, -3.3009e-02, -7.8497e-02, -3.3009e-02, -4.6745e+00,
         -7.4036e-01, -6.6505e-02, -3.3009e-02],
        [-7.7598e-03, -4.4457e-01, -7.7598e-03, -4.2591e-02, -5.5822e+00,
         -7.7598e-03, -7.7598e-03, -3.6605e-02, -7.7598e-03, -4.7108e+00,
         -5.1341e-01, -1.1290e-01, -7.7598e-03],
        [ 3.2671e-01,  8.5333e-02,  3.2671e-01, -3.3977e-01,  3.4022e+00,
          3.2671e-01,  3.2671e-01,  9.0860e-01,  3.2671e-01, -1.5650e+00,
          4.1903e+00, -1.2974e+01,  3.2671e-01]], device='cuda:0'))])
xi:  [358.06625]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1608.612909459365
W_T_median: 1176.8912121210587
W_T_pctile_5: 359.4620419928046
W_T_CVAR_5_pct: 153.62509313568148
Average q (qsum/M+1):  54.763608870967744
Optimal xi:  [358.06625]
Expected(across Rb) median(across samples) p_equity:  0.5547688990831375
obj fun:  tensor(-1774.4429, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-14.4542,   6.3244],
        [-21.6430,   5.7563],
        [ -4.3192,  11.5413],
        [ 10.5369,  -0.7778],
        [ -1.2376,   0.2472],
        [ 10.1247,  -1.3231],
        [ -3.9736, -11.3762],
        [ -1.2364,   0.2468],
        [ -1.2372,   0.2485],
        [-20.3076,   1.1323],
        [ -1.7628,   0.4485],
        [ 10.9581,  -1.1977],
        [ -1.2380,   0.2488]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 1.6353,  2.3077,  2.7537, -9.5389, -3.3575, -9.5667, -3.0303, -3.3574,
        -3.3527,  0.5916, -3.5792, -9.5250, -3.3529], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 4.8416e+00,  8.3440e+00,  3.1889e+00, -6.5168e+00, -1.0448e-02,
         -5.2987e+00, -1.3131e+01, -1.0101e-02, -1.5220e-03,  7.0981e+00,
         -4.9385e-02, -6.1850e+00, -8.3122e-04],
        [-1.7364e-02, -2.1530e-02, -5.3872e-01, -1.7144e-01, -4.5278e-03,
         -9.4956e-02, -1.3985e-01, -4.5235e-03, -4.5531e-03, -9.8032e-03,
         -2.5353e-03, -2.0813e-01, -4.5509e-03],
        [-4.1474e+00, -2.8300e+00, -4.8798e+00,  6.5471e+00, -2.1112e-03,
          3.7359e+00,  1.2993e+01, -4.7161e-03,  1.3555e-02, -1.3893e+00,
         -3.5713e-01,  7.6811e+00,  1.3742e-02],
        [-3.9186e+00, -6.5791e+00,  1.8838e+00, -4.6285e+00, -2.9657e-02,
         -3.9012e+00,  7.3106e+00, -2.6545e-02, -2.6923e-02, -5.1802e+00,
          3.9985e-01, -6.0662e+00, -2.5000e-02],
        [-1.4405e-01,  4.3095e-02, -5.6602e+00,  1.0200e+01,  7.5546e-02,
          7.8346e+00,  5.3538e-01,  7.5140e-02,  8.0183e-02,  6.0962e-02,
          2.2457e-01,  1.0239e+01,  8.2183e-02],
        [-1.8629e+00, -1.8299e-01, -4.1163e+00,  3.7973e+00, -2.6630e-03,
          1.9743e+00,  7.1164e+00, -1.8633e-03, -3.8655e-03,  7.1234e-01,
         -3.4499e-02,  5.0399e+00, -4.8408e-03],
        [-1.7363e-02, -2.1531e-02, -5.3868e-01, -1.7143e-01, -4.5271e-03,
         -9.4948e-02, -1.3983e-01, -4.5227e-03, -4.5524e-03, -9.8038e-03,
         -2.5349e-03, -2.0811e-01, -4.5502e-03],
        [ 5.2233e+00,  9.4775e+00, -6.0526e+00, -1.5990e-02, -3.1497e-01,
         -1.6015e-02,  2.5455e+00, -3.1228e-01, -3.4322e-01,  8.1954e+00,
          1.2771e+00, -1.5643e-02, -3.4343e-01],
        [-1.7363e-02, -2.1530e-02, -5.3872e-01, -1.7144e-01, -4.5278e-03,
         -9.4956e-02, -1.3985e-01, -4.5235e-03, -4.5531e-03, -9.8032e-03,
         -2.5353e-03, -2.0813e-01, -4.5509e-03],
        [-1.7477e-02, -2.1599e-02, -5.4394e-01, -1.7278e-01, -4.6139e-03,
         -9.5884e-02, -1.4204e-01, -4.6095e-03, -4.6396e-03, -9.7664e-03,
         -2.5777e-03, -2.0974e-01, -4.6374e-03],
        [-4.3845e+00, -3.3116e+00, -3.6642e+00, -1.8621e+00,  2.4023e-01,
         -2.0626e+00,  1.4154e+01,  2.3393e-01,  2.4558e-01, -2.3118e+00,
         -5.2930e-01, -2.6583e+00,  2.4745e-01],
        [-1.8535e+00, -1.3782e-01, -3.8451e+00,  9.1882e-01, -2.2488e-02,
          5.1900e-01,  6.6365e+00, -2.2278e-02, -2.3446e-02,  6.5286e-01,
         -3.3087e-02,  1.4662e+00, -2.4032e-02],
        [-3.4185e+00, -2.2988e+00, -4.3975e+00,  5.4071e+00,  1.3531e-02,
          2.6422e+00,  1.1565e+01,  2.1897e-02,  1.7482e-02, -1.4883e+00,
         -3.0650e-01,  6.4334e+00,  1.5727e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([  2.1748,  -1.7139,  -2.7922,  -1.2006,  -5.1024,  -3.9849,  -1.7140,
        -11.0701,  -1.7139,  -1.6977,  -1.6448,  -4.0085,  -2.6904],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[18.7463, -0.0223, -7.4147, -6.8402, -7.3919, -3.7230, -0.0223, -9.4159,
         -0.0223, -0.0223, -7.2307, -3.0364, -5.2623]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.8584,   1.5354],
        [-12.1717,   1.3800],
        [ -1.9192,   1.5423],
        [-10.5652,   0.1222],
        [ 11.1492,  -0.6810],
        [  8.7778,   8.4348],
        [ -5.2611,   2.4817],
        [ -9.0888,   8.2833],
        [ -1.7881,   0.3896],
        [ 12.2036,   7.3683],
        [ -1.6626,   0.3282],
        [  3.4836,   4.1266],
        [-14.9405,  -5.0585]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -5.0680,  -1.1817,  -5.1620,   8.8596, -10.8796,   1.3780,  -8.1331,
          4.1703,  -4.4849,   3.3561,  -4.4471,  -3.4030,  -2.0440],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 6.9390e-01, -3.0384e+00,  6.9157e-01, -3.6820e+00,  1.9689e+01,
         -4.4868e-01,  8.7773e-01, -5.7250e+00,  6.2621e-03, -6.4982e-01,
          2.3670e-03,  1.9306e+00,  8.7319e+00],
        [-8.3391e-01,  7.4143e+00, -9.1391e-01,  2.5283e+00, -1.3061e+01,
         -2.5758e+00, -1.6309e+00,  5.3868e+00,  4.5328e-01, -5.9747e-01,
          2.1378e-01, -8.2045e-01, -1.3988e+00],
        [-1.1901e+00, -1.9165e-01, -1.2270e+00, -8.4861e-01, -1.8080e-01,
         -1.6561e+00, -5.7497e-01,  1.8336e+00, -6.3104e-02, -1.5528e+00,
         -4.3668e-02,  1.1323e+00, -1.6608e+00],
        [-1.2081e+00, -2.4028e-01, -1.2439e+00, -7.6965e-01, -1.1340e-01,
         -1.7027e+00, -6.0399e-01,  1.8839e+00, -7.7582e-02, -1.4686e+00,
         -4.8199e-02,  1.1807e+00, -1.7475e+00],
        [-1.2143e+00, -2.6035e-01, -1.2496e+00, -7.4017e-01, -8.8292e-02,
         -1.7202e+00, -6.1481e-01,  1.9021e+00, -8.3457e-02, -1.4368e+00,
         -4.9839e-02,  1.1984e+00, -1.7790e+00],
        [-1.1902e+00, -1.9198e-01, -1.2271e+00, -8.4795e-01, -1.8035e-01,
         -1.6567e+00, -5.7514e-01,  1.8339e+00, -6.3188e-02, -1.5521e+00,
         -4.3694e-02,  1.1326e+00, -1.6615e+00],
        [-1.1861e+00, -1.8152e-01, -1.2232e+00, -8.6774e-01, -1.9590e-01,
         -1.6430e+00, -5.6864e-01,  1.8224e+00, -6.0171e-02, -1.5736e+00,
         -4.2682e-02,  1.1219e+00, -1.6403e+00],
        [-1.2123e+00, -2.5317e-01, -1.2477e+00, -7.5078e-01, -9.6970e-02,
         -1.7133e+00, -6.1113e-01,  1.8959e+00, -8.1423e-02, -1.4483e+00,
         -4.9296e-02,  1.1925e+00, -1.7679e+00],
        [-5.3891e-02,  5.6156e-01,  1.3262e-02, -3.0827e+00, -2.9470e+00,
         -1.6255e+00, -1.1565e+00,  1.6963e+01, -2.5347e-02, -5.1542e+00,
         -1.0665e-01, -8.6043e+00,  3.4776e+00],
        [-2.9821e-01,  2.3003e+00, -2.9367e-01,  2.2694e+00, -5.7721e+00,
         -3.6640e+00,  3.1122e-02, -6.7126e+00, -5.2613e-02, -4.5773e+00,
         -4.1336e-02, -7.9816e+00,  5.7028e+00],
        [-1.1958e+00, -2.0482e-01, -1.2323e+00, -8.2639e-01, -1.6107e-01,
         -1.6682e+00, -5.8353e-01,  1.8487e+00, -6.7162e-02, -1.5294e+00,
         -4.5029e-02,  1.1469e+00, -1.6860e+00],
        [ 2.0976e-01,  1.1155e-01,  2.6676e-01, -3.9453e+00,  1.4654e+00,
          1.9091e+00,  9.4995e-02, -5.3937e-01, -5.7011e-02,  6.3653e-01,
         -7.3564e-02, -3.4884e+00, -1.1061e+00],
        [-1.2040e+00, -2.2745e-01, -1.2400e+00, -7.8973e-01, -1.3009e-01,
         -1.6902e+00, -5.9691e-01,  1.8718e+00, -7.3869e-02, -1.4902e+00,
         -4.7122e-02,  1.1691e+00, -1.7262e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 3.8303, -2.9933, -4.4105, -4.3602, -4.3411, -4.4102, -4.4222, -4.3479,
        -5.2801, -0.3422, -4.3964, -4.6635, -4.3729], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.4597e+01,  2.9580e+00,  6.1799e-01,  6.8095e-01,  7.0424e-01,
          6.1835e-01,  6.0439e-01,  6.9639e-01,  3.8385e+00, -2.1068e+01,
          6.3675e-01, -8.7176e-01,  6.6572e-01],
        [ 8.8606e-01,  9.5761e-01, -1.2842e+00, -1.3684e+00, -1.3999e+00,
         -1.2852e+00, -1.2602e+00, -1.3874e+00,  7.9450e-01, -3.9719e+00,
         -1.3060e+00,  2.8190e+00, -1.3454e+00],
        [-2.7139e+01, -2.3062e+01, -8.8052e-02, -1.0815e-01, -1.1696e-01,
         -8.8179e-02, -8.4074e-02, -1.1379e-01, -8.5839e-03, -8.2218e-02,
         -9.3360e-02, -3.1511e-01, -1.0269e-01],
        [-2.5198e+01, -2.0092e+01, -9.1118e-02, -1.1418e-01, -1.2414e-01,
         -9.1264e-02, -8.6509e-02, -1.2057e-01, -3.4988e-03, -8.0575e-02,
         -9.7268e-02, -2.8181e-01, -1.0797e-01],
        [ 1.1755e+00,  1.1353e+00,  1.2961e+00,  1.3741e+00,  1.4035e+00,
          1.2963e+00,  1.2821e+00,  1.3944e+00,  2.4930e-01,  9.0823e+00,
          1.3206e+00, -1.3931e+00,  1.3566e+00]], device='cuda:0'))])
loaded xi:  737.17993
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 946.50330641104
W_T_median: 707.5512584605915
W_T_pctile_5: -238.22847581189643
W_T_CVAR_5_pct: -367.3344080824147
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1887.2033993117702
Current xi:  [730.84247]
objective value function right now is: -1887.2033993117702
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [720.773]
objective value function right now is: -1885.1575134845111
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1887.9952810754528
Current xi:  [709.0124]
objective value function right now is: -1887.9952810754528
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [697.57745]
objective value function right now is: -1887.8912909071512
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1889.8609193190407
Current xi:  [687.96405]
objective value function right now is: -1889.8609193190407
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1892.992765339738
Current xi:  [678.58716]
objective value function right now is: -1892.992765339738
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [669.82416]
objective value function right now is: -1880.5839752034713
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1894.5248424306365
Current xi:  [660.8819]
objective value function right now is: -1894.5248424306365
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [652.6254]
objective value function right now is: -1894.1688845228898
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [645.2936]
objective value function right now is: -1893.3750751627936
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1895.5874634315417
Current xi:  [640.0953]
objective value function right now is: -1895.5874634315417
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [634.74854]
objective value function right now is: -1893.6382226716405
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [630.2587]
objective value function right now is: -1891.3858126155696
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1897.5188838674928
Current xi:  [625.1321]
objective value function right now is: -1897.5188838674928
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [623.7543]
objective value function right now is: -1878.7223017014041
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [621.3607]
objective value function right now is: -1895.9952034358853
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [618.31085]
objective value function right now is: -1895.079712039747
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [615.47986]
objective value function right now is: -1891.8268780627488
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [614.5128]
objective value function right now is: -1894.0611609670614
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [612.3272]
objective value function right now is: -1892.8093931994997
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [610.8044]
objective value function right now is: -1893.9205475871297
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [609.5562]
objective value function right now is: -1894.5567127800077
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [609.1924]
objective value function right now is: -1896.052064909539
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [607.11615]
objective value function right now is: -1896.4988732967347
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [607.56757]
objective value function right now is: -1896.8639473631047
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [606.0641]
objective value function right now is: -1894.258867622554
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [606.57355]
objective value function right now is: -1890.7048986247398
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [605.79407]
objective value function right now is: -1889.1239480926154
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [603.42285]
objective value function right now is: -1896.7036217125772
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [603.81665]
objective value function right now is: -1896.2672217062686
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [603.9625]
objective value function right now is: -1895.525757347055
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [604.0292]
objective value function right now is: -1896.0522000998617
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [604.0822]
objective value function right now is: -1896.6540030402439
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [603.9023]
objective value function right now is: -1897.3384816664714
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [604.9133]
objective value function right now is: -1896.9567597240596
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [604.82745]
objective value function right now is: -1897.2660612607947
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1898.7934721526103
Current xi:  [605.0804]
objective value function right now is: -1898.7934721526103
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1899.0486977609974
Current xi:  [604.72186]
objective value function right now is: -1899.0486977609974
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [605.04926]
objective value function right now is: -1898.9348953720225
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [605.42194]
objective value function right now is: -1898.931512067636
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [605.3605]
objective value function right now is: -1899.025544197077
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [605.96497]
objective value function right now is: -1897.9251923197733
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [606.30707]
objective value function right now is: -1898.7625174621407
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [606.31067]
objective value function right now is: -1898.8646587609169
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [606.2074]
objective value function right now is: -1898.2322874904826
new min fval from sgd:  -1899.102192124989
new min fval from sgd:  -1899.1224706769333
new min fval from sgd:  -1899.1335132792244
new min fval from sgd:  -1899.2314992215938
new min fval from sgd:  -1899.2330376668879
new min fval from sgd:  -1899.2441743493202
new min fval from sgd:  -1899.2777667506753
new min fval from sgd:  -1899.3145448666903
new min fval from sgd:  -1899.3205488916353
new min fval from sgd:  -1899.3444320239594
new min fval from sgd:  -1899.4098389785538
new min fval from sgd:  -1899.4715790738728
new min fval from sgd:  -1899.5085505225836
new min fval from sgd:  -1899.5315497938536
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [606.24414]
objective value function right now is: -1899.1746661292702
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [605.9436]
objective value function right now is: -1898.718921815939
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [606.3593]
objective value function right now is: -1899.2314101813306
new min fval from sgd:  -1899.532088463755
new min fval from sgd:  -1899.537674642975
new min fval from sgd:  -1899.5425248937206
new min fval from sgd:  -1899.5435534208523
new min fval from sgd:  -1899.5463068833017
new min fval from sgd:  -1899.5495173898798
new min fval from sgd:  -1899.553192760303
new min fval from sgd:  -1899.5555288262767
new min fval from sgd:  -1899.5562564042305
new min fval from sgd:  -1899.5599215061466
new min fval from sgd:  -1899.562745573381
new min fval from sgd:  -1899.5699754375617
new min fval from sgd:  -1899.5740281404894
new min fval from sgd:  -1899.5742176666242
new min fval from sgd:  -1899.57480036708
new min fval from sgd:  -1899.577034958521
new min fval from sgd:  -1899.578715737766
new min fval from sgd:  -1899.582146949952
new min fval from sgd:  -1899.5852017895716
new min fval from sgd:  -1899.58658633408
new min fval from sgd:  -1899.5881314798044
new min fval from sgd:  -1899.590251054666
new min fval from sgd:  -1899.590797201116
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [606.3014]
objective value function right now is: -1899.585833227492
new min fval from sgd:  -1899.5933247658045
new min fval from sgd:  -1899.5933570930874
new min fval from sgd:  -1899.5961866381974
new min fval from sgd:  -1899.5988595806705
new min fval from sgd:  -1899.602453962237
new min fval from sgd:  -1899.606226788403
new min fval from sgd:  -1899.6119603736042
new min fval from sgd:  -1899.6149570783036
new min fval from sgd:  -1899.6156858443235
new min fval from sgd:  -1899.619552992773
new min fval from sgd:  -1899.620133749005
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [606.3338]
objective value function right now is: -1899.5106354252687
min fval:  -1899.620133749005
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-13.4592,   5.7673],
        [-20.2289,   5.7117],
        [ -3.4201,  13.1972],
        [ 13.9069,  -0.6138],
        [ -1.0623,   0.2353],
        [ 13.7831,  -4.7689],
        [ -4.6385, -14.8536],
        [ -1.0623,   0.2353],
        [ -1.0623,   0.2353],
        [-15.6384,  -2.2530],
        [ -1.0623,   0.2353],
        [ 14.5677,  -1.3643],
        [ -1.0623,   0.2353]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  0.8383,   1.9023,   2.9223, -12.2899,  -3.2290, -11.1119,  -3.5562,
         -3.2290,  -3.2290,  -0.9967,  -3.2290, -11.9714,  -3.2290],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 2.5980e+00,  5.6116e+00,  4.5611e+00, -8.2917e+00,  4.9828e-02,
         -5.3023e+00, -1.0504e+01,  4.9828e-02,  4.9828e-02,  2.4209e+00,
          4.9828e-02, -7.9257e+00,  4.9828e-02],
        [-5.4874e-02, -8.9600e-02, -4.7439e-01, -1.5455e-01, -1.6735e-02,
         -1.7620e-01, -3.1573e-01, -1.6735e-02, -1.6735e-02, -6.6148e-02,
         -1.6735e-02, -2.0936e-01, -1.6735e-02],
        [-1.5052e+00, -2.3534e+00, -3.9739e+00,  2.1386e+00, -1.0956e-02,
          1.5281e+00,  7.7218e+00, -1.0956e-02, -1.0956e-02, -4.1795e-01,
         -1.0956e-02,  2.8309e+00, -1.0956e-02],
        [-5.1411e+00, -8.4133e+00, -1.9677e+00, -1.6531e+00,  5.3013e-02,
         -8.9013e+00,  7.6226e+00,  5.3013e-02,  5.3013e-02, -5.7207e+00,
          5.3013e-02, -5.4066e+00,  5.3013e-02],
        [ 1.2093e+00,  1.3738e+00, -8.3800e+00,  1.1290e+01,  2.7523e-02,
          8.4515e+00,  7.8851e+00,  2.7523e-02,  2.7523e-02,  2.9202e+00,
          2.7523e-02,  1.2098e+01,  2.7523e-02],
        [-5.4874e-02, -8.9600e-02, -4.7439e-01, -1.5455e-01, -1.6735e-02,
         -1.7620e-01, -3.1573e-01, -1.6735e-02, -1.6735e-02, -6.6148e-02,
         -1.6735e-02, -2.0936e-01, -1.6735e-02],
        [-5.4874e-02, -8.9600e-02, -4.7439e-01, -1.5455e-01, -1.6735e-02,
         -1.7620e-01, -3.1573e-01, -1.6735e-02, -1.6735e-02, -6.6148e-02,
         -1.6735e-02, -2.0936e-01, -1.6735e-02],
        [ 6.7054e+00,  1.0720e+01, -7.0846e+00, -9.9006e-04, -7.5647e-02,
         -7.7222e-03,  4.7297e+00, -7.5647e-02, -7.5647e-02,  9.0898e+00,
         -7.5647e-02, -1.8718e-03, -7.5647e-02],
        [-5.4874e-02, -8.9600e-02, -4.7439e-01, -1.5455e-01, -1.6735e-02,
         -1.7620e-01, -3.1573e-01, -1.6735e-02, -1.6735e-02, -6.6148e-02,
         -1.6735e-02, -2.0936e-01, -1.6735e-02],
        [-5.4874e-02, -8.9600e-02, -4.7439e-01, -1.5455e-01, -1.6735e-02,
         -1.7620e-01, -3.1573e-01, -1.6735e-02, -1.6735e-02, -6.6148e-02,
         -1.6735e-02, -2.0936e-01, -1.6735e-02],
        [-3.6696e+00, -5.0263e+00, -4.7067e+00, -1.1263e+00,  1.5073e-02,
         -8.1770e+00,  1.4555e+01,  1.5073e-02,  1.5073e-02, -3.9334e+00,
          1.5073e-02, -4.6841e+00,  1.5073e-02],
        [-5.4874e-02, -8.9600e-02, -4.7439e-01, -1.5455e-01, -1.6735e-02,
         -1.7620e-01, -3.1573e-01, -1.6735e-02, -1.6735e-02, -6.6148e-02,
         -1.6735e-02, -2.0936e-01, -1.6735e-02],
        [-5.4874e-02, -8.9600e-02, -4.7439e-01, -1.5455e-01, -1.6735e-02,
         -1.7620e-01, -3.1573e-01, -1.6735e-02, -1.6735e-02, -6.6148e-02,
         -1.6735e-02, -2.0936e-01, -1.6735e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([  4.1985,  -1.6660,  -4.2294,   0.3991,  -6.3848,  -1.6660,  -1.6660,
        -11.4532,  -1.6660,  -1.6660,  -1.1496,  -1.6660,  -1.6660],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 15.2891,   0.0182,  -3.3646, -10.6392, -10.1862,   0.0182,   0.0182,
          -8.1279,   0.0182,   0.0182, -10.2208,   0.0182,   0.0182]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.1005,   0.3638],
        [ -2.1278,   0.3935],
        [ -2.0855,   0.3710],
        [-14.7694,   0.4069],
        [ 14.0232,  -0.6696],
        [ 12.7375,   9.3752],
        [ -6.3283,   3.6610],
        [ -7.1813,   7.1696],
        [ -2.1007,   0.3676],
        [ 15.3456,   8.7492],
        [ -2.0851,   0.3727],
        [  2.7789,   9.8703],
        [-17.5457,  -7.0387]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -4.3850,  -4.3971,  -4.4051,  11.3932, -14.4880,  -0.7122,  -8.8987,
          3.1105,  -4.3848,   1.9338,  -4.4096,  -5.1540,  -2.5523],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-9.8897e-03, -3.3020e-02, -1.7136e-02, -3.4323e+00,  1.3030e+01,
          1.0823e-01,  1.1130e+00, -7.9618e+00, -1.1818e-02, -2.5483e+00,
         -1.8772e-02,  3.3460e+00,  5.0162e+00],
        [ 5.2748e-01,  7.5599e-01,  5.8523e-01,  2.3924e+00,  1.9269e+00,
         -1.3023e+00, -4.9724e+00,  5.5765e+00,  5.2947e-01, -1.8267e+00,
          6.0189e-01, -7.9335e-01,  7.7073e-01],
        [ 7.5199e-02,  7.6758e-02,  7.4556e-02, -7.7974e-01, -2.5655e+00,
         -3.9130e+00, -4.3100e-01, -1.8646e+00,  7.5461e-02, -6.1575e-01,
          7.4625e-02, -2.1880e-01, -1.0244e+00],
        [ 1.3604e-01,  1.4087e-01,  1.3520e-01, -6.1480e-01, -2.3373e+00,
         -4.9916e+00, -2.4322e-01, -2.7491e+00,  1.3684e-01, -3.8246e-01,
          1.3543e-01, -8.1658e-02, -1.1121e+00],
        [ 1.6622e-01,  1.7391e-01,  1.6537e-01, -5.1889e-01, -2.2082e+00,
         -5.4207e+00, -1.7523e-01, -3.0683e+00,  1.6751e-01, -3.3665e-01,
          1.6590e-01, -4.8414e-02, -1.0649e+00],
        [ 7.5688e-02,  7.7267e-02,  7.5043e-02, -7.7877e-01, -2.5645e+00,
         -3.9257e+00, -4.2882e-01, -1.8748e+00,  7.5952e-02, -6.1203e-01,
          7.5113e-02, -2.1686e-01, -1.0266e+00],
        [ 6.4219e-02,  6.5201e-02,  6.3600e-02, -7.9908e-01, -2.5784e+00,
         -3.5916e+00, -4.8348e-01, -1.6240e+00,  6.4416e-02, -7.1406e-01,
          6.3646e-02, -2.6830e-01, -9.6012e-01],
        [ 1.5420e-01,  1.6067e-01,  1.5336e-01, -5.5725e-01, -2.2591e+00,
         -5.2524e+00, -2.0144e-01, -2.9435e+00,  1.5527e-01, -3.5248e-01,
          1.5374e-01, -5.9919e-02, -1.0891e+00],
        [ 2.0370e-01,  1.4081e-01,  1.3977e-01, -3.0488e+00,  5.6713e-01,
         -1.0435e+00, -6.9495e-01,  1.4859e+01,  1.8825e-01, -2.7493e+00,
          1.3361e-01, -9.5947e+00,  5.4384e+00],
        [ 7.4668e-03, -1.5277e-03,  4.7577e-03,  1.8761e+00, -9.0546e+00,
         -9.6620e-01, -1.8766e-02, -1.4063e+01,  1.2785e-03, -3.3796e+00,
          3.8947e-03,  3.7752e+00,  1.2511e+01],
        [ 8.8096e-02,  9.0136e-02,  8.7377e-02, -7.4872e-01, -2.5266e+00,
         -4.2051e+00, -3.7874e-01, -2.1102e+00,  8.8427e-02, -5.3494e-01,
          8.7452e-02, -1.7494e-01, -1.0710e+00],
        [-5.3228e-02, -5.8211e-02, -5.1256e-02, -6.2772e+00,  2.1627e-01,
          5.6089e-01,  4.8808e-02, -6.2772e+00, -6.1015e-02,  1.8609e+00,
         -4.5081e-02, -4.3946e+00, -1.9860e+00],
        [ 1.1691e-01,  1.2034e-01,  1.1608e-01, -6.7133e-01, -2.4164e+00,
         -4.7035e+00, -2.9085e-01, -2.5273e+00,  1.1748e-01, -4.2641e-01,
          1.1621e-01, -1.1103e-01, -1.1152e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 5.5366, -2.9664, -4.8611, -4.5387, -4.4461, -4.8568, -4.9710, -4.4807,
        -5.8855, -2.4258, -4.7635, -5.7604, -4.6121], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.5822e+01,  3.4112e+00,  4.3126e-03, -7.9418e-02, -1.1670e-01,
          3.4975e-03,  2.1999e-02, -1.0161e-01,  3.2326e+00, -3.2130e+00,
         -1.6402e-02,  1.2752e-02, -5.5963e-02],
        [ 9.5699e-01,  8.8540e-01, -2.7353e+00, -3.5652e+00, -3.8152e+00,
         -2.7461e+00, -2.4637e+00, -3.7215e+00,  8.1216e-01, -5.5617e+00,
         -2.9808e+00,  5.1203e+00, -3.3705e+00],
        [-2.7906e+01, -2.4492e+01, -9.1423e-03, -6.8296e-03, -5.9113e-03,
         -9.1118e-03, -9.9843e-03, -6.2739e-03, -2.4328e-01, -7.5863e-02,
         -8.4786e-03, -4.6804e-03, -7.4338e-03],
        [-2.6851e+01, -2.3422e+01, -8.7131e-03, -6.6345e-03, -5.8557e-03,
         -8.6880e-03, -9.3539e-03, -6.1525e-03, -2.4393e-01, -7.0706e-02,
         -8.1402e-03, -2.6841e-03, -7.1826e-03],
        [ 1.2482e+00,  1.6967e+00,  2.8951e+00,  3.8134e+00,  4.0924e+00,
          2.9068e+00,  2.5992e+00,  3.9887e+00,  6.5848e-01,  1.1082e+01,
          3.1667e+00, -3.8815e+00,  3.5980e+00]], device='cuda:0'))])
xi:  [606.32104]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 922.0591181836132
W_T_median: 774.5610151343033
W_T_pctile_5: 606.3497710549311
W_T_CVAR_5_pct: 296.6718439665235
Average q (qsum/M+1):  51.70800387474798
Optimal xi:  [606.32104]
Expected(across Rb) median(across samples) p_equity:  0.24567879286284247
obj fun:  tensor(-1899.6201, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -1.3593,  -0.0983],
        [ -1.3592,  -0.0981],
        [ -1.3592,  -0.0982],
        [ -7.8370,   9.4666],
        [ -1.3648,  -0.1025],
        [-25.8194,   5.7097],
        [ -1.3593,  -0.0983],
        [ -5.1386,  14.5854],
        [ -1.6162,  -0.1445],
        [ -1.3591,  -0.0981],
        [ 11.8271,  -0.6396],
        [ -1.3592,  -0.0982],
        [ -3.0647, -11.8478]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.5698,  -3.5700,  -3.5699,  -0.0363,  -3.5664,   4.3364,  -3.5698,
          1.2994,  -3.4794,  -3.5700, -10.4544,  -3.5699,  -2.3601],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-8.6133e-03, -8.6132e-03, -8.6134e-03, -2.4693e-01, -8.5864e-03,
         -3.1614e-01, -8.6134e-03, -5.5529e-01, -6.6706e-03, -8.6134e-03,
         -2.1155e-01, -8.6134e-03, -3.0015e-01],
        [-9.5185e-02, -9.3790e-02, -9.4951e-02,  1.1445e+00, -9.0904e-02,
          5.4003e+00, -9.5156e-02,  2.6075e+00, -1.6895e-01, -9.4088e-02,
         -1.2461e+01, -9.4808e-02, -1.1721e+01],
        [-8.6133e-03, -8.6132e-03, -8.6134e-03, -2.4693e-01, -8.5864e-03,
         -3.1614e-01, -8.6134e-03, -5.5529e-01, -6.6706e-03, -8.6134e-03,
         -2.1155e-01, -8.6134e-03, -3.0015e-01],
        [-1.2864e-02, -1.6180e-02, -1.5039e-02,  1.2379e+00,  1.1042e-02,
          5.6305e+00, -1.4003e-02,  2.8683e+00,  1.1627e-02, -1.6681e-02,
         -1.3147e+01, -1.5424e-02, -1.1924e+01],
        [-8.6133e-03, -8.6132e-03, -8.6134e-03, -2.4693e-01, -8.5864e-03,
         -3.1614e-01, -8.6134e-03, -5.5529e-01, -6.6706e-03, -8.6134e-03,
         -2.1155e-01, -8.6134e-03, -3.0015e-01],
        [-8.6133e-03, -8.6132e-03, -8.6134e-03, -2.4693e-01, -8.5864e-03,
         -3.1614e-01, -8.6133e-03, -5.5529e-01, -6.6706e-03, -8.6133e-03,
         -2.1155e-01, -8.6134e-03, -3.0015e-01],
        [ 2.9468e-01,  2.9418e-01,  2.9441e-01, -3.0739e+00,  3.1408e-01,
         -8.3359e+00,  2.9454e-01, -1.9151e+00,  4.7476e-01,  2.9411e-01,
          1.2268e+01,  2.9432e-01,  1.1707e+01],
        [-8.6133e-03, -8.6132e-03, -8.6134e-03, -2.4693e-01, -8.5864e-03,
         -3.1614e-01, -8.6134e-03, -5.5529e-01, -6.6706e-03, -8.6134e-03,
         -2.1155e-01, -8.6134e-03, -3.0015e-01],
        [-7.3727e-03, -7.3720e-03, -7.3729e-03, -2.9458e-01, -7.3615e-03,
         -1.3543e+00, -7.3728e-03, -8.4815e-01, -6.1090e-03, -7.3723e-03,
         -1.7226e-01, -7.3727e-03,  9.2280e-01],
        [-8.6133e-03, -8.6132e-03, -8.6134e-03, -2.4693e-01, -8.5864e-03,
         -3.1614e-01, -8.6134e-03, -5.5529e-01, -6.6706e-03, -8.6134e-03,
         -2.1155e-01, -8.6134e-03, -3.0015e-01],
        [-6.0157e-02, -6.2065e-02, -6.1990e-02, -8.8182e-01, -4.1217e-02,
         -7.3537e+00, -6.1182e-02, -3.8081e+00,  9.6333e-02, -6.2741e-02,
          1.5537e+01, -6.2164e-02,  1.2288e+01],
        [-1.2828e-01, -1.2821e-01, -1.2826e-01,  1.1715e+00, -1.3296e-01,
          5.2859e+00, -1.2826e-01,  2.0798e+00, -2.3220e-01, -1.2817e-01,
         -1.1812e+01, -1.2818e-01, -1.1713e+01],
        [ 2.3864e-01,  2.3857e-01,  2.3861e-01, -1.2448e+00,  2.3967e-01,
         -4.6038e+00,  2.3863e-01, -1.7652e+00,  2.8182e-01,  2.3856e-01,
          7.7805e+00,  2.3860e-01,  8.4581e+00]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.0942,  1.0770, -2.0942,  1.3002, -2.0942, -2.0942, -1.1844, -2.0942,
        -3.0033, -2.0942, -2.0898,  0.9831, -2.8700], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0327,   8.7809,  -0.0327,  10.1471,  -0.0327,  -0.0327,  -7.5316,
          -0.0327,  -0.7048,  -0.0327, -11.4681,   8.0000,  -3.7338]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-15.8160,   7.4294],
        [  5.1853,   2.0445],
        [ -1.4664,   1.7364],
        [  7.7101,   3.5681],
        [-11.4078,  -3.7920],
        [  9.4160,   0.2820],
        [  9.6473,   3.9284],
        [-12.0819,  -4.4252],
        [-11.3059,  -3.4820],
        [ -9.1236,  -3.3043],
        [  0.7655,   6.0340],
        [ -6.1574,   8.3664],
        [  3.3508,  -3.3904]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  2.2137,   8.0224,  -4.9246,  -4.6288,  -2.9717,  -9.1993,  -2.8545,
         -1.8847,  -3.5332,   0.3909,   4.6418,   2.8359, -11.7022],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.1751e-01, -3.3548e+00, -1.3344e+00,  2.2531e-01,  9.3155e-01,
         -4.7388e-01, -1.9075e+00,  1.1062e+00,  5.8061e-01,  8.7216e-01,
         -6.1858e-01,  1.7826e+00,  1.8769e-01],
        [-3.7971e+00, -5.9695e+00, -8.4684e-02, -4.6941e+00,  1.0309e+01,
         -1.3560e+01, -4.9923e+00,  1.2216e+01,  6.6656e+00,  7.5997e+00,
         -6.7574e+00, -1.1152e+01, -5.2933e+00],
        [ 2.0477e-01,  3.5177e+00, -3.4829e-01, -5.7903e-01,  1.6010e-01,
         -2.4163e+00, -7.8539e-01,  6.0641e-01,  6.0148e-02,  1.6433e+00,
          9.4767e-02,  3.2278e+00,  6.7565e-01],
        [-7.8741e-01, -4.8205e+00, -1.5540e+00, -1.4230e+00, -2.3778e+00,
         -1.4583e+00,  2.3276e-01, -2.4544e+00, -2.4016e+00,  9.5402e+00,
         -1.9173e+00,  9.5774e+00, -3.0032e+00],
        [ 5.6780e+00, -1.4793e-01,  5.8470e-01, -2.0398e+00, -6.2756e+00,
         -1.7600e+01, -2.4224e+00, -6.7997e+00, -6.2643e+00, -2.4783e+00,
          6.2399e+00,  5.5670e+00, -2.9082e-01],
        [ 1.6664e-01,  3.5120e+00, -3.6527e-01, -5.3591e-01,  1.4432e-01,
         -2.3507e+00, -7.3965e-01,  5.6586e-01,  5.3418e-02,  1.5992e+00,
          1.1143e-01,  3.1263e+00,  6.6974e-01],
        [ 4.8090e+00, -1.4834e+01,  1.6642e+00, -1.7067e+00,  5.7514e+00,
         -7.7476e-01, -2.8591e+00,  7.2391e+00,  6.0955e+00,  3.0134e+00,
          4.2458e+00,  4.4390e+00, -1.1275e+00],
        [ 1.4091e+00,  7.0922e+00,  3.5499e-01,  2.0138e+00, -7.5539e+00,
         -7.6680e+00,  2.8162e+00, -7.7642e+00, -5.0256e+00, -8.3087e-03,
          1.3982e+00,  1.1990e+00,  8.2806e-01],
        [-1.7315e-01, -3.3018e+00, -1.3644e+00,  1.6217e-01,  1.0558e+00,
         -4.9142e-01, -1.9674e+00,  1.2106e+00,  6.8842e-01,  9.1898e-01,
         -5.7046e-01,  1.8081e+00,  1.8309e-01],
        [-2.2934e-01, -3.3789e+00, -1.3263e+00,  2.4392e-01,  8.9613e-01,
         -4.6921e-01, -1.8889e+00,  1.0762e+00,  5.5097e-01,  8.5944e-01,
         -6.3145e-01,  1.7761e+00,  1.8902e-01],
        [-1.3856e-01, -3.2721e+00, -1.3713e+00,  9.0169e-02,  1.1222e+00,
         -5.0944e-01, -2.0136e+00,  1.2676e+00,  7.4894e-01,  9.3243e-01,
         -5.4132e-01,  1.8120e+00,  1.8098e-01],
        [ 1.2833e-01, -2.0689e+00, -2.7686e-01,  2.2210e-01,  4.9322e+00,
         -7.7126e+00, -1.8531e+00,  8.0097e+00,  2.6780e+00,  2.5487e-01,
         -3.9186e+01,  5.2130e-02, -4.2598e+00],
        [ 6.6891e+00,  2.8773e+00,  1.2282e-01,  1.8907e+00, -3.9835e+00,
          2.8652e+00,  1.5625e+00, -4.9834e+00, -2.8658e+00, -1.9728e+00,
          1.4160e+00,  2.0542e+01,  1.8681e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.9376,  2.3320,  3.7783, -1.6537, -3.8984,  3.7914, -1.1016,  1.6152,
        -2.9198, -2.9341, -2.9023, -4.3321, -1.8808], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.3054e+00, -2.2759e+01, -4.4752e+00,  3.1575e+00,  7.6587e+00,
         -4.5020e+00,  5.3098e+00,  5.7352e+00,  1.4151e+00,  1.2726e+00,
          1.4905e+00, -8.7881e-03, -1.1382e+00],
        [-1.0973e+00, -1.9400e+00, -1.1748e+00,  1.3728e-01, -7.0269e-01,
         -6.6027e-01,  4.1210e+00,  3.7117e+00, -1.1690e+00, -1.0783e+00,
         -1.2063e+00,  1.4045e+01,  2.0295e+00],
        [-2.4929e-03, -3.8890e-02, -2.6585e+00, -7.0508e-02, -3.9453e-01,
         -2.6587e+00, -2.4114e-03, -2.6479e+00, -3.3608e-03, -2.2473e-03,
         -3.8197e-03, -7.4191e-03, -2.4107e+00],
        [-1.9910e-03, -3.7934e-02, -2.5390e+00, -7.5193e-02, -3.8216e-01,
         -2.5392e+00, -2.9724e-03, -2.5192e+00, -2.0831e-03, -2.0030e-03,
         -2.1951e-03, -1.1459e-02, -2.3070e+00],
        [ 6.9448e-01,  7.3720e+00,  2.6860e+00,  7.8404e-01, -9.3887e-01,
          2.8681e+00, -6.8992e+00, -5.2394e-02,  6.9569e-01,  6.9377e-01,
          7.0305e-01, -1.3447e+01, -1.4653e+00]], device='cuda:0'))])
loaded xi:  790.78876
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 946.50330641104
W_T_median: 707.5512584605915
W_T_pctile_5: -238.22847581189643
W_T_CVAR_5_pct: -367.3344080824147
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2056.942536056522
Current xi:  [783.43884]
objective value function right now is: -2056.942536056522
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [772.22174]
objective value function right now is: -2048.0788473526623
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -2065.6535246419003
Current xi:  [760.88995]
objective value function right now is: -2065.6535246419003
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -2072.8982405796246
Current xi:  [750.75903]
objective value function right now is: -2072.8982405796246
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -2075.949265580763
Current xi:  [742.54626]
objective value function right now is: -2075.949265580763
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [732.6073]
objective value function right now is: -2072.082433617971
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [724.44135]
objective value function right now is: -2037.1111009768774
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [717.7322]
objective value function right now is: -2058.5715747863333
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [711.0993]
objective value function right now is: -2056.1985067461437
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [704.2831]
objective value function right now is: -2039.3538033890059
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [699.9677]
objective value function right now is: -2075.097962125004
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [696.7845]
objective value function right now is: -2075.2540656908554
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [694.88446]
objective value function right now is: -2059.3731557805495
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [693.4309]
objective value function right now is: -2073.278999281312
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [691.43396]
objective value function right now is: -2072.5711236270467
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [688.8067]
objective value function right now is: -2044.8580548998182
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [685.7738]
objective value function right now is: -2047.7366209707743
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [685.0159]
objective value function right now is: -2073.202171061618
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [683.8527]
objective value function right now is: -2073.69174497936
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2076.946966569802
Current xi:  [683.53796]
objective value function right now is: -2076.946966569802
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [681.25714]
objective value function right now is: -2064.604719001492
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [682.8655]
objective value function right now is: -2061.5764200996914
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2077.9898674490137
Current xi:  [682.08624]
objective value function right now is: -2077.9898674490137
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [681.4605]
objective value function right now is: -2062.6645789008776
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -2083.150944797412
Current xi:  [682.10803]
objective value function right now is: -2083.150944797412
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [681.43896]
objective value function right now is: -2059.324035384209
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [680.1668]
objective value function right now is: -2081.859925694193
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [679.6962]
objective value function right now is: -2081.954920807192
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [680.4536]
objective value function right now is: -2078.8793359245756
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [679.7495]
objective value function right now is: -2069.4025625212566
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [678.9798]
objective value function right now is: -2082.599889820114
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [677.694]
objective value function right now is: -2066.3397090556973
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -2083.16967001372
Current xi:  [676.84766]
objective value function right now is: -2083.16967001372
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [675.49524]
objective value function right now is: -2082.3049785752573
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [675.44275]
objective value function right now is: -2070.193225026462
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [676.0155]
objective value function right now is: -2082.0763217007548
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [676.2844]
objective value function right now is: -2078.645631212161
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -2086.1655543817183
Current xi:  [676.6072]
objective value function right now is: -2086.1655543817183
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2087.338986939384
Current xi:  [676.72504]
objective value function right now is: -2087.338986939384
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [677.18195]
objective value function right now is: -2086.105120638073
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -2087.758881740411
Current xi:  [677.7197]
objective value function right now is: -2087.758881740411
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -2087.766559198113
Current xi:  [678.0405]
objective value function right now is: -2087.766559198113
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [678.3285]
objective value function right now is: -2085.9468670012425
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -2087.895437979592
Current xi:  [678.6977]
objective value function right now is: -2087.895437979592
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [679.26196]
objective value function right now is: -2087.817336831626
new min fval from sgd:  -2087.9016152388954
new min fval from sgd:  -2087.944202410817
new min fval from sgd:  -2088.0076417601667
new min fval from sgd:  -2088.015263408865
new min fval from sgd:  -2088.026785202594
new min fval from sgd:  -2088.1224641702247
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [679.24054]
objective value function right now is: -2087.5482196180224
new min fval from sgd:  -2088.122832001474
new min fval from sgd:  -2088.1467955452345
new min fval from sgd:  -2088.1530943991975
new min fval from sgd:  -2088.1796985151896
new min fval from sgd:  -2088.3876394820104
new min fval from sgd:  -2088.4406637884185
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [679.7354]
objective value function right now is: -2084.3837619251563
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [679.72076]
objective value function right now is: -2088.0287941681454
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [679.78284]
objective value function right now is: -2088.093063650793
new min fval from sgd:  -2088.4466853622325
new min fval from sgd:  -2088.4763500182066
new min fval from sgd:  -2088.482432173434
new min fval from sgd:  -2088.5084902176513
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [679.8903]
objective value function right now is: -2086.7609738659958
min fval:  -2088.5084902176513
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -1.1759,  -0.3461],
        [ -1.1759,  -0.3461],
        [ -1.1759,  -0.3461],
        [ -7.9930,  13.3588],
        [ -1.1759,  -0.3461],
        [-30.8472,   5.7735],
        [ -1.1759,  -0.3461],
        [ -4.8614,  17.4946],
        [ -1.1759,  -0.3461],
        [ -1.1759,  -0.3461],
        [ 16.4088,  -1.0376],
        [ -1.1759,  -0.3461],
        [ -4.0886, -15.4378]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.2766,  -3.2766,  -3.2766,  -0.1864,  -3.2766,   4.3489,  -3.2766,
          1.6612,  -3.2766,  -3.2766, -13.6028,  -3.2766,  -2.3237],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.7435e-02, -2.7435e-02, -2.7435e-02, -1.2606e-01, -2.7435e-02,
         -3.7011e-01, -2.7435e-02, -5.2426e-01, -2.7435e-02, -2.7435e-02,
         -2.3286e-01, -2.7435e-02, -3.1121e-01],
        [-1.3616e-01, -1.3616e-01, -1.3616e-01,  2.8544e+00, -1.3616e-01,
          6.9480e+00, -1.3616e-01,  2.4316e+00, -1.3616e-01, -1.3616e-01,
         -1.1321e+01, -1.3616e-01, -1.2969e+01],
        [-2.7435e-02, -2.7435e-02, -2.7435e-02, -1.2606e-01, -2.7435e-02,
         -3.7011e-01, -2.7435e-02, -5.2426e-01, -2.7435e-02, -2.7435e-02,
         -2.3286e-01, -2.7435e-02, -3.1121e-01],
        [-2.8926e-01, -2.8926e-01, -2.8926e-01,  3.8061e+00, -2.8926e-01,
          7.6667e+00, -2.8926e-01,  4.0563e+00, -2.8925e-01, -2.8926e-01,
         -1.5445e+01, -2.8926e-01, -1.4969e+01],
        [-2.7435e-02, -2.7435e-02, -2.7435e-02, -1.2606e-01, -2.7435e-02,
         -3.7011e-01, -2.7435e-02, -5.2426e-01, -2.7435e-02, -2.7435e-02,
         -2.3286e-01, -2.7435e-02, -3.1121e-01],
        [-2.7435e-02, -2.7435e-02, -2.7435e-02, -1.2606e-01, -2.7435e-02,
         -3.7011e-01, -2.7435e-02, -5.2426e-01, -2.7435e-02, -2.7435e-02,
         -2.3286e-01, -2.7435e-02, -3.1121e-01],
        [ 1.5931e-01,  1.5931e-01,  1.5931e-01, -1.7182e+00,  1.5931e-01,
         -9.1204e+00,  1.5931e-01, -3.7716e+00,  1.5931e-01,  1.5931e-01,
          4.8693e+00,  1.5931e-01,  1.4024e+01],
        [-2.7435e-02, -2.7435e-02, -2.7435e-02, -1.2606e-01, -2.7435e-02,
         -3.7011e-01, -2.7435e-02, -5.2426e-01, -2.7435e-02, -2.7435e-02,
         -2.3286e-01, -2.7435e-02, -3.1121e-01],
        [-2.7435e-02, -2.7435e-02, -2.7435e-02, -1.2606e-01, -2.7435e-02,
         -3.7011e-01, -2.7435e-02, -5.2426e-01, -2.7435e-02, -2.7435e-02,
         -2.3286e-01, -2.7435e-02, -3.1121e-01],
        [-2.7435e-02, -2.7435e-02, -2.7435e-02, -1.2606e-01, -2.7435e-02,
         -3.7011e-01, -2.7435e-02, -5.2426e-01, -2.7435e-02, -2.7435e-02,
         -2.3286e-01, -2.7435e-02, -3.1121e-01],
        [ 9.1057e-02,  9.1057e-02,  9.1057e-02, -3.5164e+00,  9.1058e-02,
         -9.6877e+00,  9.1057e-02, -5.2853e+00,  9.1063e-02,  9.1057e-02,
          2.0102e+01,  9.1057e-02,  1.5715e+01],
        [-3.9769e-02, -3.9769e-02, -3.9769e-02, -1.5444e-02, -3.9770e-02,
          3.1243e+00, -3.9770e-02, -3.9113e-01, -3.9770e-02, -3.9770e-02,
         -2.1657e+00, -3.9770e-02, -5.2390e+00],
        [-2.7435e-02, -2.7435e-02, -2.7435e-02, -1.2606e-01, -2.7435e-02,
         -3.7011e-01, -2.7435e-02, -5.2426e-01, -2.7435e-02, -2.7435e-02,
         -2.3286e-01, -2.7435e-02, -3.1121e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.1087,  0.0315, -2.1087,  1.0086, -2.1087, -2.1087, -2.5372, -2.1087,
        -2.1087, -2.1087, -2.9264, -2.3113, -2.1087], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0168,   5.9861,   0.0168,  10.0022,   0.0168,   0.0168,  -7.2193,
           0.0168,   0.0168,   0.0168, -14.4670,   1.2226,   0.0168]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-16.9509,  12.0602],
        [  1.9505,   0.2468],
        [ -4.5105,   5.9722],
        [  1.5306,   1.7579],
        [-13.7627,  -4.5296],
        [ 14.4147,   0.5086],
        [ 12.7458,   7.5624],
        [-14.3024,  -5.0432],
        [-12.7942,  -4.2368],
        [-13.6911,  -4.6829],
        [  2.9131,   5.6941],
        [ -1.5669,   8.6198],
        [  4.5495,  -5.0279]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  3.9770,   6.0373,  -6.2618,  -8.4263,  -3.6618, -12.4922,  -2.9268,
         -1.6600,  -3.7919,   1.3503,   4.2087,   2.9987, -16.8945],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.1204e-01, -3.1481e+00, -8.4102e-02, -7.0383e-02, -2.3994e-03,
         -6.5181e-01, -5.8716e-01, -5.3323e-03, -2.1357e-03, -2.0969e-01,
         -1.8370e+00, -2.9008e-01,  1.1968e-01],
        [-7.0270e+00, -4.7077e+00, -5.7387e-03,  4.6000e-02,  1.0814e+01,
         -1.7309e+01, -1.3310e+01,  1.4143e+01,  6.4344e+00,  8.4811e+00,
         -8.9571e+00, -1.1822e+01, -9.5034e+00],
        [ 2.5100e+00,  4.7320e+00, -1.1049e+00, -1.0770e+00,  1.5688e-03,
         -5.3794e+00, -1.6762e+00, -2.0046e-01,  1.4948e-03, -4.6408e-02,
         -8.4839e-01,  3.7366e+00,  6.9952e-01],
        [ 4.1670e+00, -5.4373e+00,  3.3069e+00, -4.3376e-01, -6.6494e+00,
         -2.5040e+01, -6.3390e-01, -6.6946e+00, -4.1523e+00,  5.1402e+00,
         -2.0328e+00,  9.0671e+00, -3.4596e-03],
        [ 8.4580e+00, -1.7421e+00,  6.6494e+00, -1.0764e+00, -5.3125e+00,
         -2.0186e+01, -5.0766e+00, -8.7230e+00, -2.6068e+00, -2.6187e+00,
          6.2087e+00,  9.3789e+00,  6.2429e-03],
        [-3.7125e-01,  3.4985e+00, -5.2013e-01, -4.0721e-01, -6.5964e-02,
          3.2292e-01,  9.0527e-01, -6.2848e-01, -6.0052e-02, -6.7688e-01,
          2.1854e+00,  5.3315e-01, -6.3356e-01],
        [ 9.2423e+00, -1.4568e+01,  5.9123e-01, -4.4427e+00,  7.9127e+00,
         -4.6366e-02, -2.0329e+00,  6.4352e+00,  5.8072e+00,  3.0809e+00,
          1.7511e+00,  5.7305e+00,  1.5263e-03],
        [ 2.2800e-01,  6.0896e+00,  6.3137e-01,  5.8184e-01,  3.0514e-02,
         -1.8899e+00,  1.6295e+00,  6.3472e-02,  3.3759e-02,  1.0436e+00,
          1.2556e+00, -2.1373e-01,  3.1998e-02],
        [-1.1203e-01, -3.1481e+00, -8.4102e-02, -7.0384e-02, -2.3987e-03,
         -6.5181e-01, -5.8716e-01, -5.3306e-03, -2.1351e-03, -2.0969e-01,
         -1.8370e+00, -2.9008e-01,  1.1968e-01],
        [-1.1204e-01, -3.1481e+00, -8.4102e-02, -7.0383e-02, -2.3996e-03,
         -6.5181e-01, -5.8716e-01, -5.3331e-03, -2.1359e-03, -2.0970e-01,
         -1.8370e+00, -2.9008e-01,  1.1968e-01],
        [-1.1203e-01, -3.1481e+00, -8.4102e-02, -7.0385e-02, -2.3972e-03,
         -6.5181e-01, -5.8716e-01, -5.3268e-03, -2.1336e-03, -2.0968e-01,
         -1.8370e+00, -2.9008e-01,  1.1969e-01],
        [ 2.7301e-02, -1.6502e+00,  2.2518e-03,  4.6658e-03,  5.8114e+00,
         -1.9448e+00, -1.3924e-01,  7.2461e+00,  3.7072e+00,  1.7165e+00,
         -3.0807e+01,  1.0597e-01, -9.1472e+00],
        [ 8.3407e+00,  1.8490e+00, -2.0466e-02,  4.8655e-02, -6.2532e+00,
          2.9757e+00,  5.9127e-01, -9.1067e+00, -5.1979e+00, -7.0709e+00,
          2.1417e+00,  1.4183e+01,  5.6846e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.1497,  3.4315,  4.9790, -2.3806, -5.5100,  3.5010, -3.0733,  3.0394,
        -3.1497, -3.1497, -3.1497, -3.6598, -2.8803], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.1682e-01,  1.3863e+00, -4.7205e+00,  3.0844e+00,  7.4269e+00,
         -4.7495e+00,  3.8708e+00,  5.3978e+00, -1.1682e-01, -1.1681e-01,
         -1.1684e-01,  1.4607e+01,  1.1826e-01],
        [ 1.6493e-02, -6.3303e+00, -6.7118e-01,  5.2181e-01, -6.8162e-01,
         -2.0589e-01, -2.7895e+00,  4.1206e+00,  1.6464e-02,  1.6502e-02,
          1.6399e-02,  1.3336e+00,  7.4670e-01],
        [-1.0370e-03, -3.1545e-02, -2.8873e+00, -2.3301e-02, -2.6079e-01,
         -2.8912e+00, -4.0822e-03, -2.8918e+00, -1.0370e-03, -1.0370e-03,
         -1.0370e-03, -1.1202e-03, -1.7616e+00],
        [-1.4977e-03, -2.9073e-02, -2.7944e+00, -2.8242e-02, -2.8961e-01,
         -2.7979e+00, -5.1338e-03, -2.7984e+00, -1.4977e-03, -1.4977e-03,
         -1.4977e-03, -8.9725e-04, -1.7550e+00],
        [-2.6194e-02,  1.0275e+01,  2.2792e+00,  1.0884e+00, -6.2351e-01,
          2.5093e+00, -9.8628e-02, -3.6744e-01, -2.6223e-02, -2.6183e-02,
         -2.6290e-02, -1.6448e+01, -2.6880e-01]], device='cuda:0'))])
xi:  [679.8554]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 930.8655001459153
W_T_median: 875.8431650230639
W_T_pctile_5: 679.9504160285724
W_T_CVAR_5_pct: 347.7578366965656
Average q (qsum/M+1):  50.544366651965724
Optimal xi:  [679.8554]
Expected(across Rb) median(across samples) p_equity:  0.24051075881870929
obj fun:  tensor(-2088.5085, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-2.5263e+01,  5.5626e+00],
        [-9.2375e+00,  8.9720e+00],
        [ 1.2454e+01, -7.0490e-01],
        [ 6.8132e+00, -1.0601e+00],
        [ 1.0194e+01, -5.7355e-01],
        [ 8.5200e+00, -5.0510e-01],
        [ 1.1747e+01, -5.8749e-01],
        [-1.2529e+00, -3.7962e-01],
        [ 9.9800e+00, -1.0363e-02],
        [-6.7443e-01, -1.1170e+01],
        [-6.4751e+00,  1.1722e+01],
        [ 6.5122e+00, -1.0558e+00],
        [-1.1024e+01,  4.9609e+00]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  3.9078,  -0.8962, -10.3990,  -9.7810, -10.0079, -10.0329, -10.1872,
         -3.7385,  -9.9710,  -1.7351,   1.0056,  -9.6171,  -4.7921],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0527e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6752e-02, -3.5521e-01,
         -4.2893e-01, -1.5199e-03, -2.8530e-02],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0528e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6752e-02, -3.5521e-01,
         -4.2893e-01, -1.5199e-03, -2.8530e-02],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0528e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6753e-02, -3.5521e-01,
         -4.2893e-01, -1.5199e-03, -2.8530e-02],
        [-9.5480e+00, -3.4013e+00,  9.0569e+00,  4.7854e-01,  3.7939e+00,
          2.4102e+00,  5.9198e+00,  2.8689e-02,  5.9013e+00,  1.6841e+01,
         -2.7326e+00,  4.0908e-01, -7.0008e-01],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0527e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6752e-02, -3.5521e-01,
         -4.2894e-01, -1.5199e-03, -2.8530e-02],
        [ 7.6600e+00,  3.2058e+00, -9.1472e+00, -1.0581e+00, -3.1984e+00,
         -1.8967e+00, -5.8390e+00, -2.6283e-01, -5.3336e+00, -1.6576e+01,
          2.6031e+00, -9.2098e-01,  2.0315e+00],
        [-1.0339e+01, -3.7270e+00,  9.9564e+00,  1.2205e+00,  4.5562e+00,
          3.3894e+00,  6.5062e+00, -5.9699e-03,  6.9342e+00,  1.8001e+01,
         -3.0201e+00,  1.1625e+00, -1.8375e+00],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0528e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6753e-02, -3.5521e-01,
         -4.2893e-01, -1.5199e-03, -2.8530e-02],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0527e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6752e-02, -3.5521e-01,
         -4.2894e-01, -1.5199e-03, -2.8530e-02],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0527e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6752e-02, -3.5521e-01,
         -4.2893e-01, -1.5199e-03, -2.8530e-02],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0527e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6752e-02, -3.5521e-01,
         -4.2893e-01, -1.5199e-03, -2.8530e-02],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0527e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6752e-02, -3.5521e-01,
         -4.2894e-01, -1.5199e-03, -2.8530e-02],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0527e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6752e-02, -3.5521e-01,
         -4.2893e-01, -1.5199e-03, -2.8530e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.2065, -2.2065, -2.2065, -3.0125, -2.2065,  1.8656, -2.9748, -2.2065,
        -2.2065, -2.2065, -2.2065, -2.2065, -2.2065], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0522,   0.0522,   0.0522,  -8.5903,   0.0522,  16.0263, -12.2656,
           0.0522,   0.0522,   0.0522,   0.0522,   0.0522,   0.0522]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.0146,   0.2543],
        [ -2.1249,   7.7531],
        [ -1.1371,   8.0667],
        [ 10.9639,   3.0109],
        [-12.7895,  -3.5973],
        [-11.2361,   6.5161],
        [ -2.0737,   6.2144],
        [-19.3236,   7.6972],
        [  1.2787,   6.0057],
        [-12.0030,  -3.2098],
        [-10.3500,  -0.4829],
        [-13.4132,   6.4745],
        [-12.4046,  -3.6595]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-5.4028, -5.3351,  4.2484, -0.5421, -1.2992,  2.8738, -9.7406,  1.2844,
         5.5323,  1.3639,  8.0985, -5.1474, -2.5011], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.0551e-02,  8.9914e-02, -5.0418e+00, -6.7311e-01,  7.3789e+00,
         -1.0598e+01,  1.6490e-02, -4.4531e+00, -1.8672e+00,  7.1606e+00,
          2.7157e+00, -9.6447e-02,  5.3128e+00],
        [ 2.6873e-03, -3.1556e-01, -8.5696e-02, -3.4595e+00,  4.4744e-02,
         -2.5078e-01, -1.5108e-01, -9.9928e-02, -1.7793e+00,  2.1748e-02,
         -1.7037e+00, -1.1603e-01,  3.0568e-02],
        [-5.2017e-04, -1.6029e-01, -3.5824e-01, -3.0002e+00, -2.8015e-02,
         -2.0338e-01, -9.4977e-02, -1.1359e-01, -1.3914e+00, -9.0692e-02,
         -1.4340e+00, -8.7701e-02, -3.2229e-02],
        [ 1.0517e-01, -3.4911e+00, -4.9575e+00,  1.7137e-01,  9.3268e-02,
         -1.6276e+00, -5.9251e-01, -5.0836e-01,  2.2484e+00, -5.1966e+00,
         -1.0420e+01, -4.7317e-01,  6.6864e-02],
        [ 4.9735e-03, -4.1971e-01,  3.3231e-02, -3.4748e+00,  4.8848e-02,
         -3.1744e-01, -1.9678e-01, -1.2257e-01, -1.9328e+00,  1.2741e-01,
         -1.8670e+00, -1.4829e-01,  4.7496e-02],
        [-1.4225e-02,  1.9113e+00,  9.5669e-01, -3.1535e+00, -5.5510e-01,
          2.2057e+00, -3.2462e+00,  7.2864e+00,  2.0217e+00, -1.7873e+00,
          4.7190e+00, -5.0543e+00, -4.2296e+00],
        [ 1.3719e-01, -4.7523e-02, -1.6247e+01, -5.1811e+00,  1.8992e+01,
         -5.9705e+00, -1.1030e-02, -5.5893e+00, -1.0384e+01,  1.3992e+01,
          6.2219e+00, -6.0265e-02,  1.6660e+01],
        [ 4.9505e-03, -4.1852e-01,  3.1437e-02, -3.4720e+00,  4.8924e-02,
         -3.1669e-01, -1.9640e-01, -1.2258e-01, -1.9327e+00,  1.2526e-01,
         -1.8663e+00, -1.4806e-01,  4.7379e-02],
        [ 2.4954e-03, -3.0735e-01, -9.6036e-02, -3.4596e+00,  4.4300e-02,
         -2.4509e-01, -1.4776e-01, -9.8355e-02, -1.7729e+00,  1.5476e-02,
         -1.6948e+00, -1.1379e-01,  2.9337e-02],
        [ 1.8381e-02,  4.3796e+00, -7.2229e-01, -1.2604e+00, -1.4757e+00,
         -5.5794e+00,  3.7404e+00, -2.2716e+00,  1.3132e+00, -3.9707e+00,
         -1.5020e+00, -9.0181e-01,  5.5348e-01],
        [-7.7680e-02,  2.4098e-04,  4.5947e-01, -4.2478e+00,  8.2098e+00,
          1.5562e+00, -7.8526e-05,  5.7730e-02, -3.9597e+01,  7.0748e+00,
         -5.1515e+00, -9.8557e-05,  8.7370e+00],
        [ 9.1487e-02, -3.1155e+00, -6.1387e-01, -9.8513e-01,  3.4467e-02,
         -1.9774e+00, -5.9494e-01, -4.8375e-01,  1.3646e+00,  3.4214e-01,
         -7.5207e+00, -4.5388e-01,  1.0033e-02],
        [ 3.0955e-03, -3.2801e-01, -7.3509e-02, -3.4659e+00,  4.7533e-02,
         -2.5577e-01, -1.5808e-01, -1.0255e-01, -1.8168e+00,  3.3280e-02,
         -1.7323e+00, -1.2120e-01,  3.5054e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.5076, -3.5384, -3.9057, -2.8815, -3.5488, -6.3215, -3.8458, -3.5511,
        -3.5315, -0.2162, -3.9220, -4.7765, -3.5351], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-6.8541e+01,  1.6155e-02, -6.9425e-03, -2.4388e+00,  2.5506e-02,
          4.1831e+00,  8.0546e-01,  2.4758e-02,  1.4161e-02, -1.4113e+01,
          1.4917e-06, -3.3799e-01,  1.3780e-02],
        [ 4.0970e-01,  1.0230e-01,  2.2945e-01,  5.1799e+00,  1.8352e-01,
          1.0252e-01, -5.7467e+00,  1.8250e-01,  9.6292e-02,  1.3207e+00,
          3.9403e-01,  3.4988e+00,  9.9380e-02],
        [-1.2342e+01, -8.0051e-03,  2.0171e-02, -5.5101e-02, -1.6493e-02,
         -3.9222e+01,  5.1248e+00, -1.6181e-02, -7.4177e-03, -5.0522e+01,
          1.8666e+01, -1.8603e-02, -9.3465e-03],
        [-1.6464e+01, -2.3833e-02, -3.5650e-02, -2.8042e-02, -2.0353e-02,
         -3.5048e+01, -1.1445e-02, -2.0343e-02, -2.4065e-02, -4.7368e+01,
          2.2236e-04, -1.9383e-02, -2.2851e-02],
        [ 2.0931e+00, -2.4194e-02,  2.1556e-01, -2.5696e+00, -8.3338e-02,
          2.4551e-01,  7.0172e+00, -8.3109e-02, -2.2976e-02,  1.5327e+00,
         -1.0617e+01, -2.2547e+00, -4.5724e-02]], device='cuda:0'))])
loaded xi:  845.18384
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 946.50330641104
W_T_median: 707.5512584605915
W_T_pctile_5: -238.22847581189643
W_T_CVAR_5_pct: -367.3344080824147
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2567.8027633278284
Current xi:  [839.2861]
objective value function right now is: -2567.8027633278284
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [829.6857]
objective value function right now is: -2564.5868254187662
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -2571.8348468081513
Current xi:  [818.96643]
objective value function right now is: -2571.8348468081513
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [807.056]
objective value function right now is: -2554.2461282209492
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -2576.7061857831345
Current xi:  [796.43616]
objective value function right now is: -2576.7061857831345
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [786.7383]
objective value function right now is: -2571.740173024763
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [779.04175]
objective value function right now is: -2564.739521834251
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -2583.7852068121715
Current xi:  [770.069]
objective value function right now is: -2583.7852068121715
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -2611.5122527927515
Current xi:  [762.9059]
objective value function right now is: -2611.5122527927515
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -2619.3039944094926
Current xi:  [758.03375]
objective value function right now is: -2619.3039944094926
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [752.9802]
objective value function right now is: -2618.961311297564
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [748.6444]
objective value function right now is: -2613.3515639862994
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [745.8377]
objective value function right now is: -2611.5735251821293
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [742.7913]
objective value function right now is: -2603.1158042822676
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [742.22845]
objective value function right now is: -2606.911041567465
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [740.53314]
objective value function right now is: -2619.0289092898815
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [739.3854]
objective value function right now is: -2613.3706914937497
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [738.53674]
objective value function right now is: -2614.9888536232133
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [737.0075]
objective value function right now is: -2604.0385003159963
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [737.88605]
objective value function right now is: -2592.175696304804
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [737.6243]
objective value function right now is: -2605.1952138057436
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [737.6082]
objective value function right now is: -2607.3135226117392
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2619.9102099156967
Current xi:  [736.91547]
objective value function right now is: -2619.9102099156967
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -2621.8137355506747
Current xi:  [735.2879]
objective value function right now is: -2621.8137355506747
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [734.6448]
objective value function right now is: -2604.9421616023333
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [735.0673]
objective value function right now is: -2620.986929730139
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [735.92523]
objective value function right now is: -2616.2208138014157
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [734.291]
objective value function right now is: -2620.0829960770734
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [735.9354]
objective value function right now is: -2617.102456838118
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [736.2148]
objective value function right now is: -2620.2903292318274
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [736.59796]
objective value function right now is: -2596.0324430812143
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [735.93964]
objective value function right now is: -2616.392652490503
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [737.15283]
objective value function right now is: -2619.516190412577
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [736.23663]
objective value function right now is: -2616.207847294244
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -2622.051312565291
Current xi:  [734.6824]
objective value function right now is: -2622.051312565291
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2627.201168564958
Current xi:  [734.8477]
objective value function right now is: -2627.201168564958
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [735.0472]
objective value function right now is: -2621.866630013831
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -2627.583148290484
Current xi:  [735.2748]
objective value function right now is: -2627.583148290484
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [735.2843]
objective value function right now is: -2626.158139835283
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [735.6683]
objective value function right now is: -2623.8675749178506
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -2627.8592724605137
Current xi:  [735.7593]
objective value function right now is: -2627.8592724605137
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [736.0544]
objective value function right now is: -2627.283396764329
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [736.23914]
objective value function right now is: -2625.916039267101
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [736.19525]
objective value function right now is: -2627.379596672101
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [736.4508]
objective value function right now is: -2626.400932676926
new min fval from sgd:  -2627.917692420588
new min fval from sgd:  -2627.943722756411
new min fval from sgd:  -2627.9495051082326
new min fval from sgd:  -2628.009893804612
new min fval from sgd:  -2628.0638425152574
new min fval from sgd:  -2628.115917419523
new min fval from sgd:  -2628.1253079428757
new min fval from sgd:  -2628.2658961870516
new min fval from sgd:  -2628.334838724033
new min fval from sgd:  -2628.373499489515
new min fval from sgd:  -2628.428999662968
new min fval from sgd:  -2628.460772783247
new min fval from sgd:  -2628.6227412771536
new min fval from sgd:  -2628.837865457343
new min fval from sgd:  -2628.95997688433
new min fval from sgd:  -2629.0248441830345
new min fval from sgd:  -2629.034655733107
new min fval from sgd:  -2629.0947084728537
new min fval from sgd:  -2629.139807186203
new min fval from sgd:  -2629.226724800219
new min fval from sgd:  -2629.2857572733296
new min fval from sgd:  -2629.292411337633
new min fval from sgd:  -2629.297840304066
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [736.67834]
objective value function right now is: -2629.203017036452
new min fval from sgd:  -2629.3146063107943
new min fval from sgd:  -2629.318947392937
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [736.80176]
objective value function right now is: -2625.9562447265744
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [737.0562]
objective value function right now is: -2626.4782492913996
new min fval from sgd:  -2629.3353975842983
new min fval from sgd:  -2629.3654822807475
new min fval from sgd:  -2629.3961575242697
new min fval from sgd:  -2629.4188947373323
new min fval from sgd:  -2629.4316914393166
new min fval from sgd:  -2629.43329810175
new min fval from sgd:  -2629.439224040736
new min fval from sgd:  -2629.4415414682826
new min fval from sgd:  -2629.444601528667
new min fval from sgd:  -2629.457363551212
new min fval from sgd:  -2629.4677816760104
new min fval from sgd:  -2629.4765958192743
new min fval from sgd:  -2629.4855840625187
new min fval from sgd:  -2629.498081320209
new min fval from sgd:  -2629.5122392219187
new min fval from sgd:  -2629.5203423678427
new min fval from sgd:  -2629.5226402273124
new min fval from sgd:  -2629.5268300061953
new min fval from sgd:  -2629.5279676845976
new min fval from sgd:  -2629.5297973044
new min fval from sgd:  -2629.541584960496
new min fval from sgd:  -2629.5438729791745
new min fval from sgd:  -2629.5443952722317
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [737.28406]
objective value function right now is: -2629.516329629957
new min fval from sgd:  -2629.561191593228
new min fval from sgd:  -2629.579909931507
new min fval from sgd:  -2629.594194468594
new min fval from sgd:  -2629.604128285172
new min fval from sgd:  -2629.606727784819
new min fval from sgd:  -2629.6071372049773
new min fval from sgd:  -2629.608414588954
new min fval from sgd:  -2629.612744001989
new min fval from sgd:  -2629.626490843973
new min fval from sgd:  -2629.6433927020753
new min fval from sgd:  -2629.6498128369285
new min fval from sgd:  -2629.6536975603326
new min fval from sgd:  -2629.653826397035
new min fval from sgd:  -2629.6573192759774
new min fval from sgd:  -2629.6577944317214
new min fval from sgd:  -2629.658749249078
new min fval from sgd:  -2629.6635677807144
new min fval from sgd:  -2629.664662012648
new min fval from sgd:  -2629.666077822332
new min fval from sgd:  -2629.6666210129774
new min fval from sgd:  -2629.6781395309285
new min fval from sgd:  -2629.688818668939
new min fval from sgd:  -2629.694116790011
new min fval from sgd:  -2629.7100249535342
new min fval from sgd:  -2629.726872256431
new min fval from sgd:  -2629.737787140718
new min fval from sgd:  -2629.7390254985485
new min fval from sgd:  -2629.741532596681
new min fval from sgd:  -2629.754067454279
new min fval from sgd:  -2629.7638666712764
new min fval from sgd:  -2629.770491938863
new min fval from sgd:  -2629.780157488345
new min fval from sgd:  -2629.7855702209094
new min fval from sgd:  -2629.788548469588
new min fval from sgd:  -2629.791352199792
new min fval from sgd:  -2629.7953064692138
new min fval from sgd:  -2629.798082970721
new min fval from sgd:  -2629.800509697103
new min fval from sgd:  -2629.8029776442863
new min fval from sgd:  -2629.806576052227
new min fval from sgd:  -2629.827249510847
new min fval from sgd:  -2629.8441442381513
new min fval from sgd:  -2629.8546428337363
new min fval from sgd:  -2629.8655089754266
new min fval from sgd:  -2629.8702256576653
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [737.34033]
objective value function right now is: -2629.652818759594
min fval:  -2629.8702256576653
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-31.5009,   8.1903],
        [ -0.6438,  -1.5192],
        [ 17.0700,  -1.1691],
        [ -0.6446,  -1.5183],
        [ -0.6389,  -1.5237],
        [ -0.6446,  -1.5183],
        [ 15.8227,  -1.0241],
        [ -0.6447,  -1.5182],
        [ -0.7031,  -1.4468],
        [  1.1734, -17.7575],
        [ -9.7389,  11.7090],
        [ -0.6446,  -1.5183],
        [ -0.6445,  -1.5184]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  4.2002,  -2.7808, -13.6697,  -2.7806,  -2.7813,  -2.7806, -13.6557,
         -2.7806,  -2.8581,  -2.0192,   0.5951,  -2.7806,  -2.7806],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -0.1909,  -0.0451,  -0.3167,  -0.0451,  -0.0452,  -0.0451,  -0.1599,
          -0.0451,  -0.0382,  -0.2987,  -0.5181,  -0.0451,  -0.0451],
        [ -0.1909,  -0.0451,  -0.3167,  -0.0451,  -0.0452,  -0.0451,  -0.1599,
          -0.0451,  -0.0382,  -0.2987,  -0.5181,  -0.0451,  -0.0451],
        [ -0.1909,  -0.0451,  -0.3167,  -0.0451,  -0.0452,  -0.0451,  -0.1599,
          -0.0451,  -0.0382,  -0.2987,  -0.5181,  -0.0451,  -0.0451],
        [ -7.5699,   0.0649,   9.6133,   0.0649,   0.0653,   0.0649,   5.3687,
           0.0649,   0.0505,  16.1769,  -5.4457,   0.0649,   0.0649],
        [ -0.1909,  -0.0451,  -0.3167,  -0.0451,  -0.0452,  -0.0451,  -0.1599,
          -0.0451,  -0.0382,  -0.2987,  -0.5181,  -0.0451,  -0.0451],
        [  9.7025,  -1.0416, -11.3238,  -1.0419,  -1.0428,  -1.0419,  -5.8780,
          -1.0418,  -0.9778, -20.0085,   5.4833,  -1.0419,  -1.0419],
        [-10.4268,  -0.0780,  15.2548,  -0.0791,  -0.0756,  -0.0791,   9.3469,
          -0.0792,  -0.0423,  21.2912,  -7.2086,  -0.0791,  -0.0790],
        [ -0.1909,  -0.0451,  -0.3167,  -0.0451,  -0.0452,  -0.0451,  -0.1599,
          -0.0451,  -0.0382,  -0.2987,  -0.5181,  -0.0451,  -0.0451],
        [ -0.1909,  -0.0451,  -0.3167,  -0.0451,  -0.0452,  -0.0451,  -0.1599,
          -0.0451,  -0.0382,  -0.2987,  -0.5181,  -0.0451,  -0.0451],
        [ -0.1909,  -0.0451,  -0.3167,  -0.0451,  -0.0452,  -0.0451,  -0.1599,
          -0.0451,  -0.0382,  -0.2987,  -0.5181,  -0.0451,  -0.0451],
        [ -0.1909,  -0.0451,  -0.3167,  -0.0451,  -0.0452,  -0.0451,  -0.1599,
          -0.0451,  -0.0382,  -0.2987,  -0.5181,  -0.0451,  -0.0451],
        [ -0.1909,  -0.0451,  -0.3167,  -0.0451,  -0.0452,  -0.0451,  -0.1599,
          -0.0451,  -0.0382,  -0.2987,  -0.5181,  -0.0451,  -0.0451],
        [ -0.1909,  -0.0451,  -0.3167,  -0.0451,  -0.0452,  -0.0451,  -0.1599,
          -0.0451,  -0.0382,  -0.2987,  -0.5181,  -0.0451,  -0.0451]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.5476, -2.5476, -2.5476, -4.9647, -2.5476,  1.1271, -4.9331, -2.5476,
        -2.5476, -2.5476, -2.5476, -2.5476, -2.5476], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.8822e-04,  1.8816e-04,  1.8815e-04, -6.5819e+00,  1.8827e-04,
          1.4561e+01, -1.4480e+01,  1.8822e-04,  1.8821e-04,  1.8820e-04,
          1.8819e-04,  1.8821e-04,  1.8817e-04]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.0883,   0.3475],
        [ -1.6591,   7.8061],
        [ -1.2072,   8.0933],
        [ 14.4147,   2.6307],
        [-16.8141,  -5.9501],
        [-12.6805,   6.4229],
        [ -3.7372,   5.8500],
        [-30.0618,  11.6777],
        [  3.0061,   8.1815],
        [-13.2092,  -3.5220],
        [-14.0553,  -0.4045],
        [ -2.0907,   0.3483],
        [-21.2199,  -5.0411]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -5.1637,  -8.8872,   4.5829,  -5.5440,  -1.1689,   3.6026, -12.9977,
          2.8796,   7.0203,   1.8509,  11.8034,  -5.1622,  -3.1467],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.1618e-02,  6.2106e-01, -3.8642e+00, -1.2659e+00,  1.1410e+01,
         -8.1991e+00,  7.5407e-04, -9.8766e+00, -1.7938e+00,  4.9223e+00,
          1.7240e+00, -2.3074e-02,  9.3055e+00],
        [ 1.3458e-01, -4.7253e-01, -3.5649e-01, -3.4716e+00, -6.7327e-01,
          5.7892e-01, -3.0797e-01, -3.8982e-01, -1.4725e+00,  4.5992e-01,
         -4.1970e-01,  1.3487e-01, -1.9069e-01],
        [ 1.3667e-01, -4.7669e-01, -3.5452e-01, -3.4712e+00, -6.8605e-01,
          5.8414e-01, -3.0966e-01, -4.0254e-01, -1.4648e+00,  4.6015e-01,
         -4.1976e-01,  1.3697e-01, -1.9766e-01],
        [ 5.6481e-03, -6.8309e+00, -9.6334e-01, -9.4006e-01, -1.2307e-01,
         -6.2143e+00, -4.3404e-02, -3.8549e-02,  2.7759e+00, -1.0914e+00,
         -1.1525e+01,  6.0575e-03,  1.3112e-03],
        [ 1.3458e-01, -4.7253e-01, -3.5649e-01, -3.4716e+00, -6.7327e-01,
          5.7892e-01, -3.0798e-01, -3.8981e-01, -1.4725e+00,  4.5993e-01,
         -4.1969e-01,  1.3487e-01, -1.9069e-01],
        [-4.1483e-03, -3.4720e+00,  5.4864e-01,  8.1151e-01, -7.9706e+00,
          2.3554e+00, -1.1188e+01,  1.0498e+01,  1.3279e+00,  1.1922e+00,
          1.5260e+00, -3.0538e-03, -1.1865e+01],
        [ 4.6566e-02,  8.7802e-03, -9.9912e+00, -5.8898e+00,  2.1845e+01,
         -6.8128e+00, -1.1684e-03, -1.9953e+01, -1.1072e+01,  1.3559e+01,
          8.4705e+00,  5.4180e-02,  1.2813e+01],
        [ 1.3673e-01, -4.7683e-01, -3.5441e-01, -3.4711e+00, -6.8654e-01,
          5.8428e-01, -3.0970e-01, -4.0313e-01, -1.4645e+00,  4.6010e-01,
         -4.1993e-01,  1.3702e-01, -1.9788e-01],
        [ 1.0722e-01, -4.1823e-01, -4.0738e-01, -3.4675e+00, -5.4903e-01,
          5.1701e-01, -2.7702e-01, -3.0308e-01, -1.5336e+00,  4.2600e-01,
         -4.3880e-01,  1.0742e-01, -1.1306e-01],
        [ 1.3609e-02,  2.9109e+00, -6.0574e-01, -2.6418e-01,  2.8354e-01,
         -3.0082e+00,  6.2753e-01, -9.8805e-01,  1.5937e+00, -1.5855e+01,
         -2.5138e+00,  1.3378e-02, -2.5046e-02],
        [-5.6330e-01,  5.9429e-05, -8.2776e-01, -7.9250e+00,  3.3245e+00,
          1.9868e-01,  4.6108e-06,  5.7049e-04, -4.0748e+01,  3.5345e+00,
          2.2223e+00, -5.6463e-01,  1.8196e+01],
        [ 1.3491e-01, -4.7322e-01, -3.5614e-01, -3.4714e+00, -6.7551e-01,
          5.7978e-01, -3.0824e-01, -3.9215e-01, -1.4712e+00,  4.5989e-01,
         -4.1987e-01,  1.3520e-01, -1.9185e-01],
        [ 1.2512e-01, -4.5168e-01, -3.7637e-01, -3.4747e+00, -6.2440e-01,
          5.5727e-01, -2.9661e-01, -3.4794e-01, -1.4987e+00,  4.5021e-01,
         -4.1632e-01,  1.2538e-01, -1.6097e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-0.3672, -3.4890, -3.4840, -3.8420, -3.4890, -5.6119, -6.3894, -3.4839,
        -3.5619, -1.5682, -7.6076, -3.4882, -3.5155], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-5.4150e+01,  2.6291e-01,  2.5552e-01,  1.5348e-01,  2.6291e-01,
          3.8680e+00,  3.6376e-01,  2.5511e-01,  3.0149e-01, -3.3811e+01,
          5.0016e-10,  2.6149e-01,  2.8680e-01],
        [ 4.7401e-01, -1.0350e+00, -1.0352e+00,  5.5327e+00, -1.0350e+00,
          3.1236e-02, -4.6156e+00, -1.0351e+00, -9.7308e-01,  1.8130e+00,
          1.5010e+01, -1.0349e+00, -1.0169e+00],
        [-2.3215e+01, -1.8455e-01, -1.8722e-01, -4.4000e-02, -1.8455e-01,
         -4.7052e+01, -5.4954e-03, -1.8729e-01, -1.5140e-01, -3.7397e+01,
          2.0939e-06, -1.8497e-01, -1.7245e-01],
        [-2.2300e+01, -1.8235e-01, -1.8496e-01, -3.5331e-02, -1.8235e-01,
         -4.5599e+01, -1.6829e-03, -1.8503e-01, -1.4917e-01, -3.6956e+01,
         -2.9764e-05, -1.8276e-01, -1.7040e-01],
        [ 2.3522e+00,  9.3002e-01,  9.3988e-01, -3.8489e+00,  9.3002e-01,
          1.1279e+00,  7.4374e+00,  9.4026e-01,  8.5936e-01,  1.8171e+00,
         -1.6179e+01,  9.3175e-01,  9.0226e-01]], device='cuda:0'))])
xi:  [737.3135]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 971.2508382135884
W_T_median: 924.1193721779883
W_T_pctile_5: 737.3070911433092
W_T_CVAR_5_pct: 373.34905369164716
Average q (qsum/M+1):  48.70394011466734
Optimal xi:  [737.3135]
Expected(across Rb) median(across samples) p_equity:  0.2514693388094505
obj fun:  tensor(-2629.8702, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  5.4842,   1.7457],
        [ -2.3237,  20.9502],
        [ -1.0837,  -0.6091],
        [ 11.1950,  -0.7713],
        [ 10.0334,  -7.7157],
        [ 12.4092,  -3.3441],
        [-11.4096,   0.9930],
        [ 15.9614,  -3.8913],
        [-12.0443,   1.0789],
        [  2.3284, -14.6345],
        [  0.0765, -15.8995],
        [-14.4642,   3.2955],
        [ -1.0842,  -0.6099]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-11.0128,  -0.3604,  -3.4295,  -8.6444,  -3.2018,  -2.2513,   7.9982,
         -2.7143,   8.5276,  -2.8851,  -3.5168,   2.0183,  -3.4291],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 4.7015e+00, -9.1661e+00,  2.0161e-01,  1.1218e+01,  6.4544e+00,
          4.3695e+00, -8.1933e+00,  5.1638e+00, -1.0706e+01,  1.3552e+01,
          1.8211e+01, -8.7633e-01,  2.0267e-01],
        [ 2.3724e-02, -2.4174e-01, -1.1759e-02, -5.6326e-02, -3.8082e-02,
         -3.8133e-01, -6.7608e-01, -3.7028e-01, -6.8493e-01, -2.2704e-01,
         -1.7073e-01, -3.8394e-01, -1.1773e-02],
        [ 2.3724e-02, -2.4174e-01, -1.1759e-02, -5.6326e-02, -3.8082e-02,
         -3.8133e-01, -6.7608e-01, -3.7028e-01, -6.8493e-01, -2.2704e-01,
         -1.7073e-01, -3.8394e-01, -1.1773e-02],
        [ 2.3724e-02, -2.4174e-01, -1.1759e-02, -5.6326e-02, -3.8082e-02,
         -3.8133e-01, -6.7608e-01, -3.7028e-01, -6.8493e-01, -2.2704e-01,
         -1.7073e-01, -3.8394e-01, -1.1773e-02],
        [ 2.3724e-02, -2.4174e-01, -1.1759e-02, -5.6326e-02, -3.8082e-02,
         -3.8133e-01, -6.7608e-01, -3.7028e-01, -6.8493e-01, -2.2704e-01,
         -1.7073e-01, -3.8394e-01, -1.1773e-02],
        [ 4.8601e+00, -1.5939e+01,  1.3467e-01,  8.7291e+00,  6.2330e+00,
          7.8241e+00, -4.3390e+00,  7.4038e+00, -6.4961e+00,  1.5376e+01,
          1.9386e+01, -5.7770e+00,  1.3612e-01],
        [ 4.8187e+00, -1.3998e+01,  2.6784e-02,  1.1906e+01,  7.2629e+00,
          7.2438e+00, -6.7163e+00,  6.4444e+00, -8.4976e+00,  1.1575e+01,
          1.4942e+01, -3.7489e+00,  2.5027e-02],
        [ 2.3724e-02, -2.4174e-01, -1.1759e-02, -5.6326e-02, -3.8082e-02,
         -3.8133e-01, -6.7608e-01, -3.7028e-01, -6.8493e-01, -2.2704e-01,
         -1.7073e-01, -3.8394e-01, -1.1773e-02],
        [-1.9032e-02, -2.1767e+00, -2.2652e-01, -5.4142e-03,  1.4409e+01,
          1.5874e+01, -4.2952e+00,  1.7182e+01, -6.0572e+00,  2.6094e+00,
          3.4386e+00, -8.8028e+00, -2.2807e-01],
        [ 1.0027e-01,  1.1041e+00,  3.3275e-02,  2.8062e-01,  6.1554e-01,
          1.4340e+00,  1.6799e+00,  1.3600e+00,  1.7245e+00,  3.5172e-01,
          1.1985e-01,  5.7216e-01,  3.3285e-02],
        [ 2.3724e-02, -2.4174e-01, -1.1759e-02, -5.6326e-02, -3.8082e-02,
         -3.8133e-01, -6.7608e-01, -3.7028e-01, -6.8493e-01, -2.2704e-01,
         -1.7073e-01, -3.8394e-01, -1.1773e-02],
        [ 2.3724e-02, -2.4174e-01, -1.1759e-02, -5.6326e-02, -3.8082e-02,
         -3.8133e-01, -6.7608e-01, -3.7028e-01, -6.8493e-01, -2.2704e-01,
         -1.7073e-01, -3.8394e-01, -1.1773e-02],
        [ 2.3724e-02, -2.4174e-01, -1.1759e-02, -5.6326e-02, -3.8082e-02,
         -3.8133e-01, -6.7608e-01, -3.7028e-01, -6.8493e-01, -2.2704e-01,
         -1.7073e-01, -3.8394e-01, -1.1773e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 0.5472, -0.8975, -0.8975, -0.8975, -0.8975, -0.2979, -0.5856, -0.8975,
         1.5826,  2.1912, -0.8975, -0.8975, -0.8975], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-11.8518,   0.0591,   0.0591,   0.0591,   0.0591, -12.1750,  -9.8670,
           0.0591,  -4.5685,  13.6369,   0.0591,   0.0591,   0.0591]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  5.7392,   0.8660],
        [ -2.5619,   0.7036],
        [-10.7902,  -3.7287],
        [-11.4305,  -3.7065],
        [-12.1812,  -4.0507],
        [ -2.6153,   3.4839],
        [-12.2496,  -4.1285],
        [-17.1808,  -5.6229],
        [-12.7091,  -4.1861],
        [  2.8712,   8.1084],
        [ -6.0131,   2.5466],
        [-16.9900,   4.9966],
        [-16.6843,   4.0052]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-7.6089, -5.7072, -3.6444,  1.8692, -3.9748, -8.8058, -3.9268, -2.2799,
        -3.3852,  0.1138,  3.5286,  1.1892, -0.9168], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.1373e+01, -2.9831e-01,  8.3475e+00,  4.2671e+00,  7.7420e+00,
         -7.8562e-02,  7.1344e+00,  1.0472e+01,  7.7434e+00, -1.0344e+01,
         -3.8165e+00, -1.1814e+01, -1.2950e+01],
        [-8.3675e-01,  1.7674e-02,  1.1230e-01, -1.6329e-01,  1.0499e-01,
         -3.6365e-01,  1.0897e-01,  2.2473e-01,  1.3684e-01, -8.1323e-01,
         -4.4943e-02, -3.3660e-01, -4.0293e-01],
        [-3.1643e-01, -1.7327e-02, -1.1764e-02, -7.9038e-01, -7.0790e-03,
          1.2625e-02, -8.2534e-03, -5.9328e-02, -1.6020e-02, -1.5194e+00,
         -3.3765e+00, -3.9219e-02,  1.0099e-02],
        [ 1.3774e+00,  6.8305e-02, -1.5884e+00,  1.0529e-01, -1.2915e+00,
          5.2336e+00, -1.3934e+00, -5.9350e+00, -2.2535e+00,  3.0932e+00,
          1.0517e+00,  1.1633e+00,  5.8054e+00],
        [-1.3238e+01, -4.9447e-02, -5.6764e+00, -2.4895e+00, -7.5824e+00,
         -9.2110e+00, -5.4895e+00, -1.2219e+01, -8.3252e+00, -1.1805e+00,
          7.8824e+00,  1.0993e+01,  5.9382e+00],
        [-8.8699e-01,  1.8873e-02,  1.3085e-01, -1.3791e-01,  1.2101e-01,
         -3.4931e-01,  1.2580e-01,  2.5074e-01,  1.6204e-01, -9.1326e-01,
          8.7020e-02, -4.3226e-01, -4.3116e-01],
        [ 7.4857e-01, -1.3851e-02,  5.3160e-01, -1.8688e+01, -9.5339e-02,
         -2.7508e-02, -5.9419e-02,  2.0516e+00,  4.0850e-01,  1.9026e-01,
         -2.1448e+01, -1.3353e-02, -6.9958e-03],
        [ 2.2794e+00, -3.4815e-02,  1.6837e-01, -6.3021e-01,  1.5744e-01,
          5.4897e+00,  1.5955e-01,  5.0741e-01,  2.1283e-01, -3.6183e-01,
         -1.9075e+00,  5.0728e+00,  7.1068e+00],
        [-8.8086e-01,  1.8649e-02,  1.2734e-01, -1.3891e-01,  1.1844e-01,
         -3.5789e-01,  1.2295e-01,  2.5140e-01,  1.5728e-01, -8.7466e-01,
          5.9758e-02, -4.0179e-01, -4.2614e-01],
        [ 1.0044e+01,  2.3484e-02,  5.7609e+00, -1.0873e+01,  7.7832e+00,
          1.4628e-02,  5.6381e+00,  2.0713e+01,  1.0207e+01, -5.7883e+00,
         -3.0012e+01,  2.2697e+00,  1.2560e+00],
        [-9.1466e-01,  1.9029e-02,  1.3426e-01, -8.1645e-02,  1.2447e-01,
         -3.4435e-01,  1.2936e-01,  2.4386e-01,  1.6679e-01, -8.4137e-01,
          5.7138e-02, -4.6326e-01, -4.4021e-01],
        [ 5.2903e+00, -3.2259e-01, -2.1667e-01,  6.8081e-01, -1.9747e+00,
          6.5223e-01, -1.3919e+00, -4.5889e+00, -5.0826e-01, -4.8570e-01,
         -2.2097e-01, -9.6557e+00, -4.5325e+00],
        [-9.6375e-01,  1.7983e-02,  1.0976e-01, -1.3910e-01,  1.0165e-01,
         -3.5397e-01,  1.0554e-01,  2.0655e-01,  1.3414e-01, -5.8843e-01,
          6.8727e-03, -4.4446e-01, -4.3234e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.0970, -7.7415, -7.2619, -7.6831, -5.4331, -7.6561, -1.5352, -8.3373,
        -7.6885, -4.4867, -7.6672, -1.7167, -7.7992], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.2927e+02, -7.6102e-01, -1.8073e+00, -4.0198e-01,  3.9245e+00,
         -7.4481e-01, -3.7356e+01,  1.1835e+00, -7.3947e-01, -2.3678e+00,
         -7.8160e-01, -7.1474e+00, -8.0916e-01],
        [-1.8818e+00,  8.3704e-01,  2.2961e+00, -2.6810e-01, -9.9666e-02,
          1.3500e+00,  2.4298e+00,  2.8459e+00,  1.2271e+00,  1.1100e+01,
          1.5159e+00,  2.0585e+00,  1.4031e+00],
        [-1.9794e+01, -1.4639e-01, -6.0995e-02, -6.3540e+00, -2.6278e+01,
         -1.6553e-01, -5.1866e+00, -2.9503e+00, -1.6047e-01, -9.5866e-02,
         -1.6829e-01, -5.9174e+01, -1.5312e-01],
        [-2.0261e+01, -1.1894e-01, -4.5814e-02, -4.9551e+00, -2.5522e+01,
         -1.3805e-01, -3.8821e+00, -3.8095e+00, -1.3297e-01, -8.6666e-02,
         -1.4056e-01, -5.6315e+01, -1.2485e-01],
        [ 8.5377e+00,  1.9001e-01,  2.4545e+00,  2.4847e+00,  1.9109e-01,
          6.4641e-01,  2.1924e+00,  1.6503e-01,  5.2918e-01, -8.0863e+00,
          7.7861e-01,  2.2829e+00,  6.8163e-01]], device='cuda:0'))])
loaded xi:  860.8614
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 946.50330641104
W_T_median: 707.5512584605915
W_T_pctile_5: -238.22847581189643
W_T_CVAR_5_pct: -367.3344080824147
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -3311.2401727174392
Current xi:  [855.35944]
objective value function right now is: -3311.2401727174392
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -3315.9529474571655
Current xi:  [846.7281]
objective value function right now is: -3315.9529474571655
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -3328.6421285592637
Current xi:  [836.246]
objective value function right now is: -3328.6421285592637
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -3332.623689491556
Current xi:  [825.8757]
objective value function right now is: -3332.623689491556
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -3350.7797387314827
Current xi:  [816.66187]
objective value function right now is: -3350.7797387314827
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [801.4278]
objective value function right now is: -3294.383360453451
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [790.8445]
objective value function right now is: -3266.498305295347
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [780.07996]
objective value function right now is: -3290.412176453376
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [772.26166]
objective value function right now is: -3316.685831438547
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [764.83325]
objective value function right now is: -2806.226446630642
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [761.10016]
objective value function right now is: -3340.0594042573375
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [758.75446]
objective value function right now is: -3339.006368600711
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -3364.8776434497686
Current xi:  [755.23834]
objective value function right now is: -3364.8776434497686
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [753.91516]
objective value function right now is: -3335.9881641715197
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [755.12897]
objective value function right now is: -3360.8006712007673
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -3365.0619193483403
Current xi:  [753.4488]
objective value function right now is: -3365.0619193483403
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [752.92993]
objective value function right now is: -3362.2060602522033
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [751.9082]
objective value function right now is: -3356.045735588245
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -3372.5817697390635
Current xi:  [749.40125]
objective value function right now is: -3372.5817697390635
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [749.85864]
objective value function right now is: -3371.4376457081644
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [749.8545]
objective value function right now is: -3360.438134233477
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [749.1666]
objective value function right now is: -3355.6250227578953
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [747.8356]
objective value function right now is: -3269.967982175623
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [748.8132]
objective value function right now is: -3357.1532430936645
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [746.84753]
objective value function right now is: -3352.852925959156
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [746.65186]
objective value function right now is: -3335.5030626605044
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [746.7353]
objective value function right now is: -3355.473046933163
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [747.6998]
objective value function right now is: -3357.450849551071
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [748.02496]
objective value function right now is: -3362.1908404819847
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [748.4376]
objective value function right now is: -3347.972779563902
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [746.7026]
objective value function right now is: -3356.5869924504063
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [747.3498]
objective value function right now is: -3368.5883082265054
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [746.1276]
objective value function right now is: -3366.309047860074
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [746.0028]
objective value function right now is: -3356.095752334114
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [746.29114]
objective value function right now is: -3332.1905581486567
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -3377.876592868169
Current xi:  [746.0266]
objective value function right now is: -3377.876592868169
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -3378.8435429361916
Current xi:  [746.281]
objective value function right now is: -3378.8435429361916
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -3381.091944468788
Current xi:  [746.6094]
objective value function right now is: -3381.091944468788
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [746.7643]
objective value function right now is: -3372.955147323224
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [747.2222]
objective value function right now is: -3380.744349406775
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -3382.7477917065376
Current xi:  [747.5182]
objective value function right now is: -3382.7477917065376
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [748.0004]
objective value function right now is: -3381.353147208402
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [748.4532]
objective value function right now is: -3382.587910459186
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [748.37494]
objective value function right now is: -3381.357567852732
new min fval from sgd:  -3383.873549442684
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [748.6401]
objective value function right now is: -3383.873549442684
new min fval from sgd:  -3383.905181459128
new min fval from sgd:  -3384.060624716774
new min fval from sgd:  -3384.0689884887015
new min fval from sgd:  -3384.1166723374795
new min fval from sgd:  -3384.1878047316036
new min fval from sgd:  -3384.2325179380628
new min fval from sgd:  -3384.345960897923
new min fval from sgd:  -3384.446392163821
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [748.97906]
objective value function right now is: -3381.8301589418056
new min fval from sgd:  -3384.519985590633
new min fval from sgd:  -3384.58938757702
new min fval from sgd:  -3384.6463731524623
new min fval from sgd:  -3384.6723740630523
new min fval from sgd:  -3384.7200838287836
new min fval from sgd:  -3384.743496745268
new min fval from sgd:  -3384.7735054062587
new min fval from sgd:  -3384.7758719242065
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [749.02]
objective value function right now is: -3381.896821507696
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [749.5966]
objective value function right now is: -3383.390086570665
new min fval from sgd:  -3384.813229353249
new min fval from sgd:  -3384.81409180312
new min fval from sgd:  -3384.8242577480114
new min fval from sgd:  -3384.8335912300086
new min fval from sgd:  -3384.84771748834
new min fval from sgd:  -3384.862197161076
new min fval from sgd:  -3384.8736816890123
new min fval from sgd:  -3384.874135729541
new min fval from sgd:  -3384.8852446973597
new min fval from sgd:  -3384.8941483199437
new min fval from sgd:  -3384.900560670213
new min fval from sgd:  -3384.9007339845402
new min fval from sgd:  -3384.90473031528
new min fval from sgd:  -3384.913614489636
new min fval from sgd:  -3384.9311088783725
new min fval from sgd:  -3384.941336242587
new min fval from sgd:  -3384.945368174372
new min fval from sgd:  -3384.955675063028
new min fval from sgd:  -3384.968060546813
new min fval from sgd:  -3384.9789521972048
new min fval from sgd:  -3384.9819399743237
new min fval from sgd:  -3384.9903219345565
new min fval from sgd:  -3385.008094379481
new min fval from sgd:  -3385.0243416009184
new min fval from sgd:  -3385.0452348781428
new min fval from sgd:  -3385.0667906020794
new min fval from sgd:  -3385.0779506172203
new min fval from sgd:  -3385.0792249346246
new min fval from sgd:  -3385.0850050725458
new min fval from sgd:  -3385.0986402778894
new min fval from sgd:  -3385.109870217623
new min fval from sgd:  -3385.1124910157755
new min fval from sgd:  -3385.1152964683165
new min fval from sgd:  -3385.116710957601
new min fval from sgd:  -3385.128044614427
new min fval from sgd:  -3385.1523437715655
new min fval from sgd:  -3385.164660199856
new min fval from sgd:  -3385.1922994385927
new min fval from sgd:  -3385.2218325104
new min fval from sgd:  -3385.2289594714653
new min fval from sgd:  -3385.2421882409003
new min fval from sgd:  -3385.281549151878
new min fval from sgd:  -3385.2961735265367
new min fval from sgd:  -3385.301981814001
new min fval from sgd:  -3385.30636917004
new min fval from sgd:  -3385.3077893503155
new min fval from sgd:  -3385.3185245025925
new min fval from sgd:  -3385.325434260073
new min fval from sgd:  -3385.326985652769
new min fval from sgd:  -3385.330277169575
new min fval from sgd:  -3385.3348276055094
new min fval from sgd:  -3385.345555744358
new min fval from sgd:  -3385.3585021471868
new min fval from sgd:  -3385.3599961423
new min fval from sgd:  -3385.360019459442
new min fval from sgd:  -3385.3627170104787
new min fval from sgd:  -3385.365266361123
new min fval from sgd:  -3385.3696384852296
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [749.923]
objective value function right now is: -3384.187552477666
new min fval from sgd:  -3385.3742629618228
new min fval from sgd:  -3385.3909071279068
new min fval from sgd:  -3385.3970509133696
new min fval from sgd:  -3385.3998831016
new min fval from sgd:  -3385.418853381686
new min fval from sgd:  -3385.439179108421
new min fval from sgd:  -3385.454321768231
new min fval from sgd:  -3385.4673164029764
new min fval from sgd:  -3385.48468551016
new min fval from sgd:  -3385.5037145142023
new min fval from sgd:  -3385.5227297363485
new min fval from sgd:  -3385.5382135356044
new min fval from sgd:  -3385.5675924689754
new min fval from sgd:  -3385.604463978705
new min fval from sgd:  -3385.631568765794
new min fval from sgd:  -3385.6518655026193
new min fval from sgd:  -3385.6677985134106
new min fval from sgd:  -3385.676310095712
new min fval from sgd:  -3385.677046255081
new min fval from sgd:  -3385.6842291571265
new min fval from sgd:  -3385.690720367403
new min fval from sgd:  -3385.695263480056
new min fval from sgd:  -3385.7028666976394
new min fval from sgd:  -3385.7309762291284
new min fval from sgd:  -3385.755753744781
new min fval from sgd:  -3385.7695405378304
new min fval from sgd:  -3385.778053955708
new min fval from sgd:  -3385.7859840667174
new min fval from sgd:  -3385.7865537168204
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [750.022]
objective value function right now is: -3385.6231340260824
min fval:  -3385.7865537168204
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  7.9775,   2.9851],
        [  2.5531,  29.8728],
        [ -0.9327,  -0.5216],
        [ 16.0186,  -1.4325],
        [ -0.9009,  -0.5257],
        [ 11.3073,  -6.4122],
        [-15.9750,   1.4366],
        [ 15.1605, -13.7134],
        [-16.6405,   1.5885],
        [  5.5163, -20.8629],
        [  6.2094, -21.8266],
        [-25.0272,   5.1020],
        [ -0.9327,  -0.5216]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-13.0440,  -5.6582,  -3.3215, -11.4108,  -3.3886,  -4.4538,  10.7587,
         -3.2673,  11.4242,  -2.7748,  -2.9113,   2.1215,  -3.3215],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 3.5947e+00, -6.5387e+00,  8.6093e-02,  1.6516e+01,  1.5040e-02,
          5.5686e+00, -1.1055e+01,  6.9901e+00, -1.4943e+01,  1.4345e+01,
          1.3589e+01,  5.1440e+00,  8.6093e-02],
        [ 2.4965e-02, -5.1686e-01, -2.9629e-03, -2.1503e-01, -2.7609e-03,
         -1.6869e-01, -8.4917e-01, -2.9396e-01, -8.9056e-01, -2.5025e-01,
         -2.4787e-01, -3.8385e-01, -2.9629e-03],
        [ 2.4965e-02, -5.1686e-01, -2.9629e-03, -2.1503e-01, -2.7609e-03,
         -1.6869e-01, -8.4917e-01, -2.9396e-01, -8.9056e-01, -2.5025e-01,
         -2.4787e-01, -3.8385e-01, -2.9629e-03],
        [ 2.4965e-02, -5.1686e-01, -2.9629e-03, -2.1503e-01, -2.7609e-03,
         -1.6869e-01, -8.4917e-01, -2.9396e-01, -8.9056e-01, -2.5025e-01,
         -2.4787e-01, -3.8385e-01, -2.9629e-03],
        [ 2.4965e-02, -5.1686e-01, -2.9629e-03, -2.1503e-01, -2.7609e-03,
         -1.6869e-01, -8.4917e-01, -2.9396e-01, -8.9056e-01, -2.5025e-01,
         -2.4787e-01, -3.8385e-01, -2.9629e-03],
        [-1.6183e-02, -1.6097e+01, -1.0866e-01, -2.1752e+00, -1.8715e-01,
          7.8805e-01, -3.9336e+00,  1.0061e+01, -6.6200e+00,  1.2362e+01,
          1.3844e+01, -1.3607e+01, -1.0866e-01],
        [ 4.7205e+00, -1.3311e+01,  1.2014e-01,  1.5790e+01,  2.3635e-01,
          3.0547e+00, -3.6979e+00,  1.1101e+01, -6.5681e+00,  1.5125e+01,
          1.5085e+01, -1.0038e+01,  1.2014e-01],
        [ 2.4965e-02, -5.1686e-01, -2.9629e-03, -2.1503e-01, -2.7609e-03,
         -1.6869e-01, -8.4917e-01, -2.9396e-01, -8.9056e-01, -2.5025e-01,
         -2.4787e-01, -3.8385e-01, -2.9629e-03],
        [-4.2217e-02, -9.3496e+00, -1.9202e-01, -3.0426e+00, -2.0787e-01,
         -4.0516e+00,  2.4742e+00, -5.1032e-01,  1.4584e+00,  5.0798e-01,
          2.5522e-01, -5.3292e-01, -1.9202e-01],
        [ 2.2788e-01,  1.7979e+00,  7.0063e-02,  9.6925e-01,  6.7828e-02,
          4.9018e-01,  1.6774e+00,  5.3761e-01,  1.7978e+00,  5.8403e-01,
          5.8954e-01,  8.5549e-01,  7.0063e-02],
        [ 2.4965e-02, -5.1686e-01, -2.9629e-03, -2.1503e-01, -2.7609e-03,
         -1.6869e-01, -8.4917e-01, -2.9396e-01, -8.9056e-01, -2.5025e-01,
         -2.4787e-01, -3.8385e-01, -2.9629e-03],
        [ 2.4965e-02, -5.1686e-01, -2.9629e-03, -2.1503e-01, -2.7609e-03,
         -1.6869e-01, -8.4917e-01, -2.9396e-01, -8.9056e-01, -2.5025e-01,
         -2.4787e-01, -3.8385e-01, -2.9629e-03],
        [ 2.4965e-02, -5.1686e-01, -2.9629e-03, -2.1503e-01, -2.7609e-03,
         -1.6869e-01, -8.4917e-01, -2.9396e-01, -8.9056e-01, -2.5025e-01,
         -2.4787e-01, -3.8385e-01, -2.9629e-03]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 2.5809, -1.3188, -1.3188, -1.3188, -1.3188,  0.8373,  3.2251, -1.3188,
         1.8060,  3.0532, -1.3188, -1.3188, -1.3188], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-15.7115,   0.0359,   0.0359,   0.0359,   0.0359,  -9.8108, -10.3854,
           0.0359,  -1.7957,  13.0043,   0.0359,   0.0359,   0.0359]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  2.6400,   4.8946],
        [ -2.4965,   0.2171],
        [-14.9756,  -4.0659],
        [ -9.9949,  -5.4884],
        [-13.0723,  -5.5689],
        [ -0.9058,   6.6268],
        [-15.0195,  -6.0646],
        [-17.8449,  -7.0168],
        [-18.3742,  -4.4337],
        [  7.3984,  -1.3882],
        [ -0.4119,  -0.1213],
        [ -8.3240,   1.3847],
        [-21.7699,   8.8634]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-12.1585,  -5.6315,  -3.6808,   3.2874,  -0.4879, -10.7540,  -1.7035,
         -1.8260,  -3.5133,  -9.9031,   9.1469,   2.1171,   1.7494],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.1295e-01,  2.2052e-01,  8.7081e+00,  7.6886e+00,  7.6289e+00,
          2.5475e-01,  1.8337e+00,  3.9202e+00,  7.1780e+00, -9.8400e+00,
         -5.9337e+00, -5.9189e+00, -1.7501e+01],
        [-3.2178e-01, -9.8714e-03, -2.0175e-02,  1.0321e-01, -2.2700e-01,
         -9.3834e-02, -2.3404e-01, -2.5285e-01, -2.7345e-02, -1.5140e+00,
         -2.3563e+00, -5.9449e-01, -6.5546e-01],
        [-3.9362e-01,  4.5157e-03, -1.8809e-03, -1.4955e+00, -3.1797e-03,
         -3.8103e-01, -1.1279e-02, -2.4747e-02, -4.4468e-03,  4.2072e-01,
         -3.3883e+00, -4.8800e-01, -4.2842e-01],
        [ 2.8627e+00, -4.7007e-02,  4.7227e-01,  7.3824e-01, -8.1787e-01,
          4.3674e+00, -4.7547e-01,  1.7591e+00,  4.6686e-01, -3.7754e-01,
          2.7057e+00, -2.6333e+00, -3.1052e+00],
        [-9.6755e+00,  2.9104e-01, -4.3281e+00, -1.3225e-01, -4.4596e+00,
         -1.3597e+01, -3.4061e+00, -7.4784e+00, -3.8741e+00, -6.2932e+00,
          4.5227e+00,  4.7322e+00,  1.4005e+01],
        [-4.1169e-01,  4.2038e-03, -1.9159e-03, -1.5503e+00,  1.5657e-02,
         -3.9627e-01,  2.5818e-04, -1.4734e-02, -4.2415e-03,  4.7303e-01,
         -3.3979e+00, -5.0668e-01, -4.4238e-01],
        [-5.5819e-01, -1.8889e-03, -2.1434e-03, -2.0210e+00,  9.8739e-02,
         -5.3023e-01,  1.1432e-01,  7.8741e-02, -4.4020e-03,  1.1111e+00,
         -4.2473e+00, -7.1740e-01, -5.9947e-01],
        [-2.7485e-01, -7.8524e-03, -5.2615e-02,  4.1118e-01, -5.9048e-01,
         -1.5618e-01, -4.9667e-01, -5.3675e-01, -6.7678e-02, -1.3288e+00,
         -2.6833e+00, -4.1651e-01, -6.5554e-01],
        [-4.1298e-01,  4.1794e-03, -1.9181e-03, -1.5541e+00,  1.6978e-02,
         -3.9735e-01,  1.1035e-03, -1.4008e-02, -4.2274e-03,  4.7687e-01,
         -3.3983e+00, -5.0798e-01, -4.4339e-01],
        [-3.5487e-04, -3.4774e-01,  1.0514e+01, -8.5833e+00,  1.1861e+01,
         -1.0608e-05,  1.5417e+01,  2.5233e+01,  1.8156e+01, -1.5135e+01,
         -3.3234e+01, -3.0958e+01, -1.1934e-01],
        [-4.1004e-01,  4.2351e-03, -1.9130e-03, -1.5453e+00,  1.3965e-02,
         -3.9489e-01, -8.1817e-04, -1.5659e-02, -4.2594e-03,  4.6812e-01,
         -3.3972e+00, -5.0499e-01, -4.4107e-01],
        [ 1.1361e+00, -1.2504e+00,  1.6778e+00,  2.7384e+00,  1.2798e+00,
          2.9687e+00,  1.2054e+01,  2.0842e+01,  1.0445e+00,  1.0996e+00,
         -2.1639e+00, -4.1088e+01,  5.0948e+00],
        [-4.1136e-01,  4.2103e-03, -1.9153e-03, -1.5492e+00,  1.5313e-02,
         -3.9599e-01,  3.8169e-05, -1.4923e-02, -4.2451e-03,  4.7203e-01,
         -3.3977e+00, -5.0633e-01, -4.4211e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.9347, -6.8648, -4.6576, -4.3655, -6.4239, -4.6206, -3.3536, -6.3014,
        -4.6181, -4.1353, -4.6237, -1.0633, -4.6212], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.0722e+02, -4.3978e-01, -3.8418e-01, -5.1325e-01,  3.4982e+00,
         -3.9654e-01, -3.7509e-01, -8.7065e-01, -3.9739e-01,  3.2558e-11,
         -3.9545e-01, -3.7812e+01, -3.9632e-01],
        [-1.3928e-01,  9.7234e-02,  5.8235e-01,  3.3134e+00, -3.9049e-02,
          5.8653e-01,  1.2415e+00,  9.3288e-01,  5.8692e-01,  1.5348e+01,
          5.8600e-01,  2.2126e+00,  5.8641e-01],
        [-1.3166e+01, -2.6277e-02, -6.5970e-02, -3.7122e+01, -2.8464e+01,
         -6.7395e-02, -9.5348e-02, -3.3844e-02, -6.7499e-02, -1.3052e-12,
         -6.7261e-02, -7.2075e+00, -6.7367e-02],
        [-1.5309e+01, -2.3461e-02, -5.9590e-02, -3.5584e+01, -2.9058e+01,
         -6.0681e-02, -8.6011e-02, -3.2004e-02, -6.0763e-02, -5.4943e-13,
         -6.0577e-02, -6.6250e+00, -6.0659e-02],
        [ 8.7752e+00,  1.0983e+00,  1.7812e-01,  3.3637e+00,  9.3711e-01,
          1.4154e-01,  2.9257e-01,  1.7576e+00,  1.3891e-01, -1.2969e+01,
          1.4486e-01,  3.1375e+00,  1.4221e-01]], device='cuda:0'))])
xi:  [750.04913]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1048.7862936069266
W_T_median: 921.6795911105428
W_T_pctile_5: 750.2305403733781
W_T_CVAR_5_pct: 384.31606966404814
Average q (qsum/M+1):  47.232847152217744
Optimal xi:  [750.04913]
Expected(across Rb) median(across samples) p_equity:  0.24094383887325724
obj fun:  tensor(-3385.7866, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([0.2313, 0.2313, 0.2313, 0.2313, 0.2313, 0.2313, 0.2313, 0.2313, 0.2313,
        0.2313, 0.2313, 0.2313, 0.2313], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([0.3517, 0.3517, 0.3517, 0.3517, 0.3517, 0.3517, 0.3517, 0.3517, 0.3517,
        0.3517, 0.3517, 0.3517, 0.3517], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.7196, -1.7196, -1.7196, -1.7196, -1.7196, -1.7196, -1.7196, -1.7196,
         -1.7196, -1.7196, -1.7196, -1.7196, -1.7196]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-10.7834,  -3.3367],
        [ -9.5023,  -2.6927],
        [ -4.0806,   4.8939],
        [  6.4023,   0.5775],
        [-13.1594,   5.5594],
        [-15.2797,   6.7979],
        [-20.9911,   8.9603],
        [ -5.8247,   4.0560],
        [ -2.8822,   0.6886],
        [-11.2051,  -3.4935],
        [-17.1253,   7.2599],
        [ -9.2238,   6.8781],
        [  7.1920,   6.5082]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-0.6867,  1.0557, -7.9647, -7.0451,  4.1105, -0.0603,  2.8838,  5.2928,
        -7.5512, -0.7086, -2.6104, -0.4280, -1.8994], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-6.1840e+00, -4.0300e+00,  6.7230e-01, -8.7433e+00,  4.9619e+00,
          3.5240e+00,  1.3665e+01,  4.3033e+00,  1.9278e-01, -8.5220e+00,
          5.5900e+00,  5.8267e-01,  1.4234e+00],
        [ 8.9565e+00,  8.4084e+00, -2.1482e-01, -3.8297e+00, -1.1307e+01,
         -9.0172e+00, -7.9172e+00, -3.9407e+00,  2.7186e-01,  1.1643e+01,
         -7.0817e+00, -1.5448e+01, -1.4800e+00],
        [-5.0391e+00, -5.9890e+00, -9.0398e-03, -8.1836e+00, -4.2707e+00,
         -1.9289e-01, -5.0504e-01,  3.1921e+00,  4.2272e-02, -5.4949e+00,
         -2.1002e-02, -1.3154e+00, -1.1644e+01],
        [ 1.1020e+01,  8.0680e+00, -3.2168e-01, -2.3057e+01, -6.1697e+00,
         -8.8288e+00, -4.5553e+00, -9.6446e+00, -1.3967e-01,  1.3095e+01,
         -7.9016e+00, -1.4550e+01, -9.8644e+00],
        [-1.8250e-01, -8.3369e-01, -9.1884e-02, -7.8173e-01, -2.0327e-01,
         -1.0853e-01, -1.5925e-01, -1.5944e+00, -3.0435e-03, -1.6182e-01,
         -9.3304e-02, -6.6412e-02, -9.1861e-01],
        [-2.0194e-01, -8.9846e-01, -1.1953e-01, -6.0599e-01, -2.4169e-01,
         -1.3953e-01, -2.0331e-01, -1.9067e+00, -7.2574e-04, -1.7970e-01,
         -1.2194e-01, -8.3862e-02, -1.0632e+00],
        [-2.3542e+01, -2.5981e+01, -2.0617e-02,  4.9505e+00, -1.1755e-01,
         -5.3358e-03, -4.1763e-04, -3.4001e+01, -2.1342e-01, -2.2267e+01,
         -1.1207e-04, -3.9470e-02,  1.6500e+00],
        [-2.3557e+01, -2.7808e+01,  3.0241e+00,  4.1482e+00,  1.0980e+00,
          1.3570e+00,  1.8858e-01, -3.8561e+00, -5.0918e-01, -2.1496e+01,
          2.9611e-01,  2.0051e+00,  1.9709e+00],
        [-6.1498e+00, -6.0934e+00,  1.0393e-01,  1.6573e+00,  1.7891e+00,
         -1.0556e+00, -2.0269e+00,  5.8258e+00,  7.6056e-02, -6.9260e+00,
         -1.6583e+00, -2.2214e+00, -3.4700e+00],
        [-6.4960e+00, -1.8472e+01, -2.7811e-04,  8.0951e-01, -1.0104e-02,
          1.4101e-05, -2.2582e-05, -2.0124e+01, -2.1728e-02, -5.3192e+00,
         -2.5020e-05, -1.2738e-03, -1.2177e+00],
        [-1.8772e-01, -8.4996e-01, -9.5735e-02, -7.7303e-01, -2.2901e-01,
         -1.1504e-01, -1.7268e-01, -1.6074e+00, -3.0761e-03, -1.6674e-01,
         -9.7120e-02, -6.9475e-02, -8.9016e-01],
        [-2.1054e-01, -9.2220e-01, -1.1061e-01, -7.3823e-01, -3.4380e-01,
         -1.4310e-01, -2.3385e-01, -1.5912e+00, -3.1803e-03, -1.8858e-01,
         -1.1182e-01, -8.4845e-02, -7.5565e-01],
        [-1.8376e+00, -9.4792e-01, -3.2928e+00,  1.6705e+00, -1.2567e+00,
         -4.0231e+00, -2.8440e+00,  3.3299e+00,  1.3279e-01, -2.0275e+00,
         -1.6163e+00, -3.6771e+00,  2.3123e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-5.6791, -0.8108, -6.8804, -4.6567, -9.8524, -9.8244, -3.7420, -2.2029,
        -6.8923, -7.1539, -9.8569, -9.8885, -9.0150], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 3.8194e+00, -3.9052e+01, -1.2230e+01, -4.1683e+01,  3.1235e-01,
          3.7172e-01, -1.0024e+01, -2.4706e+00, -8.2299e+00, -2.7773e+00,
          2.8753e-01,  1.7390e-01,  3.8215e+00],
        [ 4.0737e-01,  1.2821e+00,  7.8172e-01, -5.7906e+00,  4.0975e-01,
          5.0652e-01,  5.2298e+00,  1.9991e+00, -1.0615e+00,  2.2931e+00,
          4.6926e-01,  8.3136e-01, -4.9475e+00],
        [-9.6272e+01, -8.8708e+01, -1.3613e-01, -3.8722e-02, -1.9520e-02,
         -1.5910e-02, -4.2049e-04, -4.8650e+01, -1.1766e+01, -4.2487e-04,
         -1.9503e-02, -2.0168e-02, -6.6272e+00],
        [-8.8676e+01, -8.7348e+01, -9.1593e-02, -3.2694e-02, -1.4067e-02,
         -9.9616e-03,  1.2445e-04, -4.9205e+01, -1.0223e+01, -2.7381e-04,
         -1.4045e-02, -1.4736e-02, -5.1341e+00],
        [ 5.8239e-01,  2.4476e+00,  3.2148e+00,  8.5433e+00,  6.5953e-01,
          9.9856e-01, -2.1942e+00,  1.9454e+00,  4.1462e+00,  1.0315e+00,
          7.2836e-01,  1.1040e+00,  2.9719e+00]], device='cuda:0'))])
loaded xi:  869.86163
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 946.50330641104
W_T_median: 707.5512584605915
W_T_pctile_5: -238.22847581189643
W_T_CVAR_5_pct: -367.3344080824147
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -19487.813867201694
Current xi:  [860.28455]
objective value function right now is: -19487.813867201694
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [846.97845]
objective value function right now is: -19381.66551881937
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -19523.875054955282
Current xi:  [835.5856]
objective value function right now is: -19523.875054955282
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [822.6445]
objective value function right now is: -19354.08709014484
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -19854.629110922455
Current xi:  [811.45966]
objective value function right now is: -19854.629110922455
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -19937.498588709033
Current xi:  [802.3482]
objective value function right now is: -19937.498588709033
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [792.7965]
objective value function right now is: -19850.15534113639
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -19986.855048939364
Current xi:  [785.1118]
objective value function right now is: -19986.855048939364
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [779.2295]
objective value function right now is: -19777.487073630728
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [774.3324]
objective value function right now is: -19649.396770846975
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [769.8429]
objective value function right now is: -19963.569655630105
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [765.83405]
objective value function right now is: -19887.638155744065
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [762.9381]
objective value function right now is: -19936.25795891832
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [760.47253]
objective value function right now is: -19874.21140855377
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [760.12775]
objective value function right now is: -19964.470837415935
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [758.4652]
objective value function right now is: -19852.4530890479
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [758.1601]
objective value function right now is: -19879.495166403594
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [756.7413]
objective value function right now is: -19985.84011625785
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -20054.39105423227
Current xi:  [755.4316]
objective value function right now is: -20054.39105423227
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [754.6545]
objective value function right now is: -19840.66732145376
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -20054.597187528016
Current xi:  [753.5009]
objective value function right now is: -20054.597187528016
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [756.4557]
objective value function right now is: -20028.89437988472
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [754.22833]
objective value function right now is: -19480.95593652653
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [756.1321]
objective value function right now is: -20010.821516149485
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [755.40356]
objective value function right now is: -20046.54371532887
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [755.3931]
objective value function right now is: -19949.107389854664
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [754.55237]
objective value function right now is: -19772.649314797618
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -20084.365317874828
Current xi:  [754.3919]
objective value function right now is: -20084.365317874828
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [752.29376]
objective value function right now is: -19933.62990512877
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [751.26624]
objective value function right now is: -19926.97615013275
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [752.90796]
objective value function right now is: -19889.46414073277
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [752.0551]
objective value function right now is: -20039.384716461114
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [752.8726]
objective value function right now is: -19816.587573971276
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [752.6976]
objective value function right now is: -19961.838586172056
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [752.25037]
objective value function right now is: -20051.604968140982
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -20087.41482354761
Current xi:  [752.5979]
objective value function right now is: -20087.41482354761
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -20126.690881788756
Current xi:  [752.39453]
objective value function right now is: -20126.690881788756
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [752.45465]
objective value function right now is: -20124.604327558198
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -20134.576196580725
Current xi:  [752.61523]
objective value function right now is: -20134.576196580725
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -20140.122770995335
Current xi:  [752.79974]
objective value function right now is: -20140.122770995335
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [753.0013]
objective value function right now is: -20123.261270209918
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [753.06964]
objective value function right now is: -20118.28449562683
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [753.13434]
objective value function right now is: -20117.89359120835
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [753.25616]
objective value function right now is: -20131.279097541792
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [753.3384]
objective value function right now is: -20133.504956345892
new min fval from sgd:  -20140.795743367766
new min fval from sgd:  -20141.199933111067
new min fval from sgd:  -20142.035612680895
new min fval from sgd:  -20142.45958490022
new min fval from sgd:  -20142.79895397523
new min fval from sgd:  -20143.50932073338
new min fval from sgd:  -20145.007288166904
new min fval from sgd:  -20146.31601376397
new min fval from sgd:  -20148.53138749345
new min fval from sgd:  -20150.29040239675
new min fval from sgd:  -20151.419322523725
new min fval from sgd:  -20152.000403161903
new min fval from sgd:  -20152.36664223169
new min fval from sgd:  -20152.648559946596
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [753.1821]
objective value function right now is: -20140.126571753004
new min fval from sgd:  -20154.049743811607
new min fval from sgd:  -20155.359243710493
new min fval from sgd:  -20156.350493291295
new min fval from sgd:  -20156.944856206002
new min fval from sgd:  -20157.490194200105
new min fval from sgd:  -20157.785779517704
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [753.5162]
objective value function right now is: -20113.12733483003
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [753.49756]
objective value function right now is: -20130.717546507236
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [753.3041]
objective value function right now is: -20154.25988926263
new min fval from sgd:  -20157.836831924033
new min fval from sgd:  -20158.112656384194
new min fval from sgd:  -20158.42213217874
new min fval from sgd:  -20158.76478037969
new min fval from sgd:  -20158.992090685657
new min fval from sgd:  -20159.135434979275
new min fval from sgd:  -20159.24116772687
new min fval from sgd:  -20159.36320538098
new min fval from sgd:  -20159.425511796875
new min fval from sgd:  -20159.508283172258
new min fval from sgd:  -20159.54489410036
new min fval from sgd:  -20159.597676495316
new min fval from sgd:  -20159.63805160289
new min fval from sgd:  -20159.710088006832
new min fval from sgd:  -20159.874083320818
new min fval from sgd:  -20159.97026670545
new min fval from sgd:  -20160.08645900304
new min fval from sgd:  -20160.199462644367
new min fval from sgd:  -20160.300291580275
new min fval from sgd:  -20160.336930032718
new min fval from sgd:  -20160.404216257662
new min fval from sgd:  -20160.45820508954
new min fval from sgd:  -20160.4648447071
new min fval from sgd:  -20160.51422600443
new min fval from sgd:  -20160.522918086106
new min fval from sgd:  -20160.527982404874
new min fval from sgd:  -20160.551965864845
new min fval from sgd:  -20160.608968717836
new min fval from sgd:  -20160.612620413616
new min fval from sgd:  -20160.872441195424
new min fval from sgd:  -20161.122111777033
new min fval from sgd:  -20161.18757789571
new min fval from sgd:  -20161.235496487403
new min fval from sgd:  -20161.32394430022
new min fval from sgd:  -20161.40587123548
new min fval from sgd:  -20161.47423688458
new min fval from sgd:  -20161.586391146233
new min fval from sgd:  -20161.676049121903
new min fval from sgd:  -20161.747262072524
new min fval from sgd:  -20161.81651286777
new min fval from sgd:  -20161.837230004207
new min fval from sgd:  -20161.868238044968
new min fval from sgd:  -20161.90139563019
new min fval from sgd:  -20161.908703003686
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [753.38684]
objective value function right now is: -20159.704796048474
min fval:  -20161.908703003686
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.1290, -0.1603],
        [ 0.1290, -0.1603],
        [ 0.1290, -0.1603],
        [ 0.1290, -0.1603],
        [ 0.1290, -0.1603],
        [ 0.1290, -0.1603],
        [ 0.1290, -0.1603],
        [ 0.1290, -0.1603],
        [ 0.1290, -0.1603],
        [ 0.1290, -0.1603],
        [ 0.1290, -0.1603],
        [ 0.1290, -0.1603],
        [ 0.1290, -0.1603]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([0.2288, 0.2288, 0.2288, 0.2288, 0.2288, 0.2288, 0.2288, 0.2288, 0.2288,
        0.2288, 0.2288, 0.2288, 0.2288], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107,
         0.2107, 0.2107, 0.2107, 0.2107],
        [0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107,
         0.2107, 0.2107, 0.2107, 0.2107],
        [0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107,
         0.2107, 0.2107, 0.2107, 0.2107],
        [0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107,
         0.2107, 0.2107, 0.2107, 0.2107],
        [0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107,
         0.2107, 0.2107, 0.2107, 0.2107],
        [0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107,
         0.2107, 0.2107, 0.2107, 0.2107],
        [0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107,
         0.2107, 0.2107, 0.2107, 0.2107],
        [0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107,
         0.2107, 0.2107, 0.2107, 0.2107],
        [0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107,
         0.2107, 0.2107, 0.2107, 0.2107],
        [0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107,
         0.2107, 0.2107, 0.2107, 0.2107],
        [0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107,
         0.2107, 0.2107, 0.2107, 0.2107],
        [0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107,
         0.2107, 0.2107, 0.2107, 0.2107],
        [0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107, 0.2107,
         0.2107, 0.2107, 0.2107, 0.2107]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([0.3497, 0.3497, 0.3497, 0.3497, 0.3497, 0.3497, 0.3497, 0.3497, 0.3497,
        0.3497, 0.3497, 0.3497, 0.3497], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.7185, -1.7185, -1.7185, -1.7185, -1.7185, -1.7185, -1.7185, -1.7185,
         -1.7185, -1.7185, -1.7185, -1.7185, -1.7185]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-16.0515,  -5.2047],
        [-12.0155,  -2.8588],
        [ -7.0048,   4.2339],
        [ 12.1381,   0.1156],
        [-14.3029,   5.5022],
        [-20.1197,   6.4691],
        [-28.1059,  10.7231],
        [ -7.1179,   2.9795],
        [ -2.6009,   0.8759],
        [-16.3797,  -5.3030],
        [-20.0804,   6.8397],
        [-12.8085,   3.6746],
        [  6.8583,   5.9966]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -1.0031,   2.6830, -10.4278, -12.6226,   4.1385,  -3.1247,   3.4107,
          4.7693,  -7.5138,  -0.7039,  -3.5069,  -7.1115,  -0.8560],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-9.2200e+00,  5.6300e-01, -9.9673e+00, -2.5475e+01,  4.4287e+00,
         -2.8768e+00,  1.3958e+01,  1.9097e-01, -2.2882e-02, -1.1389e+01,
         -1.9213e+00, -5.2121e+00,  4.9256e+00],
        [ 1.0277e+01,  5.5226e+00,  1.0614e-02, -2.0479e+00, -8.7941e+00,
         -1.2399e+01, -1.3140e+01, -3.3953e+00, -1.6579e-02,  1.2381e+01,
         -1.0879e+01, -5.5261e-01, -1.7761e+00],
        [ 3.4371e+00, -3.6824e+00,  1.3625e-02, -9.4830e+00, -4.1693e+01,
         -1.9204e-02, -3.0841e-01,  5.0853e+00,  1.2928e-01,  3.2184e+00,
         -4.0321e-03,  4.4728e-03, -9.9071e+00],
        [ 1.7192e+01,  6.9425e+00,  3.3101e-03, -2.7953e+01, -9.7639e+00,
         -1.4874e+01, -2.0520e+01, -6.8478e+00, -1.7040e-01,  2.0667e+01,
         -1.0601e+01, -5.0843e-01, -4.4340e+01],
        [-2.4598e+00, -1.0939e+00, -8.7098e-01, -1.3145e+00,  5.6316e+00,
         -3.7516e+00, -2.7526e+00,  9.2666e-01,  1.3764e-01, -2.8161e+00,
         -3.4076e+00, -1.0699e+00,  1.5560e+00],
        [-2.6149e+00,  5.9548e-01, -3.0205e+00, -5.9048e-01,  3.3708e+00,
         -1.1279e+00, -1.9689e+00,  1.1156e+00, -1.2814e-01, -2.9285e+00,
         -2.7245e+00, -2.4858e-03,  1.0155e+00],
        [-2.3504e+01, -5.4204e+01, -2.8832e-04,  5.5174e-01, -1.7707e-01,
          3.8842e-06,  1.0820e-07, -4.0632e+01,  2.4194e-02, -2.6398e+01,
         -6.0940e-07,  4.3618e-04,  1.1984e-01],
        [ 1.1588e-01, -5.6368e+01,  9.9476e-01,  4.0337e+00,  2.2741e+00,
          7.1915e-01,  1.2606e+01, -1.0506e+00, -1.5454e-01,  1.3380e-01,
          1.5628e+00,  4.4148e-02,  1.2964e+00],
        [-4.9507e+00, -4.8091e-01, -1.0530e+00,  6.2587e-01, -5.6634e+00,
         -1.1920e+00, -1.4047e+00,  3.8760e+00,  3.6157e-02, -6.2053e+00,
         -7.1592e-01, -8.9357e-02, -1.5471e+00],
        [ 1.3843e+00,  1.5166e+00, -1.2335e+00, -1.7531e+00,  1.8514e+00,
         -6.9861e-01,  2.8242e+00,  1.2087e+00, -9.7924e-03,  1.5342e+00,
         -8.3663e-01, -8.4050e-01, -6.3364e-01],
        [ 8.9632e-01,  1.5159e+00,  4.9158e+00, -4.0294e+00,  1.4626e+00,
         -3.8516e+00, -4.7221e+00,  2.1607e-01, -1.1776e-01,  6.6113e-01,
         -2.5173e+00, -4.8849e-01,  1.1523e+00],
        [-1.0888e+00,  6.3465e-02, -5.3757e-02, -2.2626e-01,  4.5916e+00,
         -3.3899e+00, -4.7505e+00,  7.2787e-01, -5.2237e-02, -1.3674e+00,
         -2.5125e+00,  5.5926e-02,  1.6102e+00],
        [-5.0355e+00,  1.6715e+00, -1.3436e+01, -2.3807e+00,  2.1820e+00,
          7.1151e-01,  5.3134e+00,  5.0510e-01, -8.0381e-02, -5.3292e+00,
         -2.1265e-01, -2.7194e+00,  1.7413e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-5.1828, -1.4213, -3.4141, -4.4241, -8.4656, -9.8435, -3.8153, -3.9824,
        -7.4935, -9.2427, -8.3725, -9.4241, -7.2741], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 4.0082e+00, -4.4999e+01, -3.0638e+01, -1.7149e-01,  4.9714e+00,
          6.0350e+00, -4.0498e+01,  8.0357e-01,  2.4250e+00,  2.5190e+00,
          7.5499e+00,  6.0784e+00,  4.5970e+00],
        [ 4.7484e-01,  1.0795e+00, -5.5075e+00, -1.0047e+01, -9.3733e+00,
         -6.6307e+00,  6.2534e+00,  2.0823e+00, -7.6019e+00, -1.8864e-01,
         -7.4240e+00, -7.8383e+00, -7.6719e+00],
        [-8.0286e+01, -8.6357e+01, -1.9015e+00, -3.8247e-04, -3.8528e+00,
         -1.1192e+00, -1.8933e+00, -8.7494e+01, -2.9665e+00, -7.2957e+00,
         -3.5662e+00, -2.5951e+00, -1.0733e+01],
        [-7.9185e+01, -8.3715e+01, -1.7120e+00,  7.5196e-04, -3.6493e+00,
         -1.4019e+00, -1.7662e+00, -8.8061e+01, -2.2659e+00, -6.2097e+00,
         -3.8959e+00, -2.7681e+00, -1.1595e+01],
        [ 1.3555e+00,  2.8232e+00,  9.8477e+00,  1.2823e+01,  7.2907e+00,
          5.7370e+00, -1.8937e+00,  2.0970e+00,  5.6820e+00, -4.6112e-01,
          5.6025e+00,  6.4482e+00,  5.8962e+00]], device='cuda:0'))])
xi:  [753.3744]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1236.0430635744895
W_T_median: 1084.8637203701123
W_T_pctile_5: 754.8816258832352
W_T_CVAR_5_pct: 381.54929131826196
Average q (qsum/M+1):  35.0
Optimal xi:  [753.3744]
Expected(across Rb) median(across samples) p_equity:  0.15045021420357993
obj fun:  tensor(-20161.9087, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
