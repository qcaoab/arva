Starting at: 
07-06-23_17:38

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = MC_everything
timeseries_basket['basket_desc'] = marc_test1_all_assets_longfactors
timeseries_basket['basket_columns'] = 
['Size_Lo30_real_ret', 'Value_Hi30_real_ret', 'Oprof_Hi30_real_ret', 'Inv_Lo30_real_ret', 'Mom_Hi30_real_ret', 'EP_Hi30_real_ret', 'Vol_Lo20_real_ret', 'Div_Hi30_real_ret', 'EQWFact_real_ret', 'T30_real_ret', 'T90_real_ret', 'B10_real_ret', 'VWD_real_ret', 'EWD_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = MC_everything
timeseries_basket['basket_desc'] = marc_test1_all_assets_longfactors
timeseries_basket['basket_columns'] = 
['Size_Lo30_nom_ret', 'Value_Hi30_nom_ret', 'Oprof_Hi30_nom_ret', 'Inv_Lo30_nom_ret', 'Mom_Hi30_nom_ret', 'EP_Hi30_nom_ret', 'Vol_Lo20_nom_ret', 'Div_Hi30_nom_ret', 'EQWFact_nom_ret', 'T30_nom_ret', 'T90_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'EWD_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.031411     0.013051
192608                    0.0319              0.0561  ...     0.028647     0.031002
192609                   -0.0173             -0.0071  ...     0.005787    -0.006499
192610                   -0.0294             -0.0355  ...    -0.028996    -0.034630
192611                   -0.0038              0.0294  ...     0.028554     0.024776

[5 rows x 15 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.036240    -0.011556
202209                   -0.0955             -0.0871  ...    -0.091324    -0.099903
202210                    0.0883              0.1486  ...     0.077403     0.049863
202211                   -0.0076              0.0462  ...     0.052365     0.028123
202212                   -0.0457             -0.0499  ...    -0.057116    -0.047241

[5 rows x 15 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'Oprof_Hi30_nom_ret_ind', 'Inv_Lo30_nom_ret_ind',
       'Mom_Hi30_nom_ret_ind', 'EP_Hi30_nom_ret_ind', 'Vol_Lo20_nom_ret_ind',
       'Div_Hi30_nom_ret_ind', 'EQWFact_nom_ret_ind', 'CPI_nom_ret_ind',
       'T30_nom_ret_ind', 'T90_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind', 'EWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
Oprof_Hi30_real_ret    0.003948
Inv_Lo30_real_ret      0.004513
Mom_Hi30_real_ret      0.011386
EP_Hi30_real_ret       0.007033
Vol_Lo20_real_ret      0.003529
Div_Hi30_real_ret      0.007888
EQWFact_real_ret       0.004508
T30_real_ret           0.000229
T90_real_ret           0.000501
B10_real_ret           0.001637
VWD_real_ret           0.006759
EWD_real_ret           0.009545
dtype: float64


timeseries_basket['data_df_stdev'] = 
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
Oprof_Hi30_real_ret    0.037444
Inv_Lo30_real_ret      0.037639
Mom_Hi30_real_ret      0.061421
EP_Hi30_real_ret       0.041390
Vol_Lo20_real_ret      0.030737
Div_Hi30_real_ret      0.056728
EQWFact_real_ret       0.037131
T30_real_ret           0.005227
T90_real_ret           0.005373
B10_real_ret           0.019258
VWD_real_ret           0.053610
EWD_real_ret           0.071360
dtype: float64


timeseries_basket['data_df_corr'] = 
                     Size_Lo30_real_ret  ...  EWD_real_ret
Size_Lo30_real_ret             1.000000  ...      0.977206
Value_Hi30_real_ret            0.908542  ...      0.919912
Oprof_Hi30_real_ret            0.433774  ...      0.462336
Inv_Lo30_real_ret              0.455237  ...      0.479661
Mom_Hi30_real_ret              0.903222  ...      0.912002
EP_Hi30_real_ret               0.476966  ...      0.506661
Vol_Lo20_real_ret              0.360014  ...      0.382411
Div_Hi30_real_ret              0.816292  ...      0.849068
EQWFact_real_ret               0.494572  ...      0.513359
T30_real_ret                   0.014412  ...      0.029084
T90_real_ret                   0.021968  ...      0.037909
B10_real_ret                   0.012916  ...      0.024853
VWD_real_ret                   0.865290  ...      0.907369
EWD_real_ret                   0.977206  ...      1.000000

[14 rows x 14 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      28  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      28  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 28)     True          28  
2     (28, 28)     True          28  
3      (28, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer      14       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      28  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      28  logistic_sigmoid   
3        obj.layers[3]        3  output_layer      14           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 28)     True          28  
2     (28, 28)     True          28  
3     (28, 14)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       28  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       28  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 28)      True          28  
0     (28, 28)      True          28  
0      (28, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       28  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       28  logistic_sigmoid   
0        obj.layers[3]         3  output_layer       14           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 28)      True          28  
0     (28, 28)      True          28  
0     (28, 14)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       28  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       28  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 28)      True          28  
0     (28, 28)      True          28  
0      (28, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       28  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       28  logistic_sigmoid   
0        obj.layers[3]         3  output_layer       14           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 28)      True          28  
0     (28, 28)      True          28  
0     (28, 14)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857
 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857
 0.07142857 0.07142857]
W_T_mean: 8576.841924009083
W_T_median: 5042.380921625826
W_T_pctile_5: 103.64169553244135
W_T_CVAR_5_pct: -371.9331258181762
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1820.2991148854744
Current xi:  [122.6073]
objective value function right now is: -1820.2991148854744
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1838.7003970224273
Current xi:  [146.4011]
objective value function right now is: -1838.7003970224273
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1863.2566464750748
Current xi:  [170.09319]
objective value function right now is: -1863.2566464750748
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1890.574661840748
Current xi:  [193.80289]
objective value function right now is: -1890.574661840748
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1905.3058359962336
Current xi:  [217.24875]
objective value function right now is: -1905.3058359962336
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1924.9046261309666
Current xi:  [240.83618]
objective value function right now is: -1924.9046261309666
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1949.7526293407097
Current xi:  [264.21182]
objective value function right now is: -1949.7526293407097
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1953.4278544325039
Current xi:  [287.00662]
objective value function right now is: -1953.4278544325039
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1967.3072826947528
Current xi:  [309.84314]
objective value function right now is: -1967.3072826947528
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1991.3322855152255
Current xi:  [332.43475]
objective value function right now is: -1991.3322855152255
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -2001.8607038402986
Current xi:  [355.5362]
objective value function right now is: -2001.8607038402986
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -2009.7516804207792
Current xi:  [378.06308]
objective value function right now is: -2009.7516804207792
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -2028.4267156557194
Current xi:  [400.16324]
objective value function right now is: -2028.4267156557194
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -2033.7951939407358
Current xi:  [422.2695]
objective value function right now is: -2033.7951939407358
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -2054.1920893591773
Current xi:  [444.64114]
objective value function right now is: -2054.1920893591773
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -2056.8072260114236
Current xi:  [466.76678]
objective value function right now is: -2056.8072260114236
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -2068.0166554369885
Current xi:  [489.0528]
objective value function right now is: -2068.0166554369885
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -2088.7125533079716
Current xi:  [510.9744]
objective value function right now is: -2088.7125533079716
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -2101.804443338683
Current xi:  [533.0806]
objective value function right now is: -2101.804443338683
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2108.196673373256
Current xi:  [554.8481]
objective value function right now is: -2108.196673373256
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -2117.9156733688183
Current xi:  [576.01605]
objective value function right now is: -2117.9156733688183
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -2118.8439530868886
Current xi:  [597.55096]
objective value function right now is: -2118.8439530868886
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2139.185698339891
Current xi:  [619.85895]
objective value function right now is: -2139.185698339891
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -2160.1458205815097
Current xi:  [641.1924]
objective value function right now is: -2160.1458205815097
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -2175.245364199723
Current xi:  [664.6545]
objective value function right now is: -2175.245364199723
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -2184.3995678489746
Current xi:  [686.96545]
objective value function right now is: -2184.3995678489746
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -2189.1818101563667
Current xi:  [708.7561]
objective value function right now is: -2189.1818101563667
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -2196.818054527147
Current xi:  [729.6252]
objective value function right now is: -2196.818054527147
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -2210.0371185490194
Current xi:  [750.56683]
objective value function right now is: -2210.0371185490194
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -2218.032076864658
Current xi:  [771.77246]
objective value function right now is: -2218.032076864658
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -2223.628165650637
Current xi:  [792.55]
objective value function right now is: -2223.628165650637
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -2229.6011293726715
Current xi:  [813.56683]
objective value function right now is: -2229.6011293726715
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -2236.5561439934413
Current xi:  [834.1796]
objective value function right now is: -2236.5561439934413
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -2244.445400550167
Current xi:  [854.4489]
objective value function right now is: -2244.445400550167
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -2260.5835405041003
Current xi:  [873.82697]
objective value function right now is: -2260.5835405041003
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2263.6133056555004
Current xi:  [877.8717]
objective value function right now is: -2263.6133056555004
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -2264.5778782871416
Current xi:  [882.06824]
objective value function right now is: -2264.5778782871416
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -2268.607922632157
Current xi:  [886.44183]
objective value function right now is: -2268.607922632157
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2269.132459594368
Current xi:  [890.67615]
objective value function right now is: -2269.132459594368
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -2270.3277302246656
Current xi:  [894.9132]
objective value function right now is: -2270.3277302246656
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -2272.8551115718324
Current xi:  [899.2058]
objective value function right now is: -2272.8551115718324
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [903.4746]
objective value function right now is: -2272.379861725113
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -2273.6461633966946
Current xi:  [907.7835]
objective value function right now is: -2273.6461633966946
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -2276.782737051112
Current xi:  [912.03595]
objective value function right now is: -2276.782737051112
new min fval from sgd:  -2279.0790427065913
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [916.3057]
objective value function right now is: -2279.0790427065913
new min fval from sgd:  -2279.116447871025
new min fval from sgd:  -2279.169480229199
new min fval from sgd:  -2279.240389535927
new min fval from sgd:  -2279.4444002328073
new min fval from sgd:  -2279.5165032710743
new min fval from sgd:  -2279.592306299704
new min fval from sgd:  -2279.7142168519713
new min fval from sgd:  -2279.721743087047
new min fval from sgd:  -2279.7723996448694
new min fval from sgd:  -2279.7730568514075
new min fval from sgd:  -2280.002113859291
new min fval from sgd:  -2280.0991084459683
new min fval from sgd:  -2280.162007042113
new min fval from sgd:  -2280.2293389674455
new min fval from sgd:  -2280.240455295632
new min fval from sgd:  -2280.2454931885636
new min fval from sgd:  -2280.292240129974
new min fval from sgd:  -2280.3199811793547
new min fval from sgd:  -2280.3339026404988
new min fval from sgd:  -2280.4397712331106
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [920.52765]
objective value function right now is: -2278.45372217572
new min fval from sgd:  -2280.5128947059857
new min fval from sgd:  -2280.540542867024
new min fval from sgd:  -2280.5901229034394
new min fval from sgd:  -2280.629527130883
new min fval from sgd:  -2280.7374989330765
new min fval from sgd:  -2280.8201004836073
new min fval from sgd:  -2280.838423442201
new min fval from sgd:  -2280.857479120604
new min fval from sgd:  -2280.909213067862
new min fval from sgd:  -2281.101155914849
new min fval from sgd:  -2281.1843683000325
new min fval from sgd:  -2281.266418576047
new min fval from sgd:  -2281.36455062016
new min fval from sgd:  -2281.4270064428883
new min fval from sgd:  -2281.5241167867202
new min fval from sgd:  -2281.649197020744
new min fval from sgd:  -2281.6699656349074
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [924.8012]
objective value function right now is: -2274.695951334773
new min fval from sgd:  -2281.771306237013
new min fval from sgd:  -2281.7768361937337
new min fval from sgd:  -2281.930081153835
new min fval from sgd:  -2282.128657854436
new min fval from sgd:  -2282.1294777858407
new min fval from sgd:  -2282.2451052772244
new min fval from sgd:  -2282.2536446740846
new min fval from sgd:  -2282.281250753598
new min fval from sgd:  -2282.3898053519115
new min fval from sgd:  -2282.4476434110597
new min fval from sgd:  -2282.4997972888264
new min fval from sgd:  -2282.57269443676
new min fval from sgd:  -2282.6217547508813
new min fval from sgd:  -2282.625797363837
new min fval from sgd:  -2282.6277383421493
new min fval from sgd:  -2282.888611829885
new min fval from sgd:  -2283.133146826253
new min fval from sgd:  -2283.1911752182496
new min fval from sgd:  -2283.2434771306644
new min fval from sgd:  -2283.2667339912377
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [929.18585]
objective value function right now is: -2282.715874776644
new min fval from sgd:  -2283.2670561749005
new min fval from sgd:  -2283.317863486348
new min fval from sgd:  -2283.5332410403653
new min fval from sgd:  -2283.623598823633
new min fval from sgd:  -2283.766878372427
new min fval from sgd:  -2283.9020354162803
new min fval from sgd:  -2284.1866431471913
new min fval from sgd:  -2284.220363408531
new min fval from sgd:  -2284.24238306678
new min fval from sgd:  -2284.2871817927203
new min fval from sgd:  -2284.3251430860264
new min fval from sgd:  -2284.3578590708403
new min fval from sgd:  -2284.389210835657
new min fval from sgd:  -2284.4231569135964
new min fval from sgd:  -2284.4416667038413
new min fval from sgd:  -2284.4598362687543
new min fval from sgd:  -2284.4660704947323
new min fval from sgd:  -2284.473590492157
new min fval from sgd:  -2284.4765952354983
new min fval from sgd:  -2284.484316694569
new min fval from sgd:  -2284.495110829338
new min fval from sgd:  -2284.498462328895
new min fval from sgd:  -2284.5019292453208
new min fval from sgd:  -2284.539098641853
new min fval from sgd:  -2284.5802531075556
new min fval from sgd:  -2284.619858424505
new min fval from sgd:  -2284.6509554376967
new min fval from sgd:  -2284.666715577105
new min fval from sgd:  -2284.6721655905963
new min fval from sgd:  -2284.68825093105
new min fval from sgd:  -2284.6970269735625
new min fval from sgd:  -2284.7059310372265
new min fval from sgd:  -2284.707534074555
new min fval from sgd:  -2284.7258999847445
new min fval from sgd:  -2284.749616530415
new min fval from sgd:  -2284.7707490965668
new min fval from sgd:  -2284.7904200378434
new min fval from sgd:  -2284.8012259303478
new min fval from sgd:  -2284.8185740589975
new min fval from sgd:  -2284.8408244935767
new min fval from sgd:  -2284.8521212949786
new min fval from sgd:  -2284.857933537383
new min fval from sgd:  -2284.862216720366
new min fval from sgd:  -2284.8725823715145
new min fval from sgd:  -2284.8827316258726
new min fval from sgd:  -2284.8907664483218
new min fval from sgd:  -2284.8949607907693
new min fval from sgd:  -2284.898641809362
new min fval from sgd:  -2284.902137887949
new min fval from sgd:  -2284.9090705434382
new min fval from sgd:  -2284.9109925092953
new min fval from sgd:  -2284.915426578562
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [931.7089]
objective value function right now is: -2284.291442260024
new min fval from sgd:  -2284.919750498519
new min fval from sgd:  -2284.9395335493723
new min fval from sgd:  -2284.9517463458924
new min fval from sgd:  -2284.958420899979
new min fval from sgd:  -2284.9687780561617
new min fval from sgd:  -2284.977102931388
new min fval from sgd:  -2284.996818472592
new min fval from sgd:  -2285.01173805127
new min fval from sgd:  -2285.015771670514
new min fval from sgd:  -2285.018395539095
new min fval from sgd:  -2285.0278496954325
new min fval from sgd:  -2285.039551826207
new min fval from sgd:  -2285.0486781156324
new min fval from sgd:  -2285.0608197417046
new min fval from sgd:  -2285.0691030682865
new min fval from sgd:  -2285.0757486764855
new min fval from sgd:  -2285.081899939928
new min fval from sgd:  -2285.087107225636
new min fval from sgd:  -2285.093877803936
new min fval from sgd:  -2285.0987658866366
new min fval from sgd:  -2285.1147891039705
new min fval from sgd:  -2285.1200337757587
new min fval from sgd:  -2285.1244569638957
new min fval from sgd:  -2285.1260129644083
new min fval from sgd:  -2285.130664873981
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [932.5708]
objective value function right now is: -2284.8919878253523
min fval:  -2285.130664873981
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -1.2589,   2.3219],
        [ -7.9272,   5.7826],
        [ -1.2601,   2.3184],
        [ -7.4502,   5.7256],
        [ -7.2956,   5.7072],
        [ -8.2394,   6.0275],
        [ -8.1638,   5.9487],
        [ -1.2601,   2.3184],
        [ -8.0175,   5.8795],
        [ -8.0884,   5.9180],
        [ -1.2600,   2.3188],
        [ -1.2601,   2.3184],
        [ -1.4092,   2.8135],
        [ -1.2601,   2.3184],
        [ -7.6706,   5.5844],
        [-20.2482,  -4.2956],
        [-10.8618,  -5.5567],
        [ -7.9178,   5.7840],
        [ -1.2601,   2.3184],
        [ -7.5359,   5.6885],
        [ -7.8080,   5.7893],
        [ -1.2601,   2.3184],
        [-40.9176,  -2.7905],
        [  6.0692,   5.8486],
        [ -7.6786,   5.7414],
        [ -8.0738,   5.9390],
        [ -7.9449,   5.8044],
        [ -1.2601,   2.3184]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-3.4608,  1.7386, -3.4618,  0.3799,  0.1831,  2.1056,  2.3387, -3.4618,
         1.6258,  1.8197, -3.4617, -3.4618, -3.0800, -3.4618,  3.0935, -1.4338,
        -1.9507,  3.0894, -3.4618,  4.0412,  3.5971, -3.4618, -2.1177, -7.6806,
         3.8374,  1.5628,  3.0921, -3.4618], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.3643e-03, -3.6172e-03, -2.3533e-03, -1.2442e-03, -1.2485e-03,
         -4.2353e-03, -6.2306e-03, -2.3533e-03, -2.9410e-03, -3.4539e-03,
         -2.3546e-03, -2.3534e-03, -3.3067e-03, -2.3534e-03, -3.0170e-02,
         -5.2059e-02, -1.8961e-01, -2.3045e-02, -2.3534e-03, -8.2321e-02,
         -4.4239e-02, -2.3533e-03,  6.0977e-03,  2.4817e-02, -6.1882e-02,
         -2.6356e-03, -2.2500e-02, -2.3534e-03],
        [-2.4824e-02,  2.7972e+00, -2.4511e-02,  1.4496e+00,  1.2438e+00,
          3.0769e+00,  3.3124e+00, -2.4510e-02,  2.7918e+00,  2.8373e+00,
         -2.4581e-02, -2.4510e-02, -2.1137e-02, -2.4511e-02,  3.6887e+00,
         -6.0927e+00, -6.4522e+00,  3.9846e+00, -2.4511e-02,  4.4600e+00,
          4.4126e+00, -2.4510e-02, -4.7823e+00, -1.1044e+01,  4.5702e+00,
          2.6300e+00,  4.0537e+00, -2.4510e-02],
        [ 1.4382e-01, -3.1561e+00,  1.4274e-01, -1.4022e+00, -1.1592e+00,
         -3.2708e+00, -3.4025e+00,  1.4274e-01, -2.6758e+00, -3.1089e+00,
          1.4286e-01,  1.4274e-01,  1.9607e-01,  1.4274e-01, -5.1707e+00,
          8.0385e+00,  8.3224e+00, -4.7489e+00,  1.4274e-01, -6.7281e+00,
         -5.6743e+00,  1.4274e-01,  6.2489e+00,  1.2300e+01, -6.2289e+00,
         -2.6088e+00, -4.8745e+00,  1.4274e-01],
        [ 2.1685e-01, -1.9367e+00,  2.1627e-01, -3.2734e-01, -1.6228e-01,
         -1.9222e+00, -2.2641e+00,  2.1627e-01, -1.5565e+00, -1.6171e+00,
          2.1632e-01,  2.1627e-01,  3.5646e-01,  2.1627e-01, -4.1464e+00,
          6.2815e+00,  6.2949e+00, -3.3395e+00,  2.1627e-01, -5.4054e+00,
         -4.2435e+00,  2.1627e-01,  4.3619e+00,  1.0344e+01, -4.5514e+00,
         -1.4712e+00, -3.4284e+00,  2.1627e-01],
        [ 1.0363e-01, -2.8192e+00,  1.0286e-01, -1.1194e+00, -8.9324e-01,
         -2.8288e+00, -3.2624e+00,  1.0286e-01, -2.4996e+00, -2.7675e+00,
          1.0291e-01,  1.0286e-01,  1.8523e-01,  1.0286e-01, -5.0704e+00,
          7.4874e+00,  7.7410e+00, -4.4797e+00,  1.0286e-01, -6.2634e+00,
         -5.2072e+00,  1.0286e-01,  5.7064e+00,  1.1137e+01, -5.7061e+00,
         -2.2839e+00, -4.3090e+00,  1.0286e-01],
        [-1.7643e-01, -2.4192e+00, -1.7574e-01, -7.7037e-01, -6.1502e-01,
         -2.6637e+00, -2.9394e+00, -1.7574e-01, -2.0944e+00, -2.3635e+00,
         -1.7583e-01, -1.7574e-01, -2.2686e-01, -1.7574e-01, -4.3739e+00,
          6.7784e+00,  7.0475e+00, -4.0229e+00, -1.7574e-01, -5.7605e+00,
         -4.9453e+00, -1.7574e-01,  5.0423e+00,  5.8037e+00, -5.0874e+00,
         -2.0275e+00, -4.1460e+00, -1.7574e-01],
        [-2.3644e-03, -3.6172e-03, -2.3534e-03, -1.2442e-03, -1.2485e-03,
         -4.2352e-03, -6.2305e-03, -2.3534e-03, -2.9409e-03, -3.4539e-03,
         -2.3547e-03, -2.3534e-03, -3.3067e-03, -2.3534e-03, -3.0169e-02,
         -5.2059e-02, -1.8961e-01, -2.3044e-02, -2.3534e-03, -8.2320e-02,
         -4.4238e-02, -2.3534e-03,  6.0977e-03,  2.4817e-02, -6.1881e-02,
         -2.6355e-03, -2.2500e-02, -2.3534e-03],
        [-6.1471e-02,  3.0405e+00, -6.1943e-02,  1.6226e+00,  1.2729e+00,
          2.8067e+00,  3.2837e+00, -6.1944e-02,  2.8952e+00,  2.8279e+00,
         -6.1938e-02, -6.1943e-02, -4.4170e-02, -6.1943e-02,  4.4653e+00,
         -6.6376e+00, -6.6977e+00,  4.0373e+00, -6.1944e-02,  5.1070e+00,
          4.4489e+00, -6.1944e-02, -4.8591e+00, -1.1588e+01,  5.0088e+00,
          2.6546e+00,  3.9267e+00, -6.1943e-02],
        [ 1.4218e-01, -2.1467e+00,  1.4176e-01, -5.7678e-01, -3.7195e-01,
         -2.2177e+00, -2.4926e+00,  1.4176e-01, -1.8145e+00, -1.9532e+00,
          1.4178e-01,  1.4176e-01,  2.5186e-01,  1.4176e-01, -4.4086e+00,
          6.6001e+00,  6.8181e+00, -3.7429e+00,  1.4176e-01, -5.7984e+00,
         -4.3805e+00,  1.4176e-01,  4.7581e+00,  9.9034e+00, -5.0562e+00,
         -1.7550e+00, -3.7489e+00,  1.4176e-01],
        [-1.2619e-01, -1.8704e+00, -1.2572e-01, -4.6858e-01, -3.0460e-01,
         -1.8937e+00, -2.4442e+00, -1.2572e-01, -1.6818e+00, -1.8529e+00,
         -1.2577e-01, -1.2572e-01, -1.4662e-01, -1.2572e-01, -4.2430e+00,
          6.2011e+00,  6.4123e+00, -3.5271e+00, -1.2572e-01, -5.4770e+00,
         -4.2597e+00, -1.2572e-01,  4.5230e+00,  6.4150e+00, -4.8561e+00,
         -1.5076e+00, -3.5961e+00, -1.2572e-01],
        [-2.3645e-03, -3.6158e-03, -2.3535e-03, -1.2440e-03, -1.2484e-03,
         -4.2336e-03, -6.2279e-03, -2.3535e-03, -2.9399e-03, -3.4526e-03,
         -2.3548e-03, -2.3535e-03, -3.3072e-03, -2.3535e-03, -3.0156e-02,
         -5.2057e-02, -1.8951e-01, -2.3034e-02, -2.3535e-03, -8.2285e-02,
         -4.4220e-02, -2.3535e-03,  6.0945e-03,  2.4802e-02, -6.1856e-02,
         -2.6346e-03, -2.2489e-02, -2.3535e-03],
        [-4.2293e-02, -3.7642e-02, -4.2073e-02, -2.8676e-02, -2.9865e-02,
         -4.5034e-02, -3.7444e-02, -4.2073e-02, -4.0147e-02, -4.1825e-02,
         -4.2098e-02, -4.2073e-02, -6.7333e-02, -4.2073e-02,  5.6504e-02,
          3.4469e-01,  5.5958e-01,  2.3792e-02, -4.2073e-02,  2.1562e-01,
          9.0767e-02, -4.2073e-02,  1.3634e-01, -7.4040e-01,  1.4982e-01,
         -4.0584e-02,  2.1369e-02, -4.2073e-02],
        [ 1.1956e-02,  2.6930e+00,  1.2962e-02,  1.3597e+00,  1.0700e+00,
          2.4961e+00,  2.9892e+00,  1.2961e-02,  2.5063e+00,  2.4455e+00,
          1.2825e-02,  1.2961e-02, -3.0598e-02,  1.2961e-02,  3.9957e+00,
         -5.9052e+00, -6.0587e+00,  3.6843e+00,  1.2962e-02,  4.4285e+00,
          3.9732e+00,  1.2962e-02, -4.5969e+00, -1.0423e+01,  4.2942e+00,
          2.2604e+00,  3.6176e+00,  1.2961e-02],
        [ 1.8671e-01, -2.5598e+00,  1.8556e-01, -8.6792e-01, -6.2690e-01,
         -2.3695e+00, -2.8245e+00,  1.8556e-01, -2.2178e+00, -2.3188e+00,
          1.8567e-01,  1.8556e-01,  3.1605e-01,  1.8556e-01, -4.6924e+00,
          7.0394e+00,  7.1090e+00, -4.1320e+00,  1.8556e-01, -6.1600e+00,
         -4.6494e+00,  1.8556e-01,  5.2943e+00,  1.1654e+01, -5.2304e+00,
         -2.0547e+00, -3.8391e+00,  1.8556e-01],
        [-3.4499e-02, -1.1736e+00, -3.4383e-02, -1.1256e-01, -6.4347e-02,
         -1.3214e+00, -1.7092e+00, -3.4383e-02, -9.5508e-01, -1.1307e+00,
         -3.4396e-02, -3.4383e-02, -5.3250e-02, -3.4383e-02, -3.4844e+00,
          5.3066e+00,  5.5767e+00, -3.0617e+00, -3.4383e-02, -5.0561e+00,
         -3.6474e+00, -3.4383e-02,  3.5784e+00,  1.5456e+00, -4.3296e+00,
         -7.9910e-01, -2.9723e+00, -3.4383e-02],
        [-1.5703e-01, -2.1770e+00, -1.5643e-01, -6.3053e-01, -4.7033e-01,
         -2.2951e+00, -2.5830e+00, -1.5643e-01, -1.9297e+00, -2.0671e+00,
         -1.5651e-01, -1.5643e-01, -1.8711e-01, -1.5643e-01, -4.3910e+00,
          6.5489e+00,  6.7673e+00, -3.9310e+00, -1.5643e-01, -5.7952e+00,
         -4.5333e+00, -1.5643e-01,  4.7765e+00,  6.6329e+00, -5.0798e+00,
         -1.6925e+00, -3.6625e+00, -1.5643e-01],
        [ 1.8875e-01, -2.9032e+00,  1.8743e-01, -1.0778e+00, -8.8490e-01,
         -2.7843e+00, -3.0572e+00,  1.8743e-01, -2.4640e+00, -2.6882e+00,
          1.8755e-01,  1.8743e-01,  2.9886e-01,  1.8743e-01, -4.8354e+00,
          7.3492e+00,  7.7179e+00, -4.2420e+00,  1.8743e-01, -6.4361e+00,
         -4.9289e+00,  1.8743e-01,  5.6595e+00,  1.1852e+01, -5.7446e+00,
         -2.2242e+00, -4.3778e+00,  1.8743e-01],
        [ 2.2633e-01, -1.5212e+00,  2.2581e-01, -7.2921e-02,  6.3025e-02,
         -1.4547e+00, -1.9105e+00,  2.2581e-01, -1.1572e+00, -1.3154e+00,
          2.2586e-01,  2.2581e-01,  3.5944e-01,  2.2581e-01, -3.7289e+00,
          5.6038e+00,  5.7260e+00, -3.0268e+00,  2.2581e-01, -5.0159e+00,
         -3.5418e+00,  2.2581e-01,  4.0176e+00,  9.8329e+00, -4.2329e+00,
         -1.0220e+00, -3.0362e+00,  2.2581e-01],
        [-2.3644e-03, -3.6172e-03, -2.3534e-03, -1.2442e-03, -1.2485e-03,
         -4.2352e-03, -6.2305e-03, -2.3534e-03, -2.9409e-03, -3.4539e-03,
         -2.3547e-03, -2.3534e-03, -3.3067e-03, -2.3534e-03, -3.0169e-02,
         -5.2059e-02, -1.8961e-01, -2.3044e-02, -2.3534e-03, -8.2320e-02,
         -4.4238e-02, -2.3534e-03,  6.0977e-03,  2.4817e-02, -6.1881e-02,
         -2.6355e-03, -2.2500e-02, -2.3534e-03],
        [ 1.4546e-02,  2.2761e+00,  1.5649e-02,  9.9131e-01,  7.6626e-01,
          2.3824e+00,  2.6839e+00,  1.5649e-02,  2.0453e+00,  2.2294e+00,
          1.5534e-02,  1.5649e-02, -7.9193e-02,  1.5649e-02,  3.6671e+00,
         -5.6637e+00, -5.7486e+00,  3.3527e+00,  1.5649e-02,  4.5342e+00,
          3.6527e+00,  1.5649e-02, -4.3405e+00, -9.7970e+00,  4.2530e+00,
          2.0082e+00,  3.2738e+00,  1.5649e-02],
        [ 1.4928e-01, -3.2066e+00,  1.4842e-01, -1.4050e+00, -1.2677e+00,
         -3.2300e+00, -3.6185e+00,  1.4842e-01, -2.7220e+00, -3.1130e+00,
          1.4851e-01,  1.4842e-01,  1.8612e-01,  1.4842e-01, -5.3366e+00,
          8.0613e+00,  8.2580e+00, -4.7524e+00,  1.4842e-01, -6.7638e+00,
         -5.5350e+00,  1.4842e-01,  6.2706e+00,  1.2314e+01, -6.0826e+00,
         -2.6983e+00, -4.8256e+00,  1.4842e-01],
        [-2.3644e-03, -3.6172e-03, -2.3534e-03, -1.2442e-03, -1.2485e-03,
         -4.2352e-03, -6.2305e-03, -2.3534e-03, -2.9409e-03, -3.4539e-03,
         -2.3547e-03, -2.3534e-03, -3.3068e-03, -2.3534e-03, -3.0169e-02,
         -5.2059e-02, -1.8961e-01, -2.3044e-02, -2.3534e-03, -8.2320e-02,
         -4.4238e-02, -2.3534e-03,  6.0977e-03,  2.4817e-02, -6.1881e-02,
         -2.6355e-03, -2.2500e-02, -2.3534e-03],
        [-2.3644e-03, -3.6172e-03, -2.3534e-03, -1.2442e-03, -1.2485e-03,
         -4.2352e-03, -6.2305e-03, -2.3534e-03, -2.9409e-03, -3.4539e-03,
         -2.3547e-03, -2.3534e-03, -3.3067e-03, -2.3534e-03, -3.0169e-02,
         -5.2059e-02, -1.8961e-01, -2.3044e-02, -2.3534e-03, -8.2320e-02,
         -4.4238e-02, -2.3534e-03,  6.0977e-03,  2.4817e-02, -6.1881e-02,
         -2.6355e-03, -2.2500e-02, -2.3534e-03],
        [-2.7720e-01, -3.7716e+00, -2.7484e-01, -1.8209e+00, -1.7823e+00,
         -3.9159e+00, -4.0444e+00, -2.7484e-01, -3.3326e+00, -3.5695e+00,
         -2.7510e-01, -2.7484e-01, -4.6059e-01, -2.7484e-01, -5.6597e+00,
          8.9551e+00,  8.9065e+00, -5.3542e+00, -2.7484e-01, -7.3352e+00,
         -6.3432e+00, -2.7484e-01,  6.9451e+00,  1.0734e+01, -6.6099e+00,
         -3.2336e+00, -5.3931e+00, -2.7484e-01],
        [-3.8304e-02, -2.9335e-02, -3.8096e-02, -2.5808e-02, -2.7119e-02,
         -3.5513e-02, -2.4405e-02, -3.8096e-02, -3.3445e-02, -3.3941e-02,
         -3.8120e-02, -3.8096e-02, -6.1277e-02, -3.8096e-02,  9.9716e-02,
          3.3026e-01,  5.1565e-01,  5.8784e-02, -3.8096e-02,  3.4477e-01,
          1.5717e-01, -3.8096e-02,  1.2560e-01, -7.2262e-01,  2.4478e-01,
         -3.4694e-02,  5.5723e-02, -3.8096e-02],
        [-2.3268e-02, -2.0265e+00, -2.3146e-02, -5.4993e-01, -3.8416e-01,
         -2.2013e+00, -2.5341e+00, -2.3146e-02, -1.7515e+00, -1.9550e+00,
         -2.3174e-02, -2.3146e-02,  2.0915e-02, -2.3146e-02, -4.2165e+00,
          6.3394e+00,  6.6579e+00, -3.6692e+00, -2.3146e-02, -5.6888e+00,
         -4.4096e+00, -2.3146e-02,  4.7333e+00,  8.4448e+00, -4.9778e+00,
         -1.6135e+00, -3.5370e+00, -2.3146e-02],
        [-2.3644e-03, -3.6172e-03, -2.3534e-03, -1.2442e-03, -1.2485e-03,
         -4.2352e-03, -6.2305e-03, -2.3534e-03, -2.9409e-03, -3.4539e-03,
         -2.3547e-03, -2.3534e-03, -3.3067e-03, -2.3534e-03, -3.0169e-02,
         -5.2059e-02, -1.8961e-01, -2.3044e-02, -2.3534e-03, -8.2320e-02,
         -4.4238e-02, -2.3534e-03,  6.0977e-03,  2.4817e-02, -6.1881e-02,
         -2.6355e-03, -2.2500e-02, -2.3534e-03],
        [-4.3081e-02, -3.9486e-02, -4.2859e-02, -2.9854e-02, -3.0963e-02,
         -4.7053e-02, -3.9163e-02, -4.2859e-02, -4.2098e-02, -4.3800e-02,
         -4.2884e-02, -4.2859e-02, -6.8398e-02, -4.2859e-02,  5.8999e-02,
          3.4509e-01,  5.6282e-01,  2.4809e-02, -4.2859e-02,  2.3189e-01,
          9.6506e-02, -4.2859e-02,  1.3539e-01, -7.5208e-01,  1.6021e-01,
         -4.2562e-02,  2.2274e-02, -4.2859e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.1568, -0.2021,  0.3294, -0.2700,  0.1896, -0.2194, -2.1568, -0.3290,
        -0.1200, -0.5013, -2.1576,  4.6776, -0.1770,  0.0984, -1.1455, -0.3098,
         0.2161, -0.5876, -2.1568, -0.2191,  0.3422, -2.1568, -2.1568,  0.3612,
         4.4663, -0.3035, -2.1568,  4.6092], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0188,  10.0789, -10.3800,  -5.3103,  -8.3438,  -5.7062,   0.0188,
          11.3413,  -5.9363,  -4.8227,   0.0188,   6.1484,   9.2900,  -7.5272,
          -3.4257,  -5.5096,  -8.6585,  -4.6174,   0.0188,   7.8606, -10.8727,
           0.0188,   0.0188, -12.4343,   5.7178,  -5.4928,   0.0188,   5.9402]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -8.9879,  -3.2507],
        [ -9.4773,  -3.2545],
        [ -1.7581,   0.2068],
        [  6.6339,  -0.2780],
        [ -1.7482,   0.2101],
        [  4.4671,  10.4790],
        [  8.3732,  -0.4640],
        [  9.7111,   9.2492],
        [ 13.2124,   1.4280],
        [ -4.6026,   1.7006],
        [ -1.7416,   0.2103],
        [ -1.7567,   0.2073],
        [ -1.7697,   0.2083],
        [ -2.2022,  -0.3026],
        [-11.8780,  -2.4998],
        [ -1.7434,   0.2081],
        [ -1.6813,   0.2064],
        [ -1.7434,   0.2096],
        [ -9.3592,  -3.2022],
        [-10.9250,  -3.0106],
        [ -2.1508,  11.3268],
        [-10.1142,  -2.1099],
        [  2.3051,  11.3658],
        [ -7.0808,   8.7942],
        [  7.8472,   7.7003],
        [ 11.1183,  -0.3484],
        [-12.0592,   6.4253],
        [-14.6353,   3.8262]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -0.3631,  -2.0191,  -4.8677, -11.2109,  -4.8595,   3.1883, -11.0535,
          5.3558,  -5.8367,  -9.2446,  -4.8578,  -4.8691,  -4.8627,  -5.3315,
         -3.8274,  -4.8625,  -4.8633,  -4.8628,  -1.5166,  -2.9522,   7.1869,
         -4.0480,   7.6832,   4.8308,  -4.8247, -10.5689,   0.9697,  -1.5212],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 6.6524e+00,  6.4472e+00,  2.8929e-01, -1.9866e+00,  2.8869e-01,
         -5.6439e+00, -3.0352e+00, -8.8682e+00, -6.8737e+00,  3.0804e-03,
          2.8556e-01,  2.9180e-01,  2.8993e-01,  1.2069e+00,  3.7936e+00,
          2.7917e-01,  2.4412e-01,  2.9283e-01,  6.8215e+00,  5.5187e+00,
         -2.1089e+01,  3.2798e+00, -1.9421e+01, -8.6197e+00,  1.8280e-02,
         -8.4573e+00, -1.0789e+00,  1.6994e+00],
        [-5.2833e-01,  4.3536e-01,  3.8305e-02,  9.1776e-01,  4.1995e-02,
          9.1368e-01,  1.3580e+00,  7.2043e-01, -1.5696e+00,  2.1865e-01,
          3.6613e-02,  4.8430e-02,  5.4834e-02,  1.1287e-02,  2.7251e-01,
          2.3090e-02,  4.0734e-02,  4.5979e-02,  4.2909e-02,  4.1834e-01,
          5.6897e-01,  2.3964e-01, -1.2668e+00, -4.1623e+00,  7.3560e-01,
          3.7359e+00,  4.3902e-01,  1.0560e+00],
        [-7.0821e-01, -5.1070e-01,  1.8491e-02, -5.4851e-03,  1.8476e-02,
         -4.5191e-01, -7.0292e-02, -1.3016e+00, -2.4898e+00, -7.0088e-03,
          1.8515e-02,  1.8478e-02,  1.8402e-02,  1.7115e-02, -5.1090e-02,
          1.8598e-02,  1.9165e-02,  1.8497e-02, -6.2416e-01, -2.5502e-01,
         -9.8812e-01, -3.1127e-02, -1.1346e+00, -3.5140e-01,  1.7729e-02,
         -7.8409e-01, -1.5683e-01, -1.5936e-01],
        [-7.1617e-01, -5.1171e-01,  1.8619e-02, -5.4895e-03,  1.8607e-02,
         -4.5196e-01, -7.1550e-02, -1.3088e+00, -2.4915e+00, -6.9781e-03,
          1.8647e-02,  1.8605e-02,  1.8529e-02,  1.7232e-02, -5.0648e-02,
          1.8730e-02,  1.9296e-02,  1.8627e-02, -6.2721e-01, -2.5439e-01,
         -9.9103e-01, -3.0845e-02, -1.1384e+00, -3.4906e-01,  1.9283e-02,
         -7.9197e-01, -1.5580e-01, -1.5863e-01],
        [ 1.3295e+00, -7.6176e-01,  2.0608e-01, -2.9379e-01,  1.7795e-01,
         -9.6395e-01, -1.1453e+00, -3.7945e-01, -1.0791e+01, -1.4391e-01,
          1.5861e-01,  2.1160e-01,  2.2375e-01, -7.4504e-01, -2.5507e+00,
          1.5984e-01,  1.0066e-01,  1.7399e-01, -4.6391e-02, -2.9692e+00,
          5.7071e+00, -1.7118e+00,  2.8811e+00,  1.0108e+01, -2.3423e+00,
         -9.4896e+00,  2.7343e-01, -3.7956e+00],
        [ 1.8745e+00,  1.0162e+00, -3.9384e-02, -2.4196e+00, -3.2591e-02,
         -1.1952e+00, -5.0014e+00,  1.0256e+00, -5.7426e+00, -3.0230e+00,
         -3.2808e-02, -3.4664e-02, -3.2606e-02,  6.4528e-02, -3.2219e-01,
         -4.3562e-02, -3.7412e-02, -3.1731e-02,  1.3793e+00,  4.3438e-01,
          5.8951e+00, -1.3687e-01,  4.6535e+00,  4.0507e+00, -2.4955e+00,
         -7.3959e+00,  1.2559e+00, -2.6435e+00],
        [-7.0032e-01, -5.1012e-01,  1.8358e-02, -5.4631e-03,  1.8340e-02,
         -4.5216e-01, -6.9066e-02, -1.2955e+00, -2.4890e+00, -7.0374e-03,
          1.8378e-02,  1.8346e-02,  1.8270e-02,  1.6992e-02, -5.1668e-02,
          1.8462e-02,  1.9030e-02,  1.8361e-02, -6.2151e-01, -2.5597e-01,
         -9.8552e-01, -3.1507e-02, -1.1298e+00, -3.5399e-01,  1.6136e-02,
         -7.7513e-01, -1.5795e-01, -1.6019e-01],
        [ 9.4708e+00,  5.3370e+00, -1.4221e-01, -6.6112e-01, -1.2913e-01,
         -1.9494e+00, -9.7362e-01, -1.7153e+00,  1.8027e+00, -1.3586e+00,
         -1.3004e-01, -1.3501e-01, -1.3156e-01,  5.5274e-02,  8.5356e-02,
         -1.3485e-01, -1.0933e-01, -1.3008e-01,  7.1980e+00,  4.2059e-01,
          2.3594e+00,  1.0447e-01,  2.4088e+00,  5.0161e+00, -1.4119e+00,
         -4.6466e+00, -2.8046e+00, -2.8863e+00],
        [-7.2785e-01, -5.1060e-01,  1.8631e-02, -5.2419e-03,  1.8630e-02,
         -4.4833e-01, -7.2496e-02, -1.3237e+00, -2.4877e+00, -6.9638e-03,
          1.8673e-02,  1.8616e-02,  1.8541e-02,  1.7275e-02, -4.8670e-02,
          1.8751e-02,  1.9311e-02,  1.8650e-02, -6.2912e-01, -2.5085e-01,
         -9.8744e-01, -2.9472e-02, -1.1420e+00, -3.4211e-01,  2.0953e-02,
         -7.9944e-01, -1.5364e-01, -1.5692e-01],
        [-7.3866e-01, -5.0768e-01,  1.8482e-02, -4.8414e-03,  1.8493e-02,
         -4.4230e-01, -7.2731e-02, -1.3406e+00, -2.4797e+00, -6.9713e-03,
          1.8541e-02,  1.8465e-02,  1.8391e-02,  1.7192e-02, -4.5981e-02,
          1.8612e-02,  1.9161e-02,  1.8513e-02, -6.2896e-01, -2.4579e-01,
         -9.7834e-01, -2.7605e-02, -1.1437e+00, -3.3318e-01,  2.2040e-02,
         -8.0412e-01, -1.5120e-01, -1.5489e-01],
        [ 4.4831e+00,  5.1969e+00,  1.7162e-02, -5.0147e-02,  2.0453e-02,
         -1.5412e+00, -6.5284e-01, -2.0224e+01, -1.0645e+01, -4.0697e-03,
          2.5668e-02,  1.5776e-02,  1.1938e-02,  3.3984e-01,  6.3400e+00,
          2.5003e-02,  4.6593e-02,  2.2935e-02,  4.8546e+00,  5.7984e+00,
         -1.8690e+01,  4.9006e+00, -2.7703e+01,  1.7413e-01, -2.5906e-03,
         -6.4523e+00,  1.8541e+00,  8.9874e-01],
        [-7.0154e-01, -5.1046e-01,  1.8394e-02, -5.4923e-03,  1.8376e-02,
         -4.5249e-01, -6.9348e-02, -1.2960e+00, -2.4897e+00, -7.0299e-03,
          1.8413e-02,  1.8382e-02,  1.8305e-02,  1.7022e-02, -5.1712e-02,
          1.8498e-02,  1.9066e-02,  1.8396e-02, -6.2217e-01, -2.5608e-01,
         -9.8673e-01, -3.1547e-02, -1.1308e+00, -3.5393e-01,  1.6464e-02,
         -7.7698e-01, -1.5785e-01, -1.6013e-01],
        [-7.0065e-01, -5.1008e-01,  1.8365e-02, -5.4686e-03,  1.8347e-02,
         -4.5210e-01, -6.9137e-02, -1.2955e+00, -2.4889e+00, -7.0364e-03,
          1.8385e-02,  1.8353e-02,  1.8277e-02,  1.6999e-02, -5.1618e-02,
          1.8469e-02,  1.9037e-02,  1.8368e-02, -6.2156e-01, -2.5588e-01,
         -9.8556e-01, -3.1473e-02, -1.1302e+00, -3.5381e-01,  1.6216e-02,
         -7.7567e-01, -1.5789e-01, -1.6014e-01],
        [ 5.0479e+00,  4.8665e+00,  1.6843e-01, -2.4138e+00,  1.3702e-01,
         -3.5398e+00, -3.3970e+00, -1.6231e+00, -2.4713e+00,  9.9972e-03,
          1.2134e-01,  1.7172e-01,  1.8035e-01,  2.3326e-01,  2.1037e+00,
          1.3530e-01,  8.5568e-02,  1.3350e-01,  5.2691e+00,  3.7185e+00,
         -1.6525e+01,  1.9165e+00, -1.4304e+01, -8.2664e+00, -4.3396e-01,
         -4.4429e+00, -4.9281e+00, -1.2905e+00],
        [-3.9186e-01, -2.7830e-02,  6.1881e-02, -4.8692e-02,  6.1264e-02,
         -2.5417e-01, -2.1853e-01, -1.4741e-01, -1.6489e+00, -3.8301e-03,
          6.1293e-02,  6.1610e-02,  6.1443e-02,  5.3026e-02, -1.4473e-01,
          6.2296e-02,  6.3073e-02,  6.1183e-02, -1.9933e-01, -2.7781e-01,
         -2.9608e+00, -1.0096e-01, -1.1257e+00, -5.7135e-01,  1.4935e-01,
         -2.9967e+00, -1.6308e-01, -1.5885e-01],
        [-7.2171e-01, -5.1029e-01,  1.8578e-02, -5.2987e-03,  1.8573e-02,
         -4.4909e-01, -7.1673e-02, -1.3167e+00, -2.4877e+00, -6.9818e-03,
          1.8615e-02,  1.8563e-02,  1.8488e-02,  1.7218e-02, -4.9280e-02,
          1.8694e-02,  1.9255e-02,  1.8593e-02, -6.2722e-01, -2.5186e-01,
         -9.8691e-01, -2.9881e-02, -1.1398e+00, -3.4474e-01,  1.9867e-02,
         -7.9448e-01, -1.5455e-01, -1.5759e-01],
        [-7.9706e-01, -4.7990e-01,  1.6089e-02, -2.1888e-03,  1.6156e-02,
         -3.9432e-01, -7.1451e-02, -1.4533e+00, -2.4078e+00, -7.0744e-03,
          1.6221e-02,  1.6066e-02,  1.6002e-02,  1.5459e-02, -2.9666e-02,
          1.6254e-02,  1.6739e-02,  1.6178e-02, -6.1519e-01, -2.0887e-01,
         -8.9725e-01, -1.7058e-02, -1.1405e+00, -2.7124e-01,  2.7299e-02,
         -8.2554e-01, -1.3648e-01, -1.4284e-01],
        [-7.1453e-01, -5.1009e-01,  1.8519e-02, -5.3805e-03,  1.8510e-02,
         -4.5011e-01, -7.0868e-02, -1.3086e+00, -2.4878e+00, -6.9999e-03,
          1.8551e-02,  1.8505e-02,  1.8430e-02,  1.7155e-02, -5.0043e-02,
          1.8631e-02,  1.9195e-02,  1.8530e-02, -6.2519e-01, -2.5315e-01,
         -9.8653e-01, -3.0397e-02, -1.1372e+00, -3.4779e-01,  1.8645e-02,
         -7.8886e-01, -1.5564e-01, -1.5841e-01],
        [-7.0057e-01, -5.1003e-01,  1.8360e-02, -5.4618e-03,  1.8342e-02,
         -4.5204e-01, -6.9094e-02, -1.2956e+00, -2.4888e+00, -7.0374e-03,
          1.8380e-02,  1.8348e-02,  1.8272e-02,  1.6995e-02, -5.1600e-02,
          1.8464e-02,  1.9032e-02,  1.8363e-02, -6.2149e-01, -2.5584e-01,
         -9.8540e-01, -3.1460e-02, -1.1301e+00, -3.5379e-01,  1.6185e-02,
         -7.7544e-01, -1.5789e-01, -1.6014e-01],
        [-7.0069e-01, -5.1040e-01,  1.8380e-02, -5.4923e-03,  1.8361e-02,
         -4.5251e-01, -6.9225e-02, -1.2953e+00, -2.4897e+00, -7.0328e-03,
          1.8399e-02,  1.8368e-02,  1.8292e-02,  1.7009e-02, -5.1776e-02,
          1.8484e-02,  1.9052e-02,  1.8382e-02, -6.2188e-01, -2.5618e-01,
         -9.8646e-01, -3.1589e-02, -1.1303e+00, -3.5421e-01,  1.6294e-02,
         -7.7604e-01, -1.5797e-01, -1.6022e-01],
        [-7.8534e-01, -5.0036e-01,  1.7832e-02, -3.5100e-03,  1.7879e-02,
         -4.2148e-01, -7.5249e-02, -1.4141e+00, -2.4511e+00, -6.9368e-03,
          1.7938e-02,  1.7809e-02,  1.7740e-02,  1.6794e-02, -3.7603e-02,
          1.7991e-02,  1.8511e-02,  1.7899e-02, -6.3348e-01, -2.2876e-01,
         -9.5455e-01, -2.2164e-02, -1.1502e+00, -3.0048e-01,  2.9470e-02,
         -8.3776e-01, -1.4293e-01, -1.4870e-01],
        [-7.2075e-01, -5.0989e-01,  1.8550e-02, -5.2835e-03,  1.8545e-02,
         -4.4873e-01, -7.1479e-02, -1.3160e+00, -2.4869e+00, -6.9879e-03,
          1.8587e-02,  1.8535e-02,  1.8460e-02,  1.7196e-02, -4.9195e-02,
          1.8666e-02,  1.9227e-02,  1.8565e-02, -6.2658e-01, -2.5167e-01,
         -9.8575e-01, -2.9813e-02, -1.1394e+00, -3.4464e-01,  1.9595e-02,
         -7.9340e-01, -1.5458e-01, -1.5758e-01],
        [-7.1620e-01, -5.0879e-01,  1.8625e-02, -5.5718e-03,  1.8618e-02,
         -4.4936e-01, -7.2283e-02, -1.3046e+00, -2.4840e+00, -6.9775e-03,
          1.8660e-02,  1.8610e-02,  1.8535e-02,  1.7250e-02, -4.9398e-02,
          1.8739e-02,  1.9301e-02,  1.8638e-02, -6.2470e-01, -2.5176e-01,
         -9.8699e-01, -2.9968e-02, -1.1427e+00, -3.4513e-01,  2.0163e-02,
         -7.9479e-01, -1.5540e-01, -1.5850e-01],
        [ 2.4128e+00,  3.1289e+00, -7.9878e-02,  6.4631e-02, -1.1382e-01,
          5.2773e-01, -1.2613e-01, -5.2695e+00, -1.2304e+00,  4.5961e-01,
         -1.2318e-01, -9.1728e-02, -9.3456e-02,  1.0271e+00,  4.4024e+00,
         -6.8056e-02, -8.9192e-02, -1.1985e-01,  2.8530e+00,  4.4888e+00,
         -2.6985e+00,  3.7086e+00, -4.0244e+00,  4.7082e+00, -1.6876e-02,
         -3.2725e+00,  3.3178e+00,  2.5940e+00],
        [-2.0069e+00, -5.6551e-01,  3.5154e-02, -1.6111e-01,  4.2114e-02,
          8.2278e-01, -1.4687e+00,  7.3412e-01, -1.4118e-01, -2.6964e-03,
          4.7490e-02,  3.1667e-02,  2.7915e-02,  7.5755e-02,  1.1538e-01,
          3.4304e-02,  7.3008e-02,  4.2322e-02, -1.1925e+00,  6.7338e-01,
         -4.3780e+00,  1.6324e-01, -1.0170e+00, -1.4688e+00, -1.0813e+00,
         -4.5203e+00, -1.1265e-01, -1.1155e-01],
        [-1.3553e+00, -4.4886e-01, -1.0320e-02,  1.1499e-03, -1.0296e-02,
          1.6361e-01, -8.5054e-02, -1.8747e+00, -1.9585e+00,  1.5983e-02,
         -1.0294e-02, -1.0352e-02, -1.0344e-02, -1.5547e-02, -3.8960e-02,
         -1.0179e-02, -1.0157e-02, -1.0296e-02, -7.1831e-01, -9.9440e-02,
         -3.6164e-02, -2.8379e-02, -9.6288e-01,  8.8324e-01,  7.9527e-01,
         -9.8621e-01,  2.0357e-01,  6.9138e-02],
        [-7.5323e-01, -5.0034e-01,  1.7968e-02, -4.0703e-03,  1.7999e-02,
         -4.2982e-01, -7.2084e-02, -1.3680e+00, -2.4613e+00, -7.0133e-03,
          1.8054e-02,  1.7948e-02,  1.7878e-02,  1.6837e-02, -4.1110e-02,
          1.8113e-02,  1.8643e-02,  1.8020e-02, -6.2523e-01, -2.3595e-01,
         -9.5644e-01, -2.4270e-02, -1.1435e+00, -3.1706e-01,  2.2811e-02,
         -8.0705e-01, -1.4715e-01, -1.5144e-01],
        [-7.0218e-01, -5.1037e-01,  1.8394e-02, -5.4746e-03,  1.8376e-02,
         -4.5224e-01, -6.9375e-02, -1.2969e+00, -2.4895e+00, -7.0296e-03,
          1.8414e-02,  1.8382e-02,  1.8305e-02,  1.7024e-02, -5.1583e-02,
          1.8498e-02,  1.9066e-02,  1.8397e-02, -6.2224e-01, -2.5585e-01,
         -9.8641e-01, -3.1455e-02, -1.1310e+00, -3.5352e-01,  1.6520e-02,
         -7.7739e-01, -1.5770e-01, -1.6001e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 4.9479, -2.6834, -3.5807, -3.5713, -5.3174, -4.2947, -3.5892,  2.3018,
        -3.5673, -3.5702,  2.6683, -3.5869, -3.5890,  3.5006, -4.3777, -3.5724,
        -3.6206, -3.5782, -3.5893, -3.5879, -3.5647, -3.5745, -3.5831, -1.1766,
        -4.0810, -4.6712, -3.5855, -3.5868], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.7225e-04, -4.7302e+00, -7.2495e-03, -7.2206e-03, -5.5154e-03,
         -1.5731e-01, -7.2752e-03, -1.3716e+01, -7.1419e-03, -7.0433e-03,
         -1.9885e-06, -7.2746e-03, -7.2742e-03, -6.3986e-01, -5.8635e-03,
         -7.1750e-03, -6.4013e-03, -7.2111e-03, -7.2740e-03, -7.2777e-03,
         -6.6727e-03, -7.1747e-03, -7.1988e-03, -9.4657e-03, -4.3361e-02,
         -2.6970e-03, -6.8718e-03, -7.2700e-03],
        [ 1.1776e-02,  2.8204e+00, -3.3210e-03, -3.5655e-03,  1.0946e-02,
         -1.3909e+00, -3.0865e-03, -6.2032e+00, -3.9630e-03, -4.3578e-03,
         -2.0578e-03, -3.1193e-03, -3.0955e-03,  5.0150e-01, -1.6047e-02,
         -3.7600e-03, -6.4884e-03, -3.5296e-03, -3.0936e-03, -3.0928e-03,
         -5.9828e-03, -3.7357e-03, -3.5780e-03, -1.1293e-01,  3.2097e-02,
         -2.0884e-02, -4.9176e-03, -3.1418e-03],
        [-2.6682e-04, -4.7261e+00, -1.2682e-02, -1.2647e-02, -5.2342e-03,
         -1.5143e-01, -1.2714e-02, -1.3107e+01, -1.2549e-02, -1.2426e-02,
         -8.1424e-07, -1.2713e-02, -1.2713e-02, -6.5149e-01, -1.0849e-02,
         -1.2591e-02, -1.1564e-02, -1.2636e-02, -1.2712e-02, -1.2717e-02,
         -1.1935e-02, -1.2591e-02, -1.2623e-02, -9.1403e-03, -4.6298e-02,
         -4.3745e-03, -1.2208e-02, -1.2707e-02],
        [-1.0741e-03, -3.5031e+00, -1.0410e-02, -1.0362e-02, -9.8382e-03,
         -2.9965e-01, -1.0453e-02, -1.1608e+01, -1.0235e-02, -1.0077e-02,
         -7.8223e-06, -1.0452e-02, -1.0451e-02, -5.3764e-01, -7.8775e-03,
         -1.0289e-02, -9.0174e-03, -1.0348e-02, -1.0451e-02, -1.0457e-02,
         -9.4632e-03, -1.0289e-02, -1.0329e-02, -1.7679e-02, -5.6463e-02,
         -2.8648e-03, -9.7995e-03, -1.0444e-02],
        [ 1.0858e+01,  1.6070e+00,  5.1184e-01,  5.0084e-01,  2.7133e-01,
         -4.4227e-01,  5.1597e-01,  1.8996e+00,  4.9322e-01,  4.8576e-01,
         -1.3684e+01,  5.1542e-01,  5.1602e-01,  2.8424e+00,  1.6574e+00,
          5.0335e-01,  4.2447e-01,  5.0674e-01,  5.1687e-01,  5.1552e-01,
          5.1215e-01,  5.0057e-01,  5.3828e-01,  5.3550e-01,  3.0016e+00,
         -4.7032e-01,  4.7154e-01,  5.1364e-01],
        [ 1.8905e-03, -1.0890e+00, -3.2818e-02, -3.2764e-02, -3.0489e-02,
         -6.9819e-01, -3.2864e-02, -9.2921e+00, -3.2585e-02, -3.2343e-02,
          6.2057e-05, -3.2866e-02, -3.2862e-02, -5.1291e-01, -2.8333e-02,
         -3.2657e-02, -3.0560e-02, -3.2734e-02, -3.2861e-02, -3.2871e-02,
         -3.1381e-02, -3.2654e-02, -3.2712e-02, -3.4221e-02, -8.5595e-02,
         -7.8910e-03, -3.1897e-02, -3.2855e-02],
        [-1.0055e-03, -4.4628e+00, -1.0804e-02, -1.0767e-02, -3.4164e-03,
         -1.4634e-01, -1.0837e-02, -1.2341e+01, -1.0667e-02, -1.0543e-02,
         -3.1168e-06, -1.0836e-02, -1.0835e-02, -6.0204e-01, -8.3225e-03,
         -1.0710e-02, -9.7008e-03, -1.0755e-02, -1.0835e-02, -1.0840e-02,
         -1.0057e-02, -1.0709e-02, -1.0740e-02, -8.9301e-03, -4.6558e-02,
         -3.0136e-03, -1.0324e-02, -1.0830e-02],
        [ 1.1331e+00, -9.9980e-01, -2.3153e-01, -2.3354e-01, -4.4486e-01,
          4.6936e-01, -2.3706e-01,  8.3293e-01, -2.2409e-01, -2.1273e-01,
         -6.9304e+00, -2.3653e-01, -2.3643e-01,  3.0206e+00,  3.3736e-01,
         -2.2170e-01, -1.5366e-01, -2.2720e-01, -2.3566e-01, -2.3745e-01,
         -1.1466e-01, -2.2487e-01, -1.9051e-01,  1.5202e+00,  1.2368e-01,
          1.3887e+00, -1.9633e-01, -2.3726e-01],
        [-7.0339e-04, -4.2268e+00, -8.0488e-03, -8.0121e-03, -5.6664e-03,
         -1.8673e-01, -8.0817e-03, -1.2541e+01, -7.9139e-03, -7.7913e-03,
         -1.7806e-06, -8.0807e-03, -8.0804e-03, -6.3540e-01, -6.0981e-03,
         -7.9555e-03, -6.9910e-03, -8.0008e-03, -8.0801e-03, -8.0847e-03,
         -7.3260e-03, -7.9553e-03, -7.9854e-03, -8.9296e-03, -4.6651e-02,
         -3.2703e-03, -7.5783e-03, -8.0750e-03],
        [-2.6344e-03, -3.0366e+00, -8.5247e-03, -8.5421e-03, -3.2832e-03,
         -1.8748e-01, -8.5084e-03, -1.1229e+01, -8.5761e-03, -8.6140e-03,
         -2.0693e-05, -8.5102e-03, -8.5090e-03, -5.8076e-01, -9.6975e-03,
         -8.5600e-03, -8.8397e-03, -8.5419e-03, -8.5089e-03, -8.5083e-03,
         -8.7648e-03, -8.5588e-03, -8.5455e-03, -1.1214e-02, -2.0350e-02,
         -5.3512e-03, -8.6741e-03, -8.5121e-03],
        [-3.6602e+00, -1.6116e+00,  3.2895e-01,  3.2105e-01,  3.9191e+00,
          4.5244e+00,  3.3122e-01, -1.8795e+00,  3.1731e-01,  3.1417e-01,
          3.1892e+01,  3.3089e-01,  3.3134e-01, -1.1916e+01, -1.4724e-01,
          3.2454e-01,  2.8507e-01,  3.2603e-01,  3.3203e-01,  3.3081e-01,
          3.4905e-01,  3.2213e-01,  3.5490e-01,  2.2958e+00, -5.0473e+00,
         -1.4700e-01,  3.0767e-01,  3.2950e-01],
        [-1.5533e+01,  2.6576e+00, -6.3027e-01, -6.4453e-01, -4.5715e+00,
         -2.9999e-01, -6.2402e-01,  1.1930e+00, -6.4589e-01, -6.3957e-01,
         -7.7196e-02, -6.2608e-01, -6.2392e-01,  1.1009e+00, -2.3807e+00,
         -6.3525e-01, -5.9130e-01, -6.3206e-01, -6.2284e-01, -6.2575e-01,
         -5.8169e-01, -6.3640e-01, -6.0337e-01,  2.2720e+00, -2.6985e+00,
         -3.2637e-01, -6.2229e-01, -6.2721e-01],
        [-4.2601e-04, -4.9192e+00, -8.2968e-03, -8.2581e-03, -5.2763e-03,
         -1.4632e-01, -8.3316e-03, -1.3644e+01, -8.1567e-03, -8.0316e-03,
         -1.0497e-06, -8.3303e-03, -8.3302e-03, -6.1828e-01, -7.0322e-03,
         -8.1998e-03, -7.2494e-03, -8.2471e-03, -8.3299e-03, -8.3345e-03,
         -7.5663e-03, -8.1999e-03, -8.2329e-03, -9.2990e-03, -4.3846e-02,
         -3.2851e-03, -7.8178e-03, -8.3244e-03],
        [-8.1296e-05, -4.5833e+00, -5.7686e-03, -5.7418e-03, -6.4546e-03,
         -1.7133e-01, -5.7925e-03, -1.3448e+01, -5.6702e-03, -5.5813e-03,
         -2.8734e-06, -5.7919e-03, -5.7916e-03, -6.1030e-01, -4.8949e-03,
         -5.7004e-03, -5.0178e-03, -5.7335e-03, -5.7914e-03, -5.7948e-03,
         -5.2515e-03, -5.7003e-03, -5.7232e-03, -9.8831e-03, -4.2481e-02,
         -2.4867e-03, -5.4284e-03, -5.7876e-03]], device='cuda:0'))])
xi:  [932.49854]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1255.5989130155174
W_T_median: 1240.9205250682508
W_T_pctile_5: 941.4049110902541
W_T_CVAR_5_pct: 630.8213419873746
Average q (qsum/M+1):  53.43944524949597
Optimal xi:  [932.49854]
Expected(across Rb) median(across samples) p_equity:  7.16369669390815e-05
obj fun:  tensor(-2285.1307, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: MC_everything
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
