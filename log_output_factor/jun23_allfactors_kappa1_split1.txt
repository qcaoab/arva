Starting at: 
23-06-23_18:16

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = MC_everything
timeseries_basket['basket_desc'] = marc_test1_all_assets_longfactors
timeseries_basket['basket_columns'] = 
['Size_Lo30_real_ret', 'Value_Hi30_real_ret', 'Oprof_Hi30_real_ret', 'Inv_Lo30_real_ret', 'Mom_Hi30_real_ret', 'EP_Hi30_real_ret', 'Vol_Lo20_real_ret', 'Div_Hi30_real_ret', 'EQWFact_real_ret', 'T30_real_ret', 'T90_real_ret', 'B10_real_ret', 'VWD_real_ret', 'EWD_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = MC_everything
timeseries_basket['basket_desc'] = marc_test1_all_assets_longfactors
timeseries_basket['basket_columns'] = 
['Size_Lo30_nom_ret', 'Value_Hi30_nom_ret', 'Oprof_Hi30_nom_ret', 'Inv_Lo30_nom_ret', 'Mom_Hi30_nom_ret', 'EP_Hi30_nom_ret', 'Vol_Lo20_nom_ret', 'Div_Hi30_nom_ret', 'EQWFact_nom_ret', 'T30_nom_ret', 'T90_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'EWD_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.031411     0.013051
192608                    0.0319              0.0561  ...     0.028647     0.031002
192609                   -0.0173             -0.0071  ...     0.005787    -0.006499
192610                   -0.0294             -0.0355  ...    -0.028996    -0.034630
192611                   -0.0038              0.0294  ...     0.028554     0.024776

[5 rows x 15 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.036240    -0.011556
202209                   -0.0955             -0.0871  ...    -0.091324    -0.099903
202210                    0.0883              0.1486  ...     0.077403     0.049863
202211                   -0.0076              0.0462  ...     0.052365     0.028123
202212                   -0.0457             -0.0499  ...    -0.057116    -0.047241

[5 rows x 15 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'Oprof_Hi30_nom_ret_ind', 'Inv_Lo30_nom_ret_ind',
       'Mom_Hi30_nom_ret_ind', 'EP_Hi30_nom_ret_ind', 'Vol_Lo20_nom_ret_ind',
       'Div_Hi30_nom_ret_ind', 'EQWFact_nom_ret_ind', 'CPI_nom_ret_ind',
       'T30_nom_ret_ind', 'T90_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind', 'EWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
Oprof_Hi30_real_ret    0.003948
Inv_Lo30_real_ret      0.004513
Mom_Hi30_real_ret      0.011386
EP_Hi30_real_ret       0.007033
Vol_Lo20_real_ret      0.003529
Div_Hi30_real_ret      0.007888
EQWFact_real_ret       0.004508
T30_real_ret           0.000229
T90_real_ret           0.000501
B10_real_ret           0.001637
VWD_real_ret           0.006759
EWD_real_ret           0.009545
dtype: float64


timeseries_basket['data_df_stdev'] = 
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
Oprof_Hi30_real_ret    0.037444
Inv_Lo30_real_ret      0.037639
Mom_Hi30_real_ret      0.061421
EP_Hi30_real_ret       0.041390
Vol_Lo20_real_ret      0.030737
Div_Hi30_real_ret      0.056728
EQWFact_real_ret       0.037131
T30_real_ret           0.005227
T90_real_ret           0.005373
B10_real_ret           0.019258
VWD_real_ret           0.053610
EWD_real_ret           0.071360
dtype: float64


timeseries_basket['data_df_corr'] = 
                     Size_Lo30_real_ret  ...  EWD_real_ret
Size_Lo30_real_ret             1.000000  ...      0.977206
Value_Hi30_real_ret            0.908542  ...      0.919912
Oprof_Hi30_real_ret            0.433774  ...      0.462336
Inv_Lo30_real_ret              0.455237  ...      0.479661
Mom_Hi30_real_ret              0.903222  ...      0.912002
EP_Hi30_real_ret               0.476966  ...      0.506661
Vol_Lo20_real_ret              0.360014  ...      0.382411
Div_Hi30_real_ret              0.816292  ...      0.849068
EQWFact_real_ret               0.494572  ...      0.513359
T30_real_ret                   0.014412  ...      0.029084
T90_real_ret                   0.021968  ...      0.037909
B10_real_ret                   0.012916  ...      0.024853
VWD_real_ret                   0.865290  ...      0.907369
EWD_real_ret                   0.977206  ...      1.000000

[14 rows x 14 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 199201
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      22  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      22  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 22)     True          22  
2     (22, 22)     True          22  
3      (22, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer      14       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      22  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      22  logistic_sigmoid   
3        obj.layers[3]        3  output_layer      14           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 22)     True          22  
2     (22, 22)     True          22  
3     (22, 14)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0      (22, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer       14           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0     (22, 14)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0      (22, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer       14           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0     (22, 14)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857
 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857
 0.07142857 0.07142857]
W_T_mean: 9438.548856964324
W_T_median: 5685.968426194605
W_T_pctile_5: 125.54011840495605
W_T_CVAR_5_pct: -416.7378752066434
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1855.0257049244651
Current xi:  [121.94486]
objective value function right now is: -1855.0257049244651
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1864.9747467948232
Current xi:  [146.3883]
objective value function right now is: -1864.9747467948232
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1887.9705691335844
Current xi:  [170.17778]
objective value function right now is: -1887.9705691335844
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1897.3490621548901
Current xi:  [193.72066]
objective value function right now is: -1897.3490621548901
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1935.184728494501
Current xi:  [216.73189]
objective value function right now is: -1935.184728494501
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1962.5073240498118
Current xi:  [240.25879]
objective value function right now is: -1962.5073240498118
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1982.0852113481465
Current xi:  [263.60205]
objective value function right now is: -1982.0852113481465
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [286.76477]
objective value function right now is: -1831.562402887207
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -2011.4492273985497
Current xi:  [309.11023]
objective value function right now is: -2011.4492273985497
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -2021.5582423041076
Current xi:  [332.14407]
objective value function right now is: -2021.5582423041076
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -2040.9546540156537
Current xi:  [355.01926]
objective value function right now is: -2040.9546540156537
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [377.54266]
objective value function right now is: -2034.3621036920993
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [399.03192]
objective value function right now is: -2031.9646024367028
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -2048.700061330214
Current xi:  [420.41968]
objective value function right now is: -2048.700061330214
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [442.66095]
objective value function right now is: -2048.2945003714826
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -2075.11271496303
Current xi:  [464.72372]
objective value function right now is: -2075.11271496303
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -2088.6373824315633
Current xi:  [486.29105]
objective value function right now is: -2088.6373824315633
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -2093.3294960479943
Current xi:  [508.37592]
objective value function right now is: -2093.3294960479943
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -2108.2739942024887
Current xi:  [529.9515]
objective value function right now is: -2108.2739942024887
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2113.5210709043527
Current xi:  [551.0987]
objective value function right now is: -2113.5210709043527
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -2135.25499535244
Current xi:  [572.6655]
objective value function right now is: -2135.25499535244
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -2146.9968261972017
Current xi:  [593.4135]
objective value function right now is: -2146.9968261972017
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2211.025564411329
Current xi:  [616.31256]
objective value function right now is: -2211.025564411329
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [637.17346]
objective value function right now is: -2147.693343353424
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [656.4318]
objective value function right now is: -2167.2776787498055
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [676.3476]
objective value function right now is: -2185.838901254766
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [696.1445]
objective value function right now is: -2183.7259719090725
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -2260.9096259843373
Current xi:  [719.4371]
objective value function right now is: -2260.9096259843373
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -2277.9162822598305
Current xi:  [742.868]
objective value function right now is: -2277.9162822598305
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [765.45874]
objective value function right now is: -2262.2643013252
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -2288.7734762144687
Current xi:  [787.11145]
objective value function right now is: -2288.7734762144687
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -2289.059116228154
Current xi:  [808.9293]
objective value function right now is: -2289.059116228154
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -2315.8997369491562
Current xi:  [829.7861]
objective value function right now is: -2315.8997369491562
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -2322.3322743742497
Current xi:  [850.9695]
objective value function right now is: -2322.3322743742497
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -2329.5659321728067
Current xi:  [872.35394]
objective value function right now is: -2329.5659321728067
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2347.1785362085575
Current xi:  [876.7216]
objective value function right now is: -2347.1785362085575
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -2351.869597451946
Current xi:  [881.07526]
objective value function right now is: -2351.869597451946
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -2352.140071997256
Current xi:  [885.4848]
objective value function right now is: -2352.140071997256
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2355.021753646556
Current xi:  [889.99396]
objective value function right now is: -2355.021753646556
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -2356.5888345071066
Current xi:  [894.50586]
objective value function right now is: -2356.5888345071066
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -2359.1801053391746
Current xi:  [898.8878]
objective value function right now is: -2359.1801053391746
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -2360.773119689467
Current xi:  [903.41595]
objective value function right now is: -2360.773119689467
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [907.9525]
objective value function right now is: -2357.7964349984154
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -2366.269553390396
Current xi:  [912.3194]
objective value function right now is: -2366.269553390396
new min fval from sgd:  -2366.9238942040215
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [916.7916]
objective value function right now is: -2366.9238942040215
new min fval from sgd:  -2366.9684008545482
new min fval from sgd:  -2367.3276326884743
new min fval from sgd:  -2367.5104588213544
new min fval from sgd:  -2367.7145259828594
new min fval from sgd:  -2368.029056800026
new min fval from sgd:  -2368.0981886432396
new min fval from sgd:  -2368.290681673702
new min fval from sgd:  -2368.5040496487272
new min fval from sgd:  -2368.6040744738707
new min fval from sgd:  -2368.9224433164627
new min fval from sgd:  -2369.0112899074984
new min fval from sgd:  -2369.081296034719
new min fval from sgd:  -2369.341319980267
new min fval from sgd:  -2369.409950706821
new min fval from sgd:  -2369.556743109312
new min fval from sgd:  -2369.573495823767
new min fval from sgd:  -2369.7728302790088
new min fval from sgd:  -2369.8391631373756
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [921.2797]
objective value function right now is: -2368.8835334079845
new min fval from sgd:  -2369.8600586835882
new min fval from sgd:  -2370.14633869217
new min fval from sgd:  -2370.2495745398314
new min fval from sgd:  -2370.2604090286754
new min fval from sgd:  -2370.277840697308
new min fval from sgd:  -2370.426205409341
new min fval from sgd:  -2370.7229587000043
new min fval from sgd:  -2371.220265521757
new min fval from sgd:  -2371.259759706183
new min fval from sgd:  -2371.3314286861
new min fval from sgd:  -2371.4380218546507
new min fval from sgd:  -2371.586884605864
new min fval from sgd:  -2371.88171443033
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [925.7644]
objective value function right now is: -2368.1546987574065
new min fval from sgd:  -2371.8896280538647
new min fval from sgd:  -2372.23251864184
new min fval from sgd:  -2372.3310926834174
new min fval from sgd:  -2372.331853339453
new min fval from sgd:  -2372.347690176088
new min fval from sgd:  -2372.5401605337793
new min fval from sgd:  -2372.7321450653103
new min fval from sgd:  -2373.153401015184
new min fval from sgd:  -2373.358276906888
new min fval from sgd:  -2373.454515901395
new min fval from sgd:  -2373.5585990081863
new min fval from sgd:  -2373.6194479086043
new min fval from sgd:  -2373.6823002775823
new min fval from sgd:  -2373.8520471755637
new min fval from sgd:  -2373.9378295900488
new min fval from sgd:  -2373.9450115915497
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [930.1567]
objective value function right now is: -2372.4212663294675
new min fval from sgd:  -2373.960788283321
new min fval from sgd:  -2373.9887522500158
new min fval from sgd:  -2374.0168044982993
new min fval from sgd:  -2374.0531319682104
new min fval from sgd:  -2374.079605748404
new min fval from sgd:  -2374.1050055301125
new min fval from sgd:  -2374.1301412317966
new min fval from sgd:  -2374.139816539026
new min fval from sgd:  -2374.149952535726
new min fval from sgd:  -2374.16528472517
new min fval from sgd:  -2374.1762576863075
new min fval from sgd:  -2374.18229662969
new min fval from sgd:  -2374.196710742932
new min fval from sgd:  -2374.2017575598693
new min fval from sgd:  -2374.2197898343047
new min fval from sgd:  -2374.249078396818
new min fval from sgd:  -2374.2782761529866
new min fval from sgd:  -2374.3077219582233
new min fval from sgd:  -2374.330077129181
new min fval from sgd:  -2374.375262173145
new min fval from sgd:  -2374.399829461304
new min fval from sgd:  -2374.4155372623013
new min fval from sgd:  -2374.417642630296
new min fval from sgd:  -2374.441195418236
new min fval from sgd:  -2374.465104967854
new min fval from sgd:  -2374.500513167787
new min fval from sgd:  -2374.537325706472
new min fval from sgd:  -2374.5495745058133
new min fval from sgd:  -2374.5640432206906
new min fval from sgd:  -2374.5651813718214
new min fval from sgd:  -2374.586622962768
new min fval from sgd:  -2374.612587726608
new min fval from sgd:  -2374.644854499211
new min fval from sgd:  -2374.661407316028
new min fval from sgd:  -2374.6715781210055
new min fval from sgd:  -2374.6741324543073
new min fval from sgd:  -2374.731971183989
new min fval from sgd:  -2374.799971861765
new min fval from sgd:  -2374.8486730848617
new min fval from sgd:  -2374.8760007028095
new min fval from sgd:  -2374.891658341988
new min fval from sgd:  -2374.8929198126175
new min fval from sgd:  -2374.8997981103466
new min fval from sgd:  -2374.909251739765
new min fval from sgd:  -2374.9206388378198
new min fval from sgd:  -2374.937729865945
new min fval from sgd:  -2374.9425259080576
new min fval from sgd:  -2375.024438581235
new min fval from sgd:  -2375.083746755036
new min fval from sgd:  -2375.0852657546448
new min fval from sgd:  -2375.089645611218
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [932.7268]
objective value function right now is: -2374.3884485097255
new min fval from sgd:  -2375.113101429696
new min fval from sgd:  -2375.128929684852
new min fval from sgd:  -2375.1540235195475
new min fval from sgd:  -2375.1765911091334
new min fval from sgd:  -2375.203450092796
new min fval from sgd:  -2375.2137517899814
new min fval from sgd:  -2375.2285982528974
new min fval from sgd:  -2375.2438120441016
new min fval from sgd:  -2375.2545922014974
new min fval from sgd:  -2375.2602074192782
new min fval from sgd:  -2375.2711875083587
new min fval from sgd:  -2375.2797303029415
new min fval from sgd:  -2375.2826395231336
new min fval from sgd:  -2375.3078418513473
new min fval from sgd:  -2375.3233263064435
new min fval from sgd:  -2375.3522962757106
new min fval from sgd:  -2375.3673615569696
new min fval from sgd:  -2375.3776051478285
new min fval from sgd:  -2375.381620638042
new min fval from sgd:  -2375.390816761947
new min fval from sgd:  -2375.418983284419
new min fval from sgd:  -2375.4587797018653
new min fval from sgd:  -2375.477229344246
new min fval from sgd:  -2375.5052190191504
new min fval from sgd:  -2375.585598050646
new min fval from sgd:  -2375.6253651785173
new min fval from sgd:  -2375.649429463343
new min fval from sgd:  -2375.654752026245
new min fval from sgd:  -2375.6711808538575
new min fval from sgd:  -2375.687791140702
new min fval from sgd:  -2375.7076464074366
new min fval from sgd:  -2375.7213091330677
new min fval from sgd:  -2375.721632160581
new min fval from sgd:  -2375.7382454236745
new min fval from sgd:  -2375.7423782699057
new min fval from sgd:  -2375.745587942794
new min fval from sgd:  -2375.7770610196894
new min fval from sgd:  -2375.8052554506257
new min fval from sgd:  -2375.8151120418097
new min fval from sgd:  -2375.8191017079453
new min fval from sgd:  -2375.8313826464882
new min fval from sgd:  -2375.841470439941
new min fval from sgd:  -2375.871107549461
new min fval from sgd:  -2375.9027946830906
new min fval from sgd:  -2375.9095070937715
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [933.63666]
objective value function right now is: -2375.8199533361235
min fval:  -2375.9095070937715
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -6.3987,   6.9391],
        [ -1.2509,   1.4706],
        [ -6.9866,   6.0934],
        [  5.8441,  10.5557],
        [ -6.0472,   5.3884],
        [-35.7412,  -2.8755],
        [  2.3214,   0.7783],
        [  7.6909,   2.2364],
        [ -6.3597,   6.7542],
        [ -6.9407,   6.4037],
        [ -6.3853,   6.9021],
        [ -7.2693,   6.4016],
        [  7.3091,   2.8603],
        [  4.6821,   9.2675],
        [  7.1643,   3.2810],
        [  6.5952,   2.8958],
        [-15.1054,  -6.0605],
        [ -8.2356,  -6.7967],
        [ -7.1744,   6.3800],
        [ -7.1455,   6.2319],
        [ -1.2508,   1.4704],
        [ -7.1901,   6.2471]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 4.5055, -3.5189,  0.2899, -9.5582, -0.7953, -2.6045, -6.9030, -7.0483,
         4.2706,  3.2373,  4.4960,  2.6377, -7.3493, -8.1616, -7.5861, -6.8855,
        -3.0260, -3.3832,  2.8150,  2.1775, -3.5190,  1.9270], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-6.1134e-02, -6.4041e-03,  5.7987e-03,  6.1674e-04, -1.1968e-03,
          3.4576e-02, -4.2485e-03,  1.2600e-01, -5.5257e-02, -2.0635e-02,
         -6.4615e-02, -3.0874e-03,  1.1134e-01,  2.5010e-03,  9.6498e-02,
          9.9959e-02, -1.0530e-01, -3.3771e-01, -8.0775e-03,  3.3237e-03,
         -6.4052e-03,  6.8826e-03],
        [-9.0769e+00, -3.5828e-02, -1.1217e+00,  8.9451e+00, -5.7987e-01,
          6.0841e+00,  2.8391e-01,  6.3579e+00, -8.3961e+00, -4.5723e+00,
         -9.1176e+00, -3.1265e+00,  6.5008e+00,  3.9785e+00,  6.4538e+00,
          3.5529e+00,  8.8441e+00,  6.0146e+00, -3.6424e+00, -3.9488e+00,
         -3.5734e-02, -3.4418e+00],
        [-6.1134e-02, -6.4041e-03,  5.7987e-03,  6.1688e-04, -1.1968e-03,
          3.4576e-02, -4.2485e-03,  1.2600e-01, -5.5257e-02, -2.0635e-02,
         -6.4615e-02, -3.0874e-03,  1.1134e-01,  2.5011e-03,  9.6498e-02,
          9.9959e-02, -1.0530e-01, -3.3771e-01, -8.0775e-03,  3.3237e-03,
         -6.4052e-03,  6.8826e-03],
        [-8.6669e+00,  3.9336e-02, -1.1030e+00,  8.3722e+00, -5.1216e-01,
          5.9851e+00,  3.4664e-01,  5.9206e+00, -8.2813e+00, -5.0441e+00,
         -9.1848e+00, -3.5269e+00,  6.0276e+00,  4.6190e+00,  6.0447e+00,
          3.8448e+00,  8.9783e+00,  6.0900e+00, -4.0733e+00, -4.3060e+00,
          3.9989e-02, -3.7061e+00],
        [-8.5695e+00, -1.3324e-02, -2.2947e+00,  2.6388e+00, -1.2006e+00,
          6.0094e+00,  3.7945e-01,  5.9713e+00, -8.1881e+00, -6.2557e+00,
         -8.6872e+00, -5.2121e+00,  5.4908e+00,  1.2240e+00,  5.4794e+00,
          4.1883e+00,  9.6984e+00,  6.1559e+00, -5.7977e+00, -5.1111e+00,
         -1.3303e-02, -4.7080e+00],
        [ 7.9096e+00,  4.7045e-02,  1.5825e+00, -8.9506e+00,  7.5583e-01,
         -5.6159e+00, -3.9749e-01, -6.4072e+00,  7.6111e+00,  5.0772e+00,
          8.5913e+00,  4.4119e+00, -6.5005e+00, -3.7204e+00, -6.6508e+00,
         -3.2327e+00, -8.7559e+00, -5.3887e+00,  4.5637e+00,  4.3810e+00,
          4.6073e-02,  3.7600e+00],
        [ 5.2866e-01,  4.8266e-02,  4.3599e-03,  1.1345e-02,  7.8786e-03,
          1.2984e-01,  3.2075e-02, -4.0423e-02,  4.9057e-01,  2.4692e-01,
          5.3321e-01,  1.2759e-01, -4.6205e-03,  1.5440e-03,  6.3182e-03,
         -9.0556e-03,  3.6958e-01,  5.5172e-01,  1.6045e-01,  8.2710e-02,
          4.8274e-02,  5.5800e-02],
        [-6.1134e-02, -6.4041e-03,  5.7987e-03,  6.1670e-04, -1.1968e-03,
          3.4576e-02, -4.2485e-03,  1.2600e-01, -5.5257e-02, -2.0635e-02,
         -6.4615e-02, -3.0874e-03,  1.1134e-01,  2.5009e-03,  9.6498e-02,
          9.9958e-02, -1.0530e-01, -3.3771e-01, -8.0775e-03,  3.3237e-03,
         -6.4052e-03,  6.8826e-03],
        [-7.3072e+00,  2.5337e-02, -1.2804e+00,  8.0132e+00, -3.8894e-01,
          4.9565e+00,  1.4822e-01,  6.1521e+00, -6.6590e+00, -4.8071e+00,
         -7.7192e+00, -4.1352e+00,  6.4185e+00,  3.3022e+00,  6.7040e+00,
          2.6685e+00,  8.3676e+00,  4.7848e+00, -4.2563e+00, -3.4942e+00,
          2.4057e-02, -3.1412e+00],
        [-6.1134e-02, -6.4041e-03,  5.7987e-03,  6.1673e-04, -1.1968e-03,
          3.4576e-02, -4.2485e-03,  1.2600e-01, -5.5257e-02, -2.0635e-02,
         -6.4615e-02, -3.0874e-03,  1.1134e-01,  2.5010e-03,  9.6498e-02,
          9.9959e-02, -1.0530e-01, -3.3771e-01, -8.0775e-03,  3.3237e-03,
         -6.4052e-03,  6.8826e-03],
        [ 5.7270e+00,  2.8422e-02,  4.5724e-01, -5.4770e+00,  8.8416e-02,
         -3.7640e+00, -1.7664e-02, -4.8989e+00,  4.9825e+00,  3.4037e+00,
          5.8281e+00,  2.7863e+00, -5.0746e+00, -2.7433e+00, -5.3165e+00,
         -2.0493e+00, -6.1115e+00, -2.8183e+00,  2.9527e+00,  2.3888e+00,
          2.8536e-02,  2.0349e+00],
        [ 7.9129e+00,  2.2238e-02,  1.5877e+00, -9.2213e+00,  7.2649e-01,
         -5.7219e+00, -5.8178e-01, -6.3805e+00,  7.6027e+00,  5.5718e+00,
          8.5870e+00,  4.2948e+00, -6.3198e+00, -3.9894e+00, -6.6670e+00,
         -3.5854e+00, -8.7506e+00, -5.5798e+00,  4.8732e+00,  4.2975e+00,
          2.1853e-02,  4.0038e+00],
        [-6.1134e-02, -6.4041e-03,  5.7987e-03,  6.1677e-04, -1.1968e-03,
          3.4576e-02, -4.2485e-03,  1.2600e-01, -5.5257e-02, -2.0635e-02,
         -6.4615e-02, -3.0874e-03,  1.1134e-01,  2.5010e-03,  9.6498e-02,
          9.9959e-02, -1.0530e-01, -3.3771e-01, -8.0775e-03,  3.3237e-03,
         -6.4052e-03,  6.8826e-03],
        [-6.1135e-02, -6.4041e-03,  5.7987e-03,  6.1699e-04, -1.1968e-03,
          3.4576e-02, -4.2485e-03,  1.2600e-01, -5.5257e-02, -2.0635e-02,
         -6.4615e-02, -3.0873e-03,  1.1134e-01,  2.5013e-03,  9.6499e-02,
          9.9959e-02, -1.0530e-01, -3.3771e-01, -8.0774e-03,  3.3237e-03,
         -6.4052e-03,  6.8826e-03],
        [ 5.2824e-01,  4.8187e-02,  4.3627e-03,  1.1339e-02,  7.8804e-03,
          1.2984e-01,  3.2023e-02, -4.0223e-02,  4.9018e-01,  2.4674e-01,
          5.3277e-01,  1.2750e-01, -4.5041e-03,  1.5435e-03,  6.3948e-03,
         -8.9518e-03,  3.6939e-01,  5.5091e-01,  1.6034e-01,  8.2646e-02,
          4.8194e-02,  5.5755e-02],
        [-6.3727e+00,  3.6473e-02, -4.6392e-01,  6.4219e+00, -3.1687e-03,
          4.3335e+00, -1.3139e-02,  5.7176e+00, -5.8317e+00, -4.1136e+00,
         -6.9266e+00, -3.3465e+00,  5.9660e+00,  3.2332e+00,  6.1581e+00,
          2.4459e+00,  7.5744e+00,  3.7132e+00, -3.5062e+00, -2.7882e+00,
          3.6064e-02, -2.3209e+00],
        [ 4.7982e+00,  5.8327e-02,  9.0953e-02, -2.6091e+00, -1.9446e-03,
         -1.8983e+00,  9.0625e-02, -3.8984e+00,  4.2890e+00,  2.7806e+00,
          4.9256e+00,  1.9601e+00, -3.7430e+00, -1.0554e+00, -3.7930e+00,
         -1.1066e+00, -3.7230e+00, -4.9882e-01,  2.2329e+00,  1.5036e+00,
          5.8527e-02,  1.1349e+00],
        [-6.1135e-02, -6.4041e-03,  5.7987e-03,  6.1650e-04, -1.1968e-03,
          3.4577e-02, -4.2485e-03,  1.2600e-01, -5.5258e-02, -2.0636e-02,
         -6.4615e-02, -3.0875e-03,  1.1134e-01,  2.5008e-03,  9.6498e-02,
          9.9958e-02, -1.0530e-01, -3.3771e-01, -8.0776e-03,  3.3236e-03,
         -6.4052e-03,  6.8826e-03],
        [-7.0594e+00,  3.0117e-02, -9.8632e-01,  7.6895e+00, -2.3885e-01,
          5.0534e+00,  1.0687e-01,  6.1885e+00, -6.4837e+00, -4.7508e+00,
         -7.8145e+00, -3.7817e+00,  6.2449e+00,  3.3265e+00,  6.7794e+00,
          2.6589e+00,  8.0863e+00,  4.5880e+00, -3.9880e+00, -3.4165e+00,
          2.9204e-02, -3.1713e+00],
        [-6.1134e-02, -6.4041e-03,  5.7987e-03,  6.1664e-04, -1.1968e-03,
          3.4576e-02, -4.2485e-03,  1.2600e-01, -5.5257e-02, -2.0636e-02,
         -6.4615e-02, -3.0874e-03,  1.1134e-01,  2.5009e-03,  9.6498e-02,
          9.9958e-02, -1.0530e-01, -3.3771e-01, -8.0775e-03,  3.3237e-03,
         -6.4052e-03,  6.8826e-03],
        [-8.7458e+00, -2.0733e-02, -2.0859e+00,  7.7702e-01, -9.9506e-01,
          6.1766e+00,  2.9819e-01,  5.7924e+00, -8.2157e+00, -5.9164e+00,
         -9.2721e+00, -5.1044e+00,  5.3943e+00,  5.4983e-01,  4.9829e+00,
          4.3517e+00,  9.5084e+00,  6.1891e+00, -5.2942e+00, -4.8538e+00,
         -2.1572e-02, -4.4089e+00],
        [-6.1015e-02, -6.4010e-03,  5.7956e-03,  4.7033e-04, -1.1983e-03,
          3.4503e-02, -4.2499e-03,  1.2566e-01, -5.5184e-02, -2.0674e-02,
         -6.4486e-02, -3.1169e-03,  1.1100e-01,  2.3573e-03,  9.6168e-02,
          9.9631e-02, -1.0514e-01, -3.3654e-01, -8.1101e-03,  3.3017e-03,
         -6.4021e-03,  6.8661e-03]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.0610,  0.0927, -2.0610,  0.0870,  0.1996, -0.2847,  4.1278, -2.0610,
        -0.0709, -2.0610, -0.2927, -0.2709, -2.0610, -2.0610,  4.1146, -0.2536,
        -0.8228, -2.0610, -0.0091, -2.0610,  0.3571, -2.0643], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0319, -12.8764,   0.0319, -12.0680, -13.9350,  13.7408,   3.6018,
           0.0319, -10.2156,   0.0319,   5.0524,  14.3208,   0.0319,   0.0319,
           3.5501,  -7.0622,   3.6516,   0.0319,  -9.0683,   0.0319, -12.6715,
           0.0320]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 11.6944,   2.6835],
        [ -1.3103,   7.8014],
        [  5.1835,   8.9815],
        [ 11.0532,   0.0593],
        [-11.4837,  -1.8557],
        [ -8.4101,   6.9689],
        [ -1.7113,   0.3062],
        [ -1.7697,   0.3069],
        [ -4.7323,   2.9192],
        [  2.3899,  10.4329],
        [ -9.0448,  -3.0150],
        [ -5.4654,  -1.2301],
        [-11.2276,  -0.4135],
        [ -1.6009,   0.2889],
        [ -1.8297,   0.3174],
        [ -9.7373, -10.3620],
        [  2.6809,  10.5366],
        [ -1.6020,   0.2925],
        [-15.3568,  -4.0899],
        [  1.9909,  11.2984],
        [ 13.7826,   5.4318],
        [ -6.7169,  10.9307]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-2.5985, -7.4448, -0.8093, -9.1814,  5.4172,  3.6466, -4.7310, -4.7208,
        -5.7229,  6.5311, -5.1570, -5.0255,  7.3666, -4.7518, -4.7223, -7.3908,
         6.6705, -4.7464, -2.5547,  7.4197,  1.8800,  7.1319], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.6769e+00,  9.6392e-02, -4.7989e-01, -8.9558e-01, -6.8138e-01,
          1.3147e-01, -1.3423e-02, -1.3069e-02,  2.8400e-02, -6.0603e-01,
         -6.4959e-04, -1.3895e-02, -9.3264e-01, -1.3608e-02, -1.2146e-02,
         -4.2083e-01, -6.0005e-01, -1.3506e-02,  5.1069e-03, -6.1537e-01,
         -1.9716e+00, -1.3142e-01],
        [ 5.0898e+00, -3.3922e+00,  2.9875e+00,  9.5329e+00, -2.2128e+00,
         -2.8524e+00, -2.6384e-02, -5.0269e-02,  3.5553e+00, -2.3542e+00,
         -2.0831e+00, -2.1425e+00, -8.5573e-01, -6.0384e-02, -7.0598e-02,
          7.0808e+00, -2.3220e+00, -4.2739e-02, -2.1156e+00, -3.4091e+00,
          2.2471e+00, -6.4551e+00],
        [-7.9447e-02, -2.7589e-03, -4.6577e-01, -2.5729e+00, -1.0921e+00,
          2.0792e+00,  2.0957e-01,  2.0459e-01,  6.8333e-02, -5.7857e+00,
          1.3411e+00,  6.8352e-01,  9.2032e-01,  1.7599e-01,  1.9938e-01,
         -4.1433e-01, -6.0117e+00,  1.7863e-01,  5.5538e+00, -5.7810e+00,
         -9.1181e-01,  1.9281e+00],
        [ 6.8966e-01, -7.5043e-01,  4.0088e-01, -3.6915e+00,  7.9671e+00,
          2.3881e-01,  8.8399e-02,  6.2822e-02, -6.4257e-01,  6.5962e-01,
          9.1984e-02,  1.2831e-01,  6.5016e+00,  1.0611e-01,  4.4819e-02,
          1.8726e+00,  1.4632e-01,  9.7682e-02, -3.4647e-02,  1.7287e+00,
          2.9153e+00,  5.4001e+00],
        [-6.5476e+00, -3.0237e+00, -5.9917e+00, -4.7328e+00, -3.5251e-01,
          1.1153e+00,  6.8888e-02,  6.5999e-02,  9.0551e-02,  3.0274e+00,
          2.4182e+00,  1.6756e+00, -2.2909e+00,  1.5920e-03,  6.2748e-02,
         -4.9944e+00,  3.1948e+00,  1.0348e-02,  3.7881e+00,  3.9833e+00,
         -2.6954e+00,  4.5844e+00],
        [-4.0001e-01,  1.3226e-01, -1.4480e+00, -4.8489e-01,  3.5541e-01,
          1.6769e-01,  1.7820e-03,  2.2435e-03,  1.3280e-01, -1.0948e+00,
          2.6814e-01,  1.7774e-01,  1.9183e-01, -4.9201e-02,  1.4883e-02,
         -5.1819e-01, -1.0827e+00, -4.7261e-02,  7.6818e-01, -1.3231e+00,
         -7.8785e-01, -1.0482e+00],
        [ 3.1896e+00, -2.5222e-03,  3.2829e-01,  2.6703e+00, -3.5516e+00,
          3.0736e+00, -6.9097e-02, -8.1958e-02,  1.8389e-02,  9.6460e+00,
         -3.6155e+00, -2.9290e+00, -5.2247e+00, -6.0765e-02, -8.1878e-02,
         -6.0676e+00,  9.8094e+00, -2.4613e-02, -6.2944e+00,  1.0578e+01,
          5.5208e+00,  1.4671e+01],
        [-1.1417e+00, -4.8904e-02, -8.8663e-01, -5.0940e-01, -3.5180e-01,
          2.3823e-01, -2.2372e-02, -2.1774e-02, -1.4245e-01, -9.6920e-01,
          1.3953e-02,  5.9918e-03, -7.6244e-01, -2.2691e-02, -2.0935e-02,
         -2.7172e-02, -8.8639e-01, -2.2897e-02,  6.4949e-01, -1.0975e+00,
         -1.7358e+00, -9.9271e-02],
        [-1.5141e+00,  1.4582e-02, -8.4504e-01, -6.1722e-01, -5.2332e-01,
          2.7734e-01, -2.4449e-02, -2.4600e-02, -1.1590e-01, -6.8572e-01,
          2.4915e-02, -3.6588e-03, -1.1393e+00, -2.4447e-02, -2.4245e-02,
          3.0466e-01, -5.8611e-01, -2.4407e-02,  2.4964e-01, -7.3061e-01,
         -1.9616e+00,  1.0107e-01],
        [-3.0111e-01, -4.5077e-03, -3.6418e-01, -1.1048e+00,  6.9254e-01,
         -1.0465e-01,  2.5253e-01,  2.5870e-01,  1.4906e-02, -2.8390e+00,
          1.8862e+00,  1.1633e+00,  2.2159e+00,  2.3529e-01,  2.5092e-01,
          2.4563e-01, -2.8289e+00,  2.4273e-01,  9.1357e-01, -3.3132e+00,
         -1.3040e+00, -3.7887e+00],
        [-8.7399e-01, -3.4130e-02, -4.1856e-01, -1.2800e+00, -3.7009e-01,
         -1.3694e-01, -3.3985e-02, -3.3728e-02, -4.3915e-02, -1.4613e+00,
          1.9711e-02, -2.9237e-02, -5.1953e-01, -3.2826e-02, -3.3386e-02,
         -7.2063e-01, -1.6136e+00, -3.3685e-02,  1.1591e-01, -1.5824e+00,
         -1.2924e+00, -7.8217e-01],
        [-1.3559e+00,  2.0217e-01, -1.8535e+00, -2.2473e-01, -1.3723e+00,
          6.3135e-01, -8.0850e-03, -3.6908e-03, -9.9657e-03, -7.6867e-01,
         -4.3805e-02, -6.5436e-03, -1.7420e+00, -1.0862e-02,  3.1951e-04,
          1.2521e+00, -7.7869e-01, -1.1215e-02,  3.4276e-01, -7.3945e-01,
         -1.6713e+00, -9.3784e-03],
        [-7.5825e+00, -4.6863e-04,  1.7502e-02, -1.2562e+01,  4.0847e+00,
          2.1155e+00,  3.0267e-01,  3.0827e-01, -9.5460e-02, -2.1448e+01,
          2.9023e+00,  8.9348e-01,  4.2219e+00,  2.8917e-01,  3.0652e-01,
          9.5059e+00, -2.2733e+01,  2.9481e-01,  5.4545e+00, -2.2138e+01,
         -8.5727e+00, -4.4102e+00],
        [-1.2888e+00,  5.5430e-01, -2.4009e-01, -6.6933e-01, -9.6214e-02,
          8.1262e-01, -1.9718e-02, -1.9205e-02,  3.9145e-01, -9.7526e-01,
          6.0917e-02,  4.3866e-02, -8.8300e-01, -2.1068e-02, -1.3541e-02,
          2.6896e-01, -1.0765e+00, -2.1407e-02,  1.2004e+00, -9.6454e-01,
         -1.6312e+00,  2.0125e-01],
        [ 4.4346e+00,  1.1106e+00,  1.2388e+00,  1.0860e+01, -3.8811e+00,
         -5.9119e+00, -1.5878e-02, -1.3078e-02, -3.5731e-01, -2.9652e+00,
          2.1569e+00,  9.0722e-01, -3.0138e+00, -1.0249e-02, -6.9511e-03,
          1.3231e+01, -2.9800e+00, -1.0102e-02,  3.4019e+00, -4.1099e+00,
          4.1520e+00, -7.7451e+00],
        [-7.4320e-01,  3.7991e-01, -4.7732e+00,  1.2970e+00, -4.2881e+00,
          6.3980e-01,  2.1631e-02,  2.8085e-02,  3.2073e-02,  1.0748e-01,
         -1.1674e-01,  1.0121e-02, -4.2620e+00,  1.0466e-02,  3.0804e-02,
          1.0219e+00,  2.5386e-01,  1.1722e-02, -2.9493e-02, -4.5252e-02,
         -1.2623e+00, -3.0572e+00],
        [-1.6856e+00, -3.1025e-03, -4.1672e-01, -8.1757e-01, -7.8077e-01,
         -5.3149e-03, -2.2509e-03, -2.0353e-03, -3.2359e-02, -5.3821e-01,
         -6.0379e-03, -3.1663e-03, -1.0379e+00, -2.1312e-03, -1.6278e-03,
         -3.2319e-01, -5.4654e-01, -2.0889e-03, -1.6491e-02, -5.4762e-01,
         -1.9314e+00, -2.1428e-01],
        [-1.0592e+00,  3.3748e-01, -1.3432e+00,  2.5279e-01, -8.5082e-01,
          7.3502e-01,  2.7438e-03,  7.8648e-03,  1.4509e-02, -1.0528e+00,
          3.0238e-02, -7.0032e-04, -1.1514e+00, -8.7436e-04,  1.3036e-02,
         -6.0815e-01, -9.5628e-01, -1.4723e-03,  6.7033e-01, -1.1547e+00,
         -1.5130e+00, -3.0179e-02],
        [-8.1015e-01,  3.7575e-01, -5.0205e+00,  8.5302e-01, -9.3863e-01,
          7.8641e-01,  5.6993e-02,  6.8898e-02,  1.5735e-01,  1.2101e-01,
          8.5524e-02,  5.5415e-02, -2.0387e+00,  5.1726e-02,  8.1428e-02,
         -3.3161e-01,  6.9844e-02,  5.1260e-02,  8.0353e-01,  1.6520e-01,
         -1.2942e+00, -1.2668e+00],
        [ 5.3871e+00, -7.0975e-03,  1.5242e-01,  8.2683e+00, -5.1470e+00,
          1.5232e-01, -7.9263e-02, -8.5661e-02, -3.4874e-02,  1.6223e+01,
         -3.8873e+00, -2.5158e+00, -6.6235e+00, -8.4662e-02, -9.5271e-02,
         -1.0265e+01,  1.7005e+01, -8.6658e-02, -5.7772e+00,  1.6127e+01,
          9.5148e+00,  1.2211e+01],
        [-1.0468e+00, -6.8348e-02, -9.1527e-01, -6.1650e-01, -3.6673e-01,
          2.0053e-01, -2.3530e-02, -2.2172e-02, -1.5457e-01, -1.0331e+00,
          1.1614e-02,  2.6266e-03, -7.1415e-01, -2.4165e-02, -2.0541e-02,
          4.2234e-02, -9.5609e-01, -2.4425e-02,  6.7725e-01, -1.1726e+00,
         -1.6159e+00, -1.7869e-01],
        [-1.0246e+00,  2.2404e-01, -2.2112e+00,  2.2033e-01, -8.7676e-01,
          8.5340e-01,  1.6969e-02,  2.4921e-02, -1.1512e-01, -7.6657e-01,
          1.4219e-01,  6.0782e-02, -8.0301e-01,  1.1112e-02,  3.4627e-02,
         -1.2342e+00, -6.0344e-01,  1.0376e-02,  1.0068e+00, -9.7998e-01,
         -1.4378e+00, -6.9736e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.4319,  5.0782, -1.0661,  0.3459, -2.4306, -1.7274, -2.6675, -2.3135,
        -2.6618, -1.9254, -2.3296, -2.3297, -3.9320, -2.3980,  5.2936, -2.0061,
        -2.5301, -2.2994, -2.0266, -1.6701, -2.2226, -2.0172], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.1391e-02, -1.8957e+00, -3.2514e-02, -3.0252e+00, -3.9338e-01,
         -1.3539e-01, -1.9453e+00, -1.9226e-02, -1.2101e-02, -1.5032e-01,
         -1.0413e-02, -5.9650e-03, -2.8746e-03, -2.9732e-02, -1.3081e+00,
          1.9484e-04, -9.0453e-03, -8.9667e-03, -1.4863e-02, -2.6680e+00,
         -2.0932e-02, -1.7426e-02],
        [ 8.0136e-01,  1.8535e+00,  2.7373e+00, -6.6074e-01, -5.4028e+00,
          2.9063e+00, -6.6263e-01,  1.6059e+00,  1.3397e+00,  2.9969e+00,
          1.9884e+00,  2.5853e+00, -1.2892e+01,  2.2739e+00,  1.5069e+00,
          4.5224e+00,  5.2160e-01,  2.5206e+00,  2.9945e+00,  2.8044e+00,
          1.6776e+00,  3.0180e+00],
        [-8.0883e-03, -1.8789e+00, -3.3765e-02, -3.0728e+00, -4.1567e-01,
         -1.3584e-01, -1.9082e+00, -1.7077e-02, -8.9786e-03, -1.5660e-01,
         -7.5174e-03, -3.6366e-03, -2.3261e-03, -2.9552e-02, -1.3283e+00,
         -1.0698e-03, -5.8925e-03, -5.8837e-03, -1.1160e-02, -2.6211e+00,
         -1.9175e-02, -1.4428e-02],
        [-5.8099e-03, -1.8262e+00, -2.5944e-02, -2.8349e+00, -3.4100e-01,
         -1.2108e-01, -1.8133e+00, -1.1628e-02, -5.7222e-03, -1.4722e-01,
         -6.5898e-03, -3.6410e-03, -2.9575e-03, -2.1643e-02, -1.2627e+00,
         -7.6474e-04, -5.2070e-03, -5.2327e-03, -8.3647e-03, -2.3897e+00,
         -1.3425e-02, -9.3971e-03],
        [ 1.5539e-01,  2.8340e+00,  3.5220e+00,  4.2629e+00, -5.0306e-01,
         -2.7027e-01, -2.5712e-01, -3.3087e-01, -1.8888e-01,  3.1984e+00,
          1.9061e-01, -7.7009e-01, -9.8570e+00, -6.5797e-01,  1.6304e+00,
         -2.3765e+00,  1.6522e-01, -9.9840e-01, -1.6128e+00, -2.9784e+00,
         -2.9848e-01, -1.0005e+00],
        [-4.9236e-03, -1.6602e+00, -2.4909e-02, -2.6106e+00, -3.4334e-01,
         -1.1340e-01, -1.8041e+00, -1.1569e-02, -5.0335e-03, -1.3807e-01,
         -5.3889e-03, -2.9675e-03, -6.6581e-03, -2.1590e-02, -1.1101e+00,
          5.3341e-05, -4.3583e-03, -4.0665e-03, -6.5774e-03, -2.2629e+00,
         -1.3346e-02, -9.0755e-03],
        [-9.2968e-03, -1.8572e+00, -3.5837e-02, -3.0415e+00, -4.1593e-01,
         -1.3937e-01, -1.8968e+00, -1.7985e-02, -1.0132e-02, -1.6382e-01,
         -8.6678e-03, -4.4867e-03, -2.2156e-03, -3.1188e-02, -1.3135e+00,
         -9.5765e-04, -7.1134e-03, -7.0501e-03, -1.2248e-02, -2.5657e+00,
         -2.0066e-02, -1.5383e-02],
        [-6.7102e-03, -1.7097e+00, -3.1543e-02, -2.7066e+00, -3.5957e-01,
         -1.2412e-01, -1.7909e+00, -1.4359e-02, -7.1309e-03, -1.5676e-01,
         -6.8552e-03, -3.8557e-03, -6.2508e-03, -2.5669e-02, -1.1747e+00,
         -1.1913e-03, -5.6541e-03, -5.4825e-03, -8.5159e-03, -2.2531e+00,
         -1.6281e-02, -1.1432e-02],
        [-8.7089e-03, -1.8296e+00, -2.8988e-02, -2.8969e+00, -3.7066e-01,
         -1.2717e-01, -1.8549e+00, -1.4665e-02, -8.9876e-03, -1.4969e-01,
         -8.6576e-03, -4.7307e-03, -3.0614e-03, -2.5256e-02, -1.2738e+00,
         -1.1430e-03, -7.1439e-03, -7.1708e-03, -1.1690e-02, -2.4765e+00,
         -1.6433e-02, -1.2865e-02],
        [-1.0612e-02, -1.8109e+00, -4.7776e-02, -2.6281e+00, -2.8606e-01,
         -1.3275e-01, -1.2800e+00, -2.1752e-02, -1.2101e-02, -2.0064e-01,
         -1.0120e-02, -5.8758e-03, -6.8441e-03, -3.2125e-02, -1.4679e+00,
         -3.7220e-03, -8.0104e-03, -8.5839e-03, -1.4586e-02, -1.8342e+00,
         -2.3889e-02, -1.8077e-02],
        [ 2.7622e-01, -2.7441e-01, -4.9225e+00, -1.3791e+00,  4.3303e+00,
         -1.4147e+00,  2.5814e+00, -5.6735e-01, -6.8091e-02, -3.9993e+00,
         -1.4111e+00, -2.6248e-01,  1.8830e+01, -3.9806e-01,  9.0207e-01,
          2.4057e-01,  2.1498e-01, -5.0997e-01, -3.4524e-01,  3.6150e+00,
         -6.7352e-01, -4.6470e-01],
        [-8.5085e-03, -1.8234e+00, -4.5059e-02, -2.9267e+00, -3.8086e-01,
         -1.4246e-01, -1.7027e+00, -2.1217e-02, -9.8741e-03, -1.8457e-01,
         -7.8865e-03, -4.3664e-03, -3.4598e-03, -3.6189e-02, -1.3242e+00,
         -3.0968e-03, -6.3520e-03, -6.2988e-03, -1.1571e-02, -2.3294e+00,
         -2.3773e-02, -1.6999e-02],
        [-7.7244e-03, -1.8858e+00, -3.4369e-02, -3.0698e+00, -4.0795e-01,
         -1.3678e-01, -1.9126e+00, -1.8116e-02, -8.7324e-03, -1.5836e-01,
         -6.8649e-03, -2.9867e-03, -2.4702e-03, -3.0999e-02, -1.3289e+00,
         -1.1245e-03, -5.4419e-03, -5.3062e-03, -1.0535e-02, -2.6237e+00,
         -2.0302e-02, -1.4990e-02],
        [-1.0060e-02, -1.8682e+00, -2.9143e-02, -2.9191e+00, -3.6489e-01,
         -1.2746e-01, -1.8956e+00, -1.6227e-02, -1.0456e-02, -1.4337e-01,
         -9.6933e-03, -5.6441e-03, -3.0654e-03, -2.5778e-02, -1.2873e+00,
          6.5739e-04, -8.4301e-03, -8.3923e-03, -1.3324e-02, -2.5725e+00,
         -1.7775e-02, -1.4874e-02]], device='cuda:0'))])
xi:  [933.4266]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1234.6463246838694
W_T_median: 1197.6052084280668
W_T_pctile_5: 940.3737409624418
W_T_CVAR_5_pct: 714.4224182416787
Average q (qsum/M+1):  53.65617912046371
Optimal xi:  [933.4266]
Expected(across Rb) median(across samples) p_equity:  0.09706124307025069
obj fun:  tensor(-2375.9095, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: MC_everything
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
