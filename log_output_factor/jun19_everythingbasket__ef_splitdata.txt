Starting at: 
19-06-23_14:55

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = MC_everything
timeseries_basket['basket_desc'] = marc_test1_all_assets_longfactors
timeseries_basket['basket_columns'] = 
['Size_Lo30_real_ret', 'Value_Hi30_real_ret', 'Oprof_Hi30_real_ret', 'Inv_Lo30_real_ret', 'Mom_Hi30_real_ret', 'EP_Hi30_real_ret', 'Vol_Lo20_real_ret', 'Div_Hi30_real_ret', 'EQWFact_real_ret', 'T30_real_ret', 'T90_real_ret', 'B10_real_ret', 'VWD_real_ret', 'EWD_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = MC_everything
timeseries_basket['basket_desc'] = marc_test1_all_assets_longfactors
timeseries_basket['basket_columns'] = 
['Size_Lo30_nom_ret', 'Value_Hi30_nom_ret', 'Oprof_Hi30_nom_ret', 'Inv_Lo30_nom_ret', 'Mom_Hi30_nom_ret', 'EP_Hi30_nom_ret', 'Vol_Lo20_nom_ret', 'Div_Hi30_nom_ret', 'EQWFact_nom_ret', 'T30_nom_ret', 'T90_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'EWD_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.031411     0.013051
192608                    0.0319              0.0561  ...     0.028647     0.031002
192609                   -0.0173             -0.0071  ...     0.005787    -0.006499
192610                   -0.0294             -0.0355  ...    -0.028996    -0.034630
192611                   -0.0038              0.0294  ...     0.028554     0.024776

[5 rows x 15 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.036240    -0.011556
202209                   -0.0955             -0.0871  ...    -0.091324    -0.099903
202210                    0.0883              0.1486  ...     0.077403     0.049863
202211                   -0.0076              0.0462  ...     0.052365     0.028123
202212                   -0.0457             -0.0499  ...    -0.057116    -0.047241

[5 rows x 15 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'Oprof_Hi30_nom_ret_ind', 'Inv_Lo30_nom_ret_ind',
       'Mom_Hi30_nom_ret_ind', 'EP_Hi30_nom_ret_ind', 'Vol_Lo20_nom_ret_ind',
       'Div_Hi30_nom_ret_ind', 'EQWFact_nom_ret_ind', 'CPI_nom_ret_ind',
       'T30_nom_ret_ind', 'T90_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind', 'EWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
Oprof_Hi30_real_ret    0.003948
Inv_Lo30_real_ret      0.004513
Mom_Hi30_real_ret      0.011386
EP_Hi30_real_ret       0.007033
Vol_Lo20_real_ret      0.003529
Div_Hi30_real_ret      0.007888
EQWFact_real_ret       0.004508
T30_real_ret           0.000229
T90_real_ret           0.000501
B10_real_ret           0.001637
VWD_real_ret           0.006759
EWD_real_ret           0.009545
dtype: float64


timeseries_basket['data_df_stdev'] = 
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
Oprof_Hi30_real_ret    0.037444
Inv_Lo30_real_ret      0.037639
Mom_Hi30_real_ret      0.061421
EP_Hi30_real_ret       0.041390
Vol_Lo20_real_ret      0.030737
Div_Hi30_real_ret      0.056728
EQWFact_real_ret       0.037131
T30_real_ret           0.005227
T90_real_ret           0.005373
B10_real_ret           0.019258
VWD_real_ret           0.053610
EWD_real_ret           0.071360
dtype: float64


timeseries_basket['data_df_corr'] = 
                     Size_Lo30_real_ret  ...  EWD_real_ret
Size_Lo30_real_ret             1.000000  ...      0.977206
Value_Hi30_real_ret            0.908542  ...      0.919912
Oprof_Hi30_real_ret            0.433774  ...      0.462336
Inv_Lo30_real_ret              0.455237  ...      0.479661
Mom_Hi30_real_ret              0.903222  ...      0.912002
EP_Hi30_real_ret               0.476966  ...      0.506661
Vol_Lo20_real_ret              0.360014  ...      0.382411
Div_Hi30_real_ret              0.816292  ...      0.849068
EQWFact_real_ret               0.494572  ...      0.513359
T30_real_ret                   0.014412  ...      0.029084
T90_real_ret                   0.021968  ...      0.037909
B10_real_ret                   0.012916  ...      0.024853
VWD_real_ret                   0.865290  ...      0.907369
EWD_real_ret                   0.977206  ...      1.000000

[14 rows x 14 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 199201
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      22  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      22  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 22)     True          22  
2     (22, 22)     True          22  
3      (22, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer      14       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      22  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      22  logistic_sigmoid   
3        obj.layers[3]        3  output_layer      14           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 22)     True          22  
2     (22, 22)     True          22  
3     (22, 14)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0      (22, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer       14           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0     (22, 14)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0      (22, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer       14           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0     (22, 14)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857
 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857
 0.07142857 0.07142857]
W_T_mean: 9438.548856964324
W_T_median: 5685.968426194605
W_T_pctile_5: 125.54011840495605
W_T_CVAR_5_pct: -416.7378752066434
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1805.828133503695
Current xi:  [80.21166]
objective value function right now is: -1805.828133503695
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1811.6532764455687
Current xi:  [62.196373]
objective value function right now is: -1811.6532764455687
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1814.4587937541503
Current xi:  [43.27379]
objective value function right now is: -1814.4587937541503
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1815.5490483369188
Current xi:  [23.66878]
objective value function right now is: -1815.5490483369188
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.575907]
objective value function right now is: -1814.4675745587494
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.034848]
objective value function right now is: -1812.9738114197378
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1817.0920394221653
Current xi:  [-29.807678]
objective value function right now is: -1817.0920394221653
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1818.5461069877088
Current xi:  [-45.927532]
objective value function right now is: -1818.5461069877088
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-62.706978]
objective value function right now is: -1815.4254300133482
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1818.6070694334153
Current xi:  [-78.88413]
objective value function right now is: -1818.6070694334153
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-94.84074]
objective value function right now is: -1815.2287727734317
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1819.722489901175
Current xi:  [-110.84473]
objective value function right now is: -1819.722489901175
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-125.261795]
objective value function right now is: -1819.3520097959552
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-140.7038]
objective value function right now is: -1819.5885067861327
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-153.51083]
objective value function right now is: -1815.9687420236298
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-166.36925]
objective value function right now is: -1819.5014556582275
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1820.3611183825162
Current xi:  [-178.43338]
objective value function right now is: -1820.3611183825162
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-186.13138]
objective value function right now is: -1819.8441822351613
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-193.35886]
objective value function right now is: -1819.891223970473
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-197.48553]
objective value function right now is: -1819.8023169428536
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-203.42365]
objective value function right now is: -1819.9377283417195
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-207.86658]
objective value function right now is: -1819.8463136514094
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-209.9672]
objective value function right now is: -1818.0156511120442
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-212.91454]
objective value function right now is: -1819.3150096327388
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-213.81863]
objective value function right now is: -1819.6864166041605
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-213.57675]
objective value function right now is: -1820.067128463915
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-214.98363]
objective value function right now is: -1819.789497573189
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-216.02103]
objective value function right now is: -1818.4258489274673
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-218.0457]
objective value function right now is: -1818.3392239219675
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-218.83076]
objective value function right now is: -1818.5168443728153
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-218.76859]
objective value function right now is: -1818.7994731487568
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-216.97969]
objective value function right now is: -1820.0363567297668
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-215.69359]
objective value function right now is: -1819.9855367542866
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-214.88344]
objective value function right now is: -1817.5877953953113
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-214.4756]
objective value function right now is: -1820.1748152307237
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1820.708214829586
Current xi:  [-213.44713]
objective value function right now is: -1820.708214829586
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-212.78413]
objective value function right now is: -1820.6585239643975
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-212.37883]
objective value function right now is: -1820.5008175990897
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1820.7735118924115
Current xi:  [-211.54723]
objective value function right now is: -1820.7735118924115
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-210.82944]
objective value function right now is: -1820.5778527073342
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-210.2262]
objective value function right now is: -1820.4538046120842
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-209.86554]
objective value function right now is: -1820.7417231879426
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-209.35042]
objective value function right now is: -1820.5879297643453
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-208.8574]
objective value function right now is: -1820.511983049853
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-208.41902]
objective value function right now is: -1820.7539279580126
new min fval from sgd:  -1820.7772088830673
new min fval from sgd:  -1820.8080872748556
new min fval from sgd:  -1820.8203933976981
new min fval from sgd:  -1820.8273353340887
new min fval from sgd:  -1820.8337294534902
new min fval from sgd:  -1820.8338815517236
new min fval from sgd:  -1820.8355202254434
new min fval from sgd:  -1820.8640635505592
new min fval from sgd:  -1820.8756006765525
new min fval from sgd:  -1820.8762037647414
new min fval from sgd:  -1820.8762415828671
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-207.84834]
objective value function right now is: -1820.470968133407
new min fval from sgd:  -1820.8907768348702
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-207.32697]
objective value function right now is: -1820.6684315954476
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-206.98927]
objective value function right now is: -1820.4654834258608
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-206.63817]
objective value function right now is: -1820.8768769726712
new min fval from sgd:  -1820.893052724252
new min fval from sgd:  -1820.8948490194111
new min fval from sgd:  -1820.8951232129025
new min fval from sgd:  -1820.896824049923
new min fval from sgd:  -1820.8990074574324
new min fval from sgd:  -1820.901797200196
new min fval from sgd:  -1820.903917816135
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-206.49237]
objective value function right now is: -1820.8701199152497
min fval:  -1820.903917816135
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530],
        [ 0.0416, -0.0530]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([0.0907, 0.0907, 0.0907, 0.0907, 0.0907, 0.0907, 0.0907, 0.0907, 0.0907,
        0.0907, 0.0907, 0.0907, 0.0907, 0.0907, 0.0907, 0.0907, 0.0907, 0.0907,
        0.0907, 0.0907, 0.0907, 0.0907], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941],
        [0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941,
         0.0941, 0.0941, 0.0941, 0.0941]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([0.1759, 0.1759, 0.1759, 0.1759, 0.1759, 0.1759, 0.1759, 0.1759, 0.1759,
        0.1759, 0.1759, 0.1759, 0.1759, 0.1759, 0.1759, 0.1759, 0.1759, 0.1759,
        0.1759, 0.1759, 0.1759, 0.1759], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[0.9011, 0.9011, 0.9011, 0.9011, 0.9011, 0.9011, 0.9011, 0.9011, 0.9011,
         0.9011, 0.9011, 0.9011, 0.9011, 0.9011, 0.9011, 0.9011, 0.9011, 0.9011,
         0.9011, 0.9011, 0.9011, 0.9011]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  0.9350,   8.8479],
        [  9.4910,   0.3283],
        [ -4.4324,   6.2728],
        [  9.6539,  -0.5157],
        [ -8.3571,   0.5646],
        [  9.8969,   0.4305],
        [  4.9544,  -3.5379],
        [ -8.6118,  -9.6039],
        [-11.5032,  -6.0990],
        [ -2.5244,  10.1865],
        [ -6.0398, -10.2856],
        [ -0.9752,   1.3579],
        [ -0.9752,   1.3581],
        [ -0.9752,   1.3579],
        [ -0.9752,   1.3581],
        [ -4.7149,  -9.2675],
        [  9.9818,  -0.8560],
        [-12.3358,  -8.7847],
        [ -0.9762,   1.3292],
        [ -0.9752,   1.3581],
        [ 12.4272,   3.9705],
        [ -3.4932,   6.9715]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.2680,  -5.5607,   5.3261, -10.0046,   3.6961,  -6.1834,  -9.3448,
         -7.0269,  -3.8550,   8.4860,  -7.8803,  -2.0628,  -2.0628,  -2.0628,
         -2.0628,  -7.2364, -10.6671,  -5.8653,  -2.0603,  -2.0628,   0.0989,
          5.7260], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-8.9040e-01,  1.6055e+00, -2.2349e-01,  4.4992e+00, -4.2111e+00,
          2.0277e+00,  1.6554e+00,  6.7066e-01, -6.8782e-02, -2.2187e+00,
          8.9809e-01,  7.5869e-02,  7.5905e-02,  7.5869e-02,  7.5907e-02,
         -2.3157e-01,  4.5934e+00, -1.1729e-02,  6.3827e-02,  7.5908e-02,
         -1.1382e+00, -7.2302e-01],
        [ 8.0460e-01,  1.0581e+00, -8.9653e-02,  6.5171e-01,  2.7434e-01,
          9.4804e-01,  5.4392e-01,  5.7245e-01,  4.7488e-01,  2.1156e-01,
          7.2619e-01,  5.2097e-02,  5.2094e-02,  5.2097e-02,  5.2093e-02,
          7.8738e-01,  6.1980e-01,  4.7574e-01,  5.2938e-02,  5.2093e-02,
          1.5245e+00, -5.8803e-02],
        [-4.9715e-01, -6.6091e-01, -1.0833e-01, -3.5675e-01, -3.3063e-01,
         -6.0131e-01, -3.5056e-01, -3.8898e-01, -1.6464e-01, -1.8232e-01,
         -6.5311e-01, -2.8659e-05, -3.0116e-05, -2.8637e-05, -3.0191e-05,
         -7.0652e-01, -3.6049e-01, -1.8864e-01,  1.7724e-04, -3.0228e-05,
         -1.3101e+00, -1.6536e-01],
        [-1.1525e+00, -2.0967e+00, -7.6222e-01,  1.5311e+00, -2.5236e+00,
         -2.0153e+00,  2.0317e+00,  9.8100e-01,  2.4209e+00, -8.1826e-01,
         -8.5950e-02, -7.4361e-02, -7.4352e-02, -7.4361e-02, -7.4352e-02,
         -1.1727e-01,  1.5634e+00,  1.7985e+00, -7.6196e-02, -7.4352e-02,
         -3.3853e+00, -7.2165e-01],
        [ 7.4586e-01,  1.1005e+00, -1.4550e-01,  6.6251e-01,  1.8932e-01,
          9.7504e-01,  5.5428e-01,  5.3771e-01,  4.8498e-01,  1.5270e-01,
          6.6457e-01,  5.6139e-02,  5.6135e-02,  5.6139e-02,  5.6135e-02,
          7.3018e-01,  6.3104e-01,  4.7937e-01,  5.7112e-02,  5.6135e-02,
          1.4948e+00, -1.2191e-01],
        [ 1.3170e+00, -2.3265e+00,  1.6707e+00, -5.1279e+00,  4.3719e+00,
         -4.4573e+00, -1.0758e-01, -4.9402e+00,  2.7221e+00,  2.4992e+00,
         -6.0155e+00, -1.5609e-01, -1.5614e-01, -1.5609e-01, -1.5614e-01,
         -4.5397e+00, -4.8116e+00, -2.3803e-01, -1.4527e-01, -1.5614e-01,
         -5.6606e-01,  1.4001e+00],
        [-7.2743e+00, -5.7651e+00, -3.7946e+00, -1.2783e+00,  2.5498e+00,
         -4.7749e+00, -4.5452e+00,  5.1011e+00,  4.8863e+00, -8.4543e+00,
          4.9256e+00, -4.8305e-02, -4.8314e-02, -4.8305e-02, -4.8315e-02,
          3.5191e+00, -1.5101e+00,  6.7982e+00, -4.6189e-02, -4.8315e-02,
         -9.1502e+00, -4.4424e+00],
        [ 7.0403e-01,  1.1136e+00, -1.9173e-01,  6.6574e-01,  1.3687e-01,
          9.8147e-01,  5.5305e-01,  5.1477e-01,  4.8826e-01,  1.1522e-01,
          6.2199e-01,  5.7170e-02,  5.7166e-02,  5.7170e-02,  5.7166e-02,
          6.8702e-01,  6.3414e-01,  4.7918e-01,  5.8211e-02,  5.7166e-02,
          1.4457e+00, -1.7270e-01],
        [-4.9703e-01, -6.6081e-01, -1.0849e-01, -3.5679e-01, -3.3076e-01,
         -6.0124e-01, -3.5063e-01, -3.8902e-01, -1.6458e-01, -1.8235e-01,
         -6.5324e-01, -1.9494e-05, -2.0963e-05, -1.9472e-05, -2.1007e-05,
         -7.0665e-01, -3.6054e-01, -1.8860e-01,  1.8711e-04, -2.1034e-05,
         -1.3100e+00, -1.6553e-01],
        [ 1.0188e+00,  2.0042e+00,  6.9284e-01, -1.3505e+00,  2.2614e+00,
          1.8372e+00, -1.9814e+00, -1.2810e+00, -1.8494e+00,  7.2600e-01,
         -1.1908e-01,  6.3798e-02,  6.3790e-02,  6.3798e-02,  6.3790e-02,
         -3.6989e-02, -1.3995e+00, -1.5958e+00,  6.5363e-02,  6.3790e-02,
          2.9602e+00,  6.5064e-01],
        [-6.4706e+00, -5.2757e-01, -3.3826e+00, -4.9326e+00,  1.1490e+00,
         -3.3040e-02, -1.8821e+00,  3.0611e+00,  4.7004e+00, -8.9158e+00,
          3.1873e+00, -8.6414e-03, -8.6760e-03, -8.6408e-03, -8.6769e-03,
          2.8726e+00, -5.7719e+00,  7.8298e+00, -9.9602e-03, -8.6767e-03,
         -3.8864e+00, -4.0936e+00],
        [ 9.2414e+00,  4.4621e+00,  2.6564e+00,  6.2279e+00, -3.1422e-01,
          4.6093e+00,  4.2647e+00, -6.8752e+00, -5.2671e+00,  9.8909e+00,
         -7.5102e+00,  1.2107e-01,  1.2106e-01,  1.2107e-01,  1.2106e-01,
         -6.4070e+00,  7.2738e+00, -6.4607e+00,  1.1043e-01,  1.2106e-01,
          7.4638e+00,  4.0809e+00],
        [-5.3086e-01, -6.8922e-01, -5.7209e-02, -3.4612e-01, -2.7658e-01,
         -6.1441e-01, -3.3373e-01, -3.7492e-01, -1.9154e-01, -1.6501e-01,
         -6.0104e-01, -2.7156e-03, -2.7156e-03, -2.7156e-03, -2.7156e-03,
         -6.5759e-01, -3.4617e-01, -2.1074e-01, -2.8200e-03, -2.7156e-03,
         -1.3370e+00, -1.0927e-01],
        [ 8.2027e-01,  1.0665e+00, -8.0771e-02,  6.5657e-01,  2.9361e-01,
          9.5644e-01,  5.5041e-01,  5.8243e-01,  4.7647e-01,  2.1888e-01,
          7.4555e-01,  5.1941e-02,  5.1938e-02,  5.1941e-02,  5.1938e-02,
          8.0810e-01,  6.2506e-01,  4.7833e-01,  5.2783e-02,  5.1937e-02,
          1.5630e+00, -4.8654e-02],
        [ 7.4112e-01,  1.1133e+00, -1.5199e-01,  6.6684e-01,  1.8058e-01,
          9.8481e-01,  5.5956e-01,  5.3518e-01,  4.8840e-01,  1.4461e-01,
          6.6031e-01,  5.6849e-02,  5.6845e-02,  5.6849e-02,  5.6845e-02,
          7.2731e-01,  6.3573e-01,  4.8193e-01,  5.7850e-02,  5.6845e-02,
          1.5064e+00, -1.2931e-01],
        [-4.9727e-01, -6.6100e-01, -1.0818e-01, -3.5672e-01, -3.3050e-01,
         -6.0138e-01, -3.5050e-01, -3.8893e-01, -1.6470e-01, -1.8229e-01,
         -6.5299e-01, -3.7039e-05, -3.8484e-05, -3.7020e-05, -3.8545e-05,
         -7.0641e-01, -3.6044e-01, -1.8869e-01,  1.6820e-04, -3.8572e-05,
         -1.3101e+00, -1.6520e-01],
        [-5.6223e-01,  1.2482e+00, -1.4649e+00, -2.3896e-01, -7.6460e-02,
          9.8539e-01, -3.2125e-01, -1.4859e+00, -2.7750e+00, -1.9010e+00,
         -3.3034e-01,  7.8889e-02,  7.8902e-02,  7.8889e-02,  7.8903e-02,
         -1.3581e-01, -1.9421e-01, -2.9225e+00,  7.7703e-02,  7.8903e-02,
          5.5483e+00, -1.2255e+00],
        [ 1.6732e+00, -1.0188e+00,  3.9146e+00,  3.3813e+00, -1.3475e+00,
         -1.1458e+00,  1.8641e+00, -6.0076e+00, -1.7069e+00,  6.7482e+00,
         -1.6268e+00,  8.7496e-02,  8.7519e-02,  8.7496e-02,  8.7520e-02,
         -6.5711e-01,  4.3085e+00, -3.3867e+00,  7.7286e-02,  8.7521e-02,
          1.0471e+00,  4.3875e+00],
        [ 7.8001e-01,  1.0955e+00, -1.1516e-01,  6.6315e-01,  2.3388e-01,
          9.7461e-01,  5.5770e-01,  5.5787e-01,  4.8334e-01,  1.7929e-01,
          7.0228e-01,  5.5001e-02,  5.4997e-02,  5.5001e-02,  5.4997e-02,
          7.6844e-01,  6.3195e-01,  4.8061e-01,  5.5933e-02,  5.4997e-02,
          1.5434e+00, -8.8073e-02],
        [-4.9515e-01, -6.5927e-01, -1.1096e-01, -3.5742e-01, -3.3290e-01,
         -6.0022e-01, -3.5163e-01, -3.8976e-01, -1.6370e-01, -1.8288e-01,
         -6.5533e-01,  1.2137e-04,  1.1985e-04,  1.2138e-04,  1.1979e-04,
         -7.0854e-01, -3.6134e-01, -1.8790e-01,  3.3866e-04,  1.1978e-04,
         -1.3086e+00, -1.6816e-01],
        [-8.9195e+00, -7.9638e+00, -4.3314e+00, -2.0102e+00,  2.6197e+00,
         -6.8154e+00, -6.3697e+00,  6.2302e+00,  5.8013e+00, -1.0285e+01,
          5.7302e+00, -5.5338e-02, -5.5353e-02, -5.5338e-02, -5.5353e-02,
          4.2288e+00, -2.4347e+00,  8.9088e+00, -5.2102e-02, -5.5354e-02,
         -1.1283e+01, -5.2864e+00],
        [ 7.7412e-01,  1.0735e+00, -1.1599e-01,  6.5446e-01,  2.3099e-01,
          9.5648e-01,  5.4609e-01,  5.5396e-01,  4.7837e-01,  1.8366e-01,
          6.9320e-01,  5.4022e-02,  5.4019e-02,  5.4022e-02,  5.4018e-02,
          7.5612e-01,  6.2253e-01,  4.7606e-01,  5.4920e-02,  5.4018e-02,
          1.4992e+00, -8.8711e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.8919,  2.1221, -1.8270, -2.6556,  2.0445, -0.8769, -4.6253,  1.9677,
        -1.8272,  2.6088,  0.3772,  1.7675, -1.7628,  2.1734,  2.0524, -1.8268,
         2.0245, -1.0003,  2.1187, -1.8297, -5.1344,  2.0710], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-2.4348e-01, -5.5208e-01, -5.6378e-03, -6.2791e-02, -5.5163e-01,
         -1.6697e-02, -6.4648e-02, -5.5109e-01, -5.6369e-03, -4.8834e-01,
         -9.6074e-02, -4.7892e-01, -6.0790e-03, -5.5231e-01, -5.5166e-01,
         -5.6387e-03, -4.7064e-01, -2.7240e-01, -5.5204e-01, -5.6238e-03,
         -6.4638e-02, -5.5180e-01],
        [ 1.1503e+00,  5.3488e-01, -2.6635e-01,  3.1287e+00,  4.1587e-01,
         -2.6558e+00, -3.7484e-01,  3.5394e-01, -2.6657e-01, -9.5009e-01,
          3.3428e+00,  4.0463e+00, -2.1041e-01,  6.0045e-01,  4.1687e-01,
         -2.6615e-01,  4.7837e-01, -1.2141e-01,  4.5462e-01, -2.6965e-01,
         -2.2467e+00,  4.6602e-01],
        [-2.3651e-01, -5.3614e-01, -5.8023e-03, -6.0223e-02, -5.3570e-01,
         -1.8798e-02, -6.1609e-02, -5.3518e-01, -5.8013e-03, -4.7471e-01,
         -9.2880e-02, -4.6444e-01, -6.2220e-03, -5.3637e-01, -5.3574e-01,
         -5.8031e-03, -4.5691e-01, -2.6288e-01, -5.3611e-01, -5.7878e-03,
         -6.1502e-02, -5.3588e-01],
        [-2.0537e-01, -4.8716e-01, -3.6967e-03, -5.3952e-02, -4.8676e-01,
         -1.6188e-02, -5.6345e-02, -4.8628e-01, -3.6960e-03, -4.3224e-01,
         -8.4314e-02, -4.2116e-01, -4.0609e-03, -4.8737e-01, -4.8679e-01,
         -3.6973e-03, -4.1468e-01, -2.4660e-01, -4.8713e-01, -3.6859e-03,
         -5.6330e-02, -4.8692e-01],
        [ 8.9303e-01,  1.3179e+00,  2.5668e-01,  4.0453e+00,  1.3502e+00,
         -2.1485e+00, -4.1375e+00,  1.2637e+00,  2.5686e-01, -8.7832e-01,
          4.0327e+00, -3.7944e+00,  1.6747e-01,  1.4464e+00,  1.4146e+00,
          2.5651e-01,  1.9851e+00,  4.5643e-01,  1.4294e+00,  2.5982e-01,
         -5.7557e+00,  1.3001e+00],
        [-1.9358e-01, -4.3448e-01, -5.2507e-03, -4.5153e-02, -4.3411e-01,
         -1.9706e-02, -4.9856e-02, -4.3367e-01, -5.2500e-03, -3.9086e-01,
         -7.6921e-02, -3.7779e-01, -5.6272e-03, -4.3467e-01, -4.3414e-01,
         -5.2514e-03, -3.7156e-01, -2.1316e-01, -4.3445e-01, -5.2394e-03,
         -5.0060e-02, -4.3426e-01],
        [-2.2592e-01, -5.2050e-01, -5.3006e-03, -5.4530e-02, -5.2007e-01,
         -1.9129e-02, -5.5838e-02, -5.1956e-01, -5.2998e-03, -4.6185e-01,
         -8.6745e-02, -4.5128e-01, -5.6902e-03, -5.2072e-01, -5.2011e-01,
         -5.3014e-03, -4.4413e-01, -2.5907e-01, -5.2047e-01, -5.2876e-03,
         -5.5696e-02, -5.2024e-01],
        [-1.9068e-01, -4.3653e-01, -5.0720e-03, -4.6495e-02, -4.3615e-01,
         -1.9156e-02, -5.0065e-02, -4.3571e-01, -5.0713e-03, -3.9076e-01,
         -7.8073e-02, -3.7718e-01, -5.4861e-03, -4.3672e-01, -4.3618e-01,
         -5.0727e-03, -3.7151e-01, -2.2054e-01, -4.3650e-01, -5.0603e-03,
         -5.0097e-02, -4.3630e-01],
        [-2.1615e-01, -5.0242e-01, -4.5960e-03, -5.2054e-02, -5.0201e-01,
         -1.7653e-02, -5.4208e-02, -5.0151e-01, -4.5953e-03, -4.4819e-01,
         -8.3820e-02, -4.3762e-01, -4.9577e-03, -5.0263e-01, -5.0204e-01,
         -4.5967e-03, -4.3074e-01, -2.5076e-01, -5.0239e-01, -4.5845e-03,
         -5.4186e-02, -5.0217e-01],
        [-1.4753e-01, -3.7435e-01, -4.9351e-03, -3.1175e-02, -3.7397e-01,
         -1.5871e-02, -3.3122e-02, -3.7353e-01, -4.9341e-03, -3.4619e-01,
         -7.6822e-02, -3.2243e-01, -5.5052e-03, -3.7453e-01, -3.7399e-01,
         -4.9362e-03, -3.2264e-01, -1.7796e-01, -3.7430e-01, -4.9187e-03,
         -3.2529e-02, -3.7412e-01],
        [ 2.8305e-01,  1.4410e-01,  9.0765e-02, -2.6518e+00,  3.2245e-01,
          4.1138e+00,  5.3801e+00,  3.6467e-01,  9.0841e-02,  2.8789e+00,
         -2.9832e+00,  2.7082e+00,  3.5609e-02,  1.3487e-01,  3.6380e-01,
          9.0683e-02, -4.7284e-01,  1.5431e+00,  2.4228e-01,  9.2297e-02,
          7.5003e+00,  2.3334e-01],
        [-1.8930e-01, -4.6278e-01, -4.5012e-03, -5.0983e-02, -4.6238e-01,
         -1.7369e-02, -5.2209e-02, -4.6190e-01, -4.5004e-03, -4.1278e-01,
         -8.2344e-02, -3.9900e-01, -4.9137e-03, -4.6298e-01, -4.6241e-01,
         -4.5020e-03, -3.9330e-01, -2.3653e-01, -4.6274e-01, -4.4887e-03,
         -5.1917e-02, -4.6254e-01],
        [-2.4051e-01, -5.3969e-01, -6.2024e-03, -6.1542e-02, -5.3925e-01,
         -1.8250e-02, -6.2935e-02, -5.3873e-01, -6.2015e-03, -4.7629e-01,
         -9.4279e-02, -4.6621e-01, -6.6499e-03, -5.3992e-01, -5.3929e-01,
         -6.2033e-03, -4.5874e-01, -2.6608e-01, -5.3966e-01, -6.1879e-03,
         -6.2827e-02, -5.3943e-01],
        [-2.3511e-01, -5.2972e-01, -5.4614e-03, -5.9598e-02, -5.2929e-01,
         -1.5588e-02, -6.1878e-02, -5.2878e-01, -5.4606e-03, -4.6981e-01,
         -9.2219e-02, -4.6004e-01, -5.8938e-03, -5.2994e-01, -5.2933e-01,
         -5.4623e-03, -4.5238e-01, -2.6138e-01, -5.2969e-01, -5.4478e-03,
         -6.1907e-02, -5.2946e-01]], device='cuda:0'))])
xi:  [-206.5749]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 113.74188871248157
W_T_median: 114.32633551641106
W_T_pctile_5: -199.82730793958024
W_T_CVAR_5_pct: -435.57682675272684
Average q (qsum/M+1):  59.44145350302419
Optimal xi:  [-206.5749]
Expected(across Rb) median(across samples) p_equity:  0.05703896258492023
obj fun:  tensor(-1820.9039, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: MC_everything
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0      (22, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer       14           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0     (22, 14)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857
 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857
 0.07142857 0.07142857]
W_T_mean: 9438.548856964324
W_T_median: 5685.968426194605
W_T_pctile_5: 125.54011840495605
W_T_CVAR_5_pct: -416.7378752066434
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1813.049967419623
Current xi:  [111.579285]
objective value function right now is: -1813.049967419623
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1819.5349926148422
Current xi:  [130.43806]
objective value function right now is: -1819.5349926148422
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1820.4895131654484
Current xi:  [150.24356]
objective value function right now is: -1820.4895131654484
