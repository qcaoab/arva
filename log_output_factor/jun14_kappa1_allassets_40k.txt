Starting at: 
14-06-23_13:50

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 40000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = MC_everything
timeseries_basket['basket_desc'] = marc_test1_all_assets_longfactors
timeseries_basket['basket_columns'] = 
['Size_Lo30_real_ret', 'Value_Hi30_real_ret', 'Oprof_Hi30_real_ret', 'Inv_Lo30_real_ret', 'Mom_Hi30_real_ret', 'EP_Hi30_real_ret', 'Vol_Lo20_real_ret', 'Div_Hi30_real_ret', 'EQWFact_real_ret', 'T30_real_ret', 'T90_real_ret', 'B10_real_ret', 'VWD_real_ret', 'EWD_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = MC_everything
timeseries_basket['basket_desc'] = marc_test1_all_assets_longfactors
timeseries_basket['basket_columns'] = 
['Size_Lo30_nom_ret', 'Value_Hi30_nom_ret', 'Oprof_Hi30_nom_ret', 'Inv_Lo30_nom_ret', 'Mom_Hi30_nom_ret', 'EP_Hi30_nom_ret', 'Vol_Lo20_nom_ret', 'Div_Hi30_nom_ret', 'EQWFact_nom_ret', 'T30_nom_ret', 'T90_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'EWD_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.031411     0.013051
192608                    0.0319              0.0561  ...     0.028647     0.031002
192609                   -0.0173             -0.0071  ...     0.005787    -0.006499
192610                   -0.0294             -0.0355  ...    -0.028996    -0.034630
192611                   -0.0038              0.0294  ...     0.028554     0.024776

[5 rows x 15 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.036240    -0.011556
202209                   -0.0955             -0.0871  ...    -0.091324    -0.099903
202210                    0.0883              0.1486  ...     0.077403     0.049863
202211                   -0.0076              0.0462  ...     0.052365     0.028123
202212                   -0.0457             -0.0499  ...    -0.057116    -0.047241

[5 rows x 15 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'Oprof_Hi30_nom_ret_ind', 'Inv_Lo30_nom_ret_ind',
       'Mom_Hi30_nom_ret_ind', 'EP_Hi30_nom_ret_ind', 'Vol_Lo20_nom_ret_ind',
       'Div_Hi30_nom_ret_ind', 'EQWFact_nom_ret_ind', 'CPI_nom_ret_ind',
       'T30_nom_ret_ind', 'T90_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind', 'EWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
Oprof_Hi30_real_ret    0.003948
Inv_Lo30_real_ret      0.004513
Mom_Hi30_real_ret      0.011386
EP_Hi30_real_ret       0.007033
Vol_Lo20_real_ret      0.003529
Div_Hi30_real_ret      0.007888
EQWFact_real_ret       0.004508
T30_real_ret           0.000229
T90_real_ret           0.000501
B10_real_ret           0.001637
VWD_real_ret           0.006759
EWD_real_ret           0.009545
dtype: float64


timeseries_basket['data_df_stdev'] = 
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
Oprof_Hi30_real_ret    0.037444
Inv_Lo30_real_ret      0.037639
Mom_Hi30_real_ret      0.061421
EP_Hi30_real_ret       0.041390
Vol_Lo20_real_ret      0.030737
Div_Hi30_real_ret      0.056728
EQWFact_real_ret       0.037131
T30_real_ret           0.005227
T90_real_ret           0.005373
B10_real_ret           0.019258
VWD_real_ret           0.053610
EWD_real_ret           0.071360
dtype: float64


timeseries_basket['data_df_corr'] = 
                     Size_Lo30_real_ret  ...  EWD_real_ret
Size_Lo30_real_ret             1.000000  ...      0.977206
Value_Hi30_real_ret            0.908542  ...      0.919912
Oprof_Hi30_real_ret            0.433774  ...      0.462336
Inv_Lo30_real_ret              0.455237  ...      0.479661
Mom_Hi30_real_ret              0.903222  ...      0.912002
EP_Hi30_real_ret               0.476966  ...      0.506661
Vol_Lo20_real_ret              0.360014  ...      0.382411
Div_Hi30_real_ret              0.816292  ...      0.849068
EQWFact_real_ret               0.494572  ...      0.513359
T30_real_ret                   0.014412  ...      0.029084
T90_real_ret                   0.021968  ...      0.037909
B10_real_ret                   0.012916  ...      0.024853
VWD_real_ret                   0.865290  ...      0.907369
EWD_real_ret                   0.977206  ...      1.000000

[14 rows x 14 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      22  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      22  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 22)     True          22  
2     (22, 22)     True          22  
3      (22, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer      14       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      22  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      22  logistic_sigmoid   
3        obj.layers[3]        3  output_layer      14           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 22)     True          22  
2     (22, 22)     True          22  
3     (22, 14)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0      (22, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer       14           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0     (22, 14)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 4000, 'itbound_SGD_algorithms': 40000, 'nit_IterateAveragingStart': 36000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0      (22, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer       14           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0     (22, 14)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -8.5715,   6.4621],
        [ -8.3669,  -9.0182],
        [ -5.1223,   3.2445],
        [  8.0669,   4.1508],
        [-11.4280,  -6.0457],
        [ -6.4858,   4.0851],
        [  6.0779,   4.9126],
        [  9.1747,   0.7713],
        [ -8.3119,   6.0846],
        [ -8.3147,   6.0977],
        [ -8.4882,   6.6116],
        [ -8.6931,   5.7529],
        [  7.8819,   3.3034],
        [ -1.2974,   1.0853],
        [  5.4594,  -0.3288],
        [  5.2299,   5.2824],
        [-11.7613,  -7.4762],
        [  7.7973,   4.2640],
        [ -8.5799,   5.8746],
        [ -7.8535,   5.7472],
        [ -1.2944,   1.0845],
        [-21.4386,  -5.7294]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 4.8764, -5.7404, -3.0406, -9.5323, -3.7687, -2.3429, -8.4043, -9.1228,
         4.2727,  3.0356,  5.1757,  1.5461, -8.9288, -3.8883, -8.9159, -8.5427,
        -4.0692, -9.2749,  1.8685,  2.4405, -3.8889, -4.6902], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.2585e-01, -1.6547e-01,  1.7509e-03, -1.2930e-02, -1.1901e-01,
          1.1574e-03, -1.6502e-02, -2.2728e-03, -1.8736e-01, -6.9325e-02,
         -2.5298e-01, -1.2688e-02, -9.1088e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7960e-01, -1.2929e-02, -1.9678e-02, -4.9674e-02,
          1.2308e-02, -3.1398e-02],
        [-9.2453e+00,  1.2482e+01, -7.2066e-01,  7.0332e+00,  4.6981e+00,
         -1.2007e+00,  4.5523e+00,  6.2080e+00, -7.5674e+00, -3.0869e+00,
         -1.0021e+01, -1.8126e+00,  5.8578e+00, -1.6237e-01,  6.8999e-01,
          5.4520e+00,  8.4865e+00,  8.0426e+00, -2.2762e+00, -3.8127e+00,
         -1.9618e-01,  8.7135e+00],
        [-2.2586e-01, -1.6548e-01,  1.7510e-03, -1.2930e-02, -1.1901e-01,
          1.1575e-03, -1.6502e-02, -2.2751e-03, -1.8737e-01, -6.9327e-02,
         -2.5298e-01, -1.2689e-02, -9.1085e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7961e-01, -1.2929e-02, -1.9679e-02, -4.9675e-02,
          1.2307e-02, -3.1398e-02],
        [-9.6515e+00,  1.3280e+01, -9.1434e-01,  9.3871e+00,  5.7620e+00,
         -1.3190e+00,  6.4927e+00,  6.8045e+00, -8.3770e+00, -3.3906e+00,
         -1.0939e+01, -2.1716e+00,  6.5179e+00, -2.1870e-01,  2.0491e+00,
          4.1453e+00,  9.2888e+00,  7.8384e+00, -2.6898e+00, -4.4743e+00,
         -1.9003e-01,  9.7752e+00],
        [-7.9773e+00,  1.0315e+01,  2.5895e-01,  4.6608e+00,  4.9211e+00,
          1.7041e-02,  2.1882e+00,  6.8647e+00, -6.6734e+00, -4.8263e+00,
         -8.8073e+00, -2.8795e+00,  6.4311e+00,  1.6629e-01,  1.1111e+00,
          5.0411e+00,  7.6765e+00,  1.1081e+01, -3.5739e+00, -4.0927e+00,
          1.6082e-01,  8.5887e+00],
        [-7.6589e+00,  9.2592e+00,  7.9739e-02,  3.9257e+00,  4.5700e+00,
          1.0005e-01,  2.1053e+00,  4.6166e+00, -6.0015e+00, -3.8545e+00,
         -9.2106e+00, -1.7570e+00,  5.7099e+00, -1.2986e-02,  5.9836e-02,
          4.9159e+00,  7.0161e+00,  9.4703e+00, -2.3319e+00, -3.1763e+00,
         -1.3208e-02,  7.3285e+00],
        [-2.2581e-01, -1.6549e-01,  1.7511e-03, -1.2938e-02, -1.1903e-01,
          1.1578e-03, -1.6509e-02, -2.2839e-03, -1.8738e-01, -6.9338e-02,
         -2.5288e-01, -1.2692e-02, -9.1165e-03,  1.2292e-02,  1.9855e-02,
         -1.7662e-02, -1.7966e-01, -1.2937e-02, -1.9683e-02, -4.9682e-02,
          1.2307e-02, -3.1400e-02],
        [-2.2584e-01, -1.6546e-01,  1.7508e-03, -1.2931e-02, -1.1901e-01,
          1.1573e-03, -1.6502e-02, -2.2704e-03, -1.8735e-01, -6.9322e-02,
         -2.5298e-01, -1.2688e-02, -9.1089e-03,  1.2292e-02,  1.9856e-02,
         -1.7657e-02, -1.7959e-01, -1.2929e-02, -1.9678e-02, -4.9672e-02,
          1.2308e-02, -3.1398e-02],
        [-2.2586e-01, -1.6548e-01,  1.7510e-03, -1.2930e-02, -1.1901e-01,
          1.1575e-03, -1.6502e-02, -2.2752e-03, -1.8737e-01, -6.9327e-02,
         -2.5298e-01, -1.2689e-02, -9.1085e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7961e-01, -1.2929e-02, -1.9679e-02, -4.9675e-02,
          1.2307e-02, -3.1398e-02],
        [-2.2585e-01, -1.6547e-01,  1.7509e-03, -1.2930e-02, -1.1901e-01,
          1.1575e-03, -1.6502e-02, -2.2730e-03, -1.8736e-01, -6.9325e-02,
         -2.5298e-01, -1.2688e-02, -9.1088e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7960e-01, -1.2929e-02, -1.9678e-02, -4.9674e-02,
          1.2308e-02, -3.1398e-02],
        [-7.4230e+00,  9.2761e+00,  8.2691e-02,  4.6883e+00,  4.4150e+00,
          1.2211e-01,  2.9895e+00,  3.5408e+00, -6.2114e+00, -3.5923e+00,
         -9.4198e+00, -1.8627e+00,  4.7011e+00, -1.9985e-02, -6.0281e-02,
          5.2032e+00,  7.0797e+00,  8.3206e+00, -2.3235e+00, -3.3515e+00,
         -1.9947e-02,  7.4397e+00],
        [ 7.9326e+00, -1.0706e+01,  4.0837e-01, -9.2089e+00, -5.7725e+00,
          6.6403e-01, -4.8629e+00, -5.8385e+00,  6.6214e+00,  5.0109e+00,
          9.6232e+00,  3.3999e+00, -5.8456e+00, -1.3669e-01, -1.4453e+00,
         -3.3601e+00, -8.4017e+00, -9.5444e+00,  3.9084e+00,  4.3412e+00,
         -1.3169e-01, -9.7519e+00],
        [-2.2583e-01, -1.6545e-01,  1.7507e-03, -1.2931e-02, -1.1900e-01,
          1.1572e-03, -1.6503e-02, -2.2678e-03, -1.8733e-01, -6.9320e-02,
         -2.5297e-01, -1.2687e-02, -9.1093e-03,  1.2293e-02,  1.9856e-02,
         -1.7657e-02, -1.7958e-01, -1.2930e-02, -1.9677e-02, -4.9670e-02,
          1.2308e-02, -3.1399e-02],
        [-2.2585e-01, -1.6547e-01,  1.7509e-03, -1.2931e-02, -1.1901e-01,
          1.1575e-03, -1.6502e-02, -2.2732e-03, -1.8736e-01, -6.9325e-02,
         -2.5298e-01, -1.2688e-02, -9.1089e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7960e-01, -1.2929e-02, -1.9678e-02, -4.9674e-02,
          1.2308e-02, -3.1398e-02],
        [ 6.5891e-01,  3.0868e-01,  8.3566e-03,  3.5747e-02,  2.1159e-01,
          5.3475e-03,  5.7464e-02,  2.6629e-01,  5.5272e-01,  2.6083e-01,
          7.4008e-01,  8.5701e-02,  7.1654e-02,  3.7717e-02,  5.6900e-02,
          8.1005e-02,  2.9146e-01,  3.6840e-02,  1.1119e-01,  2.1066e-01,
          3.7716e-02,  1.3910e-01],
        [-2.2585e-01, -1.6548e-01,  1.7509e-03, -1.2930e-02, -1.1901e-01,
          1.1575e-03, -1.6502e-02, -2.2733e-03, -1.8736e-01, -6.9325e-02,
         -2.5298e-01, -1.2688e-02, -9.1087e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7961e-01, -1.2929e-02, -1.9678e-02, -4.9674e-02,
          1.2307e-02, -3.1398e-02],
        [-7.5648e+00,  9.6323e+00,  3.4567e-02,  4.2023e+00,  4.4635e+00,
         -1.3328e-01,  2.0401e+00,  5.3264e+00, -6.2784e+00, -3.7048e+00,
         -9.3470e+00, -2.0562e+00,  5.8618e+00, -3.3635e-02,  2.0363e-01,
          5.0433e+00,  7.2814e+00,  9.5323e+00, -2.4148e+00, -3.3678e+00,
         -3.5727e-02,  7.7623e+00],
        [-2.2586e-01, -1.6548e-01,  1.7510e-03, -1.2930e-02, -1.1901e-01,
          1.1575e-03, -1.6502e-02, -2.2750e-03, -1.8737e-01, -6.9327e-02,
         -2.5298e-01, -1.2689e-02, -9.1085e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7961e-01, -1.2929e-02, -1.9679e-02, -4.9675e-02,
          1.2307e-02, -3.1398e-02],
        [-2.2585e-01, -1.6548e-01,  1.7510e-03, -1.2930e-02, -1.1901e-01,
          1.1575e-03, -1.6502e-02, -2.2744e-03, -1.8737e-01, -6.9326e-02,
         -2.5298e-01, -1.2689e-02, -9.1087e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7961e-01, -1.2929e-02, -1.9679e-02, -4.9675e-02,
          1.2307e-02, -3.1398e-02],
        [-2.2585e-01, -1.6547e-01,  1.7509e-03, -1.2931e-02, -1.1901e-01,
          1.1574e-03, -1.6502e-02, -2.2722e-03, -1.8735e-01, -6.9324e-02,
         -2.5298e-01, -1.2688e-02, -9.1089e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7960e-01, -1.2929e-02, -1.9678e-02, -4.9673e-02,
          1.2308e-02, -3.1398e-02],
        [ 6.6450e-01,  3.1201e-01,  8.4127e-03,  3.4696e-02,  2.1235e-01,
          5.3877e-03,  5.6376e-02,  2.6891e-01,  5.5678e-01,  2.6166e-01,
          7.4701e-01,  8.5769e-02,  7.1346e-02,  3.8211e-02,  5.7275e-02,
          8.0058e-02,  2.9347e-01,  3.5786e-02,  1.1132e-01,  2.1139e-01,
          3.8209e-02,  1.3913e-01],
        [-2.2586e-01, -1.6548e-01,  1.7510e-03, -1.2930e-02, -1.1901e-01,
          1.1575e-03, -1.6502e-02, -2.2753e-03, -1.8737e-01, -6.9327e-02,
         -2.5298e-01, -1.2689e-02, -9.1085e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7961e-01, -1.2929e-02, -1.9679e-02, -4.9675e-02,
          1.2307e-02, -3.1398e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.4313,  0.0078, -2.4313,  0.0873, -0.2625, -0.5234, -2.4313, -2.4314,
        -2.4313, -2.4313, -0.4369, -0.2571, -2.4314, -2.4313,  4.5769, -2.4313,
        -0.4006, -2.4313, -2.4313, -2.4313,  4.6668, -2.4313], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0334, -10.4940,   0.0334, -15.8008,  -9.7979,  -6.9452,   0.0334,
           0.0334,   0.0334,   0.0334,  -7.0927,  14.9535,   0.0334,   0.0334,
           4.2936,   0.0334,  -7.0565,   0.0334,   0.0334,   0.0334,   4.7631,
           0.0334]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 1.0813e+01,  2.9656e+00],
        [ 5.5664e-01,  1.1757e+01],
        [ 1.8618e+01,  2.3664e+00],
        [ 1.0637e+01,  1.5273e-02],
        [-1.4941e+01, -2.2181e+00],
        [-1.2849e+00,  1.3570e+00],
        [-1.2800e+00,  1.3116e+00],
        [-1.7122e+00,  6.5637e-01],
        [-2.0284e+00,  2.5259e-01],
        [ 9.3477e+00, -5.2188e-01],
        [-1.9314e+00,  2.3251e-01],
        [-1.8788e+00,  4.0167e-01],
        [-1.0523e+01, -1.6251e+00],
        [-8.3398e+00,  5.3496e+00],
        [-7.4426e+00,  1.0247e+01],
        [-1.2446e+01, -2.8588e+00],
        [-1.4240e+01, -3.1016e+00],
        [-1.0901e+01, -2.0063e+00],
        [-1.0106e+01, -2.4687e+00],
        [ 3.1771e+00,  1.1429e+01],
        [ 1.0907e+01,  2.2476e+00],
        [ 1.1102e+01,  1.0847e+01]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-5.5365,  7.0663,  2.9709, -9.6018, -1.5848, -4.5127, -4.5185, -4.9090,
        -4.9531, -9.4875, -4.9847, -4.9560,  5.4223,  3.0397,  6.7011, -3.5168,
        -2.9913,  3.2472,  0.4201,  7.7652, -4.3018,  6.4326], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 6.1957e-01, -3.4634e+00, -1.7479e+00,  1.1905e+00,  1.2460e-02,
          1.1948e-01,  1.3226e-01,  4.3984e-02, -1.1598e-02,  1.0215e+00,
         -6.1706e-03,  2.4414e-03, -2.5313e+00,  1.1746e+00, -7.8526e-01,
         -9.2734e-03,  2.7572e-03, -2.7562e+00, -1.8971e+00, -2.6434e+00,
         -4.2419e-01,  3.0293e-02],
        [-4.6968e-01, -1.5418e-01, -2.2179e+00, -4.7229e-01, -1.4815e-02,
         -2.6861e-02, -2.4100e-02,  6.5293e-03, -1.5585e-02, -3.8766e-01,
         -1.6068e-02, -1.1227e-02, -1.0214e+00, -1.1716e-01, -1.0616e-01,
         -5.8386e-03, -7.9498e-03, -5.2026e-01, -1.9652e-01, -4.4158e-01,
         -1.2068e+00, -1.2289e+00],
        [-9.0043e-01, -3.1328e-01, -2.4749e+00, -1.0214e+00, -5.5072e-03,
         -2.2407e-02, -1.9453e-02, -7.7448e-03,  4.4606e-03, -9.3610e-01,
          4.1020e-03,  2.5876e-03, -1.2488e+00, -4.6992e-02, -9.2699e-02,
         -2.2626e-03, -3.5666e-03, -6.7176e-01, -2.4800e-01, -8.7567e-01,
         -1.3802e+00, -7.7155e-01],
        [-4.7184e-01, -1.5540e-01, -2.2143e+00, -4.7815e-01, -1.4473e-02,
         -2.7281e-02, -2.4476e-02,  6.5809e-03, -1.5495e-02, -3.9563e-01,
         -1.5975e-02, -1.1107e-02, -1.0396e+00, -1.1822e-01, -1.0605e-01,
         -5.8775e-03, -7.9716e-03, -5.2621e-01, -1.9589e-01, -4.4421e-01,
         -1.2127e+00, -1.2341e+00],
        [-4.8450e-01, -1.6921e-01, -2.2399e+00, -4.9569e-01, -1.4212e-02,
         -2.5472e-02, -2.2971e-02,  5.3544e-03, -1.0943e-02, -4.2767e-01,
         -1.1482e-02, -7.9784e-03, -1.0434e+00, -1.0826e-01, -9.0719e-02,
         -6.2509e-03, -8.3116e-03, -5.2892e-01, -2.1129e-01, -4.5420e-01,
         -1.2241e+00, -1.0613e+00],
        [ 3.0119e-01,  9.1885e-01, -1.5837e+00, -8.5542e-01, -1.1953e-02,
         -1.9274e+00, -1.8752e+00, -1.4092e+00, -5.5076e-01,  1.0780e+00,
         -4.5548e-01, -9.9188e-01, -3.9286e+00,  1.1415e+00,  2.2999e+00,
         -1.2239e-02, -1.3323e-02, -2.7068e+00, -9.7625e-01,  4.2057e-01,
         -1.0808e+00, -1.2393e+00],
        [ 2.4582e-01, -1.1721e+01,  5.7285e-01, -6.6182e+00,  3.3458e+00,
          1.7362e-01,  1.7999e-01,  2.0080e-01,  2.8786e-01, -4.5146e+00,
          2.9196e-01,  1.9339e-01,  3.6973e+00, -2.2846e+00, -8.2839e+00,
          2.5629e+00,  2.7897e+00,  3.3397e+00,  3.6998e+00, -1.2216e+01,
          1.0269e-01, -7.8724e+00],
        [-2.7710e+00,  3.8384e+00, -3.8614e+00, -5.2497e+00,  1.4955e-01,
         -5.1488e-01, -5.0931e-01, -4.0831e-01, -1.3933e-01, -4.9036e+00,
         -1.0681e-01, -2.8754e-01,  2.6586e+00,  3.1018e+00,  6.0454e+00,
          5.3721e-01,  5.1709e-01,  2.4000e+00,  1.7467e+00,  2.8980e+00,
         -1.8615e+00,  1.3298e+00],
        [ 5.3996e+00,  1.7695e+01,  2.7980e+00,  6.7002e+00, -6.8933e+00,
          1.9716e-02,  1.5955e-02,  1.6031e-02, -1.9886e-01,  4.1219e+00,
         -1.8883e-01,  2.3240e-02, -8.0362e+00,  3.0849e+00,  1.4710e+01,
         -3.3033e+00, -3.8057e+00, -7.4740e+00, -6.2744e+00,  1.9113e+01,
          5.0855e+00,  2.1580e+01],
        [-4.6054e-01, -1.5864e-01, -2.1828e+00, -4.7962e-01, -1.3132e-02,
         -2.7434e-02, -2.4527e-02,  7.6509e-03, -1.3661e-02, -3.9854e-01,
         -1.4154e-02, -9.5939e-03, -1.0591e+00, -1.2246e-01, -9.9910e-02,
         -5.2767e-03, -7.3107e-03, -5.5050e-01, -2.1282e-01, -4.5037e-01,
         -1.2105e+00, -1.2384e+00],
        [-9.3210e-01, -5.0637e-01, -2.5650e+00, -1.2297e+00,  5.1007e-03,
         -1.3266e-02, -1.0812e-02, -6.5037e-03, -1.3910e-03, -1.1288e+00,
         -1.7836e-03, -1.9456e-03, -7.9371e-01, -5.5349e-02, -8.6527e-02,
          5.2059e-03,  5.8080e-03, -8.8163e-01, -4.5943e-01, -1.6628e+00,
         -1.1149e+00, -5.6400e-01],
        [-2.8460e+00, -1.8825e+01, -9.7046e+00, -3.0631e+00,  6.2381e+00,
          2.2284e-02,  2.2216e-02,  3.8058e-02,  1.2254e-02, -4.6393e+00,
          2.0356e-03,  4.3669e-02,  7.6060e+00, -3.1393e+00, -1.1899e+01,
          2.9927e+00,  3.5772e+00,  7.5127e+00,  7.0486e+00, -2.2653e+01,
         -1.0357e+01, -2.5603e+01],
        [-4.7448e-01, -1.5413e-01, -2.2021e+00, -4.7761e-01, -1.5017e-02,
         -2.6936e-02, -2.4189e-02,  6.2971e-03, -1.5709e-02, -3.9518e-01,
         -1.6194e-02, -1.1365e-02, -1.0353e+00, -1.1675e-01, -1.0676e-01,
         -6.0131e-03, -8.0879e-03, -5.2030e-01, -1.9318e-01, -4.4328e-01,
         -1.2192e+00, -1.2259e+00],
        [-6.9231e-01,  3.0014e+00,  2.7786e+00, -1.3175e+00,  3.0620e-04,
         -1.3308e-01, -1.3059e-01, -4.6677e-03, -1.5150e-02, -1.3574e+00,
         -1.8928e-02, -2.6791e-03,  2.1880e+00, -4.4040e-01,  8.1953e-01,
         -1.8323e-02, -1.1176e-02,  1.4720e+00,  4.3629e-01,  2.4554e+00,
          8.6002e-02, -2.4341e-01],
        [ 3.0175e+00, -1.5929e-01,  3.1082e+00,  1.6836e+00, -3.7655e+00,
          1.7569e-01,  1.8810e-01,  2.3150e-01, -5.4313e-01,  3.8072e+00,
         -5.3327e-01,  1.6719e-01,  6.0672e-02, -2.5586e+00, -3.6155e+00,
         -4.5190e+00, -4.8662e+00,  1.6404e-01, -1.7814e-01, -2.9936e-01,
          2.7050e+00, -1.8862e+00],
        [-6.1343e-01, -2.0997e-01, -2.3786e+00, -7.6186e-01, -1.9840e-02,
         -3.2273e-02, -3.0458e-02, -4.7454e-03,  5.8161e-03, -6.4208e-01,
          6.0534e-03,  3.5540e-03, -1.0330e+00, -6.3106e-02, -1.0050e-01,
         -7.6629e-03, -1.3412e-02, -4.8643e-01, -1.8407e-01, -5.2743e-01,
         -1.3866e+00, -9.2565e-01],
        [ 2.3232e-01,  3.1081e+00,  2.1861e+00,  1.7924e+00,  8.3474e-01,
         -4.2057e-01, -3.7750e-01, -8.6959e-02, -6.3586e-02,  1.4179e+00,
         -6.1088e-02, -4.4329e-02,  1.9436e-01, -4.4091e-01,  1.1123e+00,
          4.9760e-01,  7.0823e-01, -1.6870e-01, -3.9429e-01, -1.1351e+00,
          1.7979e-02, -1.2019e+00],
        [-9.3911e-03, -3.8280e+00, -3.1866e-01, -2.0025e+00, -3.0075e-02,
          4.3973e-02,  5.3990e-02,  1.0353e-01,  1.4874e-01, -1.8588e+00,
          1.5683e-01,  1.3914e-01, -1.0725e-01, -2.1520e-01, -2.7783e-01,
          5.4541e-02,  7.3731e-03, -1.3880e+00, -8.0744e-01, -5.4071e+00,
         -9.3933e-01,  6.6525e-02],
        [-2.0923e+00,  1.6783e+00,  2.1031e+00, -6.6800e+00,  1.9109e-04,
         -8.2756e-01, -7.5222e-01, -4.3503e-01, -1.2713e-01, -3.5556e+00,
         -7.5366e-02, -3.3382e-01,  9.4887e+00, -1.2212e+00,  5.7869e+00,
         -6.4339e-04, -3.5999e-03,  9.1765e+00,  6.6382e+00,  6.8362e-01,
          2.7993e-01, -2.2562e+00],
        [-5.5183e+00, -1.5095e+01, -2.8824e-01, -4.9346e+00,  5.3321e+00,
          1.5448e-01,  1.5735e-01,  1.6104e-01,  5.9928e-01, -2.7075e+00,
          5.5855e-01,  1.3923e-01,  3.0535e+00,  8.4196e-01, -6.5453e+00,
          4.6053e+00,  5.0922e+00,  3.6158e+00,  3.8865e+00, -1.4997e+01,
         -2.8857e+00, -4.2056e+00],
        [ 4.0156e-01, -1.0952e+00, -1.5738e+00, -4.3123e-01, -8.5229e-03,
         -6.6402e-01, -6.0136e-01, -2.4497e-01, -5.3144e-02, -2.8255e-01,
         -5.5575e-02, -9.1214e-02, -2.1121e+00,  1.5978e+00,  1.4435e+00,
         -1.0813e-03, -1.8711e-03, -2.1573e+00, -8.6345e-01, -1.7321e+00,
         -7.5512e-01,  5.2300e-01],
        [-1.9858e+00,  2.9033e+00, -2.5269e+00, -2.1485e+00,  2.2962e+00,
         -1.4441e+00, -1.3739e+00, -7.3427e-01,  6.2891e-02,  2.1944e-01,
          6.1358e-02, -2.6440e-01,  7.4516e-02,  1.7016e+00,  3.0581e+00,
          1.5196e+00,  1.8766e+00,  6.5632e-01,  1.1298e+00,  1.3664e+00,
         -2.7325e+00, -7.5516e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.8466, -2.2256, -2.5068, -2.1943, -2.2514, -1.8834,  0.6255, -4.9456,
        -2.2024, -2.2158, -2.5964,  0.0491, -2.2135,  3.0825,  3.0734, -2.4047,
         2.5296, -1.0040,  2.4991, -0.5295, -2.0766, -2.4070], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.5234e-02,  7.2296e-05, -1.5502e-04,  5.9106e-05,  9.2680e-05,
         -2.2228e-02, -7.7352e-03, -1.5990e-01, -1.5103e+00,  6.1218e-05,
         -1.5306e-04, -1.3007e-05,  6.3929e-05, -1.5060e+00, -1.5008e+00,
         -2.1007e-04, -1.4999e+00, -5.4381e-03, -6.9054e-01, -3.7369e-03,
         -4.7102e-03, -3.5212e-02],
        [-8.7292e-03, -1.4222e-03,  7.6021e-04, -1.3898e-03, -1.5771e-03,
         -1.3307e-02, -5.1882e-03, -1.0878e-01, -8.2132e-01, -1.3260e-03,
          8.8522e-04, -2.6449e-04, -1.4244e-03, -8.2289e-01, -8.1366e-01,
          5.1250e-04, -8.1735e-01,  1.2221e-03, -3.1415e-01, -6.8756e-03,
          4.4122e-03, -3.5269e-02],
        [-2.1229e-02, -5.1093e-04, -3.4543e-04, -5.1941e-04, -5.4632e-04,
         -1.8385e-02, -9.4420e-03, -1.3731e-01, -1.4218e+00, -5.2075e-04,
         -3.2211e-04,  2.3184e-05, -5.1689e-04, -1.4173e+00, -1.4127e+00,
         -3.8173e-04, -1.4125e+00, -6.1909e-03, -6.2055e-01, -3.7929e-03,
         -7.1839e-03, -2.7972e-02],
        [-9.4172e-03, -2.1938e-04, -2.2482e-04, -2.0759e-04, -2.8288e-04,
         -1.6622e-02, -7.4112e-03, -1.3137e-01, -1.2415e+00, -2.0252e-04,
         -2.1593e-04, -7.4338e-05, -2.1553e-04, -1.2385e+00, -1.2343e+00,
         -1.4845e-04, -1.2328e+00, -5.0361e-03, -5.7108e-01, -3.7365e-03,
         -7.0540e-03, -2.5170e-02],
        [-1.0969e+00,  7.0714e-02,  2.8031e-01,  4.2693e-02,  2.8583e-01,
          5.4877e-01,  3.6379e+00, -1.3225e+00, -3.0535e+00,  2.8854e-01,
          9.2331e-01, -1.4907e+01,  2.7428e-02,  3.6135e+00,  3.4661e+00,
         -2.3553e-02,  3.3911e+00,  1.2464e+00,  2.0600e+00,  5.6362e+00,
         -4.8358e-01,  4.4222e-01],
        [-1.1325e-02, -1.4138e-04,  1.4500e-04, -1.5008e-04, -1.0196e-03,
         -8.7395e-03, -5.0212e-04, -1.4891e-01, -9.6424e-01, -1.5847e-04,
          8.4481e-05, -6.7668e-04, -1.4540e-04, -9.6261e-01, -9.4804e-01,
         -9.4017e-05, -9.5700e-01,  4.4294e-03, -4.5741e-01, -5.5136e-03,
          2.2288e-03, -3.6784e-02],
        [-2.4271e-02, -7.7962e-04, -2.2040e-04, -7.9007e-04, -5.4691e-04,
         -1.4880e-02, -9.2166e-03, -1.1032e-01, -1.2944e+00, -7.8678e-04,
         -1.4609e-04, -3.3668e-05, -7.9327e-04, -1.2897e+00, -1.2879e+00,
         -3.7474e-04, -1.2861e+00, -9.7262e-03, -5.3746e-01, -3.8536e-03,
         -1.0548e-02, -2.4508e-02],
        [ 3.9237e+00, -6.2937e-02,  8.2976e-01, -8.9553e-02,  1.8773e-01,
          2.5280e+00,  4.4063e+00, -5.1176e-01,  2.7121e+00,  1.5038e-01,
          2.1898e+00, -6.3975e+00, -1.0291e-01,  9.8757e-01,  1.1535e+00,
          5.7770e-02,  9.4780e-01,  2.9512e+00,  2.3504e+00,  1.6411e+00,
          3.1498e+00,  1.2727e-01],
        [-1.4494e-02, -1.9430e-04, -1.7724e-04, -2.0127e-04, -2.0117e-04,
         -1.5493e-02, -8.1680e-03, -1.3258e-01, -1.3345e+00, -2.0053e-04,
         -1.8098e-04, -1.8463e-05, -1.9899e-04, -1.3303e+00, -1.3262e+00,
         -2.7675e-04, -1.3255e+00, -6.2074e-03, -5.8710e-01, -4.1919e-03,
         -7.5895e-03, -2.9301e-02],
        [-2.1892e-02, -3.5780e-04, -4.8405e-04, -3.6317e-04, -3.6356e-04,
         -2.3657e-02, -1.6951e-02, -9.6588e-02, -1.1849e+00, -3.5150e-04,
         -4.0838e-04, -1.7243e-05, -3.6464e-04, -1.1843e+00, -1.1795e+00,
         -3.3804e-04, -1.1790e+00, -7.4055e-03, -5.7283e-01, -5.4422e-03,
         -1.0076e-02, -3.1398e-02],
        [-2.3028e+00, -2.1890e-02, -8.3502e-03, -5.3786e-02,  9.5685e-02,
         -8.9290e-01, -1.4767e+01,  7.5055e+00,  3.0752e+00,  1.1812e-01,
         -7.0438e-02,  2.1906e-03, -5.7019e-02, -7.6423e-01, -1.3833e+00,
         -7.3209e-03, -7.3795e-01, -2.2229e+00,  1.8670e+00, -1.4910e+01,
         -1.6032e+00,  1.4609e+00],
        [ 2.4319e+00,  1.1747e-01, -1.6596e-01,  8.9778e-02,  3.1240e-01,
         -1.2941e+00, -9.2455e-01, -5.3120e-01,  5.2163e+00,  3.3676e-01,
         -2.0670e-01,  2.2187e+01,  7.3275e-02,  3.7788e-01,  7.5700e-01,
         -1.2275e-01,  4.8364e-01, -2.5823e+00,  1.5965e+00, -2.2668e+00,
         -6.5383e-01, -2.8611e+00],
        [-2.1115e-02, -3.0744e-04, -1.4460e-04, -2.9289e-04, -3.5131e-04,
         -2.2539e-02, -1.0971e-02, -1.5511e-01, -1.5086e+00, -2.8377e-04,
         -1.2594e-04,  2.8039e-05, -3.0274e-04, -1.5044e+00, -1.4994e+00,
         -3.7557e-04, -1.4986e+00, -6.5951e-03, -6.8458e-01, -3.5928e-03,
         -7.8039e-03, -3.0722e-02],
        [-1.4210e-02,  2.7210e-04, -1.6876e-04,  2.6752e-04,  2.9104e-04,
         -2.2712e-02, -5.7647e-03, -1.5704e-01, -1.4740e+00,  2.7486e-04,
         -1.6774e-04, -4.4364e-05,  2.6776e-04, -1.4699e+00, -1.4648e+00,
         -1.4749e-04, -1.4640e+00, -5.0059e-03, -6.6920e-01, -3.9010e-03,
         -5.3125e-03, -3.3932e-02]], device='cuda:0'))])
loaded xi:  855.6613
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857
 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857
 0.07142857 0.07142857]
W_T_mean: 8576.841924009083
W_T_median: 5042.380921625826
W_T_pctile_5: 103.64169553244135
W_T_CVAR_5_pct: -371.9331258181762
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2255.4149209885923
Current xi:  [883.6071]
objective value function right now is: -2255.4149209885923
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -2263.237650624174
Current xi:  [910.0986]
objective value function right now is: -2263.237650624174
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -2269.1890482440117
Current xi:  [936.9988]
objective value function right now is: -2269.1890482440117
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -2282.6951251797987
Current xi:  [962.46136]
objective value function right now is: -2282.6951251797987
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -2284.3988014087363
Current xi:  [987.6105]
objective value function right now is: -2284.3988014087363
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [1012.38354]
objective value function right now is: -2281.547476895004
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -2291.726860713397
Current xi:  [1035.579]
objective value function right now is: -2291.726860713397
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -2296.1317762160534
Current xi:  [1057.9753]
objective value function right now is: -2296.1317762160534
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [1080.8723]
objective value function right now is: -2290.162601779171
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -2310.16473790385
Current xi:  [1101.6467]
objective value function right now is: -2310.16473790385
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -2320.3956342530955
Current xi:  [1122.9432]
objective value function right now is: -2320.3956342530955
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [1143.505]
objective value function right now is: -2315.150110935403
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [1161.536]
objective value function right now is: -2308.7501925057554
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [1178.9172]
objective value function right now is: -2315.822745350131
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [1195.6107]
objective value function right now is: -2317.778721061975
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [1209.2003]
objective value function right now is: -2318.260985350947
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -2334.5775627788903
Current xi:  [1223.473]
objective value function right now is: -2334.5775627788903
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [1235.2797]
objective value function right now is: -2327.063016068845
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [1247.0979]
objective value function right now is: -2328.967916864779
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2335.8279934607167
Current xi:  [1257.7899]
objective value function right now is: -2335.8279934607167
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [1267.1287]
objective value function right now is: -2323.322590721856
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -2336.169123243329
Current xi:  [1276.2264]
objective value function right now is: -2336.169123243329
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2338.5675597115523
Current xi:  [1282.1313]
objective value function right now is: -2338.5675597115523
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [1290.4115]
objective value function right now is: -2334.3589355184704
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [1296.4294]
objective value function right now is: -2332.75307157201
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [1300.6637]
objective value function right now is: -2322.7361090086374
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [1305.5363]
objective value function right now is: -2322.259345606512
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [1309.2607]
objective value function right now is: -2333.267941187386
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [1314.9191]
objective value function right now is: -2325.409443590823
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [1317.9384]
objective value function right now is: -2333.249527826822
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -2341.4659053399973
Current xi:  [1321.8802]
objective value function right now is: -2341.4659053399973
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [1323.8281]
objective value function right now is: -2316.224700609938
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [1327.2162]
objective value function right now is: -2322.505784615806
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [1326.877]
objective value function right now is: -2310.2835699257366
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [1328.2875]
objective value function right now is: -2334.5887828654245
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2342.092508664832
Current xi:  [1329.1617]
objective value function right now is: -2342.092508664832
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -2344.7931696212386
Current xi:  [1329.6752]
objective value function right now is: -2344.7931696212386
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [1330.379]
objective value function right now is: -2341.43391607669
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [1331.4127]
objective value function right now is: -2342.2755438842587
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [1331.6649]
objective value function right now is: -2343.1824443460655
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -2347.329757663948
Current xi:  [1332.4003]
objective value function right now is: -2347.329757663948
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -2348.196010386015
Current xi:  [1333.522]
objective value function right now is: -2348.196010386015
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [1334.5104]
objective value function right now is: -2341.9058255874124
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [1334.8138]
objective value function right now is: -2345.294975337847
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [1335.6262]
objective value function right now is: -2347.0065781153367
new min fval from sgd:  -2348.3006504947352
new min fval from sgd:  -2348.4730469039278
new min fval from sgd:  -2348.56439194101
new min fval from sgd:  -2348.6372413130466
new min fval from sgd:  -2348.69527289729
new min fval from sgd:  -2348.916032729001
new min fval from sgd:  -2349.1033040056423
new min fval from sgd:  -2349.188786070947
new min fval from sgd:  -2349.2957626201555
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [1336.4177]
objective value function right now is: -2347.821030219044
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [1336.6948]
objective value function right now is: -2348.298210401215
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [1337.1178]
objective value function right now is: -2345.579352298553
new min fval from sgd:  -2349.3131762491585
new min fval from sgd:  -2349.3296241816247
new min fval from sgd:  -2349.4161150935006
new min fval from sgd:  -2349.425879509976
new min fval from sgd:  -2349.515053303575
new min fval from sgd:  -2349.545632943069
new min fval from sgd:  -2349.5571021126557
new min fval from sgd:  -2349.5585670943433
new min fval from sgd:  -2349.5684752400753
new min fval from sgd:  -2349.569285414026
new min fval from sgd:  -2349.5722627811683
new min fval from sgd:  -2349.614738743191
new min fval from sgd:  -2349.6443612825824
new min fval from sgd:  -2349.6752914028725
new min fval from sgd:  -2349.6951674945235
new min fval from sgd:  -2349.7061308202324
new min fval from sgd:  -2349.7134509441703
new min fval from sgd:  -2349.7359980353044
new min fval from sgd:  -2349.749627135836
new min fval from sgd:  -2349.776041916604
new min fval from sgd:  -2349.7966820243296
new min fval from sgd:  -2349.8246874136707
new min fval from sgd:  -2349.839377590353
new min fval from sgd:  -2349.847463506627
new min fval from sgd:  -2349.8493143765936
new min fval from sgd:  -2349.8945605200665
new min fval from sgd:  -2349.921428969741
new min fval from sgd:  -2349.923951558182
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [1337.6615]
objective value function right now is: -2349.7010463906377
new min fval from sgd:  -2349.932475184409
new min fval from sgd:  -2349.9598541362393
new min fval from sgd:  -2349.9798495015493
new min fval from sgd:  -2349.9845315210587
new min fval from sgd:  -2349.9985599261195
new min fval from sgd:  -2350.0201429033996
new min fval from sgd:  -2350.0281925207332
new min fval from sgd:  -2350.040333663101
new min fval from sgd:  -2350.0753582604925
new min fval from sgd:  -2350.100194147487
new min fval from sgd:  -2350.1078526818997
new min fval from sgd:  -2350.120302065768
new min fval from sgd:  -2350.140072897399
new min fval from sgd:  -2350.147058783345
new min fval from sgd:  -2350.152030461613
new min fval from sgd:  -2350.168247020345
new min fval from sgd:  -2350.1923265372907
new min fval from sgd:  -2350.207775791339
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [1337.7333]
objective value function right now is: -2349.6586112464106
min fval:  -2350.207775791339
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-11.8151,   5.4904],
        [ -4.6429, -12.7110],
        [ -1.4799,   0.8251],
        [ 10.0538,   3.9289],
        [ -8.2917, -10.0528],
        [ -1.4799,   0.8251],
        [ -1.4799,   0.8252],
        [ 12.6209,   2.2281],
        [-12.1533,   5.2974],
        [-12.7725,   6.7216],
        [-11.4607,   4.9588],
        [ -3.0597,   1.7356],
        [  9.7361,   3.5378],
        [ -1.4799,   0.8251],
        [ -1.4799,   0.8251],
        [ -1.4799,   0.8251],
        [ -8.6793, -11.2004],
        [  9.4705,   4.2645],
        [ -2.9138,   1.6524],
        [-11.0194,   5.9089],
        [ -1.4799,   0.8251],
        [ -3.9768,  -6.6327]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  5.1879,  -7.5082,  -3.5959, -13.6777,  -5.1648,  -3.5959,  -3.5960,
        -13.5972,   4.7959,  -0.3418,   6.0975,  -2.8658, -12.9080,  -3.5959,
         -3.5959,  -3.5959,  -5.8382, -13.6404,  -2.8829,  -1.0055,  -3.5959,
         -7.5469], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.1191e-01, -1.3538e-01, -1.6883e-02, -5.6782e-02, -1.3859e-01,
         -1.6883e-02, -1.6883e-02, -7.1863e-02, -1.8090e-01, -1.7919e-02,
         -3.5982e-01, -1.4081e-02, -5.6264e-02, -1.6883e-02, -1.6883e-02,
         -1.6883e-02, -1.3717e-01, -5.8578e-02, -1.4568e-02, -1.4115e-02,
         -1.6883e-02, -9.6747e-04],
        [-1.0020e+01,  1.7692e+01, -3.1625e-02,  6.0880e+00,  6.7685e+00,
         -3.1625e-02, -2.2621e-02,  1.1558e+01, -8.3791e+00, -1.1351e+00,
         -1.0651e+01,  2.1371e-01,  4.0895e+00, -3.1625e-02, -3.1626e-02,
         -3.1621e-02,  1.1229e+01,  6.1738e+00,  1.9740e-01, -6.8800e-01,
         -3.1626e-02,  2.6044e+00],
        [-2.1191e-01, -1.3538e-01, -1.6883e-02, -5.6782e-02, -1.3859e-01,
         -1.6883e-02, -1.6883e-02, -7.1863e-02, -1.8090e-01, -1.7919e-02,
         -3.5982e-01, -1.4081e-02, -5.6264e-02, -1.6883e-02, -1.6883e-02,
         -1.6883e-02, -1.3717e-01, -5.8578e-02, -1.4568e-02, -1.4115e-02,
         -1.6883e-02, -9.6748e-04],
        [-1.1103e+01,  1.9408e+01,  9.5388e-03,  8.3142e+00,  8.2652e+00,
          9.5379e-03,  2.2865e-02,  1.2331e+01, -9.8014e+00, -1.9652e+00,
         -1.2198e+01, -1.1986e-01,  4.8699e+00,  9.5387e-03,  9.5392e-03,
          9.5251e-03,  1.2639e+01,  6.7902e+00, -8.0544e-02, -1.4357e+00,
          9.5388e-03,  4.3702e+00],
        [-8.8623e+00,  1.5196e+01, -8.9554e-03,  3.0790e+00,  6.7542e+00,
         -8.9555e-03, -8.7512e-03,  9.4561e+00, -7.5702e+00, -1.1759e-01,
         -9.5301e+00,  2.1465e-01,  2.7372e+00, -8.9554e-03, -8.9554e-03,
         -8.9543e-03,  1.0172e+01,  5.5280e+00,  1.8863e-01,  1.5223e-01,
         -8.9558e-03,  1.5847e+00],
        [-2.1191e-01, -1.3537e-01, -1.6882e-02, -5.6785e-02, -1.3859e-01,
         -1.6882e-02, -1.6883e-02, -7.1864e-02, -1.8089e-01, -1.7918e-02,
         -3.5979e-01, -1.4081e-02, -5.6266e-02, -1.6882e-02, -1.6882e-02,
         -1.6882e-02, -1.3716e-01, -5.8580e-02, -1.4568e-02, -1.4114e-02,
         -1.6882e-02, -9.6746e-04],
        [-2.1191e-01, -1.3538e-01, -1.6883e-02, -5.6782e-02, -1.3859e-01,
         -1.6883e-02, -1.6883e-02, -7.1863e-02, -1.8090e-01, -1.7919e-02,
         -3.5982e-01, -1.4081e-02, -5.6264e-02, -1.6883e-02, -1.6883e-02,
         -1.6883e-02, -1.3717e-01, -5.8578e-02, -1.4568e-02, -1.4115e-02,
         -1.6883e-02, -9.6748e-04],
        [-2.1191e-01, -1.3538e-01, -1.6883e-02, -5.6782e-02, -1.3859e-01,
         -1.6883e-02, -1.6883e-02, -7.1863e-02, -1.8090e-01, -1.7919e-02,
         -3.5982e-01, -1.4081e-02, -5.6264e-02, -1.6883e-02, -1.6883e-02,
         -1.6883e-02, -1.3717e-01, -5.8578e-02, -1.4568e-02, -1.4115e-02,
         -1.6883e-02, -9.6747e-04],
        [-2.1191e-01, -1.3538e-01, -1.6883e-02, -5.6782e-02, -1.3859e-01,
         -1.6883e-02, -1.6883e-02, -7.1863e-02, -1.8090e-01, -1.7919e-02,
         -3.5982e-01, -1.4081e-02, -5.6264e-02, -1.6883e-02, -1.6883e-02,
         -1.6883e-02, -1.3717e-01, -5.8578e-02, -1.4568e-02, -1.4115e-02,
         -1.6883e-02, -9.6748e-04],
        [-2.1191e-01, -1.3538e-01, -1.6883e-02, -5.6782e-02, -1.3859e-01,
         -1.6883e-02, -1.6883e-02, -7.1863e-02, -1.8090e-01, -1.7919e-02,
         -3.5982e-01, -1.4081e-02, -5.6264e-02, -1.6883e-02, -1.6883e-02,
         -1.6883e-02, -1.3717e-01, -5.8578e-02, -1.4568e-02, -1.4115e-02,
         -1.6883e-02, -9.6748e-04],
        [-2.1192e-01, -1.3538e-01, -1.6884e-02, -5.6778e-02, -1.3860e-01,
         -1.6884e-02, -1.6884e-02, -7.1861e-02, -1.8090e-01, -1.7921e-02,
         -3.5987e-01, -1.4082e-02, -5.6260e-02, -1.6884e-02, -1.6884e-02,
         -1.6884e-02, -1.3717e-01, -5.8574e-02, -1.4569e-02, -1.4117e-02,
         -1.6884e-02, -9.6750e-04],
        [ 9.7765e+00, -1.6495e+01, -2.4907e-02, -7.1030e+00, -7.7251e+00,
         -2.4908e-02, -1.8558e-03, -1.0878e+01,  8.4744e+00,  2.2917e+00,
          1.1214e+01,  4.5333e-02, -3.3562e+00, -2.4907e-02, -2.4907e-02,
         -2.4919e-02, -1.1253e+01, -6.8642e+00,  6.1640e-02,  1.1432e+00,
         -2.4907e-02, -3.5491e+00],
        [-2.1191e-01, -1.3538e-01, -1.6883e-02, -5.6782e-02, -1.3859e-01,
         -1.6883e-02, -1.6883e-02, -7.1863e-02, -1.8090e-01, -1.7919e-02,
         -3.5982e-01, -1.4081e-02, -5.6264e-02, -1.6883e-02, -1.6883e-02,
         -1.6883e-02, -1.3717e-01, -5.8578e-02, -1.4568e-02, -1.4115e-02,
         -1.6883e-02, -9.6747e-04],
        [-2.1191e-01, -1.3538e-01, -1.6883e-02, -5.6782e-02, -1.3859e-01,
         -1.6883e-02, -1.6883e-02, -7.1863e-02, -1.8090e-01, -1.7919e-02,
         -3.5982e-01, -1.4081e-02, -5.6264e-02, -1.6883e-02, -1.6883e-02,
         -1.6883e-02, -1.3717e-01, -5.8578e-02, -1.4568e-02, -1.4115e-02,
         -1.6883e-02, -9.6747e-04],
        [ 5.7103e-01,  1.7929e-01,  1.0240e-02,  6.9822e-02,  2.6320e-01,
          1.0240e-02,  1.0268e-02,  7.4617e-02,  4.8375e-01, -3.7031e-02,
          8.0272e-01,  4.2886e-03,  7.5285e-02,  1.0240e-02,  1.0240e-02,
          1.0240e-02,  2.5779e-01,  6.8536e-02,  5.8736e-03, -3.2326e-02,
          1.0240e-02, -7.1268e-03],
        [-2.1191e-01, -1.3538e-01, -1.6883e-02, -5.6782e-02, -1.3859e-01,
         -1.6883e-02, -1.6883e-02, -7.1863e-02, -1.8090e-01, -1.7919e-02,
         -3.5982e-01, -1.4081e-02, -5.6264e-02, -1.6883e-02, -1.6883e-02,
         -1.6883e-02, -1.3717e-01, -5.8578e-02, -1.4568e-02, -1.4115e-02,
         -1.6883e-02, -9.6748e-04],
        [-2.0472e-01, -1.1700e-01, -1.5206e-02, -4.8280e-02, -1.2361e-01,
         -1.5206e-02, -1.5207e-02, -5.6556e-02, -1.7394e-01, -1.6162e-02,
         -3.5110e-01, -1.2673e-02, -4.7375e-02, -1.5206e-02, -1.5206e-02,
         -1.5206e-02, -1.2229e-01, -5.0489e-02, -1.3143e-02, -1.2684e-02,
         -1.5206e-02, -1.0133e-03],
        [-2.1191e-01, -1.3538e-01, -1.6883e-02, -5.6782e-02, -1.3859e-01,
         -1.6883e-02, -1.6883e-02, -7.1863e-02, -1.8090e-01, -1.7919e-02,
         -3.5982e-01, -1.4081e-02, -5.6264e-02, -1.6883e-02, -1.6883e-02,
         -1.6883e-02, -1.3717e-01, -5.8578e-02, -1.4568e-02, -1.4115e-02,
         -1.6883e-02, -9.6748e-04],
        [-2.1191e-01, -1.3538e-01, -1.6883e-02, -5.6782e-02, -1.3859e-01,
         -1.6883e-02, -1.6883e-02, -7.1863e-02, -1.8090e-01, -1.7919e-02,
         -3.5982e-01, -1.4081e-02, -5.6264e-02, -1.6883e-02, -1.6883e-02,
         -1.6883e-02, -1.3717e-01, -5.8578e-02, -1.4568e-02, -1.4115e-02,
         -1.6883e-02, -9.6747e-04],
        [-2.1191e-01, -1.3538e-01, -1.6883e-02, -5.6782e-02, -1.3859e-01,
         -1.6883e-02, -1.6883e-02, -7.1863e-02, -1.8090e-01, -1.7919e-02,
         -3.5982e-01, -1.4081e-02, -5.6264e-02, -1.6883e-02, -1.6883e-02,
         -1.6883e-02, -1.3717e-01, -5.8578e-02, -1.4568e-02, -1.4115e-02,
         -1.6883e-02, -9.6748e-04],
        [ 5.7342e-01,  1.7968e-01,  1.0832e-02,  7.0004e-02,  2.6341e-01,
          1.0832e-02,  1.0860e-02,  7.4862e-02,  4.8516e-01, -3.7009e-02,
          8.0839e-01,  4.6364e-03,  7.5532e-02,  1.0832e-02,  1.0832e-02,
          1.0832e-02,  2.5793e-01,  6.8713e-02,  6.2522e-03, -3.2295e-02,
          1.0832e-02, -7.3024e-03],
        [-2.1191e-01, -1.3538e-01, -1.6883e-02, -5.6782e-02, -1.3859e-01,
         -1.6883e-02, -1.6883e-02, -7.1863e-02, -1.8090e-01, -1.7919e-02,
         -3.5982e-01, -1.4081e-02, -5.6264e-02, -1.6883e-02, -1.6883e-02,
         -1.6883e-02, -1.3717e-01, -5.8578e-02, -1.4568e-02, -1.4115e-02,
         -1.6883e-02, -9.6747e-04]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.0133,  0.7421, -2.0133,  1.0429,  0.2883, -2.0135, -2.0133, -2.0133,
        -2.0133, -2.0133, -2.0131, -1.1161, -2.0133, -2.0133,  4.0552, -2.0133,
        -2.1060, -2.0133, -2.0133, -2.0133,  4.1299, -2.0133], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0564, -16.2145,   0.0564, -23.6205, -11.0874,   0.0564,   0.0564,
           0.0564,   0.0564,   0.0564,   0.0564,  20.0588,   0.0564,   0.0564,
           5.0866,   0.0564,   0.0512,   0.0564,   0.0564,   0.0564,   5.5304,
           0.0564]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.7378,   0.2313],
        [  0.7189,  14.6402],
        [  2.5845,  -0.1954],
        [ 15.4918,   0.0334],
        [ -1.7414,   0.2325],
        [ -2.0345,   4.6759],
        [ -1.7472,   0.2343],
        [ -1.7433,   0.2352],
        [ -1.7411,   0.2273],
        [ -1.7415,   0.2315],
        [ -1.4195,   4.8830],
        [ -1.7352,   0.2268],
        [-13.4692,  -0.4905],
        [ -8.5390,   7.5984],
        [ -7.7861,  12.9414],
        [ -1.7468,   0.2288],
        [ -1.7477,   0.2346],
        [-15.2084,  -4.3109],
        [-11.7742,  -5.8489],
        [  6.3180,  15.1247],
        [ 13.5536,  -0.1784],
        [ 15.8688,  13.5216]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -5.0168,   9.3053,   6.2355, -13.8810,  -5.0157,  -6.6090,  -5.0264,
         -5.0338,  -5.0050,  -5.0110,  -5.7169,  -4.9887,   9.1314,   3.6813,
          8.5539,  -5.0250,  -5.0276,   3.2308,  -4.8432,   9.4438,  -7.5638,
          7.6506], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 7.0021e-02, -1.9288e-02, -1.7913e+00,  8.3237e-01,  7.0083e-02,
          2.1647e+00,  7.2436e-02,  7.3385e-02,  6.8325e-02,  6.9455e-02,
          1.4451e+00,  6.5564e-02, -1.5058e+00, -4.3145e+00,  8.5468e-02,
          7.1921e-02,  7.2669e-02, -1.7355e+00, -8.0166e-01, -5.4767e-01,
         -9.8989e-01,  1.6707e-01],
        [ 9.7145e-02,  7.0104e-02, -1.6399e+00,  1.0550e+00,  9.7255e-02,
          2.1855e+00,  9.9783e-02,  1.0098e-01,  9.4885e-02,  9.6471e-02,
          1.3510e+00,  9.2083e-02, -1.5491e+00, -5.4820e+00,  3.3425e-01,
          9.8749e-02,  1.0005e-01, -2.0063e+00, -1.1442e+00, -4.9000e-01,
         -7.8628e-01,  2.8143e-01],
        [ 9.0841e-02,  5.3170e-02, -1.6923e+00,  1.0104e+00,  9.0963e-02,
          2.1995e+00,  9.3546e-02,  9.4708e-02,  8.8624e-02,  9.0185e-02,
          1.4037e+00,  8.5743e-02, -1.5289e+00, -5.1580e+00,  2.7228e-01,
          9.2556e-02,  9.3814e-02, -1.8842e+00, -1.0452e+00, -5.1058e-01,
         -8.4881e-01,  2.5439e-01],
        [ 9.0914e-02,  4.8652e-02, -1.6914e+00,  1.0088e+00,  9.1038e-02,
          2.1985e+00,  9.3622e-02,  9.4779e-02,  8.8702e-02,  9.0261e-02,
          1.4034e+00,  8.5820e-02, -1.5303e+00, -5.1518e+00,  2.7297e-01,
          9.2634e-02,  9.3890e-02, -1.8948e+00, -1.0434e+00, -5.1065e-01,
         -8.4718e-01,  2.5296e-01],
        [ 9.5500e-02,  6.2154e-02, -1.6560e+00,  1.0418e+00,  9.5618e-02,
          2.1906e+00,  9.8175e-02,  9.9360e-02,  9.3244e-02,  9.4833e-02,
          1.3689e+00,  9.0407e-02, -1.5444e+00, -5.3792e+00,  3.1667e-01,
          9.7144e-02,  9.8440e-02, -1.9779e+00, -1.1129e+00, -4.9760e-01,
         -8.0336e-01,  2.7334e-01],
        [ 9.2976e-02,  5.4675e-02, -1.6759e+00,  1.0238e+00,  9.3099e-02,
          2.1959e+00,  9.5676e-02,  9.6847e-02,  9.0739e-02,  9.2317e-02,
          1.3896e+00,  8.7873e-02, -1.5368e+00, -5.2499e+00,  2.9196e-01,
          9.4664e-02,  9.5943e-02, -1.9314e+00, -1.0736e+00, -5.0562e-01,
         -8.2891e-01,  2.6198e-01],
        [ 3.2359e-02, -1.3768e+01,  8.3908e-01, -7.8707e+00,  1.7687e-02,
          1.4183e-02,  1.3133e-02,  3.8324e-02,  1.6144e-02,  1.4201e-02,
          1.9405e-02,  1.5421e-02,  5.8653e+00,  1.9557e+00, -6.3774e+00,
          2.3906e-02,  1.2424e-02,  4.5155e+00,  2.3169e+00, -1.1752e+01,
         -3.8919e+00, -7.2728e+00],
        [-1.9965e-01,  5.1213e+00, -6.6639e+00, -8.5077e+00, -1.9914e-01,
          1.7484e+00, -2.0988e-01, -2.1213e-01, -1.9665e-01, -1.9671e-01,
          1.8332e+00, -1.8050e-01,  4.1696e+00,  6.1525e+00,  5.8277e+00,
         -2.1595e-01, -2.1090e-01,  3.6083e+00, -5.9827e+00,  4.4438e+00,
         -6.6663e+00,  3.1935e+00],
        [-1.5900e-02,  1.8696e+01,  2.4774e+00,  1.1429e+01, -1.9386e-02,
          2.6039e-02, -2.1538e-02, -2.1698e-02, -1.4095e-02, -1.7748e-02,
          6.3425e-02, -1.0683e-02, -1.0819e+01, -1.0166e+00,  8.6240e+00,
         -1.8016e-02, -2.1857e-02, -1.1166e+01, -9.5998e+00,  2.3682e+01,
          8.2356e+00,  1.8943e+01],
        [ 9.0837e-02,  5.2974e-02, -1.6923e+00,  1.0103e+00,  9.0959e-02,
          2.1995e+00,  9.3543e-02,  9.4704e-02,  8.8620e-02,  9.0181e-02,
          1.4037e+00,  8.5740e-02, -1.5290e+00, -5.1575e+00,  2.7226e-01,
          9.2553e-02,  9.3810e-02, -1.8845e+00, -1.0450e+00, -5.1059e-01,
         -8.4881e-01,  2.5430e-01],
        [ 9.6561e-02,  6.9096e-02, -1.6455e+00,  1.0513e+00,  9.6673e-02,
          2.1874e+00,  9.9209e-02,  1.0040e-01,  9.4300e-02,  9.5888e-02,
          1.3570e+00,  9.1487e-02, -1.5472e+00, -5.4502e+00,  3.2818e-01,
          9.8175e-02,  9.9473e-02, -1.9938e+00, -1.1345e+00, -4.9266e-01,
         -7.9267e-01,  2.7924e-01],
        [-1.2217e-01, -1.6267e+01, -9.6150e+00, -4.3850e-02, -1.2259e-01,
         -8.8076e-04, -1.2334e-01, -1.2358e-01, -1.2186e-01, -1.2266e-01,
          9.4600e-04, -1.2162e-01,  7.6433e+00,  4.6317e-01, -2.0187e+00,
         -1.2215e-01, -1.2337e-01,  9.3260e+00,  5.4016e+00, -2.8521e+01,
         -8.4014e+00, -2.6799e+01],
        [ 9.0897e-02,  4.9034e-02, -1.6916e+00,  1.0088e+00,  9.1021e-02,
          2.1986e+00,  9.3605e-02,  9.4763e-02,  8.8684e-02,  9.0244e-02,
          1.4035e+00,  8.5803e-02, -1.5301e+00, -5.1518e+00,  2.7280e-01,
          9.2617e-02,  9.3872e-02, -1.8936e+00, -1.0434e+00, -5.1067e-01,
         -8.4742e-01,  2.5304e-01],
        [-4.8171e-02,  1.2003e-01,  3.0316e+00, -4.5699e+00, -4.7906e-02,
         -4.6098e-01, -5.1308e-02, -5.0920e-02, -4.9972e-02, -4.7740e-02,
         -2.9593e-01, -4.4262e-02,  9.9569e+00, -7.0321e-01,  2.8936e+00,
         -5.5971e-02, -5.1580e-02,  8.6776e+00, -9.2245e-01,  7.6562e-01,
         -9.5574e-01, -8.4257e-01],
        [-1.0360e-01, -1.0168e+00,  5.1522e+00,  1.2644e+00, -9.6465e-02,
         -3.6620e-01, -1.0427e-01, -1.2688e-01, -7.5221e-02, -8.7854e-02,
         -2.7704e-01, -6.3414e-02, -3.3661e-01, -4.2641e+00, -7.3403e+00,
         -9.8914e-02, -1.0532e-01, -4.4951e+00, -3.6715e+00, -1.7393e+00,
          9.0737e+00, -9.3769e-01],
        [ 9.6146e-02,  6.6607e-02, -1.6488e+00,  1.0474e+00,  9.6260e-02,
          2.1887e+00,  9.8802e-02,  9.9994e-02,  9.3887e-02,  9.5475e-02,
          1.3615e+00,  9.1067e-02, -1.5463e+00, -5.4245e+00,  3.2368e-01,
          9.7770e-02,  9.9067e-02, -1.9875e+00, -1.1268e+00, -4.9409e-01,
         -7.9711e-01,  2.7658e-01],
        [-9.2279e-03, -7.7780e-01,  2.3513e+00, -1.6252e+00, -1.0575e-02,
          1.4786e+00, -1.0665e-02, -6.8181e-03, -1.4281e-02, -1.2357e-02,
          1.5222e+00, -1.5251e-02,  6.5954e-01, -2.0234e+00, -4.3341e-01,
         -1.1060e-02, -1.0556e-02, -1.8219e+00,  3.6219e-02, -3.7040e-01,
          1.5661e+00,  1.8551e+00],
        [-2.6913e-02, -1.5855e+00, -4.7603e-01, -6.2988e+00, -2.8792e-02,
         -9.1581e-03, -2.8410e-02, -2.8493e-02, -2.4471e-02, -2.8597e-02,
         -7.0929e-03, -2.6716e-02, -4.9256e-01, -2.7085e-02, -4.1334e-02,
         -2.3073e-02, -2.8388e-02, -2.7760e+00,  2.3308e-01, -1.3864e+01,
          1.6091e-01,  7.1759e-02],
        [ 4.8630e-02, -8.0505e+00,  1.6629e+00, -1.0865e+01,  5.6087e-02,
         -9.7967e-01,  6.5794e-02,  4.9837e-02,  5.4081e-02,  5.9619e-02,
          5.3825e-02,  5.2379e-02,  2.8118e+00,  8.6083e-01,  3.1187e+00,
          5.2126e-02,  6.6565e-02,  1.2140e+00,  3.2287e+00, -1.3476e+00,
          1.6238e+00, -4.0730e+00],
        [ 1.5861e-02, -8.2406e+00, -1.4593e+00, -5.9725e+00,  1.6185e-02,
         -1.4666e-03,  1.9320e-02,  1.8757e-02,  1.3161e-02,  1.5867e-02,
         -1.4028e-02,  1.1048e-02,  5.8038e+00,  1.0605e+00,  1.5401e-01,
          1.6027e-02,  1.9622e-02,  1.5912e+00,  4.2615e+00, -1.6216e+01,
         -3.3776e+00, -2.9488e+00],
        [ 9.0838e-02,  5.2889e-02, -1.6923e+00,  1.0103e+00,  9.0960e-02,
          2.1994e+00,  9.3544e-02,  9.4705e-02,  8.8621e-02,  9.0182e-02,
          1.4037e+00,  8.5741e-02, -1.5290e+00, -5.1574e+00,  2.7227e-01,
          9.2554e-02,  9.3811e-02, -1.8847e+00, -1.0450e+00, -5.1060e-01,
         -8.4878e-01,  2.5428e-01],
        [-1.4138e-02,  5.8337e+00, -6.2167e+00, -9.9371e+00, -1.7090e-02,
          3.0453e+00, -2.9566e-02, -3.7449e-02,  1.1841e-02, -1.0679e-02,
          2.7941e+00,  1.7821e-02,  6.7928e-01,  5.6252e+00,  7.3373e+00,
         -2.7971e-03, -3.1247e-02,  3.1442e+00, -1.2034e-01,  4.0853e+00,
         -1.1867e+01,  6.7547e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.7919, -1.6405, -1.6929, -1.6920, -1.6566, -1.6765,  3.0128, -8.1774,
        -3.8068, -1.6929, -1.6461, -0.6023, -1.6922,  3.3223,  6.7737, -1.6494,
         2.3532, -1.0863,  2.1341, -0.7408, -1.6929, -6.8230], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-9.8164e-03, -2.2065e-02, -1.7782e-02, -1.7760e-02, -2.0689e-02,
         -1.8991e-02, -2.6531e-03, -2.8894e-01, -1.6770e+00, -1.7777e-02,
         -2.1608e-02, -2.7391e-05, -1.7755e-02, -1.6422e+00, -1.6722e+00,
         -2.1269e-02, -1.6713e+00, -1.0777e-03, -1.1531e-01, -1.1719e-04,
         -1.7777e-02, -3.7940e-02],
        [-1.5351e-02, -2.6189e-02, -2.2528e-02, -2.2497e-02, -2.5027e-02,
         -2.3568e-02, -3.6148e-03, -1.5394e-01, -8.6329e-01, -2.2523e-02,
         -2.5810e-02, -2.8285e-04, -2.2494e-02, -8.6947e-01, -8.6191e-01,
         -2.5523e-02, -8.6284e-01, -6.3917e-03, -8.1811e-02, -1.0603e-03,
         -2.2522e-02, -3.9493e-02],
        [-9.3080e-03, -1.9694e-02, -1.5775e-02, -1.5756e-02, -1.8363e-02,
         -1.6813e-02, -1.5920e-03, -2.5640e-01, -1.5454e+00, -1.5771e-02,
         -1.9246e-02, -5.4477e-05, -1.5752e-02, -1.5120e+00, -1.5413e+00,
         -1.8916e-02, -1.5405e+00, -3.5986e-04, -9.4917e-02, -2.5483e-04,
         -1.5771e-02, -2.7300e-02],
        [-5.0211e-03, -1.3559e-02, -9.8739e-03, -9.8533e-03, -1.2340e-02,
         -1.0871e-02, -3.1437e-03, -2.3269e-01, -1.3761e+00, -9.8701e-03,
         -1.3154e-02, -8.7121e-05, -9.8499e-03, -1.3493e+00, -1.3728e+00,
         -1.2852e-02, -1.3712e+00, -2.5952e-03, -8.6573e-02, -1.3365e-04,
         -9.8696e-03, -3.7080e-02],
        [ 6.0420e-01,  6.0212e-01,  5.9835e-01,  5.9738e-01,  5.9841e-01,
          5.9954e-01,  5.1085e+00, -2.0312e-01, -2.7194e+00,  5.9831e-01,
          6.0194e-01, -1.7769e+01,  5.9746e-01,  3.4078e+00,  3.6072e+00,
          6.0168e-01,  4.0402e+00,  2.2147e+00,  2.4560e+00,  3.3236e+00,
          5.9829e-01,  1.1207e+00],
        [-1.6954e-02, -2.4801e-02, -2.1746e-02, -2.1716e-02, -2.3752e-02,
         -2.2539e-02, -4.6638e-03, -2.0249e-01, -1.0865e+00, -2.1743e-02,
         -2.4453e-02, -1.0226e-04, -2.1715e-02, -1.0701e+00, -1.0800e+00,
         -2.4195e-02, -1.0823e+00,  2.5637e-04, -5.8276e-02, -1.4548e-04,
         -2.1742e-02, -4.1345e-02],
        [-1.0164e-02, -2.1132e-02, -1.6641e-02, -1.6631e-02, -1.9675e-02,
         -1.7888e-02, -2.7652e-03, -2.1126e-01, -1.3920e+00, -1.6637e-02,
         -2.0644e-02, -3.3274e-05, -1.6626e-02, -1.3569e+00, -1.3890e+00,
         -2.0284e-02, -1.3879e+00, -3.9422e-03, -8.5943e-02, -2.6188e-04,
         -1.6636e-02, -2.4765e-02],
        [-2.4748e+00, -2.7004e+00, -2.6392e+00, -2.6379e+00, -2.6828e+00,
         -2.6556e+00,  4.7626e+00,  7.3272e-02,  3.2477e+00, -2.6391e+00,
         -2.6943e+00, -1.3550e+00, -2.6380e+00,  2.2044e+00,  1.2493e+00,
         -2.6891e+00,  9.0579e-01,  5.3754e+00,  2.1022e+00, -3.5127e+00,
         -2.6391e+00,  8.2277e-01],
        [-9.1263e-03, -2.0048e-02, -1.5791e-02, -1.5773e-02, -1.8621e-02,
         -1.6931e-02, -3.5499e-03, -2.4525e-01, -1.4699e+00, -1.5787e-02,
         -1.9569e-02, -5.4823e-05, -1.5768e-02, -1.4381e+00, -1.4659e+00,
         -1.9216e-02, -1.4651e+00, -7.0165e-04, -9.8613e-02, -2.5162e-04,
         -1.5786e-02, -3.1243e-02],
        [-2.6827e-03, -1.8933e-02, -1.2533e-02, -1.2509e-02, -1.6906e-02,
         -1.4356e-02, -8.5497e-03, -1.5688e-01, -1.2920e+00, -1.2527e-02,
         -1.8264e-02, -9.1254e-05, -1.2501e-02, -1.2670e+00, -1.2900e+00,
         -1.7763e-02, -1.2872e+00, -5.1538e-03, -1.1975e-01, -1.4707e-04,
         -1.2526e-02, -1.4759e-02],
        [ 2.8057e-01, -1.0681e-01,  3.8959e-02,  3.2725e-02, -6.8848e-02,
         -1.0233e-02, -2.9360e+01,  6.8458e+00,  3.3280e+00,  3.8836e-02,
         -9.1893e-02, -1.2122e-04,  3.3521e-02, -8.4431e-01, -7.6214e-01,
         -8.2174e-02, -8.5641e-01, -3.9275e+00,  3.2761e+00, -9.3427e-01,
          3.8723e-02,  2.7167e+00],
        [ 8.6543e-01,  8.7529e-01,  8.7271e-01,  8.7169e-01,  8.7247e-01,
          8.7411e-01, -4.2686e+00,  5.8703e-02,  5.2277e+00,  8.7267e-01,
          8.7547e-01,  2.0532e+01,  8.7177e-01,  7.4254e-01,  1.1096e+00,
          8.7547e-01,  4.9954e-01, -5.7578e+00,  1.7891e+00,  2.0605e+00,
          8.7265e-01, -2.6214e-01],
        [-9.8195e-03, -2.2756e-02, -1.8238e-02, -1.8214e-02, -2.1250e-02,
         -1.9460e-02, -3.3375e-03, -2.7216e-01, -1.6582e+00, -1.8233e-02,
         -2.2252e-02, -3.1222e-05, -1.8209e-02, -1.6221e+00, -1.6538e+00,
         -2.1879e-02, -1.6528e+00, -1.8040e-03, -1.0452e-01, -1.8108e-04,
         -1.8233e-02, -3.2051e-02],
        [-9.3700e-03, -2.0501e-02, -1.6431e-02, -1.6410e-02, -1.9163e-02,
         -1.7551e-02, -2.7174e-03, -2.8659e-01, -1.6391e+00, -1.6427e-02,
         -2.0054e-02, -4.1094e-05, -1.6405e-02, -1.6053e+00, -1.6343e+00,
         -1.9724e-02, -1.6334e+00, -4.5459e-04, -1.0931e-01, -1.5513e-04,
         -1.6427e-02, -3.7790e-02]], device='cuda:0'))])
xi:  [1337.7327]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1720.1141882758839
W_T_median: 1702.7727604331076
W_T_pctile_5: 1338.6529681071079
W_T_CVAR_5_pct: 751.8947213765177
Average q (qsum/M+1):  51.55859768775202
Optimal xi:  [1337.7327]
Expected(across Rb) median(across samples) p_equity:  1.7115177975313145e-06
obj fun:  tensor(-2350.2078, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: MC_everything
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
