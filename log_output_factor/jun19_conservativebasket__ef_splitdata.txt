Starting at: 
19-06-23_14:45

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = MC_conservative
timeseries_basket['basket_desc'] = marc_conservative_basket
timeseries_basket['basket_columns'] = 
['Vol_Lo20_real_ret', 'T90_real_ret', 'B10_real_ret', 'EWD_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = MC_conservative
timeseries_basket['basket_desc'] = marc_conservative_basket
timeseries_basket['basket_columns'] = 
['Vol_Lo20_nom_ret', 'T90_nom_ret', 'B10_nom_ret', 'EWD_nom_ret']
############# End: defined asset  basket #################
               Vol_Lo20_nom_ret  CPI_nom_ret  ...  B10_nom_ret  EWD_nom_ret
My identifier                                 ...                          
192607                      NaN    -0.011299  ...     0.005383     0.013051
192608                      NaN    -0.005714  ...     0.005363     0.031002
192609                      NaN     0.005747  ...     0.005343    -0.006499
192610                      NaN     0.005714  ...     0.005323    -0.034630
192611                      NaN     0.005682  ...     0.005303     0.024776

[5 rows x 5 columns]
               Vol_Lo20_nom_ret  CPI_nom_ret  ...  B10_nom_ret  EWD_nom_ret
My identifier                                 ...                          
202208                  -0.0323    -0.000354  ...    -0.043289    -0.011556
202209                  -0.0632     0.002151  ...    -0.050056    -0.099903
202210                   0.1046     0.004056  ...    -0.014968     0.049863
202211                   0.0564    -0.001010  ...     0.040789     0.028123
202212                  -0.0296    -0.003070  ...    -0.018566    -0.047241

[5 rows x 5 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Vol_Lo20_nom_ret_ind', 'CPI_nom_ret_ind',
       'T90_nom_ret_ind', 'B10_nom_ret_ind', 'EWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
Vol_Lo20_real_ret    0.003529
T90_real_ret         0.000501
B10_real_ret         0.001637
EWD_real_ret         0.009545
dtype: float64


timeseries_basket['data_df_stdev'] = 
Vol_Lo20_real_ret    0.030737
T90_real_ret         0.005373
B10_real_ret         0.019258
EWD_real_ret         0.071360
dtype: float64


timeseries_basket['data_df_corr'] = 
                   Vol_Lo20_real_ret  T90_real_ret  B10_real_ret  EWD_real_ret
Vol_Lo20_real_ret           1.000000      0.095472      0.204677      0.382411
T90_real_ret                0.095472      1.000000      0.395793      0.037909
B10_real_ret                0.204677      0.395793      1.000000      0.024853
EWD_real_ret                0.382411      0.037909      0.024853      1.000000


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 199201
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      12  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      12  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 12)     True          12  
2     (12, 12)     True          12  
3      (12, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       4       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      12  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      12  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       4           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 12)     True          12  
2     (12, 12)     True          12  
3      (12, 4)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       12  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       12  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 12)      True          12  
0     (12, 12)      True          12  
0      (12, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       12  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       12  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        4           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 12)      True          12  
0     (12, 12)      True          12  
0      (12, 4)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       12  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       12  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 12)      True          12  
0     (12, 12)      True          12  
0      (12, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       12  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       12  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        4           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 12)      True          12  
0     (12, 12)      True          12  
0      (12, 4)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.25 0.25 0.25 0.25]
W_T_mean: 811.8578605438024
W_T_median: 642.0338180246349
W_T_pctile_5: -264.1607927261681
W_T_CVAR_5_pct: -459.41152902166783
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1728.806717343377
Current xi:  [81.35267]
objective value function right now is: -1728.806717343377
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1735.496148851646
Current xi:  [60.9591]
objective value function right now is: -1735.496148851646
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1739.8911603450008
Current xi:  [39.40407]
objective value function right now is: -1739.8911603450008
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1743.3179248133415
Current xi:  [17.174582]
objective value function right now is: -1743.3179248133415
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1743.9196373909256
Current xi:  [-3.2568104]
objective value function right now is: -1743.9196373909256
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1748.083803136013
Current xi:  [-20.270308]
objective value function right now is: -1748.083803136013
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1749.885343880909
Current xi:  [-38.383785]
objective value function right now is: -1749.885343880909
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1750.6792865476846
Current xi:  [-56.92385]
objective value function right now is: -1750.6792865476846
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1753.4288238374418
Current xi:  [-76.37166]
objective value function right now is: -1753.4288238374418
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1755.1547579891196
Current xi:  [-96.25831]
objective value function right now is: -1755.1547579891196
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1755.7790110383994
Current xi:  [-115.97931]
objective value function right now is: -1755.7790110383994
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1758.2376585890959
Current xi:  [-136.5064]
objective value function right now is: -1758.2376585890959
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1759.2980514407416
Current xi:  [-155.75063]
objective value function right now is: -1759.2980514407416
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1759.9610101406226
Current xi:  [-176.52864]
objective value function right now is: -1759.9610101406226
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1761.5942285998442
Current xi:  [-194.85338]
objective value function right now is: -1761.5942285998442
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1762.2909361428362
Current xi:  [-215.19981]
objective value function right now is: -1762.2909361428362
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-233.48756]
objective value function right now is: -1761.8906346991962
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1763.6397639501015
Current xi:  [-253.28831]
objective value function right now is: -1763.6397639501015
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1763.9105991738832
Current xi:  [-270.5949]
objective value function right now is: -1763.9105991738832
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1765.4942767964983
Current xi:  [-289.61554]
objective value function right now is: -1765.4942767964983
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1765.78006194684
Current xi:  [-306.94763]
objective value function right now is: -1765.78006194684
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1765.8283651997765
Current xi:  [-323.27637]
objective value function right now is: -1765.8283651997765
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-338.90842]
objective value function right now is: -1764.5274393996383
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1766.0816217680936
Current xi:  [-353.11707]
objective value function right now is: -1766.0816217680936
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1766.2852505530395
Current xi:  [-366.12424]
objective value function right now is: -1766.2852505530395
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-378.78503]
objective value function right now is: -1764.8632943183836
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1766.9826137464108
Current xi:  [-388.77997]
objective value function right now is: -1766.9826137464108
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-397.14514]
objective value function right now is: -1766.714190412503
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-405.63086]
objective value function right now is: -1765.6182238490774
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-413.49405]
objective value function right now is: -1764.7712005267078
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1767.1153207503048
Current xi:  [-418.34753]
objective value function right now is: -1767.1153207503048
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-421.85532]
objective value function right now is: -1766.4888655972813
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-422.78763]
objective value function right now is: -1766.3731616934083
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-423.596]
objective value function right now is: -1766.550633243275
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-424.78162]
objective value function right now is: -1765.577017440973
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1767.4095938022565
Current xi:  [-424.51566]
objective value function right now is: -1767.4095938022565
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-423.64127]
objective value function right now is: -1767.3696728531788
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-423.22556]
objective value function right now is: -1767.389358365478
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-423.13336]
objective value function right now is: -1767.3840991812274
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-423.14206]
objective value function right now is: -1767.133943326115
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-422.6165]
objective value function right now is: -1767.2088414629172
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-423.00198]
objective value function right now is: -1767.1788777652716
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1767.414385222865
Current xi:  [-422.48587]
objective value function right now is: -1767.414385222865
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-421.80807]
objective value function right now is: -1767.1618848987978
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-422.03864]
objective value function right now is: -1767.093010101006
new min fval from sgd:  -1767.4162582637696
new min fval from sgd:  -1767.4269492428684
new min fval from sgd:  -1767.4311302350613
new min fval from sgd:  -1767.440682984347
new min fval from sgd:  -1767.4508366185335
new min fval from sgd:  -1767.458581771166
new min fval from sgd:  -1767.4824459047272
new min fval from sgd:  -1767.4890470312034
new min fval from sgd:  -1767.495689783249
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-421.5689]
objective value function right now is: -1767.3932274237334
new min fval from sgd:  -1767.4971911424934
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-421.88107]
objective value function right now is: -1767.3600103565743
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-421.20428]
objective value function right now is: -1767.4844227267342
new min fval from sgd:  -1767.5011552196906
new min fval from sgd:  -1767.505078028574
new min fval from sgd:  -1767.5054107636695
new min fval from sgd:  -1767.5058479435004
new min fval from sgd:  -1767.5062659788018
new min fval from sgd:  -1767.5066452394617
new min fval from sgd:  -1767.507051753431
new min fval from sgd:  -1767.5082331015112
new min fval from sgd:  -1767.509014606858
new min fval from sgd:  -1767.5091471622668
new min fval from sgd:  -1767.5109388749543
new min fval from sgd:  -1767.5126091254785
new min fval from sgd:  -1767.5132389168275
new min fval from sgd:  -1767.5158009758027
new min fval from sgd:  -1767.518358315787
new min fval from sgd:  -1767.51856330592
new min fval from sgd:  -1767.5187383581438
new min fval from sgd:  -1767.5193114624205
new min fval from sgd:  -1767.519677247232
new min fval from sgd:  -1767.5201111934846
new min fval from sgd:  -1767.52054885299
new min fval from sgd:  -1767.521795944272
new min fval from sgd:  -1767.5224219428237
new min fval from sgd:  -1767.5234589955141
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-421.015]
objective value function right now is: -1767.4459199742712
new min fval from sgd:  -1767.5235566042747
new min fval from sgd:  -1767.5235569143515
new min fval from sgd:  -1767.5238394904825
new min fval from sgd:  -1767.5240410030735
new min fval from sgd:  -1767.5262464289033
new min fval from sgd:  -1767.527831644078
new min fval from sgd:  -1767.5286471098573
new min fval from sgd:  -1767.52982821676
new min fval from sgd:  -1767.5301231878889
new min fval from sgd:  -1767.530278503808
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-420.98642]
objective value function right now is: -1767.499306805335
min fval:  -1767.530278503808
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -0.3566,   1.1379],
        [ -0.3515,   1.1192],
        [-12.0116,  -4.6011],
        [ -5.1323,   5.5014],
        [ -3.6738,   6.0531],
        [ 10.6140,   0.9564],
        [ -0.8192,   2.5063],
        [ -0.3515,   1.1192],
        [ 12.1874,   1.6482],
        [ -7.9442,   5.0095],
        [-10.8554,  -4.0092],
        [ -0.3515,   1.1192]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-0.6368, -0.6293,  2.8193,  9.6217, 10.5498, -7.3485,  0.9242, -0.6293,
        -6.9935, 10.4328,  2.8564, -0.6293], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 1.4342e-01,  1.4433e-01, -1.0541e+00,  1.1659e+00,  1.1267e+00,
         -1.8071e-03,  3.1070e-01,  1.4433e-01,  4.1013e-02,  1.2450e+00,
         -9.2584e-01,  1.4433e-01],
        [ 3.1725e-02,  8.3714e-02,  4.2956e+00, -6.9052e+00, -8.3197e+00,
         -5.6031e+00, -3.3132e-01,  8.3714e-02, -7.5650e+00, -9.1785e+00,
          3.0539e+00,  8.3714e-02],
        [-1.8359e-02, -1.8742e-02, -3.3502e-01, -3.4922e-02, -6.6787e-02,
         -6.0952e-02, -1.1028e-02, -1.8742e-02, -8.9139e-02, -3.7050e-02,
         -3.2993e-01, -1.8742e-02],
        [ 8.7393e-02,  9.3071e-02,  4.6604e-01,  5.4017e-01,  8.1310e-01,
          5.6248e-01,  8.6000e-02,  9.3071e-02,  8.3863e-01,  5.9811e-01,
          6.7465e-01,  9.3071e-02],
        [-1.8359e-02, -1.8742e-02, -3.3502e-01, -3.4922e-02, -6.6787e-02,
         -6.0952e-02, -1.1028e-02, -1.8742e-02, -8.9139e-02, -3.7050e-02,
         -3.2993e-01, -1.8742e-02],
        [-1.8359e-02, -1.8742e-02, -3.3502e-01, -3.4922e-02, -6.6787e-02,
         -6.0952e-02, -1.1028e-02, -1.8742e-02, -8.9139e-02, -3.7050e-02,
         -3.2993e-01, -1.8742e-02],
        [ 1.5557e-01,  1.5703e-01, -1.1309e+00,  1.3137e+00,  1.2988e+00,
          1.8146e-02,  3.2918e-01,  1.5703e-01,  7.1280e-02,  1.4087e+00,
         -9.8422e-01,  1.5703e-01],
        [-1.8359e-02, -1.8742e-02, -3.3502e-01, -3.4922e-02, -6.6787e-02,
         -6.0952e-02, -1.1028e-02, -1.8742e-02, -8.9139e-02, -3.7050e-02,
         -3.2993e-01, -1.8742e-02],
        [ 2.7552e-02,  1.5346e-01, -2.2140e+00,  2.8918e+00,  3.7720e+00,
          2.0974e+00,  1.8353e-02,  1.5346e-01,  3.3735e+00,  3.9686e+00,
         -1.2490e+00,  1.5346e-01],
        [-1.8359e-02, -1.8742e-02, -3.3502e-01, -3.4922e-02, -6.6787e-02,
         -6.0952e-02, -1.1028e-02, -1.8742e-02, -8.9139e-02, -3.7050e-02,
         -3.2993e-01, -1.8742e-02],
        [-1.8359e-02, -1.8742e-02, -3.3502e-01, -3.4922e-02, -6.6787e-02,
         -6.0952e-02, -1.1028e-02, -1.8742e-02, -8.9139e-02, -3.7050e-02,
         -3.2993e-01, -1.8742e-02],
        [-1.8359e-02, -1.8742e-02, -3.3502e-01, -3.4922e-02, -6.6787e-02,
         -6.0952e-02, -1.1028e-02, -1.8742e-02, -8.9139e-02, -3.7050e-02,
         -3.2993e-01, -1.8742e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.0059,  1.4051, -0.4280,  1.5571, -0.4280, -0.4280, -1.0592, -0.4280,
        -0.9943, -0.4280, -0.4280, -0.4280], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.7829e+00, -1.4473e+01,  9.1621e-03,  4.7998e+00,  9.1622e-03,
          9.1621e-03,  2.2291e+00,  9.1621e-03,  6.2468e+00,  9.1621e-03,
          9.1621e-03,  9.1621e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-12.5787,  -1.1425],
        [ 13.0612,   7.8389],
        [  6.9899,  -1.0322],
        [-12.5959,   1.4118],
        [  9.5608,  -0.0869],
        [ -1.7245,  -0.6542],
        [ 12.2618,   4.9474],
        [ -6.3093,   6.8022],
        [ -8.8901, -10.5694],
        [ -0.3263,   9.1346],
        [ -1.8173,  -0.6551],
        [  6.0753,   3.6210]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  3.5852,   8.2362,  -6.5140,   2.7607,  -9.8893,  -4.0663,  -0.4483,
          5.9455, -11.9172,   7.3309,  -4.1497,  -3.7195], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 7.8029e-01, -4.1324e+00, -1.9067e-01,  1.0790e+00, -3.4130e+00,
          2.2874e+00,  7.4906e-01,  1.0110e+00,  2.3966e+00,  5.4938e+00,
          2.3990e+00,  1.5910e+00],
        [-5.6591e-01, -1.2458e+00, -8.8219e-01, -2.7793e-02, -5.1341e-01,
         -4.4958e-01, -1.1057e+00, -8.5523e-02, -4.3641e-01, -6.2616e-01,
         -4.4863e-01, -2.0133e-01],
        [ 1.4979e+00,  5.3420e+00, -2.9893e+00,  2.2473e-01, -3.1690e+00,
         -2.6586e+00,  4.5463e+00,  1.3485e-02,  4.5855e-01,  2.9957e-02,
         -2.6534e+00,  1.4096e-01],
        [-3.2114e+00,  3.9078e+00,  2.1856e+00,  8.4664e-01,  1.1313e+01,
          3.1403e-01,  8.3479e-01, -2.5211e+00, -3.8422e+00, -8.3354e+00,
          2.9462e-01,  3.5980e-01],
        [-2.8759e+00,  3.4649e+00,  1.5496e+00,  5.2059e+00, -5.2528e+00,
         -4.9545e-02, -9.5977e+00, -3.6811e+00, -3.7952e+00, -1.7945e+01,
         -1.6656e-01, -3.1319e+00],
        [-5.6591e-01, -1.2458e+00, -8.8219e-01, -2.7793e-02, -5.1341e-01,
         -4.4958e-01, -1.1057e+00, -8.5524e-02, -4.3641e-01, -6.2616e-01,
         -4.4863e-01, -2.0133e-01],
        [-5.6591e-01, -1.2458e+00, -8.8219e-01, -2.7793e-02, -5.1341e-01,
         -4.4958e-01, -1.1057e+00, -8.5524e-02, -4.3641e-01, -6.2616e-01,
         -4.4863e-01, -2.0133e-01],
        [ 2.6422e+00, -7.5708e+00, -5.7039e+00,  4.5398e+00, -4.1303e+00,
          3.8356e+00, -5.2007e-01,  5.7310e+00,  9.1201e+00,  9.3333e+00,
          4.0326e+00, -3.0153e+00],
        [-1.7348e+00,  5.9470e+00,  5.1247e+00, -4.5211e+00,  4.2213e+00,
          1.9699e+00,  2.1058e+00, -4.9767e+00, -7.0266e+00, -4.9870e+00,
          1.9703e+00,  6.7135e-01],
        [-2.4836e-02,  2.5225e+00,  3.9982e-02, -5.8319e-02, -7.0282e+00,
         -1.1771e+00, -3.3786e+00, -4.6059e+00,  2.1412e+00, -5.3034e+00,
         -1.4671e+00, -2.0586e+00],
        [-2.4376e+00,  3.9495e+00,  3.4928e+00,  1.8413e+00,  2.8007e+00,
          2.5860e+00,  1.0439e+00,  8.4422e-01, -3.0198e+00,  6.8115e-01,
          2.5908e+00, -1.5243e-01],
        [ 5.3411e+00, -6.3539e+00, -2.5578e+00, -2.7290e+00, -7.4011e-02,
          1.2515e+00, -1.9672e-01, -1.5296e+00,  3.1326e+00, -1.0468e+00,
          1.2620e+00, -1.4256e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.1870, -1.7588,  1.3412,  2.5026,  1.9724, -1.7588, -1.7588, -4.2518,
         2.8608,  1.6052,  0.6804, -1.1920], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.1604, -0.0355, -2.6501, -1.6251, -0.0633, -0.0355, -0.0355, -0.6840,
         -2.5043, -0.5845, -2.5805, -0.5056],
        [ 0.2293,  0.0294,  3.9615, -0.2259, -8.4610,  0.0294,  0.0294,  2.2314,
         -1.4098, -0.3833, -0.7965,  1.9650],
        [-1.1517, -0.0239, -2.5028, -1.7292, -0.0410, -0.0239, -0.0239, -0.5711,
         -2.6398, -0.3795, -2.7044, -0.4072],
        [-0.0813, -0.0090, -3.2868,  0.9474,  8.7414, -0.0091, -0.0090, -1.3780,
          1.8946,  1.0353,  1.6796, -1.5915]], device='cuda:0'))])
xi:  [-421.0062]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 7.129743686686403
W_T_median: 39.17840484862056
W_T_pctile_5: -421.1644696144616
W_T_CVAR_5_pct: -517.3140315047364
Average q (qsum/M+1):  57.85148768271169
Optimal xi:  [-421.0062]
Expected(across Rb) median(across samples) p_equity:  0.8348884026209513
obj fun:  tensor(-1767.5303, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: MC_conservative
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       12  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       12  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 12)      True          12  
0     (12, 12)      True          12  
0      (12, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       12  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       12  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        4           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 12)      True          12  
0     (12, 12)      True          12  
0      (12, 4)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.25 0.25 0.25 0.25]
W_T_mean: 811.8578605438024
W_T_median: 642.0338180246349
W_T_pctile_5: -264.1607927261681
W_T_CVAR_5_pct: -459.41152902166783
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1664.645441422606
Current xi:  [80.54579]
objective value function right now is: -1664.645441422606
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1677.2056632843282
Current xi:  [61.8823]
objective value function right now is: -1677.2056632843282
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.2532909618183
Current xi:  [43.26172]
objective value function right now is: -1686.2532909618183
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1690.6489499169993
Current xi:  [24.59172]
objective value function right now is: -1690.6489499169993
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1693.2126497255651
Current xi:  [4.4307303]
objective value function right now is: -1693.2126497255651
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1699.2456201504212
Current xi:  [-3.0130882]
objective value function right now is: -1699.2456201504212
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-7.408307]
objective value function right now is: -1697.2698461113232
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1699.7535958430954
Current xi:  [-19.122932]
objective value function right now is: -1699.7535958430954
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1702.3797168282906
Current xi:  [-35.64974]
objective value function right now is: -1702.3797168282906
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1702.7189232664025
Current xi:  [-45.273716]
objective value function right now is: -1702.7189232664025
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-58.27844]
objective value function right now is: -1700.1776521301874
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1705.142672384391
Current xi:  [-73.965126]
objective value function right now is: -1705.142672384391
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1705.1982543722318
Current xi:  [-81.92513]
objective value function right now is: -1705.1982543722318
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-92.75585]
objective value function right now is: -1703.3467354927511
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1706.9720474522535
Current xi:  [-108.20069]
objective value function right now is: -1706.9720474522535
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1707.3491677011843
Current xi:  [-113.757195]
objective value function right now is: -1707.3491677011843
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1707.4563006771982
Current xi:  [-117.943085]
objective value function right now is: -1707.4563006771982
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-123.37966]
objective value function right now is: -1707.4138064620417
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1707.9964611551611
Current xi:  [-134.01532]
objective value function right now is: -1707.9964611551611
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1708.1458896942781
Current xi:  [-147.46608]
objective value function right now is: -1708.1458896942781
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1708.5809808594
Current xi:  [-149.66869]
objective value function right now is: -1708.5809808594
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1708.6432801794426
Current xi:  [-150.94736]
objective value function right now is: -1708.6432801794426
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-151.08154]
objective value function right now is: -1708.2657158105335
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1708.6762511121497
Current xi:  [-150.56964]
objective value function right now is: -1708.6762511121497
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-150.4839]
objective value function right now is: -1707.92892105734
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1708.6804231982658
Current xi:  [-150.87123]
objective value function right now is: -1708.6804231982658
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-150.7431]
objective value function right now is: -1708.551665322262
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-150.74988]
objective value function right now is: -1707.341671947216
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1708.8602963438248
Current xi:  [-150.39609]
objective value function right now is: -1708.8602963438248
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-150.95473]
objective value function right now is: -1708.6642092591985
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-151.29938]
objective value function right now is: -1708.0642517578103
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-150.64151]
objective value function right now is: -1708.1979272947128
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-150.68866]
objective value function right now is: -1708.581204824212
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-150.92912]
objective value function right now is: -1708.7069060619854
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-150.88968]
objective value function right now is: -1708.2390577572096
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1708.9971504026696
Current xi:  [-150.53384]
objective value function right now is: -1708.9971504026696
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-150.54755]
objective value function right now is: -1708.5193400354483
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1709.0321996203527
Current xi:  [-150.48523]
objective value function right now is: -1709.0321996203527
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-150.42937]
objective value function right now is: -1708.7219880044822
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-150.39693]
objective value function right now is: -1708.8893521420828
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1709.2108281641806
Current xi:  [-150.18964]
objective value function right now is: -1709.2108281641806
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-149.80278]
objective value function right now is: -1708.8622569401423
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-149.97844]
objective value function right now is: -1709.1681483014067
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-149.9432]
objective value function right now is: -1709.1205298534
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-149.87947]
objective value function right now is: -1709.1588843879013
new min fval from sgd:  -1709.224267237539
new min fval from sgd:  -1709.2256211614251
new min fval from sgd:  -1709.2334286471296
new min fval from sgd:  -1709.2339064886546
new min fval from sgd:  -1709.2368662136012
new min fval from sgd:  -1709.2388405709323
new min fval from sgd:  -1709.2464963657576
new min fval from sgd:  -1709.246528265741
new min fval from sgd:  -1709.257028622758
new min fval from sgd:  -1709.2624850427335
new min fval from sgd:  -1709.2673870126055
new min fval from sgd:  -1709.270762878632
new min fval from sgd:  -1709.2715997626512
new min fval from sgd:  -1709.2742265663674
new min fval from sgd:  -1709.274592088832
new min fval from sgd:  -1709.2766822392848
new min fval from sgd:  -1709.2931214925022
new min fval from sgd:  -1709.2983122828284
new min fval from sgd:  -1709.3154616629422
new min fval from sgd:  -1709.319463311223
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-150.02655]
objective value function right now is: -1709.1349491402955
new min fval from sgd:  -1709.3239306750559
new min fval from sgd:  -1709.335173487039
new min fval from sgd:  -1709.336365750467
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-149.79608]
objective value function right now is: -1709.185854905088
new min fval from sgd:  -1709.3365216424354
new min fval from sgd:  -1709.3372632036846
new min fval from sgd:  -1709.3422610094185
new min fval from sgd:  -1709.3539773801085
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-149.84349]
objective value function right now is: -1709.2260148558846
new min fval from sgd:  -1709.354561664699
new min fval from sgd:  -1709.3569507460772
new min fval from sgd:  -1709.3594626573915
new min fval from sgd:  -1709.3614882132235
new min fval from sgd:  -1709.363377298223
new min fval from sgd:  -1709.364322936751
new min fval from sgd:  -1709.3658919779323
new min fval from sgd:  -1709.369116419012
new min fval from sgd:  -1709.3707218574555
new min fval from sgd:  -1709.3718309412052
new min fval from sgd:  -1709.3722805700177
new min fval from sgd:  -1709.3729672128973
new min fval from sgd:  -1709.3742059512153
new min fval from sgd:  -1709.3751346939334
new min fval from sgd:  -1709.3762527860742
new min fval from sgd:  -1709.376524678042
new min fval from sgd:  -1709.377401414528
new min fval from sgd:  -1709.377897499989
new min fval from sgd:  -1709.378114350587
new min fval from sgd:  -1709.3784025534053
new min fval from sgd:  -1709.3795191256097
new min fval from sgd:  -1709.3804227730411
new min fval from sgd:  -1709.3813670818122
new min fval from sgd:  -1709.3818825532085
new min fval from sgd:  -1709.3825169930683
new min fval from sgd:  -1709.3829137159032
new min fval from sgd:  -1709.383333780769
new min fval from sgd:  -1709.3838997679895
new min fval from sgd:  -1709.384490281755
new min fval from sgd:  -1709.3854755469756
new min fval from sgd:  -1709.3855877311805
new min fval from sgd:  -1709.3863484880171
new min fval from sgd:  -1709.3871056372525
new min fval from sgd:  -1709.3874605955896
new min fval from sgd:  -1709.3882037165608
new min fval from sgd:  -1709.3894219724211
new min fval from sgd:  -1709.3901169548515
new min fval from sgd:  -1709.3906737973834
new min fval from sgd:  -1709.3914472861663
new min fval from sgd:  -1709.392026156259
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-149.71692]
objective value function right now is: -1709.3883076590043
new min fval from sgd:  -1709.3927421541534
new min fval from sgd:  -1709.3944895282007
new min fval from sgd:  -1709.3964573600178
new min fval from sgd:  -1709.3985720807211
new min fval from sgd:  -1709.4002476115104
new min fval from sgd:  -1709.4026637985958
new min fval from sgd:  -1709.404409324708
new min fval from sgd:  -1709.4045927184109
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-149.71346]
objective value function right now is: -1709.3767459888693
min fval:  -1709.4045927184109
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-8.3118,  5.0104],
        [-3.5807, -6.0488],
        [-0.6049,  1.3578],
        [-0.6049,  1.3578],
        [-1.9477, -3.4420],
        [-3.3472, -5.7923],
        [-3.6145, -6.0697],
        [-1.9275, -3.3752],
        [-0.6052,  1.3426],
        [-0.6049,  1.3578],
        [-6.2686,  5.2637],
        [-3.2338, -5.6214]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 6.6002, -5.1828, -1.2617, -1.2618, -5.4982, -5.2175, -5.1563, -5.4519,
        -1.2773, -1.2618,  7.3280, -5.1712], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.0725e-02, -1.4853e-01, -6.1219e-03, -6.1215e-03, -7.8428e-03,
         -1.2775e-01, -1.5218e-01, -7.6678e-03, -6.1300e-03, -6.1232e-03,
         -5.6174e-02, -1.1970e-01],
        [-1.0725e-02, -1.4853e-01, -6.1219e-03, -6.1215e-03, -7.8428e-03,
         -1.2775e-01, -1.5218e-01, -7.6678e-03, -6.1300e-03, -6.1232e-03,
         -5.6174e-02, -1.1970e-01],
        [ 3.9152e+00, -2.0766e+00,  4.0606e-02,  4.0701e-02, -1.7595e-01,
         -1.6451e+00, -2.0995e+00, -1.5801e-01,  5.0383e-02,  4.1157e-02,
          4.6420e+00, -1.5007e+00],
        [-1.3688e-02, -1.7995e-01, -1.3329e-02, -1.3327e-02, -1.7838e-02,
         -1.5688e-01, -1.8402e-01, -1.7483e-02, -1.3352e-02, -1.3332e-02,
         -5.3925e-02, -1.4794e-01],
        [ 4.4848e+00, -2.3213e+00,  2.3289e-02,  2.3268e-02, -3.0943e-01,
         -1.9252e+00, -2.3533e+00, -3.0901e-01,  3.1542e-02,  2.4240e-02,
          5.1898e+00, -1.7159e+00],
        [ 4.7674e+00, -2.3985e+00,  7.5401e-03,  7.5075e-03, -3.9231e-01,
         -1.9988e+00, -2.5580e+00, -3.6891e-01,  1.4171e-02,  8.5146e-03,
          5.4292e+00, -1.8823e+00],
        [-6.0544e+00,  3.2565e+00,  8.8694e-02,  8.7887e-02,  8.2431e-01,
          2.6126e+00,  3.1837e+00,  7.9344e-01,  1.1873e-01,  9.1957e-02,
         -6.6107e+00,  2.4179e+00],
        [-1.0725e-02, -1.4853e-01, -6.1219e-03, -6.1215e-03, -7.8428e-03,
         -1.2775e-01, -1.5218e-01, -7.6679e-03, -6.1300e-03, -6.1233e-03,
         -5.6174e-02, -1.1970e-01],
        [-1.0725e-02, -1.4853e-01, -6.1219e-03, -6.1215e-03, -7.8428e-03,
         -1.2775e-01, -1.5218e-01, -7.6678e-03, -6.1300e-03, -6.1232e-03,
         -5.6174e-02, -1.1970e-01],
        [-5.0910e+00,  2.8682e+00,  7.0172e-02,  7.0669e-02,  6.0629e-01,
          2.2606e+00,  2.8118e+00,  6.0659e-01,  6.7693e-02,  6.9774e-02,
         -5.8854e+00,  2.1458e+00],
        [ 4.2250e+00, -2.2185e+00,  2.8289e-02,  2.8303e-02, -2.4384e-01,
         -1.8305e+00, -2.2764e+00, -2.4806e-01,  3.7447e-02,  2.9066e-02,
          4.9718e+00, -1.5786e+00],
        [-4.9467e+00,  2.7671e+00,  6.7636e-02,  6.8179e-02,  5.9347e-01,
          2.3083e+00,  2.7979e+00,  5.7364e-01,  6.2223e-02,  6.6907e-02,
         -5.8083e+00,  2.0158e+00]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.7856, -0.7856,  0.7562, -0.9146,  0.9354,  1.0556, -1.8145, -0.7856,
        -0.7856, -1.6946,  0.8729, -1.6710], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-6.4934e-03, -6.4934e-03,  3.9586e+00,  1.2877e-02,  4.7121e+00,
          5.0717e+00, -6.7613e+00, -6.4934e-03, -6.4934e-03, -5.5107e+00,
          4.3738e+00, -5.3600e+00]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-13.8471,  -6.7130],
        [ -4.7663, -14.3625],
        [  0.4712,   4.0597],
        [ 10.9735,  -1.1264],
        [-10.1225,  -6.3348],
        [ -1.5571,  -0.5721],
        [  1.6850,  -8.9452],
        [ -8.0298, -12.3395],
        [ -1.5841,   0.9481],
        [-10.0315,  -5.3077],
        [ -9.8949,   2.1797],
        [  1.5850,  -5.3004]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -5.1564, -14.6444,  -6.0738, -10.6887,  -5.8601,  -5.2635,  -8.4337,
        -13.9387,  -2.9085,  -2.7990,   5.3799,  -2.7519], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.1363e+00, -1.1177e+01, -2.7014e+00, -5.3190e+00, -2.1075e+00,
          1.2000e-02, -1.9545e+00, -1.5066e+00,  5.3602e-02, -3.4756e+00,
          5.4390e+00, -3.1045e-01],
        [ 1.3952e+00, -5.8332e+00, -1.7965e+00, -1.1452e+01,  2.6122e+00,
         -4.2454e+00,  1.5660e+00,  9.3018e+00, -6.4496e-02,  3.7513e+00,
          9.0185e-01, -4.5169e-01],
        [ 2.9651e+00, -9.8450e+00, -2.3596e+00, -6.5597e+00,  1.0836e+00,
          1.1558e-01, -1.0882e+01, -4.4670e+00,  1.9204e-02,  5.4914e+00,
          2.6619e+00, -1.1170e+00],
        [-7.5567e-01,  2.2249e-01, -2.5743e+00,  4.1517e+00, -1.7775e+00,
         -3.7194e-01, -9.6271e-01, -6.0133e-01, -8.2419e-02, -4.4581e+00,
         -1.0756e+00,  9.8489e-03],
        [ 2.2650e+00,  2.4998e+00, -1.7871e+00, -4.9245e+00,  1.4440e+00,
         -1.9310e+00, -1.3560e+00, -1.9407e+00, -1.1788e-01,  4.0436e+00,
          3.6802e+00, -2.4835e+00],
        [ 3.6717e-02, -2.3391e-01, -1.0601e-01, -8.2123e-01,  2.4030e-02,
          2.9456e-02, -6.9152e-01, -6.6515e-02, -5.5590e-02, -1.3350e-02,
         -1.6587e-01, -1.6698e+00],
        [ 8.6593e-01, -6.7176e-01,  1.6098e-01, -2.3821e+00,  1.8067e+00,
         -1.1839e-01,  2.0264e+00,  5.8518e-01,  1.3353e-01,  3.8885e+00,
          3.6757e+00, -5.2841e-02],
        [ 3.2589e+00, -3.6488e-01, -5.1681e-01, -6.8585e-01,  2.7144e+00,
          6.5711e+00, -6.7937e-01,  1.0775e+00, -7.3879e-02,  1.0527e+00,
         -7.4676e-01, -1.6380e+00],
        [ 8.6676e+00,  3.0800e+00, -6.0425e-01, -1.0926e+01,  4.6313e+00,
         -3.4688e+00,  5.2012e-01,  7.9217e+00,  9.3558e-03,  1.1153e+00,
          7.0412e+00, -9.4553e+00],
        [ 3.6718e-02, -2.3395e-01, -1.0618e-01, -8.2097e-01,  2.4018e-02,
          2.9453e-02, -6.9148e-01, -6.6636e-02, -5.5650e-02, -1.3403e-02,
         -1.6598e-01, -1.6700e+00],
        [ 4.5149e+00, -4.6095e-01, -2.0212e-02, -2.7838e+00,  3.7561e+00,
          1.6410e+00, -9.9585e-01,  3.1656e+00, -4.4632e-02,  2.9026e+00,
         -2.0328e+00, -1.6706e+00],
        [ 3.6715e-02, -2.3397e-01, -1.0619e-01, -8.2076e-01,  2.4011e-02,
          2.9449e-02, -6.9153e-01, -6.6676e-02, -5.5655e-02, -1.3418e-02,
         -1.6597e-01, -1.6700e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  1.2174,  -3.7939,  -2.4355,  -2.2680,  -3.4679,  -3.4899,  -0.7687,
         -4.2006, -10.8933,  -3.4883,  -3.4742,  -3.4881], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-6.0433e+00, -3.8920e-02, -9.2896e-02, -9.4817e+00, -5.8243e-02,
         -1.9550e-01, -1.1188e+01, -1.2572e-01,  6.1911e-05, -1.9581e-01,
         -1.2576e-01, -1.9588e-01],
        [ 1.4488e+00, -6.2345e+00,  8.9875e+00,  1.4901e+00,  2.5034e+00,
          1.0067e-01,  8.3025e-01,  1.4110e+00,  1.1372e+01,  1.1075e-01,
          3.2906e-02,  1.1376e-01],
        [-6.6923e+00, -4.3755e-02, -9.6892e-02, -9.9860e+00, -6.2827e-02,
         -2.4219e-01, -1.0691e+01, -1.8164e-01, -3.8657e-04, -2.4257e-01,
         -1.7546e-01, -2.4265e-01],
        [-3.9610e-02,  6.8967e+00, -8.5818e+00,  9.3085e-01, -2.1767e+00,
          1.1322e-01,  1.1214e+00, -5.1616e-01, -1.1371e+01,  1.2328e-01,
          1.2294e+00,  1.2630e-01]], device='cuda:0'))])
xi:  [-149.72112]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 32.82296180980938
W_T_median: 28.574027849595055
W_T_pctile_5: -149.58935358516106
W_T_CVAR_5_pct: -248.58340796595613
Average q (qsum/M+1):  56.74585354712702
Optimal xi:  [-149.72112]
Expected(across Rb) median(across samples) p_equity:  0.825036135315895
obj fun:  tensor(-1709.4046, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: MC_conservative
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       12  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       12  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 12)      True          12  
0     (12, 12)      True          12  
0      (12, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       12  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       12  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        4           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 12)      True          12  
0     (12, 12)      True          12  
0      (12, 4)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.25 0.25 0.25 0.25]
W_T_mean: 811.8578605438024
W_T_median: 642.0338180246349
W_T_pctile_5: -264.1607927261681
W_T_CVAR_5_pct: -459.41152902166783
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1628.3497256561627
Current xi:  [84.480545]
objective value function right now is: -1628.3497256561627
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1643.0721197244793
Current xi:  [72.943344]
objective value function right now is: -1643.0721197244793
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1651.330009364312
Current xi:  [65.912]
objective value function right now is: -1651.330009364312
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1655.642146162929
Current xi:  [60.573963]
objective value function right now is: -1655.642146162929
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1659.2245995659227
Current xi:  [56.18473]
objective value function right now is: -1659.2245995659227
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1661.6698170695743
Current xi:  [52.4176]
objective value function right now is: -1661.6698170695743
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1662.8483736399562
Current xi:  [48.66773]
objective value function right now is: -1662.8483736399562
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.398323]
objective value function right now is: -1662.116714482448
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [41.57436]
objective value function right now is: -1661.2075160507484
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1664.0406501051175
Current xi:  [38.437244]
objective value function right now is: -1664.0406501051175
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1664.1574388942736
Current xi:  [35.608383]
objective value function right now is: -1664.1574388942736
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [32.810837]
objective value function right now is: -1661.5791165905675
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1664.5846550046508
Current xi:  [30.15207]
objective value function right now is: -1664.5846550046508
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [27.470432]
objective value function right now is: -1663.9589478943842
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1666.148296375021
Current xi:  [24.863028]
objective value function right now is: -1666.148296375021
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [22.415373]
objective value function right now is: -1665.3680823873688
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [20.167782]
objective value function right now is: -1663.0466390511467
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1667.3958493186988
Current xi:  [18.21326]
objective value function right now is: -1667.3958493186988
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.891679]
objective value function right now is: -1666.0172531820735
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1667.70810406035
Current xi:  [13.959057]
objective value function right now is: -1667.70810406035
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.956243]
objective value function right now is: -1666.3022651545514
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.596741]
objective value function right now is: -1667.3917947409657
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1668.3029194872763
Current xi:  [7.501534]
objective value function right now is: -1668.3029194872763
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.9172142]
objective value function right now is: -1667.7979068012678
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1668.638382966897
Current xi:  [-0.03969318]
objective value function right now is: -1668.638382966897
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06972169]
objective value function right now is: -1664.801017586224
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00380345]
objective value function right now is: -1667.9534457722355
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01872646]
objective value function right now is: -1661.1563235783428
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1668.9277394001838
Current xi:  [-0.00640773]
objective value function right now is: -1668.9277394001838
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03067394]
objective value function right now is: -1666.4328582193245
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05147446]
objective value function right now is: -1668.0319176937148
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0118943]
objective value function right now is: -1668.5315499429917
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05102518]
objective value function right now is: -1668.8711081368874
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00539962]
objective value function right now is: -1667.354655581714
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03115437]
objective value function right now is: -1665.7891280440479
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1669.2699374661763
Current xi:  [-0.00031856]
objective value function right now is: -1669.2699374661763
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1669.8592516098888
Current xi:  [-0.00163488]
objective value function right now is: -1669.8592516098888
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00016]
objective value function right now is: -1669.5737525871773
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1669.886151892362
Current xi:  [-0.00139153]
objective value function right now is: -1669.886151892362
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0008337]
objective value function right now is: -1669.7885518112555
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0052879]
objective value function right now is: -1669.5696359632727
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00064227]
objective value function right now is: -1669.5994711448539
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.4588447e-05]
objective value function right now is: -1669.7235857141093
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1669.9186276350958
Current xi:  [-0.00068552]
objective value function right now is: -1669.9186276350958
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00167891]
objective value function right now is: -1669.7369978057552
new min fval from sgd:  -1669.9455755337374
new min fval from sgd:  -1669.9669479454285
new min fval from sgd:  -1669.9745791025473
new min fval from sgd:  -1669.9850686213351
new min fval from sgd:  -1669.9866533096545
new min fval from sgd:  -1669.9880825296375
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0047139]
objective value function right now is: -1669.7720530966758
new min fval from sgd:  -1669.991496955636
new min fval from sgd:  -1670.0135493989355
new min fval from sgd:  -1670.035806129436
new min fval from sgd:  -1670.0358850687767
new min fval from sgd:  -1670.0459703295787
new min fval from sgd:  -1670.046945106536
new min fval from sgd:  -1670.050960088591
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00048599]
objective value function right now is: -1669.5147270380105
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00103785]
objective value function right now is: -1669.8395768679943
