Starting at: 
04-05-23_16:42

 numpy seed:  3  


 pytorch seed:  3  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  FF_Mkt_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                ...                          
192601                  0.0             NaN  ...     0.000561     0.023174
192602                  0.0             NaN  ...    -0.033046    -0.053510
192603                  0.0             NaN  ...    -0.064002    -0.096824
192604                  0.0             NaN  ...     0.037029     0.032975
192605                  0.0             NaN  ...     0.012095     0.001035

[5 rows x 26 columns]
               Cash_nom_ret  FF_Mkt_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                ...                          
202010                  0.0         -0.0209  ...    -0.020178     0.000584
202011                  0.0          0.1248  ...     0.123706     0.174412
202012                  0.0          0.0464  ...     0.045048     0.072853
202101                  0.0         -0.0004  ...          NaN          NaN
202102                  0.0          0.0279  ...          NaN          NaN

[5 rows x 26 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0106  ...     0.005383     0.031411
192608                    0.0319              0.0609  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0359  ...     0.005323    -0.028996
192611                   -0.0038              0.0313  ...     0.005303     0.028554

[5 rows x 6 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
201908                   -0.0645             -0.0693  ...     0.040344    -0.020339
201909                    0.0276              0.0574  ...    -0.013852     0.016033
201910                    0.0092              0.0225  ...    -0.000742     0.019256
201911                    0.0526              0.0302  ...    -0.007410     0.034997
201912                    0.0578              0.0439  ...    -0.011292     0.028491

[5 rows x 6 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'CPI_nom_ret_ind', 'T30_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000347
B10_real_ret           0.001910
VWD_real_ret           0.006882
Size_Lo30_real_ret     0.010078
Value_Hi30_real_ret    0.010111
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005195
B10_real_ret           0.018984
VWD_real_ret           0.053322
Size_Lo30_real_ret     0.082573
Value_Hi30_real_ret    0.071650
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Value_Hi30_real_ret
T30_real_ret             1.000000  ...             0.016933
B10_real_ret             0.345630  ...             0.037604
VWD_real_ret             0.064422  ...             0.911324
Size_Lo30_real_ret       0.009866  ...             0.910471
Value_Hi30_real_ret      0.016933  ...             1.000000

[5 rows x 5 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 201912
-----------------------------------------------
Bootstrap block size: 6
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       5       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       5           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 5)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 1138.474873568312
W_T_median: 891.4398755591162
W_T_pctile_5: -169.98079142996517
W_T_CVAR_5_pct: -317.5137579687884
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1768.028502643482
Current xi:  [78.185555]
objective value function right now is: -1768.028502643482
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1789.3513297411112
Current xi:  [56.511337]
objective value function right now is: -1789.3513297411112
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1804.803144997994
Current xi:  [45.327015]
objective value function right now is: -1804.803144997994
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1805.5491748244144
Current xi:  [31.180658]
objective value function right now is: -1805.5491748244144
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1806.6875400101524
Current xi:  [14.708448]
objective value function right now is: -1806.6875400101524
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1806.861724049192
Current xi:  [-1.0625956]
objective value function right now is: -1806.861724049192
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1807.4594884865219
Current xi:  [-9.74084]
objective value function right now is: -1807.4594884865219
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1808.1671412983942
Current xi:  [-26.23254]
objective value function right now is: -1808.1671412983942
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-37.532364]
objective value function right now is: -1807.2777395993603
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-49.150627]
objective value function right now is: -1808.1593020158753
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1808.2178828462354
Current xi:  [-66.05826]
objective value function right now is: -1808.2178828462354
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1808.8188611622943
Current xi:  [-73.214294]
objective value function right now is: -1808.8188611622943
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1808.936127786955
Current xi:  [-80.66903]
objective value function right now is: -1808.936127786955
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-94.526115]
objective value function right now is: -1808.7116998307995
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1809.0408657521657
Current xi:  [-106.550125]
objective value function right now is: -1809.0408657521657
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1809.2100356693422
Current xi:  [-107.16985]
objective value function right now is: -1809.2100356693422
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-107.12327]
objective value function right now is: -1808.8016241433745
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1809.4623517848584
Current xi:  [-107.332214]
objective value function right now is: -1809.4623517848584
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.89139]
objective value function right now is: -1809.2724500939453
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.9064]
objective value function right now is: -1808.3118058789453
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1809.475395372562
Current xi:  [-106.63418]
objective value function right now is: -1809.475395372562
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.51348]
objective value function right now is: -1808.4342541964402
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.76479]
objective value function right now is: -1809.3737130877453
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.43718]
objective value function right now is: -1809.1919592556685
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.56588]
objective value function right now is: -1808.9859487554634
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-107.127106]
objective value function right now is: -1809.2904901084817
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-107.07322]
objective value function right now is: -1809.414607742244
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-106.28724]
objective value function right now is: -1809.1136396257104
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1809.5202781351254
Current xi:  [-106.76014]
objective value function right now is: -1809.5202781351254
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.34354]
objective value function right now is: -1809.2621600991683
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-107.247925]
objective value function right now is: -1809.3017862575862
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.659996]
objective value function right now is: -1808.1149522285746
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-107.09094]
objective value function right now is: -1808.9966311107855
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.84853]
objective value function right now is: -1809.434115917198
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.79535]
objective value function right now is: -1809.4015422601035
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1809.6164186316016
Current xi:  [-106.29654]
objective value function right now is: -1809.6164186316016
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.222855]
objective value function right now is: -1809.5063207376618
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.14567]
objective value function right now is: -1809.561027086289
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1809.6225790352453
Current xi:  [-106.04648]
objective value function right now is: -1809.6225790352453
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-105.911804]
objective value function right now is: -1809.5684565579245
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1809.6284957664848
Current xi:  [-106.093445]
objective value function right now is: -1809.6284957664848
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-105.84215]
objective value function right now is: -1809.600878550458
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.15467]
objective value function right now is: -1809.43478303756
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.09205]
objective value function right now is: -1809.6064118548675
new min fval from sgd:  -1809.6745859285495
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-105.95679]
objective value function right now is: -1809.6745859285495
new min fval from sgd:  -1809.6773636207067
new min fval from sgd:  -1809.6774647966054
new min fval from sgd:  -1809.6784691682008
new min fval from sgd:  -1809.6821362666303
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.04774]
objective value function right now is: -1809.591191654217
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-105.94157]
objective value function right now is: -1809.6238036017323
new min fval from sgd:  -1809.6838518872726
new min fval from sgd:  -1809.6863753949203
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-105.837296]
objective value function right now is: -1809.4942364692456
new min fval from sgd:  -1809.6868685091426
new min fval from sgd:  -1809.6875479384503
new min fval from sgd:  -1809.6883444129219
new min fval from sgd:  -1809.688567892049
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-105.99946]
objective value function right now is: -1809.6812313117618
new min fval from sgd:  -1809.6897574779746
new min fval from sgd:  -1809.692256848251
new min fval from sgd:  -1809.6929255505736
new min fval from sgd:  -1809.6936271178774
new min fval from sgd:  -1809.6939558012411
new min fval from sgd:  -1809.694036255308
new min fval from sgd:  -1809.6943375697542
new min fval from sgd:  -1809.6946501041934
new min fval from sgd:  -1809.695193919919
new min fval from sgd:  -1809.6954301990406
new min fval from sgd:  -1809.6955735022048
new min fval from sgd:  -1809.695841025412
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-105.89313]
objective value function right now is: -1809.6813131058695
min fval:  -1809.695841025412
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -0.5448,   4.9815],
        [-11.2190,   5.0415],
        [ -0.1984,   1.1038],
        [  0.1972,   4.9215],
        [ -0.1984,   1.1038],
        [ -0.1984,   1.1038],
        [ 10.8644,   0.5850],
        [  5.8310,   9.0238],
        [ -0.1984,   1.1038],
        [ -0.1984,   1.1038],
        [ 10.0973,   3.5341],
        [ 11.1774,   0.3604],
        [ -0.1984,   1.1038]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 3.7413,  6.7544, -0.9277,  3.2615, -0.9277, -0.9277, -8.3267,  8.8024,
        -0.9277, -0.9277, -3.1127, -9.2957, -0.9277], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.0147, -0.0299, -0.0201, -0.0128, -0.0201, -0.0201, -0.0418, -0.0975,
         -0.0201, -0.0201, -0.0222, -0.0383, -0.0201],
        [-0.8823, -5.1312,  0.0223, -0.7225,  0.0228,  0.0223, -3.4577, -7.0258,
          0.0223,  0.0223, -1.7175, -4.3522,  0.0239],
        [ 0.5244,  3.3293,  0.0442,  0.3916,  0.0438,  0.0442,  1.9752,  4.4816,
          0.0442,  0.0442,  0.7923,  2.4487,  0.0430],
        [ 0.5656,  3.5762,  0.0348,  0.4268,  0.0344,  0.0348,  2.2155,  4.7905,
          0.0348,  0.0348,  0.9048,  2.7600,  0.0335],
        [-0.0147, -0.0299, -0.0201, -0.0128, -0.0201, -0.0201, -0.0418, -0.0975,
         -0.0201, -0.0201, -0.0222, -0.0383, -0.0201],
        [ 1.1758,  6.0665, -0.0167,  0.9964, -0.0163, -0.0167,  3.9857,  7.9798,
         -0.0167, -0.0167,  1.9411,  5.0626, -0.0154],
        [-0.0147, -0.0299, -0.0201, -0.0128, -0.0201, -0.0201, -0.0418, -0.0975,
         -0.0201, -0.0201, -0.0222, -0.0383, -0.0201],
        [-0.0147, -0.0299, -0.0201, -0.0128, -0.0201, -0.0201, -0.0418, -0.0975,
         -0.0201, -0.0201, -0.0222, -0.0383, -0.0201],
        [-0.0147, -0.0299, -0.0201, -0.0128, -0.0201, -0.0201, -0.0418, -0.0975,
         -0.0201, -0.0201, -0.0222, -0.0383, -0.0201],
        [-0.0147, -0.0299, -0.0201, -0.0128, -0.0201, -0.0201, -0.0418, -0.0975,
         -0.0201, -0.0201, -0.0222, -0.0383, -0.0201],
        [-1.1430, -6.0410,  0.0291, -0.9817,  0.0287,  0.0291, -4.0280, -8.1659,
          0.0291,  0.0291, -2.0099, -5.1625,  0.0280],
        [-0.0147, -0.0299, -0.0201, -0.0128, -0.0201, -0.0201, -0.0418, -0.0975,
         -0.0201, -0.0201, -0.0222, -0.0383, -0.0201],
        [-0.0119, -0.0709, -0.0121, -0.0083, -0.0121, -0.0121, -0.1168, -0.2328,
         -0.0121, -0.0121, -0.0485, -0.1051, -0.0121]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.3521,  4.6399, -3.0141, -3.2108, -0.3521, -5.4275, -0.3521, -0.3521,
        -0.3521, -0.3521,  5.4711, -0.3521, -0.5395], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-5.5984e-03, -8.1214e+00,  4.3754e+00,  4.8556e+00, -5.5983e-03,
          1.0210e+01, -5.5983e-03, -5.5983e-03, -5.5983e-03, -5.5983e-03,
         -1.0122e+01, -5.5984e-03, -6.4945e-02]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-1.5585, -0.5557],
        [ 2.3244,  5.7005],
        [-9.0479,  0.3702],
        [-2.5081, -0.5453],
        [ 7.0797,  2.7386],
        [-3.3911, -6.9716],
        [ 9.6950, -0.3889],
        [ 6.7624,  2.4309],
        [ 7.7130,  3.7511],
        [-4.1180, -5.9741],
        [ 0.1364,  9.0038],
        [-3.5287,  7.4713],
        [-1.5590, -0.5555]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-3.8862,  3.4174,  7.4650, -3.2162,  1.1102, -7.0996, -9.8191,  0.7106,
         2.7241, -6.3967,  8.4295,  6.8838, -3.8842], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.4230, -2.0390,  4.4893, -2.0396, -2.9295,  4.6458, -7.0249, -2.6802,
         -2.1431,  2.2411, -8.1740, -5.9432, -2.4170],
        [ 2.0134,  1.0286, -3.2436,  1.7844,  0.9220, -3.3341,  4.3194,  0.7270,
          0.0414, -1.7689,  4.9092,  2.2053,  2.0134],
        [-1.5496,  0.2222,  5.2814, -0.8335, -4.9875,  2.3637, -5.4553, -4.6167,
         -5.7510,  4.0941,  0.2084,  1.0956, -1.5489],
        [-3.0952,  1.0696, -7.2767, -2.5803,  1.4821, -0.2697,  4.4460,  1.1652,
          1.7369, -0.5340, -0.7383, -4.2227, -3.0854],
        [-0.4033, -0.3434, -0.3804, -0.4047, -0.7349, -0.5067, -0.5866, -0.7230,
         -0.7689, -0.4590, -0.3873, -0.2127, -0.4033],
        [ 0.0640, -0.4337, -7.4795,  0.0672, -0.1682,  0.3304,  7.0196, -0.1720,
         -0.3947, -0.5505, -1.8728, -1.0197,  0.0638],
        [-0.4033, -0.3434, -0.3804, -0.4047, -0.7349, -0.5067, -0.5866, -0.7230,
         -0.7689, -0.4590, -0.3873, -0.2127, -0.4033],
        [ 0.3769, -0.9851, -5.5795,  0.3140,  1.1839,  1.8124,  1.2725,  1.4159,
          1.2871,  0.2946, -4.9656, -1.1548,  0.3770],
        [-0.5148,  0.5716, -0.4047, -0.7070,  3.5430, -2.5209,  0.3070,  3.2351,
          4.0543, -3.5664,  0.7451,  0.3850, -0.5149],
        [-1.0174,  0.6242, -1.0090, -1.1405,  2.6147, -2.1563,  0.9753,  2.4713,
          3.2338, -2.2026,  2.4579,  2.0370, -1.0176],
        [-0.2349,  0.3080,  3.4844, -0.2292, -3.2914, -2.7196, -3.4517, -2.9003,
         -3.4806, -0.9499,  5.0554,  2.5789, -0.2351],
        [-0.4033, -0.3434, -0.3804, -0.4047, -0.7349, -0.5067, -0.5866, -0.7230,
         -0.7689, -0.4590, -0.3873, -0.2127, -0.4033],
        [ 1.7084, -0.3602, -0.8062,  1.6567, -1.7059,  0.0130,  0.6819, -1.6875,
         -1.7966,  0.2635, -0.3659, -0.4405,  1.7083]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 0.3641, -1.0828, -4.2378,  0.9596, -1.3045, -0.9579, -1.3045,  0.3335,
         0.8127,  0.7756, -2.3363, -1.3045, -0.6180], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.9686,  -1.5961,  10.6004,  -0.9046,  -0.0119,  -1.6195,  -0.0119,
          -2.3301,  -6.7529,  -6.7677,  -2.7633,  -0.0119,  -0.9185],
        [ -6.7605,   0.9032,   0.8470,   0.0524,   0.0164,   1.2380,   0.0166,
           0.3538,  -0.2095,   1.0015,   1.9648,   0.0166,  -0.5257],
        [ -0.9154,  -2.4808,  -0.5196,  -0.6984,  -0.1407,  -1.4504,  -0.1407,
          -1.8519,  -3.5169,  -3.4975,  -0.2657,  -0.1407,  -1.5086],
        [ -0.8492,  -2.3344,  -0.5620,  -0.6656,  -0.1234,  -1.3559,  -0.1234,
          -1.7402,  -3.3198,  -3.2989,  -0.2234,  -0.1234,  -1.4184],
        [  6.9672,   0.0654, -10.1260,   0.6217,   0.0623,   0.2123,   0.0624,
           0.9620,   1.6335,  -0.2842,  -1.1921,   0.0624,   1.1050]],
       device='cuda:0'))])
xi:  [-105.932106]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 835.6385437978039
W_T_median: 494.1945295805484
W_T_pctile_5: -105.9607364905928
W_T_CVAR_5_pct: -223.58196279518836
Average q (qsum/M+1):  58.73787959929435
Optimal xi:  [-105.932106]
Expected(across Rb) median(across samples) p_equity:  0.7020774647593498
obj fun:  tensor(-1809.6958, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 1138.474873568312
W_T_median: 891.4398755591162
W_T_pctile_5: -169.98079142996517
W_T_CVAR_5_pct: -317.5137579687884
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1774.2932386416132
Current xi:  [94.76348]
objective value function right now is: -1774.2932386416132
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1780.0927565309
Current xi:  [94.21553]
objective value function right now is: -1780.0927565309
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1781.2794972502445
Current xi:  [95.749855]
objective value function right now is: -1781.2794972502445
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [97.43782]
objective value function right now is: -1780.5855861748375
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [99.88344]
objective value function right now is: -1780.961535846589
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [101.61166]
objective value function right now is: -1780.4022145426188
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1781.375499246705
Current xi:  [102.73151]
objective value function right now is: -1781.375499246705
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.45101]
objective value function right now is: -1780.9464101195042
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.54038]
objective value function right now is: -1779.9039822111974
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1781.8199501312756
Current xi:  [106.66432]
objective value function right now is: -1781.8199501312756
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1782.3912551968162
Current xi:  [106.264084]
objective value function right now is: -1782.3912551968162
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.16475]
objective value function right now is: -1782.2751182745978
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.38135]
objective value function right now is: -1781.25969909776
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [107.74764]
objective value function right now is: -1781.807016471739
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.056595]
objective value function right now is: -1782.3861259680614
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.59794]
objective value function right now is: -1781.713446640144
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.754425]
objective value function right now is: -1781.8313065844884
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.08054]
objective value function right now is: -1781.2392000353154
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.98762]
objective value function right now is: -1782.26622823351
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1782.6355183712974
Current xi:  [108.1061]
objective value function right now is: -1782.6355183712974
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1782.8015085314826
Current xi:  [108.58893]
objective value function right now is: -1782.8015085314826
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1782.8482058441275
Current xi:  [108.69066]
objective value function right now is: -1782.8482058441275
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.36856]
objective value function right now is: -1782.3718939583637
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.88786]
objective value function right now is: -1782.0379373886647
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.29504]
objective value function right now is: -1781.809548443996
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.43033]
objective value function right now is: -1782.363415514758
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.10538]
objective value function right now is: -1782.5607806947949
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [108.14753]
objective value function right now is: -1782.597987342185
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [108.52878]
objective value function right now is: -1782.0735724117108
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.61778]
objective value function right now is: -1782.2648798271446
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1782.923909771592
Current xi:  [109.00431]
objective value function right now is: -1782.923909771592
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.08221]
objective value function right now is: -1782.3271229931217
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.315025]
objective value function right now is: -1782.6060108766997
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.97519]
objective value function right now is: -1781.5921523262296
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.34236]
objective value function right now is: -1782.859468583595
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1783.2093737702724
Current xi:  [108.44897]
objective value function right now is: -1783.2093737702724
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1783.2647406263911
Current xi:  [108.59027]
objective value function right now is: -1783.2647406263911
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.68591]
objective value function right now is: -1783.158510725045
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.75568]
objective value function right now is: -1783.251656481229
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.86485]
objective value function right now is: -1783.2322299877458
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.86259]
objective value function right now is: -1783.1712323618326
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.95473]
objective value function right now is: -1783.1894466071114
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.860825]
objective value function right now is: -1783.22444795405
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.81893]
objective value function right now is: -1783.2459112789481
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.97283]
objective value function right now is: -1783.1583532166687
new min fval from sgd:  -1783.2707976992829
new min fval from sgd:  -1783.2916293766373
new min fval from sgd:  -1783.3126794783213
new min fval from sgd:  -1783.325645112477
new min fval from sgd:  -1783.326284959495
new min fval from sgd:  -1783.3312986544788
new min fval from sgd:  -1783.33717238061
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.05035]
objective value function right now is: -1783.1990833385462
new min fval from sgd:  -1783.3389417675087
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.092926]
objective value function right now is: -1783.1840409137294
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.03708]
objective value function right now is: -1783.2382516017385
new min fval from sgd:  -1783.3430405413662
new min fval from sgd:  -1783.343160286467
new min fval from sgd:  -1783.3450526373053
new min fval from sgd:  -1783.3485429336995
new min fval from sgd:  -1783.3522410870992
new min fval from sgd:  -1783.357126278895
new min fval from sgd:  -1783.3614942647373
new min fval from sgd:  -1783.3628441559056
new min fval from sgd:  -1783.3632461972256
new min fval from sgd:  -1783.3654151179603
new min fval from sgd:  -1783.3655952903014
new min fval from sgd:  -1783.3675620278582
new min fval from sgd:  -1783.3675750574507
new min fval from sgd:  -1783.367604991965
new min fval from sgd:  -1783.368658677625
new min fval from sgd:  -1783.3694119809945
new min fval from sgd:  -1783.3707152896125
new min fval from sgd:  -1783.3709997021188
new min fval from sgd:  -1783.3734194032781
new min fval from sgd:  -1783.3737792190993
new min fval from sgd:  -1783.374908765473
new min fval from sgd:  -1783.3754127439306
new min fval from sgd:  -1783.375944049207
new min fval from sgd:  -1783.3763791752906
new min fval from sgd:  -1783.3770647515985
new min fval from sgd:  -1783.3777063328907
new min fval from sgd:  -1783.3807402362982
new min fval from sgd:  -1783.381666583578
new min fval from sgd:  -1783.382436764211
new min fval from sgd:  -1783.3825443022233
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.16544]
objective value function right now is: -1783.3707216282992
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.15177]
objective value function right now is: -1783.3699924486255
min fval:  -1783.3825443022233
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-27.4193,  -3.0640],
        [  1.6440,   6.0462],
        [  1.4182,   5.7601],
        [  1.8946,   6.1474],
        [-17.4430,   4.6301],
        [  1.7809,   6.0646],
        [ -3.2937,  -4.5061],
        [ -3.6052,  -5.0399],
        [ -0.5695,  -5.1054],
        [  2.1892,   6.2793],
        [ -3.8324,  -5.5411],
        [  1.3981,   5.6450],
        [ -0.1237,   1.3278]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-3.4665,  3.7394,  3.1007,  3.9473,  5.5996,  3.7783, -4.3590, -4.4339,
        -4.3720,  4.2351, -4.7003,  2.8424, -1.5628], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 2.5044, -1.3001, -0.7147, -1.6094, -2.1733, -1.4138,  1.3561,  1.6474,
          0.1426, -2.0168,  2.2099, -0.5779, -0.0211],
        [-0.0067, -0.1102, -0.0688, -0.1326, -0.0915, -0.1176, -0.1751, -0.2230,
         -0.4347, -0.1671, -0.2449, -0.0570, -0.0200],
        [ 4.7079, -2.5648, -1.7793, -2.6966, -4.2775, -2.4625,  1.3332,  1.9557,
          2.6529, -2.9678,  2.7232, -1.4970, -0.1105],
        [-0.0067, -0.1102, -0.0688, -0.1326, -0.0915, -0.1176, -0.1751, -0.2230,
         -0.4347, -0.1671, -0.2449, -0.0570, -0.0200],
        [ 0.0739, -0.1803, -0.1230, -0.2122, -0.5341, -0.1883, -0.1463, -0.1153,
         -0.5070, -0.2727, -0.0743, -0.1066, -0.0445],
        [-5.9091,  3.0526,  2.2903,  3.0700,  4.4306,  2.9685, -1.4868, -2.2511,
         -2.9180,  3.3480, -2.9176,  1.9641,  0.0702],
        [ 6.1990, -3.2345, -2.3015, -3.8470, -2.6985, -3.5986,  3.1311,  3.4695,
          0.9508, -4.4587,  4.2591, -2.1659,  0.2486],
        [-4.9855,  2.2080,  1.6042,  2.3838,  3.8049,  2.2497, -0.9931, -1.8327,
         -2.4111,  2.6483, -2.3305,  1.3703,  0.1692],
        [-0.0067, -0.1102, -0.0688, -0.1326, -0.0915, -0.1176, -0.1751, -0.2230,
         -0.4347, -0.1671, -0.2449, -0.0570, -0.0200],
        [-5.5805,  3.0666,  2.2448,  3.4206,  3.0735,  3.0767, -2.3909, -3.1885,
         -1.3093,  3.8087, -3.5443,  2.0288, -0.0622],
        [-4.9964,  2.2928,  1.6664,  2.4302,  3.8388,  2.2836, -1.1445, -1.7916,
         -2.5440,  2.6905, -2.5982,  1.3958,  0.1640],
        [ 3.3654, -1.5484, -0.8862, -1.8808, -2.2697, -1.6668,  1.6377,  1.9685,
          0.4640, -2.3061,  2.5025, -0.7339,  0.0117],
        [-0.0067, -0.1102, -0.0688, -0.1326, -0.0915, -0.1176, -0.1751, -0.2230,
         -0.4347, -0.1671, -0.2449, -0.0570, -0.0200]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.0891, -0.7195,  0.3982, -0.7195, -1.2426, -0.7655, -0.8213, -0.6956,
        -0.7195,  0.3284, -0.5331, -1.1321, -0.7195], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-2.1546e+00, -2.8415e-03, -4.1425e+00, -2.8415e-03, -1.8998e-01,
          6.2780e+00, -6.5454e+00,  4.3297e+00, -2.8415e-03,  5.7933e+00,
          4.4656e+00, -2.5867e+00, -2.8415e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-1.8395,  2.7129],
        [-1.6234,  0.5981],
        [-7.7627, -0.3968],
        [14.0214,  5.8924],
        [-2.6932, 10.9161],
        [-1.6164,  0.5930],
        [-1.6246,  0.5991],
        [ 8.2469,  7.8832],
        [-6.1322, -7.2559],
        [ 9.9933, -0.7365],
        [ 3.3069, -0.8839],
        [10.3110, -0.5446],
        [ 6.2503, -5.8150]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -2.9289,  -2.5406,   5.0678,   0.3627,   9.2281,  -2.5414,  -2.5405,
          6.3350,  -6.9545, -10.2317,  -7.4052, -10.1698,  -5.1490],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.1319e-01,  2.9096e-01,  3.0937e-01, -5.1651e+00, -1.6734e+01,
          2.7858e-01,  2.9316e-01,  2.3746e+00, -5.6274e-01, -3.8493e+00,
         -9.5915e-01, -3.6169e+00,  1.8319e+00],
        [ 7.0438e-03, -9.8765e-04, -8.0580e-01, -1.4444e+00, -5.0061e-01,
         -1.0580e-03, -9.7425e-04, -1.2157e+00, -5.8459e-01, -5.1133e-01,
         -1.4502e-01, -5.6229e-01, -1.6381e+00],
        [-5.4782e-02,  2.0757e-01,  2.3828e+00, -9.0628e+00, -1.1470e+01,
          2.0313e-01,  2.0834e-01, -1.2172e+01,  3.7069e+00, -7.7470e+00,
         -8.4761e-01, -8.6579e+00,  1.8675e+00],
        [ 7.1802e-03, -9.5000e-04, -8.0509e-01, -1.4444e+00, -5.0041e-01,
         -1.0203e-03, -9.3662e-04, -1.2149e+00, -5.8485e-01, -5.1154e-01,
         -1.4542e-01, -5.6251e-01, -1.6383e+00],
        [ 7.1071e-03, -9.7022e-04, -8.0548e-01, -1.4444e+00, -5.0052e-01,
         -1.0405e-03, -9.5684e-04, -1.2153e+00, -5.8471e-01, -5.1143e-01,
         -1.4520e-01, -5.6239e-01, -1.6382e+00],
        [ 1.7355e+00,  1.1367e-01, -1.7311e+00, -8.5874e-01, -3.5069e+00,
          1.0206e-01,  1.1587e-01, -1.2835e-01, -3.4636e+00,  4.7155e+00,
          8.5108e-01,  5.0353e+00,  1.3769e+00],
        [ 7.1802e-03, -9.4997e-04, -8.0509e-01, -1.4444e+00, -5.0041e-01,
         -1.0203e-03, -9.3660e-04, -1.2149e+00, -5.8485e-01, -5.1154e-01,
         -1.4542e-01, -5.6251e-01, -1.6383e+00],
        [ 1.4727e+00,  1.4110e-01,  3.9059e+00, -3.3137e+00,  6.9012e+00,
          1.2964e-01,  1.4314e-01, -2.2599e+00, -1.2410e+00, -4.0612e+00,
         -7.8249e-02, -5.7457e+00, -3.2716e+00],
        [ 7.1542e-03, -9.5720e-04, -8.0523e-01, -1.4444e+00, -5.0045e-01,
         -1.0275e-03, -9.4382e-04, -1.2151e+00, -5.8480e-01, -5.1150e-01,
         -1.4534e-01, -5.6247e-01, -1.6383e+00],
        [ 7.1846e-03, -9.4876e-04, -8.0507e-01, -1.4444e+00, -5.0041e-01,
         -1.0190e-03, -9.3537e-04, -1.2149e+00, -5.8486e-01, -5.1155e-01,
         -1.4544e-01, -5.6252e-01, -1.6383e+00],
        [ 8.7391e-01, -3.0530e-02,  2.6762e+00, -9.3027e-01,  3.7926e+00,
         -3.5531e-02, -2.9590e-02, -1.4421e-01,  2.0954e+00, -2.2998e+00,
          1.0554e+00, -3.4052e+00, -1.3197e+00],
        [ 1.3594e+00,  2.0036e-01, -1.0095e+00, -8.0021e-01,  8.7529e-01,
          1.9979e-01,  2.0054e-01, -3.6440e-01,  1.1917e-01,  1.5775e-01,
          5.1862e-01,  2.1414e-01, -4.6089e+00],
        [ 2.7299e-02,  1.3029e-03, -1.0905e+00, -1.8835e+00, -1.9927e-01,
          1.1547e-03,  1.3347e-03, -1.4960e+00, -1.0489e+00,  3.3693e-01,
         -7.7090e-01,  7.6276e-01, -1.5161e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 1.6894, -1.9467,  0.0116, -1.9467, -1.9467,  0.4407, -1.9467, -2.6117,
        -1.9467, -1.9467, -0.9375, -1.8611, -2.6310], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.1086e+01,  7.5686e-02, -1.9976e+00,  7.5069e-02,  7.5385e-02,
         -1.1610e+01,  7.5068e-02,  3.8522e+00,  7.5175e-02,  7.5050e-02,
          1.9220e+00,  1.6149e+00,  1.0886e-01],
        [ 4.7196e-01, -3.9595e-02, -6.2809e+00, -4.2239e-02, -4.0830e-02,
          5.1823e-01, -4.2241e-02, -6.2344e-01, -4.1741e-02, -4.2324e-02,
          1.3394e+00, -1.1914e+00,  3.3863e-01],
        [-2.9999e+00, -2.8762e-02, -1.1846e-02, -2.8781e-02, -2.8770e-02,
         -1.0962e+01, -2.8781e-02, -1.1811e-01, -2.8777e-02, -2.8781e-02,
         -1.0319e+01, -1.0338e-01, -1.2737e-02],
        [-3.0144e+00, -3.2550e-02, -1.3117e-02, -3.2573e-02, -3.2560e-02,
         -1.0357e+01, -3.2573e-02, -1.5191e-01, -3.2568e-02, -3.2573e-02,
         -9.5572e+00, -1.4187e-01, -1.3336e-02],
        [ 1.4280e+00,  2.1419e-03,  6.7391e+00, -5.3612e-04,  8.9219e-04,
          9.6695e-01, -5.3831e-04, -7.4492e-01, -3.1595e-05, -6.2347e-04,
          1.4097e+00,  6.2159e-01,  7.1333e-01]], device='cuda:0'))])
xi:  [109.16449]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 218.34226463297568
W_T_median: 198.7428259102094
W_T_pctile_5: 109.23891863719528
W_T_CVAR_5_pct: -32.64612463457819
Average q (qsum/M+1):  57.73909242691532
Optimal xi:  [109.16449]
Expected(across Rb) median(across samples) p_equity:  0.18421252880555888
obj fun:  tensor(-1783.3825, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 1138.474873568312
W_T_median: 891.4398755591162
W_T_pctile_5: -169.98079142996517
W_T_CVAR_5_pct: -317.5137579687884
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1775.9034273893462
Current xi:  [119.3917]
objective value function right now is: -1775.9034273893462
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1780.5500959450317
Current xi:  [138.53627]
objective value function right now is: -1780.5500959450317
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1793.193925311365
Current xi:  [159.8968]
objective value function right now is: -1793.193925311365
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1797.6913378202257
Current xi:  [179.56482]
objective value function right now is: -1797.6913378202257
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1801.2671517020194
Current xi:  [198.13162]
objective value function right now is: -1801.2671517020194
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1803.7807820531712
Current xi:  [216.02669]
objective value function right now is: -1803.7807820531712
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [233.35184]
objective value function right now is: -1801.4571817407293
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1806.2854494324424
Current xi:  [249.14485]
objective value function right now is: -1806.2854494324424
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1808.6982595195075
Current xi:  [265.15732]
objective value function right now is: -1808.6982595195075
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1810.3129685284068
Current xi:  [280.32434]
objective value function right now is: -1810.3129685284068
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [295.31744]
objective value function right now is: -1809.4252478872872
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1812.1011786016775
Current xi:  [309.84622]
objective value function right now is: -1812.1011786016775
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1816.388502193841
Current xi:  [324.2186]
objective value function right now is: -1816.388502193841
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1816.9100647162625
Current xi:  [336.89487]
objective value function right now is: -1816.9100647162625
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [349.83725]
objective value function right now is: -1816.335781859352
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1819.4004046257642
Current xi:  [362.94025]
objective value function right now is: -1819.4004046257642
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1819.4172639374071
Current xi:  [375.1216]
objective value function right now is: -1819.4172639374071
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1821.2505446908804
Current xi:  [386.46716]
objective value function right now is: -1821.2505446908804
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [396.14258]
objective value function right now is: -1820.9695577918508
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1821.7041381166946
Current xi:  [406.07892]
objective value function right now is: -1821.7041381166946
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1822.040125468194
Current xi:  [414.803]
objective value function right now is: -1822.040125468194
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1823.4816485841186
Current xi:  [422.68384]
objective value function right now is: -1823.4816485841186
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [429.1115]
objective value function right now is: -1821.7071659028677
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [434.55743]
objective value function right now is: -1822.7288929843455
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [440.06567]
objective value function right now is: -1819.9141333711432
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [445.0016]
objective value function right now is: -1823.2929963409724
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1823.5081630521017
Current xi:  [449.2364]
objective value function right now is: -1823.5081630521017
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [452.99112]
objective value function right now is: -1822.4710907236943
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [456.03387]
objective value function right now is: -1823.0710801059126
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [461.05566]
objective value function right now is: -1814.4216998580787
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [462.92117]
objective value function right now is: -1821.5273207006628
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1824.0931949356852
Current xi:  [463.45618]
objective value function right now is: -1824.0931949356852
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [465.61313]
objective value function right now is: -1823.6638690721595
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [467.34366]
objective value function right now is: -1821.7620542124398
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [468.39047]
objective value function right now is: -1822.1857380317047
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1825.2530751062875
Current xi:  [468.47122]
objective value function right now is: -1825.2530751062875
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [468.6243]
objective value function right now is: -1825.215494080598
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [468.99002]
objective value function right now is: -1825.1032305652718
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [469.13065]
objective value function right now is: -1824.9147292099594
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [469.32416]
objective value function right now is: -1824.933744325602
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [469.65723]
objective value function right now is: -1824.9032022865722
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1829.092676128633
Current xi:  [473.02985]
objective value function right now is: -1829.092676128633
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [475.87375]
objective value function right now is: -1828.7528447616016
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [478.33374]
objective value function right now is: -1828.5421670936294
new min fval from sgd:  -1829.7042943668803
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [480.88336]
objective value function right now is: -1829.7042943668803
new min fval from sgd:  -1829.7214472911214
new min fval from sgd:  -1829.734541719014
new min fval from sgd:  -1829.7420394423823
new min fval from sgd:  -1829.7859983490082
new min fval from sgd:  -1829.8077736238145
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [483.10034]
objective value function right now is: -1829.7064918934357
new min fval from sgd:  -1829.8331484831215
new min fval from sgd:  -1829.8714967773515
new min fval from sgd:  -1829.8909080742542
new min fval from sgd:  -1829.891624441557
new min fval from sgd:  -1829.9221191828638
new min fval from sgd:  -1829.9428430134158
new min fval from sgd:  -1829.958529167469
new min fval from sgd:  -1829.971525021834
new min fval from sgd:  -1829.9762072565734
new min fval from sgd:  -1829.99053041706
new min fval from sgd:  -1829.9932885734868
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [485.32166]
objective value function right now is: -1829.009054363031
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [485.4007]
objective value function right now is: -1823.7488906665583
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [485.4157]
objective value function right now is: -1825.7181045126808
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [485.5175]
objective value function right now is: -1826.4381642696528
min fval:  -1829.9932885734868
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-13.9193,  -9.4814],
        [ -4.0635, -10.0859],
        [ -3.7680,   6.3815],
        [ -4.1112,   6.5912],
        [ -2.6124,   2.8879],
        [ -4.6375,   6.1590],
        [ -3.9421,   6.5035],
        [  0.2053,   2.6008],
        [  7.1332,  -0.7121],
        [  8.4325,  -1.7700],
        [ -4.1473,   5.1382],
        [  8.0891,   1.3692],
        [  8.4909,   2.4764]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-0.5965, -5.4013,  4.2614,  3.9439, -1.4830,  0.6895,  4.1248, -1.6968,
        -8.3507, -8.5455, -0.2842, -8.2528, -8.4217], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.0851e-01,  1.0838e-01, -1.7656e+00, -1.2170e+00,  4.5265e-03,
         -4.2701e-02, -1.4878e+00,  2.6715e-02,  5.6006e-01,  1.6241e+00,
         -1.6665e-02,  1.1955e+00,  1.4190e+00],
        [ 4.4284e+00,  4.4435e+00, -3.0776e+00, -2.6918e+00, -4.9765e-03,
         -1.9605e-01, -2.8261e+00,  1.4640e-02,  7.8317e-01,  2.4084e+00,
         -1.2010e-01,  1.1927e+00,  1.4787e+00],
        [-2.7029e-01, -3.8280e-01,  3.8048e-01,  4.2380e-01, -1.7282e-01,
         -2.4522e-01,  4.1204e-01,  2.3622e-01,  4.3298e-01,  4.3465e-01,
         -2.5213e-01,  6.7016e-01,  7.5427e-01],
        [ 5.9008e+00,  5.8063e+00, -3.7487e+00, -3.3579e+00, -6.2299e-02,
         -6.3839e-01, -3.4991e+00,  8.9013e-02,  1.0563e+00,  2.7656e+00,
         -4.2648e-01,  1.3942e+00,  1.5627e+00],
        [-2.7011e-01, -3.8274e-01,  3.8010e-01,  4.2347e-01, -1.7272e-01,
         -2.4510e-01,  4.1169e-01,  2.3584e-01,  4.3247e-01,  4.3428e-01,
         -2.5198e-01,  6.6947e-01,  7.5358e-01],
        [-2.7011e-01, -3.8274e-01,  3.8010e-01,  4.2347e-01, -1.7272e-01,
         -2.4510e-01,  4.1169e-01,  2.3584e-01,  4.3247e-01,  4.3428e-01,
         -2.5198e-01,  6.6947e-01,  7.5358e-01],
        [ 6.8954e+00,  8.9894e+00, -5.0100e+00, -4.5978e+00, -8.6199e-01,
         -1.8107e+00, -4.6836e+00,  7.8542e-01,  3.8400e+00,  6.8900e+00,
         -1.4918e+00,  5.7511e+00,  7.0410e+00],
        [-2.7080e-01, -3.8306e-01,  3.8145e-01,  4.2453e-01, -1.7319e-01,
         -2.4565e-01,  4.1291e-01,  2.3744e-01,  4.3361e-01,  4.3595e-01,
         -2.5268e-01,  6.7153e-01,  7.5579e-01],
        [-6.5181e+00, -7.7325e+00,  4.2892e+00,  3.8151e+00,  4.9481e-02,
          8.7090e-01,  4.0676e+00,  4.0157e-01, -3.8879e+00, -6.7266e+00,
          4.6984e-01, -6.0587e+00, -7.2564e+00],
        [ 4.1094e+00,  3.9593e+00, -2.6601e+00, -2.3351e+00,  2.7441e-02,
         -1.0458e-01, -2.4297e+00,  1.3929e-01,  1.0831e+00,  2.8707e+00,
         -8.4861e-02,  1.6582e+00,  2.0296e+00],
        [ 9.1538e-02,  1.1433e-02, -4.5969e-01, -5.5030e-01,  7.9296e-02,
          1.2314e-01, -5.2011e-01, -7.7216e-01, -8.9524e-01, -8.0913e-01,
          2.6480e-01, -1.0249e+00, -1.0745e+00],
        [-2.7011e-01, -3.8274e-01,  3.8009e-01,  4.2346e-01, -1.7272e-01,
         -2.4510e-01,  4.1168e-01,  2.3583e-01,  4.3246e-01,  4.3427e-01,
         -2.5197e-01,  6.6945e-01,  7.5356e-01],
        [-5.9444e+00, -8.5876e+00,  4.1212e+00,  3.7874e+00,  1.9993e-01,
          9.4620e-01,  4.0082e+00,  7.1781e-01, -3.9988e+00, -6.4882e+00,
          5.1646e-01, -5.8764e+00, -7.4145e+00]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-3.9881, -1.9749, -1.4296, -2.2109, -1.4277, -1.4277, -1.8161, -1.4351,
         1.6336, -2.2885,  2.4262, -1.4277,  1.8564], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-2.3734, -2.3577, -0.4406, -3.5981, -0.4402, -0.4402, -9.3949, -0.4418,
          7.2232, -2.8069,  1.0345, -0.4402,  7.6960]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 11.9181,   4.0088],
        [ -1.2643,   1.3101],
        [ -3.5113,   2.7685],
        [ -0.4595,  10.8757],
        [ -8.2363,   3.6134],
        [ -1.3933,  -0.8612],
        [-12.7377,   0.7511],
        [  7.2150,   8.7777],
        [ -9.7717,  -3.5038],
        [-10.9268,   0.5853],
        [  9.8567,   7.7442],
        [ -1.8700,  -1.1236],
        [ 10.0067,  -0.0468]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  0.1735,  -3.6938,  -7.6988,   5.9521,   5.2625,  -5.0771,  -0.6058,
          4.7066,  -3.2193,   9.4473,   6.2407,  -5.5687, -10.3012],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.6602e+00,  6.2425e-01,  7.1238e-01,  2.4862e-01,  1.1649e-01,
          1.6227e-02, -8.4174e-02, -9.4818e-01, -1.3434e-01, -1.1821e+00,
         -1.6839e+00, -1.3421e-02,  2.8699e-02],
        [-4.2847e+00, -1.7835e-01, -5.4477e+00,  7.9615e+00,  2.0292e+00,
         -1.6190e+00,  3.1775e+00,  1.3874e+00,  2.5172e-01,  3.2368e+00,
         -3.6006e+00, -9.9907e-01, -1.0370e+01],
        [-1.6602e+00,  6.2424e-01,  7.1238e-01,  2.4862e-01,  1.1649e-01,
          1.6227e-02, -8.4174e-02, -9.4818e-01, -1.3434e-01, -1.1821e+00,
         -1.6839e+00, -1.3421e-02,  2.8703e-02],
        [-4.2248e+00, -1.6574e+00, -2.5166e+00,  7.7978e+00,  1.4286e+00,
          1.8750e+00,  3.5323e+00, -1.3036e+00,  5.3291e+00,  2.5482e-01,
         -6.1487e+00,  1.8649e+00, -3.8582e+00],
        [ 1.2138e+00,  1.2596e+00,  1.9248e+00,  6.3574e-01, -4.3035e-01,
          1.3083e-01,  1.0481e+00, -1.3703e+00,  1.4158e+00, -5.4719e-01,
          9.5239e-01,  7.7539e-02,  2.7585e+00],
        [-1.6602e+00,  6.2424e-01,  7.1238e-01,  2.4862e-01,  1.1649e-01,
          1.6226e-02, -8.4173e-02, -9.4818e-01, -1.3434e-01, -1.1821e+00,
         -1.6839e+00, -1.3421e-02,  2.8699e-02],
        [-1.6602e+00,  6.2424e-01,  7.1238e-01,  2.4862e-01,  1.1649e-01,
          1.6227e-02, -8.4174e-02, -9.4818e-01, -1.3434e-01, -1.1821e+00,
         -1.6839e+00, -1.3421e-02,  2.8702e-02],
        [ 8.7695e-01, -1.2088e-01, -3.4276e-03,  1.0753e+01,  2.7210e+00,
          1.2052e+00, -6.9686e-02, -1.3815e+00,  1.3082e+00,  4.2485e+00,
         -2.2177e+00,  1.4302e+00, -4.5515e-01],
        [-1.6602e+00,  6.2424e-01,  7.1238e-01,  2.4862e-01,  1.1649e-01,
          1.6227e-02, -8.4174e-02, -9.4818e-01, -1.3434e-01, -1.1821e+00,
         -1.6839e+00, -1.3420e-02,  2.8703e-02],
        [ 3.9884e+00,  2.1640e-01,  4.8553e-03,  1.1544e+01, -2.8488e-01,
         -2.9657e+00, -2.2956e+00,  6.9070e+00, -4.4489e+00, -7.0601e+00,
          5.8253e+00, -3.1000e+00,  3.5295e+00],
        [-2.2533e+00,  1.1872e-01, -3.4843e-03, -1.0215e+01,  4.1189e-01,
         -2.3052e+00,  3.2778e+00, -4.7182e+00,  2.9881e+00,  3.9972e+00,
         -3.0825e+00, -3.0815e+00, -3.0683e+00],
        [-8.9891e+00, -1.2579e-01,  1.4932e-03, -4.7517e-01, -2.2527e-01,
         -9.0926e-01, -6.2097e+00, -1.2974e+00,  1.7524e+00,  5.7490e+00,
         -1.8143e+01, -3.4321e-01, -3.8105e+00],
        [-1.6602e+00,  6.2424e-01,  7.1238e-01,  2.4861e-01,  1.1649e-01,
          1.6223e-02, -8.4172e-02, -9.4817e-01, -1.3435e-01, -1.1821e+00,
         -1.6839e+00, -1.3422e-02,  2.8678e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.5250, -4.5330, -2.5250, -2.6387,  3.4759, -2.5250, -2.5250,  3.0636,
        -2.5250,  0.0245, -0.8955, -2.3337, -2.5250], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.0179e-02,  1.7568e-01, -1.0179e-02, -3.8203e-03, -3.5104e+00,
         -1.0179e-02, -1.0179e-02, -3.4491e+00, -1.0179e-02, -3.1488e+00,
         -3.0845e-01, -4.6061e-03, -1.0179e-02],
        [-3.1109e-01,  1.2230e+00, -3.1153e-01,  3.2400e+00, -1.4943e+00,
         -3.1047e-01, -3.1151e-01, -1.9758e-01, -3.1154e-01,  3.6840e+00,
         -4.2899e+00,  1.2964e+01, -3.1092e-01],
        [-7.7125e-03, -1.8817e-01, -7.7125e-03, -1.2127e-02, -3.5807e+00,
         -7.7125e-03, -7.7125e-03, -3.5273e+00, -7.7125e-03, -3.4574e+00,
         -1.4078e-01, -1.8215e-02, -7.7124e-03],
        [-4.7418e-03, -2.0762e-01, -4.7417e-03, -1.7918e-02, -3.3317e+00,
         -4.7418e-03, -4.7417e-03, -3.3015e+00, -4.7417e-03, -3.2399e+00,
         -1.2576e-01, -2.7883e-02, -4.7421e-03],
        [ 3.1653e-01,  2.8862e-02,  3.1610e-01, -2.1932e+00,  2.9705e+00,
          3.1715e-01,  3.1611e-01,  1.6275e+00,  3.1608e-01, -2.0671e+00,
          4.9758e+00, -1.2923e+01,  3.1670e-01]], device='cuda:0'))])
xi:  [485.26944]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1999.6249919986465
W_T_median: 1413.551881514287
W_T_pctile_5: 487.7665047065673
W_T_CVAR_5_pct: 272.7736972670416
Average q (qsum/M+1):  54.635320848034276
Optimal xi:  [485.26944]
Expected(across Rb) median(across samples) p_equity:  0.5660796016454697
obj fun:  tensor(-1829.9933, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 1138.474873568312
W_T_median: 891.4398755591162
W_T_pctile_5: -169.98079142996517
W_T_CVAR_5_pct: -317.5137579687884
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1782.523594649996
Current xi:  [122.42289]
objective value function right now is: -1782.523594649996
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1804.9800960215891
Current xi:  [143.11694]
objective value function right now is: -1804.9800960215891
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1827.725173519524
Current xi:  [164.79825]
objective value function right now is: -1827.725173519524
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1833.7545537048793
Current xi:  [186.83617]
objective value function right now is: -1833.7545537048793
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1849.5963729959499
Current xi:  [208.79883]
objective value function right now is: -1849.5963729959499
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1863.7504600239827
Current xi:  [230.40097]
objective value function right now is: -1863.7504600239827
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1874.3878338201816
Current xi:  [251.75108]
objective value function right now is: -1874.3878338201816
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1880.6093737396275
Current xi:  [272.6989]
objective value function right now is: -1880.6093737396275
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1895.7448282266737
Current xi:  [294.09537]
objective value function right now is: -1895.7448282266737
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1910.5686599151522
Current xi:  [316.69705]
objective value function right now is: -1910.5686599151522
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1919.1466990582949
Current xi:  [339.647]
objective value function right now is: -1919.1466990582949
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [360.58054]
objective value function right now is: -1912.991057269026
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1937.8752523408255
Current xi:  [379.49017]
objective value function right now is: -1937.8752523408255
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1949.470127400774
Current xi:  [400.78778]
objective value function right now is: -1949.470127400774
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1953.9369878388875
Current xi:  [421.33853]
objective value function right now is: -1953.9369878388875
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1964.870068579792
Current xi:  [441.57837]
objective value function right now is: -1964.870068579792
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1973.3789371458097
Current xi:  [461.42575]
objective value function right now is: -1973.3789371458097
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1979.371918740935
Current xi:  [481.04318]
objective value function right now is: -1979.371918740935
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1982.210609008113
Current xi:  [500.13977]
objective value function right now is: -1982.210609008113
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1987.1570270370958
Current xi:  [519.23285]
objective value function right now is: -1987.1570270370958
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1992.249506369622
Current xi:  [537.16736]
objective value function right now is: -1992.249506369622
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1996.5416955783783
Current xi:  [554.98883]
objective value function right now is: -1996.5416955783783
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2000.942398886502
Current xi:  [572.37024]
objective value function right now is: -2000.942398886502
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -2006.4647312655152
Current xi:  [588.087]
objective value function right now is: -2006.4647312655152
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [604.2013]
objective value function right now is: -1995.9076745619468
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -2014.405758612323
Current xi:  [619.7483]
objective value function right now is: -2014.405758612323
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [634.06244]
objective value function right now is: -2011.7872856104332
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [647.1713]
objective value function right now is: -2013.2636759447057
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -2019.6236059136572
Current xi:  [659.4747]
objective value function right now is: -2019.6236059136572
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [670.53906]
objective value function right now is: -2005.8409695371106
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -2024.521576307745
Current xi:  [680.8155]
objective value function right now is: -2024.521576307745
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -2025.8251142211761
Current xi:  [690.8146]
objective value function right now is: -2025.8251142211761
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [699.06165]
objective value function right now is: -2024.7910650858526
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [706.29474]
objective value function right now is: -2023.2603648139864
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [714.2604]
objective value function right now is: -2024.0476793444095
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2028.6380482742181
Current xi:  [716.11597]
objective value function right now is: -2028.6380482742181
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -2028.9054162604227
Current xi:  [717.91864]
objective value function right now is: -2028.9054162604227
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -2029.2231090172497
Current xi:  [719.60046]
objective value function right now is: -2029.2231090172497
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [721.49725]
objective value function right now is: -2024.4976418075519
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -2029.8816904835599
Current xi:  [723.3813]
objective value function right now is: -2029.8816904835599
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [725.01025]
objective value function right now is: -2027.7492611323369
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [726.5836]
objective value function right now is: -2029.3958376143182
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -2030.516979018073
Current xi:  [728.2752]
objective value function right now is: -2030.516979018073
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [729.72986]
objective value function right now is: -2030.465419661928
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [731.306]
objective value function right now is: -2030.328761478638
new min fval from sgd:  -2030.5347071502265
new min fval from sgd:  -2030.547955610635
new min fval from sgd:  -2030.5781676028773
new min fval from sgd:  -2030.651183849242
new min fval from sgd:  -2030.6748445447101
new min fval from sgd:  -2030.691370014422
new min fval from sgd:  -2030.6935507935016
new min fval from sgd:  -2030.7289669133202
new min fval from sgd:  -2030.7318198785122
new min fval from sgd:  -2030.7693216977725
new min fval from sgd:  -2030.8010358645924
new min fval from sgd:  -2030.806675229256
new min fval from sgd:  -2030.8173500932824
new min fval from sgd:  -2030.8351010201682
new min fval from sgd:  -2030.9234041561513
new min fval from sgd:  -2030.9880002710368
new min fval from sgd:  -2031.0287210001238
new min fval from sgd:  -2031.0364691781244
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [732.7406]
objective value function right now is: -2030.1330446637412
new min fval from sgd:  -2031.057586429439
new min fval from sgd:  -2031.0632330217968
new min fval from sgd:  -2031.1054975205798
new min fval from sgd:  -2031.1219283894693
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [734.4138]
objective value function right now is: -2029.4672178906271
new min fval from sgd:  -2031.129688406869
new min fval from sgd:  -2031.2191809518672
new min fval from sgd:  -2031.2229907379035
new min fval from sgd:  -2031.2286121971915
new min fval from sgd:  -2031.2443740964445
new min fval from sgd:  -2031.2827235146333
new min fval from sgd:  -2031.3143609488307
new min fval from sgd:  -2031.3435143296201
new min fval from sgd:  -2031.348801655856
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [735.9531]
objective value function right now is: -2030.2674660577195
new min fval from sgd:  -2031.3530937222286
new min fval from sgd:  -2031.3711120400005
new min fval from sgd:  -2031.3771544050471
new min fval from sgd:  -2031.3790987983741
new min fval from sgd:  -2031.380087343408
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [736.9812]
objective value function right now is: -2031.1310053349932
new min fval from sgd:  -2031.3802462114515
new min fval from sgd:  -2031.3976247896974
new min fval from sgd:  -2031.408591990556
new min fval from sgd:  -2031.415253364782
new min fval from sgd:  -2031.4185976033523
new min fval from sgd:  -2031.4192319471326
new min fval from sgd:  -2031.4209940882733
new min fval from sgd:  -2031.4235256518382
new min fval from sgd:  -2031.4332181597217
new min fval from sgd:  -2031.4447756867112
new min fval from sgd:  -2031.4512928477936
new min fval from sgd:  -2031.4544715120805
new min fval from sgd:  -2031.4558461533475
new min fval from sgd:  -2031.4561663743816
new min fval from sgd:  -2031.461644238973
new min fval from sgd:  -2031.4703878400476
new min fval from sgd:  -2031.4798635876132
new min fval from sgd:  -2031.4885793598526
new min fval from sgd:  -2031.4947945852527
new min fval from sgd:  -2031.4990848385512
new min fval from sgd:  -2031.5008051508448
new min fval from sgd:  -2031.503064776891
new min fval from sgd:  -2031.5063461217128
new min fval from sgd:  -2031.5087553236503
new min fval from sgd:  -2031.5108849489452
new min fval from sgd:  -2031.5124089423275
new min fval from sgd:  -2031.512943001231
new min fval from sgd:  -2031.5166840920099
new min fval from sgd:  -2031.5238721117396
new min fval from sgd:  -2031.527527983423
new min fval from sgd:  -2031.5276595586483
new min fval from sgd:  -2031.5303769869627
new min fval from sgd:  -2031.5327793064914
new min fval from sgd:  -2031.5376296868496
new min fval from sgd:  -2031.5417740776152
new min fval from sgd:  -2031.5450031116245
new min fval from sgd:  -2031.5500597790578
new min fval from sgd:  -2031.5545114962235
new min fval from sgd:  -2031.5546561533797
new min fval from sgd:  -2031.5574339338286
new min fval from sgd:  -2031.557732033747
new min fval from sgd:  -2031.5625988353165
new min fval from sgd:  -2031.5652570288685
new min fval from sgd:  -2031.5685743564338
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [737.2034]
objective value function right now is: -2031.341392280405
min fval:  -2031.5685743564338
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-14.4542,   6.3244],
        [-21.6430,   5.7563],
        [ -4.3192,  11.5413],
        [ 10.5369,  -0.7778],
        [ -1.2376,   0.2472],
        [ 10.1247,  -1.3231],
        [ -3.9736, -11.3762],
        [ -1.2364,   0.2468],
        [ -1.2372,   0.2485],
        [-20.3076,   1.1323],
        [ -1.7628,   0.4485],
        [ 10.9581,  -1.1977],
        [ -1.2380,   0.2488]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 1.6353,  2.3077,  2.7537, -9.5389, -3.3575, -9.5667, -3.0303, -3.3574,
        -3.3527,  0.5916, -3.5792, -9.5250, -3.3529], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 4.8416e+00,  8.3440e+00,  3.1889e+00, -6.5168e+00, -1.0448e-02,
         -5.2987e+00, -1.3131e+01, -1.0101e-02, -1.5220e-03,  7.0981e+00,
         -4.9385e-02, -6.1850e+00, -8.3122e-04],
        [-1.7364e-02, -2.1530e-02, -5.3872e-01, -1.7144e-01, -4.5278e-03,
         -9.4956e-02, -1.3985e-01, -4.5235e-03, -4.5531e-03, -9.8032e-03,
         -2.5353e-03, -2.0813e-01, -4.5509e-03],
        [-4.1474e+00, -2.8300e+00, -4.8798e+00,  6.5471e+00, -2.1112e-03,
          3.7359e+00,  1.2993e+01, -4.7161e-03,  1.3555e-02, -1.3893e+00,
         -3.5713e-01,  7.6811e+00,  1.3742e-02],
        [-3.9186e+00, -6.5791e+00,  1.8838e+00, -4.6285e+00, -2.9657e-02,
         -3.9012e+00,  7.3106e+00, -2.6545e-02, -2.6923e-02, -5.1802e+00,
          3.9985e-01, -6.0662e+00, -2.5000e-02],
        [-1.4405e-01,  4.3095e-02, -5.6602e+00,  1.0200e+01,  7.5546e-02,
          7.8346e+00,  5.3538e-01,  7.5140e-02,  8.0183e-02,  6.0962e-02,
          2.2457e-01,  1.0239e+01,  8.2183e-02],
        [-1.8629e+00, -1.8299e-01, -4.1163e+00,  3.7973e+00, -2.6630e-03,
          1.9743e+00,  7.1164e+00, -1.8633e-03, -3.8655e-03,  7.1234e-01,
         -3.4499e-02,  5.0399e+00, -4.8408e-03],
        [-1.7363e-02, -2.1531e-02, -5.3868e-01, -1.7143e-01, -4.5271e-03,
         -9.4948e-02, -1.3983e-01, -4.5227e-03, -4.5524e-03, -9.8038e-03,
         -2.5349e-03, -2.0811e-01, -4.5502e-03],
        [ 5.2233e+00,  9.4775e+00, -6.0526e+00, -1.5990e-02, -3.1497e-01,
         -1.6015e-02,  2.5455e+00, -3.1228e-01, -3.4322e-01,  8.1954e+00,
          1.2771e+00, -1.5643e-02, -3.4343e-01],
        [-1.7363e-02, -2.1530e-02, -5.3872e-01, -1.7144e-01, -4.5278e-03,
         -9.4956e-02, -1.3985e-01, -4.5235e-03, -4.5531e-03, -9.8032e-03,
         -2.5353e-03, -2.0813e-01, -4.5509e-03],
        [-1.7477e-02, -2.1599e-02, -5.4394e-01, -1.7278e-01, -4.6139e-03,
         -9.5884e-02, -1.4204e-01, -4.6095e-03, -4.6396e-03, -9.7664e-03,
         -2.5777e-03, -2.0974e-01, -4.6374e-03],
        [-4.3845e+00, -3.3116e+00, -3.6642e+00, -1.8621e+00,  2.4023e-01,
         -2.0626e+00,  1.4154e+01,  2.3393e-01,  2.4558e-01, -2.3118e+00,
         -5.2930e-01, -2.6583e+00,  2.4745e-01],
        [-1.8535e+00, -1.3782e-01, -3.8451e+00,  9.1882e-01, -2.2488e-02,
          5.1900e-01,  6.6365e+00, -2.2278e-02, -2.3446e-02,  6.5286e-01,
         -3.3087e-02,  1.4662e+00, -2.4032e-02],
        [-3.4185e+00, -2.2988e+00, -4.3975e+00,  5.4071e+00,  1.3531e-02,
          2.6422e+00,  1.1565e+01,  2.1897e-02,  1.7482e-02, -1.4883e+00,
         -3.0650e-01,  6.4334e+00,  1.5727e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([  2.1748,  -1.7139,  -2.7922,  -1.2006,  -5.1024,  -3.9849,  -1.7140,
        -11.0701,  -1.7139,  -1.6977,  -1.6448,  -4.0085,  -2.6904],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[18.7463, -0.0223, -7.4147, -6.8402, -7.3919, -3.7230, -0.0223, -9.4159,
         -0.0223, -0.0223, -7.2307, -3.0364, -5.2623]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.8584,   1.5354],
        [-12.1717,   1.3800],
        [ -1.9192,   1.5423],
        [-10.5652,   0.1222],
        [ 11.1492,  -0.6810],
        [  8.7778,   8.4348],
        [ -5.2611,   2.4817],
        [ -9.0888,   8.2833],
        [ -1.7881,   0.3896],
        [ 12.2036,   7.3683],
        [ -1.6626,   0.3282],
        [  3.4836,   4.1266],
        [-14.9405,  -5.0585]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -5.0680,  -1.1817,  -5.1620,   8.8596, -10.8796,   1.3780,  -8.1331,
          4.1703,  -4.4849,   3.3561,  -4.4471,  -3.4030,  -2.0440],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 6.9390e-01, -3.0384e+00,  6.9157e-01, -3.6820e+00,  1.9689e+01,
         -4.4868e-01,  8.7773e-01, -5.7250e+00,  6.2621e-03, -6.4982e-01,
          2.3670e-03,  1.9306e+00,  8.7319e+00],
        [-8.3391e-01,  7.4143e+00, -9.1391e-01,  2.5283e+00, -1.3061e+01,
         -2.5758e+00, -1.6309e+00,  5.3868e+00,  4.5328e-01, -5.9747e-01,
          2.1378e-01, -8.2045e-01, -1.3988e+00],
        [-1.1901e+00, -1.9165e-01, -1.2270e+00, -8.4861e-01, -1.8080e-01,
         -1.6561e+00, -5.7497e-01,  1.8336e+00, -6.3104e-02, -1.5528e+00,
         -4.3668e-02,  1.1323e+00, -1.6608e+00],
        [-1.2081e+00, -2.4028e-01, -1.2439e+00, -7.6965e-01, -1.1340e-01,
         -1.7027e+00, -6.0399e-01,  1.8839e+00, -7.7582e-02, -1.4686e+00,
         -4.8199e-02,  1.1807e+00, -1.7475e+00],
        [-1.2143e+00, -2.6035e-01, -1.2496e+00, -7.4017e-01, -8.8292e-02,
         -1.7202e+00, -6.1481e-01,  1.9021e+00, -8.3457e-02, -1.4368e+00,
         -4.9839e-02,  1.1984e+00, -1.7790e+00],
        [-1.1902e+00, -1.9198e-01, -1.2271e+00, -8.4795e-01, -1.8035e-01,
         -1.6567e+00, -5.7514e-01,  1.8339e+00, -6.3188e-02, -1.5521e+00,
         -4.3694e-02,  1.1326e+00, -1.6615e+00],
        [-1.1861e+00, -1.8152e-01, -1.2232e+00, -8.6774e-01, -1.9590e-01,
         -1.6430e+00, -5.6864e-01,  1.8224e+00, -6.0171e-02, -1.5736e+00,
         -4.2682e-02,  1.1219e+00, -1.6403e+00],
        [-1.2123e+00, -2.5317e-01, -1.2477e+00, -7.5078e-01, -9.6970e-02,
         -1.7133e+00, -6.1113e-01,  1.8959e+00, -8.1423e-02, -1.4483e+00,
         -4.9296e-02,  1.1925e+00, -1.7679e+00],
        [-5.3891e-02,  5.6156e-01,  1.3262e-02, -3.0827e+00, -2.9470e+00,
         -1.6255e+00, -1.1565e+00,  1.6963e+01, -2.5347e-02, -5.1542e+00,
         -1.0665e-01, -8.6043e+00,  3.4776e+00],
        [-2.9821e-01,  2.3003e+00, -2.9367e-01,  2.2694e+00, -5.7721e+00,
         -3.6640e+00,  3.1122e-02, -6.7126e+00, -5.2613e-02, -4.5773e+00,
         -4.1336e-02, -7.9816e+00,  5.7028e+00],
        [-1.1958e+00, -2.0482e-01, -1.2323e+00, -8.2639e-01, -1.6107e-01,
         -1.6682e+00, -5.8353e-01,  1.8487e+00, -6.7162e-02, -1.5294e+00,
         -4.5029e-02,  1.1469e+00, -1.6860e+00],
        [ 2.0976e-01,  1.1155e-01,  2.6676e-01, -3.9453e+00,  1.4654e+00,
          1.9091e+00,  9.4995e-02, -5.3937e-01, -5.7011e-02,  6.3653e-01,
         -7.3564e-02, -3.4884e+00, -1.1061e+00],
        [-1.2040e+00, -2.2745e-01, -1.2400e+00, -7.8973e-01, -1.3009e-01,
         -1.6902e+00, -5.9691e-01,  1.8718e+00, -7.3869e-02, -1.4902e+00,
         -4.7122e-02,  1.1691e+00, -1.7262e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 3.8303, -2.9933, -4.4105, -4.3602, -4.3411, -4.4102, -4.4222, -4.3479,
        -5.2801, -0.3422, -4.3964, -4.6635, -4.3729], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.4597e+01,  2.9580e+00,  6.1799e-01,  6.8095e-01,  7.0424e-01,
          6.1835e-01,  6.0439e-01,  6.9639e-01,  3.8385e+00, -2.1068e+01,
          6.3675e-01, -8.7176e-01,  6.6572e-01],
        [ 8.8606e-01,  9.5761e-01, -1.2842e+00, -1.3684e+00, -1.3999e+00,
         -1.2852e+00, -1.2602e+00, -1.3874e+00,  7.9450e-01, -3.9719e+00,
         -1.3060e+00,  2.8190e+00, -1.3454e+00],
        [-2.7139e+01, -2.3062e+01, -8.8052e-02, -1.0815e-01, -1.1696e-01,
         -8.8179e-02, -8.4074e-02, -1.1379e-01, -8.5839e-03, -8.2218e-02,
         -9.3360e-02, -3.1511e-01, -1.0269e-01],
        [-2.5198e+01, -2.0092e+01, -9.1118e-02, -1.1418e-01, -1.2414e-01,
         -9.1264e-02, -8.6509e-02, -1.2057e-01, -3.4988e-03, -8.0575e-02,
         -9.7268e-02, -2.8181e-01, -1.0797e-01],
        [ 1.1755e+00,  1.1353e+00,  1.2961e+00,  1.3741e+00,  1.4035e+00,
          1.2963e+00,  1.2821e+00,  1.3944e+00,  2.4930e-01,  9.0823e+00,
          1.3206e+00, -1.3931e+00,  1.3566e+00]], device='cuda:0'))])
xi:  [737.17993]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1051.5819307909958
W_T_median: 903.9570421087788
W_T_pctile_5: 737.2035073025071
W_T_CVAR_5_pct: 430.1741131323368
Average q (qsum/M+1):  51.657860540574596
Optimal xi:  [737.17993]
Expected(across Rb) median(across samples) p_equity:  0.25812382583195964
obj fun:  tensor(-2031.5686, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 1138.474873568312
W_T_median: 891.4398755591162
W_T_pctile_5: -169.98079142996517
W_T_CVAR_5_pct: -317.5137579687884
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1820.1600935609001
Current xi:  [121.54975]
objective value function right now is: -1820.1600935609001
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1835.3511283901187
Current xi:  [144.17294]
objective value function right now is: -1835.3511283901187
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1876.7780125174027
Current xi:  [167.53204]
objective value function right now is: -1876.7780125174027
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1904.3660875566534
Current xi:  [190.70132]
objective value function right now is: -1904.3660875566534
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1920.8628343625733
Current xi:  [213.64668]
objective value function right now is: -1920.8628343625733
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1948.4163394983425
Current xi:  [236.56813]
objective value function right now is: -1948.4163394983425
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1967.366263659653
Current xi:  [260.5131]
objective value function right now is: -1967.366263659653
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1995.5253774336973
Current xi:  [283.91257]
objective value function right now is: -1995.5253774336973
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -2018.3085291612967
Current xi:  [306.66956]
objective value function right now is: -2018.3085291612967
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -2038.4650841666441
Current xi:  [329.15347]
objective value function right now is: -2038.4650841666441
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -2051.3473127636253
Current xi:  [351.418]
objective value function right now is: -2051.3473127636253
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -2071.8728327713757
Current xi:  [373.30093]
objective value function right now is: -2071.8728327713757
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -2080.1326886376596
Current xi:  [395.088]
objective value function right now is: -2080.1326886376596
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -2096.867713288365
Current xi:  [416.69]
objective value function right now is: -2096.867713288365
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -2116.543396178513
Current xi:  [438.6251]
objective value function right now is: -2116.543396178513
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -2126.9845764307865
Current xi:  [460.21454]
objective value function right now is: -2126.9845764307865
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -2149.0473875382095
Current xi:  [480.83658]
objective value function right now is: -2149.0473875382095
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -2165.0124951971325
Current xi:  [501.64685]
objective value function right now is: -2165.0124951971325
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -2175.5938276027505
Current xi:  [522.1036]
objective value function right now is: -2175.5938276027505
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [541.99866]
objective value function right now is: -2166.56530922162
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -2189.164494783292
Current xi:  [561.7139]
objective value function right now is: -2189.164494783292
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -2204.5457302888844
Current xi:  [580.7974]
objective value function right now is: -2204.5457302888844
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2218.146041731612
Current xi:  [599.4518]
objective value function right now is: -2218.146041731612
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -2222.6300480291593
Current xi:  [616.366]
objective value function right now is: -2222.6300480291593
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -2230.9461192261915
Current xi:  [633.0353]
objective value function right now is: -2230.9461192261915
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -2237.998021961384
Current xi:  [649.0828]
objective value function right now is: -2237.998021961384
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -2242.0769581801005
Current xi:  [665.5549]
objective value function right now is: -2242.0769581801005
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -2244.7603290374636
Current xi:  [680.6366]
objective value function right now is: -2244.7603290374636
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [693.2262]
objective value function right now is: -1974.1550938641396
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [704.538]
objective value function right now is: -2231.418416211259
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [717.22064]
objective value function right now is: -2241.3525900644586
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -2255.676669397184
Current xi:  [729.80597]
objective value function right now is: -2255.676669397184
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [740.77783]
objective value function right now is: -2252.400260868231
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -2265.4459407684126
Current xi:  [751.186]
objective value function right now is: -2265.4459407684126
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [761.86707]
objective value function right now is: -2263.82627514376
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [764.1177]
objective value function right now is: -2259.9698144659556
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -2269.690570513567
Current xi:  [766.4269]
objective value function right now is: -2269.690570513567
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [768.5502]
objective value function right now is: -2269.3200154944316
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [771.18274]
objective value function right now is: -2267.864809391214
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -2270.8935806871186
Current xi:  [773.333]
objective value function right now is: -2270.8935806871186
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [775.3778]
objective value function right now is: -2269.9684905398285
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [777.65234]
objective value function right now is: -2269.2033103564977
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [779.8503]
objective value function right now is: -2264.0518313196703
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -2273.4027168346233
Current xi:  [781.86426]
objective value function right now is: -2273.4027168346233
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [783.6844]
objective value function right now is: -2271.2194381433787
new min fval from sgd:  -2273.59112148905
new min fval from sgd:  -2273.654187866797
new min fval from sgd:  -2274.2045935845645
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [785.59503]
objective value function right now is: -2273.053571373793
new min fval from sgd:  -2274.2370755779943
new min fval from sgd:  -2274.28053509652
new min fval from sgd:  -2274.3438748480166
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [787.8255]
objective value function right now is: -2271.9995387537933
new min fval from sgd:  -2274.36097941156
new min fval from sgd:  -2274.364298851086
new min fval from sgd:  -2274.396179736645
new min fval from sgd:  -2274.839745356385
new min fval from sgd:  -2274.915353008191
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [789.45087]
objective value function right now is: -2274.132113201276
new min fval from sgd:  -2274.923942840178
new min fval from sgd:  -2274.9793677974176
new min fval from sgd:  -2275.039386067119
new min fval from sgd:  -2275.07642625756
new min fval from sgd:  -2275.115594403178
new min fval from sgd:  -2275.128423541774
new min fval from sgd:  -2275.136190332395
new min fval from sgd:  -2275.1532119913
new min fval from sgd:  -2275.1554089934434
new min fval from sgd:  -2275.2192839292215
new min fval from sgd:  -2275.2307162120537
new min fval from sgd:  -2275.2537563351952
new min fval from sgd:  -2275.275942654834
new min fval from sgd:  -2275.2818561366694
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [790.4534]
objective value function right now is: -2274.514735084568
new min fval from sgd:  -2275.284301528032
new min fval from sgd:  -2275.2886377941313
new min fval from sgd:  -2275.2958042637274
new min fval from sgd:  -2275.305914635313
new min fval from sgd:  -2275.3097121199353
new min fval from sgd:  -2275.319226010729
new min fval from sgd:  -2275.3273670662984
new min fval from sgd:  -2275.330592525418
new min fval from sgd:  -2275.331120495791
new min fval from sgd:  -2275.3465247253066
new min fval from sgd:  -2275.360189409283
new min fval from sgd:  -2275.364943456482
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [790.85754]
objective value function right now is: -2274.59079037057
min fval:  -2275.364943456482
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -1.3593,  -0.0983],
        [ -1.3592,  -0.0981],
        [ -1.3592,  -0.0982],
        [ -7.8370,   9.4666],
        [ -1.3648,  -0.1025],
        [-25.8194,   5.7097],
        [ -1.3593,  -0.0983],
        [ -5.1386,  14.5854],
        [ -1.6162,  -0.1445],
        [ -1.3591,  -0.0981],
        [ 11.8271,  -0.6396],
        [ -1.3592,  -0.0982],
        [ -3.0647, -11.8478]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.5698,  -3.5700,  -3.5699,  -0.0363,  -3.5664,   4.3364,  -3.5698,
          1.2994,  -3.4794,  -3.5700, -10.4544,  -3.5699,  -2.3601],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-8.6133e-03, -8.6132e-03, -8.6134e-03, -2.4693e-01, -8.5864e-03,
         -3.1614e-01, -8.6134e-03, -5.5529e-01, -6.6706e-03, -8.6134e-03,
         -2.1155e-01, -8.6134e-03, -3.0015e-01],
        [-9.5185e-02, -9.3790e-02, -9.4951e-02,  1.1445e+00, -9.0904e-02,
          5.4003e+00, -9.5156e-02,  2.6075e+00, -1.6895e-01, -9.4088e-02,
         -1.2461e+01, -9.4808e-02, -1.1721e+01],
        [-8.6133e-03, -8.6132e-03, -8.6134e-03, -2.4693e-01, -8.5864e-03,
         -3.1614e-01, -8.6134e-03, -5.5529e-01, -6.6706e-03, -8.6134e-03,
         -2.1155e-01, -8.6134e-03, -3.0015e-01],
        [-1.2864e-02, -1.6180e-02, -1.5039e-02,  1.2379e+00,  1.1042e-02,
          5.6305e+00, -1.4003e-02,  2.8683e+00,  1.1627e-02, -1.6681e-02,
         -1.3147e+01, -1.5424e-02, -1.1924e+01],
        [-8.6133e-03, -8.6132e-03, -8.6134e-03, -2.4693e-01, -8.5864e-03,
         -3.1614e-01, -8.6134e-03, -5.5529e-01, -6.6706e-03, -8.6134e-03,
         -2.1155e-01, -8.6134e-03, -3.0015e-01],
        [-8.6133e-03, -8.6132e-03, -8.6134e-03, -2.4693e-01, -8.5864e-03,
         -3.1614e-01, -8.6133e-03, -5.5529e-01, -6.6706e-03, -8.6133e-03,
         -2.1155e-01, -8.6134e-03, -3.0015e-01],
        [ 2.9468e-01,  2.9418e-01,  2.9441e-01, -3.0739e+00,  3.1408e-01,
         -8.3359e+00,  2.9454e-01, -1.9151e+00,  4.7476e-01,  2.9411e-01,
          1.2268e+01,  2.9432e-01,  1.1707e+01],
        [-8.6133e-03, -8.6132e-03, -8.6134e-03, -2.4693e-01, -8.5864e-03,
         -3.1614e-01, -8.6134e-03, -5.5529e-01, -6.6706e-03, -8.6134e-03,
         -2.1155e-01, -8.6134e-03, -3.0015e-01],
        [-7.3727e-03, -7.3720e-03, -7.3729e-03, -2.9458e-01, -7.3615e-03,
         -1.3543e+00, -7.3728e-03, -8.4815e-01, -6.1090e-03, -7.3723e-03,
         -1.7226e-01, -7.3727e-03,  9.2280e-01],
        [-8.6133e-03, -8.6132e-03, -8.6134e-03, -2.4693e-01, -8.5864e-03,
         -3.1614e-01, -8.6134e-03, -5.5529e-01, -6.6706e-03, -8.6134e-03,
         -2.1155e-01, -8.6134e-03, -3.0015e-01],
        [-6.0157e-02, -6.2065e-02, -6.1990e-02, -8.8182e-01, -4.1217e-02,
         -7.3537e+00, -6.1182e-02, -3.8081e+00,  9.6333e-02, -6.2741e-02,
          1.5537e+01, -6.2164e-02,  1.2288e+01],
        [-1.2828e-01, -1.2821e-01, -1.2826e-01,  1.1715e+00, -1.3296e-01,
          5.2859e+00, -1.2826e-01,  2.0798e+00, -2.3220e-01, -1.2817e-01,
         -1.1812e+01, -1.2818e-01, -1.1713e+01],
        [ 2.3864e-01,  2.3857e-01,  2.3861e-01, -1.2448e+00,  2.3967e-01,
         -4.6038e+00,  2.3863e-01, -1.7652e+00,  2.8182e-01,  2.3856e-01,
          7.7805e+00,  2.3860e-01,  8.4581e+00]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.0942,  1.0770, -2.0942,  1.3002, -2.0942, -2.0942, -1.1844, -2.0942,
        -3.0033, -2.0942, -2.0898,  0.9831, -2.8700], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0327,   8.7809,  -0.0327,  10.1471,  -0.0327,  -0.0327,  -7.5316,
          -0.0327,  -0.7048,  -0.0327, -11.4681,   8.0000,  -3.7338]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-15.8160,   7.4294],
        [  5.1853,   2.0445],
        [ -1.4664,   1.7364],
        [  7.7101,   3.5681],
        [-11.4078,  -3.7920],
        [  9.4160,   0.2820],
        [  9.6473,   3.9284],
        [-12.0819,  -4.4252],
        [-11.3059,  -3.4820],
        [ -9.1236,  -3.3043],
        [  0.7655,   6.0340],
        [ -6.1574,   8.3664],
        [  3.3508,  -3.3904]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  2.2137,   8.0224,  -4.9246,  -4.6288,  -2.9717,  -9.1993,  -2.8545,
         -1.8847,  -3.5332,   0.3909,   4.6418,   2.8359, -11.7022],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.1751e-01, -3.3548e+00, -1.3344e+00,  2.2531e-01,  9.3155e-01,
         -4.7388e-01, -1.9075e+00,  1.1062e+00,  5.8061e-01,  8.7216e-01,
         -6.1858e-01,  1.7826e+00,  1.8769e-01],
        [-3.7971e+00, -5.9695e+00, -8.4684e-02, -4.6941e+00,  1.0309e+01,
         -1.3560e+01, -4.9923e+00,  1.2216e+01,  6.6656e+00,  7.5997e+00,
         -6.7574e+00, -1.1152e+01, -5.2933e+00],
        [ 2.0477e-01,  3.5177e+00, -3.4829e-01, -5.7903e-01,  1.6010e-01,
         -2.4163e+00, -7.8539e-01,  6.0641e-01,  6.0148e-02,  1.6433e+00,
          9.4767e-02,  3.2278e+00,  6.7565e-01],
        [-7.8741e-01, -4.8205e+00, -1.5540e+00, -1.4230e+00, -2.3778e+00,
         -1.4583e+00,  2.3276e-01, -2.4544e+00, -2.4016e+00,  9.5402e+00,
         -1.9173e+00,  9.5774e+00, -3.0032e+00],
        [ 5.6780e+00, -1.4793e-01,  5.8470e-01, -2.0398e+00, -6.2756e+00,
         -1.7600e+01, -2.4224e+00, -6.7997e+00, -6.2643e+00, -2.4783e+00,
          6.2399e+00,  5.5670e+00, -2.9082e-01],
        [ 1.6664e-01,  3.5120e+00, -3.6527e-01, -5.3591e-01,  1.4432e-01,
         -2.3507e+00, -7.3965e-01,  5.6586e-01,  5.3418e-02,  1.5992e+00,
          1.1143e-01,  3.1263e+00,  6.6974e-01],
        [ 4.8090e+00, -1.4834e+01,  1.6642e+00, -1.7067e+00,  5.7514e+00,
         -7.7476e-01, -2.8591e+00,  7.2391e+00,  6.0955e+00,  3.0134e+00,
          4.2458e+00,  4.4390e+00, -1.1275e+00],
        [ 1.4091e+00,  7.0922e+00,  3.5499e-01,  2.0138e+00, -7.5539e+00,
         -7.6680e+00,  2.8162e+00, -7.7642e+00, -5.0256e+00, -8.3087e-03,
          1.3982e+00,  1.1990e+00,  8.2806e-01],
        [-1.7315e-01, -3.3018e+00, -1.3644e+00,  1.6217e-01,  1.0558e+00,
         -4.9142e-01, -1.9674e+00,  1.2106e+00,  6.8842e-01,  9.1898e-01,
         -5.7046e-01,  1.8081e+00,  1.8309e-01],
        [-2.2934e-01, -3.3789e+00, -1.3263e+00,  2.4392e-01,  8.9613e-01,
         -4.6921e-01, -1.8889e+00,  1.0762e+00,  5.5097e-01,  8.5944e-01,
         -6.3145e-01,  1.7761e+00,  1.8902e-01],
        [-1.3856e-01, -3.2721e+00, -1.3713e+00,  9.0169e-02,  1.1222e+00,
         -5.0944e-01, -2.0136e+00,  1.2676e+00,  7.4894e-01,  9.3243e-01,
         -5.4132e-01,  1.8120e+00,  1.8098e-01],
        [ 1.2833e-01, -2.0689e+00, -2.7686e-01,  2.2210e-01,  4.9322e+00,
         -7.7126e+00, -1.8531e+00,  8.0097e+00,  2.6780e+00,  2.5487e-01,
         -3.9186e+01,  5.2130e-02, -4.2598e+00],
        [ 6.6891e+00,  2.8773e+00,  1.2282e-01,  1.8907e+00, -3.9835e+00,
          2.8652e+00,  1.5625e+00, -4.9834e+00, -2.8658e+00, -1.9728e+00,
          1.4160e+00,  2.0542e+01,  1.8681e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.9376,  2.3320,  3.7783, -1.6537, -3.8984,  3.7914, -1.1016,  1.6152,
        -2.9198, -2.9341, -2.9023, -4.3321, -1.8808], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.3054e+00, -2.2759e+01, -4.4752e+00,  3.1575e+00,  7.6587e+00,
         -4.5020e+00,  5.3098e+00,  5.7352e+00,  1.4151e+00,  1.2726e+00,
          1.4905e+00, -8.7881e-03, -1.1382e+00],
        [-1.0973e+00, -1.9400e+00, -1.1748e+00,  1.3728e-01, -7.0269e-01,
         -6.6027e-01,  4.1210e+00,  3.7117e+00, -1.1690e+00, -1.0783e+00,
         -1.2063e+00,  1.4045e+01,  2.0295e+00],
        [-2.4929e-03, -3.8890e-02, -2.6585e+00, -7.0508e-02, -3.9453e-01,
         -2.6587e+00, -2.4114e-03, -2.6479e+00, -3.3608e-03, -2.2473e-03,
         -3.8197e-03, -7.4191e-03, -2.4107e+00],
        [-1.9910e-03, -3.7934e-02, -2.5390e+00, -7.5193e-02, -3.8216e-01,
         -2.5392e+00, -2.9724e-03, -2.5192e+00, -2.0831e-03, -2.0030e-03,
         -2.1951e-03, -1.1459e-02, -2.3070e+00],
        [ 6.9448e-01,  7.3720e+00,  2.6860e+00,  7.8404e-01, -9.3887e-01,
          2.8681e+00, -6.8992e+00, -5.2394e-02,  6.9569e-01,  6.9377e-01,
          7.0305e-01, -1.3447e+01, -1.4653e+00]], device='cuda:0'))])
xi:  [790.78876]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1160.7482429418967
W_T_median: 1016.30439547044
W_T_pctile_5: 791.0484286988617
W_T_CVAR_5_pct: 470.0146489946138
Average q (qsum/M+1):  50.656958795362904
Optimal xi:  [790.78876]
Expected(across Rb) median(across samples) p_equity:  0.2807714051877459
obj fun:  tensor(-2275.3649, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 1138.474873568312
W_T_median: 891.4398755591162
W_T_pctile_5: -169.98079142996517
W_T_CVAR_5_pct: -317.5137579687884
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1912.0283887083
Current xi:  [121.45034]
objective value function right now is: -1912.0283887083
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1984.4368572780236
Current xi:  [144.53001]
objective value function right now is: -1984.4368572780236
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -2011.9570435536223
Current xi:  [167.79944]
objective value function right now is: -2011.9570435536223
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -2081.573187819963
Current xi:  [191.19086]
objective value function right now is: -2081.573187819963
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -2174.8806554793
Current xi:  [215.06435]
objective value function right now is: -2174.8806554793
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -2217.426994572919
Current xi:  [239.18405]
objective value function right now is: -2217.426994572919
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -2278.8864918388826
Current xi:  [262.6363]
objective value function right now is: -2278.8864918388826
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -2328.1893162961733
Current xi:  [285.88455]
objective value function right now is: -2328.1893162961733
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -2350.7594171067835
Current xi:  [308.91165]
objective value function right now is: -2350.7594171067835
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -2411.4471494590234
Current xi:  [331.67596]
objective value function right now is: -2411.4471494590234
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -2457.2026671794547
Current xi:  [354.32748]
objective value function right now is: -2457.2026671794547
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -2501.881048147084
Current xi:  [376.63995]
objective value function right now is: -2501.881048147084
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -2542.604332298036
Current xi:  [399.0752]
objective value function right now is: -2542.604332298036
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -2581.44832107633
Current xi:  [421.19226]
objective value function right now is: -2581.44832107633
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -2600.6283885667854
Current xi:  [443.12982]
objective value function right now is: -2600.6283885667854
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -2623.0809082326127
Current xi:  [464.53983]
objective value function right now is: -2623.0809082326127
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -2683.1160356712244
Current xi:  [486.2224]
objective value function right now is: -2683.1160356712244
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -2713.9998116721895
Current xi:  [507.70425]
objective value function right now is: -2713.9998116721895
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -2751.2236430265143
Current xi:  [529.68207]
objective value function right now is: -2751.2236430265143
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2773.5124328564284
Current xi:  [550.46594]
objective value function right now is: -2773.5124328564284
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -2796.6519123547387
Current xi:  [571.4329]
objective value function right now is: -2796.6519123547387
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -2813.8103406679907
Current xi:  [591.6048]
objective value function right now is: -2813.8103406679907
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2845.2423209877297
Current xi:  [611.5535]
objective value function right now is: -2845.2423209877297
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -2863.5953001756016
Current xi:  [631.3432]
objective value function right now is: -2863.5953001756016
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -2884.659585925815
Current xi:  [650.81964]
objective value function right now is: -2884.659585925815
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -2897.482890580402
Current xi:  [670.3867]
objective value function right now is: -2897.482890580402
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -2915.1727571425095
Current xi:  [688.82385]
objective value function right now is: -2915.1727571425095
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -2920.6853883161575
Current xi:  [706.8129]
objective value function right now is: -2920.6853883161575
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -2936.8373990834484
Current xi:  [723.3656]
objective value function right now is: -2936.8373990834484
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -2939.3748983396404
Current xi:  [740.07623]
objective value function right now is: -2939.3748983396404
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -2952.019814324311
Current xi:  [756.91504]
objective value function right now is: -2952.019814324311
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -2966.588789315562
Current xi:  [772.3394]
objective value function right now is: -2966.588789315562
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -2978.4785301168645
Current xi:  [786.6673]
objective value function right now is: -2978.4785301168645
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -2978.9054796511064
Current xi:  [799.8606]
objective value function right now is: -2978.9054796511064
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [813.3307]
objective value function right now is: -2977.8032196250656
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2991.73203454753
Current xi:  [815.6364]
objective value function right now is: -2991.73203454753
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -2992.4864413254986
Current xi:  [818.3622]
objective value function right now is: -2992.4864413254986
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -2994.913023860448
Current xi:  [820.58594]
objective value function right now is: -2994.913023860448
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2996.607159377933
Current xi:  [823.19885]
objective value function right now is: -2996.607159377933
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [825.6642]
objective value function right now is: -2992.9043080548095
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [828.11206]
objective value function right now is: -2992.61482461138
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -2998.335391246526
Current xi:  [830.3203]
objective value function right now is: -2998.335391246526
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [832.9122]
objective value function right now is: -2998.2243636120443
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -2999.3467164120757
Current xi:  [834.8691]
objective value function right now is: -2999.3467164120757
new min fval from sgd:  -3000.327285946655
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [837.2366]
objective value function right now is: -3000.327285946655
new min fval from sgd:  -3000.345704481194
new min fval from sgd:  -3000.3550862790726
new min fval from sgd:  -3000.3966090592708
new min fval from sgd:  -3000.492724759044
new min fval from sgd:  -3000.6718482979054
new min fval from sgd:  -3000.7392440796775
new min fval from sgd:  -3000.7410657787023
new min fval from sgd:  -3000.871664613533
new min fval from sgd:  -3000.9635126006224
new min fval from sgd:  -3001.034669181955
new min fval from sgd:  -3001.1092151457406
new min fval from sgd:  -3001.124136047777
new min fval from sgd:  -3001.312421500373
new min fval from sgd:  -3001.480562493063
new min fval from sgd:  -3001.588952437753
new min fval from sgd:  -3001.5918223721183
new min fval from sgd:  -3001.6373040923036
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [839.03064]
objective value function right now is: -2999.262552234439
new min fval from sgd:  -3001.6442110261037
new min fval from sgd:  -3001.671464828298
new min fval from sgd:  -3001.841275801109
new min fval from sgd:  -3001.8717949255292
new min fval from sgd:  -3001.9033377614273
new min fval from sgd:  -3001.9877432430144
new min fval from sgd:  -3002.031785533168
new min fval from sgd:  -3002.0821189742455
new min fval from sgd:  -3002.1156038901836
new min fval from sgd:  -3002.178200764482
new min fval from sgd:  -3002.2305263993335
new min fval from sgd:  -3002.3962098637912
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [841.2439]
objective value function right now is: -2999.9492970099495
new min fval from sgd:  -3002.41933531321
new min fval from sgd:  -3002.4433807005007
new min fval from sgd:  -3002.567128692953
new min fval from sgd:  -3002.666993258346
new min fval from sgd:  -3002.7086041708003
new min fval from sgd:  -3002.7113222730964
new min fval from sgd:  -3002.7510631464384
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [843.33875]
objective value function right now is: -3001.1994391912735
new min fval from sgd:  -3002.751690854449
new min fval from sgd:  -3002.784183244806
new min fval from sgd:  -3002.7994049765557
new min fval from sgd:  -3002.824033653237
new min fval from sgd:  -3002.8355951610088
new min fval from sgd:  -3002.8453742573256
new min fval from sgd:  -3002.8681959561222
new min fval from sgd:  -3002.8965049294643
new min fval from sgd:  -3002.9283338747573
new min fval from sgd:  -3002.958171957545
new min fval from sgd:  -3002.982074507623
new min fval from sgd:  -3003.0062640152555
new min fval from sgd:  -3003.020386970396
new min fval from sgd:  -3003.0331119657862
new min fval from sgd:  -3003.0501968451063
new min fval from sgd:  -3003.080697419553
new min fval from sgd:  -3003.1029128765695
new min fval from sgd:  -3003.1171008820666
new min fval from sgd:  -3003.119123136961
new min fval from sgd:  -3003.1431564322775
new min fval from sgd:  -3003.176145548335
new min fval from sgd:  -3003.2060910027612
new min fval from sgd:  -3003.232058240387
new min fval from sgd:  -3003.265982521865
new min fval from sgd:  -3003.2851983994583
new min fval from sgd:  -3003.2940113942045
new min fval from sgd:  -3003.306360385182
new min fval from sgd:  -3003.3137910304986
new min fval from sgd:  -3003.3180913465017
new min fval from sgd:  -3003.3299276185153
new min fval from sgd:  -3003.3407061037205
new min fval from sgd:  -3003.346162842453
new min fval from sgd:  -3003.356431948174
new min fval from sgd:  -3003.36358739233
new min fval from sgd:  -3003.3698455040417
new min fval from sgd:  -3003.3733159019193
new min fval from sgd:  -3003.3759956371227
new min fval from sgd:  -3003.377317622101
new min fval from sgd:  -3003.3822308938347
new min fval from sgd:  -3003.407626155857
new min fval from sgd:  -3003.428967543367
new min fval from sgd:  -3003.434878962541
new min fval from sgd:  -3003.436948785081
new min fval from sgd:  -3003.437653661056
new min fval from sgd:  -3003.4382475077978
new min fval from sgd:  -3003.452293574926
new min fval from sgd:  -3003.4992410496225
new min fval from sgd:  -3003.529393667996
new min fval from sgd:  -3003.555714627214
new min fval from sgd:  -3003.584996946294
new min fval from sgd:  -3003.610927466821
new min fval from sgd:  -3003.627428903837
new min fval from sgd:  -3003.637404081654
new min fval from sgd:  -3003.6516567267136
new min fval from sgd:  -3003.651836229504
new min fval from sgd:  -3003.6543785859535
new min fval from sgd:  -3003.6555808251146
new min fval from sgd:  -3003.6556112826156
new min fval from sgd:  -3003.6671837400877
new min fval from sgd:  -3003.669154325453
new min fval from sgd:  -3003.678106684632
new min fval from sgd:  -3003.6875719404093
new min fval from sgd:  -3003.6993318591135
new min fval from sgd:  -3003.7053111762098
new min fval from sgd:  -3003.7088737317044
new min fval from sgd:  -3003.710658009367
new min fval from sgd:  -3003.722909819209
new min fval from sgd:  -3003.7324624951493
new min fval from sgd:  -3003.7405299768498
new min fval from sgd:  -3003.743055343243
new min fval from sgd:  -3003.7435429024026
new min fval from sgd:  -3003.7469590445685
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [844.71185]
objective value function right now is: -3003.7433816523567
new min fval from sgd:  -3003.7622514572145
new min fval from sgd:  -3003.8181697898835
new min fval from sgd:  -3003.8617226953975
new min fval from sgd:  -3003.8839856139903
new min fval from sgd:  -3003.891644621718
new min fval from sgd:  -3003.9089953875914
new min fval from sgd:  -3003.921322207916
new min fval from sgd:  -3003.93200804138
new min fval from sgd:  -3003.9343898783864
new min fval from sgd:  -3003.939430727995
new min fval from sgd:  -3003.9448189402474
new min fval from sgd:  -3003.953178179566
new min fval from sgd:  -3003.9611158994244
new min fval from sgd:  -3003.9637559179523
new min fval from sgd:  -3003.9670644788844
new min fval from sgd:  -3003.967638934424
new min fval from sgd:  -3003.9677010836585
new min fval from sgd:  -3003.972746039252
new min fval from sgd:  -3003.978811622144
new min fval from sgd:  -3004.023573199063
new min fval from sgd:  -3004.060656222273
new min fval from sgd:  -3004.0908030035685
new min fval from sgd:  -3004.1138594992462
new min fval from sgd:  -3004.131244098695
new min fval from sgd:  -3004.1435849639865
new min fval from sgd:  -3004.1481979852692
new min fval from sgd:  -3004.1498973577254
new min fval from sgd:  -3004.1538852311633
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [845.1891]
objective value function right now is: -3004.0812758631623
min fval:  -3004.1538852311633
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-2.5263e+01,  5.5626e+00],
        [-9.2375e+00,  8.9720e+00],
        [ 1.2454e+01, -7.0490e-01],
        [ 6.8132e+00, -1.0601e+00],
        [ 1.0194e+01, -5.7355e-01],
        [ 8.5200e+00, -5.0510e-01],
        [ 1.1747e+01, -5.8749e-01],
        [-1.2529e+00, -3.7962e-01],
        [ 9.9800e+00, -1.0363e-02],
        [-6.7443e-01, -1.1170e+01],
        [-6.4751e+00,  1.1722e+01],
        [ 6.5122e+00, -1.0558e+00],
        [-1.1024e+01,  4.9609e+00]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  3.9078,  -0.8962, -10.3990,  -9.7810, -10.0079, -10.0329, -10.1872,
         -3.7385,  -9.9710,  -1.7351,   1.0056,  -9.6171,  -4.7921],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0527e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6752e-02, -3.5521e-01,
         -4.2893e-01, -1.5199e-03, -2.8530e-02],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0528e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6752e-02, -3.5521e-01,
         -4.2893e-01, -1.5199e-03, -2.8530e-02],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0528e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6753e-02, -3.5521e-01,
         -4.2893e-01, -1.5199e-03, -2.8530e-02],
        [-9.5480e+00, -3.4013e+00,  9.0569e+00,  4.7854e-01,  3.7939e+00,
          2.4102e+00,  5.9198e+00,  2.8689e-02,  5.9013e+00,  1.6841e+01,
         -2.7326e+00,  4.0908e-01, -7.0008e-01],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0527e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6752e-02, -3.5521e-01,
         -4.2894e-01, -1.5199e-03, -2.8530e-02],
        [ 7.6600e+00,  3.2058e+00, -9.1472e+00, -1.0581e+00, -3.1984e+00,
         -1.8967e+00, -5.8390e+00, -2.6283e-01, -5.3336e+00, -1.6576e+01,
          2.6031e+00, -9.2098e-01,  2.0315e+00],
        [-1.0339e+01, -3.7270e+00,  9.9564e+00,  1.2205e+00,  4.5562e+00,
          3.3894e+00,  6.5062e+00, -5.9699e-03,  6.9342e+00,  1.8001e+01,
         -3.0201e+00,  1.1625e+00, -1.8375e+00],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0528e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6753e-02, -3.5521e-01,
         -4.2893e-01, -1.5199e-03, -2.8530e-02],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0527e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6752e-02, -3.5521e-01,
         -4.2894e-01, -1.5199e-03, -2.8530e-02],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0527e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6752e-02, -3.5521e-01,
         -4.2893e-01, -1.5199e-03, -2.8530e-02],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0527e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6752e-02, -3.5521e-01,
         -4.2893e-01, -1.5199e-03, -2.8530e-02],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0527e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6752e-02, -3.5521e-01,
         -4.2894e-01, -1.5199e-03, -2.8530e-02],
        [-3.1553e-01, -1.8219e-01, -7.4247e-02, -2.0576e-03, -1.0527e-02,
         -1.8054e-02, -5.1256e-02, -1.3665e-02, -4.6752e-02, -3.5521e-01,
         -4.2893e-01, -1.5199e-03, -2.8530e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.2065, -2.2065, -2.2065, -3.0125, -2.2065,  1.8656, -2.9748, -2.2065,
        -2.2065, -2.2065, -2.2065, -2.2065, -2.2065], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0522,   0.0522,   0.0522,  -8.5903,   0.0522,  16.0263, -12.2656,
           0.0522,   0.0522,   0.0522,   0.0522,   0.0522,   0.0522]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.0146,   0.2543],
        [ -2.1249,   7.7531],
        [ -1.1371,   8.0667],
        [ 10.9639,   3.0109],
        [-12.7895,  -3.5973],
        [-11.2361,   6.5161],
        [ -2.0737,   6.2144],
        [-19.3236,   7.6972],
        [  1.2787,   6.0057],
        [-12.0030,  -3.2098],
        [-10.3500,  -0.4829],
        [-13.4132,   6.4745],
        [-12.4046,  -3.6595]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-5.4028, -5.3351,  4.2484, -0.5421, -1.2992,  2.8738, -9.7406,  1.2844,
         5.5323,  1.3639,  8.0985, -5.1474, -2.5011], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.0551e-02,  8.9914e-02, -5.0418e+00, -6.7311e-01,  7.3789e+00,
         -1.0598e+01,  1.6490e-02, -4.4531e+00, -1.8672e+00,  7.1606e+00,
          2.7157e+00, -9.6447e-02,  5.3128e+00],
        [ 2.6873e-03, -3.1556e-01, -8.5696e-02, -3.4595e+00,  4.4744e-02,
         -2.5078e-01, -1.5108e-01, -9.9928e-02, -1.7793e+00,  2.1748e-02,
         -1.7037e+00, -1.1603e-01,  3.0568e-02],
        [-5.2017e-04, -1.6029e-01, -3.5824e-01, -3.0002e+00, -2.8015e-02,
         -2.0338e-01, -9.4977e-02, -1.1359e-01, -1.3914e+00, -9.0692e-02,
         -1.4340e+00, -8.7701e-02, -3.2229e-02],
        [ 1.0517e-01, -3.4911e+00, -4.9575e+00,  1.7137e-01,  9.3268e-02,
         -1.6276e+00, -5.9251e-01, -5.0836e-01,  2.2484e+00, -5.1966e+00,
         -1.0420e+01, -4.7317e-01,  6.6864e-02],
        [ 4.9735e-03, -4.1971e-01,  3.3231e-02, -3.4748e+00,  4.8848e-02,
         -3.1744e-01, -1.9678e-01, -1.2257e-01, -1.9328e+00,  1.2741e-01,
         -1.8670e+00, -1.4829e-01,  4.7496e-02],
        [-1.4225e-02,  1.9113e+00,  9.5669e-01, -3.1535e+00, -5.5510e-01,
          2.2057e+00, -3.2462e+00,  7.2864e+00,  2.0217e+00, -1.7873e+00,
          4.7190e+00, -5.0543e+00, -4.2296e+00],
        [ 1.3719e-01, -4.7523e-02, -1.6247e+01, -5.1811e+00,  1.8992e+01,
         -5.9705e+00, -1.1030e-02, -5.5893e+00, -1.0384e+01,  1.3992e+01,
          6.2219e+00, -6.0265e-02,  1.6660e+01],
        [ 4.9505e-03, -4.1852e-01,  3.1437e-02, -3.4720e+00,  4.8924e-02,
         -3.1669e-01, -1.9640e-01, -1.2258e-01, -1.9327e+00,  1.2526e-01,
         -1.8663e+00, -1.4806e-01,  4.7379e-02],
        [ 2.4954e-03, -3.0735e-01, -9.6036e-02, -3.4596e+00,  4.4300e-02,
         -2.4509e-01, -1.4776e-01, -9.8355e-02, -1.7729e+00,  1.5476e-02,
         -1.6948e+00, -1.1379e-01,  2.9337e-02],
        [ 1.8381e-02,  4.3796e+00, -7.2229e-01, -1.2604e+00, -1.4757e+00,
         -5.5794e+00,  3.7404e+00, -2.2716e+00,  1.3132e+00, -3.9707e+00,
         -1.5020e+00, -9.0181e-01,  5.5348e-01],
        [-7.7680e-02,  2.4098e-04,  4.5947e-01, -4.2478e+00,  8.2098e+00,
          1.5562e+00, -7.8526e-05,  5.7730e-02, -3.9597e+01,  7.0748e+00,
         -5.1515e+00, -9.8557e-05,  8.7370e+00],
        [ 9.1487e-02, -3.1155e+00, -6.1387e-01, -9.8513e-01,  3.4467e-02,
         -1.9774e+00, -5.9494e-01, -4.8375e-01,  1.3646e+00,  3.4214e-01,
         -7.5207e+00, -4.5388e-01,  1.0033e-02],
        [ 3.0955e-03, -3.2801e-01, -7.3509e-02, -3.4659e+00,  4.7533e-02,
         -2.5577e-01, -1.5808e-01, -1.0255e-01, -1.8168e+00,  3.3280e-02,
         -1.7323e+00, -1.2120e-01,  3.5054e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.5076, -3.5384, -3.9057, -2.8815, -3.5488, -6.3215, -3.8458, -3.5511,
        -3.5315, -0.2162, -3.9220, -4.7765, -3.5351], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-6.8541e+01,  1.6155e-02, -6.9425e-03, -2.4388e+00,  2.5506e-02,
          4.1831e+00,  8.0546e-01,  2.4758e-02,  1.4161e-02, -1.4113e+01,
          1.4917e-06, -3.3799e-01,  1.3780e-02],
        [ 4.0970e-01,  1.0230e-01,  2.2945e-01,  5.1799e+00,  1.8352e-01,
          1.0252e-01, -5.7467e+00,  1.8250e-01,  9.6292e-02,  1.3207e+00,
          3.9403e-01,  3.4988e+00,  9.9380e-02],
        [-1.2342e+01, -8.0051e-03,  2.0171e-02, -5.5101e-02, -1.6493e-02,
         -3.9222e+01,  5.1248e+00, -1.6181e-02, -7.4177e-03, -5.0522e+01,
          1.8666e+01, -1.8603e-02, -9.3465e-03],
        [-1.6464e+01, -2.3833e-02, -3.5650e-02, -2.8042e-02, -2.0353e-02,
         -3.5048e+01, -1.1445e-02, -2.0343e-02, -2.4065e-02, -4.7368e+01,
          2.2236e-04, -1.9383e-02, -2.2851e-02],
        [ 2.0931e+00, -2.4194e-02,  2.1556e-01, -2.5696e+00, -8.3338e-02,
          2.4551e-01,  7.0172e+00, -8.3109e-02, -2.2976e-02,  1.5327e+00,
         -1.0617e+01, -2.2547e+00, -4.5724e-02]], device='cuda:0'))])
xi:  [845.18384]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1023.7898159476138
W_T_median: 998.5138431701287
W_T_pctile_5: 845.3003970862228
W_T_CVAR_5_pct: 498.59037201214795
Average q (qsum/M+1):  48.657864478326616
Optimal xi:  [845.18384]
Expected(across Rb) median(across samples) p_equity:  0.27700424681728086
obj fun:  tensor(-3004.1539, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 1138.474873568312
W_T_median: 891.4398755591162
W_T_pctile_5: -169.98079142996517
W_T_CVAR_5_pct: -317.5137579687884
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1645.4926375859418
Current xi:  [124.27457]
objective value function right now is: -1645.4926375859418
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1952.4406876191601
Current xi:  [147.86685]
objective value function right now is: -1952.4406876191601
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -2122.885493148254
Current xi:  [170.69444]
objective value function right now is: -2122.885493148254
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -2230.5144356755727
Current xi:  [194.38408]
objective value function right now is: -2230.5144356755727
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -2328.956035099756
Current xi:  [217.65753]
objective value function right now is: -2328.956035099756
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -2406.006043728513
Current xi:  [241.0572]
objective value function right now is: -2406.006043728513
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -2727.302651340619
Current xi:  [263.56995]
objective value function right now is: -2727.302651340619
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -2814.185273348811
Current xi:  [286.32776]
objective value function right now is: -2814.185273348811
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -2899.582898120059
Current xi:  [309.04678]
objective value function right now is: -2899.582898120059
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -2968.900594101528
Current xi:  [331.7151]
objective value function right now is: -2968.900594101528
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -3027.2763243359955
Current xi:  [354.294]
objective value function right now is: -3027.2763243359955
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -3113.6984078135624
Current xi:  [376.79095]
objective value function right now is: -3113.6984078135624
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -3152.4135345857303
Current xi:  [399.13174]
objective value function right now is: -3152.4135345857303
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -3219.339926580926
Current xi:  [421.25446]
objective value function right now is: -3219.339926580926
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -3319.794486232321
Current xi:  [443.6125]
objective value function right now is: -3319.794486232321
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -3387.364784210307
Current xi:  [465.50415]
objective value function right now is: -3387.364784210307
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -3443.544266337374
Current xi:  [487.3424]
objective value function right now is: -3443.544266337374
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [506.8198]
objective value function right now is: -3441.410505820311
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -3528.1781939517323
Current xi:  [528.3714]
objective value function right now is: -3528.1781939517323
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -3588.740458362648
Current xi:  [550.0896]
objective value function right now is: -3588.740458362648
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [571.7858]
objective value function right now is: -3482.9001791344663
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -3667.3036291709486
Current xi:  [592.8871]
objective value function right now is: -3667.3036291709486
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -3720.1237213254453
Current xi:  [613.45337]
objective value function right now is: -3720.1237213254453
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -3762.9335629782454
Current xi:  [634.15607]
objective value function right now is: -3762.9335629782454
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -3794.2523216113495
Current xi:  [654.2723]
objective value function right now is: -3794.2523216113495
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -3836.7911030917176
Current xi:  [673.9335]
objective value function right now is: -3836.7911030917176
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -3872.2113946904396
Current xi:  [693.1419]
objective value function right now is: -3872.2113946904396
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -3895.1121610623827
Current xi:  [712.36426]
objective value function right now is: -3895.1121610623827
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -3919.4564360091026
Current xi:  [730.8265]
objective value function right now is: -3919.4564360091026
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [748.00793]
objective value function right now is: -3903.4295462786904
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -3951.553851366567
Current xi:  [764.81415]
objective value function right now is: -3951.553851366567
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -3977.1912603147716
Current xi:  [780.9254]
objective value function right now is: -3977.1912603147716
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [796.9487]
objective value function right now is: -3973.5252316413344
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -3985.25125803512
Current xi:  [812.1608]
objective value function right now is: -3985.25125803512
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [825.2325]
objective value function right now is: -3973.6020106799992
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -4012.6358467661767
Current xi:  [827.47833]
objective value function right now is: -4012.6358467661767
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -4020.602847067237
Current xi:  [830.56915]
objective value function right now is: -4020.602847067237
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [833.48065]
objective value function right now is: -4020.2291840371963
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [836.2815]
objective value function right now is: -4018.6934051624316
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -4025.1939786021217
Current xi:  [839.0471]
objective value function right now is: -4025.1939786021217
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -4026.69975491268
Current xi:  [841.77747]
objective value function right now is: -4026.69975491268
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -4029.1900246293376
Current xi:  [844.35626]
objective value function right now is: -4029.1900246293376
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [846.8545]
objective value function right now is: -4028.196604928798
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -4030.850706150759
Current xi:  [849.3821]
objective value function right now is: -4030.850706150759
new min fval from sgd:  -4034.926044251686
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [851.8567]
objective value function right now is: -4034.926044251686
new min fval from sgd:  -4034.9653827359753
new min fval from sgd:  -4035.0736327772634
new min fval from sgd:  -4035.1235976779276
new min fval from sgd:  -4035.1875793489958
new min fval from sgd:  -4035.3017264434097
new min fval from sgd:  -4035.4324821083815
new min fval from sgd:  -4035.501134922321
new min fval from sgd:  -4035.5535406228296
new min fval from sgd:  -4035.618181972831
new min fval from sgd:  -4035.913719203428
new min fval from sgd:  -4035.9560001739915
new min fval from sgd:  -4035.9657143648706
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [854.2334]
objective value function right now is: -4035.5522464736046
new min fval from sgd:  -4036.1486833831987
new min fval from sgd:  -4036.3191527589806
new min fval from sgd:  -4036.398951417802
new min fval from sgd:  -4036.4568750678454
new min fval from sgd:  -4036.477630995308
new min fval from sgd:  -4036.4795281151487
new min fval from sgd:  -4036.5228662542154
new min fval from sgd:  -4036.6933209456843
new min fval from sgd:  -4036.828902235536
new min fval from sgd:  -4036.894166289449
new min fval from sgd:  -4036.971919417024
new min fval from sgd:  -4037.062285774093
new min fval from sgd:  -4037.0907286434185
new min fval from sgd:  -4037.1056172050294
new min fval from sgd:  -4037.1432016933736
new min fval from sgd:  -4037.1770060599088
new min fval from sgd:  -4037.2121793633632
new min fval from sgd:  -4037.3592741099746
new min fval from sgd:  -4037.480556929697
new min fval from sgd:  -4037.5150189623255
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [856.6069]
objective value function right now is: -4035.978499906589
new min fval from sgd:  -4037.531118396854
new min fval from sgd:  -4037.6639299430717
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [859.06775]
objective value function right now is: -4034.7539195280947
new min fval from sgd:  -4037.737143782546
new min fval from sgd:  -4037.8313728983535
new min fval from sgd:  -4038.0648568260767
new min fval from sgd:  -4038.2546904521864
new min fval from sgd:  -4038.5719574902355
new min fval from sgd:  -4038.8253654564965
new min fval from sgd:  -4038.9386384650725
new min fval from sgd:  -4038.9785393917573
new min fval from sgd:  -4039.0209096900526
new min fval from sgd:  -4039.069139472519
new min fval from sgd:  -4039.1185480117024
new min fval from sgd:  -4039.1513361501165
new min fval from sgd:  -4039.172226351888
new min fval from sgd:  -4039.1851046110487
new min fval from sgd:  -4039.1979412324094
new min fval from sgd:  -4039.2091874474063
new min fval from sgd:  -4039.2132376160744
new min fval from sgd:  -4039.2204649315736
new min fval from sgd:  -4039.222042436696
new min fval from sgd:  -4039.2901927210373
new min fval from sgd:  -4039.356688876921
new min fval from sgd:  -4039.3905945105985
new min fval from sgd:  -4039.4111308191827
new min fval from sgd:  -4039.4259046157163
new min fval from sgd:  -4039.428771787948
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [860.32513]
objective value function right now is: -4039.428771787948
new min fval from sgd:  -4039.4603071073993
new min fval from sgd:  -4039.564034889713
new min fval from sgd:  -4039.629899314027
new min fval from sgd:  -4039.6803425940216
new min fval from sgd:  -4039.71052922756
new min fval from sgd:  -4039.7392933123087
new min fval from sgd:  -4039.7833493153985
new min fval from sgd:  -4039.819621331076
new min fval from sgd:  -4039.8499717194063
new min fval from sgd:  -4039.8736546427854
new min fval from sgd:  -4039.8844275188853
new min fval from sgd:  -4039.890768850541
new min fval from sgd:  -4039.910081469449
new min fval from sgd:  -4039.93136742812
new min fval from sgd:  -4039.939083402473
new min fval from sgd:  -4039.943986966026
new min fval from sgd:  -4039.953006128792
new min fval from sgd:  -4039.9571781532222
new min fval from sgd:  -4039.969715038854
new min fval from sgd:  -4039.9872890704096
new min fval from sgd:  -4040.0025401439134
new min fval from sgd:  -4040.0225626318193
new min fval from sgd:  -4040.0467710466805
new min fval from sgd:  -4040.061726193747
new min fval from sgd:  -4040.075897651199
new min fval from sgd:  -4040.0824114252423
new min fval from sgd:  -4040.094940109393
new min fval from sgd:  -4040.107456478555
new min fval from sgd:  -4040.121796339712
new min fval from sgd:  -4040.135444025588
new min fval from sgd:  -4040.1446848615815
new min fval from sgd:  -4040.1545948735834
new min fval from sgd:  -4040.157154586415
new min fval from sgd:  -4040.160681797954
new min fval from sgd:  -4040.175441643173
new min fval from sgd:  -4040.196631709384
new min fval from sgd:  -4040.234682639864
new min fval from sgd:  -4040.2689506431902
new min fval from sgd:  -4040.2960116858735
new min fval from sgd:  -4040.3138927383793
new min fval from sgd:  -4040.321186250678
new min fval from sgd:  -4040.3300302352213
new min fval from sgd:  -4040.333224582649
new min fval from sgd:  -4040.337111492686
new min fval from sgd:  -4040.3384810086322
new min fval from sgd:  -4040.3660265013846
new min fval from sgd:  -4040.3771415772176
new min fval from sgd:  -4040.3937125646503
new min fval from sgd:  -4040.426376601871
new min fval from sgd:  -4040.4298652966304
new min fval from sgd:  -4040.430672614389
new min fval from sgd:  -4040.432344960211
new min fval from sgd:  -4040.444158765953
new min fval from sgd:  -4040.4562714424487
new min fval from sgd:  -4040.4623651807883
new min fval from sgd:  -4040.466663810108
new min fval from sgd:  -4040.4691026916066
new min fval from sgd:  -4040.469691980942
new min fval from sgd:  -4040.4722924591965
new min fval from sgd:  -4040.4734398633905
new min fval from sgd:  -4040.4748191898652
new min fval from sgd:  -4040.4801883289806
new min fval from sgd:  -4040.4854429540183
new min fval from sgd:  -4040.48877601595
new min fval from sgd:  -4040.495640878666
new min fval from sgd:  -4040.5021310644242
new min fval from sgd:  -4040.5103578321427
new min fval from sgd:  -4040.519455877506
new min fval from sgd:  -4040.5305327591714
new min fval from sgd:  -4040.5332551241404
new min fval from sgd:  -4040.536880109959
new min fval from sgd:  -4040.5451305993665
new min fval from sgd:  -4040.556762813895
new min fval from sgd:  -4040.5785365345005
new min fval from sgd:  -4040.5906438491916
new min fval from sgd:  -4040.5925667786255
new min fval from sgd:  -4040.593068254399
new min fval from sgd:  -4040.597785871257
new min fval from sgd:  -4040.599478724201
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [860.86566]
objective value function right now is: -4040.5660242453605
min fval:  -4040.599478724201
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  5.4842,   1.7457],
        [ -2.3237,  20.9502],
        [ -1.0837,  -0.6091],
        [ 11.1950,  -0.7713],
        [ 10.0334,  -7.7157],
        [ 12.4092,  -3.3441],
        [-11.4096,   0.9930],
        [ 15.9614,  -3.8913],
        [-12.0443,   1.0789],
        [  2.3284, -14.6345],
        [  0.0765, -15.8995],
        [-14.4642,   3.2955],
        [ -1.0842,  -0.6099]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-11.0128,  -0.3604,  -3.4295,  -8.6444,  -3.2018,  -2.2513,   7.9982,
         -2.7143,   8.5276,  -2.8851,  -3.5168,   2.0183,  -3.4291],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 4.7015e+00, -9.1661e+00,  2.0161e-01,  1.1218e+01,  6.4544e+00,
          4.3695e+00, -8.1933e+00,  5.1638e+00, -1.0706e+01,  1.3552e+01,
          1.8211e+01, -8.7633e-01,  2.0267e-01],
        [ 2.3724e-02, -2.4174e-01, -1.1759e-02, -5.6326e-02, -3.8082e-02,
         -3.8133e-01, -6.7608e-01, -3.7028e-01, -6.8493e-01, -2.2704e-01,
         -1.7073e-01, -3.8394e-01, -1.1773e-02],
        [ 2.3724e-02, -2.4174e-01, -1.1759e-02, -5.6326e-02, -3.8082e-02,
         -3.8133e-01, -6.7608e-01, -3.7028e-01, -6.8493e-01, -2.2704e-01,
         -1.7073e-01, -3.8394e-01, -1.1773e-02],
        [ 2.3724e-02, -2.4174e-01, -1.1759e-02, -5.6326e-02, -3.8082e-02,
         -3.8133e-01, -6.7608e-01, -3.7028e-01, -6.8493e-01, -2.2704e-01,
         -1.7073e-01, -3.8394e-01, -1.1773e-02],
        [ 2.3724e-02, -2.4174e-01, -1.1759e-02, -5.6326e-02, -3.8082e-02,
         -3.8133e-01, -6.7608e-01, -3.7028e-01, -6.8493e-01, -2.2704e-01,
         -1.7073e-01, -3.8394e-01, -1.1773e-02],
        [ 4.8601e+00, -1.5939e+01,  1.3467e-01,  8.7291e+00,  6.2330e+00,
          7.8241e+00, -4.3390e+00,  7.4038e+00, -6.4961e+00,  1.5376e+01,
          1.9386e+01, -5.7770e+00,  1.3612e-01],
        [ 4.8187e+00, -1.3998e+01,  2.6784e-02,  1.1906e+01,  7.2629e+00,
          7.2438e+00, -6.7163e+00,  6.4444e+00, -8.4976e+00,  1.1575e+01,
          1.4942e+01, -3.7489e+00,  2.5027e-02],
        [ 2.3724e-02, -2.4174e-01, -1.1759e-02, -5.6326e-02, -3.8082e-02,
         -3.8133e-01, -6.7608e-01, -3.7028e-01, -6.8493e-01, -2.2704e-01,
         -1.7073e-01, -3.8394e-01, -1.1773e-02],
        [-1.9032e-02, -2.1767e+00, -2.2652e-01, -5.4142e-03,  1.4409e+01,
          1.5874e+01, -4.2952e+00,  1.7182e+01, -6.0572e+00,  2.6094e+00,
          3.4386e+00, -8.8028e+00, -2.2807e-01],
        [ 1.0027e-01,  1.1041e+00,  3.3275e-02,  2.8062e-01,  6.1554e-01,
          1.4340e+00,  1.6799e+00,  1.3600e+00,  1.7245e+00,  3.5172e-01,
          1.1985e-01,  5.7216e-01,  3.3285e-02],
        [ 2.3724e-02, -2.4174e-01, -1.1759e-02, -5.6326e-02, -3.8082e-02,
         -3.8133e-01, -6.7608e-01, -3.7028e-01, -6.8493e-01, -2.2704e-01,
         -1.7073e-01, -3.8394e-01, -1.1773e-02],
        [ 2.3724e-02, -2.4174e-01, -1.1759e-02, -5.6326e-02, -3.8082e-02,
         -3.8133e-01, -6.7608e-01, -3.7028e-01, -6.8493e-01, -2.2704e-01,
         -1.7073e-01, -3.8394e-01, -1.1773e-02],
        [ 2.3724e-02, -2.4174e-01, -1.1759e-02, -5.6326e-02, -3.8082e-02,
         -3.8133e-01, -6.7608e-01, -3.7028e-01, -6.8493e-01, -2.2704e-01,
         -1.7073e-01, -3.8394e-01, -1.1773e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 0.5472, -0.8975, -0.8975, -0.8975, -0.8975, -0.2979, -0.5856, -0.8975,
         1.5826,  2.1912, -0.8975, -0.8975, -0.8975], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-11.8518,   0.0591,   0.0591,   0.0591,   0.0591, -12.1750,  -9.8670,
           0.0591,  -4.5685,  13.6369,   0.0591,   0.0591,   0.0591]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  5.7392,   0.8660],
        [ -2.5619,   0.7036],
        [-10.7902,  -3.7287],
        [-11.4305,  -3.7065],
        [-12.1812,  -4.0507],
        [ -2.6153,   3.4839],
        [-12.2496,  -4.1285],
        [-17.1808,  -5.6229],
        [-12.7091,  -4.1861],
        [  2.8712,   8.1084],
        [ -6.0131,   2.5466],
        [-16.9900,   4.9966],
        [-16.6843,   4.0052]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-7.6089, -5.7072, -3.6444,  1.8692, -3.9748, -8.8058, -3.9268, -2.2799,
        -3.3852,  0.1138,  3.5286,  1.1892, -0.9168], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.1373e+01, -2.9831e-01,  8.3475e+00,  4.2671e+00,  7.7420e+00,
         -7.8562e-02,  7.1344e+00,  1.0472e+01,  7.7434e+00, -1.0344e+01,
         -3.8165e+00, -1.1814e+01, -1.2950e+01],
        [-8.3675e-01,  1.7674e-02,  1.1230e-01, -1.6329e-01,  1.0499e-01,
         -3.6365e-01,  1.0897e-01,  2.2473e-01,  1.3684e-01, -8.1323e-01,
         -4.4943e-02, -3.3660e-01, -4.0293e-01],
        [-3.1643e-01, -1.7327e-02, -1.1764e-02, -7.9038e-01, -7.0790e-03,
          1.2625e-02, -8.2534e-03, -5.9328e-02, -1.6020e-02, -1.5194e+00,
         -3.3765e+00, -3.9219e-02,  1.0099e-02],
        [ 1.3774e+00,  6.8305e-02, -1.5884e+00,  1.0529e-01, -1.2915e+00,
          5.2336e+00, -1.3934e+00, -5.9350e+00, -2.2535e+00,  3.0932e+00,
          1.0517e+00,  1.1633e+00,  5.8054e+00],
        [-1.3238e+01, -4.9447e-02, -5.6764e+00, -2.4895e+00, -7.5824e+00,
         -9.2110e+00, -5.4895e+00, -1.2219e+01, -8.3252e+00, -1.1805e+00,
          7.8824e+00,  1.0993e+01,  5.9382e+00],
        [-8.8699e-01,  1.8873e-02,  1.3085e-01, -1.3791e-01,  1.2101e-01,
         -3.4931e-01,  1.2580e-01,  2.5074e-01,  1.6204e-01, -9.1326e-01,
          8.7020e-02, -4.3226e-01, -4.3116e-01],
        [ 7.4857e-01, -1.3851e-02,  5.3160e-01, -1.8688e+01, -9.5339e-02,
         -2.7508e-02, -5.9419e-02,  2.0516e+00,  4.0850e-01,  1.9026e-01,
         -2.1448e+01, -1.3353e-02, -6.9958e-03],
        [ 2.2794e+00, -3.4815e-02,  1.6837e-01, -6.3021e-01,  1.5744e-01,
          5.4897e+00,  1.5955e-01,  5.0741e-01,  2.1283e-01, -3.6183e-01,
         -1.9075e+00,  5.0728e+00,  7.1068e+00],
        [-8.8086e-01,  1.8649e-02,  1.2734e-01, -1.3891e-01,  1.1844e-01,
         -3.5789e-01,  1.2295e-01,  2.5140e-01,  1.5728e-01, -8.7466e-01,
          5.9758e-02, -4.0179e-01, -4.2614e-01],
        [ 1.0044e+01,  2.3484e-02,  5.7609e+00, -1.0873e+01,  7.7832e+00,
          1.4628e-02,  5.6381e+00,  2.0713e+01,  1.0207e+01, -5.7883e+00,
         -3.0012e+01,  2.2697e+00,  1.2560e+00],
        [-9.1466e-01,  1.9029e-02,  1.3426e-01, -8.1645e-02,  1.2447e-01,
         -3.4435e-01,  1.2936e-01,  2.4386e-01,  1.6679e-01, -8.4137e-01,
          5.7138e-02, -4.6326e-01, -4.4021e-01],
        [ 5.2903e+00, -3.2259e-01, -2.1667e-01,  6.8081e-01, -1.9747e+00,
          6.5223e-01, -1.3919e+00, -4.5889e+00, -5.0826e-01, -4.8570e-01,
         -2.2097e-01, -9.6557e+00, -4.5325e+00],
        [-9.6375e-01,  1.7983e-02,  1.0976e-01, -1.3910e-01,  1.0165e-01,
         -3.5397e-01,  1.0554e-01,  2.0655e-01,  1.3414e-01, -5.8843e-01,
          6.8727e-03, -4.4446e-01, -4.3234e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.0970, -7.7415, -7.2619, -7.6831, -5.4331, -7.6561, -1.5352, -8.3373,
        -7.6885, -4.4867, -7.6672, -1.7167, -7.7992], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.2927e+02, -7.6102e-01, -1.8073e+00, -4.0198e-01,  3.9245e+00,
         -7.4481e-01, -3.7356e+01,  1.1835e+00, -7.3947e-01, -2.3678e+00,
         -7.8160e-01, -7.1474e+00, -8.0916e-01],
        [-1.8818e+00,  8.3704e-01,  2.2961e+00, -2.6810e-01, -9.9666e-02,
          1.3500e+00,  2.4298e+00,  2.8459e+00,  1.2271e+00,  1.1100e+01,
          1.5159e+00,  2.0585e+00,  1.4031e+00],
        [-1.9794e+01, -1.4639e-01, -6.0995e-02, -6.3540e+00, -2.6278e+01,
         -1.6553e-01, -5.1866e+00, -2.9503e+00, -1.6047e-01, -9.5866e-02,
         -1.6829e-01, -5.9174e+01, -1.5312e-01],
        [-2.0261e+01, -1.1894e-01, -4.5814e-02, -4.9551e+00, -2.5522e+01,
         -1.3805e-01, -3.8821e+00, -3.8095e+00, -1.3297e-01, -8.6666e-02,
         -1.4056e-01, -5.6315e+01, -1.2485e-01],
        [ 8.5377e+00,  1.9001e-01,  2.4545e+00,  2.4847e+00,  1.9109e-01,
          6.4641e-01,  2.1924e+00,  1.6503e-01,  5.2918e-01, -8.0863e+00,
          7.7861e-01,  2.2829e+00,  6.8163e-01]], device='cuda:0'))])
xi:  [860.8614]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1051.122700569297
W_T_median: 1005.2766092210193
W_T_pctile_5: 861.1825872348179
W_T_CVAR_5_pct: 513.2041114654016
Average q (qsum/M+1):  47.57047001008065
Optimal xi:  [860.8614]
Expected(across Rb) median(across samples) p_equity:  0.24962407955899835
obj fun:  tensor(-4040.5995, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.2 0.2 0.2 0.2 0.2]
W_T_mean: 1138.474873568312
W_T_median: 891.4398755591162
W_T_pctile_5: -169.98079142996517
W_T_CVAR_5_pct: -317.5137579687884
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -6693.3320010596835
Current xi:  [123.781075]
objective value function right now is: -6693.3320010596835
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -7682.621882711945
Current xi:  [147.85707]
objective value function right now is: -7682.621882711945
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -8892.6683143143
Current xi:  [171.51741]
objective value function right now is: -8892.6683143143
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -9945.870429909997
Current xi:  [194.94334]
objective value function right now is: -9945.870429909997
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -10761.683973697447
Current xi:  [218.43202]
objective value function right now is: -10761.683973697447
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -11886.710343845196
Current xi:  [241.56532]
objective value function right now is: -11886.710343845196
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -12801.110973049894
Current xi:  [264.59985]
objective value function right now is: -12801.110973049894
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -13686.603394393409
Current xi:  [287.6321]
objective value function right now is: -13686.603394393409
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -14578.967486237907
Current xi:  [310.50482]
objective value function right now is: -14578.967486237907
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -15363.290058956587
Current xi:  [333.47638]
objective value function right now is: -15363.290058956587
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -15991.036252347123
Current xi:  [355.88605]
objective value function right now is: -15991.036252347123
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -16973.586551206037
Current xi:  [378.71005]
objective value function right now is: -16973.586551206037
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -17695.310924229794
Current xi:  [401.12375]
objective value function right now is: -17695.310924229794
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -18449.351805333143
Current xi:  [423.70554]
objective value function right now is: -18449.351805333143
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -19126.297748127156
Current xi:  [445.83417]
objective value function right now is: -19126.297748127156
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -19869.198615269666
Current xi:  [467.97736]
objective value function right now is: -19869.198615269666
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -20542.701005794846
Current xi:  [490.2004]
objective value function right now is: -20542.701005794846
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -20936.69180363273
Current xi:  [511.73737]
objective value function right now is: -20936.69180363273
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -21610.685655301066
Current xi:  [533.192]
objective value function right now is: -21610.685655301066
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -22194.776292148024
Current xi:  [554.709]
objective value function right now is: -22194.776292148024
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -22735.22380383384
Current xi:  [576.1327]
objective value function right now is: -22735.22380383384
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -23102.479010011622
Current xi:  [597.1214]
objective value function right now is: -23102.479010011622
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -23635.456997762183
Current xi:  [618.21136]
objective value function right now is: -23635.456997762183
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -24015.893827467582
Current xi:  [638.8818]
objective value function right now is: -24015.893827467582
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -24455.913523987332
Current xi:  [659.0912]
objective value function right now is: -24455.913523987332
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -24836.56178044714
Current xi:  [679.5228]
objective value function right now is: -24836.56178044714
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -25148.684919059717
Current xi:  [699.44214]
objective value function right now is: -25148.684919059717
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -25364.064226372342
Current xi:  [718.3495]
objective value function right now is: -25364.064226372342
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -25664.95862377695
Current xi:  [736.58716]
objective value function right now is: -25664.95862377695
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -25731.39226993622
Current xi:  [754.8887]
objective value function right now is: -25731.39226993622
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -26001.94458118187
Current xi:  [772.0291]
objective value function right now is: -26001.94458118187
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -26037.826703134866
Current xi:  [788.82513]
objective value function right now is: -26037.826703134866
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -26235.92398065138
Current xi:  [804.38306]
objective value function right now is: -26235.92398065138
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -26384.326577796714
Current xi:  [818.6972]
objective value function right now is: -26384.326577796714
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -26455.23258867258
Current xi:  [832.09595]
objective value function right now is: -26455.23258867258
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -26678.10890849977
Current xi:  [834.68286]
objective value function right now is: -26678.10890849977
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -26701.161641497903
Current xi:  [837.595]
objective value function right now is: -26701.161641497903
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [840.6446]
objective value function right now is: -26681.09158202056
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -26751.68525140497
Current xi:  [843.78015]
objective value function right now is: -26751.68525140497
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [846.75714]
objective value function right now is: -26749.75461999545
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -26777.96745726543
Current xi:  [849.6527]
objective value function right now is: -26777.96745726543
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -26789.769771632007
Current xi:  [852.67804]
objective value function right now is: -26789.769771632007
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -26789.86464620966
Current xi:  [855.5866]
objective value function right now is: -26789.86464620966
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -26846.532905590753
Current xi:  [858.2998]
objective value function right now is: -26846.532905590753
new min fval from sgd:  -26874.14783298793
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [860.8948]
objective value function right now is: -26874.14783298793
new min fval from sgd:  -26874.38048164823
new min fval from sgd:  -26874.632898440002
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [863.4323]
objective value function right now is: -26803.802896040343
new min fval from sgd:  -26874.70696809843
new min fval from sgd:  -26875.108321045605
new min fval from sgd:  -26875.33989912886
new min fval from sgd:  -26875.453383967928
new min fval from sgd:  -26875.751702851612
new min fval from sgd:  -26877.967815481694
new min fval from sgd:  -26878.890246076844
new min fval from sgd:  -26879.22649593556
new min fval from sgd:  -26879.856443623306
new min fval from sgd:  -26880.63617169354
new min fval from sgd:  -26881.394098249573
new min fval from sgd:  -26881.73406167886
new min fval from sgd:  -26882.143420940927
new min fval from sgd:  -26883.888273641714
new min fval from sgd:  -26885.41656112605
new min fval from sgd:  -26886.370080704484
new min fval from sgd:  -26886.72831546785
new min fval from sgd:  -26889.463548792624
new min fval from sgd:  -26891.222101939285
new min fval from sgd:  -26891.42625201715
new min fval from sgd:  -26891.847972778996
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [865.6492]
objective value function right now is: -26874.424909402445
new min fval from sgd:  -26892.00360390518
new min fval from sgd:  -26894.026143976072
new min fval from sgd:  -26894.466932716176
new min fval from sgd:  -26896.299363592745
new min fval from sgd:  -26899.1669902326
new min fval from sgd:  -26901.09415101047
new min fval from sgd:  -26901.779234411235
new min fval from sgd:  -26902.23069656825
new min fval from sgd:  -26902.576902466855
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [868.27405]
objective value function right now is: -26851.6030163141
new min fval from sgd:  -26903.236614269546
new min fval from sgd:  -26904.143927999594
new min fval from sgd:  -26904.437496440845
new min fval from sgd:  -26905.90161886362
new min fval from sgd:  -26906.253691858477
new min fval from sgd:  -26906.638220939753
new min fval from sgd:  -26906.925151303116
new min fval from sgd:  -26907.232901117066
new min fval from sgd:  -26907.298144617478
new min fval from sgd:  -26907.451528181984
new min fval from sgd:  -26907.627937232763
new min fval from sgd:  -26908.028274146258
new min fval from sgd:  -26908.427466417386
new min fval from sgd:  -26908.82019885391
new min fval from sgd:  -26909.179579368643
new min fval from sgd:  -26909.509800327512
new min fval from sgd:  -26909.807638016835
new min fval from sgd:  -26909.973181751437
new min fval from sgd:  -26910.269294378308
new min fval from sgd:  -26910.511923152655
new min fval from sgd:  -26910.675009829185
new min fval from sgd:  -26910.845777685412
new min fval from sgd:  -26911.01376440199
new min fval from sgd:  -26911.19442625574
new min fval from sgd:  -26911.401549005273
new min fval from sgd:  -26911.60272714763
new min fval from sgd:  -26911.895463395063
new min fval from sgd:  -26912.20856693293
new min fval from sgd:  -26912.32163543416
new min fval from sgd:  -26912.496084787275
new min fval from sgd:  -26912.70767416203
new min fval from sgd:  -26912.91020301677
new min fval from sgd:  -26913.14301976836
new min fval from sgd:  -26913.32282250406
new min fval from sgd:  -26913.499613642423
new min fval from sgd:  -26913.75717302601
new min fval from sgd:  -26914.02291906224
new min fval from sgd:  -26914.23678012607
new min fval from sgd:  -26914.440442361072
new min fval from sgd:  -26914.54059814315
new min fval from sgd:  -26914.55704696738
new min fval from sgd:  -26914.55833675236
new min fval from sgd:  -26914.738113903455
new min fval from sgd:  -26914.80286559951
new min fval from sgd:  -26914.83681313896
new min fval from sgd:  -26914.88321887157
new min fval from sgd:  -26914.90328252215
new min fval from sgd:  -26915.090044885637
new min fval from sgd:  -26915.156339850433
new min fval from sgd:  -26915.200628485054
new min fval from sgd:  -26915.607347162033
new min fval from sgd:  -26916.409144158522
new min fval from sgd:  -26916.99363053924
new min fval from sgd:  -26917.36093843576
new min fval from sgd:  -26917.60700465305
new min fval from sgd:  -26917.701990131154
new min fval from sgd:  -26917.77199098563
new min fval from sgd:  -26917.781742720486
new min fval from sgd:  -26917.8850020677
new min fval from sgd:  -26917.9782597581
new min fval from sgd:  -26917.996231735066
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [869.7336]
objective value function right now is: -26916.23410876295
new min fval from sgd:  -26918.233663886822
new min fval from sgd:  -26918.303098023072
new min fval from sgd:  -26918.467631982374
new min fval from sgd:  -26918.547358009488
new min fval from sgd:  -26918.613350946078
new min fval from sgd:  -26918.765572479813
new min fval from sgd:  -26918.903641589397
new min fval from sgd:  -26919.063516358296
new min fval from sgd:  -26919.244912244994
new min fval from sgd:  -26919.408497206354
new min fval from sgd:  -26919.456624784965
new min fval from sgd:  -26919.595098718914
new min fval from sgd:  -26919.63330978768
new min fval from sgd:  -26919.63894106962
new min fval from sgd:  -26919.842618391038
new min fval from sgd:  -26920.07375383474
new min fval from sgd:  -26920.290789231076
new min fval from sgd:  -26920.464300063377
new min fval from sgd:  -26920.639579829338
new min fval from sgd:  -26920.805562740774
new min fval from sgd:  -26920.948205485845
new min fval from sgd:  -26920.97076786458
new min fval from sgd:  -26920.97120157127
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [870.21985]
objective value function right now is: -26916.69515598126
min fval:  -26920.97120157127
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638],
        [ 0.1325, -0.1638]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([0.2313, 0.2313, 0.2313, 0.2313, 0.2313, 0.2313, 0.2313, 0.2313, 0.2313,
        0.2313, 0.2313, 0.2313, 0.2313], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130],
        [0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130, 0.2130,
         0.2130, 0.2130, 0.2130, 0.2130]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([0.3517, 0.3517, 0.3517, 0.3517, 0.3517, 0.3517, 0.3517, 0.3517, 0.3517,
        0.3517, 0.3517, 0.3517, 0.3517], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.7196, -1.7196, -1.7196, -1.7196, -1.7196, -1.7196, -1.7196, -1.7196,
         -1.7196, -1.7196, -1.7196, -1.7196, -1.7196]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-10.7834,  -3.3367],
        [ -9.5023,  -2.6927],
        [ -4.0806,   4.8939],
        [  6.4023,   0.5775],
        [-13.1594,   5.5594],
        [-15.2797,   6.7979],
        [-20.9911,   8.9603],
        [ -5.8247,   4.0560],
        [ -2.8822,   0.6886],
        [-11.2051,  -3.4935],
        [-17.1253,   7.2599],
        [ -9.2238,   6.8781],
        [  7.1920,   6.5082]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-0.6867,  1.0557, -7.9647, -7.0451,  4.1105, -0.0603,  2.8838,  5.2928,
        -7.5512, -0.7086, -2.6104, -0.4280, -1.8994], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-6.1840e+00, -4.0300e+00,  6.7230e-01, -8.7433e+00,  4.9619e+00,
          3.5240e+00,  1.3665e+01,  4.3033e+00,  1.9278e-01, -8.5220e+00,
          5.5900e+00,  5.8267e-01,  1.4234e+00],
        [ 8.9565e+00,  8.4084e+00, -2.1482e-01, -3.8297e+00, -1.1307e+01,
         -9.0172e+00, -7.9172e+00, -3.9407e+00,  2.7186e-01,  1.1643e+01,
         -7.0817e+00, -1.5448e+01, -1.4800e+00],
        [-5.0391e+00, -5.9890e+00, -9.0398e-03, -8.1836e+00, -4.2707e+00,
         -1.9289e-01, -5.0504e-01,  3.1921e+00,  4.2272e-02, -5.4949e+00,
         -2.1002e-02, -1.3154e+00, -1.1644e+01],
        [ 1.1020e+01,  8.0680e+00, -3.2168e-01, -2.3057e+01, -6.1697e+00,
         -8.8288e+00, -4.5553e+00, -9.6446e+00, -1.3967e-01,  1.3095e+01,
         -7.9016e+00, -1.4550e+01, -9.8644e+00],
        [-1.8250e-01, -8.3369e-01, -9.1884e-02, -7.8173e-01, -2.0327e-01,
         -1.0853e-01, -1.5925e-01, -1.5944e+00, -3.0435e-03, -1.6182e-01,
         -9.3304e-02, -6.6412e-02, -9.1861e-01],
        [-2.0194e-01, -8.9846e-01, -1.1953e-01, -6.0599e-01, -2.4169e-01,
         -1.3953e-01, -2.0331e-01, -1.9067e+00, -7.2574e-04, -1.7970e-01,
         -1.2194e-01, -8.3862e-02, -1.0632e+00],
        [-2.3542e+01, -2.5981e+01, -2.0617e-02,  4.9505e+00, -1.1755e-01,
         -5.3358e-03, -4.1763e-04, -3.4001e+01, -2.1342e-01, -2.2267e+01,
         -1.1207e-04, -3.9470e-02,  1.6500e+00],
        [-2.3557e+01, -2.7808e+01,  3.0241e+00,  4.1482e+00,  1.0980e+00,
          1.3570e+00,  1.8858e-01, -3.8561e+00, -5.0918e-01, -2.1496e+01,
          2.9611e-01,  2.0051e+00,  1.9709e+00],
        [-6.1498e+00, -6.0934e+00,  1.0393e-01,  1.6573e+00,  1.7891e+00,
         -1.0556e+00, -2.0269e+00,  5.8258e+00,  7.6056e-02, -6.9260e+00,
         -1.6583e+00, -2.2214e+00, -3.4700e+00],
        [-6.4960e+00, -1.8472e+01, -2.7811e-04,  8.0951e-01, -1.0104e-02,
          1.4101e-05, -2.2582e-05, -2.0124e+01, -2.1728e-02, -5.3192e+00,
         -2.5020e-05, -1.2738e-03, -1.2177e+00],
        [-1.8772e-01, -8.4996e-01, -9.5735e-02, -7.7303e-01, -2.2901e-01,
         -1.1504e-01, -1.7268e-01, -1.6074e+00, -3.0761e-03, -1.6674e-01,
         -9.7120e-02, -6.9475e-02, -8.9016e-01],
        [-2.1054e-01, -9.2220e-01, -1.1061e-01, -7.3823e-01, -3.4380e-01,
         -1.4310e-01, -2.3385e-01, -1.5912e+00, -3.1803e-03, -1.8858e-01,
         -1.1182e-01, -8.4845e-02, -7.5565e-01],
        [-1.8376e+00, -9.4792e-01, -3.2928e+00,  1.6705e+00, -1.2567e+00,
         -4.0231e+00, -2.8440e+00,  3.3299e+00,  1.3279e-01, -2.0275e+00,
         -1.6163e+00, -3.6771e+00,  2.3123e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-5.6791, -0.8108, -6.8804, -4.6567, -9.8524, -9.8244, -3.7420, -2.2029,
        -6.8923, -7.1539, -9.8569, -9.8885, -9.0150], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 3.8194e+00, -3.9052e+01, -1.2230e+01, -4.1683e+01,  3.1235e-01,
          3.7172e-01, -1.0024e+01, -2.4706e+00, -8.2299e+00, -2.7773e+00,
          2.8753e-01,  1.7390e-01,  3.8215e+00],
        [ 4.0737e-01,  1.2821e+00,  7.8172e-01, -5.7906e+00,  4.0975e-01,
          5.0652e-01,  5.2298e+00,  1.9991e+00, -1.0615e+00,  2.2931e+00,
          4.6926e-01,  8.3136e-01, -4.9475e+00],
        [-9.6272e+01, -8.8708e+01, -1.3613e-01, -3.8722e-02, -1.9520e-02,
         -1.5910e-02, -4.2049e-04, -4.8650e+01, -1.1766e+01, -4.2487e-04,
         -1.9503e-02, -2.0168e-02, -6.6272e+00],
        [-8.8676e+01, -8.7348e+01, -9.1593e-02, -3.2694e-02, -1.4067e-02,
         -9.9616e-03,  1.2445e-04, -4.9205e+01, -1.0223e+01, -2.7381e-04,
         -1.4045e-02, -1.4736e-02, -5.1341e+00],
        [ 5.8239e-01,  2.4476e+00,  3.2148e+00,  8.5433e+00,  6.5953e-01,
          9.9856e-01, -2.1942e+00,  1.9454e+00,  4.1462e+00,  1.0315e+00,
          7.2836e-01,  1.1040e+00,  2.9719e+00]], device='cuda:0'))])
xi:  [869.86163]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1207.8723535703464
W_T_median: 1134.1291383822695
W_T_pctile_5: 876.874793310531
W_T_CVAR_5_pct: 516.9969216514892
Average q (qsum/M+1):  35.0
Optimal xi:  [869.86163]
Expected(across Rb) median(across samples) p_equity:  0.18202188089489937
obj fun:  tensor(-26920.9712, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
