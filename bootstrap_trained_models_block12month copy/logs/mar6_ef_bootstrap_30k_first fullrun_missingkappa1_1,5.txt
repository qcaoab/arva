Starting at: 
06-03-23_14:57

 numpy seed:  3  


 pytorch seed:  3  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
               CPI  CPI_nom_ret  ...  VWD_real_ret  EWD_real_ret
date                             ...                            
192512  100.000000          NaN  ...           NaN           NaN
192601  100.000000     0.000000  ...      0.000561      0.023174
192602  100.000000     0.000000  ...     -0.033046     -0.053510
192603   99.441303    -0.005587  ...     -0.058743     -0.091750
192604  100.000000     0.005618  ...      0.031235      0.027204

[5 rows x 12 columns]
                CPI  CPI_nom_ret  ...  VWD_real_ret  EWD_real_ret
date                              ...                            
202008  1452.055956     0.003153  ...      0.065084      0.035618
202009  1454.078149     0.001393  ...     -0.036399     -0.028709
202010  1454.681696     0.000415  ...     -0.020584      0.000169
202011  1453.793232    -0.000611  ...      0.124393      0.175130
202012  1455.162018     0.000942  ...      0.044065      0.071843

[5 rows x 12 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_nom_ret', 'VWD_nom_ret']
############# End: defined asset  basket #################
        CPI_nom_ret  B10_nom_ret  VWD_nom_ret
date                                         
192601     0.000000     0.004350     0.000561
192602     0.000000     0.004338    -0.033046
192603    -0.005587     0.004327    -0.064002
192604     0.005618     0.004316     0.037029
192605    -0.005587     0.004304     0.012095
        CPI_nom_ret  B10_nom_ret  VWD_nom_ret
date                                         
202008     0.003153    -0.014709     0.068443
202009     0.001393     0.002560    -0.035057
202010     0.000415    -0.017311    -0.020178
202011    -0.000611     0.004016     0.123706
202012     0.000942    -0.004965     0.045048
Indices constructed with column names:
Index(['date_for_plt', 'CPI_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
B10_real_ret    0.001986
VWD_real_ret    0.006974
dtype: float64


timeseries_basket['data_df_stdev'] = 
B10_real_ret    0.018939
VWD_real_ret    0.053569
dtype: float64


timeseries_basket['data_df_corr'] = 
              B10_real_ret  VWD_real_ret
B10_real_ret      1.000000      0.075353
VWD_real_ret      0.075353      1.000000


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192601
End: 202012
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1518.318040318632
W_T_median: 1157.8841391994329
W_T_pctile_5: -114.48374804507138
W_T_CVAR_5_pct: -279.1514474953861
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1697.017608888961
Current xi:  [80.339096]
objective value function right now is: -1697.017608888961
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1704.1407590889955
Current xi:  [59.007587]
objective value function right now is: -1704.1407590889955
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1711.175269847435
Current xi:  [36.70709]
objective value function right now is: -1711.175269847435
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1714.912576884184
Current xi:  [14.0474415]
objective value function right now is: -1714.912576884184
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1719.6997260912049
Current xi:  [-5.9922214]
objective value function right now is: -1719.6997260912049
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1722.6077175740318
Current xi:  [-26.467245]
objective value function right now is: -1722.6077175740318
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1725.074149319436
Current xi:  [-45.75223]
objective value function right now is: -1725.074149319436
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1727.6778132021652
Current xi:  [-66.797935]
objective value function right now is: -1727.6778132021652
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1729.7216801597783
Current xi:  [-85.84215]
objective value function right now is: -1729.7216801597783
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1732.1098570618421
Current xi:  [-106.65884]
objective value function right now is: -1732.1098570618421
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-125.87067]
objective value function right now is: -1726.668451890045
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1735.974374913558
Current xi:  [-146.3688]
objective value function right now is: -1735.974374913558
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1737.0455405837176
Current xi:  [-166.46574]
objective value function right now is: -1737.0455405837176
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1738.8801416249612
Current xi:  [-186.24532]
objective value function right now is: -1738.8801416249612
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1740.1893374400959
Current xi:  [-206.08397]
objective value function right now is: -1740.1893374400959
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1740.3017799724012
Current xi:  [-225.36684]
objective value function right now is: -1740.3017799724012
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1742.2149054832278
Current xi:  [-244.82603]
objective value function right now is: -1742.2149054832278
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1743.4780850772931
Current xi:  [-263.6233]
objective value function right now is: -1743.4780850772931
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1744.67006833065
Current xi:  [-282.30402]
objective value function right now is: -1744.67006833065
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1745.2895466186399
Current xi:  [-300.50433]
objective value function right now is: -1745.2895466186399
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1745.5037356414523
Current xi:  [-318.1571]
objective value function right now is: -1745.5037356414523
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1745.9496709273972
Current xi:  [-335.91592]
objective value function right now is: -1745.9496709273972
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1746.8002833092567
Current xi:  [-352.58493]
objective value function right now is: -1746.8002833092567
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1747.0358705005106
Current xi:  [-369.09653]
objective value function right now is: -1747.0358705005106
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1747.4253427177207
Current xi:  [-384.07513]
objective value function right now is: -1747.4253427177207
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1747.631705058009
Current xi:  [-397.14185]
objective value function right now is: -1747.631705058009
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1747.7349482557606
Current xi:  [-410.11575]
objective value function right now is: -1747.7349482557606
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-422.40982]
objective value function right now is: -1747.5532650086607
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-433.88242]
objective value function right now is: -1747.0897837390326
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-439.63632]
objective value function right now is: -1747.4570755791403
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1747.7743829253254
Current xi:  [-445.7059]
objective value function right now is: -1747.7743829253254
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-448.5983]
objective value function right now is: -1747.757128538301
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-452.54904]
objective value function right now is: -1747.262291890598
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-454.1759]
objective value function right now is: -1747.4931982137778
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-457.17874]
objective value function right now is: -1747.3538520950483
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1748.2634608309008
Current xi:  [-457.4853]
objective value function right now is: -1748.2634608309008
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1748.337108507995
Current xi:  [-457.9412]
objective value function right now is: -1748.337108507995
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1748.3807865350898
Current xi:  [-458.53574]
objective value function right now is: -1748.3807865350898
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1748.388864590491
Current xi:  [-459.00253]
objective value function right now is: -1748.388864590491
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-459.72354]
objective value function right now is: -1748.3703402191081
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-459.66046]
objective value function right now is: -1748.3538730876148
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1748.3913708178513
Current xi:  [-459.9355]
objective value function right now is: -1748.3913708178513
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-459.96948]
objective value function right now is: -1748.2925927351364
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1748.4305351957846
Current xi:  [-460.15305]
objective value function right now is: -1748.4305351957846
new min fval from sgd:  -1748.435260461189
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-460.2004]
objective value function right now is: -1748.435260461189
new min fval from sgd:  -1748.4358647722927
new min fval from sgd:  -1748.4425159919563
new min fval from sgd:  -1748.4466817526018
new min fval from sgd:  -1748.4542044423456
new min fval from sgd:  -1748.4726541309988
new min fval from sgd:  -1748.4888630753978
new min fval from sgd:  -1748.4961782090481
new min fval from sgd:  -1748.4975712744922
new min fval from sgd:  -1748.5021698309743
new min fval from sgd:  -1748.5079046901585
new min fval from sgd:  -1748.5123128722523
new min fval from sgd:  -1748.514362346178
new min fval from sgd:  -1748.5151206783887
new min fval from sgd:  -1748.5156911941783
new min fval from sgd:  -1748.5174872137252
new min fval from sgd:  -1748.523013831799
new min fval from sgd:  -1748.5258775907025
new min fval from sgd:  -1748.5268253720105
new min fval from sgd:  -1748.5298093811828
new min fval from sgd:  -1748.5327991545485
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-460.12024]
objective value function right now is: -1748.4242778522691
new min fval from sgd:  -1748.533315873516
new min fval from sgd:  -1748.5356255835313
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-459.7204]
objective value function right now is: -1748.5088236541255
new min fval from sgd:  -1748.5382573481102
new min fval from sgd:  -1748.5391550551853
new min fval from sgd:  -1748.541734904515
new min fval from sgd:  -1748.5419326945741
new min fval from sgd:  -1748.5453636750724
new min fval from sgd:  -1748.5554399339353
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-459.90634]
objective value function right now is: -1748.3834904615587
new min fval from sgd:  -1748.562312030268
new min fval from sgd:  -1748.5625249166721
new min fval from sgd:  -1748.563471059586
new min fval from sgd:  -1748.5643235943517
new min fval from sgd:  -1748.5649881193258
new min fval from sgd:  -1748.5670655217255
new min fval from sgd:  -1748.5699321954967
new min fval from sgd:  -1748.5712513905046
new min fval from sgd:  -1748.5725898846508
new min fval from sgd:  -1748.5736325566388
new min fval from sgd:  -1748.5750876990485
new min fval from sgd:  -1748.5764084767213
new min fval from sgd:  -1748.5766408239149
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-459.9539]
objective value function right now is: -1748.5430511992977
new min fval from sgd:  -1748.5767934200974
new min fval from sgd:  -1748.577196920623
new min fval from sgd:  -1748.5772184144269
new min fval from sgd:  -1748.577533884049
new min fval from sgd:  -1748.577839342836
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-460.01425]
objective value function right now is: -1748.5714214552352
min fval:  -1748.577839342836
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.3071,  1.2394],
        [-0.3071,  1.2394],
        [-4.8982,  5.6927],
        [13.1485,  1.0610],
        [-0.3071,  1.2394],
        [-0.3071,  1.2394],
        [-0.3067,  1.2458],
        [-0.3071,  1.2394],
        [-2.9520,  5.7725],
        [-0.3084,  1.2378]])), ('0.model.hidden_layer_1.bias', tensor([-0.6547, -0.6547, 10.6766, -8.5432, -0.6547, -0.6547, -0.6562, -0.6547,
        10.8639, -0.6566])), ('0.model.hidden_layer_2.weight', tensor([[-1.0928e-03, -1.0926e-03, -4.8933e-02, -2.3496e-01, -1.0929e-03,
         -1.0929e-03, -9.3688e-04, -1.0929e-03, -1.4032e-01, -1.1064e-03],
        [-1.0928e-03, -1.0926e-03, -4.8933e-02, -2.3496e-01, -1.0929e-03,
         -1.0929e-03, -9.3688e-04, -1.0929e-03, -1.4032e-01, -1.1064e-03],
        [ 1.6557e-01,  1.6550e-01, -7.7911e+00, -9.0572e+00,  1.6566e-01,
          1.6566e-01,  1.1629e-01,  1.6566e-01, -7.2562e+00,  1.8047e-01],
        [ 1.0254e-01,  1.0249e-01,  3.1937e+00,  4.3867e+00,  1.0257e-01,
          1.0257e-01,  6.8739e-02,  1.0256e-01,  3.1074e+00,  1.1285e-01],
        [-1.0928e-03, -1.0926e-03, -4.8933e-02, -2.3496e-01, -1.0929e-03,
         -1.0929e-03, -9.3689e-04, -1.0929e-03, -1.4032e-01, -1.1064e-03],
        [-1.0928e-03, -1.0926e-03, -4.8933e-02, -2.3496e-01, -1.0929e-03,
         -1.0929e-03, -9.3688e-04, -1.0929e-03, -1.4032e-01, -1.1064e-03],
        [ 8.5369e-02,  8.5306e-02,  3.7364e+00,  4.8170e+00,  8.5494e-02,
          8.5494e-02,  4.2657e-02,  8.5491e-02,  3.5833e+00,  9.8042e-02],
        [ 6.0243e-02,  6.0169e-02,  4.3384e+00,  5.3789e+00,  6.0308e-02,
          6.0307e-02,  9.9348e-03,  6.0303e-02,  4.1421e+00,  7.5093e-02],
        [-1.0928e-03, -1.0926e-03, -4.8933e-02, -2.3496e-01, -1.0929e-03,
         -1.0929e-03, -9.3688e-04, -1.0929e-03, -1.4032e-01, -1.1064e-03],
        [-1.0929e-03, -1.0927e-03, -4.8934e-02, -2.3496e-01, -1.0930e-03,
         -1.0930e-03, -9.3694e-04, -1.0930e-03, -1.4033e-01, -1.1064e-03]])), ('0.model.hidden_layer_2.bias', tensor([-0.7904, -0.7904,  6.1571, -3.1699, -0.7904, -0.7904, -3.3933, -3.7342,
        -0.7904, -0.7904])), ('0.model.output_layer_3.weight', tensor([[ -0.0265,  -0.0265, -14.8421,   4.3500,  -0.0265,  -0.0265,   5.1253,
           6.1710,  -0.0265,  -0.0265]])), ('1.model.hidden_layer_1.weight', tensor([[ 16.8635,   5.9910],
        [ -8.6801,   1.1813],
        [ -1.0344,  -8.8765],
        [-10.9241,   0.3336],
        [  3.3680,  -2.9547],
        [ -6.1388,   6.7634],
        [ -5.5954,  -0.5844],
        [  1.8254,   1.6754],
        [ -9.0611,   1.7643],
        [  9.9938,   3.1102]])), ('1.model.hidden_layer_1.bias', tensor([  3.6324,   1.7179,  -8.8058,  11.4021, -10.5574,   6.9868,  -0.3572,
         -6.0403,   4.1376,  -1.1949])), ('1.model.hidden_layer_2.weight', tensor([[ 7.8125e-01, -7.5402e-01, -4.7739e+00, -6.8207e+00, -2.0944e+00,
          7.1490e-01, -8.4823e-01,  1.3692e+00, -2.1261e+00,  1.0101e+00],
        [-4.8970e+00,  3.0076e-01, -4.2365e+00,  3.7364e+00, -1.5102e+00,
         -4.7758e-01,  1.9767e+00, -6.5183e-01,  1.7999e+00, -1.8628e+00],
        [-3.1516e+00,  4.2175e+00, -7.4128e+00, -6.6254e-01, -6.8155e-03,
          5.5622e+00, -5.4769e+00, -4.5604e+00,  6.6223e+00, -6.4359e-01],
        [-6.4965e+00, -1.8330e+00,  7.4624e+00,  7.0827e+00, -8.2248e-01,
         -1.3719e+01,  2.3094e+00, -4.8966e-02, -1.1100e+00, -7.2514e+00],
        [-6.6531e+00, -1.6035e+00,  6.9976e+00, -2.9508e+00, -3.6311e+00,
         -2.1896e+00,  4.9774e+00,  3.5000e-02, -3.4842e+00,  4.6453e-01],
        [-9.3171e+00, -2.7030e+00,  4.1540e+00,  5.0890e+00, -1.0082e+01,
         -3.1632e+00,  3.2229e-01,  1.4792e-02, -4.8349e+00, -3.5422e+00],
        [-2.7349e+00, -2.6226e+00,  2.1398e+00,  5.6341e+00, -2.9992e+00,
         -1.0175e+01,  2.6172e+00,  1.1767e-01, -2.9595e+00, -4.2366e+00],
        [-1.0014e+00, -1.4151e-02, -1.0662e+00, -8.0230e-01, -8.7620e-01,
         -4.3828e-01, -8.7010e-01, -7.4700e-02, -8.7208e-02, -9.0431e-01],
        [-1.0014e+00, -1.4151e-02, -1.0662e+00, -8.0230e-01, -8.7620e-01,
         -4.3828e-01, -8.7010e-01, -7.4700e-02, -8.7207e-02, -9.0431e-01],
        [-4.5450e+00,  1.4865e+00, -3.3089e+00,  8.7777e+00, -1.6116e+00,
          3.0105e+00,  4.3768e-01, -2.2472e+00,  5.5856e+00, -2.6165e+00]])), ('1.model.hidden_layer_2.bias', tensor([-0.0882, -2.3083, -5.8194, -2.5287, -1.2284, -2.8469,  0.8334, -1.9254,
        -1.9254, -3.5360])), ('1.model.output_layer_3.weight', tensor([[-2.8239,  2.3244, -4.5291, -5.2436,  5.8113,  9.2186, -0.6085, -0.0216,
         -0.0216,  0.4127],
        [ 2.7442, -2.3072,  4.5360,  5.2387, -5.8473, -9.2137,  0.8632,  0.0216,
          0.0217, -0.5559]]))])
xi:  [-459.98447]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1228.5306629329275
W_T_median: 143.93707733774966
W_T_pctile_5: -461.19673987099645
W_T_CVAR_5_pct: -570.8722274655776
Average q (qsum/M+1):  57.286880985383064
Optimal xi:  [-459.98447]
Observed VAR:  143.93707733774966
Expected(across Rb) median(across samples) p_equity:  0.6175125350554784
obj fun:  tensor(-1748.5778, dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.3071,  1.2394],
        [-0.3071,  1.2394],
        [-4.8982,  5.6927],
        [13.1485,  1.0610],
        [-0.3071,  1.2394],
        [-0.3071,  1.2394],
        [-0.3067,  1.2458],
        [-0.3071,  1.2394],
        [-2.9520,  5.7725],
        [-0.3084,  1.2378]])), ('0.model.hidden_layer_1.bias', tensor([-0.6547, -0.6547, 10.6766, -8.5432, -0.6547, -0.6547, -0.6562, -0.6547,
        10.8639, -0.6566])), ('0.model.hidden_layer_2.weight', tensor([[-1.0928e-03, -1.0926e-03, -4.8933e-02, -2.3496e-01, -1.0929e-03,
         -1.0929e-03, -9.3688e-04, -1.0929e-03, -1.4032e-01, -1.1064e-03],
        [-1.0928e-03, -1.0926e-03, -4.8933e-02, -2.3496e-01, -1.0929e-03,
         -1.0929e-03, -9.3688e-04, -1.0929e-03, -1.4032e-01, -1.1064e-03],
        [ 1.6557e-01,  1.6550e-01, -7.7911e+00, -9.0572e+00,  1.6566e-01,
          1.6566e-01,  1.1629e-01,  1.6566e-01, -7.2562e+00,  1.8047e-01],
        [ 1.0254e-01,  1.0249e-01,  3.1937e+00,  4.3867e+00,  1.0257e-01,
          1.0257e-01,  6.8739e-02,  1.0256e-01,  3.1074e+00,  1.1285e-01],
        [-1.0928e-03, -1.0926e-03, -4.8933e-02, -2.3496e-01, -1.0929e-03,
         -1.0929e-03, -9.3689e-04, -1.0929e-03, -1.4032e-01, -1.1064e-03],
        [-1.0928e-03, -1.0926e-03, -4.8933e-02, -2.3496e-01, -1.0929e-03,
         -1.0929e-03, -9.3688e-04, -1.0929e-03, -1.4032e-01, -1.1064e-03],
        [ 8.5369e-02,  8.5306e-02,  3.7364e+00,  4.8170e+00,  8.5494e-02,
          8.5494e-02,  4.2657e-02,  8.5491e-02,  3.5833e+00,  9.8042e-02],
        [ 6.0243e-02,  6.0169e-02,  4.3384e+00,  5.3789e+00,  6.0308e-02,
          6.0307e-02,  9.9348e-03,  6.0303e-02,  4.1421e+00,  7.5093e-02],
        [-1.0928e-03, -1.0926e-03, -4.8933e-02, -2.3496e-01, -1.0929e-03,
         -1.0929e-03, -9.3688e-04, -1.0929e-03, -1.4032e-01, -1.1064e-03],
        [-1.0929e-03, -1.0927e-03, -4.8934e-02, -2.3496e-01, -1.0930e-03,
         -1.0930e-03, -9.3694e-04, -1.0930e-03, -1.4033e-01, -1.1064e-03]])), ('0.model.hidden_layer_2.bias', tensor([-0.7904, -0.7904,  6.1571, -3.1699, -0.7904, -0.7904, -3.3933, -3.7342,
        -0.7904, -0.7904])), ('0.model.output_layer_3.weight', tensor([[ -0.0265,  -0.0265, -14.8421,   4.3500,  -0.0265,  -0.0265,   5.1253,
           6.1710,  -0.0265,  -0.0265]])), ('1.model.hidden_layer_1.weight', tensor([[ 16.8635,   5.9910],
        [ -8.6801,   1.1813],
        [ -1.0344,  -8.8765],
        [-10.9241,   0.3336],
        [  3.3680,  -2.9547],
        [ -6.1388,   6.7634],
        [ -5.5954,  -0.5844],
        [  1.8254,   1.6754],
        [ -9.0611,   1.7643],
        [  9.9938,   3.1102]])), ('1.model.hidden_layer_1.bias', tensor([  3.6324,   1.7179,  -8.8058,  11.4021, -10.5574,   6.9868,  -0.3572,
         -6.0403,   4.1376,  -1.1949])), ('1.model.hidden_layer_2.weight', tensor([[ 7.8125e-01, -7.5402e-01, -4.7739e+00, -6.8207e+00, -2.0944e+00,
          7.1490e-01, -8.4823e-01,  1.3692e+00, -2.1261e+00,  1.0101e+00],
        [-4.8970e+00,  3.0076e-01, -4.2365e+00,  3.7364e+00, -1.5102e+00,
         -4.7758e-01,  1.9767e+00, -6.5183e-01,  1.7999e+00, -1.8628e+00],
        [-3.1516e+00,  4.2175e+00, -7.4128e+00, -6.6254e-01, -6.8155e-03,
          5.5622e+00, -5.4769e+00, -4.5604e+00,  6.6223e+00, -6.4359e-01],
        [-6.4965e+00, -1.8330e+00,  7.4624e+00,  7.0827e+00, -8.2248e-01,
         -1.3719e+01,  2.3094e+00, -4.8966e-02, -1.1100e+00, -7.2514e+00],
        [-6.6531e+00, -1.6035e+00,  6.9976e+00, -2.9508e+00, -3.6311e+00,
         -2.1896e+00,  4.9774e+00,  3.5000e-02, -3.4842e+00,  4.6453e-01],
        [-9.3171e+00, -2.7030e+00,  4.1540e+00,  5.0890e+00, -1.0082e+01,
         -3.1632e+00,  3.2229e-01,  1.4792e-02, -4.8349e+00, -3.5422e+00],
        [-2.7349e+00, -2.6226e+00,  2.1398e+00,  5.6341e+00, -2.9992e+00,
         -1.0175e+01,  2.6172e+00,  1.1767e-01, -2.9595e+00, -4.2366e+00],
        [-1.0014e+00, -1.4151e-02, -1.0662e+00, -8.0230e-01, -8.7620e-01,
         -4.3828e-01, -8.7010e-01, -7.4700e-02, -8.7208e-02, -9.0431e-01],
        [-1.0014e+00, -1.4151e-02, -1.0662e+00, -8.0230e-01, -8.7620e-01,
         -4.3828e-01, -8.7010e-01, -7.4700e-02, -8.7207e-02, -9.0431e-01],
        [-4.5450e+00,  1.4865e+00, -3.3089e+00,  8.7777e+00, -1.6116e+00,
          3.0105e+00,  4.3768e-01, -2.2472e+00,  5.5856e+00, -2.6165e+00]])), ('1.model.hidden_layer_2.bias', tensor([-0.0882, -2.3083, -5.8194, -2.5287, -1.2284, -2.8469,  0.8334, -1.9254,
        -1.9254, -3.5360])), ('1.model.output_layer_3.weight', tensor([[-2.8239,  2.3244, -4.5291, -5.2436,  5.8113,  9.2186, -0.6085, -0.0216,
         -0.0216,  0.4127],
        [ 2.7442, -2.3072,  4.5360,  5.2387, -5.8473, -9.2137,  0.8632,  0.0216,
          0.0217, -0.5559]]))])
loaded xi:  -459.98447
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1518.318040318632
W_T_median: 1157.8841391994329
W_T_pctile_5: -114.48374804507138
W_T_CVAR_5_pct: -279.1514474953861
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1671.7027973444294
Current xi:  [-436.79868]
objective value function right now is: -1671.7027973444294
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1674.5332816056273
Current xi:  [-414.133]
objective value function right now is: -1674.5332816056273
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1676.4668072612878
Current xi:  [-392.60233]
objective value function right now is: -1676.4668072612878
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.4482625151807
Current xi:  [-372.449]
objective value function right now is: -1678.4482625151807
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1680.2963104269209
Current xi:  [-351.73352]
objective value function right now is: -1680.2963104269209
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1681.5421239320056
Current xi:  [-333.3646]
objective value function right now is: -1681.5421239320056
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1682.543403469377
Current xi:  [-314.83862]
objective value function right now is: -1682.543403469377
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1683.5989826594703
Current xi:  [-296.96027]
objective value function right now is: -1683.5989826594703
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1684.1699382003103
Current xi:  [-283.026]
objective value function right now is: -1684.1699382003103
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1684.652968951615
Current xi:  [-269.64]
objective value function right now is: -1684.652968951615
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1685.0040584103008
Current xi:  [-254.74435]
objective value function right now is: -1685.0040584103008
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.19507]
objective value function right now is: -1684.9263749373993
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1685.7041222202204
Current xi:  [-238.37209]
objective value function right now is: -1685.7041222202204
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-235.29137]
objective value function right now is: -1685.417025625224
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1685.7413870403054
Current xi:  [-233.85957]
objective value function right now is: -1685.7413870403054
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1685.7582544501288
Current xi:  [-233.6241]
objective value function right now is: -1685.7582544501288
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1685.8168072448186
Current xi:  [-231.31017]
objective value function right now is: -1685.8168072448186
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-228.78432]
objective value function right now is: -1684.9219085560057
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-226.65314]
objective value function right now is: -1685.4027066361607
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.1274889920774
Current xi:  [-222.67673]
objective value function right now is: -1686.1274889920774
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-218.81711]
objective value function right now is: -1685.632945725607
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-215.08792]
objective value function right now is: -1685.436304334582
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-210.85068]
objective value function right now is: -1685.7853656359716
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-207.61139]
objective value function right now is: -1684.944094846338
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-205.44153]
objective value function right now is: -1685.9304949665373
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-205.7876]
objective value function right now is: -1685.1353288146775
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-206.0265]
objective value function right now is: -1685.9611650778686
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-205.62717]
objective value function right now is: -1685.9486607139759
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-206.16928]
objective value function right now is: -1685.8708080505828
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.319011304202
Current xi:  [-205.87415]
objective value function right now is: -1686.319011304202
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-206.06557]
objective value function right now is: -1685.6434499456234
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-205.40477]
objective value function right now is: -1685.2446370635164
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-206.07741]
objective value function right now is: -1685.9152801630908
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.337993635742
Current xi:  [-205.49695]
objective value function right now is: -1686.337993635742
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.91756]
objective value function right now is: -1686.1919991321577
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.5644754009363
Current xi:  [-204.71217]
objective value function right now is: -1686.5644754009363
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.5991280590918
Current xi:  [-204.7753]
objective value function right now is: -1686.5991280590918
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.6528941579556
Current xi:  [-204.36496]
objective value function right now is: -1686.6528941579556
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.7087804235305
Current xi:  [-204.33049]
objective value function right now is: -1686.7087804235305
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.31827]
objective value function right now is: -1686.5899968335375
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.06609]
objective value function right now is: -1686.6344393433271
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-203.97147]
objective value function right now is: -1686.3700772306204
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.28941]
objective value function right now is: -1686.7070562924769
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.2394]
objective value function right now is: -1686.5738735584168
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.28055]
objective value function right now is: -1686.636231898511
new min fval from sgd:  -1686.7105922460291
new min fval from sgd:  -1686.7217450886512
new min fval from sgd:  -1686.7310867240326
new min fval from sgd:  -1686.7393017000848
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.32533]
objective value function right now is: -1686.6049048678283
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.25972]
objective value function right now is: -1686.5183936169003
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.41736]
objective value function right now is: -1686.491928074624
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.26619]
objective value function right now is: -1686.7102330599803
new min fval from sgd:  -1686.739813020472
new min fval from sgd:  -1686.7421243361991
new min fval from sgd:  -1686.744337776456
new min fval from sgd:  -1686.746129507908
new min fval from sgd:  -1686.748341570473
new min fval from sgd:  -1686.7504467002555
new min fval from sgd:  -1686.7523470049216
new min fval from sgd:  -1686.753481768268
new min fval from sgd:  -1686.7544093289991
new min fval from sgd:  -1686.7546138358432
new min fval from sgd:  -1686.7550345597392
new min fval from sgd:  -1686.7554090743474
new min fval from sgd:  -1686.7559915259076
new min fval from sgd:  -1686.7566803315385
new min fval from sgd:  -1686.7571655285644
new min fval from sgd:  -1686.7582599326383
new min fval from sgd:  -1686.7587596154804
new min fval from sgd:  -1686.7589219507156
new min fval from sgd:  -1686.759046293491
new min fval from sgd:  -1686.759444826279
new min fval from sgd:  -1686.7594710905023
new min fval from sgd:  -1686.759792702616
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.23346]
objective value function right now is: -1686.7112406466972
min fval:  -1686.759792702616
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.3423,  1.3577],
        [-0.3423,  1.3577],
        [-1.0143, 10.0116],
        [15.0662,  2.6456],
        [-0.3422,  1.3577],
        [-0.3422,  1.3577],
        [-0.3422,  1.3577],
        [-0.3422,  1.3577],
        [-0.5960, 10.2783],
        [-0.3422,  1.3577]])), ('0.model.hidden_layer_1.bias', tensor([ -1.0488,  -1.0488,  11.2943, -10.4091,  -1.0488,  -1.0488,  -1.0488,
         -1.0488,  11.5239,  -1.0488])), ('0.model.hidden_layer_2.weight', tensor([[ -0.0143,  -0.0143,  -0.0486,  -0.1928,  -0.0143,  -0.0143,  -0.0143,
          -0.0143,  -0.0595,  -0.0143],
        [ -0.0143,  -0.0143,  -0.0486,  -0.1928,  -0.0143,  -0.0143,  -0.0143,
          -0.0143,  -0.0595,  -0.0143],
        [  0.0661,   0.0661,  -8.5470, -10.4511,   0.0661,   0.0661,   0.0661,
           0.0661, -10.1132,   0.0661],
        [  0.0542,   0.0542,   1.3426,  -0.1276,   0.0542,   0.0542,   0.0542,
           0.0542,   1.4618,   0.0542],
        [ -0.0143,  -0.0143,  -0.0486,  -0.1928,  -0.0143,  -0.0143,  -0.0143,
          -0.0143,  -0.0595,  -0.0143],
        [ -0.0143,  -0.0143,  -0.0486,  -0.1928,  -0.0143,  -0.0143,  -0.0143,
          -0.0143,  -0.0595,  -0.0143],
        [  0.1100,   0.1100,   6.1343,  -3.1501,   0.1100,   0.1100,   0.1100,
           0.1100,   6.7198,   0.1100],
        [  0.0308,   0.0308,   5.4141,   5.4541,   0.0308,   0.0308,   0.0308,
           0.0308,   5.7186,   0.0308],
        [ -0.0143,  -0.0143,  -0.0486,  -0.1928,  -0.0143,  -0.0143,  -0.0143,
          -0.0143,  -0.0595,  -0.0143],
        [ -0.0143,  -0.0143,  -0.0486,  -0.1928,  -0.0143,  -0.0143,  -0.0143,
          -0.0143,  -0.0595,  -0.0143]])), ('0.model.hidden_layer_2.bias', tensor([-0.8762, -0.8762,  4.6276, -2.3755, -0.8762, -0.8762, -3.3725, -3.0383,
        -0.8762, -0.8762])), ('0.model.output_layer_3.weight', tensor([[ 5.1034e-03,  5.1034e-03, -1.3910e+01,  1.5852e+00,  5.1034e-03,
          5.1034e-03,  7.3761e+00,  7.2616e+00,  5.1034e-03,  5.1034e-03]])), ('1.model.hidden_layer_1.weight', tensor([[ 15.0042,  12.3535],
        [-12.8020,   2.0660],
        [ -5.9297, -14.7085],
        [-13.8602,   1.8298],
        [ -2.9042, -12.3586],
        [ -5.7133,  11.1590],
        [ -1.8977,  -1.3606],
        [  7.2315,   1.2971],
        [ -9.2971,   2.5285],
        [ 12.3426,   3.4484]])), ('1.model.hidden_layer_1.bias', tensor([ 12.3623,   2.5071, -12.6020,  14.1407, -13.9984,  10.5956,  -5.1893,
         -8.2618,   5.2824,   0.1192])), ('1.model.hidden_layer_2.weight', tensor([[-6.5391e-01, -1.0594e+00, -4.3970e+00, -6.2208e+00, -2.0306e+00,
          2.5993e+00, -6.5017e-01,  4.5558e+00, -1.4876e+00, -3.4298e-01],
        [-1.1124e+00,  7.4874e-04, -1.1214e+00, -7.6276e-01, -1.0747e+00,
         -4.9859e-01, -8.1781e-01, -2.4224e-02,  5.0611e-02, -1.2347e+00],
        [-2.7947e+00,  6.1756e+00, -3.2215e+00, -5.2561e+00, -1.3194e+00,
          5.6841e+00, -3.0416e-01, -4.9495e+00,  7.7483e+00,  1.0479e+00],
        [-1.0103e+01, -4.9866e+00,  7.7196e+00,  7.3617e+00, -7.3632e+00,
         -1.2227e+01,  7.2496e+00, -6.4304e-01,  1.0859e+00, -3.5149e+00],
        [ 4.1353e+00,  3.1692e+00,  1.2961e+00, -1.1658e+01, -2.1112e+00,
         -6.4139e-01,  7.6315e+00,  2.3263e+00,  6.9784e+00, -4.9372e+00],
        [-1.2185e+01, -4.1575e+00,  1.0675e-01,  8.3986e+00,  1.9788e+00,
         -2.2557e+00, -5.1320e+00, -6.9419e-03,  4.4197e+00, -1.2986e+01],
        [-3.7874e+00, -8.7039e-01,  3.5797e+00,  3.2214e+00, -5.3503e+00,
         -8.7067e+00, -4.7255e+00,  1.7260e+00, -2.7376e+00, -1.0598e-01],
        [-1.1100e+00,  5.4552e-04, -1.1334e+00, -7.7924e-01, -1.0679e+00,
         -4.9842e-01, -8.1127e-01, -2.3157e-02,  5.0622e-02, -1.2175e+00],
        [-1.1099e+00,  5.4417e-04, -1.1336e+00, -7.7951e-01, -1.0678e+00,
         -4.9835e-01, -8.1121e-01, -2.3145e-02,  5.0625e-02, -1.2173e+00],
        [-4.4756e+00,  1.9510e+00, -4.1172e+00,  8.7249e+00, -4.1103e+00,
          7.6290e-01, -1.6113e-01, -3.2814e+00,  2.8167e+00, -1.3267e+00]])), ('1.model.hidden_layer_2.bias', tensor([-1.4009, -2.2165, -6.1326, -1.4287, -0.7034, -4.6225,  2.1984, -2.2332,
        -2.2334, -3.6748])), ('1.model.output_layer_3.weight', tensor([[ -2.5552,   0.0589,  -4.7211,  -5.4033,   5.3139,  11.8122,  -0.2446,
           0.0592,   0.0592,   0.5086],
        [  2.5416,  -0.0589,   4.7214,   5.3988,  -5.3488, -11.8122,   0.4917,
          -0.0592,  -0.0592,  -0.6394]]))])
xi:  [-204.23682]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1139.842601644607
W_T_median: 127.46931559332805
W_T_pctile_5: -204.0146748432053
W_T_CVAR_5_pct: -297.58336604441575
Average q (qsum/M+1):  56.294736800655244
Optimal xi:  [-204.23682]
Observed VAR:  127.46931559332805
Expected(across Rb) median(across samples) p_equity:  0.5970269540945689
obj fun:  tensor(-1686.7598, dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.3423,  1.3577],
        [-0.3423,  1.3577],
        [-1.0143, 10.0116],
        [15.0662,  2.6456],
        [-0.3422,  1.3577],
        [-0.3422,  1.3577],
        [-0.3422,  1.3577],
        [-0.3422,  1.3577],
        [-0.5960, 10.2783],
        [-0.3422,  1.3577]])), ('0.model.hidden_layer_1.bias', tensor([ -1.0488,  -1.0488,  11.2943, -10.4091,  -1.0488,  -1.0488,  -1.0488,
         -1.0488,  11.5239,  -1.0488])), ('0.model.hidden_layer_2.weight', tensor([[ -0.0143,  -0.0143,  -0.0486,  -0.1928,  -0.0143,  -0.0143,  -0.0143,
          -0.0143,  -0.0595,  -0.0143],
        [ -0.0143,  -0.0143,  -0.0486,  -0.1928,  -0.0143,  -0.0143,  -0.0143,
          -0.0143,  -0.0595,  -0.0143],
        [  0.0661,   0.0661,  -8.5470, -10.4511,   0.0661,   0.0661,   0.0661,
           0.0661, -10.1132,   0.0661],
        [  0.0542,   0.0542,   1.3426,  -0.1276,   0.0542,   0.0542,   0.0542,
           0.0542,   1.4618,   0.0542],
        [ -0.0143,  -0.0143,  -0.0486,  -0.1928,  -0.0143,  -0.0143,  -0.0143,
          -0.0143,  -0.0595,  -0.0143],
        [ -0.0143,  -0.0143,  -0.0486,  -0.1928,  -0.0143,  -0.0143,  -0.0143,
          -0.0143,  -0.0595,  -0.0143],
        [  0.1100,   0.1100,   6.1343,  -3.1501,   0.1100,   0.1100,   0.1100,
           0.1100,   6.7198,   0.1100],
        [  0.0308,   0.0308,   5.4141,   5.4541,   0.0308,   0.0308,   0.0308,
           0.0308,   5.7186,   0.0308],
        [ -0.0143,  -0.0143,  -0.0486,  -0.1928,  -0.0143,  -0.0143,  -0.0143,
          -0.0143,  -0.0595,  -0.0143],
        [ -0.0143,  -0.0143,  -0.0486,  -0.1928,  -0.0143,  -0.0143,  -0.0143,
          -0.0143,  -0.0595,  -0.0143]])), ('0.model.hidden_layer_2.bias', tensor([-0.8762, -0.8762,  4.6276, -2.3755, -0.8762, -0.8762, -3.3725, -3.0383,
        -0.8762, -0.8762])), ('0.model.output_layer_3.weight', tensor([[ 5.1034e-03,  5.1034e-03, -1.3910e+01,  1.5852e+00,  5.1034e-03,
          5.1034e-03,  7.3761e+00,  7.2616e+00,  5.1034e-03,  5.1034e-03]])), ('1.model.hidden_layer_1.weight', tensor([[ 15.0042,  12.3535],
        [-12.8020,   2.0660],
        [ -5.9297, -14.7085],
        [-13.8602,   1.8298],
        [ -2.9042, -12.3586],
        [ -5.7133,  11.1590],
        [ -1.8977,  -1.3606],
        [  7.2315,   1.2971],
        [ -9.2971,   2.5285],
        [ 12.3426,   3.4484]])), ('1.model.hidden_layer_1.bias', tensor([ 12.3623,   2.5071, -12.6020,  14.1407, -13.9984,  10.5956,  -5.1893,
         -8.2618,   5.2824,   0.1192])), ('1.model.hidden_layer_2.weight', tensor([[-6.5391e-01, -1.0594e+00, -4.3970e+00, -6.2208e+00, -2.0306e+00,
          2.5993e+00, -6.5017e-01,  4.5558e+00, -1.4876e+00, -3.4298e-01],
        [-1.1124e+00,  7.4874e-04, -1.1214e+00, -7.6276e-01, -1.0747e+00,
         -4.9859e-01, -8.1781e-01, -2.4224e-02,  5.0611e-02, -1.2347e+00],
        [-2.7947e+00,  6.1756e+00, -3.2215e+00, -5.2561e+00, -1.3194e+00,
          5.6841e+00, -3.0416e-01, -4.9495e+00,  7.7483e+00,  1.0479e+00],
        [-1.0103e+01, -4.9866e+00,  7.7196e+00,  7.3617e+00, -7.3632e+00,
         -1.2227e+01,  7.2496e+00, -6.4304e-01,  1.0859e+00, -3.5149e+00],
        [ 4.1353e+00,  3.1692e+00,  1.2961e+00, -1.1658e+01, -2.1112e+00,
         -6.4139e-01,  7.6315e+00,  2.3263e+00,  6.9784e+00, -4.9372e+00],
        [-1.2185e+01, -4.1575e+00,  1.0675e-01,  8.3986e+00,  1.9788e+00,
         -2.2557e+00, -5.1320e+00, -6.9419e-03,  4.4197e+00, -1.2986e+01],
        [-3.7874e+00, -8.7039e-01,  3.5797e+00,  3.2214e+00, -5.3503e+00,
         -8.7067e+00, -4.7255e+00,  1.7260e+00, -2.7376e+00, -1.0598e-01],
        [-1.1100e+00,  5.4552e-04, -1.1334e+00, -7.7924e-01, -1.0679e+00,
         -4.9842e-01, -8.1127e-01, -2.3157e-02,  5.0622e-02, -1.2175e+00],
        [-1.1099e+00,  5.4417e-04, -1.1336e+00, -7.7951e-01, -1.0678e+00,
         -4.9835e-01, -8.1121e-01, -2.3145e-02,  5.0625e-02, -1.2173e+00],
        [-4.4756e+00,  1.9510e+00, -4.1172e+00,  8.7249e+00, -4.1103e+00,
          7.6290e-01, -1.6113e-01, -3.2814e+00,  2.8167e+00, -1.3267e+00]])), ('1.model.hidden_layer_2.bias', tensor([-1.4009, -2.2165, -6.1326, -1.4287, -0.7034, -4.6225,  2.1984, -2.2332,
        -2.2334, -3.6748])), ('1.model.output_layer_3.weight', tensor([[ -2.5552,   0.0589,  -4.7211,  -5.4033,   5.3139,  11.8122,  -0.2446,
           0.0592,   0.0592,   0.5086],
        [  2.5416,  -0.0589,   4.7214,   5.3988,  -5.3488, -11.8122,   0.4917,
          -0.0592,  -0.0592,  -0.6394]]))])
loaded xi:  -204.23682
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1518.318040318632
W_T_median: 1157.8841391994329
W_T_pctile_5: -114.48374804507138
W_T_CVAR_5_pct: -279.1514474953861
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1609.6023037998525
Current xi:  [-181.5324]
objective value function right now is: -1609.6023037998525
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1612.3743453185446
Current xi:  [-161.08667]
objective value function right now is: -1612.3743453185446
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1614.4795007622165
Current xi:  [-145.32945]
objective value function right now is: -1614.4795007622165
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1617.639894137629
Current xi:  [-126.55285]
objective value function right now is: -1617.639894137629
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1618.6858753816205
Current xi:  [-113.63182]
objective value function right now is: -1618.6858753816205
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1619.7522396588347
Current xi:  [-107.7567]
objective value function right now is: -1619.7522396588347
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1621.100084950481
Current xi:  [-96.831085]
objective value function right now is: -1621.100084950481
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1621.4701685909167
Current xi:  [-81.71607]
objective value function right now is: -1621.4701685909167
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1621.929378560082
Current xi:  [-73.446304]
objective value function right now is: -1621.929378560082
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.75102]
objective value function right now is: -1621.235981323055
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1622.5985983357727
Current xi:  [-73.42818]
objective value function right now is: -1622.5985983357727
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1622.6509622594654
Current xi:  [-73.28779]
objective value function right now is: -1622.6509622594654
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1623.6284688513708
Current xi:  [-73.28324]
objective value function right now is: -1623.6284688513708
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-73.700775]
objective value function right now is: -1623.2958976324708
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.41832]
objective value function right now is: -1622.989377929043
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.35369]
objective value function right now is: -1621.2902226444069
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.33025]
objective value function right now is: -1623.4096207839618
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.164734]
objective value function right now is: -1623.1554858743414
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.28555]
objective value function right now is: -1621.9696693188528
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.36199]
objective value function right now is: -1623.3279906530095
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.51338]
objective value function right now is: -1621.346084982837
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1623.6403074381267
Current xi:  [-73.03209]
objective value function right now is: -1623.6403074381267
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-72.984146]
objective value function right now is: -1623.6053325365626
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1623.6741708713662
Current xi:  [-73.105354]
objective value function right now is: -1623.6741708713662
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.40774]
objective value function right now is: -1623.108377006503
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.3015]
objective value function right now is: -1623.4249976573
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-72.98998]
objective value function right now is: -1623.3964253636632
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-73.020256]
objective value function right now is: -1619.6412703428205
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-73.19235]
objective value function right now is: -1623.6202887393492
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.01563]
objective value function right now is: -1623.6160923062919
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.08209]
objective value function right now is: -1622.9617249384569
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.02188]
objective value function right now is: -1621.7456983505404
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1623.783041482334
Current xi:  [-72.98243]
objective value function right now is: -1623.783041482334
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1624.0201751224772
Current xi:  [-73.12195]
objective value function right now is: -1624.0201751224772
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-72.74869]
objective value function right now is: -1621.4784635633928
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1624.6563441145827
Current xi:  [-72.7318]
objective value function right now is: -1624.6563441145827
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1624.7250198686108
Current xi:  [-72.69439]
objective value function right now is: -1624.7250198686108
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1624.7696469562272
Current xi:  [-72.452446]
objective value function right now is: -1624.7696469562272
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1624.8976480718836
Current xi:  [-72.54586]
objective value function right now is: -1624.8976480718836
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-72.42368]
objective value function right now is: -1624.7827755832764
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-72.239]
objective value function right now is: -1624.8232792661954
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-72.17695]
objective value function right now is: -1624.782412782049
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-71.9459]
objective value function right now is: -1624.8429556518588
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-71.84529]
objective value function right now is: -1624.675546196469
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-71.949875]
objective value function right now is: -1623.421796502849
new min fval from sgd:  -1624.9263849843217
new min fval from sgd:  -1624.9562241076194
new min fval from sgd:  -1624.9753680680146
new min fval from sgd:  -1624.987525606097
new min fval from sgd:  -1624.993450241419
new min fval from sgd:  -1624.9993055005577
new min fval from sgd:  -1625.004248727999
new min fval from sgd:  -1625.0048520743092
new min fval from sgd:  -1625.0132744398984
new min fval from sgd:  -1625.025615983415
new min fval from sgd:  -1625.0406337955892
new min fval from sgd:  -1625.0545693679103
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-71.984474]
objective value function right now is: -1625.0127817781563
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-71.83446]
objective value function right now is: -1624.888206466369
new min fval from sgd:  -1625.0553131024253
new min fval from sgd:  -1625.0704037826774
new min fval from sgd:  -1625.0723667369646
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-71.95009]
objective value function right now is: -1624.8073945480905
new min fval from sgd:  -1625.0739996751681
new min fval from sgd:  -1625.074962559623
new min fval from sgd:  -1625.0762547186273
new min fval from sgd:  -1625.0790558755891
new min fval from sgd:  -1625.0813819040507
new min fval from sgd:  -1625.0838722186945
new min fval from sgd:  -1625.0870810450967
new min fval from sgd:  -1625.0903396131075
new min fval from sgd:  -1625.0926412606364
new min fval from sgd:  -1625.0930837540043
new min fval from sgd:  -1625.0938119825591
new min fval from sgd:  -1625.0938578718683
new min fval from sgd:  -1625.0947336496986
new min fval from sgd:  -1625.096049863764
new min fval from sgd:  -1625.097871613184
new min fval from sgd:  -1625.1000692240077
new min fval from sgd:  -1625.1005211475615
new min fval from sgd:  -1625.1024142156648
new min fval from sgd:  -1625.1024460840772
new min fval from sgd:  -1625.1026987480363
new min fval from sgd:  -1625.1038740494132
new min fval from sgd:  -1625.105823289205
new min fval from sgd:  -1625.1078815887938
new min fval from sgd:  -1625.107929135485
new min fval from sgd:  -1625.1099601647554
new min fval from sgd:  -1625.1118797158158
new min fval from sgd:  -1625.1123240694333
new min fval from sgd:  -1625.1134147613425
new min fval from sgd:  -1625.1138158718302
new min fval from sgd:  -1625.114214008725
new min fval from sgd:  -1625.11580854953
new min fval from sgd:  -1625.1178315697673
new min fval from sgd:  -1625.11805490881
new min fval from sgd:  -1625.1195374785996
new min fval from sgd:  -1625.1209100848046
new min fval from sgd:  -1625.1223459099922
new min fval from sgd:  -1625.1239924459192
new min fval from sgd:  -1625.1256670492185
new min fval from sgd:  -1625.127074643679
new min fval from sgd:  -1625.1284840279768
new min fval from sgd:  -1625.129576984628
new min fval from sgd:  -1625.130167150928
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-71.81175]
objective value function right now is: -1625.1190216902783
new min fval from sgd:  -1625.1310541476128
new min fval from sgd:  -1625.134862215916
new min fval from sgd:  -1625.1365547768519
new min fval from sgd:  -1625.1378197072652
new min fval from sgd:  -1625.1383937488433
new min fval from sgd:  -1625.1387686371618
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-71.7834]
objective value function right now is: -1625.1105897851219
min fval:  -1625.1387686371618
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.5691,  1.3571],
        [-0.5691,  1.3571],
        [-1.7786, 11.3296],
        [10.0719, -3.5218],
        [-0.5691,  1.3571],
        [-0.5691,  1.3571],
        [-0.5691,  1.3571],
        [-0.5691,  1.3571],
        [ 1.3464, 11.8066],
        [-0.5691,  1.3571]])), ('0.model.hidden_layer_1.bias', tensor([ -1.4737,  -1.4737,  11.7579, -10.3454,  -1.4737,  -1.4737,  -1.4737,
         -1.4737,  12.0645,  -1.4737])), ('0.model.hidden_layer_2.weight', tensor([[-1.3915e-02, -1.3915e-02, -5.3638e-01, -2.6596e-01, -1.3917e-02,
         -1.3917e-02, -1.3917e-02, -1.3917e-02, -6.4912e-01, -1.3917e-02],
        [-1.3914e-02, -1.3914e-02, -5.3638e-01, -2.6596e-01, -1.3917e-02,
         -1.3917e-02, -1.3917e-02, -1.3917e-02, -6.4912e-01, -1.3917e-02],
        [ 5.3049e-02,  5.3049e-02, -5.7575e+00,  1.0777e+01,  5.2979e-02,
          5.2979e-02,  5.2978e-02,  5.2978e-02, -1.0656e+01,  5.2979e-02],
        [-1.3914e-02, -1.3914e-02, -5.3638e-01, -2.6596e-01, -1.3917e-02,
         -1.3917e-02, -1.3917e-02, -1.3917e-02, -6.4912e-01, -1.3917e-02],
        [-1.3915e-02, -1.3915e-02, -5.3638e-01, -2.6596e-01, -1.3917e-02,
         -1.3917e-02, -1.3917e-02, -1.3917e-02, -6.4912e-01, -1.3917e-02],
        [-1.3914e-02, -1.3914e-02, -5.3638e-01, -2.6596e-01, -1.3917e-02,
         -1.3917e-02, -1.3917e-02, -1.3917e-02, -6.4912e-01, -1.3917e-02],
        [ 6.8537e-02,  6.8538e-02,  7.8774e+00, -1.1827e+01,  6.8509e-02,
          6.8509e-02,  6.8508e-02,  6.8508e-02,  4.3046e+00,  6.8508e-02],
        [-2.0595e-03, -2.0598e-03,  6.6403e+00, -9.8723e+00, -2.2113e-03,
         -2.2115e-03, -2.2113e-03, -2.2116e-03,  3.4489e+00, -2.2113e-03],
        [-1.3915e-02, -1.3915e-02, -5.3638e-01, -2.6596e-01, -1.3917e-02,
         -1.3917e-02, -1.3917e-02, -1.3917e-02, -6.4912e-01, -1.3917e-02],
        [-1.3914e-02, -1.3914e-02, -5.3638e-01, -2.6596e-01, -1.3917e-02,
         -1.3917e-02, -1.3917e-02, -1.3917e-02, -6.4912e-01, -1.3917e-02]])), ('0.model.hidden_layer_2.bias', tensor([-0.8886, -0.8886,  5.3408, -0.8886, -0.8886, -0.8886, -4.9005, -4.4195,
        -0.8886, -0.8886])), ('0.model.output_layer_3.weight', tensor([[ -0.0933,  -0.0933, -15.5406,  -0.0933,  -0.0933,  -0.0933,   8.7989,
           6.1559,  -0.0933,  -0.0933]])), ('1.model.hidden_layer_1.weight', tensor([[ 20.2705,  12.8347],
        [-12.7112,   3.4282],
        [-10.2512, -14.1797],
        [-15.8884,   1.4786],
        [ -5.9761, -17.8052],
        [ -5.7838,   9.7688],
        [ -3.2451,  -1.5819],
        [  3.8914,   3.2312],
        [-12.8977,   0.8692],
        [ 14.9595,   2.8429]])), ('1.model.hidden_layer_1.bias', tensor([ 11.8236,   4.9143, -14.0658,  15.1130, -18.2875,  11.1435,  -5.2670,
         -8.3820,   5.3610,  -3.9133])), ('1.model.hidden_layer_2.weight', tensor([[-1.5955e+00,  3.0404e-03, -1.0766e+00, -1.0367e+00, -1.2611e+00,
         -3.2245e-01, -8.6644e-01, -3.3177e-02, -2.7153e-02, -1.3730e+00],
        [-1.5330e+00,  2.6038e-03, -1.0501e+00, -1.0290e+00, -1.2192e+00,
         -3.1714e-01, -8.3800e-01, -3.2109e-02, -2.4700e-02, -1.3410e+00],
        [-3.7140e+00,  4.9666e+00, -1.0005e+01, -5.5466e+00, -7.2172e+00,
          6.6331e+00, -4.9340e-01, -1.0172e+00,  6.6123e+00,  3.2206e+00],
        [-1.1462e+01,  1.4786e+00,  7.2457e+00,  1.3239e+01, -4.3379e+00,
         -2.0101e+01,  8.8606e+00, -2.2402e-03,  1.5929e+00, -2.3370e+00],
        [ 4.3080e+00, -6.1772e-04, -6.3989e-01, -1.3010e+01, -5.0680e-01,
         -4.3343e-01,  9.8342e+00,  9.4165e-02, -4.5935e-02, -4.4217e+00],
        [-2.2096e+01, -4.0124e-02,  1.2619e+00,  5.6164e+00,  2.4401e+00,
         -9.5014e+00, -4.9855e+00, -1.8709e-04,  9.7138e+00, -1.2266e+01],
        [ 6.5422e-01,  8.2182e+00, -5.6597e+00,  1.4832e+00, -9.6641e+00,
         -6.1648e-01, -6.1438e-01, -6.7049e+00,  5.0496e+00, -2.8071e+00],
        [ 1.6642e+00, -4.7501e-01, -3.2953e+00,  7.0514e-02,  3.5368e-01,
         -8.4017e+00, -7.8363e-01,  6.0294e-02,  2.8307e+00, -3.8580e-01],
        [ 1.4088e+00,  5.2892e+00, -1.2469e+00, -1.4624e+00,  2.7369e-01,
         -8.0410e+00, -2.2184e+00, -2.3936e-01,  5.8928e+00,  7.1219e-01],
        [-2.9638e+00,  2.1405e+00, -9.5127e+00,  6.8203e+00, -9.0542e+00,
          3.8321e+00, -4.7324e-01, -1.7363e+00,  1.2941e-01, -5.1441e+00]])), ('1.model.hidden_layer_2.bias', tensor([-2.6295, -2.7399, -7.1854, -4.5462, -1.7952, -6.4020, -2.1755, -4.9998,
        -5.1176, -3.7577])), ('1.model.output_layer_3.weight', tensor([[  0.0174,   0.0165,  -4.6136,  -5.1820,   5.1688,  10.8919,  -1.3578,
           3.7451,   3.1030,   1.9664],
        [ -0.0174,  -0.0165,   4.6137,   5.1777,  -5.2032, -10.8906,   1.5794,
          -3.7450,  -3.1028,  -2.0853]]))])
xi:  [-71.82591]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1119.466397884803
W_T_median: 154.63024539293573
W_T_pctile_5: -71.72922762425868
W_T_CVAR_5_pct: -157.94587682442886
Average q (qsum/M+1):  54.935247605846776
Optimal xi:  [-71.82591]
Observed VAR:  154.63024539293573
Expected(across Rb) median(across samples) p_equity:  0.5275298118591308
obj fun:  tensor(-1625.1388, dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5

  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-6.9778, -0.3578],
        [-6.9780, -0.3579],
        [-1.6275,  1.0240],
        [13.1635, -2.0706],
        [-6.9789, -0.3586],
        [-6.9795, -0.3589],
        [-6.9802, -0.3594],
        [-6.9806, -0.3597],
        [-3.6438, 12.2427],
        [-6.9804, -0.3596]])), ('0.model.hidden_layer_1.bias', tensor([ -2.3534,  -2.3533,  -3.9858, -10.8851,  -2.3530,  -2.3528,  -2.3525,
         -2.3524,  11.2077,  -2.3524])), ('0.model.hidden_layer_2.weight', tensor([[-5.4026e-02, -5.4034e-02, -1.0761e-02, -5.2111e-01, -5.4072e-02,
         -5.4093e-02, -5.4122e-02, -5.4141e-02, -1.1077e+00, -5.4132e-02],
        [-5.4026e-02, -5.4034e-02, -1.0761e-02, -5.2111e-01, -5.4072e-02,
         -5.4093e-02, -5.4122e-02, -5.4141e-02, -1.1077e+00, -5.4132e-02],
        [ 1.8848e+00,  1.8848e+00,  7.7571e-02,  1.4555e+01,  1.8851e+00,
          1.8852e+00,  1.8855e+00,  1.8856e+00, -1.4386e+01,  1.8855e+00],
        [-5.4026e-02, -5.4034e-02, -1.0761e-02, -5.2111e-01, -5.4072e-02,
         -5.4093e-02, -5.4122e-02, -5.4141e-02, -1.1077e+00, -5.4133e-02],
        [-5.4026e-02, -5.4034e-02, -1.0761e-02, -5.2111e-01, -5.4072e-02,
         -5.4093e-02, -5.4122e-02, -5.4141e-02, -1.1077e+00, -5.4132e-02],
        [-5.4026e-02, -5.4034e-02, -1.0761e-02, -5.2111e-01, -5.4072e-02,
         -5.4093e-02, -5.4122e-02, -5.4141e-02, -1.1077e+00, -5.4133e-02],
        [-1.3563e+00, -1.3564e+00,  3.2627e-01, -1.3355e+01, -1.3566e+00,
         -1.3568e+00, -1.3570e+00, -1.3571e+00,  9.8349e+00, -1.3570e+00],
        [-1.9555e+00, -1.9556e+00, -1.8530e-01, -1.2073e+01, -1.9559e+00,
         -1.9561e+00, -1.9564e+00, -1.9565e+00,  7.8586e+00, -1.9564e+00],
        [-5.4026e-02, -5.4034e-02, -1.0761e-02, -5.2111e-01, -5.4072e-02,
         -5.4093e-02, -5.4122e-02, -5.4141e-02, -1.1077e+00, -5.4132e-02],
        [-5.4026e-02, -5.4034e-02, -1.0761e-02, -5.2111e-01, -5.4072e-02,
         -5.4093e-02, -5.4122e-02, -5.4141e-02, -1.1077e+00, -5.4133e-02]])), ('0.model.hidden_layer_2.bias', tensor([-2.6422, -2.6422,  1.8892, -2.6422, -2.6422, -2.6422, -1.5695, -1.4112,
        -2.6422, -2.6422])), ('0.model.output_layer_3.weight', tensor([[ -0.0488,  -0.0488, -23.2926,  -0.0488,  -0.0488,  -0.0488,  15.8858,
          11.1962,  -0.0488,  -0.0488]])), ('1.model.hidden_layer_1.weight', tensor([[ 21.7751,  14.1380],
        [-12.9537,   4.4092],
        [-12.1199, -17.8177],
        [-19.5215,   1.1662],
        [  2.5659, -16.9228],
        [-11.4155,   9.3749],
        [ -2.6509,   0.3104],
        [  4.3903,   0.0636],
        [-13.1177,   0.3781],
        [ 14.3653,   3.9651]])), ('1.model.hidden_layer_1.bias', tensor([ 12.4127,   2.3527, -16.6322,  17.1072, -15.6864,  11.6106,  -5.3983,
        -10.2550,   6.1572,  -5.0764])), ('1.model.hidden_layer_2.weight', tensor([[-2.0408e-01,  3.4090e-01, -1.2936e+00, -2.0510e-01, -3.7235e-02,
         -1.1993e+00, -1.0615e-01, -2.7942e-01, -2.9698e+00,  2.6614e-02],
        [-5.4006e-01,  3.1054e-01, -1.0153e+00, -7.7015e-01, -3.7052e-02,
         -1.0195e+00, -2.2558e-01, -1.5042e-01,  1.2032e+00,  2.2656e-01],
        [-6.8104e+00,  2.4785e+00, -4.3031e+00, -5.3647e+00, -9.9871e+00,
          5.7089e+00, -1.0805e-01, -7.6478e-01,  1.6675e+01,  6.7887e+00],
        [-1.3778e+01, -6.2688e-01,  6.3165e+00,  1.5518e+01, -2.5456e+00,
         -1.3190e+01,  1.1705e-01, -3.6238e-01,  2.1861e+00, -4.2316e+00],
        [-8.5914e-01,  8.9605e-01, -1.7405e+00, -1.4420e+00, -9.3677e-01,
         -1.6623e-01,  6.9308e-03, -1.0137e-01,  1.4404e+00, -1.4466e+00],
        [-3.3202e+01, -1.7197e+00,  4.9593e+00,  4.8674e+00, -3.3002e-01,
         -5.7562e+00, -3.7305e-02,  1.0547e-01,  1.2080e+01, -1.5120e+00],
        [-2.8024e+00,  2.8662e-01, -1.3595e+00, -1.1784e+00, -1.5635e+00,
         -8.3817e-01, -9.5326e-04, -8.2227e-04,  2.2563e+00, -1.2445e+00],
        [ 1.4260e+00,  1.0823e+00, -2.8780e+00,  3.8434e-02,  5.5773e-01,
         -1.1013e+01,  6.7368e-02,  5.6519e-01,  1.5926e+00, -4.7931e-01],
        [ 7.3673e-01,  1.6165e+00,  1.3768e+00, -1.7692e+00, -6.4780e-02,
         -1.6569e+00, -2.5266e-01,  1.1361e+00,  5.0527e+00,  2.3548e+00],
        [-4.2100e+00, -2.8232e+00, -7.1655e+00,  7.5227e+00, -8.2915e+00,
          5.4056e+00, -2.6917e-02, -4.0934e-01,  4.0991e+00, -2.3889e+00]])), ('1.model.hidden_layer_2.bias', tensor([ -3.3226,  -3.9021, -11.1092,  -7.5625,  -4.6410, -11.4952,  -4.5168,
         -5.2042,  -6.0314,  -5.3263])), ('1.model.output_layer_3.weight', tensor([[  3.9289,   3.7315,  -5.3029,  -5.4534,   1.0145,  13.0670,   0.0359,
           4.6847,   2.9230,   0.4701],
        [ -3.9289,  -3.7315,   5.3030,   5.4534,  -1.0163, -13.0624,  -0.0201,
          -4.6846,  -2.9227,  -0.5845]]))])
loaded xi:  0.0011623579
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1518.318040318632
W_T_median: 1157.8841391994329
W_T_pctile_5: -114.48374804507138
W_T_CVAR_5_pct: -279.1514474953861
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1444.7080393259282
Current xi:  [-0.06334562]
objective value function right now is: -1444.7080393259282
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0086607]
objective value function right now is: -1439.2266200273564
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1446.781499110575
Current xi:  [-0.01785422]
objective value function right now is: -1446.781499110575
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0144627]
objective value function right now is: -1443.0055904311744
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03114636]
objective value function right now is: -1436.8826867320715
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0139696]
objective value function right now is: -1435.1909957978328
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1452.8097538095624
Current xi:  [4.256538]
objective value function right now is: -1452.8097538095624
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1464.6593102791471
Current xi:  [16.56861]
objective value function right now is: -1464.6593102791471
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1475.7382055006724
Current xi:  [31.253204]
objective value function right now is: -1475.7382055006724
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [46.744366]
objective value function right now is: -1474.8509702804886
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1506.021658099907
Current xi:  [61.95477]
objective value function right now is: -1506.021658099907
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1530.4966581782494
Current xi:  [77.34092]
objective value function right now is: -1530.4966581782494
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1535.7697315469206
Current xi:  [92.33318]
objective value function right now is: -1535.7697315469206
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [106.902855]
objective value function right now is: -1530.7594911841145
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1565.1335437612381
Current xi:  [119.81698]
objective value function right now is: -1565.1335437612381
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1572.1503703852143
Current xi:  [133.45747]
objective value function right now is: -1572.1503703852143
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1582.9928663816752
Current xi:  [145.28488]
objective value function right now is: -1582.9928663816752
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [153.45673]
objective value function right now is: -1568.4562788469862
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.08363]
objective value function right now is: -1562.491135931558
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1583.9734014563576
Current xi:  [169.1025]
objective value function right now is: -1583.9734014563576
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [173.98141]
objective value function right now is: -1575.6382302605946
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [176.84595]
objective value function right now is: -1580.6427145173664
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1584.8547695503723
Current xi:  [179.56667]
objective value function right now is: -1584.8547695503723
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.71841]
objective value function right now is: -1578.1214735345188
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1586.5103115265915
Current xi:  [183.50026]
objective value function right now is: -1586.5103115265915
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.97873]
objective value function right now is: -1585.1325682269917
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.0485]
objective value function right now is: -1585.601645103415
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [185.45706]
objective value function right now is: -1586.1145528292777
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [185.31116]
objective value function right now is: -1462.0042522710319
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1589.2421438687793
Current xi:  [187.23555]
objective value function right now is: -1589.2421438687793
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.65851]
objective value function right now is: -1586.344439970208
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1589.599469785216
Current xi:  [185.92453]
objective value function right now is: -1589.599469785216
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.68314]
objective value function right now is: -1587.7635293770115
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.94226]
objective value function right now is: -1581.243360580467
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.77826]
objective value function right now is: -1589.5693294276196
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1591.9772085301017
Current xi:  [186.6297]
objective value function right now is: -1591.9772085301017
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1593.5005846621764
Current xi:  [187.28552]
objective value function right now is: -1593.5005846621764
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1593.9819330417968
Current xi:  [187.34656]
objective value function right now is: -1593.9819330417968
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.98706]
objective value function right now is: -1592.4202871997534
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.4145]
objective value function right now is: -1591.592695333628
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.50514]
objective value function right now is: -1593.778234110855
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.53362]
objective value function right now is: -1593.2074928121226
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.5332]
objective value function right now is: -1593.8041071030273
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.32066]
objective value function right now is: -1593.0006201669278
new min fval from sgd:  -1594.339249267186
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.63266]
objective value function right now is: -1594.339249267186
new min fval from sgd:  -1594.3560252976376
new min fval from sgd:  -1594.4161843200661
new min fval from sgd:  -1594.4320131387815
new min fval from sgd:  -1594.4450776167055
new min fval from sgd:  -1594.4737016789072
new min fval from sgd:  -1594.4931056470375
new min fval from sgd:  -1594.5109680081869
new min fval from sgd:  -1594.5459932491826
new min fval from sgd:  -1594.56162415941
new min fval from sgd:  -1594.5732262796248
new min fval from sgd:  -1594.6655611624717
new min fval from sgd:  -1594.7335474727074
new min fval from sgd:  -1594.8211907003758
new min fval from sgd:  -1594.8277213080958
new min fval from sgd:  -1594.9116692514535
new min fval from sgd:  -1594.964140133786
new min fval from sgd:  -1594.9721593308254
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.7087]
objective value function right now is: -1593.1061104113298
new min fval from sgd:  -1594.9740172270488
new min fval from sgd:  -1594.9815057453131
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.56798]
objective value function right now is: -1593.1802192710998
new min fval from sgd:  -1594.9880938284061
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.33057]
objective value function right now is: -1593.2493165584413
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.29211]
objective value function right now is: -1594.037744194995
new min fval from sgd:  -1594.9897586558277
new min fval from sgd:  -1595.0028729128276
new min fval from sgd:  -1595.0148389048832
new min fval from sgd:  -1595.0210484592901
new min fval from sgd:  -1595.0252795022677
new min fval from sgd:  -1595.0321015744378
new min fval from sgd:  -1595.0339473650208
new min fval from sgd:  -1595.0356795980633
new min fval from sgd:  -1595.0357483692367
new min fval from sgd:  -1595.0378251999848
new min fval from sgd:  -1595.0443607197242
new min fval from sgd:  -1595.0532018197694
new min fval from sgd:  -1595.0608022840136
new min fval from sgd:  -1595.0670579734635
new min fval from sgd:  -1595.071575922647
new min fval from sgd:  -1595.0766174053572
new min fval from sgd:  -1595.0830052151896
new min fval from sgd:  -1595.087094046672
new min fval from sgd:  -1595.0871621352808
new min fval from sgd:  -1595.0951508893938
new min fval from sgd:  -1595.100529839553
new min fval from sgd:  -1595.1053526259634
new min fval from sgd:  -1595.11589109567
new min fval from sgd:  -1595.1261490542886
new min fval from sgd:  -1595.1263674935312
new min fval from sgd:  -1595.1273216794893
new min fval from sgd:  -1595.1287285908575
new min fval from sgd:  -1595.140735161564
new min fval from sgd:  -1595.1598676156343
new min fval from sgd:  -1595.1668214833014
new min fval from sgd:  -1595.176212623294
new min fval from sgd:  -1595.1863360590073
new min fval from sgd:  -1595.2097070019988
new min fval from sgd:  -1595.2403261900417
new min fval from sgd:  -1595.2755672112792
new min fval from sgd:  -1595.310400493135
new min fval from sgd:  -1595.3407825496326
new min fval from sgd:  -1595.3564521304504
new min fval from sgd:  -1595.3600779292333
new min fval from sgd:  -1595.3627598715382
new min fval from sgd:  -1595.3714163066495
new min fval from sgd:  -1595.3752883200666
new min fval from sgd:  -1595.3787632118258
new min fval from sgd:  -1595.4018639877477
new min fval from sgd:  -1595.416677243386
new min fval from sgd:  -1595.4253852159954
new min fval from sgd:  -1595.432036614316
new min fval from sgd:  -1595.4380113928896
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.32991]
objective value function right now is: -1595.365182686897
min fval:  -1595.4380113928896
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -2.2179,  -0.5368],
        [ -2.3361,  -0.6335],
        [ -0.5311,   7.2470],
        [ 18.3118,  -3.3749],
        [ -3.0461,  -1.1329],
        [ -3.5639,  -1.4705],
        [ -4.5593,  -2.1009],
        [-45.9455,  -2.2657],
        [ -7.3396,  12.9378],
        [ -6.1134,  -1.8969]])), ('0.model.hidden_layer_1.bias', tensor([ -4.5377,  -4.5159,  -7.5165, -11.9533,  -4.2881,  -4.0527,  -3.4724,
         -3.2549,  10.0112,  -3.5799])), ('0.model.hidden_layer_2.weight', tensor([[-4.7313e-03, -5.3840e-03, -1.0303e-01, -1.3855e-01, -1.0744e-02,
         -2.0453e-02, -1.0473e-01, -3.5139e-02, -5.6899e-01, -6.2270e-02],
        [-4.7313e-03, -5.3840e-03, -1.0303e-01, -1.3855e-01, -1.0744e-02,
         -2.0453e-02, -1.0473e-01, -3.5139e-02, -5.6899e-01, -6.2270e-02],
        [ 1.5288e+00,  1.6712e+00,  6.4439e+00,  1.6816e+01,  2.3897e+00,
          2.8510e+00,  3.6484e+00,  5.3923e+00, -1.6772e+01,  3.7052e+00],
        [-4.7313e-03, -5.3840e-03, -1.0303e-01, -1.3855e-01, -1.0744e-02,
         -2.0453e-02, -1.0473e-01, -3.5139e-02, -5.6899e-01, -6.2270e-02],
        [-4.7313e-03, -5.3841e-03, -1.0303e-01, -1.3855e-01, -1.0744e-02,
         -2.0453e-02, -1.0473e-01, -3.5138e-02, -5.6899e-01, -6.2270e-02],
        [-4.7313e-03, -5.3841e-03, -1.0303e-01, -1.3855e-01, -1.0744e-02,
         -2.0453e-02, -1.0473e-01, -3.5138e-02, -5.6899e-01, -6.2270e-02],
        [-9.5099e-01, -1.0782e+00, -5.8579e+00, -1.3409e+01, -1.7662e+00,
         -2.2459e+00, -3.1307e+00, -5.0592e+00,  1.3433e+01, -3.2707e+00],
        [-3.6874e-01, -4.8142e-01, -3.5478e+00, -1.1698e+01, -1.2813e+00,
         -1.9924e+00, -3.3302e+00, -4.7743e+00,  8.8089e+00, -3.2709e+00],
        [-4.7313e-03, -5.3840e-03, -1.0303e-01, -1.3855e-01, -1.0744e-02,
         -2.0453e-02, -1.0473e-01, -3.5139e-02, -5.6899e-01, -6.2270e-02],
        [-4.7313e-03, -5.3840e-03, -1.0303e-01, -1.3855e-01, -1.0744e-02,
         -2.0453e-02, -1.0473e-01, -3.5139e-02, -5.6899e-01, -6.2270e-02]])), ('0.model.hidden_layer_2.bias', tensor([-1.7904, -1.7904,  1.9355, -1.7904, -1.7904, -1.7904, -1.4514, -1.5835,
        -1.7904, -1.7904])), ('0.model.output_layer_3.weight', tensor([[-1.3662e-02, -1.3662e-02, -2.4030e+01, -1.3662e-02, -1.3662e-02,
         -1.3662e-02,  1.3710e+01,  7.0183e+00, -1.3662e-02, -1.3662e-02]])), ('1.model.hidden_layer_1.weight', tensor([[ 19.5117,  14.3907],
        [ -2.5644,   0.4779],
        [ -8.3021, -19.4571],
        [-21.2397,   0.9369],
        [-12.0931, -13.6297],
        [-14.0515,  10.9846],
        [ -2.5652,   0.4770],
        [ -2.5644,   0.4778],
        [-13.1529,   0.7819],
        [  7.6098,   0.7808]])), ('1.model.hidden_layer_1.bias', tensor([ 12.8193,  -5.3244, -17.1896,  18.8142, -15.7450,   9.6578,  -5.3236,
         -5.3245,   6.0103, -13.5785])), ('1.model.hidden_layer_2.weight', tensor([[ 2.5085e-01,  1.0614e-01, -7.1542e-01, -5.9082e-02, -1.4498e+01,
         -2.8926e-01,  1.0657e-01,  1.0615e-01,  3.0840e-01,  5.7330e-01],
        [-8.9377e-01,  4.8060e-02, -6.4845e+00,  5.5669e-01, -3.5923e-01,
         -5.9691e-01,  4.7696e-02,  4.8234e-02, -5.8208e-01,  7.5836e-01],
        [-8.0173e+00,  7.9951e-01, -1.2934e+01, -3.8292e+00, -5.1322e-01,
          1.0487e+01,  7.9792e-01,  7.9941e-01,  1.6765e+01,  2.3710e-01],
        [-1.3174e+01,  6.0600e-02,  5.3740e+00,  1.5542e+01, -6.0685e-01,
         -1.0175e+01,  6.0677e-02,  6.0610e-02,  1.8239e+00, -2.2404e-01],
        [-3.5431e+00, -3.0188e-02, -8.2304e-02, -1.9599e+00,  2.1496e-01,
         -7.9330e-01, -3.0223e-02, -3.0186e-02, -5.1007e-01, -1.5589e-01],
        [-4.3552e+01,  2.0517e-01,  4.1050e+00,  6.1175e+00,  1.4757e+00,
         -4.5472e+00,  2.0548e-01,  2.0512e-01,  1.2210e+01,  2.8057e-02],
        [-3.5864e+00, -4.6397e-02,  1.0876e-02, -1.9545e+00,  2.9955e-01,
         -8.1923e-01, -4.6513e-02, -4.6399e-02, -5.2739e-01, -1.9041e-01],
        [ 1.1861e+00,  1.5128e-02, -2.4234e+01, -3.2882e-01,  3.0464e-01,
         -1.4878e+01,  1.4104e-02,  1.5280e-02,  8.0196e-01,  1.4370e+00],
        [ 2.7058e+00, -1.2929e-02,  9.1525e-01, -1.1284e+01, -1.0266e+00,
         -8.8435e+00, -1.3503e-02, -1.2854e-02, -2.0613e-01,  1.7618e+00],
        [-5.4868e+00, -1.0893e-01, -1.0118e+00,  5.6628e+00, -5.8591e+00,
          4.5001e+00, -1.1076e-01, -1.0905e-01,  4.3777e+00, -1.3787e+00]])), ('1.model.hidden_layer_2.bias', tensor([ -3.1012,  -4.7878, -13.4441,  -9.9347,  -3.9917, -13.9742,  -3.8785,
         -5.4041,  -6.1475,  -6.1022])), ('1.model.output_layer_3.weight', tensor([[  4.4998,   5.2407,  -3.0046,  -5.1409,  -0.6420,  13.6732,  -0.7320,
           6.5317,   4.2916,   0.8377],
        [ -4.4998,  -5.2407,   3.0047,   5.1464,   0.6420, -13.6630,   0.7322,
          -6.5316,  -4.2913,  -0.9501]]))])
xi:  [188.32353]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1027.3312764570587
W_T_median: 443.10787401605813
W_T_pctile_5: 189.08693955964665
W_T_CVAR_5_pct: 20.788116822631277
Average q (qsum/M+1):  49.42281218497984
Optimal xi:  [188.32353]
Observed VAR:  443.10787401605813
Expected(across Rb) median(across samples) p_equity:  0.3580169975757599
obj fun:  tensor(-1595.4380, dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -2.2179,  -0.5368],
        [ -2.3361,  -0.6335],
        [ -0.5311,   7.2470],
        [ 18.3118,  -3.3749],
        [ -3.0461,  -1.1329],
        [ -3.5639,  -1.4705],
        [ -4.5593,  -2.1009],
        [-45.9455,  -2.2657],
        [ -7.3396,  12.9378],
        [ -6.1134,  -1.8969]])), ('0.model.hidden_layer_1.bias', tensor([ -4.5377,  -4.5159,  -7.5165, -11.9533,  -4.2881,  -4.0527,  -3.4724,
         -3.2549,  10.0112,  -3.5799])), ('0.model.hidden_layer_2.weight', tensor([[-4.7313e-03, -5.3840e-03, -1.0303e-01, -1.3855e-01, -1.0744e-02,
         -2.0453e-02, -1.0473e-01, -3.5139e-02, -5.6899e-01, -6.2270e-02],
        [-4.7313e-03, -5.3840e-03, -1.0303e-01, -1.3855e-01, -1.0744e-02,
         -2.0453e-02, -1.0473e-01, -3.5139e-02, -5.6899e-01, -6.2270e-02],
        [ 1.5288e+00,  1.6712e+00,  6.4439e+00,  1.6816e+01,  2.3897e+00,
          2.8510e+00,  3.6484e+00,  5.3923e+00, -1.6772e+01,  3.7052e+00],
        [-4.7313e-03, -5.3840e-03, -1.0303e-01, -1.3855e-01, -1.0744e-02,
         -2.0453e-02, -1.0473e-01, -3.5139e-02, -5.6899e-01, -6.2270e-02],
        [-4.7313e-03, -5.3841e-03, -1.0303e-01, -1.3855e-01, -1.0744e-02,
         -2.0453e-02, -1.0473e-01, -3.5138e-02, -5.6899e-01, -6.2270e-02],
        [-4.7313e-03, -5.3841e-03, -1.0303e-01, -1.3855e-01, -1.0744e-02,
         -2.0453e-02, -1.0473e-01, -3.5138e-02, -5.6899e-01, -6.2270e-02],
        [-9.5099e-01, -1.0782e+00, -5.8579e+00, -1.3409e+01, -1.7662e+00,
         -2.2459e+00, -3.1307e+00, -5.0592e+00,  1.3433e+01, -3.2707e+00],
        [-3.6874e-01, -4.8142e-01, -3.5478e+00, -1.1698e+01, -1.2813e+00,
         -1.9924e+00, -3.3302e+00, -4.7743e+00,  8.8089e+00, -3.2709e+00],
        [-4.7313e-03, -5.3840e-03, -1.0303e-01, -1.3855e-01, -1.0744e-02,
         -2.0453e-02, -1.0473e-01, -3.5139e-02, -5.6899e-01, -6.2270e-02],
        [-4.7313e-03, -5.3840e-03, -1.0303e-01, -1.3855e-01, -1.0744e-02,
         -2.0453e-02, -1.0473e-01, -3.5139e-02, -5.6899e-01, -6.2270e-02]])), ('0.model.hidden_layer_2.bias', tensor([-1.7904, -1.7904,  1.9355, -1.7904, -1.7904, -1.7904, -1.4514, -1.5835,
        -1.7904, -1.7904])), ('0.model.output_layer_3.weight', tensor([[-1.3662e-02, -1.3662e-02, -2.4030e+01, -1.3662e-02, -1.3662e-02,
         -1.3662e-02,  1.3710e+01,  7.0183e+00, -1.3662e-02, -1.3662e-02]])), ('1.model.hidden_layer_1.weight', tensor([[ 19.5117,  14.3907],
        [ -2.5644,   0.4779],
        [ -8.3021, -19.4571],
        [-21.2397,   0.9369],
        [-12.0931, -13.6297],
        [-14.0515,  10.9846],
        [ -2.5652,   0.4770],
        [ -2.5644,   0.4778],
        [-13.1529,   0.7819],
        [  7.6098,   0.7808]])), ('1.model.hidden_layer_1.bias', tensor([ 12.8193,  -5.3244, -17.1896,  18.8142, -15.7450,   9.6578,  -5.3236,
         -5.3245,   6.0103, -13.5785])), ('1.model.hidden_layer_2.weight', tensor([[ 2.5085e-01,  1.0614e-01, -7.1542e-01, -5.9082e-02, -1.4498e+01,
         -2.8926e-01,  1.0657e-01,  1.0615e-01,  3.0840e-01,  5.7330e-01],
        [-8.9377e-01,  4.8060e-02, -6.4845e+00,  5.5669e-01, -3.5923e-01,
         -5.9691e-01,  4.7696e-02,  4.8234e-02, -5.8208e-01,  7.5836e-01],
        [-8.0173e+00,  7.9951e-01, -1.2934e+01, -3.8292e+00, -5.1322e-01,
          1.0487e+01,  7.9792e-01,  7.9941e-01,  1.6765e+01,  2.3710e-01],
        [-1.3174e+01,  6.0600e-02,  5.3740e+00,  1.5542e+01, -6.0685e-01,
         -1.0175e+01,  6.0677e-02,  6.0610e-02,  1.8239e+00, -2.2404e-01],
        [-3.5431e+00, -3.0188e-02, -8.2304e-02, -1.9599e+00,  2.1496e-01,
         -7.9330e-01, -3.0223e-02, -3.0186e-02, -5.1007e-01, -1.5589e-01],
        [-4.3552e+01,  2.0517e-01,  4.1050e+00,  6.1175e+00,  1.4757e+00,
         -4.5472e+00,  2.0548e-01,  2.0512e-01,  1.2210e+01,  2.8057e-02],
        [-3.5864e+00, -4.6397e-02,  1.0876e-02, -1.9545e+00,  2.9955e-01,
         -8.1923e-01, -4.6513e-02, -4.6399e-02, -5.2739e-01, -1.9041e-01],
        [ 1.1861e+00,  1.5128e-02, -2.4234e+01, -3.2882e-01,  3.0464e-01,
         -1.4878e+01,  1.4104e-02,  1.5280e-02,  8.0196e-01,  1.4370e+00],
        [ 2.7058e+00, -1.2929e-02,  9.1525e-01, -1.1284e+01, -1.0266e+00,
         -8.8435e+00, -1.3503e-02, -1.2854e-02, -2.0613e-01,  1.7618e+00],
        [-5.4868e+00, -1.0893e-01, -1.0118e+00,  5.6628e+00, -5.8591e+00,
          4.5001e+00, -1.1076e-01, -1.0905e-01,  4.3777e+00, -1.3787e+00]])), ('1.model.hidden_layer_2.bias', tensor([ -3.1012,  -4.7878, -13.4441,  -9.9347,  -3.9917, -13.9742,  -3.8785,
         -5.4041,  -6.1475,  -6.1022])), ('1.model.output_layer_3.weight', tensor([[  4.4998,   5.2407,  -3.0046,  -5.1409,  -0.6420,  13.6732,  -0.7320,
           6.5317,   4.2916,   0.8377],
        [ -4.4998,  -5.2407,   3.0047,   5.1464,   0.6420, -13.6630,   0.7322,
          -6.5316,  -4.2913,  -0.9501]]))])
loaded xi:  188.32353
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1518.318040318632
W_T_median: 1157.8841391994329
W_T_pctile_5: -114.48374804507138
W_T_CVAR_5_pct: -279.1514474953861
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1639.781823507035
Current xi:  [195.61833]
objective value function right now is: -1639.781823507035
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.5188]
objective value function right now is: -1625.8682541230523
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [202.60555]
objective value function right now is: -1637.6166216351053
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.56502]
objective value function right now is: -1630.8516559446339
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.17682]
objective value function right now is: -1635.7517684906618
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.5656]
objective value function right now is: -1624.6682793083503
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [206.20383]
objective value function right now is: -1603.5238572870264
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.9534]
objective value function right now is: -1636.008679155678
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.53584]
objective value function right now is: -1609.3200358237514
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.57925]
objective value function right now is: -1633.732560130996
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.90926]
objective value function right now is: -1636.444711747403
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.34593]
objective value function right now is: -1628.1944911620724
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.03134]
objective value function right now is: -1587.752718824159
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [208.16014]
objective value function right now is: -1627.8633492119275
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.33347]
objective value function right now is: -1635.2487443684913
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.129]
objective value function right now is: -1634.3401222922291
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.73624]
objective value function right now is: -1634.8706358534562
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1639.9857023181253
Current xi:  [206.75256]
objective value function right now is: -1639.9857023181253
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.31221]
objective value function right now is: -1631.0846468308876
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.90482]
objective value function right now is: -1625.5700741791077
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.14107]
objective value function right now is: -1629.4011836294276
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.45293]
objective value function right now is: -1636.879115722765
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.71104]
objective value function right now is: -1624.038581448312
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.51335]
objective value function right now is: -1606.3905128593058
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.59059]
objective value function right now is: -1614.1856908814193
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.9964]
objective value function right now is: -1620.2866298267543
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.2535]
objective value function right now is: -1607.2293942486249
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [205.26305]
objective value function right now is: -1618.027162464571
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [206.1835]
objective value function right now is: -1621.699304081513
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.74559]
objective value function right now is: -1626.4228825438774
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.76013]
objective value function right now is: -1627.7783565864017
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.96857]
objective value function right now is: -1634.158867123878
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.62094]
objective value function right now is: -1625.0424575316724
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.46928]
objective value function right now is: -1626.2141706049495
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.07675]
objective value function right now is: -1417.8195429201312
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.90416]
objective value function right now is: -1580.1240331293222
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.824]
objective value function right now is: -1589.8571618171036
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.2595]
objective value function right now is: -1624.1016703297262
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.95692]
objective value function right now is: -1626.992564383058
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.6506]
objective value function right now is: -1629.8608908382607
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.15382]
objective value function right now is: -1631.4058552598804
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.56859]
objective value function right now is: -1632.6219838843992
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.3591]
objective value function right now is: -1632.7459121790616
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.53812]
objective value function right now is: -1618.8458413293297
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.36095]
objective value function right now is: -1630.993047324132
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.03276]
objective value function right now is: -1632.3373447085241
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.33923]
objective value function right now is: -1635.8336492284382
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.45296]
objective value function right now is: -1634.5242987509184
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.57469]
objective value function right now is: -1637.810262106201
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.6621]
objective value function right now is: -1638.4059993017056
min fval:  -1639.8981299681739
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -1.8962,   0.8885],
        [ -1.9468,   0.8699],
        [ -0.3956,   7.4999],
        [ 19.9361,  -4.2059],
        [ -1.9027,   0.9529],
        [ -2.4937,   0.7088],
        [ -4.9977,  -1.5423],
        [-41.7101,  -3.1514],
        [ -7.4937,  13.4551],
        [ -2.8875,  -0.6494]])), ('0.model.hidden_layer_1.bias', tensor([ -5.6919,  -5.7178,  -9.2120, -12.5505,  -5.6258,  -5.9789,  -5.6479,
         -3.7316,  10.0946,  -5.9226])), ('0.model.hidden_layer_2.weight', tensor([[ 1.1052e-02,  1.0996e-02,  3.9892e-02, -1.3165e-01,  1.1588e-02,
          1.0400e-02, -4.8001e-03, -5.0947e-01, -1.1285e+00,  1.3738e-02],
        [ 1.1052e-02,  1.0996e-02,  3.9892e-02, -1.3165e-01,  1.1587e-02,
          1.0400e-02, -4.8001e-03, -5.0947e-01, -1.1285e+00,  1.3738e-02],
        [-1.8546e-01, -1.9959e-01,  8.2571e+00,  1.8361e+01, -1.5319e-01,
         -3.4019e-01,  2.1614e+00,  5.5035e+00, -1.7277e+01,  1.4333e+00],
        [ 1.1052e-02,  1.0996e-02,  3.9892e-02, -1.3165e-01,  1.1588e-02,
          1.0400e-02, -4.8001e-03, -5.0947e-01, -1.1285e+00,  1.3738e-02],
        [ 1.1052e-02,  1.0996e-02,  3.9892e-02, -1.3165e-01,  1.1588e-02,
          1.0400e-02, -4.8000e-03, -5.0947e-01, -1.1285e+00,  1.3738e-02],
        [ 1.1052e-02,  1.0996e-02,  3.9892e-02, -1.3165e-01,  1.1588e-02,
          1.0400e-02, -4.8001e-03, -5.0947e-01, -1.1285e+00,  1.3738e-02],
        [ 1.7612e-01,  1.9155e-01, -6.8027e+00, -1.4236e+01,  1.7308e-01,
          3.7527e-01, -1.4571e+00, -4.9998e+00,  1.4492e+01, -7.1348e-01],
        [ 3.4193e-03,  1.0541e-02, -3.4057e+00, -9.0286e+00, -9.3851e-03,
          8.4696e-02, -5.0278e-01, -4.3536e+00,  6.2415e+00, -2.9611e-02],
        [ 1.1052e-02,  1.0996e-02,  3.9892e-02, -1.3165e-01,  1.1588e-02,
          1.0400e-02, -4.8001e-03, -5.0947e-01, -1.1285e+00,  1.3738e-02],
        [ 1.1052e-02,  1.0996e-02,  3.9892e-02, -1.3165e-01,  1.1588e-02,
          1.0400e-02, -4.8001e-03, -5.0947e-01, -1.1285e+00,  1.3738e-02]])), ('0.model.hidden_layer_2.bias', tensor([-3.8850, -3.8850,  2.4856, -3.8850, -3.8850, -3.8850, -1.7200, -3.7851,
        -3.8850, -3.8850])), ('0.model.output_layer_3.weight', tensor([[ -0.0927,  -0.0927, -25.2735,  -0.0927,  -0.0927,  -0.0927,  13.4449,
           3.8082,  -0.0927,  -0.0927]])), ('1.model.hidden_layer_1.weight', tensor([[ 2.0932e+01,  1.3429e+01],
        [-2.8629e+00,  1.4875e-01],
        [-6.5019e+00, -1.9099e+01],
        [-2.3468e+01,  2.3691e-01],
        [-1.5084e+01, -1.4383e+01],
        [-1.5432e+01,  1.0814e+01],
        [-2.8922e+00,  3.6766e-03],
        [-2.8609e+00,  1.6828e-01],
        [-1.4716e+01, -9.2113e-01],
        [-2.8751e+00,  1.1281e-01]])), ('1.model.hidden_layer_1.bias', tensor([ 11.0875,  -6.8914, -17.9279,  18.2678, -12.7253,  10.6302,  -6.9453,
         -6.8851,   4.3527,  -6.9015])), ('1.model.hidden_layer_2.weight', tensor([[ 2.2056e-01, -7.6943e-02, -4.1820e-01, -4.4276e-01, -1.9908e+01,
          6.9539e-02, -3.0113e-01, -4.8227e-02, -5.0398e-01, -1.3559e-01],
        [-6.5376e-01,  2.5642e-01, -1.3165e+01,  5.4477e-01,  9.3208e+00,
         -3.2304e-01,  1.5508e-01,  2.6465e-01,  2.1341e-01,  2.3803e-01],
        [-4.7687e+00,  1.5889e-02, -3.9889e-01, -3.2283e+00, -2.0469e-01,
          4.5829e+00,  1.9209e-02,  1.5972e-02,  5.5005e+00,  1.7805e-02],
        [-1.2466e+01, -8.2426e-02,  5.0491e+00,  1.5951e+01,  2.0332e+00,
         -8.2959e+00, -7.6430e-02, -8.3252e-02,  3.0783e+00, -8.0810e-02],
        [-3.1612e+00, -4.3217e-02, -2.9628e+00,  7.4194e-01, -1.9238e+00,
          1.1686e+00, -5.5714e-02, -4.2080e-02,  7.1302e-01, -4.5445e-02],
        [-4.3318e+01, -3.1186e-01,  4.1767e+00,  4.4573e+00,  1.4947e+00,
         -1.4813e+01, -3.1955e-01, -3.1145e-01,  1.4862e+01, -3.1539e-01],
        [-3.2742e+00,  4.0509e-02, -2.8416e+00,  7.0489e-01, -1.9622e+00,
          1.0400e+00,  2.6374e-02,  4.1503e-02,  8.3789e-01,  3.8439e-02],
        [ 1.1338e+00,  1.4838e-01, -3.7526e+01,  6.3333e-03,  1.5273e+00,
         -1.7674e+01,  1.3377e-01,  1.4561e-01, -7.7862e-01,  1.5298e-01],
        [ 2.0867e+00, -1.0803e-01,  1.3481e+00, -1.3045e+01,  6.4246e-03,
         -2.2418e+01, -1.0868e-01, -1.0840e-01, -9.5415e+00, -1.0468e-01],
        [-4.5537e+00,  2.3203e-01, -6.8879e+00,  6.4620e+00, -1.7918e+00,
          1.1835e+00,  3.4012e-01,  2.2075e-01,  2.5552e+00,  2.5772e-01]])), ('1.model.hidden_layer_2.bias', tensor([ -3.1409,  -4.3438,  -9.3798, -11.1797,  -5.0968, -15.8304,  -5.2315,
         -5.4373,  -6.9818,  -4.4518])), ('1.model.output_layer_3.weight', tensor([[  4.8124,   6.9305,   0.5580,  -4.9400,   1.9306,  13.5032,   1.8433,
           7.5487,   4.2789,   0.7895],
        [ -4.8124,  -6.9304,  -0.5579,   4.9477,  -1.9306, -13.4822,  -1.8432,
          -7.5485,  -4.2786,  -0.9013]]))])
xi:  [206.36095]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 773.9341687428387
W_T_median: 509.3915843098899
W_T_pctile_5: 209.52356901682137
W_T_CVAR_5_pct: 26.057494862133385
Average q (qsum/M+1):  48.68419228830645
Optimal xi:  [206.36095]
Observed VAR:  509.3915843098899
Expected(across Rb) median(across samples) p_equity:  0.2917142629623413
obj fun:  tensor(-1639.8981, dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -1.8962,   0.8885],
        [ -1.9468,   0.8699],
        [ -0.3956,   7.4999],
        [ 19.9361,  -4.2059],
        [ -1.9027,   0.9529],
        [ -2.4937,   0.7088],
        [ -4.9977,  -1.5423],
        [-41.7101,  -3.1514],
        [ -7.4937,  13.4551],
        [ -2.8875,  -0.6494]])), ('0.model.hidden_layer_1.bias', tensor([ -5.6919,  -5.7178,  -9.2120, -12.5505,  -5.6258,  -5.9789,  -5.6479,
         -3.7316,  10.0946,  -5.9226])), ('0.model.hidden_layer_2.weight', tensor([[ 1.1052e-02,  1.0996e-02,  3.9892e-02, -1.3165e-01,  1.1588e-02,
          1.0400e-02, -4.8001e-03, -5.0947e-01, -1.1285e+00,  1.3738e-02],
        [ 1.1052e-02,  1.0996e-02,  3.9892e-02, -1.3165e-01,  1.1587e-02,
          1.0400e-02, -4.8001e-03, -5.0947e-01, -1.1285e+00,  1.3738e-02],
        [-1.8546e-01, -1.9959e-01,  8.2571e+00,  1.8361e+01, -1.5319e-01,
         -3.4019e-01,  2.1614e+00,  5.5035e+00, -1.7277e+01,  1.4333e+00],
        [ 1.1052e-02,  1.0996e-02,  3.9892e-02, -1.3165e-01,  1.1588e-02,
          1.0400e-02, -4.8001e-03, -5.0947e-01, -1.1285e+00,  1.3738e-02],
        [ 1.1052e-02,  1.0996e-02,  3.9892e-02, -1.3165e-01,  1.1588e-02,
          1.0400e-02, -4.8000e-03, -5.0947e-01, -1.1285e+00,  1.3738e-02],
        [ 1.1052e-02,  1.0996e-02,  3.9892e-02, -1.3165e-01,  1.1588e-02,
          1.0400e-02, -4.8001e-03, -5.0947e-01, -1.1285e+00,  1.3738e-02],
        [ 1.7612e-01,  1.9155e-01, -6.8027e+00, -1.4236e+01,  1.7308e-01,
          3.7527e-01, -1.4571e+00, -4.9998e+00,  1.4492e+01, -7.1348e-01],
        [ 3.4193e-03,  1.0541e-02, -3.4057e+00, -9.0286e+00, -9.3851e-03,
          8.4696e-02, -5.0278e-01, -4.3536e+00,  6.2415e+00, -2.9611e-02],
        [ 1.1052e-02,  1.0996e-02,  3.9892e-02, -1.3165e-01,  1.1588e-02,
          1.0400e-02, -4.8001e-03, -5.0947e-01, -1.1285e+00,  1.3738e-02],
        [ 1.1052e-02,  1.0996e-02,  3.9892e-02, -1.3165e-01,  1.1588e-02,
          1.0400e-02, -4.8001e-03, -5.0947e-01, -1.1285e+00,  1.3738e-02]])), ('0.model.hidden_layer_2.bias', tensor([-3.8850, -3.8850,  2.4856, -3.8850, -3.8850, -3.8850, -1.7200, -3.7851,
        -3.8850, -3.8850])), ('0.model.output_layer_3.weight', tensor([[ -0.0927,  -0.0927, -25.2735,  -0.0927,  -0.0927,  -0.0927,  13.4449,
           3.8082,  -0.0927,  -0.0927]])), ('1.model.hidden_layer_1.weight', tensor([[ 2.0932e+01,  1.3429e+01],
        [-2.8629e+00,  1.4875e-01],
        [-6.5019e+00, -1.9099e+01],
        [-2.3468e+01,  2.3691e-01],
        [-1.5084e+01, -1.4383e+01],
        [-1.5432e+01,  1.0814e+01],
        [-2.8922e+00,  3.6766e-03],
        [-2.8609e+00,  1.6828e-01],
        [-1.4716e+01, -9.2113e-01],
        [-2.8751e+00,  1.1281e-01]])), ('1.model.hidden_layer_1.bias', tensor([ 11.0875,  -6.8914, -17.9279,  18.2678, -12.7253,  10.6302,  -6.9453,
         -6.8851,   4.3527,  -6.9015])), ('1.model.hidden_layer_2.weight', tensor([[ 2.2056e-01, -7.6943e-02, -4.1820e-01, -4.4276e-01, -1.9908e+01,
          6.9539e-02, -3.0113e-01, -4.8227e-02, -5.0398e-01, -1.3559e-01],
        [-6.5376e-01,  2.5642e-01, -1.3165e+01,  5.4477e-01,  9.3208e+00,
         -3.2304e-01,  1.5508e-01,  2.6465e-01,  2.1341e-01,  2.3803e-01],
        [-4.7687e+00,  1.5889e-02, -3.9889e-01, -3.2283e+00, -2.0469e-01,
          4.5829e+00,  1.9209e-02,  1.5972e-02,  5.5005e+00,  1.7805e-02],
        [-1.2466e+01, -8.2426e-02,  5.0491e+00,  1.5951e+01,  2.0332e+00,
         -8.2959e+00, -7.6430e-02, -8.3252e-02,  3.0783e+00, -8.0810e-02],
        [-3.1612e+00, -4.3217e-02, -2.9628e+00,  7.4194e-01, -1.9238e+00,
          1.1686e+00, -5.5714e-02, -4.2080e-02,  7.1302e-01, -4.5445e-02],
        [-4.3318e+01, -3.1186e-01,  4.1767e+00,  4.4573e+00,  1.4947e+00,
         -1.4813e+01, -3.1955e-01, -3.1145e-01,  1.4862e+01, -3.1539e-01],
        [-3.2742e+00,  4.0509e-02, -2.8416e+00,  7.0489e-01, -1.9622e+00,
          1.0400e+00,  2.6374e-02,  4.1503e-02,  8.3789e-01,  3.8439e-02],
        [ 1.1338e+00,  1.4838e-01, -3.7526e+01,  6.3333e-03,  1.5273e+00,
         -1.7674e+01,  1.3377e-01,  1.4561e-01, -7.7862e-01,  1.5298e-01],
        [ 2.0867e+00, -1.0803e-01,  1.3481e+00, -1.3045e+01,  6.4246e-03,
         -2.2418e+01, -1.0868e-01, -1.0840e-01, -9.5415e+00, -1.0468e-01],
        [-4.5537e+00,  2.3203e-01, -6.8879e+00,  6.4620e+00, -1.7918e+00,
          1.1835e+00,  3.4012e-01,  2.2075e-01,  2.5552e+00,  2.5772e-01]])), ('1.model.hidden_layer_2.bias', tensor([ -3.1409,  -4.3438,  -9.3798, -11.1797,  -5.0968, -15.8304,  -5.2315,
         -5.4373,  -6.9818,  -4.4518])), ('1.model.output_layer_3.weight', tensor([[  4.8124,   6.9305,   0.5580,  -4.9400,   1.9306,  13.5032,   1.8433,
           7.5487,   4.2789,   0.7895],
        [ -4.8124,  -6.9304,  -0.5579,   4.9477,  -1.9306, -13.4822,  -1.8432,
          -7.5485,  -4.2786,  -0.9013]]))])
loaded xi:  206.36095
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1518.318040318632
W_T_median: 1157.8841391994329
W_T_pctile_5: -114.48374804507138
W_T_CVAR_5_pct: -279.1514474953861
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -3011.7143331458847
Current xi:  [216.39964]
objective value function right now is: -3011.7143331458847
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -3020.3303518508806
Current xi:  [223.51532]
objective value function right now is: -3020.3303518508806
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -3069.3196402485537
Current xi:  [228.9057]
objective value function right now is: -3069.3196402485537
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [229.0923]
objective value function right now is: -2914.388766870642
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [231.8182]
objective value function right now is: -3068.165221806243
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [233.11612]
objective value function right now is: -2819.0973649392085
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [233.80518]
objective value function right now is: -3036.2410953051017
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [233.31099]
objective value function right now is: -2937.7203251243145
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [234.12726]
objective value function right now is: -2856.7631027510815
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [233.15099]
objective value function right now is: -2876.6476802028565
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [232.11493]
objective value function right now is: -3058.6187097092893
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -3139.085301447642
Current xi:  [233.21664]
objective value function right now is: -3139.085301447642
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [234.40634]
objective value function right now is: -3021.730477276355
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [234.39162]
objective value function right now is: -3024.4199166697467
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [234.0908]
objective value function right now is: -3084.48846163796
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [234.67593]
objective value function right now is: -3024.1316826806515
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [233.75641]
objective value function right now is: -3091.9521003431223
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [234.39142]
objective value function right now is: -3048.2776045444402
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [234.06912]
objective value function right now is: -2937.999858895199
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [234.08841]
objective value function right now is: -3108.1575931568386
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [233.58736]
objective value function right now is: -2897.6418701622365
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [232.67282]
objective value function right now is: -3074.5352176004394
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [233.55647]
objective value function right now is: -2855.4332081091984
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [232.74515]
objective value function right now is: -3093.0550899679315
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [232.12534]
objective value function right now is: -3138.0714990555416
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [231.22623]
objective value function right now is: -3057.3060578385143
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [232.80125]
objective value function right now is: -2983.667622970475
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [233.81438]
objective value function right now is: -2817.372473655603
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [235.51332]
objective value function right now is: -3114.4694423144942
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [235.29086]
objective value function right now is: -2852.893279212919
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [235.85335]
objective value function right now is: -3072.9509319895656
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [237.1327]
objective value function right now is: -3115.4158996076017
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [235.57167]
objective value function right now is: -3090.742631388951
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [236.58214]
objective value function right now is: -3065.112097061324
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -3151.873560269054
Current xi:  [234.37971]
objective value function right now is: -3151.873560269054
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -3153.6922411969863
Current xi:  [235.20633]
objective value function right now is: -3153.6922411969863
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -3161.818989812211
Current xi:  [235.26791]
objective value function right now is: -3161.818989812211
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -3170.3349132844883
Current xi:  [235.88768]
objective value function right now is: -3170.3349132844883
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -3187.2152556100605
Current xi:  [236.19493]
objective value function right now is: -3187.2152556100605
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [236.36267]
objective value function right now is: -3158.5482507841334
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [236.36778]
objective value function right now is: -3187.0126346012216
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [236.21234]
objective value function right now is: -3184.997810058892
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [236.36046]
objective value function right now is: -3149.9459488148746
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [236.39523]
objective value function right now is: -3171.8003390332424
new min fval from sgd:  -3187.7552626594743
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [237.06575]
objective value function right now is: -3187.7552626594743
new min fval from sgd:  -3187.8007371272183
new min fval from sgd:  -3187.9504471473538
new min fval from sgd:  -3189.371778118468
new min fval from sgd:  -3192.4018417802918
new min fval from sgd:  -3195.265534313902
new min fval from sgd:  -3197.1747243737786
new min fval from sgd:  -3198.6968184769394
new min fval from sgd:  -3199.8380470531847
new min fval from sgd:  -3200.599966056471
new min fval from sgd:  -3201.0830425287204
new min fval from sgd:  -3201.548641907631
new min fval from sgd:  -3201.737552013308
new min fval from sgd:  -3201.8567243447956
new min fval from sgd:  -3201.8719783242614
new min fval from sgd:  -3201.931268446436
new min fval from sgd:  -3202.027358514271
new min fval from sgd:  -3202.468872765471
new min fval from sgd:  -3202.8200016467586
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [236.45801]
objective value function right now is: -3183.2331679361278
new min fval from sgd:  -3203.023551769694
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [236.5283]
objective value function right now is: -3197.9877073144253
new min fval from sgd:  -3203.6126839219396
new min fval from sgd:  -3204.07859905693
new min fval from sgd:  -3204.7234288245177
new min fval from sgd:  -3204.873069667165
new min fval from sgd:  -3204.991533453728
new min fval from sgd:  -3205.04372824604
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [236.45789]
objective value function right now is: -3190.6641724988585
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [236.63263]
objective value function right now is: -3196.2211736988324
new min fval from sgd:  -3205.161949542353
new min fval from sgd:  -3205.2160422187676
new min fval from sgd:  -3205.242564312511
new min fval from sgd:  -3205.275916339315
new min fval from sgd:  -3205.290428409334
new min fval from sgd:  -3205.517256182358
new min fval from sgd:  -3205.6863314553807
new min fval from sgd:  -3205.7699159413733
new min fval from sgd:  -3205.875374771352
new min fval from sgd:  -3205.9018053537116
new min fval from sgd:  -3205.9772207013657
new min fval from sgd:  -3206.1938136058943
new min fval from sgd:  -3206.301058644777
new min fval from sgd:  -3206.350801557775
new min fval from sgd:  -3206.387198479023
new min fval from sgd:  -3206.426624220308
new min fval from sgd:  -3206.4909614137596
new min fval from sgd:  -3206.5086522394054
new min fval from sgd:  -3206.5112883086704
new min fval from sgd:  -3206.532273521999
new min fval from sgd:  -3206.5416984646254
new min fval from sgd:  -3206.582232072218
new min fval from sgd:  -3206.7081985792997
new min fval from sgd:  -3206.82510280159
new min fval from sgd:  -3206.9817419047326
new min fval from sgd:  -3207.04923109423
new min fval from sgd:  -3207.187619118224
new min fval from sgd:  -3207.3891883459137
new min fval from sgd:  -3207.5654544490176
new min fval from sgd:  -3207.8062925774457
new min fval from sgd:  -3207.9865901149456
new min fval from sgd:  -3208.108308752953
new min fval from sgd:  -3208.183834045845
new min fval from sgd:  -3208.2664199767055
new min fval from sgd:  -3208.327592146123
new min fval from sgd:  -3208.426237613174
new min fval from sgd:  -3208.513565230754
new min fval from sgd:  -3208.6086952531514
new min fval from sgd:  -3208.6446520819245
new min fval from sgd:  -3208.672458651283
new min fval from sgd:  -3208.6837551772387
new min fval from sgd:  -3208.718252726694
new min fval from sgd:  -3208.894893761241
new min fval from sgd:  -3209.080805645061
new min fval from sgd:  -3209.1867246689335
new min fval from sgd:  -3209.243848505512
new min fval from sgd:  -3209.270850494452
new min fval from sgd:  -3209.279727274507
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [236.55632]
objective value function right now is: -3209.2702534013474
min fval:  -3209.279727274507
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -1.0133,   0.2215],
        [ -1.0134,   0.2216],
        [ -0.4955,   6.0374],
        [ 23.6806,  -3.6386],
        [ -1.0133,   0.2215],
        [ -1.0133,   0.2215],
        [ -1.0134,   0.2216],
        [-30.2162,  -5.7676],
        [-13.3266,  13.0983],
        [ -1.0134,   0.2216]])), ('0.model.hidden_layer_1.bias', tensor([ -4.6582,  -4.6583, -13.0889, -12.8214,  -4.6582,  -4.6581,  -4.6583,
         -4.8503,   9.6628,  -4.6583])), ('0.model.hidden_layer_2.weight', tensor([[ 1.2780e-02,  1.2781e-02, -1.5443e-01,  4.0430e-02,  1.2780e-02,
          1.2780e-02,  1.2781e-02, -1.1376e-01, -6.6382e-01,  1.2781e-02],
        [ 1.2780e-02,  1.2781e-02, -1.5443e-01,  4.0430e-02,  1.2780e-02,
          1.2780e-02,  1.2781e-02, -1.1376e-01, -6.6382e-01,  1.2781e-02],
        [ 8.7273e-03,  8.7905e-03,  1.4532e+01,  1.9971e+01,  8.7197e-03,
          8.6245e-03,  8.8003e-03,  4.2361e+00, -1.8282e+01,  8.8015e-03],
        [ 1.2780e-02,  1.2781e-02, -1.5443e-01,  4.0430e-02,  1.2780e-02,
          1.2780e-02,  1.2781e-02, -1.1376e-01, -6.6382e-01,  1.2781e-02],
        [ 1.2780e-02,  1.2781e-02, -1.5443e-01,  4.0430e-02,  1.2780e-02,
          1.2780e-02,  1.2781e-02, -1.1376e-01, -6.6382e-01,  1.2781e-02],
        [ 1.2780e-02,  1.2781e-02, -1.5443e-01,  4.0430e-02,  1.2780e-02,
          1.2780e-02,  1.2781e-02, -1.1376e-01, -6.6382e-01,  1.2781e-02],
        [-1.9378e-01, -1.9357e-01, -1.2134e+01, -1.5920e+01, -1.9380e-01,
         -1.9410e-01, -1.9354e-01, -4.1535e+00,  1.5535e+01, -1.9354e-01],
        [ 1.2780e-02,  1.2780e-02, -1.5443e-01,  4.0417e-02,  1.2780e-02,
          1.2779e-02,  1.2780e-02, -1.1376e-01, -6.6380e-01,  1.2780e-02],
        [ 1.2780e-02,  1.2781e-02, -1.5443e-01,  4.0430e-02,  1.2780e-02,
          1.2780e-02,  1.2781e-02, -1.1376e-01, -6.6382e-01,  1.2781e-02],
        [ 1.2780e-02,  1.2781e-02, -1.5443e-01,  4.0430e-02,  1.2780e-02,
          1.2780e-02,  1.2781e-02, -1.1376e-01, -6.6382e-01,  1.2781e-02]])), ('0.model.hidden_layer_2.bias', tensor([-2.9440, -2.9440,  2.6362, -2.9440, -2.9440, -2.9440, -1.5391, -2.9440,
        -2.9440, -2.9440])), ('0.model.output_layer_3.weight', tensor([[ -0.0391,  -0.0391, -27.9279,  -0.0391,  -0.0391,  -0.0391,  13.6829,
          -0.0391,  -0.0391,  -0.0391]])), ('1.model.hidden_layer_1.weight', tensor([[ 21.5911,  13.3273],
        [ -3.0337,   0.1778],
        [ -2.9615, -19.0886],
        [-24.0049,   0.0631],
        [-16.8081, -14.7095],
        [-12.1107,  13.0744],
        [ -3.0336,   0.1777],
        [ -3.0340,   0.1778],
        [-16.1331,  -2.6769],
        [ -3.0337,   0.1778]])), ('1.model.hidden_layer_1.bias', tensor([ 10.7416,  -7.4434, -19.1229,  20.3186, -11.9012,  12.2163,  -7.4434,
         -7.4432,   3.3989,  -7.4434])), ('1.model.hidden_layer_2.weight', tensor([[ 5.5614e-02, -6.0472e-02, -1.7755e-02, -3.1374e-01, -3.4433e+01,
         -3.9594e-01, -6.0512e-02, -6.0350e-02, -4.7966e-01, -6.0464e-02],
        [-6.0729e-01,  1.5433e-01, -2.0088e+01,  7.7637e-01,  2.3514e+00,
          5.3500e-01,  1.5436e-01,  1.5424e-01,  5.2129e-01,  1.5433e-01],
        [-4.0903e+00, -1.5389e-03, -1.2668e+00, -9.8427e-01, -1.3857e+00,
         -6.5813e-01, -1.5390e-03, -1.5387e-03,  4.0008e-02, -1.5389e-03],
        [-1.1260e+01,  2.5419e-01,  5.0672e+00,  1.5978e+01,  2.2370e+00,
         -8.5082e+00,  2.5418e-01,  2.5422e-01,  3.9151e+00,  2.5419e-01],
        [-2.4097e+00,  2.3306e-01, -8.7243e-01,  3.7386e-01, -6.5676e+00,
         -2.2921e-01,  2.3298e-01,  2.3329e-01,  2.2998e+00,  2.3307e-01],
        [-5.2311e+01, -3.1976e-01,  6.4659e+00,  2.0193e+00,  7.2747e-01,
         -1.5445e+01, -3.1969e-01, -3.1996e-01,  1.9104e+01, -3.1977e-01],
        [-2.5081e+00,  1.9549e-01, -8.8973e-01,  3.7654e-01, -6.5825e+00,
         -2.8781e-01,  1.9545e-01,  1.9561e-01,  2.3763e+00,  1.9550e-01],
        [ 1.2083e+00,  3.1547e-01, -3.1787e+01, -6.9683e+00, -1.4808e-02,
         -4.1054e+00,  3.1548e-01,  3.1547e-01, -4.5888e-01,  3.1547e-01],
        [ 3.3397e+00, -2.4130e-01,  1.7390e+00, -2.3684e+01, -5.7918e-02,
         -7.8839e+00, -2.4129e-01, -2.4132e-01, -2.2807e+01, -2.4130e-01],
        [-3.4256e+00, -1.5098e-01, -2.1836e+00,  1.2343e+00, -4.8054e+00,
          9.4388e-01, -1.5092e-01, -1.5106e-01,  6.1634e-01, -1.5099e-01]])), ('1.model.hidden_layer_2.bias', tensor([ -3.3335,  -3.7097,  -6.1191, -12.7589,  -4.2958, -21.1445,  -4.4647,
         -5.3381,  -7.4170,  -2.7609])), ('1.model.output_layer_3.weight', tensor([[  5.7223,   7.8469,   0.1013,  -4.8224,   7.0181,  18.5480,   6.8534,
           9.4680,   5.7943,   3.6896],
        [ -5.7223,  -7.8469,  -0.1012,   4.8390,  -7.0181, -18.5203,  -6.8533,
          -9.4679,  -5.7940,  -3.8009]]))])
xi:  [236.55698]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 816.5997904101869
W_T_median: 591.1369466419576
W_T_pctile_5: 237.38335154716643
W_T_CVAR_5_pct: 35.68763146815354
Average q (qsum/M+1):  45.94369408392137
Optimal xi:  [236.55698]
Observed VAR:  591.1369466419576
Expected(across Rb) median(across samples) p_equity:  0.24166680624087652
obj fun:  tensor(-3209.2797, dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
