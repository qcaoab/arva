Starting at: 
10-02-23_06:47

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.1, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1485.9305216604191
Current xi:  [99.6002]
objective value function right now is: -1485.9305216604191
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1554.8039195912754
Current xi:  [77.18428]
objective value function right now is: -1554.8039195912754
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1557.4679061197405
Current xi:  [68.4885]
objective value function right now is: -1557.4679061197405
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [63.483322]
objective value function right now is: -1553.5931888538003
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [63.01314]
objective value function right now is: -1554.5924210275566
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [61.985188]
objective value function right now is: -1506.069046433221
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [53.836918]
objective value function right now is: -1543.8055089739323
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.345917]
objective value function right now is: -1547.7792442006353
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.927567]
objective value function right now is: -1553.6434040429226
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.42033]
objective value function right now is: -1554.2128358358386
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.107258]
objective value function right now is: -1555.8871823412646
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.030167]
objective value function right now is: -1552.4868103391404
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.22962]
objective value function right now is: -1513.8406499460025
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [55.147285]
objective value function right now is: -1555.6795195398367
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.176476]
objective value function right now is: -1552.4659583602454
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.298088]
objective value function right now is: -1554.803225814003
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.47997]
objective value function right now is: -1547.7208867162062
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.551943499254
Current xi:  [55.860863]
objective value function right now is: -1562.551943499254
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.93552]
objective value function right now is: -1552.765894647838
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.74764]
objective value function right now is: -1556.1465139067977
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.513847]
objective value function right now is: -1552.9382816806124
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.17199]
objective value function right now is: -1551.811838755081
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.44205]
objective value function right now is: -1555.5877359666688
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.59335]
objective value function right now is: -1557.1382959131824
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.36419]
objective value function right now is: -1550.775604141588
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.539406]
objective value function right now is: -1555.8720548730698
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.58907]
objective value function right now is: -1548.5058922028338
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [54.095814]
objective value function right now is: -1540.0416329626291
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [50.00944]
objective value function right now is: -1549.8333665620166
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [50.07514]
objective value function right now is: -1555.0465390782267
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [49.558308]
objective value function right now is: -1550.7933584997645
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [47.993877]
objective value function right now is: -1560.3927199763996
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [34.56438]
objective value function right now is: -1456.553541400975
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [21.604136]
objective value function right now is: -1533.6757400560332
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [20.5418]
objective value function right now is: -1552.9455832415308
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [21.763224]
objective value function right now is: -1559.820426782035
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [23.334526]
objective value function right now is: -1561.3373826375148
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [24.99433]
objective value function right now is: -1561.224117220122
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [26.644789]
objective value function right now is: -1561.2712484435579
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [28.462023]
objective value function right now is: -1561.7341949483014
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [29.938631]
objective value function right now is: -1562.151763773671
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.6618140469343
Current xi:  [31.535269]
objective value function right now is: -1562.6618140469343
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [32.99404]
objective value function right now is: -1562.3037805949134
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [34.807365]
objective value function right now is: -1562.5916090852745
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [36.102932]
objective value function right now is: -1562.439246695841
new min fval from sgd:  -1562.6735612350487
new min fval from sgd:  -1562.775068792267
new min fval from sgd:  -1562.8700632248867
new min fval from sgd:  -1562.9971672852523
new min fval from sgd:  -1563.0134544108203
new min fval from sgd:  -1563.0565511196246
new min fval from sgd:  -1563.176233640171
new min fval from sgd:  -1563.189511679196
new min fval from sgd:  -1563.2096734477068
new min fval from sgd:  -1563.247004691989
new min fval from sgd:  -1563.3753980956064
new min fval from sgd:  -1563.4455179672693
new min fval from sgd:  -1563.4705099298787
new min fval from sgd:  -1563.5742178395674
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [37.58334]
objective value function right now is: -1562.7580677964297
new min fval from sgd:  -1563.7412064736886
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [38.97227]
objective value function right now is: -1562.9669034248132
new min fval from sgd:  -1563.863916832117
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [39.901787]
objective value function right now is: -1563.0181486797624
new min fval from sgd:  -1563.8733026002287
new min fval from sgd:  -1563.875772651256
new min fval from sgd:  -1563.8863389099101
new min fval from sgd:  -1563.901342091886
new min fval from sgd:  -1563.910764005125
new min fval from sgd:  -1563.946038103466
new min fval from sgd:  -1563.9484280182132
new min fval from sgd:  -1563.9581286372368
new min fval from sgd:  -1563.97549758829
new min fval from sgd:  -1563.9949634163563
new min fval from sgd:  -1564.018181162829
new min fval from sgd:  -1564.0313080977644
new min fval from sgd:  -1564.0352771458477
new min fval from sgd:  -1564.0475719543551
new min fval from sgd:  -1564.0634436020625
new min fval from sgd:  -1564.0806401656605
new min fval from sgd:  -1564.0882867822859
new min fval from sgd:  -1564.1320682379435
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [40.430088]
objective value function right now is: -1563.6487652516248
new min fval from sgd:  -1564.1387089789039
new min fval from sgd:  -1564.1423985177812
new min fval from sgd:  -1564.1519825118887
new min fval from sgd:  -1564.1776222411956
new min fval from sgd:  -1564.1890665803146
new min fval from sgd:  -1564.1901846630929
new min fval from sgd:  -1564.1950214364947
new min fval from sgd:  -1564.1985019204203
new min fval from sgd:  -1564.1987326733554
new min fval from sgd:  -1564.2035633349358
new min fval from sgd:  -1564.2042986401957
new min fval from sgd:  -1564.2173718987756
new min fval from sgd:  -1564.23229976806
new min fval from sgd:  -1564.233890725679
new min fval from sgd:  -1564.2406197857172
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [40.663265]
objective value function right now is: -1563.9674959748406
min fval:  -1564.2406197857172
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 11.5266,   7.7696],
        [ -0.9333,   1.3816],
        [ 16.3317,  -5.6050],
        [ -0.9333,   1.3816],
        [ -0.9333,   1.3816],
        [  1.5476,  -1.8289],
        [ -1.0630,  14.6694],
        [ -0.9324,   1.3813],
        [-53.2942,  -8.0439],
        [ -0.9330,   1.3816]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-12.9429,  -2.8837, -13.3912,  -2.8837,  -2.8837,   4.5213,  11.5816,
         -2.8836,  -7.4315,  -2.8836], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-6.2735e-02, -1.3979e-02, -9.3228e-02, -1.3979e-02, -1.3979e-02,
         -1.0948e+00, -3.2649e-01, -1.3983e-02, -1.5224e-01, -1.3980e-02],
        [ 1.2731e-02,  2.9162e-02,  1.7974e-01,  2.9162e-02,  2.9162e-02,
          2.3665e+00,  1.0105e+00,  2.9168e-02,  2.3867e-01,  2.9164e-02],
        [-6.2735e-02, -1.3979e-02, -9.3228e-02, -1.3979e-02, -1.3979e-02,
         -1.0948e+00, -3.2649e-01, -1.3983e-02, -1.5224e-01, -1.3980e-02],
        [ 1.0196e+01,  7.6240e-02,  1.9937e+01,  7.6243e-02,  7.6241e-02,
          2.4707e+00, -2.2523e+01,  7.5206e-02,  5.8008e+00,  7.5941e-02],
        [ 1.5076e-02,  3.1906e-02,  1.8168e-01,  3.1906e-02,  3.1906e-02,
          2.4871e+00,  1.0510e+00,  3.1913e-02,  2.7087e-01,  3.1908e-02],
        [ 1.2002e-02,  2.8131e-02,  1.7893e-01,  2.8131e-02,  2.8131e-02,
          2.3175e+00,  9.9409e-01,  2.8137e-02,  2.2514e-01,  2.8133e-02],
        [ 8.0607e+00,  1.5089e-01,  1.8653e+01,  1.5089e-01,  1.5089e-01,
          2.1826e+00, -2.1546e+01,  1.5024e-01,  5.1415e+00,  1.5071e-01],
        [ 9.8917e-02, -1.4642e-02,  4.1208e-02, -1.4642e-02, -1.4642e-02,
         -1.1420e+00, -2.7317e+00, -1.4652e-02, -1.7219e-01, -1.4645e-02],
        [ 1.3038e-02,  2.9558e-02,  1.8002e-01,  2.9558e-02,  2.9558e-02,
          2.3847e+00,  1.0166e+00,  2.9565e-02,  2.4365e-01,  2.9560e-02],
        [ 1.0582e+01, -4.4497e-02,  2.0786e+01, -4.4499e-02, -4.4498e-02,
          3.5395e+00, -2.2404e+01, -4.3536e-02,  7.6421e+00, -4.4221e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.0981,  2.3810, -1.0981,  1.4822,  2.5031,  2.3315,  1.4043, -1.6755,
         2.3995,  2.6023], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.0781e-03,  2.8190e+00,  1.0781e-03, -1.0988e+01,  3.7583e+00,
          2.5131e+00, -9.2454e+00, -4.4052e-01,  2.9434e+00, -1.2151e+01]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 21.6924,   3.1167],
        [ -4.4813,   2.3241],
        [-18.8873,  -5.7886],
        [ 18.1455,  -2.1123],
        [ 17.6860,  11.9605],
        [ 20.4956,  10.6355],
        [ -2.3563,   1.9208],
        [ 11.0977,  17.9594],
        [ 20.0680,   0.9382],
        [  7.3047,  -1.3251]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -6.5171,  -2.9352,  -0.9824, -16.9308,   9.1785,   3.0355,  -4.0981,
         14.8856,  -8.3155,  -0.6965], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -4.8318,  -1.6915,  10.7010,  17.6716,   5.1407,   4.7985,  -1.1879,
          19.0471,   3.1775,  -4.9879],
        [ -0.4125,   0.1389,   3.6639,   1.5832,  -3.0046,  -0.7897,  -1.0027,
          -2.3374,  -1.1829,   1.1269],
        [ -1.1895,  -0.1873,  -0.7344,   0.8176,  -2.8834,  -0.9814,  -0.1899,
          -0.5189,  -1.0159,   1.2913],
        [-11.9928,   0.2989,  12.8994,  -9.1476, -16.4812,  -0.8842,   0.0778,
         -26.4104,   4.5766,   5.8410],
        [ -2.1630,  -0.0328,  -1.0312,  -0.1381,  -1.8944,  -0.4742,  -0.1596,
          -1.1249,  -0.4216,  -1.5412],
        [ -2.0991,  -0.9514,  -3.8377,  -3.0515,  -2.3297,   2.5730,  -1.9140,
           0.5278,  -2.4474,   1.1329],
        [ -0.8636,  -0.1350,  -0.6322,  -0.2197,  -1.6551,  -0.6179,  -0.1187,
          -0.9354,  -0.9872,  -1.4004],
        [ -2.8304,  -0.3450,   2.7702,  -6.3066,  -1.5038,  -5.1198,  -0.5201,
         -13.8543,   1.9222,   9.9740],
        [ -0.5968,  -0.3347,  -0.6837,   0.5475,  -2.4554,  -1.1305,  -0.3323,
          -0.9676,  -0.7666,  -1.5080],
        [ -1.9058,  -0.0741,  -0.8067,  -0.3278,  -1.8707,  -0.5164,  -0.0577,
          -1.2734,  -0.5746,   0.2286]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-15.5928,  -2.1520,  -4.2229,  -7.5761,  -1.6178,  -3.4586,  -2.2477,
         -5.3127,  -2.3869,  -3.2489], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 3.9212e-01,  1.4128e+00,  4.1917e-01,  1.1578e+01,  3.4906e-02,
          1.4705e+00, -5.7978e-03, -7.2452e+00,  2.9202e-01,  2.6065e-02],
        [-2.2644e-01, -1.4127e+00, -4.1560e-01, -1.1566e+01, -2.6695e-02,
         -1.4706e+00,  4.4362e-03,  7.2490e+00, -2.9197e-01, -2.5538e-02]],
       device='cuda:0'))])
xi:  [40.559574]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 620.2772031907623
W_T_median: 258.97622821014227
W_T_pctile_5: 40.56753191893496
W_T_CVAR_5_pct: -51.753973396797946
Average q (qsum/M+1):  52.12887474798387
Optimal xi:  [40.559574]
Observed VAR:  258.97622821014227
Expected(across Rb) median(across samples) p_equity:  0.33781611571709314
obj fun:  tensor(-1564.2406, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
