Starting at: 
10-02-23_15:44

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  9.8056,   0.3702],
        [ -4.3465,   8.7513],
        [ 11.6827,  -4.5826],
        [  0.3149,  11.8025],
        [ -0.7516,   1.0731],
        [ -0.7516,   1.0731],
        [-33.7974,  -9.1431],
        [  5.3946,  -4.5009],
        [ -0.7516,   1.0731],
        [ 10.9840,   1.5848]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -8.5826,   5.8637,  -9.1605,   9.5059,  -2.6000,  -2.6001,  -8.5071,
          1.6323,  -2.6001, -10.2237], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [ 5.2973e+00, -1.0926e+01,  1.1810e+01, -1.5601e+01, -4.0905e-02,
         -4.0919e-02,  1.3139e+01,  2.8679e+00, -4.0979e-02,  1.7610e+01],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-1.6569e-01,  3.5122e+00,  1.2751e+00,  1.6088e+01, -9.9319e-02,
         -9.9316e-02,  1.2755e+01, -4.1160e+00, -9.9300e-02, -6.4273e-02],
        [ 4.1958e+00, -1.0778e+01,  1.2722e+01, -1.4714e+01,  8.3874e-02,
          8.3891e-02,  1.2004e+01,  3.9389e+00,  8.3961e-02,  1.5171e+01],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.7286, -0.7286, -0.7286,  1.5126, -0.7286, -0.7286, -0.7286, -2.8475,
         2.4285, -0.7286], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 8.8609e-03,  8.8609e-03,  8.8609e-03, -1.4825e+01,  8.8609e-03,
          8.8609e-03,  8.8609e-03,  1.2082e+01, -1.0963e+01,  8.8609e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 17.5169,   2.4047],
        [-11.9075,  -5.4174],
        [ -2.9792,  -0.9803],
        [ -9.7518,  -4.0833],
        [ 11.3921,   5.1641],
        [ 13.7315,  -0.0707],
        [ -8.8519,  10.6386],
        [  7.6684,  14.1374],
        [-16.4605,  -4.5943],
        [ 14.0679,   5.0107]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-10.0398,  -4.7415,  -4.9321,  -4.8020,   4.9201, -12.9012,  10.5438,
         11.8370,   0.8090,  -3.9403], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.8323e+00,  4.5980e+00,  3.9185e-01,  3.6615e+00, -2.6056e+00,
         -5.2731e-02, -2.9941e+00,  7.1657e-01,  1.9169e+00, -3.9848e+00],
        [ 9.5454e+00,  1.7276e+00, -6.8431e-02,  9.5238e-03, -2.5478e+00,
          5.6171e+00,  1.4304e+01,  1.7821e+01, -1.7394e+00,  4.0096e+00],
        [-6.9749e-01, -1.6425e-01,  1.4400e-02, -9.4509e-02, -2.5979e+00,
         -2.4345e-01, -1.8580e-01, -1.0674e+00, -4.0907e-01, -8.7679e-01],
        [-1.5408e+00,  1.3537e+00,  1.1014e-01,  6.5129e-01, -2.6476e+00,
         -2.6944e-01, -5.7968e+00,  1.5919e+00,  1.8653e+00, -2.8750e+00],
        [ 1.5086e+00,  9.7123e+00,  5.2281e-01,  4.8078e+00, -1.4468e+01,
          2.2508e+00,  6.0173e-01, -3.3019e+01,  1.0199e+01, -2.1604e+00],
        [-3.3438e+00,  1.9099e+00, -1.5542e-01,  1.1551e+00, -1.7673e+00,
         -7.1039e+00,  1.1353e+01,  2.7938e+00, -3.5859e+00, -3.1004e+00],
        [-5.8361e+00,  4.0316e+00,  4.1967e-01,  1.6997e+00, -2.4723e+00,
         -6.8854e+00, -5.0376e+00, -3.2686e+01,  1.5702e+01, -6.0404e+00],
        [-7.3342e-01, -1.6558e-01,  1.4866e-02, -9.5380e-02, -2.5807e+00,
         -2.4601e-01, -1.7490e-01, -1.0871e+00, -4.0997e-01, -8.5503e-01],
        [-7.4823e+00,  3.6824e+00,  2.7743e-01,  3.3016e+00,  7.7796e-01,
          6.7700e-01, -9.1973e+00, -5.8992e+00,  3.2522e+00, -2.3277e+00],
        [-2.7056e-02,  1.0243e+00,  2.5504e-01,  9.7638e-01, -2.4098e+00,
          2.7460e-01, -9.3762e+00,  2.3750e+00,  2.9668e+00, -3.1138e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.3811, -8.4615, -2.6506, -2.6500,  3.0872, -5.6053,  1.2951, -2.6386,
        -1.2181, -2.5183], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.7230,   0.1490,  -0.2400,   1.4434,  14.4126,   0.3764,  -7.1038,
          -0.2402,  -2.2275,   1.6248],
        [ -1.7230,  -0.3536,   0.2400,  -1.4433, -14.4142,  -0.5248,   7.1825,
           0.2402,   2.2745,  -1.6251]], device='cuda:0'))])
loaded xi:  58.6305
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1545.8957923365335
Current xi:  [64.84638]
objective value function right now is: -1545.8957923365335
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.83303]
objective value function right now is: -1545.1151293355435
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [76.7575]
objective value function right now is: -1471.3184801244397
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [67.60881]
objective value function right now is: -1512.7992463733597
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [72.43966]
objective value function right now is: -1521.386670642837
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1546.9080544335363
Current xi:  [78.34627]
objective value function right now is: -1546.9080544335363
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [83.836624]
objective value function right now is: -1543.294453678806
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [88.65876]
objective value function right now is: -1544.246716298402
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [92.831024]
objective value function right now is: -1524.323423519177
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1547.1349905520426
Current xi:  [95.67345]
objective value function right now is: -1547.1349905520426
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1550.128168456003
Current xi:  [98.207825]
objective value function right now is: -1550.128168456003
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1550.4111777610383
Current xi:  [100.67258]
objective value function right now is: -1550.4111777610383
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1551.1438087646384
Current xi:  [102.51691]
objective value function right now is: -1551.1438087646384
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [103.03785]
objective value function right now is: -1549.6110793430662
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.217606]
objective value function right now is: -1546.657851876347
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.4236]
objective value function right now is: -1548.7431136012754
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.338005]
objective value function right now is: -1548.9258242538624
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.83593]
objective value function right now is: -1545.9299957197854
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [110.247086]
objective value function right now is: -1550.915915875869
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [111.801994]
objective value function right now is: -1549.8448638207815
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.61762]
objective value function right now is: -1549.180444145294
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1551.907124157758
Current xi:  [113.0526]
objective value function right now is: -1551.907124157758
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [113.33584]
objective value function right now is: -1546.6735389325258
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [114.315994]
objective value function right now is: -1402.6477450318862
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [114.04985]
objective value function right now is: -1550.5291607317877
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1552.581695183261
Current xi:  [114.010796]
objective value function right now is: -1552.581695183261
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.259926]
objective value function right now is: -1552.1219109518104
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [114.45879]
objective value function right now is: -1551.574571281274
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [114.483826]
objective value function right now is: -1548.7530862096148
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.0104]
objective value function right now is: -1545.862125673194
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.62316]
objective value function right now is: -1550.885316486409
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.602264]
objective value function right now is: -1540.0514437863026
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.98159]
objective value function right now is: -1546.1653706916873
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.65113]
objective value function right now is: -1549.4804358980666
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [111.752785]
objective value function right now is: -1370.0740974210291
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.57312]
objective value function right now is: -1396.7526124533447
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.17785]
objective value function right now is: -1521.0316335426191
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.495575]
objective value function right now is: -1531.6019176543025
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.29306]
objective value function right now is: -1547.0165782448598
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.554855]
objective value function right now is: -1549.6809448540519
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.608444]
objective value function right now is: -1550.4276318455504
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.94025]
objective value function right now is: -1551.2753630653228
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.2211]
objective value function right now is: -1551.667918862149
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.75619]
objective value function right now is: -1551.4897256266577
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.14832]
objective value function right now is: -1552.488905078888
new min fval from sgd:  -1552.5837736466056
new min fval from sgd:  -1552.6304816618622
new min fval from sgd:  -1552.6427377791322
new min fval from sgd:  -1552.651312134694
new min fval from sgd:  -1552.7106995404372
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.88055]
objective value function right now is: -1551.209797096538
new min fval from sgd:  -1552.7899926020998
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.25802]
objective value function right now is: -1551.9471499968959
new min fval from sgd:  -1552.8261338631526
new min fval from sgd:  -1552.8538217250295
new min fval from sgd:  -1552.8586231698084
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.46706]
objective value function right now is: -1550.7326249045234
new min fval from sgd:  -1552.9242249929869
new min fval from sgd:  -1552.9247145921254
new min fval from sgd:  -1552.931515079346
new min fval from sgd:  -1552.9319721181666
new min fval from sgd:  -1552.9326481933447
new min fval from sgd:  -1552.9406514283055
new min fval from sgd:  -1552.9452974909982
new min fval from sgd:  -1552.9476607710644
new min fval from sgd:  -1552.9583944279675
new min fval from sgd:  -1552.9664954242917
new min fval from sgd:  -1552.9865790910947
new min fval from sgd:  -1552.9977166721403
new min fval from sgd:  -1553.0134102359727
new min fval from sgd:  -1553.0220018652535
new min fval from sgd:  -1553.0262756928657
new min fval from sgd:  -1553.0311135555653
new min fval from sgd:  -1553.0393202360758
new min fval from sgd:  -1553.0469735617753
new min fval from sgd:  -1553.054353186096
new min fval from sgd:  -1553.0636135090454
new min fval from sgd:  -1553.068530552061
new min fval from sgd:  -1553.070008836226
new min fval from sgd:  -1553.071375557878
new min fval from sgd:  -1553.0723183489101
new min fval from sgd:  -1553.0728842955762
new min fval from sgd:  -1553.0752516074306
new min fval from sgd:  -1553.0809611019436
new min fval from sgd:  -1553.0817536867605
new min fval from sgd:  -1553.08320478004
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.74208]
objective value function right now is: -1553.0552341873401
new min fval from sgd:  -1553.0885189060245
new min fval from sgd:  -1553.092142338104
new min fval from sgd:  -1553.0971233997473
new min fval from sgd:  -1553.0994813901204
new min fval from sgd:  -1553.1023930966303
new min fval from sgd:  -1553.1087261362686
new min fval from sgd:  -1553.1145853039095
new min fval from sgd:  -1553.1217263223796
new min fval from sgd:  -1553.1250605622795
new min fval from sgd:  -1553.1253999397736
new min fval from sgd:  -1553.1272603915024
new min fval from sgd:  -1553.130819035832
new min fval from sgd:  -1553.1330278890064
new min fval from sgd:  -1553.1333723661974
new min fval from sgd:  -1553.1412137916946
new min fval from sgd:  -1553.1451885318827
new min fval from sgd:  -1553.1470295713484
new min fval from sgd:  -1553.1531471701287
new min fval from sgd:  -1553.1532847730175
new min fval from sgd:  -1553.156053429272
new min fval from sgd:  -1553.1622694284804
new min fval from sgd:  -1553.1682526236154
new min fval from sgd:  -1553.1753573942744
new min fval from sgd:  -1553.1783364885612
new min fval from sgd:  -1553.1829324904188
new min fval from sgd:  -1553.195682092008
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.787766]
objective value function right now is: -1552.7585374410846
min fval:  -1553.195682092008
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 11.1477,   1.3272],
        [ -5.6302,   9.6294],
        [ 14.8781,  -3.9906],
        [ -1.5710,  13.3343],
        [ -0.4008,   0.3414],
        [ -0.4008,   0.3414],
        [-36.7165, -10.2662],
        [  2.9833,  -8.5941],
        [ -0.4008,   0.3414],
        [ 14.1927,  -0.2335]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-11.7548,   6.3274,  -9.9341,  10.3273,  -4.2567,  -4.2567,  -9.1304,
         -2.6354,  -4.2567, -12.1718], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.6404e-02,  3.0250e+00,  2.4818e+00,  2.2904e+00, -8.6447e-03,
         -8.6447e-03, -9.0524e-01, -2.1188e+00, -8.6447e-03,  4.7486e-02],
        [-4.6404e-02,  3.0250e+00,  2.4818e+00,  2.2904e+00, -8.6452e-03,
         -8.6453e-03, -9.0524e-01, -2.1188e+00, -8.6453e-03,  4.7489e-02],
        [-4.6404e-02,  3.0250e+00,  2.4818e+00,  2.2904e+00, -8.6443e-03,
         -8.6443e-03, -9.0524e-01, -2.1188e+00, -8.6443e-03,  4.7488e-02],
        [ 6.0724e+00, -8.9301e+00,  1.4251e+01, -1.7659e+01,  2.8259e-02,
          2.8258e-02,  1.2834e+01,  3.0050e+00,  2.8258e-02,  1.9056e+01],
        [-4.6404e-02,  3.0250e+00,  2.4818e+00,  2.2904e+00, -8.6446e-03,
         -8.6447e-03, -9.0524e-01, -2.1188e+00, -8.6448e-03,  4.7488e-02],
        [-4.6403e-02,  3.0250e+00,  2.4818e+00,  2.2904e+00, -8.6443e-03,
         -8.6443e-03, -9.0524e-01, -2.1188e+00, -8.6443e-03,  4.7488e-02],
        [-4.6404e-02,  3.0250e+00,  2.4818e+00,  2.2904e+00, -8.6448e-03,
         -8.6448e-03, -9.0524e-01, -2.1188e+00, -8.6448e-03,  4.7487e-02],
        [ 1.1624e-02, -1.5699e+00, -6.4310e-01,  1.8923e+01, -1.5311e-01,
         -1.5311e-01,  1.1355e+01, -3.5164e+00, -1.5311e-01, -9.7663e-03],
        [ 5.3136e+00, -8.5929e+00,  1.5434e+01, -1.6694e+01,  4.3396e-01,
          4.3396e-01,  1.1182e+01,  4.5379e+00,  4.3396e-01,  1.6335e+01],
        [-4.6403e-02,  3.0250e+00,  2.4818e+00,  2.2904e+00, -8.6442e-03,
         -8.6442e-03, -9.0524e-01, -2.1188e+00, -8.6442e-03,  4.7490e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8515, -0.8515, -0.8515,  1.2022, -0.8515, -0.8515, -0.8515, -2.4319,
         2.6364, -0.8515], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  1.0746,   1.0746,   1.0746, -17.2672,   1.0746,   1.0746,   1.0746,
          11.7451, -12.7857,   1.0746]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 21.0507,   3.3036],
        [-14.4163,  -6.0958],
        [ -5.4488,   3.5885],
        [  0.4791,   4.9733],
        [ 12.9425,   5.2158],
        [ 12.9953,   2.6104],
        [-16.2044,   8.5916],
        [ 10.1923,  14.4125],
        [-17.7164,  -4.5361],
        [ -1.2676,   0.4109]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-10.6630,  -4.6648,  -7.5985,  -2.5691,   3.9719, -17.4263,  10.1136,
         10.8278,   2.0688,  -5.6747], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-8.3478e-01,  2.3353e-02, -6.9249e-02, -1.1177e-01, -2.9569e+00,
         -7.9631e-02, -2.8413e-01, -1.4528e+00, -3.1510e-01,  8.4008e-03],
        [ 4.8320e+00,  2.3921e-01, -1.6311e+00, -3.7636e+00, -6.0339e+00,
          8.7369e+00,  6.7418e+00,  7.2875e+00,  1.4865e-01, -3.0822e-01],
        [-9.8006e-01,  3.9156e+00, -3.0321e-01, -1.0719e-01, -4.4556e+00,
         -4.1085e-01,  3.3099e+00, -2.5077e+00,  1.5516e+00,  1.4207e-02],
        [-1.2938e+00, -1.5833e+00,  1.0367e-01,  1.6176e-02, -2.6836e+00,
          3.2263e-02, -6.0568e-03, -2.3457e+00,  1.5653e+00, -2.1589e-03],
        [-2.1412e-01,  8.5425e+00, -7.7203e-03, -4.6796e-01, -1.3160e+01,
          1.8033e-03, -6.2860e+00, -4.6843e+01,  8.5749e+00, -1.0773e+00],
        [-4.1778e+00, -1.8511e-02,  9.2322e-01,  3.4304e-01, -5.6148e+00,
         -3.5790e+00, -6.0346e-06,  6.7991e+00, -1.6366e+00,  4.6054e-03],
        [-6.8227e+00, -1.1160e+00, -2.8315e-02, -1.3062e-01, -2.2296e+00,
         -1.1542e+00, -6.8336e+00, -3.2953e+01,  1.5033e+01, -3.1864e-01],
        [-5.2327e-02, -2.2438e+00, -2.4979e+00, -6.9031e-01, -1.3014e+00,
         -2.2791e+00,  7.1155e-01,  1.0690e+00,  2.8918e-01,  4.9722e-01],
        [-8.7191e+00,  1.6111e+00,  1.8031e-03, -8.4926e-01,  2.2583e+00,
         -1.8656e-02, -2.3231e+01, -7.6717e+00,  1.3832e+00, -4.4816e-01],
        [-1.3598e+00, -2.1928e+00,  2.1231e-01,  1.0634e-01, -2.4014e+00,
         -6.0191e-02,  2.1866e-01, -2.1563e+00,  2.2325e+00,  1.9419e-02]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -3.1814, -11.9605,  -3.3592,  -3.1685,   1.5477,  -9.0677,   0.5820,
         -1.5393,   1.0669,  -2.9923], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.1222,   0.2675,   1.6996,  -0.6062,  14.8449,  -0.7787,  -7.9042,
           2.6155,  -0.7431,  -0.9047],
        [ -0.1222,  -0.4716,  -1.6994,   0.6062, -14.8434,   0.6391,   7.9623,
          -2.6155,   0.7875,   0.9046]], device='cuda:0'))])
xi:  [109.75892]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 584.8588823039065
W_T_median: 304.23473158571335
W_T_pctile_5: 109.7978921944892
W_T_CVAR_5_pct: -12.9460674699736
Average q (qsum/M+1):  50.72964969758065
Optimal xi:  [109.75892]
Observed VAR:  304.23473158571335
Expected(across Rb) median(across samples) p_equity:  0.2866602053244909
obj fun:  tensor(-1553.1957, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  9.8056,   0.3702],
        [ -4.3465,   8.7513],
        [ 11.6827,  -4.5826],
        [  0.3149,  11.8025],
        [ -0.7516,   1.0731],
        [ -0.7516,   1.0731],
        [-33.7974,  -9.1431],
        [  5.3946,  -4.5009],
        [ -0.7516,   1.0731],
        [ 10.9840,   1.5848]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -8.5826,   5.8637,  -9.1605,   9.5059,  -2.6000,  -2.6001,  -8.5071,
          1.6323,  -2.6001, -10.2237], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [ 5.2973e+00, -1.0926e+01,  1.1810e+01, -1.5601e+01, -4.0905e-02,
         -4.0919e-02,  1.3139e+01,  2.8679e+00, -4.0979e-02,  1.7610e+01],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-1.6569e-01,  3.5122e+00,  1.2751e+00,  1.6088e+01, -9.9319e-02,
         -9.9316e-02,  1.2755e+01, -4.1160e+00, -9.9300e-02, -6.4273e-02],
        [ 4.1958e+00, -1.0778e+01,  1.2722e+01, -1.4714e+01,  8.3874e-02,
          8.3891e-02,  1.2004e+01,  3.9389e+00,  8.3961e-02,  1.5171e+01],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.7286, -0.7286, -0.7286,  1.5126, -0.7286, -0.7286, -0.7286, -2.8475,
         2.4285, -0.7286], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 8.8609e-03,  8.8609e-03,  8.8609e-03, -1.4825e+01,  8.8609e-03,
          8.8609e-03,  8.8609e-03,  1.2082e+01, -1.0963e+01,  8.8609e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 17.5169,   2.4047],
        [-11.9075,  -5.4174],
        [ -2.9792,  -0.9803],
        [ -9.7518,  -4.0833],
        [ 11.3921,   5.1641],
        [ 13.7315,  -0.0707],
        [ -8.8519,  10.6386],
        [  7.6684,  14.1374],
        [-16.4605,  -4.5943],
        [ 14.0679,   5.0107]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-10.0398,  -4.7415,  -4.9321,  -4.8020,   4.9201, -12.9012,  10.5438,
         11.8370,   0.8090,  -3.9403], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.8323e+00,  4.5980e+00,  3.9185e-01,  3.6615e+00, -2.6056e+00,
         -5.2731e-02, -2.9941e+00,  7.1657e-01,  1.9169e+00, -3.9848e+00],
        [ 9.5454e+00,  1.7276e+00, -6.8431e-02,  9.5238e-03, -2.5478e+00,
          5.6171e+00,  1.4304e+01,  1.7821e+01, -1.7394e+00,  4.0096e+00],
        [-6.9749e-01, -1.6425e-01,  1.4400e-02, -9.4509e-02, -2.5979e+00,
         -2.4345e-01, -1.8580e-01, -1.0674e+00, -4.0907e-01, -8.7679e-01],
        [-1.5408e+00,  1.3537e+00,  1.1014e-01,  6.5129e-01, -2.6476e+00,
         -2.6944e-01, -5.7968e+00,  1.5919e+00,  1.8653e+00, -2.8750e+00],
        [ 1.5086e+00,  9.7123e+00,  5.2281e-01,  4.8078e+00, -1.4468e+01,
          2.2508e+00,  6.0173e-01, -3.3019e+01,  1.0199e+01, -2.1604e+00],
        [-3.3438e+00,  1.9099e+00, -1.5542e-01,  1.1551e+00, -1.7673e+00,
         -7.1039e+00,  1.1353e+01,  2.7938e+00, -3.5859e+00, -3.1004e+00],
        [-5.8361e+00,  4.0316e+00,  4.1967e-01,  1.6997e+00, -2.4723e+00,
         -6.8854e+00, -5.0376e+00, -3.2686e+01,  1.5702e+01, -6.0404e+00],
        [-7.3342e-01, -1.6558e-01,  1.4866e-02, -9.5380e-02, -2.5807e+00,
         -2.4601e-01, -1.7490e-01, -1.0871e+00, -4.0997e-01, -8.5503e-01],
        [-7.4823e+00,  3.6824e+00,  2.7743e-01,  3.3016e+00,  7.7796e-01,
          6.7700e-01, -9.1973e+00, -5.8992e+00,  3.2522e+00, -2.3277e+00],
        [-2.7056e-02,  1.0243e+00,  2.5504e-01,  9.7638e-01, -2.4098e+00,
          2.7460e-01, -9.3762e+00,  2.3750e+00,  2.9668e+00, -3.1138e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.3811, -8.4615, -2.6506, -2.6500,  3.0872, -5.6053,  1.2951, -2.6386,
        -1.2181, -2.5183], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.7230,   0.1490,  -0.2400,   1.4434,  14.4126,   0.3764,  -7.1038,
          -0.2402,  -2.2275,   1.6248],
        [ -1.7230,  -0.3536,   0.2400,  -1.4433, -14.4142,  -0.5248,   7.1825,
           0.2402,   2.2745,  -1.6251]], device='cuda:0'))])
loaded xi:  58.6305
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 11.1477,   1.3272],
        [ -5.6302,   9.6294],
        [ 14.8781,  -3.9906],
        [ -1.5710,  13.3343],
        [ -0.4008,   0.3414],
        [ -0.4008,   0.3414],
        [-36.7165, -10.2662],
        [  2.9833,  -8.5941],
        [ -0.4008,   0.3414],
        [ 14.1927,  -0.2335]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-11.7548,   6.3274,  -9.9341,  10.3273,  -4.2567,  -4.2567,  -9.1304,
         -2.6354,  -4.2567, -12.1718], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.6404e-02,  3.0250e+00,  2.4818e+00,  2.2904e+00, -8.6447e-03,
         -8.6447e-03, -9.0524e-01, -2.1188e+00, -8.6447e-03,  4.7486e-02],
        [-4.6404e-02,  3.0250e+00,  2.4818e+00,  2.2904e+00, -8.6452e-03,
         -8.6453e-03, -9.0524e-01, -2.1188e+00, -8.6453e-03,  4.7489e-02],
        [-4.6404e-02,  3.0250e+00,  2.4818e+00,  2.2904e+00, -8.6443e-03,
         -8.6443e-03, -9.0524e-01, -2.1188e+00, -8.6443e-03,  4.7488e-02],
        [ 6.0724e+00, -8.9301e+00,  1.4251e+01, -1.7659e+01,  2.8259e-02,
          2.8258e-02,  1.2834e+01,  3.0050e+00,  2.8258e-02,  1.9056e+01],
        [-4.6404e-02,  3.0250e+00,  2.4818e+00,  2.2904e+00, -8.6446e-03,
         -8.6447e-03, -9.0524e-01, -2.1188e+00, -8.6448e-03,  4.7488e-02],
        [-4.6403e-02,  3.0250e+00,  2.4818e+00,  2.2904e+00, -8.6443e-03,
         -8.6443e-03, -9.0524e-01, -2.1188e+00, -8.6443e-03,  4.7488e-02],
        [-4.6404e-02,  3.0250e+00,  2.4818e+00,  2.2904e+00, -8.6448e-03,
         -8.6448e-03, -9.0524e-01, -2.1188e+00, -8.6448e-03,  4.7487e-02],
        [ 1.1624e-02, -1.5699e+00, -6.4310e-01,  1.8923e+01, -1.5311e-01,
         -1.5311e-01,  1.1355e+01, -3.5164e+00, -1.5311e-01, -9.7663e-03],
        [ 5.3136e+00, -8.5929e+00,  1.5434e+01, -1.6694e+01,  4.3396e-01,
          4.3396e-01,  1.1182e+01,  4.5379e+00,  4.3396e-01,  1.6335e+01],
        [-4.6403e-02,  3.0250e+00,  2.4818e+00,  2.2904e+00, -8.6442e-03,
         -8.6442e-03, -9.0524e-01, -2.1188e+00, -8.6442e-03,  4.7490e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8515, -0.8515, -0.8515,  1.2022, -0.8515, -0.8515, -0.8515, -2.4319,
         2.6364, -0.8515], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  1.0746,   1.0746,   1.0746, -17.2672,   1.0746,   1.0746,   1.0746,
          11.7451, -12.7857,   1.0746]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 21.0507,   3.3036],
        [-14.4163,  -6.0958],
        [ -5.4488,   3.5885],
        [  0.4791,   4.9733],
        [ 12.9425,   5.2158],
        [ 12.9953,   2.6104],
        [-16.2044,   8.5916],
        [ 10.1923,  14.4125],
        [-17.7164,  -4.5361],
        [ -1.2676,   0.4109]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-10.6630,  -4.6648,  -7.5985,  -2.5691,   3.9719, -17.4263,  10.1136,
         10.8278,   2.0688,  -5.6747], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-8.3478e-01,  2.3353e-02, -6.9249e-02, -1.1177e-01, -2.9569e+00,
         -7.9631e-02, -2.8413e-01, -1.4528e+00, -3.1510e-01,  8.4008e-03],
        [ 4.8320e+00,  2.3921e-01, -1.6311e+00, -3.7636e+00, -6.0339e+00,
          8.7369e+00,  6.7418e+00,  7.2875e+00,  1.4865e-01, -3.0822e-01],
        [-9.8006e-01,  3.9156e+00, -3.0321e-01, -1.0719e-01, -4.4556e+00,
         -4.1085e-01,  3.3099e+00, -2.5077e+00,  1.5516e+00,  1.4207e-02],
        [-1.2938e+00, -1.5833e+00,  1.0367e-01,  1.6176e-02, -2.6836e+00,
          3.2263e-02, -6.0568e-03, -2.3457e+00,  1.5653e+00, -2.1589e-03],
        [-2.1412e-01,  8.5425e+00, -7.7203e-03, -4.6796e-01, -1.3160e+01,
          1.8033e-03, -6.2860e+00, -4.6843e+01,  8.5749e+00, -1.0773e+00],
        [-4.1778e+00, -1.8511e-02,  9.2322e-01,  3.4304e-01, -5.6148e+00,
         -3.5790e+00, -6.0346e-06,  6.7991e+00, -1.6366e+00,  4.6054e-03],
        [-6.8227e+00, -1.1160e+00, -2.8315e-02, -1.3062e-01, -2.2296e+00,
         -1.1542e+00, -6.8336e+00, -3.2953e+01,  1.5033e+01, -3.1864e-01],
        [-5.2327e-02, -2.2438e+00, -2.4979e+00, -6.9031e-01, -1.3014e+00,
         -2.2791e+00,  7.1155e-01,  1.0690e+00,  2.8918e-01,  4.9722e-01],
        [-8.7191e+00,  1.6111e+00,  1.8031e-03, -8.4926e-01,  2.2583e+00,
         -1.8656e-02, -2.3231e+01, -7.6717e+00,  1.3832e+00, -4.4816e-01],
        [-1.3598e+00, -2.1928e+00,  2.1231e-01,  1.0634e-01, -2.4014e+00,
         -6.0191e-02,  2.1866e-01, -2.1563e+00,  2.2325e+00,  1.9419e-02]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -3.1814, -11.9605,  -3.3592,  -3.1685,   1.5477,  -9.0677,   0.5820,
         -1.5393,   1.0669,  -2.9923], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.1222,   0.2675,   1.6996,  -0.6062,  14.8449,  -0.7787,  -7.9042,
           2.6155,  -0.7431,  -0.9047],
        [ -0.1222,  -0.4716,  -1.6994,   0.6062, -14.8434,   0.6391,   7.9623,
          -2.6155,   0.7875,   0.9046]], device='cuda:0'))])
loaded xi:  109.75892
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1541.1726906238046
Current xi:  [119.44099]
objective value function right now is: -1541.1726906238046
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [129.82118]
objective value function right now is: -1539.7590030718295
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [137.744]
objective value function right now is: -1537.2921528229
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1556.4777468412512
Current xi:  [144.5857]
objective value function right now is: -1556.4777468412512
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [151.22632]
objective value function right now is: -1534.0678787451297
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [155.56241]
objective value function right now is: -1555.9993428761625
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [157.2099]
objective value function right now is: -1369.7452410257017
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [156.377]
objective value function right now is: -1540.2566993686846
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [159.51608]
objective value function right now is: -1542.0864944345324
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.76904]
objective value function right now is: -1553.9687987654888
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.01433]
objective value function right now is: -1548.9015868887395
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.27478]
objective value function right now is: -1556.160395837924
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.34818]
objective value function right now is: -1543.684486412732
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1556.7597377044726
Current xi:  [165.84746]
objective value function right now is: -1556.7597377044726
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.9439]
objective value function right now is: -1549.910175064834
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.42484]
objective value function right now is: -1556.6619130530328
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.30406]
objective value function right now is: -1555.5147971232902
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.41702]
objective value function right now is: -1555.799601330467
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1556.895141089342
Current xi:  [166.79292]
objective value function right now is: -1556.895141089342
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1557.2801072035065
Current xi:  [165.1675]
objective value function right now is: -1557.2801072035065
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.78337]
objective value function right now is: -1551.704291763108
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.45387]
objective value function right now is: -1545.795328794493
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.96968]
objective value function right now is: -1552.8019606191615
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.1435]
objective value function right now is: -1554.6148731109272
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.37614]
objective value function right now is: -1552.5074326944377
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.36568]
objective value function right now is: -1549.661949867659
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.47713]
objective value function right now is: -1556.1261296980394
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [166.30911]
objective value function right now is: -1553.9578148817104
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [165.36581]
objective value function right now is: -1555.0568321625672
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.79959]
objective value function right now is: -1553.6795223333731
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.47568]
objective value function right now is: -1553.609307414786
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.13203]
objective value function right now is: -1552.7565404351465
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.16388]
objective value function right now is: -1556.7439239772004
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.57784]
objective value function right now is: -1554.836411336607
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -1557.9235209832
Current xi:  [166.78938]
objective value function right now is: -1557.9235209832
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.9068749092735
Current xi:  [166.75056]
objective value function right now is: -1562.9068749092735
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1563.4904268351986
Current xi:  [166.86037]
objective value function right now is: -1563.4904268351986
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1563.802615930381
Current xi:  [166.98792]
objective value function right now is: -1563.802615930381
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.97923]
objective value function right now is: -1562.1272607533735
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.72815]
objective value function right now is: -1562.4827191878367
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.07738]
objective value function right now is: -1559.921163359229
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.76859]
objective value function right now is: -1562.0068731844556
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.81339]
objective value function right now is: -1559.9181104087718
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.83829]
objective value function right now is: -1563.425237984959
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.65671]
objective value function right now is: -1560.8598709688695
new min fval from sgd:  -1563.8254268117328
new min fval from sgd:  -1563.8675533505877
new min fval from sgd:  -1563.891048710327
new min fval from sgd:  -1563.957083809455
new min fval from sgd:  -1563.9754642163218
new min fval from sgd:  -1563.981289205717
new min fval from sgd:  -1564.0034544437544
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.89064]
objective value function right now is: -1563.649050563641
new min fval from sgd:  -1564.0150268879372
new min fval from sgd:  -1564.1395483435879
new min fval from sgd:  -1564.288132300641
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.91504]
objective value function right now is: -1562.0479310349745
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.91138]
objective value function right now is: -1562.797977801874
new min fval from sgd:  -1564.3517695182236
new min fval from sgd:  -1564.4335169552455
new min fval from sgd:  -1564.436948392035
new min fval from sgd:  -1564.4376653981783
new min fval from sgd:  -1564.4614186159315
new min fval from sgd:  -1564.48204547032
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.02933]
objective value function right now is: -1563.3074950033517
new min fval from sgd:  -1564.4874192005927
new min fval from sgd:  -1564.490885977672
new min fval from sgd:  -1564.492855605675
new min fval from sgd:  -1564.5036845023437
new min fval from sgd:  -1564.5088676189516
new min fval from sgd:  -1564.5120037021893
new min fval from sgd:  -1564.5120475739668
new min fval from sgd:  -1564.5182358792135
new min fval from sgd:  -1564.5421545193801
new min fval from sgd:  -1564.5533282225883
new min fval from sgd:  -1564.558249096653
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.0835]
objective value function right now is: -1564.3165879937867
min fval:  -1564.558249096653
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 11.4549,   3.9449],
        [ -6.4981,  11.2394],
        [ 17.0986,  -4.9552],
        [ -3.9545,  14.7583],
        [ -0.9541,   0.5624],
        [ -0.9541,   0.5624],
        [-41.0944, -10.9312],
        [  5.9511,  -8.5845],
        [ -0.9541,   0.5624],
        [ 17.3398,  -1.0880]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-15.1359,   6.2196, -11.1951,  10.8642,  -3.0211,  -3.0211,  -9.8999,
         -3.2831,  -3.0211, -14.2386], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.6603e-02, -1.5154e-01, -1.7212e-01, -5.4578e-01, -6.6877e-03,
         -6.6877e-03, -1.0060e-02, -8.8342e-01, -6.6877e-03, -1.4855e-02],
        [-2.6604e-02, -1.5154e-01, -1.7212e-01, -5.4578e-01, -6.6877e-03,
         -6.6877e-03, -1.0060e-02, -8.8342e-01, -6.6877e-03, -1.4855e-02],
        [-2.6603e-02, -1.5154e-01, -1.7212e-01, -5.4578e-01, -6.6877e-03,
         -6.6877e-03, -1.0060e-02, -8.8342e-01, -6.6877e-03, -1.4855e-02],
        [ 9.8022e+00, -7.5706e+00,  1.3065e+01, -1.8725e+01,  8.3595e-02,
          8.3595e-02,  1.1582e+01,  2.4049e+00,  8.3594e-02,  2.2840e+01],
        [-2.6603e-02, -1.5154e-01, -1.7212e-01, -5.4578e-01, -6.6877e-03,
         -6.6877e-03, -1.0060e-02, -8.8342e-01, -6.6877e-03, -1.4855e-02],
        [-2.6603e-02, -1.5154e-01, -1.7212e-01, -5.4578e-01, -6.6877e-03,
         -6.6877e-03, -1.0060e-02, -8.8342e-01, -6.6877e-03, -1.4855e-02],
        [-2.6603e-02, -1.5154e-01, -1.7212e-01, -5.4578e-01, -6.6877e-03,
         -6.6877e-03, -1.0060e-02, -8.8342e-01, -6.6877e-03, -1.4855e-02],
        [ 3.1477e-01,  9.2526e-01,  4.7755e-01,  2.1968e+00,  8.1351e-02,
          8.1351e-02,  9.5969e-02,  2.0936e+00,  8.1351e-02,  3.2615e-01],
        [ 9.4629e+00, -7.6507e+00,  1.6761e+01, -1.6836e+01, -1.2408e-01,
         -1.2408e-01,  1.0729e+01,  3.4116e+00, -1.2408e-01,  1.8899e+01],
        [-2.6604e-02, -1.5154e-01, -1.7212e-01, -5.4578e-01, -6.6877e-03,
         -6.6877e-03, -1.0060e-02, -8.8342e-01, -6.6877e-03, -1.4855e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.0954, -1.0954, -1.0954,  2.3589, -1.0954, -1.0954, -1.0954,  2.7675,
         2.8596, -1.0954], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0226,   0.0226,   0.0226, -18.4952,   0.0226,   0.0226,   0.0226,
          12.1462, -12.8865,   0.0226]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 23.0104,   4.4123],
        [-22.7847,  -4.3620],
        [ -2.6555,   0.1639],
        [-13.7429,   1.3499],
        [ 12.6979,   4.4203],
        [ -2.6664,   0.1622],
        [-23.4010,  -1.7598],
        [  6.0373,  14.1248],
        [-16.9692,  -6.5826],
        [ -2.6535,   0.1642]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-11.9095,  -3.9960,  -4.1634,   2.4518,   7.1665,  -4.1436,  12.6621,
         10.7588,  -1.4149,  -4.1668], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.8530e-01,  1.1974e+00, -1.6149e-01,  3.3435e+00, -4.4658e+00,
         -1.6155e-01,  1.0719e+00,  1.7346e+00,  2.6676e+00, -1.6153e-01],
        [-1.2725e+00, -5.8291e-03, -5.3628e-03, -2.3294e-02, -3.3632e+00,
         -5.4166e-03, -1.0009e+00, -1.1783e+00, -6.7198e-02, -5.3534e-03],
        [ 4.8855e+00, -2.6169e-04,  1.8327e-01,  5.5344e-03, -3.7102e+00,
          1.8440e-01, -2.2172e+00, -3.6926e+00, -5.9487e+00,  1.8292e-01],
        [-1.2721e+00, -5.8128e-03, -5.3509e-03, -2.3273e-02, -3.3633e+00,
         -5.4046e-03, -1.0007e+00, -1.1780e+00, -6.7158e-02, -5.3415e-03],
        [ 4.6646e-01,  1.2547e+01, -2.2075e+00,  1.9973e+00, -1.3864e+01,
         -2.2362e+00,  8.2381e+00, -5.6149e+01,  8.5068e+00, -2.2024e+00],
        [-1.2721e+00, -5.8129e-03, -5.3510e-03, -2.3273e-02, -3.3633e+00,
         -5.4047e-03, -1.0007e+00, -1.1780e+00, -6.7158e-02, -5.3416e-03],
        [-1.5176e+01,  5.8481e-01, -1.7434e-01,  1.2132e+00, -2.9021e+00,
         -1.7753e-01, -7.0372e-01, -3.5432e+01,  1.4201e+01, -1.7311e-01],
        [ 2.6044e-01,  7.5654e+00,  8.2420e-02,  2.1923e+01, -1.0598e+00,
          8.2783e-02,  5.8793e-01,  2.6291e+00, -2.0128e+00,  8.2403e-02],
        [-9.9235e+00, -6.2263e+00, -1.3557e-02, -4.5634e+00,  4.1689e-01,
         -7.8380e-03,  1.2900e+00, -9.4792e+00,  6.0827e+00, -1.5025e-02],
        [-1.2721e+00, -5.8129e-03, -5.3510e-03, -2.3273e-02, -3.3633e+00,
         -5.4047e-03, -1.0007e+00, -1.1780e+00, -6.7158e-02, -5.3416e-03]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.5587, -3.3642, -3.3692, -3.3642, -1.7560, -3.3642,  0.1544, -0.9865,
        -1.3213, -3.3642], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.7506e+00, -2.8392e-02,  3.5178e+00, -1.6538e-02,  2.2386e+01,
         -1.6540e-02, -9.0239e+00,  6.1797e-01, -9.4804e-01, -1.6538e-02],
        [-1.7506e+00,  4.8103e-03, -3.5176e+00,  1.6529e-02, -2.2383e+01,
          1.6527e-02,  9.0742e+00, -6.1799e-01,  9.9206e-01,  1.6529e-02]],
       device='cuda:0'))])
xi:  [167.08896]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 621.3962082441723
W_T_median: 370.1155170698496
W_T_pctile_5: 167.09278593755397
W_T_CVAR_5_pct: 15.31528377851399
Average q (qsum/M+1):  48.98750157510081
Optimal xi:  [167.08896]
Observed VAR:  370.1155170698496
Expected(across Rb) median(across samples) p_equity:  0.25922146812081337
obj fun:  tensor(-1564.5582, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:198: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  9.8056,   0.3702],
        [ -4.3465,   8.7513],
        [ 11.6827,  -4.5826],
        [  0.3149,  11.8025],
        [ -0.7516,   1.0731],
        [ -0.7516,   1.0731],
        [-33.7974,  -9.1431],
        [  5.3946,  -4.5009],
        [ -0.7516,   1.0731],
        [ 10.9840,   1.5848]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -8.5826,   5.8637,  -9.1605,   9.5059,  -2.6000,  -2.6001,  -8.5071,
          1.6323,  -2.6001, -10.2237], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [ 5.2973e+00, -1.0926e+01,  1.1810e+01, -1.5601e+01, -4.0905e-02,
         -4.0919e-02,  1.3139e+01,  2.8679e+00, -4.0979e-02,  1.7610e+01],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-1.6569e-01,  3.5122e+00,  1.2751e+00,  1.6088e+01, -9.9319e-02,
         -9.9316e-02,  1.2755e+01, -4.1160e+00, -9.9300e-02, -6.4273e-02],
        [ 4.1958e+00, -1.0778e+01,  1.2722e+01, -1.4714e+01,  8.3874e-02,
          8.3891e-02,  1.2004e+01,  3.9389e+00,  8.3961e-02,  1.5171e+01],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.7286, -0.7286, -0.7286,  1.5126, -0.7286, -0.7286, -0.7286, -2.8475,
         2.4285, -0.7286], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 8.8609e-03,  8.8609e-03,  8.8609e-03, -1.4825e+01,  8.8609e-03,
          8.8609e-03,  8.8609e-03,  1.2082e+01, -1.0963e+01,  8.8609e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 17.5169,   2.4047],
        [-11.9075,  -5.4174],
        [ -2.9792,  -0.9803],
        [ -9.7518,  -4.0833],
        [ 11.3921,   5.1641],
        [ 13.7315,  -0.0707],
        [ -8.8519,  10.6386],
        [  7.6684,  14.1374],
        [-16.4605,  -4.5943],
        [ 14.0679,   5.0107]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-10.0398,  -4.7415,  -4.9321,  -4.8020,   4.9201, -12.9012,  10.5438,
         11.8370,   0.8090,  -3.9403], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.8323e+00,  4.5980e+00,  3.9185e-01,  3.6615e+00, -2.6056e+00,
         -5.2731e-02, -2.9941e+00,  7.1657e-01,  1.9169e+00, -3.9848e+00],
        [ 9.5454e+00,  1.7276e+00, -6.8431e-02,  9.5238e-03, -2.5478e+00,
          5.6171e+00,  1.4304e+01,  1.7821e+01, -1.7394e+00,  4.0096e+00],
        [-6.9749e-01, -1.6425e-01,  1.4400e-02, -9.4509e-02, -2.5979e+00,
         -2.4345e-01, -1.8580e-01, -1.0674e+00, -4.0907e-01, -8.7679e-01],
        [-1.5408e+00,  1.3537e+00,  1.1014e-01,  6.5129e-01, -2.6476e+00,
         -2.6944e-01, -5.7968e+00,  1.5919e+00,  1.8653e+00, -2.8750e+00],
        [ 1.5086e+00,  9.7123e+00,  5.2281e-01,  4.8078e+00, -1.4468e+01,
          2.2508e+00,  6.0173e-01, -3.3019e+01,  1.0199e+01, -2.1604e+00],
        [-3.3438e+00,  1.9099e+00, -1.5542e-01,  1.1551e+00, -1.7673e+00,
         -7.1039e+00,  1.1353e+01,  2.7938e+00, -3.5859e+00, -3.1004e+00],
        [-5.8361e+00,  4.0316e+00,  4.1967e-01,  1.6997e+00, -2.4723e+00,
         -6.8854e+00, -5.0376e+00, -3.2686e+01,  1.5702e+01, -6.0404e+00],
        [-7.3342e-01, -1.6558e-01,  1.4866e-02, -9.5380e-02, -2.5807e+00,
         -2.4601e-01, -1.7490e-01, -1.0871e+00, -4.0997e-01, -8.5503e-01],
        [-7.4823e+00,  3.6824e+00,  2.7743e-01,  3.3016e+00,  7.7796e-01,
          6.7700e-01, -9.1973e+00, -5.8992e+00,  3.2522e+00, -2.3277e+00],
        [-2.7056e-02,  1.0243e+00,  2.5504e-01,  9.7638e-01, -2.4098e+00,
          2.7460e-01, -9.3762e+00,  2.3750e+00,  2.9668e+00, -3.1138e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.3811, -8.4615, -2.6506, -2.6500,  3.0872, -5.6053,  1.2951, -2.6386,
        -1.2181, -2.5183], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.7230,   0.1490,  -0.2400,   1.4434,  14.4126,   0.3764,  -7.1038,
          -0.2402,  -2.2275,   1.6248],
        [ -1.7230,  -0.3536,   0.2400,  -1.4433, -14.4142,  -0.5248,   7.1825,
           0.2402,   2.2745,  -1.6251]], device='cuda:0'))])
loaded xi:  58.6305
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 11.4549,   3.9449],
        [ -6.4981,  11.2394],
        [ 17.0986,  -4.9552],
        [ -3.9545,  14.7583],
        [ -0.9541,   0.5624],
        [ -0.9541,   0.5624],
        [-41.0944, -10.9312],
        [  5.9511,  -8.5845],
        [ -0.9541,   0.5624],
        [ 17.3398,  -1.0880]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-15.1359,   6.2196, -11.1951,  10.8642,  -3.0211,  -3.0211,  -9.8999,
         -3.2831,  -3.0211, -14.2386], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.6603e-02, -1.5154e-01, -1.7212e-01, -5.4578e-01, -6.6877e-03,
         -6.6877e-03, -1.0060e-02, -8.8342e-01, -6.6877e-03, -1.4855e-02],
        [-2.6604e-02, -1.5154e-01, -1.7212e-01, -5.4578e-01, -6.6877e-03,
         -6.6877e-03, -1.0060e-02, -8.8342e-01, -6.6877e-03, -1.4855e-02],
        [-2.6603e-02, -1.5154e-01, -1.7212e-01, -5.4578e-01, -6.6877e-03,
         -6.6877e-03, -1.0060e-02, -8.8342e-01, -6.6877e-03, -1.4855e-02],
        [ 9.8022e+00, -7.5706e+00,  1.3065e+01, -1.8725e+01,  8.3595e-02,
          8.3595e-02,  1.1582e+01,  2.4049e+00,  8.3594e-02,  2.2840e+01],
        [-2.6603e-02, -1.5154e-01, -1.7212e-01, -5.4578e-01, -6.6877e-03,
         -6.6877e-03, -1.0060e-02, -8.8342e-01, -6.6877e-03, -1.4855e-02],
        [-2.6603e-02, -1.5154e-01, -1.7212e-01, -5.4578e-01, -6.6877e-03,
         -6.6877e-03, -1.0060e-02, -8.8342e-01, -6.6877e-03, -1.4855e-02],
        [-2.6603e-02, -1.5154e-01, -1.7212e-01, -5.4578e-01, -6.6877e-03,
         -6.6877e-03, -1.0060e-02, -8.8342e-01, -6.6877e-03, -1.4855e-02],
        [ 3.1477e-01,  9.2526e-01,  4.7755e-01,  2.1968e+00,  8.1351e-02,
          8.1351e-02,  9.5969e-02,  2.0936e+00,  8.1351e-02,  3.2615e-01],
        [ 9.4629e+00, -7.6507e+00,  1.6761e+01, -1.6836e+01, -1.2408e-01,
         -1.2408e-01,  1.0729e+01,  3.4116e+00, -1.2408e-01,  1.8899e+01],
        [-2.6604e-02, -1.5154e-01, -1.7212e-01, -5.4578e-01, -6.6877e-03,
         -6.6877e-03, -1.0060e-02, -8.8342e-01, -6.6877e-03, -1.4855e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.0954, -1.0954, -1.0954,  2.3589, -1.0954, -1.0954, -1.0954,  2.7675,
         2.8596, -1.0954], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0226,   0.0226,   0.0226, -18.4952,   0.0226,   0.0226,   0.0226,
          12.1462, -12.8865,   0.0226]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 23.0104,   4.4123],
        [-22.7847,  -4.3620],
        [ -2.6555,   0.1639],
        [-13.7429,   1.3499],
        [ 12.6979,   4.4203],
        [ -2.6664,   0.1622],
        [-23.4010,  -1.7598],
        [  6.0373,  14.1248],
        [-16.9692,  -6.5826],
        [ -2.6535,   0.1642]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-11.9095,  -3.9960,  -4.1634,   2.4518,   7.1665,  -4.1436,  12.6621,
         10.7588,  -1.4149,  -4.1668], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.8530e-01,  1.1974e+00, -1.6149e-01,  3.3435e+00, -4.4658e+00,
         -1.6155e-01,  1.0719e+00,  1.7346e+00,  2.6676e+00, -1.6153e-01],
        [-1.2725e+00, -5.8291e-03, -5.3628e-03, -2.3294e-02, -3.3632e+00,
         -5.4166e-03, -1.0009e+00, -1.1783e+00, -6.7198e-02, -5.3534e-03],
        [ 4.8855e+00, -2.6169e-04,  1.8327e-01,  5.5344e-03, -3.7102e+00,
          1.8440e-01, -2.2172e+00, -3.6926e+00, -5.9487e+00,  1.8292e-01],
        [-1.2721e+00, -5.8128e-03, -5.3509e-03, -2.3273e-02, -3.3633e+00,
         -5.4046e-03, -1.0007e+00, -1.1780e+00, -6.7158e-02, -5.3415e-03],
        [ 4.6646e-01,  1.2547e+01, -2.2075e+00,  1.9973e+00, -1.3864e+01,
         -2.2362e+00,  8.2381e+00, -5.6149e+01,  8.5068e+00, -2.2024e+00],
        [-1.2721e+00, -5.8129e-03, -5.3510e-03, -2.3273e-02, -3.3633e+00,
         -5.4047e-03, -1.0007e+00, -1.1780e+00, -6.7158e-02, -5.3416e-03],
        [-1.5176e+01,  5.8481e-01, -1.7434e-01,  1.2132e+00, -2.9021e+00,
         -1.7753e-01, -7.0372e-01, -3.5432e+01,  1.4201e+01, -1.7311e-01],
        [ 2.6044e-01,  7.5654e+00,  8.2420e-02,  2.1923e+01, -1.0598e+00,
          8.2783e-02,  5.8793e-01,  2.6291e+00, -2.0128e+00,  8.2403e-02],
        [-9.9235e+00, -6.2263e+00, -1.3557e-02, -4.5634e+00,  4.1689e-01,
         -7.8380e-03,  1.2900e+00, -9.4792e+00,  6.0827e+00, -1.5025e-02],
        [-1.2721e+00, -5.8129e-03, -5.3510e-03, -2.3273e-02, -3.3633e+00,
         -5.4047e-03, -1.0007e+00, -1.1780e+00, -6.7158e-02, -5.3416e-03]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.5587, -3.3642, -3.3692, -3.3642, -1.7560, -3.3642,  0.1544, -0.9865,
        -1.3213, -3.3642], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.7506e+00, -2.8392e-02,  3.5178e+00, -1.6538e-02,  2.2386e+01,
         -1.6540e-02, -9.0239e+00,  6.1797e-01, -9.4804e-01, -1.6538e-02],
        [-1.7506e+00,  4.8103e-03, -3.5176e+00,  1.6529e-02, -2.2383e+01,
          1.6527e-02,  9.0742e+00, -6.1799e-01,  9.9206e-01,  1.6529e-02]],
       device='cuda:0'))])
loaded xi:  167.08896
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1583.7532713174228
Current xi:  [174.18253]
objective value function right now is: -1583.7532713174228
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1592.460424764983
Current xi:  [179.08102]
objective value function right now is: -1592.460424764983
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1592.7960427266287
Current xi:  [182.73215]
objective value function right now is: -1592.7960427266287
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.776]
objective value function right now is: -1586.5130017276758
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1595.8268627473299
Current xi:  [185.67845]
objective value function right now is: -1595.8268627473299
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.50327]
objective value function right now is: -1591.214570268379
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [184.57666]
objective value function right now is: -1584.7951101177136
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.59048]
objective value function right now is: -1588.5983290009867
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.98026]
objective value function right now is: -1595.5274946481152
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.89334]
objective value function right now is: -1590.7124311391972
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.48058]
objective value function right now is: -1577.4751475897217
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.41293]
objective value function right now is: -1585.4241310695274
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.618]
objective value function right now is: -1587.7055804415384
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [184.46355]
objective value function right now is: -1588.9687301321092
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.30052]
objective value function right now is: -1591.631998865771
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [174.31906]
objective value function right now is: -1563.222281932765
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [177.322]
objective value function right now is: -1580.5009653221784
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [179.63179]
objective value function right now is: -1588.7941031884673
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [178.50008]
objective value function right now is: -1585.6776536503605
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [179.39105]
objective value function right now is: -1589.7628640177684
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.5885]
objective value function right now is: -1591.3496899544853
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.69063]
objective value function right now is: -1578.9375042145941
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.48866]
objective value function right now is: -1591.9885683842192
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.6303]
objective value function right now is: -1576.8378454208887
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.97337]
objective value function right now is: -1531.8348333409947
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [179.24377]
objective value function right now is: -1566.0920032307936
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.23624]
objective value function right now is: -1595.4348925549086
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [179.55684]
objective value function right now is: -1589.4276591689952
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [181.92531]
objective value function right now is: -1588.077705155381
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.13358]
objective value function right now is: -1579.391976935785
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.72832]
objective value function right now is: -1577.0251755847353
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1596.9111650933314
Current xi:  [185.98503]
objective value function right now is: -1596.9111650933314
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.02797]
objective value function right now is: -1584.3920378506336
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.99976]
objective value function right now is: -1591.5787872316396
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.30939]
objective value function right now is: -1575.092701743457
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1601.8081188653716
Current xi:  [187.12639]
objective value function right now is: -1601.8081188653716
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1603.327970535421
Current xi:  [187.19333]
objective value function right now is: -1603.327970535421
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.05186]
objective value function right now is: -1603.0448530292379
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.86555]
objective value function right now is: -1603.1987616340868
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.92604]
objective value function right now is: -1601.4092473221394
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.29411]
objective value function right now is: -1599.1892534233903
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.35606]
objective value function right now is: -1601.6255729273778
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.46397]
objective value function right now is: -1602.7311580060664
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.59117]
objective value function right now is: -1603.312473500894
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.23343]
objective value function right now is: -1603.1212714144285
new min fval from sgd:  -1603.3583329977644
new min fval from sgd:  -1603.4438198998719
new min fval from sgd:  -1603.553519520997
new min fval from sgd:  -1603.662825919244
new min fval from sgd:  -1603.7934406577874
new min fval from sgd:  -1603.9287622343818
new min fval from sgd:  -1603.9987520692712
new min fval from sgd:  -1604.0910828251838
new min fval from sgd:  -1604.1494035289857
new min fval from sgd:  -1604.1996472247392
new min fval from sgd:  -1604.2692799479928
new min fval from sgd:  -1604.3114678052264
new min fval from sgd:  -1604.33983458165
new min fval from sgd:  -1604.3765669253305
new min fval from sgd:  -1604.4693543047622
new min fval from sgd:  -1604.5608026104887
new min fval from sgd:  -1604.6206867328845
new min fval from sgd:  -1604.6799736426356
new min fval from sgd:  -1604.7445422239061
new min fval from sgd:  -1604.7811756703074
new min fval from sgd:  -1604.7898730373306
new min fval from sgd:  -1604.8763192301933
new min fval from sgd:  -1605.039074060321
new min fval from sgd:  -1605.33062186921
new min fval from sgd:  -1605.380842403113
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.44882]
objective value function right now is: -1600.7514101507525
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.56296]
objective value function right now is: -1602.6335297596906
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.94174]
objective value function right now is: -1603.969687499726
new min fval from sgd:  -1605.4107430175004
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.25021]
objective value function right now is: -1604.6844848811625
new min fval from sgd:  -1605.4147177377615
new min fval from sgd:  -1605.429551420607
new min fval from sgd:  -1605.4483585417636
new min fval from sgd:  -1605.4652532480206
new min fval from sgd:  -1605.4782133787787
new min fval from sgd:  -1605.4897583949842
new min fval from sgd:  -1605.497255945813
new min fval from sgd:  -1605.568519086145
new min fval from sgd:  -1605.58678821787
new min fval from sgd:  -1605.5959188485049
new min fval from sgd:  -1605.6023318739844
new min fval from sgd:  -1605.6062922539506
new min fval from sgd:  -1605.6155756415344
new min fval from sgd:  -1605.6213385141307
new min fval from sgd:  -1605.6319657689696
new min fval from sgd:  -1605.6410870593243
new min fval from sgd:  -1605.6459856571134
new min fval from sgd:  -1605.6540197478844
new min fval from sgd:  -1605.6609649684117
new min fval from sgd:  -1605.6657146601776
new min fval from sgd:  -1605.6686215672662
new min fval from sgd:  -1605.6697400531773
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.28986]
objective value function right now is: -1605.208049193736
min fval:  -1605.6697400531773
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 12.0180,   4.5747],
        [ -9.4298,  11.6964],
        [ 19.1985,  -5.5572],
        [ -5.9284,  15.9729],
        [ -1.0712,   0.4151],
        [ -1.0712,   0.4151],
        [-34.6130, -12.0360],
        [  5.2178, -10.4580],
        [ -1.0712,   0.4151],
        [ 19.8732,  -1.0409]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-16.7034,   5.8224, -11.8362,  11.5702,  -3.3279,  -3.3279, -10.3999,
         -5.8625,  -3.3279, -15.8742], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.4630e-02, -2.1259e-01, -2.8531e-01, -8.0947e-01, -6.9787e-03,
         -6.9787e-03, -5.0797e-02, -1.0727e+00, -6.9787e-03, -9.0795e-02],
        [-5.4630e-02, -2.1259e-01, -2.8531e-01, -8.0947e-01, -6.9787e-03,
         -6.9787e-03, -5.0797e-02, -1.0727e+00, -6.9787e-03, -9.0795e-02],
        [-5.4630e-02, -2.1259e-01, -2.8531e-01, -8.0947e-01, -6.9787e-03,
         -6.9787e-03, -5.0797e-02, -1.0727e+00, -6.9787e-03, -9.0795e-02],
        [ 1.0751e+01, -8.6000e+00,  1.5041e+01, -1.9637e+01,  1.6465e-01,
          1.6465e-01,  1.1279e+01,  2.2613e+00,  1.6465e-01,  2.4275e+01],
        [-5.4630e-02, -2.1259e-01, -2.8531e-01, -8.0947e-01, -6.9787e-03,
         -6.9787e-03, -5.0797e-02, -1.0727e+00, -6.9787e-03, -9.0795e-02],
        [-5.4630e-02, -2.1259e-01, -2.8531e-01, -8.0947e-01, -6.9787e-03,
         -6.9787e-03, -5.0797e-02, -1.0727e+00, -6.9787e-03, -9.0795e-02],
        [-5.4630e-02, -2.1259e-01, -2.8531e-01, -8.0947e-01, -6.9787e-03,
         -6.9787e-03, -5.0797e-02, -1.0727e+00, -6.9787e-03, -9.0795e-02],
        [ 2.2890e-01,  7.3897e-01,  8.5531e-01,  2.0668e+00,  3.9672e-02,
          3.9672e-02,  1.4347e-01,  2.0516e+00,  3.9672e-02,  2.7665e-01],
        [ 9.7122e+00, -8.1611e+00,  1.6433e+01, -1.7535e+01, -1.0961e-01,
         -1.0961e-01,  1.0525e+01,  3.5697e+00, -1.0961e-01,  1.9977e+01],
        [-5.4630e-02, -2.1259e-01, -2.8531e-01, -8.0947e-01, -6.9787e-03,
         -6.9787e-03, -5.0797e-02, -1.0727e+00, -6.9787e-03, -9.0795e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.5266, -1.5266, -1.5266,  2.2134, -1.5266, -1.5266, -1.5266,  3.4422,
         2.8742, -1.5266], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.0339e-02,  1.0339e-02,  1.0339e-02, -1.8501e+01,  1.0339e-02,
          1.0339e-02,  1.0339e-02,  1.2117e+01, -1.2410e+01,  1.0339e-02]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 24.1610,   3.4132],
        [-23.7049,  -7.2224],
        [ -1.4687,   1.7374],
        [ -5.8593,   6.6831],
        [ 16.9535,   4.9234],
        [ -1.8988,   2.3548],
        [-24.8924,  -2.5241],
        [  5.1282,  14.3796],
        [-16.3271,  -6.9659],
        [ -1.3189,   1.4282]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-14.9139,  -5.7299,  -3.1004,   5.9036,   5.3249,  -3.6878,  11.9876,
         11.3578,  -1.5630,  -3.0027], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.5919e+00,  3.8966e+00, -1.1028e-01, -2.4062e+00, -4.2669e+00,
         -9.7248e-01,  1.1625e+00,  5.2168e+00,  3.1201e+00,  3.5617e-02],
        [-2.6852e+00,  2.0021e-01, -1.3838e-01, -3.7935e-01, -3.7174e+00,
         -1.4373e-01, -1.5498e+00,  2.8993e-01,  8.1211e-01, -1.4489e-01],
        [ 5.3717e+00, -4.9736e-03, -2.3518e-02, -8.8882e-01, -4.3379e+00,
         -2.0454e-01, -5.4180e+00, -6.9954e+00, -2.2141e+00,  9.0308e-02],
        [-3.7776e+00,  1.2592e+00, -3.2737e-01, -5.3898e-01, -3.2065e+00,
         -3.5679e-01, -1.6163e+00,  1.8408e+00,  1.5766e+00, -3.4905e-01],
        [-1.6313e+00,  1.6127e+01, -2.6133e+00, -4.8148e+00, -1.4060e+01,
         -1.2893e+00,  8.4599e+00, -6.1891e+01,  1.0150e+01, -3.1518e+00],
        [-3.8108e+00,  2.3817e+00, -3.3350e-01,  5.1350e-02, -3.3740e+00,
         -3.7390e-01, -1.3338e+00,  1.8142e+00,  1.6325e+00, -3.7449e-01],
        [-2.4313e+01,  5.1662e+00, -2.5906e-01,  1.7494e+00, -3.8289e+00,
         -2.6811e-01, -7.2840e-01, -3.3657e+01,  1.6980e+01, -3.1109e-01],
        [ 1.6694e+00, -1.3061e+01,  1.4264e-01,  2.8300e+01, -8.0444e-01,
          2.5645e-01, -9.1990e-03,  1.8108e+00, -1.1287e+01,  9.5788e-02],
        [-1.4683e+01, -8.8371e-01, -7.4126e-02, -8.7237e-01,  5.2776e-01,
         -2.1157e-01,  7.8131e-01, -1.4694e+01,  7.9698e+00, -3.1829e-02],
        [-3.7148e+00,  1.3866e+00, -2.9968e-01, -5.1326e-01, -3.2885e+00,
         -3.2618e-01, -1.5305e+00,  1.7721e+00,  1.6076e+00, -3.2255e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.8066, -3.6938, -4.0019, -3.1226, -3.8779, -3.2347, -0.9053, -0.8177,
        -1.2533, -3.1860], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  2.2327,   0.7429,   4.6638,   2.0951,  22.8279,   2.1941,  -9.6664,
           0.4808,  -0.8675,   2.0484],
        [ -2.2327,  -0.7430,  -4.6636,  -2.0951, -22.8252,  -2.1941,   9.7103,
          -0.4808,   0.9110,  -2.0484]], device='cuda:0'))])
xi:  [188.25026]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 674.8521640341438
W_T_median: 412.5718774000254
W_T_pctile_5: 188.2904098617617
W_T_CVAR_5_pct: 22.946528698911575
Average q (qsum/M+1):  48.094825006300404
Optimal xi:  [188.25026]
Observed VAR:  412.5718774000254
Expected(across Rb) median(across samples) p_equity:  0.25754524270693463
obj fun:  tensor(-1605.6697, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:198: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  9.8056,   0.3702],
        [ -4.3465,   8.7513],
        [ 11.6827,  -4.5826],
        [  0.3149,  11.8025],
        [ -0.7516,   1.0731],
        [ -0.7516,   1.0731],
        [-33.7974,  -9.1431],
        [  5.3946,  -4.5009],
        [ -0.7516,   1.0731],
        [ 10.9840,   1.5848]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -8.5826,   5.8637,  -9.1605,   9.5059,  -2.6000,  -2.6001,  -8.5071,
          1.6323,  -2.6001, -10.2237], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [ 5.2973e+00, -1.0926e+01,  1.1810e+01, -1.5601e+01, -4.0905e-02,
         -4.0919e-02,  1.3139e+01,  2.8679e+00, -4.0979e-02,  1.7610e+01],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-1.6569e-01,  3.5122e+00,  1.2751e+00,  1.6088e+01, -9.9319e-02,
         -9.9316e-02,  1.2755e+01, -4.1160e+00, -9.9300e-02, -6.4273e-02],
        [ 4.1958e+00, -1.0778e+01,  1.2722e+01, -1.4714e+01,  8.3874e-02,
          8.3891e-02,  1.2004e+01,  3.9389e+00,  8.3961e-02,  1.5171e+01],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.7286, -0.7286, -0.7286,  1.5126, -0.7286, -0.7286, -0.7286, -2.8475,
         2.4285, -0.7286], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 8.8609e-03,  8.8609e-03,  8.8609e-03, -1.4825e+01,  8.8609e-03,
          8.8609e-03,  8.8609e-03,  1.2082e+01, -1.0963e+01,  8.8609e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 17.5169,   2.4047],
        [-11.9075,  -5.4174],
        [ -2.9792,  -0.9803],
        [ -9.7518,  -4.0833],
        [ 11.3921,   5.1641],
        [ 13.7315,  -0.0707],
        [ -8.8519,  10.6386],
        [  7.6684,  14.1374],
        [-16.4605,  -4.5943],
        [ 14.0679,   5.0107]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-10.0398,  -4.7415,  -4.9321,  -4.8020,   4.9201, -12.9012,  10.5438,
         11.8370,   0.8090,  -3.9403], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.8323e+00,  4.5980e+00,  3.9185e-01,  3.6615e+00, -2.6056e+00,
         -5.2731e-02, -2.9941e+00,  7.1657e-01,  1.9169e+00, -3.9848e+00],
        [ 9.5454e+00,  1.7276e+00, -6.8431e-02,  9.5238e-03, -2.5478e+00,
          5.6171e+00,  1.4304e+01,  1.7821e+01, -1.7394e+00,  4.0096e+00],
        [-6.9749e-01, -1.6425e-01,  1.4400e-02, -9.4509e-02, -2.5979e+00,
         -2.4345e-01, -1.8580e-01, -1.0674e+00, -4.0907e-01, -8.7679e-01],
        [-1.5408e+00,  1.3537e+00,  1.1014e-01,  6.5129e-01, -2.6476e+00,
         -2.6944e-01, -5.7968e+00,  1.5919e+00,  1.8653e+00, -2.8750e+00],
        [ 1.5086e+00,  9.7123e+00,  5.2281e-01,  4.8078e+00, -1.4468e+01,
          2.2508e+00,  6.0173e-01, -3.3019e+01,  1.0199e+01, -2.1604e+00],
        [-3.3438e+00,  1.9099e+00, -1.5542e-01,  1.1551e+00, -1.7673e+00,
         -7.1039e+00,  1.1353e+01,  2.7938e+00, -3.5859e+00, -3.1004e+00],
        [-5.8361e+00,  4.0316e+00,  4.1967e-01,  1.6997e+00, -2.4723e+00,
         -6.8854e+00, -5.0376e+00, -3.2686e+01,  1.5702e+01, -6.0404e+00],
        [-7.3342e-01, -1.6558e-01,  1.4866e-02, -9.5380e-02, -2.5807e+00,
         -2.4601e-01, -1.7490e-01, -1.0871e+00, -4.0997e-01, -8.5503e-01],
        [-7.4823e+00,  3.6824e+00,  2.7743e-01,  3.3016e+00,  7.7796e-01,
          6.7700e-01, -9.1973e+00, -5.8992e+00,  3.2522e+00, -2.3277e+00],
        [-2.7056e-02,  1.0243e+00,  2.5504e-01,  9.7638e-01, -2.4098e+00,
          2.7460e-01, -9.3762e+00,  2.3750e+00,  2.9668e+00, -3.1138e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.3811, -8.4615, -2.6506, -2.6500,  3.0872, -5.6053,  1.2951, -2.6386,
        -1.2181, -2.5183], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.7230,   0.1490,  -0.2400,   1.4434,  14.4126,   0.3764,  -7.1038,
          -0.2402,  -2.2275,   1.6248],
        [ -1.7230,  -0.3536,   0.2400,  -1.4433, -14.4142,  -0.5248,   7.1825,
           0.2402,   2.2745,  -1.6251]], device='cuda:0'))])
loaded xi:  58.6305
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 12.0180,   4.5747],
        [ -9.4298,  11.6964],
        [ 19.1985,  -5.5572],
        [ -5.9284,  15.9729],
        [ -1.0712,   0.4151],
        [ -1.0712,   0.4151],
        [-34.6130, -12.0360],
        [  5.2178, -10.4580],
        [ -1.0712,   0.4151],
        [ 19.8732,  -1.0409]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-16.7034,   5.8224, -11.8362,  11.5702,  -3.3279,  -3.3279, -10.3999,
         -5.8625,  -3.3279, -15.8742], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.4630e-02, -2.1259e-01, -2.8531e-01, -8.0947e-01, -6.9787e-03,
         -6.9787e-03, -5.0797e-02, -1.0727e+00, -6.9787e-03, -9.0795e-02],
        [-5.4630e-02, -2.1259e-01, -2.8531e-01, -8.0947e-01, -6.9787e-03,
         -6.9787e-03, -5.0797e-02, -1.0727e+00, -6.9787e-03, -9.0795e-02],
        [-5.4630e-02, -2.1259e-01, -2.8531e-01, -8.0947e-01, -6.9787e-03,
         -6.9787e-03, -5.0797e-02, -1.0727e+00, -6.9787e-03, -9.0795e-02],
        [ 1.0751e+01, -8.6000e+00,  1.5041e+01, -1.9637e+01,  1.6465e-01,
          1.6465e-01,  1.1279e+01,  2.2613e+00,  1.6465e-01,  2.4275e+01],
        [-5.4630e-02, -2.1259e-01, -2.8531e-01, -8.0947e-01, -6.9787e-03,
         -6.9787e-03, -5.0797e-02, -1.0727e+00, -6.9787e-03, -9.0795e-02],
        [-5.4630e-02, -2.1259e-01, -2.8531e-01, -8.0947e-01, -6.9787e-03,
         -6.9787e-03, -5.0797e-02, -1.0727e+00, -6.9787e-03, -9.0795e-02],
        [-5.4630e-02, -2.1259e-01, -2.8531e-01, -8.0947e-01, -6.9787e-03,
         -6.9787e-03, -5.0797e-02, -1.0727e+00, -6.9787e-03, -9.0795e-02],
        [ 2.2890e-01,  7.3897e-01,  8.5531e-01,  2.0668e+00,  3.9672e-02,
          3.9672e-02,  1.4347e-01,  2.0516e+00,  3.9672e-02,  2.7665e-01],
        [ 9.7122e+00, -8.1611e+00,  1.6433e+01, -1.7535e+01, -1.0961e-01,
         -1.0961e-01,  1.0525e+01,  3.5697e+00, -1.0961e-01,  1.9977e+01],
        [-5.4630e-02, -2.1259e-01, -2.8531e-01, -8.0947e-01, -6.9787e-03,
         -6.9787e-03, -5.0797e-02, -1.0727e+00, -6.9787e-03, -9.0795e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.5266, -1.5266, -1.5266,  2.2134, -1.5266, -1.5266, -1.5266,  3.4422,
         2.8742, -1.5266], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.0339e-02,  1.0339e-02,  1.0339e-02, -1.8501e+01,  1.0339e-02,
          1.0339e-02,  1.0339e-02,  1.2117e+01, -1.2410e+01,  1.0339e-02]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 24.1610,   3.4132],
        [-23.7049,  -7.2224],
        [ -1.4687,   1.7374],
        [ -5.8593,   6.6831],
        [ 16.9535,   4.9234],
        [ -1.8988,   2.3548],
        [-24.8924,  -2.5241],
        [  5.1282,  14.3796],
        [-16.3271,  -6.9659],
        [ -1.3189,   1.4282]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-14.9139,  -5.7299,  -3.1004,   5.9036,   5.3249,  -3.6878,  11.9876,
         11.3578,  -1.5630,  -3.0027], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.5919e+00,  3.8966e+00, -1.1028e-01, -2.4062e+00, -4.2669e+00,
         -9.7248e-01,  1.1625e+00,  5.2168e+00,  3.1201e+00,  3.5617e-02],
        [-2.6852e+00,  2.0021e-01, -1.3838e-01, -3.7935e-01, -3.7174e+00,
         -1.4373e-01, -1.5498e+00,  2.8993e-01,  8.1211e-01, -1.4489e-01],
        [ 5.3717e+00, -4.9736e-03, -2.3518e-02, -8.8882e-01, -4.3379e+00,
         -2.0454e-01, -5.4180e+00, -6.9954e+00, -2.2141e+00,  9.0308e-02],
        [-3.7776e+00,  1.2592e+00, -3.2737e-01, -5.3898e-01, -3.2065e+00,
         -3.5679e-01, -1.6163e+00,  1.8408e+00,  1.5766e+00, -3.4905e-01],
        [-1.6313e+00,  1.6127e+01, -2.6133e+00, -4.8148e+00, -1.4060e+01,
         -1.2893e+00,  8.4599e+00, -6.1891e+01,  1.0150e+01, -3.1518e+00],
        [-3.8108e+00,  2.3817e+00, -3.3350e-01,  5.1350e-02, -3.3740e+00,
         -3.7390e-01, -1.3338e+00,  1.8142e+00,  1.6325e+00, -3.7449e-01],
        [-2.4313e+01,  5.1662e+00, -2.5906e-01,  1.7494e+00, -3.8289e+00,
         -2.6811e-01, -7.2840e-01, -3.3657e+01,  1.6980e+01, -3.1109e-01],
        [ 1.6694e+00, -1.3061e+01,  1.4264e-01,  2.8300e+01, -8.0444e-01,
          2.5645e-01, -9.1990e-03,  1.8108e+00, -1.1287e+01,  9.5788e-02],
        [-1.4683e+01, -8.8371e-01, -7.4126e-02, -8.7237e-01,  5.2776e-01,
         -2.1157e-01,  7.8131e-01, -1.4694e+01,  7.9698e+00, -3.1829e-02],
        [-3.7148e+00,  1.3866e+00, -2.9968e-01, -5.1326e-01, -3.2885e+00,
         -3.2618e-01, -1.5305e+00,  1.7721e+00,  1.6076e+00, -3.2255e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.8066, -3.6938, -4.0019, -3.1226, -3.8779, -3.2347, -0.9053, -0.8177,
        -1.2533, -3.1860], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  2.2327,   0.7429,   4.6638,   2.0951,  22.8279,   2.1941,  -9.6664,
           0.4808,  -0.8675,   2.0484],
        [ -2.2327,  -0.7430,  -4.6636,  -2.0951, -22.8252,  -2.1941,   9.7103,
          -0.4808,   0.9110,  -2.0484]], device='cuda:0'))])
loaded xi:  188.25026
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2722.4625540934153
Current xi:  [197.06357]
objective value function right now is: -2722.4625540934153
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -2810.251590653754
Current xi:  [203.62868]
objective value function right now is: -2810.251590653754
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.072]
objective value function right now is: -2805.9189862887206
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.87683]
objective value function right now is: -2598.4630354517576
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.17125]
objective value function right now is: -2798.41470569308
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.10983]
objective value function right now is: -2555.3345748008974
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [211.7398]
objective value function right now is: -2741.5322988271073
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.59644]
objective value function right now is: -2747.824283377194
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.09557]
objective value function right now is: -2786.7701000995703
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -2815.0248936835765
Current xi:  [212.01768]
objective value function right now is: -2815.0248936835765
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.7877]
objective value function right now is: -2657.6097595876226
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.74678]
objective value function right now is: -2742.592625825226
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.72693]
objective value function right now is: -2775.482358916737
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [211.24826]
objective value function right now is: -2800.53834061093
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.97958]
objective value function right now is: -2728.640208315502
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.16736]
objective value function right now is: -2793.4825044481117
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -2823.9368493343877
Current xi:  [213.9965]
objective value function right now is: -2823.9368493343877
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.39671]
objective value function right now is: -2721.5197185447755
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.73108]
objective value function right now is: -2791.7330313137854
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.98192]
objective value function right now is: -2752.0724155446646
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.47461]
objective value function right now is: -2743.54862750064
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.93454]
objective value function right now is: -2729.817937855075
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.38707]
objective value function right now is: -2717.1471060831577
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.70363]
objective value function right now is: -2782.735295300061
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -2840.2668541857033
Current xi:  [212.66087]
objective value function right now is: -2840.2668541857033
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.04395]
objective value function right now is: -2767.917142644019
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.34424]
objective value function right now is: -2769.65783954753
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [211.3619]
objective value function right now is: -2562.069903095287
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [212.11673]
objective value function right now is: -2751.953099061672
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.70518]
objective value function right now is: -2817.2464229114785
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.41013]
objective value function right now is: -2818.7797726777167
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.7152]
objective value function right now is: -2802.3626951002284
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.3554]
objective value function right now is: -2711.3769661008037
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.4323]
objective value function right now is: -2750.5943524662193
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.04166]
objective value function right now is: -2787.544178145771
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2840.604048746112
Current xi:  [212.02975]
objective value function right now is: -2840.604048746112
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -2866.413867337117
Current xi:  [212.0195]
objective value function right now is: -2866.413867337117
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.51164]
objective value function right now is: -2808.2428842489253
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.59285]
objective value function right now is: -2845.476872209346
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -2881.951700330398
Current xi:  [212.64494]
objective value function right now is: -2881.951700330398
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.00497]
objective value function right now is: -2535.7921376943787
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -2886.2073485615138
Current xi:  [213.06792]
objective value function right now is: -2886.2073485615138
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.47522]
objective value function right now is: -2858.416917815063
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -2888.6292505194333
Current xi:  [213.4807]
objective value function right now is: -2888.6292505194333
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.51682]
objective value function right now is: -2886.1954746508823
new min fval from sgd:  -2888.991453089943
new min fval from sgd:  -2890.379091424388
new min fval from sgd:  -2890.666007675011
new min fval from sgd:  -2891.3487141844585
new min fval from sgd:  -2891.693780511843
new min fval from sgd:  -2891.9509543785707
new min fval from sgd:  -2892.2142813787696
new min fval from sgd:  -2892.347392851652
new min fval from sgd:  -2892.4591769495582
new min fval from sgd:  -2893.9250327700934
new min fval from sgd:  -2894.991491427199
new min fval from sgd:  -2895.0605121587987
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.81725]
objective value function right now is: -2850.1278910534415
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.8537]
objective value function right now is: -2888.2222624966967
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.02214]
objective value function right now is: -2878.93892048001
new min fval from sgd:  -2895.1495316661235
new min fval from sgd:  -2895.3134179507615
new min fval from sgd:  -2895.4743395295527
new min fval from sgd:  -2895.6136686233654
new min fval from sgd:  -2895.9497208111434
new min fval from sgd:  -2896.1467787582096
new min fval from sgd:  -2896.3159461405407
new min fval from sgd:  -2896.49531434105
new min fval from sgd:  -2896.6593936382164
new min fval from sgd:  -2896.8941854041964
new min fval from sgd:  -2897.0156813166645
new min fval from sgd:  -2897.065271547649
new min fval from sgd:  -2897.1658717401533
new min fval from sgd:  -2897.2544101892213
new min fval from sgd:  -2897.287642305076
new min fval from sgd:  -2897.3390677082325
new min fval from sgd:  -2897.4021871969267
new min fval from sgd:  -2897.425017386251
new min fval from sgd:  -2897.4764703077744
new min fval from sgd:  -2897.5071474628357
new min fval from sgd:  -2897.5535672830074
new min fval from sgd:  -2897.570924675625
new min fval from sgd:  -2897.5755942980836
new min fval from sgd:  -2897.5779038119254
new min fval from sgd:  -2897.636365963592
new min fval from sgd:  -2897.684280663504
new min fval from sgd:  -2897.869494624588
new min fval from sgd:  -2898.093251089431
new min fval from sgd:  -2898.3255935451416
new min fval from sgd:  -2898.50538420665
new min fval from sgd:  -2898.6956952368787
new min fval from sgd:  -2898.8268279038302
new min fval from sgd:  -2898.9099427796623
new min fval from sgd:  -2898.9448476992875
new min fval from sgd:  -2898.959075131547
new min fval from sgd:  -2898.98817463597
new min fval from sgd:  -2899.0085110717096
new min fval from sgd:  -2899.0377622624355
new min fval from sgd:  -2899.133302683462
new min fval from sgd:  -2899.2171234100965
new min fval from sgd:  -2899.298078621853
new min fval from sgd:  -2899.3514879960985
new min fval from sgd:  -2899.374286985094
new min fval from sgd:  -2899.4040703475466
new min fval from sgd:  -2899.4282595995264
new min fval from sgd:  -2899.491123926416
new min fval from sgd:  -2899.649070884043
new min fval from sgd:  -2899.7543596880773
new min fval from sgd:  -2899.7879170452234
new min fval from sgd:  -2899.8006294583156
new min fval from sgd:  -2899.8155473031456
new min fval from sgd:  -2899.835649605839
new min fval from sgd:  -2899.835781114039
new min fval from sgd:  -2899.8428099870785
new min fval from sgd:  -2899.845356792492
new min fval from sgd:  -2899.8618873847718
new min fval from sgd:  -2899.93285573826
new min fval from sgd:  -2900.0206635339714
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.87422]
objective value function right now is: -2897.383848573834
new min fval from sgd:  -2900.0262153976296
new min fval from sgd:  -2900.083907300341
new min fval from sgd:  -2900.116033434673
new min fval from sgd:  -2900.180793296663
new min fval from sgd:  -2900.231531055309
new min fval from sgd:  -2900.3013855733548
new min fval from sgd:  -2900.3736296296306
new min fval from sgd:  -2900.4432855528516
new min fval from sgd:  -2900.534162200014
new min fval from sgd:  -2900.557971070629
new min fval from sgd:  -2900.5988565343546
new min fval from sgd:  -2900.7461536217024
new min fval from sgd:  -2900.899434362302
new min fval from sgd:  -2901.0406449194543
new min fval from sgd:  -2901.1029243904113
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.90332]
objective value function right now is: -2897.153797304596
min fval:  -2901.1029243904113
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 15.9573,   4.2717],
        [-11.3126,  11.8794],
        [ 21.6399,  -7.9335],
        [ -9.0070,  18.0451],
        [ -1.4389,   0.7514],
        [ -1.4389,   0.7514],
        [-23.1971, -13.1371],
        [  1.5381,  -0.4766],
        [ -1.4389,   0.7514],
        [ 22.9603,  -1.9320]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-14.1704,   5.4255, -12.1537,  12.5694,  -4.8622,  -4.8622, -11.5721,
         -6.1358,  -4.8622, -16.2226], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.6360e-01, -1.9006e-01, -1.2299e-01, -1.1283e+00,  3.5527e-04,
          3.5526e-04, -4.7454e-02, -1.9093e-03,  3.5527e-04, -8.8472e-02],
        [-1.6360e-01, -1.9006e-01, -1.2299e-01, -1.1283e+00,  3.5526e-04,
          3.5526e-04, -4.7454e-02, -1.9093e-03,  3.5526e-04, -8.8472e-02],
        [-1.6360e-01, -1.9006e-01, -1.2299e-01, -1.1283e+00,  3.5527e-04,
          3.5526e-04, -4.7454e-02, -1.9093e-03,  3.5526e-04, -8.8472e-02],
        [ 4.7583e+00, -7.4503e+00,  1.9460e+01, -1.8158e+01,  3.9436e-01,
          3.9436e-01,  9.0553e+00, -1.2117e-01,  3.9436e-01,  2.6081e+01],
        [-1.6360e-01, -1.9006e-01, -1.2299e-01, -1.1283e+00,  3.5527e-04,
          3.5526e-04, -4.7454e-02, -1.9093e-03,  3.5526e-04, -8.8472e-02],
        [-1.6360e-01, -1.9006e-01, -1.2299e-01, -1.1283e+00,  3.5527e-04,
          3.5526e-04, -4.7454e-02, -1.9093e-03,  3.5526e-04, -8.8472e-02],
        [-1.6360e-01, -1.9006e-01, -1.2299e-01, -1.1283e+00,  3.5527e-04,
          3.5526e-04, -4.7454e-02, -1.9093e-03,  3.5526e-04, -8.8472e-02],
        [ 2.2740e-01,  3.1460e-01,  1.1077e-01,  2.1977e+00, -4.7094e-03,
         -4.7094e-03,  1.0368e-01, -2.1239e-03, -4.7094e-03,  7.9800e-02],
        [ 4.0667e+00, -5.7002e+00,  1.9946e+01, -1.6376e+01,  1.2418e-01,
          1.2418e-01,  6.0873e+00,  6.7147e-01,  1.2418e-01,  2.2867e+01],
        [-1.6360e-01, -1.9006e-01, -1.2299e-01, -1.1283e+00,  3.5526e-04,
          3.5526e-04, -4.7454e-02, -1.9093e-03,  3.5526e-04, -8.8472e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.7547, -2.7547, -2.7547,  4.4225, -2.7547, -2.7547, -2.7547,  5.0554,
         4.9517, -2.7547], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0450,  -0.0450,  -0.0450, -19.4016,  -0.0450,  -0.0450,  -0.0450,
          10.8120, -11.6812,  -0.0450]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 24.7844,   1.6897],
        [-20.0509,  -8.3261],
        [ -7.1006,   4.8047],
        [ -6.3891,   9.4631],
        [ 13.8682,   5.4033],
        [ -6.7136,   4.3453],
        [-26.1903,  -3.9824],
        [  6.6782,  16.3974],
        [-15.2954,  -8.2968],
        [ -6.7583,   4.4254]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-18.1304,  -7.3575,  -7.7457,   7.7131,   8.0369,  -7.5528,  11.5102,
         12.2634,  -4.0177,  -7.4048], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-8.2950e+00,  3.2613e+00, -9.7372e-01, -6.0969e-01, -5.0245e+00,
         -9.4784e-01, -7.3839e-01,  7.0949e+00,  4.6830e+00, -9.5051e-01],
        [-6.3215e-01, -7.2696e-01, -2.3071e+00,  2.2494e+00, -5.7568e+00,
         -2.1292e+00,  2.2286e-01,  2.1112e+00,  8.7406e-01, -1.9245e+00],
        [ 5.9199e+00, -9.0395e+00, -4.0079e+00,  4.8169e+00, -4.9018e+00,
         -1.9381e+00, -2.1048e+01, -7.7085e+00,  6.6878e-01, -1.9613e+00],
        [-1.0263e+00,  1.4082e+00, -2.0927e+00,  1.3960e+00, -5.3925e+00,
         -2.4807e+00,  5.4018e-01,  3.1141e+00,  1.8246e+00, -2.3239e+00],
        [-1.5015e+01,  1.8087e+01, -3.5146e-02, -5.3366e+00, -1.6291e+01,
         -5.5837e-02,  9.6832e+00, -8.8561e+01,  1.3159e+01, -5.7541e-02],
        [-7.7575e-01,  7.7852e-01, -2.9770e+00,  1.6564e+00, -4.8997e+00,
         -2.6101e+00,  6.2243e-01,  3.6169e+00,  2.4298e+00, -2.4161e+00],
        [-3.8120e+01,  5.4268e+00, -2.1891e-02, -3.2236e+00, -4.9821e+00,
         -5.5061e-02,  2.4079e+00, -3.3426e+01,  1.7072e+01, -6.3286e-02],
        [ 1.7544e+00, -1.1315e+01, -9.2739e-03,  3.0345e+01, -6.5036e-01,
         -8.5020e-03,  2.0240e-01,  1.5372e+00, -1.4388e+01, -7.3500e-03],
        [-1.2427e+01,  2.5582e-01, -6.5023e-02, -5.7182e+00, -4.2457e-02,
         -8.9153e-02,  9.5608e-01, -1.0679e+01,  7.3519e+00, -9.3101e-02],
        [-5.4511e-01,  3.1937e-01, -2.0239e+00,  2.0905e+00, -4.8879e+00,
         -3.1627e+00,  9.4008e-01,  3.2277e+00,  2.6202e+00, -2.9159e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-5.6526, -5.8500, -4.5669, -5.3682, -6.2237, -4.8587, -2.0798, -0.6728,
        -1.8222, -4.8813], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  4.9110,   3.7832,   6.0859,   4.6499,  31.1925,   5.2806, -11.4445,
           0.4969,  -1.6647,   4.9439],
        [ -4.9110,  -3.7832,  -6.0857,  -4.6499, -31.1899,  -5.2806,  11.4855,
          -0.4969,   1.7082,  -4.9439]], device='cuda:0'))])
xi:  [213.87166]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 683.6349172676934
W_T_median: 486.81030972255894
W_T_pctile_5: 214.11136757287875
W_T_CVAR_5_pct: 29.71390568672994
Average q (qsum/M+1):  45.658911920362904
Optimal xi:  [213.87166]
Observed VAR:  486.81030972255894
Expected(across Rb) median(across samples) p_equity:  0.20609308406710625
obj fun:  tensor(-2901.1029, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:198: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
