/home/marcchen/Documents/constrain_factor/researchcode/exp_config_json_files/multi_portfolio_TEST1.json
Starting at: 
18-07-23_10:39

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 10
batchsize: 10
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor4
timeseries_basket['basket_desc'] = Factor4 portfolio for paper: Basic, size, value, vol, mom
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret', 'Vol_Lo20_real_ret', 'Mom_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor4
timeseries_basket['basket_desc'] = Factor4 portfolio for paper: Basic, size, value, vol, mom
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret', 'Vol_Lo20_nom_ret', 'Mom_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.005383     0.031411
192608                    0.0319              0.0561  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0355  ...     0.005323    -0.028996
192611                   -0.0038              0.0294  ...     0.005303     0.028554

[5 rows x 8 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.043289    -0.036240
202209                   -0.0955             -0.0871  ...    -0.050056    -0.091324
202210                    0.0883              0.1486  ...    -0.014968     0.077403
202211                   -0.0076              0.0462  ...     0.040789     0.052365
202212                   -0.0457             -0.0499  ...    -0.018566    -0.057116

[5 rows x 8 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'Mom_Hi30_nom_ret_ind', 'Vol_Lo20_nom_ret_ind', 'CPI_nom_ret_ind',
       'T30_nom_ret_ind', 'B10_nom_ret_ind', 'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
Vol_Lo20_real_ret      0.003529
Mom_Hi30_real_ret      0.011386
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
Vol_Lo20_real_ret      0.030737
Mom_Hi30_real_ret      0.061421
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Mom_Hi30_real_ret
T30_real_ret             1.000000  ...           0.055142
B10_real_ret             0.351722  ...           0.066570
VWD_real_ret             0.068448  ...           0.936115
Size_Lo30_real_ret       0.014412  ...           0.903222
Value_Hi30_real_ret      0.018239  ...           0.869469
Vol_Lo20_real_ret        0.081282  ...           0.482682
Mom_Hi30_real_ret        0.055142  ...           1.000000

[7 rows x 7 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 18
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      15  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      15  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 15)     True          15  
2     (15, 15)     True          15  
3      (15, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       7       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      15  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      15  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       7           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 15)     True          15  
2     (15, 15)     True          15  
3      (15, 7)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       15  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       15  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 15)      True          15  
0     (15, 15)      True          15  
0      (15, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       15  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       15  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        7           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 15)      True          15  
0     (15, 15)      True          15  
0      (15, 7)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 0, 'itbound_SGD_algorithms': 10, 'nit_IterateAveragingStart': 9, 'batchsize': 10, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.0, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       15  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       15  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 15)      True          15  
0     (15, 15)      True          15  
0      (15, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       15  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       15  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        7           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 15)      True          15  
0     (15, 15)      True          15  
0      (15, 7)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  7.5853,  -2.4158],
        [-11.2703,   5.0152],
        [  9.9490,  -0.7813],
        [ -5.2532,  10.7978],
        [ -0.9049,  -0.3972],
        [  9.7710,  -0.9981],
        [ -1.6154,   0.2263],
        [ -2.4890,  11.9357],
        [-15.4529,  -9.2488],
        [ 10.0907,  -0.7558],
        [-11.0426,   5.0470],
        [ 10.2122,  -0.8103],
        [ -0.9049,  -0.3972],
        [ -0.9467,  13.8227],
        [  7.7564,  -2.1683]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-8.6100,  0.4023, -8.7903, -0.0091, -3.3355, -8.7333, -3.2129,  0.8343,
         5.1041, -8.8837,  0.1267, -8.8976, -3.3355,  1.5231, -8.7892],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 4.5794e-01, -4.5401e+00,  4.6484e+00, -2.3711e+00, -1.4731e-01,
          3.4503e+00, -1.1651e-01, -1.7946e+00,  4.7787e+00,  5.0336e+00,
         -4.2806e+00,  5.5352e+00, -1.4744e-01, -1.5902e+00,  5.4051e-01],
        [-5.9536e-02, -2.6107e-02, -5.3493e-02, -7.9777e-01, -4.1391e-02,
         -2.0935e-02, -1.0359e-02, -5.3479e-01, -5.2845e-01, -5.8833e-02,
         -6.5195e-03, -5.3934e-02, -4.1392e-02, -5.4903e-01, -4.9991e-02],
        [-6.3178e-02, -2.3072e-02, -2.2490e-02, -7.9225e-01, -4.7377e-02,
          8.9707e-04, -1.3464e-02, -4.9145e-01, -2.8756e-01, -2.4828e-02,
         -6.0819e-03, -1.9377e-02, -4.7378e-02, -4.7941e-01, -5.2529e-02],
        [-1.6246e-01,  1.5479e+00, -1.9365e-01, -3.2436e+00,  3.5504e-02,
         -1.9659e-01,  1.3514e-01, -4.0840e+00,  5.4427e+00, -1.8758e-01,
          1.7880e+00, -2.0151e-01,  3.5520e-02, -4.7788e+00, -1.4841e-01],
        [-5.4629e-02,  4.3674e-02,  5.6218e-01, -1.1173e+00, -5.6825e-02,
          3.6666e-01, -2.2125e-02, -7.9788e-01, -7.2299e-02,  6.5892e-01,
          4.9833e-02,  7.1048e-01, -5.6825e-02, -5.6026e-01, -4.2799e-02],
        [ 8.4784e-01, -4.7861e+00,  6.1889e+00, -2.8612e+00, -6.1410e-02,
          4.3638e+00, -2.0339e-01, -2.0713e+00,  6.8916e+00,  6.6928e+00,
         -4.4884e+00,  7.4215e+00, -6.1464e-02, -1.8598e+00,  1.2486e+00],
        [-1.9504e-01,  1.0148e+00, -4.1804e-01, -4.1012e+00,  3.6645e-02,
         -4.2285e-01,  2.1457e-01, -4.9066e+00,  6.9624e+00, -4.0721e-01,
          1.3263e+00, -4.3230e-01,  3.6709e-02, -5.7814e+00, -1.7827e-01],
        [ 1.8183e-01,  6.9756e-02,  4.0797e+00, -2.0958e+00, -1.4983e-02,
          3.2340e+00, -1.2090e-02, -1.8987e+00, -6.4879e-03,  4.5711e+00,
          7.4153e-02,  4.6731e+00, -1.4983e-02, -1.3424e+00,  1.8142e-01],
        [-5.9514e-02, -2.6169e-02, -5.3407e-02, -7.9729e-01, -4.1014e-02,
         -2.1056e-02, -1.0200e-02, -5.3169e-01, -5.2481e-01, -5.8707e-02,
         -6.6648e-03, -5.3814e-02, -4.1016e-02, -5.4455e-01, -4.9991e-02],
        [ 2.0563e-01,  2.6594e+00,  3.2773e-01,  3.5166e+00,  5.6551e-02,
          3.7418e-01, -6.3300e-05,  3.8221e+00, -7.8716e+00,  2.9208e-01,
          2.3704e+00,  2.8500e-01,  5.6749e-02,  4.5574e+00,  1.8497e-01],
        [ 1.3958e-01,  4.2657e+00, -1.6442e+00,  3.3046e+00, -5.3697e-02,
         -1.3324e+00, -6.0006e-02,  3.0370e+00, -1.0699e+01, -1.9740e+00,
          3.9441e+00, -1.9610e+00, -5.4075e-02,  4.1353e+00,  9.6603e-02],
        [ 5.0471e+00, -1.6574e+00,  1.0791e+01, -5.9458e+00,  1.4029e-01,
          9.6805e+00,  2.4305e-02, -5.3917e+00, -1.6123e+00,  1.1477e+01,
         -1.5293e+00,  1.1719e+01,  1.4032e-01, -4.0970e+00,  4.3942e+00],
        [ 9.5327e-01, -4.7789e+00,  6.3886e+00, -2.7667e+00, -3.1029e-02,
          4.5855e+00, -2.2168e-01, -1.9909e+00,  7.1586e+00,  6.9213e+00,
         -4.4812e+00,  7.5774e+00, -3.1035e-02, -1.8317e+00,  1.3877e+00],
        [-6.2744e-02, -2.1177e-02, -3.6872e-02, -7.8205e-01, -4.3049e-02,
         -1.0704e-02, -1.1199e-02, -5.1509e-01, -3.7102e-01, -4.0443e-02,
         -5.0260e-03, -3.5324e-02, -4.3050e-02, -5.4398e-01, -5.2808e-02],
        [ 1.3272e-01,  4.3319e+00, -1.2802e+00,  3.5993e+00, -2.7607e-02,
         -1.1133e+00,  1.4604e-02,  3.2222e+00, -1.0221e+01, -1.7027e+00,
          4.1238e+00, -1.6542e+00, -2.7400e-02,  4.3304e+00,  1.1722e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 0.1036, -1.2523, -2.0086, -1.7029, -3.3248, -1.3729, -1.4975, -5.7875,
        -1.2657,  0.5232,  0.5890, -4.2458, -1.7312, -1.5660,  0.1889],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -3.3399,  -0.1084,  -0.2994,  -4.4105,  -1.0311,  -5.1279,  -5.8858,
          -2.8673,  -0.1087,   3.6576,   7.7776, -13.0918,  -5.1789,  -0.1642,
           7.0091]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.1710,   0.2032],
        [ -2.1762,   0.1873],
        [ 11.2387,   0.2672],
        [ -0.6025,   4.8412],
        [ -2.1728,   0.2039],
        [-22.3047,   3.6989],
        [ -2.1712,   0.2031],
        [ -2.1711,   0.2032],
        [ -2.1712,   0.2031],
        [ -7.5783,  11.4133],
        [ -2.1712,   0.2031],
        [-14.1485,  -3.6735],
        [  3.9173,  -2.8464],
        [ -8.4588,   2.9460],
        [  9.1526,   4.4924]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -4.7924,  -4.7593, -10.5870,   2.1966,  -4.7844,   1.0276,  -4.7917,
         -4.7897,  -4.7917,   1.6115,  -4.7917,   1.3229, -11.5095,   2.4089,
         -2.0863], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 4.8220e-02,  5.0614e-02, -5.7704e-01, -1.2763e-01,  4.8547e-02,
         -8.7817e-01,  4.8245e-02,  4.8302e-02,  4.8247e-02,  1.8107e+00,
          4.8245e-02, -5.3373e-01, -8.2827e-01,  2.6728e-01, -1.8613e+00],
        [ 4.9516e-02,  5.2076e-02, -5.7830e-01, -1.2936e-01,  4.9859e-02,
         -8.8747e-01,  4.9542e-02,  4.9603e-02,  4.9545e-02,  1.8263e+00,
          4.9542e-02, -5.3623e-01, -8.3612e-01,  2.7001e-01, -1.8684e+00],
        [-2.0117e-01, -1.7380e-01, -4.8770e+00, -1.5537e+00, -1.9731e-01,
          5.3964e+00, -2.0055e-01, -1.9889e-01, -2.0049e-01, -1.4269e+01,
         -2.0055e-01,  3.9824e+00,  3.9332e-01, -1.4417e+00, -1.7285e-01],
        [-1.3727e-01, -1.1132e-01,  2.8035e+00, -7.1535e+00, -1.3510e-01,
          1.5234e-02, -1.3661e-01, -1.3567e-01, -1.3655e-01, -1.4952e+00,
         -1.3661e-01, -1.8737e+01,  1.1501e+00, -7.3523e-03,  3.8200e-01],
        [-7.0190e-02, -7.4248e-02,  3.7607e+00,  6.2280e-01, -7.1072e-02,
         -8.6881e-01, -7.0409e-02, -7.0735e-02, -7.0426e-02, -1.6853e+00,
         -7.0409e-02, -2.6154e+00, -1.6065e+00, -1.3981e+00,  2.6575e-01],
        [ 2.1577e-01,  2.3436e-01, -6.7538e-01,  3.7751e+00,  2.1788e-01,
         -6.6052e-02,  2.1563e-01,  2.1594e-01,  2.1562e-01,  8.6462e-01,
          2.1563e-01,  2.2246e+00,  2.8678e+00, -1.3162e+01, -7.9664e-01],
        [ 5.3008e-02,  5.6055e-02, -5.7966e-01, -1.2907e-01,  5.3401e-02,
         -9.0894e-01,  5.3040e-02,  5.3111e-02,  5.3043e-02,  1.8562e+00,
          5.3040e-02, -5.3982e-01, -8.4745e-01,  2.8741e-01, -1.8795e+00],
        [ 3.2342e-01,  2.9995e-01, -8.9214e+00, -6.5263e+00,  3.1484e-01,
          5.5681e+00,  3.2302e-01,  3.2044e-01,  3.2298e-01, -1.3595e+01,
          3.2303e-01,  1.0702e+01, -5.6913e+00, -4.4991e+00, -9.9170e+00],
        [ 4.9491e-02,  5.2049e-02, -5.7828e-01, -1.2933e-01,  4.9834e-02,
         -8.8730e-01,  4.9518e-02,  4.9578e-02,  4.9520e-02,  1.8260e+00,
          4.9518e-02, -5.3618e-01, -8.3598e-01,  2.6995e-01, -1.8683e+00],
        [ 4.8554e-02,  5.0994e-02, -5.7720e-01, -1.2775e-01,  4.8886e-02,
         -8.8057e-01,  4.8580e-02,  4.8638e-02,  4.8582e-02,  1.8144e+00,
          4.8580e-02, -5.3417e-01, -8.2996e-01,  2.6834e-01, -1.8627e+00],
        [ 4.9282e-02,  5.1803e-02, -5.7853e-01, -1.3006e-01,  4.9622e-02,
         -8.8605e-01,  4.9308e-02,  4.9368e-02,  4.9311e-02,  1.8250e+00,
          4.9308e-02, -5.3643e-01, -8.3611e-01,  2.6800e-01, -1.8686e+00],
        [ 4.9312e-02,  5.1853e-02, -5.7780e-01, -1.2842e-01,  4.9654e-02,
         -8.8587e-01,  4.9339e-02,  4.9399e-02,  4.9341e-02,  1.8229e+00,
          4.9339e-02, -5.3541e-01, -8.3401e-01,  2.7056e-01, -1.8663e+00],
        [ 5.2204e-02, -5.9867e-02, -7.0299e+00,  2.9029e+00,  3.4688e-02,
          1.0013e+01,  5.0741e-02,  4.6147e-02,  5.0595e-02,  2.0321e-01,
          5.0744e-02, -4.1707e+00,  1.3132e+00,  2.8252e+00, -1.3940e-01],
        [-5.3612e-01, -5.9654e-01, -6.9262e+00,  1.0360e+00, -5.3167e-01,
          7.3902e+00, -5.3614e-01, -5.3462e-01, -5.3613e-01,  7.6851e+00,
         -5.3614e-01, -2.0049e+00, -6.8949e-02,  2.4033e+00, -2.5729e+00],
        [ 4.9213e-02,  5.1723e-02, -5.7859e-01, -1.3026e-01,  4.9552e-02,
         -8.8563e-01,  4.9239e-02,  4.9299e-02,  4.9242e-02,  1.8246e+00,
          4.9239e-02, -5.3649e-01, -8.3610e-01,  2.6741e-01, -1.8687e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-6.0304, -6.0244, -0.7618, -4.7533, -1.6404, -6.0481, -6.0117, -2.6986,
        -6.0245, -6.0291, -6.0248, -6.0258, -3.4212, -9.3874, -6.0249],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.4111,  -0.4160, -28.7209,  -0.1482, -14.0728,  -1.9424,  -0.4208,
           0.1198,  -0.4159,  -0.4121,  -0.4164,  -0.4145,   3.3763,   4.2996,
          -0.4164],
        [ -0.7252,  -0.7458,   0.9875,   4.7715,   2.0354,  -1.8527,  -0.7575,
          -8.4980,  -0.7455,  -0.7290,  -0.7485,  -0.7386,   1.5057,  -4.6754,
          -0.7492],
        [ -0.2608,  -0.2631, -15.8732,  -0.2194, -24.6945,  -1.1986,  -0.2700,
          -0.3949,  -0.2630,  -0.2615,  -0.2625,  -0.2629, -16.5993,  -0.5841,
          -0.2623],
        [ -0.2654,  -0.2677, -16.0805,  -0.2084, -25.3724,  -1.1812,  -0.2748,
          -0.3785,  -0.2677,  -0.2660,  -0.2671,  -0.2675, -17.1200,  -0.5412,
          -0.2669],
        [ -0.1588,  -0.1614,   0.3450,   0.9318,   0.0870,   0.4675,  -0.1590,
           5.4108,  -0.1614,  -0.1592,  -0.1621,  -0.1603,   0.4061,   3.9730,
          -0.1623],
        [ -0.1760,  -0.1769, -14.8053,  -0.2921, -20.5974,  -0.5081,  -0.1798,
          -0.3778,  -0.1769,  -0.1763,  -0.1766,  -0.1768, -12.8273,  -0.1961,
          -0.1766],
        [  1.4981,   1.5122,   3.4660,  -3.5390,   2.1709,   5.5695,   1.5757,
          11.1673,   1.5119,   1.5027,   1.5057,   1.5124,   1.3572,  -1.7034,
           1.5038]], device='cuda:0'))])
loaded xi:  939.40314
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0, 0.5, 0.1, 0.1, 0.1, 0.1, 0.1]
W_T_mean: 1145.5145662869302
W_T_median: 859.9386077279269
W_T_pctile_5: -136.26604266856006
W_T_CVAR_5_pct: -277.4390415040601
-----------------------------------------------
new min fval from sgd:  -2363.7618295059347
min fval:  -2363.7618295059347
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1339.7878284264611
W_T_median: 1284.859113441848
W_T_pctile_5: 1003.1802146728097
W_T_CVAR_5_pct: 746.6259119460441
Average q (qsum/M+1):  53.108091292842744
Optimal xi:  [939.40314]
Expected(across Rb) median(across samples) p_equity:  0.17567692063876167
obj fun:  tensor(-2363.7618, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight"14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -2438.4290546879643
Current xi:  [1095.2422]
objective value function right now is: -2438.4290546879643
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -2449.3482267382433
Current xi:  [1116.8546]
objective value function right now is: -2449.3482267382433
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [1138.2871]
objective value function right now is: -2436.8783110204627
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -2459.7449243453602
Current xi:  [1159.3488]
objective value function right now is: -2459.7449243453602
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -2472.290931630589
Current xi:  [1180.4718]
objective value function right now is: -2472.290931630589
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -2481.4525850638893
Current xi:  [1201.7437]
objective value function right now is: -2481.4525850638893
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -2482.3187263181535
Current xi:  [1222.0212]
objective value function right now is: -2482.3187263181535
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [1242.0219]
objective value function right now is: -2476.50190019013
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -2498.23373490805
Current xi:  [1262.2141]
objective value function right now is: -2498.23373490805
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [1281.8898]
objective value function right now is: -2493.380527872771
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -2506.189390734532
Current xi:  [1301.7236]
objective value function right now is: -2506.189390734532
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -2509.470892674296
Current xi:  [1320.9424]
objective value function right now is: -2509.470892674296
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -2515.1988583986094
Current xi:  [1340.3168]
objective value function right now is: -2515.1988583986094
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2521.0042352825676
Current xi:  [1359.6282]
objective value function right now is: -2521.0042352825676
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -2523.3035390379773
Current xi:  [1378.0071]
objective value function right now is: -2523.3035390379773
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [1395.8099]
objective value function right now is: -2505.4680933200075
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2529.4605765097876
Current xi:  [1412.1094]
objective value function right now is: -2529.4605765097876
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -2531.043672084807
Current xi:  [1430.0056]
objective value function right now is: -2531.043672084807
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -2533.583068002427
Current xi:  [1446.588]
objective value function right now is: -2533.583068002427
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -2539.4768643514894
Current xi:  [1461.9202]
objective value function right now is: -2539.4768643514894
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [1476.6263]
objective value function right now is: -2538.4387792527964
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -2540.1544911951432
Current xi:  [1491.3937]
objective value function right now is: -2540.1544911951432
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -2545.8674982838306
Current xi:  [1505.2744]
objective value function right now is: -2545.8674982838306
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [1518.5933]
objective value function right now is: -2509.9873704549555
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -2547.162270509594
Current xi:  [1530.0193]
objective value function right now is: -2547.162270509594
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [1541.9299]
objective value function right now is: -2542.521991559919
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [1552.7932]
objective value function right now is: -2545.86377524235
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -2547.5247563314624
Current xi:  [1561.6638]
objective value function right now is: -2547.5247563314624
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -2550.2172261287055
Current xi:  [1571.7003]
objective value function right now is: -2550.2172261287055
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2554.6838897946686
Current xi:  [1574.1342]
objective value function right now is: -2554.6838897946686
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -2556.9896032693123
Current xi:  [1576.1042]
objective value function right now is: -2556.9896032693123
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -2557.3166355050103
Current xi:  [1578.0778]
objective value function right now is: -2557.3166355050103
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2557.899935633933
Current xi:  [1580.0299]
objective value function right now is: -2557.899935633933
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [1581.9735]
objective value function right now is: -2557.1484794188304
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -2558.0774326916
Current xi:  [1583.768]
objective value function right now is: -2558.0774326916
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [1585.7849]
objective value function right now is: -2557.7667114307155
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -2559.435801832947
Current xi:  [1587.7081]
objective value function right now is: -2559.435801832947
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [1589.4666]
objective value function right now is: -2558.0290514676344
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [1590.82]
objective value function right now is: -2558.4012028099783
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [1592.7513]
objective value function right now is: -2559.196823071019
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [1594.1176]
objective value function right now is: -2557.9856612767626
new min fval from sgd:  -2559.533114207471
new min fval from sgd:  -2559.675144584399
new min fval from sgd:  -2559.755955559346
new min fval from sgd:  -2559.8496547779164
new min fval from sgd:  -2559.9612578195397
new min fval from sgd:  -2560.0526853097685
new min fval from sgd:  -2560.0955193114446
new min fval from sgd:  -2560.123702662197
new min fval from sgd:  -2560.153488438595
new min fval from sgd:  -2560.1948772870137
new min fval from sgd:  -2560.2508851053735
new min fval from sgd:  -2560.278790849275
new min fval from sgd:  -2560.312826000521
new min fval from sgd:  -2560.3264899997166
new min fval from sgd:  -2560.3433191884114
new min fval from sgd:  -2560.3698803515217
new min fval from sgd:  -2560.4010272501473
new min fval from sgd:  -2560.4072295554047
new min fval from sgd:  -2560.4084183028954
new min fval from sgd:  -2560.4404979497435
new min fval from sgd:  -2560.4589258091564
new min fval from sgd:  -2560.485227653287
new min fval from sgd:  -2560.5395030937802
new min fval from sgd:  -2560.5575874367114
new min fval from sgd:  -2560.595508521909
new min fval from sgd:  -2560.650246653626
new min fval from sgd:  -2560.666742814757
new min fval from sgd:  -2560.6823235700717
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [1595.5665]
objective value function right now is: -2559.751539208712
new min fval from sgd:  -2560.7608376053713
new min fval from sgd:  -2560.8707378366285
new min fval from sgd:  -2560.9306350378683
new min fval from sgd:  -2560.960239341055
new min fval from sgd:  -2560.9812052014963
new min fval from sgd:  -2561.0008960025784
new min fval from sgd:  -2561.013396121159
new min fval from sgd:  -2561.0687344897597
new min fval from sgd:  -2561.099548105185
new min fval from sgd:  -2561.1128378546905
new min fval from sgd:  -2561.1205996534472
new min fval from sgd:  -2561.1594970360575
new min fval from sgd:  -2561.1710429002223
new min fval from sgd:  -2561.1812249415807
new min fval from sgd:  -2561.1824935870027
new min fval from sgd:  -2561.1909688807496
new min fval from sgd:  -2561.1990130022828
new min fval from sgd:  -2561.203132374923
new min fval from sgd:  -2561.207754492162
new min fval from sgd:  -2561.2151069606275
new min fval from sgd:  -2561.2215548120253
new min fval from sgd:  -2561.226504455754
new min fval from sgd:  -2561.2391685762004
new min fval from sgd:  -2561.253239336219
new min fval from sgd:  -2561.2627776124014
new min fval from sgd:  -2561.2686135576723
new min fval from sgd:  -2561.273992637027
new min fval from sgd:  -2561.279215802107
new min fval from sgd:  -2561.292218956086
new min fval from sgd:  -2561.3030083972944
new min fval from sgd:  -2561.3099701074057
new min fval from sgd:  -2561.3127709661717
new min fval from sgd:  -2561.3136804964615
new min fval from sgd:  -2561.31724490513
new min fval from sgd:  -2561.3198330721452
new min fval from sgd:  -2561.321975474489
new min fval from sgd:  -2561.3223043384414
new min fval from sgd:  -2561.3237996853113
new min fval from sgd:  -2561.3339739813878
new min fval from sgd:  -2561.3407814624284
new min fval from sgd:  -2561.3436902419367
new min fval from sgd:  -2561.352895033241
new min fval from sgd:  -2561.3644469085
new min fval from sgd:  -2561.373454101204
new min fval from sgd:  -2561.3842196550163
new min fval from sgd:  -2561.393797118802
new min fval from sgd:  -2561.3989806334666
new min fval from sgd:  -2561.405739278369
new min fval from sgd:  -2561.4079435229023
new min fval from sgd:  -2561.4151913776877
new min fval from sgd:  -2561.423547796607
new min fval from sgd:  -2561.444561290114
new min fval from sgd:  -2561.4610744377906
new min fval from sgd:  -2561.471017925758
new min fval from sgd:  -2561.4829183549396
new min fval from sgd:  -2561.496924181983
new min fval from sgd:  -2561.5067467409876
new min fval from sgd:  -2561.5106037884443
new min fval from sgd:  -2561.5161984630463
new min fval from sgd:  -2561.522322502007
new min fval from sgd:  -2561.5241347710303
new min fval from sgd:  -2561.526515713959
new min fval from sgd:  -2561.52892629538
new min fval from sgd:  -2561.5332071214075
new min fval from sgd:  -2561.533361755061
new min fval from sgd:  -2561.541200574077
new min fval from sgd:  -2561.5550736834025
new min fval from sgd:  -2561.566839561506
new min fval from sgd:  -2561.573348533866
new min fval from sgd:  -2561.584968467713
new min fval from sgd:  -2561.5914523031215
new min fval from sgd:  -2561.594475457154
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [1596.8262]
objective value function right now is: -2561.419992915199
new min fval from sgd:  -2561.5949219592057
new min fval from sgd:  -2561.602021908922
new min fval from sgd:  -2561.609866565464
new min fval from sgd:  -2561.61772316632
new min fval from sgd:  -2561.6296134753197
new min fval from sgd:  -2561.6380304742092
new min fval from sgd:  -2561.649683112505
new min fval from sgd:  -2561.660776036135
new min fval from sgd:  -2561.6723545287364
new min fval from sgd:  -2561.682920231539
new min fval from sgd:  -2561.6950095507264
new min fval from sgd:  -2561.7057110658307
new min fval from sgd:  -2561.7135792249414
new min fval from sgd:  -2561.7197570149265
new min fval from sgd:  -2561.725732391336
new min fval from sgd:  -2561.7281129039175
new min fval from sgd:  -2561.7322096429016
new min fval from sgd:  -2561.7378606953503
new min fval from sgd:  -2561.7497952589847
new min fval from sgd:  -2561.761594598071
new min fval from sgd:  -2561.771738099634
new min fval from sgd:  -2561.7762633933585
new min fval from sgd:  -2561.7782248248045
new min fval from sgd:  -2561.7815441543285
new min fval from sgd:  -2561.7851842183304
new min fval from sgd:  -2561.7871151523086
new min fval from sgd:  -2561.789508405146
new min fval from sgd:  -2561.795916498916
new min fval from sgd:  -2561.8052797407777
new min fval from sgd:  -2561.8149153929035
new min fval from sgd:  -2561.822565382793
new min fval from sgd:  -2561.8286474061106
new min fval from sgd:  -2561.8324105248475
new min fval from sgd:  -2561.8357105483215
new min fval from sgd:  -2561.8404951185544
new min fval from sgd:  -2561.843357608268
new min fval from sgd:  -2561.8546479153106
new min fval from sgd:  -2561.860953078023
new min fval from sgd:  -2561.863973839213
new min fval from sgd:  -2561.865496961481
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [1597.111]
objective value function right now is: -2561.70969875908
min fval:  -2561.865496961481
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -0.3902,  -1.2042],
        [-18.5187,   8.3085],
        [ 14.5187,  -1.9061],
        [ -0.5222,  -1.1642],
        [ -0.3902,  -1.2042],
        [ 14.0888,  -2.1873],
        [ -0.3902,  -1.2042],
        [ -0.4922,  -1.1839],
        [-16.4281, -12.4123],
        [ 14.8858,  -1.8530],
        [-18.0044,   8.0567],
        [ 15.1949,  -1.9593],
        [ -0.3902,  -1.2042],
        [ -4.3114,  13.9201],
        [ -0.3902,  -1.2042]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -2.5359,  -3.8078, -11.0364,  -2.6149,  -2.5359, -11.0680,  -2.5359,
         -2.5895,  11.0631, -10.9985,  -3.7632, -10.8910,  -2.5359,  -5.1640,
         -2.5359], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 6.4510e-03, -2.6819e-01, -5.1707e-02,  1.0150e-02,  6.4511e-03,
          1.3216e-02,  6.4511e-03,  8.9375e-03, -4.9628e-01, -9.3495e-02,
         -2.6086e-01, -9.4406e-02,  6.4511e-03, -1.0998e+00,  6.4510e-03],
        [ 6.4510e-03, -2.6819e-01, -5.1707e-02,  1.0150e-02,  6.4511e-03,
          1.3216e-02,  6.4512e-03,  8.9375e-03, -4.9628e-01, -9.3495e-02,
         -2.6086e-01, -9.4406e-02,  6.4511e-03, -1.0998e+00,  6.4510e-03],
        [ 6.4510e-03, -2.6819e-01, -5.1707e-02,  1.0150e-02,  6.4511e-03,
          1.3216e-02,  6.4511e-03,  8.9375e-03, -4.9628e-01, -9.3495e-02,
         -2.6086e-01, -9.4406e-02,  6.4511e-03, -1.0998e+00,  6.4510e-03],
        [ 6.4510e-03, -2.6819e-01, -5.1707e-02,  1.0150e-02,  6.4512e-03,
          1.3216e-02,  6.4511e-03,  8.9375e-03, -4.9628e-01, -9.3495e-02,
         -2.6086e-01, -9.4406e-02,  6.4512e-03, -1.0998e+00,  6.4510e-03],
        [ 6.4510e-03, -2.6819e-01, -5.1707e-02,  1.0150e-02,  6.4512e-03,
          1.3216e-02,  6.4512e-03,  8.9375e-03, -4.9628e-01, -9.3495e-02,
         -2.6086e-01, -9.4406e-02,  6.4512e-03, -1.0998e+00,  6.4510e-03],
        [ 2.3621e-01, -7.0248e+00,  3.1965e+00,  2.1025e-01,  2.3621e-01,
          2.0369e+00,  2.3621e-01,  2.0562e-01,  1.1195e+01,  4.1349e+00,
         -6.4441e+00,  5.0970e+00,  2.3621e-01, -4.6941e+00,  2.3621e-01],
        [ 6.4510e-03, -2.6819e-01, -5.1707e-02,  1.0150e-02,  6.4512e-03,
          1.3216e-02,  6.4511e-03,  8.9375e-03, -4.9628e-01, -9.3495e-02,
         -2.6086e-01, -9.4406e-02,  6.4511e-03, -1.0998e+00,  6.4510e-03],
        [ 6.4510e-03, -2.6819e-01, -5.1707e-02,  1.0150e-02,  6.4511e-03,
          1.3216e-02,  6.4511e-03,  8.9375e-03, -4.9628e-01, -9.3495e-02,
         -2.6086e-01, -9.4406e-02,  6.4512e-03, -1.0998e+00,  6.4510e-03],
        [ 6.4510e-03, -2.6819e-01, -5.1707e-02,  1.0150e-02,  6.4511e-03,
          1.3216e-02,  6.4511e-03,  8.9375e-03, -4.9628e-01, -9.3495e-02,
         -2.6086e-01, -9.4406e-02,  6.4511e-03, -1.0998e+00,  6.4510e-03],
        [ 6.4525e-03, -2.6821e-01, -5.1700e-02,  1.0149e-02,  6.4526e-03,
          1.3227e-02,  6.4526e-03,  8.9370e-03, -4.9630e-01, -9.3490e-02,
         -2.6088e-01, -9.4403e-02,  6.4526e-03, -1.0999e+00,  6.4525e-03],
        [-7.0497e-03,  9.3816e+00, -4.9306e+00, -3.8620e-02, -7.0454e-03,
         -4.0535e+00, -7.0455e-03, -4.3282e-02, -1.3577e+01, -5.8170e+00,
          8.6738e+00, -6.4722e+00, -7.0449e-03,  6.3588e+00, -7.0503e-03],
        [ 3.7612e-01, -1.5349e-01,  1.0744e+01,  2.4452e-01,  3.7611e-01,
          9.1365e+00,  3.7611e-01,  2.7053e-01,  2.6288e+00,  1.2130e+01,
         -3.5186e-02,  1.3146e+01,  3.7611e-01, -9.8678e+00,  3.7612e-01],
        [ 2.0236e-01, -7.4299e+00,  3.4910e+00,  1.6728e-01,  2.0237e-01,
          2.2443e+00,  2.0237e-01,  1.7213e-01,  1.1755e+01,  4.4704e+00,
         -6.8353e+00,  5.4499e+00,  2.0237e-01, -4.8421e+00,  2.0236e-01],
        [ 6.4510e-03, -2.6819e-01, -5.1707e-02,  1.0150e-02,  6.4512e-03,
          1.3216e-02,  6.4511e-03,  8.9375e-03, -4.9628e-01, -9.3495e-02,
         -2.6086e-01, -9.4406e-02,  6.4512e-03, -1.0998e+00,  6.4510e-03],
        [-1.3405e-01,  1.5546e+00, -1.4022e+00, -1.2075e-01, -1.3405e-01,
         -7.5869e-01, -1.3405e-01, -1.2461e-01, -3.2724e+00, -2.0432e+00,
          1.5264e+00, -2.5498e+00, -1.3405e-01,  1.3097e+00, -1.3405e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.2236, -1.2236, -1.2236, -1.2236, -1.2236, -2.6982, -1.2236, -1.2236,
        -1.2236, -1.2234,  1.9877, -4.7088, -2.7821, -1.2236, -1.8783],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.2474,  -0.2474,  -0.2474,  -0.2474,  -0.2474,  -7.0225,  -0.2474,
          -0.2474,  -0.2474,  -0.2474,  11.6121, -13.4141,  -7.7292,  -0.2474,
           2.3848]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-11.6115,  -4.2210],
        [ -4.9572,  -2.6144],
        [ 13.5927,   0.5177],
        [  2.5542,   5.1109],
        [-10.9018,  -3.8675],
        [-22.8901,   5.3396],
        [-10.6848,  -3.8449],
        [-10.8123,  -3.9158],
        [-10.6119,  -3.8058],
        [ -9.3849,   9.7042],
        [-10.6890,  -3.8472],
        [-13.5479,  -3.3709],
        [  6.5866,  -3.0508],
        [-11.8218,   2.3682],
        [ 14.2461,   4.9751]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -4.9199,  -4.8256, -13.6067,  -0.8729,  -4.9153,  -0.8391,  -5.5902,
         -5.4635,  -5.6445,  -2.6373,  -5.5868,   0.5531, -14.0429,   3.0285,
         -4.4082], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 6.2042e+00,  3.2735e+00, -1.6723e+00, -9.3861e-02,  5.5482e+00,
          1.5915e-02,  3.7098e+00,  4.1827e+00,  3.5285e+00, -3.5194e-01,
          3.7204e+00,  7.5741e+00,  1.1846e+00, -6.7054e+00, -1.0712e+00],
        [ 5.2536e-01,  1.0486e+00,  2.0856e-01, -2.7844e-01,  2.6431e-01,
         -3.2625e-02,  5.9414e-03,  4.2651e-02,  5.8889e-05,  4.0059e-02,
          6.5043e-03,  2.3593e+00,  2.1798e+00, -1.9593e-01, -1.4366e+00],
        [ 1.8504e+00,  4.9091e-01, -8.6022e+00, -2.6296e+00,  4.1847e-01,
          4.7796e-01,  5.3358e-01,  6.5315e-01,  4.7974e-01, -8.9733e+00,
          5.3665e-01,  1.0712e+01,  8.0185e-01, -7.8721e-01, -1.6175e+00],
        [ 4.9473e-01, -1.8762e-01,  1.0713e+01, -1.5478e+01, -1.2909e+00,
         -2.5557e-01, -2.0421e-01, -2.4276e-01, -1.7577e-01, -2.7502e-01,
         -2.0622e-01, -1.1501e+01,  8.2861e-01, -8.2135e-01, -5.9625e-01],
        [-6.1042e-02,  2.1860e+00,  2.6677e+00, -1.6981e-01,  3.6015e-01,
          1.7701e+00, -2.4861e-02, -4.3786e-02, -1.4118e-02, -3.7740e+00,
         -2.5508e-02, -4.5133e+00, -2.9083e+01, -7.9312e-01, -5.5532e-01],
        [-1.0971e-02,  3.2661e-02, -4.6854e-01,  3.6811e+00,  2.8068e-03,
          1.6135e-01,  2.0957e-02,  1.8392e-02,  2.1663e-02,  9.3968e-01,
          2.0905e-02, -2.8166e-01, -1.9527e+00, -1.4608e+01, -2.6380e-01],
        [ 3.5140e+00,  4.6763e+00,  5.8448e+00,  2.1935e+00,  2.0058e+00,
         -6.7833e+00,  1.7215e+00,  1.8869e+00,  1.6391e+00,  3.0834e+00,
          1.7265e+00,  3.0723e+00,  5.7119e+00,  1.0799e+00, -1.4066e+00],
        [ 2.8769e+00,  6.9947e-01, -1.6390e+01, -9.7824e+00,  2.3912e+00,
          1.6564e+00,  1.3309e+00,  1.5283e+00,  1.2441e+00, -7.8708e+00,
          1.3364e+00,  1.8085e+01, -8.9795e+00, -3.8271e+00, -1.6596e+01],
        [ 1.2426e+00,  2.2402e+00,  5.7862e-01,  2.8484e-01,  9.7135e-01,
          2.7344e-02,  2.5078e-01,  3.9209e-01,  2.1198e-01, -2.9545e-01,
          2.5320e-01,  5.5593e+00,  2.6729e+00, -3.4246e+00, -3.2940e+00],
        [-6.4490e-02, -2.2601e-01, -1.5871e+00, -1.3577e-01,  1.2534e-01,
          3.5498e+00,  1.1355e-01,  6.5377e-02,  1.3748e-01,  1.2925e+00,
          1.1217e-01,  4.7359e+00, -7.4501e-01,  8.5238e-01, -6.8272e+00],
        [ 5.7199e+00,  3.0385e+00, -1.5699e+00,  5.0045e-01,  5.0445e+00,
          2.0803e-02,  3.4346e+00,  3.8854e+00,  3.2363e+00, -5.3517e-01,
          3.4462e+00,  7.4102e+00,  1.4618e+00, -5.9945e+00, -5.7045e-01],
        [ 6.7818e+00,  4.0691e+00, -1.5589e+00, -5.5414e-01,  5.9231e+00,
          1.7724e-02,  5.1505e+00,  5.4309e+00,  5.0285e+00, -1.3190e-01,
          5.1576e+00,  9.7648e+00,  1.7093e+00, -1.1591e+01, -1.3120e+00],
        [-6.0338e-01, -5.8377e-01, -5.9171e+00,  1.9215e+00, -5.8847e-01,
          3.9210e+00, -2.7818e-01, -3.4041e-01, -2.5500e-01,  1.1728e+00,
         -2.7966e-01, -1.7540e+00, -2.0649e+00,  1.2966e+00, -1.8207e+00],
        [-2.9725e-01, -9.2133e-01, -2.0639e+01, -4.1291e-01, -3.5830e-01,
          4.0944e+00, -1.1723e-01, -1.4445e-01, -1.0733e-01,  1.0489e+01,
         -1.1787e-01,  1.7881e+00, -3.3188e-04,  2.0022e+00, -1.0834e+00],
        [ 1.7109e+00,  2.1238e+00,  2.9723e-01,  2.8771e-01,  1.0728e+00,
          6.1582e-02,  2.7679e-01,  4.3930e-01,  2.1900e-01, -3.4084e-01,
          2.8032e-01,  5.4200e+00,  2.6791e+00, -1.9145e+00, -1.9236e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -9.0383,  -8.3186,   0.8021,  -3.8076,  -0.4052,  -5.2520,  -7.0752,
         -0.3147,  -7.5688,  -5.1691,  -9.0663,  -9.9204,  -2.2989, -13.8256,
         -8.2488], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 8.7079e-04, -2.1769e-01, -2.7242e+01, -7.0074e-01, -3.4619e+01,
         -1.3369e+01,  3.5136e+00, -4.7238e-03, -6.8539e-02,  1.2174e+00,
         -2.7965e-02,  2.9971e-03,  3.9177e+00,  5.8546e+00, -1.6059e-01],
        [-8.0129e-01,  8.7159e-02,  1.1357e+00,  4.3090e+00,  2.2358e+00,
          2.3821e+00,  3.1202e-01, -2.3805e+01, -8.7983e-01,  1.6860e+00,
         -1.0316e+00, -6.4451e-01,  2.2338e+00, -5.1826e+00, -5.7854e-01],
        [-4.0249e-03, -1.4268e-02, -1.5845e+01, -9.4405e+00, -2.5507e+01,
         -1.3785e+01, -1.1802e+01, -1.2474e-03, -9.0248e-03, -1.1230e-02,
         -8.8608e-03, -2.0345e-03, -2.5383e+01, -3.6777e-02, -1.1502e-02],
        [-9.2238e-03, -2.2363e-02, -1.6291e+01, -9.7561e+00, -2.5984e+01,
         -1.4676e+01, -1.2413e+01, -8.1218e-04, -1.3373e-02, -1.4299e-02,
         -1.5978e-02, -3.5095e-03, -2.6381e+01, -4.4910e-02, -1.7984e-02],
        [ 5.4714e+00,  2.8548e+00,  3.3142e+00,  8.2257e+00, -1.1509e+00,
          1.4548e+00,  5.7881e+00,  5.2512e+00,  5.1225e+00, -2.6941e+00,
          5.3723e+00,  8.8903e+00,  1.3573e+00,  6.2202e+00,  4.6768e+00],
        [ 1.5721e-02,  1.2973e-02, -1.6831e+01, -1.1487e+01, -2.0791e+01,
          6.3013e+00,  6.6219e+00, -2.9144e-03,  8.8423e-03,  3.0851e-03,
          8.4236e-03, -1.0616e-03, -3.5267e+01,  2.0648e-02,  4.4916e-03],
        [-3.0963e+00,  4.8191e-01,  3.9485e+00, -2.5346e+00,  3.4888e+00,
          5.7297e+00,  3.5416e+00,  1.4994e+01, -1.4815e+00,  8.3271e-01,
         -1.8762e+00, -5.4657e+00,  2.4065e+00, -7.7953e+00, -1.1192e+00]],
       device='cuda:0'))])
xi:  [1597.0353]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 2296.7554037734726
W_T_median: 2201.290596038739
W_T_pctile_5: 1598.9422584806207
W_T_CVAR_5_pct: 962.15540514101
Average q (qsum/M+1):  51.60590977822581
Optimal xi:  [1597.0353]
Expected(across Rb) median(across samples) p_equity:  0.14842747566775263
obj fun:  tensor(-2561.8655, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor4
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
