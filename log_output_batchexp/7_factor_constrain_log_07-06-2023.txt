Starting at: 
06-07-23_21:24

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = 7_Factor_plusEWD
timeseries_basket['basket_desc'] = 7_Factor_totalmax
timeseries_basket['basket_columns'] = 
['Size_Lo30_real_ret', 'Value_Hi30_real_ret', 'Oprof_Hi30_real_ret', 'Inv_Lo30_real_ret', 'Mom_Hi30_real_ret', 'EP_Hi30_real_ret', 'Vol_Lo20_real_ret', 'Div_Hi30_real_ret', 'EQWFact_real_ret', 'T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'EWD_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = 7_Factor_plusEWD
timeseries_basket['basket_desc'] = 7_Factor_totalmax
timeseries_basket['basket_columns'] = 
['Size_Lo30_nom_ret', 'Value_Hi30_nom_ret', 'Oprof_Hi30_nom_ret', 'Inv_Lo30_nom_ret', 'Mom_Hi30_nom_ret', 'EP_Hi30_nom_ret', 'Vol_Lo20_nom_ret', 'Div_Hi30_nom_ret', 'EQWFact_nom_ret', 'T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'EWD_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.031411     0.013051
192608                    0.0319              0.0561  ...     0.028647     0.031002
192609                   -0.0173             -0.0071  ...     0.005787    -0.006499
192610                   -0.0294             -0.0355  ...    -0.028996    -0.034630
192611                   -0.0038              0.0294  ...     0.028554     0.024776

[5 rows x 14 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.036240    -0.011556
202209                   -0.0955             -0.0871  ...    -0.091324    -0.099903
202210                    0.0883              0.1486  ...     0.077403     0.049863
202211                   -0.0076              0.0462  ...     0.052365     0.028123
202212                   -0.0457             -0.0499  ...    -0.057116    -0.047241

[5 rows x 14 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'Oprof_Hi30_nom_ret_ind', 'Inv_Lo30_nom_ret_ind',
       'Mom_Hi30_nom_ret_ind', 'EP_Hi30_nom_ret_ind', 'Vol_Lo20_nom_ret_ind',
       'Div_Hi30_nom_ret_ind', 'EQWFact_nom_ret_ind', 'CPI_nom_ret_ind',
       'T30_nom_ret_ind', 'B10_nom_ret_ind', 'VWD_nom_ret_ind',
       'EWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
Oprof_Hi30_real_ret    0.003948
Inv_Lo30_real_ret      0.004513
Mom_Hi30_real_ret      0.011386
EP_Hi30_real_ret       0.007033
Vol_Lo20_real_ret      0.003529
Div_Hi30_real_ret      0.007888
EQWFact_real_ret       0.004508
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
EWD_real_ret           0.009545
dtype: float64


timeseries_basket['data_df_stdev'] = 
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
Oprof_Hi30_real_ret    0.037444
Inv_Lo30_real_ret      0.037639
Mom_Hi30_real_ret      0.061421
EP_Hi30_real_ret       0.041390
Vol_Lo20_real_ret      0.030737
Div_Hi30_real_ret      0.056728
EQWFact_real_ret       0.037131
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
EWD_real_ret           0.071360
dtype: float64


timeseries_basket['data_df_corr'] = 
                     Size_Lo30_real_ret  ...  EWD_real_ret
Size_Lo30_real_ret             1.000000  ...      0.977206
Value_Hi30_real_ret            0.908542  ...      0.919912
Oprof_Hi30_real_ret            0.433774  ...      0.462336
Inv_Lo30_real_ret              0.455237  ...      0.479661
Mom_Hi30_real_ret              0.903222  ...      0.912002
EP_Hi30_real_ret               0.476966  ...      0.506661
Vol_Lo20_real_ret              0.360014  ...      0.382411
Div_Hi30_real_ret              0.816292  ...      0.849068
EQWFact_real_ret               0.494572  ...      0.513359
T30_real_ret                   0.014412  ...      0.029084
B10_real_ret                   0.012916  ...      0.024853
VWD_real_ret                   0.865290  ...      0.907369
EWD_real_ret                   0.977206  ...      1.000000

[13 rows x 13 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      21  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      21  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 21)     True          21  
2     (21, 21)     True          21  
3      (21, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer      14       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      21  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      21  logistic_sigmoid   
3        obj.layers[3]        3  output_layer      14              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 21)     True          21  
2     (21, 21)     True          21  
3     (21, 14)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       21  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       21  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 21)      True          21  
0     (21, 21)      True          21  
0      (21, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       21  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       21  logistic_sigmoid   
0        obj.layers[3]         3  output_layer       14              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 21)      True          21  
0     (21, 21)      True          21  
0     (21, 14)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 1500, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       21  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       21  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 21)      True          21  
0     (21, 21)      True          21  
0      (21, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       21  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       21  logistic_sigmoid   
0        obj.layers[3]         3  output_layer       14              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 21)      True          21  
0     (21, 21)      True          21  
0     (21, 14)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0, 0, 0, 0, 0, 0, 0, 0, 0.4, 0, 0.5, 0.1, 0]
W_T_mean: 4967.027961772471
W_T_median: 3161.3881202316225
W_T_pctile_5: 29.592754121860242
W_T_CVAR_5_pct: -312.1291917887027
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1796.1213805005584
Current xi:  [119.41667]
objective value function right now is: -1796.1213805005584
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1816.337513535664
Current xi:  [142.26396]
objective value function right now is: -1816.337513535664
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1829.4142244741145
Current xi:  [164.61728]
objective value function right now is: -1829.4142244741145
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1843.2679035763574
Current xi:  [187.49498]
objective value function right now is: -1843.2679035763574
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1856.9013913961546
Current xi:  [209.66821]
objective value function right now is: -1856.9013913961546
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1866.1051167145429
Current xi:  [231.42207]
objective value function right now is: -1866.1051167145429
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1876.706824438603
Current xi:  [252.65718]
objective value function right now is: -1876.706824438603
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1888.338912930538
Current xi:  [274.93585]
objective value function right now is: -1888.338912930538
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1900.3202909458537
Current xi:  [298.09387]
objective value function right now is: -1900.3202909458537
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1911.925298494367
Current xi:  [320.45688]
objective value function right now is: -1911.925298494367
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1921.6761953036018
Current xi:  [342.14703]
objective value function right now is: -1921.6761953036018
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1930.9419945901586
Current xi:  [363.15198]
objective value function right now is: -1930.9419945901586
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1939.6420359369895
Current xi:  [383.5474]
objective value function right now is: -1939.6420359369895
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1943.5725940442935
Current xi:  [403.38873]
objective value function right now is: -1943.5725940442935
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1951.6379469639205
Current xi:  [422.615]
objective value function right now is: -1951.6379469639205
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1956.4318895321403
Current xi:  [441.3475]
objective value function right now is: -1956.4318895321403
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1962.3141970290892
Current xi:  [459.05334]
objective value function right now is: -1962.3141970290892
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1963.1146668187394
Current xi:  [476.5613]
objective value function right now is: -1963.1146668187394
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1969.7428854915934
Current xi:  [493.949]
objective value function right now is: -1969.7428854915934
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1970.085976299904
Current xi:  [509.9774]
objective value function right now is: -1970.085976299904
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1976.4563517613733
Current xi:  [524.11633]
objective value function right now is: -1976.4563517613733
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1979.161696376264
Current xi:  [538.50275]
objective value function right now is: -1979.161696376264
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [550.7134]
objective value function right now is: -1976.5794024471263
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [562.18616]
objective value function right now is: -1975.4107068341261
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [572.8413]
objective value function right now is: -1978.9761735643528
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [582.2604]
objective value function right now is: -1978.5874096669584
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1983.9326082947318
Current xi:  [590.27765]
objective value function right now is: -1983.9326082947318
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [596.4766]
objective value function right now is: -1983.6085085172497
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [602.49506]
objective value function right now is: -1983.3634772917553
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [607.2552]
objective value function right now is: -1982.9899170218655
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [611.5397]
objective value function right now is: -1982.9867568582702
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1984.0045243909663
Current xi:  [615.5941]
objective value function right now is: -1984.0045243909663
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1984.9175552363572
Current xi:  [619.04767]
objective value function right now is: -1984.9175552363572
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [621.6257]
objective value function right now is: -1981.3627821012471
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [623.813]
objective value function right now is: -1982.8634192967318
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1989.117437149329
Current xi:  [624.1196]
objective value function right now is: -1989.117437149329
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1989.1677528411121
Current xi:  [624.65967]
objective value function right now is: -1989.1677528411121
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [625.368]
objective value function right now is: -1988.799028947301
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [625.7553]
objective value function right now is: -1989.1350934574662
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [626.13477]
objective value function right now is: -1988.205636212219
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1989.943303327145
Current xi:  [626.5491]
objective value function right now is: -1989.943303327145
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [627.2767]
objective value function right now is: -1989.2103605469526
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [627.66254]
objective value function right now is: -1989.7385997998902
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [628.1017]
objective value function right now is: -1989.9087975953482
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [628.5539]
objective value function right now is: -1989.70133565337
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [628.8699]
objective value function right now is: -1989.7967828736655
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [629.37036]
objective value function right now is: -1989.707276881093
new min fval from sgd:  -1989.9547943624134
new min fval from sgd:  -1990.0240306378837
new min fval from sgd:  -1990.1778532461487
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [630.03534]
objective value function right now is: -1989.769688951638
new min fval from sgd:  -1990.1934803174563
new min fval from sgd:  -1990.2144098553072
new min fval from sgd:  -1990.2352209450012
new min fval from sgd:  -1990.2490867582053
new min fval from sgd:  -1990.2546108531346
new min fval from sgd:  -1990.2569706112995
new min fval from sgd:  -1990.2635311540712
new min fval from sgd:  -1990.270098379599
new min fval from sgd:  -1990.275252417407
new min fval from sgd:  -1990.2815432757038
new min fval from sgd:  -1990.2863656453765
new min fval from sgd:  -1990.2939649094274
new min fval from sgd:  -1990.3050940561527
new min fval from sgd:  -1990.3066433400809
new min fval from sgd:  -1990.3150311152867
new min fval from sgd:  -1990.3221947196485
new min fval from sgd:  -1990.3360446109311
new min fval from sgd:  -1990.3462595896349
new min fval from sgd:  -1990.3494489571278
new min fval from sgd:  -1990.3630344136986
new min fval from sgd:  -1990.3746893996342
new min fval from sgd:  -1990.375059954574
new min fval from sgd:  -1990.3826318802667
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [630.2071]
objective value function right now is: -1990.3553291478065
new min fval from sgd:  -1990.3828760623276
new min fval from sgd:  -1990.3860632156773
new min fval from sgd:  -1990.3872137686726
new min fval from sgd:  -1990.3924939822919
new min fval from sgd:  -1990.41121849857
new min fval from sgd:  -1990.4112267256198
new min fval from sgd:  -1990.422175194715
new min fval from sgd:  -1990.4305038589096
new min fval from sgd:  -1990.4374191448944
new min fval from sgd:  -1990.438246042656
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [630.3174]
objective value function right now is: -1990.3940928910417
min fval:  -1990.438246042656
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -1.2850,   0.8452],
        [ -1.2753,   0.8410],
        [  6.1685,  -0.2901],
        [ -1.2754,   0.8410],
        [ -1.2754,   0.8410],
        [ -9.5167,   4.3206],
        [ -8.1962,  -7.0663],
        [-10.6338,   5.2457],
        [-11.0542,   5.6202],
        [ -1.2745,   0.8407],
        [ -1.2754,   0.8410],
        [ -8.3054,  -6.5495],
        [ -7.9865,   4.7839],
        [  7.4388,   4.3694],
        [ -7.5627,   5.2527],
        [ -1.2821,   0.8438],
        [ -1.2754,   0.8410],
        [ -1.2754,   0.8410],
        [  6.6153,   3.7761],
        [ -1.2756,   0.8410],
        [ -1.2754,   0.8410]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.7931,  -3.7969,  -7.9680,  -3.7970,  -3.7969,   4.5663,  -4.7345,
          5.1190,   5.3493,  -3.7971,  -3.7969,  -4.7556,   3.2486, -11.1871,
         -2.1269,  -3.7942,  -3.7970,  -3.7969,  -9.0536,  -3.7970,  -3.7969],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.6936e-02, -1.6986e-02, -6.3126e-03, -1.6985e-02, -1.6986e-02,
         -2.8059e-01, -9.9383e-02, -2.4432e-01, -2.3347e-01, -1.6992e-02,
         -1.6986e-02, -7.3366e-02, -1.6468e-01, -5.8686e-02, -1.1744e-02,
         -1.6951e-02, -1.6985e-02, -1.6985e-02, -6.7078e-02, -1.6983e-02,
         -1.6985e-02],
        [-1.6884e-02, -1.6934e-02, -5.7956e-03, -1.6933e-02, -1.6934e-02,
         -2.8357e-01, -1.0215e-01, -2.4625e-01, -2.3488e-01, -1.6940e-02,
         -1.6934e-02, -7.5288e-02, -1.6727e-01, -5.8632e-02, -1.2266e-02,
         -1.6899e-02, -1.6933e-02, -1.6933e-02, -6.8315e-02, -1.6931e-02,
         -1.6933e-02],
        [-1.6884e-02, -1.6934e-02, -5.7956e-03, -1.6933e-02, -1.6934e-02,
         -2.8357e-01, -1.0215e-01, -2.4625e-01, -2.3488e-01, -1.6940e-02,
         -1.6934e-02, -7.5288e-02, -1.6727e-01, -5.8632e-02, -1.2266e-02,
         -1.6899e-02, -1.6933e-02, -1.6933e-02, -6.8315e-02, -1.6931e-02,
         -1.6933e-02],
        [ 1.4546e-01,  1.3975e-01,  1.2047e+00,  1.3994e-01,  1.3987e-01,
         -4.3115e+00,  1.6482e+01, -4.9420e+00, -5.0420e+00,  1.3931e-01,
          1.3984e-01,  4.7955e+00, -2.0078e+00,  8.1090e+00,  3.0171e-01,
          1.4381e-01,  1.3995e-01,  1.3992e-01,  4.6714e+00,  1.4024e-01,
          1.3991e-01],
        [-1.6897e-02, -1.6947e-02, -5.8668e-03, -1.6946e-02, -1.6947e-02,
         -2.8365e-01, -1.0179e-01, -2.4629e-01, -2.3496e-01, -1.6953e-02,
         -1.6947e-02, -7.5013e-02, -1.6705e-01, -5.8648e-02, -1.2190e-02,
         -1.6912e-02, -1.6946e-02, -1.6946e-02, -6.8173e-02, -1.6944e-02,
         -1.6946e-02],
        [-1.6884e-02, -1.6934e-02, -5.7956e-03, -1.6933e-02, -1.6934e-02,
         -2.8357e-01, -1.0214e-01, -2.4625e-01, -2.3488e-01, -1.6940e-02,
         -1.6934e-02, -7.5288e-02, -1.6727e-01, -5.8632e-02, -1.2266e-02,
         -1.6899e-02, -1.6933e-02, -1.6933e-02, -6.8314e-02, -1.6931e-02,
         -1.6933e-02],
        [-1.6891e-02, -1.6941e-02, -5.8310e-03, -1.6940e-02, -1.6940e-02,
         -2.8349e-01, -1.0198e-01, -2.4620e-01, -2.3486e-01, -1.6946e-02,
         -1.6940e-02, -7.5158e-02, -1.6714e-01, -5.8641e-02, -1.2227e-02,
         -1.6905e-02, -1.6939e-02, -1.6940e-02, -6.8238e-02, -1.6938e-02,
         -1.6940e-02],
        [ 1.6431e-01,  1.5971e-01,  1.2190e+00,  1.5989e-01,  1.5979e-01,
         -4.3554e+00,  1.6794e+01, -4.9250e+00, -5.0617e+00,  1.5942e-01,
          1.5980e-01,  4.5605e+00, -1.8860e+00,  6.7260e+00,  7.3058e-01,
          1.6285e-01,  1.5990e-01,  1.5987e-01,  5.0913e+00,  1.6013e-01,
          1.5986e-01],
        [-6.0747e-02, -6.6248e-02,  1.4817e+00, -6.5968e-02, -6.6161e-02,
         -4.5299e+00,  1.7766e+01, -5.3315e+00, -5.6826e+00, -6.6141e-02,
         -6.6124e-02,  5.0180e+00, -1.9958e+00,  1.0873e+01, -1.2152e+00,
         -6.1325e-02, -6.5956e-02, -6.6017e-02,  4.7573e+00, -6.5496e-02,
         -6.6024e-02],
        [-1.6884e-02, -1.6934e-02, -5.7956e-03, -1.6933e-02, -1.6934e-02,
         -2.8357e-01, -1.0214e-01, -2.4625e-01, -2.3488e-01, -1.6940e-02,
         -1.6934e-02, -7.5288e-02, -1.6727e-01, -5.8632e-02, -1.2266e-02,
         -1.6899e-02, -1.6933e-02, -1.6933e-02, -6.8314e-02, -1.6931e-02,
         -1.6933e-02],
        [-1.7524e-01, -1.6799e-01, -1.4359e+00, -1.6822e-01, -1.6794e-01,
          3.4954e+00, -1.3647e+01,  4.3674e+00,  4.3012e+00, -1.6496e-01,
         -1.6814e-01, -3.8777e+00,  1.4411e+00, -1.0445e+01,  6.9358e-01,
         -1.7319e-01, -1.6822e-01, -1.6818e-01, -4.0245e+00, -1.6848e-01,
         -1.6818e-01],
        [ 1.2320e-01,  1.2535e-01,  4.0976e-01,  1.2538e-01,  1.2538e-01,
         -3.6324e+00,  1.4014e+01, -4.2960e+00, -4.2597e+00,  1.2586e-01,
          1.2536e-01,  4.1275e+00, -1.8673e+00,  1.8825e+00,  2.1454e-01,
          1.2374e-01,  1.2539e-01,  1.2538e-01,  2.0209e+00,  1.2540e-01,
          1.2538e-01],
        [ 1.9959e-02,  1.9953e-02, -6.9307e-02,  1.9952e-02,  1.9952e-02,
          8.3581e-01,  1.4307e-01,  7.1998e-01,  6.8128e-01,  1.9950e-02,
          1.9953e-02,  1.3389e-01,  3.8351e-01,  5.7495e-02, -2.4914e-02,
          1.9961e-02,  1.9952e-02,  1.9952e-02, -4.5470e-02,  1.9951e-02,
          1.9952e-02],
        [-1.6890e-02, -1.6940e-02, -5.8264e-03, -1.6939e-02, -1.6940e-02,
         -2.8355e-01, -1.0200e-01, -2.4624e-01, -2.3489e-01, -1.6946e-02,
         -1.6939e-02, -7.5176e-02, -1.6717e-01, -5.8640e-02, -1.2233e-02,
         -1.6905e-02, -1.6939e-02, -1.6939e-02, -6.8252e-02, -1.6937e-02,
         -1.6939e-02],
        [-4.5050e-03, -1.1498e-02,  1.0975e+00, -1.1352e-02, -1.1467e-02,
         -4.4484e+00,  1.6082e+01, -4.9699e+00, -4.9162e+00, -1.2599e-02,
         -1.1423e-02,  4.9491e+00, -2.2115e+00,  1.1154e+01, -8.0720e-01,
         -6.3584e-03, -1.1349e-02, -1.1378e-02,  3.1749e+00, -1.1094e-02,
         -1.1377e-02],
        [-1.6884e-02, -1.6934e-02, -5.7956e-03, -1.6933e-02, -1.6934e-02,
         -2.8357e-01, -1.0215e-01, -2.4625e-01, -2.3488e-01, -1.6940e-02,
         -1.6934e-02, -7.5288e-02, -1.6727e-01, -5.8632e-02, -1.2266e-02,
         -1.6899e-02, -1.6933e-02, -1.6933e-02, -6.8314e-02, -1.6931e-02,
         -1.6933e-02],
        [ 1.7494e-01,  1.5235e-01, -1.9524e+00,  1.5357e-01,  1.5283e-01,
          3.9144e+00, -1.5955e+01,  4.7884e+00,  4.9636e+00,  1.4929e-01,
          1.5294e-01, -4.3128e+00,  2.1636e+00, -1.1807e+01,  1.6107e+00,
          1.7037e-01,  1.5361e-01,  1.5339e-01, -4.1452e+00,  1.5517e-01,
          1.5336e-01],
        [-1.6884e-02, -1.6934e-02, -5.7956e-03, -1.6933e-02, -1.6934e-02,
         -2.8357e-01, -1.0215e-01, -2.4625e-01, -2.3488e-01, -1.6940e-02,
         -1.6934e-02, -7.5288e-02, -1.6727e-01, -5.8632e-02, -1.2266e-02,
         -1.6899e-02, -1.6933e-02, -1.6933e-02, -6.8315e-02, -1.6931e-02,
         -1.6933e-02],
        [-9.6632e-02, -1.0035e-01,  1.6253e+00, -9.9796e-02, -1.0001e-01,
         -4.5970e+00,  1.8369e+01, -5.5369e+00, -5.9196e+00, -1.0034e-01,
         -1.0010e-01,  4.9617e+00, -1.9049e+00,  1.0787e+01, -1.5239e+00,
         -9.6486e-02, -9.9769e-02, -9.9855e-02,  5.6379e+00, -9.9179e-02,
         -9.9883e-02],
        [-1.6884e-02, -1.6934e-02, -5.7956e-03, -1.6933e-02, -1.6934e-02,
         -2.8357e-01, -1.0214e-01, -2.4625e-01, -2.3488e-01, -1.6940e-02,
         -1.6934e-02, -7.5288e-02, -1.6727e-01, -5.8632e-02, -1.2266e-02,
         -1.6899e-02, -1.6933e-02, -1.6933e-02, -6.8315e-02, -1.6931e-02,
         -1.6933e-02],
        [-1.6888e-02, -1.6938e-02, -5.8158e-03, -1.6937e-02, -1.6938e-02,
         -2.8358e-01, -1.0205e-01, -2.4626e-01, -2.3490e-01, -1.6944e-02,
         -1.6938e-02, -7.5215e-02, -1.6721e-01, -5.8637e-02, -1.2245e-02,
         -1.6903e-02, -1.6937e-02, -1.6937e-02, -6.8276e-02, -1.6935e-02,
         -1.6937e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.8903, -1.8772, -1.8772,  0.2110, -1.8760, -1.8772, -1.8770,  0.1510,
         0.4090, -1.8772, -0.1568, -0.0573,  4.4463, -1.8768,  0.4352, -1.8772,
        -0.3069, -1.8772,  0.4827, -1.8772, -1.8768], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0213,   0.0228,   0.0228, -11.6157,   0.0225,   0.0228,   0.0226,
         -10.9526, -13.3876,   0.0228,  13.1827,  -6.9725,   7.5211,   0.0227,
         -10.9675,   0.0228,  16.9599,   0.0228, -14.5530,   0.0228,   0.0227]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  0.3643, -10.0173],
        [ -1.3759,   0.4978],
        [  3.8074,  -6.9557],
        [ -1.3741,   0.4932],
        [ -1.3730,   0.4958],
        [ -1.2847,   0.3280],
        [ -1.3761,   0.4920],
        [ 11.5432,   2.4168],
        [  9.4337,   4.7664],
        [ -5.6038,   8.4728],
        [ 10.5329,  -0.0383],
        [-15.9227,  -5.5748],
        [ -1.3761,   0.4976],
        [ -5.7532,   3.3253],
        [ -1.3797,   0.4857],
        [ -1.3797,   0.4857],
        [ 10.3681,  -2.5889],
        [ -1.3736,   0.4937],
        [-12.7641,  -4.7126],
        [  0.6158,   9.0595],
        [-14.6906,  -5.2648]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-7.7408, -4.1198, -7.9561, -4.1177, -4.1191, -4.4437, -4.1169, -2.8317,
        -3.1782,  6.1350, -9.2869, -2.8658, -4.1196, -9.4177, -4.1136, -4.1136,
        -6.6144, -4.1180, -4.4063,  5.9743, -3.7624], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 7.8523e+00, -3.5945e-02,  3.7748e+00, -3.4704e-02, -3.6695e-02,
          6.1199e-01, -3.3138e-02, -4.0633e+00, -3.8834e+00, -1.9737e+01,
         -6.1035e+00,  9.6794e+00, -3.5859e-02,  7.7039e-03, -2.9539e-02,
         -2.9540e-02, -5.3906e+00, -3.5125e-02,  5.7477e+00, -1.5350e+01,
          7.0403e+00],
        [ 1.1467e+00, -2.2351e-01,  6.9510e-01, -2.2466e-01, -2.2628e-01,
         -1.0466e-01, -2.2241e-01,  1.0868e+00,  2.8637e+00, -5.8700e+00,
          5.9318e+00, -2.4860e+00, -2.2314e-01,  6.0759e+00, -2.1621e-01,
         -2.1621e-01,  7.4000e+00, -2.2522e-01, -3.9545e-02, -6.7778e-01,
         -4.3958e-01],
        [ 4.0482e-03, -5.3896e-02,  8.9510e-01, -5.3924e-02, -5.3906e-02,
         -3.6792e-02, -5.3936e-02, -6.1301e-01, -1.7037e+00, -9.2228e-01,
          7.1206e-01, -1.7397e+00, -5.3898e-02,  5.5254e-02, -5.3981e-02,
         -5.3980e-02,  1.6086e-01, -5.3919e-02, -4.4048e-01, -2.8451e+00,
         -9.5067e-01],
        [-1.4652e-02, -5.6275e-02,  8.7752e-01, -5.6321e-02, -5.6296e-02,
         -3.9903e-02, -5.6334e-02, -6.2640e-01, -1.6949e+00, -9.1605e-01,
          7.0082e-01, -1.7280e+00, -5.6278e-02,  5.1377e-02, -5.6393e-02,
         -5.6393e-02,  1.4561e-01, -5.6315e-02, -4.3656e-01, -2.8064e+00,
         -9.4206e-01],
        [-1.4730e-02, -5.6284e-02,  8.7745e-01, -5.6331e-02, -5.6306e-02,
         -3.9916e-02, -5.6343e-02, -6.2646e-01, -1.6949e+00, -9.1603e-01,
          7.0076e-01, -1.7279e+00, -5.6288e-02,  5.1361e-02, -5.6403e-02,
         -5.6403e-02,  1.4554e-01, -5.6325e-02, -4.3654e-01, -2.8062e+00,
         -9.4202e-01],
        [-1.2085e-02, -5.5963e-02,  8.7994e-01, -5.6007e-02, -5.5983e-02,
         -3.9493e-02, -5.6019e-02, -6.2458e-01, -1.6961e+00, -9.1688e-01,
          7.0236e-01, -1.7296e+00, -5.5966e-02,  5.1912e-02, -5.6077e-02,
         -5.6077e-02,  1.4769e-01, -5.6001e-02, -4.3711e-01, -2.8116e+00,
         -9.4326e-01],
        [-1.4585e-02, -5.6267e-02,  8.7759e-01, -5.6313e-02, -5.6289e-02,
         -3.9893e-02, -5.6326e-02, -6.2636e-01, -1.6949e+00, -9.1608e-01,
          7.0085e-01, -1.7280e+00, -5.6270e-02,  5.1391e-02, -5.6385e-02,
         -5.6385e-02,  1.4566e-01, -5.6307e-02, -4.3657e-01, -2.8065e+00,
         -9.4209e-01],
        [-1.2488e-02, -5.6012e-02,  8.7956e-01, -5.6057e-02, -5.6033e-02,
         -3.9558e-02, -5.6069e-02, -6.2486e-01, -1.6959e+00, -9.1675e-01,
          7.0212e-01, -1.7293e+00, -5.6015e-02,  5.1829e-02, -5.6127e-02,
         -5.6126e-02,  1.4737e-01, -5.6050e-02, -4.3702e-01, -2.8108e+00,
         -9.4306e-01],
        [-1.2433e-02, -5.6006e-02,  8.7962e-01, -5.6050e-02, -5.6026e-02,
         -3.9548e-02, -5.6062e-02, -6.2483e-01, -1.6960e+00, -9.1677e-01,
          7.0217e-01, -1.7294e+00, -5.6009e-02,  5.1842e-02, -5.6120e-02,
         -5.6120e-02,  1.4741e-01, -5.6044e-02, -4.3704e-01, -2.8109e+00,
         -9.4309e-01],
        [-5.7047e+00, -2.3378e-01, -2.5448e+00, -2.2514e-01, -2.2675e-01,
         -3.1456e-02, -2.2658e-01, -2.0926e+00, -8.3604e+00,  1.0653e+01,
         -8.2040e+00, -1.5505e+00, -2.3396e-01, -4.3192e+00, -2.2583e-01,
         -2.2584e-01, -9.5707e+00, -2.2518e-01, -7.5062e-01,  2.7696e+00,
         -1.0697e+00],
        [ 1.5639e+00, -7.7273e-02, -1.9442e+00, -7.6099e-02, -7.6576e-02,
         -2.3857e-02, -7.6033e-02,  1.1333e+00,  1.7580e+00,  7.9918e+00,
         -3.6831e+00,  2.5025e+00, -7.7200e-02, -7.3860e+00, -7.4335e-02,
         -7.4338e-02, -4.0989e-01, -7.6181e-02,  7.5490e-01, -1.8908e+00,
          1.6015e+00],
        [-1.2341e-02, -5.5994e-02,  8.7970e-01, -5.6039e-02, -5.6015e-02,
         -3.9534e-02, -5.6051e-02, -6.2476e-01, -1.6960e+00, -9.1679e-01,
          7.0221e-01, -1.7294e+00, -5.5998e-02,  5.1859e-02, -5.6109e-02,
         -5.6108e-02,  1.4748e-01, -5.6032e-02, -4.3705e-01, -2.8111e+00,
         -9.4313e-01],
        [-1.4277e-02, -5.6230e-02,  8.7788e-01, -5.6276e-02, -5.6251e-02,
         -3.9844e-02, -5.6288e-02, -6.2614e-01, -1.6951e+00, -9.1617e-01,
          7.0104e-01, -1.7282e+00, -5.6233e-02,  5.1455e-02, -5.6347e-02,
         -5.6347e-02,  1.4591e-01, -5.6269e-02, -4.3664e-01, -2.8072e+00,
         -9.4223e-01],
        [ 7.1577e-02, -4.3380e-02,  9.5860e-01, -4.3350e-02, -4.3357e-02,
         -2.3382e-02, -4.3358e-02, -5.6413e-01, -1.7315e+00, -9.4892e-01,
          7.5287e-01, -1.7811e+00, -4.3381e-02,  6.8992e-02, -4.3352e-02,
         -4.3352e-02,  2.1621e-01, -4.3348e-02, -4.5316e-01, -2.9876e+00,
         -9.8078e-01],
        [-9.3043e-03, -5.5620e-02,  8.8255e-01, -5.5661e-02, -5.5639e-02,
         -3.9042e-02, -5.5673e-02, -6.2258e-01, -1.6975e+00, -9.1778e-01,
          7.0405e-01, -1.7313e+00, -5.5623e-02,  5.2492e-02, -5.5729e-02,
         -5.5728e-02,  1.4996e-01, -5.5655e-02, -4.3769e-01, -2.8174e+00,
         -9.4453e-01],
        [-3.3176e+00, -5.5979e-02, -4.0759e+00, -6.4866e-02, -6.3826e-02,
          2.5605e-01, -6.2339e-02, -4.6761e+00, -2.5540e+00,  8.0468e+00,
         -2.1146e+00, -7.5385e-01, -5.5575e-02, -5.3663e+00, -5.7307e-02,
         -5.7300e-02, -1.0895e+01, -6.5186e-02, -1.1934e+00,  5.0837e-01,
         -1.2470e+00],
        [-7.4026e-03, -5.5382e-02,  8.8434e-01, -5.5421e-02, -5.5399e-02,
         -3.8730e-02, -5.5434e-02, -6.2122e-01, -1.6984e+00, -9.1841e-01,
          7.0519e-01, -1.7325e+00, -5.5385e-02,  5.2887e-02, -5.5487e-02,
         -5.5487e-02,  1.5151e-01, -5.5415e-02, -4.3810e-01, -2.8213e+00,
         -9.4541e-01],
        [-1.1852e-02, -5.5935e-02,  8.8016e-01, -5.5978e-02, -5.5955e-02,
         -3.9455e-02, -5.5991e-02, -6.2440e-01, -1.6963e+00, -9.1695e-01,
          7.0253e-01, -1.7297e+00, -5.5938e-02,  5.1964e-02, -5.6048e-02,
         -5.6048e-02,  1.4788e-01, -5.5972e-02, -4.3715e-01, -2.8121e+00,
         -9.4336e-01],
        [-1.4794e-02, -5.6292e-02,  8.7739e-01, -5.6339e-02, -5.6314e-02,
         -3.9927e-02, -5.6352e-02, -6.2651e-01, -1.6948e+00, -9.1600e-01,
          7.0073e-01, -1.7279e+00, -5.6296e-02,  5.1347e-02, -5.6411e-02,
         -5.6411e-02,  1.4549e-01, -5.6333e-02, -4.3653e-01, -2.8061e+00,
         -9.4199e-01],
        [-1.6085e-02, -5.6447e-02,  8.7617e-01, -5.6495e-02, -5.6470e-02,
         -4.0131e-02, -5.6507e-02, -6.2742e-01, -1.6942e+00, -9.1560e-01,
          7.0000e-01, -1.7270e+00, -5.6450e-02,  5.1086e-02, -5.6568e-02,
         -5.6568e-02,  1.4444e-01, -5.6488e-02, -4.3624e-01, -2.8035e+00,
         -9.4138e-01],
        [-6.6006e+00,  7.6261e-03, -4.1555e+00,  1.8096e-02,  1.3292e-02,
          5.0296e-01,  1.8615e-02, -7.0716e+00, -2.5024e+00,  2.9772e+00,
         -4.8041e-01,  2.4408e+00,  7.4696e-03, -5.4359e-01,  2.1141e-02,
          2.1134e-02, -3.0011e+00,  1.7674e-02,  4.0213e+00,  1.4162e+00,
          3.8640e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.0361, -2.7079, -3.5344, -3.5463, -3.5464, -3.5447, -3.5463, -3.5449,
        -3.5449,  0.3515,  0.4822, -3.5449, -3.5461, -3.4885, -3.5429, -1.5702,
        -3.5417, -3.5445, -3.5464, -3.5472, -2.4943], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-2.9403e-01, -6.8369e+00, -1.8732e-01, -1.7644e-01, -1.7639e-01,
         -1.7788e-01, -1.7648e-01, -1.7765e-01, -1.7769e-01, -1.3454e+00,
         -8.0617e+00, -1.7774e-01, -1.7665e-01, -2.3424e-01, -1.7947e-01,
         -1.3641e-01, -1.8056e-01, -1.7802e-01, -1.7635e-01, -1.7564e-01,
         -4.1726e-02],
        [-2.9412e-02,  9.7942e-01,  7.5003e-01,  7.4611e-01,  7.4609e-01,
          7.4665e-01,  7.4612e-01,  7.4656e-01,  7.4658e-01, -1.4584e+01,
          9.0319e-01,  7.4659e-01,  7.4618e-01,  7.6507e-01,  7.4722e-01,
          1.8697e+00,  7.4762e-01,  7.4669e-01,  7.4608e-01,  7.4581e-01,
         -3.7839e-01],
        [-3.7916e-01, -6.6587e+00, -2.2520e-01, -2.1229e-01, -2.1223e-01,
         -2.1401e-01, -2.1233e-01, -2.1373e-01, -2.1377e-01, -1.1944e+00,
         -7.5864e+00, -2.1383e-01, -2.1253e-01, -2.8000e-01, -2.1589e-01,
         -1.2259e-01, -2.1719e-01, -2.1417e-01, -2.1219e-01, -2.1134e-01,
         -2.9794e-02],
        [-3.7590e-01, -6.0353e+00, -1.9416e-01, -1.8355e-01, -1.8350e-01,
         -1.8496e-01, -1.8359e-01, -1.8474e-01, -1.8477e-01, -1.0947e+00,
         -6.8677e+00, -1.8482e-01, -1.8375e-01, -2.3896e-01, -1.8651e-01,
         -9.4004e-02, -1.8758e-01, -1.8510e-01, -1.8347e-01, -1.8277e-01,
         -2.4301e-02],
        [ 6.9697e+00,  1.9907e+00,  3.6667e-01,  3.6855e-01,  3.6856e-01,
          3.6830e-01,  3.6854e-01,  3.6834e-01,  3.6833e-01,  2.4752e+00,
          1.4941e+00,  3.6833e-01,  3.6851e-01,  3.5865e-01,  3.6802e-01,
          8.2121e-01,  3.6783e-01,  3.6827e-01,  3.6856e-01,  3.6867e-01,
         -6.2887e-01],
        [-1.9407e-01, -5.4950e+00, -1.0651e-01, -1.0102e-01, -1.0100e-01,
         -1.0175e-01, -1.0104e-01, -1.0164e-01, -1.0165e-01, -1.0061e+00,
         -6.1468e+00, -1.0168e-01, -1.0113e-01, -1.3021e-01, -1.0255e-01,
         -5.9496e-02, -1.0310e-01, -1.0182e-01, -1.0098e-01, -1.0062e-01,
         -2.1041e-02],
        [-4.5335e-01, -6.4000e+00, -2.1889e-01, -2.0665e-01, -2.0660e-01,
         -2.0828e-01, -2.0669e-01, -2.0802e-01, -2.0806e-01, -1.0006e+00,
         -6.9906e+00, -2.0812e-01, -2.0688e-01, -2.7092e-01, -2.1007e-01,
         -1.0239e-01, -2.1130e-01, -2.0843e-01, -2.0656e-01, -2.0575e-01,
         -3.0783e-02],
        [-1.3166e+01, -7.2823e-01, -1.7348e+00, -1.6981e+00, -1.6979e+00,
         -1.7031e+00, -1.6982e+00, -1.7023e+00, -1.7024e+00,  2.2688e+00,
          1.3842e+00, -1.7026e+00, -1.6988e+00, -1.8689e+00, -1.7085e+00,
          1.4387e+00, -1.7123e+00, -1.7035e+00, -1.6978e+00, -1.6953e+00,
          2.5727e+00],
        [-3.5076e-01, -6.4025e+00, -2.2415e-01, -2.1136e-01, -2.1131e-01,
         -2.1307e-01, -2.1141e-01, -2.1280e-01, -2.1284e-01, -1.1411e+00,
         -7.2672e+00, -2.1290e-01, -2.1161e-01, -2.7840e-01, -2.1493e-01,
         -1.1278e-01, -2.1622e-01, -2.1323e-01, -2.1127e-01, -2.1042e-01,
         -2.9719e-02],
        [-1.7478e+01,  3.7244e-01, -7.8764e-01, -7.6242e-01, -7.6231e-01,
         -7.6587e-01, -7.6251e-01, -7.6534e-01, -7.6541e-01,  1.3646e+00,
          1.1634e+00, -7.6553e-01, -7.6293e-01, -8.7805e-01, -7.6964e-01,
          5.8132e+00, -7.7220e-01, -7.6620e-01, -7.6222e-01, -7.6051e-01,
          1.9815e+00],
        [ 1.6858e+01,  1.6883e+00,  8.6259e-01,  8.3726e-01,  8.3715e-01,
          8.4073e-01,  8.3735e-01,  8.4019e-01,  8.4027e-01,  5.2859e-01,
          1.0518e+00,  8.4039e-01,  8.3777e-01,  9.5342e-01,  8.4451e-01,
         -4.8830e+00,  8.4708e-01,  8.4106e-01,  8.3706e-01,  8.3534e-01,
         -1.4446e+00],
        [-9.3497e-02, -6.7769e+00, -1.3942e-01, -1.3276e-01, -1.3273e-01,
         -1.3365e-01, -1.3278e-01, -1.3351e-01, -1.3353e-01, -1.9735e+00,
         -8.2718e+00, -1.3356e-01, -1.3289e-01, -1.6726e-01, -1.3462e-01,
         -2.0517e-01, -1.3529e-01, -1.3373e-01, -1.3271e-01, -1.3227e-01,
         -6.0013e-02],
        [-7.0834e-02, -6.7082e+00, -1.2499e-01, -1.1971e-01, -1.1969e-01,
         -1.2042e-01, -1.1973e-01, -1.2031e-01, -1.2032e-01, -2.0590e+00,
         -8.1866e+00, -1.2035e-01, -1.1982e-01, -1.4700e-01, -1.2119e-01,
         -2.0814e-01, -1.2172e-01, -1.2048e-01, -1.1967e-01, -1.1932e-01,
         -5.5446e-02],
        [ 2.7255e+01,  1.1022e+01, -8.5807e-01, -8.5612e-01, -8.5612e-01,
         -8.5640e-01, -8.5614e-01, -8.5635e-01, -8.5636e-01, -4.1437e+00,
          4.0575e+00, -8.5637e-01, -8.5616e-01, -8.6542e-01, -8.5667e-01,
         -2.7676e+00, -8.5688e-01, -8.5641e-01, -8.5611e-01, -8.5596e-01,
         -4.1324e+00]], device='cuda:0'))])
xi:  [630.29266]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 851.9669166303607
W_T_median: 823.3386221153844
W_T_pctile_5: 630.8150707675965
W_T_CVAR_5_pct: 366.10340533750934
Average q (qsum/M+1):  52.39831936743952
Optimal xi:  [630.29266]
Expected(across Rb) median(across samples) p_equity:  0.02689474241305369
obj fun:  tensor(-1990.4382, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: 7_Factor_plusEWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
