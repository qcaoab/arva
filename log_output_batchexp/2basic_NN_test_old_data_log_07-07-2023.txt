Starting at: 
07-07-23_11:29

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_nom_ret', 'VWD_nom_ret']
############# End: defined asset  basket #################
               CPI_nom_ret  B10_nom_ret  VWD_nom_ret
My identifier                                       
192607           -0.011299     0.005383     0.031411
192608           -0.005714     0.005363     0.028647
192609            0.005747     0.005343     0.005787
192610            0.005714     0.005323    -0.028996
192611            0.005682     0.005303     0.028554
               CPI_nom_ret  B10_nom_ret  VWD_nom_ret
My identifier                                       
202208           -0.000354    -0.043289    -0.036240
202209            0.002151    -0.050056    -0.091324
202210            0.004056    -0.014968     0.077403
202211           -0.001010     0.040789     0.052365
202212           -0.003070    -0.018566    -0.057116
Indices constructed with column names:
Index(['date_for_plt', 'CPI_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
B10_real_ret    0.001637
VWD_real_ret    0.006759
dtype: float64


timeseries_basket['data_df_stdev'] = 
B10_real_ret    0.019258
VWD_real_ret    0.053610
dtype: float64


timeseries_basket['data_df_corr'] = 
              B10_real_ret  VWD_real_ret
B10_real_ret      1.000000      0.090987
VWD_real_ret      0.090987      1.000000


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 3
-----------------------------------------------
Dates USED bootstrapping:
Start: 192601
End: 201912
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 1016.6696062978402
W_T_median: 795.2149830930057
W_T_pctile_5: -126.76775541961388
W_T_CVAR_5_pct: -262.80198059105965
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1528.2059983789147
Current xi:  [96.09667]
objective value function right now is: -1528.2059983789147
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1543.3349804320753
Current xi:  [87.0694]
objective value function right now is: -1543.3349804320753
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1548.0207584923855
Current xi:  [79.49054]
objective value function right now is: -1548.0207584923855
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1551.7400935178032
Current xi:  [75.05684]
objective value function right now is: -1551.7400935178032
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1553.5853682533257
Current xi:  [71.30362]
objective value function right now is: -1553.5853682533257
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1556.3322883623014
Current xi:  [68.90293]
objective value function right now is: -1556.3322883623014
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [67.30533]
objective value function right now is: -1552.6858248447963
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [66.108665]
objective value function right now is: -1555.7568999626073
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [65.210434]
objective value function right now is: -1553.207478008064
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [64.295456]
objective value function right now is: -1554.6592072553935
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1556.5007600276197
Current xi:  [63.388344]
objective value function right now is: -1556.5007600276197
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1556.690273772373
Current xi:  [62.615383]
objective value function right now is: -1556.690273772373
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1557.8681502920595
Current xi:  [62.107254]
objective value function right now is: -1557.8681502920595
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [61.44966]
objective value function right now is: -1554.1892681310267
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.99009]
objective value function right now is: -1554.0026390456455
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.656292]
objective value function right now is: -1553.4800831102925
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1558.4262536723188
Current xi:  [60.35997]
objective value function right now is: -1558.4262536723188
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [59.87604]
objective value function right now is: -1558.0561980936607
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [59.597485]
objective value function right now is: -1554.1744680732872
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [59.444424]
objective value function right now is: -1557.7731593829794
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1571.6052446274264
Current xi:  [59.093285]
objective value function right now is: -1571.6052446274264
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1572.3664891545168
Current xi:  [59.05683]
objective value function right now is: -1572.3664891545168
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [59.86681]
objective value function right now is: -1571.6572024386694
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.38783]
objective value function right now is: -1571.326273562092
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1572.7792478664499
Current xi:  [60.59572]
objective value function right now is: -1572.7792478664499
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.625275]
objective value function right now is: -1558.6974405609046
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.18121]
objective value function right now is: -1569.2042994194517
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1573.537505296968
Current xi:  [60.605564]
objective value function right now is: -1573.537505296968
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1573.970048124467
Current xi:  [60.54714]
objective value function right now is: -1573.970048124467
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.391045]
objective value function right now is: -1569.0258078890536
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.74114]
objective value function right now is: -1569.5817171292092
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [61.048042]
objective value function right now is: -1570.6475089146707
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.703556]
objective value function right now is: -1562.1854941574004
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.81639]
objective value function right now is: -1566.4203196019876
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [61.214924]
objective value function right now is: -1568.615862527559
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1574.140171250738
Current xi:  [61.147514]
objective value function right now is: -1574.140171250738
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1574.4433030089888
Current xi:  [60.962776]
objective value function right now is: -1574.4433030089888
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1574.9539038261998
Current xi:  [61.003094]
objective value function right now is: -1574.9539038261998
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1574.968997112854
Current xi:  [61.0141]
objective value function right now is: -1574.968997112854
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [61.291206]
objective value function right now is: -1574.6635798329926
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1575.2886996405618
Current xi:  [61.47896]
objective value function right now is: -1575.2886996405618
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1575.616221133297
Current xi:  [61.56385]
objective value function right now is: -1575.616221133297
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [61.647495]
objective value function right now is: -1574.672434433004
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [61.81905]
objective value function right now is: -1575.1272316149516
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [61.93823]
objective value function right now is: -1575.595276652814
new min fval from sgd:  -1575.6558144440753
new min fval from sgd:  -1575.7103109413465
new min fval from sgd:  -1575.718187778714
new min fval from sgd:  -1575.76487672337
new min fval from sgd:  -1575.8025117946377
new min fval from sgd:  -1575.8036510613995
new min fval from sgd:  -1575.862343276395
new min fval from sgd:  -1575.8904170950352
new min fval from sgd:  -1575.9140922910126
new min fval from sgd:  -1575.9202497124816
new min fval from sgd:  -1575.957186481811
new min fval from sgd:  -1575.9875835589985
new min fval from sgd:  -1575.9906517242957
new min fval from sgd:  -1575.9927886525306
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [61.98337]
objective value function right now is: -1575.3408558683298
new min fval from sgd:  -1576.0261918188933
new min fval from sgd:  -1576.0638423921453
new min fval from sgd:  -1576.0688037450948
new min fval from sgd:  -1576.0723925555985
new min fval from sgd:  -1576.0963032063128
new min fval from sgd:  -1576.1370428723421
new min fval from sgd:  -1576.175630809126
new min fval from sgd:  -1576.226881880241
new min fval from sgd:  -1576.243390047554
new min fval from sgd:  -1576.2716636048942
new min fval from sgd:  -1576.3031261826695
new min fval from sgd:  -1576.3105348432819
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [61.9179]
objective value function right now is: -1575.1792833458903
new min fval from sgd:  -1576.3127559881657
new min fval from sgd:  -1576.3326387652066
new min fval from sgd:  -1576.3541963404455
new min fval from sgd:  -1576.370894570714
new min fval from sgd:  -1576.3768145356457
new min fval from sgd:  -1576.3930419050055
new min fval from sgd:  -1576.4230138058258
new min fval from sgd:  -1576.4426806696426
new min fval from sgd:  -1576.4575949033633
new min fval from sgd:  -1576.463459322209
new min fval from sgd:  -1576.5193592699625
new min fval from sgd:  -1576.5469744547272
new min fval from sgd:  -1576.6254542934746
new min fval from sgd:  -1576.6574179711706
new min fval from sgd:  -1576.686485888689
new min fval from sgd:  -1576.704681396621
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [61.983913]
objective value function right now is: -1575.9841277021983
new min fval from sgd:  -1576.7067793225744
new min fval from sgd:  -1576.7130876516285
new min fval from sgd:  -1576.7158009002774
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [61.971138]
objective value function right now is: -1576.702902414589
new min fval from sgd:  -1576.7198168058283
new min fval from sgd:  -1576.7244176303116
new min fval from sgd:  -1576.7283033999875
new min fval from sgd:  -1576.7305837555446
new min fval from sgd:  -1576.7362759511007
new min fval from sgd:  -1576.7408437740062
new min fval from sgd:  -1576.7421603188589
new min fval from sgd:  -1576.7465958357136
new min fval from sgd:  -1576.7536119662557
new min fval from sgd:  -1576.7561630132836
new min fval from sgd:  -1576.756548074215
new min fval from sgd:  -1576.7637435658733
new min fval from sgd:  -1576.7683938720143
new min fval from sgd:  -1576.7685610470953
new min fval from sgd:  -1576.773838569575
new min fval from sgd:  -1576.778915477808
new min fval from sgd:  -1576.7851265686174
new min fval from sgd:  -1576.7870135970713
new min fval from sgd:  -1576.8090134183342
new min fval from sgd:  -1576.8323007928855
new min fval from sgd:  -1576.8324898555315
new min fval from sgd:  -1576.834240393497
new min fval from sgd:  -1576.8362871590205
new min fval from sgd:  -1576.8399712339358
new min fval from sgd:  -1576.844963099882
new min fval from sgd:  -1576.8529019122575
new min fval from sgd:  -1576.8539808552166
new min fval from sgd:  -1576.8640598588772
new min fval from sgd:  -1576.8665621293128
new min fval from sgd:  -1576.8700951400951
new min fval from sgd:  -1576.8785065899003
new min fval from sgd:  -1576.8823585148605
new min fval from sgd:  -1576.8865901894942
new min fval from sgd:  -1576.892715392835
new min fval from sgd:  -1576.8972506341838
new min fval from sgd:  -1576.8980809420852
new min fval from sgd:  -1576.8986009820396
new min fval from sgd:  -1576.9027076545772
new min fval from sgd:  -1576.907106379881
new min fval from sgd:  -1576.9168873160793
new min fval from sgd:  -1576.9178436423983
new min fval from sgd:  -1576.9209374073205
new min fval from sgd:  -1576.9255828130088
new min fval from sgd:  -1576.9256164730157
new min fval from sgd:  -1576.9300316425963
new min fval from sgd:  -1576.9338743915173
new min fval from sgd:  -1576.9390518105631
new min fval from sgd:  -1576.9432116697135
new min fval from sgd:  -1576.9470167837162
new min fval from sgd:  -1576.9558806437683
new min fval from sgd:  -1576.9559281342426
new min fval from sgd:  -1576.9575475588429
new min fval from sgd:  -1576.9703737424713
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [61.93708]
objective value function right now is: -1576.8771301620573
min fval:  -1576.9703737424713
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  0.6522,  -0.3177],
        [ -8.1846,   0.3809],
        [ -7.7465,  -1.7673],
        [-55.7014,  -7.7982],
        [  9.2044,  -2.1119],
        [  4.1789,  -8.1868],
        [  7.0066,  -6.7255],
        [  7.6882,  -0.2514],
        [ -0.9088,  -9.4878],
        [ -7.2747,   3.9779]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-4.5703,  6.3400,  8.0083, -8.0051, -7.4009, -7.7532, -7.0688, -7.6069,
        -8.6502,  1.2871], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-7.3714e-04, -5.8750e-01, -7.2778e-01, -1.0823e-01, -2.9624e-01,
         -1.7959e-01, -1.6601e-01, -2.1160e-01, -1.2383e-01, -7.1137e-02],
        [-7.3735e-04, -5.8751e-01, -7.2779e-01, -1.0823e-01, -2.9624e-01,
         -1.7959e-01, -1.6601e-01, -2.1160e-01, -1.2383e-01, -7.1137e-02],
        [-7.3734e-04, -5.8751e-01, -7.2778e-01, -1.0823e-01, -2.9624e-01,
         -1.7959e-01, -1.6601e-01, -2.1160e-01, -1.2383e-01, -7.1137e-02],
        [-7.3734e-04, -5.8751e-01, -7.2779e-01, -1.0823e-01, -2.9624e-01,
         -1.7959e-01, -1.6601e-01, -2.1160e-01, -1.2383e-01, -7.1137e-02],
        [ 6.5318e-02, -7.4206e+00, -5.5170e+00,  9.6299e+00,  6.8809e+00,
          8.3633e+00,  5.1515e+00,  4.1605e+00,  1.0702e+01, -1.5309e+00],
        [-3.3426e-02, -6.3336e+00, -4.6562e+00,  5.9313e+00,  4.2727e+00,
          6.8992e+00,  4.5975e+00,  2.1060e+00,  9.1649e+00, -1.2535e+00],
        [-7.3735e-04, -5.8751e-01, -7.2779e-01, -1.0823e-01, -2.9624e-01,
         -1.7959e-01, -1.6601e-01, -2.1160e-01, -1.2383e-01, -7.1137e-02],
        [-8.4677e-02, -7.3140e+00, -5.1594e+00,  8.5334e+00,  5.9118e+00,
          8.0074e+00,  4.9695e+00,  3.6833e+00,  1.0214e+01, -1.5444e+00],
        [-7.3734e-04, -5.8751e-01, -7.2779e-01, -1.0823e-01, -2.9624e-01,
         -1.7959e-01, -1.6601e-01, -2.1160e-01, -1.2383e-01, -7.1137e-02],
        [-2.9507e-01,  5.1809e+00,  4.4506e+00, -1.1289e+01, -8.3013e+00,
         -7.6419e+00, -6.8894e+00, -3.0887e+00, -1.0038e+01,  2.3933e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.0839, -1.0839, -1.0839, -1.0839, -1.8055, -1.7417, -1.0839, -1.6242,
        -1.0839,  1.1243], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0788,   0.0788,   0.0788,   0.0788, -10.2948,  -5.0536,   0.0788,
          -8.1644,   0.0788,  15.4722]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 11.6000,   1.8281],
        [ -6.2882,  10.7842],
        [  5.8190,  10.6785],
        [-10.1478,  -0.7792],
        [ -9.1666,  -2.5083],
        [  7.2957,   2.1463],
        [-10.8104,  -2.0013],
        [ -9.7848,  -2.6024],
        [-10.6394,  -1.3195],
        [  6.9642,   7.2602]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-1.8332, 10.0540,  9.2866,  7.3274,  6.2362,  7.4679, -1.3983, -2.4897,
         4.5949,  7.5357], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.9795e-01, -3.7132e+00, -1.6260e-01, -4.9053e+00,  2.4368e-01,
         -3.1273e-01, -2.5129e-01, -5.1265e-02, -3.9018e+00, -1.7409e+00],
        [ 1.1627e+00,  1.6806e+01,  2.2115e+01, -4.9694e+00, -5.7816e+00,
          8.7154e+00, -2.6841e+00, -1.6194e-01, -7.1536e+00,  1.5923e+00],
        [-1.5601e+00, -5.9064e-01, -5.2891e-01, -6.3829e-01, -9.3230e-01,
         -1.3660e+00, -1.5714e-03, -4.6026e-03, -2.5869e-01, -9.2781e-01],
        [-1.3343e+00,  1.5809e+00,  3.5340e-01, -1.2192e+00,  1.8553e+00,
         -1.5194e+00, -8.8133e-01,  6.4764e-01, -3.9419e-01, -2.2739e-01],
        [-7.9317e+00, -6.7214e-01, -4.8632e+00,  1.4013e+00,  2.7110e+00,
         -1.4477e+00,  6.7206e+00,  6.6869e+00,  2.7888e+00, -1.6631e+01],
        [-2.5426e+00, -1.3181e+01, -1.4727e+01,  5.1493e+00,  6.7041e+00,
         -1.3051e+01,  6.8612e+00,  4.3727e+00,  6.4718e+00, -6.8346e+00],
        [-1.2332e+00, -2.4157e+00, -1.2585e+00, -3.9089e+00, -1.3552e+00,
          1.4936e+00, -5.9043e-02, -1.0153e-01, -2.9225e+00, -1.7462e+00],
        [-1.5686e+00, -7.5559e+00,  2.6646e+00,  1.6460e-01,  3.0651e-01,
          1.3724e+00,  3.9387e+00,  3.7156e+00,  4.6095e-01, -5.9201e-01],
        [-2.2790e+00,  5.6225e+00,  5.5191e+00,  1.4761e+00,  5.2762e-01,
          9.7364e-01,  1.1920e+00,  1.7370e+00,  7.9003e-01, -9.0200e+00],
        [-2.5551e+00, -2.1551e+00,  2.8758e-01, -1.8968e+00, -1.6302e+00,
          6.2720e-01, -1.7108e-03, -5.5923e-03, -8.8570e-01, -6.7518e-02]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-0.7001, -1.2926, -2.2207, -1.5626, -3.2547,  1.0414, -1.7886, -5.1322,
        -4.1265, -4.0138], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  3.8344,   0.2019,   0.0223,  -1.1082,  12.7668,  -5.9568,   2.0430,
           1.9284,   1.4396,   0.8692],
        [ -4.0274,  -0.2423,  -0.0202,   1.1100, -12.7868,   6.1152,  -2.0788,
          -2.0078,  -1.4598,  -0.8313]], device='cuda:0'))])
xi:  [61.93805]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 520.4485746596846
W_T_median: 230.04224534949515
W_T_pctile_5: 61.92764540415762
W_T_CVAR_5_pct: -36.58716330926832
Average q (qsum/M+1):  52.05024177797379
Optimal xi:  [61.93805]
Expected(across Rb) median(across samples) p_equity:  0.3025400600085656
obj fun:  tensor(-1576.9704, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
