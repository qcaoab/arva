Starting at: 
06-07-23_16:11

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = 7_Factor_plusEWD
timeseries_basket['basket_desc'] = 7_Factor_totalmax
timeseries_basket['basket_columns'] = 
['Size_Lo30_real_ret', 'Value_Hi30_real_ret', 'Oprof_Hi30_real_ret', 'Inv_Lo30_real_ret', 'Mom_Hi30_real_ret', 'EP_Hi30_real_ret', 'Vol_Lo20_real_ret', 'Div_Hi30_real_ret', 'EQWFact_real_ret', 'T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'EWD_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = 7_Factor_plusEWD
timeseries_basket['basket_desc'] = 7_Factor_totalmax
timeseries_basket['basket_columns'] = 
['Size_Lo30_nom_ret', 'Value_Hi30_nom_ret', 'Oprof_Hi30_nom_ret', 'Inv_Lo30_nom_ret', 'Mom_Hi30_nom_ret', 'EP_Hi30_nom_ret', 'Vol_Lo20_nom_ret', 'Div_Hi30_nom_ret', 'EQWFact_nom_ret', 'T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'EWD_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.031411     0.013051
192608                    0.0319              0.0561  ...     0.028647     0.031002
192609                   -0.0173             -0.0071  ...     0.005787    -0.006499
192610                   -0.0294             -0.0355  ...    -0.028996    -0.034630
192611                   -0.0038              0.0294  ...     0.028554     0.024776

[5 rows x 14 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.036240    -0.011556
202209                   -0.0955             -0.0871  ...    -0.091324    -0.099903
202210                    0.0883              0.1486  ...     0.077403     0.049863
202211                   -0.0076              0.0462  ...     0.052365     0.028123
202212                   -0.0457             -0.0499  ...    -0.057116    -0.047241

[5 rows x 14 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'Oprof_Hi30_nom_ret_ind', 'Inv_Lo30_nom_ret_ind',
       'Mom_Hi30_nom_ret_ind', 'EP_Hi30_nom_ret_ind', 'Vol_Lo20_nom_ret_ind',
       'Div_Hi30_nom_ret_ind', 'EQWFact_nom_ret_ind', 'CPI_nom_ret_ind',
       'T30_nom_ret_ind', 'B10_nom_ret_ind', 'VWD_nom_ret_ind',
       'EWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
Oprof_Hi30_real_ret    0.003948
Inv_Lo30_real_ret      0.004513
Mom_Hi30_real_ret      0.011386
EP_Hi30_real_ret       0.007033
Vol_Lo20_real_ret      0.003529
Div_Hi30_real_ret      0.007888
EQWFact_real_ret       0.004508
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
EWD_real_ret           0.009545
dtype: float64


timeseries_basket['data_df_stdev'] = 
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
Oprof_Hi30_real_ret    0.037444
Inv_Lo30_real_ret      0.037639
Mom_Hi30_real_ret      0.061421
EP_Hi30_real_ret       0.041390
Vol_Lo20_real_ret      0.030737
Div_Hi30_real_ret      0.056728
EQWFact_real_ret       0.037131
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
EWD_real_ret           0.071360
dtype: float64


timeseries_basket['data_df_corr'] = 
                     Size_Lo30_real_ret  ...  EWD_real_ret
Size_Lo30_real_ret             1.000000  ...      0.977206
Value_Hi30_real_ret            0.908542  ...      0.919912
Oprof_Hi30_real_ret            0.433774  ...      0.462336
Inv_Lo30_real_ret              0.455237  ...      0.479661
Mom_Hi30_real_ret              0.903222  ...      0.912002
EP_Hi30_real_ret               0.476966  ...      0.506661
Vol_Lo20_real_ret              0.360014  ...      0.382411
Div_Hi30_real_ret              0.816292  ...      0.849068
EQWFact_real_ret               0.494572  ...      0.513359
T30_real_ret                   0.014412  ...      0.029084
B10_real_ret                   0.012916  ...      0.024853
VWD_real_ret                   0.865290  ...      0.907369
EWD_real_ret                   0.977206  ...      1.000000

[13 rows x 13 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      21  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      21  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 21)     True          21  
2     (21, 21)     True          21  
3      (21, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer      13       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      21  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      21  logistic_sigmoid   
3        obj.layers[3]        3  output_layer      13           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 21)     True          21  
2     (21, 21)     True          21  
3     (21, 13)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       21  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       21  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 21)      True          21  
0     (21, 21)      True          21  
0      (21, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       21  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       21  logistic_sigmoid   
0        obj.layers[3]         3  output_layer       13           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 21)      True          21  
0     (21, 21)      True          21  
0     (21, 13)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 600, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       21  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       21  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 21)      True          21  
0     (21, 21)      True          21  
0      (21, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       21  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       21  logistic_sigmoid   
0        obj.layers[3]         3  output_layer       13           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 21)      True          21  
0     (21, 21)      True          21  
0     (21, 13)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0, 0, 0, 0, 0, 0, 0, 0, 0.4, 0, 0.5, 0.1, 0]
W_T_mean: 4967.027961772471
W_T_median: 3161.3881202316225
W_T_pctile_5: 29.592754121860242
W_T_CVAR_5_pct: -312.1291917887027
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1822.6081158824788
Current xi:  [122.46447]
objective value function right now is: -1822.6081158824788
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1841.3323568070828
Current xi:  [146.35631]
objective value function right now is: -1841.3323568070828
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1864.6930941190694
Current xi:  [170.10396]
objective value function right now is: -1864.6930941190694
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1881.781505568961
Current xi:  [194.06125]
objective value function right now is: -1881.781505568961
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1899.3434836659887
Current xi:  [217.29892]
objective value function right now is: -1899.3434836659887
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1918.269939918906
Current xi:  [240.70102]
objective value function right now is: -1918.269939918906
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1922.8274118854376
Current xi:  [263.3018]
objective value function right now is: -1922.8274118854376
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1944.0417027480278
Current xi:  [286.5392]
objective value function right now is: -1944.0417027480278
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1950.2120009741745
Current xi:  [309.42188]
objective value function right now is: -1950.2120009741745
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1971.2697352381824
Current xi:  [332.09186]
objective value function right now is: -1971.2697352381824
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1983.1814634592386
Current xi:  [355.2048]
objective value function right now is: -1983.1814634592386
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1993.179059578536
Current xi:  [378.08374]
objective value function right now is: -1993.179059578536
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [400.28937]
objective value function right now is: -1987.609681383832
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -2019.4278876174847
Current xi:  [421.73932]
objective value function right now is: -2019.4278876174847
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -2035.9505161309444
Current xi:  [444.44025]
objective value function right now is: -2035.9505161309444
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -2043.8971152929098
Current xi:  [466.86456]
objective value function right now is: -2043.8971152929098
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -2063.1174705351305
Current xi:  [490.20557]
objective value function right now is: -2063.1174705351305
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -2074.2736753593445
Current xi:  [513.7064]
objective value function right now is: -2074.2736753593445
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -2075.3237357023954
Current xi:  [535.34973]
objective value function right now is: -2075.3237357023954
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2091.787179213533
Current xi:  [557.79517]
objective value function right now is: -2091.787179213533
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -2108.0530157115377
Current xi:  [580.67316]
objective value function right now is: -2108.0530157115377
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -2121.1181115752993
Current xi:  [603.2993]
objective value function right now is: -2121.1181115752993
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2129.7002093777005
Current xi:  [625.5274]
objective value function right now is: -2129.7002093777005
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -2142.2226286045657
Current xi:  [647.1043]
objective value function right now is: -2142.2226286045657
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -2152.366730520157
Current xi:  [669.2308]
objective value function right now is: -2152.366730520157
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -2160.737370669634
Current xi:  [690.9038]
objective value function right now is: -2160.737370669634
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -2173.620094345714
Current xi:  [712.7836]
objective value function right now is: -2173.620094345714
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -2180.8194775807224
Current xi:  [734.02026]
objective value function right now is: -2180.8194775807224
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -2191.1124604474335
Current xi:  [755.32477]
objective value function right now is: -2191.1124604474335
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -2194.307511747799
Current xi:  [776.1444]
objective value function right now is: -2194.307511747799
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -2204.7760305523148
Current xi:  [796.7078]
objective value function right now is: -2204.7760305523148
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -2215.2207049030317
Current xi:  [818.3154]
objective value function right now is: -2215.2207049030317
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -2215.730546832725
Current xi:  [838.86536]
objective value function right now is: -2215.730546832725
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -2228.8420455506143
Current xi:  [859.3515]
objective value function right now is: -2228.8420455506143
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -2231.68667940821
Current xi:  [879.30426]
objective value function right now is: -2231.68667940821
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2241.059165295218
Current xi:  [883.4307]
objective value function right now is: -2241.059165295218
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -2243.691023077598
Current xi:  [887.6564]
objective value function right now is: -2243.691023077598
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -2245.3269496247494
Current xi:  [892.23303]
objective value function right now is: -2245.3269496247494
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2247.049589669105
Current xi:  [896.5608]
objective value function right now is: -2247.049589669105
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [900.8568]
objective value function right now is: -2246.5430591438935
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -2248.6763111695836
Current xi:  [905.20197]
objective value function right now is: -2248.6763111695836
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -2251.9551876019227
Current xi:  [909.5684]
objective value function right now is: -2251.9551876019227
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -2252.743316231415
Current xi:  [913.90094]
objective value function right now is: -2252.743316231415
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [918.23145]
objective value function right now is: -2252.5903830314987
90.0% of gradient descent iterations done. Method = Adam
new min fval:  -2253.8776411357353
Current xi:  [922.5539]
objective value function right now is: -2253.8776411357353
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -2256.616259766329
Current xi:  [926.79474]
objective value function right now is: -2256.616259766329
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [931.01935]
objective value function right now is: -2254.7852052479443
96.0% of gradient descent iterations done. Method = Adam
new min fval:  -2257.54605202467
Current xi:  [935.37585]
objective value function right now is: -2257.54605202467
new min fval from sgd:  -2260.5132755355535
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [937.88184]
objective value function right now is: -2260.5132755355535
new min fval from sgd:  -2260.5319381756667
new min fval from sgd:  -2260.5428369389847
new min fval from sgd:  -2260.55931219394
new min fval from sgd:  -2260.5735290924144
new min fval from sgd:  -2260.5782673221975
new min fval from sgd:  -2260.5814883069033
new min fval from sgd:  -2260.615960606177
new min fval from sgd:  -2260.6396174565416
new min fval from sgd:  -2260.663863739033
new min fval from sgd:  -2260.6755247793562
new min fval from sgd:  -2260.6782696407654
new min fval from sgd:  -2260.698570845958
new min fval from sgd:  -2260.7192150232595
new min fval from sgd:  -2260.7383526474323
new min fval from sgd:  -2260.750804701049
new min fval from sgd:  -2260.767893221205
new min fval from sgd:  -2260.7869964247575
new min fval from sgd:  -2260.808457959202
new min fval from sgd:  -2260.8357595827515
new min fval from sgd:  -2260.8558651735493
new min fval from sgd:  -2260.8698197708977
new min fval from sgd:  -2260.8804378357595
new min fval from sgd:  -2260.887876232341
new min fval from sgd:  -2260.898942204873
new min fval from sgd:  -2260.909685435224
new min fval from sgd:  -2260.912754922809
new min fval from sgd:  -2260.9181623134064
new min fval from sgd:  -2260.920084841878
new min fval from sgd:  -2260.9227554175523
new min fval from sgd:  -2260.931692727376
new min fval from sgd:  -2260.9345586018526
new min fval from sgd:  -2260.9416599840197
new min fval from sgd:  -2260.9523359064
new min fval from sgd:  -2260.9630088521512
new min fval from sgd:  -2260.967164064488
new min fval from sgd:  -2260.984030754192
new min fval from sgd:  -2261.0339416397483
new min fval from sgd:  -2261.074135298759
new min fval from sgd:  -2261.103892595907
new min fval from sgd:  -2261.1213563385363
new min fval from sgd:  -2261.1217410542454
new min fval from sgd:  -2261.1253023287422
new min fval from sgd:  -2261.1365233164283
new min fval from sgd:  -2261.146581411884
new min fval from sgd:  -2261.146756001682
new min fval from sgd:  -2261.1482809201457
new min fval from sgd:  -2261.1519182218917
new min fval from sgd:  -2261.151934164562
new min fval from sgd:  -2261.1528514483293
new min fval from sgd:  -2261.161516120484
new min fval from sgd:  -2261.177911576395
new min fval from sgd:  -2261.188362446745
new min fval from sgd:  -2261.1927608184637
new min fval from sgd:  -2261.2066602700625
new min fval from sgd:  -2261.2119713116404
new min fval from sgd:  -2261.213660322079
new min fval from sgd:  -2261.236999564939
new min fval from sgd:  -2261.285791294991
new min fval from sgd:  -2261.319783912621
new min fval from sgd:  -2261.3393694991723
new min fval from sgd:  -2261.3431854632777
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [938.7396]
objective value function right now is: -2261.273336416219
min fval:  -2261.3431854632777
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.1973,  0.6550],
        [ 9.0758, -5.1508],
        [-1.1619,  0.6367],
        [-1.1942,  0.6532],
        [-1.1942,  0.6532],
        [-6.9179,  5.0295],
        [-1.1203,  0.6181],
        [-1.1989,  0.6558],
        [-8.2557,  4.1167],
        [-8.3129,  4.1049],
        [ 7.0346,  4.6954],
        [-1.1940,  0.6531],
        [-4.7795,  3.4938],
        [-9.0604,  5.0197],
        [-1.2120,  0.6628],
        [-5.1211, -8.0284],
        [ 6.8829,  1.7857],
        [ 5.6373,  1.9505],
        [-5.0153,  3.6912],
        [-1.7490,  1.1552],
        [-1.8166,  1.2309]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.6256,  -5.1272,  -3.6187,  -3.6237,  -3.6237,  -1.1724,  -3.6065,
         -3.6262,   3.8808,   3.9090, -10.1034,  -3.6236,  -1.8627,   4.8527,
         -3.6299,  -5.0268,  -8.2110,  -7.6323,  -1.7315,  -3.5853,  -3.5537],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-8.7150e-03, -7.5509e-01, -8.8940e-03, -8.7349e-03, -8.7351e-03,
         -7.2096e-03, -8.9533e-03, -8.7055e-03, -3.3824e-01, -3.4041e-01,
         -1.4069e-01, -8.7360e-03, -6.0768e-03, -4.4337e-01, -8.6316e-03,
         -8.8844e-02, -1.0261e-01, -7.7773e-02, -6.6635e-03, -6.5476e-03,
         -6.4215e-03],
        [-8.7150e-03, -7.5509e-01, -8.8940e-03, -8.7349e-03, -8.7351e-03,
         -7.2095e-03, -8.9533e-03, -8.7055e-03, -3.3824e-01, -3.4041e-01,
         -1.4069e-01, -8.7360e-03, -6.0768e-03, -4.4337e-01, -8.6316e-03,
         -8.8844e-02, -1.0261e-01, -7.7772e-02, -6.6635e-03, -6.5476e-03,
         -6.4215e-03],
        [-8.7150e-03, -7.5509e-01, -8.8940e-03, -8.7350e-03, -8.7351e-03,
         -7.2096e-03, -8.9533e-03, -8.7055e-03, -3.3824e-01, -3.4041e-01,
         -1.4069e-01, -8.7360e-03, -6.0768e-03, -4.4337e-01, -8.6316e-03,
         -8.8844e-02, -1.0261e-01, -7.7773e-02, -6.6635e-03, -6.5476e-03,
         -6.4215e-03],
        [-2.3117e-02,  3.0950e+00, -2.2230e-02, -2.1947e-02, -2.1839e-02,
         -1.1086e+00, -3.5183e-02, -2.3067e-02, -2.5895e+00, -2.9785e+00,
          7.6275e+00, -2.1844e-02, -3.2169e-01, -5.7436e+00, -2.3961e-02,
          1.6941e+01,  2.5810e+00,  1.1763e+00, -3.4018e-01, -9.7320e-02,
         -1.1225e-01],
        [ 9.1955e-02,  1.6634e+00,  9.2621e-02,  9.2329e-02,  9.2341e-02,
         -9.1394e-01,  8.5796e-02,  9.1806e-02, -1.9576e+00, -2.0954e+00,
          6.7990e+00,  9.2355e-02, -2.5206e-01, -4.7527e+00,  9.1121e-02,
          1.3291e+01,  1.6062e+00,  9.7553e-01, -2.5435e-01, -8.0157e-03,
         -2.8223e-02],
        [-2.1482e-02,  2.7387e+00, -2.0356e-02, -2.0517e-02, -2.0448e-02,
         -9.9725e-01, -2.0231e-02, -2.1528e-02, -2.6385e+00, -2.7538e+00,
          7.2327e+00, -2.0437e-02, -2.8926e-01, -5.4112e+00, -2.2479e-02,
          1.6080e+01,  2.1779e+00,  1.1316e+00, -3.0746e-01, -1.0168e-01,
         -1.1840e-01],
        [-9.1397e-02,  1.4954e+01, -8.6820e-02, -9.1425e-02, -9.1430e-02,
         -2.0989e+00, -1.3096e-01, -9.1436e-02, -5.0352e+00, -5.1011e+00,
          5.5295e+00, -9.1431e-02, -9.9624e-01, -5.4270e+00, -9.3089e-02,
          2.8901e+00,  2.4431e+00,  1.4477e+00, -1.0393e+00, -2.9160e-01,
         -3.2594e-01],
        [-8.7150e-03, -7.5509e-01, -8.8940e-03, -8.7349e-03, -8.7351e-03,
         -7.2096e-03, -8.9533e-03, -8.7055e-03, -3.3824e-01, -3.4041e-01,
         -1.4069e-01, -8.7360e-03, -6.0768e-03, -4.4337e-01, -8.6316e-03,
         -8.8844e-02, -1.0261e-01, -7.7772e-02, -6.6635e-03, -6.5476e-03,
         -6.4215e-03],
        [-2.4686e-02,  2.9193e+00, -2.2642e-02, -2.3687e-02, -2.3614e-02,
         -9.4109e-01, -2.3709e-02, -2.4801e-02, -2.6858e+00, -2.7081e+00,
          7.1986e+00, -2.3604e-02, -2.6147e-01, -5.5129e+00, -2.6101e-02,
          1.6425e+01,  2.3831e+00,  1.0726e+00, -2.9916e-01, -1.0449e-01,
         -1.2038e-01],
        [-8.7150e-03, -7.5509e-01, -8.8940e-03, -8.7349e-03, -8.7351e-03,
         -7.2096e-03, -8.9533e-03, -8.7055e-03, -3.3824e-01, -3.4041e-01,
         -1.4069e-01, -8.7360e-03, -6.0768e-03, -4.4337e-01, -8.6316e-03,
         -8.8844e-02, -1.0261e-01, -7.7772e-02, -6.6635e-03, -6.5476e-03,
         -6.4215e-03],
        [-2.8719e-02,  2.2597e+00, -2.4019e-02, -2.7819e-02, -2.7804e-02,
         -7.6874e-01, -2.4641e-03, -2.9053e-02, -2.4060e+00, -2.5029e+00,
          6.5383e+00, -2.7767e-02, -3.4718e-01, -4.8123e+00, -3.1114e-02,
          1.4639e+01,  1.9931e+00,  9.0432e-01, -3.3188e-01, -1.4246e-01,
         -1.6281e-01],
        [-8.7150e-03, -7.5509e-01, -8.8940e-03, -8.7350e-03, -8.7351e-03,
         -7.2095e-03, -8.9533e-03, -8.7055e-03, -3.3824e-01, -3.4041e-01,
         -1.4069e-01, -8.7360e-03, -6.0768e-03, -4.4337e-01, -8.6316e-03,
         -8.8844e-02, -1.0261e-01, -7.7772e-02, -6.6635e-03, -6.5476e-03,
         -6.4215e-03],
        [-8.4705e-03, -7.5508e-01, -8.6240e-03, -8.4870e-03, -8.4871e-03,
         -1.1089e-02, -8.6702e-03, -8.4625e-03, -3.4164e-01, -3.4474e-01,
         -1.6684e-01, -8.4879e-03, -8.5079e-03, -4.5398e-01, -8.3993e-03,
         -9.7252e-02, -1.0879e-01, -8.5116e-02, -9.5050e-03, -6.7092e-03,
         -6.6309e-03],
        [-8.7150e-03, -7.5509e-01, -8.8940e-03, -8.7349e-03, -8.7351e-03,
         -7.2096e-03, -8.9533e-03, -8.7055e-03, -3.3824e-01, -3.4041e-01,
         -1.4069e-01, -8.7360e-03, -6.0768e-03, -4.4337e-01, -8.6316e-03,
         -8.8844e-02, -1.0261e-01, -7.7772e-02, -6.6635e-03, -6.5476e-03,
         -6.4215e-03],
        [-8.7150e-03, -7.5509e-01, -8.8940e-03, -8.7350e-03, -8.7351e-03,
         -7.2095e-03, -8.9533e-03, -8.7055e-03, -3.3824e-01, -3.4041e-01,
         -1.4069e-01, -8.7360e-03, -6.0768e-03, -4.4337e-01, -8.6316e-03,
         -8.8844e-02, -1.0261e-01, -7.7772e-02, -6.6635e-03, -6.5476e-03,
         -6.4215e-03],
        [-4.5345e-02,  3.4858e+00, -4.4425e-02, -4.3866e-02, -4.3944e-02,
         -1.3846e+00, -5.9389e-02, -4.6107e-02, -3.1506e+00, -3.1798e+00,
          8.3218e+00, -4.3798e-02, -3.6930e-01, -5.8055e+00, -4.8273e-02,
          1.7867e+01,  3.0027e+00,  1.6164e+00, -4.6115e-01, -1.2230e-01,
         -1.3750e-01],
        [-8.7150e-03, -7.5509e-01, -8.8940e-03, -8.7350e-03, -8.7351e-03,
         -7.2096e-03, -8.9533e-03, -8.7055e-03, -3.3824e-01, -3.4041e-01,
         -1.4069e-01, -8.7360e-03, -6.0768e-03, -4.4337e-01, -8.6316e-03,
         -8.8844e-02, -1.0261e-01, -7.7772e-02, -6.6635e-03, -6.5476e-03,
         -6.4215e-03],
        [-7.9281e-02, -3.9497e+00, -8.3245e-02, -7.7810e-02, -7.7829e-02,
          1.8700e+00, -1.1029e-01, -7.9897e-02,  3.1603e+00,  3.2615e+00,
         -8.8587e+00, -7.7740e-02,  8.5780e-01,  5.9072e+00, -8.0425e-02,
         -1.8238e+01, -3.4532e+00, -1.9922e+00,  8.4805e-01,  1.3923e-01,
          1.7820e-01],
        [ 1.2778e-02,  1.8304e+00,  1.3634e-02,  1.2876e-02,  1.2877e-02,
          3.9826e-02,  1.5850e-02,  1.2735e-02,  9.8489e-01,  1.0056e+00,
         -2.8762e-01,  1.2882e-02,  1.9328e-02,  1.2535e+00,  1.2427e-02,
          2.4827e-01,  1.8766e-01,  2.6107e-01,  2.7069e-02,  4.6769e-03,
          4.0881e-03],
        [-5.8870e-02,  3.4934e+00, -5.6203e-02, -5.7584e-02, -5.7696e-02,
         -1.4119e+00, -6.9373e-02, -6.0128e-02, -3.2328e+00, -3.2140e+00,
          8.3799e+00, -5.7589e-02, -3.9024e-01, -6.0334e+00, -6.3426e-02,
          1.8093e+01,  3.2114e+00,  1.8044e+00, -5.0679e-01, -1.4120e-01,
         -1.5628e-01],
        [-8.7150e-03, -7.5509e-01, -8.8940e-03, -8.7349e-03, -8.7351e-03,
         -7.2096e-03, -8.9533e-03, -8.7055e-03, -3.3824e-01, -3.4041e-01,
         -1.4069e-01, -8.7360e-03, -6.0768e-03, -4.4337e-01, -8.6316e-03,
         -8.8844e-02, -1.0261e-01, -7.7772e-02, -6.6635e-03, -6.5476e-03,
         -6.4215e-03]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.3919, -1.3919, -1.3919, -1.6063, -1.5564, -1.5174,  4.2926, -1.3919,
        -1.6431, -1.3919, -1.5944, -1.3919, -1.4156, -1.3919, -1.3919, -1.5016,
        -1.3919,  1.5707,  3.3523, -1.3409, -1.3919], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.1287e-02,  1.1287e-02,  1.1287e-02, -1.0015e+01, -5.1195e+00,
         -8.6846e+00, -7.2461e+00,  1.1287e-02, -9.1044e+00,  1.1287e-02,
         -6.5295e+00,  1.1287e-02,  2.1933e-02,  1.1287e-02,  1.1287e-02,
         -1.1905e+01,  1.1287e-02,  2.3084e+01,  9.3647e+00, -1.1830e+01,
          1.1287e-02]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.0658,   0.2455],
        [ -2.0730,   0.2458],
        [-10.7866,   8.2497],
        [ -2.0132,   0.2562],
        [ 12.5961,   4.8389],
        [ -3.1004,   0.3511],
        [-10.7362,   0.1783],
        [  0.3152,   6.1899],
        [ 10.4199,   7.9464],
        [  0.8817,   2.8643],
        [ 12.4559,   3.0136],
        [ -1.8759,   0.2230],
        [-10.1145,   4.8172],
        [ -1.9213,   0.3415],
        [ -2.0727,   0.2458],
        [  2.9716,  11.2609],
        [ -2.0724,   0.2459],
        [-10.0130,   0.3834],
        [  8.1993,  -1.2517],
        [  7.3793,  -8.7591],
        [ 12.5022,   1.7209]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -4.5214,  -4.5136,   4.5821,  -4.6190,   3.0286,  -3.8988,   9.0670,
         -3.3704,   5.0671, -13.2709,  -2.1030,  -4.5802,   0.6486,  -4.7940,
         -4.5138,   7.1711,  -4.5145,   9.0394, -10.8016,  -6.7994,  -6.2823],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.5197e-03,  2.5368e-03, -3.4079e-01,  7.6766e-04, -1.4739e+00,
          5.8857e-03, -8.9759e-01, -4.7520e-01, -7.5122e-01,  4.1299e-01,
         -1.2769e+00,  2.4180e-03,  3.2017e-01, -1.0070e-03,  2.5450e-03,
         -5.7715e-01,  2.5467e-03, -9.2739e-01, -4.8447e-01, -1.2847e+00,
         -1.1162e+00],
        [-7.1967e-01, -7.1217e-01,  7.0923e+00, -8.0719e-01, -2.9487e+00,
         -1.8398e+00,  4.5007e+00, -9.8063e-01, -4.3293e+00, -7.9817e+00,
          6.7081e-01, -3.8675e-01,  7.0097e+00, -6.0153e-01, -7.1153e-01,
         -7.4429e+00, -7.1404e-01,  3.3972e+00, -6.4068e+00,  1.6394e+00,
         -1.3959e+00],
        [ 1.6290e-01,  1.6161e-01,  6.1195e+00,  1.5663e-01, -2.8039e+00,
          8.9123e-01,  1.3796e+00, -7.9703e-01,  1.4319e+00, -4.1947e+00,
         -5.6300e+00,  4.8929e-02,  2.9411e+00,  3.9985e-02,  1.6194e-01,
          5.4146e+00,  1.6223e-01,  1.6156e+00, -1.5174e-01, -8.4392e+00,
         -9.6821e+00],
        [ 1.5824e-01,  1.5632e-01,  3.5823e+00,  3.7828e-01, -8.7751e-01,
          2.2189e-01,  4.2141e+00,  5.7021e-01,  2.6298e+00,  2.1693e+00,
         -2.7779e+00,  1.0715e-01,  7.4866e-01,  7.9925e-01,  1.5476e-01,
          4.8550e+00,  1.5474e-01,  2.5062e+00, -2.7046e+00, -7.4890e+00,
         -8.2876e+00],
        [ 2.7380e-02,  2.7338e-02, -5.2020e-01,  3.6797e-02, -2.9368e+00,
         -1.5469e-02, -8.4394e-01, -1.4604e-02, -3.6130e+00, -2.2223e-03,
         -3.8135e+00,  2.7743e-02, -1.3054e-01,  3.1698e-02,  2.7239e-02,
         -1.7341e+00,  2.7192e-02, -9.5946e-01,  3.5156e+00,  8.5038e-01,
         -1.4427e+00],
        [-7.3273e-02, -6.9365e-02, -2.2215e+00, -1.9871e-02,  1.9645e-01,
          2.9637e-02, -3.1413e+00, -2.6905e+00,  1.2404e+00,  2.2422e+00,
          3.0484e-01, -6.4627e-02,  1.9043e+00, -8.5189e-02, -6.9914e-02,
         -2.7726e-01, -7.0510e-02, -1.9965e+00,  4.9393e-01, -4.1072e-01,
          7.9946e-01],
        [ 1.6207e-02,  1.6251e-02,  6.6815e-01,  1.5795e-02, -1.6854e+00,
          2.1052e-02, -7.6017e-01,  4.7419e-01, -7.3487e-01, -8.4643e-01,
         -1.4995e+00,  1.6316e-02, -6.6094e-01,  1.6016e-02,  1.6251e-02,
         -5.4138e-01,  1.6245e-02, -8.9032e-01, -5.0063e-01, -1.1662e+00,
         -1.1943e+00],
        [ 5.5260e-03,  5.5732e-03, -1.4361e-01,  4.9695e-03, -1.4460e+00,
          9.0147e-03, -7.1433e-01, -3.1278e-01, -8.4672e-01, -1.1814e-01,
         -1.3743e+00,  5.2301e-03, -1.3682e-01,  4.6790e-03,  5.5725e-03,
         -6.2207e-01,  5.5677e-03, -8.5487e-01, -4.7720e-01, -1.2075e+00,
         -1.1263e+00],
        [ 9.0341e-02,  9.0423e-02, -3.4837e-01,  9.0513e-02, -3.7367e-03,
          6.0250e-02, -4.0739e+00, -8.2550e-01, -6.6571e-01,  1.0322e+00,
          2.3144e-01,  9.5458e-02,  4.6537e-01,  7.3634e-02,  9.0377e-02,
         -3.6437e+00,  9.0337e-02, -2.7041e+00,  4.9121e-01, -8.1578e-01,
          8.7443e-01],
        [ 2.7662e-03,  2.7720e-03, -1.2451e-01,  2.2624e-03, -2.4335e+00,
          1.7727e-03, -1.1762e+00,  2.7043e-02, -1.1926e+00, -1.3559e-03,
         -2.5426e+00,  3.1530e-03, -2.3895e-02,  1.1406e-03,  2.7741e-03,
         -7.3592e-01,  2.7731e-03, -1.4718e+00,  7.7188e-01, -3.9077e-01,
         -1.7104e+00],
        [ 3.5982e-03,  3.6326e-03, -1.4733e-01,  3.1863e-03, -1.4275e+00,
          6.5856e-03, -7.3351e-01, -2.7166e-01, -8.4346e-01, -1.1974e-01,
         -1.3048e+00,  3.4911e-03, -1.3995e-01,  3.3685e-03,  3.6327e-03,
         -5.6448e-01,  3.6288e-03, -8.4406e-01, -4.3386e-01, -1.1572e+00,
         -1.0899e+00],
        [ 4.3455e-02,  4.0107e-02,  1.5453e+00, -3.8369e-02, -1.2842e+00,
         -1.0981e-01, -5.3278e-01,  1.4167e+00, -1.3272e-01, -1.7637e+00,
         -9.2749e-01,  6.0997e-02, -1.2303e+00, -1.9058e-01,  4.1083e-02,
          1.9368e-01,  4.1752e-02, -5.9934e-01, -4.8917e-01, -2.0192e+00,
         -5.0282e-01],
        [-1.6884e-03, -1.6745e-03, -7.2011e-02, -2.0923e-03, -2.2389e+00,
         -1.4147e-03, -1.0642e+00,  3.1805e-03, -9.2823e-01, -3.7956e-02,
         -2.1418e+00, -1.6692e-03, -6.0053e-02, -2.5986e-03, -1.6734e-03,
         -5.9590e-01, -1.6748e-03, -1.2836e+00,  3.5117e-01, -6.5317e-01,
         -1.6044e+00],
        [ 7.8548e-03,  7.8965e-03, -3.2022e-01,  6.0509e-03, -1.4189e+00,
          1.3312e-02, -7.3378e-01, -4.6150e-01, -7.1281e-01,  2.9583e-01,
         -1.4581e+00,  7.6174e-03,  2.2637e-01,  4.9974e-03,  7.9033e-03,
         -5.7548e-01,  7.9022e-03, -8.3557e-01, -4.5840e-01, -1.2399e+00,
         -1.0618e+00],
        [ 5.9861e-03,  6.0346e-03, -1.2990e-01,  5.4112e-03, -1.4755e+00,
          9.5113e-03, -6.9719e-01, -3.0017e-01, -8.3469e-01, -1.1946e-01,
         -1.3506e+00,  5.6752e-03, -1.3933e-01,  5.0662e-03,  6.0339e-03,
         -6.4324e-01,  6.0290e-03, -8.3938e-01, -4.9179e-01, -1.1857e+00,
         -1.1400e+00],
        [ 1.9322e-01,  1.9359e-01, -9.0927e+00,  1.8435e-01, -5.9012e+00,
          5.6777e-01,  7.1991e+00, -4.5841e-03, -7.1483e+00,  5.5199e-03,
         -3.8005e+00,  9.3470e-02, -2.1910e+00,  1.3361e-01,  1.9351e-01,
         -1.5723e+01,  1.9363e-01,  4.1151e+00, -2.5814e+00,  3.3997e+00,
         -7.8664e+00],
        [ 8.8175e-03,  8.8683e-03, -5.6973e-02,  8.2364e-03, -1.5135e+00,
          1.2432e-02, -6.5117e-01, -2.4889e-01, -8.1778e-01, -1.5582e-01,
         -1.3870e+00,  8.5073e-03, -1.6998e-01,  7.7617e-03,  8.8672e-03,
         -6.7522e-01,  8.8621e-03, -8.2840e-01, -5.1341e-01, -1.1254e+00,
         -1.1832e+00],
        [ 5.3607e-03,  5.4074e-03, -1.4074e-01,  4.8082e-03, -1.4543e+00,
          8.8390e-03, -7.0686e-01, -3.0828e-01, -8.4121e-01, -1.1740e-01,
         -1.3494e+00,  5.0670e-03, -1.3688e-01,  4.5424e-03,  5.4067e-03,
         -6.2594e-01,  5.4020e-03, -8.4622e-01, -4.7431e-01, -1.1984e+00,
         -1.1245e+00],
        [-4.0456e-02, -3.8918e-02, -5.9581e+00, -4.2126e-02, -9.8677e-01,
         -1.2220e-01, -3.4668e+00, -3.2895e-02, -3.0055e+00,  2.9044e+00,
          3.0268e+00, -2.6318e-02, -2.4788e+00,  1.4761e-02, -3.8965e-02,
         -2.7012e+00, -3.9306e-02, -2.2282e+00,  2.5216e+00,  5.9016e+00,
          8.2480e+00],
        [ 6.0438e-03,  6.0928e-03, -1.2934e-01,  5.4619e-03, -1.4656e+00,
          9.6104e-03, -6.9764e-01, -2.9782e-01, -8.3721e-01, -1.2019e-01,
         -1.3490e+00,  5.7316e-03, -1.4004e-01,  5.1105e-03,  6.0921e-03,
         -6.4197e-01,  6.0872e-03, -8.3574e-01, -4.9188e-01, -1.1819e+00,
         -1.1414e+00],
        [ 1.3904e-02,  1.3937e-02,  5.5586e-01,  1.3475e-02, -1.6691e+00,
          1.6161e-02, -7.2646e-01,  4.1210e-01, -8.1425e-01, -6.5166e-01,
         -1.4079e+00,  1.3975e-02, -4.5256e-01,  1.3385e-02,  1.3937e-02,
         -5.0812e-01,  1.3933e-02, -9.4615e-01, -5.1965e-01, -1.1446e+00,
         -1.1343e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.1244,  0.6462, -2.6217, -5.1968, -0.9452, -2.7422, -1.8704, -1.8980,
        -3.0547, -1.7355, -1.9992, -1.2495, -1.8593, -2.1985, -1.9292, -0.7709,
        -1.9742, -1.9303,  4.7435, -1.9383, -2.0161], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-6.8146e-03, -4.8892e+00, -2.0659e-01, -8.8817e+00, -2.9404e-06,
         -1.0194e-01, -1.6138e-02, -9.8229e-03, -3.3245e-03, -9.9964e-04,
         -1.0626e-02, -1.5334e+00, -2.2972e-03, -7.6138e-03, -9.6149e-03,
         -7.2311e-04, -8.9758e-03, -9.8304e-03, -1.2191e+01, -9.6507e-03,
         -1.3751e-02],
        [-1.8126e-02, -4.9273e+00,  2.2954e+00, -3.3565e+00, -9.3241e-03,
          4.8320e-02, -1.9778e-02, -2.2003e-02,  6.8830e-03, -2.5545e-03,
         -2.3000e-02,  1.9280e-01, -1.1444e-02, -1.8912e-02, -2.1723e-02,
         -2.7113e-03, -2.0321e-02, -2.2035e-02, -6.7615e+00, -2.1721e-02,
         -1.7201e-02],
        [-4.9631e-03, -4.3541e+00, -1.8066e-01, -8.2003e+00, -6.6817e-04,
         -1.2716e-01, -1.2925e-02, -7.4499e-03, -2.0219e-03, -1.8668e-03,
         -7.9943e-03, -1.3289e+00, -3.9128e-03, -5.9625e-03, -7.2157e-03,
         -4.0600e-03, -7.4280e-03, -7.4354e-03, -1.1294e+01, -7.2433e-03,
         -1.0494e-02],
        [-5.2512e-03, -3.9369e+00, -1.6715e-01, -7.4168e+00, -1.6857e-04,
         -1.2300e-01, -1.0359e-02, -8.0048e-03, -5.5067e-04, -1.3937e-03,
         -8.3592e-03, -8.5659e-01, -2.3983e-03, -6.2244e-03, -7.9529e-03,
         -6.1623e-03, -7.4685e-03, -7.9970e-03, -1.0375e+01, -7.9882e-03,
         -8.7688e-03],
        [-2.6072e-01,  1.5323e+00, -1.0731e+00,  1.7351e+00,  2.7265e+00,
         -1.5120e+00, -3.8332e-01, -1.2714e-01, -2.8256e-01,  5.5578e-01,
          2.7160e-01, -1.6986e-01,  1.4284e-01, -3.2366e-01, -1.6108e-01,
          1.4456e+01, -1.7386e-01, -1.3489e-01,  1.7848e+00, -1.2805e-01,
         -2.7001e-01],
        [-4.4032e-03, -4.0886e+00, -1.4444e-01, -6.7560e+00, -5.2636e-04,
         -1.5300e-01, -1.7231e-02, -6.8978e-03, -7.6682e-03, -1.0683e-03,
         -7.3417e-03, -4.5127e-01, -1.2988e-03, -4.4296e-03, -7.4658e-03,
         -2.3011e-03, -9.0180e-03, -6.9517e-03, -9.1172e+00, -7.5491e-03,
         -1.3316e-02],
        [-4.7787e-03, -3.8446e+00, -1.4056e-01, -7.3078e+00, -1.2521e-03,
         -1.6461e-01, -7.9619e-03, -7.7689e-03, -3.4241e-03, -1.5880e-03,
         -7.8686e-03, -6.3526e-01, -3.8508e-03, -5.9119e-03, -7.8703e-03,
         -4.9771e-03, -7.5065e-03, -7.7480e-03, -1.0317e+01, -7.9141e-03,
         -6.6476e-03],
        [-2.1924e-01,  1.5549e+00,  4.0909e-01,  1.1864e+00, -8.1779e-01,
         -1.1788e+00,  6.2395e-01,  1.5738e-01, -2.2762e+00,  9.5300e-01,
          4.4680e-01,  1.5738e+00,  6.5135e-01, -9.8732e-02,  1.5315e-01,
          7.5195e+00,  2.6377e-01,  1.4904e-01,  7.1822e-01,  1.8280e-01,
          5.4919e-01],
        [-5.8970e-03, -4.1111e+00, -1.7689e-01, -7.9063e+00,  1.0645e-04,
         -1.3699e-01, -1.3504e-02, -8.5948e-03, -3.7812e-03, -8.1640e-04,
         -9.2578e-03, -1.1668e+00, -2.4840e-03, -6.8388e-03, -8.3530e-03,
         -3.0979e-03, -8.1458e-03, -8.5933e-03, -1.0907e+01, -8.3836e-03,
         -1.1383e-02],
        [ 3.1377e-01,  1.8153e+00,  3.2940e+00,  2.7057e+00,  5.3315e-02,
         -1.3075e+00,  2.7102e-01,  2.4563e-01, -1.7535e-02,  2.9578e-01,
          5.1330e-01,  1.1266e-01,  3.5156e-01,  3.1484e-01,  2.2624e-01,
         -3.1269e+01,  2.9088e-01,  2.3378e-01, -9.3742e+00,  2.5164e-01,
          2.7232e-01],
        [ 4.2778e-01,  4.7382e-01, -4.2929e+00,  1.8420e+00, -2.5186e+00,
          2.1603e+00, -7.2549e-02,  1.2507e-01,  2.0606e+00, -1.0994e+00,
          4.8611e-01, -7.1046e-01, -5.9808e-01,  3.7151e-01,  8.7524e-02,
         -2.3504e+01,  1.0303e-01,  1.1184e-01,  1.2092e+00,  1.1959e-01,
         -2.4492e-02],
        [-7.2343e-03, -4.6266e+00, -2.0403e-01, -8.7294e+00, -8.3723e-04,
         -1.1260e-01, -1.6437e-02, -9.6393e-03, -2.6073e-03, -1.7486e-03,
         -1.0734e-02, -1.6911e+00, -3.0499e-03, -8.3253e-03, -8.7701e-03,
         -3.9470e-03, -7.6934e-03, -9.6137e-03, -1.1931e+01, -8.7681e-03,
         -1.3480e-02],
        [-7.2124e-03, -4.7403e+00, -2.0880e-01, -8.7355e+00,  5.6875e-04,
         -1.0567e-01, -1.5233e-02, -1.0314e-02, -3.5389e-03, -1.2183e-03,
         -1.1141e-02, -1.4409e+00, -2.4811e-03, -8.1031e-03, -1.0038e-02,
         -5.5692e-04, -9.1381e-03, -1.0318e-02, -1.1940e+01, -1.0070e-02,
         -1.2938e-02]], device='cuda:0'))])
xi:  [938.5679]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1287.3701369601754
W_T_median: 1209.5175660126033
W_T_pctile_5: 944.4535777228432
W_T_CVAR_5_pct: 614.8819130785665
Average q (qsum/M+1):  53.13179262222782
Optimal xi:  [938.5679]
Expected(across Rb) median(across samples) p_equity:  0.000338340446357203
obj fun:  tensor(-2261.3432, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: 7_Factor_plusEWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
