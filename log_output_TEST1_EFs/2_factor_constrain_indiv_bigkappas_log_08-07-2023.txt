/home/marcchen/Documents/constrain_factor/researchcode/exp_config_json_files/multi_portfolio_TEST1_EFs.json
Starting at: 
07-08-23_22:45

 numpy seed:  3  


 pytorch seed:  3  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.005383     0.031411
192608                    0.0319              0.0561  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0355  ...     0.005323    -0.028996
192611                   -0.0038              0.0294  ...     0.005303     0.028554

[5 rows x 6 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.043289    -0.036240
202209                   -0.0955             -0.0871  ...    -0.050056    -0.091324
202210                    0.0883              0.1486  ...    -0.014968     0.077403
202211                   -0.0076              0.0462  ...     0.040789     0.052365
202212                   -0.0457             -0.0499  ...    -0.018566    -0.057116

[5 rows x 6 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'CPI_nom_ret_ind', 'T30_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Value_Hi30_real_ret
T30_real_ret             1.000000  ...             0.018239
B10_real_ret             0.351722  ...             0.031301
VWD_real_ret             0.068448  ...             0.909335
Size_Lo30_real_ret       0.014412  ...             0.908542
Value_Hi30_real_ret      0.018239  ...             1.000000

[5 rows x 5 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       5       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       5              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 5)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 1500, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -2.7095,  12.9892],
        [ -8.6338,   9.4281],
        [ -0.9451,   0.4527],
        [ -0.8640,   0.3781],
        [  6.1895,  -0.6523],
        [  4.5754,  -0.4303],
        [ -0.9502,   0.4519],
        [ -0.9882,   0.4554],
        [  5.3775,  -0.2578],
        [  9.7529,  -0.6215],
        [-74.6407,   0.8966],
        [  5.4828,  -0.5841],
        [ -2.7219,  16.4087]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 7.2031,  5.6080, -3.1690, -3.3192,  2.5904,  3.5518, -3.1733, -3.1998,
         3.1859, -7.8767,  0.2375,  2.9803,  8.6844], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1333e-01,  1.6554e+00,  3.4528e-02,  3.0769e-02,  8.7976e-01,
          9.1042e-01,  3.5304e-02,  3.4772e-02,  8.9665e-01,  2.6447e-01,
          4.3032e-01,  8.9469e-01,  4.6686e-01],
        [-9.2749e+00, -6.0010e+00,  1.6064e-01,  2.4939e-01,  2.6538e+00,
          2.9059e+00,  1.6532e-01,  2.3862e-01,  2.6770e+00,  2.0542e+01,
          5.6439e+00,  2.9504e+00, -1.0311e+01],
        [-5.8631e+00, -5.5932e+00, -1.0882e-01, -2.5602e-01,  1.4267e+00,
          1.6964e+00, -8.6213e-02, -1.9126e-01,  1.4872e+00,  2.0896e+01,
          5.2545e+00,  1.4174e+00, -9.1947e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4955e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.0882e-01,  1.6256e+00,  3.4094e-02,  3.0485e-02,  8.6370e-01,
          8.9398e-01,  3.4826e-02,  3.4369e-02,  8.8039e-01,  2.5781e-01,
          4.1380e-01,  8.7846e-01,  4.6006e-01],
        [-6.2435e+00, -5.0774e+00, -7.9019e-02, -1.1667e-02,  1.6100e+00,
          1.6134e+00, -1.1318e-01, -9.1158e-02,  1.1232e+00,  2.1826e+01,
          5.7213e+00,  1.3367e+00, -9.6095e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4956e-03, -5.9415e-03, -4.7716e-01,
         -4.9046e-01, -6.4683e-03, -6.3263e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1923e-01,  1.6977e+00,  3.5087e-02,  3.1098e-02,  9.0167e-01,
          9.3283e-01,  3.5931e-02,  3.5276e-02,  9.1883e-01,  2.7424e-01,
          4.5388e-01,  9.1684e-01,  4.7618e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5021, -0.5021,  0.9316,  2.2107,  1.1816, -0.5021, -0.5021,  0.9149,
         1.2202, -0.5021, -0.5021, -0.5021,  0.9544], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0192,   0.0192,   3.5844,  -9.2131, -12.7066,   0.0192,   0.0192,
           3.1214, -12.3800,   0.0192,   0.0192,   0.0192,   4.3492]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  2.3638,  12.1836],
        [  5.6572,   7.4721],
        [  7.9224,   6.5803],
        [ -6.1235,   3.1910],
        [ 11.8466,   0.5913],
        [  0.8770,  -8.1352],
        [  8.6891,  -6.8360],
        [-10.5691,  10.8756],
        [  1.7568,  -0.0712],
        [-11.0991,  -4.2491],
        [ 11.4103,   5.2357],
        [ -0.1851,   2.9347],
        [ -7.5723,  -9.3505]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 6.3522, -5.1507, -2.0487, -4.3376, -9.8811, -6.7882, -6.3489,  5.9660,
        -7.1033, -1.4560, -0.7179, -3.6786, -6.0576], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.1852e+00,  1.5094e+00,  1.3526e+00, -5.4268e-01,  2.4223e+00,
          2.0759e+00,  9.2465e-01, -1.6151e+00, -2.8824e-01,  9.5287e-01,
          2.4642e-01, -1.1737e+00,  2.2713e+00],
        [-1.8651e+00, -5.6256e-01, -6.2129e-01, -4.2742e+00, -7.7059e-01,
         -3.4892e+00, -7.6918e-02,  6.7408e+00, -2.3096e-01,  1.3500e+00,
         -3.5500e+00, -6.0667e-01, -2.9353e+00],
        [ 1.1142e-01, -1.2473e+00,  8.1916e-01,  6.4910e-01, -1.6350e+01,
          2.2732e+00, -1.5779e+00,  7.2357e+00, -7.2982e-03,  4.4049e+00,
         -5.8977e-01,  1.7865e+00, -4.6946e+00],
        [-1.3960e+00, -8.7673e-02, -4.2780e-01,  6.7083e-03, -4.0893e-01,
         -9.1685e-01, -1.7927e+00, -5.0702e-01, -1.3463e-02, -5.2559e-01,
         -1.6941e+00,  1.8378e-03, -3.4962e-01],
        [-1.9520e+00,  1.8967e-01, -5.4411e-01, -1.2101e-01, -9.1077e-01,
         -1.2479e+00, -1.1267e+00, -9.1391e-01, -1.8068e-02, -3.8365e-01,
         -2.4970e+00, -1.1678e-01, -2.1655e-01],
        [-1.4057e+00, -8.6477e-02, -4.3293e-01,  1.5699e-02, -4.1127e-01,
         -9.1630e-01, -1.7894e+00, -5.0243e-01, -1.3300e-02, -5.1448e-01,
         -1.7010e+00,  1.2009e-02, -3.4096e-01],
        [ 1.3221e+01, -6.1845e-01,  1.5793e+00, -6.7508e+00, -1.5120e+01,
         -1.8714e+01, -8.9013e+00,  4.8410e+00, -1.2180e+00,  4.2963e+00,
          1.1099e+00,  1.4776e+00, -1.3629e+01],
        [ 4.1482e+00, -4.3534e-01, -7.3632e+00,  2.5918e-02, -1.7989e-01,
         -6.4858e+00, -8.9001e-01, -1.0932e+00,  1.0207e-01,  1.5231e+00,
         -7.6390e+00,  1.3168e-01, -1.0356e+01],
        [-1.4524e+00, -8.1858e-02, -4.5182e-01,  9.7058e-02, -4.0160e-01,
         -8.8229e-01, -1.8020e+00, -4.5301e-01, -1.2095e-02, -4.6125e-01,
         -1.7020e+00,  9.5630e-02, -3.0579e-01],
        [-2.8522e+00, -1.9497e-01, -1.6674e+00, -4.5101e-01, -1.9345e+00,
         -2.7298e+00,  5.5568e-01,  1.5671e+00, -1.2880e-02,  9.4602e-01,
         -2.4953e+00, -2.9629e-01, -2.6501e+00],
        [-2.4334e+01, -6.7212e-02, -4.8855e+00,  1.9330e-01, -7.7025e+00,
          2.7805e+00,  4.4028e-01, -1.4406e+00, -4.7447e-02,  6.3915e+00,
         -8.7817e+00, -4.8862e-01,  5.4980e+00],
        [-1.0961e+01,  5.4093e-01,  8.6739e-01, -6.0412e-02, -3.7470e+00,
          2.1004e+00,  2.4607e+00, -2.1072e+01, -1.4633e-01,  9.1990e+00,
         -5.3680e+00,  1.5515e-01,  4.1397e+00],
        [-1.2571e+01,  2.5529e-02, -4.2095e+00,  2.0473e-01, -1.3435e+01,
          2.3688e+00, -6.0068e-01, -5.7139e+00, -3.7530e-01,  4.5355e+00,
         -9.0221e+00,  3.5582e-02,  2.3988e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 2.9079, -2.1828, -0.1738, -3.3916, -2.9699, -3.3962, -3.0437, -0.0264,
        -3.4166, -2.8315, -0.3684,  1.2132,  0.0742], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 7.4891e-01,  4.5452e+00,  1.8769e+00,  4.5914e-02,  3.5496e-01,
         -9.1354e-02,  2.7526e-01, -9.7636e-01, -1.0948e-01,  3.3561e+00,
         -4.4478e+00, -1.2550e-01, -1.0160e+00],
        [ 5.4131e-01, -4.5533e+00, -3.9331e-01,  2.2764e-01, -3.5322e-01,
          9.4308e-02,  8.6633e-01,  7.9627e-01,  1.1223e-01, -3.0675e+00,
          5.4099e+00,  6.6530e-01,  1.5354e+00],
        [-1.0256e+01, -4.0894e-02, -1.1152e+00, -1.1287e-02, -1.0215e-02,
         -1.1185e-02, -8.9347e-01, -1.4354e-01, -1.1272e-02, -1.5607e-02,
          8.3038e-03, -6.6567e-01,  1.3450e-03],
        [-1.2086e+00, -1.8795e+00, -3.9948e+00,  3.7431e-01,  1.3870e+00,
          3.8828e-01,  9.5588e-01, -1.2326e+01,  4.2495e-01,  2.2745e+00,
          8.1236e-01,  1.8802e+00,  1.0604e+01],
        [ 9.1296e+00, -4.1802e+00, -1.1657e+01,  2.0360e-01,  4.0986e-01,
          2.0817e-01,  5.1155e+00, -6.9774e-01,  2.1607e-01, -5.1843e-01,
          6.2533e+00,  8.3596e+00,  2.6055e+00]], device='cuda:0'))])
loaded xi:  315.55313
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2274.0301783412797
Current xi:  [318.71152]
objective value function right now is: -2274.0301783412797
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -2277.9189534412794
Current xi:  [322.19766]
objective value function right now is: -2277.9189534412794
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [325.82742]
objective value function right now is: -2272.8258378501137
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [327.6419]
objective value function right now is: -2275.9460630909643
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -2281.7880382635503
Current xi:  [329.33572]
objective value function right now is: -2281.7880382635503
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [331.00388]
objective value function right now is: -2269.7975540184334
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [330.89703]
objective value function right now is: -2274.7242563402842
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [331.7955]
objective value function right now is: -2277.4184749803785
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [332.66592]
objective value function right now is: -2276.164710959764
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [332.34488]
objective value function right now is: -2259.9022303726247
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.13638]
objective value function right now is: -2270.0117262462945
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.37717]
objective value function right now is: -2278.8715874773293
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.5993]
objective value function right now is: -2272.9696614906557
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [333.92648]
objective value function right now is: -2279.639991182323
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.70355]
objective value function right now is: -2276.0075351303703
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.09546]
objective value function right now is: -2274.385611344739
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.5154]
objective value function right now is: -2271.531672968677
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.5753]
objective value function right now is: -2278.5485764562886
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.25735]
objective value function right now is: -2273.606834629256
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2284.0638968443
Current xi:  [332.84702]
objective value function right now is: -2284.0638968443
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [332.93613]
objective value function right now is: -2283.933873754901
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [332.66287]
objective value function right now is: -2277.7910261421325
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.58173]
objective value function right now is: -2275.3536183981887
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.1526]
objective value function right now is: -2282.979515658601
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -2284.297258145675
Current xi:  [333.5543]
objective value function right now is: -2284.297258145675
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.42688]
objective value function right now is: -2281.514628088486
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.12112]
objective value function right now is: -2281.721266644971
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [334.46317]
objective value function right now is: -2283.5075884691087
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [334.5391]
objective value function right now is: -2276.5519698996795
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.88043]
objective value function right now is: -2280.4337301755
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.8271]
objective value function right now is: -2274.7909010573917
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.66098]
objective value function right now is: -2278.1411404392243
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.39294]
objective value function right now is: -2283.1021734849005
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.6161]
objective value function right now is: -2274.2184694974385
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.2113]
objective value function right now is: -2280.349488571263
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2288.1261399820414
Current xi:  [334.07306]
objective value function right now is: -2288.1261399820414
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -2288.5222684741625
Current xi:  [333.96274]
objective value function right now is: -2288.5222684741625
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.99927]
objective value function right now is: -2288.2247247422383
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.20724]
objective value function right now is: -2286.5022589123287
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.1841]
objective value function right now is: -2288.3399622853476
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -2289.5037771608067
Current xi:  [334.22552]
objective value function right now is: -2289.5037771608067
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.0115]
objective value function right now is: -2288.931314196363
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.971]
objective value function right now is: -2289.419856322827
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.11365]
objective value function right now is: -2288.4488476012925
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.02444]
objective value function right now is: -2288.4653674002843
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -2289.552919313515
Current xi:  [334.21274]
objective value function right now is: -2289.552919313515
94.0% of gradient descent iterations done. Method = Adam
new min fval:  -2289.697967945946
Current xi:  [334.60065]
objective value function right now is: -2289.697967945946
new min fval from sgd:  -2289.7383268930726
new min fval from sgd:  -2289.845050989101
new min fval from sgd:  -2289.9282142848997
new min fval from sgd:  -2289.9843648943506
new min fval from sgd:  -2290.035672504914
new min fval from sgd:  -2290.079479365061
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.53232]
objective value function right now is: -2287.395822810047
new min fval from sgd:  -2290.087120632991
new min fval from sgd:  -2290.1091465504
new min fval from sgd:  -2290.1120146080375
new min fval from sgd:  -2290.1182086901813
new min fval from sgd:  -2290.1261115098478
new min fval from sgd:  -2290.131178733424
new min fval from sgd:  -2290.138740808353
new min fval from sgd:  -2290.144051721484
new min fval from sgd:  -2290.1459050811
new min fval from sgd:  -2290.147329166552
new min fval from sgd:  -2290.1476274412266
new min fval from sgd:  -2290.1492153541362
new min fval from sgd:  -2290.15511150687
new min fval from sgd:  -2290.158414015737
new min fval from sgd:  -2290.162849910807
new min fval from sgd:  -2290.167720971443
new min fval from sgd:  -2290.1752483948826
new min fval from sgd:  -2290.184145181878
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.43723]
objective value function right now is: -2289.976096104989
new min fval from sgd:  -2290.209236880322
new min fval from sgd:  -2290.2456883044583
new min fval from sgd:  -2290.2730519829115
new min fval from sgd:  -2290.2956297288165
new min fval from sgd:  -2290.314127247548
new min fval from sgd:  -2290.3291631274756
new min fval from sgd:  -2290.3411071265227
new min fval from sgd:  -2290.351686244444
new min fval from sgd:  -2290.3556601289893
new min fval from sgd:  -2290.358711765034
new min fval from sgd:  -2290.3602617461524
new min fval from sgd:  -2290.3651014125903
new min fval from sgd:  -2290.365547011118
new min fval from sgd:  -2290.3708409203036
new min fval from sgd:  -2290.3735900968713
new min fval from sgd:  -2290.376712417941
new min fval from sgd:  -2290.3780248608796
new min fval from sgd:  -2290.382836809998
new min fval from sgd:  -2290.3893057138175
new min fval from sgd:  -2290.396454180246
new min fval from sgd:  -2290.403043076289
new min fval from sgd:  -2290.4077361699447
new min fval from sgd:  -2290.4117462223016
new min fval from sgd:  -2290.4141786032383
new min fval from sgd:  -2290.422044649285
new min fval from sgd:  -2290.4443813931
new min fval from sgd:  -2290.4497153566535
new min fval from sgd:  -2290.452142118574
new min fval from sgd:  -2290.457166172703
new min fval from sgd:  -2290.463526592606
new min fval from sgd:  -2290.469574883207
new min fval from sgd:  -2290.4763563924894
new min fval from sgd:  -2290.479923419369
new min fval from sgd:  -2290.482598390487
new min fval from sgd:  -2290.487319372853
new min fval from sgd:  -2290.4885527936613
new min fval from sgd:  -2290.489369411123
new min fval from sgd:  -2290.492845475386
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.44376]
objective value function right now is: -2290.169819438683
min fval:  -2290.492845475386
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -4.3081,  16.3076],
        [-12.0976,  12.3699],
        [ -0.6551,   0.4772],
        [ -0.6551,   0.4772],
        [  1.4976,  -0.3099],
        [  1.4658,  -0.2981],
        [ -0.6551,   0.4772],
        [ -0.6551,   0.4772],
        [  1.4568,  -0.3050],
        [ 15.0298,  -1.1735],
        [-76.4092,  -1.5620],
        [  1.4645,  -0.3138],
        [ -4.8275,  19.6390]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  8.3468,   7.4470,  -3.0584,  -3.0584,   5.0359,   5.1430,  -3.0584,
         -3.0584,   5.0394, -11.3106,  -0.6100,   5.0999,  10.1041],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.7812e-01, -2.7570e-01,  9.0761e-03,  9.0761e-03, -4.9961e-01,
         -4.9993e-01,  9.0761e-03,  9.0761e-03, -4.9961e-01, -3.0796e-02,
         -1.6366e-01, -4.9982e-01, -1.8337e-01],
        [-1.7812e-01, -2.7570e-01,  9.0762e-03,  9.0762e-03, -4.9961e-01,
         -4.9993e-01,  9.0762e-03,  9.0762e-03, -4.9960e-01, -3.0797e-02,
         -1.6366e-01, -4.9982e-01, -1.8337e-01],
        [ 4.2490e-01,  1.2333e+00,  3.3945e-02,  3.3945e-02,  1.0968e+00,
          1.0972e+00,  3.3945e-02,  3.3945e-02,  1.0969e+00,  5.8719e-02,
          5.6895e-01,  1.0971e+00,  4.5449e-01],
        [-1.0583e+01, -9.9914e+00,  2.2093e-02,  2.2093e-02,  3.3942e+00,
          3.6883e+00,  2.2093e-02,  2.2093e-02,  3.4415e+00,  2.6580e+01,
          8.3211e+00,  3.7095e+00, -1.4899e+01],
        [-7.5588e+00, -8.2052e+00,  2.0113e-02,  2.0112e-02,  1.7455e+00,
          2.0447e+00,  2.0113e-02,  2.0112e-02,  1.8260e+00,  2.6370e+01,
          7.6703e+00,  1.7596e+00, -1.4130e+01],
        [-1.7812e-01, -2.7570e-01,  9.0761e-03,  9.0761e-03, -4.9961e-01,
         -4.9993e-01,  9.0761e-03,  9.0761e-03, -4.9960e-01, -3.0797e-02,
         -1.6366e-01, -4.9982e-01, -1.8337e-01],
        [-1.7812e-01, -2.7570e-01,  9.0761e-03,  9.0761e-03, -4.9961e-01,
         -4.9993e-01,  9.0761e-03,  9.0761e-03, -4.9960e-01, -3.0796e-02,
         -1.6366e-01, -4.9982e-01, -1.8337e-01],
        [ 4.1804e-01,  1.2149e+00,  3.3717e-02,  3.3717e-02,  1.0800e+00,
          1.0804e+00,  3.3717e-02,  3.3717e-02,  1.0800e+00,  5.5365e-02,
          5.5689e-01,  1.0803e+00,  4.4660e-01],
        [-7.9064e+00, -8.4044e+00, -1.4086e-02, -1.4085e-02,  2.0614e+00,
          2.1048e+00, -1.4086e-02, -1.4085e-02,  1.6193e+00,  2.6963e+01,
          7.6779e+00,  1.8225e+00, -1.4362e+01],
        [-1.7812e-01, -2.7570e-01,  9.0761e-03,  9.0761e-03, -4.9961e-01,
         -4.9993e-01,  9.0761e-03,  9.0761e-03, -4.9961e-01, -3.0796e-02,
         -1.6366e-01, -4.9982e-01, -1.8337e-01],
        [-1.7812e-01, -2.7570e-01,  9.0761e-03,  9.0761e-03, -4.9961e-01,
         -4.9993e-01,  9.0761e-03,  9.0761e-03, -4.9961e-01, -3.0796e-02,
         -1.6366e-01, -4.9982e-01, -1.8337e-01],
        [-1.7811e-01, -2.7568e-01,  9.0781e-03,  9.0781e-03, -4.9953e-01,
         -4.9986e-01,  9.0781e-03,  9.0781e-03, -4.9953e-01, -3.0802e-02,
         -1.6365e-01, -4.9975e-01, -1.8335e-01],
        [ 4.3451e-01,  1.2598e+00,  3.4266e-02,  3.4266e-02,  1.1205e+00,
          1.1209e+00,  3.4266e-02,  3.4266e-02,  1.1205e+00,  6.3945e-02,
          5.8643e-01,  1.1208e+00,  4.6565e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5030, -0.5030,  1.1005,  3.0624,  1.6099, -0.5030, -0.5030,  1.0837,
         1.7827, -0.5030, -0.5030, -0.5029,  1.1242], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0249,   0.0249,   3.8912, -11.4222, -12.5004,   0.0249,   0.0249,
           3.4392, -13.0598,   0.0249,   0.0249,   0.0249,   4.6370]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  1.3331,  13.7067],
        [  5.7929,  16.3699],
        [  7.6464,  13.9628],
        [ -1.7501,   0.3424],
        [ 15.2595,   0.5055],
        [ -1.7421,   0.3596],
        [  9.2892, -10.9051],
        [-14.9466,  12.6694],
        [ -3.1174,   0.9304],
        [-11.4693,  -7.3247],
        [ 17.8114,   7.4654],
        [ -4.3621,   9.3785],
        [-11.5666, -13.7911]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.6618,  -7.1785,   2.2917,  -4.7687, -13.5019,  -4.7654,  -9.0185,
          7.8486,  -5.7473,  -4.8740,  -1.7413,  -3.8965,  -7.8849],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.8697e+00,  4.7584e+00,  1.8709e+00,  1.7092e-01, -2.2513e-01,
          2.0430e-01,  1.0305e+01, -4.0848e+00, -6.4064e-01, -3.3608e+00,
          7.0354e-01, -1.2948e+00,  4.3282e+00],
        [-2.6682e+00, -2.8541e+00, -1.8665e+00,  1.6917e-02, -6.1753e+00,
          3.7623e-02, -4.3045e+00,  9.9511e+00, -2.2988e-01,  5.2709e+00,
         -1.0118e+00, -1.3680e+00, -6.0335e+00],
        [-3.3479e-01, -6.0086e+00, -1.4234e+00, -2.2557e-01, -2.2960e+01,
         -2.1026e-01, -1.5326e+00,  1.1538e+01, -2.9883e-02,  3.2248e+00,
         -1.7790e+00,  6.8557e+00, -1.5156e+01],
        [-6.1487e-01,  3.0013e+00,  2.1382e+00, -1.7927e-02,  3.8293e-01,
         -2.5213e-02,  1.5661e-01, -1.1482e+00, -1.3282e+00,  1.3030e+00,
         -2.5394e+00, -1.2191e+00,  1.0867e+00],
        [-1.0528e-01,  3.5894e-01,  1.3135e+00,  1.4266e-01, -9.6725e-01,
          1.3830e-01, -1.2157e+00, -1.6861e+00,  5.7429e-03,  4.7973e-02,
         -2.3895e+00, -1.7837e-01,  4.3259e-01],
        [-5.5304e-01,  2.5264e+00,  1.8992e+00,  3.5774e-01, -4.3556e-01,
          3.5673e-01,  8.2809e-01, -1.3666e+00, -4.4855e-01,  9.7081e-01,
         -3.5285e+00, -9.3624e-01,  1.1402e+00],
        [ 1.1654e+01, -3.0809e+00,  4.1188e-01, -2.0982e-01, -1.9445e+01,
         -1.9198e-01, -9.5057e+00,  9.5211e+00, -3.3425e-01,  3.0796e+00,
          2.6576e+00,  2.4260e+00, -1.3857e+01],
        [ 7.5441e+00,  9.7829e-02, -1.1725e+01, -1.6888e-01, -5.8574e+00,
         -1.5904e-01, -2.0643e+00, -1.4585e+00, -6.7261e-02, -5.7342e-01,
         -4.5582e-01,  1.6433e-02, -1.5743e+01],
        [-6.2362e-02,  7.8631e-01,  1.3671e+00,  2.1353e-01, -9.6232e-01,
          2.0801e-01, -6.7665e-01, -1.5469e+00, -4.6641e-04,  7.1468e-02,
         -2.9378e+00, -9.9083e-02,  3.2616e-01],
        [-1.1499e+00,  9.4009e-01,  1.2041e+00,  2.5385e-04,  1.8239e-01,
          5.4956e-04, -2.2085e+00, -1.6809e+00,  1.4034e-02,  1.1370e-02,
         -2.1094e+00, -9.8403e-01,  1.7398e-01],
        [-2.7277e+01,  1.4012e-04, -1.1567e+00, -1.3179e-01, -1.0720e+01,
         -1.3066e-01,  1.4000e+00,  6.5228e+00, -5.2777e-02,  7.5863e+00,
         -8.0256e+00,  1.1077e-02,  9.4831e+00],
        [-9.1731e+00,  1.9764e-01,  4.0194e+00, -2.5190e-01, -1.7811e+00,
         -2.5021e-01,  2.1984e+00, -1.1201e+01, -2.1792e-01,  1.0720e+01,
         -3.7466e+00,  7.9528e-03,  7.8504e+00],
        [-1.5914e+01, -7.0953e-05, -1.4676e+00, -2.2151e-01, -1.4227e+01,
         -2.0249e-01,  2.9947e-01,  5.0448e+00, -2.9670e-02,  5.1463e+00,
         -1.0477e+01, -1.4103e-02,  7.0243e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 5.7857, -3.5978,  2.2978, -3.6800, -3.5972, -3.6046, -7.6964, -4.0804,
        -3.5208, -4.2376, -0.9280,  1.8440, -1.8468], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 8.5810e-01,  5.6485e+00,  1.8086e+00,  6.5768e-01,  1.3428e+00,
          1.7441e+00,  2.0514e-01, -7.9791e-01,  1.5968e+00,  6.7531e-01,
         -3.5858e+00, -2.0052e-01, -4.0601e+00],
        [ 4.5122e-01, -5.5961e+00, -3.3054e-01, -6.5583e-01, -1.3412e+00,
         -1.7429e+00,  9.2967e-01,  6.5667e-01, -1.5952e+00, -6.7375e-01,
          4.4186e+00,  7.4145e-01,  4.3374e+00],
        [-1.0601e+01, -1.5918e-02, -9.6650e-01, -1.0478e-01, -2.5756e-02,
         -4.5603e-02, -4.2946e-01, -1.5617e-01, -2.8464e-02, -1.9070e-02,
         -1.3525e-02, -1.1583e+00,  5.9362e-05],
        [-1.7084e+00, -9.2421e+00, -3.5784e+00,  3.7740e+00,  5.9374e-01,
          2.8346e+00,  1.1043e+00, -1.8354e+01,  9.9691e-01,  1.2092e+00,
          1.2301e+00,  1.9704e+00,  9.9932e+00],
        [ 9.4367e+00, -1.8141e+00, -1.0957e+01,  4.3270e-01, -5.1879e-01,
         -1.1450e-01,  3.4657e+00,  1.3973e+00, -5.7536e-01,  1.0132e-01,
          2.4434e+00,  8.2855e+00,  1.6475e+00]], device='cuda:0'))])
xi:  [334.44583]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 418.5016122418132
W_T_median: 382.7247941059147
W_T_pctile_5: 334.46008937081143
W_T_CVAR_5_pct: 167.67057135805646
Average q (qsum/M+1):  46.84321840347782
Optimal xi:  [334.44583]
Expected(across Rb) median(across samples) p_equity:  0.19349679594076102
obj fun:  tensor(-2290.4928, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -2.7095,  12.9892],
        [ -8.6338,   9.4281],
        [ -0.9451,   0.4527],
        [ -0.8640,   0.3781],
        [  6.1895,  -0.6523],
        [  4.5754,  -0.4303],
        [ -0.9502,   0.4519],
        [ -0.9882,   0.4554],
        [  5.3775,  -0.2578],
        [  9.7529,  -0.6215],
        [-74.6407,   0.8966],
        [  5.4828,  -0.5841],
        [ -2.7219,  16.4087]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 7.2031,  5.6080, -3.1690, -3.3192,  2.5904,  3.5518, -3.1733, -3.1998,
         3.1859, -7.8767,  0.2375,  2.9803,  8.6844], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1333e-01,  1.6554e+00,  3.4528e-02,  3.0769e-02,  8.7976e-01,
          9.1042e-01,  3.5304e-02,  3.4772e-02,  8.9665e-01,  2.6447e-01,
          4.3032e-01,  8.9469e-01,  4.6686e-01],
        [-9.2749e+00, -6.0010e+00,  1.6064e-01,  2.4939e-01,  2.6538e+00,
          2.9059e+00,  1.6532e-01,  2.3862e-01,  2.6770e+00,  2.0542e+01,
          5.6439e+00,  2.9504e+00, -1.0311e+01],
        [-5.8631e+00, -5.5932e+00, -1.0882e-01, -2.5602e-01,  1.4267e+00,
          1.6964e+00, -8.6213e-02, -1.9126e-01,  1.4872e+00,  2.0896e+01,
          5.2545e+00,  1.4174e+00, -9.1947e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4955e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.0882e-01,  1.6256e+00,  3.4094e-02,  3.0485e-02,  8.6370e-01,
          8.9398e-01,  3.4826e-02,  3.4369e-02,  8.8039e-01,  2.5781e-01,
          4.1380e-01,  8.7846e-01,  4.6006e-01],
        [-6.2435e+00, -5.0774e+00, -7.9019e-02, -1.1667e-02,  1.6100e+00,
          1.6134e+00, -1.1318e-01, -9.1158e-02,  1.1232e+00,  2.1826e+01,
          5.7213e+00,  1.3367e+00, -9.6095e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4956e-03, -5.9415e-03, -4.7716e-01,
         -4.9046e-01, -6.4683e-03, -6.3263e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1923e-01,  1.6977e+00,  3.5087e-02,  3.1098e-02,  9.0167e-01,
          9.3283e-01,  3.5931e-02,  3.5276e-02,  9.1883e-01,  2.7424e-01,
          4.5388e-01,  9.1684e-01,  4.7618e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5021, -0.5021,  0.9316,  2.2107,  1.1816, -0.5021, -0.5021,  0.9149,
         1.2202, -0.5021, -0.5021, -0.5021,  0.9544], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0192,   0.0192,   3.5844,  -9.2131, -12.7066,   0.0192,   0.0192,
           3.1214, -12.3800,   0.0192,   0.0192,   0.0192,   4.3492]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  2.3638,  12.1836],
        [  5.6572,   7.4721],
        [  7.9224,   6.5803],
        [ -6.1235,   3.1910],
        [ 11.8466,   0.5913],
        [  0.8770,  -8.1352],
        [  8.6891,  -6.8360],
        [-10.5691,  10.8756],
        [  1.7568,  -0.0712],
        [-11.0991,  -4.2491],
        [ 11.4103,   5.2357],
        [ -0.1851,   2.9347],
        [ -7.5723,  -9.3505]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 6.3522, -5.1507, -2.0487, -4.3376, -9.8811, -6.7882, -6.3489,  5.9660,
        -7.1033, -1.4560, -0.7179, -3.6786, -6.0576], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.1852e+00,  1.5094e+00,  1.3526e+00, -5.4268e-01,  2.4223e+00,
          2.0759e+00,  9.2465e-01, -1.6151e+00, -2.8824e-01,  9.5287e-01,
          2.4642e-01, -1.1737e+00,  2.2713e+00],
        [-1.8651e+00, -5.6256e-01, -6.2129e-01, -4.2742e+00, -7.7059e-01,
         -3.4892e+00, -7.6918e-02,  6.7408e+00, -2.3096e-01,  1.3500e+00,
         -3.5500e+00, -6.0667e-01, -2.9353e+00],
        [ 1.1142e-01, -1.2473e+00,  8.1916e-01,  6.4910e-01, -1.6350e+01,
          2.2732e+00, -1.5779e+00,  7.2357e+00, -7.2982e-03,  4.4049e+00,
         -5.8977e-01,  1.7865e+00, -4.6946e+00],
        [-1.3960e+00, -8.7673e-02, -4.2780e-01,  6.7083e-03, -4.0893e-01,
         -9.1685e-01, -1.7927e+00, -5.0702e-01, -1.3463e-02, -5.2559e-01,
         -1.6941e+00,  1.8378e-03, -3.4962e-01],
        [-1.9520e+00,  1.8967e-01, -5.4411e-01, -1.2101e-01, -9.1077e-01,
         -1.2479e+00, -1.1267e+00, -9.1391e-01, -1.8068e-02, -3.8365e-01,
         -2.4970e+00, -1.1678e-01, -2.1655e-01],
        [-1.4057e+00, -8.6477e-02, -4.3293e-01,  1.5699e-02, -4.1127e-01,
         -9.1630e-01, -1.7894e+00, -5.0243e-01, -1.3300e-02, -5.1448e-01,
         -1.7010e+00,  1.2009e-02, -3.4096e-01],
        [ 1.3221e+01, -6.1845e-01,  1.5793e+00, -6.7508e+00, -1.5120e+01,
         -1.8714e+01, -8.9013e+00,  4.8410e+00, -1.2180e+00,  4.2963e+00,
          1.1099e+00,  1.4776e+00, -1.3629e+01],
        [ 4.1482e+00, -4.3534e-01, -7.3632e+00,  2.5918e-02, -1.7989e-01,
         -6.4858e+00, -8.9001e-01, -1.0932e+00,  1.0207e-01,  1.5231e+00,
         -7.6390e+00,  1.3168e-01, -1.0356e+01],
        [-1.4524e+00, -8.1858e-02, -4.5182e-01,  9.7058e-02, -4.0160e-01,
         -8.8229e-01, -1.8020e+00, -4.5301e-01, -1.2095e-02, -4.6125e-01,
         -1.7020e+00,  9.5630e-02, -3.0579e-01],
        [-2.8522e+00, -1.9497e-01, -1.6674e+00, -4.5101e-01, -1.9345e+00,
         -2.7298e+00,  5.5568e-01,  1.5671e+00, -1.2880e-02,  9.4602e-01,
         -2.4953e+00, -2.9629e-01, -2.6501e+00],
        [-2.4334e+01, -6.7212e-02, -4.8855e+00,  1.9330e-01, -7.7025e+00,
          2.7805e+00,  4.4028e-01, -1.4406e+00, -4.7447e-02,  6.3915e+00,
         -8.7817e+00, -4.8862e-01,  5.4980e+00],
        [-1.0961e+01,  5.4093e-01,  8.6739e-01, -6.0412e-02, -3.7470e+00,
          2.1004e+00,  2.4607e+00, -2.1072e+01, -1.4633e-01,  9.1990e+00,
         -5.3680e+00,  1.5515e-01,  4.1397e+00],
        [-1.2571e+01,  2.5529e-02, -4.2095e+00,  2.0473e-01, -1.3435e+01,
          2.3688e+00, -6.0068e-01, -5.7139e+00, -3.7530e-01,  4.5355e+00,
         -9.0221e+00,  3.5582e-02,  2.3988e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 2.9079, -2.1828, -0.1738, -3.3916, -2.9699, -3.3962, -3.0437, -0.0264,
        -3.4166, -2.8315, -0.3684,  1.2132,  0.0742], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 7.4891e-01,  4.5452e+00,  1.8769e+00,  4.5914e-02,  3.5496e-01,
         -9.1354e-02,  2.7526e-01, -9.7636e-01, -1.0948e-01,  3.3561e+00,
         -4.4478e+00, -1.2550e-01, -1.0160e+00],
        [ 5.4131e-01, -4.5533e+00, -3.9331e-01,  2.2764e-01, -3.5322e-01,
          9.4308e-02,  8.6633e-01,  7.9627e-01,  1.1223e-01, -3.0675e+00,
          5.4099e+00,  6.6530e-01,  1.5354e+00],
        [-1.0256e+01, -4.0894e-02, -1.1152e+00, -1.1287e-02, -1.0215e-02,
         -1.1185e-02, -8.9347e-01, -1.4354e-01, -1.1272e-02, -1.5607e-02,
          8.3038e-03, -6.6567e-01,  1.3450e-03],
        [-1.2086e+00, -1.8795e+00, -3.9948e+00,  3.7431e-01,  1.3870e+00,
          3.8828e-01,  9.5588e-01, -1.2326e+01,  4.2495e-01,  2.2745e+00,
          8.1236e-01,  1.8802e+00,  1.0604e+01],
        [ 9.1296e+00, -4.1802e+00, -1.1657e+01,  2.0360e-01,  4.0986e-01,
          2.0817e-01,  5.1155e+00, -6.9774e-01,  2.1607e-01, -5.1843e-01,
          6.2533e+00,  8.3596e+00,  2.6055e+00]], device='cuda:0'))])
loaded xi:  315.55313
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -3122.7997902664715
Current xi:  [322.96973]
objective value function right now is: -3122.7997902664715
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [330.14026]
objective value function right now is: -3094.24177527958
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [335.9174]
objective value function right now is: -3114.9430465632686
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [339.1002]
objective value function right now is: -3109.915338974971
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -3130.42244483862
Current xi:  [341.4037]
objective value function right now is: -3130.42244483862
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [343.6869]
objective value function right now is: -3125.6421026835983
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [344.74948]
objective value function right now is: -3102.1858863643397
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -3133.850110555717
Current xi:  [346.09164]
objective value function right now is: -3133.850110555717
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.70108]
objective value function right now is: -3119.73024851812
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -3134.4064284222213
Current xi:  [346.17288]
objective value function right now is: -3134.4064284222213
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.5281]
objective value function right now is: -3102.7757044431414
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.6768]
objective value function right now is: -3127.3417561700185
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -3134.794104220516
Current xi:  [344.96298]
objective value function right now is: -3134.794104220516
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [344.54895]
objective value function right now is: -3121.1235688870174
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.3161]
objective value function right now is: -3116.8037012081472
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [344.86624]
objective value function right now is: -3097.309719496133
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -3138.415905585396
Current xi:  [344.59988]
objective value function right now is: -3138.415905585396
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [344.84445]
objective value function right now is: -3126.55595564394
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [344.84177]
objective value function right now is: -3124.2840485836577
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.06073]
objective value function right now is: -3134.115660605109
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [347.21524]
objective value function right now is: -3131.6511231663053
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [346.14932]
objective value function right now is: -3124.178173439897
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.09256]
objective value function right now is: -3136.536524533772
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.23495]
objective value function right now is: -3137.5440259317893
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [344.3348]
objective value function right now is: -3133.703419686487
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.32358]
objective value function right now is: -3137.294022318745
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.7391]
objective value function right now is: -3132.9936127728024
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [345.7099]
objective value function right now is: -3126.106658061986
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [346.5232]
objective value function right now is: -3133.386986455755
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -3139.036909457733
Current xi:  [347.80573]
objective value function right now is: -3139.036909457733
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [346.53375]
objective value function right now is: -3134.685976437498
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [346.05447]
objective value function right now is: -3130.283111433246
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -3140.1185119323254
Current xi:  [345.30862]
objective value function right now is: -3140.1185119323254
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.39508]
objective value function right now is: -3115.8078922510203
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.24844]
objective value function right now is: -3129.5481133128683
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -3143.978167677705
Current xi:  [345.45914]
objective value function right now is: -3143.978167677705
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -3147.7825514573806
Current xi:  [345.88553]
objective value function right now is: -3147.7825514573806
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.78232]
objective value function right now is: -3147.3974953804695
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -3148.276358193664
Current xi:  [346.04385]
objective value function right now is: -3148.276358193664
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.80176]
objective value function right now is: -3146.8714304794344
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.8694]
objective value function right now is: -3147.2694846401623
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [346.07687]
objective value function right now is: -3145.5156592687817
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.589]
objective value function right now is: -3145.542478674995
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.56458]
objective value function right now is: -3147.762829578128
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.56512]
objective value function right now is: -3148.080963874695
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -3148.4844089933076
Current xi:  [345.3993]
objective value function right now is: -3148.4844089933076
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.3214]
objective value function right now is: -3147.6096210959618
new min fval from sgd:  -3148.502280094211
new min fval from sgd:  -3148.6609491365925
new min fval from sgd:  -3148.732198024437
new min fval from sgd:  -3148.778981394073
new min fval from sgd:  -3148.8934989669146
new min fval from sgd:  -3148.9838859583538
new min fval from sgd:  -3149.010564689224
new min fval from sgd:  -3149.159671206206
new min fval from sgd:  -3149.3347369211037
new min fval from sgd:  -3149.495323730873
new min fval from sgd:  -3149.586435186015
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.40524]
objective value function right now is: -3148.225410477015
new min fval from sgd:  -3149.611235519808
new min fval from sgd:  -3149.6423024961837
new min fval from sgd:  -3149.6724193960263
new min fval from sgd:  -3149.7173103964096
new min fval from sgd:  -3149.7552692155446
new min fval from sgd:  -3149.7921000461106
new min fval from sgd:  -3149.83205600106
new min fval from sgd:  -3149.8847917306275
new min fval from sgd:  -3149.925117048244
new min fval from sgd:  -3149.956627580904
new min fval from sgd:  -3149.980096895436
new min fval from sgd:  -3149.9957588245243
new min fval from sgd:  -3150.0096058288896
new min fval from sgd:  -3150.0243590733635
new min fval from sgd:  -3150.045155132206
new min fval from sgd:  -3150.0596463912475
new min fval from sgd:  -3150.078555197836
new min fval from sgd:  -3150.0964872273253
new min fval from sgd:  -3150.112610855185
new min fval from sgd:  -3150.1188852726223
new min fval from sgd:  -3150.126693375931
new min fval from sgd:  -3150.131923117109
new min fval from sgd:  -3150.1426002741673
new min fval from sgd:  -3150.151775157015
new min fval from sgd:  -3150.1545507227147
new min fval from sgd:  -3150.164408596129
new min fval from sgd:  -3150.166802866556
new min fval from sgd:  -3150.1720174715174
new min fval from sgd:  -3150.1812948899333
new min fval from sgd:  -3150.1888594631114
new min fval from sgd:  -3150.2005416388215
new min fval from sgd:  -3150.2100958766364
new min fval from sgd:  -3150.2274652338856
new min fval from sgd:  -3150.241408094331
new min fval from sgd:  -3150.2518936824013
new min fval from sgd:  -3150.2672399495004
new min fval from sgd:  -3150.2789963164446
new min fval from sgd:  -3150.290177504634
new min fval from sgd:  -3150.305332869051
new min fval from sgd:  -3150.3216450076056
new min fval from sgd:  -3150.338876246107
new min fval from sgd:  -3150.3603786880326
new min fval from sgd:  -3150.381021638282
new min fval from sgd:  -3150.399410761852
new min fval from sgd:  -3150.4152290507423
new min fval from sgd:  -3150.427552401228
new min fval from sgd:  -3150.4401740724697
new min fval from sgd:  -3150.448660725795
new min fval from sgd:  -3150.4549345204505
new min fval from sgd:  -3150.465587082183
new min fval from sgd:  -3150.47301059318
new min fval from sgd:  -3150.4742880024733
new min fval from sgd:  -3150.4754158657806
new min fval from sgd:  -3150.4763684897753
new min fval from sgd:  -3150.4830315570375
new min fval from sgd:  -3150.4847539440307
new min fval from sgd:  -3150.486510094902
new min fval from sgd:  -3150.488707426055
new min fval from sgd:  -3150.495087028474
new min fval from sgd:  -3150.511186905751
new min fval from sgd:  -3150.519240423893
new min fval from sgd:  -3150.5310231512995
new min fval from sgd:  -3150.5394050498458
new min fval from sgd:  -3150.5507642143443
new min fval from sgd:  -3150.5637575500086
new min fval from sgd:  -3150.576994051177
new min fval from sgd:  -3150.5846689269542
new min fval from sgd:  -3150.5941894721855
new min fval from sgd:  -3150.606384147512
new min fval from sgd:  -3150.620296803046
new min fval from sgd:  -3150.6302346458615
new min fval from sgd:  -3150.6455302596924
new min fval from sgd:  -3150.656611719454
new min fval from sgd:  -3150.669878686859
new min fval from sgd:  -3150.6804958801586
new min fval from sgd:  -3150.690356499333
new min fval from sgd:  -3150.698254950648
new min fval from sgd:  -3150.7064307117753
new min fval from sgd:  -3150.715296112501
new min fval from sgd:  -3150.7216587221515
new min fval from sgd:  -3150.728436349218
new min fval from sgd:  -3150.733443347468
new min fval from sgd:  -3150.7353723106826
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.3436]
objective value function right now is: -3150.2156248568876
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.3725]
objective value function right now is: -3150.445503520275
min fval:  -3150.7353723106826
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -5.9123,  16.6383],
        [-11.7143,  11.4535],
        [ -0.8727,   0.7280],
        [ -0.8727,   0.7280],
        [  1.3804,  -0.4714],
        [  1.3734,  -0.4661],
        [ -0.8727,   0.7280],
        [ -0.8727,   0.7280],
        [  1.3514,  -0.4735],
        [ 14.7532,  -1.4614],
        [-81.2479,  -1.7180],
        [  1.3611,  -0.4916],
        [ -7.1478,  19.8039]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  8.4769,   6.5685,  -3.8094,  -3.8094,   5.2978,   5.4074,  -3.8094,
         -3.8094,   5.3042, -10.5720,  -0.3963,   5.3478,  10.1535],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.0986e-01, -4.4749e-01,  8.9676e-04,  8.9676e-04, -8.2300e-01,
         -8.2328e-01,  8.9676e-04,  8.9676e-04, -8.2302e-01, -5.2856e-02,
         -2.4854e-01, -8.2316e-01, -4.1818e-01],
        [-4.0986e-01, -4.4749e-01,  8.9676e-04,  8.9676e-04, -8.2300e-01,
         -8.2328e-01,  8.9676e-04,  8.9676e-04, -8.2302e-01, -5.2856e-02,
         -2.4854e-01, -8.2316e-01, -4.1818e-01],
        [ 5.4634e-01,  1.0484e+00,  1.3423e-02,  1.3423e-02,  1.2489e+00,
          1.2492e+00,  1.3423e-02,  1.3423e-02,  1.2489e+00,  5.9258e-02,
          4.8742e-01,  1.2490e+00,  5.8852e-01],
        [-1.1455e+01, -7.9769e+00,  6.6020e-02,  6.6020e-02,  3.3376e+00,
          3.6655e+00,  6.6020e-02,  6.6020e-02,  3.4137e+00,  2.7847e+01,
          7.7721e+00,  3.6507e+00, -1.6080e+01],
        [-8.4185e+00, -5.8061e+00,  8.2704e-02,  8.2703e-02,  1.5767e+00,
          1.9310e+00,  8.2704e-02,  8.2704e-02,  1.7073e+00,  2.7184e+01,
          7.1691e+00,  1.5687e+00, -1.4876e+01],
        [-4.0986e-01, -4.4749e-01,  8.9675e-04,  8.9675e-04, -8.2300e-01,
         -8.2329e-01,  8.9675e-04,  8.9675e-04, -8.2302e-01, -5.2856e-02,
         -2.4854e-01, -8.2316e-01, -4.1818e-01],
        [-4.0986e-01, -4.4749e-01,  8.9675e-04,  8.9675e-04, -8.2300e-01,
         -8.2329e-01,  8.9675e-04,  8.9675e-04, -8.2302e-01, -5.2856e-02,
         -2.4854e-01, -8.2316e-01, -4.1818e-01],
        [ 5.3822e-01,  1.0346e+00,  1.3318e-02,  1.3318e-02,  1.2291e+00,
          1.2294e+00,  1.3318e-02,  1.3318e-02,  1.2291e+00,  5.8129e-02,
          4.7987e-01,  1.2292e+00,  5.7979e-01],
        [-8.7377e+00, -5.8816e+00, -3.3670e-02, -3.3669e-02,  1.8814e+00,
          1.9855e+00, -3.3670e-02, -3.3670e-02,  1.4902e+00,  2.7818e+01,
          7.1252e+00,  1.6206e+00, -1.5156e+01],
        [-4.0986e-01, -4.4749e-01,  8.9675e-04,  8.9675e-04, -8.2300e-01,
         -8.2329e-01,  8.9675e-04,  8.9675e-04, -8.2302e-01, -5.2856e-02,
         -2.4854e-01, -8.2316e-01, -4.1818e-01],
        [-4.0986e-01, -4.4749e-01,  8.9675e-04,  8.9675e-04, -8.2300e-01,
         -8.2329e-01,  8.9675e-04,  8.9675e-04, -8.2302e-01, -5.2856e-02,
         -2.4854e-01, -8.2316e-01, -4.1818e-01],
        [-4.0986e-01, -4.4749e-01,  8.9676e-04,  8.9676e-04, -8.2300e-01,
         -8.2329e-01,  8.9676e-04,  8.9676e-04, -8.2302e-01, -5.2856e-02,
         -2.4854e-01, -8.2316e-01, -4.1818e-01],
        [ 5.5761e-01,  1.0679e+00,  1.3606e-02,  1.3606e-02,  1.2766e+00,
          1.2769e+00,  1.3606e-02,  1.3606e-02,  1.2766e+00,  6.0862e-02,
          4.9814e-01,  1.2768e+00,  6.0068e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8258, -0.8258,  1.2516,  3.0525,  1.5295, -0.8258, -0.8258,  1.2318,
         1.7021, -0.8258, -0.8258, -0.8258,  1.2794], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0149,  -0.0149,   3.7454, -11.0345, -12.9318,  -0.0149,  -0.0149,
           3.2910, -13.4092,  -0.0149,  -0.0149,  -0.0149,   4.4953]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 1.7705e-02,  1.3863e+01],
        [ 8.2729e+00,  1.9008e+01],
        [ 9.4414e+00,  1.8406e+01],
        [-2.1159e+00,  3.2424e+00],
        [ 1.4908e+01,  3.5943e-01],
        [-1.9879e+00,  7.2265e-01],
        [ 9.5522e+00, -5.1274e+00],
        [-1.1277e+01,  1.4368e+01],
        [-1.6309e+00,  3.5285e+00],
        [-1.4020e+01, -8.6845e+00],
        [ 1.5833e+01,  6.9033e+00],
        [-1.2748e+00,  3.6543e+00],
        [-1.0670e+01, -1.3453e+01]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.8971,  -3.2772,   1.5923,  -5.0586, -13.6936,  -5.4537, -11.3648,
          8.3420,  -4.5574,  -4.7164,  -1.4242,  -4.6680,  -7.6732],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.3271e+00,  5.5556e+00,  3.6491e-01, -9.4425e-01,  6.5527e-01,
         -1.3218e-01, -5.1007e-01, -8.7396e+00, -1.1692e+00, -1.2925e+00,
          2.6332e+00, -1.1848e+00,  5.1252e+00],
        [-1.7995e+00, -4.1490e+00, -6.2814e-02, -2.4720e+00,  2.2021e+00,
         -2.8700e-01, -4.4984e+00,  6.7760e+00, -2.3940e+00,  4.9265e+00,
         -2.5974e+00, -2.4588e+00, -5.7768e+00],
        [ 4.3617e+00, -3.1329e+00, -1.2038e+01, -1.3119e-01, -3.2162e+01,
          1.2852e-01,  6.4663e-01,  1.4254e+01,  5.7892e-01,  2.3876e+00,
         -8.7064e-01,  7.4379e-01, -1.6777e+01],
        [-1.2461e+00,  5.1225e-02,  9.0212e-02, -7.0498e-03, -1.5390e-01,
          9.3264e-03, -7.1248e-01, -2.3349e-01, -9.7453e-03, -3.7873e-01,
         -2.4017e+00, -1.0513e-02, -7.0476e-01],
        [-1.5619e+00,  2.7015e-01,  4.5356e-01, -2.4981e-01, -5.8629e-01,
         -1.5931e-02, -9.9489e-01, -6.6069e-01, -2.4642e-01, -8.5223e-01,
         -2.4059e+00, -2.4355e-01, -9.0622e-01],
        [-1.2042e+00,  6.8382e-02,  8.0102e-02, -3.0226e-02, -1.4124e-01,
          9.9110e-03, -6.7560e-01, -1.9435e-01, -3.2614e-02, -3.3836e-01,
         -2.4446e+00, -3.3187e-02, -6.9339e-01],
        [ 8.5348e+00, -5.2558e+00, -1.0822e+01, -7.7808e-01, -4.0797e+00,
         -1.8879e-01, -6.6603e+00,  1.0722e+01, -1.0428e+00,  5.0026e+00,
          3.9414e+00, -1.2235e+00, -1.2613e+01],
        [ 4.5267e+00,  3.5738e+00, -1.1066e+00, -3.5331e-01, -5.4827e+00,
         -4.8951e-01, -1.6835e+00, -2.4872e+01, -6.2046e-01, -1.1326e+01,
         -2.0399e+00, -7.1454e-01, -3.0923e+01],
        [-1.2464e+00,  5.1718e-02,  9.1335e-02, -6.9584e-03, -1.5437e-01,
          9.3095e-03, -7.1301e-01, -2.3387e-01, -9.6467e-03, -3.7937e-01,
         -2.4010e+00, -1.0412e-02, -7.0495e-01],
        [-9.7170e-02,  1.4712e+00,  3.3437e+00, -1.7817e+00,  1.5981e-02,
         -1.2656e-01, -1.2547e+00, -1.2414e+00, -1.3612e+00, -2.6202e+00,
         -6.6273e+00, -1.2898e+00, -4.7387e+00],
        [-2.4988e+01,  6.6279e-03,  5.1077e-03,  1.0078e-01, -1.2754e+01,
         -7.4812e-02, -1.7359e-01,  3.3293e+00,  4.0817e-02,  1.0554e+01,
         -7.3583e+00,  2.9592e-02,  1.0172e+01],
        [-7.0225e+00, -2.3751e+00,  3.1289e+00, -3.8216e-03, -3.0268e+00,
          1.4369e-01,  1.6063e+00, -1.8132e+01, -2.2555e-01,  1.4279e+01,
         -2.9856e+00, -5.3393e-01,  5.3852e+00],
        [-1.6150e+01,  1.7911e-03, -5.1000e-02, -6.6099e-02, -1.7355e+01,
          2.6681e-01, -2.9860e+00,  1.6266e+00, -4.8645e-02,  6.5288e+00,
         -7.7175e+00, -1.7876e-02,  8.9499e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  7.9924,  -3.4066,  -1.0769,  -5.7794,  -6.0547,  -5.7663, -10.0939,
         -2.8442,  -5.7800,  -1.9584,  -0.6121,   1.9155,  -3.4088],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 8.9243e-01,  5.6911e+00,  1.5663e+00, -1.2989e-01,  1.8775e-01,
         -1.2769e-01, -2.8104e-01,  8.8669e-01, -1.2967e-01,  5.6859e+00,
         -2.5489e+00, -1.8838e-01, -8.8407e+00],
        [ 4.0833e-01, -6.0124e+00, -8.9014e-02,  1.2970e-01, -1.8718e-01,
          1.2740e-01,  1.3625e+00, -1.0528e+00,  1.2947e-01, -5.5807e+00,
          3.4531e+00,  7.2938e-01,  9.1776e+00],
        [-1.1697e+01,  1.9980e+00,  9.8437e-01, -2.5489e-02, -2.5813e-02,
         -2.2211e-02,  3.6009e+00, -4.6501e-01, -2.5537e-02,  1.8547e-01,
         -1.8549e-02, -1.0172e+00, -5.6876e-03],
        [-1.7125e+00, -3.0052e+00, -4.1191e+00, -1.5236e-02,  5.2480e-01,
         -6.0817e-02, -2.1736e+01, -1.7056e+01, -1.4635e-02, -3.2576e+00,
          7.8828e-01,  2.1253e+00,  1.0758e+01],
        [ 9.3924e+00, -3.9348e+00, -1.0063e+01, -4.8412e-01, -4.6928e-01,
         -5.0761e-01,  4.0058e+00, -6.5070e+00, -4.8384e-01, -8.7925e-01,
          6.1774e+00,  3.5045e+00, -3.2768e-01]], device='cuda:0'))])
xi:  [345.36514]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 450.71096865547344
W_T_median: 387.09146055011786
W_T_pctile_5: 345.52252546722633
W_T_CVAR_5_pct: 174.02972044459628
Average q (qsum/M+1):  45.49808231476815
Optimal xi:  [345.36514]
Expected(across Rb) median(across samples) p_equity:  0.1812028767929102
obj fun:  tensor(-3150.7354, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 10.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -2.7095,  12.9892],
        [ -8.6338,   9.4281],
        [ -0.9451,   0.4527],
        [ -0.8640,   0.3781],
        [  6.1895,  -0.6523],
        [  4.5754,  -0.4303],
        [ -0.9502,   0.4519],
        [ -0.9882,   0.4554],
        [  5.3775,  -0.2578],
        [  9.7529,  -0.6215],
        [-74.6407,   0.8966],
        [  5.4828,  -0.5841],
        [ -2.7219,  16.4087]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 7.2031,  5.6080, -3.1690, -3.3192,  2.5904,  3.5518, -3.1733, -3.1998,
         3.1859, -7.8767,  0.2375,  2.9803,  8.6844], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1333e-01,  1.6554e+00,  3.4528e-02,  3.0769e-02,  8.7976e-01,
          9.1042e-01,  3.5304e-02,  3.4772e-02,  8.9665e-01,  2.6447e-01,
          4.3032e-01,  8.9469e-01,  4.6686e-01],
        [-9.2749e+00, -6.0010e+00,  1.6064e-01,  2.4939e-01,  2.6538e+00,
          2.9059e+00,  1.6532e-01,  2.3862e-01,  2.6770e+00,  2.0542e+01,
          5.6439e+00,  2.9504e+00, -1.0311e+01],
        [-5.8631e+00, -5.5932e+00, -1.0882e-01, -2.5602e-01,  1.4267e+00,
          1.6964e+00, -8.6213e-02, -1.9126e-01,  1.4872e+00,  2.0896e+01,
          5.2545e+00,  1.4174e+00, -9.1947e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4955e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.0882e-01,  1.6256e+00,  3.4094e-02,  3.0485e-02,  8.6370e-01,
          8.9398e-01,  3.4826e-02,  3.4369e-02,  8.8039e-01,  2.5781e-01,
          4.1380e-01,  8.7846e-01,  4.6006e-01],
        [-6.2435e+00, -5.0774e+00, -7.9019e-02, -1.1667e-02,  1.6100e+00,
          1.6134e+00, -1.1318e-01, -9.1158e-02,  1.1232e+00,  2.1826e+01,
          5.7213e+00,  1.3367e+00, -9.6095e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4956e-03, -5.9415e-03, -4.7716e-01,
         -4.9046e-01, -6.4683e-03, -6.3263e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1923e-01,  1.6977e+00,  3.5087e-02,  3.1098e-02,  9.0167e-01,
          9.3283e-01,  3.5931e-02,  3.5276e-02,  9.1883e-01,  2.7424e-01,
          4.5388e-01,  9.1684e-01,  4.7618e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5021, -0.5021,  0.9316,  2.2107,  1.1816, -0.5021, -0.5021,  0.9149,
         1.2202, -0.5021, -0.5021, -0.5021,  0.9544], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0192,   0.0192,   3.5844,  -9.2131, -12.7066,   0.0192,   0.0192,
           3.1214, -12.3800,   0.0192,   0.0192,   0.0192,   4.3492]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  2.3638,  12.1836],
        [  5.6572,   7.4721],
        [  7.9224,   6.5803],
        [ -6.1235,   3.1910],
        [ 11.8466,   0.5913],
        [  0.8770,  -8.1352],
        [  8.6891,  -6.8360],
        [-10.5691,  10.8756],
        [  1.7568,  -0.0712],
        [-11.0991,  -4.2491],
        [ 11.4103,   5.2357],
        [ -0.1851,   2.9347],
        [ -7.5723,  -9.3505]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 6.3522, -5.1507, -2.0487, -4.3376, -9.8811, -6.7882, -6.3489,  5.9660,
        -7.1033, -1.4560, -0.7179, -3.6786, -6.0576], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.1852e+00,  1.5094e+00,  1.3526e+00, -5.4268e-01,  2.4223e+00,
          2.0759e+00,  9.2465e-01, -1.6151e+00, -2.8824e-01,  9.5287e-01,
          2.4642e-01, -1.1737e+00,  2.2713e+00],
        [-1.8651e+00, -5.6256e-01, -6.2129e-01, -4.2742e+00, -7.7059e-01,
         -3.4892e+00, -7.6918e-02,  6.7408e+00, -2.3096e-01,  1.3500e+00,
         -3.5500e+00, -6.0667e-01, -2.9353e+00],
        [ 1.1142e-01, -1.2473e+00,  8.1916e-01,  6.4910e-01, -1.6350e+01,
          2.2732e+00, -1.5779e+00,  7.2357e+00, -7.2982e-03,  4.4049e+00,
         -5.8977e-01,  1.7865e+00, -4.6946e+00],
        [-1.3960e+00, -8.7673e-02, -4.2780e-01,  6.7083e-03, -4.0893e-01,
         -9.1685e-01, -1.7927e+00, -5.0702e-01, -1.3463e-02, -5.2559e-01,
         -1.6941e+00,  1.8378e-03, -3.4962e-01],
        [-1.9520e+00,  1.8967e-01, -5.4411e-01, -1.2101e-01, -9.1077e-01,
         -1.2479e+00, -1.1267e+00, -9.1391e-01, -1.8068e-02, -3.8365e-01,
         -2.4970e+00, -1.1678e-01, -2.1655e-01],
        [-1.4057e+00, -8.6477e-02, -4.3293e-01,  1.5699e-02, -4.1127e-01,
         -9.1630e-01, -1.7894e+00, -5.0243e-01, -1.3300e-02, -5.1448e-01,
         -1.7010e+00,  1.2009e-02, -3.4096e-01],
        [ 1.3221e+01, -6.1845e-01,  1.5793e+00, -6.7508e+00, -1.5120e+01,
         -1.8714e+01, -8.9013e+00,  4.8410e+00, -1.2180e+00,  4.2963e+00,
          1.1099e+00,  1.4776e+00, -1.3629e+01],
        [ 4.1482e+00, -4.3534e-01, -7.3632e+00,  2.5918e-02, -1.7989e-01,
         -6.4858e+00, -8.9001e-01, -1.0932e+00,  1.0207e-01,  1.5231e+00,
         -7.6390e+00,  1.3168e-01, -1.0356e+01],
        [-1.4524e+00, -8.1858e-02, -4.5182e-01,  9.7058e-02, -4.0160e-01,
         -8.8229e-01, -1.8020e+00, -4.5301e-01, -1.2095e-02, -4.6125e-01,
         -1.7020e+00,  9.5630e-02, -3.0579e-01],
        [-2.8522e+00, -1.9497e-01, -1.6674e+00, -4.5101e-01, -1.9345e+00,
         -2.7298e+00,  5.5568e-01,  1.5671e+00, -1.2880e-02,  9.4602e-01,
         -2.4953e+00, -2.9629e-01, -2.6501e+00],
        [-2.4334e+01, -6.7212e-02, -4.8855e+00,  1.9330e-01, -7.7025e+00,
          2.7805e+00,  4.4028e-01, -1.4406e+00, -4.7447e-02,  6.3915e+00,
         -8.7817e+00, -4.8862e-01,  5.4980e+00],
        [-1.0961e+01,  5.4093e-01,  8.6739e-01, -6.0412e-02, -3.7470e+00,
          2.1004e+00,  2.4607e+00, -2.1072e+01, -1.4633e-01,  9.1990e+00,
         -5.3680e+00,  1.5515e-01,  4.1397e+00],
        [-1.2571e+01,  2.5529e-02, -4.2095e+00,  2.0473e-01, -1.3435e+01,
          2.3688e+00, -6.0068e-01, -5.7139e+00, -3.7530e-01,  4.5355e+00,
         -9.0221e+00,  3.5582e-02,  2.3988e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 2.9079, -2.1828, -0.1738, -3.3916, -2.9699, -3.3962, -3.0437, -0.0264,
        -3.4166, -2.8315, -0.3684,  1.2132,  0.0742], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 7.4891e-01,  4.5452e+00,  1.8769e+00,  4.5914e-02,  3.5496e-01,
         -9.1354e-02,  2.7526e-01, -9.7636e-01, -1.0948e-01,  3.3561e+00,
         -4.4478e+00, -1.2550e-01, -1.0160e+00],
        [ 5.4131e-01, -4.5533e+00, -3.9331e-01,  2.2764e-01, -3.5322e-01,
          9.4308e-02,  8.6633e-01,  7.9627e-01,  1.1223e-01, -3.0675e+00,
          5.4099e+00,  6.6530e-01,  1.5354e+00],
        [-1.0256e+01, -4.0894e-02, -1.1152e+00, -1.1287e-02, -1.0215e-02,
         -1.1185e-02, -8.9347e-01, -1.4354e-01, -1.1272e-02, -1.5607e-02,
          8.3038e-03, -6.6567e-01,  1.3450e-03],
        [-1.2086e+00, -1.8795e+00, -3.9948e+00,  3.7431e-01,  1.3870e+00,
          3.8828e-01,  9.5588e-01, -1.2326e+01,  4.2495e-01,  2.2745e+00,
          8.1236e-01,  1.8802e+00,  1.0604e+01],
        [ 9.1296e+00, -4.1802e+00, -1.1657e+01,  2.0360e-01,  4.0986e-01,
          2.0817e-01,  5.1155e+00, -6.9774e-01,  2.1607e-01, -5.1843e-01,
          6.2533e+00,  8.3596e+00,  2.6055e+00]], device='cuda:0'))])
loaded xi:  315.55313
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -5671.298444790548
Current xi:  [326.35953]
objective value function right now is: -5671.298444790548
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -5720.876297675906
Current xi:  [334.3547]
objective value function right now is: -5720.876297675906
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [340.59982]
objective value function right now is: -5720.40089019491
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.48233]
objective value function right now is: -5717.190286022705
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [348.1761]
objective value function right now is: -5698.450599124595
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [350.52148]
objective value function right now is: -5685.84560530749
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [350.62836]
objective value function right now is: -5706.022502723138
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -5748.358296236469
Current xi:  [352.32584]
objective value function right now is: -5748.358296236469
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -5758.242159775444
Current xi:  [352.6152]
objective value function right now is: -5758.242159775444
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.4611]
objective value function right now is: -5679.570378301915
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [350.51935]
objective value function right now is: -5750.626399019219
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.46616]
objective value function right now is: -5723.759871568029
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.33243]
objective value function right now is: -5710.2629299104465
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [353.39462]
objective value function right now is: -5721.065026015937
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.69254]
objective value function right now is: -5683.14582800344
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [350.8932]
objective value function right now is: -5713.559296342826
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [350.68356]
objective value function right now is: -5727.733962184532
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.44666]
objective value function right now is: -5746.424106020946
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -5767.796491258207
Current xi:  [350.96283]
objective value function right now is: -5767.796491258207
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.42636]
objective value function right now is: -5765.701541270848
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [350.76865]
objective value function right now is: -5744.929194570089
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [350.0483]
objective value function right now is: -5727.335562595405
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [350.80255]
objective value function right now is: -5747.628773044972
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.21402]
objective value function right now is: -5762.345475303809
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.62582]
objective value function right now is: -5748.210404622985
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.39468]
objective value function right now is: -5739.809188387198
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.95093]
objective value function right now is: -5712.9202318265925
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [353.94348]
objective value function right now is: -5710.974193053183
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [352.04346]
objective value function right now is: -5726.321256144003
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.29108]
objective value function right now is: -5741.980417559844
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.92654]
objective value function right now is: -5749.377357544501
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.70517]
objective value function right now is: -5744.001486714849
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.1092]
objective value function right now is: -5741.648743349217
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.39502]
objective value function right now is: -5733.21402884268
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.69717]
objective value function right now is: -5745.19369616829
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -5782.763555354103
Current xi:  [353.19012]
objective value function right now is: -5782.763555354103
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.29276]
objective value function right now is: -5778.296020233268
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.49622]
objective value function right now is: -5777.027723391623
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -5783.570196906547
Current xi:  [353.6125]
objective value function right now is: -5783.570196906547
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.68652]
objective value function right now is: -5772.2910562254465
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.73462]
objective value function right now is: -5781.619387354815
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.726]
objective value function right now is: -5780.287163321247
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.47183]
objective value function right now is: -5782.837096236232
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.5349]
objective value function right now is: -5780.6273058024
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.19342]
objective value function right now is: -5778.72321729453
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.84357]
objective value function right now is: -5777.782767145601
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.44144]
objective value function right now is: -5774.610967233915
new min fval from sgd:  -5783.595817849255
new min fval from sgd:  -5783.837018816697
new min fval from sgd:  -5784.205008033276
new min fval from sgd:  -5784.466779970066
new min fval from sgd:  -5784.88385582479
new min fval from sgd:  -5785.208057024388
new min fval from sgd:  -5785.422394368613
new min fval from sgd:  -5785.729429359584
new min fval from sgd:  -5786.0213066828255
new min fval from sgd:  -5786.292704472274
new min fval from sgd:  -5786.500478079444
new min fval from sgd:  -5786.678766055767
new min fval from sgd:  -5786.777063324243
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.93958]
objective value function right now is: -5774.65535601103
new min fval from sgd:  -5786.80773467819
new min fval from sgd:  -5786.873271362603
new min fval from sgd:  -5786.921462450749
new min fval from sgd:  -5786.94688676248
new min fval from sgd:  -5786.962785295909
new min fval from sgd:  -5786.980734503039
new min fval from sgd:  -5787.004784343233
new min fval from sgd:  -5787.031857173151
new min fval from sgd:  -5787.059381885587
new min fval from sgd:  -5787.079884636178
new min fval from sgd:  -5787.091475093476
new min fval from sgd:  -5787.097039800584
new min fval from sgd:  -5787.109469690999
new min fval from sgd:  -5787.124517302598
new min fval from sgd:  -5787.127586499529
new min fval from sgd:  -5787.13648026371
new min fval from sgd:  -5787.147980487364
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.8955]
objective value function right now is: -5786.519326155931
new min fval from sgd:  -5787.153316479135
new min fval from sgd:  -5787.188872229373
new min fval from sgd:  -5787.234348525884
new min fval from sgd:  -5787.2772717956
new min fval from sgd:  -5787.335237635253
new min fval from sgd:  -5787.358549621052
new min fval from sgd:  -5787.388867243488
new min fval from sgd:  -5787.421863532645
new min fval from sgd:  -5787.450926477864
new min fval from sgd:  -5787.484841774369
new min fval from sgd:  -5787.491924923217
new min fval from sgd:  -5787.495342370941
new min fval from sgd:  -5787.495755160056
new min fval from sgd:  -5787.498207353738
new min fval from sgd:  -5787.509152366419
new min fval from sgd:  -5787.523481164741
new min fval from sgd:  -5787.538855073517
new min fval from sgd:  -5787.546127378343
new min fval from sgd:  -5787.553485108692
new min fval from sgd:  -5787.592992065464
new min fval from sgd:  -5787.641310050824
new min fval from sgd:  -5787.6670556696035
new min fval from sgd:  -5787.696857991975
new min fval from sgd:  -5787.717081125184
new min fval from sgd:  -5787.736275483145
new min fval from sgd:  -5787.736569642229
new min fval from sgd:  -5787.7491781101
new min fval from sgd:  -5787.759989784406
new min fval from sgd:  -5787.767934416776
new min fval from sgd:  -5787.773429563688
new min fval from sgd:  -5787.776139830385
new min fval from sgd:  -5787.79354864139
new min fval from sgd:  -5787.798853599945
new min fval from sgd:  -5787.80331670737
new min fval from sgd:  -5787.816337594663
new min fval from sgd:  -5787.825396367999
new min fval from sgd:  -5787.83287854627
new min fval from sgd:  -5787.875265231076
new min fval from sgd:  -5787.918386708839
new min fval from sgd:  -5787.949957941026
new min fval from sgd:  -5787.993258913462
new min fval from sgd:  -5788.0304144894735
new min fval from sgd:  -5788.0780941147505
new min fval from sgd:  -5788.116374598565
new min fval from sgd:  -5788.148659002573
new min fval from sgd:  -5788.18393294994
new min fval from sgd:  -5788.2133479161475
new min fval from sgd:  -5788.221966588285
new min fval from sgd:  -5788.267769988289
new min fval from sgd:  -5788.321409874665
new min fval from sgd:  -5788.361751666825
new min fval from sgd:  -5788.409479970267
new min fval from sgd:  -5788.449512603094
new min fval from sgd:  -5788.48771090819
new min fval from sgd:  -5788.507178497163
new min fval from sgd:  -5788.522787956861
new min fval from sgd:  -5788.523863509718
new min fval from sgd:  -5788.533756035218
new min fval from sgd:  -5788.544175337758
new min fval from sgd:  -5788.549988939714
new min fval from sgd:  -5788.562960722951
new min fval from sgd:  -5788.5796217073375
new min fval from sgd:  -5788.587790924473
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.88513]
objective value function right now is: -5788.1965109332605
min fval:  -5788.587790924473
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -7.1133,  16.5854],
        [-11.6918,  10.7193],
        [ -0.6676,   0.7438],
        [ -0.6676,   0.7438],
        [  1.3260,  -0.5267],
        [  1.2715,  -0.4448],
        [ -0.6676,   0.7438],
        [ -0.6676,   0.7441],
        [  1.2624,  -0.4118],
        [ 14.4708,  -1.2155],
        [-86.4435,  -1.9067],
        [  1.2771,  -0.4269],
        [ -8.8423,  19.5972]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  8.2274,   6.0969,  -3.8725,  -3.8725,   5.4688,   5.7066,  -3.8725,
         -3.8723,   5.5985, -10.0772,  -0.2533,   5.6194,   9.8807],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.7544e-01, -3.1959e-01,  6.6009e-03,  6.6009e-03, -7.3046e-01,
         -7.3086e-01,  6.6009e-03,  6.6016e-03, -7.3062e-01, -2.6134e-02,
         -1.5062e-01, -7.3068e-01, -2.9294e-01],
        [-2.7544e-01, -3.1959e-01,  6.6007e-03,  6.6007e-03, -7.3046e-01,
         -7.3086e-01,  6.6007e-03,  6.6014e-03, -7.3062e-01, -2.6131e-02,
         -1.5061e-01, -7.3069e-01, -2.9294e-01],
        [ 4.8475e-01,  9.5489e-01, -1.5321e-02, -1.5321e-02,  1.2471e+00,
          1.2475e+00, -1.5321e-02, -1.5320e-02,  1.2473e+00,  1.1148e-01,
          4.6300e-01,  1.2473e+00,  5.7043e-01],
        [-1.2113e+01, -7.1588e+00,  1.7109e-02,  1.7109e-02,  3.2074e+00,
          3.7120e+00,  1.7109e-02,  1.6881e-02,  3.4277e+00,  2.8154e+01,
          6.7042e+00,  3.6438e+00, -1.6959e+01],
        [-8.8328e+00, -5.0043e+00,  7.3378e-02,  7.3372e-02,  1.3517e+00,
          1.9194e+00,  7.3420e-02,  7.6643e-02,  1.7142e+00,  2.7987e+01,
          6.4535e+00,  1.5183e+00, -1.5030e+01],
        [-2.7544e-01, -3.1959e-01,  6.6009e-03,  6.6009e-03, -7.3046e-01,
         -7.3086e-01,  6.6010e-03,  6.6016e-03, -7.3062e-01, -2.6134e-02,
         -1.5062e-01, -7.3068e-01, -2.9294e-01],
        [-2.7542e-01, -3.1960e-01,  6.5995e-03,  6.5995e-03, -7.3049e-01,
         -7.3089e-01,  6.5995e-03,  6.6002e-03, -7.3065e-01, -2.6119e-02,
         -1.5058e-01, -7.3071e-01, -2.9292e-01],
        [ 4.7815e-01,  9.4001e-01, -1.5290e-02, -1.5290e-02,  1.2265e+00,
          1.2269e+00, -1.5290e-02, -1.5290e-02,  1.2267e+00,  1.1003e-01,
          4.5750e-01,  1.2267e+00,  5.6269e-01],
        [-9.0748e+00, -4.9867e+00, -9.0908e-02, -9.0903e-02,  1.6255e+00,
          1.9839e+00, -9.0947e-02, -9.3606e-02,  1.5109e+00,  2.8473e+01,
          6.1481e+00,  1.5654e+00, -1.5216e+01],
        [-2.7544e-01, -3.1959e-01,  6.6010e-03,  6.6009e-03, -7.3046e-01,
         -7.3086e-01,  6.6010e-03,  6.6016e-03, -7.3062e-01, -2.6135e-02,
         -1.5062e-01, -7.3068e-01, -2.9294e-01],
        [-2.7544e-01, -3.1959e-01,  6.6009e-03,  6.6009e-03, -7.3046e-01,
         -7.3086e-01,  6.6009e-03,  6.6016e-03, -7.3062e-01, -2.6134e-02,
         -1.5062e-01, -7.3068e-01, -2.9294e-01],
        [-2.7534e-01, -3.1966e-01,  6.5946e-03,  6.5946e-03, -7.3060e-01,
         -7.3100e-01,  6.5946e-03,  6.5953e-03, -7.3076e-01, -2.6064e-02,
         -1.5046e-01, -7.3082e-01, -2.9284e-01],
        [ 4.9384e-01,  9.7598e-01, -1.5354e-02, -1.5354e-02,  1.2758e+00,
          1.2762e+00, -1.5354e-02, -1.5354e-02,  1.2760e+00,  1.1350e-01,
          4.7090e-01,  1.2760e+00,  5.8122e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.7327, -0.7327,  1.2491,  3.1229,  1.5723, -0.7327, -0.7327,  1.2285,
         1.7720, -0.7327, -0.7327, -0.7328,  1.2779], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0288,  -0.0288,   3.6911, -10.7379, -13.4157,  -0.0288,  -0.0288,
           3.2328, -13.0099,  -0.0288,  -0.0288,  -0.0288,   4.4466]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  0.7960,  12.5815],
        [ -3.3729,   3.1199],
        [ 12.2157,  11.9313],
        [ -3.5269,   9.8804],
        [ 13.4584,   1.0566],
        [ -2.3439,   0.4631],
        [ 14.9253, -11.7887],
        [ -3.6233,  16.4000],
        [ -2.3399,   0.4646],
        [-19.0629,  -7.8490],
        [ 18.2116,   7.7549],
        [ -2.7986,   0.4983],
        [-10.8864, -13.2993]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.5421,  -8.4900,  -1.8882,  -2.7958, -11.8593,  -6.0221,  -8.8786,
          8.8459,  -6.0257,  -3.8133,  -1.3272,  -5.7983,  -6.9817],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.5225e+00,  2.8050e-02, -2.0368e-01,  2.6276e-02,  2.0268e+00,
          3.9825e-01,  4.2178e-01,  1.4838e+01,  3.9627e-01,  8.0543e-01,
         -1.5818e+00,  5.0258e-01,  6.4274e+00],
        [ 2.5514e+00,  4.8311e-03, -2.0206e+01, -2.1959e-01, -3.3079e+00,
          1.4904e-01, -1.9160e+00,  1.4475e+00,  1.3917e-01,  6.4847e+00,
         -2.4712e+00,  1.6173e-01, -4.6747e+00],
        [ 6.5518e+00, -6.2535e+00, -3.8940e+00, -2.9355e-01, -1.4682e+01,
         -2.4728e-01, -6.2271e+00,  8.9668e+00, -2.3849e-01,  3.9485e+00,
          1.7959e+00, -5.2668e-01, -1.6169e+01],
        [-2.5106e+00,  5.0569e-01,  1.2715e+00,  5.3195e-01,  1.0058e+00,
          1.8194e-01, -5.0555e-01, -1.9073e+00,  1.8249e-01, -4.0566e-01,
         -1.4425e+00,  2.2278e-01, -1.7586e+00],
        [-1.9740e+00,  6.9882e-01,  9.1562e-01,  8.6322e-01,  1.3946e+00,
          2.0105e-01, -1.9343e-01, -8.6002e-01,  1.9460e-01, -1.8870e-01,
         -1.0012e+00,  8.9951e-02, -2.1018e+00],
        [-1.6782e-01, -9.0428e-01, -1.1070e+00, -1.2154e+00, -1.2980e+00,
          3.0627e-01, -9.1299e+00,  8.3804e-01,  3.0805e-01,  2.7861e+00,
         -4.0777e+00,  3.2848e-01,  4.1287e+00],
        [ 8.6645e+00, -6.1461e+00,  7.3051e+00, -1.1789e+00, -1.0436e+01,
         -3.0780e-01, -7.3702e+00,  2.1953e+00, -3.0213e-01,  8.5313e+00,
          6.1747e-01, -5.4975e-01, -1.0680e+01],
        [ 5.1987e-01, -1.1152e-02, -7.7333e+00, -1.3863e-02, -1.0008e+01,
         -5.7729e-02, -9.6704e-01, -3.0382e+00, -5.5201e-02, -3.6721e+00,
         -4.0603e+00, -6.0167e-02, -1.4620e+01],
        [-2.0095e+00, -5.4198e-01, -7.7494e-01, -6.9960e-01, -7.3312e-01,
         -1.5284e-02, -4.6156e+00, -9.4753e-01, -1.5257e-02,  8.7397e-01,
         -1.2878e+00, -1.5567e-02,  9.0128e-01],
        [-2.6176e+00,  7.2279e-03, -8.6476e+00, -7.0454e-02, -1.4442e+01,
         -1.5313e-01,  3.9611e-01, -2.6108e+00, -1.4236e-01, -2.1038e+00,
          3.1476e-03, -2.4468e-01, -1.4812e+01],
        [-2.3051e+01, -9.3289e-04, -7.2481e+00,  3.7825e-02, -1.3917e+01,
          5.3759e-01,  7.2451e-01, -3.3517e+00,  5.2876e-01,  1.3811e+01,
         -9.9932e+00,  4.7504e-01,  9.0488e+00],
        [-7.8376e+00, -3.0003e-02,  2.6811e+00,  7.1704e-02, -2.8122e+00,
         -9.9726e-01,  2.1462e+00, -1.1552e+01, -9.5682e-01,  1.3834e+01,
         -3.5896e+00, -1.1962e+00,  4.3686e+00],
        [-1.3351e+01, -5.1805e-03, -1.1779e+00, -6.0170e-02, -1.7789e+01,
          1.6521e-01, -1.0884e+00, -8.5725e+00,  1.6901e-01,  5.3773e+00,
         -9.5857e+00,  2.2125e-01,  6.1606e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 3.0036, -1.2283,  1.2185, -4.0508, -4.0489, -3.9131, -9.0197, -1.1380,
        -4.8825, -3.0645,  0.2099,  1.3212, -0.3114], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 9.4527e-01,  1.6728e+00,  1.2118e+00, -1.7257e+00, -1.7407e+00,
          1.8633e+00,  2.0573e-01,  6.2773e+00,  8.2310e-01,  6.5243e+00,
         -6.9294e+00, -3.3264e-01, -1.8484e+01],
        [ 3.4915e-01, -1.6837e+00,  2.7221e-01,  1.7260e+00,  1.7409e+00,
         -1.8639e+00,  9.3465e-01, -6.4138e+00, -8.2351e-01, -6.3140e+00,
          7.8417e+00,  8.7329e-01,  1.8886e+01],
        [-1.2367e+01, -1.0887e-01, -1.4402e+00, -3.0315e-02, -9.1937e-02,
         -1.4754e-03, -1.6624e-01, -9.2507e-03, -1.9085e-04, -4.9464e-02,
         -4.6235e-03, -1.5888e+00,  1.2553e-03],
        [-1.8582e+00,  1.2599e+00, -6.9080e+00, -1.8054e-01, -6.2808e-01,
         -2.3011e+00, -1.7673e+01, -1.1918e+00, -9.6388e-01, -4.4470e+00,
          1.6772e+00,  2.4621e+00,  1.0655e+01],
        [ 1.1046e+01, -2.5964e+00, -1.0239e+01, -6.3902e-01, -3.3781e-01,
         -5.3565e+00,  2.2677e+01, -1.3102e+00, -1.6338e+00,  8.2270e-01,
          1.4640e+01,  4.2127e-01,  1.9694e+00]], device='cuda:0'))])
xi:  [352.8699]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 549.1497038118591
W_T_median: 436.98619530942426
W_T_pctile_5: 352.89647345852353
W_T_CVAR_5_pct: 176.74222992920542
Average q (qsum/M+1):  44.19457220262097
Optimal xi:  [352.8699]
Expected(across Rb) median(across samples) p_equity:  0.16650743017283579
obj fun:  tensor(-5788.5878, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 25.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -2.7095,  12.9892],
        [ -8.6338,   9.4281],
        [ -0.9451,   0.4527],
        [ -0.8640,   0.3781],
        [  6.1895,  -0.6523],
        [  4.5754,  -0.4303],
        [ -0.9502,   0.4519],
        [ -0.9882,   0.4554],
        [  5.3775,  -0.2578],
        [  9.7529,  -0.6215],
        [-74.6407,   0.8966],
        [  5.4828,  -0.5841],
        [ -2.7219,  16.4087]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 7.2031,  5.6080, -3.1690, -3.3192,  2.5904,  3.5518, -3.1733, -3.1998,
         3.1859, -7.8767,  0.2375,  2.9803,  8.6844], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1333e-01,  1.6554e+00,  3.4528e-02,  3.0769e-02,  8.7976e-01,
          9.1042e-01,  3.5304e-02,  3.4772e-02,  8.9665e-01,  2.6447e-01,
          4.3032e-01,  8.9469e-01,  4.6686e-01],
        [-9.2749e+00, -6.0010e+00,  1.6064e-01,  2.4939e-01,  2.6538e+00,
          2.9059e+00,  1.6532e-01,  2.3862e-01,  2.6770e+00,  2.0542e+01,
          5.6439e+00,  2.9504e+00, -1.0311e+01],
        [-5.8631e+00, -5.5932e+00, -1.0882e-01, -2.5602e-01,  1.4267e+00,
          1.6964e+00, -8.6213e-02, -1.9126e-01,  1.4872e+00,  2.0896e+01,
          5.2545e+00,  1.4174e+00, -9.1947e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4955e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.0882e-01,  1.6256e+00,  3.4094e-02,  3.0485e-02,  8.6370e-01,
          8.9398e-01,  3.4826e-02,  3.4369e-02,  8.8039e-01,  2.5781e-01,
          4.1380e-01,  8.7846e-01,  4.6006e-01],
        [-6.2435e+00, -5.0774e+00, -7.9019e-02, -1.1667e-02,  1.6100e+00,
          1.6134e+00, -1.1318e-01, -9.1158e-02,  1.1232e+00,  2.1826e+01,
          5.7213e+00,  1.3367e+00, -9.6095e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4956e-03, -5.9415e-03, -4.7716e-01,
         -4.9046e-01, -6.4683e-03, -6.3263e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1923e-01,  1.6977e+00,  3.5087e-02,  3.1098e-02,  9.0167e-01,
          9.3283e-01,  3.5931e-02,  3.5276e-02,  9.1883e-01,  2.7424e-01,
          4.5388e-01,  9.1684e-01,  4.7618e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5021, -0.5021,  0.9316,  2.2107,  1.1816, -0.5021, -0.5021,  0.9149,
         1.2202, -0.5021, -0.5021, -0.5021,  0.9544], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0192,   0.0192,   3.5844,  -9.2131, -12.7066,   0.0192,   0.0192,
           3.1214, -12.3800,   0.0192,   0.0192,   0.0192,   4.3492]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  2.3638,  12.1836],
        [  5.6572,   7.4721],
        [  7.9224,   6.5803],
        [ -6.1235,   3.1910],
        [ 11.8466,   0.5913],
        [  0.8770,  -8.1352],
        [  8.6891,  -6.8360],
        [-10.5691,  10.8756],
        [  1.7568,  -0.0712],
        [-11.0991,  -4.2491],
        [ 11.4103,   5.2357],
        [ -0.1851,   2.9347],
        [ -7.5723,  -9.3505]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 6.3522, -5.1507, -2.0487, -4.3376, -9.8811, -6.7882, -6.3489,  5.9660,
        -7.1033, -1.4560, -0.7179, -3.6786, -6.0576], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.1852e+00,  1.5094e+00,  1.3526e+00, -5.4268e-01,  2.4223e+00,
          2.0759e+00,  9.2465e-01, -1.6151e+00, -2.8824e-01,  9.5287e-01,
          2.4642e-01, -1.1737e+00,  2.2713e+00],
        [-1.8651e+00, -5.6256e-01, -6.2129e-01, -4.2742e+00, -7.7059e-01,
         -3.4892e+00, -7.6918e-02,  6.7408e+00, -2.3096e-01,  1.3500e+00,
         -3.5500e+00, -6.0667e-01, -2.9353e+00],
        [ 1.1142e-01, -1.2473e+00,  8.1916e-01,  6.4910e-01, -1.6350e+01,
          2.2732e+00, -1.5779e+00,  7.2357e+00, -7.2982e-03,  4.4049e+00,
         -5.8977e-01,  1.7865e+00, -4.6946e+00],
        [-1.3960e+00, -8.7673e-02, -4.2780e-01,  6.7083e-03, -4.0893e-01,
         -9.1685e-01, -1.7927e+00, -5.0702e-01, -1.3463e-02, -5.2559e-01,
         -1.6941e+00,  1.8378e-03, -3.4962e-01],
        [-1.9520e+00,  1.8967e-01, -5.4411e-01, -1.2101e-01, -9.1077e-01,
         -1.2479e+00, -1.1267e+00, -9.1391e-01, -1.8068e-02, -3.8365e-01,
         -2.4970e+00, -1.1678e-01, -2.1655e-01],
        [-1.4057e+00, -8.6477e-02, -4.3293e-01,  1.5699e-02, -4.1127e-01,
         -9.1630e-01, -1.7894e+00, -5.0243e-01, -1.3300e-02, -5.1448e-01,
         -1.7010e+00,  1.2009e-02, -3.4096e-01],
        [ 1.3221e+01, -6.1845e-01,  1.5793e+00, -6.7508e+00, -1.5120e+01,
         -1.8714e+01, -8.9013e+00,  4.8410e+00, -1.2180e+00,  4.2963e+00,
          1.1099e+00,  1.4776e+00, -1.3629e+01],
        [ 4.1482e+00, -4.3534e-01, -7.3632e+00,  2.5918e-02, -1.7989e-01,
         -6.4858e+00, -8.9001e-01, -1.0932e+00,  1.0207e-01,  1.5231e+00,
         -7.6390e+00,  1.3168e-01, -1.0356e+01],
        [-1.4524e+00, -8.1858e-02, -4.5182e-01,  9.7058e-02, -4.0160e-01,
         -8.8229e-01, -1.8020e+00, -4.5301e-01, -1.2095e-02, -4.6125e-01,
         -1.7020e+00,  9.5630e-02, -3.0579e-01],
        [-2.8522e+00, -1.9497e-01, -1.6674e+00, -4.5101e-01, -1.9345e+00,
         -2.7298e+00,  5.5568e-01,  1.5671e+00, -1.2880e-02,  9.4602e-01,
         -2.4953e+00, -2.9629e-01, -2.6501e+00],
        [-2.4334e+01, -6.7212e-02, -4.8855e+00,  1.9330e-01, -7.7025e+00,
          2.7805e+00,  4.4028e-01, -1.4406e+00, -4.7447e-02,  6.3915e+00,
         -8.7817e+00, -4.8862e-01,  5.4980e+00],
        [-1.0961e+01,  5.4093e-01,  8.6739e-01, -6.0412e-02, -3.7470e+00,
          2.1004e+00,  2.4607e+00, -2.1072e+01, -1.4633e-01,  9.1990e+00,
         -5.3680e+00,  1.5515e-01,  4.1397e+00],
        [-1.2571e+01,  2.5529e-02, -4.2095e+00,  2.0473e-01, -1.3435e+01,
          2.3688e+00, -6.0068e-01, -5.7139e+00, -3.7530e-01,  4.5355e+00,
         -9.0221e+00,  3.5582e-02,  2.3988e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 2.9079, -2.1828, -0.1738, -3.3916, -2.9699, -3.3962, -3.0437, -0.0264,
        -3.4166, -2.8315, -0.3684,  1.2132,  0.0742], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 7.4891e-01,  4.5452e+00,  1.8769e+00,  4.5914e-02,  3.5496e-01,
         -9.1354e-02,  2.7526e-01, -9.7636e-01, -1.0948e-01,  3.3561e+00,
         -4.4478e+00, -1.2550e-01, -1.0160e+00],
        [ 5.4131e-01, -4.5533e+00, -3.9331e-01,  2.2764e-01, -3.5322e-01,
          9.4308e-02,  8.6633e-01,  7.9627e-01,  1.1223e-01, -3.0675e+00,
          5.4099e+00,  6.6530e-01,  1.5354e+00],
        [-1.0256e+01, -4.0894e-02, -1.1152e+00, -1.1287e-02, -1.0215e-02,
         -1.1185e-02, -8.9347e-01, -1.4354e-01, -1.1272e-02, -1.5607e-02,
          8.3038e-03, -6.6567e-01,  1.3450e-03],
        [-1.2086e+00, -1.8795e+00, -3.9948e+00,  3.7431e-01,  1.3870e+00,
          3.8828e-01,  9.5588e-01, -1.2326e+01,  4.2495e-01,  2.2745e+00,
          8.1236e-01,  1.8802e+00,  1.0604e+01],
        [ 9.1296e+00, -4.1802e+00, -1.1657e+01,  2.0360e-01,  4.0986e-01,
          2.0817e-01,  5.1155e+00, -6.9774e-01,  2.1607e-01, -5.1843e-01,
          6.2533e+00,  8.3596e+00,  2.6055e+00]], device='cuda:0'))])
loaded xi:  315.55313
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -9864.778322213255
Current xi:  [326.5874]
objective value function right now is: -9864.778322213255
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -10018.574540029083
Current xi:  [335.15482]
objective value function right now is: -10018.574540029083
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [340.19147]
objective value function right now is: -9984.106015217032
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -10062.90790656891
Current xi:  [345.2796]
objective value function right now is: -10062.90790656891
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -10149.103643438313
Current xi:  [348.5025]
objective value function right now is: -10149.103643438313
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -10159.141774885353
Current xi:  [349.80884]
objective value function right now is: -10159.141774885353
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [351.40094]
objective value function right now is: -10072.94402022675
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.356]
objective value function right now is: -10050.186400113806
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.8359]
objective value function right now is: -10141.6919714567
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.00043]
objective value function right now is: -10129.528254537414
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.8415]
objective value function right now is: -10140.591082261533
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.3811]
objective value function right now is: -10156.392242878923
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.1684]
objective value function right now is: -10103.685028182277
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [353.65475]
objective value function right now is: -10113.97258538501
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [356.04025]
objective value function right now is: -10107.989888805978
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.74463]
objective value function right now is: -10132.509580226011
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -10165.95565634894
Current xi:  [354.8529]
objective value function right now is: -10165.95565634894
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.92737]
objective value function right now is: -10078.089720737074
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.7238]
objective value function right now is: -10082.830028040345
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.77414]
objective value function right now is: -10097.812823443786
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.41296]
objective value function right now is: -10067.885481619243
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.33783]
objective value function right now is: -10123.647145593643
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.68274]
objective value function right now is: -10037.96660607031
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.98105]
objective value function right now is: -10074.344229691036
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -10180.992512414463
Current xi:  [353.183]
objective value function right now is: -10180.992512414463
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.99387]
objective value function right now is: -10014.658173531916
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.4824]
objective value function right now is: -10113.66077809794
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [352.503]
objective value function right now is: -10121.973983589114
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [354.4337]
objective value function right now is: -10134.878294350792
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.6977]
objective value function right now is: -10067.57224024917
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.5998]
objective value function right now is: -10090.43404605966
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.81866]
objective value function right now is: -10146.290269917172
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.93903]
objective value function right now is: -10122.297197352824
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.83392]
objective value function right now is: -10093.3603897021
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.0802]
objective value function right now is: -10089.570890460609
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -10181.997545133248
Current xi:  [355.06366]
objective value function right now is: -10181.997545133248
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -10195.603440876786
Current xi:  [354.56305]
objective value function right now is: -10195.603440876786
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.67172]
objective value function right now is: -10187.940249676212
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.69553]
objective value function right now is: -10189.40984206508
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -10195.872267224948
Current xi:  [354.61047]
objective value function right now is: -10195.872267224948
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -10201.416109807626
Current xi:  [354.59604]
objective value function right now is: -10201.416109807626
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -10201.826748056017
Current xi:  [354.72247]
objective value function right now is: -10201.826748056017
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.01508]
objective value function right now is: -10201.430612884114
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.97046]
objective value function right now is: -10199.565499686933
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.58508]
objective value function right now is: -10201.385854069342
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -10202.061082528067
Current xi:  [354.6961]
objective value function right now is: -10202.061082528067
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.85217]
objective value function right now is: -10201.711578642033
new min fval from sgd:  -10202.835919927873
new min fval from sgd:  -10204.39454956465
new min fval from sgd:  -10205.753822970184
new min fval from sgd:  -10207.596516581254
new min fval from sgd:  -10209.203320613044
new min fval from sgd:  -10209.926620767581
new min fval from sgd:  -10210.148307159427
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.4861]
objective value function right now is: -10203.463231062498
new min fval from sgd:  -10210.33474277408
new min fval from sgd:  -10211.170327316879
new min fval from sgd:  -10211.674188089803
new min fval from sgd:  -10212.390308313528
new min fval from sgd:  -10212.816302116373
new min fval from sgd:  -10212.846922700432
new min fval from sgd:  -10212.864814250921
new min fval from sgd:  -10212.929129347449
new min fval from sgd:  -10212.95265321409
new min fval from sgd:  -10212.968276968628
new min fval from sgd:  -10213.01030453505
new min fval from sgd:  -10213.027924732585
new min fval from sgd:  -10213.052934828967
new min fval from sgd:  -10213.06046773377
new min fval from sgd:  -10213.09922762074
new min fval from sgd:  -10213.107829423345
new min fval from sgd:  -10213.205237754652
new min fval from sgd:  -10213.308677534584
new min fval from sgd:  -10213.355809413966
new min fval from sgd:  -10213.385028346156
new min fval from sgd:  -10213.424693331175
new min fval from sgd:  -10213.514277951885
new min fval from sgd:  -10213.690876417246
new min fval from sgd:  -10213.829467048177
new min fval from sgd:  -10213.955414440627
new min fval from sgd:  -10214.024564823985
new min fval from sgd:  -10214.043163283774
new min fval from sgd:  -10214.053363172703
new min fval from sgd:  -10214.091840338466
new min fval from sgd:  -10214.147182782703
new min fval from sgd:  -10214.270449081678
new min fval from sgd:  -10214.316198802353
new min fval from sgd:  -10214.402847869276
new min fval from sgd:  -10214.468038883519
new min fval from sgd:  -10214.545682772532
new min fval from sgd:  -10214.625868787505
new min fval from sgd:  -10214.758062564313
new min fval from sgd:  -10214.845084749084
new min fval from sgd:  -10214.924217163347
new min fval from sgd:  -10214.958414154347
new min fval from sgd:  -10214.977318047546
new min fval from sgd:  -10214.978645070882
new min fval from sgd:  -10214.979127702512
new min fval from sgd:  -10214.999478665144
new min fval from sgd:  -10215.019813586629
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.32135]
objective value function right now is: -10210.573301259728
new min fval from sgd:  -10215.052378163018
new min fval from sgd:  -10215.141984900476
new min fval from sgd:  -10215.192688919724
new min fval from sgd:  -10215.265878625165
new min fval from sgd:  -10215.29662214273
new min fval from sgd:  -10215.29684043434
new min fval from sgd:  -10215.310028816464
new min fval from sgd:  -10215.379979353655
new min fval from sgd:  -10215.490130015123
new min fval from sgd:  -10215.616940747015
new min fval from sgd:  -10215.706815970972
new min fval from sgd:  -10215.807336413289
new min fval from sgd:  -10215.833534728505
new min fval from sgd:  -10215.889233327076
new min fval from sgd:  -10215.956814723124
new min fval from sgd:  -10216.047415882951
new min fval from sgd:  -10216.11578447878
new min fval from sgd:  -10216.180584371876
new min fval from sgd:  -10216.235285731622
new min fval from sgd:  -10216.296395789066
new min fval from sgd:  -10216.400715677546
new min fval from sgd:  -10216.481274788737
new min fval from sgd:  -10216.522774664283
new min fval from sgd:  -10216.562834966304
new min fval from sgd:  -10216.607533383409
new min fval from sgd:  -10216.623642099948
new min fval from sgd:  -10216.645746695891
new min fval from sgd:  -10216.686606879848
new min fval from sgd:  -10216.741066036817
new min fval from sgd:  -10216.763567954067
new min fval from sgd:  -10216.778484855768
new min fval from sgd:  -10216.837282280794
new min fval from sgd:  -10216.869083097861
new min fval from sgd:  -10216.902470854015
new min fval from sgd:  -10216.93295866632
new min fval from sgd:  -10216.938260193987
new min fval from sgd:  -10216.967731539953
new min fval from sgd:  -10216.999848227824
new min fval from sgd:  -10217.020040651321
new min fval from sgd:  -10217.040000574887
new min fval from sgd:  -10217.05930499236
new min fval from sgd:  -10217.066736177321
new min fval from sgd:  -10217.09756946854
new min fval from sgd:  -10217.12835948385
new min fval from sgd:  -10217.148094823879
new min fval from sgd:  -10217.151600231253
new min fval from sgd:  -10217.16312812648
new min fval from sgd:  -10217.175981295784
new min fval from sgd:  -10217.199102964905
new min fval from sgd:  -10217.216965499145
new min fval from sgd:  -10217.219228009542
new min fval from sgd:  -10217.248433707566
new min fval from sgd:  -10217.274826001485
new min fval from sgd:  -10217.303651799479
new min fval from sgd:  -10217.364729823576
new min fval from sgd:  -10217.429565435361
new min fval from sgd:  -10217.481825986997
new min fval from sgd:  -10217.529703775786
new min fval from sgd:  -10217.5780445938
new min fval from sgd:  -10217.615767465555
new min fval from sgd:  -10217.629644715247
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.25143]
objective value function right now is: -10216.7289079385
min fval:  -10217.629644715247
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -8.2480,  16.4527],
        [-10.8500,  11.6466],
        [ -0.7125,   0.6867],
        [ -0.6995,   0.6772],
        [  1.1291,  -0.3687],
        [ -3.1804, -12.4894],
        [ -0.7123,   0.6865],
        [ -0.7055,   0.6812],
        [  0.9374,  -0.2823],
        [ 14.4125,  -1.3077],
        [-94.0129,  -2.0972],
        [  0.8134,  -4.2266],
        [ -9.9109,  19.4298]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 8.0095,  5.7501, -4.2390, -4.2604,  5.9186,  4.0752, -4.2393, -4.2509,
         6.1469, -9.5588, -0.0245, -0.5992,  9.6430], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.2538e-01, -3.0680e-01, -1.3540e-02, -1.3355e-02, -9.1686e-01,
         -8.6667e-01, -1.3538e-02, -1.3437e-02, -9.1723e-01, -2.7831e-02,
         -2.0100e-01, -6.7660e-01, -3.5016e-01],
        [-3.2538e-01, -3.0680e-01, -1.3540e-02, -1.3355e-02, -9.1686e-01,
         -8.6667e-01, -1.3538e-02, -1.3437e-02, -9.1723e-01, -2.7830e-02,
         -2.0099e-01, -6.7659e-01, -3.5016e-01],
        [ 6.6370e-01,  6.0593e-01,  1.0010e-02,  1.0104e-02,  1.4709e+00,
          1.2800e+00,  1.0011e-02,  1.0063e-02,  1.4711e+00,  1.0022e-01,
          4.2712e-01,  1.1093e+00,  7.1792e-01],
        [-1.1629e+01, -5.9137e+00,  9.6126e-02,  1.0075e-01,  3.1050e+00,
          2.1355e+00,  9.6187e-02,  9.8645e-02,  3.5605e+00,  3.1898e+01,
          5.7073e+00,  2.1333e+00, -1.6326e+01],
        [-8.2729e+00, -5.2098e+00, -1.7338e-02, -1.8243e-02,  1.6093e+00,
          9.3318e-01, -1.7354e-02, -1.8047e-02,  2.1057e+00,  2.7636e+01,
          5.4719e+00, -9.6169e-01, -1.4272e+01],
        [-3.2538e-01, -3.0680e-01, -1.3540e-02, -1.3355e-02, -9.1686e-01,
         -8.6667e-01, -1.3538e-02, -1.3437e-02, -9.1723e-01, -2.7830e-02,
         -2.0099e-01, -6.7659e-01, -3.5016e-01],
        [-3.2538e-01, -3.0680e-01, -1.3540e-02, -1.3355e-02, -9.1686e-01,
         -8.6667e-01, -1.3538e-02, -1.3437e-02, -9.1723e-01, -2.7830e-02,
         -2.0099e-01, -6.7659e-01, -3.5016e-01],
        [ 6.5195e-01,  5.9441e-01,  1.0135e-02,  1.0229e-02,  1.4446e+00,
          1.2604e+00,  1.0137e-02,  1.0188e-02,  1.4449e+00,  9.8782e-02,
          4.2405e-01,  1.0905e+00,  7.0551e-01],
        [-8.5841e+00, -4.8091e+00, -4.3152e-02, -4.6935e-02,  1.8838e+00,
          1.0363e+00, -4.3209e-02, -4.5143e-02,  1.7399e+00,  2.7795e+01,
          5.3603e+00, -9.3107e-01, -1.4466e+01],
        [-3.2538e-01, -3.0680e-01, -1.3540e-02, -1.3355e-02, -9.1686e-01,
         -8.6667e-01, -1.3538e-02, -1.3437e-02, -9.1723e-01, -2.7830e-02,
         -2.0099e-01, -6.7659e-01, -3.5016e-01],
        [-3.2538e-01, -3.0680e-01, -1.3540e-02, -1.3355e-02, -9.1686e-01,
         -8.6667e-01, -1.3538e-02, -1.3437e-02, -9.1723e-01, -2.7830e-02,
         -2.0099e-01, -6.7659e-01, -3.5016e-01],
        [-3.2538e-01, -3.0680e-01, -1.3540e-02, -1.3355e-02, -9.1686e-01,
         -8.6667e-01, -1.3538e-02, -1.3437e-02, -9.1723e-01, -2.7829e-02,
         -2.0099e-01, -6.7659e-01, -3.5016e-01],
        [ 6.8018e-01,  6.2221e-01,  9.8116e-03,  9.9070e-03,  1.5072e+00,
          1.3071e+00,  9.8131e-03,  9.8650e-03,  1.5075e+00,  1.0221e-01,
          4.3144e-01,  1.1356e+00,  7.3536e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.9187, -0.9187,  1.4721,  3.3831,  2.2360, -0.9187, -0.9187,  1.4458,
         2.3474, -0.9187, -0.9187, -0.9187,  1.5084], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0261,  -0.0261,   3.5073,  -9.9719, -14.4398,  -0.0261,  -0.0261,
           3.0485, -13.8120,  -0.0261,  -0.0261,  -0.0261,   4.2641]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  0.9919,  12.7487],
        [ -2.6065,   0.0370],
        [ 11.8561,  10.2859],
        [ -4.7811,   0.5195],
        [ 14.8809,   0.7713],
        [ -2.3147,   0.1875],
        [  3.8342, -14.5919],
        [-11.9986,  14.0702],
        [ -2.4317,   0.2589],
        [-15.7101,  -8.0295],
        [ 18.0039,   7.1013],
        [ -1.5506,   2.4373],
        [-10.7590, -13.3341]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.1782,  -6.5481,  -2.8585,  -6.4118, -13.2089,  -6.6775,  -8.7211,
          8.7423,  -6.6119,  -4.0470,  -1.8796,  -7.6924,  -7.1921],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.7537e+00, -3.4886e-03,  3.2255e-01,  6.8219e-03,  6.2059e-01,
          4.6213e-04,  9.6074e-01,  2.4021e+00, -8.9646e-04,  5.6019e-01,
          2.0619e+00,  1.4704e-01,  1.0117e+00],
        [-2.3248e-01,  9.0492e-02, -1.0554e+01,  1.2627e-01, -3.5902e+00,
          1.6677e-02, -1.1778e+00,  4.5513e+00,  9.7500e-02,  3.9041e+00,
         -3.6334e+00,  8.0701e-02, -6.6032e+00],
        [ 4.4567e+00, -3.1680e-01, -1.6518e+00,  5.7480e-02, -2.9319e+01,
         -2.6097e-01, -3.6606e+00,  1.4138e+01, -2.6136e-01,  1.7834e+00,
          9.4577e-01,  1.0182e-01, -1.8793e+01],
        [-2.1587e+00, -2.9049e-02, -5.1752e-01, -2.2808e-02, -6.6170e-01,
         -2.8872e-02, -2.4281e+00, -1.0545e+00, -2.8728e-02, -3.4732e-01,
         -2.1571e+00, -2.1323e-01, -6.2456e-01],
        [-2.4551e+00,  9.4484e-03,  5.0474e-02, -2.0856e-03, -1.1079e-01,
          8.6260e-03, -1.9476e+00, -1.8505e+00,  8.0149e-03, -3.8970e-01,
         -1.9638e+00, -5.6123e-01, -1.0223e+00],
        [-2.1593e+00, -2.9008e-02, -5.1644e-01, -2.2797e-02, -6.6104e-01,
         -2.8837e-02, -2.4270e+00, -1.0553e+00, -2.8694e-02, -3.4741e-01,
         -2.1572e+00, -2.1377e-01, -6.2509e-01],
        [ 9.6620e+00,  1.0894e+00,  2.6929e+00,  5.0650e-01, -2.0324e+01,
          2.3329e-01, -1.4979e+01,  1.0387e+01,  5.8482e-02,  9.7785e+00,
         -1.3497e+00,  2.6660e+00, -1.6345e+01],
        [ 6.9721e+00, -1.0887e+00, -1.6223e+00, -1.2665e+00, -7.7247e+00,
         -1.2264e-01, -3.7248e+00,  4.2086e+00, -6.1332e-02,  1.0017e+01,
         -1.0733e+00, -4.6149e+00, -3.0449e+01],
        [-2.4810e+00,  1.7349e-02,  1.4749e-01,  4.1762e-03,  2.1453e-02,
          1.6568e-02, -1.8405e+00, -2.0638e+00,  1.5873e-02, -4.1397e-01,
         -1.8975e+00, -6.3554e-01, -1.1081e+00],
        [ 2.1890e+00, -3.7069e-01, -1.0053e+01, -1.9874e-01, -1.1570e+00,
         -4.1863e-01, -7.3157e-01, -3.1621e+00, -4.9438e-01, -4.6060e+00,
         -1.6220e+00, -9.7975e-02, -7.4458e+00],
        [-2.6817e+01,  3.3584e-01, -1.5175e+01,  2.6967e-01, -1.3777e+01,
          6.1780e-02,  1.5828e+00,  1.4508e-01,  2.8021e-01,  1.0523e+01,
         -1.0346e+01,  5.4742e-02,  9.7065e+00],
        [-1.3236e+01, -6.6862e-01,  2.3014e+00, -3.0612e-01, -1.9251e+00,
         -4.5793e-01,  1.8863e+00, -1.0039e+01, -7.5767e-01,  1.4840e+01,
         -3.4899e+00, -4.5892e-01,  4.0034e+00],
        [-1.9058e+01, -1.1738e-01, -2.6915e+00, -1.0202e-01, -1.8437e+01,
          1.3861e-01,  3.5468e-01, -3.0101e-02, -5.3325e-02,  5.5768e+00,
         -1.1093e+01, -2.4411e-02,  8.7566e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 4.8525, -1.8191,  0.3721, -4.7825, -4.6061, -4.7823, -9.5603,  0.3135,
        -4.5594, -2.4247, -0.5129,  1.3080, -2.7194], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 9.0362e-01,  4.7138e+00,  1.0003e+00, -2.6889e-01, -7.6767e-01,
         -2.6968e-01,  1.1384e-01,  2.8043e-01, -8.7358e-01,  4.2955e+00,
         -5.2086e+00, -3.0882e-01, -9.7631e+00],
        [ 3.8832e-01, -4.7253e+00,  4.7945e-01,  2.6890e-01,  7.6770e-01,
          2.6967e-01,  1.0227e+00, -4.6106e-01,  8.7363e-01, -4.0284e+00,
          6.1523e+00,  8.4892e-01,  1.0204e+01],
        [-1.2752e+01, -4.7522e-02, -1.0322e+00, -6.2093e-04, -2.4546e-03,
         -6.2117e-04, -3.1298e-02, -1.4407e+00, -3.1550e-03, -5.0777e-02,
         -1.1625e-02, -9.9269e-01,  2.5472e-04],
        [-1.6375e+00,  2.9555e+00, -6.3279e+00,  8.2375e-01,  7.0505e-01,
          8.2393e-01, -1.7728e+01, -3.5940e+00,  6.5098e-01,  1.1941e+01,
          1.5271e+00,  2.2016e+00,  1.1624e+01],
        [ 9.7446e+00, -2.6416e+00, -1.0110e+01, -2.7995e-01, -2.9130e-01,
         -2.7996e-01,  2.0567e+00,  6.9809e-01, -2.7569e-01, -5.4987e+00,
          1.5646e+01,  1.8403e+00, -1.0221e+00]], device='cuda:0'))])
xi:  [354.2585]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 493.7346816382711
W_T_median: 403.2491629698859
W_T_pctile_5: 353.82244637137774
W_T_CVAR_5_pct: 177.4660045362472
Average q (qsum/M+1):  43.36819950226815
Optimal xi:  [354.2585]
Expected(across Rb) median(across samples) p_equity:  0.16272929583501536
obj fun:  tensor(-10217.6296, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -2.7095,  12.9892],
        [ -8.6338,   9.4281],
        [ -0.9451,   0.4527],
        [ -0.8640,   0.3781],
        [  6.1895,  -0.6523],
        [  4.5754,  -0.4303],
        [ -0.9502,   0.4519],
        [ -0.9882,   0.4554],
        [  5.3775,  -0.2578],
        [  9.7529,  -0.6215],
        [-74.6407,   0.8966],
        [  5.4828,  -0.5841],
        [ -2.7219,  16.4087]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 7.2031,  5.6080, -3.1690, -3.3192,  2.5904,  3.5518, -3.1733, -3.1998,
         3.1859, -7.8767,  0.2375,  2.9803,  8.6844], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1333e-01,  1.6554e+00,  3.4528e-02,  3.0769e-02,  8.7976e-01,
          9.1042e-01,  3.5304e-02,  3.4772e-02,  8.9665e-01,  2.6447e-01,
          4.3032e-01,  8.9469e-01,  4.6686e-01],
        [-9.2749e+00, -6.0010e+00,  1.6064e-01,  2.4939e-01,  2.6538e+00,
          2.9059e+00,  1.6532e-01,  2.3862e-01,  2.6770e+00,  2.0542e+01,
          5.6439e+00,  2.9504e+00, -1.0311e+01],
        [-5.8631e+00, -5.5932e+00, -1.0882e-01, -2.5602e-01,  1.4267e+00,
          1.6964e+00, -8.6213e-02, -1.9126e-01,  1.4872e+00,  2.0896e+01,
          5.2545e+00,  1.4174e+00, -9.1947e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4955e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.0882e-01,  1.6256e+00,  3.4094e-02,  3.0485e-02,  8.6370e-01,
          8.9398e-01,  3.4826e-02,  3.4369e-02,  8.8039e-01,  2.5781e-01,
          4.1380e-01,  8.7846e-01,  4.6006e-01],
        [-6.2435e+00, -5.0774e+00, -7.9019e-02, -1.1667e-02,  1.6100e+00,
          1.6134e+00, -1.1318e-01, -9.1158e-02,  1.1232e+00,  2.1826e+01,
          5.7213e+00,  1.3367e+00, -9.6095e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4956e-03, -5.9415e-03, -4.7716e-01,
         -4.9046e-01, -6.4683e-03, -6.3263e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1923e-01,  1.6977e+00,  3.5087e-02,  3.1098e-02,  9.0167e-01,
          9.3283e-01,  3.5931e-02,  3.5276e-02,  9.1883e-01,  2.7424e-01,
          4.5388e-01,  9.1684e-01,  4.7618e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5021, -0.5021,  0.9316,  2.2107,  1.1816, -0.5021, -0.5021,  0.9149,
         1.2202, -0.5021, -0.5021, -0.5021,  0.9544], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0192,   0.0192,   3.5844,  -9.2131, -12.7066,   0.0192,   0.0192,
           3.1214, -12.3800,   0.0192,   0.0192,   0.0192,   4.3492]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  2.3638,  12.1836],
        [  5.6572,   7.4721],
        [  7.9224,   6.5803],
        [ -6.1235,   3.1910],
        [ 11.8466,   0.5913],
        [  0.8770,  -8.1352],
        [  8.6891,  -6.8360],
        [-10.5691,  10.8756],
        [  1.7568,  -0.0712],
        [-11.0991,  -4.2491],
        [ 11.4103,   5.2357],
        [ -0.1851,   2.9347],
        [ -7.5723,  -9.3505]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 6.3522, -5.1507, -2.0487, -4.3376, -9.8811, -6.7882, -6.3489,  5.9660,
        -7.1033, -1.4560, -0.7179, -3.6786, -6.0576], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.1852e+00,  1.5094e+00,  1.3526e+00, -5.4268e-01,  2.4223e+00,
          2.0759e+00,  9.2465e-01, -1.6151e+00, -2.8824e-01,  9.5287e-01,
          2.4642e-01, -1.1737e+00,  2.2713e+00],
        [-1.8651e+00, -5.6256e-01, -6.2129e-01, -4.2742e+00, -7.7059e-01,
         -3.4892e+00, -7.6918e-02,  6.7408e+00, -2.3096e-01,  1.3500e+00,
         -3.5500e+00, -6.0667e-01, -2.9353e+00],
        [ 1.1142e-01, -1.2473e+00,  8.1916e-01,  6.4910e-01, -1.6350e+01,
          2.2732e+00, -1.5779e+00,  7.2357e+00, -7.2982e-03,  4.4049e+00,
         -5.8977e-01,  1.7865e+00, -4.6946e+00],
        [-1.3960e+00, -8.7673e-02, -4.2780e-01,  6.7083e-03, -4.0893e-01,
         -9.1685e-01, -1.7927e+00, -5.0702e-01, -1.3463e-02, -5.2559e-01,
         -1.6941e+00,  1.8378e-03, -3.4962e-01],
        [-1.9520e+00,  1.8967e-01, -5.4411e-01, -1.2101e-01, -9.1077e-01,
         -1.2479e+00, -1.1267e+00, -9.1391e-01, -1.8068e-02, -3.8365e-01,
         -2.4970e+00, -1.1678e-01, -2.1655e-01],
        [-1.4057e+00, -8.6477e-02, -4.3293e-01,  1.5699e-02, -4.1127e-01,
         -9.1630e-01, -1.7894e+00, -5.0243e-01, -1.3300e-02, -5.1448e-01,
         -1.7010e+00,  1.2009e-02, -3.4096e-01],
        [ 1.3221e+01, -6.1845e-01,  1.5793e+00, -6.7508e+00, -1.5120e+01,
         -1.8714e+01, -8.9013e+00,  4.8410e+00, -1.2180e+00,  4.2963e+00,
          1.1099e+00,  1.4776e+00, -1.3629e+01],
        [ 4.1482e+00, -4.3534e-01, -7.3632e+00,  2.5918e-02, -1.7989e-01,
         -6.4858e+00, -8.9001e-01, -1.0932e+00,  1.0207e-01,  1.5231e+00,
         -7.6390e+00,  1.3168e-01, -1.0356e+01],
        [-1.4524e+00, -8.1858e-02, -4.5182e-01,  9.7058e-02, -4.0160e-01,
         -8.8229e-01, -1.8020e+00, -4.5301e-01, -1.2095e-02, -4.6125e-01,
         -1.7020e+00,  9.5630e-02, -3.0579e-01],
        [-2.8522e+00, -1.9497e-01, -1.6674e+00, -4.5101e-01, -1.9345e+00,
         -2.7298e+00,  5.5568e-01,  1.5671e+00, -1.2880e-02,  9.4602e-01,
         -2.4953e+00, -2.9629e-01, -2.6501e+00],
        [-2.4334e+01, -6.7212e-02, -4.8855e+00,  1.9330e-01, -7.7025e+00,
          2.7805e+00,  4.4028e-01, -1.4406e+00, -4.7447e-02,  6.3915e+00,
         -8.7817e+00, -4.8862e-01,  5.4980e+00],
        [-1.0961e+01,  5.4093e-01,  8.6739e-01, -6.0412e-02, -3.7470e+00,
          2.1004e+00,  2.4607e+00, -2.1072e+01, -1.4633e-01,  9.1990e+00,
         -5.3680e+00,  1.5515e-01,  4.1397e+00],
        [-1.2571e+01,  2.5529e-02, -4.2095e+00,  2.0473e-01, -1.3435e+01,
          2.3688e+00, -6.0068e-01, -5.7139e+00, -3.7530e-01,  4.5355e+00,
         -9.0221e+00,  3.5582e-02,  2.3988e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 2.9079, -2.1828, -0.1738, -3.3916, -2.9699, -3.3962, -3.0437, -0.0264,
        -3.4166, -2.8315, -0.3684,  1.2132,  0.0742], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 7.4891e-01,  4.5452e+00,  1.8769e+00,  4.5914e-02,  3.5496e-01,
         -9.1354e-02,  2.7526e-01, -9.7636e-01, -1.0948e-01,  3.3561e+00,
         -4.4478e+00, -1.2550e-01, -1.0160e+00],
        [ 5.4131e-01, -4.5533e+00, -3.9331e-01,  2.2764e-01, -3.5322e-01,
          9.4308e-02,  8.6633e-01,  7.9627e-01,  1.1223e-01, -3.0675e+00,
          5.4099e+00,  6.6530e-01,  1.5354e+00],
        [-1.0256e+01, -4.0894e-02, -1.1152e+00, -1.1287e-02, -1.0215e-02,
         -1.1185e-02, -8.9347e-01, -1.4354e-01, -1.1272e-02, -1.5607e-02,
          8.3038e-03, -6.6567e-01,  1.3450e-03],
        [-1.2086e+00, -1.8795e+00, -3.9948e+00,  3.7431e-01,  1.3870e+00,
          3.8828e-01,  9.5588e-01, -1.2326e+01,  4.2495e-01,  2.2745e+00,
          8.1236e-01,  1.8802e+00,  1.0604e+01],
        [ 9.1296e+00, -4.1802e+00, -1.1657e+01,  2.0360e-01,  4.0986e-01,
          2.0817e-01,  5.1155e+00, -6.9774e-01,  2.1607e-01, -5.1843e-01,
          6.2533e+00,  8.3596e+00,  2.6055e+00]], device='cuda:0'))])
loaded xi:  315.55313
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -174.40453954980057
Current xi:  [328.72275]
objective value function right now is: -174.40453954980057
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [340.0464]
objective value function right now is: -173.23931576420065
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -175.79363020089974
Current xi:  [345.2679]
objective value function right now is: -175.79363020089974
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -176.3921059406449
Current xi:  [349.4294]
objective value function right now is: -176.3921059406449
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.1006]
objective value function right now is: -176.2535318891518
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -176.8054769946647
Current xi:  [352.83612]
objective value function right now is: -176.8054769946647
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [353.9571]
objective value function right now is: -176.67129184805472
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.9219]
objective value function right now is: -176.61066577620144
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.442]
objective value function right now is: -176.50959633924242
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.2655]
objective value function right now is: -176.12983241550322
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.74945]
objective value function right now is: -175.80547039255268
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.4165]
objective value function right now is: -175.94460899380496
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -176.82584273942584
Current xi:  [355.5706]
objective value function right now is: -176.82584273942584
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [355.5653]
objective value function right now is: -176.43013798448757
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [356.42313]
objective value function right now is: -175.0994621548268
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.89264]
objective value function right now is: -175.36199445252353
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.77963]
objective value function right now is: -176.27354402431763
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -177.2229326789941
Current xi:  [355.8344]
objective value function right now is: -177.2229326789941
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.5786]
objective value function right now is: -177.1121351611289
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [357.03772]
objective value function right now is: -177.06803023623485
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.9977]
objective value function right now is: -176.44192951516743
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.47778]
objective value function right now is: -175.6275004912377
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.8791]
objective value function right now is: -177.08366075033183
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.43982]
objective value function right now is: -176.0043424539303
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.89706]
objective value function right now is: -175.68063376858382
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.87094]
objective value function right now is: -175.22314757537066
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.55408]
objective value function right now is: -176.50719029118926
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [354.28748]
objective value function right now is: -176.72630429882352
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [355.32513]
objective value function right now is: -176.08849431920984
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -177.40357141508105
Current xi:  [356.24258]
objective value function right now is: -177.40357141508105
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [357.6534]
objective value function right now is: -176.43134862668876
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [356.35104]
objective value function right now is: -177.01801452736353
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [356.72757]
objective value function right now is: -177.27785575178626
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [358.36032]
objective value function right now is: -176.36304796713696
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [357.23593]
objective value function right now is: -176.05121827688464
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -177.74523641643665
Current xi:  [357.50317]
objective value function right now is: -177.74523641643665
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [357.28775]
objective value function right now is: -177.50781391456684
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -177.79408389877145
Current xi:  [357.42648]
objective value function right now is: -177.79408389877145
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [357.22162]
objective value function right now is: -177.61677488461018
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -177.8803746016302
Current xi:  [357.2507]
objective value function right now is: -177.8803746016302
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [357.17215]
objective value function right now is: -177.72719826567027
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [356.83032]
objective value function right now is: -177.60570133711508
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [356.4719]
objective value function right now is: -177.65474081350084
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -177.9504264771086
Current xi:  [356.28958]
objective value function right now is: -177.9504264771086
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [356.0111]
objective value function right now is: -177.81950276956033
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [356.13147]
objective value function right now is: -177.93830493080307
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.9113]
objective value function right now is: -177.786144978204
new min fval from sgd:  -177.96037055509817
new min fval from sgd:  -177.96052739422703
new min fval from sgd:  -177.96624592167873
new min fval from sgd:  -177.97558926166997
new min fval from sgd:  -177.98234857078089
new min fval from sgd:  -177.98872917541004
new min fval from sgd:  -177.9975797648255
new min fval from sgd:  -178.0058952160919
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.91992]
objective value function right now is: -177.89127361609505
new min fval from sgd:  -178.00735020501452
new min fval from sgd:  -178.00923756041877
new min fval from sgd:  -178.0109829602794
new min fval from sgd:  -178.0116344092515
new min fval from sgd:  -178.02888817990183
new min fval from sgd:  -178.0385434149995
new min fval from sgd:  -178.0438012384268
new min fval from sgd:  -178.04998663742515
new min fval from sgd:  -178.05348858607636
new min fval from sgd:  -178.06229789752817
new min fval from sgd:  -178.06434524781008
new min fval from sgd:  -178.064774589621
new min fval from sgd:  -178.06691741044116
new min fval from sgd:  -178.0678115581536
new min fval from sgd:  -178.06863150859712
new min fval from sgd:  -178.0692930562168
new min fval from sgd:  -178.0703528413952
new min fval from sgd:  -178.07164530994055
new min fval from sgd:  -178.0722635899111
new min fval from sgd:  -178.07477231737982
new min fval from sgd:  -178.07599308458578
new min fval from sgd:  -178.07609705255123
new min fval from sgd:  -178.07642782977715
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.96466]
objective value function right now is: -177.9990169217045
new min fval from sgd:  -178.07733680325754
new min fval from sgd:  -178.07854770762614
new min fval from sgd:  -178.07862471298967
new min fval from sgd:  -178.07927881471244
new min fval from sgd:  -178.08116230843612
new min fval from sgd:  -178.0815775413973
new min fval from sgd:  -178.08171938804426
new min fval from sgd:  -178.08204984402315
new min fval from sgd:  -178.08243537621894
new min fval from sgd:  -178.08498294557336
new min fval from sgd:  -178.0882751917235
new min fval from sgd:  -178.09107240836357
new min fval from sgd:  -178.0922231419794
new min fval from sgd:  -178.09339622758722
new min fval from sgd:  -178.09535358026903
new min fval from sgd:  -178.09790994826082
new min fval from sgd:  -178.09990660782282
new min fval from sgd:  -178.1018164405598
new min fval from sgd:  -178.10284956426236
new min fval from sgd:  -178.10302268431616
new min fval from sgd:  -178.1033884350416
new min fval from sgd:  -178.10471381134906
new min fval from sgd:  -178.10614622343385
new min fval from sgd:  -178.1067657025683
new min fval from sgd:  -178.10735426611325
new min fval from sgd:  -178.10876181408472
new min fval from sgd:  -178.1096278302907
new min fval from sgd:  -178.10997355337744
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [356.02258]
objective value function right now is: -178.0994384040686
min fval:  -178.10997355337744
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.0958, -0.1421],
        [ 0.0958, -0.1421],
        [ 0.0958, -0.1421],
        [ 0.0958, -0.1421],
        [ 0.0958, -0.1421],
        [ 0.0958, -0.1421],
        [ 0.0958, -0.1421],
        [ 0.0958, -0.1421],
        [ 0.0958, -0.1421],
        [ 0.0958, -0.1421],
        [ 0.0958, -0.1421],
        [ 0.0958, -0.1421],
        [ 0.0958, -0.1421]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783, 0.1783,
        0.1783, 0.1783, 0.1783, 0.1783], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813,
         0.1813, 0.1813, 0.1813, 0.1813],
        [0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813,
         0.1813, 0.1813, 0.1813, 0.1813],
        [0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813,
         0.1813, 0.1813, 0.1813, 0.1813],
        [0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813,
         0.1813, 0.1813, 0.1813, 0.1813],
        [0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813,
         0.1813, 0.1813, 0.1813, 0.1813],
        [0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813,
         0.1813, 0.1813, 0.1813, 0.1813],
        [0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813,
         0.1813, 0.1813, 0.1813, 0.1813],
        [0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813,
         0.1813, 0.1813, 0.1813, 0.1813],
        [0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813,
         0.1813, 0.1813, 0.1813, 0.1813],
        [0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813,
         0.1813, 0.1813, 0.1813, 0.1813],
        [0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813,
         0.1813, 0.1813, 0.1813, 0.1813],
        [0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813,
         0.1813, 0.1813, 0.1813, 0.1813],
        [0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813, 0.1813,
         0.1813, 0.1813, 0.1813, 0.1813]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([0.3106, 0.3106, 0.3106, 0.3106, 0.3106, 0.3106, 0.3106, 0.3106, 0.3106,
        0.3106, 0.3106, 0.3106, 0.3106], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.4295, -1.4295, -1.4295, -1.4295, -1.4295, -1.4295, -1.4295, -1.4295,
         -1.4295, -1.4295, -1.4295, -1.4295, -1.4295]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.4525,  11.8955],
        [ -1.5367,   0.4991],
        [ -1.4924,   0.5254],
        [ -1.8206,   2.0121],
        [ 13.6975,   0.4983],
        [ -1.4388,   0.5505],
        [  9.5365,  -6.6489],
        [-10.6330,  13.2941],
        [ -1.4400,   1.1003],
        [-12.5832,  -7.2534],
        [ 14.7530,   6.2973],
        [ -1.4626,   0.5490],
        [-10.7371, -12.9022]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.0248,  -3.6999,  -3.6897,  -3.5907, -11.5575,  -3.7036, -12.0322,
          7.7772,  -3.3056,  -3.9723,  -1.9509,  -3.6814,  -7.0644],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 9.4073e-01,  6.0766e-02,  8.4205e-02,  2.5033e-01,  7.9457e-01,
          1.1842e-01,  8.8246e-01, -1.9088e-01,  2.3122e-01, -2.2138e+00,
          1.3376e+00,  1.1593e-01, -1.2824e+00],
        [-1.0998e+00,  6.1912e-02,  8.7156e-02,  8.7048e-02, -2.3452e-01,
          1.0725e-01, -2.9070e+00, -9.2375e-01, -2.4856e-02,  4.9160e-01,
         -5.6476e+00,  1.1290e-01, -1.8543e+00],
        [ 6.2178e+00,  2.0932e-01,  2.7378e-01,  8.2398e-01, -2.0734e+01,
          4.1589e-01, -1.4341e-01,  1.0759e+01,  7.2350e-01,  2.2428e+00,
          7.9844e-01,  3.9193e-01, -1.3558e+01],
        [-3.0620e-01, -7.3385e-02, -7.5752e-02, -1.5082e-01, -7.3672e-01,
         -7.3896e-02, -9.0489e-01, -2.3670e-01, -1.8423e-01,  3.3605e-01,
         -2.0307e+00, -7.5925e-02,  4.4289e-01],
        [-3.0620e-01, -7.3385e-02, -7.5752e-02, -1.5082e-01, -7.3672e-01,
         -7.3896e-02, -9.0489e-01, -2.3670e-01, -1.8423e-01,  3.3605e-01,
         -2.0307e+00, -7.5926e-02,  4.4289e-01],
        [-3.0620e-01, -7.3385e-02, -7.5752e-02, -1.5082e-01, -7.3672e-01,
         -7.3896e-02, -9.0489e-01, -2.3670e-01, -1.8423e-01,  3.3605e-01,
         -2.0307e+00, -7.5925e-02,  4.4289e-01],
        [ 9.2680e+00, -6.4943e-02, -8.0579e-02, -6.6947e+00, -1.0688e+01,
         -1.1337e-01, -1.5241e+00,  6.0373e+00, -5.1160e-01,  8.4560e-02,
          2.5250e+00, -1.0532e-01, -1.2772e+01],
        [ 2.7991e+00,  4.4665e-01,  4.4362e-01,  6.3445e-01, -1.7534e+01,
          2.2202e-01,  3.2313e+00,  4.3868e+00,  7.5791e-01,  4.7164e+00,
         -9.6594e-02,  2.7912e-01, -5.7585e+00],
        [-3.0620e-01, -7.3385e-02, -7.5752e-02, -1.5082e-01, -7.3672e-01,
         -7.3896e-02, -9.0489e-01, -2.3670e-01, -1.8423e-01,  3.3605e-01,
         -2.0307e+00, -7.5925e-02,  4.4289e-01],
        [-3.0661e-01, -7.3318e-02, -7.5681e-02, -1.5078e-01, -7.3647e-01,
         -7.3824e-02, -9.0480e-01, -2.3679e-01, -1.8416e-01,  3.3552e-01,
         -2.0306e+00, -7.5853e-02,  4.4246e-01],
        [-1.5256e+01,  8.0013e-02,  7.7590e-02, -7.5374e-02, -8.0356e+00,
          4.7064e-02, -8.7262e-01,  1.9884e+00,  2.5184e-02,  7.0206e+00,
         -5.2843e+00,  4.9435e-02,  1.0237e+01],
        [-7.6042e+00,  1.0246e-01,  9.3312e-02,  1.1120e-01, -2.2351e+00,
          7.0093e-02,  2.3033e+00, -8.1482e+00,  2.6898e-01,  7.8234e+00,
         -2.7330e+00,  7.2254e-02,  5.4696e+00],
        [-9.3318e+00,  7.2888e-05,  3.1023e-02, -3.1434e-02, -7.7239e+00,
          5.2829e-02, -2.3284e+00,  9.3876e-01, -8.0881e-03,  3.5291e+00,
         -1.0371e+01,  4.4866e-02,  9.4804e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 4.0767, -1.9459, -2.0844, -4.1590, -4.1590, -4.1590, -4.7094, -3.9055,
        -4.1590, -4.1591, -2.1132,  1.3478, -3.8933], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 9.7498e-01,  2.6362e+00,  7.9329e-01, -5.9472e-02, -5.9472e-02,
         -5.9473e-02,  9.3846e-01,  3.8547e+00, -5.9473e-02, -5.9588e-02,
         -3.3052e+00, -2.9663e-01, -4.3787e+00],
        [ 3.8700e-01, -2.6282e+00,  6.5135e-01,  6.5690e-02,  6.5689e-02,
          6.5690e-02,  1.9264e-01, -3.8770e+00,  6.5690e-02,  6.5807e-02,
          3.7919e+00,  8.4822e-01,  4.3706e+00],
        [-8.9821e+00, -6.7732e-03, -8.3517e-01, -8.5162e-03, -8.5162e-03,
         -8.5162e-03, -1.1870e+00, -1.1596e-01, -8.5162e-03, -8.5137e-03,
          3.5152e-02, -1.2587e+00,  1.0320e-02],
        [-1.9498e+00,  3.0012e+00, -1.0970e+00,  7.4055e-01,  7.4055e-01,
          7.4055e-01, -8.8323e+00, -2.6387e+00,  7.4055e-01,  7.4008e-01,
          1.1947e+00,  2.3097e+00,  9.8957e+00],
        [ 8.8264e+00,  5.7114e-01, -9.6372e+00, -4.4999e-01, -4.4999e-01,
         -4.4999e-01,  9.9367e-01, -2.1020e+00, -4.4999e-01, -4.4973e-01,
         -6.2680e-01,  7.5084e+00, -1.7184e+00]], device='cuda:0'))])
xi:  [356.0261]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 531.7814144230615
W_T_median: 524.0121823102764
W_T_pctile_5: 355.56051020728125
W_T_CVAR_5_pct: 178.1118242389635
Average q (qsum/M+1):  35.0
Optimal xi:  [356.0261]
Expected(across Rb) median(across samples) p_equity:  0.08249795103862805
obj fun:  tensor(-178.1100, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 999.0
-----------------------------------------------
