/home/marcchen/Documents/constrain_factor/researchcode/exp_config_json_files/multi_portfolio_TEST1_EFs.json
Starting at: 
06-08-23_10:31

 numpy seed:  3  


 pytorch seed:  3  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.005383     0.031411
192608                    0.0319              0.0561  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0355  ...     0.005323    -0.028996
192611                   -0.0038              0.0294  ...     0.005303     0.028554

[5 rows x 6 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.043289    -0.036240
202209                   -0.0955             -0.0871  ...    -0.050056    -0.091324
202210                    0.0883              0.1486  ...    -0.014968     0.077403
202211                   -0.0076              0.0462  ...     0.040789     0.052365
202212                   -0.0457             -0.0499  ...    -0.018566    -0.057116

[5 rows x 6 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'CPI_nom_ret_ind', 'T30_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Value_Hi30_real_ret
T30_real_ret             1.000000  ...             0.018239
B10_real_ret             0.351722  ...             0.031301
VWD_real_ret             0.068448  ...             0.909335
Size_Lo30_real_ret       0.014412  ...             0.908542
Value_Hi30_real_ret      0.018239  ...             1.000000

[5 rows x 5 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       5       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       5              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 5)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 1500, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1734.2999240676877
Current xi:  [81.727066]
objective value function right now is: -1734.2999240676877
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.6796503593284
Current xi:  [61.157047]
objective value function right now is: -1741.6796503593284
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1746.837567963611
Current xi:  [39.337975]
objective value function right now is: -1746.837567963611
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1751.1271490909369
Current xi:  [16.833477]
objective value function right now is: -1751.1271490909369
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1756.0658564130067
Current xi:  [-3.3109794]
objective value function right now is: -1756.0658564130067
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1757.9715413613685
Current xi:  [-21.386465]
objective value function right now is: -1757.9715413613685
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1759.9862758827494
Current xi:  [-39.54191]
objective value function right now is: -1759.9862758827494
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1761.2880045735567
Current xi:  [-58.307186]
objective value function right now is: -1761.2880045735567
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1762.2165259605567
Current xi:  [-77.13406]
objective value function right now is: -1762.2165259605567
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1764.0387703502126
Current xi:  [-96.7218]
objective value function right now is: -1764.0387703502126
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1765.0698417652395
Current xi:  [-114.7502]
objective value function right now is: -1765.0698417652395
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1765.7759900674982
Current xi:  [-133.86313]
objective value function right now is: -1765.7759900674982
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1766.5613118010285
Current xi:  [-151.61116]
objective value function right now is: -1766.5613118010285
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1767.3928415229261
Current xi:  [-170.27405]
objective value function right now is: -1767.3928415229261
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1768.311448185435
Current xi:  [-187.48251]
objective value function right now is: -1768.311448185435
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1768.4232447420477
Current xi:  [-205.01085]
objective value function right now is: -1768.4232447420477
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1769.3287321830728
Current xi:  [-221.74901]
objective value function right now is: -1769.3287321830728
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1769.5430404544913
Current xi:  [-237.94388]
objective value function right now is: -1769.5430404544913
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1769.7094728863026
Current xi:  [-253.98485]
objective value function right now is: -1769.7094728863026
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1769.9278725232891
Current xi:  [-268.45947]
objective value function right now is: -1769.9278725232891
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1770.1919245911458
Current xi:  [-282.51703]
objective value function right now is: -1770.1919245911458
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-294.64563]
objective value function right now is: -1769.8942269598883
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1773.2771369683896
Current xi:  [-310.47513]
objective value function right now is: -1773.2771369683896
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-322.28418]
objective value function right now is: -1773.1346406251305
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1773.455687188923
Current xi:  [-331.38287]
objective value function right now is: -1773.455687188923
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1773.4910671517375
Current xi:  [-338.82477]
objective value function right now is: -1773.4910671517375
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1773.7367157416047
Current xi:  [-346.93497]
objective value function right now is: -1773.7367157416047
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-349.32867]
objective value function right now is: -1773.6840188905583
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-352.2605]
objective value function right now is: -1773.5728770060275
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1773.7676656183894
Current xi:  [-354.27982]
objective value function right now is: -1773.7676656183894
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-353.5231]
objective value function right now is: -1773.7448517095859
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-355.1661]
objective value function right now is: -1773.4767254713763
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1773.8741611350051
Current xi:  [-355.64944]
objective value function right now is: -1773.8741611350051
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-355.36765]
objective value function right now is: -1773.5727827109704
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-355.25128]
objective value function right now is: -1773.751183526646
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1774.0798415932752
Current xi:  [-355.45544]
objective value function right now is: -1774.0798415932752
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-354.5189]
objective value function right now is: -1774.0288395618713
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-354.63745]
objective value function right now is: -1773.8774283709613
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1774.1096886499088
Current xi:  [-354.35022]
objective value function right now is: -1774.1096886499088
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-354.19196]
objective value function right now is: -1774.095487841604
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-353.63556]
objective value function right now is: -1774.0346907203505
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-353.11398]
objective value function right now is: -1773.987022812037
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1774.1352593165636
Current xi:  [-353.02945]
objective value function right now is: -1774.1352593165636
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-352.89496]
objective value function right now is: -1773.7094095522666
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-352.67444]
objective value function right now is: -1774.071637374343
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-352.137]
objective value function right now is: -1774.0876579735027
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-351.90756]
objective value function right now is: -1774.103068802875
new min fval from sgd:  -1774.148204758544
new min fval from sgd:  -1774.1500632157363
new min fval from sgd:  -1774.1553390130164
new min fval from sgd:  -1774.159317199886
new min fval from sgd:  -1774.161479422276
new min fval from sgd:  -1774.1625098557795
new min fval from sgd:  -1774.1625554894354
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-351.75568]
objective value function right now is: -1774.1219407033823
new min fval from sgd:  -1774.1627896061996
new min fval from sgd:  -1774.16319355292
new min fval from sgd:  -1774.163579706275
new min fval from sgd:  -1774.1637928262196
new min fval from sgd:  -1774.1638055947776
new min fval from sgd:  -1774.1646521035927
new min fval from sgd:  -1774.1652306489
new min fval from sgd:  -1774.1653488842585
new min fval from sgd:  -1774.1666757263104
new min fval from sgd:  -1774.1676018975568
new min fval from sgd:  -1774.1677784930505
new min fval from sgd:  -1774.1683667453804
new min fval from sgd:  -1774.168925933056
new min fval from sgd:  -1774.1693169148023
new min fval from sgd:  -1774.1695841021092
new min fval from sgd:  -1774.1699481714122
new min fval from sgd:  -1774.1702772885046
new min fval from sgd:  -1774.1703748583636
new min fval from sgd:  -1774.1707258687256
new min fval from sgd:  -1774.1709866405265
new min fval from sgd:  -1774.1717939860425
new min fval from sgd:  -1774.1726965399819
new min fval from sgd:  -1774.1727508697843
new min fval from sgd:  -1774.173010872362
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-351.6926]
objective value function right now is: -1774.1601444480257
new min fval from sgd:  -1774.1732393668053
new min fval from sgd:  -1774.173812526701
new min fval from sgd:  -1774.1738517833674
new min fval from sgd:  -1774.1739764774159
new min fval from sgd:  -1774.1744017362846
new min fval from sgd:  -1774.174692482967
new min fval from sgd:  -1774.1749857871798
new min fval from sgd:  -1774.1752592552398
new min fval from sgd:  -1774.1752965971395
new min fval from sgd:  -1774.1755301580438
new min fval from sgd:  -1774.1755497356412
new min fval from sgd:  -1774.1756644778047
new min fval from sgd:  -1774.1757481819918
new min fval from sgd:  -1774.1763236765203
new min fval from sgd:  -1774.1771112390288
new min fval from sgd:  -1774.177427422981
new min fval from sgd:  -1774.1778461919528
new min fval from sgd:  -1774.1782309592702
new min fval from sgd:  -1774.1785869157413
new min fval from sgd:  -1774.179031571568
new min fval from sgd:  -1774.179115025064
new min fval from sgd:  -1774.1793152218956
new min fval from sgd:  -1774.1794767773613
new min fval from sgd:  -1774.1794809010833
new min fval from sgd:  -1774.1797065962503
new min fval from sgd:  -1774.1800387043813
new min fval from sgd:  -1774.1803081027786
new min fval from sgd:  -1774.1804108430754
new min fval from sgd:  -1774.1806057416786
new min fval from sgd:  -1774.1809902240323
new min fval from sgd:  -1774.1815714807703
new min fval from sgd:  -1774.1817834117721
new min fval from sgd:  -1774.1818444412904
new min fval from sgd:  -1774.182088208152
new min fval from sgd:  -1774.1824279730936
new min fval from sgd:  -1774.1828812903907
new min fval from sgd:  -1774.182980821393
new min fval from sgd:  -1774.1833633593562
new min fval from sgd:  -1774.1835721569005
new min fval from sgd:  -1774.1836200461114
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-351.6287]
objective value function right now is: -1774.1551227035584
min fval:  -1774.1836200461114
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.4597,  3.2625],
        [-0.3560,  1.2194],
        [-0.3560,  1.2194],
        [-0.3560,  1.2194],
        [-0.3560,  1.2194],
        [12.7338,  1.6196],
        [-0.3499,  1.2390],
        [-1.2706,  7.6130],
        [-0.3560,  1.2194],
        [ 9.5948,  0.8040],
        [-0.3553,  1.2217],
        [10.0875,  1.1012],
        [-0.6036,  7.5266]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 2.3582, -0.7418, -0.7418, -0.7418, -0.7418, -7.4562, -0.7460, 10.9451,
        -0.7418, -7.1454, -0.7440, -6.9494, 11.0085], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 4.2792e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03,
         -1.7373e-01, -1.7909e-03, -2.7790e-02, -2.2436e-03, -8.5478e-02,
         -2.1735e-03, -8.9012e-02, -5.8238e-02],
        [-2.2470e-01, -6.2390e-02, -6.2390e-02, -6.2390e-02, -6.2390e-02,
         -4.0864e+00,  1.9790e-02, -3.9552e+00, -6.2390e-02, -1.3823e+00,
         -2.5244e-02, -1.3947e+00, -4.0799e+00],
        [ 4.2791e-03, -2.2437e-03, -2.2437e-03, -2.2437e-03, -2.2437e-03,
         -1.7374e-01, -1.7909e-03, -2.7791e-02, -2.2437e-03, -8.5481e-02,
         -2.1736e-03, -8.9016e-02, -5.8242e-02],
        [ 7.0344e-01,  2.7685e-01,  2.7685e-01,  2.7685e-01,  2.7685e-01,
          3.0395e-01,  2.4323e-01,  1.9913e+00,  2.7685e-01,  2.0024e-01,
          2.7120e-01,  1.6256e-01,  1.8469e+00],
        [ 4.2792e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03,
         -1.7373e-01, -1.7909e-03, -2.7790e-02, -2.2436e-03, -8.5478e-02,
         -2.1735e-03, -8.9012e-02, -5.8238e-02],
        [ 5.3863e-01, -6.8714e-02, -6.8714e-02, -6.8713e-02, -6.8714e-02,
          6.1585e+00,  6.6109e-03,  6.6320e+00, -6.8713e-02,  2.4574e+00,
         -3.1630e-02,  2.4337e+00,  6.5764e+00],
        [ 4.2792e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03,
         -1.7373e-01, -1.7909e-03, -2.7790e-02, -2.2436e-03, -8.5478e-02,
         -2.1735e-03, -8.9012e-02, -5.8238e-02],
        [ 4.2792e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03,
         -1.7373e-01, -1.7909e-03, -2.7790e-02, -2.2436e-03, -8.5478e-02,
         -2.1736e-03, -8.9012e-02, -5.8238e-02],
        [ 4.2792e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03,
         -1.7373e-01, -1.7909e-03, -2.7790e-02, -2.2436e-03, -8.5478e-02,
         -2.1736e-03, -8.9012e-02, -5.8238e-02],
        [ 4.2792e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03,
         -1.7373e-01, -1.7909e-03, -2.7790e-02, -2.2436e-03, -8.5478e-02,
         -2.1735e-03, -8.9012e-02, -5.8238e-02],
        [-3.3321e-01,  6.5519e-03,  6.5517e-03,  6.5526e-03,  6.5519e-03,
         -5.4764e+00,  4.6396e-02, -5.7181e+00,  6.5520e-03, -2.0994e+00,
          3.1393e-02, -2.0956e+00, -5.7795e+00],
        [ 4.2792e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03,
         -1.7373e-01, -1.7909e-03, -2.7790e-02, -2.2436e-03, -8.5478e-02,
         -2.1735e-03, -8.9012e-02, -5.8238e-02],
        [ 4.2792e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03,
         -1.7373e-01, -1.7909e-03, -2.7790e-02, -2.2436e-03, -8.5478e-02,
         -2.1735e-03, -8.9012e-02, -5.8238e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.6710,  3.6008, -0.6710, -2.4764, -0.6710, -5.9962, -0.6710, -0.6710,
        -0.6710, -0.6710,  5.1983, -0.6710, -0.6710], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 7.8654e-03, -5.3701e+00,  7.8658e-03,  2.8330e+00,  7.8655e-03,
          1.1939e+01,  7.8655e-03,  7.8654e-03,  7.8654e-03,  7.8655e-03,
         -8.6780e+00,  7.8654e-03,  7.8655e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.1290,   2.2354],
        [  9.9578,  -0.2012],
        [ -9.2052,  -2.3352],
        [ -7.0765,  -5.4092],
        [  9.1141,   1.4405],
        [ -2.9863,  -8.3056],
        [  8.8256,  -0.4319],
        [-13.3526,  -7.7232],
        [  8.4474,  -0.5845],
        [ -4.7547,  -7.2172],
        [ -7.3572,   5.6033],
        [ -3.9218,   8.7993],
        [ -3.9879,  -7.2170]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-2.0335, -9.7274, -0.3503, -2.9623, -2.4352, -5.7432, -8.6229, -6.8084,
        -8.0845, -6.0544,  5.5830,  7.7284, -7.3971], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 7.5467e-03, -4.4422e-01,  6.5127e-02,  2.6291e-02, -1.6036e+00,
         -5.8891e-01, -5.2200e-01,  1.0746e-01, -6.1075e-01, -3.2098e-02,
         -3.7434e-01, -5.8369e-01,  9.8300e-02],
        [ 3.1961e-01,  5.1040e+00, -2.7372e+00, -2.0388e+00,  6.3792e+00,
         -4.1245e+00,  3.5451e+00, -4.2381e+00,  3.1308e+00, -3.6596e+00,
          5.3024e+00,  1.0497e+01, -2.6777e+00],
        [ 7.5468e-03, -4.4422e-01,  6.5127e-02,  2.6291e-02, -1.6036e+00,
         -5.8891e-01, -5.2200e-01,  1.0746e-01, -6.1075e-01, -3.2098e-02,
         -3.7434e-01, -5.8369e-01,  9.8300e-02],
        [ 2.1164e-01,  4.9299e+00,  5.2987e+00, -2.0819e+00,  4.1222e+00,
         -4.0632e+00,  3.3278e+00,  1.6897e-01,  3.0269e+00, -3.5214e+00,
          3.8367e+00,  2.3122e+00, -2.2311e+00],
        [ 2.6406e-02,  1.9239e+00, -3.1235e-01, -5.1968e-01,  2.1328e+00,
          5.6777e-01,  2.0834e+00, -1.5420e+00,  2.1744e+00, -8.1908e-01,
          1.3811e+00,  1.1207e+00, -1.3606e+00],
        [ 7.4358e-03, -4.3888e-01,  6.2620e-02,  2.3147e-02, -1.6031e+00,
         -5.8790e-01, -5.1565e-01,  9.9726e-02, -6.0358e-01, -3.6817e-02,
         -3.7323e-01, -5.7987e-01,  9.1547e-02],
        [ 2.1068e-01,  5.9890e+00, -1.3896e+00,  1.0300e+00,  2.4980e+00,
          4.7783e+00,  5.5445e+00, -3.7709e+00,  5.5781e+00,  1.1631e-01,
         -2.3171e+00, -3.7621e+00, -5.6633e+00],
        [ 1.6957e+00, -1.2258e-01,  2.1276e+00,  2.2077e-01, -4.7749e+00,
         -2.0187e+00, -8.6316e-01,  3.9724e+00, -1.1398e+00, -1.8733e-01,
          2.1761e+00,  1.4296e+00,  2.7197e+00],
        [-2.4693e-01,  4.0556e+00, -2.5084e+00,  6.1702e-01,  1.5856e-01,
         -1.3050e+00,  4.8247e+00, -2.2546e+00,  5.1231e+00,  1.2432e+00,
         -4.0753e+00, -4.4670e+00,  1.9708e+00],
        [-1.2973e-01, -3.9569e-01,  5.4164e+00, -2.1065e-01, -8.2201e+00,
         -2.3439e+00, -1.2509e+00,  7.2642e+00, -1.6475e+00,  1.0710e+00,
         -4.4928e+00, -3.1289e+00,  3.4475e+00],
        [ 3.3464e-02, -2.7973e+00, -1.1532e+00,  8.3467e-01, -9.4756e-01,
         -3.8722e-01, -2.7950e+00,  2.7091e+00, -2.8292e+00,  1.7928e+00,
         -2.3524e+00, -8.3484e-01,  2.6360e+00],
        [ 9.7518e-01,  7.4936e+00, -4.0896e-01, -8.8524e-01, -3.0205e+00,
         -2.7676e+00,  5.0123e+00, -2.6903e+00,  4.3130e+00, -5.1447e-01,
          1.4774e+00, -1.5497e+00,  1.4762e-01],
        [-2.0422e-01,  2.2351e+00, -1.1835e+00,  3.9131e+00, -3.9204e+00,
          4.8356e+00,  1.8979e+00, -3.9875e+00,  1.6485e+00,  4.6392e+00,
         -5.5804e+00, -8.8920e+00,  2.7096e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.6784,  2.0250, -1.6784, -1.3038,  2.5627, -1.6775,  0.1498, -1.3590,
        -1.6633, -4.9876, -2.7829, -4.0145,  0.5822], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 3.2296e-01,  2.6579e+00,  3.2296e-01,  2.5039e+00,  9.9966e-01,
          4.0333e-01, -2.7102e+00,  3.8170e+00, -5.1736e+00,  5.1634e+00,
          6.6638e+00, -2.2581e-01, -1.0729e+01],
        [-1.2045e-01,  4.5025e+00, -1.2045e-01,  6.6008e-01, -1.2905e+00,
         -2.3253e-02, -4.4787e-03, -2.4414e+00, -9.1472e-02,  2.1038e+00,
         -4.2766e+00,  1.4616e+00, -5.8404e-01],
        [-9.2158e-02, -4.6798e+00, -9.2157e-02, -5.3503e-01,  2.0339e+00,
          3.7147e-03,  1.6739e+00, -3.3252e-01,  1.5723e+00, -8.3855e+00,
         -1.6644e+00, -1.1490e+00,  2.3498e+00],
        [-8.6392e-02, -5.2595e+00, -8.6392e-02, -2.8715e+00,  2.6091e+00,
         -8.5086e-02,  5.9591e+00, -2.9978e+00,  1.4042e+00, -9.6820e+00,
         -3.5163e+00,  2.3107e+00,  5.0575e+00],
        [-7.7170e-01, -2.8727e+00, -7.7170e-01, -1.3994e+00,  2.9643e+00,
         -7.7092e-01,  1.1722e+01, -5.2625e+00,  1.5335e+00, -7.2514e+00,
         -6.3452e+00,  1.9064e+00,  5.5718e+00]], device='cuda:0'))])
xi:  [-351.60715]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 26.51257371422491
W_T_median: 49.7449591549839
W_T_pctile_5: -352.5283900906803
W_T_CVAR_5_pct: -408.7358305395361
Average q (qsum/M+1):  57.89100302419355
Optimal xi:  [-351.60715]
Expected(across Rb) median(across samples) p_equity:  0.1551597759205227
obj fun:  tensor(-1774.1836, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1689.666647076611
Current xi:  [78.833595]
objective value function right now is: -1689.666647076611
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1694.4293484631391
Current xi:  [57.47797]
objective value function right now is: -1694.4293484631391
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1705.147029636026
Current xi:  [36.82469]
objective value function right now is: -1705.147029636026
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1709.0541346399755
Current xi:  [16.443485]
objective value function right now is: -1709.0541346399755
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1713.0205634012689
Current xi:  [-1.3003788]
objective value function right now is: -1713.0205634012689
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1713.0494276761817
Current xi:  [-3.0478797]
objective value function right now is: -1713.0494276761817
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1713.225466658496
Current xi:  [-5.2619333]
objective value function right now is: -1713.225466658496
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.1988554]
objective value function right now is: -1712.9767029723891
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1713.3121106519857
Current xi:  [-9.411982]
objective value function right now is: -1713.3121106519857
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1713.4557052243497
Current xi:  [-11.953723]
objective value function right now is: -1713.4557052243497
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1714.7366270919313
Current xi:  [-25.678877]
objective value function right now is: -1714.7366270919313
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1718.1458341194914
Current xi:  [-36.209583]
objective value function right now is: -1718.1458341194914
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.047054]
objective value function right now is: -1716.1755204418278
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1720.5118827722213
Current xi:  [-35.00717]
objective value function right now is: -1720.5118827722213
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1724.3240708462479
Current xi:  [-35.679165]
objective value function right now is: -1724.3240708462479
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1724.516576021966
Current xi:  [-35.00564]
objective value function right now is: -1724.516576021966
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-36.56023]
objective value function right now is: -1724.5042869733586
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1724.9363548284039
Current xi:  [-35.033264]
objective value function right now is: -1724.9363548284039
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.74685]
objective value function right now is: -1723.9866263847682
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.71466]
objective value function right now is: -1724.6120235798248
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.15695]
objective value function right now is: -1724.4849276831126
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.177532]
objective value function right now is: -1723.1404652536671
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.1294859729935
Current xi:  [-35.822933]
objective value function right now is: -1725.1294859729935
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.151474]
objective value function right now is: -1724.787757232829
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.550014]
objective value function right now is: -1724.902442308464
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.513775]
objective value function right now is: -1724.0623976692516
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.370205]
objective value function right now is: -1724.3200669109651
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1725.3627425482468
Current xi:  [-35.10229]
objective value function right now is: -1725.3627425482468
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-35.064022]
objective value function right now is: -1724.9996968977482
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.132187]
objective value function right now is: -1724.7657168958222
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.32205]
objective value function right now is: -1724.695926740831
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.5033861233546
Current xi:  [-35.17775]
objective value function right now is: -1725.5033861233546
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.575268]
objective value function right now is: -1724.442223743273
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.351997]
objective value function right now is: -1724.8558833673992
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.001514]
objective value function right now is: -1725.2615496635049
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.01116]
objective value function right now is: -1725.4590216942938
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.6285914088644
Current xi:  [-34.98783]
objective value function right now is: -1725.6285914088644
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.7191409853133
Current xi:  [-35.099796]
objective value function right now is: -1725.7191409853133
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.99473]
objective value function right now is: -1725.5746456265958
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.7808409775014
Current xi:  [-35.000515]
objective value function right now is: -1725.7808409775014
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.781766452251
Current xi:  [-35.029266]
objective value function right now is: -1725.781766452251
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.8803129705384
Current xi:  [-34.99262]
objective value function right now is: -1725.8803129705384
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.996025]
objective value function right now is: -1725.5721568103581
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.000763]
objective value function right now is: -1725.7825051191342
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.007927]
objective value function right now is: -1725.8298945058496
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.013206]
objective value function right now is: -1725.8040478631333
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.02488]
objective value function right now is: -1725.7946787521373
new min fval from sgd:  -1725.9032052839284
new min fval from sgd:  -1725.9104706251044
new min fval from sgd:  -1725.9136392128785
new min fval from sgd:  -1725.915760173067
new min fval from sgd:  -1725.9163929762292
new min fval from sgd:  -1725.9195380840424
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.996483]
objective value function right now is: -1725.9014493444251
new min fval from sgd:  -1725.9198867747243
new min fval from sgd:  -1725.9211209181315
new min fval from sgd:  -1725.9224766935538
new min fval from sgd:  -1725.9236591582035
new min fval from sgd:  -1725.923892680938
new min fval from sgd:  -1725.9240716101122
new min fval from sgd:  -1725.9243724697983
new min fval from sgd:  -1725.9244598622267
new min fval from sgd:  -1725.9247033717977
new min fval from sgd:  -1725.9251106026518
new min fval from sgd:  -1725.925259330103
new min fval from sgd:  -1725.9255319849879
new min fval from sgd:  -1725.9280320811697
new min fval from sgd:  -1725.9302023738187
new min fval from sgd:  -1725.932447467389
new min fval from sgd:  -1725.9342217645496
new min fval from sgd:  -1725.9354830747436
new min fval from sgd:  -1725.937464669656
new min fval from sgd:  -1725.9389884435602
new min fval from sgd:  -1725.9396497912285
new min fval from sgd:  -1725.9399631241863
new min fval from sgd:  -1725.9403922556673
new min fval from sgd:  -1725.9411689257663
new min fval from sgd:  -1725.9442503476168
new min fval from sgd:  -1725.945402396903
new min fval from sgd:  -1725.945915252564
new min fval from sgd:  -1725.9475294928905
new min fval from sgd:  -1725.9497921116767
new min fval from sgd:  -1725.949877748492
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.000362]
objective value function right now is: -1725.9342762556523
new min fval from sgd:  -1725.9504922308308
new min fval from sgd:  -1725.9512503137078
new min fval from sgd:  -1725.9522669559965
new min fval from sgd:  -1725.9528730412821
new min fval from sgd:  -1725.9538402752783
new min fval from sgd:  -1725.954503879335
new min fval from sgd:  -1725.9549428291177
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.996006]
objective value function right now is: -1725.914133868964
min fval:  -1725.9549428291177
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -0.9214,  -1.5943],
        [  0.8022,   5.8627],
        [-14.2608,   5.0825],
        [  0.8640,   6.1070],
        [  0.7475,   5.7130],
        [  0.6678,   5.4693],
        [ -2.7090,  -4.7644],
        [ -2.7820,  -5.1715],
        [ -2.4079,  -4.3967],
        [  0.9314,   6.3407],
        [ -3.8231,  -5.6715],
        [  0.5968,   5.2933],
        [ -2.8102,  -5.0001]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-4.0638,  3.8138,  4.4039,  4.2819,  3.5294,  3.0198, -4.6942, -4.7580,
        -4.7594,  4.6569, -4.8500,  2.7167, -4.7523], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.0106, -0.0582, -0.0224, -0.0789, -0.0479, -0.0332, -0.0959, -0.1284,
         -0.0667, -0.0958, -0.1359, -0.0278, -0.1099],
        [-0.1747,  2.4699,  8.9758,  3.0808,  2.2388,  1.7361, -1.7044, -2.4510,
         -1.2933,  3.5352, -3.0655,  1.5773, -1.8695],
        [-0.0106, -0.0582, -0.0224, -0.0789, -0.0479, -0.0332, -0.0959, -0.1284,
         -0.0667, -0.0958, -0.1359, -0.0278, -0.1099],
        [-0.0652,  2.1052,  7.8803,  2.6104,  1.8669,  1.4274, -1.3771, -2.0878,
         -1.0084,  3.0029, -2.7722,  1.2345, -1.7061],
        [-0.0106, -0.0582, -0.0224, -0.0789, -0.0479, -0.0332, -0.0959, -0.1284,
         -0.0667, -0.0958, -0.1359, -0.0278, -0.1099],
        [-0.0106, -0.0582, -0.0224, -0.0789, -0.0479, -0.0332, -0.0959, -0.1284,
         -0.0667, -0.0958, -0.1359, -0.0278, -0.1099],
        [-0.0106, -0.0582, -0.0224, -0.0789, -0.0479, -0.0332, -0.0959, -0.1284,
         -0.0667, -0.0958, -0.1359, -0.0278, -0.1099],
        [ 0.0919, -1.5762, -3.4632, -2.0822, -1.2951, -0.9219,  1.5344,  1.7313,
          1.2387, -2.5681,  2.7799, -0.7518,  1.6976],
        [-0.1227,  2.2585,  8.3687,  2.8203,  2.0021,  1.5321, -1.5069, -2.1328,
         -1.1225,  3.2506, -2.7449,  1.3601, -1.7534],
        [-0.0106, -0.0582, -0.0224, -0.0789, -0.0479, -0.0332, -0.0959, -0.1284,
         -0.0667, -0.0958, -0.1359, -0.0278, -0.1099],
        [ 0.0153, -1.0601, -2.5014, -1.4681, -0.8555, -0.5782,  1.1256,  1.3982,
          0.8615, -1.8636,  2.1469, -0.4586,  1.2623],
        [-0.0106, -0.0582, -0.0224, -0.0789, -0.0479, -0.0332, -0.0959, -0.1284,
         -0.0667, -0.0958, -0.1359, -0.0278, -0.1099],
        [ 0.5685, -3.1084, -6.3155, -3.8515, -2.6626, -2.0236,  2.4258,  2.7590,
          2.0938, -4.5225,  3.8663, -1.7568,  2.6387]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5387, -0.9358, -0.5387, -0.8204, -0.5387, -0.5387, -0.5387, -1.1480,
        -1.0357, -0.5387, -1.0600, -0.5387, -0.8583], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-0.0217,  8.2007, -0.0217,  6.7293, -0.0217, -0.0217, -0.0217, -4.4658,
          7.3636, -0.0217, -3.0646, -0.0217, -9.2003]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-13.5176,   3.4650],
        [ -0.2529,   4.1713],
        [-11.6996,  -5.3564],
        [-12.2829,  -5.6871],
        [ -9.5907,   0.6988],
        [ -1.5667,   0.7141],
        [-12.2844,  -5.5930],
        [-11.2152,   3.2412],
        [  7.6067,  -1.5256],
        [  6.6461,  -1.7005],
        [-11.8935,  -5.4630],
        [ -7.6431,  -8.2227],
        [  0.9144,  -7.3686]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 3.7712, -5.2630, -4.4382, -4.6665,  8.6270, -2.4438, -4.3672,  4.7635,
        -8.2738, -8.0849, -4.5084, -6.2644, -6.7467], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.7797e+00,  3.8233e-01,  3.7710e-01,  4.9127e-01,  4.2058e+00,
         -1.8293e-01, -5.3113e-02, -3.9395e+00,  3.3507e+00,  2.7498e+00,
          4.5366e-01,  2.5370e+00,  7.4430e-01],
        [-3.3482e-03,  6.4244e-02,  9.6604e-01,  1.2215e+00, -1.2945e+01,
         -8.3449e-02,  2.0160e-01, -5.9297e-03,  4.9309e+00,  3.6538e+00,
          1.1400e+00, -8.8598e-01, -1.1208e+00],
        [-1.3884e+00, -2.0482e-02,  5.5513e+00,  6.2881e+00, -7.1239e+00,
         -4.6920e-01,  7.7667e+00, -3.5432e+00, -9.1497e+00, -8.0132e+00,
          5.6087e+00,  4.9587e+00,  3.7453e+00],
        [-5.4946e-02, -2.6427e-03, -2.4055e-02, -2.5872e-02, -1.7280e+00,
         -1.5405e-02, -3.5266e-02, -8.6526e-02, -2.8911e-01, -1.7607e-01,
         -2.4543e-02, -2.6750e-01, -6.6667e-01],
        [-2.3751e-01, -1.4308e+00,  1.2138e+00,  1.7783e+00,  5.8588e+00,
         -1.6535e-01,  9.2264e-01, -1.0050e-01, -3.9141e+00, -4.0810e+00,
          1.4206e+00, -4.2985e+00, -5.2204e+00],
        [-9.6068e-02,  3.1103e-01, -9.7185e-02, -1.0045e-01, -1.6744e+00,
         -7.0254e-03, -1.1940e-01, -1.0522e-01, -3.4784e-01, -2.0301e-01,
         -9.8196e-02, -3.0089e-01, -7.5165e-01],
        [-6.5754e+00,  3.5873e+00, -1.3521e-01, -2.3714e-01, -5.7709e+00,
         -5.4098e-01, -9.6948e-02, -6.6980e+00,  1.1709e+01,  8.1666e+00,
         -1.8993e-01,  7.5929e+00,  1.3055e+01],
        [-7.1066e+00, -1.2672e-02,  7.5641e+00,  8.3696e+00, -1.0404e+00,
         -1.2575e-01,  9.4910e+00, -1.4253e+01, -3.5974e+00, -2.0179e+00,
          7.7579e+00,  5.8995e+00,  3.2659e+00],
        [ 1.2109e-01,  2.5115e+00, -9.6673e-01, -9.6479e-01, -7.8738e-01,
          2.7809e-01, -1.1264e+00,  1.0245e+00, -4.4533e-01, -2.8929e-01,
         -9.6748e-01, -1.9928e+00, -1.2026e+00],
        [-5.4948e-02, -2.6403e-03, -2.4057e-02, -2.5874e-02, -1.7280e+00,
         -1.5404e-02, -3.5269e-02, -8.6528e-02, -2.8911e-01, -1.7607e-01,
         -2.4545e-02, -2.6751e-01, -6.6667e-01],
        [-1.0599e+01,  4.2814e-01, -6.0665e-01, -7.8414e-01, -4.3081e+00,
          1.4336e-01,  1.0796e-01, -4.0897e+00,  1.3379e+00, -1.2894e-02,
         -7.2199e-01,  2.0918e+01,  1.0155e+01],
        [-1.2402e-02,  2.5954e+00, -1.1032e+00, -1.1007e+00, -5.9016e-01,
          3.1643e-01, -1.2892e+00,  1.0557e+00, -4.2306e-01, -2.7122e-01,
         -1.1039e+00, -2.2698e+00, -1.2087e+00],
        [-5.4954e-02, -2.6339e-03, -2.4062e-02, -2.5879e-02, -1.7280e+00,
         -1.5403e-02, -3.5275e-02, -8.6531e-02, -2.8911e-01, -1.7606e-01,
         -2.4550e-02, -2.6751e-01, -6.6668e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.1306, -2.9405, -8.7911, -2.4957, -0.9357, -2.4592,  3.7194, -8.3933,
        -1.6859, -2.4957, -3.0232, -1.5351, -2.4957], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.1984,  -5.8715,  10.9930,  -0.0755,   3.8476,  -0.1864,  -1.3366,
           5.4926,  -0.5063,  -0.0755,  -6.9337,  -0.5635,  -0.0755],
        [  3.3988,   8.4003,  -0.2607,   0.0578,  -2.4763,   0.1032,   1.2350,
          -3.8966,   0.0835,   0.0578,   0.7269,   0.0980,   0.0578],
        [ -3.8641,  -2.8818,  -9.7390,   0.0997,  -0.4434,   0.1976,   0.9071,
           6.2570,   1.0699,   0.0997,   6.2080,   1.1676,   0.0997],
        [ -3.8552,  10.3276, -13.7901,   0.0886,  -2.8207,   0.0600,   6.2346,
          -1.9127,  -0.0947,   0.0886,   7.0312,  -0.1022,   0.0886],
        [ -2.3490,   0.2028,   0.1382,   0.1805,  -0.6648,   0.4272,  12.5348,
           0.1748,   3.3917,   0.1805,   1.9571,   3.8528,   0.1806]],
       device='cuda:0'))])
xi:  [-34.984634]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 93.66299785864865
W_T_median: 56.6712336261876
W_T_pctile_5: -34.98671723732592
W_T_CVAR_5_pct: -162.12822211129208
Average q (qsum/M+1):  56.72195533014113
Optimal xi:  [-34.984634]
Expected(across Rb) median(across samples) p_equity:  0.2074291372982164
obj fun:  tensor(-1725.9549, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1658.9474001867854
Current xi:  [83.20504]
objective value function right now is: -1658.9474001867854
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1667.659024862609
Current xi:  [67.853065]
objective value function right now is: -1667.659024862609
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1670.0474725614497
Current xi:  [56.247356]
objective value function right now is: -1670.0474725614497
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [46.656384]
objective value function right now is: -1669.2101988475117
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1671.7751062616553
Current xi:  [38.6347]
objective value function right now is: -1671.7751062616553
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.91932390495
Current xi:  [32.099594]
objective value function right now is: -1672.91932390495
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1674.0461099610727
Current xi:  [26.606077]
objective value function right now is: -1674.0461099610727
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [22.407063]
objective value function right now is: -1673.1229736350751
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [18.610266]
objective value function right now is: -1673.488950320531
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1675.3310735002422
Current xi:  [14.652503]
objective value function right now is: -1675.3310735002422
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.404875]
objective value function right now is: -1674.9992791424131
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1675.9145355810658
Current xi:  [8.067849]
objective value function right now is: -1675.9145355810658
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1676.7299533443606
Current xi:  [3.9889717]
objective value function right now is: -1676.7299533443606
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1677.0381325541596
Current xi:  [0.03605846]
objective value function right now is: -1677.0381325541596
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1677.4660312310994
Current xi:  [-0.07699565]
objective value function right now is: -1677.4660312310994
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.4826131167472
Current xi:  [-0.05311142]
objective value function right now is: -1678.4826131167472
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03768815]
objective value function right now is: -1678.3669981534858
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01137648]
objective value function right now is: -1678.3352490281688
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00465491]
objective value function right now is: -1678.168032821686
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.6332173857904
Current xi:  [-0.01856417]
objective value function right now is: -1678.6332173857904
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04530668]
objective value function right now is: -1678.4791490512537
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00588132]
objective value function right now is: -1678.0273305057335
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02040805]
objective value function right now is: -1678.4213624020797
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02765922]
objective value function right now is: -1677.9540578852561
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.754003566123
Current xi:  [-0.02126112]
objective value function right now is: -1678.754003566123
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02824489]
objective value function right now is: -1678.5082889410085
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1679.070080471713
Current xi:  [-0.00219251]
objective value function right now is: -1679.070080471713
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01837733]
objective value function right now is: -1678.839207993333
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01492309]
objective value function right now is: -1678.912647309674
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01890842]
objective value function right now is: -1677.6344677036573
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03865939]
objective value function right now is: -1678.429139226897
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03283072]
objective value function right now is: -1678.2497020264143
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1679.2777546209898
Current xi:  [-0.0227301]
objective value function right now is: -1679.2777546209898
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01591519]
objective value function right now is: -1678.9490062943335
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02164577]
objective value function right now is: -1678.428689645568
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1679.315264473157
Current xi:  [0.00140852]
objective value function right now is: -1679.315264473157
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1679.504514664238
Current xi:  [0.00146504]
objective value function right now is: -1679.504514664238
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00071413]
objective value function right now is: -1679.3756971456623
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00092147]
objective value function right now is: -1679.3914236069875
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00451802]
objective value function right now is: -1679.4371433947972
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00040882]
objective value function right now is: -1679.4823474187558
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0008372]
objective value function right now is: -1679.4140933435165
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00027755]
objective value function right now is: -1679.3812177171308
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00213835]
objective value function right now is: -1679.3992139287059
90.0% of gradient descent iterations done. Method = Adam
new min fval:  -1679.5699916747678
Current xi:  [0.0009497]
objective value function right now is: -1679.5699916747678
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00096805]
objective value function right now is: -1679.42914582132
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00267382]
objective value function right now is: -1679.4405763628
new min fval from sgd:  -1679.5883123682254
new min fval from sgd:  -1679.6031341940077
new min fval from sgd:  -1679.6097876025933
new min fval from sgd:  -1679.610704962498
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00190952]
objective value function right now is: -1679.3124635789065
new min fval from sgd:  -1679.6123121355506
new min fval from sgd:  -1679.6148672437696
new min fval from sgd:  -1679.6167850656589
new min fval from sgd:  -1679.6184399323647
new min fval from sgd:  -1679.6190788430115
new min fval from sgd:  -1679.619855338241
new min fval from sgd:  -1679.62014280286
new min fval from sgd:  -1679.6209192910214
new min fval from sgd:  -1679.6216051358488
new min fval from sgd:  -1679.6233272877298
new min fval from sgd:  -1679.625821753704
new min fval from sgd:  -1679.6281256289317
new min fval from sgd:  -1679.6302710660282
new min fval from sgd:  -1679.6320981458064
new min fval from sgd:  -1679.633232266016
new min fval from sgd:  -1679.634529792002
new min fval from sgd:  -1679.6350412309416
new min fval from sgd:  -1679.635117090291
new min fval from sgd:  -1679.635218111339
new min fval from sgd:  -1679.63599569675
new min fval from sgd:  -1679.6365215740968
new min fval from sgd:  -1679.636964689697
new min fval from sgd:  -1679.6374692604438
new min fval from sgd:  -1679.637661819611
new min fval from sgd:  -1679.6381319185623
new min fval from sgd:  -1679.638304447937
new min fval from sgd:  -1679.6389401329463
new min fval from sgd:  -1679.639508033517
new min fval from sgd:  -1679.6416982145938
new min fval from sgd:  -1679.6434559889613
new min fval from sgd:  -1679.6438888679436
new min fval from sgd:  -1679.6442138770014
new min fval from sgd:  -1679.6446053091984
new min fval from sgd:  -1679.6451278966956
new min fval from sgd:  -1679.645691962012
new min fval from sgd:  -1679.646532948298
new min fval from sgd:  -1679.6469181503248
new min fval from sgd:  -1679.6472460984905
new min fval from sgd:  -1679.6473079452626
new min fval from sgd:  -1679.6477022620938
new min fval from sgd:  -1679.6483060389692
new min fval from sgd:  -1679.6493145342513
new min fval from sgd:  -1679.6502306365883
new min fval from sgd:  -1679.6511033405095
new min fval from sgd:  -1679.651291865892
new min fval from sgd:  -1679.6513389194072
new min fval from sgd:  -1679.653546661478
new min fval from sgd:  -1679.6557433666098
new min fval from sgd:  -1679.6574088398572
new min fval from sgd:  -1679.6584434980373
new min fval from sgd:  -1679.6587245427138
new min fval from sgd:  -1679.6589101676673
new min fval from sgd:  -1679.6595530192233
new min fval from sgd:  -1679.6596088551044
new min fval from sgd:  -1679.6598604490848
new min fval from sgd:  -1679.6605458622091
new min fval from sgd:  -1679.6610617657332
new min fval from sgd:  -1679.6614320744332
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00135527]
objective value function right now is: -1679.6535674742715
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00150059]
objective value function right now is: -1679.6340644358015
min fval:  -1679.6614320744332
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -6.9129,  -5.5756],
        [  6.9423,  -0.1279],
        [ -2.7023,   6.1489],
        [ -2.6881,   6.1270],
        [ -0.7182,   1.3694],
        [ -2.0942,   4.8914],
        [ -2.6637,   6.0119],
        [ -0.7182,   1.3694],
        [  7.2637,  -0.1346],
        [  7.6517,  -0.1171],
        [ -0.7177,   1.3691],
        [  8.2513,   0.5804],
        [-10.2430,  -8.7138]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-4.2193, -7.3598,  5.3272,  5.2753, -1.6118,  1.8168,  5.0811, -1.6118,
        -7.3937, -7.4698, -1.6117, -7.4841, -3.3853], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.3278e-02, -9.9191e-02, -1.3258e-01, -1.2925e-01, -9.9666e-03,
         -9.1774e-03, -1.1822e-01, -9.9666e-03, -1.2894e-01, -1.6235e-01,
         -9.9682e-03, -1.5369e-01, -1.0693e-01],
        [-3.3278e-02, -9.9192e-02, -1.3258e-01, -1.2925e-01, -9.9667e-03,
         -9.1775e-03, -1.1822e-01, -9.9667e-03, -1.2894e-01, -1.6236e-01,
         -9.9683e-03, -1.5369e-01, -1.0693e-01],
        [-3.3278e-02, -9.9191e-02, -1.3258e-01, -1.2925e-01, -9.9666e-03,
         -9.1774e-03, -1.1822e-01, -9.9666e-03, -1.2894e-01, -1.6235e-01,
         -9.9682e-03, -1.5369e-01, -1.0693e-01],
        [ 1.6173e+00,  3.2321e-01, -3.3286e+00, -3.1816e+00, -6.8405e-02,
         -2.7828e-01, -2.9744e+00, -6.8404e-02,  5.0966e-01,  8.8364e-01,
         -6.8791e-02,  1.3245e+00,  2.3700e+00],
        [-2.1144e+00,  1.2892e+00, -1.5148e+00, -1.4776e+00, -8.1134e-04,
         -2.3420e-01, -1.3908e+00, -8.1099e-04,  1.6003e+00,  2.0141e+00,
         -8.2251e-04,  1.6942e+00, -8.0240e+00],
        [ 4.7532e+00,  2.0108e+00, -5.2247e+00, -4.9953e+00, -1.4880e-03,
         -6.9636e-01, -4.5089e+00, -1.4904e-03,  2.2067e+00,  2.7988e+00,
          1.3072e-04,  3.5448e+00,  3.8718e+00],
        [ 4.7411e+00,  2.2908e+00, -5.4346e+00, -5.1729e+00,  1.3541e-02,
         -7.3113e-01, -4.8255e+00,  1.3534e-02,  2.5663e+00,  3.2196e+00,
          1.6513e-02,  3.9994e+00,  4.1110e+00],
        [-3.3277e-02, -9.9751e-02, -1.3309e-01, -1.2975e-01, -1.0011e-02,
         -9.2152e-03, -1.1871e-01, -1.0011e-02, -1.2959e-01, -1.6309e-01,
         -1.0013e-02, -1.5439e-01, -1.0718e-01],
        [-5.1224e+00, -2.4476e+00,  5.5280e+00,  5.2221e+00,  3.1160e-02,
          1.1239e+00,  5.0637e+00,  3.1175e-02, -2.8312e+00, -3.3376e+00,
          2.9865e-02, -4.4902e+00, -4.0941e+00],
        [ 3.6366e+00,  6.2728e-01, -4.7170e+00, -4.5498e+00,  1.7884e-02,
         -5.6402e-01, -4.1774e+00,  1.7883e-02,  8.2788e-01,  1.2462e+00,
          1.8472e-02,  1.6394e+00,  3.6394e+00],
        [ 5.8899e-02, -1.0555e-01, -1.1830e+00, -1.1445e+00, -8.2920e-02,
         -1.0873e-01, -1.0392e+00, -8.2920e-02, -9.8416e-02, -6.7950e-02,
         -8.3012e-02, -7.1886e-02,  1.6781e-01],
        [-3.3278e-02, -9.9191e-02, -1.3258e-01, -1.2925e-01, -9.9666e-03,
         -9.1774e-03, -1.1822e-01, -9.9666e-03, -1.2894e-01, -1.6235e-01,
         -9.9682e-03, -1.5369e-01, -1.0693e-01],
        [-6.0473e+00, -2.7952e+00,  6.0076e+00,  5.7977e+00,  2.4430e-02,
          1.4193e+00,  5.6013e+00,  2.4411e-02, -3.3050e+00, -3.6080e+00,
          2.9249e-02, -4.7735e+00, -4.3872e+00]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.0894, -1.0894, -1.0894, -0.6252,  3.1448, -0.0434,  0.3398, -1.0994,
        -0.4799, -0.2699, -1.5554, -1.0894, -0.3954], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 7.1618e-03,  7.1620e-03,  7.1618e-03, -2.7460e+00,  4.8866e+00,
         -5.8856e+00, -6.6030e+00,  7.2883e-03,  6.8556e+00, -4.6016e+00,
         -1.0635e+00,  7.1618e-03,  8.7391e+00]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  6.2054,   2.2075],
        [ -3.5841,   9.6302],
        [ -5.4254, -10.1925],
        [ -1.5645,   0.5875],
        [ -1.5712,   0.5851],
        [ 11.4366,  -0.3531],
        [  8.9493,   9.3587],
        [-12.2547,   4.9888],
        [  1.7511,   4.3508],
        [-10.7398,  -5.3793],
        [ 11.6358,   4.9495],
        [  6.9609,  -6.0148],
        [ -1.5721,   0.5849]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-7.4902,  9.4850, -8.4611, -2.3789, -2.3702, -9.5837,  6.5757,  5.5331,
        -6.5913, -2.3382, -1.4083, -6.4915, -2.3686], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-7.6907e-02, -7.5930e+00,  5.6670e+00, -1.7500e-01, -1.7484e-01,
         -6.2683e+00, -1.0717e+01, -4.7850e-01,  1.2080e-03,  5.0798e+00,
         -5.6883e+00,  1.0131e+00, -1.7473e-01],
        [-6.6406e-01,  2.8331e-01, -3.4407e-02,  8.9284e-02,  8.9346e-02,
         -1.4912e+00, -2.0455e+00,  2.3188e+00, -4.1079e-01,  1.8613e+00,
         -1.1245e+00, -1.9662e+00,  8.9625e-02],
        [-2.8254e+00,  7.4215e+00, -1.0421e+01,  1.9782e-01,  2.0916e-01,
         -1.3945e+01,  2.8416e+00,  8.2484e+00, -3.2488e+00,  1.0915e+00,
         -1.3317e-01, -6.5653e+00,  2.0864e-01],
        [ 5.4700e-01,  3.3111e-01, -1.0061e+00,  1.3517e-01,  1.3560e-01,
          2.3255e-01, -7.8290e-01,  1.6862e+00,  7.8397e-01, -1.2921e+00,
         -2.6590e-01, -2.6086e+00,  1.3452e-01],
        [ 4.6589e-01, -1.1178e+01,  5.6960e+00, -9.7619e-02, -1.0238e-01,
          4.5457e+00, -7.0550e+00, -5.7470e+00, -2.0705e-02,  6.0796e+00,
         -2.3872e+00,  4.9258e+00, -1.0414e-01],
        [ 4.3332e+00, -5.3812e+00,  9.2572e+00, -7.8220e-02, -7.9752e-02,
          1.3098e+01, -3.9035e+00, -8.4554e+00,  1.8662e+00,  2.7731e+00,
         -3.4259e+00,  6.4812e+00, -8.0078e-02],
        [-5.6848e-01,  2.0924e-01, -1.4934e-01,  9.2208e-02,  9.2795e-02,
         -1.1327e+00, -1.7449e+00,  1.8000e+00, -4.6369e-01,  1.3176e+00,
         -1.0022e+00, -1.8861e+00,  9.3040e-02],
        [ 6.7443e-01,  4.2684e+00, -2.8831e+00,  1.3882e-01,  1.4046e-01,
          3.8522e+00, -1.3628e+00,  1.7259e+00,  5.5657e-02,  2.4067e+00,
         -2.6520e-01, -1.1519e+00,  1.4223e-01],
        [-1.1391e-01, -5.9082e-01, -6.2726e-01, -4.3607e-03, -4.5164e-03,
         -5.6398e-01, -1.3291e+00, -9.6560e-02, -1.3871e-01, -2.9010e-01,
         -8.0819e-01, -1.4162e+00, -4.5321e-03],
        [-2.4575e+00, -9.8241e-01, -5.6782e+00,  1.1868e-01,  1.3305e-01,
         -5.3697e+00,  1.9832e+00,  8.0376e+00, -1.3422e+00,  1.3424e+00,
         -2.4953e+00, -1.3951e+00,  1.3240e-01],
        [-9.9390e-02, -6.6623e+00,  5.9398e+00,  4.9121e-01,  4.9433e-01,
         -1.1030e+01, -1.0579e+01,  1.9127e+00,  1.5565e-03,  4.1146e+00,
         -4.3908e+00, -2.4749e-01,  4.9535e-01],
        [-6.9014e-01,  2.8237e-01, -2.2954e-02,  8.8226e-02,  8.8288e-02,
         -1.5246e+00, -2.0578e+00,  2.3292e+00, -4.3918e-01,  1.8139e+00,
         -1.1096e+00, -1.8938e+00,  8.8564e-02],
        [ 2.6582e+00, -3.6078e+00,  7.1081e+00, -1.5593e-02, -2.1058e-02,
          5.9805e+00, -4.8014e+00, -7.3405e+00,  4.7119e-01,  2.7404e+00,
         -3.8724e+00,  6.4743e+00, -2.1100e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.6238, -1.6321, -5.4538, -1.5946,  1.1030,  1.7483, -1.5910,  1.1959,
        -1.7281, -1.1401, -2.0444, -1.6024,  1.1806], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-2.6571e+00,  7.8199e-01,  1.7762e+00,  7.3247e-02,  3.9138e+00,
         -8.3079e-01,  7.2786e-01,  9.0185e-01,  2.2740e-01,  5.9552e-01,
         -4.9489e+00,  7.6466e-01, -2.7476e+00],
        [ 2.5057e+00, -7.5097e-01, -2.0821e-01, -5.3214e-02, -2.6589e+00,
          1.5382e+00, -6.9515e-01,  4.4606e-01, -1.9326e-01, -1.2644e-01,
          4.9559e+00, -7.3238e-01,  2.7043e+00],
        [-3.8685e-02, -2.3722e-02, -1.5952e+00, -5.2407e-02, -1.8141e+00,
         -2.3921e+00, -3.4706e-02, -6.4457e+00, -3.2477e-02, -3.3368e-01,
         -1.2507e-03, -2.6123e-02, -2.2696e+00],
        [ 1.7163e+00, -3.4329e-01, -1.2399e+01,  9.3239e-01, -1.8099e+00,
          2.0410e+00, -2.6452e-01, -8.2488e-01, -1.0410e-01,  6.6554e+00,
          8.5563e+00, -3.6002e-01,  1.9873e-01],
        [ 6.3048e-01, -3.6344e+00, -9.1845e+00,  3.2742e+00,  2.0413e+00,
          6.1336e+00, -2.2209e+00,  8.4466e+00, -4.6288e-02, -1.0354e-01,
          2.6766e-01, -3.6075e+00,  2.1789e+00]], device='cuda:0'))])
xi:  [0.00097516]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 92.67046315807922
W_T_median: 53.62214137869065
W_T_pctile_5: 0.0010725803371013144
W_T_CVAR_5_pct: -90.90154905077759
Average q (qsum/M+1):  55.648780084425404
Optimal xi:  [0.00097516]
Expected(across Rb) median(across samples) p_equity:  0.22352741369977594
obj fun:  tensor(-1679.6614, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1431.372927519675
Current xi:  [114.575745]
objective value function right now is: -1431.372927519675
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1443.3603191202665
Current xi:  [133.13344]
objective value function right now is: -1443.3603191202665
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1461.966415578716
Current xi:  [154.08272]
objective value function right now is: -1461.966415578716
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1657.2289352338016
Current xi:  [172.57024]
objective value function right now is: -1657.2289352338016
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1665.5545100111449
Current xi:  [174.05177]
objective value function right now is: -1665.5545100111449
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1696.054763289059
Current xi:  [179.14833]
objective value function right now is: -1696.054763289059
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1697.0621458600635
Current xi:  [190.81172]
objective value function right now is: -1697.0621458600635
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1700.8446815183017
Current xi:  [200.92377]
objective value function right now is: -1700.8446815183017
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1702.6543169510762
Current xi:  [210.1935]
objective value function right now is: -1702.6543169510762
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1705.3891721390614
Current xi:  [217.70268]
objective value function right now is: -1705.3891721390614
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [224.6165]
objective value function right now is: -1703.9605340010485
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1707.1556871492373
Current xi:  [229.39471]
objective value function right now is: -1707.1556871492373
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [233.84868]
objective value function right now is: -1704.9563337868342
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [238.32384]
objective value function right now is: -1705.6300809840718
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1709.1634559266693
Current xi:  [242.63565]
objective value function right now is: -1709.1634559266693
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [246.34003]
objective value function right now is: -1707.4806558441192
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [248.3782]
objective value function right now is: -1708.4402348584322
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [249.92613]
objective value function right now is: -1707.7205505452519
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1709.7020592257475
Current xi:  [251.38863]
objective value function right now is: -1709.7020592257475
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [253.14957]
objective value function right now is: -1708.7614004149482
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [254.43823]
objective value function right now is: -1707.547799473391
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [255.49481]
objective value function right now is: -1709.5861539170528
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1710.4039236124067
Current xi:  [255.63257]
objective value function right now is: -1710.4039236124067
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [256.5961]
objective value function right now is: -1707.261085255861
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [257.13657]
objective value function right now is: -1710.060400935362
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [257.94473]
objective value function right now is: -1706.2562782636057
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [257.05905]
objective value function right now is: -1706.4565206908399
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [257.31772]
objective value function right now is: -1710.2723978759707
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [258.92343]
objective value function right now is: -1706.224710032286
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [258.47543]
objective value function right now is: -1706.7835762031502
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [258.92487]
objective value function right now is: -1706.7829882145156
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [259.20737]
objective value function right now is: -1708.9066917583882
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [260.3428]
objective value function right now is: -1708.4144268732005
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [260.8037]
objective value function right now is: -1709.5087119398988
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [260.3011]
objective value function right now is: -1708.3918290602467
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1710.97856680729
Current xi:  [260.16406]
objective value function right now is: -1710.97856680729
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1711.6394054408904
Current xi:  [260.0661]
objective value function right now is: -1711.6394054408904
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1711.8140698958496
Current xi:  [260.22165]
objective value function right now is: -1711.8140698958496
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [260.33694]
objective value function right now is: -1711.7588597650965
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [260.5106]
objective value function right now is: -1711.6113193669933
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [260.82153]
objective value function right now is: -1711.7418040095713
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [260.9132]
objective value function right now is: -1711.5789689194119
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1712.199280236968
Current xi:  [261.1363]
objective value function right now is: -1712.199280236968
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [261.31244]
objective value function right now is: -1712.1284602360936
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [261.32367]
objective value function right now is: -1711.794373245473
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [261.50412]
objective value function right now is: -1711.3877514866492
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [261.6696]
objective value function right now is: -1712.0966531954057
new min fval from sgd:  -1712.2023674794784
new min fval from sgd:  -1712.2435385368503
new min fval from sgd:  -1712.2464103586756
new min fval from sgd:  -1712.2647974830102
new min fval from sgd:  -1712.31890479706
new min fval from sgd:  -1712.357842695042
new min fval from sgd:  -1712.3815744863991
new min fval from sgd:  -1712.4116830478927
new min fval from sgd:  -1712.4438546846911
new min fval from sgd:  -1712.4471274215468
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [261.7294]
objective value function right now is: -1712.394478403478
new min fval from sgd:  -1712.4494465617483
new min fval from sgd:  -1712.4496885885367
new min fval from sgd:  -1712.4817681622233
new min fval from sgd:  -1712.4823665486256
new min fval from sgd:  -1712.499780474441
new min fval from sgd:  -1712.5239829342086
new min fval from sgd:  -1712.5355862802608
new min fval from sgd:  -1712.5464370285595
new min fval from sgd:  -1712.5563079345125
new min fval from sgd:  -1712.5697464394664
new min fval from sgd:  -1712.5700470974903
new min fval from sgd:  -1712.575500208517
new min fval from sgd:  -1712.5760270399414
new min fval from sgd:  -1712.5787603574552
new min fval from sgd:  -1712.581385768862
new min fval from sgd:  -1712.5849533914723
new min fval from sgd:  -1712.5880905055
new min fval from sgd:  -1712.5910604977255
new min fval from sgd:  -1712.5962383936608
new min fval from sgd:  -1712.603309458367
new min fval from sgd:  -1712.6144481301799
new min fval from sgd:  -1712.6182182736106
new min fval from sgd:  -1712.6214630787592
new min fval from sgd:  -1712.622558618955
new min fval from sgd:  -1712.6264218409426
new min fval from sgd:  -1712.634252180763
new min fval from sgd:  -1712.6425841693506
new min fval from sgd:  -1712.6436040878164
new min fval from sgd:  -1712.6446948547373
new min fval from sgd:  -1712.6485879845816
new min fval from sgd:  -1712.6495577973901
new min fval from sgd:  -1712.657692711942
new min fval from sgd:  -1712.6596990559221
new min fval from sgd:  -1712.6664148726543
new min fval from sgd:  -1712.6683898598496
new min fval from sgd:  -1712.6694533295595
new min fval from sgd:  -1712.6740542237192
new min fval from sgd:  -1712.6757726549915
new min fval from sgd:  -1712.6787899989738
new min fval from sgd:  -1712.6859826926059
new min fval from sgd:  -1712.6868570597592
new min fval from sgd:  -1712.7003383231258
new min fval from sgd:  -1712.7086727738265
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [261.94992]
objective value function right now is: -1712.6549490495129
new min fval from sgd:  -1712.7114891787508
new min fval from sgd:  -1712.7145315799341
new min fval from sgd:  -1712.7190973474853
new min fval from sgd:  -1712.7212430099478
new min fval from sgd:  -1712.7232364333256
new min fval from sgd:  -1712.7268754363813
new min fval from sgd:  -1712.727643451454
new min fval from sgd:  -1712.7302167011658
new min fval from sgd:  -1712.7306762840694
new min fval from sgd:  -1712.7318332927205
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [261.88385]
objective value function right now is: -1712.5736086327477
min fval:  -1712.7318332927205
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-19.2010,   8.8941],
        [ -1.1616,   0.5743],
        [ -1.1597,   0.5748],
        [ -1.1577,   0.5754],
        [ -1.1585,   0.5752],
        [ -1.1578,   0.5754],
        [ -5.8459,  -8.6554],
        [ 10.5575,  -0.2619],
        [ 10.6012,  -1.1028],
        [ -6.8086,   8.1425],
        [ -1.1594,   0.5751],
        [ -1.1584,   0.5753],
        [ -1.1580,   0.5753]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  8.3978,  -2.7932,  -2.7925,  -2.7930,  -2.7933,  -2.7935,  -4.2312,
        -10.1800, -10.0707,   7.4403,  -2.7946,  -2.7941,  -2.7934],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 5.4204e-01,  8.3361e-02,  8.3256e-02,  8.3045e-02,  8.3086e-02,
          8.3013e-02, -1.0115e-01,  3.8863e-01,  5.0295e-01,  1.6767e+00,
          8.3057e-02,  8.3011e-02,  8.3034e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-7.5221e+00,  6.2216e-02,  6.1504e-02,  6.5443e-02,  6.5902e-02,
          6.7372e-02,  1.8094e+01,  8.1252e+00,  6.9384e+00, -4.2298e+00,
          7.0226e-02,  6.9325e-02,  6.6908e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.1658e+00,  2.1394e-02,  1.6289e-02,  7.5557e-03,  9.5136e-03,
          6.5203e-03,  1.8232e+01,  6.0558e+00,  7.1338e+00, -4.0712e+00,
          9.1865e-03,  6.9017e-03,  7.3557e-03],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-8.1081e+00, -7.0500e-02, -6.5133e-02, -6.1419e-02, -6.3677e-02,
         -6.2530e-02,  1.7942e+01,  8.7360e+00,  7.8395e+00, -4.7533e+00,
         -6.7970e-02, -6.4947e-02, -6.2793e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-5.2019e+00,  2.9834e-01,  2.9866e-01,  2.9911e-01,  2.9898e-01,
          2.9913e-01,  1.1758e+01,  2.8283e+00,  2.6221e+00, -4.0749e+00,
          2.9890e-01,  2.9906e-01,  2.9909e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.3868, -1.6059, -0.9413, -1.6059, -1.0647, -1.6059, -1.6059, -1.6059,
        -1.6059, -1.6059, -1.3230, -1.6059, -1.6457], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.2981e+01,  1.2623e-02, -1.0597e+01,  1.2623e-02, -1.0303e+01,
          1.2623e-02,  1.2623e-02,  1.2623e-02,  1.2623e-02,  1.2623e-02,
         -1.0107e+01,  1.2623e-02, -4.7178e+00]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -6.8981,  -7.9649],
        [-10.9503,   6.5660],
        [ -1.8814,   0.1924],
        [-11.8131,  -4.8737],
        [  4.1311,   8.9081],
        [ 12.4619,   6.2340],
        [-11.1937,   0.2291],
        [ -1.0297,  13.7987],
        [ -1.8814,   0.1927],
        [  6.2985,   8.7234],
        [  1.5562,   9.6525],
        [ 11.3925,   4.4920],
        [-12.1812,  -0.1090]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-6.1847,  5.5629, -3.7034, -3.2978, -7.9624,  2.9958,  8.3330,  6.6302,
        -3.7044,  2.6487,  6.2144, -0.6567,  9.9923], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.9546e+00, -2.5927e-01,  1.2440e-01, -7.7089e-01, -1.6169e-01,
          1.9287e+00, -2.6426e+00, -1.3133e+01,  1.2459e-01,  3.2064e+00,
          1.3232e+00,  2.6703e-01, -6.1990e+00],
        [ 5.7482e-02,  2.8332e+00, -2.4149e-01,  2.2683e-02,  6.3432e+00,
         -1.7663e+00, -1.1378e+01,  1.8372e+00, -2.4149e-01, -2.7482e-01,
          1.6546e+00, -1.7156e+00, -1.4493e+01],
        [-1.5705e+01,  8.5627e+00,  1.7504e-01, -1.8245e+00, -3.4437e+00,
         -4.8747e+00,  4.2497e+00,  8.4923e+00,  1.7496e-01,  1.8658e+00,
         -1.3945e+00, -3.9132e+00,  5.3923e+00],
        [-2.3983e+00, -1.3750e+01,  1.8326e-01, -3.0140e+00,  3.6535e+00,
          5.4045e+00, -4.2681e+00, -1.0690e+01,  1.8262e-01,  1.2769e+00,
          6.8748e+00,  2.9334e+00, -8.0795e+00],
        [-2.5300e-01, -5.9313e-01,  6.8050e-03, -8.3695e-02, -1.3481e-01,
         -1.8726e+00, -5.9762e-01, -2.8057e-01,  6.8429e-03, -1.1632e+00,
         -8.6191e-01, -1.9899e+00, -8.2710e-01],
        [-2.5303e-01, -5.9329e-01,  6.8086e-03, -8.3941e-02, -1.3514e-01,
         -1.8726e+00, -5.9815e-01, -2.7942e-01,  6.8464e-03, -1.1625e+00,
         -8.6146e-01, -1.9897e+00, -8.2773e-01],
        [-2.5266e-01, -5.9572e-01,  6.8198e-03, -8.7278e-02, -1.4048e-01,
         -1.8723e+00, -6.0665e-01, -2.6186e-01,  6.8571e-03, -1.1521e+00,
         -8.5434e-01, -1.9862e+00, -8.3775e-01],
        [-2.7205e-01, -6.0582e-01,  8.1218e-03, -1.0733e-01, -1.6508e-01,
         -1.8679e+00, -6.5034e-01, -1.5885e-01,  8.1541e-03, -1.0841e+00,
         -8.2225e-01, -1.9486e+00, -8.8637e-01],
        [-3.6862e+00,  3.0295e+00, -1.6090e-01, -4.5702e+00,  3.8485e-05,
          9.4414e+00, -3.6149e+00,  7.3836e-02, -1.6069e-01,  4.1823e-01,
          2.7291e+00,  2.8268e+00, -1.3360e+00],
        [ 3.3685e+00, -4.4660e-01, -1.2750e-01,  4.7870e+00, -1.2035e-03,
         -1.1253e+01,  4.2450e+00, -6.9133e+00, -1.2845e-01, -1.1255e+01,
         -1.5008e+01, -5.9594e+00,  5.6190e+00],
        [ 4.6716e+00, -9.6638e+00,  2.6870e-01,  9.3119e+00,  4.1322e-03,
         -7.1652e+00,  1.1206e+00, -2.1356e+01,  2.6997e-01, -3.2723e+00,
         -6.5481e+00, -4.0466e+00,  4.0470e+00],
        [-1.3765e+00, -1.2187e+01, -7.6873e-03,  3.8040e-01,  2.2691e+00,
          3.9446e-01, -1.3470e+00, -2.3680e+00, -7.6696e-03,  3.1427e+00,
          9.8105e+00, -1.3591e+00, -5.5141e+00],
        [ 3.6637e-01,  4.3879e-01,  5.2748e-01,  2.0317e+00,  9.2198e-03,
         -1.5847e+00,  7.5107e-01, -3.2943e+00,  5.2678e-01, -2.8752e+00,
         -5.7165e-01, -2.8248e+00,  6.4352e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  0.3862,  -2.4238, -10.3032,   2.7196,  -1.9168,  -1.9169,  -1.9168,
         -1.9408,   8.5897,  -2.2269,   3.3610,  -2.3266,  -1.0903],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.1000e+00, -1.0857e-01,  1.4673e+00, -1.6669e-01,  5.5109e-01,
          5.5057e-01,  5.4300e-01,  4.9721e-01,  1.6029e+00, -3.0176e+00,
         -1.4634e+01,  5.4741e-01,  1.5294e+00],
        [ 1.3754e-01,  1.1647e+00, -2.1128e-01,  1.3751e+00, -6.1155e-01,
         -6.1140e-01, -6.0938e-01, -5.8715e-01,  1.2712e+00, -1.8266e+00,
         -3.0872e-02,  1.0355e+00, -8.8206e-01],
        [-2.1030e+00, -5.0101e-02, -2.8225e+00, -2.8128e+00,  2.5894e-01,
          2.5912e-01,  2.6232e-01,  2.6933e-01, -5.4392e+00,  7.1206e+00,
          5.6263e+00, -4.1053e-01, -9.9450e-02],
        [-8.5912e-01,  1.2201e+00, -7.6673e+00,  1.0562e+00, -1.0571e+00,
         -1.0568e+00, -1.0534e+00, -1.0078e+00, -7.4609e-01,  1.6258e+01,
          8.9904e+00,  4.8649e-01, -1.7048e+00],
        [ 3.2684e-04,  6.5766e-05,  2.3863e-03,  4.7762e-04,  6.1573e-02,
          6.2129e-02,  7.0301e-02,  1.1758e-01,  1.9040e+01, -3.9017e+00,
         -3.0945e+00, -3.3614e-03, -2.0512e+00]], device='cuda:0'))])
xi:  [261.8972]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 581.8731876802971
W_T_median: 459.7263737685786
W_T_pctile_5: 261.9537859587363
W_T_CVAR_5_pct: 109.01314952403092
Average q (qsum/M+1):  51.73291409400202
Optimal xi:  [261.8972]
Expected(across Rb) median(across samples) p_equity:  0.275883549451828
obj fun:  tensor(-1712.7318, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1769.608911703187
Current xi:  [272.21408]
objective value function right now is: -1769.608911703187
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1771.0204913307903
Current xi:  [281.22372]
objective value function right now is: -1771.0204913307903
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [289.07095]
objective value function right now is: -1768.5710462560185
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1773.6614884493083
Current xi:  [295.0627]
objective value function right now is: -1773.6614884493083
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [300.29947]
objective value function right now is: -1772.2995560456263
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [304.5752]
objective value function right now is: -1765.8543666664136
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [306.58517]
objective value function right now is: -1771.2637872108758
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [309.80164]
objective value function right now is: -1773.0937100918327
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [312.57104]
objective value function right now is: -1772.392159562546
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [313.40018]
objective value function right now is: -1767.4255699057617
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1774.6768261044247
Current xi:  [315.6176]
objective value function right now is: -1774.6768261044247
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [316.81863]
objective value function right now is: -1773.5600966891188
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [317.66663]
objective value function right now is: -1770.799574058201
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [317.36884]
objective value function right now is: -1774.343288651085
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [318.16867]
objective value function right now is: -1774.2731851070153
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [318.74435]
objective value function right now is: -1774.6468653482116
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [318.54898]
objective value function right now is: -1771.702245510359
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1776.035500868591
Current xi:  [319.95486]
objective value function right now is: -1776.035500868591
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [319.36722]
objective value function right now is: -1773.9035188311045
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [319.0596]
objective value function right now is: -1768.013437322692
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [319.03107]
objective value function right now is: -1774.9264105855366
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [319.13113]
objective value function right now is: -1774.065503321563
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [319.67487]
objective value function right now is: -1775.8034650059215
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [320.02628]
objective value function right now is: -1775.8767795398012
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1776.0793743638262
Current xi:  [320.01022]
objective value function right now is: -1776.0793743638262
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1776.2272261760263
Current xi:  [319.97156]
objective value function right now is: -1776.2272261760263
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [320.92163]
objective value function right now is: -1773.9057559180221
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [320.40543]
objective value function right now is: -1774.2716017174562
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [319.84225]
objective value function right now is: -1774.5118650689303
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [319.72177]
objective value function right now is: -1773.7805451500194
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [321.21954]
objective value function right now is: -1768.1346108857351
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [320.99554]
objective value function right now is: -1774.4163608869092
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [320.11]
objective value function right now is: -1762.882607464347
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [319.8317]
objective value function right now is: -1776.0720070034624
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [320.9122]
objective value function right now is: -1767.7413800439938
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1778.4550549283945
Current xi:  [320.84277]
objective value function right now is: -1778.4550549283945
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1779.2235474494644
Current xi:  [320.99094]
objective value function right now is: -1779.2235474494644
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [320.9658]
objective value function right now is: -1778.631642103307
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [321.16254]
objective value function right now is: -1777.519527232295
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [321.0602]
objective value function right now is: -1779.064148760629
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [321.39172]
objective value function right now is: -1777.9503985006581
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1779.765888042894
Current xi:  [321.15582]
objective value function right now is: -1779.765888042894
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [321.3759]
objective value function right now is: -1778.7430202094356
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [321.34332]
objective value function right now is: -1779.2798293584938
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [321.45236]
objective value function right now is: -1779.143655277017
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [321.65952]
objective value function right now is: -1778.677731134569
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [321.70758]
objective value function right now is: -1779.352970290678
new min fval from sgd:  -1779.7738969086636
new min fval from sgd:  -1779.7889312617829
new min fval from sgd:  -1779.8079528815729
new min fval from sgd:  -1779.8370522476223
new min fval from sgd:  -1779.8563592338044
new min fval from sgd:  -1779.857932976635
new min fval from sgd:  -1779.8651357820695
new min fval from sgd:  -1779.8754957493
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [321.69775]
objective value function right now is: -1779.444907214055
new min fval from sgd:  -1779.8756419678382
new min fval from sgd:  -1779.8763449934854
new min fval from sgd:  -1779.8788913999997
new min fval from sgd:  -1779.881968918681
new min fval from sgd:  -1779.8865806055906
new min fval from sgd:  -1779.8918243931732
new min fval from sgd:  -1779.8981623836416
new min fval from sgd:  -1779.903453006002
new min fval from sgd:  -1779.906261195166
new min fval from sgd:  -1779.912132495074
new min fval from sgd:  -1779.9152894442907
new min fval from sgd:  -1779.9204886738294
new min fval from sgd:  -1779.9247016459442
new min fval from sgd:  -1779.927232992022
new min fval from sgd:  -1779.932267106806
new min fval from sgd:  -1779.9364309266114
new min fval from sgd:  -1779.9410921623032
new min fval from sgd:  -1779.9459693582792
new min fval from sgd:  -1779.9497997775643
new min fval from sgd:  -1779.9560004854682
new min fval from sgd:  -1779.9587104928535
new min fval from sgd:  -1779.9674728205973
new min fval from sgd:  -1779.969857044734
new min fval from sgd:  -1779.982426843612
new min fval from sgd:  -1779.9890076497413
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [321.67368]
objective value function right now is: -1779.97800255818
new min fval from sgd:  -1779.9891262641295
new min fval from sgd:  -1779.9999186902587
new min fval from sgd:  -1780.0110731037807
new min fval from sgd:  -1780.0215291055617
new min fval from sgd:  -1780.0279040333673
new min fval from sgd:  -1780.0397604529849
new min fval from sgd:  -1780.0526144092728
new min fval from sgd:  -1780.0632080125808
new min fval from sgd:  -1780.07252528517
new min fval from sgd:  -1780.0796547996165
new min fval from sgd:  -1780.0855951926849
new min fval from sgd:  -1780.092960545858
new min fval from sgd:  -1780.1005243626382
new min fval from sgd:  -1780.1077006417404
new min fval from sgd:  -1780.1109936774512
new min fval from sgd:  -1780.1110717584575
new min fval from sgd:  -1780.1150824828549
new min fval from sgd:  -1780.119169845819
new min fval from sgd:  -1780.1276236859733
new min fval from sgd:  -1780.1401460121451
new min fval from sgd:  -1780.1512052225755
new min fval from sgd:  -1780.1601761244883
new min fval from sgd:  -1780.168514482555
new min fval from sgd:  -1780.16922611022
new min fval from sgd:  -1780.1707454534894
new min fval from sgd:  -1780.1746178500841
new min fval from sgd:  -1780.1828506386166
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [321.68658]
objective value function right now is: -1779.9778335581827
min fval:  -1780.1828506386166
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-23.5094,   8.0489],
        [ -1.2747,   0.2578],
        [ -1.2747,   0.2578],
        [ -1.2747,   0.2578],
        [ -1.2747,   0.2578],
        [ -1.2747,   0.2578],
        [ -4.0538, -10.4452],
        [ 14.0512,  -0.8609],
        [ 14.1068,  -0.8871],
        [-10.3960,  10.3269],
        [ -1.2747,   0.2578],
        [ -1.2747,   0.2578],
        [ -1.2747,   0.2578]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  7.4893,  -2.9942,  -2.9942,  -2.9942,  -2.9942,  -2.9942,  -4.9093,
        -12.6374, -12.6481,   8.6837,  -2.9942,  -2.9942,  -2.9942],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 5.3498e-01,  1.0296e-01,  1.0296e-01,  1.0296e-01,  1.0296e-01,
          1.0296e-01,  2.1883e-02,  4.6557e-01,  4.7328e-01,  1.5558e+00,
          1.0296e-01,  1.0296e-01,  1.0296e-01],
        [-5.7293e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02,
         -1.3968e-02, -1.2288e-01, -1.1978e-01, -1.2423e-01, -6.4107e-01,
         -1.3969e-02, -1.3969e-02, -1.3968e-02],
        [-7.0564e+00,  1.1007e-01,  1.1007e-01,  1.1008e-01,  1.1008e-01,
          1.1008e-01,  2.0498e+01,  7.8040e+00,  7.1049e+00, -4.7754e+00,
          1.1008e-01,  1.1008e-01,  1.1008e-01],
        [-5.7292e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02,
         -1.3968e-02, -1.2288e-01, -1.1978e-01, -1.2423e-01, -6.4107e-01,
         -1.3969e-02, -1.3969e-02, -1.3968e-02],
        [-6.6544e+00, -6.4806e-02, -6.4807e-02, -6.4811e-02, -6.4811e-02,
         -6.4812e-02,  1.9507e+01,  7.9182e+00,  8.8845e+00, -6.3789e+00,
         -6.4814e-02, -6.4813e-02, -6.4811e-02],
        [-5.7292e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02,
         -1.3968e-02, -1.2288e-01, -1.1978e-01, -1.2423e-01, -6.4107e-01,
         -1.3969e-02, -1.3968e-02, -1.3968e-02],
        [-5.7293e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02,
         -1.3968e-02, -1.2288e-01, -1.1978e-01, -1.2423e-01, -6.4107e-01,
         -1.3969e-02, -1.3968e-02, -1.3968e-02],
        [-5.7293e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02,
         -1.3968e-02, -1.2288e-01, -1.1978e-01, -1.2423e-01, -6.4107e-01,
         -1.3969e-02, -1.3969e-02, -1.3968e-02],
        [-5.7292e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02,
         -1.3968e-02, -1.2288e-01, -1.1978e-01, -1.2423e-01, -6.4107e-01,
         -1.3968e-02, -1.3969e-02, -1.3968e-02],
        [-5.7293e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02,
         -1.3968e-02, -1.2288e-01, -1.1978e-01, -1.2423e-01, -6.4107e-01,
         -1.3969e-02, -1.3968e-02, -1.3968e-02],
        [-5.7995e+00, -3.8167e-03, -3.8157e-03, -3.8195e-03, -3.8195e-03,
         -3.8213e-03,  1.9650e+01,  1.0701e+01,  1.0308e+01, -6.9556e+00,
         -3.8241e-03, -3.8234e-03, -3.8209e-03],
        [-5.7293e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02,
         -1.3968e-02, -1.2288e-01, -1.1978e-01, -1.2423e-01, -6.4107e-01,
         -1.3969e-02, -1.3969e-02, -1.3968e-02],
        [-5.7292e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02, -1.3968e-02,
         -1.3968e-02, -1.2288e-01, -1.1978e-01, -1.2423e-01, -6.4107e-01,
         -1.3969e-02, -1.3968e-02, -1.3968e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.5777, -1.8803, -0.7269, -1.8803, -1.8836, -1.8803, -1.8803, -1.8803,
        -1.8803, -1.8803, -2.1935, -1.8803, -1.8803], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 13.3111,   0.0466, -11.2773,   0.0466, -10.8368,   0.0466,   0.0466,
           0.0466,   0.0466,   0.0466, -12.9859,   0.0466,   0.0466]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-1.1243e+01, -1.2032e+01],
        [-1.2912e+01,  7.3524e+00],
        [-1.9772e+00,  6.8682e-01],
        [-1.4697e+01, -7.3322e+00],
        [ 3.1209e+00,  1.2514e+01],
        [ 1.6151e+01,  7.0802e+00],
        [-1.5300e+01, -6.8209e-01],
        [-7.5849e+00,  1.4807e+01],
        [-1.7954e+00,  6.8443e-01],
        [ 5.9682e+00,  1.0453e+01],
        [-1.5286e+00,  1.3350e+01],
        [ 1.6216e+01,  5.9495e+00],
        [-1.5702e+01, -7.5932e-03]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-7.5186,  5.0226, -3.9297, -5.6039, -7.0568,  2.1852,  9.8822,  7.6548,
        -4.0947,  5.5251,  7.6400, -1.9395, 13.4441], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.4635e+00, -1.3475e+00,  1.6768e-01, -5.6915e-01, -3.6756e+00,
          1.5926e+00, -3.0172e+00, -1.5954e+00,  3.1344e-02,  1.0799e+00,
         -2.1623e+00,  3.4982e-01, -3.4321e+00],
        [ 1.4054e-01, -7.2965e+00, -5.1161e-01, -2.7952e-02,  7.9321e+00,
         -2.6258e+00, -7.6187e-01,  3.8991e+00, -5.9651e-01, -1.1111e+00,
          3.5253e-01, -2.4503e+00, -4.7903e+00],
        [-2.0577e+01,  4.4353e+00, -7.5328e-02,  1.2954e+00,  1.5613e+00,
         -4.2754e+00,  6.0701e+00,  9.1368e+00,  3.9018e-02, -6.9035e-02,
         -2.9963e+00, -2.8513e+00,  9.6744e+00],
        [-3.1320e+00, -1.1252e+01, -2.7264e-01, -3.1555e+00,  1.3868e+00,
          5.7717e+00, -5.7149e+00, -1.0075e+01, -7.2456e-02, -6.1744e+00,
          9.3889e+00,  6.0094e+00, -9.0941e+00],
        [-2.8115e-01,  2.7592e-01, -2.3598e-02,  4.1689e-01, -7.0703e-01,
         -1.8522e+00, -4.8244e-02, -7.2895e-02, -2.9536e-02, -1.3089e+00,
         -1.0509e+00, -1.3081e+00, -1.2269e+00],
        [ 1.3108e+00,  2.6722e-01, -4.5741e-02,  1.8338e-01, -5.9861e-01,
         -2.6373e+00, -1.0145e+00,  2.8076e-01, -4.6005e-02, -9.4576e-01,
         -3.3252e-01, -1.8041e+00, -1.2438e+00],
        [-1.8490e-01,  3.7887e-01, -1.9425e-02,  5.0514e-01, -7.9546e-01,
         -1.8936e+00, -1.6310e-01, -4.6946e-02, -2.5833e-02, -1.2958e+00,
         -1.0133e+00, -1.3686e+00, -1.2655e+00],
        [-1.8651e-01,  3.4141e-01, -2.2625e-02,  4.9161e-01, -7.6849e-01,
         -1.8839e+00, -1.8786e-01, -6.1455e-02, -2.8651e-02, -1.3012e+00,
         -1.0315e+00, -1.3624e+00, -1.2858e+00],
        [-4.9360e+00,  9.4629e+00,  8.4550e-01, -7.0800e+00, -1.2044e-05,
          2.1154e+00, -4.4544e+00,  1.8161e-01,  7.0257e-01,  4.3915e+00,
          1.0599e+00, -2.3701e+00, -8.1986e-01],
        [ 6.6800e+00,  5.6386e+00, -1.0820e-01,  3.3986e+00, -1.2228e-03,
         -1.1534e+01,  3.8410e+00,  2.5195e+00, -2.1339e-01, -2.0799e+01,
         -1.3647e+01, -7.0864e+00,  6.6965e+00],
        [ 2.6217e+00, -3.5021e+00,  1.9006e-01,  1.1428e+01,  1.4604e-03,
         -7.9143e+00,  1.2605e+00, -1.9293e+01,  2.4032e-01, -4.2716e+00,
         -1.3890e+01, -2.3483e+00,  3.5328e+00],
        [-2.5080e-01,  3.2664e-01, -2.0980e-02,  4.5187e-01, -7.4845e-01,
         -1.8664e+00, -7.5304e-02, -6.0659e-02, -2.7249e-02, -1.3091e+00,
         -1.0322e+00, -1.3266e+00, -1.2333e+00],
        [-3.0359e+00, -1.8802e+00,  8.5473e-02,  5.1556e+00,  1.7830e-03,
         -3.1732e+00,  1.5161e+00, -7.6610e+00,  1.0796e-01, -6.5351e-01,
         -7.0441e-01, -4.1448e+00,  9.9480e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  0.0842,  -3.2044, -11.5738,   2.2573,  -2.3343,  -2.7002,  -2.2275,
         -2.2345,  13.4704,  -6.8962,   3.3937,  -2.2957,  -1.1723],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.0915e+00,  5.2563e-01,  1.8862e+00,  2.1003e-01,  1.1638e+00,
          1.2654e+00,  1.1893e+00,  1.1780e+00,  1.4988e+00, -1.7940e+00,
         -1.7031e+01,  1.1770e+00,  7.5763e+00],
        [ 1.5857e-01,  4.4831e-01, -2.2045e-01,  9.7593e-01, -5.7984e-01,
         -1.2091e+00, -6.2623e-01, -6.2179e-01,  1.8117e+00, -2.4711e+00,
         -9.4077e-01, -5.9949e-01, -1.8708e+00],
        [-8.2577e-01,  3.3042e-02, -5.2291e+00,  9.0543e-01, -6.2706e-02,
          1.1590e+00, -4.3012e-03, -5.5602e-04, -6.2400e+00,  8.0816e+00,
          6.8798e+00, -4.7969e-02, -2.9211e+00],
        [-1.1202e+00,  1.7421e+00, -6.2995e+00,  7.0258e-01, -6.4531e-01,
         -7.6992e-01, -7.1057e-01, -6.9726e-01, -2.8785e-01,  1.9884e+00,
          9.9417e+00, -6.7474e-01,  3.6963e+00],
        [ 4.8235e-03,  2.7656e-03, -4.3449e-05, -1.1472e-05,  1.9136e-01,
          4.6464e-01,  2.5939e-01,  2.5844e-01,  1.9502e+01, -2.3865e+00,
         -2.2717e+00,  2.1331e-01, -6.0578e+00]], device='cuda:0'))])
xi:  [321.69473]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 645.5995489292662
W_T_median: 518.6586769544526
W_T_pctile_5: 321.70073296211604
W_T_CVAR_5_pct: 152.46329210098213
Average q (qsum/M+1):  50.04797757056452
Optimal xi:  [321.69473]
Expected(across Rb) median(across samples) p_equity:  0.27404393466810384
obj fun:  tensor(-1780.1829, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-19.2010,   8.8941],
        [ -1.1616,   0.5743],
        [ -1.1597,   0.5748],
        [ -1.1577,   0.5754],
        [ -1.1585,   0.5752],
        [ -1.1578,   0.5754],
        [ -5.8459,  -8.6554],
        [ 10.5575,  -0.2619],
        [ 10.6012,  -1.1028],
        [ -6.8086,   8.1425],
        [ -1.1594,   0.5751],
        [ -1.1584,   0.5753],
        [ -1.1580,   0.5753]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  8.3978,  -2.7932,  -2.7925,  -2.7930,  -2.7933,  -2.7935,  -4.2312,
        -10.1800, -10.0707,   7.4403,  -2.7946,  -2.7941,  -2.7934],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 5.4204e-01,  8.3361e-02,  8.3256e-02,  8.3045e-02,  8.3086e-02,
          8.3013e-02, -1.0115e-01,  3.8863e-01,  5.0295e-01,  1.6767e+00,
          8.3057e-02,  8.3011e-02,  8.3034e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-7.5221e+00,  6.2216e-02,  6.1504e-02,  6.5443e-02,  6.5902e-02,
          6.7372e-02,  1.8094e+01,  8.1252e+00,  6.9384e+00, -4.2298e+00,
          7.0226e-02,  6.9325e-02,  6.6908e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.1658e+00,  2.1394e-02,  1.6289e-02,  7.5557e-03,  9.5136e-03,
          6.5203e-03,  1.8232e+01,  6.0558e+00,  7.1338e+00, -4.0712e+00,
          9.1865e-03,  6.9017e-03,  7.3557e-03],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-8.1081e+00, -7.0500e-02, -6.5133e-02, -6.1419e-02, -6.3677e-02,
         -6.2530e-02,  1.7942e+01,  8.7360e+00,  7.8395e+00, -4.7533e+00,
         -6.7970e-02, -6.4947e-02, -6.2793e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-5.2019e+00,  2.9834e-01,  2.9866e-01,  2.9911e-01,  2.9898e-01,
          2.9913e-01,  1.1758e+01,  2.8283e+00,  2.6221e+00, -4.0749e+00,
          2.9890e-01,  2.9906e-01,  2.9909e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.3868, -1.6059, -0.9413, -1.6059, -1.0647, -1.6059, -1.6059, -1.6059,
        -1.6059, -1.6059, -1.3230, -1.6059, -1.6457], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.2981e+01,  1.2623e-02, -1.0597e+01,  1.2623e-02, -1.0303e+01,
          1.2623e-02,  1.2623e-02,  1.2623e-02,  1.2623e-02,  1.2623e-02,
         -1.0107e+01,  1.2623e-02, -4.7178e+00]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -6.8981,  -7.9649],
        [-10.9503,   6.5660],
        [ -1.8814,   0.1924],
        [-11.8131,  -4.8737],
        [  4.1311,   8.9081],
        [ 12.4619,   6.2340],
        [-11.1937,   0.2291],
        [ -1.0297,  13.7987],
        [ -1.8814,   0.1927],
        [  6.2985,   8.7234],
        [  1.5562,   9.6525],
        [ 11.3925,   4.4920],
        [-12.1812,  -0.1090]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-6.1847,  5.5629, -3.7034, -3.2978, -7.9624,  2.9958,  8.3330,  6.6302,
        -3.7044,  2.6487,  6.2144, -0.6567,  9.9923], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.9546e+00, -2.5927e-01,  1.2440e-01, -7.7089e-01, -1.6169e-01,
          1.9287e+00, -2.6426e+00, -1.3133e+01,  1.2459e-01,  3.2064e+00,
          1.3232e+00,  2.6703e-01, -6.1990e+00],
        [ 5.7482e-02,  2.8332e+00, -2.4149e-01,  2.2683e-02,  6.3432e+00,
         -1.7663e+00, -1.1378e+01,  1.8372e+00, -2.4149e-01, -2.7482e-01,
          1.6546e+00, -1.7156e+00, -1.4493e+01],
        [-1.5705e+01,  8.5627e+00,  1.7504e-01, -1.8245e+00, -3.4437e+00,
         -4.8747e+00,  4.2497e+00,  8.4923e+00,  1.7496e-01,  1.8658e+00,
         -1.3945e+00, -3.9132e+00,  5.3923e+00],
        [-2.3983e+00, -1.3750e+01,  1.8326e-01, -3.0140e+00,  3.6535e+00,
          5.4045e+00, -4.2681e+00, -1.0690e+01,  1.8262e-01,  1.2769e+00,
          6.8748e+00,  2.9334e+00, -8.0795e+00],
        [-2.5300e-01, -5.9313e-01,  6.8050e-03, -8.3695e-02, -1.3481e-01,
         -1.8726e+00, -5.9762e-01, -2.8057e-01,  6.8429e-03, -1.1632e+00,
         -8.6191e-01, -1.9899e+00, -8.2710e-01],
        [-2.5303e-01, -5.9329e-01,  6.8086e-03, -8.3941e-02, -1.3514e-01,
         -1.8726e+00, -5.9815e-01, -2.7942e-01,  6.8464e-03, -1.1625e+00,
         -8.6146e-01, -1.9897e+00, -8.2773e-01],
        [-2.5266e-01, -5.9572e-01,  6.8198e-03, -8.7278e-02, -1.4048e-01,
         -1.8723e+00, -6.0665e-01, -2.6186e-01,  6.8571e-03, -1.1521e+00,
         -8.5434e-01, -1.9862e+00, -8.3775e-01],
        [-2.7205e-01, -6.0582e-01,  8.1218e-03, -1.0733e-01, -1.6508e-01,
         -1.8679e+00, -6.5034e-01, -1.5885e-01,  8.1541e-03, -1.0841e+00,
         -8.2225e-01, -1.9486e+00, -8.8637e-01],
        [-3.6862e+00,  3.0295e+00, -1.6090e-01, -4.5702e+00,  3.8485e-05,
          9.4414e+00, -3.6149e+00,  7.3836e-02, -1.6069e-01,  4.1823e-01,
          2.7291e+00,  2.8268e+00, -1.3360e+00],
        [ 3.3685e+00, -4.4660e-01, -1.2750e-01,  4.7870e+00, -1.2035e-03,
         -1.1253e+01,  4.2450e+00, -6.9133e+00, -1.2845e-01, -1.1255e+01,
         -1.5008e+01, -5.9594e+00,  5.6190e+00],
        [ 4.6716e+00, -9.6638e+00,  2.6870e-01,  9.3119e+00,  4.1322e-03,
         -7.1652e+00,  1.1206e+00, -2.1356e+01,  2.6997e-01, -3.2723e+00,
         -6.5481e+00, -4.0466e+00,  4.0470e+00],
        [-1.3765e+00, -1.2187e+01, -7.6873e-03,  3.8040e-01,  2.2691e+00,
          3.9446e-01, -1.3470e+00, -2.3680e+00, -7.6696e-03,  3.1427e+00,
          9.8105e+00, -1.3591e+00, -5.5141e+00],
        [ 3.6637e-01,  4.3879e-01,  5.2748e-01,  2.0317e+00,  9.2198e-03,
         -1.5847e+00,  7.5107e-01, -3.2943e+00,  5.2678e-01, -2.8752e+00,
         -5.7165e-01, -2.8248e+00,  6.4352e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  0.3862,  -2.4238, -10.3032,   2.7196,  -1.9168,  -1.9169,  -1.9168,
         -1.9408,   8.5897,  -2.2269,   3.3610,  -2.3266,  -1.0903],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.1000e+00, -1.0857e-01,  1.4673e+00, -1.6669e-01,  5.5109e-01,
          5.5057e-01,  5.4300e-01,  4.9721e-01,  1.6029e+00, -3.0176e+00,
         -1.4634e+01,  5.4741e-01,  1.5294e+00],
        [ 1.3754e-01,  1.1647e+00, -2.1128e-01,  1.3751e+00, -6.1155e-01,
         -6.1140e-01, -6.0938e-01, -5.8715e-01,  1.2712e+00, -1.8266e+00,
         -3.0872e-02,  1.0355e+00, -8.8206e-01],
        [-2.1030e+00, -5.0101e-02, -2.8225e+00, -2.8128e+00,  2.5894e-01,
          2.5912e-01,  2.6232e-01,  2.6933e-01, -5.4392e+00,  7.1206e+00,
          5.6263e+00, -4.1053e-01, -9.9450e-02],
        [-8.5912e-01,  1.2201e+00, -7.6673e+00,  1.0562e+00, -1.0571e+00,
         -1.0568e+00, -1.0534e+00, -1.0078e+00, -7.4609e-01,  1.6258e+01,
          8.9904e+00,  4.8649e-01, -1.7048e+00],
        [ 3.2684e-04,  6.5766e-05,  2.3863e-03,  4.7762e-04,  6.1573e-02,
          6.2129e-02,  7.0301e-02,  1.1758e-01,  1.9040e+01, -3.9017e+00,
         -3.0945e+00, -3.3614e-03, -2.0512e+00]], device='cuda:0'))])
loaded xi:  261.8972
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1986.098664397795
Current xi:  [280.80283]
objective value function right now is: -1986.098664397795
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -2007.2994834359297
Current xi:  [297.6865]
objective value function right now is: -2007.2994834359297
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -2010.2545903978453
Current xi:  [312.50165]
objective value function right now is: -2010.2545903978453
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [325.14667]
objective value function right now is: -2009.308318579434
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -2023.5835842742238
Current xi:  [334.99026]
objective value function right now is: -2023.5835842742238
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -2027.2575855451375
Current xi:  [344.42847]
objective value function right now is: -2027.2575855451375
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [350.66238]
objective value function right now is: -2022.38868870021
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -2030.7777038952545
Current xi:  [355.50116]
objective value function right now is: -2030.7777038952545
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [359.8319]
objective value function right now is: -2026.396263742162
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -2032.4567343284018
Current xi:  [363.34103]
objective value function right now is: -2032.4567343284018
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [363.41043]
objective value function right now is: -2030.6249315756918
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [363.8818]
objective value function right now is: -2030.750027922787
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -2033.5487766186495
Current xi:  [364.9458]
objective value function right now is: -2033.5487766186495
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [364.47223]
objective value function right now is: -2031.201295600133
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [365.98248]
objective value function right now is: -2030.945673550593
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [366.38617]
objective value function right now is: -2031.3920880841301
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [367.0907]
objective value function right now is: -2024.4914861553855
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -2034.488300762842
Current xi:  [367.09113]
objective value function right now is: -2034.488300762842
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [367.0436]
objective value function right now is: -2029.5863340560493
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [367.1697]
objective value function right now is: -2013.7479587377518
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [368.86636]
objective value function right now is: -2026.414952523637
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [369.43292]
objective value function right now is: -2022.9179110933774
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [368.84128]
objective value function right now is: -2023.1236074027647
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [367.475]
objective value function right now is: -2028.2514281142307
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [368.00723]
objective value function right now is: -2028.1544194694075
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [369.1942]
objective value function right now is: -2033.1353350581837
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [369.3831]
objective value function right now is: -2031.5211799323079
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [369.4997]
objective value function right now is: -2024.4683934624109
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [370.95746]
objective value function right now is: -2032.854952707163
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [373.32367]
objective value function right now is: -2030.5266027989128
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [372.91525]
objective value function right now is: -2033.5344301913335
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [372.309]
objective value function right now is: -2030.1155239495156
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [371.4281]
objective value function right now is: -2032.5413492651624
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [369.25204]
objective value function right now is: -2022.7070487591443
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [370.04755]
objective value function right now is: -2028.404583666601
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2036.3154896069177
Current xi:  [370.29028]
objective value function right now is: -2036.3154896069177
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [370.58182]
objective value function right now is: -2035.8527830823934
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -2038.01196238797
Current xi:  [370.56537]
objective value function right now is: -2038.01196238797
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2038.5905467571547
Current xi:  [370.8045]
objective value function right now is: -2038.5905467571547
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -2039.1773063213554
Current xi:  [370.93082]
objective value function right now is: -2039.1773063213554
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [370.75635]
objective value function right now is: -2038.5642869509538
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [371.11792]
objective value function right now is: -2038.2477783410416
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [370.9254]
objective value function right now is: -2039.1641568870107
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [370.73615]
objective value function right now is: -2039.0399697590938
90.0% of gradient descent iterations done. Method = Adam
new min fval:  -2039.270586138579
Current xi:  [370.6492]
objective value function right now is: -2039.270586138579
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -2039.4236921836407
Current xi:  [370.76498]
objective value function right now is: -2039.4236921836407
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [370.8023]
objective value function right now is: -2034.9304573592547
new min fval from sgd:  -2039.445631092983
new min fval from sgd:  -2039.4935520609365
new min fval from sgd:  -2039.5277624288756
new min fval from sgd:  -2039.567418393845
new min fval from sgd:  -2039.5827823780305
new min fval from sgd:  -2039.5973392973158
new min fval from sgd:  -2039.6163305726975
new min fval from sgd:  -2039.6414096630206
new min fval from sgd:  -2039.6463737468328
new min fval from sgd:  -2039.6721736217216
new min fval from sgd:  -2039.675370839754
new min fval from sgd:  -2039.7003097867291
new min fval from sgd:  -2039.7217468179315
new min fval from sgd:  -2039.753354296349
new min fval from sgd:  -2039.7568157551518
new min fval from sgd:  -2039.8027702350932
new min fval from sgd:  -2039.815326939665
new min fval from sgd:  -2039.9132753881613
new min fval from sgd:  -2040.0250364140593
new min fval from sgd:  -2040.1422718095432
new min fval from sgd:  -2040.2290998937033
new min fval from sgd:  -2040.2439974453118
new min fval from sgd:  -2040.3138135519332
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [371.10992]
objective value function right now is: -2039.0630683601821
new min fval from sgd:  -2040.327576568854
new min fval from sgd:  -2040.341341728271
new min fval from sgd:  -2040.3521993649927
new min fval from sgd:  -2040.3695654525236
new min fval from sgd:  -2040.381057475694
new min fval from sgd:  -2040.387592779906
new min fval from sgd:  -2040.3919073566567
new min fval from sgd:  -2040.392705954739
new min fval from sgd:  -2040.3972199281895
new min fval from sgd:  -2040.403734242285
new min fval from sgd:  -2040.4086566873934
new min fval from sgd:  -2040.4161116124249
new min fval from sgd:  -2040.4252410257436
new min fval from sgd:  -2040.4323148385956
new min fval from sgd:  -2040.433743952648
new min fval from sgd:  -2040.4649528721293
new min fval from sgd:  -2040.4725117781256
new min fval from sgd:  -2040.4769882664118
new min fval from sgd:  -2040.481041363577
new min fval from sgd:  -2040.4838649809822
new min fval from sgd:  -2040.4871896071759
new min fval from sgd:  -2040.4878971360904
new min fval from sgd:  -2040.488386386226
new min fval from sgd:  -2040.4903062244869
new min fval from sgd:  -2040.4917465217402
new min fval from sgd:  -2040.4930222452438
new min fval from sgd:  -2040.4935315463026
new min fval from sgd:  -2040.4946737779364
new min fval from sgd:  -2040.500045843128
new min fval from sgd:  -2040.5016103384216
new min fval from sgd:  -2040.5129887249434
new min fval from sgd:  -2040.5230463990858
new min fval from sgd:  -2040.527831683604
new min fval from sgd:  -2040.5350771769101
new min fval from sgd:  -2040.5399855502098
new min fval from sgd:  -2040.54569769818
new min fval from sgd:  -2040.5477402342901
new min fval from sgd:  -2040.5488262075592
new min fval from sgd:  -2040.554849491593
new min fval from sgd:  -2040.5556059028875
new min fval from sgd:  -2040.5626544161844
new min fval from sgd:  -2040.563496523765
new min fval from sgd:  -2040.564315752245
new min fval from sgd:  -2040.5708296654338
new min fval from sgd:  -2040.5749987906445
new min fval from sgd:  -2040.578298455905
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [370.8631]
objective value function right now is: -2040.4798435986686
new min fval from sgd:  -2040.5841025757393
new min fval from sgd:  -2040.5940589757547
new min fval from sgd:  -2040.5991242422122
new min fval from sgd:  -2040.599517505382
new min fval from sgd:  -2040.5996834845187
new min fval from sgd:  -2040.600014017081
new min fval from sgd:  -2040.605057907682
new min fval from sgd:  -2040.618927715692
new min fval from sgd:  -2040.6327485069544
new min fval from sgd:  -2040.6508071164876
new min fval from sgd:  -2040.6637019607342
new min fval from sgd:  -2040.6764853515715
new min fval from sgd:  -2040.6854348803258
new min fval from sgd:  -2040.6891269638045
new min fval from sgd:  -2040.6938347488158
new min fval from sgd:  -2040.7005410787128
new min fval from sgd:  -2040.7073071371474
new min fval from sgd:  -2040.7174397254898
new min fval from sgd:  -2040.7251404623444
new min fval from sgd:  -2040.7317659332352
new min fval from sgd:  -2040.7351578904982
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [370.8813]
objective value function right now is: -2040.5808280695583
min fval:  -2040.7351578904982
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-22.8786,   5.5658],
        [ -1.1088,   0.3523],
        [ -1.1088,   0.3523],
        [ -1.1088,   0.3523],
        [ -1.1088,   0.3523],
        [ -1.1088,   0.3523],
        [ -1.1464, -12.0524],
        [ 14.8048,  -1.0572],
        [ 14.8425,  -1.0768],
        [-11.0281,   8.5953],
        [ -1.1088,   0.3523],
        [ -1.1088,   0.3523],
        [ -1.1088,   0.3523]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  5.5720,  -3.2298,  -3.2298,  -3.2298,  -3.2298,  -3.2298,  -5.3376,
        -12.5235, -12.5156,   7.1652,  -3.2298,  -3.2298,  -3.2298],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.6324, -0.0245, -0.0245, -0.0245, -0.0245, -0.0245,  0.2273,  0.2678,
          0.2709,  1.4120, -0.0245, -0.0245, -0.0245],
        [-0.2904, -0.0269, -0.0269, -0.0269, -0.0269, -0.0269, -0.2886, -0.2666,
         -0.2725, -0.8376, -0.0269, -0.0269, -0.0269],
        [-6.4459,  0.2754,  0.2754,  0.2754,  0.2754,  0.2754, 21.7308,  8.2166,
          7.3436, -5.1508,  0.2755,  0.2754,  0.2754],
        [-0.2904, -0.0269, -0.0269, -0.0269, -0.0269, -0.0269, -0.2886, -0.2666,
         -0.2725, -0.8376, -0.0269, -0.0269, -0.0269],
        [-5.1907, -0.1518, -0.1518, -0.1518, -0.1518, -0.1518, 20.2070,  8.9973,
          9.9113, -6.9163, -0.1518, -0.1518, -0.1518],
        [-0.2904, -0.0269, -0.0269, -0.0269, -0.0269, -0.0269, -0.2886, -0.2666,
         -0.2725, -0.8376, -0.0269, -0.0269, -0.0269],
        [-0.2904, -0.0269, -0.0269, -0.0269, -0.0269, -0.0269, -0.2886, -0.2666,
         -0.2725, -0.8376, -0.0269, -0.0269, -0.0269],
        [-0.2904, -0.0269, -0.0269, -0.0269, -0.0269, -0.0269, -0.2886, -0.2666,
         -0.2725, -0.8376, -0.0269, -0.0269, -0.0269],
        [-0.2904, -0.0269, -0.0269, -0.0269, -0.0269, -0.0269, -0.2886, -0.2666,
         -0.2725, -0.8376, -0.0269, -0.0269, -0.0269],
        [-0.2904, -0.0269, -0.0269, -0.0269, -0.0269, -0.0269, -0.2886, -0.2666,
         -0.2725, -0.8376, -0.0269, -0.0269, -0.0269],
        [-4.5260, -0.0437, -0.0437, -0.0437, -0.0437, -0.0437, 20.5726, 11.5472,
         11.0013, -7.5483, -0.0437, -0.0437, -0.0437],
        [-0.2904, -0.0269, -0.0269, -0.0269, -0.0269, -0.0269, -0.2886, -0.2666,
         -0.2725, -0.8376, -0.0269, -0.0269, -0.0269],
        [-0.2904, -0.0269, -0.0269, -0.0269, -0.0269, -0.0269, -0.2886, -0.2666,
         -0.2725, -0.8376, -0.0269, -0.0269, -0.0269]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.6835, -1.9957, -1.0858, -1.9957, -2.9884, -1.9957, -1.9957, -1.9957,
        -1.9957, -1.9957, -3.2458, -1.9957, -1.9957], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 13.1258,  -0.0908, -11.3504,  -0.0908, -10.5086,  -0.0908,  -0.0908,
          -0.0908,  -0.0908,  -0.0908, -13.4109,  -0.0908,  -0.0908]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-10.6689, -11.5810],
        [-13.7878,  12.0204],
        [ -2.7004,   3.6442],
        [-14.3815,  -7.2571],
        [  6.4094,  14.8318],
        [ 16.2995,   7.2268],
        [-14.3981,  -0.3101],
        [  4.7426,  15.2086],
        [ -1.7383,   2.9051],
        [  8.9935,   9.8983],
        [  1.0318,  18.5409],
        [ 17.7805,   5.5677],
        [-15.4730,  -0.0539]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-6.4964,  7.6899, -3.9000, -5.5525, -6.8624,  2.3943, 11.1484,  7.9654,
        -3.3602,  5.1201,  7.7239, -2.8968, 13.4447], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.3460e+00, -1.9539e+00, -9.0714e-01,  7.2211e-02, -1.9170e+00,
          3.7402e-01, -5.1164e+00, -1.6170e+00, -6.2691e-02,  1.6442e+00,
         -1.1981e+00, -1.1676e+00, -8.2698e+00],
        [-1.2354e+00, -7.9161e-02,  1.5656e+00,  1.6239e-03,  1.4795e+00,
         -3.2227e+00,  1.3043e+00,  6.8405e-01,  1.5141e+00, -6.3936e-01,
         -7.7628e-01, -2.5684e+00,  1.0502e+00],
        [-2.9420e+01,  1.6217e+01,  1.1668e+00,  3.9055e-02, -6.9894e+00,
         -3.7063e+00,  4.5962e+00,  5.8968e+00,  1.0692e+00,  1.8634e+00,
         -5.4520e+00, -3.3093e+00,  6.7117e+00],
        [ 2.3163e+00, -2.2157e-01, -2.7249e-01, -1.2471e+00, -1.4452e-01,
          8.2424e-01, -3.3712e+00, -5.2259e-01, -2.6686e-01, -5.3946e+00,
         -4.7359e-01, -2.1216e-01, -1.1560e+01],
        [ 3.0710e-01,  1.6475e-01,  2.8189e-01, -3.3454e-01,  4.6999e-01,
         -2.1027e+00, -7.9671e-01, -1.0300e+00,  2.9007e-01, -1.4943e+00,
          3.0092e-01, -1.2768e+00, -1.4178e+00],
        [-9.9073e-01,  4.7035e-01, -1.9890e+00, -2.3171e-01,  3.7382e+00,
         -5.8245e-01, -7.5942e-01, -1.5993e+00,  2.2475e-01, -1.4589e+00,
          1.7855e-01, -2.5772e+00, -1.0927e+00],
        [-4.0511e+00, -2.1805e+00,  8.7402e-02, -7.1702e-01,  7.2991e-02,
         -1.7725e+00,  1.3604e+00,  1.6390e+00,  1.9658e-01, -2.4154e-01,
         -5.0288e+00, -4.4264e+00,  1.3211e-01],
        [-2.5122e+00,  2.1072e-01, -2.0775e+00, -2.1249e+00,  3.2934e+00,
         -7.1121e-01, -9.5137e-01, -4.5604e+00, -1.5585e+00,  4.8077e-01,
          1.5895e+00, -5.3824e-01,  1.3623e+00],
        [-4.4939e+00,  8.6623e+00,  2.6208e-01, -5.6520e+00, -7.5727e-06,
          5.3546e+00, -6.3029e+00,  1.5508e+00,  6.6131e-01,  1.5538e+01,
          6.2379e-02,  9.6790e-01,  8.5355e-01],
        [ 7.4486e+00,  1.5939e+00, -1.7322e-01,  6.4905e+00,  4.1252e-05,
         -1.1579e+01,  5.1576e+00, -1.8697e+01, -3.1444e-01, -1.6585e+01,
         -3.0479e+00, -8.7528e+00,  5.1712e+00],
        [ 1.9372e+00, -6.7880e+00, -9.0766e-02,  1.0151e+01, -7.7689e-04,
         -6.4139e+00,  5.5024e-02, -1.1579e+01, -5.8842e-01, -9.7301e-01,
         -1.8505e+01, -3.2092e+00,  2.9517e+00],
        [-3.2724e+00, -5.3388e-01, -9.0881e-03, -1.6594e+00,  5.2938e-02,
         -2.6572e+00,  6.2138e-01, -6.5564e-01, -4.0504e-02, -8.1741e-01,
         -2.9884e+00, -4.0453e+00,  2.0283e-01],
        [ 7.5556e-01,  6.0849e-01,  1.4767e-01,  2.8595e-01,  7.3656e-01,
         -1.9127e+00, -3.8168e-02,  1.7867e-02,  2.8104e-01, -8.9838e-01,
          5.1063e-01, -1.6271e+00, -1.5719e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -0.9378,  -3.0498, -12.1849,  -0.8466,  -2.4052,  -2.7052,  -2.1314,
         -1.7798,  12.2911,  -4.6018,   4.5893,  -2.3877,  -2.3439],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.4018e-01,  3.7375e+00,  1.5586e+00,  7.1812e-01,  1.2369e+00,
          1.6419e+00,  6.1117e+00,  6.4016e-01,  1.7516e+00, -5.6926e-01,
         -1.7211e+01,  4.9012e+00,  1.4900e+00],
        [ 3.5300e-01, -1.8007e+00,  2.2016e-02, -3.6335e-01, -5.9958e-01,
         -8.8487e-01, -4.7629e-01,  8.5106e-01,  1.8004e+00, -2.3290e+00,
         -1.6257e+00, -4.7070e-01, -8.5186e-01],
        [-3.7892e-01, -1.3902e+00, -2.5135e+00,  1.6977e+00, -2.5768e-01,
         -1.3202e+00, -4.1823e+00, -1.8242e+00, -7.0553e+00,  8.0302e+00,
          7.6446e+00, -2.8817e+00, -3.7612e-01],
        [-6.5712e+00, -4.8689e-01, -5.7408e+00,  3.7045e+00,  2.5689e-01,
          2.3193e+00,  3.9753e+00,  4.5143e+00, -7.1316e-01,  1.3207e+00,
          1.1270e+01,  2.8658e+00, -2.5292e-01],
        [-3.2825e-04, -1.0875e+00,  9.9186e-08,  8.3818e-04,  1.6044e-01,
          2.6653e-02, -1.1181e-01,  2.1528e-02,  2.4339e+01, -5.5094e+00,
         -5.2603e+00, -5.1989e-02, -1.9075e-01]], device='cuda:0'))])
xi:  [370.86798]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 665.4957062559743
W_T_median: 540.08410751671
W_T_pctile_5: 370.85129252741814
W_T_CVAR_5_pct: 186.15556165517185
Average q (qsum/M+1):  47.81510679183468
Optimal xi:  [370.86798]
Expected(across Rb) median(across samples) p_equity:  0.2597390150030454
obj fun:  tensor(-2040.7352, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-19.2010,   8.8941],
        [ -1.1616,   0.5743],
        [ -1.1597,   0.5748],
        [ -1.1577,   0.5754],
        [ -1.1585,   0.5752],
        [ -1.1578,   0.5754],
        [ -5.8459,  -8.6554],
        [ 10.5575,  -0.2619],
        [ 10.6012,  -1.1028],
        [ -6.8086,   8.1425],
        [ -1.1594,   0.5751],
        [ -1.1584,   0.5753],
        [ -1.1580,   0.5753]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  8.3978,  -2.7932,  -2.7925,  -2.7930,  -2.7933,  -2.7935,  -4.2312,
        -10.1800, -10.0707,   7.4403,  -2.7946,  -2.7941,  -2.7934],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 5.4204e-01,  8.3361e-02,  8.3256e-02,  8.3045e-02,  8.3086e-02,
          8.3013e-02, -1.0115e-01,  3.8863e-01,  5.0295e-01,  1.6767e+00,
          8.3057e-02,  8.3011e-02,  8.3034e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-7.5221e+00,  6.2216e-02,  6.1504e-02,  6.5443e-02,  6.5902e-02,
          6.7372e-02,  1.8094e+01,  8.1252e+00,  6.9384e+00, -4.2298e+00,
          7.0226e-02,  6.9325e-02,  6.6908e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.1658e+00,  2.1394e-02,  1.6289e-02,  7.5557e-03,  9.5136e-03,
          6.5203e-03,  1.8232e+01,  6.0558e+00,  7.1338e+00, -4.0712e+00,
          9.1865e-03,  6.9017e-03,  7.3557e-03],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-8.1081e+00, -7.0500e-02, -6.5133e-02, -6.1419e-02, -6.3677e-02,
         -6.2530e-02,  1.7942e+01,  8.7360e+00,  7.8395e+00, -4.7533e+00,
         -6.7970e-02, -6.4947e-02, -6.2793e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-5.2019e+00,  2.9834e-01,  2.9866e-01,  2.9911e-01,  2.9898e-01,
          2.9913e-01,  1.1758e+01,  2.8283e+00,  2.6221e+00, -4.0749e+00,
          2.9890e-01,  2.9906e-01,  2.9909e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.3868, -1.6059, -0.9413, -1.6059, -1.0647, -1.6059, -1.6059, -1.6059,
        -1.6059, -1.6059, -1.3230, -1.6059, -1.6457], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.2981e+01,  1.2623e-02, -1.0597e+01,  1.2623e-02, -1.0303e+01,
          1.2623e-02,  1.2623e-02,  1.2623e-02,  1.2623e-02,  1.2623e-02,
         -1.0107e+01,  1.2623e-02, -4.7178e+00]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -6.8981,  -7.9649],
        [-10.9503,   6.5660],
        [ -1.8814,   0.1924],
        [-11.8131,  -4.8737],
        [  4.1311,   8.9081],
        [ 12.4619,   6.2340],
        [-11.1937,   0.2291],
        [ -1.0297,  13.7987],
        [ -1.8814,   0.1927],
        [  6.2985,   8.7234],
        [  1.5562,   9.6525],
        [ 11.3925,   4.4920],
        [-12.1812,  -0.1090]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-6.1847,  5.5629, -3.7034, -3.2978, -7.9624,  2.9958,  8.3330,  6.6302,
        -3.7044,  2.6487,  6.2144, -0.6567,  9.9923], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.9546e+00, -2.5927e-01,  1.2440e-01, -7.7089e-01, -1.6169e-01,
          1.9287e+00, -2.6426e+00, -1.3133e+01,  1.2459e-01,  3.2064e+00,
          1.3232e+00,  2.6703e-01, -6.1990e+00],
        [ 5.7482e-02,  2.8332e+00, -2.4149e-01,  2.2683e-02,  6.3432e+00,
         -1.7663e+00, -1.1378e+01,  1.8372e+00, -2.4149e-01, -2.7482e-01,
          1.6546e+00, -1.7156e+00, -1.4493e+01],
        [-1.5705e+01,  8.5627e+00,  1.7504e-01, -1.8245e+00, -3.4437e+00,
         -4.8747e+00,  4.2497e+00,  8.4923e+00,  1.7496e-01,  1.8658e+00,
         -1.3945e+00, -3.9132e+00,  5.3923e+00],
        [-2.3983e+00, -1.3750e+01,  1.8326e-01, -3.0140e+00,  3.6535e+00,
          5.4045e+00, -4.2681e+00, -1.0690e+01,  1.8262e-01,  1.2769e+00,
          6.8748e+00,  2.9334e+00, -8.0795e+00],
        [-2.5300e-01, -5.9313e-01,  6.8050e-03, -8.3695e-02, -1.3481e-01,
         -1.8726e+00, -5.9762e-01, -2.8057e-01,  6.8429e-03, -1.1632e+00,
         -8.6191e-01, -1.9899e+00, -8.2710e-01],
        [-2.5303e-01, -5.9329e-01,  6.8086e-03, -8.3941e-02, -1.3514e-01,
         -1.8726e+00, -5.9815e-01, -2.7942e-01,  6.8464e-03, -1.1625e+00,
         -8.6146e-01, -1.9897e+00, -8.2773e-01],
        [-2.5266e-01, -5.9572e-01,  6.8198e-03, -8.7278e-02, -1.4048e-01,
         -1.8723e+00, -6.0665e-01, -2.6186e-01,  6.8571e-03, -1.1521e+00,
         -8.5434e-01, -1.9862e+00, -8.3775e-01],
        [-2.7205e-01, -6.0582e-01,  8.1218e-03, -1.0733e-01, -1.6508e-01,
         -1.8679e+00, -6.5034e-01, -1.5885e-01,  8.1541e-03, -1.0841e+00,
         -8.2225e-01, -1.9486e+00, -8.8637e-01],
        [-3.6862e+00,  3.0295e+00, -1.6090e-01, -4.5702e+00,  3.8485e-05,
          9.4414e+00, -3.6149e+00,  7.3836e-02, -1.6069e-01,  4.1823e-01,
          2.7291e+00,  2.8268e+00, -1.3360e+00],
        [ 3.3685e+00, -4.4660e-01, -1.2750e-01,  4.7870e+00, -1.2035e-03,
         -1.1253e+01,  4.2450e+00, -6.9133e+00, -1.2845e-01, -1.1255e+01,
         -1.5008e+01, -5.9594e+00,  5.6190e+00],
        [ 4.6716e+00, -9.6638e+00,  2.6870e-01,  9.3119e+00,  4.1322e-03,
         -7.1652e+00,  1.1206e+00, -2.1356e+01,  2.6997e-01, -3.2723e+00,
         -6.5481e+00, -4.0466e+00,  4.0470e+00],
        [-1.3765e+00, -1.2187e+01, -7.6873e-03,  3.8040e-01,  2.2691e+00,
          3.9446e-01, -1.3470e+00, -2.3680e+00, -7.6696e-03,  3.1427e+00,
          9.8105e+00, -1.3591e+00, -5.5141e+00],
        [ 3.6637e-01,  4.3879e-01,  5.2748e-01,  2.0317e+00,  9.2198e-03,
         -1.5847e+00,  7.5107e-01, -3.2943e+00,  5.2678e-01, -2.8752e+00,
         -5.7165e-01, -2.8248e+00,  6.4352e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  0.3862,  -2.4238, -10.3032,   2.7196,  -1.9168,  -1.9169,  -1.9168,
         -1.9408,   8.5897,  -2.2269,   3.3610,  -2.3266,  -1.0903],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.1000e+00, -1.0857e-01,  1.4673e+00, -1.6669e-01,  5.5109e-01,
          5.5057e-01,  5.4300e-01,  4.9721e-01,  1.6029e+00, -3.0176e+00,
         -1.4634e+01,  5.4741e-01,  1.5294e+00],
        [ 1.3754e-01,  1.1647e+00, -2.1128e-01,  1.3751e+00, -6.1155e-01,
         -6.1140e-01, -6.0938e-01, -5.8715e-01,  1.2712e+00, -1.8266e+00,
         -3.0872e-02,  1.0355e+00, -8.8206e-01],
        [-2.1030e+00, -5.0101e-02, -2.8225e+00, -2.8128e+00,  2.5894e-01,
          2.5912e-01,  2.6232e-01,  2.6933e-01, -5.4392e+00,  7.1206e+00,
          5.6263e+00, -4.1053e-01, -9.9450e-02],
        [-8.5912e-01,  1.2201e+00, -7.6673e+00,  1.0562e+00, -1.0571e+00,
         -1.0568e+00, -1.0534e+00, -1.0078e+00, -7.4609e-01,  1.6258e+01,
          8.9904e+00,  4.8649e-01, -1.7048e+00],
        [ 3.2684e-04,  6.5766e-05,  2.3863e-03,  4.7762e-04,  6.1573e-02,
          6.2129e-02,  7.0301e-02,  1.1758e-01,  1.9040e+01, -3.9017e+00,
         -3.0945e+00, -3.3614e-03, -2.0512e+00]], device='cuda:0'))])
loaded xi:  261.8972
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2338.6771957093983
Current xi:  [282.2467]
objective value function right now is: -2338.6771957093983
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -2352.7196510483077
Current xi:  [300.63583]
objective value function right now is: -2352.7196510483077
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -2361.3882385307784
Current xi:  [317.39935]
objective value function right now is: -2361.3882385307784
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -2382.742842952407
Current xi:  [331.71512]
objective value function right now is: -2382.742842952407
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [344.31284]
objective value function right now is: -2369.747241660193
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -2384.452787552782
Current xi:  [354.50742]
objective value function right now is: -2384.452787552782
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -2391.030100514517
Current xi:  [362.47006]
objective value function right now is: -2391.030100514517
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [369.74597]
objective value function right now is: -2363.987323479169
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -2399.3126046129782
Current xi:  [374.3203]
objective value function right now is: -2399.3126046129782
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [376.32877]
objective value function right now is: -2365.636904896923
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [379.41812]
objective value function right now is: -2392.275905392695
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -2411.4167699302316
Current xi:  [380.69095]
objective value function right now is: -2411.4167699302316
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [382.70737]
objective value function right now is: -2407.9014202149037
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [384.5027]
objective value function right now is: -2408.4711486266724
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -2416.6115776788247
Current xi:  [383.555]
objective value function right now is: -2416.6115776788247
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [383.63693]
objective value function right now is: -2416.573615571484
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [383.09793]
objective value function right now is: -2389.440608826295
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [384.34952]
objective value function right now is: -2409.106978075424
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [383.47784]
objective value function right now is: -2402.194840610371
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [383.2353]
objective value function right now is: -2409.774284476837
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [383.53668]
objective value function right now is: -2416.1131454107535
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [383.03656]
objective value function right now is: -2415.1886721116334
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [384.34204]
objective value function right now is: -2395.369473075659
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [385.1072]
objective value function right now is: -2415.5258861872026
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [385.91766]
objective value function right now is: -2399.85845018069
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [388.26538]
objective value function right now is: -2412.780246015954
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [388.1419]
objective value function right now is: -2401.858329832892
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [388.4485]
objective value function right now is: -2412.3251058754886
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [388.21127]
objective value function right now is: -2407.031615461232
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [388.75797]
objective value function right now is: -2410.011388722001
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [388.77356]
objective value function right now is: -2411.435154523328
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [387.86398]
objective value function right now is: -2410.3805526549377
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [385.436]
objective value function right now is: -2413.047884790723
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -2418.205229928825
Current xi:  [386.5177]
objective value function right now is: -2418.205229928825
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [386.7301]
objective value function right now is: -2394.4665967354827
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2420.5556699196363
Current xi:  [386.7159]
objective value function right now is: -2420.5556699196363
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [387.11295]
objective value function right now is: -2420.527773848478
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -2421.803224958923
Current xi:  [387.37314]
objective value function right now is: -2421.803224958923
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2423.2120116207548
Current xi:  [387.40076]
objective value function right now is: -2423.2120116207548
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -2423.408212328272
Current xi:  [388.04953]
objective value function right now is: -2423.408212328272
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [388.2621]
objective value function right now is: -2418.6820453798528
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [388.3002]
objective value function right now is: -2422.1043398799284
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -2423.9154281850647
Current xi:  [388.2998]
objective value function right now is: -2423.9154281850647
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [388.5275]
objective value function right now is: -2420.3436113103644
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [388.4843]
objective value function right now is: -2423.575649984644
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -2424.572938677919
Current xi:  [388.1027]
objective value function right now is: -2424.572938677919
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [387.6602]
objective value function right now is: -2417.344639436217
new min fval from sgd:  -2424.6234957319703
new min fval from sgd:  -2424.674244812099
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [387.9139]
objective value function right now is: -2424.017991936734
new min fval from sgd:  -2424.6934042573666
new min fval from sgd:  -2424.756254814122
new min fval from sgd:  -2424.7856351146324
new min fval from sgd:  -2424.7926012312723
new min fval from sgd:  -2424.799258330628
new min fval from sgd:  -2424.802374732855
new min fval from sgd:  -2424.8118624467156
new min fval from sgd:  -2424.823924570579
new min fval from sgd:  -2424.836268046145
new min fval from sgd:  -2424.8580509614962
new min fval from sgd:  -2424.876163041817
new min fval from sgd:  -2424.8825353734865
new min fval from sgd:  -2424.8969062070273
new min fval from sgd:  -2424.9056642275705
new min fval from sgd:  -2424.9145949959066
new min fval from sgd:  -2424.9271822043133
new min fval from sgd:  -2424.934698599622
new min fval from sgd:  -2424.9415733182127
new min fval from sgd:  -2424.948344796831
new min fval from sgd:  -2424.953309482433
new min fval from sgd:  -2424.958424090942
new min fval from sgd:  -2424.9640422803163
new min fval from sgd:  -2424.9704391965697
new min fval from sgd:  -2424.975184068202
new min fval from sgd:  -2424.9794597079167
new min fval from sgd:  -2424.9874538177837
new min fval from sgd:  -2424.995828789899
new min fval from sgd:  -2425.0024483482935
new min fval from sgd:  -2425.0102025432006
new min fval from sgd:  -2425.0140063954545
new min fval from sgd:  -2425.0146782978336
new min fval from sgd:  -2425.0179822955524
new min fval from sgd:  -2425.019656259137
new min fval from sgd:  -2425.0203461900196
new min fval from sgd:  -2425.022014514066
new min fval from sgd:  -2425.02276184854
new min fval from sgd:  -2425.0232961524007
new min fval from sgd:  -2425.026603356539
new min fval from sgd:  -2425.0320031622637
new min fval from sgd:  -2425.033865459488
new min fval from sgd:  -2425.03467323926
new min fval from sgd:  -2425.0388866306635
new min fval from sgd:  -2425.0401588473937
new min fval from sgd:  -2425.0427692335256
new min fval from sgd:  -2425.0465477728735
new min fval from sgd:  -2425.0516553959314
new min fval from sgd:  -2425.0588493788064
new min fval from sgd:  -2425.061385682618
new min fval from sgd:  -2425.0625692209474
new min fval from sgd:  -2425.062849572968
new min fval from sgd:  -2425.079825059798
new min fval from sgd:  -2425.0951715903257
new min fval from sgd:  -2425.104050299284
new min fval from sgd:  -2425.10821423995
new min fval from sgd:  -2425.1129416195686
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [387.81952]
objective value function right now is: -2424.872282205212
new min fval from sgd:  -2425.1150858590595
new min fval from sgd:  -2425.137672048959
new min fval from sgd:  -2425.157934463321
new min fval from sgd:  -2425.178191995666
new min fval from sgd:  -2425.2095019013586
new min fval from sgd:  -2425.2297240002563
new min fval from sgd:  -2425.2508407953806
new min fval from sgd:  -2425.269769302845
new min fval from sgd:  -2425.279726031668
new min fval from sgd:  -2425.282325361956
new min fval from sgd:  -2425.2890938268874
new min fval from sgd:  -2425.299604829286
new min fval from sgd:  -2425.307408979104
new min fval from sgd:  -2425.3178180954533
new min fval from sgd:  -2425.324290286256
new min fval from sgd:  -2425.3244370730004
new min fval from sgd:  -2425.3264101424393
new min fval from sgd:  -2425.3270025714987
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [387.89413]
objective value function right now is: -2424.953238339415
min fval:  -2425.3270025714987
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-23.3864,   5.5968],
        [ -1.1716,   0.2895],
        [ -1.1716,   0.2895],
        [ -1.1716,   0.2895],
        [ -1.1716,   0.2895],
        [ -1.1716,   0.2895],
        [ -0.2186, -12.7347],
        [ 14.8048,  -0.9603],
        [ 14.8753,  -0.9864],
        [-12.0001,   8.1444],
        [ -1.1716,   0.2895],
        [ -1.1716,   0.2895],
        [ -1.1716,   0.2895]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  5.2205,  -3.4097,  -3.4097,  -3.4097,  -3.4097,  -3.4097,  -5.5237,
        -12.3431, -12.3099,   6.4891,  -3.4097,  -3.4097,  -3.4097],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 6.3860e-01,  3.1048e-02,  3.1048e-02,  3.1048e-02,  3.1048e-02,
          3.1048e-02,  2.5522e-01,  3.1368e-01,  3.3396e-01,  1.2174e+00,
          3.1048e-02,  3.1048e-02,  3.1048e-02],
        [-1.3748e-01, -1.4311e-02, -1.4311e-02, -1.4311e-02, -1.4311e-02,
         -1.4311e-02, -2.3567e-01, -1.8777e-01, -2.0183e-01, -7.1662e-01,
         -1.4311e-02, -1.4311e-02, -1.4311e-02],
        [-6.6016e+00,  1.7968e-01,  1.7968e-01,  1.7968e-01,  1.7968e-01,
          1.7968e-01,  2.1958e+01,  7.4228e+00,  6.6961e+00, -5.1485e+00,
          1.7968e-01,  1.7968e-01,  1.7968e-01],
        [-1.3748e-01, -1.4311e-02, -1.4311e-02, -1.4311e-02, -1.4311e-02,
         -1.4311e-02, -2.3567e-01, -1.8777e-01, -2.0183e-01, -7.1662e-01,
         -1.4311e-02, -1.4311e-02, -1.4311e-02],
        [-5.0609e+00, -1.4157e-01, -1.4157e-01, -1.4158e-01, -1.4158e-01,
         -1.4158e-01,  2.0613e+01,  8.7951e+00,  9.8082e+00, -7.1069e+00,
         -1.4158e-01, -1.4158e-01, -1.4158e-01],
        [-1.3748e-01, -1.4311e-02, -1.4311e-02, -1.4311e-02, -1.4311e-02,
         -1.4311e-02, -2.3567e-01, -1.8777e-01, -2.0183e-01, -7.1662e-01,
         -1.4311e-02, -1.4311e-02, -1.4311e-02],
        [-1.3748e-01, -1.4311e-02, -1.4311e-02, -1.4311e-02, -1.4311e-02,
         -1.4311e-02, -2.3567e-01, -1.8777e-01, -2.0183e-01, -7.1662e-01,
         -1.4311e-02, -1.4311e-02, -1.4311e-02],
        [-1.3748e-01, -1.4311e-02, -1.4311e-02, -1.4311e-02, -1.4311e-02,
         -1.4311e-02, -2.3567e-01, -1.8777e-01, -2.0183e-01, -7.1662e-01,
         -1.4311e-02, -1.4311e-02, -1.4311e-02],
        [-1.3748e-01, -1.4311e-02, -1.4311e-02, -1.4311e-02, -1.4311e-02,
         -1.4311e-02, -2.3567e-01, -1.8777e-01, -2.0183e-01, -7.1662e-01,
         -1.4311e-02, -1.4311e-02, -1.4311e-02],
        [-1.3748e-01, -1.4311e-02, -1.4311e-02, -1.4311e-02, -1.4311e-02,
         -1.4311e-02, -2.3567e-01, -1.8777e-01, -2.0183e-01, -7.1662e-01,
         -1.4311e-02, -1.4311e-02, -1.4311e-02],
        [-4.1148e+00, -7.6840e-02, -7.6840e-02, -7.6840e-02, -7.6840e-02,
         -7.6841e-02,  2.0945e+01,  1.1365e+01,  1.0967e+01, -8.0292e+00,
         -7.6842e-02, -7.6842e-02, -7.6841e-02],
        [-1.3748e-01, -1.4311e-02, -1.4311e-02, -1.4311e-02, -1.4311e-02,
         -1.4311e-02, -2.3567e-01, -1.8777e-01, -2.0183e-01, -7.1662e-01,
         -1.4311e-02, -1.4311e-02, -1.4311e-02],
        [-1.3748e-01, -1.4311e-02, -1.4311e-02, -1.4311e-02, -1.4311e-02,
         -1.4311e-02, -2.3566e-01, -1.8777e-01, -2.0183e-01, -7.1661e-01,
         -1.4311e-02, -1.4311e-02, -1.4311e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 5.2510, -2.3180, -0.9704, -2.3180, -3.2182, -2.3180, -2.3180, -2.3180,
        -2.3180, -2.3180, -3.4594, -2.3180, -2.3180], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 12.8116,   0.0140, -11.3648,   0.0140, -10.6900,   0.0140,   0.0140,
           0.0140,   0.0140,   0.0140, -13.8141,   0.0140,   0.0140]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-11.7046, -11.5253],
        [-15.6724,  10.5881],
        [ -2.6378,   0.1630],
        [-16.2609,  -7.7898],
        [  5.7817,  12.0858],
        [ 17.0657,   7.4759],
        [-12.1996,   1.3303],
        [ -4.5638,  16.6531],
        [ -2.7681,   0.2525],
        [  1.5408,  11.1876],
        [ -0.4603,  12.8548],
        [ 17.5983,   5.1625],
        [-14.5730,  -0.4271]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-5.8099,  7.2788, -4.4051, -4.4533, -9.0927,  2.3748, 12.3650,  7.6902,
        -4.2784,  7.1992,  4.6498, -3.4254, 12.2607], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.5509e-01, -5.1194e-01, -2.1126e-02, -9.4732e-02, -8.6002e-02,
         -2.0601e+00, -1.6971e+00, -3.9414e-02, -2.1302e-02, -1.0885e+00,
         -1.1487e-01, -1.5725e+00, -1.3974e+00],
        [-5.7360e+00,  6.2929e-01,  2.0002e-01, -2.2916e-01, -2.8558e+00,
         -9.8187e-01, -1.0370e+00, -6.6982e-02,  2.6508e-01,  1.2069e+00,
          6.1942e-01, -1.7175e-01, -2.2503e+00],
        [-3.4662e+01,  1.7651e+01,  6.1042e-02,  3.2925e+00, -9.8850e+00,
         -3.4357e+00,  3.5272e+00, -4.6741e+00,  1.1586e-01,  6.3417e+00,
         -1.7931e-01, -2.2508e+00,  6.8009e+00],
        [-1.1125e+01,  1.8257e+00,  7.5992e-02, -1.4339e-02,  7.6716e-01,
          1.9844e+00, -6.6043e+00, -5.3494e-01,  8.9573e-02, -1.7714e+00,
         -5.0944e-01,  7.2458e-01, -9.8052e+00],
        [-2.4122e-01,  7.4549e-01, -7.2034e-02,  9.4750e-01, -7.9433e-01,
         -2.3286e+00, -5.9931e-01,  1.5637e+00, -2.1145e-01,  9.8400e-01,
          8.1108e-01, -1.2725e+00, -2.3136e+00],
        [-3.3331e+00,  5.3455e-01, -6.6547e-03, -5.0778e-02, -1.9224e+00,
         -1.7113e+00, -1.7197e+00, -7.6258e-01, -3.3881e-03,  6.1840e-01,
         -4.1881e-02, -9.8494e-01, -2.9264e+00],
        [-3.5977e+00,  5.2835e-01, -3.8230e-03, -7.8051e-02, -2.1857e+00,
         -1.6771e+00, -1.7429e+00, -6.6859e-01,  5.0422e-04,  6.7488e-01,
          5.6116e-03, -9.5053e-01, -2.7458e+00],
        [-5.9446e-01,  2.2947e+00,  1.3654e-01, -3.7991e-01, -1.3217e+00,
         -3.1109e+00, -2.7122e-01,  7.8126e-01,  8.5196e-02,  4.4107e-01,
         -9.0586e-01, -4.6490e+00,  6.0865e-01],
        [-3.3310e+00,  9.8997e+00,  2.1933e+00, -8.6812e+00, -4.3067e-06,
          1.0222e+01, -4.7705e+00,  1.2305e-01,  2.4697e+00,  2.2438e+01,
          3.6807e-01, -2.1428e-01,  1.9732e+00],
        [ 8.2174e+00,  3.4064e+00, -1.5429e-01,  1.3083e+01, -5.0365e-04,
         -1.3505e+01,  3.4975e+00, -5.6593e+00, -1.8108e-01, -2.1744e+01,
         -8.4148e+00, -6.7472e+00,  4.2304e+00],
        [ 2.1552e+00, -4.3704e+00, -4.1682e-03,  1.1919e+01, -3.2529e-03,
         -6.4903e+00,  9.8009e-01, -2.6158e+01,  5.6474e-03, -5.2135e+00,
         -1.4013e+01, -2.4566e+00,  1.4742e+00],
        [-1.3059e+00,  8.0375e-01, -1.0379e-01, -2.4246e-01, -3.2294e-01,
         -2.6110e+00, -9.0309e-01, -1.6309e+00, -1.0521e-01, -1.2710e+00,
         -1.3842e+00, -1.4253e+00,  2.8911e-01],
        [-7.1543e+00,  5.1710e-01,  1.0486e-01,  3.3318e-01, -2.0763e-01,
         -1.7690e+00,  1.3359e+00, -5.6265e+00,  1.7683e-01,  1.3466e+00,
         -1.2775e+00, -4.8812e+00,  2.9284e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -2.5048,  -1.6909, -11.1764,  -0.4762,  -3.1610,  -2.3565,  -2.2766,
         -2.7110,  12.2027,  -4.8199,   4.5830,  -2.8437,  -2.1778],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 3.4467e-01, -6.8774e-01,  1.2843e+00, -4.3610e-01,  2.2612e-01,
         -8.2526e-01, -8.5774e-01,  2.5603e+00,  1.8908e+00, -1.7532e+00,
         -1.2901e+01,  2.0360e+00,  4.0122e+00],
        [-2.2596e-01,  6.5653e-01,  1.1951e-01,  1.5442e+00, -3.4362e-01,
          7.5742e-01,  7.9667e-01, -1.1056e+00,  1.6452e+00, -2.2847e+00,
         -1.8121e+00, -9.6626e-01, -1.5080e+00],
        [-3.1463e-01, -6.6696e-01, -4.1023e+00, -8.3025e-01, -1.0058e+00,
         -7.8605e-02, -9.3414e-02, -5.5378e-01, -7.1368e+00,  8.2504e+00,
          7.3283e+00, -4.3239e-01, -1.5102e+00],
        [ 1.7612e-01, -3.3544e+00, -6.2624e+00, -4.1181e+00, -3.1150e+00,
         -1.9928e+00, -2.1262e+00, -2.6537e+00, -7.7557e-01,  2.4632e+00,
          1.0222e+01,  2.0903e+00,  4.3669e+00],
        [-6.6368e-02, -4.2774e-03,  1.5098e-08,  1.0594e-07, -2.0003e-02,
         -2.3157e-03, -2.4867e-03,  2.5097e-01,  2.5511e+01, -6.2573e+00,
         -5.9105e+00,  4.0240e-03,  4.0209e-02]], device='cuda:0'))])
xi:  [387.86282]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 671.3652861684112
W_T_median: 553.5519740066784
W_T_pctile_5: 387.88054741332974
W_T_CVAR_5_pct: 196.462054702714
Average q (qsum/M+1):  46.54894231980847
Optimal xi:  [387.86282]
Expected(across Rb) median(across samples) p_equity:  0.238874156276385
obj fun:  tensor(-2425.3270, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-19.2010,   8.8941],
        [ -1.1616,   0.5743],
        [ -1.1597,   0.5748],
        [ -1.1577,   0.5754],
        [ -1.1585,   0.5752],
        [ -1.1578,   0.5754],
        [ -5.8459,  -8.6554],
        [ 10.5575,  -0.2619],
        [ 10.6012,  -1.1028],
        [ -6.8086,   8.1425],
        [ -1.1594,   0.5751],
        [ -1.1584,   0.5753],
        [ -1.1580,   0.5753]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  8.3978,  -2.7932,  -2.7925,  -2.7930,  -2.7933,  -2.7935,  -4.2312,
        -10.1800, -10.0707,   7.4403,  -2.7946,  -2.7941,  -2.7934],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 5.4204e-01,  8.3361e-02,  8.3256e-02,  8.3045e-02,  8.3086e-02,
          8.3013e-02, -1.0115e-01,  3.8863e-01,  5.0295e-01,  1.6767e+00,
          8.3057e-02,  8.3011e-02,  8.3034e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-7.5221e+00,  6.2216e-02,  6.1504e-02,  6.5443e-02,  6.5902e-02,
          6.7372e-02,  1.8094e+01,  8.1252e+00,  6.9384e+00, -4.2298e+00,
          7.0226e-02,  6.9325e-02,  6.6908e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.1658e+00,  2.1394e-02,  1.6289e-02,  7.5557e-03,  9.5136e-03,
          6.5203e-03,  1.8232e+01,  6.0558e+00,  7.1338e+00, -4.0712e+00,
          9.1865e-03,  6.9017e-03,  7.3557e-03],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-8.1081e+00, -7.0500e-02, -6.5133e-02, -6.1419e-02, -6.3677e-02,
         -6.2530e-02,  1.7942e+01,  8.7360e+00,  7.8395e+00, -4.7533e+00,
         -6.7970e-02, -6.4947e-02, -6.2793e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-5.2019e+00,  2.9834e-01,  2.9866e-01,  2.9911e-01,  2.9898e-01,
          2.9913e-01,  1.1758e+01,  2.8283e+00,  2.6221e+00, -4.0749e+00,
          2.9890e-01,  2.9906e-01,  2.9909e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.3868, -1.6059, -0.9413, -1.6059, -1.0647, -1.6059, -1.6059, -1.6059,
        -1.6059, -1.6059, -1.3230, -1.6059, -1.6457], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.2981e+01,  1.2623e-02, -1.0597e+01,  1.2623e-02, -1.0303e+01,
          1.2623e-02,  1.2623e-02,  1.2623e-02,  1.2623e-02,  1.2623e-02,
         -1.0107e+01,  1.2623e-02, -4.7178e+00]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -6.8981,  -7.9649],
        [-10.9503,   6.5660],
        [ -1.8814,   0.1924],
        [-11.8131,  -4.8737],
        [  4.1311,   8.9081],
        [ 12.4619,   6.2340],
        [-11.1937,   0.2291],
        [ -1.0297,  13.7987],
        [ -1.8814,   0.1927],
        [  6.2985,   8.7234],
        [  1.5562,   9.6525],
        [ 11.3925,   4.4920],
        [-12.1812,  -0.1090]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-6.1847,  5.5629, -3.7034, -3.2978, -7.9624,  2.9958,  8.3330,  6.6302,
        -3.7044,  2.6487,  6.2144, -0.6567,  9.9923], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.9546e+00, -2.5927e-01,  1.2440e-01, -7.7089e-01, -1.6169e-01,
          1.9287e+00, -2.6426e+00, -1.3133e+01,  1.2459e-01,  3.2064e+00,
          1.3232e+00,  2.6703e-01, -6.1990e+00],
        [ 5.7482e-02,  2.8332e+00, -2.4149e-01,  2.2683e-02,  6.3432e+00,
         -1.7663e+00, -1.1378e+01,  1.8372e+00, -2.4149e-01, -2.7482e-01,
          1.6546e+00, -1.7156e+00, -1.4493e+01],
        [-1.5705e+01,  8.5627e+00,  1.7504e-01, -1.8245e+00, -3.4437e+00,
         -4.8747e+00,  4.2497e+00,  8.4923e+00,  1.7496e-01,  1.8658e+00,
         -1.3945e+00, -3.9132e+00,  5.3923e+00],
        [-2.3983e+00, -1.3750e+01,  1.8326e-01, -3.0140e+00,  3.6535e+00,
          5.4045e+00, -4.2681e+00, -1.0690e+01,  1.8262e-01,  1.2769e+00,
          6.8748e+00,  2.9334e+00, -8.0795e+00],
        [-2.5300e-01, -5.9313e-01,  6.8050e-03, -8.3695e-02, -1.3481e-01,
         -1.8726e+00, -5.9762e-01, -2.8057e-01,  6.8429e-03, -1.1632e+00,
         -8.6191e-01, -1.9899e+00, -8.2710e-01],
        [-2.5303e-01, -5.9329e-01,  6.8086e-03, -8.3941e-02, -1.3514e-01,
         -1.8726e+00, -5.9815e-01, -2.7942e-01,  6.8464e-03, -1.1625e+00,
         -8.6146e-01, -1.9897e+00, -8.2773e-01],
        [-2.5266e-01, -5.9572e-01,  6.8198e-03, -8.7278e-02, -1.4048e-01,
         -1.8723e+00, -6.0665e-01, -2.6186e-01,  6.8571e-03, -1.1521e+00,
         -8.5434e-01, -1.9862e+00, -8.3775e-01],
        [-2.7205e-01, -6.0582e-01,  8.1218e-03, -1.0733e-01, -1.6508e-01,
         -1.8679e+00, -6.5034e-01, -1.5885e-01,  8.1541e-03, -1.0841e+00,
         -8.2225e-01, -1.9486e+00, -8.8637e-01],
        [-3.6862e+00,  3.0295e+00, -1.6090e-01, -4.5702e+00,  3.8485e-05,
          9.4414e+00, -3.6149e+00,  7.3836e-02, -1.6069e-01,  4.1823e-01,
          2.7291e+00,  2.8268e+00, -1.3360e+00],
        [ 3.3685e+00, -4.4660e-01, -1.2750e-01,  4.7870e+00, -1.2035e-03,
         -1.1253e+01,  4.2450e+00, -6.9133e+00, -1.2845e-01, -1.1255e+01,
         -1.5008e+01, -5.9594e+00,  5.6190e+00],
        [ 4.6716e+00, -9.6638e+00,  2.6870e-01,  9.3119e+00,  4.1322e-03,
         -7.1652e+00,  1.1206e+00, -2.1356e+01,  2.6997e-01, -3.2723e+00,
         -6.5481e+00, -4.0466e+00,  4.0470e+00],
        [-1.3765e+00, -1.2187e+01, -7.6873e-03,  3.8040e-01,  2.2691e+00,
          3.9446e-01, -1.3470e+00, -2.3680e+00, -7.6696e-03,  3.1427e+00,
          9.8105e+00, -1.3591e+00, -5.5141e+00],
        [ 3.6637e-01,  4.3879e-01,  5.2748e-01,  2.0317e+00,  9.2198e-03,
         -1.5847e+00,  7.5107e-01, -3.2943e+00,  5.2678e-01, -2.8752e+00,
         -5.7165e-01, -2.8248e+00,  6.4352e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  0.3862,  -2.4238, -10.3032,   2.7196,  -1.9168,  -1.9169,  -1.9168,
         -1.9408,   8.5897,  -2.2269,   3.3610,  -2.3266,  -1.0903],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.1000e+00, -1.0857e-01,  1.4673e+00, -1.6669e-01,  5.5109e-01,
          5.5057e-01,  5.4300e-01,  4.9721e-01,  1.6029e+00, -3.0176e+00,
         -1.4634e+01,  5.4741e-01,  1.5294e+00],
        [ 1.3754e-01,  1.1647e+00, -2.1128e-01,  1.3751e+00, -6.1155e-01,
         -6.1140e-01, -6.0938e-01, -5.8715e-01,  1.2712e+00, -1.8266e+00,
         -3.0872e-02,  1.0355e+00, -8.8206e-01],
        [-2.1030e+00, -5.0101e-02, -2.8225e+00, -2.8128e+00,  2.5894e-01,
          2.5912e-01,  2.6232e-01,  2.6933e-01, -5.4392e+00,  7.1206e+00,
          5.6263e+00, -4.1053e-01, -9.9450e-02],
        [-8.5912e-01,  1.2201e+00, -7.6673e+00,  1.0562e+00, -1.0571e+00,
         -1.0568e+00, -1.0534e+00, -1.0078e+00, -7.4609e-01,  1.6258e+01,
          8.9904e+00,  4.8649e-01, -1.7048e+00],
        [ 3.2684e-04,  6.5766e-05,  2.3863e-03,  4.7762e-04,  6.1573e-02,
          6.2129e-02,  7.0301e-02,  1.1758e-01,  1.9040e+01, -3.9017e+00,
         -3.0945e+00, -3.3614e-03, -2.0512e+00]], device='cuda:0'))])
loaded xi:  261.8972
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -3192.4174177598697
Current xi:  [283.48657]
objective value function right now is: -3192.4174177598697
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -3270.4018791156022
Current xi:  [302.91425]
objective value function right now is: -3270.4018791156022
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -3310.126437790393
Current xi:  [319.10748]
objective value function right now is: -3310.126437790393
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -3346.427096545481
Current xi:  [334.84558]
objective value function right now is: -3346.427096545481
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -3379.0653373213336
Current xi:  [349.3708]
objective value function right now is: -3379.0653373213336
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [360.7161]
objective value function right now is: -3361.7299640269443
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [370.66672]
objective value function right now is: -3359.7810220338683
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -3388.1730302205397
Current xi:  [378.05423]
objective value function right now is: -3388.1730302205397
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [384.0938]
objective value function right now is: -3373.7525794270246
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -3403.8197031185523
Current xi:  [388.05075]
objective value function right now is: -3403.8197031185523
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [390.7891]
objective value function right now is: -3401.4551826115744
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [392.56122]
objective value function right now is: -3397.0344983919663
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [394.59485]
objective value function right now is: -3346.862064458274
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [396.0083]
objective value function right now is: -3310.2382227665566
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [398.9836]
objective value function right now is: -3372.661460392975
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [399.6489]
objective value function right now is: -3366.8621114915104
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -3406.1545341803335
Current xi:  [398.44092]
objective value function right now is: -3406.1545341803335
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -3416.453644548653
Current xi:  [398.23398]
objective value function right now is: -3416.453644548653
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [398.8017]
objective value function right now is: -3412.754643497394
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [397.77414]
objective value function right now is: -3413.5333798179468
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [397.41574]
objective value function right now is: -3395.140932430377
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [398.9037]
objective value function right now is: -3412.30098491601
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [397.96176]
objective value function right now is: -3400.237147274691
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [398.9296]
objective value function right now is: -3415.2274904877545
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [398.34708]
objective value function right now is: -3408.3502544232138
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [399.8917]
objective value function right now is: -3411.0716353766566
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [398.06332]
objective value function right now is: -3407.7563758055458
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [397.70697]
objective value function right now is: -3415.7773002712033
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [400.16486]
objective value function right now is: -3384.158696796108
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -3422.1759778717487
Current xi:  [399.46396]
objective value function right now is: -3422.1759778717487
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [399.3886]
objective value function right now is: -3400.9702584240054
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [399.65424]
objective value function right now is: -3397.0668239434626
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [399.13437]
objective value function right now is: -3393.2713015053823
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [400.0665]
objective value function right now is: -3416.706188151964
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [399.41208]
objective value function right now is: -3415.391916172142
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -3422.7174109040443
Current xi:  [399.35678]
objective value function right now is: -3422.7174109040443
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -3429.728654575207
Current xi:  [399.03384]
objective value function right now is: -3429.728654575207
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -3432.3331925391726
Current xi:  [399.22845]
objective value function right now is: -3432.3331925391726
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [399.4571]
objective value function right now is: -3425.8429986952146
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [399.92056]
objective value function right now is: -3426.486186303628
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [400.24152]
objective value function right now is: -3431.872755370564
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [400.09366]
objective value function right now is: -3430.0938154119785
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [400.47635]
objective value function right now is: -3430.003071994136
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [400.41385]
objective value function right now is: -3430.5976545631174
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [400.14386]
objective value function right now is: -3430.2645038918586
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [400.39786]
objective value function right now is: -3426.8717003416446
94.0% of gradient descent iterations done. Method = Adam
new min fval:  -3433.5603128407183
Current xi:  [400.79932]
objective value function right now is: -3433.5603128407183
new min fval from sgd:  -3433.6176492889876
new min fval from sgd:  -3433.752544308477
new min fval from sgd:  -3433.888729173161
new min fval from sgd:  -3433.9610840434007
new min fval from sgd:  -3434.014469081141
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [400.58246]
objective value function right now is: -3429.3922308364304
new min fval from sgd:  -3434.025810298677
new min fval from sgd:  -3434.043434383406
new min fval from sgd:  -3434.0596743974884
new min fval from sgd:  -3434.084872457664
new min fval from sgd:  -3434.1082173548666
new min fval from sgd:  -3434.131092312288
new min fval from sgd:  -3434.141298550861
new min fval from sgd:  -3434.153287453672
new min fval from sgd:  -3434.1662824309674
new min fval from sgd:  -3434.169521178426
new min fval from sgd:  -3434.1722924547335
new min fval from sgd:  -3434.1786849615955
new min fval from sgd:  -3434.1895105794474
new min fval from sgd:  -3434.2070402888407
new min fval from sgd:  -3434.227413473255
new min fval from sgd:  -3434.2464700954038
new min fval from sgd:  -3434.2620869391926
new min fval from sgd:  -3434.271559114158
new min fval from sgd:  -3434.2800064151243
new min fval from sgd:  -3434.2930110558123
new min fval from sgd:  -3434.294919871039
new min fval from sgd:  -3434.310846291965
new min fval from sgd:  -3434.3266771053995
new min fval from sgd:  -3434.347385221215
new min fval from sgd:  -3434.3681708615777
new min fval from sgd:  -3434.3772464421404
new min fval from sgd:  -3434.3917856603084
new min fval from sgd:  -3434.4312356909004
new min fval from sgd:  -3434.459075609565
new min fval from sgd:  -3434.476800632814
new min fval from sgd:  -3434.489496066628
new min fval from sgd:  -3434.4977688984104
new min fval from sgd:  -3434.5038175182767
new min fval from sgd:  -3434.50497408401
new min fval from sgd:  -3434.508320625676
new min fval from sgd:  -3434.514263788728
new min fval from sgd:  -3434.5182413123
new min fval from sgd:  -3434.522635428124
new min fval from sgd:  -3434.524443937389
new min fval from sgd:  -3434.528774873908
new min fval from sgd:  -3434.5370282046647
new min fval from sgd:  -3434.542283692669
new min fval from sgd:  -3434.552965260163
new min fval from sgd:  -3434.564080524662
new min fval from sgd:  -3434.5817404705044
new min fval from sgd:  -3434.5904980032183
new min fval from sgd:  -3434.6072285317464
new min fval from sgd:  -3434.611302839529
new min fval from sgd:  -3434.623997765135
new min fval from sgd:  -3434.6397659921126
new min fval from sgd:  -3434.64449516453
new min fval from sgd:  -3434.647605279305
new min fval from sgd:  -3434.6570954712006
new min fval from sgd:  -3434.662332840516
new min fval from sgd:  -3434.663627231475
new min fval from sgd:  -3434.6654353803106
new min fval from sgd:  -3434.6762256106235
new min fval from sgd:  -3434.7086018296213
new min fval from sgd:  -3434.728368873997
new min fval from sgd:  -3434.7390194544632
new min fval from sgd:  -3434.7494788556032
new min fval from sgd:  -3434.7718095049945
new min fval from sgd:  -3434.783242369856
new min fval from sgd:  -3434.8129363458215
new min fval from sgd:  -3434.81346918187
new min fval from sgd:  -3434.8240354492236
new min fval from sgd:  -3434.841162948779
new min fval from sgd:  -3434.8537168864555
new min fval from sgd:  -3434.8541230394276
new min fval from sgd:  -3434.8577547211844
new min fval from sgd:  -3434.862379310637
new min fval from sgd:  -3434.8658514327212
new min fval from sgd:  -3434.8712449414825
new min fval from sgd:  -3434.878958918243
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [400.64474]
objective value function right now is: -3434.772238961273
new min fval from sgd:  -3434.881981208335
new min fval from sgd:  -3434.898474428561
new min fval from sgd:  -3434.912279719525
new min fval from sgd:  -3434.9177398695824
new min fval from sgd:  -3434.934002781886
new min fval from sgd:  -3434.943983932053
new min fval from sgd:  -3434.9711025237284
new min fval from sgd:  -3434.985025219246
new min fval from sgd:  -3434.9900716160832
new min fval from sgd:  -3434.9917864995077
new min fval from sgd:  -3435.0312955888558
new min fval from sgd:  -3435.039207743994
new min fval from sgd:  -3435.049536386339
new min fval from sgd:  -3435.0535545092334
new min fval from sgd:  -3435.059236146362
new min fval from sgd:  -3435.069141405547
new min fval from sgd:  -3435.077307982834
new min fval from sgd:  -3435.0830028835976
new min fval from sgd:  -3435.0883123868907
new min fval from sgd:  -3435.093140337216
new min fval from sgd:  -3435.0972925435003
new min fval from sgd:  -3435.1078333708765
new min fval from sgd:  -3435.116462566828
new min fval from sgd:  -3435.125560248737
new min fval from sgd:  -3435.130109060373
new min fval from sgd:  -3435.130922436445
new min fval from sgd:  -3435.140451085098
new min fval from sgd:  -3435.1405961973032
new min fval from sgd:  -3435.145672049314
new min fval from sgd:  -3435.1792215647174
new min fval from sgd:  -3435.216694696442
new min fval from sgd:  -3435.258519409245
new min fval from sgd:  -3435.297900986896
new min fval from sgd:  -3435.321136189442
new min fval from sgd:  -3435.3499497026896
new min fval from sgd:  -3435.369618436466
new min fval from sgd:  -3435.383490984451
new min fval from sgd:  -3435.406911663806
new min fval from sgd:  -3435.43191105717
new min fval from sgd:  -3435.4624409348908
new min fval from sgd:  -3435.4915263841162
new min fval from sgd:  -3435.505359823421
new min fval from sgd:  -3435.511693342248
new min fval from sgd:  -3435.5248632623407
new min fval from sgd:  -3435.5376319668176
new min fval from sgd:  -3435.5426015312014
new min fval from sgd:  -3435.558799955625
new min fval from sgd:  -3435.5780722478016
new min fval from sgd:  -3435.6052064487135
new min fval from sgd:  -3435.6356075560543
new min fval from sgd:  -3435.6603941599633
new min fval from sgd:  -3435.678768420729
new min fval from sgd:  -3435.6935948144505
new min fval from sgd:  -3435.6941089657125
new min fval from sgd:  -3435.6989661154794
new min fval from sgd:  -3435.7006127295526
new min fval from sgd:  -3435.709574079747
new min fval from sgd:  -3435.7110793784404
new min fval from sgd:  -3435.7142031284216
new min fval from sgd:  -3435.7158991819724
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [400.59238]
objective value function right now is: -3435.066676828826
min fval:  -3435.7158991819724
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-14.7327,  13.0090],
        [ -0.6056,   0.5566],
        [ -0.6056,   0.5566],
        [ -0.6056,   0.5566],
        [ -0.6056,   0.5566],
        [ -0.6056,   0.5566],
        [  2.3645, -12.2769],
        [ 15.7212,  -1.2953],
        [ 15.7703,  -1.4349],
        [ -7.4609,  14.5847],
        [ -0.6056,   0.5566],
        [ -0.6056,   0.5566],
        [ -0.6056,   0.5566]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  7.4039,  -3.5872,  -3.5872,  -3.5871,  -3.5872,  -3.5871,  -5.4880,
        -11.8573, -11.8477,   6.5074,  -3.5872,  -3.5872,  -3.5871],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 1.7189, -0.1198, -0.1198, -0.1198, -0.1198, -0.1198,  0.6718,  0.3993,
          0.4108,  1.6320, -0.1198, -0.1198, -0.1198],
        [-0.9950, -0.0653, -0.0653, -0.0653, -0.0653, -0.0653, -0.6585, -0.1338,
         -0.1446, -1.0869, -0.0653, -0.0653, -0.0653],
        [-8.9893,  0.4130,  0.4130,  0.4130,  0.4130,  0.4130, 18.7729, 12.1197,
         11.1947, -5.9992,  0.4130,  0.4130,  0.4130],
        [-0.9950, -0.0653, -0.0653, -0.0653, -0.0653, -0.0653, -0.6585, -0.1338,
         -0.1446, -1.0869, -0.0653, -0.0653, -0.0653],
        [-8.7432, -0.1276, -0.1276, -0.1276, -0.1276, -0.1276, 18.9537,  3.2541,
          3.4985, -8.0731, -0.1276, -0.1276, -0.1276],
        [-0.9950, -0.0653, -0.0653, -0.0653, -0.0653, -0.0653, -0.6585, -0.1338,
         -0.1446, -1.0869, -0.0653, -0.0653, -0.0653],
        [-0.9950, -0.0653, -0.0653, -0.0653, -0.0653, -0.0653, -0.6585, -0.1338,
         -0.1446, -1.0869, -0.0653, -0.0653, -0.0653],
        [-0.9950, -0.0653, -0.0653, -0.0653, -0.0653, -0.0653, -0.6585, -0.1338,
         -0.1446, -1.0869, -0.0653, -0.0653, -0.0653],
        [-0.9950, -0.0653, -0.0653, -0.0653, -0.0653, -0.0653, -0.6585, -0.1338,
         -0.1446, -1.0869, -0.0653, -0.0653, -0.0653],
        [-0.9950, -0.0653, -0.0653, -0.0653, -0.0653, -0.0653, -0.6585, -0.1338,
         -0.1446, -1.0869, -0.0653, -0.0653, -0.0653],
        [-7.6085, -0.1959, -0.1958, -0.1958, -0.1958, -0.1958, 17.8988, 15.6143,
         15.1346, -6.8063, -0.1958, -0.1958, -0.1958],
        [-0.9950, -0.0653, -0.0653, -0.0653, -0.0653, -0.0653, -0.6585, -0.1338,
         -0.1446, -1.0869, -0.0653, -0.0653, -0.0653],
        [-0.9950, -0.0653, -0.0653, -0.0653, -0.0653, -0.0653, -0.6585, -0.1338,
         -0.1446, -1.0869, -0.0653, -0.0653, -0.0653]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 3.9782, -2.1763,  1.8934, -2.1763, -2.1535, -2.1763, -2.1763, -2.1763,
        -2.1763, -2.1763, -2.7879, -2.1763, -2.1763], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 11.9861,  -0.1679, -11.2327,  -0.1679,  -9.6189,  -0.1679,  -0.1679,
          -0.1679,  -0.1679,  -0.1679, -14.6498,  -0.1679,  -0.1679]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-10.9040, -11.4181],
        [-15.9873,   9.7508],
        [  1.2582,  10.5971],
        [-15.4874,  -8.0663],
        [  3.0086,   8.2371],
        [ 16.8240,   8.1077],
        [-14.5969,  -0.0357],
        [ -7.6841,  17.4233],
        [  1.2549,  10.5927],
        [  8.5324,  11.3673],
        [  1.9684,  14.3184],
        [ 16.6130,   5.3181],
        [-12.7742,   2.1736]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-5.8052,  6.9686, -4.3960, -5.0496, -8.6176,  2.8716, 11.2915,  7.6783,
        -4.3964,  4.9245,  7.4954, -2.9440, 14.4793], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.2031e+01,  1.3075e+01, -3.8958e-01, -7.3413e+00, -1.0083e+00,
          3.3320e-01,  3.8788e+00,  1.9263e+01, -3.8906e-01,  3.9453e+00,
          2.0927e+00,  1.7670e-01, -4.4822e+00],
        [-1.3218e+00, -7.8235e-01, -4.8489e-01, -1.8808e-01, -7.0055e-01,
         -2.0219e+00, -1.8445e+00, -2.6419e-02, -4.8535e-01, -7.8992e-01,
          2.0760e-01, -1.4434e+00, -1.8112e+00],
        [-2.9738e+01,  1.8198e+01, -5.6395e+00, -2.6653e+00, -8.0809e+00,
         -4.5248e+00,  4.6179e+00, -6.0832e+00, -5.6359e+00,  4.8554e+00,
          5.5875e+00, -2.7580e+00,  5.4009e+00],
        [ 1.1283e+00, -3.3914e+00,  3.6687e-01,  2.1890e-01, -7.1206e-01,
         -8.4206e-01, -3.0314e+00,  2.2192e+00,  3.6531e-01, -7.4667e-01,
          1.0676e+00, -4.2393e-01, -1.5822e+00],
        [-4.5317e+00,  7.3963e-02, -3.1609e+00,  2.4773e-01, -9.1494e-01,
         -4.6216e-01, -6.2451e+00, -1.4963e+00, -3.1588e+00,  1.1643e+00,
         -1.4653e+00,  3.2863e-01, -2.9747e+00],
        [-7.1556e-01, -9.9303e-01, -2.9562e-01, -1.7466e-01, -6.9175e-01,
         -2.1019e+00, -1.9198e+00,  3.6748e-01, -2.9631e-01, -8.5637e-01,
          3.0805e-01, -1.3091e+00, -1.9803e+00],
        [ 1.4741e-01,  1.4308e+00, -1.0899e+00, -5.3869e-01, -1.3494e+00,
         -2.1773e+00,  1.0737e+00, -4.1320e-01, -1.0903e+00, -6.2150e-01,
          1.9367e+00, -4.2785e+00, -1.5300e+00],
        [-3.8815e-01, -2.2849e-01, -1.2808e-01, -2.8822e-01, -1.6064e-01,
         -2.3403e+00, -5.3678e-01, -1.8176e-01, -1.2822e-01, -1.3048e+00,
         -9.2384e-02, -9.3786e-01, -2.5635e+00],
        [-1.5639e+00,  1.7826e+01, -1.8926e-04, -8.8377e+00, -2.2401e-04,
          2.4813e+01, -4.9394e+00,  2.4464e-02, -1.9072e-04,  5.6836e+00,
          2.3089e+00,  2.3043e+00, -5.1563e-01],
        [ 7.0100e+00,  4.6724e-01,  4.3203e-02,  1.1346e+01,  8.8080e-03,
         -1.3420e+01,  3.5650e+00, -8.8839e+00,  4.3264e-02, -2.6801e+01,
         -1.7628e+01, -6.0163e+00,  4.6624e+00],
        [ 2.9906e+00, -4.3814e+00,  2.7511e-02,  1.2442e+01,  5.0712e-03,
         -7.9100e+00,  1.0207e+00, -2.8838e+01,  2.7509e-02, -2.4832e+00,
         -1.0510e+01, -2.7858e+00,  3.2197e+00],
        [ 2.5413e-01, -7.7074e-01, -2.1158e-01, -1.7862e-01, -5.3675e-01,
         -2.0641e+00, -1.6529e+00,  4.2597e-01, -2.1231e-01, -1.2225e+00,
          1.3577e-01, -1.0761e+00, -2.6573e+00],
        [-4.9655e+00,  3.1927e-01, -1.0880e-01,  5.8975e-01, -1.9099e-01,
         -1.3512e+00,  1.3234e+00, -8.1827e+00, -1.0915e-01,  7.4919e-01,
          2.9986e+00, -4.0225e+00, -3.1608e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -2.0406,  -2.3661, -12.6875,  -1.8430,  -1.2653,  -2.6145,  -2.6116,
         -3.4797,  12.8242,  -4.2293,   3.3606,  -2.6643,  -3.0979],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.0050e+00,  8.9834e-01,  1.1399e+00,  2.0369e+00, -6.3716e-01,
          8.8759e-01,  4.4642e+00,  8.3844e-01,  1.5395e+00, -6.3035e-02,
         -2.3749e+01,  8.1937e-01,  2.8126e+00],
        [ 2.4638e-01, -6.3967e-01,  2.9889e-01, -1.6781e+00,  5.9767e-01,
         -5.6085e-01, -1.9366e+00, -5.4964e-01,  1.8319e+00, -2.7125e+00,
         -1.3739e+00, -7.3745e-01, -1.5568e+00],
        [-5.4431e+00, -1.2835e-01, -1.2443e-01,  7.3106e-01, -3.4120e-01,
          9.3940e-02, -1.4194e+00,  2.4572e-02, -6.8469e+00,  8.3745e+00,
          7.4347e+00, -6.7719e-03, -2.5735e+00],
        [-1.4826e+00, -7.1513e-01, -7.1959e+00,  1.0503e+00, -7.4467e+00,
         -2.0640e-01, -4.8634e-01,  3.7039e-01, -1.6348e-02,  6.1240e-01,
          1.3379e+01,  6.2269e-01, -2.4633e-01],
        [-1.4975e-08,  1.9453e-02,  1.9585e-09,  3.2942e-01, -8.7078e-03,
          4.9723e-02,  1.4538e-02,  4.3419e-02,  2.9072e+01, -6.7972e+00,
         -6.2666e+00,  2.0558e-01, -2.7105e-02]], device='cuda:0'))])
xi:  [400.6068]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 678.738483790545
W_T_median: 564.6473786163705
W_T_pctile_5: 400.786944185983
W_T_CVAR_5_pct: 203.66298957981292
Average q (qsum/M+1):  45.13191075478831
Optimal xi:  [400.6068]
Expected(across Rb) median(across samples) p_equity:  0.2170789498835802
obj fun:  tensor(-3435.7159, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 10.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-19.2010,   8.8941],
        [ -1.1616,   0.5743],
        [ -1.1597,   0.5748],
        [ -1.1577,   0.5754],
        [ -1.1585,   0.5752],
        [ -1.1578,   0.5754],
        [ -5.8459,  -8.6554],
        [ 10.5575,  -0.2619],
        [ 10.6012,  -1.1028],
        [ -6.8086,   8.1425],
        [ -1.1594,   0.5751],
        [ -1.1584,   0.5753],
        [ -1.1580,   0.5753]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  8.3978,  -2.7932,  -2.7925,  -2.7930,  -2.7933,  -2.7935,  -4.2312,
        -10.1800, -10.0707,   7.4403,  -2.7946,  -2.7941,  -2.7934],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 5.4204e-01,  8.3361e-02,  8.3256e-02,  8.3045e-02,  8.3086e-02,
          8.3013e-02, -1.0115e-01,  3.8863e-01,  5.0295e-01,  1.6767e+00,
          8.3057e-02,  8.3011e-02,  8.3034e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-7.5221e+00,  6.2216e-02,  6.1504e-02,  6.5443e-02,  6.5902e-02,
          6.7372e-02,  1.8094e+01,  8.1252e+00,  6.9384e+00, -4.2298e+00,
          7.0226e-02,  6.9325e-02,  6.6908e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.1658e+00,  2.1394e-02,  1.6289e-02,  7.5557e-03,  9.5136e-03,
          6.5203e-03,  1.8232e+01,  6.0558e+00,  7.1338e+00, -4.0712e+00,
          9.1865e-03,  6.9017e-03,  7.3557e-03],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-8.1081e+00, -7.0500e-02, -6.5133e-02, -6.1419e-02, -6.3677e-02,
         -6.2530e-02,  1.7942e+01,  8.7360e+00,  7.8395e+00, -4.7533e+00,
         -6.7970e-02, -6.4947e-02, -6.2793e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-5.2019e+00,  2.9834e-01,  2.9866e-01,  2.9911e-01,  2.9898e-01,
          2.9913e-01,  1.1758e+01,  2.8283e+00,  2.6221e+00, -4.0749e+00,
          2.9890e-01,  2.9906e-01,  2.9909e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.3868, -1.6059, -0.9413, -1.6059, -1.0647, -1.6059, -1.6059, -1.6059,
        -1.6059, -1.6059, -1.3230, -1.6059, -1.6457], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.2981e+01,  1.2623e-02, -1.0597e+01,  1.2623e-02, -1.0303e+01,
          1.2623e-02,  1.2623e-02,  1.2623e-02,  1.2623e-02,  1.2623e-02,
         -1.0107e+01,  1.2623e-02, -4.7178e+00]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -6.8981,  -7.9649],
        [-10.9503,   6.5660],
        [ -1.8814,   0.1924],
        [-11.8131,  -4.8737],
        [  4.1311,   8.9081],
        [ 12.4619,   6.2340],
        [-11.1937,   0.2291],
        [ -1.0297,  13.7987],
        [ -1.8814,   0.1927],
        [  6.2985,   8.7234],
        [  1.5562,   9.6525],
        [ 11.3925,   4.4920],
        [-12.1812,  -0.1090]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-6.1847,  5.5629, -3.7034, -3.2978, -7.9624,  2.9958,  8.3330,  6.6302,
        -3.7044,  2.6487,  6.2144, -0.6567,  9.9923], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.9546e+00, -2.5927e-01,  1.2440e-01, -7.7089e-01, -1.6169e-01,
          1.9287e+00, -2.6426e+00, -1.3133e+01,  1.2459e-01,  3.2064e+00,
          1.3232e+00,  2.6703e-01, -6.1990e+00],
        [ 5.7482e-02,  2.8332e+00, -2.4149e-01,  2.2683e-02,  6.3432e+00,
         -1.7663e+00, -1.1378e+01,  1.8372e+00, -2.4149e-01, -2.7482e-01,
          1.6546e+00, -1.7156e+00, -1.4493e+01],
        [-1.5705e+01,  8.5627e+00,  1.7504e-01, -1.8245e+00, -3.4437e+00,
         -4.8747e+00,  4.2497e+00,  8.4923e+00,  1.7496e-01,  1.8658e+00,
         -1.3945e+00, -3.9132e+00,  5.3923e+00],
        [-2.3983e+00, -1.3750e+01,  1.8326e-01, -3.0140e+00,  3.6535e+00,
          5.4045e+00, -4.2681e+00, -1.0690e+01,  1.8262e-01,  1.2769e+00,
          6.8748e+00,  2.9334e+00, -8.0795e+00],
        [-2.5300e-01, -5.9313e-01,  6.8050e-03, -8.3695e-02, -1.3481e-01,
         -1.8726e+00, -5.9762e-01, -2.8057e-01,  6.8429e-03, -1.1632e+00,
         -8.6191e-01, -1.9899e+00, -8.2710e-01],
        [-2.5303e-01, -5.9329e-01,  6.8086e-03, -8.3941e-02, -1.3514e-01,
         -1.8726e+00, -5.9815e-01, -2.7942e-01,  6.8464e-03, -1.1625e+00,
         -8.6146e-01, -1.9897e+00, -8.2773e-01],
        [-2.5266e-01, -5.9572e-01,  6.8198e-03, -8.7278e-02, -1.4048e-01,
         -1.8723e+00, -6.0665e-01, -2.6186e-01,  6.8571e-03, -1.1521e+00,
         -8.5434e-01, -1.9862e+00, -8.3775e-01],
        [-2.7205e-01, -6.0582e-01,  8.1218e-03, -1.0733e-01, -1.6508e-01,
         -1.8679e+00, -6.5034e-01, -1.5885e-01,  8.1541e-03, -1.0841e+00,
         -8.2225e-01, -1.9486e+00, -8.8637e-01],
        [-3.6862e+00,  3.0295e+00, -1.6090e-01, -4.5702e+00,  3.8485e-05,
          9.4414e+00, -3.6149e+00,  7.3836e-02, -1.6069e-01,  4.1823e-01,
          2.7291e+00,  2.8268e+00, -1.3360e+00],
        [ 3.3685e+00, -4.4660e-01, -1.2750e-01,  4.7870e+00, -1.2035e-03,
         -1.1253e+01,  4.2450e+00, -6.9133e+00, -1.2845e-01, -1.1255e+01,
         -1.5008e+01, -5.9594e+00,  5.6190e+00],
        [ 4.6716e+00, -9.6638e+00,  2.6870e-01,  9.3119e+00,  4.1322e-03,
         -7.1652e+00,  1.1206e+00, -2.1356e+01,  2.6997e-01, -3.2723e+00,
         -6.5481e+00, -4.0466e+00,  4.0470e+00],
        [-1.3765e+00, -1.2187e+01, -7.6873e-03,  3.8040e-01,  2.2691e+00,
          3.9446e-01, -1.3470e+00, -2.3680e+00, -7.6696e-03,  3.1427e+00,
          9.8105e+00, -1.3591e+00, -5.5141e+00],
        [ 3.6637e-01,  4.3879e-01,  5.2748e-01,  2.0317e+00,  9.2198e-03,
         -1.5847e+00,  7.5107e-01, -3.2943e+00,  5.2678e-01, -2.8752e+00,
         -5.7165e-01, -2.8248e+00,  6.4352e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  0.3862,  -2.4238, -10.3032,   2.7196,  -1.9168,  -1.9169,  -1.9168,
         -1.9408,   8.5897,  -2.2269,   3.3610,  -2.3266,  -1.0903],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.1000e+00, -1.0857e-01,  1.4673e+00, -1.6669e-01,  5.5109e-01,
          5.5057e-01,  5.4300e-01,  4.9721e-01,  1.6029e+00, -3.0176e+00,
         -1.4634e+01,  5.4741e-01,  1.5294e+00],
        [ 1.3754e-01,  1.1647e+00, -2.1128e-01,  1.3751e+00, -6.1155e-01,
         -6.1140e-01, -6.0938e-01, -5.8715e-01,  1.2712e+00, -1.8266e+00,
         -3.0872e-02,  1.0355e+00, -8.8206e-01],
        [-2.1030e+00, -5.0101e-02, -2.8225e+00, -2.8128e+00,  2.5894e-01,
          2.5912e-01,  2.6232e-01,  2.6933e-01, -5.4392e+00,  7.1206e+00,
          5.6263e+00, -4.1053e-01, -9.9450e-02],
        [-8.5912e-01,  1.2201e+00, -7.6673e+00,  1.0562e+00, -1.0571e+00,
         -1.0568e+00, -1.0534e+00, -1.0078e+00, -7.4609e-01,  1.6258e+01,
          8.9904e+00,  4.8649e-01, -1.7048e+00],
        [ 3.2684e-04,  6.5766e-05,  2.3863e-03,  4.7762e-04,  6.1573e-02,
          6.2129e-02,  7.0301e-02,  1.1758e-01,  1.9040e+01, -3.9017e+00,
         -3.0945e+00, -3.3614e-03, -2.0512e+00]], device='cuda:0'))])
loaded xi:  261.8972
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -5869.656369842405
Current xi:  [283.24573]
objective value function right now is: -5869.656369842405
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -6019.036176588246
Current xi:  [303.71237]
objective value function right now is: -6019.036176588246
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -6166.140445365992
Current xi:  [321.1082]
objective value function right now is: -6166.140445365992
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -6236.399558027258
Current xi:  [337.53445]
objective value function right now is: -6236.399558027258
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -6317.395875504379
Current xi:  [351.70026]
objective value function right now is: -6317.395875504379
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -6358.784543979974
Current xi:  [364.35156]
objective value function right now is: -6358.784543979974
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -6373.112007009875
Current xi:  [373.6925]
objective value function right now is: -6373.112007009875
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [382.48428]
objective value function right now is: -6357.211209539196
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -6414.1340678768065
Current xi:  [389.78854]
objective value function right now is: -6414.1340678768065
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -6449.112049964404
Current xi:  [394.6592]
objective value function right now is: -6449.112049964404
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [399.1908]
objective value function right now is: -6397.741424432568
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [400.33615]
objective value function right now is: -6403.774549173313
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -6459.175513052899
Current xi:  [402.08536]
objective value function right now is: -6459.175513052899
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [404.6205]
objective value function right now is: -6437.52785403476
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [404.1505]
objective value function right now is: -6451.539123361765
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -6466.702845786544
Current xi:  [404.58893]
objective value function right now is: -6466.702845786544
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -6506.807791082849
Current xi:  [406.0364]
objective value function right now is: -6506.807791082849
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.57776]
objective value function right now is: -6403.361522585249
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.29132]
objective value function right now is: -6457.219702095688
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -6511.390619881273
Current xi:  [408.92212]
objective value function right now is: -6511.390619881273
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.7788]
objective value function right now is: -6505.6810365196125
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.65643]
objective value function right now is: -6240.821598871444
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -6524.07252829476
Current xi:  [407.7084]
objective value function right now is: -6524.07252829476
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [407.1999]
objective value function right now is: -6480.127905656104
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [407.68243]
objective value function right now is: -6477.54948561655
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.03403]
objective value function right now is: -6493.346429671105
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.3561]
objective value function right now is: -6295.616455245916
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [407.7716]
objective value function right now is: -6446.445802068727
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [408.52325]
objective value function right now is: -6432.550631921051
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.57144]
objective value function right now is: -6497.270038813482
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [409.67447]
objective value function right now is: -6452.6488765170425
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [407.5903]
objective value function right now is: -6407.442540611344
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.0184]
objective value function right now is: -6490.723529670531
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.39102]
objective value function right now is: -6481.4737882189565
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [407.32803]
objective value function right now is: -6488.288653023111
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [407.6602]
objective value function right now is: -6503.905240826609
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -6547.151501494833
Current xi:  [408.07874]
objective value function right now is: -6547.151501494833
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.13107]
objective value function right now is: -6536.069708040599
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.85068]
objective value function right now is: -6526.909063028449
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -6554.478350196025
Current xi:  [408.95718]
objective value function right now is: -6554.478350196025
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.99515]
objective value function right now is: -6534.609906116223
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.8067]
objective value function right now is: -6553.010323680423
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.85385]
objective value function right now is: -6546.729418680004
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.91782]
objective value function right now is: -6529.061319081608
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.85004]
objective value function right now is: -6552.134765781173
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.91898]
objective value function right now is: -6550.767541008095
94.0% of gradient descent iterations done. Method = Adam
new min fval:  -6558.358301089694
Current xi:  [409.10803]
objective value function right now is: -6558.358301089694
new min fval from sgd:  -6558.407215727426
new min fval from sgd:  -6558.814477443649
new min fval from sgd:  -6559.748052696674
new min fval from sgd:  -6560.692991986836
new min fval from sgd:  -6561.3167789195295
new min fval from sgd:  -6561.674737304316
new min fval from sgd:  -6561.991198040284
new min fval from sgd:  -6562.256416548861
new min fval from sgd:  -6562.360525000922
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [409.31384]
objective value function right now is: -6550.718847649492
new min fval from sgd:  -6562.461497246366
new min fval from sgd:  -6562.81095127742
new min fval from sgd:  -6563.00427149206
new min fval from sgd:  -6563.022666603575
new min fval from sgd:  -6563.094818733633
new min fval from sgd:  -6563.220240204439
new min fval from sgd:  -6563.2755816116505
new min fval from sgd:  -6563.304828186175
new min fval from sgd:  -6563.320148074338
new min fval from sgd:  -6563.410700689817
new min fval from sgd:  -6563.45543522092
new min fval from sgd:  -6563.458569957457
new min fval from sgd:  -6563.479433488045
new min fval from sgd:  -6563.513549663222
new min fval from sgd:  -6563.564585546611
new min fval from sgd:  -6563.627658534556
new min fval from sgd:  -6563.633344245834
new min fval from sgd:  -6563.679932505999
new min fval from sgd:  -6563.733308992594
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [409.32974]
objective value function right now is: -6563.235954222255
new min fval from sgd:  -6563.940774716422
new min fval from sgd:  -6564.218242783446
new min fval from sgd:  -6564.44874535836
new min fval from sgd:  -6564.753077641254
new min fval from sgd:  -6564.945523311131
new min fval from sgd:  -6565.089978520453
new min fval from sgd:  -6565.263822659034
new min fval from sgd:  -6565.361275985865
new min fval from sgd:  -6565.450861126941
new min fval from sgd:  -6565.561754099985
new min fval from sgd:  -6565.654961351516
new min fval from sgd:  -6565.706028256538
new min fval from sgd:  -6565.744376074646
new min fval from sgd:  -6565.78740952994
new min fval from sgd:  -6565.815126023718
new min fval from sgd:  -6565.824496892687
new min fval from sgd:  -6565.839050139515
new min fval from sgd:  -6565.855071375089
new min fval from sgd:  -6565.8685336364915
new min fval from sgd:  -6565.886158032541
new min fval from sgd:  -6565.913118213262
new min fval from sgd:  -6565.952591521717
new min fval from sgd:  -6565.981989540842
new min fval from sgd:  -6566.012954919688
new min fval from sgd:  -6566.033573586887
new min fval from sgd:  -6566.037821179597
new min fval from sgd:  -6566.05542962013
new min fval from sgd:  -6566.080539245204
new min fval from sgd:  -6566.1063173618895
new min fval from sgd:  -6566.114378700904
new min fval from sgd:  -6566.124972866709
new min fval from sgd:  -6566.166881962746
new min fval from sgd:  -6566.1854592381815
new min fval from sgd:  -6566.211394261739
new min fval from sgd:  -6566.231020096699
new min fval from sgd:  -6566.248371851886
new min fval from sgd:  -6566.251969523623
new min fval from sgd:  -6566.26429179145
new min fval from sgd:  -6566.287011238628
new min fval from sgd:  -6566.403084710921
new min fval from sgd:  -6566.562008171946
new min fval from sgd:  -6566.715216106359
new min fval from sgd:  -6566.810189049233
new min fval from sgd:  -6566.884549065089
new min fval from sgd:  -6566.9213374189385
new min fval from sgd:  -6566.9383008002305
new min fval from sgd:  -6566.960892968795
new min fval from sgd:  -6566.981758575588
new min fval from sgd:  -6566.991795275504
new min fval from sgd:  -6567.042843726997
new min fval from sgd:  -6567.119090623617
new min fval from sgd:  -6567.2051031383835
new min fval from sgd:  -6567.286166596508
new min fval from sgd:  -6567.355077475316
new min fval from sgd:  -6567.403923162787
new min fval from sgd:  -6567.468569489219
new min fval from sgd:  -6567.517859192489
new min fval from sgd:  -6567.570794790071
new min fval from sgd:  -6567.578743098478
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [409.41718]
objective value function right now is: -6566.944387262322
min fval:  -6567.578743098478
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-19.1963,  15.8122],
        [ -0.9889,   0.6926],
        [ -1.0436,   0.6969],
        [ -0.9989,   5.7468],
        [ -0.9706,   0.6608],
        [ -0.2596,   4.5734],
        [-20.2277, -10.6335],
        [ 15.1323,  -0.4197],
        [ 15.4997,  -1.1045],
        [ -7.3752,  16.0455],
        [ -0.6202,   0.4858],
        [ -1.0644,   0.6023],
        [ -1.0246,   0.7028]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  7.5187,  -4.3572,  -4.3141,  -4.9720,  -4.7144,  -5.0483,  -6.2040,
        -11.9268, -11.5596,   6.2622,  -5.0868,  -4.4180,  -4.3310],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 1.2265e+00, -2.2988e-01, -2.2309e-01,  1.3366e-01, -1.4892e-01,
          9.9000e-02,  2.6589e+00, -3.5840e-01, -6.2412e-01,  3.4845e+00,
         -1.1636e-01, -1.9624e-01, -2.2371e-01],
        [-8.1651e-01, -8.9243e-03, -9.1032e-03,  1.5104e-02, -5.4834e-03,
          1.1249e-02, -1.3594e-01,  5.6281e-02,  6.5337e-02, -7.0903e-01,
         -3.5040e-03, -8.2725e-03, -8.9892e-03],
        [-7.3406e+00,  2.3185e-01,  1.1747e-01,  8.0784e-01,  5.5625e-01,
         -3.3736e+00,  1.9381e+01,  1.1887e+01,  1.0922e+01, -9.3909e+00,
          9.6928e-01, -1.9265e-02,  1.5339e-01],
        [-8.1651e-01, -8.9243e-03, -9.1033e-03,  1.5104e-02, -5.4835e-03,
          1.1249e-02, -1.3594e-01,  5.6281e-02,  6.5337e-02, -7.0903e-01,
         -3.5040e-03, -8.2725e-03, -8.9892e-03],
        [-9.9555e+00, -4.0892e-01, -2.8156e-01, -3.1408e-01,  4.6946e-02,
          2.1088e+00,  2.1071e+01,  1.1458e+01,  1.2487e+01, -9.1629e+00,
          2.6118e-01, -2.7587e-01, -3.1138e-01],
        [-8.1651e-01, -8.9243e-03, -9.1033e-03,  1.5104e-02, -5.4835e-03,
          1.1249e-02, -1.3594e-01,  5.6281e-02,  6.5337e-02, -7.0903e-01,
         -3.5040e-03, -8.2725e-03, -8.9892e-03],
        [-8.1651e-01, -8.9243e-03, -9.1033e-03,  1.5104e-02, -5.4835e-03,
          1.1249e-02, -1.3594e-01,  5.6281e-02,  6.5337e-02, -7.0903e-01,
         -3.5040e-03, -8.2725e-03, -8.9892e-03],
        [-8.1651e-01, -8.9243e-03, -9.1033e-03,  1.5104e-02, -5.4835e-03,
          1.1249e-02, -1.3594e-01,  5.6281e-02,  6.5337e-02, -7.0903e-01,
         -3.5040e-03, -8.2725e-03, -8.9892e-03],
        [-8.1651e-01, -8.9243e-03, -9.1032e-03,  1.5104e-02, -5.4835e-03,
          1.1249e-02, -1.3594e-01,  5.6281e-02,  6.5337e-02, -7.0903e-01,
         -3.5040e-03, -8.2725e-03, -8.9892e-03],
        [-8.1651e-01, -8.9243e-03, -9.1032e-03,  1.5104e-02, -5.4835e-03,
          1.1249e-02, -1.3594e-01,  5.6281e-02,  6.5337e-02, -7.0903e-01,
         -3.5040e-03, -8.2725e-03, -8.9892e-03],
        [-8.0381e+00,  1.6455e-01,  2.0993e-01, -3.0675e+00, -1.6073e-01,
         -4.8594e-01,  1.8665e+01,  1.6550e+01,  1.6137e+01, -9.1348e+00,
         -9.3292e-02,  4.3130e-01,  1.8440e-01],
        [-8.1651e-01, -8.9243e-03, -9.1033e-03,  1.5104e-02, -5.4835e-03,
          1.1249e-02, -1.3594e-01,  5.6281e-02,  6.5337e-02, -7.0903e-01,
         -3.5040e-03, -8.2725e-03, -8.9892e-03],
        [-1.4489e+00,  6.8172e-02,  6.5725e-02, -9.6043e-02,  4.4675e-02,
         -6.7669e-02, -1.8584e+00,  1.9115e-01,  3.4800e-01, -3.2040e+00,
          3.8983e-02,  6.0048e-02,  6.5925e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 3.7427, -2.9067,  4.8874, -2.9067,  1.4475, -2.9067, -2.9067, -2.9067,
        -2.9067, -2.9067,  0.9794, -2.9067, -2.4695], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.1423e+01, -6.4774e-03, -1.0971e+01, -6.4774e-03, -1.5763e+01,
         -6.4774e-03, -6.4774e-03, -6.4774e-03, -6.4774e-03, -6.4774e-03,
         -1.7398e+01, -6.4774e-03, -1.5692e+00]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-10.0810, -11.7190],
        [-16.1068,  11.2908],
        [ -1.4953,   1.5357],
        [-16.0535,  -7.3949],
        [  1.6950,   8.1816],
        [ 17.5235,   7.9001],
        [-16.1033,  -0.5048],
        [ -3.1534,  16.2331],
        [ -1.4927,   1.5387],
        [ 10.4053,  10.2645],
        [  4.4664,  12.8993],
        [ 16.4820,   5.1854],
        [-14.7537,   0.4630]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-5.8264,  7.3549, -5.9828, -5.0350, -8.8331,  1.7575, 11.4228,  9.0796,
        -5.9826,  3.3186,  6.4119, -3.5992, 13.1089], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.3868e+00,  1.5199e+00,  7.2987e-01, -6.6075e-01,  1.8377e+00,
         -2.1205e-01, -6.8519e+00, -4.5646e+00,  7.3277e-01,  3.3456e+00,
         -2.0492e+00, -1.6611e+00, -8.0067e+00],
        [-1.9292e+00, -3.4249e-01,  2.4224e-02,  6.0127e-05,  1.6231e-01,
         -2.2319e+00,  3.2511e-01, -7.9766e-01,  2.5069e-02, -6.6928e-01,
         -1.2621e-01, -1.8879e+00, -1.9832e+00],
        [-2.6641e+01,  1.0150e+01,  6.0096e-01,  4.9307e+00, -1.4206e+01,
         -6.5372e+00,  6.5029e+00,  6.4642e+00,  6.0107e-01,  3.8471e+00,
          3.5674e+00,  9.6977e-02,  6.2446e+00],
        [-1.8908e+00, -6.4385e-01,  4.1347e-01, -3.5852e-02,  4.7437e-01,
         -1.7756e+00,  3.2398e-01,  4.8794e-02,  4.1381e-01, -4.9836e-01,
         -3.7268e-01, -2.4429e+00, -9.3479e-01],
        [-2.0435e+00, -2.3840e-01,  3.6933e-01,  3.0744e-02,  6.9776e-01,
         -2.0670e+00,  8.4060e-02, -2.1243e-01,  3.6976e-01, -5.9722e-01,
         -7.3695e-01, -2.1366e+00, -1.1522e+00],
        [-3.1173e+00,  4.3342e-01,  6.4967e-01,  3.2362e-02,  8.9526e-01,
         -1.1042e+00, -2.0228e+00, -4.0183e+00,  6.4930e-01,  5.9604e-02,
         -3.9661e-01, -3.0398e-01, -2.2870e+00],
        [ 3.6841e+00,  1.9217e+00,  2.4727e-01,  6.0458e+00,  3.9785e-01,
         -6.6643e+00,  1.2416e+00,  1.9102e+00,  2.4653e-01, -3.0464e+00,
         -3.8506e-01, -3.1549e+00, -7.6321e-01],
        [ 5.9878e+00,  6.5843e-01, -1.5012e+00,  3.8545e+00, -1.7168e+00,
         -4.7372e+00,  7.0209e-01,  4.5603e+00, -1.5031e+00, -1.5761e+00,
          3.4404e+00, -1.4197e+00, -2.6431e+00],
        [ 3.0952e-01,  8.7433e-01, -7.7632e-02, -3.2094e-01, -2.3890e-01,
          2.0472e+00,  7.9467e-01, -2.8679e-01, -7.7835e-02,  1.6582e-01,
          1.1561e+00,  2.6450e-01,  3.1991e+00],
        [ 6.2038e+00,  6.3154e+00,  1.3890e-01,  1.1994e+01, -1.7149e-02,
         -1.3500e+01,  3.0003e+00, -1.4032e+01,  1.3872e-01, -2.7130e+01,
         -2.2537e+01, -6.6299e+00,  4.1271e+00],
        [ 3.7228e+00,  8.8939e-02,  1.2040e-03,  7.4249e+00,  2.1663e-02,
         -8.1000e+00,  6.4904e-01, -1.3758e+01,  1.3039e-03, -2.0222e+00,
         -4.5848e+00, -1.9063e+00,  3.1894e+00],
        [-1.5635e+01,  8.9785e+00,  2.1059e-01, -3.3444e+00,  8.8083e-01,
         -2.7833e-01,  3.2302e+00,  4.3563e+00,  2.1064e-01,  2.5658e+00,
          1.6120e+00,  1.8175e+00, -1.3645e+00],
        [-1.1458e+01, -3.7925e-01,  5.1725e-01,  4.3405e+00, -1.9166e+00,
         -2.7205e+00,  1.3377e+00,  7.2291e+00,  5.1652e-01,  1.3882e+00,
          6.4128e+00, -1.2125e+01, -8.8168e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -1.4877,  -3.3280, -13.1826,  -2.7251,  -2.9394,  -2.2135,  -4.3960,
         -2.8442,   4.5354,  -4.2523,   2.6078,  -4.4896,  -2.5344],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.3397e-01, -3.3048e-01,  1.1187e+00,  4.2012e-01,  3.1191e-01,
         -1.1221e+00, -5.5403e-01,  1.5383e+00,  1.5266e+00, -3.6890e-02,
         -2.2662e+01,  1.2458e+00,  9.0532e-01],
        [ 1.3803e+00,  1.1116e-01,  2.5660e-01,  2.4361e-02,  5.3522e-02,
          7.2919e-01,  1.8802e+00, -1.1136e+00,  1.8835e+00, -2.4847e+00,
         -1.4120e+00,  3.4640e-01,  6.5758e-02],
        [ 6.9249e-01, -3.1623e-01, -1.3104e+01, -4.1508e-01, -4.9437e-01,
          1.1509e+00, -3.2163e-01,  1.1115e+00, -6.9448e+00,  8.1457e+00,
          7.3819e+00, -1.5352e+01, -4.5267e+00],
        [-6.6903e+00, -6.1678e-01, -4.6964e+00, -1.0358e+00, -1.1526e+00,
         -5.1279e+00, -2.4797e+00,  1.0657e+00,  4.6798e-02,  1.8628e+00,
          1.2208e+01, -1.6372e+00, -1.0435e+00],
        [ 2.5699e-01, -1.9302e+00, -8.8278e+00, -3.1375e+00, -2.6139e+00,
          7.4297e-02, -3.8523e+00, -1.1578e+00,  1.4051e+01,  1.9739e-03,
          2.1770e+01, -2.5286e+00, -2.5741e+00]], device='cuda:0'))])
xi:  [409.3948]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 519.8240259527314
W_T_median: 448.3952602238221
W_T_pctile_5: 409.6598059031056
W_T_CVAR_5_pct: 208.79382408758158
Average q (qsum/M+1):  43.476011214717744
Optimal xi:  [409.3948]
Expected(across Rb) median(across samples) p_equity:  0.13799813219035664
obj fun:  tensor(-6567.5787, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 25.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-19.2010,   8.8941],
        [ -1.1616,   0.5743],
        [ -1.1597,   0.5748],
        [ -1.1577,   0.5754],
        [ -1.1585,   0.5752],
        [ -1.1578,   0.5754],
        [ -5.8459,  -8.6554],
        [ 10.5575,  -0.2619],
        [ 10.6012,  -1.1028],
        [ -6.8086,   8.1425],
        [ -1.1594,   0.5751],
        [ -1.1584,   0.5753],
        [ -1.1580,   0.5753]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  8.3978,  -2.7932,  -2.7925,  -2.7930,  -2.7933,  -2.7935,  -4.2312,
        -10.1800, -10.0707,   7.4403,  -2.7946,  -2.7941,  -2.7934],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 5.4204e-01,  8.3361e-02,  8.3256e-02,  8.3045e-02,  8.3086e-02,
          8.3013e-02, -1.0115e-01,  3.8863e-01,  5.0295e-01,  1.6767e+00,
          8.3057e-02,  8.3011e-02,  8.3034e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-7.5221e+00,  6.2216e-02,  6.1504e-02,  6.5443e-02,  6.5902e-02,
          6.7372e-02,  1.8094e+01,  8.1252e+00,  6.9384e+00, -4.2298e+00,
          7.0226e-02,  6.9325e-02,  6.6908e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.1658e+00,  2.1394e-02,  1.6289e-02,  7.5557e-03,  9.5136e-03,
          6.5203e-03,  1.8232e+01,  6.0558e+00,  7.1338e+00, -4.0712e+00,
          9.1865e-03,  6.9017e-03,  7.3557e-03],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-8.1081e+00, -7.0500e-02, -6.5133e-02, -6.1419e-02, -6.3677e-02,
         -6.2530e-02,  1.7942e+01,  8.7360e+00,  7.8395e+00, -4.7533e+00,
         -6.7970e-02, -6.4947e-02, -6.2793e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-5.2019e+00,  2.9834e-01,  2.9866e-01,  2.9911e-01,  2.9898e-01,
          2.9913e-01,  1.1758e+01,  2.8283e+00,  2.6221e+00, -4.0749e+00,
          2.9890e-01,  2.9906e-01,  2.9909e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.3868, -1.6059, -0.9413, -1.6059, -1.0647, -1.6059, -1.6059, -1.6059,
        -1.6059, -1.6059, -1.3230, -1.6059, -1.6457], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.2981e+01,  1.2623e-02, -1.0597e+01,  1.2623e-02, -1.0303e+01,
          1.2623e-02,  1.2623e-02,  1.2623e-02,  1.2623e-02,  1.2623e-02,
         -1.0107e+01,  1.2623e-02, -4.7178e+00]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -6.8981,  -7.9649],
        [-10.9503,   6.5660],
        [ -1.8814,   0.1924],
        [-11.8131,  -4.8737],
        [  4.1311,   8.9081],
        [ 12.4619,   6.2340],
        [-11.1937,   0.2291],
        [ -1.0297,  13.7987],
        [ -1.8814,   0.1927],
        [  6.2985,   8.7234],
        [  1.5562,   9.6525],
        [ 11.3925,   4.4920],
        [-12.1812,  -0.1090]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-6.1847,  5.5629, -3.7034, -3.2978, -7.9624,  2.9958,  8.3330,  6.6302,
        -3.7044,  2.6487,  6.2144, -0.6567,  9.9923], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.9546e+00, -2.5927e-01,  1.2440e-01, -7.7089e-01, -1.6169e-01,
          1.9287e+00, -2.6426e+00, -1.3133e+01,  1.2459e-01,  3.2064e+00,
          1.3232e+00,  2.6703e-01, -6.1990e+00],
        [ 5.7482e-02,  2.8332e+00, -2.4149e-01,  2.2683e-02,  6.3432e+00,
         -1.7663e+00, -1.1378e+01,  1.8372e+00, -2.4149e-01, -2.7482e-01,
          1.6546e+00, -1.7156e+00, -1.4493e+01],
        [-1.5705e+01,  8.5627e+00,  1.7504e-01, -1.8245e+00, -3.4437e+00,
         -4.8747e+00,  4.2497e+00,  8.4923e+00,  1.7496e-01,  1.8658e+00,
         -1.3945e+00, -3.9132e+00,  5.3923e+00],
        [-2.3983e+00, -1.3750e+01,  1.8326e-01, -3.0140e+00,  3.6535e+00,
          5.4045e+00, -4.2681e+00, -1.0690e+01,  1.8262e-01,  1.2769e+00,
          6.8748e+00,  2.9334e+00, -8.0795e+00],
        [-2.5300e-01, -5.9313e-01,  6.8050e-03, -8.3695e-02, -1.3481e-01,
         -1.8726e+00, -5.9762e-01, -2.8057e-01,  6.8429e-03, -1.1632e+00,
         -8.6191e-01, -1.9899e+00, -8.2710e-01],
        [-2.5303e-01, -5.9329e-01,  6.8086e-03, -8.3941e-02, -1.3514e-01,
         -1.8726e+00, -5.9815e-01, -2.7942e-01,  6.8464e-03, -1.1625e+00,
         -8.6146e-01, -1.9897e+00, -8.2773e-01],
        [-2.5266e-01, -5.9572e-01,  6.8198e-03, -8.7278e-02, -1.4048e-01,
         -1.8723e+00, -6.0665e-01, -2.6186e-01,  6.8571e-03, -1.1521e+00,
         -8.5434e-01, -1.9862e+00, -8.3775e-01],
        [-2.7205e-01, -6.0582e-01,  8.1218e-03, -1.0733e-01, -1.6508e-01,
         -1.8679e+00, -6.5034e-01, -1.5885e-01,  8.1541e-03, -1.0841e+00,
         -8.2225e-01, -1.9486e+00, -8.8637e-01],
        [-3.6862e+00,  3.0295e+00, -1.6090e-01, -4.5702e+00,  3.8485e-05,
          9.4414e+00, -3.6149e+00,  7.3836e-02, -1.6069e-01,  4.1823e-01,
          2.7291e+00,  2.8268e+00, -1.3360e+00],
        [ 3.3685e+00, -4.4660e-01, -1.2750e-01,  4.7870e+00, -1.2035e-03,
         -1.1253e+01,  4.2450e+00, -6.9133e+00, -1.2845e-01, -1.1255e+01,
         -1.5008e+01, -5.9594e+00,  5.6190e+00],
        [ 4.6716e+00, -9.6638e+00,  2.6870e-01,  9.3119e+00,  4.1322e-03,
         -7.1652e+00,  1.1206e+00, -2.1356e+01,  2.6997e-01, -3.2723e+00,
         -6.5481e+00, -4.0466e+00,  4.0470e+00],
        [-1.3765e+00, -1.2187e+01, -7.6873e-03,  3.8040e-01,  2.2691e+00,
          3.9446e-01, -1.3470e+00, -2.3680e+00, -7.6696e-03,  3.1427e+00,
          9.8105e+00, -1.3591e+00, -5.5141e+00],
        [ 3.6637e-01,  4.3879e-01,  5.2748e-01,  2.0317e+00,  9.2198e-03,
         -1.5847e+00,  7.5107e-01, -3.2943e+00,  5.2678e-01, -2.8752e+00,
         -5.7165e-01, -2.8248e+00,  6.4352e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  0.3862,  -2.4238, -10.3032,   2.7196,  -1.9168,  -1.9169,  -1.9168,
         -1.9408,   8.5897,  -2.2269,   3.3610,  -2.3266,  -1.0903],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.1000e+00, -1.0857e-01,  1.4673e+00, -1.6669e-01,  5.5109e-01,
          5.5057e-01,  5.4300e-01,  4.9721e-01,  1.6029e+00, -3.0176e+00,
         -1.4634e+01,  5.4741e-01,  1.5294e+00],
        [ 1.3754e-01,  1.1647e+00, -2.1128e-01,  1.3751e+00, -6.1155e-01,
         -6.1140e-01, -6.0938e-01, -5.8715e-01,  1.2712e+00, -1.8266e+00,
         -3.0872e-02,  1.0355e+00, -8.8206e-01],
        [-2.1030e+00, -5.0101e-02, -2.8225e+00, -2.8128e+00,  2.5894e-01,
          2.5912e-01,  2.6232e-01,  2.6933e-01, -5.4392e+00,  7.1206e+00,
          5.6263e+00, -4.1053e-01, -9.9450e-02],
        [-8.5912e-01,  1.2201e+00, -7.6673e+00,  1.0562e+00, -1.0571e+00,
         -1.0568e+00, -1.0534e+00, -1.0078e+00, -7.4609e-01,  1.6258e+01,
          8.9904e+00,  4.8649e-01, -1.7048e+00],
        [ 3.2684e-04,  6.5766e-05,  2.3863e-03,  4.7762e-04,  6.1573e-02,
          6.2129e-02,  7.0301e-02,  1.1758e-01,  1.9040e+01, -3.9017e+00,
         -3.0945e+00, -3.3614e-03, -2.0512e+00]], device='cuda:0'))])
loaded xi:  261.8972
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -10392.659361126147
Current xi:  [283.73688]
objective value function right now is: -10392.659361126147
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -10613.470787266779
Current xi:  [304.6616]
objective value function right now is: -10613.470787266779
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -10904.834073183041
Current xi:  [322.9479]
objective value function right now is: -10904.834073183041
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -11081.68836030329
Current xi:  [338.6239]
objective value function right now is: -11081.68836030329
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -11316.897052173375
Current xi:  [352.69308]
objective value function right now is: -11316.897052173375
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -11385.325724744696
Current xi:  [366.61212]
objective value function right now is: -11385.325724744696
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [376.02905]
objective value function right now is: -11280.293976010915
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -11460.894972553298
Current xi:  [384.0977]
objective value function right now is: -11460.894972553298
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -11473.37149041853
Current xi:  [388.85718]
objective value function right now is: -11473.37149041853
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [394.1416]
objective value function right now is: -11283.905417368485
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -11560.739848297735
Current xi:  [399.66306]
objective value function right now is: -11560.739848297735
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [401.86044]
objective value function right now is: -11426.447190664969
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [404.41748]
objective value function right now is: -11477.005558829804
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [405.1032]
objective value function right now is: -11542.012137662408
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -11599.54015353975
Current xi:  [406.65985]
objective value function right now is: -11599.54015353975
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [407.42853]
objective value function right now is: -11562.44521027616
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.6722]
objective value function right now is: -11561.767446042963
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.2529]
objective value function right now is: -11538.223676820411
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [407.2223]
objective value function right now is: -11530.579241579659
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [407.2355]
objective value function right now is: -11403.566736646366
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.80698]
objective value function right now is: -11544.838901933343
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.48972]
objective value function right now is: -11377.974531805314
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [407.06885]
objective value function right now is: -11240.795525024916
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [405.5141]
objective value function right now is: -11540.174632131457
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.64236]
objective value function right now is: -11465.45121653766
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [405.6036]
objective value function right now is: -11416.529521992607
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [407.20953]
objective value function right now is: -11590.527120613042
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [408.92108]
objective value function right now is: -11530.28663169426
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [408.56577]
objective value function right now is: -11387.65867431182
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [409.17166]
objective value function right now is: -11437.57491648758
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.24094]
objective value function right now is: -11582.435090395751
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [407.33054]
objective value function right now is: -11412.33327297137
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [407.694]
objective value function right now is: -11577.422210930627
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [404.7224]
objective value function right now is: -11515.968654862774
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [404.5519]
objective value function right now is: -11587.08779937825
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -11642.461527224583
Current xi:  [404.71582]
objective value function right now is: -11642.461527224583
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [404.9942]
objective value function right now is: -11642.291426138376
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -11645.654399796802
Current xi:  [405.7379]
objective value function right now is: -11645.654399796802
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [405.81354]
objective value function right now is: -11641.83610753797
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -11667.267830369663
Current xi:  [406.02493]
objective value function right now is: -11667.267830369663
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.24835]
objective value function right now is: -11633.664240736842
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.51028]
objective value function right now is: -11600.548615633943
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.4315]
objective value function right now is: -11643.206278192796
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.2425]
objective value function right now is: -11651.5147451546
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.15955]
objective value function right now is: -11654.895956275126
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.18686]
objective value function right now is: -11659.887376682229
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.02686]
objective value function right now is: -11646.46268105343
new min fval from sgd:  -11668.046936851899
new min fval from sgd:  -11669.564689240993
new min fval from sgd:  -11670.816906494263
new min fval from sgd:  -11671.424792875227
new min fval from sgd:  -11672.074961057939
new min fval from sgd:  -11673.51439230064
new min fval from sgd:  -11674.527528579089
new min fval from sgd:  -11675.111817291667
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.07632]
objective value function right now is: -11667.713242628832
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.1666]
objective value function right now is: -11669.476846073601
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.24756]
objective value function right now is: -11669.400022530304
min fval:  -11675.111817291667
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-16.9244,  12.3940],
        [ -1.0466,   0.5345],
        [ -1.0466,   0.5345],
        [ -1.0466,   0.5346],
        [ -1.0466,   0.5345],
        [ -1.0466,   0.5346],
        [  3.3596, -12.2144],
        [ 15.0859,  -0.0862],
        [ 15.2504,  -1.2293],
        [ -9.9573,  15.8238],
        [ -1.0466,   0.5346],
        [ -1.0466,   0.5346],
        [ -1.0466,   0.5346]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  6.2609,  -4.2275,  -4.2275,  -4.2275,  -4.2275,  -4.2275,  -5.3035,
        -11.2389, -11.0298,   6.6743,  -4.2275,  -4.2275,  -4.2275],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  1.2956,   0.0683,   0.0683,   0.0683,   0.0683,   0.0683,   0.9799,
          -0.1244,  -0.1127,   2.3138,   0.0683,   0.0683,   0.0683],
        [ -0.7928,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.8144,
          -0.0277,  -0.0761,  -1.2126,  -0.0177,  -0.0177,  -0.0177],
        [ -9.7978,   0.3557,   0.3557,   0.3557,   0.3557,   0.3557,  15.7028,
          12.4365,  11.3364,  -7.4752,   0.3557,   0.3557,   0.3557],
        [ -0.7928,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.8144,
          -0.0277,  -0.0761,  -1.2126,  -0.0177,  -0.0177,  -0.0177],
        [-11.9276,  -0.1347,  -0.1347,  -0.1347,  -0.1347,  -0.1347,  17.2897,
           1.7283,   2.7259, -10.2461,  -0.1347,  -0.1347,  -0.1347],
        [ -0.7928,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.8144,
          -0.0277,  -0.0761,  -1.2126,  -0.0177,  -0.0177,  -0.0177],
        [ -0.7928,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.8144,
          -0.0277,  -0.0761,  -1.2126,  -0.0177,  -0.0177,  -0.0177],
        [ -0.7928,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.8144,
          -0.0277,  -0.0761,  -1.2126,  -0.0177,  -0.0177,  -0.0177],
        [ -0.7928,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.8144,
          -0.0277,  -0.0761,  -1.2126,  -0.0177,  -0.0177,  -0.0177],
        [ -0.7928,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.8144,
          -0.0277,  -0.0761,  -1.2126,  -0.0177,  -0.0177,  -0.0177],
        [-10.3057,  -0.1151,  -0.1151,  -0.1151,  -0.1151,  -0.1151,  14.0435,
          15.1687,  14.6900,  -8.4255,  -0.1151,  -0.1151,  -0.1151],
        [ -0.7928,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.8144,
          -0.0277,  -0.0761,  -1.2126,  -0.0177,  -0.0177,  -0.0177],
        [ -0.7926,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.0177,  -0.8142,
          -0.0276,  -0.0761,  -1.2128,  -0.0177,  -0.0177,  -0.0177]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.9455, -2.7540,  3.5813, -2.7540, -0.1718, -2.7540, -2.7540, -2.7540,
        -2.7540, -2.7540,  0.4240, -2.7540, -2.7542], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.1483e+01,  1.4634e-02, -1.0201e+01,  1.4634e-02, -9.9271e+00,
          1.4634e-02,  1.4634e-02,  1.4634e-02,  1.4633e-02,  1.4633e-02,
         -1.5739e+01,  1.4634e-02,  1.4621e-02]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -9.8753, -10.6674],
        [-17.6310,  10.9904],
        [ -2.6808,   0.7150],
        [-15.4903,  -7.5353],
        [ -2.3446,   4.6596],
        [ 17.1013,   8.0774],
        [-14.7977,   1.0660],
        [ -2.4312,  16.8698],
        [ -2.5141,   0.7484],
        [  9.4938,   9.6798],
        [  5.8644,  13.0776],
        [ 16.5916,   5.9080],
        [-13.9386,   0.9417]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -5.3586,   6.9875,  -6.5695,  -4.4714, -10.6208,   2.5333,  12.2364,
          9.4364,  -6.7067,   4.1269,   5.5570,  -1.7635,  13.1548],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-6.8321e+00, -5.0099e-03, -9.1599e-02,  1.4466e+00, -2.0931e-02,
          7.1861e-01, -1.0237e+01, -1.7730e+01, -8.5292e-02,  2.9159e+00,
         -1.4378e+00, -9.0694e-01, -7.2489e+00],
        [-1.3450e+00, -1.4967e+00, -4.6622e-01, -6.6383e-01, -3.5613e+00,
         -1.2979e+00, -7.2752e-01,  2.9285e+00, -3.6271e-01,  8.5260e-01,
          1.2105e+00, -3.5466e+00, -5.7764e-01],
        [-2.5389e+01,  1.6950e+01,  6.1640e-01, -4.0448e-01, -1.9252e-02,
         -2.1801e+00,  2.8534e+00,  7.1152e+00,  5.8696e-01,  6.1527e+00,
          3.6018e+00,  8.6520e-01,  1.8618e+00],
        [ 5.8770e-01,  1.0799e-01,  1.1255e-02,  2.1918e+00,  6.0822e-02,
          1.1720e+00, -7.5923e-01, -1.9064e+00,  1.2001e-02, -2.3062e+00,
         -2.0125e+00, -7.5267e-01, -8.2819e+00],
        [-2.5484e+00, -6.6528e-01,  2.4731e-02, -4.0124e+00, -1.7000e+00,
         -1.5848e-01,  2.2509e+00, -1.3465e+00,  1.9892e-02,  6.3054e-01,
         -6.7184e-01, -6.0717e+00, -1.1491e+00],
        [-8.7202e-01, -2.8589e+00, -2.6226e-02, -3.2181e+00, -3.8595e+00,
         -1.2466e+00,  2.2251e-01,  3.7362e+00,  8.1681e-03,  1.2409e+00,
          3.6278e+00, -7.2884e+00, -1.4512e+00],
        [ 6.9482e-01, -9.3615e-01,  8.3746e-02,  2.9673e+00, -2.4007e+00,
         -1.5404e+00,  3.6359e-01,  1.8043e+00,  6.7133e-02, -3.8384e-02,
          1.3820e+00, -3.4278e+00, -1.0979e+00],
        [-9.1611e-01, -2.0685e-01, -5.2338e-02, -2.1774e-02,  4.9377e-02,
         -2.7026e+00, -9.4184e-01,  1.1760e-01, -4.7624e-02, -1.3950e+00,
         -8.6333e-01, -2.2693e+00, -1.8578e+00],
        [-3.3419e+00,  4.0967e+01,  1.6312e+00, -7.1163e+00,  3.1904e-03,
          4.2149e+01, -3.8909e+00,  4.4306e+00,  1.3944e+00,  4.8144e+01,
          1.2790e+01,  2.7448e-01, -1.3725e+00],
        [ 6.9597e+00, -5.6440e-01, -1.5291e-02,  1.2338e+01, -4.4368e-02,
         -1.3773e+01,  3.4978e+00, -1.7309e+01,  3.3392e-03, -2.0594e+01,
         -2.2884e+01, -7.8188e+00,  5.9329e+00],
        [ 3.1643e+00, -1.4983e+00,  2.5155e-01,  1.0199e+01,  3.8478e-03,
         -7.1470e+00,  7.6256e-01, -1.7676e+01,  2.6044e-01, -1.4003e+00,
         -5.7197e+00, -3.1397e+00,  3.1088e+00],
        [-6.6405e-01, -1.1619e-01, -5.6908e-02, -7.6954e-04,  3.6476e-02,
         -2.4904e+00, -7.4257e-01,  2.6584e-01, -5.1881e-02, -1.4457e+00,
         -8.4070e-01, -2.1549e+00, -1.9304e+00],
        [-2.6412e+01,  8.6863e+00, -2.3653e-01, -3.7118e-02, -1.6399e+01,
         -5.5322e+00,  8.4509e+00,  5.2678e+00, -2.6912e-01,  8.1570e-01,
          4.9447e+00, -3.2913e+00,  2.0098e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-0.8308, -3.5484, -8.1964, -1.6248, -2.7685, -3.2759, -3.4188, -3.5997,
        12.2973, -4.3349,  3.8010, -4.0286, -7.8756], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 5.4362e-01,  2.4178e+00,  1.0279e+00,  3.6200e+00,  5.7756e+00,
          1.1644e+00,  4.5827e+00,  1.1641e+00,  1.6197e+00, -3.9276e-02,
         -2.1689e+01,  1.1746e+00,  1.4012e+00],
        [ 7.0457e-01, -1.9487e+00,  2.7864e-01,  2.1033e-01, -1.9075e+00,
         -8.4426e-01, -3.1572e+00,  1.2300e-01,  1.9183e+00, -2.7681e+00,
         -1.3744e+00, -5.4207e-02,  4.9159e-01],
        [-6.1482e+00, -4.2632e-02, -5.2237e+00, -3.5689e+00, -4.9656e+00,
          4.8638e-01, -1.6186e-02, -3.8825e-01, -6.8301e+00,  8.2139e+00,
          7.3491e+00, -5.0715e-01, -5.2721e+00],
        [-3.0566e+00,  1.1307e+00, -1.1099e+00,  2.5570e+00,  1.3913e+00,
         -3.1613e+00,  1.7582e+00,  2.8227e-01, -1.8607e-01,  1.6351e+00,
          1.3476e+01,  4.5831e-01, -6.0854e+00],
        [ 5.4501e-05, -5.4581e-02,  4.0807e-08, -5.3704e-02, -7.8607e-02,
         -4.9417e-02, -9.8898e-01, -8.9523e-02,  3.7801e+01, -8.4873e+00,
         -7.7740e+00, -9.0355e-02, -2.5435e-05]], device='cuda:0'))])
xi:  [406.0969]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 668.9961802566204
W_T_median: 590.3771878381627
W_T_pctile_5: 406.95577166792293
W_T_CVAR_5_pct: 206.57886914138322
Average q (qsum/M+1):  43.431782384072584
Optimal xi:  [406.0969]
Expected(across Rb) median(across samples) p_equity:  0.15919209644198418
obj fun:  tensor(-11675.1118, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-19.2010,   8.8941],
        [ -1.1616,   0.5743],
        [ -1.1597,   0.5748],
        [ -1.1577,   0.5754],
        [ -1.1585,   0.5752],
        [ -1.1578,   0.5754],
        [ -5.8459,  -8.6554],
        [ 10.5575,  -0.2619],
        [ 10.6012,  -1.1028],
        [ -6.8086,   8.1425],
        [ -1.1594,   0.5751],
        [ -1.1584,   0.5753],
        [ -1.1580,   0.5753]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  8.3978,  -2.7932,  -2.7925,  -2.7930,  -2.7933,  -2.7935,  -4.2312,
        -10.1800, -10.0707,   7.4403,  -2.7946,  -2.7941,  -2.7934],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 5.4204e-01,  8.3361e-02,  8.3256e-02,  8.3045e-02,  8.3086e-02,
          8.3013e-02, -1.0115e-01,  3.8863e-01,  5.0295e-01,  1.6767e+00,
          8.3057e-02,  8.3011e-02,  8.3034e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-7.5221e+00,  6.2216e-02,  6.1504e-02,  6.5443e-02,  6.5902e-02,
          6.7372e-02,  1.8094e+01,  8.1252e+00,  6.9384e+00, -4.2298e+00,
          7.0226e-02,  6.9325e-02,  6.6908e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.1658e+00,  2.1394e-02,  1.6289e-02,  7.5557e-03,  9.5136e-03,
          6.5203e-03,  1.8232e+01,  6.0558e+00,  7.1338e+00, -4.0712e+00,
          9.1865e-03,  6.9017e-03,  7.3557e-03],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-8.1081e+00, -7.0500e-02, -6.5133e-02, -6.1419e-02, -6.3677e-02,
         -6.2530e-02,  1.7942e+01,  8.7360e+00,  7.8395e+00, -4.7533e+00,
         -6.7970e-02, -6.4947e-02, -6.2793e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-5.2019e+00,  2.9834e-01,  2.9866e-01,  2.9911e-01,  2.9898e-01,
          2.9913e-01,  1.1758e+01,  2.8283e+00,  2.6221e+00, -4.0749e+00,
          2.9890e-01,  2.9906e-01,  2.9909e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.3868, -1.6059, -0.9413, -1.6059, -1.0647, -1.6059, -1.6059, -1.6059,
        -1.6059, -1.6059, -1.3230, -1.6059, -1.6457], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.2981e+01,  1.2623e-02, -1.0597e+01,  1.2623e-02, -1.0303e+01,
          1.2623e-02,  1.2623e-02,  1.2623e-02,  1.2623e-02,  1.2623e-02,
         -1.0107e+01,  1.2623e-02, -4.7178e+00]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -6.8981,  -7.9649],
        [-10.9503,   6.5660],
        [ -1.8814,   0.1924],
        [-11.8131,  -4.8737],
        [  4.1311,   8.9081],
        [ 12.4619,   6.2340],
        [-11.1937,   0.2291],
        [ -1.0297,  13.7987],
        [ -1.8814,   0.1927],
        [  6.2985,   8.7234],
        [  1.5562,   9.6525],
        [ 11.3925,   4.4920],
        [-12.1812,  -0.1090]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-6.1847,  5.5629, -3.7034, -3.2978, -7.9624,  2.9958,  8.3330,  6.6302,
        -3.7044,  2.6487,  6.2144, -0.6567,  9.9923], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.9546e+00, -2.5927e-01,  1.2440e-01, -7.7089e-01, -1.6169e-01,
          1.9287e+00, -2.6426e+00, -1.3133e+01,  1.2459e-01,  3.2064e+00,
          1.3232e+00,  2.6703e-01, -6.1990e+00],
        [ 5.7482e-02,  2.8332e+00, -2.4149e-01,  2.2683e-02,  6.3432e+00,
         -1.7663e+00, -1.1378e+01,  1.8372e+00, -2.4149e-01, -2.7482e-01,
          1.6546e+00, -1.7156e+00, -1.4493e+01],
        [-1.5705e+01,  8.5627e+00,  1.7504e-01, -1.8245e+00, -3.4437e+00,
         -4.8747e+00,  4.2497e+00,  8.4923e+00,  1.7496e-01,  1.8658e+00,
         -1.3945e+00, -3.9132e+00,  5.3923e+00],
        [-2.3983e+00, -1.3750e+01,  1.8326e-01, -3.0140e+00,  3.6535e+00,
          5.4045e+00, -4.2681e+00, -1.0690e+01,  1.8262e-01,  1.2769e+00,
          6.8748e+00,  2.9334e+00, -8.0795e+00],
        [-2.5300e-01, -5.9313e-01,  6.8050e-03, -8.3695e-02, -1.3481e-01,
         -1.8726e+00, -5.9762e-01, -2.8057e-01,  6.8429e-03, -1.1632e+00,
         -8.6191e-01, -1.9899e+00, -8.2710e-01],
        [-2.5303e-01, -5.9329e-01,  6.8086e-03, -8.3941e-02, -1.3514e-01,
         -1.8726e+00, -5.9815e-01, -2.7942e-01,  6.8464e-03, -1.1625e+00,
         -8.6146e-01, -1.9897e+00, -8.2773e-01],
        [-2.5266e-01, -5.9572e-01,  6.8198e-03, -8.7278e-02, -1.4048e-01,
         -1.8723e+00, -6.0665e-01, -2.6186e-01,  6.8571e-03, -1.1521e+00,
         -8.5434e-01, -1.9862e+00, -8.3775e-01],
        [-2.7205e-01, -6.0582e-01,  8.1218e-03, -1.0733e-01, -1.6508e-01,
         -1.8679e+00, -6.5034e-01, -1.5885e-01,  8.1541e-03, -1.0841e+00,
         -8.2225e-01, -1.9486e+00, -8.8637e-01],
        [-3.6862e+00,  3.0295e+00, -1.6090e-01, -4.5702e+00,  3.8485e-05,
          9.4414e+00, -3.6149e+00,  7.3836e-02, -1.6069e-01,  4.1823e-01,
          2.7291e+00,  2.8268e+00, -1.3360e+00],
        [ 3.3685e+00, -4.4660e-01, -1.2750e-01,  4.7870e+00, -1.2035e-03,
         -1.1253e+01,  4.2450e+00, -6.9133e+00, -1.2845e-01, -1.1255e+01,
         -1.5008e+01, -5.9594e+00,  5.6190e+00],
        [ 4.6716e+00, -9.6638e+00,  2.6870e-01,  9.3119e+00,  4.1322e-03,
         -7.1652e+00,  1.1206e+00, -2.1356e+01,  2.6997e-01, -3.2723e+00,
         -6.5481e+00, -4.0466e+00,  4.0470e+00],
        [-1.3765e+00, -1.2187e+01, -7.6873e-03,  3.8040e-01,  2.2691e+00,
          3.9446e-01, -1.3470e+00, -2.3680e+00, -7.6696e-03,  3.1427e+00,
          9.8105e+00, -1.3591e+00, -5.5141e+00],
        [ 3.6637e-01,  4.3879e-01,  5.2748e-01,  2.0317e+00,  9.2198e-03,
         -1.5847e+00,  7.5107e-01, -3.2943e+00,  5.2678e-01, -2.8752e+00,
         -5.7165e-01, -2.8248e+00,  6.4352e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  0.3862,  -2.4238, -10.3032,   2.7196,  -1.9168,  -1.9169,  -1.9168,
         -1.9408,   8.5897,  -2.2269,   3.3610,  -2.3266,  -1.0903],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.1000e+00, -1.0857e-01,  1.4673e+00, -1.6669e-01,  5.5109e-01,
          5.5057e-01,  5.4300e-01,  4.9721e-01,  1.6029e+00, -3.0176e+00,
         -1.4634e+01,  5.4741e-01,  1.5294e+00],
        [ 1.3754e-01,  1.1647e+00, -2.1128e-01,  1.3751e+00, -6.1155e-01,
         -6.1140e-01, -6.0938e-01, -5.8715e-01,  1.2712e+00, -1.8266e+00,
         -3.0872e-02,  1.0355e+00, -8.8206e-01],
        [-2.1030e+00, -5.0101e-02, -2.8225e+00, -2.8128e+00,  2.5894e-01,
          2.5912e-01,  2.6232e-01,  2.6933e-01, -5.4392e+00,  7.1206e+00,
          5.6263e+00, -4.1053e-01, -9.9450e-02],
        [-8.5912e-01,  1.2201e+00, -7.6673e+00,  1.0562e+00, -1.0571e+00,
         -1.0568e+00, -1.0534e+00, -1.0078e+00, -7.4609e-01,  1.6258e+01,
          8.9904e+00,  4.8649e-01, -1.7048e+00],
        [ 3.2684e-04,  6.5766e-05,  2.3863e-03,  4.7762e-04,  6.1573e-02,
          6.2129e-02,  7.0301e-02,  1.1758e-01,  1.9040e+01, -3.9017e+00,
         -3.0945e+00, -3.3614e-03, -2.0512e+00]], device='cuda:0'))])
loaded xi:  261.8972
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -180.9918246498812
Current xi:  [284.35825]
objective value function right now is: -180.9918246498812
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -187.7777855278083
Current xi:  [304.76273]
objective value function right now is: -187.7777855278083
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -188.9904732447404
Current xi:  [323.5147]
objective value function right now is: -188.9904732447404
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -196.04686482665687
Current xi:  [339.30362]
objective value function right now is: -196.04686482665687
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -197.25835085382448
Current xi:  [354.17538]
objective value function right now is: -197.25835085382448
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -201.9922711018286
Current xi:  [366.85828]
objective value function right now is: -201.9922711018286
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -202.5600029570964
Current xi:  [377.843]
objective value function right now is: -202.5600029570964
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -203.49102783167828
Current xi:  [385.93073]
objective value function right now is: -203.49102783167828
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -203.58452634651266
Current xi:  [392.15875]
objective value function right now is: -203.58452634651266
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [398.17386]
objective value function right now is: -201.86074908027382
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -204.75145720854402
Current xi:  [400.21744]
objective value function right now is: -204.75145720854402
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [402.98227]
objective value function right now is: -203.71213585712962
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [403.2018]
objective value function right now is: -202.94796134589012
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [403.48376]
objective value function right now is: -203.1467585918067
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [404.0743]
objective value function right now is: -201.33889581447156
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [403.2342]
objective value function right now is: -203.1638165547972
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [404.028]
objective value function right now is: -199.8055754687858
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [403.59048]
objective value function right now is: -204.17294413678698
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [403.12607]
objective value function right now is: -203.75177788681106
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [405.9727]
objective value function right now is: -199.0533380305351
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.93976]
objective value function right now is: -202.68286291712693
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [407.85522]
objective value function right now is: -203.3992937928572
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.37488]
objective value function right now is: -204.0022561615528
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.8957]
objective value function right now is: -199.5240077276075
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 668.9961802566204
W_T_median: 590.3771878381627
W_T_pctile_5: 406.95577166792293
W_T_CVAR_5_pct: 207.82734714138322
Average q (qsum/M+1):  35.0000000072584
Optimal xi:  [406.0969]
Expected(across Rb) median(across samples) p_equity:  0.15919209644198418
obj fun:  tensor(-11675.1118, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 999.0
-----------------------------------------------