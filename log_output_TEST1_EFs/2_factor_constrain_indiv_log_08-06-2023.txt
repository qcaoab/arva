/home/marcchen/Documents/constrain_factor/researchcode/exp_config_json_files/multi_portfolio_TEST1_EFs.json
Starting at: 
06-08-23_10:31

 numpy seed:  3  


 pytorch seed:  3  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.005383     0.031411
192608                    0.0319              0.0561  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0355  ...     0.005323    -0.028996
192611                   -0.0038              0.0294  ...     0.005303     0.028554

[5 rows x 6 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.043289    -0.036240
202209                   -0.0955             -0.0871  ...    -0.050056    -0.091324
202210                    0.0883              0.1486  ...    -0.014968     0.077403
202211                   -0.0076              0.0462  ...     0.040789     0.052365
202212                   -0.0457             -0.0499  ...    -0.018566    -0.057116

[5 rows x 6 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'CPI_nom_ret_ind', 'T30_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Value_Hi30_real_ret
T30_real_ret             1.000000  ...             0.018239
B10_real_ret             0.351722  ...             0.031301
VWD_real_ret             0.068448  ...             0.909335
Size_Lo30_real_ret       0.014412  ...             0.908542
Value_Hi30_real_ret      0.018239  ...             1.000000

[5 rows x 5 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       5       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       5              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 5)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 1500, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1734.2999240676877
Current xi:  [81.727066]
objective value function right now is: -1734.2999240676877
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.6796503593284
Current xi:  [61.157047]
objective value function right now is: -1741.6796503593284
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1746.837567963611
Current xi:  [39.337975]
objective value function right now is: -1746.837567963611
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1751.1271490909369
Current xi:  [16.833477]
objective value function right now is: -1751.1271490909369
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1756.0658564130067
Current xi:  [-3.3109794]
objective value function right now is: -1756.0658564130067
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1757.9715413613685
Current xi:  [-21.386465]
objective value function right now is: -1757.9715413613685
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1759.9862758827494
Current xi:  [-39.54191]
objective value function right now is: -1759.9862758827494
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1761.2880045735567
Current xi:  [-58.307186]
objective value function right now is: -1761.2880045735567
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1762.2165259605567
Current xi:  [-77.13406]
objective value function right now is: -1762.2165259605567
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1764.0387703502126
Current xi:  [-96.7218]
objective value function right now is: -1764.0387703502126
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1765.0698417652395
Current xi:  [-114.7502]
objective value function right now is: -1765.0698417652395
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1765.7759900674982
Current xi:  [-133.86313]
objective value function right now is: -1765.7759900674982
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1766.5613118010285
Current xi:  [-151.61116]
objective value function right now is: -1766.5613118010285
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1767.3928415229261
Current xi:  [-170.27405]
objective value function right now is: -1767.3928415229261
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1768.311448185435
Current xi:  [-187.48251]
objective value function right now is: -1768.311448185435
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1768.4232447420477
Current xi:  [-205.01085]
objective value function right now is: -1768.4232447420477
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1769.3287321830728
Current xi:  [-221.74901]
objective value function right now is: -1769.3287321830728
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1769.5430404544913
Current xi:  [-237.94388]
objective value function right now is: -1769.5430404544913
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1769.7094728863026
Current xi:  [-253.98485]
objective value function right now is: -1769.7094728863026
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1769.9278725232891
Current xi:  [-268.45947]
objective value function right now is: -1769.9278725232891
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1770.1919245911458
Current xi:  [-282.51703]
objective value function right now is: -1770.1919245911458
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-294.64563]
objective value function right now is: -1769.8942269598883
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1773.2771369683896
Current xi:  [-310.47513]
objective value function right now is: -1773.2771369683896
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-322.28418]
objective value function right now is: -1773.1346406251305
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1773.455687188923
Current xi:  [-331.38287]
objective value function right now is: -1773.455687188923
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1773.4910671517375
Current xi:  [-338.82477]
objective value function right now is: -1773.4910671517375
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1773.7367157416047
Current xi:  [-346.93497]
objective value function right now is: -1773.7367157416047
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-349.32867]
objective value function right now is: -1773.6840188905583
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-352.2605]
objective value function right now is: -1773.5728770060275
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1773.7676656183894
Current xi:  [-354.27982]
objective value function right now is: -1773.7676656183894
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-353.5231]
objective value function right now is: -1773.7448517095859
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-355.1661]
objective value function right now is: -1773.4767254713763
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1773.8741611350051
Current xi:  [-355.64944]
objective value function right now is: -1773.8741611350051
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-355.36765]
objective value function right now is: -1773.5727827109704
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-355.25128]
objective value function right now is: -1773.751183526646
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1774.0798415932752
Current xi:  [-355.45544]
objective value function right now is: -1774.0798415932752
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-354.5189]
objective value function right now is: -1774.0288395618713
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-354.63745]
objective value function right now is: -1773.8774283709613
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1774.1096886499088
Current xi:  [-354.35022]
objective value function right now is: -1774.1096886499088
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-354.19196]
objective value function right now is: -1774.095487841604
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-353.63556]
objective value function right now is: -1774.0346907203505
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-353.11398]
objective value function right now is: -1773.987022812037
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1774.1352593165636
Current xi:  [-353.02945]
objective value function right now is: -1774.1352593165636
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-352.89496]
objective value function right now is: -1773.7094095522666
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-352.67444]
objective value function right now is: -1774.071637374343
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-352.137]
objective value function right now is: -1774.0876579735027
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-351.90756]
objective value function right now is: -1774.103068802875
new min fval from sgd:  -1774.148204758544
new min fval from sgd:  -1774.1500632157363
new min fval from sgd:  -1774.1553390130164
new min fval from sgd:  -1774.159317199886
new min fval from sgd:  -1774.161479422276
new min fval from sgd:  -1774.1625098557795
new min fval from sgd:  -1774.1625554894354
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-351.75568]
objective value function right now is: -1774.1219407033823
new min fval from sgd:  -1774.1627896061996
new min fval from sgd:  -1774.16319355292
new min fval from sgd:  -1774.163579706275
new min fval from sgd:  -1774.1637928262196
new min fval from sgd:  -1774.1638055947776
new min fval from sgd:  -1774.1646521035927
new min fval from sgd:  -1774.1652306489
new min fval from sgd:  -1774.1653488842585
new min fval from sgd:  -1774.1666757263104
new min fval from sgd:  -1774.1676018975568
new min fval from sgd:  -1774.1677784930505
new min fval from sgd:  -1774.1683667453804
new min fval from sgd:  -1774.168925933056
new min fval from sgd:  -1774.1693169148023
new min fval from sgd:  -1774.1695841021092
new min fval from sgd:  -1774.1699481714122
new min fval from sgd:  -1774.1702772885046
new min fval from sgd:  -1774.1703748583636
new min fval from sgd:  -1774.1707258687256
new min fval from sgd:  -1774.1709866405265
new min fval from sgd:  -1774.1717939860425
new min fval from sgd:  -1774.1726965399819
new min fval from sgd:  -1774.1727508697843
new min fval from sgd:  -1774.173010872362
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-351.6926]
objective value function right now is: -1774.1601444480257
new min fval from sgd:  -1774.1732393668053
new min fval from sgd:  -1774.173812526701
new min fval from sgd:  -1774.1738517833674
new min fval from sgd:  -1774.1739764774159
new min fval from sgd:  -1774.1744017362846
new min fval from sgd:  -1774.174692482967
new min fval from sgd:  -1774.1749857871798
new min fval from sgd:  -1774.1752592552398
new min fval from sgd:  -1774.1752965971395
new min fval from sgd:  -1774.1755301580438
new min fval from sgd:  -1774.1755497356412
new min fval from sgd:  -1774.1756644778047
new min fval from sgd:  -1774.1757481819918
new min fval from sgd:  -1774.1763236765203
new min fval from sgd:  -1774.1771112390288
new min fval from sgd:  -1774.177427422981
new min fval from sgd:  -1774.1778461919528
new min fval from sgd:  -1774.1782309592702
new min fval from sgd:  -1774.1785869157413
new min fval from sgd:  -1774.179031571568
new min fval from sgd:  -1774.179115025064
new min fval from sgd:  -1774.1793152218956
new min fval from sgd:  -1774.1794767773613
new min fval from sgd:  -1774.1794809010833
new min fval from sgd:  -1774.1797065962503
new min fval from sgd:  -1774.1800387043813
new min fval from sgd:  -1774.1803081027786
new min fval from sgd:  -1774.1804108430754
new min fval from sgd:  -1774.1806057416786
new min fval from sgd:  -1774.1809902240323
new min fval from sgd:  -1774.1815714807703
new min fval from sgd:  -1774.1817834117721
new min fval from sgd:  -1774.1818444412904
new min fval from sgd:  -1774.182088208152
new min fval from sgd:  -1774.1824279730936
new min fval from sgd:  -1774.1828812903907
new min fval from sgd:  -1774.182980821393
new min fval from sgd:  -1774.1833633593562
new min fval from sgd:  -1774.1835721569005
new min fval from sgd:  -1774.1836200461114
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-351.6287]
objective value function right now is: -1774.1551227035584
min fval:  -1774.1836200461114
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.4597,  3.2625],
        [-0.3560,  1.2194],
        [-0.3560,  1.2194],
        [-0.3560,  1.2194],
        [-0.3560,  1.2194],
        [12.7338,  1.6196],
        [-0.3499,  1.2390],
        [-1.2706,  7.6130],
        [-0.3560,  1.2194],
        [ 9.5948,  0.8040],
        [-0.3553,  1.2217],
        [10.0875,  1.1012],
        [-0.6036,  7.5266]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 2.3582, -0.7418, -0.7418, -0.7418, -0.7418, -7.4562, -0.7460, 10.9451,
        -0.7418, -7.1454, -0.7440, -6.9494, 11.0085], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 4.2792e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03,
         -1.7373e-01, -1.7909e-03, -2.7790e-02, -2.2436e-03, -8.5478e-02,
         -2.1735e-03, -8.9012e-02, -5.8238e-02],
        [-2.2470e-01, -6.2390e-02, -6.2390e-02, -6.2390e-02, -6.2390e-02,
         -4.0864e+00,  1.9790e-02, -3.9552e+00, -6.2390e-02, -1.3823e+00,
         -2.5244e-02, -1.3947e+00, -4.0799e+00],
        [ 4.2791e-03, -2.2437e-03, -2.2437e-03, -2.2437e-03, -2.2437e-03,
         -1.7374e-01, -1.7909e-03, -2.7791e-02, -2.2437e-03, -8.5481e-02,
         -2.1736e-03, -8.9016e-02, -5.8242e-02],
        [ 7.0344e-01,  2.7685e-01,  2.7685e-01,  2.7685e-01,  2.7685e-01,
          3.0395e-01,  2.4323e-01,  1.9913e+00,  2.7685e-01,  2.0024e-01,
          2.7120e-01,  1.6256e-01,  1.8469e+00],
        [ 4.2792e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03,
         -1.7373e-01, -1.7909e-03, -2.7790e-02, -2.2436e-03, -8.5478e-02,
         -2.1735e-03, -8.9012e-02, -5.8238e-02],
        [ 5.3863e-01, -6.8714e-02, -6.8714e-02, -6.8713e-02, -6.8714e-02,
          6.1585e+00,  6.6109e-03,  6.6320e+00, -6.8713e-02,  2.4574e+00,
         -3.1630e-02,  2.4337e+00,  6.5764e+00],
        [ 4.2792e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03,
         -1.7373e-01, -1.7909e-03, -2.7790e-02, -2.2436e-03, -8.5478e-02,
         -2.1735e-03, -8.9012e-02, -5.8238e-02],
        [ 4.2792e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03,
         -1.7373e-01, -1.7909e-03, -2.7790e-02, -2.2436e-03, -8.5478e-02,
         -2.1736e-03, -8.9012e-02, -5.8238e-02],
        [ 4.2792e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03,
         -1.7373e-01, -1.7909e-03, -2.7790e-02, -2.2436e-03, -8.5478e-02,
         -2.1736e-03, -8.9012e-02, -5.8238e-02],
        [ 4.2792e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03,
         -1.7373e-01, -1.7909e-03, -2.7790e-02, -2.2436e-03, -8.5478e-02,
         -2.1735e-03, -8.9012e-02, -5.8238e-02],
        [-3.3321e-01,  6.5519e-03,  6.5517e-03,  6.5526e-03,  6.5519e-03,
         -5.4764e+00,  4.6396e-02, -5.7181e+00,  6.5520e-03, -2.0994e+00,
          3.1393e-02, -2.0956e+00, -5.7795e+00],
        [ 4.2792e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03,
         -1.7373e-01, -1.7909e-03, -2.7790e-02, -2.2436e-03, -8.5478e-02,
         -2.1735e-03, -8.9012e-02, -5.8238e-02],
        [ 4.2792e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03, -2.2436e-03,
         -1.7373e-01, -1.7909e-03, -2.7790e-02, -2.2436e-03, -8.5478e-02,
         -2.1735e-03, -8.9012e-02, -5.8238e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.6710,  3.6008, -0.6710, -2.4764, -0.6710, -5.9962, -0.6710, -0.6710,
        -0.6710, -0.6710,  5.1983, -0.6710, -0.6710], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 7.8654e-03, -5.3701e+00,  7.8658e-03,  2.8330e+00,  7.8655e-03,
          1.1939e+01,  7.8655e-03,  7.8654e-03,  7.8654e-03,  7.8655e-03,
         -8.6780e+00,  7.8654e-03,  7.8655e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.1290,   2.2354],
        [  9.9578,  -0.2012],
        [ -9.2052,  -2.3352],
        [ -7.0765,  -5.4092],
        [  9.1141,   1.4405],
        [ -2.9863,  -8.3056],
        [  8.8256,  -0.4319],
        [-13.3526,  -7.7232],
        [  8.4474,  -0.5845],
        [ -4.7547,  -7.2172],
        [ -7.3572,   5.6033],
        [ -3.9218,   8.7993],
        [ -3.9879,  -7.2170]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-2.0335, -9.7274, -0.3503, -2.9623, -2.4352, -5.7432, -8.6229, -6.8084,
        -8.0845, -6.0544,  5.5830,  7.7284, -7.3971], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 7.5467e-03, -4.4422e-01,  6.5127e-02,  2.6291e-02, -1.6036e+00,
         -5.8891e-01, -5.2200e-01,  1.0746e-01, -6.1075e-01, -3.2098e-02,
         -3.7434e-01, -5.8369e-01,  9.8300e-02],
        [ 3.1961e-01,  5.1040e+00, -2.7372e+00, -2.0388e+00,  6.3792e+00,
         -4.1245e+00,  3.5451e+00, -4.2381e+00,  3.1308e+00, -3.6596e+00,
          5.3024e+00,  1.0497e+01, -2.6777e+00],
        [ 7.5468e-03, -4.4422e-01,  6.5127e-02,  2.6291e-02, -1.6036e+00,
         -5.8891e-01, -5.2200e-01,  1.0746e-01, -6.1075e-01, -3.2098e-02,
         -3.7434e-01, -5.8369e-01,  9.8300e-02],
        [ 2.1164e-01,  4.9299e+00,  5.2987e+00, -2.0819e+00,  4.1222e+00,
         -4.0632e+00,  3.3278e+00,  1.6897e-01,  3.0269e+00, -3.5214e+00,
          3.8367e+00,  2.3122e+00, -2.2311e+00],
        [ 2.6406e-02,  1.9239e+00, -3.1235e-01, -5.1968e-01,  2.1328e+00,
          5.6777e-01,  2.0834e+00, -1.5420e+00,  2.1744e+00, -8.1908e-01,
          1.3811e+00,  1.1207e+00, -1.3606e+00],
        [ 7.4358e-03, -4.3888e-01,  6.2620e-02,  2.3147e-02, -1.6031e+00,
         -5.8790e-01, -5.1565e-01,  9.9726e-02, -6.0358e-01, -3.6817e-02,
         -3.7323e-01, -5.7987e-01,  9.1547e-02],
        [ 2.1068e-01,  5.9890e+00, -1.3896e+00,  1.0300e+00,  2.4980e+00,
          4.7783e+00,  5.5445e+00, -3.7709e+00,  5.5781e+00,  1.1631e-01,
         -2.3171e+00, -3.7621e+00, -5.6633e+00],
        [ 1.6957e+00, -1.2258e-01,  2.1276e+00,  2.2077e-01, -4.7749e+00,
         -2.0187e+00, -8.6316e-01,  3.9724e+00, -1.1398e+00, -1.8733e-01,
          2.1761e+00,  1.4296e+00,  2.7197e+00],
        [-2.4693e-01,  4.0556e+00, -2.5084e+00,  6.1702e-01,  1.5856e-01,
         -1.3050e+00,  4.8247e+00, -2.2546e+00,  5.1231e+00,  1.2432e+00,
         -4.0753e+00, -4.4670e+00,  1.9708e+00],
        [-1.2973e-01, -3.9569e-01,  5.4164e+00, -2.1065e-01, -8.2201e+00,
         -2.3439e+00, -1.2509e+00,  7.2642e+00, -1.6475e+00,  1.0710e+00,
         -4.4928e+00, -3.1289e+00,  3.4475e+00],
        [ 3.3464e-02, -2.7973e+00, -1.1532e+00,  8.3467e-01, -9.4756e-01,
         -3.8722e-01, -2.7950e+00,  2.7091e+00, -2.8292e+00,  1.7928e+00,
         -2.3524e+00, -8.3484e-01,  2.6360e+00],
        [ 9.7518e-01,  7.4936e+00, -4.0896e-01, -8.8524e-01, -3.0205e+00,
         -2.7676e+00,  5.0123e+00, -2.6903e+00,  4.3130e+00, -5.1447e-01,
          1.4774e+00, -1.5497e+00,  1.4762e-01],
        [-2.0422e-01,  2.2351e+00, -1.1835e+00,  3.9131e+00, -3.9204e+00,
          4.8356e+00,  1.8979e+00, -3.9875e+00,  1.6485e+00,  4.6392e+00,
         -5.5804e+00, -8.8920e+00,  2.7096e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.6784,  2.0250, -1.6784, -1.3038,  2.5627, -1.6775,  0.1498, -1.3590,
        -1.6633, -4.9876, -2.7829, -4.0145,  0.5822], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 3.2296e-01,  2.6579e+00,  3.2296e-01,  2.5039e+00,  9.9966e-01,
          4.0333e-01, -2.7102e+00,  3.8170e+00, -5.1736e+00,  5.1634e+00,
          6.6638e+00, -2.2581e-01, -1.0729e+01],
        [-1.2045e-01,  4.5025e+00, -1.2045e-01,  6.6008e-01, -1.2905e+00,
         -2.3253e-02, -4.4787e-03, -2.4414e+00, -9.1472e-02,  2.1038e+00,
         -4.2766e+00,  1.4616e+00, -5.8404e-01],
        [-9.2158e-02, -4.6798e+00, -9.2157e-02, -5.3503e-01,  2.0339e+00,
          3.7147e-03,  1.6739e+00, -3.3252e-01,  1.5723e+00, -8.3855e+00,
         -1.6644e+00, -1.1490e+00,  2.3498e+00],
        [-8.6392e-02, -5.2595e+00, -8.6392e-02, -2.8715e+00,  2.6091e+00,
         -8.5086e-02,  5.9591e+00, -2.9978e+00,  1.4042e+00, -9.6820e+00,
         -3.5163e+00,  2.3107e+00,  5.0575e+00],
        [-7.7170e-01, -2.8727e+00, -7.7170e-01, -1.3994e+00,  2.9643e+00,
         -7.7092e-01,  1.1722e+01, -5.2625e+00,  1.5335e+00, -7.2514e+00,
         -6.3452e+00,  1.9064e+00,  5.5718e+00]], device='cuda:0'))])
xi:  [-351.60715]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 26.51257371422491
W_T_median: 49.7449591549839
W_T_pctile_5: -352.5283900906803
W_T_CVAR_5_pct: -408.7358305395361
Average q (qsum/M+1):  57.89100302419355
Optimal xi:  [-351.60715]
Expected(across Rb) median(across samples) p_equity:  0.1551597759205227
obj fun:  tensor(-1774.1836, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1689.666647076611
Current xi:  [78.833595]
objective value function right now is: -1689.666647076611
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1694.4293484631391
Current xi:  [57.47797]
objective value function right now is: -1694.4293484631391
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1705.147029636026
Current xi:  [36.82469]
objective value function right now is: -1705.147029636026
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1709.0541346399755
Current xi:  [16.443485]
objective value function right now is: -1709.0541346399755
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1713.0205634012689
Current xi:  [-1.3003788]
objective value function right now is: -1713.0205634012689
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1713.0494276761817
Current xi:  [-3.0478797]
objective value function right now is: -1713.0494276761817
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1713.225466658496
Current xi:  [-5.2619333]
objective value function right now is: -1713.225466658496
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.1988554]
objective value function right now is: -1712.9767029723891
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1713.3121106519857
Current xi:  [-9.411982]
objective value function right now is: -1713.3121106519857
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1713.4557052243497
Current xi:  [-11.953723]
objective value function right now is: -1713.4557052243497
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1714.7366270919313
Current xi:  [-25.678877]
objective value function right now is: -1714.7366270919313
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1718.1458341194914
Current xi:  [-36.209583]
objective value function right now is: -1718.1458341194914
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.047054]
objective value function right now is: -1716.1755204418278
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1720.5118827722213
Current xi:  [-35.00717]
objective value function right now is: -1720.5118827722213
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1724.3240708462479
Current xi:  [-35.679165]
objective value function right now is: -1724.3240708462479
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1724.516576021966
Current xi:  [-35.00564]
objective value function right now is: -1724.516576021966
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-36.56023]
objective value function right now is: -1724.5042869733586
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1724.9363548284039
Current xi:  [-35.033264]
objective value function right now is: -1724.9363548284039
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.74685]
objective value function right now is: -1723.9866263847682
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.71466]
objective value function right now is: -1724.6120235798248
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.15695]
objective value function right now is: -1724.4849276831126
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.177532]
objective value function right now is: -1723.1404652536671
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.1294859729935
Current xi:  [-35.822933]
objective value function right now is: -1725.1294859729935
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.151474]
objective value function right now is: -1724.787757232829
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.550014]
objective value function right now is: -1724.902442308464
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.513775]
objective value function right now is: -1724.0623976692516
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.370205]
objective value function right now is: -1724.3200669109651
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1725.3627425482468
Current xi:  [-35.10229]
objective value function right now is: -1725.3627425482468
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-35.064022]
objective value function right now is: -1724.9996968977482
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.132187]
objective value function right now is: -1724.7657168958222
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.32205]
objective value function right now is: -1724.695926740831
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.5033861233546
Current xi:  [-35.17775]
objective value function right now is: -1725.5033861233546
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.575268]
objective value function right now is: -1724.442223743273
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.351997]
objective value function right now is: -1724.8558833673992
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.001514]
objective value function right now is: -1725.2615496635049
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.01116]
objective value function right now is: -1725.4590216942938
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.6285914088644
Current xi:  [-34.98783]
objective value function right now is: -1725.6285914088644
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.7191409853133
Current xi:  [-35.099796]
objective value function right now is: -1725.7191409853133
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.99473]
objective value function right now is: -1725.5746456265958
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.7808409775014
Current xi:  [-35.000515]
objective value function right now is: -1725.7808409775014
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.781766452251
Current xi:  [-35.029266]
objective value function right now is: -1725.781766452251
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.8803129705384
Current xi:  [-34.99262]
objective value function right now is: -1725.8803129705384
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.996025]
objective value function right now is: -1725.5721568103581
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.000763]
objective value function right now is: -1725.7825051191342
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.007927]
objective value function right now is: -1725.8298945058496
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.013206]
objective value function right now is: -1725.8040478631333
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.02488]
objective value function right now is: -1725.7946787521373
new min fval from sgd:  -1725.9032052839284
new min fval from sgd:  -1725.9104706251044
new min fval from sgd:  -1725.9136392128785
new min fval from sgd:  -1725.915760173067
new min fval from sgd:  -1725.9163929762292
new min fval from sgd:  -1725.9195380840424
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.996483]
objective value function right now is: -1725.9014493444251
new min fval from sgd:  -1725.9198867747243
new min fval from sgd:  -1725.9211209181315
new min fval from sgd:  -1725.9224766935538
new min fval from sgd:  -1725.9236591582035
new min fval from sgd:  -1725.923892680938
new min fval from sgd:  -1725.9240716101122
new min fval from sgd:  -1725.9243724697983
new min fval from sgd:  -1725.9244598622267
new min fval from sgd:  -1725.9247033717977
new min fval from sgd:  -1725.9251106026518
new min fval from sgd:  -1725.925259330103
new min fval from sgd:  -1725.9255319849879
new min fval from sgd:  -1725.9280320811697
new min fval from sgd:  -1725.9302023738187
new min fval from sgd:  -1725.932447467389
new min fval from sgd:  -1725.9342217645496
new min fval from sgd:  -1725.9354830747436
new min fval from sgd:  -1725.937464669656
new min fval from sgd:  -1725.9389884435602
new min fval from sgd:  -1725.9396497912285
new min fval from sgd:  -1725.9399631241863
new min fval from sgd:  -1725.9403922556673
new min fval from sgd:  -1725.9411689257663
new min fval from sgd:  -1725.9442503476168
new min fval from sgd:  -1725.945402396903
new min fval from sgd:  -1725.945915252564
new min fval from sgd:  -1725.9475294928905
new min fval from sgd:  -1725.9497921116767
new min fval from sgd:  -1725.949877748492
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.000362]
objective value function right now is: -1725.9342762556523
new min fval from sgd:  -1725.9504922308308
new min fval from sgd:  -1725.9512503137078
new min fval from sgd:  -1725.9522669559965
new min fval from sgd:  -1725.9528730412821
new min fval from sgd:  -1725.9538402752783
new min fval from sgd:  -1725.954503879335
new min fval from sgd:  -1725.9549428291177
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.996006]
objective value function right now is: -1725.914133868964
min fval:  -1725.9549428291177
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -0.9214,  -1.5943],
        [  0.8022,   5.8627],
        [-14.2608,   5.0825],
        [  0.8640,   6.1070],
        [  0.7475,   5.7130],
        [  0.6678,   5.4693],
        [ -2.7090,  -4.7644],
        [ -2.7820,  -5.1715],
        [ -2.4079,  -4.3967],
        [  0.9314,   6.3407],
        [ -3.8231,  -5.6715],
        [  0.5968,   5.2933],
        [ -2.8102,  -5.0001]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-4.0638,  3.8138,  4.4039,  4.2819,  3.5294,  3.0198, -4.6942, -4.7580,
        -4.7594,  4.6569, -4.8500,  2.7167, -4.7523], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.0106, -0.0582, -0.0224, -0.0789, -0.0479, -0.0332, -0.0959, -0.1284,
         -0.0667, -0.0958, -0.1359, -0.0278, -0.1099],
        [-0.1747,  2.4699,  8.9758,  3.0808,  2.2388,  1.7361, -1.7044, -2.4510,
         -1.2933,  3.5352, -3.0655,  1.5773, -1.8695],
        [-0.0106, -0.0582, -0.0224, -0.0789, -0.0479, -0.0332, -0.0959, -0.1284,
         -0.0667, -0.0958, -0.1359, -0.0278, -0.1099],
        [-0.0652,  2.1052,  7.8803,  2.6104,  1.8669,  1.4274, -1.3771, -2.0878,
         -1.0084,  3.0029, -2.7722,  1.2345, -1.7061],
        [-0.0106, -0.0582, -0.0224, -0.0789, -0.0479, -0.0332, -0.0959, -0.1284,
         -0.0667, -0.0958, -0.1359, -0.0278, -0.1099],
        [-0.0106, -0.0582, -0.0224, -0.0789, -0.0479, -0.0332, -0.0959, -0.1284,
         -0.0667, -0.0958, -0.1359, -0.0278, -0.1099],
        [-0.0106, -0.0582, -0.0224, -0.0789, -0.0479, -0.0332, -0.0959, -0.1284,
         -0.0667, -0.0958, -0.1359, -0.0278, -0.1099],
        [ 0.0919, -1.5762, -3.4632, -2.0822, -1.2951, -0.9219,  1.5344,  1.7313,
          1.2387, -2.5681,  2.7799, -0.7518,  1.6976],
        [-0.1227,  2.2585,  8.3687,  2.8203,  2.0021,  1.5321, -1.5069, -2.1328,
         -1.1225,  3.2506, -2.7449,  1.3601, -1.7534],
        [-0.0106, -0.0582, -0.0224, -0.0789, -0.0479, -0.0332, -0.0959, -0.1284,
         -0.0667, -0.0958, -0.1359, -0.0278, -0.1099],
        [ 0.0153, -1.0601, -2.5014, -1.4681, -0.8555, -0.5782,  1.1256,  1.3982,
          0.8615, -1.8636,  2.1469, -0.4586,  1.2623],
        [-0.0106, -0.0582, -0.0224, -0.0789, -0.0479, -0.0332, -0.0959, -0.1284,
         -0.0667, -0.0958, -0.1359, -0.0278, -0.1099],
        [ 0.5685, -3.1084, -6.3155, -3.8515, -2.6626, -2.0236,  2.4258,  2.7590,
          2.0938, -4.5225,  3.8663, -1.7568,  2.6387]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5387, -0.9358, -0.5387, -0.8204, -0.5387, -0.5387, -0.5387, -1.1480,
        -1.0357, -0.5387, -1.0600, -0.5387, -0.8583], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-0.0217,  8.2007, -0.0217,  6.7293, -0.0217, -0.0217, -0.0217, -4.4658,
          7.3636, -0.0217, -3.0646, -0.0217, -9.2003]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-13.5176,   3.4650],
        [ -0.2529,   4.1713],
        [-11.6996,  -5.3564],
        [-12.2829,  -5.6871],
        [ -9.5907,   0.6988],
        [ -1.5667,   0.7141],
        [-12.2844,  -5.5930],
        [-11.2152,   3.2412],
        [  7.6067,  -1.5256],
        [  6.6461,  -1.7005],
        [-11.8935,  -5.4630],
        [ -7.6431,  -8.2227],
        [  0.9144,  -7.3686]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 3.7712, -5.2630, -4.4382, -4.6665,  8.6270, -2.4438, -4.3672,  4.7635,
        -8.2738, -8.0849, -4.5084, -6.2644, -6.7467], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.7797e+00,  3.8233e-01,  3.7710e-01,  4.9127e-01,  4.2058e+00,
         -1.8293e-01, -5.3113e-02, -3.9395e+00,  3.3507e+00,  2.7498e+00,
          4.5366e-01,  2.5370e+00,  7.4430e-01],
        [-3.3482e-03,  6.4244e-02,  9.6604e-01,  1.2215e+00, -1.2945e+01,
         -8.3449e-02,  2.0160e-01, -5.9297e-03,  4.9309e+00,  3.6538e+00,
          1.1400e+00, -8.8598e-01, -1.1208e+00],
        [-1.3884e+00, -2.0482e-02,  5.5513e+00,  6.2881e+00, -7.1239e+00,
         -4.6920e-01,  7.7667e+00, -3.5432e+00, -9.1497e+00, -8.0132e+00,
          5.6087e+00,  4.9587e+00,  3.7453e+00],
        [-5.4946e-02, -2.6427e-03, -2.4055e-02, -2.5872e-02, -1.7280e+00,
         -1.5405e-02, -3.5266e-02, -8.6526e-02, -2.8911e-01, -1.7607e-01,
         -2.4543e-02, -2.6750e-01, -6.6667e-01],
        [-2.3751e-01, -1.4308e+00,  1.2138e+00,  1.7783e+00,  5.8588e+00,
         -1.6535e-01,  9.2264e-01, -1.0050e-01, -3.9141e+00, -4.0810e+00,
          1.4206e+00, -4.2985e+00, -5.2204e+00],
        [-9.6068e-02,  3.1103e-01, -9.7185e-02, -1.0045e-01, -1.6744e+00,
         -7.0254e-03, -1.1940e-01, -1.0522e-01, -3.4784e-01, -2.0301e-01,
         -9.8196e-02, -3.0089e-01, -7.5165e-01],
        [-6.5754e+00,  3.5873e+00, -1.3521e-01, -2.3714e-01, -5.7709e+00,
         -5.4098e-01, -9.6948e-02, -6.6980e+00,  1.1709e+01,  8.1666e+00,
         -1.8993e-01,  7.5929e+00,  1.3055e+01],
        [-7.1066e+00, -1.2672e-02,  7.5641e+00,  8.3696e+00, -1.0404e+00,
         -1.2575e-01,  9.4910e+00, -1.4253e+01, -3.5974e+00, -2.0179e+00,
          7.7579e+00,  5.8995e+00,  3.2659e+00],
        [ 1.2109e-01,  2.5115e+00, -9.6673e-01, -9.6479e-01, -7.8738e-01,
          2.7809e-01, -1.1264e+00,  1.0245e+00, -4.4533e-01, -2.8929e-01,
         -9.6748e-01, -1.9928e+00, -1.2026e+00],
        [-5.4948e-02, -2.6403e-03, -2.4057e-02, -2.5874e-02, -1.7280e+00,
         -1.5404e-02, -3.5269e-02, -8.6528e-02, -2.8911e-01, -1.7607e-01,
         -2.4545e-02, -2.6751e-01, -6.6667e-01],
        [-1.0599e+01,  4.2814e-01, -6.0665e-01, -7.8414e-01, -4.3081e+00,
          1.4336e-01,  1.0796e-01, -4.0897e+00,  1.3379e+00, -1.2894e-02,
         -7.2199e-01,  2.0918e+01,  1.0155e+01],
        [-1.2402e-02,  2.5954e+00, -1.1032e+00, -1.1007e+00, -5.9016e-01,
          3.1643e-01, -1.2892e+00,  1.0557e+00, -4.2306e-01, -2.7122e-01,
         -1.1039e+00, -2.2698e+00, -1.2087e+00],
        [-5.4954e-02, -2.6339e-03, -2.4062e-02, -2.5879e-02, -1.7280e+00,
         -1.5403e-02, -3.5275e-02, -8.6531e-02, -2.8911e-01, -1.7606e-01,
         -2.4550e-02, -2.6751e-01, -6.6668e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.1306, -2.9405, -8.7911, -2.4957, -0.9357, -2.4592,  3.7194, -8.3933,
        -1.6859, -2.4957, -3.0232, -1.5351, -2.4957], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.1984,  -5.8715,  10.9930,  -0.0755,   3.8476,  -0.1864,  -1.3366,
           5.4926,  -0.5063,  -0.0755,  -6.9337,  -0.5635,  -0.0755],
        [  3.3988,   8.4003,  -0.2607,   0.0578,  -2.4763,   0.1032,   1.2350,
          -3.8966,   0.0835,   0.0578,   0.7269,   0.0980,   0.0578],
        [ -3.8641,  -2.8818,  -9.7390,   0.0997,  -0.4434,   0.1976,   0.9071,
           6.2570,   1.0699,   0.0997,   6.2080,   1.1676,   0.0997],
        [ -3.8552,  10.3276, -13.7901,   0.0886,  -2.8207,   0.0600,   6.2346,
          -1.9127,  -0.0947,   0.0886,   7.0312,  -0.1022,   0.0886],
        [ -2.3490,   0.2028,   0.1382,   0.1805,  -0.6648,   0.4272,  12.5348,
           0.1748,   3.3917,   0.1805,   1.9571,   3.8528,   0.1806]],
       device='cuda:0'))])
xi:  [-34.984634]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 93.66299785864865
W_T_median: 56.6712336261876
W_T_pctile_5: -34.98671723732592
W_T_CVAR_5_pct: -162.12822211129208
Average q (qsum/M+1):  56.72195533014113
Optimal xi:  [-34.984634]
Expected(across Rb) median(across samples) p_equity:  0.2074291372982164
obj fun:  tensor(-1725.9549, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1658.9474001867854
Current xi:  [83.20504]
objective value function right now is: -1658.9474001867854
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1667.659024862609
Current xi:  [67.853065]
objective value function right now is: -1667.659024862609
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1670.0474725614497
Current xi:  [56.247356]
objective value function right now is: -1670.0474725614497
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [46.656384]
objective value function right now is: -1669.2101988475117
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1671.7751062616553
Current xi:  [38.6347]
objective value function right now is: -1671.7751062616553
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.91932390495
Current xi:  [32.099594]
objective value function right now is: -1672.91932390495
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1674.0461099610727
Current xi:  [26.606077]
objective value function right now is: -1674.0461099610727
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [22.407063]
objective value function right now is: -1673.1229736350751
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [18.610266]
objective value function right now is: -1673.488950320531
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1675.3310735002422
Current xi:  [14.652503]
objective value function right now is: -1675.3310735002422
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.404875]
objective value function right now is: -1674.9992791424131
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1675.9145355810658
Current xi:  [8.067849]
objective value function right now is: -1675.9145355810658
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1676.7299533443606
Current xi:  [3.9889717]
objective value function right now is: -1676.7299533443606
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1677.0381325541596
Current xi:  [0.03605846]
objective value function right now is: -1677.0381325541596
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1677.4660312310994
Current xi:  [-0.07699565]
objective value function right now is: -1677.4660312310994
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.4826131167472
Current xi:  [-0.05311142]
objective value function right now is: -1678.4826131167472
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03768815]
objective value function right now is: -1678.3669981534858
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01137648]
objective value function right now is: -1678.3352490281688
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00465491]
objective value function right now is: -1678.168032821686
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.6332173857904
Current xi:  [-0.01856417]
objective value function right now is: -1678.6332173857904
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04530668]
objective value function right now is: -1678.4791490512537
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00588132]
objective value function right now is: -1678.0273305057335
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02040805]
objective value function right now is: -1678.4213624020797
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02765922]
objective value function right now is: -1677.9540578852561
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.754003566123
Current xi:  [-0.02126112]
objective value function right now is: -1678.754003566123
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02824489]
objective value function right now is: -1678.5082889410085
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1679.070080471713
Current xi:  [-0.00219251]
objective value function right now is: -1679.070080471713
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01837733]
objective value function right now is: -1678.839207993333
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01492309]
objective value function right now is: -1678.912647309674
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01890842]
objective value function right now is: -1677.6344677036573
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03865939]
objective value function right now is: -1678.429139226897
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03283072]
objective value function right now is: -1678.2497020264143
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1679.2777546209898
Current xi:  [-0.0227301]
objective value function right now is: -1679.2777546209898
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01591519]
objective value function right now is: -1678.9490062943335
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02164577]
objective value function right now is: -1678.428689645568
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1679.315264473157
Current xi:  [0.00140852]
objective value function right now is: -1679.315264473157
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1679.504514664238
Current xi:  [0.00146504]
objective value function right now is: -1679.504514664238
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00071413]
objective value function right now is: -1679.3756971456623
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00092147]
objective value function right now is: -1679.3914236069875
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00451802]
objective value function right now is: -1679.4371433947972
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00040882]
objective value function right now is: -1679.4823474187558
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0008372]
objective value function right now is: -1679.4140933435165
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00027755]
objective value function right now is: -1679.3812177171308
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00213835]
objective value function right now is: -1679.3992139287059
90.0% of gradient descent iterations done. Method = Adam
new min fval:  -1679.5699916747678
Current xi:  [0.0009497]
objective value function right now is: -1679.5699916747678
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00096805]
objective value function right now is: -1679.42914582132
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00267382]
objective value function right now is: -1679.4405763628
new min fval from sgd:  -1679.5883123682254
new min fval from sgd:  -1679.6031341940077
new min fval from sgd:  -1679.6097876025933
new min fval from sgd:  -1679.610704962498
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00190952]
objective value function right now is: -1679.3124635789065
new min fval from sgd:  -1679.6123121355506
new min fval from sgd:  -1679.6148672437696
new min fval from sgd:  -1679.6167850656589
new min fval from sgd:  -1679.6184399323647
new min fval from sgd:  -1679.6190788430115
new min fval from sgd:  -1679.619855338241
new min fval from sgd:  -1679.62014280286
new min fval from sgd:  -1679.6209192910214
new min fval from sgd:  -1679.6216051358488
new min fval from sgd:  -1679.6233272877298
new min fval from sgd:  -1679.625821753704
new min fval from sgd:  -1679.6281256289317
new min fval from sgd:  -1679.6302710660282
new min fval from sgd:  -1679.6320981458064
new min fval from sgd:  -1679.633232266016
new min fval from sgd:  -1679.634529792002
new min fval from sgd:  -1679.6350412309416
new min fval from sgd:  -1679.635117090291
new min fval from sgd:  -1679.635218111339
new min fval from sgd:  -1679.63599569675
new min fval from sgd:  -1679.6365215740968
new min fval from sgd:  -1679.636964689697
new min fval from sgd:  -1679.6374692604438
new min fval from sgd:  -1679.637661819611
new min fval from sgd:  -1679.6381319185623
new min fval from sgd:  -1679.638304447937
new min fval from sgd:  -1679.6389401329463
new min fval from sgd:  -1679.639508033517
new min fval from sgd:  -1679.6416982145938
new min fval from sgd:  -1679.6434559889613
new min fval from sgd:  -1679.6438888679436
new min fval from sgd:  -1679.6442138770014
new min fval from sgd:  -1679.6446053091984
new min fval from sgd:  -1679.6451278966956
new min fval from sgd:  -1679.645691962012
new min fval from sgd:  -1679.646532948298
new min fval from sgd:  -1679.6469181503248
new min fval from sgd:  -1679.6472460984905
new min fval from sgd:  -1679.6473079452626
new min fval from sgd:  -1679.6477022620938
new min fval from sgd:  -1679.6483060389692
new min fval from sgd:  -1679.6493145342513
new min fval from sgd:  -1679.6502306365883
new min fval from sgd:  -1679.6511033405095
new min fval from sgd:  -1679.651291865892
new min fval from sgd:  -1679.6513389194072
new min fval from sgd:  -1679.653546661478
new min fval from sgd:  -1679.6557433666098
new min fval from sgd:  -1679.6574088398572
new min fval from sgd:  -1679.6584434980373
new min fval from sgd:  -1679.6587245427138
new min fval from sgd:  -1679.6589101676673
new min fval from sgd:  -1679.6595530192233
new min fval from sgd:  -1679.6596088551044
new min fval from sgd:  -1679.6598604490848
new min fval from sgd:  -1679.6605458622091
new min fval from sgd:  -1679.6610617657332
new min fval from sgd:  -1679.6614320744332
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00135527]
objective value function right now is: -1679.6535674742715
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00150059]
objective value function right now is: -1679.6340644358015
min fval:  -1679.6614320744332
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -6.9129,  -5.5756],
        [  6.9423,  -0.1279],
        [ -2.7023,   6.1489],
        [ -2.6881,   6.1270],
        [ -0.7182,   1.3694],
        [ -2.0942,   4.8914],
        [ -2.6637,   6.0119],
        [ -0.7182,   1.3694],
        [  7.2637,  -0.1346],
        [  7.6517,  -0.1171],
        [ -0.7177,   1.3691],
        [  8.2513,   0.5804],
        [-10.2430,  -8.7138]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-4.2193, -7.3598,  5.3272,  5.2753, -1.6118,  1.8168,  5.0811, -1.6118,
        -7.3937, -7.4698, -1.6117, -7.4841, -3.3853], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.3278e-02, -9.9191e-02, -1.3258e-01, -1.2925e-01, -9.9666e-03,
         -9.1774e-03, -1.1822e-01, -9.9666e-03, -1.2894e-01, -1.6235e-01,
         -9.9682e-03, -1.5369e-01, -1.0693e-01],
        [-3.3278e-02, -9.9192e-02, -1.3258e-01, -1.2925e-01, -9.9667e-03,
         -9.1775e-03, -1.1822e-01, -9.9667e-03, -1.2894e-01, -1.6236e-01,
         -9.9683e-03, -1.5369e-01, -1.0693e-01],
        [-3.3278e-02, -9.9191e-02, -1.3258e-01, -1.2925e-01, -9.9666e-03,
         -9.1774e-03, -1.1822e-01, -9.9666e-03, -1.2894e-01, -1.6235e-01,
         -9.9682e-03, -1.5369e-01, -1.0693e-01],
        [ 1.6173e+00,  3.2321e-01, -3.3286e+00, -3.1816e+00, -6.8405e-02,
         -2.7828e-01, -2.9744e+00, -6.8404e-02,  5.0966e-01,  8.8364e-01,
         -6.8791e-02,  1.3245e+00,  2.3700e+00],
        [-2.1144e+00,  1.2892e+00, -1.5148e+00, -1.4776e+00, -8.1134e-04,
         -2.3420e-01, -1.3908e+00, -8.1099e-04,  1.6003e+00,  2.0141e+00,
         -8.2251e-04,  1.6942e+00, -8.0240e+00],
        [ 4.7532e+00,  2.0108e+00, -5.2247e+00, -4.9953e+00, -1.4880e-03,
         -6.9636e-01, -4.5089e+00, -1.4904e-03,  2.2067e+00,  2.7988e+00,
          1.3072e-04,  3.5448e+00,  3.8718e+00],
        [ 4.7411e+00,  2.2908e+00, -5.4346e+00, -5.1729e+00,  1.3541e-02,
         -7.3113e-01, -4.8255e+00,  1.3534e-02,  2.5663e+00,  3.2196e+00,
          1.6513e-02,  3.9994e+00,  4.1110e+00],
        [-3.3277e-02, -9.9751e-02, -1.3309e-01, -1.2975e-01, -1.0011e-02,
         -9.2152e-03, -1.1871e-01, -1.0011e-02, -1.2959e-01, -1.6309e-01,
         -1.0013e-02, -1.5439e-01, -1.0718e-01],
        [-5.1224e+00, -2.4476e+00,  5.5280e+00,  5.2221e+00,  3.1160e-02,
          1.1239e+00,  5.0637e+00,  3.1175e-02, -2.8312e+00, -3.3376e+00,
          2.9865e-02, -4.4902e+00, -4.0941e+00],
        [ 3.6366e+00,  6.2728e-01, -4.7170e+00, -4.5498e+00,  1.7884e-02,
         -5.6402e-01, -4.1774e+00,  1.7883e-02,  8.2788e-01,  1.2462e+00,
          1.8472e-02,  1.6394e+00,  3.6394e+00],
        [ 5.8899e-02, -1.0555e-01, -1.1830e+00, -1.1445e+00, -8.2920e-02,
         -1.0873e-01, -1.0392e+00, -8.2920e-02, -9.8416e-02, -6.7950e-02,
         -8.3012e-02, -7.1886e-02,  1.6781e-01],
        [-3.3278e-02, -9.9191e-02, -1.3258e-01, -1.2925e-01, -9.9666e-03,
         -9.1774e-03, -1.1822e-01, -9.9666e-03, -1.2894e-01, -1.6235e-01,
         -9.9682e-03, -1.5369e-01, -1.0693e-01],
        [-6.0473e+00, -2.7952e+00,  6.0076e+00,  5.7977e+00,  2.4430e-02,
          1.4193e+00,  5.6013e+00,  2.4411e-02, -3.3050e+00, -3.6080e+00,
          2.9249e-02, -4.7735e+00, -4.3872e+00]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.0894, -1.0894, -1.0894, -0.6252,  3.1448, -0.0434,  0.3398, -1.0994,
        -0.4799, -0.2699, -1.5554, -1.0894, -0.3954], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 7.1618e-03,  7.1620e-03,  7.1618e-03, -2.7460e+00,  4.8866e+00,
         -5.8856e+00, -6.6030e+00,  7.2883e-03,  6.8556e+00, -4.6016e+00,
         -1.0635e+00,  7.1618e-03,  8.7391e+00]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  6.2054,   2.2075],
        [ -3.5841,   9.6302],
        [ -5.4254, -10.1925],
        [ -1.5645,   0.5875],
        [ -1.5712,   0.5851],
        [ 11.4366,  -0.3531],
        [  8.9493,   9.3587],
        [-12.2547,   4.9888],
        [  1.7511,   4.3508],
        [-10.7398,  -5.3793],
        [ 11.6358,   4.9495],
        [  6.9609,  -6.0148],
        [ -1.5721,   0.5849]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-7.4902,  9.4850, -8.4611, -2.3789, -2.3702, -9.5837,  6.5757,  5.5331,
        -6.5913, -2.3382, -1.4083, -6.4915, -2.3686], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-7.6907e-02, -7.5930e+00,  5.6670e+00, -1.7500e-01, -1.7484e-01,
         -6.2683e+00, -1.0717e+01, -4.7850e-01,  1.2080e-03,  5.0798e+00,
         -5.6883e+00,  1.0131e+00, -1.7473e-01],
        [-6.6406e-01,  2.8331e-01, -3.4407e-02,  8.9284e-02,  8.9346e-02,
         -1.4912e+00, -2.0455e+00,  2.3188e+00, -4.1079e-01,  1.8613e+00,
         -1.1245e+00, -1.9662e+00,  8.9625e-02],
        [-2.8254e+00,  7.4215e+00, -1.0421e+01,  1.9782e-01,  2.0916e-01,
         -1.3945e+01,  2.8416e+00,  8.2484e+00, -3.2488e+00,  1.0915e+00,
         -1.3317e-01, -6.5653e+00,  2.0864e-01],
        [ 5.4700e-01,  3.3111e-01, -1.0061e+00,  1.3517e-01,  1.3560e-01,
          2.3255e-01, -7.8290e-01,  1.6862e+00,  7.8397e-01, -1.2921e+00,
         -2.6590e-01, -2.6086e+00,  1.3452e-01],
        [ 4.6589e-01, -1.1178e+01,  5.6960e+00, -9.7619e-02, -1.0238e-01,
          4.5457e+00, -7.0550e+00, -5.7470e+00, -2.0705e-02,  6.0796e+00,
         -2.3872e+00,  4.9258e+00, -1.0414e-01],
        [ 4.3332e+00, -5.3812e+00,  9.2572e+00, -7.8220e-02, -7.9752e-02,
          1.3098e+01, -3.9035e+00, -8.4554e+00,  1.8662e+00,  2.7731e+00,
         -3.4259e+00,  6.4812e+00, -8.0078e-02],
        [-5.6848e-01,  2.0924e-01, -1.4934e-01,  9.2208e-02,  9.2795e-02,
         -1.1327e+00, -1.7449e+00,  1.8000e+00, -4.6369e-01,  1.3176e+00,
         -1.0022e+00, -1.8861e+00,  9.3040e-02],
        [ 6.7443e-01,  4.2684e+00, -2.8831e+00,  1.3882e-01,  1.4046e-01,
          3.8522e+00, -1.3628e+00,  1.7259e+00,  5.5657e-02,  2.4067e+00,
         -2.6520e-01, -1.1519e+00,  1.4223e-01],
        [-1.1391e-01, -5.9082e-01, -6.2726e-01, -4.3607e-03, -4.5164e-03,
         -5.6398e-01, -1.3291e+00, -9.6560e-02, -1.3871e-01, -2.9010e-01,
         -8.0819e-01, -1.4162e+00, -4.5321e-03],
        [-2.4575e+00, -9.8241e-01, -5.6782e+00,  1.1868e-01,  1.3305e-01,
         -5.3697e+00,  1.9832e+00,  8.0376e+00, -1.3422e+00,  1.3424e+00,
         -2.4953e+00, -1.3951e+00,  1.3240e-01],
        [-9.9390e-02, -6.6623e+00,  5.9398e+00,  4.9121e-01,  4.9433e-01,
         -1.1030e+01, -1.0579e+01,  1.9127e+00,  1.5565e-03,  4.1146e+00,
         -4.3908e+00, -2.4749e-01,  4.9535e-01],
        [-6.9014e-01,  2.8237e-01, -2.2954e-02,  8.8226e-02,  8.8288e-02,
         -1.5246e+00, -2.0578e+00,  2.3292e+00, -4.3918e-01,  1.8139e+00,
         -1.1096e+00, -1.8938e+00,  8.8564e-02],
        [ 2.6582e+00, -3.6078e+00,  7.1081e+00, -1.5593e-02, -2.1058e-02,
          5.9805e+00, -4.8014e+00, -7.3405e+00,  4.7119e-01,  2.7404e+00,
         -3.8724e+00,  6.4743e+00, -2.1100e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.6238, -1.6321, -5.4538, -1.5946,  1.1030,  1.7483, -1.5910,  1.1959,
        -1.7281, -1.1401, -2.0444, -1.6024,  1.1806], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-2.6571e+00,  7.8199e-01,  1.7762e+00,  7.3247e-02,  3.9138e+00,
         -8.3079e-01,  7.2786e-01,  9.0185e-01,  2.2740e-01,  5.9552e-01,
         -4.9489e+00,  7.6466e-01, -2.7476e+00],
        [ 2.5057e+00, -7.5097e-01, -2.0821e-01, -5.3214e-02, -2.6589e+00,
          1.5382e+00, -6.9515e-01,  4.4606e-01, -1.9326e-01, -1.2644e-01,
          4.9559e+00, -7.3238e-01,  2.7043e+00],
        [-3.8685e-02, -2.3722e-02, -1.5952e+00, -5.2407e-02, -1.8141e+00,
         -2.3921e+00, -3.4706e-02, -6.4457e+00, -3.2477e-02, -3.3368e-01,
         -1.2507e-03, -2.6123e-02, -2.2696e+00],
        [ 1.7163e+00, -3.4329e-01, -1.2399e+01,  9.3239e-01, -1.8099e+00,
          2.0410e+00, -2.6452e-01, -8.2488e-01, -1.0410e-01,  6.6554e+00,
          8.5563e+00, -3.6002e-01,  1.9873e-01],
        [ 6.3048e-01, -3.6344e+00, -9.1845e+00,  3.2742e+00,  2.0413e+00,
          6.1336e+00, -2.2209e+00,  8.4466e+00, -4.6288e-02, -1.0354e-01,
          2.6766e-01, -3.6075e+00,  2.1789e+00]], device='cuda:0'))])
xi:  [0.00097516]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 92.67046315807922
W_T_median: 53.62214137869065
W_T_pctile_5: 0.0010725803371013144
W_T_CVAR_5_pct: -90.90154905077759
Average q (qsum/M+1):  55.648780084425404
Optimal xi:  [0.00097516]
Expected(across Rb) median(across samples) p_equity:  0.22352741369977594
obj fun:  tensor(-1679.6614, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1431.372927519675
Current xi:  [114.575745]
objective value function right now is: -1431.372927519675
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1443.3603191202665
Current xi:  [133.13344]
objective value function right now is: -1443.3603191202665
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1461.966415578716
Current xi:  [154.08272]
objective value function right now is: -1461.966415578716
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1657.2289352338016
Current xi:  [172.57024]
objective value function right now is: -1657.2289352338016
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1665.5545100111449
Current xi:  [174.05177]
objective value function right now is: -1665.5545100111449
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1696.054763289059
Current xi:  [179.14833]
objective value function right now is: -1696.054763289059
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1697.0621458600635
Current xi:  [190.81172]
objective value function right now is: -1697.0621458600635
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1700.8446815183017
Current xi:  [200.92377]
objective value function right now is: -1700.8446815183017
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1702.6543169510762
Current xi:  [210.1935]
objective value function right now is: -1702.6543169510762
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1705.3891721390614
Current xi:  [217.70268]
objective value function right now is: -1705.3891721390614
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [224.6165]
objective value function right now is: -1703.9605340010485
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1707.1556871492373
Current xi:  [229.39471]
objective value function right now is: -1707.1556871492373
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [233.84868]
objective value function right now is: -1704.9563337868342
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [238.32384]
objective value function right now is: -1705.6300809840718
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1709.1634559266693
Current xi:  [242.63565]
objective value function right now is: -1709.1634559266693
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [246.34003]
objective value function right now is: -1707.4806558441192
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [248.3782]
objective value function right now is: -1708.4402348584322
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [249.92613]
objective value function right now is: -1707.7205505452519
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1709.7020592257475
Current xi:  [251.38863]
objective value function right now is: -1709.7020592257475
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [253.14957]
objective value function right now is: -1708.7614004149482
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [254.43823]
objective value function right now is: -1707.547799473391
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [255.49481]
objective value function right now is: -1709.5861539170528
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1710.4039236124067
Current xi:  [255.63257]
objective value function right now is: -1710.4039236124067
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [256.5961]
objective value function right now is: -1707.261085255861
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [257.13657]
objective value function right now is: -1710.060400935362
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [257.94473]
objective value function right now is: -1706.2562782636057
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [257.05905]
objective value function right now is: -1706.4565206908399
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [257.31772]
objective value function right now is: -1710.2723978759707
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [258.92343]
objective value function right now is: -1706.224710032286
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [258.47543]
objective value function right now is: -1706.7835762031502
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [258.92487]
objective value function right now is: -1706.7829882145156
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [259.20737]
objective value function right now is: -1708.9066917583882
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [260.3428]
objective value function right now is: -1708.4144268732005
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [260.8037]
objective value function right now is: -1709.5087119398988
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [260.3011]
objective value function right now is: -1708.3918290602467
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1710.97856680729
Current xi:  [260.16406]
objective value function right now is: -1710.97856680729
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1711.6394054408904
Current xi:  [260.0661]
objective value function right now is: -1711.6394054408904
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1711.8140698958496
Current xi:  [260.22165]
objective value function right now is: -1711.8140698958496
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [260.33694]
objective value function right now is: -1711.7588597650965
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [260.5106]
objective value function right now is: -1711.6113193669933
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [260.82153]
objective value function right now is: -1711.7418040095713
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [260.9132]
objective value function right now is: -1711.5789689194119
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1712.199280236968
Current xi:  [261.1363]
objective value function right now is: -1712.199280236968
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [261.31244]
objective value function right now is: -1712.1284602360936
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [261.32367]
objective value function right now is: -1711.794373245473
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [261.50412]
objective value function right now is: -1711.3877514866492
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [261.6696]
objective value function right now is: -1712.0966531954057
new min fval from sgd:  -1712.2023674794784
new min fval from sgd:  -1712.2435385368503
new min fval from sgd:  -1712.2464103586756
new min fval from sgd:  -1712.2647974830102
new min fval from sgd:  -1712.31890479706
new min fval from sgd:  -1712.357842695042
new min fval from sgd:  -1712.3815744863991
new min fval from sgd:  -1712.4116830478927
new min fval from sgd:  -1712.4438546846911
new min fval from sgd:  -1712.4471274215468
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [261.7294]
objective value function right now is: -1712.394478403478
new min fval from sgd:  -1712.4494465617483
new min fval from sgd:  -1712.4496885885367
new min fval from sgd:  -1712.4817681622233
new min fval from sgd:  -1712.4823665486256
new min fval from sgd:  -1712.499780474441
new min fval from sgd:  -1712.5239829342086
new min fval from sgd:  -1712.5355862802608
new min fval from sgd:  -1712.5464370285595
new min fval from sgd:  -1712.5563079345125
new min fval from sgd:  -1712.5697464394664
new min fval from sgd:  -1712.5700470974903
new min fval from sgd:  -1712.575500208517
new min fval from sgd:  -1712.5760270399414
new min fval from sgd:  -1712.5787603574552
new min fval from sgd:  -1712.581385768862
new min fval from sgd:  -1712.5849533914723
new min fval from sgd:  -1712.5880905055
new min fval from sgd:  -1712.5910604977255
new min fval from sgd:  -1712.5962383936608
new min fval from sgd:  -1712.603309458367
new min fval from sgd:  -1712.6144481301799
new min fval from sgd:  -1712.6182182736106
new min fval from sgd:  -1712.6214630787592
new min fval from sgd:  -1712.622558618955
new min fval from sgd:  -1712.6264218409426
new min fval from sgd:  -1712.634252180763
new min fval from sgd:  -1712.6425841693506
new min fval from sgd:  -1712.6436040878164
new min fval from sgd:  -1712.6446948547373
new min fval from sgd:  -1712.6485879845816
new min fval from sgd:  -1712.6495577973901
new min fval from sgd:  -1712.657692711942
new min fval from sgd:  -1712.6596990559221
new min fval from sgd:  -1712.6664148726543
new min fval from sgd:  -1712.6683898598496
new min fval from sgd:  -1712.6694533295595
new min fval from sgd:  -1712.6740542237192
new min fval from sgd:  -1712.6757726549915
new min fval from sgd:  -1712.6787899989738
new min fval from sgd:  -1712.6859826926059
new min fval from sgd:  -1712.6868570597592
new min fval from sgd:  -1712.7003383231258
new min fval from sgd:  -1712.7086727738265
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [261.94992]
objective value function right now is: -1712.6549490495129
new min fval from sgd:  -1712.7114891787508
new min fval from sgd:  -1712.7145315799341
new min fval from sgd:  -1712.7190973474853
new min fval from sgd:  -1712.7212430099478
new min fval from sgd:  -1712.7232364333256
new min fval from sgd:  -1712.7268754363813
new min fval from sgd:  -1712.727643451454
new min fval from sgd:  -1712.7302167011658
new min fval from sgd:  -1712.7306762840694
new min fval from sgd:  -1712.7318332927205
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [261.88385]
objective value function right now is: -1712.5736086327477
min fval:  -1712.7318332927205
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-19.2010,   8.8941],
        [ -1.1616,   0.5743],
        [ -1.1597,   0.5748],
        [ -1.1577,   0.5754],
        [ -1.1585,   0.5752],
        [ -1.1578,   0.5754],
        [ -5.8459,  -8.6554],
        [ 10.5575,  -0.2619],
        [ 10.6012,  -1.1028],
        [ -6.8086,   8.1425],
        [ -1.1594,   0.5751],
        [ -1.1584,   0.5753],
        [ -1.1580,   0.5753]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  8.3978,  -2.7932,  -2.7925,  -2.7930,  -2.7933,  -2.7935,  -4.2312,
        -10.1800, -10.0707,   7.4403,  -2.7946,  -2.7941,  -2.7934],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 5.4204e-01,  8.3361e-02,  8.3256e-02,  8.3045e-02,  8.3086e-02,
          8.3013e-02, -1.0115e-01,  3.8863e-01,  5.0295e-01,  1.6767e+00,
          8.3057e-02,  8.3011e-02,  8.3034e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-7.5221e+00,  6.2216e-02,  6.1504e-02,  6.5443e-02,  6.5902e-02,
          6.7372e-02,  1.8094e+01,  8.1252e+00,  6.9384e+00, -4.2298e+00,
          7.0226e-02,  6.9325e-02,  6.6908e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.1658e+00,  2.1394e-02,  1.6289e-02,  7.5557e-03,  9.5136e-03,
          6.5203e-03,  1.8232e+01,  6.0558e+00,  7.1338e+00, -4.0712e+00,
          9.1865e-03,  6.9017e-03,  7.3557e-03],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-8.1081e+00, -7.0500e-02, -6.5133e-02, -6.1419e-02, -6.3677e-02,
         -6.2530e-02,  1.7942e+01,  8.7360e+00,  7.8395e+00, -4.7533e+00,
         -6.7970e-02, -6.4947e-02, -6.2793e-02],
        [-9.5282e-02, -1.1151e-02, -1.1163e-02, -1.1166e-02, -1.1160e-02,
         -1.1161e-02, -1.0855e-01, -1.5672e-01, -1.6606e-01, -7.0344e-01,
         -1.1146e-02, -1.1153e-02, -1.1161e-02],
        [-5.2019e+00,  2.9834e-01,  2.9866e-01,  2.9911e-01,  2.9898e-01,
          2.9913e-01,  1.1758e+01,  2.8283e+00,  2.6221e+00, -4.0749e+00,
          2.9890e-01,  2.9906e-01,  2.9909e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 4.3868, -1.6059, -0.9413, -1.6059, -1.0647, -1.6059, -1.6059, -1.6059,
        -1.6059, -1.6059, -1.3230, -1.6059, -1.6457], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.2981e+01,  1.2623e-02, -1.0597e+01,  1.2623e-02, -1.0303e+01,
          1.2623e-02,  1.2623e-02,  1.2623e-02,  1.2623e-02,  1.2623e-02,
         -1.0107e+01,  1.2623e-02, -4.7178e+00]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -6.8981,  -7.9649],
        [-10.9503,   6.5660],
        [ -1.8814,   0.1924],
        [-11.8131,  -4.8737],
        [  4.1311,   8.9081],
        [ 12.4619,   6.2340],
        [-11.1937,   0.2291],
        [ -1.0297,  13.7987],
        [ -1.8814,   0.1927],
        [  6.2985,   8.7234],
        [  1.5562,   9.6525],
        [ 11.3925,   4.4920],
        [-12.1812,  -0.1090]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-6.1847,  5.5629, -3.7034, -3.2978, -7.9624,  2.9958,  8.3330,  6.6302,
        -3.7044,  2.6487,  6.2144, -0.6567,  9.9923], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.9546e+00, -2.5927e-01,  1.2440e-01, -7.7089e-01, -1.6169e-01,
          1.9287e+00, -2.6426e+00, -1.3133e+01,  1.2459e-01,  3.2064e+00,
          1.3232e+00,  2.6703e-01, -6.1990e+00],
        [ 5.7482e-02,  2.8332e+00, -2.4149e-01,  2.2683e-02,  6.3432e+00,
         -1.7663e+00, -1.1378e+01,  1.8372e+00, -2.4149e-01, -2.7482e-01,
          1.6546e+00, -1.7156e+00, -1.4493e+01],
        [-1.5705e+01,  8.5627e+00,  1.7504e-01, -1.8245e+00, -3.4437e+00,
         -4.8747e+00,  4.2497e+00,  8.4923e+00,  1.7496e-01,  1.8658e+00,
         -1.3945e+00, -3.9132e+00,  5.3923e+00],
        [-2.3983e+00, -1.3750e+01,  1.8326e-01, -3.0140e+00,  3.6535e+00,
          5.4045e+00, -4.2681e+00, -1.0690e+01,  1.8262e-01,  1.2769e+00,
          6.8748e+00,  2.9334e+00, -8.0795e+00],
        [-2.5300e-01, -5.9313e-01,  6.8050e-03, -8.3695e-02, -1.3481e-01,
         -1.8726e+00, -5.9762e-01, -2.8057e-01,  6.8429e-03, -1.1632e+00,
         -8.6191e-01, -1.9899e+00, -8.2710e-01],
        [-2.5303e-01, -5.9329e-01,  6.8086e-03, -8.3941e-02, -1.3514e-01,
         -1.8726e+00, -5.9815e-01, -2.7942e-01,  6.8464e-03, -1.1625e+00,
         -8.6146e-01, -1.9897e+00, -8.2773e-01],
        [-2.5266e-01, -5.9572e-01,  6.8198e-03, -8.7278e-02, -1.4048e-01,
         -1.8723e+00, -6.0665e-01, -2.6186e-01,  6.8571e-03, -1.1521e+00,
         -8.5434e-01, -1.9862e+00, -8.3775e-01],
        [-2.7205e-01, -6.0582e-01,  8.1218e-03, -1.0733e-01, -1.6508e-01,
         -1.8679e+00, -6.5034e-01, -1.5885e-01,  8.1541e-03, -1.0841e+00,
         -8.2225e-01, -1.9486e+00, -8.8637e-01],
        [-3.6862e+00,  3.0295e+00, -1.6090e-01, -4.5702e+00,  3.8485e-05,
          9.4414e+00, -3.6149e+00,  7.3836e-02, -1.6069e-01,  4.1823e-01,
          2.7291e+00,  2.8268e+00, -1.3360e+00],
        [ 3.3685e+00, -4.4660e-01, -1.2750e-01,  4.7870e+00, -1.2035e-03,
         -1.1253e+01,  4.2450e+00, -6.9133e+00, -1.2845e-01, -1.1255e+01,
         -1.5008e+01, -5.9594e+00,  5.6190e+00],
        [ 4.6716e+00, -9.6638e+00,  2.6870e-01,  9.3119e+00,  4.1322e-03,
         -7.1652e+00,  1.1206e+00, -2.1356e+01,  2.6997e-01, -3.2723e+00,
         -6.5481e+00, -4.0466e+00,  4.0470e+00],
        [-1.3765e+00, -1.2187e+01, -7.6873e-03,  3.8040e-01,  2.2691e+00,
          3.9446e-01, -1.3470e+00, -2.3680e+00, -7.6696e-03,  3.1427e+00,
          9.8105e+00, -1.3591e+00, -5.5141e+00],
        [ 3.6637e-01,  4.3879e-01,  5.2748e-01,  2.0317e+00,  9.2198e-03,
         -1.5847e+00,  7.5107e-01, -3.2943e+00,  5.2678e-01, -2.8752e+00,
         -5.7165e-01, -2.8248e+00,  6.4352e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  0.3862,  -2.4238, -10.3032,   2.7196,  -1.9168,  -1.9169,  -1.9168,
         -1.9408,   8.5897,  -2.2269,   3.3610,  -2.3266,  -1.0903],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.1000e+00, -1.0857e-01,  1.4673e+00, -1.6669e-01,  5.5109e-01,
          5.5057e-01,  5.4300e-01,  4.9721e-01,  1.6029e+00, -3.0176e+00,
         -1.4634e+01,  5.4741e-01,  1.5294e+00],
        [ 1.3754e-01,  1.1647e+00, -2.1128e-01,  1.3751e+00, -6.1155e-01,
         -6.1140e-01, -6.0938e-01, -5.8715e-01,  1.2712e+00, -1.8266e+00,
         -3.0872e-02,  1.0355e+00, -8.8206e-01],
        [-2.1030e+00, -5.0101e-02, -2.8225e+00, -2.8128e+00,  2.5894e-01,
          2.5912e-01,  2.6232e-01,  2.6933e-01, -5.4392e+00,  7.1206e+00,
          5.6263e+00, -4.1053e-01, -9.9450e-02],
        [-8.5912e-01,  1.2201e+00, -7.6673e+00,  1.0562e+00, -1.0571e+00,
         -1.0568e+00, -1.0534e+00, -1.0078e+00, -7.4609e-01,  1.6258e+01,
          8.9904e+00,  4.8649e-01, -1.7048e+00],
        [ 3.2684e-04,  6.5766e-05,  2.3863e-03,  4.7762e-04,  6.1573e-02,
          6.2129e-02,  7.0301e-02,  1.1758e-01,  1.9040e+01, -3.9017e+00,
         -3.0945e+00, -3.3614e-03, -2.0512e+00]], device='cuda:0'))])
xi:  [261.8972]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 581.8731876802971
W_T_median: 459.7263737685786
W_T_pctile_5: 261.9537859587363
W_T_CVAR_5_pct: 109.01314952403092
Average q (qsum/M+1):  51.73291409400202
Optimal xi:  [261.8972]
Expected(across Rb) median(across samples) p_equity:  0.275883549451828
obj fun:  tensor(-1712.7318, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1622.8765234864566
Current xi:  [115.063194]
objective value function right now is: -1622.8765234864566
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1636.3216269170375
Current xi:  [123.30033]
objective value function right now is: -1636.3216269170375
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1699.140369327279
Current xi:  [136.89532]
objective value function right now is: -1699.140369327279
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1703.730505301482
Current xi:  [157.10585]
objective value function right now is: -1703.730505301482
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1727.1991305510446
Current xi:  [174.46432]
objective value function right now is: -1727.1991305510446
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [193.31425]
objective value function right now is: -1726.1003616978921
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1749.0145559891666
Current xi:  [208.60918]
objective value function right now is: -1749.0145559891666
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [222.9858]
objective value function right now is: -1737.0598007531464
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1766.8128599182926
Current xi:  [236.24474]
objective value function right now is: -1766.8128599182926
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [248.24956]
objective value function right now is: -1760.691437601402
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1767.5548359456059
Current xi:  [259.7236]
objective value function right now is: -1767.5548359456059
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1776.822528624788
Current xi:  [268.39066]
objective value function right now is: -1776.822528624788
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [277.15802]
objective value function right now is: -1769.5872799079468
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1779.7073426681138
Current xi:  [285.86362]
objective value function right now is: -1779.7073426681138
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1783.7885146588646
Current xi:  [293.39307]
objective value function right now is: -1783.7885146588646
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1785.6440802486684
Current xi:  [299.39383]
objective value function right now is: -1785.6440802486684
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [304.95438]
objective value function right now is: -1783.6692000951796
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1785.757478265348
Current xi:  [308.90665]
objective value function right now is: -1785.757478265348
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [312.70844]
objective value function right now is: -1782.6254960636861
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1786.810608899042
Current xi:  [315.74594]
objective value function right now is: -1786.810608899042
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [318.09155]
objective value function right now is: -1784.4188589624503
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [319.4387]
objective value function right now is: -1785.966946458147
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1790.0597551604912
Current xi:  [320.443]
objective value function right now is: -1790.0597551604912
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [322.3799]
objective value function right now is: -1787.8794108096417
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [323.36832]
objective value function right now is: -1788.625306292138
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [324.21237]
objective value function right now is: -1789.0666836666423
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [324.89145]
objective value function right now is: -1789.1523878267515
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [324.6261]
objective value function right now is: -1787.14861561583
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [325.32422]
objective value function right now is: -1752.5491289151441
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [325.10773]
objective value function right now is: -1775.8107692107296
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [313.07727]
objective value function right now is: -1596.6735327753438
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [299.99146]
objective value function right now is: -1663.8989570873453
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [277.75806]
objective value function right now is: -1664.5398955044245
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [269.83484]
objective value function right now is: -1661.4020968455031
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [264.69226]
objective value function right now is: -1694.8845453292622
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [263.79034]
objective value function right now is: -1697.9883785799957
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [262.86304]
objective value function right now is: -1700.3083637613076
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [263.06946]
objective value function right now is: -1730.6351935809619
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [265.46225]
objective value function right now is: -1759.084134261086
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [267.72067]
objective value function right now is: -1765.3689517423782
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [270.07037]
objective value function right now is: -1768.2124258284928
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [272.5626]
objective value function right now is: -1773.4382640286656
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [274.9973]
objective value function right now is: -1774.9456013068786
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [277.23627]
objective value function right now is: -1776.0092164466596
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [279.46164]
objective value function right now is: -1777.4926000081823
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [281.78073]
objective value function right now is: -1777.5771073430064
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [283.65826]
objective value function right now is: -1779.3654936852022
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [285.9778]
objective value function right now is: -1779.8355412664605
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [287.26746]
objective value function right now is: -1783.7513358370145
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [287.7453]
objective value function right now is: -1785.684048294483
min fval:  -1770.4149106061975
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 6.8691,  0.4991],
        [ 5.3481, -3.6888],
        [ 6.6349, -5.0485],
        [-3.4826,  2.8792],
        [ 3.8488,  7.9475],
        [-7.0617,  5.2385],
        [ 5.9280,  3.1621],
        [-8.0496,  6.4424],
        [-1.4520,  1.4474],
        [11.8409, -5.7979],
        [ 8.6245, -0.7025],
        [-4.9385, -3.0418],
        [ 1.0748,  0.1861]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-6.6941, -4.3319, -4.6135, -4.7102,  2.9032, -2.0332, -8.3677, -1.7945,
        -5.7559, -4.6679, -7.4236, -4.9147, -6.4223], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 3.9697e-01,  3.1981e+00,  4.8610e+00, -3.9676e-01,  7.8986e+00,
         -2.1244e+00,  9.6757e-02, -1.2545e+00, -1.2658e-01,  1.0819e+01,
          3.7456e-01, -1.5057e+00,  7.1324e-02],
        [-1.8798e-01, -2.0381e+00, -3.3830e+00, -1.1158e-01, -5.8494e+00,
          2.0337e-02, -1.0740e-01, -1.4427e-01, -1.5314e-01, -6.5306e+00,
         -1.5974e-01,  3.6816e-01, -1.4182e-01],
        [-6.8815e-01, -8.9275e-01, -1.1120e+00,  4.0699e-03,  1.7391e+00,
         -4.9932e-02, -3.4206e-01, -6.7084e-02,  9.0987e-03, -1.9080e+00,
         -8.1415e-01, -2.5243e-01,  2.3475e-03],
        [-1.5980e-01, -1.9678e+00, -3.2583e+00, -1.1935e-01, -5.6206e+00,
         -1.1298e-01, -9.2935e-02, -1.6823e-01, -1.3807e-01, -6.0011e+00,
         -1.3450e-01,  9.1327e-02, -1.2751e-01],
        [-2.6656e-02, -2.2061e-01, -4.6799e-01, -5.4150e-04, -9.3769e-01,
         -1.7948e-02, -4.1459e-02, -2.3284e-02,  3.9216e-03, -7.0311e-01,
         -1.1447e-02, -2.8074e-02,  1.9559e-03],
        [ 5.3182e-01,  3.4665e+00,  5.2864e+00, -5.8525e-01,  8.2398e+00,
         -2.8305e+00,  7.4304e-02, -1.7713e+00, -2.4706e-01,  1.1523e+01,
          4.9901e-01, -1.5125e+00,  7.6980e-02],
        [-3.7312e+00, -2.8954e+00, -3.7048e+00, -3.2174e-03,  1.2820e+01,
          9.6897e-01, -4.3328e+00,  1.3528e+00,  1.4203e-01, -4.8722e+00,
         -8.1394e+00, -1.6225e+00, -1.4208e-01],
        [ 3.4228e-01,  3.0671e+00,  4.7407e+00, -3.1210e-01,  7.5374e+00,
         -1.7584e+00,  1.1392e-01, -9.9307e-01, -7.5887e-02,  1.0302e+01,
          3.2074e-01, -1.3861e+00,  7.0029e-02],
        [ 4.8712e+00,  2.2463e+00,  5.1592e+00, -3.8883e-01, -1.4986e+01,
         -1.7782e+00,  5.8662e+00, -3.0740e+00, -9.9131e-03,  6.3133e+00,
          8.7569e+00,  5.5243e-01,  1.5939e-01],
        [-6.0532e-01, -2.3837e+00, -4.0215e+00,  3.1015e-01, -6.4077e+00,
          1.8979e+00, -1.8093e-01,  9.6158e-01,  6.7628e-02, -9.0299e+00,
         -5.1285e-01,  1.6991e+00, -1.5519e-01],
        [ 4.7114e+00,  1.9977e+00,  4.9172e+00, -4.6450e-01, -1.4973e+01,
         -1.7798e+00,  6.2613e+00, -3.4824e+00, -1.0373e-02,  6.3109e+00,
          8.6642e+00,  8.3190e-01,  1.5607e-01],
        [-3.9988e-01, -2.2220e+00, -3.7445e+00,  9.9659e-02, -6.4036e+00,
          1.0758e+00, -1.6261e-01,  3.9121e-01, -8.4220e-02, -8.1703e+00,
         -3.4044e-01,  1.3040e+00, -1.6199e-01],
        [-2.7256e+00, -2.3939e+00, -3.0816e+00, -1.2100e-02,  1.0524e+01,
          8.9496e-01, -2.0470e+00,  1.2411e+00,  1.5609e-02, -4.3289e+00,
         -7.0385e+00, -9.8838e-01,  1.8035e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-4.4415,  0.6584, -3.2474,  0.1529, -3.6932, -4.8489, -1.1901, -4.1639,
         0.2883,  2.8189,  0.6129,  2.2316, -1.0885], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 4.4548e+00, -3.6592e+00,  4.3750e-01, -3.1080e+00,  2.7207e-03,
          5.4063e+00,  7.7844e+00,  3.9399e+00, -1.3015e+01, -6.8487e+00,
         -1.3551e+01, -5.6636e+00,  5.3308e+00]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -9.1263,  -3.6167],
        [ -4.0589,   9.6397],
        [ -7.8078,  -2.8976],
        [ 11.7799,   0.2399],
        [ -1.8647,   0.4227],
        [ 10.0118,   3.4445],
        [-12.9126,   6.8396],
        [ -9.5172,  -3.8349],
        [ -9.2555,  -3.6467],
        [ -8.7832,  -3.3965],
        [ -9.0393,  -3.5887],
        [  2.7767,   7.8355],
        [ -7.1488,  -2.6163]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-2.6143,  5.9473, -4.8072, -9.6803, -5.5558, -0.2314,  5.5458, -2.5157,
        -2.8676, -0.1712, -2.2908,  5.6779, -5.4527], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-7.4954e+00,  6.8945e+00, -8.6223e-01, -8.2891e+00,  1.0415e-02,
         -2.2444e+00,  2.4047e+01, -7.6819e+00, -5.6224e+00, -1.7612e+01,
         -9.5907e+00,  5.4632e+00, -2.6959e-01],
        [ 4.3737e+00, -1.9419e+00,  2.7658e+00, -5.4232e+00,  4.5687e-01,
         -1.4917e+01,  8.5131e-01,  4.6768e+00,  4.3403e+00,  1.3883e+00,
          4.0554e+00, -2.5981e+01,  2.1589e+00],
        [-7.1076e+00,  1.2860e+01, -4.5590e+00,  8.9153e+00,  3.1607e-01,
          5.7982e+00,  4.0624e+00, -7.7586e+00, -7.0261e+00, -5.9878e+00,
         -6.9827e+00,  1.7215e+01, -3.4170e+00],
        [-5.6231e-01,  7.6034e+00,  1.1545e-01, -1.1078e+01,  1.1793e-02,
         -2.5846e+00,  3.8612e+00, -1.6826e-01,  1.1940e-01, -6.4080e+00,
         -1.6953e+00, -2.5106e-01, -7.7679e-02],
        [-1.7224e-01, -3.2408e-01,  1.5797e-02, -9.0950e-01, -2.8041e-04,
         -3.1375e+00, -1.1983e-01, -2.0743e-01, -1.2355e-01, -9.9530e-01,
         -2.4493e-01, -1.7982e+00,  4.3307e-02],
        [ 2.4250e+00, -2.0986e+00,  4.0620e-01, -3.4883e+00, -3.1699e-02,
         -3.0782e+00, -2.1041e+00,  2.7077e+00,  2.5084e+00,  5.4422e-01,
          2.3666e+00, -7.5172e+00,  1.7464e-01],
        [ 4.0951e+00, -3.3554e+00,  1.5958e+00, -5.4206e+00,  1.3449e-01,
         -3.0521e+00,  5.4311e+00,  4.0932e+00,  3.9556e+00,  2.7876e+00,
          3.9740e+00, -8.3681e+00,  7.9759e-01],
        [-6.2275e+00,  1.5323e+01, -1.5515e+00,  4.7091e+00, -2.7373e-01,
          3.8458e-01,  1.1730e+01, -7.2348e+00, -5.0662e+00, -8.7700e+00,
         -7.1841e+00,  5.2824e+00, -7.9477e-01],
        [-2.9049e-01, -1.8882e-01, -1.7901e-02, -4.0805e-01, -7.8855e-03,
         -3.0447e+00, -3.3416e-02, -3.3970e-01, -2.2618e-01, -1.1763e+00,
         -3.7965e-01, -1.8576e+00, -1.1498e-02],
        [-1.5413e+00,  3.2951e+00, -1.3205e-01, -6.4976e+00, -5.9808e-02,
         -3.7684e+00,  4.7007e+00, -1.9385e+00, -1.3015e+00, -7.7594e-01,
         -1.6285e+00, -5.6343e-01, -5.3215e-02],
        [-2.6041e-01,  5.6287e+00,  5.5370e-01, -1.3120e+01,  1.8162e-01,
         -3.7505e+00,  8.1664e+00, -4.8862e-01,  2.5302e-01, -3.1763e+00,
         -1.0096e+00,  1.2523e+00,  4.0153e-01],
        [ 7.3711e+00, -1.3505e+01,  3.4799e+00, -5.9603e+00,  3.2353e-01,
         -3.5434e+00, -1.1162e+01,  8.1866e+00,  6.9860e+00,  7.7672e+00,
          7.8654e+00, -8.7256e+00,  2.7191e+00],
        [-2.0729e+00, -4.3912e+00, -6.5355e-01,  8.1337e+00,  2.4465e-02,
          1.0644e+00, -7.5345e+00, -2.3196e+00, -2.0551e+00, -7.9579e-01,
         -1.7963e+00,  8.4230e-01, -2.8002e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.0775, -6.8768,  0.0692, -3.3207, -4.2583, -2.7668, -1.4044, -3.8015,
        -4.2018, -6.0988, -4.8378,  3.3074, -0.4592], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.9912e-01,  1.1994e-05,  8.7193e-01,  1.4740e+00,  3.4622e-01,
         -2.6592e-01,  4.5203e-01,  4.8623e-01,  2.9775e-02,  9.2006e-01,
          2.0667e+00, -1.3855e+01, -6.8671e-01],
        [ 1.3763e+00,  1.0876e+01,  6.9493e-01, -7.7032e-01,  8.3497e-02,
         -9.7665e-02, -4.4662e-01,  2.8707e-03,  5.3425e-02, -4.5495e-01,
         -7.0776e-01, -1.2510e+00,  9.5570e-01],
        [ 1.7599e-01, -1.1579e+01, -7.7177e+00, -1.1831e+00, -9.9336e-02,
          1.7612e+00,  2.0274e+00, -6.6755e+00,  1.2992e-01, -4.5873e-01,
          1.6612e-01,  6.2068e+00,  4.3715e-01],
        [ 1.3428e+00, -1.7760e+01, -7.0717e-01, -1.5940e+00,  1.3845e-01,
         -3.2850e+00,  3.5474e+00, -6.9027e-01,  8.8071e-02, -5.2370e-01,
         -6.5079e+00,  8.6914e+00,  1.4040e+00],
        [ 2.7877e+00, -1.7689e+01,  6.8156e+00, -2.4106e+00,  1.7874e-01,
         -5.9917e+00, -5.3175e-01, -2.1683e+00, -9.5235e-02, -1.3876e+00,
         -6.1157e+00,  1.3796e+01,  1.1774e+01]], device='cuda:0'))])
xi:  [279.46164]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 460.40498061378224
W_T_median: 443.43202852178484
W_T_pctile_5: 322.54824532834397
W_T_CVAR_5_pct: 147.921094606491
Average q (qsum/M+1):  50.59237178679435
Optimal xi:  [279.46164]
Expected(across Rb) median(across samples) p_equity:  0.25170307280495763
obj fun:  tensor(-1770.4149, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1372.025574974281
Current xi:  [118.99509]
objective value function right now is: -1372.025574974281
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1664.0932006092962
Current xi:  [133.88837]
objective value function right now is: -1664.0932006092962
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1687.0186252790184
Current xi:  [147.01096]
objective value function right now is: -1687.0186252790184
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1698.39288667064
Current xi:  [161.21257]
objective value function right now is: -1698.39288667064
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1710.6609081782133
Current xi:  [174.49532]
objective value function right now is: -1710.6609081782133
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.93622]
objective value function right now is: -1686.93362602102
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1716.175932624442
Current xi:  [197.28247]
objective value function right now is: -1716.175932624442
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1726.863295279943
Current xi:  [207.26497]
objective value function right now is: -1726.863295279943
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.64691]
objective value function right now is: -1716.1565310355456
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [222.4032]
objective value function right now is: -1725.8236056726723
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1831.3260578652869
Current xi:  [229.37018]
objective value function right now is: -1831.3260578652869
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1922.87885744052
Current xi:  [253.5047]
objective value function right now is: -1922.87885744052
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1934.4777414449159
Current xi:  [267.81192]
objective value function right now is: -1934.4777414449159
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1937.5463897204497
Current xi:  [279.7348]
objective value function right now is: -1937.5463897204497
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1939.83624753389
Current xi:  [289.0317]
objective value function right now is: -1939.83624753389
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1942.0840534073661
Current xi:  [296.46066]
objective value function right now is: -1942.0840534073661
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1944.7041466477688
Current xi:  [300.90714]
objective value function right now is: -1944.7041466477688
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1951.1370894716722
Current xi:  [303.79108]
objective value function right now is: -1951.1370894716722
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [306.48575]
objective value function right now is: -1949.6347238971084
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [308.92014]
objective value function right now is: -1947.1151171203512
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1952.3948747622208
Current xi:  [311.01523]
objective value function right now is: -1952.3948747622208
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1952.89142479399
Current xi:  [311.6547]
objective value function right now is: -1952.89142479399
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [313.03296]
objective value function right now is: -1936.7409929656117
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [312.99725]
objective value function right now is: -1948.7506823872257
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [313.24112]
objective value function right now is: -1950.3267865428165
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [313.23157]
objective value function right now is: -1952.069878223443
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1953.8489317338897
Current xi:  [313.7131]
objective value function right now is: -1953.8489317338897
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [314.8668]
objective value function right now is: -1951.1279737074292
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [314.97202]
objective value function right now is: -1953.6034537852584
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1954.1199860944148
Current xi:  [315.17465]
objective value function right now is: -1954.1199860944148
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [314.9252]
objective value function right now is: -1949.8002611463792
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [315.16452]
objective value function right now is: -1951.0703940297235
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1955.9241621617787
Current xi:  [315.4838]
objective value function right now is: -1955.9241621617787
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [315.06122]
objective value function right now is: -1954.7692154267481
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -1957.745364799952
Current xi:  [314.6829]
objective value function right now is: -1957.745364799952
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1960.1788968811916
Current xi:  [314.7862]
objective value function right now is: -1960.1788968811916
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [315.03198]
objective value function right now is: -1960.025969688708
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [315.42154]
objective value function right now is: -1959.8949592222307
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1960.7883814833824
Current xi:  [315.4112]
objective value function right now is: -1960.7883814833824
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [315.47058]
objective value function right now is: -1960.7499148158452
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1961.060162666874
Current xi:  [315.54144]
objective value function right now is: -1961.060162666874
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1961.3274609583461
Current xi:  [315.64056]
objective value function right now is: -1961.3274609583461
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [315.58386]
objective value function right now is: -1960.962205922853
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [315.44797]
objective value function right now is: -1960.8469704787192
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [315.53595]
objective value function right now is: -1960.6848014410502
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [315.62802]
objective value function right now is: -1961.1977275639713
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [315.65158]
objective value function right now is: -1961.0444583257506
new min fval from sgd:  -1961.3344958359949
new min fval from sgd:  -1961.3915851596735
new min fval from sgd:  -1961.4532386288954
new min fval from sgd:  -1961.5511109101817
new min fval from sgd:  -1961.641615024785
new min fval from sgd:  -1961.7606112853107
new min fval from sgd:  -1961.8582996994658
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [315.47537]
objective value function right now is: -1960.8532000978012
new min fval from sgd:  -1961.8627017116894
new min fval from sgd:  -1961.9115531750576
new min fval from sgd:  -1961.915844043889
new min fval from sgd:  -1961.918379091309
new min fval from sgd:  -1961.9373671178753
new min fval from sgd:  -1961.9488921851837
new min fval from sgd:  -1961.9576452461695
new min fval from sgd:  -1961.9787381997514
new min fval from sgd:  -1962.0007313488954
new min fval from sgd:  -1962.020920935396
new min fval from sgd:  -1962.039410145786
new min fval from sgd:  -1962.0508652413298
new min fval from sgd:  -1962.0631954346659
new min fval from sgd:  -1962.0715517147407
new min fval from sgd:  -1962.0727769773443
new min fval from sgd:  -1962.0776107093461
new min fval from sgd:  -1962.0908343621727
new min fval from sgd:  -1962.1054044438642
new min fval from sgd:  -1962.1202969295337
new min fval from sgd:  -1962.1314392859224
new min fval from sgd:  -1962.1448276056867
new min fval from sgd:  -1962.1608471343357
new min fval from sgd:  -1962.170584642501
new min fval from sgd:  -1962.1755518787293
new min fval from sgd:  -1962.1864581361203
new min fval from sgd:  -1962.1931974354586
new min fval from sgd:  -1962.1961649276752
new min fval from sgd:  -1962.2094996311544
new min fval from sgd:  -1962.2154316378897
new min fval from sgd:  -1962.2242249191524
new min fval from sgd:  -1962.2272436265637
new min fval from sgd:  -1962.2317931380119
new min fval from sgd:  -1962.2342586699422
new min fval from sgd:  -1962.2370658148757
new min fval from sgd:  -1962.2432955270774
new min fval from sgd:  -1962.2501165709598
new min fval from sgd:  -1962.2577194372045
new min fval from sgd:  -1962.2672704103188
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [315.49158]
objective value function right now is: -1962.2307952720819
new min fval from sgd:  -1962.2685360735738
new min fval from sgd:  -1962.2721602221561
new min fval from sgd:  -1962.277968699187
new min fval from sgd:  -1962.2881036748631
new min fval from sgd:  -1962.2896993653994
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [315.55515]
objective value function right now is: -1962.2322086290442
min fval:  -1962.2896993653994
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -2.7095,  12.9892],
        [ -8.6338,   9.4281],
        [ -0.9451,   0.4527],
        [ -0.8640,   0.3781],
        [  6.1895,  -0.6523],
        [  4.5754,  -0.4303],
        [ -0.9502,   0.4519],
        [ -0.9882,   0.4554],
        [  5.3775,  -0.2578],
        [  9.7529,  -0.6215],
        [-74.6407,   0.8966],
        [  5.4828,  -0.5841],
        [ -2.7219,  16.4087]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 7.2031,  5.6080, -3.1690, -3.3192,  2.5904,  3.5518, -3.1733, -3.1998,
         3.1859, -7.8767,  0.2375,  2.9803,  8.6844], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1333e-01,  1.6554e+00,  3.4528e-02,  3.0769e-02,  8.7976e-01,
          9.1042e-01,  3.5304e-02,  3.4772e-02,  8.9665e-01,  2.6447e-01,
          4.3032e-01,  8.9469e-01,  4.6686e-01],
        [-9.2749e+00, -6.0010e+00,  1.6064e-01,  2.4939e-01,  2.6538e+00,
          2.9059e+00,  1.6532e-01,  2.3862e-01,  2.6770e+00,  2.0542e+01,
          5.6439e+00,  2.9504e+00, -1.0311e+01],
        [-5.8631e+00, -5.5932e+00, -1.0882e-01, -2.5602e-01,  1.4267e+00,
          1.6964e+00, -8.6213e-02, -1.9126e-01,  1.4872e+00,  2.0896e+01,
          5.2545e+00,  1.4174e+00, -9.1947e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4955e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.0882e-01,  1.6256e+00,  3.4094e-02,  3.0485e-02,  8.6370e-01,
          8.9398e-01,  3.4826e-02,  3.4369e-02,  8.8039e-01,  2.5781e-01,
          4.1380e-01,  8.7846e-01,  4.6006e-01],
        [-6.2435e+00, -5.0774e+00, -7.9019e-02, -1.1667e-02,  1.6100e+00,
          1.6134e+00, -1.1318e-01, -9.1158e-02,  1.1232e+00,  2.1826e+01,
          5.7213e+00,  1.3367e+00, -9.6095e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4956e-03, -5.9415e-03, -4.7716e-01,
         -4.9046e-01, -6.4683e-03, -6.3263e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1923e-01,  1.6977e+00,  3.5087e-02,  3.1098e-02,  9.0167e-01,
          9.3283e-01,  3.5931e-02,  3.5276e-02,  9.1883e-01,  2.7424e-01,
          4.5388e-01,  9.1684e-01,  4.7618e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5021, -0.5021,  0.9316,  2.2107,  1.1816, -0.5021, -0.5021,  0.9149,
         1.2202, -0.5021, -0.5021, -0.5021,  0.9544], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0192,   0.0192,   3.5844,  -9.2131, -12.7066,   0.0192,   0.0192,
           3.1214, -12.3800,   0.0192,   0.0192,   0.0192,   4.3492]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  2.3638,  12.1836],
        [  5.6572,   7.4721],
        [  7.9224,   6.5803],
        [ -6.1235,   3.1910],
        [ 11.8466,   0.5913],
        [  0.8770,  -8.1352],
        [  8.6891,  -6.8360],
        [-10.5691,  10.8756],
        [  1.7568,  -0.0712],
        [-11.0991,  -4.2491],
        [ 11.4103,   5.2357],
        [ -0.1851,   2.9347],
        [ -7.5723,  -9.3505]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 6.3522, -5.1507, -2.0487, -4.3376, -9.8811, -6.7882, -6.3489,  5.9660,
        -7.1033, -1.4560, -0.7179, -3.6786, -6.0576], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.1852e+00,  1.5094e+00,  1.3526e+00, -5.4268e-01,  2.4223e+00,
          2.0759e+00,  9.2465e-01, -1.6151e+00, -2.8824e-01,  9.5287e-01,
          2.4642e-01, -1.1737e+00,  2.2713e+00],
        [-1.8651e+00, -5.6256e-01, -6.2129e-01, -4.2742e+00, -7.7059e-01,
         -3.4892e+00, -7.6918e-02,  6.7408e+00, -2.3096e-01,  1.3500e+00,
         -3.5500e+00, -6.0667e-01, -2.9353e+00],
        [ 1.1142e-01, -1.2473e+00,  8.1916e-01,  6.4910e-01, -1.6350e+01,
          2.2732e+00, -1.5779e+00,  7.2357e+00, -7.2982e-03,  4.4049e+00,
         -5.8977e-01,  1.7865e+00, -4.6946e+00],
        [-1.3960e+00, -8.7673e-02, -4.2780e-01,  6.7083e-03, -4.0893e-01,
         -9.1685e-01, -1.7927e+00, -5.0702e-01, -1.3463e-02, -5.2559e-01,
         -1.6941e+00,  1.8378e-03, -3.4962e-01],
        [-1.9520e+00,  1.8967e-01, -5.4411e-01, -1.2101e-01, -9.1077e-01,
         -1.2479e+00, -1.1267e+00, -9.1391e-01, -1.8068e-02, -3.8365e-01,
         -2.4970e+00, -1.1678e-01, -2.1655e-01],
        [-1.4057e+00, -8.6477e-02, -4.3293e-01,  1.5699e-02, -4.1127e-01,
         -9.1630e-01, -1.7894e+00, -5.0243e-01, -1.3300e-02, -5.1448e-01,
         -1.7010e+00,  1.2009e-02, -3.4096e-01],
        [ 1.3221e+01, -6.1845e-01,  1.5793e+00, -6.7508e+00, -1.5120e+01,
         -1.8714e+01, -8.9013e+00,  4.8410e+00, -1.2180e+00,  4.2963e+00,
          1.1099e+00,  1.4776e+00, -1.3629e+01],
        [ 4.1482e+00, -4.3534e-01, -7.3632e+00,  2.5918e-02, -1.7989e-01,
         -6.4858e+00, -8.9001e-01, -1.0932e+00,  1.0207e-01,  1.5231e+00,
         -7.6390e+00,  1.3168e-01, -1.0356e+01],
        [-1.4524e+00, -8.1858e-02, -4.5182e-01,  9.7058e-02, -4.0160e-01,
         -8.8229e-01, -1.8020e+00, -4.5301e-01, -1.2095e-02, -4.6125e-01,
         -1.7020e+00,  9.5630e-02, -3.0579e-01],
        [-2.8522e+00, -1.9497e-01, -1.6674e+00, -4.5101e-01, -1.9345e+00,
         -2.7298e+00,  5.5568e-01,  1.5671e+00, -1.2880e-02,  9.4602e-01,
         -2.4953e+00, -2.9629e-01, -2.6501e+00],
        [-2.4334e+01, -6.7212e-02, -4.8855e+00,  1.9330e-01, -7.7025e+00,
          2.7805e+00,  4.4028e-01, -1.4406e+00, -4.7447e-02,  6.3915e+00,
         -8.7817e+00, -4.8862e-01,  5.4980e+00],
        [-1.0961e+01,  5.4093e-01,  8.6739e-01, -6.0412e-02, -3.7470e+00,
          2.1004e+00,  2.4607e+00, -2.1072e+01, -1.4633e-01,  9.1990e+00,
         -5.3680e+00,  1.5515e-01,  4.1397e+00],
        [-1.2571e+01,  2.5529e-02, -4.2095e+00,  2.0473e-01, -1.3435e+01,
          2.3688e+00, -6.0068e-01, -5.7139e+00, -3.7530e-01,  4.5355e+00,
         -9.0221e+00,  3.5582e-02,  2.3988e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 2.9079, -2.1828, -0.1738, -3.3916, -2.9699, -3.3962, -3.0437, -0.0264,
        -3.4166, -2.8315, -0.3684,  1.2132,  0.0742], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 7.4891e-01,  4.5452e+00,  1.8769e+00,  4.5914e-02,  3.5496e-01,
         -9.1354e-02,  2.7526e-01, -9.7636e-01, -1.0948e-01,  3.3561e+00,
         -4.4478e+00, -1.2550e-01, -1.0160e+00],
        [ 5.4131e-01, -4.5533e+00, -3.9331e-01,  2.2764e-01, -3.5322e-01,
          9.4308e-02,  8.6633e-01,  7.9627e-01,  1.1223e-01, -3.0675e+00,
          5.4099e+00,  6.6530e-01,  1.5354e+00],
        [-1.0256e+01, -4.0894e-02, -1.1152e+00, -1.1287e-02, -1.0215e-02,
         -1.1185e-02, -8.9347e-01, -1.4354e-01, -1.1272e-02, -1.5607e-02,
          8.3038e-03, -6.6567e-01,  1.3450e-03],
        [-1.2086e+00, -1.8795e+00, -3.9948e+00,  3.7431e-01,  1.3870e+00,
          3.8828e-01,  9.5588e-01, -1.2326e+01,  4.2495e-01,  2.2745e+00,
          8.1236e-01,  1.8802e+00,  1.0604e+01],
        [ 9.1296e+00, -4.1802e+00, -1.1657e+01,  2.0360e-01,  4.0986e-01,
          2.0817e-01,  5.1155e+00, -6.9774e-01,  2.1607e-01, -5.1843e-01,
          6.2533e+00,  8.3596e+00,  2.6055e+00]], device='cuda:0'))])
xi:  [315.55313]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 415.8129048742505
W_T_median: 382.8353568133376
W_T_pctile_5: 315.53009807596135
W_T_CVAR_5_pct: 157.42776919642714
Average q (qsum/M+1):  48.06485871345766
Optimal xi:  [315.55313]
Expected(across Rb) median(across samples) p_equity:  0.22592176882317289
obj fun:  tensor(-1962.2897, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2274.0301783412797
Current xi:  [318.71152]
objective value function right now is: -2274.0301783412797
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -2277.9189534412794
Current xi:  [322.19766]
objective value function right now is: -2277.9189534412794
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [325.82742]
objective value function right now is: -2272.8258378501137
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [327.6419]
objective value function right now is: -2275.9460630909643
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -2281.7880382635503
Current xi:  [329.33572]
objective value function right now is: -2281.7880382635503
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [331.00388]
objective value function right now is: -2269.7975540184334
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [330.89703]
objective value function right now is: -2274.7242563402842
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [331.7955]
objective value function right now is: -2277.4184749803785
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [332.66592]
objective value function right now is: -2276.164710959764
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [332.34488]
objective value function right now is: -2259.9022303726247
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.13638]
objective value function right now is: -2270.0117262462945
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.37717]
objective value function right now is: -2278.8715874773293
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.5993]
objective value function right now is: -2272.9696614906557
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [333.92648]
objective value function right now is: -2279.639991182323
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.70355]
objective value function right now is: -2276.0075351303703
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.09546]
objective value function right now is: -2274.385611344739
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.5154]
objective value function right now is: -2271.531672968677
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.5753]
objective value function right now is: -2278.5485764562886
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.25735]
objective value function right now is: -2273.606834629256
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2284.0638968443
Current xi:  [332.84702]
objective value function right now is: -2284.0638968443
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [332.93613]
objective value function right now is: -2283.933873754901
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [332.66287]
objective value function right now is: -2277.7910261421325
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.58173]
objective value function right now is: -2275.3536183981887
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.1526]
objective value function right now is: -2282.979515658601
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -2284.297258145675
Current xi:  [333.5543]
objective value function right now is: -2284.297258145675
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.42688]
objective value function right now is: -2281.514628088486
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.12112]
objective value function right now is: -2281.721266644971
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [334.46317]
objective value function right now is: -2283.5075884691087
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [334.5391]
objective value function right now is: -2276.5519698996795
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.88043]
objective value function right now is: -2280.4337301755
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.8271]
objective value function right now is: -2274.7909010573917
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.66098]
objective value function right now is: -2278.1411404392243
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.39294]
objective value function right now is: -2283.1021734849005
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.6161]
objective value function right now is: -2274.2184694974385
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.2113]
objective value function right now is: -2280.349488571263
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2288.1261399820414
Current xi:  [334.07306]
objective value function right now is: -2288.1261399820414
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -2288.5222684741625
Current xi:  [333.96274]
objective value function right now is: -2288.5222684741625
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.99927]
objective value function right now is: -2288.2247247422383
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.20724]
objective value function right now is: -2286.5022589123287
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.1841]
objective value function right now is: -2288.3399622853476
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -2289.5037771608067
Current xi:  [334.22552]
objective value function right now is: -2289.5037771608067
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.0115]
objective value function right now is: -2288.931314196363
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [333.971]
objective value function right now is: -2289.419856322827
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.11365]
objective value function right now is: -2288.4488476012925
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.02444]
objective value function right now is: -2288.4653674002843
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -2289.552919313515
Current xi:  [334.21274]
objective value function right now is: -2289.552919313515
94.0% of gradient descent iterations done. Method = Adam
new min fval:  -2289.697967945946
Current xi:  [334.60065]
objective value function right now is: -2289.697967945946
new min fval from sgd:  -2289.7383268930726
new min fval from sgd:  -2289.845050989101
new min fval from sgd:  -2289.9282142848997
new min fval from sgd:  -2289.9843648943506
new min fval from sgd:  -2290.035672504914
new min fval from sgd:  -2290.079479365061
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.53232]
objective value function right now is: -2287.395822810047
new min fval from sgd:  -2290.087120632991
new min fval from sgd:  -2290.1091465504
new min fval from sgd:  -2290.1120146080375
new min fval from sgd:  -2290.1182086901813
new min fval from sgd:  -2290.1261115098478
new min fval from sgd:  -2290.131178733424
new min fval from sgd:  -2290.138740808353
new min fval from sgd:  -2290.144051721484
new min fval from sgd:  -2290.1459050811
new min fval from sgd:  -2290.147329166552
new min fval from sgd:  -2290.1476274412266
new min fval from sgd:  -2290.1492153541362
new min fval from sgd:  -2290.15511150687
new min fval from sgd:  -2290.158414015737
new min fval from sgd:  -2290.162849910807
new min fval from sgd:  -2290.167720971443
new min fval from sgd:  -2290.1752483948826
new min fval from sgd:  -2290.184145181878
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.43723]
objective value function right now is: -2289.976096104989
new min fval from sgd:  -2290.209236880322
new min fval from sgd:  -2290.2456883044583
new min fval from sgd:  -2290.2730519829115
new min fval from sgd:  -2290.2956297288165
new min fval from sgd:  -2290.314127247548
new min fval from sgd:  -2290.3291631274756
new min fval from sgd:  -2290.3411071265227
new min fval from sgd:  -2290.351686244444
new min fval from sgd:  -2290.3556601289893
new min fval from sgd:  -2290.358711765034
new min fval from sgd:  -2290.3602617461524
new min fval from sgd:  -2290.3651014125903
new min fval from sgd:  -2290.365547011118
new min fval from sgd:  -2290.3708409203036
new min fval from sgd:  -2290.3735900968713
new min fval from sgd:  -2290.376712417941
new min fval from sgd:  -2290.3780248608796
new min fval from sgd:  -2290.382836809998
new min fval from sgd:  -2290.3893057138175
new min fval from sgd:  -2290.396454180246
new min fval from sgd:  -2290.403043076289
new min fval from sgd:  -2290.4077361699447
new min fval from sgd:  -2290.4117462223016
new min fval from sgd:  -2290.4141786032383
new min fval from sgd:  -2290.422044649285
new min fval from sgd:  -2290.4443813931
new min fval from sgd:  -2290.4497153566535
new min fval from sgd:  -2290.452142118574
new min fval from sgd:  -2290.457166172703
new min fval from sgd:  -2290.463526592606
new min fval from sgd:  -2290.469574883207
new min fval from sgd:  -2290.4763563924894
new min fval from sgd:  -2290.479923419369
new min fval from sgd:  -2290.482598390487
new min fval from sgd:  -2290.487319372853
new min fval from sgd:  -2290.4885527936613
new min fval from sgd:  -2290.489369411123
new min fval from sgd:  -2290.492845475386
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [334.44376]
objective value function right now is: -2290.169819438683
min fval:  -2290.492845475386
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -4.3081,  16.3076],
        [-12.0976,  12.3699],
        [ -0.6551,   0.4772],
        [ -0.6551,   0.4772],
        [  1.4976,  -0.3099],
        [  1.4658,  -0.2981],
        [ -0.6551,   0.4772],
        [ -0.6551,   0.4772],
        [  1.4568,  -0.3050],
        [ 15.0298,  -1.1735],
        [-76.4092,  -1.5620],
        [  1.4645,  -0.3138],
        [ -4.8275,  19.6390]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  8.3468,   7.4470,  -3.0584,  -3.0584,   5.0359,   5.1430,  -3.0584,
         -3.0584,   5.0394, -11.3106,  -0.6100,   5.0999,  10.1041],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.7812e-01, -2.7570e-01,  9.0761e-03,  9.0761e-03, -4.9961e-01,
         -4.9993e-01,  9.0761e-03,  9.0761e-03, -4.9961e-01, -3.0796e-02,
         -1.6366e-01, -4.9982e-01, -1.8337e-01],
        [-1.7812e-01, -2.7570e-01,  9.0762e-03,  9.0762e-03, -4.9961e-01,
         -4.9993e-01,  9.0762e-03,  9.0762e-03, -4.9960e-01, -3.0797e-02,
         -1.6366e-01, -4.9982e-01, -1.8337e-01],
        [ 4.2490e-01,  1.2333e+00,  3.3945e-02,  3.3945e-02,  1.0968e+00,
          1.0972e+00,  3.3945e-02,  3.3945e-02,  1.0969e+00,  5.8719e-02,
          5.6895e-01,  1.0971e+00,  4.5449e-01],
        [-1.0583e+01, -9.9914e+00,  2.2093e-02,  2.2093e-02,  3.3942e+00,
          3.6883e+00,  2.2093e-02,  2.2093e-02,  3.4415e+00,  2.6580e+01,
          8.3211e+00,  3.7095e+00, -1.4899e+01],
        [-7.5588e+00, -8.2052e+00,  2.0113e-02,  2.0112e-02,  1.7455e+00,
          2.0447e+00,  2.0113e-02,  2.0112e-02,  1.8260e+00,  2.6370e+01,
          7.6703e+00,  1.7596e+00, -1.4130e+01],
        [-1.7812e-01, -2.7570e-01,  9.0761e-03,  9.0761e-03, -4.9961e-01,
         -4.9993e-01,  9.0761e-03,  9.0761e-03, -4.9960e-01, -3.0797e-02,
         -1.6366e-01, -4.9982e-01, -1.8337e-01],
        [-1.7812e-01, -2.7570e-01,  9.0761e-03,  9.0761e-03, -4.9961e-01,
         -4.9993e-01,  9.0761e-03,  9.0761e-03, -4.9960e-01, -3.0796e-02,
         -1.6366e-01, -4.9982e-01, -1.8337e-01],
        [ 4.1804e-01,  1.2149e+00,  3.3717e-02,  3.3717e-02,  1.0800e+00,
          1.0804e+00,  3.3717e-02,  3.3717e-02,  1.0800e+00,  5.5365e-02,
          5.5689e-01,  1.0803e+00,  4.4660e-01],
        [-7.9064e+00, -8.4044e+00, -1.4086e-02, -1.4085e-02,  2.0614e+00,
          2.1048e+00, -1.4086e-02, -1.4085e-02,  1.6193e+00,  2.6963e+01,
          7.6779e+00,  1.8225e+00, -1.4362e+01],
        [-1.7812e-01, -2.7570e-01,  9.0761e-03,  9.0761e-03, -4.9961e-01,
         -4.9993e-01,  9.0761e-03,  9.0761e-03, -4.9961e-01, -3.0796e-02,
         -1.6366e-01, -4.9982e-01, -1.8337e-01],
        [-1.7812e-01, -2.7570e-01,  9.0761e-03,  9.0761e-03, -4.9961e-01,
         -4.9993e-01,  9.0761e-03,  9.0761e-03, -4.9961e-01, -3.0796e-02,
         -1.6366e-01, -4.9982e-01, -1.8337e-01],
        [-1.7811e-01, -2.7568e-01,  9.0781e-03,  9.0781e-03, -4.9953e-01,
         -4.9986e-01,  9.0781e-03,  9.0781e-03, -4.9953e-01, -3.0802e-02,
         -1.6365e-01, -4.9975e-01, -1.8335e-01],
        [ 4.3451e-01,  1.2598e+00,  3.4266e-02,  3.4266e-02,  1.1205e+00,
          1.1209e+00,  3.4266e-02,  3.4266e-02,  1.1205e+00,  6.3945e-02,
          5.8643e-01,  1.1208e+00,  4.6565e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5030, -0.5030,  1.1005,  3.0624,  1.6099, -0.5030, -0.5030,  1.0837,
         1.7827, -0.5030, -0.5030, -0.5029,  1.1242], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0249,   0.0249,   3.8912, -11.4222, -12.5004,   0.0249,   0.0249,
           3.4392, -13.0598,   0.0249,   0.0249,   0.0249,   4.6370]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  1.3331,  13.7067],
        [  5.7929,  16.3699],
        [  7.6464,  13.9628],
        [ -1.7501,   0.3424],
        [ 15.2595,   0.5055],
        [ -1.7421,   0.3596],
        [  9.2892, -10.9051],
        [-14.9466,  12.6694],
        [ -3.1174,   0.9304],
        [-11.4693,  -7.3247],
        [ 17.8114,   7.4654],
        [ -4.3621,   9.3785],
        [-11.5666, -13.7911]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.6618,  -7.1785,   2.2917,  -4.7687, -13.5019,  -4.7654,  -9.0185,
          7.8486,  -5.7473,  -4.8740,  -1.7413,  -3.8965,  -7.8849],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.8697e+00,  4.7584e+00,  1.8709e+00,  1.7092e-01, -2.2513e-01,
          2.0430e-01,  1.0305e+01, -4.0848e+00, -6.4064e-01, -3.3608e+00,
          7.0354e-01, -1.2948e+00,  4.3282e+00],
        [-2.6682e+00, -2.8541e+00, -1.8665e+00,  1.6917e-02, -6.1753e+00,
          3.7623e-02, -4.3045e+00,  9.9511e+00, -2.2988e-01,  5.2709e+00,
         -1.0118e+00, -1.3680e+00, -6.0335e+00],
        [-3.3479e-01, -6.0086e+00, -1.4234e+00, -2.2557e-01, -2.2960e+01,
         -2.1026e-01, -1.5326e+00,  1.1538e+01, -2.9883e-02,  3.2248e+00,
         -1.7790e+00,  6.8557e+00, -1.5156e+01],
        [-6.1487e-01,  3.0013e+00,  2.1382e+00, -1.7927e-02,  3.8293e-01,
         -2.5213e-02,  1.5661e-01, -1.1482e+00, -1.3282e+00,  1.3030e+00,
         -2.5394e+00, -1.2191e+00,  1.0867e+00],
        [-1.0528e-01,  3.5894e-01,  1.3135e+00,  1.4266e-01, -9.6725e-01,
          1.3830e-01, -1.2157e+00, -1.6861e+00,  5.7429e-03,  4.7973e-02,
         -2.3895e+00, -1.7837e-01,  4.3259e-01],
        [-5.5304e-01,  2.5264e+00,  1.8992e+00,  3.5774e-01, -4.3556e-01,
          3.5673e-01,  8.2809e-01, -1.3666e+00, -4.4855e-01,  9.7081e-01,
         -3.5285e+00, -9.3624e-01,  1.1402e+00],
        [ 1.1654e+01, -3.0809e+00,  4.1188e-01, -2.0982e-01, -1.9445e+01,
         -1.9198e-01, -9.5057e+00,  9.5211e+00, -3.3425e-01,  3.0796e+00,
          2.6576e+00,  2.4260e+00, -1.3857e+01],
        [ 7.5441e+00,  9.7829e-02, -1.1725e+01, -1.6888e-01, -5.8574e+00,
         -1.5904e-01, -2.0643e+00, -1.4585e+00, -6.7261e-02, -5.7342e-01,
         -4.5582e-01,  1.6433e-02, -1.5743e+01],
        [-6.2362e-02,  7.8631e-01,  1.3671e+00,  2.1353e-01, -9.6232e-01,
          2.0801e-01, -6.7665e-01, -1.5469e+00, -4.6641e-04,  7.1468e-02,
         -2.9378e+00, -9.9083e-02,  3.2616e-01],
        [-1.1499e+00,  9.4009e-01,  1.2041e+00,  2.5385e-04,  1.8239e-01,
          5.4956e-04, -2.2085e+00, -1.6809e+00,  1.4034e-02,  1.1370e-02,
         -2.1094e+00, -9.8403e-01,  1.7398e-01],
        [-2.7277e+01,  1.4012e-04, -1.1567e+00, -1.3179e-01, -1.0720e+01,
         -1.3066e-01,  1.4000e+00,  6.5228e+00, -5.2777e-02,  7.5863e+00,
         -8.0256e+00,  1.1077e-02,  9.4831e+00],
        [-9.1731e+00,  1.9764e-01,  4.0194e+00, -2.5190e-01, -1.7811e+00,
         -2.5021e-01,  2.1984e+00, -1.1201e+01, -2.1792e-01,  1.0720e+01,
         -3.7466e+00,  7.9528e-03,  7.8504e+00],
        [-1.5914e+01, -7.0953e-05, -1.4676e+00, -2.2151e-01, -1.4227e+01,
         -2.0249e-01,  2.9947e-01,  5.0448e+00, -2.9670e-02,  5.1463e+00,
         -1.0477e+01, -1.4103e-02,  7.0243e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 5.7857, -3.5978,  2.2978, -3.6800, -3.5972, -3.6046, -7.6964, -4.0804,
        -3.5208, -4.2376, -0.9280,  1.8440, -1.8468], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 8.5810e-01,  5.6485e+00,  1.8086e+00,  6.5768e-01,  1.3428e+00,
          1.7441e+00,  2.0514e-01, -7.9791e-01,  1.5968e+00,  6.7531e-01,
         -3.5858e+00, -2.0052e-01, -4.0601e+00],
        [ 4.5122e-01, -5.5961e+00, -3.3054e-01, -6.5583e-01, -1.3412e+00,
         -1.7429e+00,  9.2967e-01,  6.5667e-01, -1.5952e+00, -6.7375e-01,
          4.4186e+00,  7.4145e-01,  4.3374e+00],
        [-1.0601e+01, -1.5918e-02, -9.6650e-01, -1.0478e-01, -2.5756e-02,
         -4.5603e-02, -4.2946e-01, -1.5617e-01, -2.8464e-02, -1.9070e-02,
         -1.3525e-02, -1.1583e+00,  5.9362e-05],
        [-1.7084e+00, -9.2421e+00, -3.5784e+00,  3.7740e+00,  5.9374e-01,
          2.8346e+00,  1.1043e+00, -1.8354e+01,  9.9691e-01,  1.2092e+00,
          1.2301e+00,  1.9704e+00,  9.9932e+00],
        [ 9.4367e+00, -1.8141e+00, -1.0957e+01,  4.3270e-01, -5.1879e-01,
         -1.1450e-01,  3.4657e+00,  1.3973e+00, -5.7536e-01,  1.0132e-01,
          2.4434e+00,  8.2855e+00,  1.6475e+00]], device='cuda:0'))])
xi:  [334.44583]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 418.5016122418132
W_T_median: 382.7247941059147
W_T_pctile_5: 334.46008937081143
W_T_CVAR_5_pct: 167.67057135805646
Average q (qsum/M+1):  46.84321840347782
Optimal xi:  [334.44583]
Expected(across Rb) median(across samples) p_equity:  0.19349679594076102
obj fun:  tensor(-2290.4928, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -2.7095,  12.9892],
        [ -8.6338,   9.4281],
        [ -0.9451,   0.4527],
        [ -0.8640,   0.3781],
        [  6.1895,  -0.6523],
        [  4.5754,  -0.4303],
        [ -0.9502,   0.4519],
        [ -0.9882,   0.4554],
        [  5.3775,  -0.2578],
        [  9.7529,  -0.6215],
        [-74.6407,   0.8966],
        [  5.4828,  -0.5841],
        [ -2.7219,  16.4087]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 7.2031,  5.6080, -3.1690, -3.3192,  2.5904,  3.5518, -3.1733, -3.1998,
         3.1859, -7.8767,  0.2375,  2.9803,  8.6844], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1333e-01,  1.6554e+00,  3.4528e-02,  3.0769e-02,  8.7976e-01,
          9.1042e-01,  3.5304e-02,  3.4772e-02,  8.9665e-01,  2.6447e-01,
          4.3032e-01,  8.9469e-01,  4.6686e-01],
        [-9.2749e+00, -6.0010e+00,  1.6064e-01,  2.4939e-01,  2.6538e+00,
          2.9059e+00,  1.6532e-01,  2.3862e-01,  2.6770e+00,  2.0542e+01,
          5.6439e+00,  2.9504e+00, -1.0311e+01],
        [-5.8631e+00, -5.5932e+00, -1.0882e-01, -2.5602e-01,  1.4267e+00,
          1.6964e+00, -8.6213e-02, -1.9126e-01,  1.4872e+00,  2.0896e+01,
          5.2545e+00,  1.4174e+00, -9.1947e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4955e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.0882e-01,  1.6256e+00,  3.4094e-02,  3.0485e-02,  8.6370e-01,
          8.9398e-01,  3.4826e-02,  3.4369e-02,  8.8039e-01,  2.5781e-01,
          4.1380e-01,  8.7846e-01,  4.6006e-01],
        [-6.2435e+00, -5.0774e+00, -7.9019e-02, -1.1667e-02,  1.6100e+00,
          1.6134e+00, -1.1318e-01, -9.1158e-02,  1.1232e+00,  2.1826e+01,
          5.7213e+00,  1.3367e+00, -9.6095e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4956e-03, -5.9415e-03, -4.7716e-01,
         -4.9046e-01, -6.4683e-03, -6.3263e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1923e-01,  1.6977e+00,  3.5087e-02,  3.1098e-02,  9.0167e-01,
          9.3283e-01,  3.5931e-02,  3.5276e-02,  9.1883e-01,  2.7424e-01,
          4.5388e-01,  9.1684e-01,  4.7618e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5021, -0.5021,  0.9316,  2.2107,  1.1816, -0.5021, -0.5021,  0.9149,
         1.2202, -0.5021, -0.5021, -0.5021,  0.9544], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0192,   0.0192,   3.5844,  -9.2131, -12.7066,   0.0192,   0.0192,
           3.1214, -12.3800,   0.0192,   0.0192,   0.0192,   4.3492]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  2.3638,  12.1836],
        [  5.6572,   7.4721],
        [  7.9224,   6.5803],
        [ -6.1235,   3.1910],
        [ 11.8466,   0.5913],
        [  0.8770,  -8.1352],
        [  8.6891,  -6.8360],
        [-10.5691,  10.8756],
        [  1.7568,  -0.0712],
        [-11.0991,  -4.2491],
        [ 11.4103,   5.2357],
        [ -0.1851,   2.9347],
        [ -7.5723,  -9.3505]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 6.3522, -5.1507, -2.0487, -4.3376, -9.8811, -6.7882, -6.3489,  5.9660,
        -7.1033, -1.4560, -0.7179, -3.6786, -6.0576], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.1852e+00,  1.5094e+00,  1.3526e+00, -5.4268e-01,  2.4223e+00,
          2.0759e+00,  9.2465e-01, -1.6151e+00, -2.8824e-01,  9.5287e-01,
          2.4642e-01, -1.1737e+00,  2.2713e+00],
        [-1.8651e+00, -5.6256e-01, -6.2129e-01, -4.2742e+00, -7.7059e-01,
         -3.4892e+00, -7.6918e-02,  6.7408e+00, -2.3096e-01,  1.3500e+00,
         -3.5500e+00, -6.0667e-01, -2.9353e+00],
        [ 1.1142e-01, -1.2473e+00,  8.1916e-01,  6.4910e-01, -1.6350e+01,
          2.2732e+00, -1.5779e+00,  7.2357e+00, -7.2982e-03,  4.4049e+00,
         -5.8977e-01,  1.7865e+00, -4.6946e+00],
        [-1.3960e+00, -8.7673e-02, -4.2780e-01,  6.7083e-03, -4.0893e-01,
         -9.1685e-01, -1.7927e+00, -5.0702e-01, -1.3463e-02, -5.2559e-01,
         -1.6941e+00,  1.8378e-03, -3.4962e-01],
        [-1.9520e+00,  1.8967e-01, -5.4411e-01, -1.2101e-01, -9.1077e-01,
         -1.2479e+00, -1.1267e+00, -9.1391e-01, -1.8068e-02, -3.8365e-01,
         -2.4970e+00, -1.1678e-01, -2.1655e-01],
        [-1.4057e+00, -8.6477e-02, -4.3293e-01,  1.5699e-02, -4.1127e-01,
         -9.1630e-01, -1.7894e+00, -5.0243e-01, -1.3300e-02, -5.1448e-01,
         -1.7010e+00,  1.2009e-02, -3.4096e-01],
        [ 1.3221e+01, -6.1845e-01,  1.5793e+00, -6.7508e+00, -1.5120e+01,
         -1.8714e+01, -8.9013e+00,  4.8410e+00, -1.2180e+00,  4.2963e+00,
          1.1099e+00,  1.4776e+00, -1.3629e+01],
        [ 4.1482e+00, -4.3534e-01, -7.3632e+00,  2.5918e-02, -1.7989e-01,
         -6.4858e+00, -8.9001e-01, -1.0932e+00,  1.0207e-01,  1.5231e+00,
         -7.6390e+00,  1.3168e-01, -1.0356e+01],
        [-1.4524e+00, -8.1858e-02, -4.5182e-01,  9.7058e-02, -4.0160e-01,
         -8.8229e-01, -1.8020e+00, -4.5301e-01, -1.2095e-02, -4.6125e-01,
         -1.7020e+00,  9.5630e-02, -3.0579e-01],
        [-2.8522e+00, -1.9497e-01, -1.6674e+00, -4.5101e-01, -1.9345e+00,
         -2.7298e+00,  5.5568e-01,  1.5671e+00, -1.2880e-02,  9.4602e-01,
         -2.4953e+00, -2.9629e-01, -2.6501e+00],
        [-2.4334e+01, -6.7212e-02, -4.8855e+00,  1.9330e-01, -7.7025e+00,
          2.7805e+00,  4.4028e-01, -1.4406e+00, -4.7447e-02,  6.3915e+00,
         -8.7817e+00, -4.8862e-01,  5.4980e+00],
        [-1.0961e+01,  5.4093e-01,  8.6739e-01, -6.0412e-02, -3.7470e+00,
          2.1004e+00,  2.4607e+00, -2.1072e+01, -1.4633e-01,  9.1990e+00,
         -5.3680e+00,  1.5515e-01,  4.1397e+00],
        [-1.2571e+01,  2.5529e-02, -4.2095e+00,  2.0473e-01, -1.3435e+01,
          2.3688e+00, -6.0068e-01, -5.7139e+00, -3.7530e-01,  4.5355e+00,
         -9.0221e+00,  3.5582e-02,  2.3988e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 2.9079, -2.1828, -0.1738, -3.3916, -2.9699, -3.3962, -3.0437, -0.0264,
        -3.4166, -2.8315, -0.3684,  1.2132,  0.0742], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 7.4891e-01,  4.5452e+00,  1.8769e+00,  4.5914e-02,  3.5496e-01,
         -9.1354e-02,  2.7526e-01, -9.7636e-01, -1.0948e-01,  3.3561e+00,
         -4.4478e+00, -1.2550e-01, -1.0160e+00],
        [ 5.4131e-01, -4.5533e+00, -3.9331e-01,  2.2764e-01, -3.5322e-01,
          9.4308e-02,  8.6633e-01,  7.9627e-01,  1.1223e-01, -3.0675e+00,
          5.4099e+00,  6.6530e-01,  1.5354e+00],
        [-1.0256e+01, -4.0894e-02, -1.1152e+00, -1.1287e-02, -1.0215e-02,
         -1.1185e-02, -8.9347e-01, -1.4354e-01, -1.1272e-02, -1.5607e-02,
          8.3038e-03, -6.6567e-01,  1.3450e-03],
        [-1.2086e+00, -1.8795e+00, -3.9948e+00,  3.7431e-01,  1.3870e+00,
          3.8828e-01,  9.5588e-01, -1.2326e+01,  4.2495e-01,  2.2745e+00,
          8.1236e-01,  1.8802e+00,  1.0604e+01],
        [ 9.1296e+00, -4.1802e+00, -1.1657e+01,  2.0360e-01,  4.0986e-01,
          2.0817e-01,  5.1155e+00, -6.9774e-01,  2.1607e-01, -5.1843e-01,
          6.2533e+00,  8.3596e+00,  2.6055e+00]], device='cuda:0'))])
loaded xi:  315.55313
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -3122.7997902664715
Current xi:  [322.96973]
objective value function right now is: -3122.7997902664715
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [330.14026]
objective value function right now is: -3094.24177527958
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [335.9174]
objective value function right now is: -3114.9430465632686
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [339.1002]
objective value function right now is: -3109.915338974971
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -3130.42244483862
Current xi:  [341.4037]
objective value function right now is: -3130.42244483862
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [343.6869]
objective value function right now is: -3125.6421026835983
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [344.74948]
objective value function right now is: -3102.1858863643397
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -3133.850110555717
Current xi:  [346.09164]
objective value function right now is: -3133.850110555717
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.70108]
objective value function right now is: -3119.73024851812
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -3134.4064284222213
Current xi:  [346.17288]
objective value function right now is: -3134.4064284222213
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.5281]
objective value function right now is: -3102.7757044431414
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.6768]
objective value function right now is: -3127.3417561700185
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -3134.794104220516
Current xi:  [344.96298]
objective value function right now is: -3134.794104220516
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [344.54895]
objective value function right now is: -3121.1235688870174
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.3161]
objective value function right now is: -3116.8037012081472
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [344.86624]
objective value function right now is: -3097.309719496133
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -3138.415905585396
Current xi:  [344.59988]
objective value function right now is: -3138.415905585396
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [344.84445]
objective value function right now is: -3126.55595564394
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [344.84177]
objective value function right now is: -3124.2840485836577
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.06073]
objective value function right now is: -3134.115660605109
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [347.21524]
objective value function right now is: -3131.6511231663053
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [346.14932]
objective value function right now is: -3124.178173439897
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.09256]
objective value function right now is: -3136.536524533772
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.23495]
objective value function right now is: -3137.5440259317893
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [344.3348]
objective value function right now is: -3133.703419686487
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.32358]
objective value function right now is: -3137.294022318745
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.7391]
objective value function right now is: -3132.9936127728024
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [345.7099]
objective value function right now is: -3126.106658061986
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [346.5232]
objective value function right now is: -3133.386986455755
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -3139.036909457733
Current xi:  [347.80573]
objective value function right now is: -3139.036909457733
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [346.53375]
objective value function right now is: -3134.685976437498
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [346.05447]
objective value function right now is: -3130.283111433246
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -3140.1185119323254
Current xi:  [345.30862]
objective value function right now is: -3140.1185119323254
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.39508]
objective value function right now is: -3115.8078922510203
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.24844]
objective value function right now is: -3129.5481133128683
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -3143.978167677705
Current xi:  [345.45914]
objective value function right now is: -3143.978167677705
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -3147.7825514573806
Current xi:  [345.88553]
objective value function right now is: -3147.7825514573806
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.78232]
objective value function right now is: -3147.3974953804695
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -3148.276358193664
Current xi:  [346.04385]
objective value function right now is: -3148.276358193664
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.80176]
objective value function right now is: -3146.8714304794344
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.8694]
objective value function right now is: -3147.2694846401623
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [346.07687]
objective value function right now is: -3145.5156592687817
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.589]
objective value function right now is: -3145.542478674995
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.56458]
objective value function right now is: -3147.762829578128
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.56512]
objective value function right now is: -3148.080963874695
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -3148.4844089933076
Current xi:  [345.3993]
objective value function right now is: -3148.4844089933076
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.3214]
objective value function right now is: -3147.6096210959618
new min fval from sgd:  -3148.502280094211
new min fval from sgd:  -3148.6609491365925
new min fval from sgd:  -3148.732198024437
new min fval from sgd:  -3148.778981394073
new min fval from sgd:  -3148.8934989669146
new min fval from sgd:  -3148.9838859583538
new min fval from sgd:  -3149.010564689224
new min fval from sgd:  -3149.159671206206
new min fval from sgd:  -3149.3347369211037
new min fval from sgd:  -3149.495323730873
new min fval from sgd:  -3149.586435186015
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.40524]
objective value function right now is: -3148.225410477015
new min fval from sgd:  -3149.611235519808
new min fval from sgd:  -3149.6423024961837
new min fval from sgd:  -3149.6724193960263
new min fval from sgd:  -3149.7173103964096
new min fval from sgd:  -3149.7552692155446
new min fval from sgd:  -3149.7921000461106
new min fval from sgd:  -3149.83205600106
new min fval from sgd:  -3149.8847917306275
new min fval from sgd:  -3149.925117048244
new min fval from sgd:  -3149.956627580904
new min fval from sgd:  -3149.980096895436
new min fval from sgd:  -3149.9957588245243
new min fval from sgd:  -3150.0096058288896
new min fval from sgd:  -3150.0243590733635
new min fval from sgd:  -3150.045155132206
new min fval from sgd:  -3150.0596463912475
new min fval from sgd:  -3150.078555197836
new min fval from sgd:  -3150.0964872273253
new min fval from sgd:  -3150.112610855185
new min fval from sgd:  -3150.1188852726223
new min fval from sgd:  -3150.126693375931
new min fval from sgd:  -3150.131923117109
new min fval from sgd:  -3150.1426002741673
new min fval from sgd:  -3150.151775157015
new min fval from sgd:  -3150.1545507227147
new min fval from sgd:  -3150.164408596129
new min fval from sgd:  -3150.166802866556
new min fval from sgd:  -3150.1720174715174
new min fval from sgd:  -3150.1812948899333
new min fval from sgd:  -3150.1888594631114
new min fval from sgd:  -3150.2005416388215
new min fval from sgd:  -3150.2100958766364
new min fval from sgd:  -3150.2274652338856
new min fval from sgd:  -3150.241408094331
new min fval from sgd:  -3150.2518936824013
new min fval from sgd:  -3150.2672399495004
new min fval from sgd:  -3150.2789963164446
new min fval from sgd:  -3150.290177504634
new min fval from sgd:  -3150.305332869051
new min fval from sgd:  -3150.3216450076056
new min fval from sgd:  -3150.338876246107
new min fval from sgd:  -3150.3603786880326
new min fval from sgd:  -3150.381021638282
new min fval from sgd:  -3150.399410761852
new min fval from sgd:  -3150.4152290507423
new min fval from sgd:  -3150.427552401228
new min fval from sgd:  -3150.4401740724697
new min fval from sgd:  -3150.448660725795
new min fval from sgd:  -3150.4549345204505
new min fval from sgd:  -3150.465587082183
new min fval from sgd:  -3150.47301059318
new min fval from sgd:  -3150.4742880024733
new min fval from sgd:  -3150.4754158657806
new min fval from sgd:  -3150.4763684897753
new min fval from sgd:  -3150.4830315570375
new min fval from sgd:  -3150.4847539440307
new min fval from sgd:  -3150.486510094902
new min fval from sgd:  -3150.488707426055
new min fval from sgd:  -3150.495087028474
new min fval from sgd:  -3150.511186905751
new min fval from sgd:  -3150.519240423893
new min fval from sgd:  -3150.5310231512995
new min fval from sgd:  -3150.5394050498458
new min fval from sgd:  -3150.5507642143443
new min fval from sgd:  -3150.5637575500086
new min fval from sgd:  -3150.576994051177
new min fval from sgd:  -3150.5846689269542
new min fval from sgd:  -3150.5941894721855
new min fval from sgd:  -3150.606384147512
new min fval from sgd:  -3150.620296803046
new min fval from sgd:  -3150.6302346458615
new min fval from sgd:  -3150.6455302596924
new min fval from sgd:  -3150.656611719454
new min fval from sgd:  -3150.669878686859
new min fval from sgd:  -3150.6804958801586
new min fval from sgd:  -3150.690356499333
new min fval from sgd:  -3150.698254950648
new min fval from sgd:  -3150.7064307117753
new min fval from sgd:  -3150.715296112501
new min fval from sgd:  -3150.7216587221515
new min fval from sgd:  -3150.728436349218
new min fval from sgd:  -3150.733443347468
new min fval from sgd:  -3150.7353723106826
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.3436]
objective value function right now is: -3150.2156248568876
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.3725]
objective value function right now is: -3150.445503520275
min fval:  -3150.7353723106826
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -5.9123,  16.6383],
        [-11.7143,  11.4535],
        [ -0.8727,   0.7280],
        [ -0.8727,   0.7280],
        [  1.3804,  -0.4714],
        [  1.3734,  -0.4661],
        [ -0.8727,   0.7280],
        [ -0.8727,   0.7280],
        [  1.3514,  -0.4735],
        [ 14.7532,  -1.4614],
        [-81.2479,  -1.7180],
        [  1.3611,  -0.4916],
        [ -7.1478,  19.8039]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  8.4769,   6.5685,  -3.8094,  -3.8094,   5.2978,   5.4074,  -3.8094,
         -3.8094,   5.3042, -10.5720,  -0.3963,   5.3478,  10.1535],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.0986e-01, -4.4749e-01,  8.9676e-04,  8.9676e-04, -8.2300e-01,
         -8.2328e-01,  8.9676e-04,  8.9676e-04, -8.2302e-01, -5.2856e-02,
         -2.4854e-01, -8.2316e-01, -4.1818e-01],
        [-4.0986e-01, -4.4749e-01,  8.9676e-04,  8.9676e-04, -8.2300e-01,
         -8.2328e-01,  8.9676e-04,  8.9676e-04, -8.2302e-01, -5.2856e-02,
         -2.4854e-01, -8.2316e-01, -4.1818e-01],
        [ 5.4634e-01,  1.0484e+00,  1.3423e-02,  1.3423e-02,  1.2489e+00,
          1.2492e+00,  1.3423e-02,  1.3423e-02,  1.2489e+00,  5.9258e-02,
          4.8742e-01,  1.2490e+00,  5.8852e-01],
        [-1.1455e+01, -7.9769e+00,  6.6020e-02,  6.6020e-02,  3.3376e+00,
          3.6655e+00,  6.6020e-02,  6.6020e-02,  3.4137e+00,  2.7847e+01,
          7.7721e+00,  3.6507e+00, -1.6080e+01],
        [-8.4185e+00, -5.8061e+00,  8.2704e-02,  8.2703e-02,  1.5767e+00,
          1.9310e+00,  8.2704e-02,  8.2704e-02,  1.7073e+00,  2.7184e+01,
          7.1691e+00,  1.5687e+00, -1.4876e+01],
        [-4.0986e-01, -4.4749e-01,  8.9675e-04,  8.9675e-04, -8.2300e-01,
         -8.2329e-01,  8.9675e-04,  8.9675e-04, -8.2302e-01, -5.2856e-02,
         -2.4854e-01, -8.2316e-01, -4.1818e-01],
        [-4.0986e-01, -4.4749e-01,  8.9675e-04,  8.9675e-04, -8.2300e-01,
         -8.2329e-01,  8.9675e-04,  8.9675e-04, -8.2302e-01, -5.2856e-02,
         -2.4854e-01, -8.2316e-01, -4.1818e-01],
        [ 5.3822e-01,  1.0346e+00,  1.3318e-02,  1.3318e-02,  1.2291e+00,
          1.2294e+00,  1.3318e-02,  1.3318e-02,  1.2291e+00,  5.8129e-02,
          4.7987e-01,  1.2292e+00,  5.7979e-01],
        [-8.7377e+00, -5.8816e+00, -3.3670e-02, -3.3669e-02,  1.8814e+00,
          1.9855e+00, -3.3670e-02, -3.3670e-02,  1.4902e+00,  2.7818e+01,
          7.1252e+00,  1.6206e+00, -1.5156e+01],
        [-4.0986e-01, -4.4749e-01,  8.9675e-04,  8.9675e-04, -8.2300e-01,
         -8.2329e-01,  8.9675e-04,  8.9675e-04, -8.2302e-01, -5.2856e-02,
         -2.4854e-01, -8.2316e-01, -4.1818e-01],
        [-4.0986e-01, -4.4749e-01,  8.9675e-04,  8.9675e-04, -8.2300e-01,
         -8.2329e-01,  8.9675e-04,  8.9675e-04, -8.2302e-01, -5.2856e-02,
         -2.4854e-01, -8.2316e-01, -4.1818e-01],
        [-4.0986e-01, -4.4749e-01,  8.9676e-04,  8.9676e-04, -8.2300e-01,
         -8.2329e-01,  8.9676e-04,  8.9676e-04, -8.2302e-01, -5.2856e-02,
         -2.4854e-01, -8.2316e-01, -4.1818e-01],
        [ 5.5761e-01,  1.0679e+00,  1.3606e-02,  1.3606e-02,  1.2766e+00,
          1.2769e+00,  1.3606e-02,  1.3606e-02,  1.2766e+00,  6.0862e-02,
          4.9814e-01,  1.2768e+00,  6.0068e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8258, -0.8258,  1.2516,  3.0525,  1.5295, -0.8258, -0.8258,  1.2318,
         1.7021, -0.8258, -0.8258, -0.8258,  1.2794], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0149,  -0.0149,   3.7454, -11.0345, -12.9318,  -0.0149,  -0.0149,
           3.2910, -13.4092,  -0.0149,  -0.0149,  -0.0149,   4.4953]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 1.7705e-02,  1.3863e+01],
        [ 8.2729e+00,  1.9008e+01],
        [ 9.4414e+00,  1.8406e+01],
        [-2.1159e+00,  3.2424e+00],
        [ 1.4908e+01,  3.5943e-01],
        [-1.9879e+00,  7.2265e-01],
        [ 9.5522e+00, -5.1274e+00],
        [-1.1277e+01,  1.4368e+01],
        [-1.6309e+00,  3.5285e+00],
        [-1.4020e+01, -8.6845e+00],
        [ 1.5833e+01,  6.9033e+00],
        [-1.2748e+00,  3.6543e+00],
        [-1.0670e+01, -1.3453e+01]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.8971,  -3.2772,   1.5923,  -5.0586, -13.6936,  -5.4537, -11.3648,
          8.3420,  -4.5574,  -4.7164,  -1.4242,  -4.6680,  -7.6732],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.3271e+00,  5.5556e+00,  3.6491e-01, -9.4425e-01,  6.5527e-01,
         -1.3218e-01, -5.1007e-01, -8.7396e+00, -1.1692e+00, -1.2925e+00,
          2.6332e+00, -1.1848e+00,  5.1252e+00],
        [-1.7995e+00, -4.1490e+00, -6.2814e-02, -2.4720e+00,  2.2021e+00,
         -2.8700e-01, -4.4984e+00,  6.7760e+00, -2.3940e+00,  4.9265e+00,
         -2.5974e+00, -2.4588e+00, -5.7768e+00],
        [ 4.3617e+00, -3.1329e+00, -1.2038e+01, -1.3119e-01, -3.2162e+01,
          1.2852e-01,  6.4663e-01,  1.4254e+01,  5.7892e-01,  2.3876e+00,
         -8.7064e-01,  7.4379e-01, -1.6777e+01],
        [-1.2461e+00,  5.1225e-02,  9.0212e-02, -7.0498e-03, -1.5390e-01,
          9.3264e-03, -7.1248e-01, -2.3349e-01, -9.7453e-03, -3.7873e-01,
         -2.4017e+00, -1.0513e-02, -7.0476e-01],
        [-1.5619e+00,  2.7015e-01,  4.5356e-01, -2.4981e-01, -5.8629e-01,
         -1.5931e-02, -9.9489e-01, -6.6069e-01, -2.4642e-01, -8.5223e-01,
         -2.4059e+00, -2.4355e-01, -9.0622e-01],
        [-1.2042e+00,  6.8382e-02,  8.0102e-02, -3.0226e-02, -1.4124e-01,
          9.9110e-03, -6.7560e-01, -1.9435e-01, -3.2614e-02, -3.3836e-01,
         -2.4446e+00, -3.3187e-02, -6.9339e-01],
        [ 8.5348e+00, -5.2558e+00, -1.0822e+01, -7.7808e-01, -4.0797e+00,
         -1.8879e-01, -6.6603e+00,  1.0722e+01, -1.0428e+00,  5.0026e+00,
          3.9414e+00, -1.2235e+00, -1.2613e+01],
        [ 4.5267e+00,  3.5738e+00, -1.1066e+00, -3.5331e-01, -5.4827e+00,
         -4.8951e-01, -1.6835e+00, -2.4872e+01, -6.2046e-01, -1.1326e+01,
         -2.0399e+00, -7.1454e-01, -3.0923e+01],
        [-1.2464e+00,  5.1718e-02,  9.1335e-02, -6.9584e-03, -1.5437e-01,
          9.3095e-03, -7.1301e-01, -2.3387e-01, -9.6467e-03, -3.7937e-01,
         -2.4010e+00, -1.0412e-02, -7.0495e-01],
        [-9.7170e-02,  1.4712e+00,  3.3437e+00, -1.7817e+00,  1.5981e-02,
         -1.2656e-01, -1.2547e+00, -1.2414e+00, -1.3612e+00, -2.6202e+00,
         -6.6273e+00, -1.2898e+00, -4.7387e+00],
        [-2.4988e+01,  6.6279e-03,  5.1077e-03,  1.0078e-01, -1.2754e+01,
         -7.4812e-02, -1.7359e-01,  3.3293e+00,  4.0817e-02,  1.0554e+01,
         -7.3583e+00,  2.9592e-02,  1.0172e+01],
        [-7.0225e+00, -2.3751e+00,  3.1289e+00, -3.8216e-03, -3.0268e+00,
          1.4369e-01,  1.6063e+00, -1.8132e+01, -2.2555e-01,  1.4279e+01,
         -2.9856e+00, -5.3393e-01,  5.3852e+00],
        [-1.6150e+01,  1.7911e-03, -5.1000e-02, -6.6099e-02, -1.7355e+01,
          2.6681e-01, -2.9860e+00,  1.6266e+00, -4.8645e-02,  6.5288e+00,
         -7.7175e+00, -1.7876e-02,  8.9499e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  7.9924,  -3.4066,  -1.0769,  -5.7794,  -6.0547,  -5.7663, -10.0939,
         -2.8442,  -5.7800,  -1.9584,  -0.6121,   1.9155,  -3.4088],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 8.9243e-01,  5.6911e+00,  1.5663e+00, -1.2989e-01,  1.8775e-01,
         -1.2769e-01, -2.8104e-01,  8.8669e-01, -1.2967e-01,  5.6859e+00,
         -2.5489e+00, -1.8838e-01, -8.8407e+00],
        [ 4.0833e-01, -6.0124e+00, -8.9014e-02,  1.2970e-01, -1.8718e-01,
          1.2740e-01,  1.3625e+00, -1.0528e+00,  1.2947e-01, -5.5807e+00,
          3.4531e+00,  7.2938e-01,  9.1776e+00],
        [-1.1697e+01,  1.9980e+00,  9.8437e-01, -2.5489e-02, -2.5813e-02,
         -2.2211e-02,  3.6009e+00, -4.6501e-01, -2.5537e-02,  1.8547e-01,
         -1.8549e-02, -1.0172e+00, -5.6876e-03],
        [-1.7125e+00, -3.0052e+00, -4.1191e+00, -1.5236e-02,  5.2480e-01,
         -6.0817e-02, -2.1736e+01, -1.7056e+01, -1.4635e-02, -3.2576e+00,
          7.8828e-01,  2.1253e+00,  1.0758e+01],
        [ 9.3924e+00, -3.9348e+00, -1.0063e+01, -4.8412e-01, -4.6928e-01,
         -5.0761e-01,  4.0058e+00, -6.5070e+00, -4.8384e-01, -8.7925e-01,
          6.1774e+00,  3.5045e+00, -3.2768e-01]], device='cuda:0'))])
xi:  [345.36514]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 450.71096865547344
W_T_median: 387.09146055011786
W_T_pctile_5: 345.52252546722633
W_T_CVAR_5_pct: 174.02972044459628
Average q (qsum/M+1):  45.49808231476815
Optimal xi:  [345.36514]
Expected(across Rb) median(across samples) p_equity:  0.1812028767929102
obj fun:  tensor(-3150.7354, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 10.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -2.7095,  12.9892],
        [ -8.6338,   9.4281],
        [ -0.9451,   0.4527],
        [ -0.8640,   0.3781],
        [  6.1895,  -0.6523],
        [  4.5754,  -0.4303],
        [ -0.9502,   0.4519],
        [ -0.9882,   0.4554],
        [  5.3775,  -0.2578],
        [  9.7529,  -0.6215],
        [-74.6407,   0.8966],
        [  5.4828,  -0.5841],
        [ -2.7219,  16.4087]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 7.2031,  5.6080, -3.1690, -3.3192,  2.5904,  3.5518, -3.1733, -3.1998,
         3.1859, -7.8767,  0.2375,  2.9803,  8.6844], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1333e-01,  1.6554e+00,  3.4528e-02,  3.0769e-02,  8.7976e-01,
          9.1042e-01,  3.5304e-02,  3.4772e-02,  8.9665e-01,  2.6447e-01,
          4.3032e-01,  8.9469e-01,  4.6686e-01],
        [-9.2749e+00, -6.0010e+00,  1.6064e-01,  2.4939e-01,  2.6538e+00,
          2.9059e+00,  1.6532e-01,  2.3862e-01,  2.6770e+00,  2.0542e+01,
          5.6439e+00,  2.9504e+00, -1.0311e+01],
        [-5.8631e+00, -5.5932e+00, -1.0882e-01, -2.5602e-01,  1.4267e+00,
          1.6964e+00, -8.6213e-02, -1.9126e-01,  1.4872e+00,  2.0896e+01,
          5.2545e+00,  1.4174e+00, -9.1947e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4955e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.0882e-01,  1.6256e+00,  3.4094e-02,  3.0485e-02,  8.6370e-01,
          8.9398e-01,  3.4826e-02,  3.4369e-02,  8.8039e-01,  2.5781e-01,
          4.1380e-01,  8.7846e-01,  4.6006e-01],
        [-6.2435e+00, -5.0774e+00, -7.9019e-02, -1.1667e-02,  1.6100e+00,
          1.6134e+00, -1.1318e-01, -9.1158e-02,  1.1232e+00,  2.1826e+01,
          5.7213e+00,  1.3367e+00, -9.6095e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4956e-03, -5.9415e-03, -4.7716e-01,
         -4.9046e-01, -6.4683e-03, -6.3263e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1923e-01,  1.6977e+00,  3.5087e-02,  3.1098e-02,  9.0167e-01,
          9.3283e-01,  3.5931e-02,  3.5276e-02,  9.1883e-01,  2.7424e-01,
          4.5388e-01,  9.1684e-01,  4.7618e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5021, -0.5021,  0.9316,  2.2107,  1.1816, -0.5021, -0.5021,  0.9149,
         1.2202, -0.5021, -0.5021, -0.5021,  0.9544], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0192,   0.0192,   3.5844,  -9.2131, -12.7066,   0.0192,   0.0192,
           3.1214, -12.3800,   0.0192,   0.0192,   0.0192,   4.3492]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  2.3638,  12.1836],
        [  5.6572,   7.4721],
        [  7.9224,   6.5803],
        [ -6.1235,   3.1910],
        [ 11.8466,   0.5913],
        [  0.8770,  -8.1352],
        [  8.6891,  -6.8360],
        [-10.5691,  10.8756],
        [  1.7568,  -0.0712],
        [-11.0991,  -4.2491],
        [ 11.4103,   5.2357],
        [ -0.1851,   2.9347],
        [ -7.5723,  -9.3505]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 6.3522, -5.1507, -2.0487, -4.3376, -9.8811, -6.7882, -6.3489,  5.9660,
        -7.1033, -1.4560, -0.7179, -3.6786, -6.0576], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.1852e+00,  1.5094e+00,  1.3526e+00, -5.4268e-01,  2.4223e+00,
          2.0759e+00,  9.2465e-01, -1.6151e+00, -2.8824e-01,  9.5287e-01,
          2.4642e-01, -1.1737e+00,  2.2713e+00],
        [-1.8651e+00, -5.6256e-01, -6.2129e-01, -4.2742e+00, -7.7059e-01,
         -3.4892e+00, -7.6918e-02,  6.7408e+00, -2.3096e-01,  1.3500e+00,
         -3.5500e+00, -6.0667e-01, -2.9353e+00],
        [ 1.1142e-01, -1.2473e+00,  8.1916e-01,  6.4910e-01, -1.6350e+01,
          2.2732e+00, -1.5779e+00,  7.2357e+00, -7.2982e-03,  4.4049e+00,
         -5.8977e-01,  1.7865e+00, -4.6946e+00],
        [-1.3960e+00, -8.7673e-02, -4.2780e-01,  6.7083e-03, -4.0893e-01,
         -9.1685e-01, -1.7927e+00, -5.0702e-01, -1.3463e-02, -5.2559e-01,
         -1.6941e+00,  1.8378e-03, -3.4962e-01],
        [-1.9520e+00,  1.8967e-01, -5.4411e-01, -1.2101e-01, -9.1077e-01,
         -1.2479e+00, -1.1267e+00, -9.1391e-01, -1.8068e-02, -3.8365e-01,
         -2.4970e+00, -1.1678e-01, -2.1655e-01],
        [-1.4057e+00, -8.6477e-02, -4.3293e-01,  1.5699e-02, -4.1127e-01,
         -9.1630e-01, -1.7894e+00, -5.0243e-01, -1.3300e-02, -5.1448e-01,
         -1.7010e+00,  1.2009e-02, -3.4096e-01],
        [ 1.3221e+01, -6.1845e-01,  1.5793e+00, -6.7508e+00, -1.5120e+01,
         -1.8714e+01, -8.9013e+00,  4.8410e+00, -1.2180e+00,  4.2963e+00,
          1.1099e+00,  1.4776e+00, -1.3629e+01],
        [ 4.1482e+00, -4.3534e-01, -7.3632e+00,  2.5918e-02, -1.7989e-01,
         -6.4858e+00, -8.9001e-01, -1.0932e+00,  1.0207e-01,  1.5231e+00,
         -7.6390e+00,  1.3168e-01, -1.0356e+01],
        [-1.4524e+00, -8.1858e-02, -4.5182e-01,  9.7058e-02, -4.0160e-01,
         -8.8229e-01, -1.8020e+00, -4.5301e-01, -1.2095e-02, -4.6125e-01,
         -1.7020e+00,  9.5630e-02, -3.0579e-01],
        [-2.8522e+00, -1.9497e-01, -1.6674e+00, -4.5101e-01, -1.9345e+00,
         -2.7298e+00,  5.5568e-01,  1.5671e+00, -1.2880e-02,  9.4602e-01,
         -2.4953e+00, -2.9629e-01, -2.6501e+00],
        [-2.4334e+01, -6.7212e-02, -4.8855e+00,  1.9330e-01, -7.7025e+00,
          2.7805e+00,  4.4028e-01, -1.4406e+00, -4.7447e-02,  6.3915e+00,
         -8.7817e+00, -4.8862e-01,  5.4980e+00],
        [-1.0961e+01,  5.4093e-01,  8.6739e-01, -6.0412e-02, -3.7470e+00,
          2.1004e+00,  2.4607e+00, -2.1072e+01, -1.4633e-01,  9.1990e+00,
         -5.3680e+00,  1.5515e-01,  4.1397e+00],
        [-1.2571e+01,  2.5529e-02, -4.2095e+00,  2.0473e-01, -1.3435e+01,
          2.3688e+00, -6.0068e-01, -5.7139e+00, -3.7530e-01,  4.5355e+00,
         -9.0221e+00,  3.5582e-02,  2.3988e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 2.9079, -2.1828, -0.1738, -3.3916, -2.9699, -3.3962, -3.0437, -0.0264,
        -3.4166, -2.8315, -0.3684,  1.2132,  0.0742], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 7.4891e-01,  4.5452e+00,  1.8769e+00,  4.5914e-02,  3.5496e-01,
         -9.1354e-02,  2.7526e-01, -9.7636e-01, -1.0948e-01,  3.3561e+00,
         -4.4478e+00, -1.2550e-01, -1.0160e+00],
        [ 5.4131e-01, -4.5533e+00, -3.9331e-01,  2.2764e-01, -3.5322e-01,
          9.4308e-02,  8.6633e-01,  7.9627e-01,  1.1223e-01, -3.0675e+00,
          5.4099e+00,  6.6530e-01,  1.5354e+00],
        [-1.0256e+01, -4.0894e-02, -1.1152e+00, -1.1287e-02, -1.0215e-02,
         -1.1185e-02, -8.9347e-01, -1.4354e-01, -1.1272e-02, -1.5607e-02,
          8.3038e-03, -6.6567e-01,  1.3450e-03],
        [-1.2086e+00, -1.8795e+00, -3.9948e+00,  3.7431e-01,  1.3870e+00,
          3.8828e-01,  9.5588e-01, -1.2326e+01,  4.2495e-01,  2.2745e+00,
          8.1236e-01,  1.8802e+00,  1.0604e+01],
        [ 9.1296e+00, -4.1802e+00, -1.1657e+01,  2.0360e-01,  4.0986e-01,
          2.0817e-01,  5.1155e+00, -6.9774e-01,  2.1607e-01, -5.1843e-01,
          6.2533e+00,  8.3596e+00,  2.6055e+00]], device='cuda:0'))])
loaded xi:  315.55313
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -5671.298444790548
Current xi:  [326.35953]
objective value function right now is: -5671.298444790548
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -5720.876297675906
Current xi:  [334.3547]
objective value function right now is: -5720.876297675906
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [340.59982]
objective value function right now is: -5720.40089019491
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [345.48233]
objective value function right now is: -5717.190286022705
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [348.1761]
objective value function right now is: -5698.450599124595
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [350.52148]
objective value function right now is: -5685.84560530749
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [350.62836]
objective value function right now is: -5706.022502723138
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -5748.358296236469
Current xi:  [352.32584]
objective value function right now is: -5748.358296236469
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -5758.242159775444
Current xi:  [352.6152]
objective value function right now is: -5758.242159775444
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.4611]
objective value function right now is: -5679.570378301915
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [350.51935]
objective value function right now is: -5750.626399019219
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.46616]
objective value function right now is: -5723.759871568029
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.33243]
objective value function right now is: -5710.2629299104465
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [353.39462]
objective value function right now is: -5721.065026015937
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.69254]
objective value function right now is: -5683.14582800344
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [350.8932]
objective value function right now is: -5713.559296342826
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [350.68356]
objective value function right now is: -5727.733962184532
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.44666]
objective value function right now is: -5746.424106020946
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -5767.796491258207
Current xi:  [350.96283]
objective value function right now is: -5767.796491258207
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.42636]
objective value function right now is: -5765.701541270848
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [350.76865]
objective value function right now is: -5744.929194570089
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [350.0483]
objective value function right now is: -5727.335562595405
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [350.80255]
objective value function right now is: -5747.628773044972
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.21402]
objective value function right now is: -5762.345475303809
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.62582]
objective value function right now is: -5748.210404622985
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.39468]
objective value function right now is: -5739.809188387198
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.95093]
objective value function right now is: -5712.9202318265925
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [353.94348]
objective value function right now is: -5710.974193053183
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [352.04346]
objective value function right now is: -5726.321256144003
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.29108]
objective value function right now is: -5741.980417559844
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.92654]
objective value function right now is: -5749.377357544501
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.70517]
objective value function right now is: -5744.001486714849
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.1092]
objective value function right now is: -5741.648743349217
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.39502]
objective value function right now is: -5733.21402884268
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.69717]
objective value function right now is: -5745.19369616829
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -5782.763555354103
Current xi:  [353.19012]
objective value function right now is: -5782.763555354103
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.29276]
objective value function right now is: -5778.296020233268
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.49622]
objective value function right now is: -5777.027723391623
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -5783.570196906547
Current xi:  [353.6125]
objective value function right now is: -5783.570196906547
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.68652]
objective value function right now is: -5772.2910562254465
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.73462]
objective value function right now is: -5781.619387354815
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.726]
objective value function right now is: -5780.287163321247
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.47183]
objective value function right now is: -5782.837096236232
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.5349]
objective value function right now is: -5780.6273058024
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.19342]
objective value function right now is: -5778.72321729453
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.84357]
objective value function right now is: -5777.782767145601
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.44144]
objective value function right now is: -5774.610967233915
new min fval from sgd:  -5783.595817849255
new min fval from sgd:  -5783.837018816697
new min fval from sgd:  -5784.205008033276
new min fval from sgd:  -5784.466779970066
new min fval from sgd:  -5784.88385582479
new min fval from sgd:  -5785.208057024388
new min fval from sgd:  -5785.422394368613
new min fval from sgd:  -5785.729429359584
new min fval from sgd:  -5786.0213066828255
new min fval from sgd:  -5786.292704472274
new min fval from sgd:  -5786.500478079444
new min fval from sgd:  -5786.678766055767
new min fval from sgd:  -5786.777063324243
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.93958]
objective value function right now is: -5774.65535601103
new min fval from sgd:  -5786.80773467819
new min fval from sgd:  -5786.873271362603
new min fval from sgd:  -5786.921462450749
new min fval from sgd:  -5786.94688676248
new min fval from sgd:  -5786.962785295909
new min fval from sgd:  -5786.980734503039
new min fval from sgd:  -5787.004784343233
new min fval from sgd:  -5787.031857173151
new min fval from sgd:  -5787.059381885587
new min fval from sgd:  -5787.079884636178
new min fval from sgd:  -5787.091475093476
new min fval from sgd:  -5787.097039800584
new min fval from sgd:  -5787.109469690999
new min fval from sgd:  -5787.124517302598
new min fval from sgd:  -5787.127586499529
new min fval from sgd:  -5787.13648026371
new min fval from sgd:  -5787.147980487364
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.8955]
objective value function right now is: -5786.519326155931
new min fval from sgd:  -5787.153316479135
new min fval from sgd:  -5787.188872229373
new min fval from sgd:  -5787.234348525884
new min fval from sgd:  -5787.2772717956
new min fval from sgd:  -5787.335237635253
new min fval from sgd:  -5787.358549621052
new min fval from sgd:  -5787.388867243488
new min fval from sgd:  -5787.421863532645
new min fval from sgd:  -5787.450926477864
new min fval from sgd:  -5787.484841774369
new min fval from sgd:  -5787.491924923217
new min fval from sgd:  -5787.495342370941
new min fval from sgd:  -5787.495755160056
new min fval from sgd:  -5787.498207353738
new min fval from sgd:  -5787.509152366419
new min fval from sgd:  -5787.523481164741
new min fval from sgd:  -5787.538855073517
new min fval from sgd:  -5787.546127378343
new min fval from sgd:  -5787.553485108692
new min fval from sgd:  -5787.592992065464
new min fval from sgd:  -5787.641310050824
new min fval from sgd:  -5787.6670556696035
new min fval from sgd:  -5787.696857991975
new min fval from sgd:  -5787.717081125184
new min fval from sgd:  -5787.736275483145
new min fval from sgd:  -5787.736569642229
new min fval from sgd:  -5787.7491781101
new min fval from sgd:  -5787.759989784406
new min fval from sgd:  -5787.767934416776
new min fval from sgd:  -5787.773429563688
new min fval from sgd:  -5787.776139830385
new min fval from sgd:  -5787.79354864139
new min fval from sgd:  -5787.798853599945
new min fval from sgd:  -5787.80331670737
new min fval from sgd:  -5787.816337594663
new min fval from sgd:  -5787.825396367999
new min fval from sgd:  -5787.83287854627
new min fval from sgd:  -5787.875265231076
new min fval from sgd:  -5787.918386708839
new min fval from sgd:  -5787.949957941026
new min fval from sgd:  -5787.993258913462
new min fval from sgd:  -5788.0304144894735
new min fval from sgd:  -5788.0780941147505
new min fval from sgd:  -5788.116374598565
new min fval from sgd:  -5788.148659002573
new min fval from sgd:  -5788.18393294994
new min fval from sgd:  -5788.2133479161475
new min fval from sgd:  -5788.221966588285
new min fval from sgd:  -5788.267769988289
new min fval from sgd:  -5788.321409874665
new min fval from sgd:  -5788.361751666825
new min fval from sgd:  -5788.409479970267
new min fval from sgd:  -5788.449512603094
new min fval from sgd:  -5788.48771090819
new min fval from sgd:  -5788.507178497163
new min fval from sgd:  -5788.522787956861
new min fval from sgd:  -5788.523863509718
new min fval from sgd:  -5788.533756035218
new min fval from sgd:  -5788.544175337758
new min fval from sgd:  -5788.549988939714
new min fval from sgd:  -5788.562960722951
new min fval from sgd:  -5788.5796217073375
new min fval from sgd:  -5788.587790924473
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.88513]
objective value function right now is: -5788.1965109332605
min fval:  -5788.587790924473
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -7.1133,  16.5854],
        [-11.6918,  10.7193],
        [ -0.6676,   0.7438],
        [ -0.6676,   0.7438],
        [  1.3260,  -0.5267],
        [  1.2715,  -0.4448],
        [ -0.6676,   0.7438],
        [ -0.6676,   0.7441],
        [  1.2624,  -0.4118],
        [ 14.4708,  -1.2155],
        [-86.4435,  -1.9067],
        [  1.2771,  -0.4269],
        [ -8.8423,  19.5972]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  8.2274,   6.0969,  -3.8725,  -3.8725,   5.4688,   5.7066,  -3.8725,
         -3.8723,   5.5985, -10.0772,  -0.2533,   5.6194,   9.8807],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.7544e-01, -3.1959e-01,  6.6009e-03,  6.6009e-03, -7.3046e-01,
         -7.3086e-01,  6.6009e-03,  6.6016e-03, -7.3062e-01, -2.6134e-02,
         -1.5062e-01, -7.3068e-01, -2.9294e-01],
        [-2.7544e-01, -3.1959e-01,  6.6007e-03,  6.6007e-03, -7.3046e-01,
         -7.3086e-01,  6.6007e-03,  6.6014e-03, -7.3062e-01, -2.6131e-02,
         -1.5061e-01, -7.3069e-01, -2.9294e-01],
        [ 4.8475e-01,  9.5489e-01, -1.5321e-02, -1.5321e-02,  1.2471e+00,
          1.2475e+00, -1.5321e-02, -1.5320e-02,  1.2473e+00,  1.1148e-01,
          4.6300e-01,  1.2473e+00,  5.7043e-01],
        [-1.2113e+01, -7.1588e+00,  1.7109e-02,  1.7109e-02,  3.2074e+00,
          3.7120e+00,  1.7109e-02,  1.6881e-02,  3.4277e+00,  2.8154e+01,
          6.7042e+00,  3.6438e+00, -1.6959e+01],
        [-8.8328e+00, -5.0043e+00,  7.3378e-02,  7.3372e-02,  1.3517e+00,
          1.9194e+00,  7.3420e-02,  7.6643e-02,  1.7142e+00,  2.7987e+01,
          6.4535e+00,  1.5183e+00, -1.5030e+01],
        [-2.7544e-01, -3.1959e-01,  6.6009e-03,  6.6009e-03, -7.3046e-01,
         -7.3086e-01,  6.6010e-03,  6.6016e-03, -7.3062e-01, -2.6134e-02,
         -1.5062e-01, -7.3068e-01, -2.9294e-01],
        [-2.7542e-01, -3.1960e-01,  6.5995e-03,  6.5995e-03, -7.3049e-01,
         -7.3089e-01,  6.5995e-03,  6.6002e-03, -7.3065e-01, -2.6119e-02,
         -1.5058e-01, -7.3071e-01, -2.9292e-01],
        [ 4.7815e-01,  9.4001e-01, -1.5290e-02, -1.5290e-02,  1.2265e+00,
          1.2269e+00, -1.5290e-02, -1.5290e-02,  1.2267e+00,  1.1003e-01,
          4.5750e-01,  1.2267e+00,  5.6269e-01],
        [-9.0748e+00, -4.9867e+00, -9.0908e-02, -9.0903e-02,  1.6255e+00,
          1.9839e+00, -9.0947e-02, -9.3606e-02,  1.5109e+00,  2.8473e+01,
          6.1481e+00,  1.5654e+00, -1.5216e+01],
        [-2.7544e-01, -3.1959e-01,  6.6010e-03,  6.6009e-03, -7.3046e-01,
         -7.3086e-01,  6.6010e-03,  6.6016e-03, -7.3062e-01, -2.6135e-02,
         -1.5062e-01, -7.3068e-01, -2.9294e-01],
        [-2.7544e-01, -3.1959e-01,  6.6009e-03,  6.6009e-03, -7.3046e-01,
         -7.3086e-01,  6.6009e-03,  6.6016e-03, -7.3062e-01, -2.6134e-02,
         -1.5062e-01, -7.3068e-01, -2.9294e-01],
        [-2.7534e-01, -3.1966e-01,  6.5946e-03,  6.5946e-03, -7.3060e-01,
         -7.3100e-01,  6.5946e-03,  6.5953e-03, -7.3076e-01, -2.6064e-02,
         -1.5046e-01, -7.3082e-01, -2.9284e-01],
        [ 4.9384e-01,  9.7598e-01, -1.5354e-02, -1.5354e-02,  1.2758e+00,
          1.2762e+00, -1.5354e-02, -1.5354e-02,  1.2760e+00,  1.1350e-01,
          4.7090e-01,  1.2760e+00,  5.8122e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.7327, -0.7327,  1.2491,  3.1229,  1.5723, -0.7327, -0.7327,  1.2285,
         1.7720, -0.7327, -0.7327, -0.7328,  1.2779], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0288,  -0.0288,   3.6911, -10.7379, -13.4157,  -0.0288,  -0.0288,
           3.2328, -13.0099,  -0.0288,  -0.0288,  -0.0288,   4.4466]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  0.7960,  12.5815],
        [ -3.3729,   3.1199],
        [ 12.2157,  11.9313],
        [ -3.5269,   9.8804],
        [ 13.4584,   1.0566],
        [ -2.3439,   0.4631],
        [ 14.9253, -11.7887],
        [ -3.6233,  16.4000],
        [ -2.3399,   0.4646],
        [-19.0629,  -7.8490],
        [ 18.2116,   7.7549],
        [ -2.7986,   0.4983],
        [-10.8864, -13.2993]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.5421,  -8.4900,  -1.8882,  -2.7958, -11.8593,  -6.0221,  -8.8786,
          8.8459,  -6.0257,  -3.8133,  -1.3272,  -5.7983,  -6.9817],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.5225e+00,  2.8050e-02, -2.0368e-01,  2.6276e-02,  2.0268e+00,
          3.9825e-01,  4.2178e-01,  1.4838e+01,  3.9627e-01,  8.0543e-01,
         -1.5818e+00,  5.0258e-01,  6.4274e+00],
        [ 2.5514e+00,  4.8311e-03, -2.0206e+01, -2.1959e-01, -3.3079e+00,
          1.4904e-01, -1.9160e+00,  1.4475e+00,  1.3917e-01,  6.4847e+00,
         -2.4712e+00,  1.6173e-01, -4.6747e+00],
        [ 6.5518e+00, -6.2535e+00, -3.8940e+00, -2.9355e-01, -1.4682e+01,
         -2.4728e-01, -6.2271e+00,  8.9668e+00, -2.3849e-01,  3.9485e+00,
          1.7959e+00, -5.2668e-01, -1.6169e+01],
        [-2.5106e+00,  5.0569e-01,  1.2715e+00,  5.3195e-01,  1.0058e+00,
          1.8194e-01, -5.0555e-01, -1.9073e+00,  1.8249e-01, -4.0566e-01,
         -1.4425e+00,  2.2278e-01, -1.7586e+00],
        [-1.9740e+00,  6.9882e-01,  9.1562e-01,  8.6322e-01,  1.3946e+00,
          2.0105e-01, -1.9343e-01, -8.6002e-01,  1.9460e-01, -1.8870e-01,
         -1.0012e+00,  8.9951e-02, -2.1018e+00],
        [-1.6782e-01, -9.0428e-01, -1.1070e+00, -1.2154e+00, -1.2980e+00,
          3.0627e-01, -9.1299e+00,  8.3804e-01,  3.0805e-01,  2.7861e+00,
         -4.0777e+00,  3.2848e-01,  4.1287e+00],
        [ 8.6645e+00, -6.1461e+00,  7.3051e+00, -1.1789e+00, -1.0436e+01,
         -3.0780e-01, -7.3702e+00,  2.1953e+00, -3.0213e-01,  8.5313e+00,
          6.1747e-01, -5.4975e-01, -1.0680e+01],
        [ 5.1987e-01, -1.1152e-02, -7.7333e+00, -1.3863e-02, -1.0008e+01,
         -5.7729e-02, -9.6704e-01, -3.0382e+00, -5.5201e-02, -3.6721e+00,
         -4.0603e+00, -6.0167e-02, -1.4620e+01],
        [-2.0095e+00, -5.4198e-01, -7.7494e-01, -6.9960e-01, -7.3312e-01,
         -1.5284e-02, -4.6156e+00, -9.4753e-01, -1.5257e-02,  8.7397e-01,
         -1.2878e+00, -1.5567e-02,  9.0128e-01],
        [-2.6176e+00,  7.2279e-03, -8.6476e+00, -7.0454e-02, -1.4442e+01,
         -1.5313e-01,  3.9611e-01, -2.6108e+00, -1.4236e-01, -2.1038e+00,
          3.1476e-03, -2.4468e-01, -1.4812e+01],
        [-2.3051e+01, -9.3289e-04, -7.2481e+00,  3.7825e-02, -1.3917e+01,
          5.3759e-01,  7.2451e-01, -3.3517e+00,  5.2876e-01,  1.3811e+01,
         -9.9932e+00,  4.7504e-01,  9.0488e+00],
        [-7.8376e+00, -3.0003e-02,  2.6811e+00,  7.1704e-02, -2.8122e+00,
         -9.9726e-01,  2.1462e+00, -1.1552e+01, -9.5682e-01,  1.3834e+01,
         -3.5896e+00, -1.1962e+00,  4.3686e+00],
        [-1.3351e+01, -5.1805e-03, -1.1779e+00, -6.0170e-02, -1.7789e+01,
          1.6521e-01, -1.0884e+00, -8.5725e+00,  1.6901e-01,  5.3773e+00,
         -9.5857e+00,  2.2125e-01,  6.1606e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 3.0036, -1.2283,  1.2185, -4.0508, -4.0489, -3.9131, -9.0197, -1.1380,
        -4.8825, -3.0645,  0.2099,  1.3212, -0.3114], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 9.4527e-01,  1.6728e+00,  1.2118e+00, -1.7257e+00, -1.7407e+00,
          1.8633e+00,  2.0573e-01,  6.2773e+00,  8.2310e-01,  6.5243e+00,
         -6.9294e+00, -3.3264e-01, -1.8484e+01],
        [ 3.4915e-01, -1.6837e+00,  2.7221e-01,  1.7260e+00,  1.7409e+00,
         -1.8639e+00,  9.3465e-01, -6.4138e+00, -8.2351e-01, -6.3140e+00,
          7.8417e+00,  8.7329e-01,  1.8886e+01],
        [-1.2367e+01, -1.0887e-01, -1.4402e+00, -3.0315e-02, -9.1937e-02,
         -1.4754e-03, -1.6624e-01, -9.2507e-03, -1.9085e-04, -4.9464e-02,
         -4.6235e-03, -1.5888e+00,  1.2553e-03],
        [-1.8582e+00,  1.2599e+00, -6.9080e+00, -1.8054e-01, -6.2808e-01,
         -2.3011e+00, -1.7673e+01, -1.1918e+00, -9.6388e-01, -4.4470e+00,
          1.6772e+00,  2.4621e+00,  1.0655e+01],
        [ 1.1046e+01, -2.5964e+00, -1.0239e+01, -6.3902e-01, -3.3781e-01,
         -5.3565e+00,  2.2677e+01, -1.3102e+00, -1.6338e+00,  8.2270e-01,
          1.4640e+01,  4.2127e-01,  1.9694e+00]], device='cuda:0'))])
xi:  [352.8699]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 549.1497038118591
W_T_median: 436.98619530942426
W_T_pctile_5: 352.89647345852353
W_T_CVAR_5_pct: 176.74222992920542
Average q (qsum/M+1):  44.19457220262097
Optimal xi:  [352.8699]
Expected(across Rb) median(across samples) p_equity:  0.16650743017283579
obj fun:  tensor(-5788.5878, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 25.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -2.7095,  12.9892],
        [ -8.6338,   9.4281],
        [ -0.9451,   0.4527],
        [ -0.8640,   0.3781],
        [  6.1895,  -0.6523],
        [  4.5754,  -0.4303],
        [ -0.9502,   0.4519],
        [ -0.9882,   0.4554],
        [  5.3775,  -0.2578],
        [  9.7529,  -0.6215],
        [-74.6407,   0.8966],
        [  5.4828,  -0.5841],
        [ -2.7219,  16.4087]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 7.2031,  5.6080, -3.1690, -3.3192,  2.5904,  3.5518, -3.1733, -3.1998,
         3.1859, -7.8767,  0.2375,  2.9803,  8.6844], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1333e-01,  1.6554e+00,  3.4528e-02,  3.0769e-02,  8.7976e-01,
          9.1042e-01,  3.5304e-02,  3.4772e-02,  8.9665e-01,  2.6447e-01,
          4.3032e-01,  8.9469e-01,  4.6686e-01],
        [-9.2749e+00, -6.0010e+00,  1.6064e-01,  2.4939e-01,  2.6538e+00,
          2.9059e+00,  1.6532e-01,  2.3862e-01,  2.6770e+00,  2.0542e+01,
          5.6439e+00,  2.9504e+00, -1.0311e+01],
        [-5.8631e+00, -5.5932e+00, -1.0882e-01, -2.5602e-01,  1.4267e+00,
          1.6964e+00, -8.6213e-02, -1.9126e-01,  1.4872e+00,  2.0896e+01,
          5.2545e+00,  1.4174e+00, -9.1947e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4955e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.0882e-01,  1.6256e+00,  3.4094e-02,  3.0485e-02,  8.6370e-01,
          8.9398e-01,  3.4826e-02,  3.4369e-02,  8.8039e-01,  2.5781e-01,
          4.1380e-01,  8.7846e-01,  4.6006e-01],
        [-6.2435e+00, -5.0774e+00, -7.9019e-02, -1.1667e-02,  1.6100e+00,
          1.6134e+00, -1.1318e-01, -9.1158e-02,  1.1232e+00,  2.1826e+01,
          5.7213e+00,  1.3367e+00, -9.6095e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4956e-03, -5.9415e-03, -4.7716e-01,
         -4.9046e-01, -6.4683e-03, -6.3263e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1923e-01,  1.6977e+00,  3.5087e-02,  3.1098e-02,  9.0167e-01,
          9.3283e-01,  3.5931e-02,  3.5276e-02,  9.1883e-01,  2.7424e-01,
          4.5388e-01,  9.1684e-01,  4.7618e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5021, -0.5021,  0.9316,  2.2107,  1.1816, -0.5021, -0.5021,  0.9149,
         1.2202, -0.5021, -0.5021, -0.5021,  0.9544], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0192,   0.0192,   3.5844,  -9.2131, -12.7066,   0.0192,   0.0192,
           3.1214, -12.3800,   0.0192,   0.0192,   0.0192,   4.3492]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  2.3638,  12.1836],
        [  5.6572,   7.4721],
        [  7.9224,   6.5803],
        [ -6.1235,   3.1910],
        [ 11.8466,   0.5913],
        [  0.8770,  -8.1352],
        [  8.6891,  -6.8360],
        [-10.5691,  10.8756],
        [  1.7568,  -0.0712],
        [-11.0991,  -4.2491],
        [ 11.4103,   5.2357],
        [ -0.1851,   2.9347],
        [ -7.5723,  -9.3505]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 6.3522, -5.1507, -2.0487, -4.3376, -9.8811, -6.7882, -6.3489,  5.9660,
        -7.1033, -1.4560, -0.7179, -3.6786, -6.0576], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.1852e+00,  1.5094e+00,  1.3526e+00, -5.4268e-01,  2.4223e+00,
          2.0759e+00,  9.2465e-01, -1.6151e+00, -2.8824e-01,  9.5287e-01,
          2.4642e-01, -1.1737e+00,  2.2713e+00],
        [-1.8651e+00, -5.6256e-01, -6.2129e-01, -4.2742e+00, -7.7059e-01,
         -3.4892e+00, -7.6918e-02,  6.7408e+00, -2.3096e-01,  1.3500e+00,
         -3.5500e+00, -6.0667e-01, -2.9353e+00],
        [ 1.1142e-01, -1.2473e+00,  8.1916e-01,  6.4910e-01, -1.6350e+01,
          2.2732e+00, -1.5779e+00,  7.2357e+00, -7.2982e-03,  4.4049e+00,
         -5.8977e-01,  1.7865e+00, -4.6946e+00],
        [-1.3960e+00, -8.7673e-02, -4.2780e-01,  6.7083e-03, -4.0893e-01,
         -9.1685e-01, -1.7927e+00, -5.0702e-01, -1.3463e-02, -5.2559e-01,
         -1.6941e+00,  1.8378e-03, -3.4962e-01],
        [-1.9520e+00,  1.8967e-01, -5.4411e-01, -1.2101e-01, -9.1077e-01,
         -1.2479e+00, -1.1267e+00, -9.1391e-01, -1.8068e-02, -3.8365e-01,
         -2.4970e+00, -1.1678e-01, -2.1655e-01],
        [-1.4057e+00, -8.6477e-02, -4.3293e-01,  1.5699e-02, -4.1127e-01,
         -9.1630e-01, -1.7894e+00, -5.0243e-01, -1.3300e-02, -5.1448e-01,
         -1.7010e+00,  1.2009e-02, -3.4096e-01],
        [ 1.3221e+01, -6.1845e-01,  1.5793e+00, -6.7508e+00, -1.5120e+01,
         -1.8714e+01, -8.9013e+00,  4.8410e+00, -1.2180e+00,  4.2963e+00,
          1.1099e+00,  1.4776e+00, -1.3629e+01],
        [ 4.1482e+00, -4.3534e-01, -7.3632e+00,  2.5918e-02, -1.7989e-01,
         -6.4858e+00, -8.9001e-01, -1.0932e+00,  1.0207e-01,  1.5231e+00,
         -7.6390e+00,  1.3168e-01, -1.0356e+01],
        [-1.4524e+00, -8.1858e-02, -4.5182e-01,  9.7058e-02, -4.0160e-01,
         -8.8229e-01, -1.8020e+00, -4.5301e-01, -1.2095e-02, -4.6125e-01,
         -1.7020e+00,  9.5630e-02, -3.0579e-01],
        [-2.8522e+00, -1.9497e-01, -1.6674e+00, -4.5101e-01, -1.9345e+00,
         -2.7298e+00,  5.5568e-01,  1.5671e+00, -1.2880e-02,  9.4602e-01,
         -2.4953e+00, -2.9629e-01, -2.6501e+00],
        [-2.4334e+01, -6.7212e-02, -4.8855e+00,  1.9330e-01, -7.7025e+00,
          2.7805e+00,  4.4028e-01, -1.4406e+00, -4.7447e-02,  6.3915e+00,
         -8.7817e+00, -4.8862e-01,  5.4980e+00],
        [-1.0961e+01,  5.4093e-01,  8.6739e-01, -6.0412e-02, -3.7470e+00,
          2.1004e+00,  2.4607e+00, -2.1072e+01, -1.4633e-01,  9.1990e+00,
         -5.3680e+00,  1.5515e-01,  4.1397e+00],
        [-1.2571e+01,  2.5529e-02, -4.2095e+00,  2.0473e-01, -1.3435e+01,
          2.3688e+00, -6.0068e-01, -5.7139e+00, -3.7530e-01,  4.5355e+00,
         -9.0221e+00,  3.5582e-02,  2.3988e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 2.9079, -2.1828, -0.1738, -3.3916, -2.9699, -3.3962, -3.0437, -0.0264,
        -3.4166, -2.8315, -0.3684,  1.2132,  0.0742], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 7.4891e-01,  4.5452e+00,  1.8769e+00,  4.5914e-02,  3.5496e-01,
         -9.1354e-02,  2.7526e-01, -9.7636e-01, -1.0948e-01,  3.3561e+00,
         -4.4478e+00, -1.2550e-01, -1.0160e+00],
        [ 5.4131e-01, -4.5533e+00, -3.9331e-01,  2.2764e-01, -3.5322e-01,
          9.4308e-02,  8.6633e-01,  7.9627e-01,  1.1223e-01, -3.0675e+00,
          5.4099e+00,  6.6530e-01,  1.5354e+00],
        [-1.0256e+01, -4.0894e-02, -1.1152e+00, -1.1287e-02, -1.0215e-02,
         -1.1185e-02, -8.9347e-01, -1.4354e-01, -1.1272e-02, -1.5607e-02,
          8.3038e-03, -6.6567e-01,  1.3450e-03],
        [-1.2086e+00, -1.8795e+00, -3.9948e+00,  3.7431e-01,  1.3870e+00,
          3.8828e-01,  9.5588e-01, -1.2326e+01,  4.2495e-01,  2.2745e+00,
          8.1236e-01,  1.8802e+00,  1.0604e+01],
        [ 9.1296e+00, -4.1802e+00, -1.1657e+01,  2.0360e-01,  4.0986e-01,
          2.0817e-01,  5.1155e+00, -6.9774e-01,  2.1607e-01, -5.1843e-01,
          6.2533e+00,  8.3596e+00,  2.6055e+00]], device='cuda:0'))])
loaded xi:  315.55313
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -9864.778322213255
Current xi:  [326.5874]
objective value function right now is: -9864.778322213255
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -10018.574540029083
Current xi:  [335.15482]
objective value function right now is: -10018.574540029083
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [340.19147]
objective value function right now is: -9984.106015217032
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -10062.90790656891
Current xi:  [345.2796]
objective value function right now is: -10062.90790656891
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -10149.103643438313
Current xi:  [348.5025]
objective value function right now is: -10149.103643438313
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -10159.141774885353
Current xi:  [349.80884]
objective value function right now is: -10159.141774885353
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [351.40094]
objective value function right now is: -10072.94402022675
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.356]
objective value function right now is: -10050.186400113806
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.8359]
objective value function right now is: -10141.6919714567
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.00043]
objective value function right now is: -10129.528254537414
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.8415]
objective value function right now is: -10140.591082261533
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.3811]
objective value function right now is: -10156.392242878923
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.1684]
objective value function right now is: -10103.685028182277
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [353.65475]
objective value function right now is: -10113.97258538501
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [356.04025]
objective value function right now is: -10107.989888805978
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.74463]
objective value function right now is: -10132.509580226011
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -10165.95565634894
Current xi:  [354.8529]
objective value function right now is: -10165.95565634894
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.92737]
objective value function right now is: -10078.089720737074
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.7238]
objective value function right now is: -10082.830028040345
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.77414]
objective value function right now is: -10097.812823443786
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.41296]
objective value function right now is: -10067.885481619243
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.33783]
objective value function right now is: -10123.647145593643
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.68274]
objective value function right now is: -10037.96660607031
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.98105]
objective value function right now is: -10074.344229691036
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -10180.992512414463
Current xi:  [353.183]
objective value function right now is: -10180.992512414463
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.99387]
objective value function right now is: -10014.658173531916
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.4824]
objective value function right now is: -10113.66077809794
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [352.503]
objective value function right now is: -10121.973983589114
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [354.4337]
objective value function right now is: -10134.878294350792
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [352.6977]
objective value function right now is: -10067.57224024917
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.5998]
objective value function right now is: -10090.43404605966
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [353.81866]
objective value function right now is: -10146.290269917172
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.93903]
objective value function right now is: -10122.297197352824
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.83392]
objective value function right now is: -10093.3603897021
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.0802]
objective value function right now is: -10089.570890460609
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -10181.997545133248
Current xi:  [355.06366]
objective value function right now is: -10181.997545133248
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -10195.603440876786
Current xi:  [354.56305]
objective value function right now is: -10195.603440876786
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.67172]
objective value function right now is: -10187.940249676212
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.69553]
objective value function right now is: -10189.40984206508
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -10195.872267224948
Current xi:  [354.61047]
objective value function right now is: -10195.872267224948
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -10201.416109807626
Current xi:  [354.59604]
objective value function right now is: -10201.416109807626
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -10201.826748056017
Current xi:  [354.72247]
objective value function right now is: -10201.826748056017
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.01508]
objective value function right now is: -10201.430612884114
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.97046]
objective value function right now is: -10199.565499686933
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.58508]
objective value function right now is: -10201.385854069342
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -10202.061082528067
Current xi:  [354.6961]
objective value function right now is: -10202.061082528067
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.85217]
objective value function right now is: -10201.711578642033
new min fval from sgd:  -10202.835919927873
new min fval from sgd:  -10204.39454956465
new min fval from sgd:  -10205.753822970184
new min fval from sgd:  -10207.596516581254
new min fval from sgd:  -10209.203320613044
new min fval from sgd:  -10209.926620767581
new min fval from sgd:  -10210.148307159427
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.4861]
objective value function right now is: -10203.463231062498
new min fval from sgd:  -10210.33474277408
new min fval from sgd:  -10211.170327316879
new min fval from sgd:  -10211.674188089803
new min fval from sgd:  -10212.390308313528
new min fval from sgd:  -10212.816302116373
new min fval from sgd:  -10212.846922700432
new min fval from sgd:  -10212.864814250921
new min fval from sgd:  -10212.929129347449
new min fval from sgd:  -10212.95265321409
new min fval from sgd:  -10212.968276968628
new min fval from sgd:  -10213.01030453505
new min fval from sgd:  -10213.027924732585
new min fval from sgd:  -10213.052934828967
new min fval from sgd:  -10213.06046773377
new min fval from sgd:  -10213.09922762074
new min fval from sgd:  -10213.107829423345
new min fval from sgd:  -10213.205237754652
new min fval from sgd:  -10213.308677534584
new min fval from sgd:  -10213.355809413966
new min fval from sgd:  -10213.385028346156
new min fval from sgd:  -10213.424693331175
new min fval from sgd:  -10213.514277951885
new min fval from sgd:  -10213.690876417246
new min fval from sgd:  -10213.829467048177
new min fval from sgd:  -10213.955414440627
new min fval from sgd:  -10214.024564823985
new min fval from sgd:  -10214.043163283774
new min fval from sgd:  -10214.053363172703
new min fval from sgd:  -10214.091840338466
new min fval from sgd:  -10214.147182782703
new min fval from sgd:  -10214.270449081678
new min fval from sgd:  -10214.316198802353
new min fval from sgd:  -10214.402847869276
new min fval from sgd:  -10214.468038883519
new min fval from sgd:  -10214.545682772532
new min fval from sgd:  -10214.625868787505
new min fval from sgd:  -10214.758062564313
new min fval from sgd:  -10214.845084749084
new min fval from sgd:  -10214.924217163347
new min fval from sgd:  -10214.958414154347
new min fval from sgd:  -10214.977318047546
new min fval from sgd:  -10214.978645070882
new min fval from sgd:  -10214.979127702512
new min fval from sgd:  -10214.999478665144
new min fval from sgd:  -10215.019813586629
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.32135]
objective value function right now is: -10210.573301259728
new min fval from sgd:  -10215.052378163018
new min fval from sgd:  -10215.141984900476
new min fval from sgd:  -10215.192688919724
new min fval from sgd:  -10215.265878625165
new min fval from sgd:  -10215.29662214273
new min fval from sgd:  -10215.29684043434
new min fval from sgd:  -10215.310028816464
new min fval from sgd:  -10215.379979353655
new min fval from sgd:  -10215.490130015123
new min fval from sgd:  -10215.616940747015
new min fval from sgd:  -10215.706815970972
new min fval from sgd:  -10215.807336413289
new min fval from sgd:  -10215.833534728505
new min fval from sgd:  -10215.889233327076
new min fval from sgd:  -10215.956814723124
new min fval from sgd:  -10216.047415882951
new min fval from sgd:  -10216.11578447878
new min fval from sgd:  -10216.180584371876
new min fval from sgd:  -10216.235285731622
new min fval from sgd:  -10216.296395789066
new min fval from sgd:  -10216.400715677546
new min fval from sgd:  -10216.481274788737
new min fval from sgd:  -10216.522774664283
new min fval from sgd:  -10216.562834966304
new min fval from sgd:  -10216.607533383409
new min fval from sgd:  -10216.623642099948
new min fval from sgd:  -10216.645746695891
new min fval from sgd:  -10216.686606879848
new min fval from sgd:  -10216.741066036817
new min fval from sgd:  -10216.763567954067
new min fval from sgd:  -10216.778484855768
new min fval from sgd:  -10216.837282280794
new min fval from sgd:  -10216.869083097861
new min fval from sgd:  -10216.902470854015
new min fval from sgd:  -10216.93295866632
new min fval from sgd:  -10216.938260193987
new min fval from sgd:  -10216.967731539953
new min fval from sgd:  -10216.999848227824
new min fval from sgd:  -10217.020040651321
new min fval from sgd:  -10217.040000574887
new min fval from sgd:  -10217.05930499236
new min fval from sgd:  -10217.066736177321
new min fval from sgd:  -10217.09756946854
new min fval from sgd:  -10217.12835948385
new min fval from sgd:  -10217.148094823879
new min fval from sgd:  -10217.151600231253
new min fval from sgd:  -10217.16312812648
new min fval from sgd:  -10217.175981295784
new min fval from sgd:  -10217.199102964905
new min fval from sgd:  -10217.216965499145
new min fval from sgd:  -10217.219228009542
new min fval from sgd:  -10217.248433707566
new min fval from sgd:  -10217.274826001485
new min fval from sgd:  -10217.303651799479
new min fval from sgd:  -10217.364729823576
new min fval from sgd:  -10217.429565435361
new min fval from sgd:  -10217.481825986997
new min fval from sgd:  -10217.529703775786
new min fval from sgd:  -10217.5780445938
new min fval from sgd:  -10217.615767465555
new min fval from sgd:  -10217.629644715247
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.25143]
objective value function right now is: -10216.7289079385
min fval:  -10217.629644715247
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -8.2480,  16.4527],
        [-10.8500,  11.6466],
        [ -0.7125,   0.6867],
        [ -0.6995,   0.6772],
        [  1.1291,  -0.3687],
        [ -3.1804, -12.4894],
        [ -0.7123,   0.6865],
        [ -0.7055,   0.6812],
        [  0.9374,  -0.2823],
        [ 14.4125,  -1.3077],
        [-94.0129,  -2.0972],
        [  0.8134,  -4.2266],
        [ -9.9109,  19.4298]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 8.0095,  5.7501, -4.2390, -4.2604,  5.9186,  4.0752, -4.2393, -4.2509,
         6.1469, -9.5588, -0.0245, -0.5992,  9.6430], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.2538e-01, -3.0680e-01, -1.3540e-02, -1.3355e-02, -9.1686e-01,
         -8.6667e-01, -1.3538e-02, -1.3437e-02, -9.1723e-01, -2.7831e-02,
         -2.0100e-01, -6.7660e-01, -3.5016e-01],
        [-3.2538e-01, -3.0680e-01, -1.3540e-02, -1.3355e-02, -9.1686e-01,
         -8.6667e-01, -1.3538e-02, -1.3437e-02, -9.1723e-01, -2.7830e-02,
         -2.0099e-01, -6.7659e-01, -3.5016e-01],
        [ 6.6370e-01,  6.0593e-01,  1.0010e-02,  1.0104e-02,  1.4709e+00,
          1.2800e+00,  1.0011e-02,  1.0063e-02,  1.4711e+00,  1.0022e-01,
          4.2712e-01,  1.1093e+00,  7.1792e-01],
        [-1.1629e+01, -5.9137e+00,  9.6126e-02,  1.0075e-01,  3.1050e+00,
          2.1355e+00,  9.6187e-02,  9.8645e-02,  3.5605e+00,  3.1898e+01,
          5.7073e+00,  2.1333e+00, -1.6326e+01],
        [-8.2729e+00, -5.2098e+00, -1.7338e-02, -1.8243e-02,  1.6093e+00,
          9.3318e-01, -1.7354e-02, -1.8047e-02,  2.1057e+00,  2.7636e+01,
          5.4719e+00, -9.6169e-01, -1.4272e+01],
        [-3.2538e-01, -3.0680e-01, -1.3540e-02, -1.3355e-02, -9.1686e-01,
         -8.6667e-01, -1.3538e-02, -1.3437e-02, -9.1723e-01, -2.7830e-02,
         -2.0099e-01, -6.7659e-01, -3.5016e-01],
        [-3.2538e-01, -3.0680e-01, -1.3540e-02, -1.3355e-02, -9.1686e-01,
         -8.6667e-01, -1.3538e-02, -1.3437e-02, -9.1723e-01, -2.7830e-02,
         -2.0099e-01, -6.7659e-01, -3.5016e-01],
        [ 6.5195e-01,  5.9441e-01,  1.0135e-02,  1.0229e-02,  1.4446e+00,
          1.2604e+00,  1.0137e-02,  1.0188e-02,  1.4449e+00,  9.8782e-02,
          4.2405e-01,  1.0905e+00,  7.0551e-01],
        [-8.5841e+00, -4.8091e+00, -4.3152e-02, -4.6935e-02,  1.8838e+00,
          1.0363e+00, -4.3209e-02, -4.5143e-02,  1.7399e+00,  2.7795e+01,
          5.3603e+00, -9.3107e-01, -1.4466e+01],
        [-3.2538e-01, -3.0680e-01, -1.3540e-02, -1.3355e-02, -9.1686e-01,
         -8.6667e-01, -1.3538e-02, -1.3437e-02, -9.1723e-01, -2.7830e-02,
         -2.0099e-01, -6.7659e-01, -3.5016e-01],
        [-3.2538e-01, -3.0680e-01, -1.3540e-02, -1.3355e-02, -9.1686e-01,
         -8.6667e-01, -1.3538e-02, -1.3437e-02, -9.1723e-01, -2.7830e-02,
         -2.0099e-01, -6.7659e-01, -3.5016e-01],
        [-3.2538e-01, -3.0680e-01, -1.3540e-02, -1.3355e-02, -9.1686e-01,
         -8.6667e-01, -1.3538e-02, -1.3437e-02, -9.1723e-01, -2.7829e-02,
         -2.0099e-01, -6.7659e-01, -3.5016e-01],
        [ 6.8018e-01,  6.2221e-01,  9.8116e-03,  9.9070e-03,  1.5072e+00,
          1.3071e+00,  9.8131e-03,  9.8650e-03,  1.5075e+00,  1.0221e-01,
          4.3144e-01,  1.1356e+00,  7.3536e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.9187, -0.9187,  1.4721,  3.3831,  2.2360, -0.9187, -0.9187,  1.4458,
         2.3474, -0.9187, -0.9187, -0.9187,  1.5084], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0261,  -0.0261,   3.5073,  -9.9719, -14.4398,  -0.0261,  -0.0261,
           3.0485, -13.8120,  -0.0261,  -0.0261,  -0.0261,   4.2641]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  0.9919,  12.7487],
        [ -2.6065,   0.0370],
        [ 11.8561,  10.2859],
        [ -4.7811,   0.5195],
        [ 14.8809,   0.7713],
        [ -2.3147,   0.1875],
        [  3.8342, -14.5919],
        [-11.9986,  14.0702],
        [ -2.4317,   0.2589],
        [-15.7101,  -8.0295],
        [ 18.0039,   7.1013],
        [ -1.5506,   2.4373],
        [-10.7590, -13.3341]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.1782,  -6.5481,  -2.8585,  -6.4118, -13.2089,  -6.6775,  -8.7211,
          8.7423,  -6.6119,  -4.0470,  -1.8796,  -7.6924,  -7.1921],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.7537e+00, -3.4886e-03,  3.2255e-01,  6.8219e-03,  6.2059e-01,
          4.6213e-04,  9.6074e-01,  2.4021e+00, -8.9646e-04,  5.6019e-01,
          2.0619e+00,  1.4704e-01,  1.0117e+00],
        [-2.3248e-01,  9.0492e-02, -1.0554e+01,  1.2627e-01, -3.5902e+00,
          1.6677e-02, -1.1778e+00,  4.5513e+00,  9.7500e-02,  3.9041e+00,
         -3.6334e+00,  8.0701e-02, -6.6032e+00],
        [ 4.4567e+00, -3.1680e-01, -1.6518e+00,  5.7480e-02, -2.9319e+01,
         -2.6097e-01, -3.6606e+00,  1.4138e+01, -2.6136e-01,  1.7834e+00,
          9.4577e-01,  1.0182e-01, -1.8793e+01],
        [-2.1587e+00, -2.9049e-02, -5.1752e-01, -2.2808e-02, -6.6170e-01,
         -2.8872e-02, -2.4281e+00, -1.0545e+00, -2.8728e-02, -3.4732e-01,
         -2.1571e+00, -2.1323e-01, -6.2456e-01],
        [-2.4551e+00,  9.4484e-03,  5.0474e-02, -2.0856e-03, -1.1079e-01,
          8.6260e-03, -1.9476e+00, -1.8505e+00,  8.0149e-03, -3.8970e-01,
         -1.9638e+00, -5.6123e-01, -1.0223e+00],
        [-2.1593e+00, -2.9008e-02, -5.1644e-01, -2.2797e-02, -6.6104e-01,
         -2.8837e-02, -2.4270e+00, -1.0553e+00, -2.8694e-02, -3.4741e-01,
         -2.1572e+00, -2.1377e-01, -6.2509e-01],
        [ 9.6620e+00,  1.0894e+00,  2.6929e+00,  5.0650e-01, -2.0324e+01,
          2.3329e-01, -1.4979e+01,  1.0387e+01,  5.8482e-02,  9.7785e+00,
         -1.3497e+00,  2.6660e+00, -1.6345e+01],
        [ 6.9721e+00, -1.0887e+00, -1.6223e+00, -1.2665e+00, -7.7247e+00,
         -1.2264e-01, -3.7248e+00,  4.2086e+00, -6.1332e-02,  1.0017e+01,
         -1.0733e+00, -4.6149e+00, -3.0449e+01],
        [-2.4810e+00,  1.7349e-02,  1.4749e-01,  4.1762e-03,  2.1453e-02,
          1.6568e-02, -1.8405e+00, -2.0638e+00,  1.5873e-02, -4.1397e-01,
         -1.8975e+00, -6.3554e-01, -1.1081e+00],
        [ 2.1890e+00, -3.7069e-01, -1.0053e+01, -1.9874e-01, -1.1570e+00,
         -4.1863e-01, -7.3157e-01, -3.1621e+00, -4.9438e-01, -4.6060e+00,
         -1.6220e+00, -9.7975e-02, -7.4458e+00],
        [-2.6817e+01,  3.3584e-01, -1.5175e+01,  2.6967e-01, -1.3777e+01,
          6.1780e-02,  1.5828e+00,  1.4508e-01,  2.8021e-01,  1.0523e+01,
         -1.0346e+01,  5.4742e-02,  9.7065e+00],
        [-1.3236e+01, -6.6862e-01,  2.3014e+00, -3.0612e-01, -1.9251e+00,
         -4.5793e-01,  1.8863e+00, -1.0039e+01, -7.5767e-01,  1.4840e+01,
         -3.4899e+00, -4.5892e-01,  4.0034e+00],
        [-1.9058e+01, -1.1738e-01, -2.6915e+00, -1.0202e-01, -1.8437e+01,
          1.3861e-01,  3.5468e-01, -3.0101e-02, -5.3325e-02,  5.5768e+00,
         -1.1093e+01, -2.4411e-02,  8.7566e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 4.8525, -1.8191,  0.3721, -4.7825, -4.6061, -4.7823, -9.5603,  0.3135,
        -4.5594, -2.4247, -0.5129,  1.3080, -2.7194], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 9.0362e-01,  4.7138e+00,  1.0003e+00, -2.6889e-01, -7.6767e-01,
         -2.6968e-01,  1.1384e-01,  2.8043e-01, -8.7358e-01,  4.2955e+00,
         -5.2086e+00, -3.0882e-01, -9.7631e+00],
        [ 3.8832e-01, -4.7253e+00,  4.7945e-01,  2.6890e-01,  7.6770e-01,
          2.6967e-01,  1.0227e+00, -4.6106e-01,  8.7363e-01, -4.0284e+00,
          6.1523e+00,  8.4892e-01,  1.0204e+01],
        [-1.2752e+01, -4.7522e-02, -1.0322e+00, -6.2093e-04, -2.4546e-03,
         -6.2117e-04, -3.1298e-02, -1.4407e+00, -3.1550e-03, -5.0777e-02,
         -1.1625e-02, -9.9269e-01,  2.5472e-04],
        [-1.6375e+00,  2.9555e+00, -6.3279e+00,  8.2375e-01,  7.0505e-01,
          8.2393e-01, -1.7728e+01, -3.5940e+00,  6.5098e-01,  1.1941e+01,
          1.5271e+00,  2.2016e+00,  1.1624e+01],
        [ 9.7446e+00, -2.6416e+00, -1.0110e+01, -2.7995e-01, -2.9130e-01,
         -2.7996e-01,  2.0567e+00,  6.9809e-01, -2.7569e-01, -5.4987e+00,
          1.5646e+01,  1.8403e+00, -1.0221e+00]], device='cuda:0'))])
xi:  [354.2585]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 493.7346816382711
W_T_median: 403.2491629698859
W_T_pctile_5: 353.82244637137774
W_T_CVAR_5_pct: 177.4660045362472
Average q (qsum/M+1):  43.36819950226815
Optimal xi:  [354.2585]
Expected(across Rb) median(across samples) p_equity:  0.16272929583501536
obj fun:  tensor(-10217.6296, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -2.7095,  12.9892],
        [ -8.6338,   9.4281],
        [ -0.9451,   0.4527],
        [ -0.8640,   0.3781],
        [  6.1895,  -0.6523],
        [  4.5754,  -0.4303],
        [ -0.9502,   0.4519],
        [ -0.9882,   0.4554],
        [  5.3775,  -0.2578],
        [  9.7529,  -0.6215],
        [-74.6407,   0.8966],
        [  5.4828,  -0.5841],
        [ -2.7219,  16.4087]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 7.2031,  5.6080, -3.1690, -3.3192,  2.5904,  3.5518, -3.1733, -3.1998,
         3.1859, -7.8767,  0.2375,  2.9803,  8.6844], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1333e-01,  1.6554e+00,  3.4528e-02,  3.0769e-02,  8.7976e-01,
          9.1042e-01,  3.5304e-02,  3.4772e-02,  8.9665e-01,  2.6447e-01,
          4.3032e-01,  8.9469e-01,  4.6686e-01],
        [-9.2749e+00, -6.0010e+00,  1.6064e-01,  2.4939e-01,  2.6538e+00,
          2.9059e+00,  1.6532e-01,  2.3862e-01,  2.6770e+00,  2.0542e+01,
          5.6439e+00,  2.9504e+00, -1.0311e+01],
        [-5.8631e+00, -5.5932e+00, -1.0882e-01, -2.5602e-01,  1.4267e+00,
          1.6964e+00, -8.6213e-02, -1.9126e-01,  1.4872e+00,  2.0896e+01,
          5.2545e+00,  1.4174e+00, -9.1947e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4955e-03, -5.9414e-03, -4.7716e-01,
         -4.9047e-01, -6.4682e-03, -6.3262e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.0882e-01,  1.6256e+00,  3.4094e-02,  3.0485e-02,  8.6370e-01,
          8.9398e-01,  3.4826e-02,  3.4369e-02,  8.8039e-01,  2.5781e-01,
          4.1380e-01,  8.7846e-01,  4.6006e-01],
        [-6.2435e+00, -5.0774e+00, -7.9019e-02, -1.1667e-02,  1.6100e+00,
          1.6134e+00, -1.1318e-01, -9.1158e-02,  1.1232e+00,  2.1826e+01,
          5.7213e+00,  1.3367e+00, -9.6095e+00],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3077e-01, -2.6879e-01, -6.4954e-03, -5.9413e-03, -4.7716e-01,
         -4.9047e-01, -6.4681e-03, -6.3261e-03, -4.8446e-01,  7.4677e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [-2.3076e-01, -2.6878e-01, -6.4956e-03, -5.9415e-03, -4.7716e-01,
         -4.9046e-01, -6.4683e-03, -6.3263e-03, -4.8445e-01,  7.4676e-03,
         -1.3302e-01, -4.8384e-01, -1.8613e-01],
        [ 4.1923e-01,  1.6977e+00,  3.5087e-02,  3.1098e-02,  9.0167e-01,
          9.3283e-01,  3.5931e-02,  3.5276e-02,  9.1883e-01,  2.7424e-01,
          4.5388e-01,  9.1684e-01,  4.7618e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5021, -0.5021,  0.9316,  2.2107,  1.1816, -0.5021, -0.5021,  0.9149,
         1.2202, -0.5021, -0.5021, -0.5021,  0.9544], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0192,   0.0192,   3.5844,  -9.2131, -12.7066,   0.0192,   0.0192,
           3.1214, -12.3800,   0.0192,   0.0192,   0.0192,   4.3492]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  2.3638,  12.1836],
        [  5.6572,   7.4721],
        [  7.9224,   6.5803],
        [ -6.1235,   3.1910],
        [ 11.8466,   0.5913],
        [  0.8770,  -8.1352],
        [  8.6891,  -6.8360],
        [-10.5691,  10.8756],
        [  1.7568,  -0.0712],
        [-11.0991,  -4.2491],
        [ 11.4103,   5.2357],
        [ -0.1851,   2.9347],
        [ -7.5723,  -9.3505]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 6.3522, -5.1507, -2.0487, -4.3376, -9.8811, -6.7882, -6.3489,  5.9660,
        -7.1033, -1.4560, -0.7179, -3.6786, -6.0576], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.1852e+00,  1.5094e+00,  1.3526e+00, -5.4268e-01,  2.4223e+00,
          2.0759e+00,  9.2465e-01, -1.6151e+00, -2.8824e-01,  9.5287e-01,
          2.4642e-01, -1.1737e+00,  2.2713e+00],
        [-1.8651e+00, -5.6256e-01, -6.2129e-01, -4.2742e+00, -7.7059e-01,
         -3.4892e+00, -7.6918e-02,  6.7408e+00, -2.3096e-01,  1.3500e+00,
         -3.5500e+00, -6.0667e-01, -2.9353e+00],
        [ 1.1142e-01, -1.2473e+00,  8.1916e-01,  6.4910e-01, -1.6350e+01,
          2.2732e+00, -1.5779e+00,  7.2357e+00, -7.2982e-03,  4.4049e+00,
         -5.8977e-01,  1.7865e+00, -4.6946e+00],
        [-1.3960e+00, -8.7673e-02, -4.2780e-01,  6.7083e-03, -4.0893e-01,
         -9.1685e-01, -1.7927e+00, -5.0702e-01, -1.3463e-02, -5.2559e-01,
         -1.6941e+00,  1.8378e-03, -3.4962e-01],
        [-1.9520e+00,  1.8967e-01, -5.4411e-01, -1.2101e-01, -9.1077e-01,
         -1.2479e+00, -1.1267e+00, -9.1391e-01, -1.8068e-02, -3.8365e-01,
         -2.4970e+00, -1.1678e-01, -2.1655e-01],
        [-1.4057e+00, -8.6477e-02, -4.3293e-01,  1.5699e-02, -4.1127e-01,
         -9.1630e-01, -1.7894e+00, -5.0243e-01, -1.3300e-02, -5.1448e-01,
         -1.7010e+00,  1.2009e-02, -3.4096e-01],
        [ 1.3221e+01, -6.1845e-01,  1.5793e+00, -6.7508e+00, -1.5120e+01,
         -1.8714e+01, -8.9013e+00,  4.8410e+00, -1.2180e+00,  4.2963e+00,
          1.1099e+00,  1.4776e+00, -1.3629e+01],
        [ 4.1482e+00, -4.3534e-01, -7.3632e+00,  2.5918e-02, -1.7989e-01,
         -6.4858e+00, -8.9001e-01, -1.0932e+00,  1.0207e-01,  1.5231e+00,
         -7.6390e+00,  1.3168e-01, -1.0356e+01],
        [-1.4524e+00, -8.1858e-02, -4.5182e-01,  9.7058e-02, -4.0160e-01,
         -8.8229e-01, -1.8020e+00, -4.5301e-01, -1.2095e-02, -4.6125e-01,
         -1.7020e+00,  9.5630e-02, -3.0579e-01],
        [-2.8522e+00, -1.9497e-01, -1.6674e+00, -4.5101e-01, -1.9345e+00,
         -2.7298e+00,  5.5568e-01,  1.5671e+00, -1.2880e-02,  9.4602e-01,
         -2.4953e+00, -2.9629e-01, -2.6501e+00],
        [-2.4334e+01, -6.7212e-02, -4.8855e+00,  1.9330e-01, -7.7025e+00,
          2.7805e+00,  4.4028e-01, -1.4406e+00, -4.7447e-02,  6.3915e+00,
         -8.7817e+00, -4.8862e-01,  5.4980e+00],
        [-1.0961e+01,  5.4093e-01,  8.6739e-01, -6.0412e-02, -3.7470e+00,
          2.1004e+00,  2.4607e+00, -2.1072e+01, -1.4633e-01,  9.1990e+00,
         -5.3680e+00,  1.5515e-01,  4.1397e+00],
        [-1.2571e+01,  2.5529e-02, -4.2095e+00,  2.0473e-01, -1.3435e+01,
          2.3688e+00, -6.0068e-01, -5.7139e+00, -3.7530e-01,  4.5355e+00,
         -9.0221e+00,  3.5582e-02,  2.3988e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 2.9079, -2.1828, -0.1738, -3.3916, -2.9699, -3.3962, -3.0437, -0.0264,
        -3.4166, -2.8315, -0.3684,  1.2132,  0.0742], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 7.4891e-01,  4.5452e+00,  1.8769e+00,  4.5914e-02,  3.5496e-01,
         -9.1354e-02,  2.7526e-01, -9.7636e-01, -1.0948e-01,  3.3561e+00,
         -4.4478e+00, -1.2550e-01, -1.0160e+00],
        [ 5.4131e-01, -4.5533e+00, -3.9331e-01,  2.2764e-01, -3.5322e-01,
          9.4308e-02,  8.6633e-01,  7.9627e-01,  1.1223e-01, -3.0675e+00,
          5.4099e+00,  6.6530e-01,  1.5354e+00],
        [-1.0256e+01, -4.0894e-02, -1.1152e+00, -1.1287e-02, -1.0215e-02,
         -1.1185e-02, -8.9347e-01, -1.4354e-01, -1.1272e-02, -1.5607e-02,
          8.3038e-03, -6.6567e-01,  1.3450e-03],
        [-1.2086e+00, -1.8795e+00, -3.9948e+00,  3.7431e-01,  1.3870e+00,
          3.8828e-01,  9.5588e-01, -1.2326e+01,  4.2495e-01,  2.2745e+00,
          8.1236e-01,  1.8802e+00,  1.0604e+01],
        [ 9.1296e+00, -4.1802e+00, -1.1657e+01,  2.0360e-01,  4.0986e-01,
          2.0817e-01,  5.1155e+00, -6.9774e-01,  2.1607e-01, -5.1843e-01,
          6.2533e+00,  8.3596e+00,  2.6055e+00]], device='cuda:0'))])
loaded xi:  315.55313
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.5, 0.2, 0.15, 0.15]
W_T_mean: 948.1501453327259
W_T_median: 690.0201979110175
W_T_pctile_5: -223.82371268015584
W_T_CVAR_5_pct: -357.6405978823259
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -174.40453954980057
Current xi:  [328.72275]
objective value function right now is: -174.40453954980057
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [340.0464]
objective value function right now is: -173.23931576420065
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -175.79363020089974
Current xi:  [345.2679]
objective value function right now is: -175.79363020089974
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -176.3921059406449
Current xi:  [349.4294]
objective value function right now is: -176.3921059406449
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [351.1006]
objective value function right now is: -176.2535318891518
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -176.8054769946647
Current xi:  [352.83612]
objective value function right now is: -176.8054769946647
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [353.9571]
objective value function right now is: -176.67129184805472
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.9219]
objective value function right now is: -176.61066577620144
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.442]
objective value function right now is: -176.50959633924242
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.2655]
objective value function right now is: -176.12983241550322
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.74945]
objective value function right now is: -175.80547039255268
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [355.4165]
objective value function right now is: -175.94460899380496
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -176.82584273942584
Current xi:  [355.5706]
objective value function right now is: -176.82584273942584
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [355.5653]
objective value function right now is: -176.43013798448757
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [356.42313]
objective value function right now is: -175.0994621548268
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [354.89264]
objective value function right now is: -175.36199445252353
34.0% of gradient descent iterations done. Method = Adam
