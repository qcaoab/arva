/home/marcchen/Documents/constrain_factor/researchcode/exp_config_json_files/multi_portfolio_TEST1_EFs.json
Starting at: 
06-08-23_10:31

 numpy seed:  3  


 pytorch seed:  3  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_nom_ret', 'VWD_nom_ret']
############# End: defined asset  basket #################
               CPI_nom_ret  B10_nom_ret  VWD_nom_ret
My identifier                                       
192607           -0.011299     0.005383     0.031411
192608           -0.005714     0.005363     0.028647
192609            0.005747     0.005343     0.005787
192610            0.005714     0.005323    -0.028996
192611            0.005682     0.005303     0.028554
               CPI_nom_ret  B10_nom_ret  VWD_nom_ret
My identifier                                       
202208           -0.000354    -0.043289    -0.036240
202209            0.002151    -0.050056    -0.091324
202210            0.004056    -0.014968     0.077403
202211           -0.001010     0.040789     0.052365
202212           -0.003070    -0.018566    -0.057116
Indices constructed with column names:
Index(['date_for_plt', 'CPI_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
B10_real_ret    0.001637
VWD_real_ret    0.006759
dtype: float64


timeseries_basket['data_df_stdev'] = 
B10_real_ret    0.019258
VWD_real_ret    0.053610
dtype: float64


timeseries_basket['data_df_corr'] = 
              B10_real_ret  VWD_real_ret
B10_real_ret      1.000000      0.090987
VWD_real_ret      0.090987      1.000000


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 808.1706015737759
W_T_median: 592.6450607954835
W_T_pctile_5: -246.30676296883496
W_T_CVAR_5_pct: -379.12258834538136
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1650.740192937743
Current xi:  [78.98993]
objective value function right now is: -1650.740192937743
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1658.0605801027218
Current xi:  [56.67125]
objective value function right now is: -1658.0605801027218
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1664.024995156051
Current xi:  [33.604797]
objective value function right now is: -1664.024995156051
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1669.8961637216385
Current xi:  [10.342754]
objective value function right now is: -1669.8961637216385
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1674.6388479666775
Current xi:  [-9.733511]
objective value function right now is: -1674.6388479666775
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.9722877503514
Current xi:  [-31.682802]
objective value function right now is: -1678.9722877503514
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1682.8692159065324
Current xi:  [-51.59847]
objective value function right now is: -1682.8692159065324
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.8899437450577
Current xi:  [-72.753]
objective value function right now is: -1686.8899437450577
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1690.2032212009333
Current xi:  [-92.93854]
objective value function right now is: -1690.2032212009333
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1692.9883381299578
Current xi:  [-113.4252]
objective value function right now is: -1692.9883381299578
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1695.9891619171915
Current xi:  [-134.43773]
objective value function right now is: -1695.9891619171915
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1698.3886306620934
Current xi:  [-154.42296]
objective value function right now is: -1698.3886306620934
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1701.014882488931
Current xi:  [-175.31131]
objective value function right now is: -1701.014882488931
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1703.258707799882
Current xi:  [-195.4253]
objective value function right now is: -1703.258707799882
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1705.3912512847776
Current xi:  [-216.00748]
objective value function right now is: -1705.3912512847776
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1706.6829708739376
Current xi:  [-236.20161]
objective value function right now is: -1706.6829708739376
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1708.8068433480614
Current xi:  [-256.17868]
objective value function right now is: -1708.8068433480614
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1710.2049076498772
Current xi:  [-275.9887]
objective value function right now is: -1710.2049076498772
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1711.2396146968733
Current xi:  [-295.80286]
objective value function right now is: -1711.2396146968733
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1712.8433259806307
Current xi:  [-314.87723]
objective value function right now is: -1712.8433259806307
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1713.9251065637095
Current xi:  [-333.9315]
objective value function right now is: -1713.9251065637095
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1714.6114205037006
Current xi:  [-351.86014]
objective value function right now is: -1714.6114205037006
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1715.5899393740874
Current xi:  [-370.3599]
objective value function right now is: -1715.5899393740874
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1715.7312574456773
Current xi:  [-387.8518]
objective value function right now is: -1715.7312574456773
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1715.8411374519706
Current xi:  [-405.47385]
objective value function right now is: -1715.8411374519706
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1717.072787442472
Current xi:  [-422.093]
objective value function right now is: -1717.072787442472
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1717.41096554161
Current xi:  [-437.24725]
objective value function right now is: -1717.41096554161
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1717.6424679100098
Current xi:  [-452.56177]
objective value function right now is: -1717.6424679100098
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1717.9545087371105
Current xi:  [-466.34814]
objective value function right now is: -1717.9545087371105
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1717.9706802828025
Current xi:  [-479.30124]
objective value function right now is: -1717.9706802828025
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1718.057362295761
Current xi:  [-489.624]
objective value function right now is: -1718.057362295761
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-500.45065]
objective value function right now is: -1717.7916054324583
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1718.0791825469048
Current xi:  [-508.005]
objective value function right now is: -1718.0791825469048
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1718.1501180752084
Current xi:  [-515.0691]
objective value function right now is: -1718.1501180752084
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-518.6213]
objective value function right now is: -1717.9942322895886
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1718.3949534563544
Current xi:  [-518.2999]
objective value function right now is: -1718.3949534563544
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1718.4772792286485
Current xi:  [-518.5543]
objective value function right now is: -1718.4772792286485
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1718.48241861939
Current xi:  [-519.1495]
objective value function right now is: -1718.48241861939
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1718.5715915109865
Current xi:  [-519.9138]
objective value function right now is: -1718.5715915109865
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-519.8939]
objective value function right now is: -1718.4870037064472
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-520.19617]
objective value function right now is: -1718.4099181899064
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-520.459]
objective value function right now is: -1718.4609775566616
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-520.44336]
objective value function right now is: -1718.4792571886028
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-520.9553]
objective value function right now is: -1718.5164554475066
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-520.951]
objective value function right now is: -1718.569402192343
new min fval from sgd:  -1718.5717266902254
new min fval from sgd:  -1718.5717467967427
new min fval from sgd:  -1718.5787000383898
new min fval from sgd:  -1718.5802039266853
new min fval from sgd:  -1718.5825242426895
new min fval from sgd:  -1718.5840821902673
new min fval from sgd:  -1718.5842227734117
new min fval from sgd:  -1718.5856197176101
new min fval from sgd:  -1718.5907584067593
new min fval from sgd:  -1718.5962039457195
new min fval from sgd:  -1718.6018804984778
new min fval from sgd:  -1718.6052087472688
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-520.8567]
objective value function right now is: -1718.566148973722
new min fval from sgd:  -1718.6061195806524
new min fval from sgd:  -1718.6102940084977
new min fval from sgd:  -1718.6137571466431
new min fval from sgd:  -1718.6157209029748
new min fval from sgd:  -1718.6174993126085
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-521.1082]
objective value function right now is: -1718.5401603015212
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-521.06836]
objective value function right now is: -1718.5072882370143
new min fval from sgd:  -1718.6177341282596
new min fval from sgd:  -1718.6184122626307
new min fval from sgd:  -1718.6189235170957
new min fval from sgd:  -1718.6193130901843
new min fval from sgd:  -1718.6197586066587
new min fval from sgd:  -1718.6200410327497
new min fval from sgd:  -1718.6201363991156
new min fval from sgd:  -1718.6202350532953
new min fval from sgd:  -1718.6223629340227
new min fval from sgd:  -1718.6239250371257
new min fval from sgd:  -1718.624042459085
new min fval from sgd:  -1718.6243102368587
new min fval from sgd:  -1718.6250905220695
new min fval from sgd:  -1718.6251942567412
new min fval from sgd:  -1718.6253345044834
new min fval from sgd:  -1718.6256214405673
new min fval from sgd:  -1718.6260606513677
new min fval from sgd:  -1718.6261495832507
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-520.9547]
objective value function right now is: -1718.623539918851
new min fval from sgd:  -1718.62671817127
new min fval from sgd:  -1718.6273965494777
new min fval from sgd:  -1718.6279602363832
new min fval from sgd:  -1718.6282977159308
new min fval from sgd:  -1718.6283903111694
new min fval from sgd:  -1718.6284793540658
new min fval from sgd:  -1718.6286276563062
new min fval from sgd:  -1718.6288984193823
new min fval from sgd:  -1718.6290531579818
new min fval from sgd:  -1718.6292097904557
new min fval from sgd:  -1718.6294436735222
new min fval from sgd:  -1718.6295334865279
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-521.0402]
objective value function right now is: -1718.6290671413747
min fval:  -1718.6295334865279
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.4030,  1.1678],
        [-6.8287,  3.8069],
        [-5.2573,  4.9220],
        [-0.4030,  1.1677],
        [-1.5898,  3.2823],
        [11.9210,  0.5791],
        [-9.9683, -2.5887],
        [-0.4031,  1.1681],
        [-5.8509,  4.7247],
        [-0.4030,  1.1677]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-0.5880,  9.0886, 10.6310, -0.5880,  2.9584, -9.2299,  5.1571, -0.5885,
        10.3841, -0.5880], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -0.0192,  -0.0135,  -0.0283,  -0.0192,  -0.0105,  -0.1134,  -0.3819,
          -0.0192,  -0.0217,  -0.0192],
        [ -0.0192,  -0.0135,  -0.0283,  -0.0192,  -0.0105,  -0.1134,  -0.3819,
          -0.0192,  -0.0217,  -0.0192],
        [  0.0879,  -5.1375,  -7.2192,   0.0880,  -0.7425, -10.0519,   3.4787,
           0.0836,  -6.7936,   0.0880],
        [  0.1705,   1.1327,   1.3636,   0.1705,   0.4571,   2.7333,  -1.4545,
           0.1700,   1.2814,   0.1705],
        [ -0.0192,  -0.0135,  -0.0283,  -0.0192,  -0.0105,  -0.1134,  -0.3819,
          -0.0192,  -0.0217,  -0.0192],
        [ -0.0192,  -0.0135,  -0.0283,  -0.0192,  -0.0105,  -0.1134,  -0.3819,
          -0.0192,  -0.0217,  -0.0192],
        [  0.0764,   2.3801,   3.3061,   0.0765,   0.1615,   5.1495,  -1.2279,
           0.0724,   3.1476,   0.0765],
        [  0.0569,   2.7377,   3.8095,   0.0570,   0.1957,   5.7165,  -1.3894,
           0.0521,   3.6337,   0.0570],
        [ -0.0192,  -0.0135,  -0.0283,  -0.0192,  -0.0105,  -0.1134,  -0.3819,
          -0.0192,  -0.0217,  -0.0192],
        [ -0.0192,  -0.0135,  -0.0283,  -0.0192,  -0.0105,  -0.1134,  -0.3819,
          -0.0192,  -0.0217,  -0.0192]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.4521, -0.4521,  2.7107, -1.4973, -0.4521, -0.4521, -1.9096, -2.0744,
        -0.4521, -0.4521], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-2.3264e-03, -2.3265e-03, -1.3537e+01,  2.8178e+00, -2.3264e-03,
         -2.3265e-03,  5.6286e+00,  6.6825e+00, -2.3265e-03, -2.3264e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  9.6619,   1.8792],
        [-11.1275,  -9.3829],
        [ -0.7640,   2.3386],
        [-15.8987,  -2.1848],
        [  1.4464,  -3.7003],
        [ -1.3485,   7.0094],
        [-10.7953,  -3.4060],
        [-16.7390,  -7.4016],
        [-10.1062,   0.2398],
        [ 13.4483,   7.3263]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -3.8265, -10.1456,  -1.7694,   3.1299,  -2.7526,   5.3468,   0.6017,
         -0.3599,  10.2350,   3.4509], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.8805e+00, -6.3897e+00, -1.1708e+00, -1.7793e+00,  5.2160e+00,
         -3.0916e+00,  1.7544e-01,  2.0189e+00, -9.8472e+00,  6.8385e+00],
        [-8.9578e-01, -4.1011e-01, -1.6003e-01, -4.8636e-01, -8.6138e-01,
         -7.5602e-01, -4.7527e-01, -4.2523e-01, -9.2097e-01, -1.0898e+00],
        [-8.9578e-01, -4.1011e-01, -1.6003e-01, -4.8635e-01, -8.6139e-01,
         -7.5601e-01, -4.7527e-01, -4.2523e-01, -9.2096e-01, -1.0898e+00],
        [-5.0507e+00, -3.2645e+00, -1.3763e-03, -7.3646e+00, -3.0815e-02,
         -1.1596e+00,  3.8202e+00,  2.4896e+00,  1.3829e-01, -1.5157e+00],
        [ 2.6932e+00, -8.9755e-01,  3.8636e-01, -3.2866e+00,  4.3133e+00,
         -3.9535e+00, -7.2974e-01,  1.3788e-01, -7.6137e+00,  3.1699e+00],
        [-8.9578e-01, -4.1011e-01, -1.6003e-01, -4.8636e-01, -8.6138e-01,
         -7.5602e-01, -4.7527e-01, -4.2523e-01, -9.2098e-01, -1.0898e+00],
        [-1.9890e+00, -1.1498e+01, -5.3293e-01, -1.0894e+01,  3.9499e-01,
         -8.9672e+00,  1.1597e+00,  7.7891e+00,  7.1118e+00, -5.1127e+00],
        [-8.9578e-01, -4.1011e-01, -1.6003e-01, -4.8636e-01, -8.6138e-01,
         -7.5602e-01, -4.7527e-01, -4.2523e-01, -9.2098e-01, -1.0898e+00],
        [-8.9578e-01, -4.1011e-01, -1.6003e-01, -4.8636e-01, -8.6138e-01,
         -7.5602e-01, -4.7527e-01, -4.2523e-01, -9.2097e-01, -1.0898e+00],
        [ 1.3516e+00,  9.9232e-01,  1.3979e-02,  8.2047e-01,  1.8862e+00,
          9.9480e-01,  9.0109e-01,  1.0626e+00,  7.7496e-01,  1.4912e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 5.9072, -1.5580, -1.5580, -1.1519,  3.9101, -1.5580, -1.5415, -1.5580,
        -1.5580,  2.6740], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-4.0472,  0.0385,  0.0385, -2.8688, -0.4765,  0.0385, -8.7037,  0.0385,
          0.0385,  3.9330],
        [ 3.8148, -0.0385, -0.0385,  2.8672,  0.4400, -0.0385,  8.7405, -0.0385,
         -0.0385, -4.0820]], device='cuda:0'))])
xi:  [-521.0411]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 210.03301921320073
W_T_median: 72.81099024125922
W_T_pctile_5: -519.0318972543776
W_T_CVAR_5_pct: -618.5772380830412
Average q (qsum/M+1):  56.437397618447584
Optimal xi:  [-521.0411]
Expected(across Rb) median(across samples) p_equity:  0.3954512720811181
obj fun:  tensor(-1718.6295, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 808.1706015737759
W_T_median: 592.6450607954835
W_T_pctile_5: -246.30676296883496
W_T_CVAR_5_pct: -379.12258834538136
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1558.3837377383313
Current xi:  [77.14665]
objective value function right now is: -1558.3837377383313
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1577.110828577247
Current xi:  [55.00065]
objective value function right now is: -1577.110828577247
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1586.7080215418325
Current xi:  [33.31293]
objective value function right now is: -1586.7080215418325
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1596.8305370062226
Current xi:  [12.315335]
objective value function right now is: -1596.8305370062226
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.2483896622862
Current xi:  [-5.3094306]
objective value function right now is: -1604.2483896622862
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1608.9731846083946
Current xi:  [-23.077024]
objective value function right now is: -1608.9731846083946
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1615.2744312403715
Current xi:  [-41.204002]
objective value function right now is: -1615.2744312403715
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1620.4776071214094
Current xi:  [-59.52582]
objective value function right now is: -1620.4776071214094
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1624.5589921879014
Current xi:  [-78.00173]
objective value function right now is: -1624.5589921879014
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1627.6825146981298
Current xi:  [-94.70122]
objective value function right now is: -1627.6825146981298
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1630.3944522503014
Current xi:  [-114.35791]
objective value function right now is: -1630.3944522503014
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1633.292031322516
Current xi:  [-129.92558]
objective value function right now is: -1633.292031322516
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1636.1678780304817
Current xi:  [-148.30743]
objective value function right now is: -1636.1678780304817
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1637.1888135277882
Current xi:  [-165.13524]
objective value function right now is: -1637.1888135277882
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1639.119733043214
Current xi:  [-179.92378]
objective value function right now is: -1639.119733043214
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1641.0174572376466
Current xi:  [-197.72713]
objective value function right now is: -1641.0174572376466
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1642.2480026118758
Current xi:  [-211.827]
objective value function right now is: -1642.2480026118758
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1643.14264478342
Current xi:  [-223.89005]
objective value function right now is: -1643.14264478342
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1644.081070555604
Current xi:  [-237.88643]
objective value function right now is: -1644.081070555604
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1644.5310067459623
Current xi:  [-251.18117]
objective value function right now is: -1644.5310067459623
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1644.8413170308681
Current xi:  [-259.66507]
objective value function right now is: -1644.8413170308681
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1644.8966390632572
Current xi:  [-266.59793]
objective value function right now is: -1644.8966390632572
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-273.53424]
objective value function right now is: -1644.667722279388
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1645.2031204007014
Current xi:  [-280.63562]
objective value function right now is: -1645.2031204007014
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-288.4675]
objective value function right now is: -1644.9950954173169
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1645.4569739089354
Current xi:  [-292.92596]
objective value function right now is: -1645.4569739089354
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1645.5308660315523
Current xi:  [-295.67572]
objective value function right now is: -1645.5308660315523
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-296.59924]
objective value function right now is: -1645.4931580062537
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-296.68344]
objective value function right now is: -1644.8086536557994
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-297.8996]
objective value function right now is: -1645.239362909474
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-297.66602]
objective value function right now is: -1645.3790478070453
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1645.7695113429697
Current xi:  [-298.0163]
objective value function right now is: -1645.7695113429697
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-298.39453]
objective value function right now is: -1644.7952557491608
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-298.54425]
objective value function right now is: -1645.7001042025959
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-298.4974]
objective value function right now is: -1645.3250042906027
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1645.9333983331865
Current xi:  [-298.23615]
objective value function right now is: -1645.9333983331865
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1646.047051799082
Current xi:  [-297.99655]
objective value function right now is: -1646.047051799082
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-297.77405]
objective value function right now is: -1645.9903355789963
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-297.52933]
objective value function right now is: -1646.0107803445412
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1646.118337568402
Current xi:  [-297.68848]
objective value function right now is: -1646.118337568402
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-297.7767]
objective value function right now is: -1646.008290204914
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1646.1522157894908
Current xi:  [-297.72833]
objective value function right now is: -1646.1522157894908
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-297.88214]
objective value function right now is: -1646.1060885763088
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-297.99823]
objective value function right now is: -1646.0778926798243
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-297.99945]
objective value function right now is: -1646.0894814846536
new min fval from sgd:  -1646.157974042269
new min fval from sgd:  -1646.1619962505258
new min fval from sgd:  -1646.166848669528
new min fval from sgd:  -1646.168968035612
new min fval from sgd:  -1646.184212410687
new min fval from sgd:  -1646.1928165082588
new min fval from sgd:  -1646.196967606042
new min fval from sgd:  -1646.1990256747786
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-297.97958]
objective value function right now is: -1646.141350934141
new min fval from sgd:  -1646.1999497874933
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-298.03323]
objective value function right now is: -1646.1727660082424
new min fval from sgd:  -1646.2010938650615
new min fval from sgd:  -1646.2019741806762
new min fval from sgd:  -1646.2022140441672
new min fval from sgd:  -1646.2063977795808
new min fval from sgd:  -1646.2076521393415
new min fval from sgd:  -1646.2087891691258
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-297.87302]
objective value function right now is: -1646.0978152756454
new min fval from sgd:  -1646.2119305746673
new min fval from sgd:  -1646.2169958284685
new min fval from sgd:  -1646.2181048186944
new min fval from sgd:  -1646.2243054042476
new min fval from sgd:  -1646.2244648395194
new min fval from sgd:  -1646.225609677414
new min fval from sgd:  -1646.22781472833
new min fval from sgd:  -1646.2295194571204
new min fval from sgd:  -1646.230473815918
new min fval from sgd:  -1646.2307229899056
new min fval from sgd:  -1646.2311614524494
new min fval from sgd:  -1646.2315634689298
new min fval from sgd:  -1646.232807405867
new min fval from sgd:  -1646.2338721608705
new min fval from sgd:  -1646.2355652871704
new min fval from sgd:  -1646.2363781876297
new min fval from sgd:  -1646.2370839252367
new min fval from sgd:  -1646.2374960181385
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-298.0119]
objective value function right now is: -1646.2352184274437
new min fval from sgd:  -1646.2376598338012
new min fval from sgd:  -1646.237883296469
new min fval from sgd:  -1646.2386245719088
new min fval from sgd:  -1646.2393636313477
new min fval from sgd:  -1646.240151518981
new min fval from sgd:  -1646.2406502693445
new min fval from sgd:  -1646.2422520713676
new min fval from sgd:  -1646.2437608713312
new min fval from sgd:  -1646.245294960563
new min fval from sgd:  -1646.2465175139441
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-298.04807]
objective value function right now is: -1646.2357668420282
min fval:  -1646.2465175139441
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -4.9996,  -5.0207],
        [ -4.2388,  -4.3007],
        [ -4.5433,  -4.4808],
        [ -7.6690,   4.8651],
        [  0.6540,  -1.2941],
        [-11.8681,   2.8184],
        [ -4.7561,  -4.9571],
        [ -4.9647,  -5.2582],
        [ -4.8669,  -4.8321],
        [ -9.1051,   4.5727]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-3.9124, -4.4252, -3.7929,  7.1633,  0.6226,  8.4996, -4.7069, -4.9971,
        -3.7386,  7.7022], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-7.9771e-02, -2.5696e-02, -6.1177e-02, -5.1995e-03, -5.2523e-01,
         -5.5381e-02, -3.5165e-02, -3.3351e-02, -8.1786e-02, -7.2078e-03],
        [ 3.1603e+00,  1.8193e+00,  2.3516e+00, -3.4929e+00, -1.9618e+00,
         -6.3941e+00,  2.6169e+00,  3.2755e+00,  2.9384e+00, -5.2699e+00],
        [-7.9771e-02, -2.5696e-02, -6.1177e-02, -5.1995e-03, -5.2523e-01,
         -5.5381e-02, -3.5165e-02, -3.3351e-02, -8.1786e-02, -7.2078e-03],
        [-4.0626e+00, -2.5755e+00, -3.0197e+00,  5.0336e+00,  3.2254e+00,
          8.5455e+00, -4.0632e+00, -4.4698e+00, -3.4702e+00,  7.2650e+00],
        [ 2.4231e+00,  1.3637e+00,  1.7360e+00, -2.0612e+00, -1.9267e+00,
         -5.5229e+00,  1.8104e+00,  2.3296e+00,  2.1971e+00, -3.6187e+00],
        [-7.9771e-02, -2.5696e-02, -6.1177e-02, -5.1995e-03, -5.2523e-01,
         -5.5381e-02, -3.5165e-02, -3.3351e-02, -8.1786e-02, -7.2078e-03],
        [-7.9772e-02, -2.5697e-02, -6.1178e-02, -5.1996e-03, -5.2524e-01,
         -5.5381e-02, -3.5166e-02, -3.3351e-02, -8.1786e-02, -7.2079e-03],
        [ 4.6490e-01,  2.4811e-01,  3.4819e-01, -4.3803e-01, -1.7183e+00,
         -1.5856e+00,  3.0376e-01,  3.3930e-01,  4.4267e-01, -6.6522e-01],
        [-2.9249e+00, -1.6287e+00, -2.1283e+00,  1.9328e+00,  1.9482e+00,
          5.4256e+00, -2.3820e+00, -2.7770e+00, -2.7905e+00,  3.6233e+00],
        [-7.9771e-02, -2.5696e-02, -6.1177e-02, -5.1995e-03, -5.2523e-01,
         -5.5381e-02, -3.5165e-02, -3.3351e-02, -8.1786e-02, -7.2078e-03]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5717,  0.5510, -0.5717, -1.5421,  0.4680, -0.5717, -0.5717, -0.4780,
        -1.0344, -0.5717], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 8.8450e-03, -6.7529e+00,  8.8450e-03,  9.9278e+00, -5.2717e+00,
          8.8450e-03,  8.8452e-03, -1.3970e+00,  5.5192e+00,  8.8451e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -7.8040,   0.9557],
        [  4.2154,  -4.9768],
        [ -1.2096, -10.2845],
        [-11.4141,   2.2034],
        [ -1.5276,  -0.6808],
        [ 17.3931,   8.0022],
        [-12.9890,  -3.6409],
        [  2.5210,   1.6282],
        [  8.6280,  -1.8883],
        [-13.0428,  -9.4733]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  1.2134,  -8.9485,  -8.4620,   7.4546,  -5.0023,   4.8641,   0.1623,
         -3.5691, -11.6727,  -9.3435], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -1.0473,  -0.1637,  -0.1333,  -0.3498,   1.0516,  -1.3947,  -2.2530,
          -5.1456,  -1.9083,   0.8584],
        [ -0.5791,  -0.3085,  -0.3350,  -7.7368,  -1.6411,  -3.2682,  -0.2366,
           2.4683,  -0.8855,  -1.1108],
        [  1.1443, -10.6033,  -5.3983,   1.3473,  -3.3343,  -2.9763,   1.2052,
          -1.2757,  -6.0244,  16.8958],
        [  5.3134,  -1.7129,   2.3942,  -0.3185,  -0.8934,  -8.4109,   0.4457,
          -0.0817,  -0.7418,  -3.3505],
        [ -3.2884,  -0.2240,  -0.0646,   0.2067,   1.1662,   0.2187,  -2.2144,
          -8.3054,  -1.9684,   1.7396],
        [  1.9452,   0.9135,  -0.2746,  -6.8251,  -4.8353,  -8.7382,   4.8302,
           2.1597,  -8.7561,  -0.1280],
        [ -0.8270,  -1.3333,   0.5282,  -2.0347,  -1.9498,  -4.9840,  -0.7622,
           4.4081,   0.2493,  -2.5716],
        [ -0.2774,  -0.2134,  -0.1439,  -0.7914,   0.4186,  -1.0172,  -1.4560,
          -2.7989,  -1.4518,   0.9198],
        [  0.5797,  -4.6976,   5.6426,   2.7800,   4.1874,  -2.5266,  -1.4811,
          -4.8494,  -0.3257,  -4.4556],
        [ -0.0537,  -0.3142,  -0.1904,  -0.9512,   0.0488,  -1.7595,  -0.9473,
          -0.5197,  -0.9891,   0.1335]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.0844,  0.1764, -1.0475, -2.0143, -2.4441,  3.0121,  0.2190, -3.3948,
        -3.4377, -3.5541], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -2.5345,  -2.8745,  12.8243,  -5.0996,  -3.3384,  -5.7047,  -2.8315,
          -1.9167,  -7.9285,  -0.8136],
        [  2.5497,   2.8446, -12.8315,   5.0998,   3.3360,   5.7693,   2.8386,
           1.9110,   8.0046,   0.8158]], device='cuda:0'))])
xi:  [-298.042]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 263.8105281086307
W_T_median: 60.408439624657284
W_T_pctile_5: -297.95177326123365
W_T_CVAR_5_pct: -386.58367165876064
Average q (qsum/M+1):  55.598810011340724
Optimal xi:  [-298.042]
Expected(across Rb) median(across samples) p_equity:  0.41569748064503076
obj fun:  tensor(-1646.2465, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 808.1706015737759
W_T_median: 592.6450607954835
W_T_pctile_5: -246.30676296883496
W_T_CVAR_5_pct: -379.12258834538136
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1480.321044927252
Current xi:  [78.509995]
objective value function right now is: -1480.321044927252
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1499.014306269779
Current xi:  [57.45797]
objective value function right now is: -1499.014306269779
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1506.0973128786245
Current xi:  [37.73802]
objective value function right now is: -1506.0973128786245
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1512.7031684110748
Current xi:  [19.680729]
objective value function right now is: -1512.7031684110748
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1521.2355567093632
Current xi:  [3.6115532]
objective value function right now is: -1521.2355567093632
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1523.802345081325
Current xi:  [-5.017417]
objective value function right now is: -1523.802345081325
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1524.8257799191804
Current xi:  [-13.366145]
objective value function right now is: -1524.8257799191804
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1527.740742598746
Current xi:  [-28.182959]
objective value function right now is: -1527.740742598746
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1531.3475570734945
Current xi:  [-37.49915]
objective value function right now is: -1531.3475570734945
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1532.0934268400688
Current xi:  [-42.450302]
objective value function right now is: -1532.0934268400688
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1533.350315170978
Current xi:  [-52.09319]
objective value function right now is: -1533.350315170978
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1535.7751374959967
Current xi:  [-65.61388]
objective value function right now is: -1535.7751374959967
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1551.5242046437413
Current xi:  [-77.791374]
objective value function right now is: -1551.5242046437413
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1551.9042487937934
Current xi:  [-81.33822]
objective value function right now is: -1551.9042487937934
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1552.6787840801492
Current xi:  [-84.70925]
objective value function right now is: -1552.6787840801492
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-103.99128]
objective value function right now is: -1551.7697357843788
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1553.4856150379028
Current xi:  [-119.68677]
objective value function right now is: -1553.4856150379028
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-125.44282]
objective value function right now is: -1551.9709274060397
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1554.9755147618457
Current xi:  [-128.83203]
objective value function right now is: -1554.9755147618457
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-131.97093]
objective value function right now is: -1554.8090793505717
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.3882384489286
Current xi:  [-137.36238]
objective value function right now is: -1555.3882384489286
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-143.15222]
objective value function right now is: -1548.790662348608
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.889338569444
Current xi:  [-147.87679]
objective value function right now is: -1555.889338569444
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-152.96017]
objective value function right now is: -1555.5127850725287
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1556.244520011258
Current xi:  [-157.75592]
objective value function right now is: -1556.244520011258
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-159.75084]
objective value function right now is: -1554.2351239195732
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-161.1008]
objective value function right now is: -1556.1614232443385
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-161.29543]
objective value function right now is: -1555.892203216341
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-161.30519]
objective value function right now is: -1555.567276721665
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-161.5969]
objective value function right now is: -1555.1893385921742
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-161.22841]
objective value function right now is: -1555.8828496524193
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-161.07034]
objective value function right now is: -1555.5598865323507
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-161.3371]
objective value function right now is: -1555.2958447994874
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-160.70923]
objective value function right now is: -1555.68591753969
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-161.28609]
objective value function right now is: -1555.5986849815185
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1556.7061949773515
Current xi:  [-160.91624]
objective value function right now is: -1556.7061949773515
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1556.9479600135862
Current xi:  [-160.53996]
objective value function right now is: -1556.9479600135862
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-160.34273]
objective value function right now is: -1556.632579874463
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-160.1129]
objective value function right now is: -1556.7866918989848
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-159.77072]
objective value function right now is: -1556.9435899532805
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-159.42857]
objective value function right now is: -1556.8810784524658
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-159.44727]
objective value function right now is: -1556.8229493947076
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-159.29364]
objective value function right now is: -1556.865718697513
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-158.77023]
objective value function right now is: -1556.9284999254364
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-158.57826]
objective value function right now is: -1556.7975400047687
new min fval from sgd:  -1556.949888311392
new min fval from sgd:  -1556.960209246744
new min fval from sgd:  -1556.9703817117656
new min fval from sgd:  -1556.978773212014
new min fval from sgd:  -1556.9861657377617
new min fval from sgd:  -1556.9885427167744
new min fval from sgd:  -1556.9959997030462
new min fval from sgd:  -1557.0021203311576
new min fval from sgd:  -1557.0100828349455
new min fval from sgd:  -1557.022359984328
new min fval from sgd:  -1557.0349900706922
new min fval from sgd:  -1557.049680685036
new min fval from sgd:  -1557.0577759393852
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-158.79901]
objective value function right now is: -1556.9073050295967
new min fval from sgd:  -1557.0699594180699
new min fval from sgd:  -1557.0759151410111
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-158.67163]
objective value function right now is: -1556.9736756501966
new min fval from sgd:  -1557.075959063938
new min fval from sgd:  -1557.0774684641217
new min fval from sgd:  -1557.0813841812312
new min fval from sgd:  -1557.0870607564545
new min fval from sgd:  -1557.0876355337823
new min fval from sgd:  -1557.088675679183
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-158.46828]
objective value function right now is: -1556.9859125158573
new min fval from sgd:  -1557.088978420218
new min fval from sgd:  -1557.0914379275277
new min fval from sgd:  -1557.0933640600033
new min fval from sgd:  -1557.0957652268178
new min fval from sgd:  -1557.098247950673
new min fval from sgd:  -1557.0996397829117
new min fval from sgd:  -1557.1003510425435
new min fval from sgd:  -1557.1005759692744
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-158.49304]
objective value function right now is: -1557.0791331113553
new min fval from sgd:  -1557.102601925726
new min fval from sgd:  -1557.1065874939832
new min fval from sgd:  -1557.1090319843138
new min fval from sgd:  -1557.109664411454
new min fval from sgd:  -1557.1100632076043
new min fval from sgd:  -1557.1112381571118
new min fval from sgd:  -1557.1119970863597
new min fval from sgd:  -1557.1134709632531
new min fval from sgd:  -1557.1155958716301
new min fval from sgd:  -1557.1169168791241
new min fval from sgd:  -1557.117880814396
new min fval from sgd:  -1557.1181229362503
new min fval from sgd:  -1557.1183298507326
new min fval from sgd:  -1557.11927497887
new min fval from sgd:  -1557.1195431467495
new min fval from sgd:  -1557.1201294647847
new min fval from sgd:  -1557.1217379391092
new min fval from sgd:  -1557.122539154816
new min fval from sgd:  -1557.1229766762794
new min fval from sgd:  -1557.1229768856444
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-158.4159]
objective value function right now is: -1557.0516264898358
min fval:  -1557.1229768856444
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-6.0617, -7.7444],
        [ 3.4359, -2.3887],
        [ 5.4774, -4.7696],
        [-0.5025,  1.1031],
        [ 5.4503, -5.9474],
        [ 4.2698, -5.4601],
        [-0.5028,  1.1037],
        [10.6503,  4.2752],
        [ 2.6495,  7.4435],
        [-5.1311,  6.5724]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-7.4821, -6.5593, -7.0787, -1.2810, -7.7782, -6.1220, -1.2816, -7.4998,
         6.6360,  7.3801], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.1490, -0.0189, -0.2497, -0.0287, -0.3052, -0.4634, -0.0287, -0.0713,
         -0.4426, -0.2562],
        [-0.1490, -0.0189, -0.2497, -0.0287, -0.3052, -0.4634, -0.0287, -0.0713,
         -0.4426, -0.2562],
        [-5.4818, -1.5415, -4.5402, -0.0589, -5.3482, -5.3686, -0.0593, -9.3721,
          6.7155,  6.4930],
        [-0.1490, -0.0189, -0.2497, -0.0287, -0.3052, -0.4634, -0.0287, -0.0713,
         -0.4426, -0.2562],
        [-0.1490, -0.0189, -0.2497, -0.0287, -0.3052, -0.4634, -0.0287, -0.0713,
         -0.4426, -0.2562],
        [ 2.9447,  0.9029,  3.1125, -0.0682,  4.3250,  4.6866, -0.0686,  4.0737,
         -6.8952, -7.8005],
        [ 2.8787,  0.9172,  3.1289, -0.0763,  4.2288,  4.3971, -0.0766,  3.8901,
         -6.8852, -7.9066],
        [-0.1490, -0.0189, -0.2497, -0.0287, -0.3052, -0.4634, -0.0287, -0.0713,
         -0.4426, -0.2562],
        [-0.1490, -0.0189, -0.2497, -0.0287, -0.3052, -0.4634, -0.0287, -0.0713,
         -0.4426, -0.2562],
        [-0.1490, -0.0189, -0.2497, -0.0287, -0.3052, -0.4634, -0.0287, -0.0713,
         -0.4426, -0.2562]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8330, -0.8330,  1.3707, -0.8330, -0.8330, -2.5553, -2.2914, -0.8330,
        -0.8330, -0.8330], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 6.4461e-03,  6.4461e-03,  1.3152e+01,  6.4461e-03,  6.4461e-03,
         -7.1134e+00, -7.0085e+00,  6.4461e-03,  6.4461e-03,  6.4461e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -9.2653,   0.2854],
        [  6.6556,  11.9399],
        [  6.8846,   6.0612],
        [-11.3674,  -2.4302],
        [  1.7267,  13.7816],
        [ -9.3475,   9.3309],
        [-17.8471,  -3.1645],
        [ -7.1805,   9.3478],
        [ -2.1959,  -0.7606],
        [-11.7675,  -2.1296]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 8.0573, 11.5150,  5.8297,  1.9729, 14.7190,  7.6001, -0.2100,  9.5073,
        -5.6134,  1.0843], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 4.9067e+00, -7.3150e+00, -4.8458e+00,  4.4848e+00, -1.2756e+01,
         -7.6073e-01,  4.5962e+00, -3.2681e+00,  3.4907e-02,  5.3308e+00],
        [-1.0255e+00, -3.7538e-01, -1.2298e+00, -6.4848e-01, -5.7876e-02,
          1.2164e-02, -3.8130e-01,  8.3243e-03, -3.8649e-01, -4.2283e-01],
        [-1.0255e+00, -3.7538e-01, -1.2298e+00, -6.4848e-01, -5.7876e-02,
          1.2164e-02, -3.8130e-01,  8.3231e-03, -3.8649e-01, -4.2283e-01],
        [-8.0891e+00,  1.4618e+00, -8.9143e-01, -8.6670e-01,  2.3714e+00,
          6.7124e-01, -6.6993e-01,  2.3478e+00, -6.9857e-01, -7.3322e-01],
        [ 5.7364e+00,  4.6798e-02, -4.2270e+00,  9.0069e-01,  1.9912e-01,
          2.7094e+00,  4.5314e+00,  2.0399e+00, -6.1616e+00,  2.0210e+00],
        [-1.0259e+00, -3.7465e-01, -1.2316e+00, -6.4918e-01, -5.7399e-02,
          1.2118e-02, -3.8135e-01,  8.0234e-03, -3.8656e-01, -4.2295e-01],
        [-5.7047e+00,  3.7630e-02, -8.5234e-01, -8.3230e+00, -1.2478e+00,
         -2.1970e-01, -2.2774e-02, -5.1309e+00,  7.4497e-02, -4.4269e+00],
        [-1.2145e+00,  6.9862e-01, -8.3962e-01, -3.5437e+00,  2.0680e+00,
         -4.4853e+00, -1.5962e-03,  2.2971e+00, -1.0998e-01, -1.4903e+00],
        [ 5.0768e+00, -8.4670e+00, -1.5051e+01,  3.5024e+00, -3.9894e+00,
         -1.1392e+00,  1.4380e+01, -6.9578e-01, -8.2442e-02,  5.4505e+00],
        [ 2.0979e+00,  5.1322e+00, -3.7124e+00,  3.2651e+00,  6.2768e+00,
         -1.3677e+01,  2.5434e+00, -1.8964e+00,  1.0269e+01,  2.5233e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -4.8879,  -4.0039,  -4.0039,  -5.0817,  -6.1701,  -4.0024,  -1.4440,
         -5.5791,  -9.3941, -12.2752], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-5.1494e+00, -1.2198e-02, -1.5256e-02, -1.7307e+00,  8.3421e-01,
         -8.9664e-03,  4.1743e+00, -1.6764e+00,  9.2457e+00, -4.5744e+00],
        [ 4.9032e+00,  1.5339e-02,  1.2280e-02,  1.6967e+00, -6.7148e-01,
          1.8311e-02, -4.3855e+00,  1.6762e+00, -9.4095e+00,  4.3752e+00]],
       device='cuda:0'))])
xi:  [-158.46225]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 323.9477735012807
W_T_median: 77.38607335932414
W_T_pctile_5: -158.41340850909887
W_T_CVAR_5_pct: -249.21979483076495
Average q (qsum/M+1):  54.24944083921371
Optimal xi:  [-158.46225]
Expected(across Rb) median(across samples) p_equity:  0.42788159449895224
obj fun:  tensor(-1557.1230, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 808.1706015737759
W_T_median: 592.6450607954835
W_T_pctile_5: -246.30676296883496
W_T_CVAR_5_pct: -379.12258834538136
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1340.7863062221982
Current xi:  [86.65036]
objective value function right now is: -1340.7863062221982
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1406.938135159563
Current xi:  [65.811844]
objective value function right now is: -1406.938135159563
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1416.828044594089
Current xi:  [46.681717]
objective value function right now is: -1416.828044594089
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1423.5688943009786
Current xi:  [30.597801]
objective value function right now is: -1423.5688943009786
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1430.3422459275919
Current xi:  [16.671953]
objective value function right now is: -1430.3422459275919
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1434.5146542320065
Current xi:  [4.787684]
objective value function right now is: -1434.5146542320065
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1436.1591657723816
Current xi:  [-1.8445115]
objective value function right now is: -1436.1591657723816
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1437.9874848738266
Current xi:  [-3.176017]
objective value function right now is: -1437.9874848738266
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1438.857462587277
Current xi:  [-4.649964]
objective value function right now is: -1438.857462587277
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.415815]
objective value function right now is: -1437.09450290281
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.075641]
objective value function right now is: -1438.4930650040671
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1439.1485181503233
Current xi:  [-10.95543]
objective value function right now is: -1439.1485181503233
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.320776]
objective value function right now is: -1438.2468059438688
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-20.761967]
objective value function right now is: -1436.82260711704
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1439.7093481707084
Current xi:  [-26.029715]
objective value function right now is: -1439.7093481707084
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1442.0764818804828
Current xi:  [-33.27268]
objective value function right now is: -1442.0764818804828
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1442.1515628512154
Current xi:  [-34.863422]
objective value function right now is: -1442.1515628512154
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.935364]
objective value function right now is: -1441.426779840839
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1442.3049783235417
Current xi:  [-34.963577]
objective value function right now is: -1442.3049783235417
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1442.4913062960627
Current xi:  [-35.001305]
objective value function right now is: -1442.4913062960627
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.085888]
objective value function right now is: -1441.899258803575
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1464.5262306356929
Current xi:  [-35.02387]
objective value function right now is: -1464.5262306356929
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.054623]
objective value function right now is: -1463.7857728476995
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.996365]
objective value function right now is: -1464.4027318125623
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1466.562354989373
Current xi:  [-35.021812]
objective value function right now is: -1466.562354989373
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.09379]
objective value function right now is: -1465.3903787735305
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1466.731151248717
Current xi:  [-35.05258]
objective value function right now is: -1466.731151248717
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-34.960056]
objective value function right now is: -1464.3649425352967
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-35.103016]
objective value function right now is: -1466.165040110072
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1466.8469951072448
Current xi:  [-35.114395]
objective value function right now is: -1466.8469951072448
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.135563]
objective value function right now is: -1465.5919260364228
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1467.4054377790733
Current xi:  [-35.048477]
objective value function right now is: -1467.4054377790733
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.0455]
objective value function right now is: -1466.210540696339
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.133396]
objective value function right now is: -1465.999249800478
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.04363]
objective value function right now is: -1467.2062226790508
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1468.5287317141638
Current xi:  [-34.98875]
objective value function right now is: -1468.5287317141638
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.02164]
objective value function right now is: -1468.3452258492514
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.994946]
objective value function right now is: -1468.480678647557
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.99562]
objective value function right now is: -1468.385903341433
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.001225]
objective value function right now is: -1467.6843085252033
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1468.5721259117452
Current xi:  [-35.02172]
objective value function right now is: -1468.5721259117452
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.977345]
objective value function right now is: -1468.4687734632043
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1468.8258836108832
Current xi:  [-34.98416]
objective value function right now is: -1468.8258836108832
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.027725]
objective value function right now is: -1468.6057093514803
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.996845]
objective value function right now is: -1468.6465877538358
new min fval from sgd:  -1468.8331627496505
new min fval from sgd:  -1468.8564765178771
new min fval from sgd:  -1468.875767451439
new min fval from sgd:  -1468.8844680057093
new min fval from sgd:  -1468.8936760225608
new min fval from sgd:  -1468.895847433395
new min fval from sgd:  -1468.9006828862832
new min fval from sgd:  -1468.902841557186
new min fval from sgd:  -1468.903539937183
new min fval from sgd:  -1468.907941807879
new min fval from sgd:  -1468.9105517519783
new min fval from sgd:  -1468.9235368722393
new min fval from sgd:  -1468.9629594861997
new min fval from sgd:  -1468.9843128410077
new min fval from sgd:  -1468.9965042842325
new min fval from sgd:  -1469.0063899465972
new min fval from sgd:  -1469.0093299941188
new min fval from sgd:  -1469.0104115817194
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.99578]
objective value function right now is: -1468.727112181054
new min fval from sgd:  -1469.018025728919
new min fval from sgd:  -1469.024448821386
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.02785]
objective value function right now is: -1468.6878827529788
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.98435]
objective value function right now is: -1468.883922319899
new min fval from sgd:  -1469.0261275614832
new min fval from sgd:  -1469.028546035023
new min fval from sgd:  -1469.0292854728634
new min fval from sgd:  -1469.0295743818172
new min fval from sgd:  -1469.0336113824928
new min fval from sgd:  -1469.0370829123021
new min fval from sgd:  -1469.0393535523517
new min fval from sgd:  -1469.0415230060107
new min fval from sgd:  -1469.0444983445047
new min fval from sgd:  -1469.0489191633537
new min fval from sgd:  -1469.052072443125
new min fval from sgd:  -1469.0542161028832
new min fval from sgd:  -1469.0556547221215
new min fval from sgd:  -1469.0568069650108
new min fval from sgd:  -1469.0576820541742
new min fval from sgd:  -1469.0581454167707
new min fval from sgd:  -1469.0587197711284
new min fval from sgd:  -1469.059340452394
new min fval from sgd:  -1469.0594517225707
new min fval from sgd:  -1469.0596559356702
new min fval from sgd:  -1469.060726681646
new min fval from sgd:  -1469.0617777938
new min fval from sgd:  -1469.0623724899483
new min fval from sgd:  -1469.0639242620346
new min fval from sgd:  -1469.064304009582
new min fval from sgd:  -1469.0646436113552
new min fval from sgd:  -1469.0657728830192
new min fval from sgd:  -1469.0671060174147
new min fval from sgd:  -1469.0673965903245
new min fval from sgd:  -1469.067615157666
new min fval from sgd:  -1469.0684909333509
new min fval from sgd:  -1469.0695901872436
new min fval from sgd:  -1469.0705793720535
new min fval from sgd:  -1469.0713140408589
new min fval from sgd:  -1469.0718367283591
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.986996]
objective value function right now is: -1469.0649263891282
new min fval from sgd:  -1469.0723034668263
new min fval from sgd:  -1469.0740808021646
new min fval from sgd:  -1469.0763291644676
new min fval from sgd:  -1469.0790468262198
new min fval from sgd:  -1469.0816094532404
new min fval from sgd:  -1469.0842119709887
new min fval from sgd:  -1469.0862292600273
new min fval from sgd:  -1469.0872012688133
new min fval from sgd:  -1469.0892684984813
new min fval from sgd:  -1469.090722402854
new min fval from sgd:  -1469.0912435009047
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.989803]
objective value function right now is: -1469.0657148116168
min fval:  -1469.0912435009047
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -9.0259,   1.6200],
        [  9.0716,   0.0578],
        [  7.1793,  -6.1605],
        [  7.5381,   3.2505],
        [  9.6693,  -5.0357],
        [ -1.3449,   0.3001],
        [  1.4948,  -2.5906],
        [ -6.5573, -10.0162],
        [  4.3191,  -8.2380],
        [  5.3129,  -8.5238]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 7.3420, -8.4146, -6.0056, -8.9126, -4.1491, -2.2581, -5.7163, -7.4441,
        -7.8151, -8.4529], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-9.2020e+00,  5.4065e+00,  2.8772e+00,  4.8714e+00,  1.6951e+00,
          9.5674e-02,  2.6977e-01,  6.2889e+00,  6.2490e+00,  3.6956e+00],
        [ 1.0659e+01, -5.1375e+00, -5.5630e+00, -5.5762e+00, -4.6171e+00,
          1.2856e-01, -1.1658e+00, -6.2914e+00, -7.4374e+00, -5.2700e+00],
        [-5.2413e-01, -1.7724e-01, -5.6925e-01, -4.3312e-02, -8.8777e-01,
         -1.0492e-02, -8.0194e-03,  1.3355e-01, -1.9131e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-8.7807e+00,  4.7062e+00,  2.8038e+00,  3.8880e+00,  1.8033e+00,
          1.4201e-01,  1.9636e-01,  6.0533e+00,  5.7920e+00,  3.4498e+00],
        [-8.5582e+00,  4.6390e+00,  2.6593e+00,  3.6119e+00,  1.8101e+00,
          1.4993e-01,  1.7458e-01,  5.8599e+00,  5.7528e+00,  3.3968e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.9148, -0.9148, -0.9148, -0.9148, -3.4273,  4.5581, -0.9151, -0.9148,
        -3.4455, -3.4434], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 6.4029e-03,  6.4029e-03,  6.4029e-03,  6.4029e-03, -6.4204e+00,
          1.6896e+01,  6.4026e-03,  6.4029e-03, -5.7940e+00, -5.5530e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  7.7801,   8.5117],
        [-12.8039,  -3.3197],
        [-14.0960,  -3.3781],
        [-10.3438,  13.1059],
        [ -6.7274,   3.2029],
        [ 11.3494,  -0.2092],
        [ 10.9206,   1.8935],
        [  6.0962,  -1.4135],
        [ -0.2963,  10.7227],
        [-11.1179,  -4.1613]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.3157,  -1.9104,  -2.0167,  11.5895,   3.0902, -10.3992,  -3.2258,
        -11.9670,   9.7899,  -0.1907], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.1204e+00, -4.8217e-01, -1.2984e-01, -3.9602e+00,  2.8329e+00,
          4.0120e+00, -1.4919e-01,  2.1667e+00, -5.0218e-01, -1.3871e+01],
        [-1.3321e+00, -5.5194e-01, -4.4665e-01,  2.4663e+00, -5.7744e-01,
          8.3623e-01, -2.1840e+00,  1.8294e-02,  1.2065e+00, -2.0505e+00],
        [ 7.6812e-01,  3.7889e+00,  4.6433e+00, -6.0243e+00,  2.2376e-01,
         -6.6894e+00, -4.5433e+00, -4.5007e-02,  5.9901e+00, -4.0033e-01],
        [ 8.0559e-01, -3.0883e+00, -2.2335e+00,  6.0347e+00,  3.1022e+00,
         -1.0734e+01, -5.7229e+00, -2.3178e-02,  3.6155e+00, -7.1234e-01],
        [-1.8985e+01,  7.2784e+00,  1.0297e+01, -4.8164e+00,  8.6606e-01,
         -5.7802e+00, -6.1329e+00, -2.3282e+00, -4.0583e+00,  3.9973e+00],
        [-8.3172e+00,  7.8510e+00,  8.0024e+00, -1.6984e+01, -2.4242e+00,
         -8.1970e+00, -4.3623e+00, -4.1766e+00, -1.4006e+01,  5.7935e+00],
        [-1.0779e-01,  4.5688e-01,  3.0178e-01,  2.5827e+00,  2.1453e+00,
         -5.3619e+00, -2.7556e+00,  4.3528e-03,  5.2351e-01,  8.5108e-01],
        [-1.6100e+00,  3.8398e+00,  3.2810e+00, -1.0011e+01, -1.5281e+00,
         -4.8466e+00, -2.2404e+00,  6.6782e-01, -1.1001e+01,  8.8575e-01],
        [ 3.7104e+00,  7.5672e+00,  8.9019e+00, -1.6178e+01, -8.3096e-01,
         -1.7622e+00, -1.5875e+00,  5.6997e-01, -9.7621e-02, -8.4607e-01],
        [-1.1953e+01,  4.3930e+00,  6.1183e+00, -5.1416e+00, -3.5599e-01,
         -4.5375e+00, -4.5225e+00, -1.8014e+00, -2.6516e+00,  3.8139e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.8622, -4.0911, -5.7102, -5.2697, -4.8399,  0.2404, -5.4978, -0.0417,
        -4.6828, -4.0470], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  3.1935,  -1.0137,  -2.4010,   0.3391,  11.2912,  -6.6207,   1.0781,
          -3.5835,   2.2661,   6.5807],
        [ -2.8140,   1.0718,   2.4322,  -0.5105, -11.2830,   6.6898,  -0.9991,
           3.6558,  -2.4912,  -6.5667]], device='cuda:0'))])
xi:  [-34.98991]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 415.16460932429413
W_T_median: 153.47452423743022
W_T_pctile_5: -34.99018535615084
W_T_CVAR_5_pct: -151.07944489314087
Average q (qsum/M+1):  52.26355768019153
Optimal xi:  [-34.98991]
Expected(across Rb) median(across samples) p_equity:  0.37185418729980785
obj fun:  tensor(-1469.0912, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 808.1706015737759
W_T_median: 592.6450607954835
W_T_pctile_5: -246.30676296883496
W_T_CVAR_5_pct: -379.12258834538136
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1390.5328477888213
Current xi:  [-34.945145]
objective value function right now is: -1390.5328477888213
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.04576]
objective value function right now is: -1388.564704700282
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1394.0518807456954
Current xi:  [-35.03454]
objective value function right now is: -1394.0518807456954
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1394.244807901565
Current xi:  [-34.91796]
objective value function right now is: -1394.244807901565
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.005688]
objective value function right now is: -1393.6119740006798
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.942463]
objective value function right now is: -1386.9373925282528
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-34.926853]
objective value function right now is: -1393.0887345622173
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1394.4199689474456
Current xi:  [-34.957657]
objective value function right now is: -1394.4199689474456
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.05617]
objective value function right now is: -1391.9560398367157
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.995647]
objective value function right now is: -1393.4629140817497
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.9243]
objective value function right now is: -1394.1851849415132
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.97849]
objective value function right now is: -1392.879826524581
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1395.112949828333
Current xi:  [-34.940887]
objective value function right now is: -1395.112949828333
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-35.00127]
objective value function right now is: -1393.8734044021749
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.946564]
objective value function right now is: -1392.4681844517245
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.03237]
objective value function right now is: -1391.5175353271784
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.98175]
objective value function right now is: -1391.7634357974157
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.019424]
objective value function right now is: -1392.9791322212977
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.02052]
objective value function right now is: -1394.6070196613289
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.23537]
objective value function right now is: -1394.1501683229437
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.98553]
objective value function right now is: -1393.168268256703
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.10154]
objective value function right now is: -1392.3997361883103
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.07498]
objective value function right now is: -1394.54944578021
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.039665]
objective value function right now is: -1392.7012011345769
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.99208]
objective value function right now is: -1394.492205879814
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.974937]
objective value function right now is: -1395.082465699336
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.94783]
objective value function right now is: -1392.7867544749888
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-35.037415]
objective value function right now is: -1394.6352292223398
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-35.038383]
objective value function right now is: -1392.637947200803
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.044506]
objective value function right now is: -1392.5043317713942
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.218464]
objective value function right now is: -1394.6801701330087
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.9162]
objective value function right now is: -1391.1165061482654
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.04]
objective value function right now is: -1392.9878045121236
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.03904]
objective value function right now is: -1393.1209138656902
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.072025]
objective value function right now is: -1379.516623134546
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.992794]
objective value function right now is: -1393.2767245740238
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.984077]
objective value function right now is: -1394.4675175122247
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.987488]
objective value function right now is: -1394.5775049384301
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1395.3084346706114
Current xi:  [-35.00308]
objective value function right now is: -1395.3084346706114
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1395.4244846045115
Current xi:  [-34.986908]
objective value function right now is: -1395.4244846045115
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1395.5446020441736
Current xi:  [-34.9874]
objective value function right now is: -1395.5446020441736
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1395.715018885593
Current xi:  [-34.98619]
objective value function right now is: -1395.715018885593
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.01386]
objective value function right now is: -1394.9284434142155
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1396.1493292823693
Current xi:  [-34.995876]
objective value function right now is: -1396.1493292823693
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.978218]
objective value function right now is: -1395.921740404571
new min fval from sgd:  -1396.1547092075707
new min fval from sgd:  -1396.1961530067551
new min fval from sgd:  -1396.2351259859106
new min fval from sgd:  -1396.2550743290415
new min fval from sgd:  -1396.2706517956317
new min fval from sgd:  -1396.27208577616
new min fval from sgd:  -1396.2739567414806
new min fval from sgd:  -1396.2773093472842
new min fval from sgd:  -1396.2792020980512
new min fval from sgd:  -1396.2859268798297
new min fval from sgd:  -1396.2965546911732
new min fval from sgd:  -1396.3021266827252
new min fval from sgd:  -1396.3021375272479
new min fval from sgd:  -1396.3070502721525
new min fval from sgd:  -1396.3080612068247
new min fval from sgd:  -1396.3238127702502
new min fval from sgd:  -1396.3466041774607
new min fval from sgd:  -1396.3525590320112
new min fval from sgd:  -1396.3538431572094
new min fval from sgd:  -1396.3545758484895
new min fval from sgd:  -1396.3595335521852
new min fval from sgd:  -1396.3678353171053
new min fval from sgd:  -1396.3735403282085
new min fval from sgd:  -1396.3770635479707
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.992653]
objective value function right now is: -1395.9969155539127
new min fval from sgd:  -1396.3895823685477
new min fval from sgd:  -1396.4065406828943
new min fval from sgd:  -1396.418013112828
new min fval from sgd:  -1396.4257133394408
new min fval from sgd:  -1396.4335014682144
new min fval from sgd:  -1396.438496478432
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.98332]
objective value function right now is: -1396.280852195843
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.99863]
objective value function right now is: -1396.192271232154
new min fval from sgd:  -1396.4447608621442
new min fval from sgd:  -1396.4499234457585
new min fval from sgd:  -1396.4562377765114
new min fval from sgd:  -1396.4621752543046
new min fval from sgd:  -1396.4666473671073
new min fval from sgd:  -1396.4705963657784
new min fval from sgd:  -1396.4726724895595
new min fval from sgd:  -1396.47363621192
new min fval from sgd:  -1396.4752473985857
new min fval from sgd:  -1396.4766179325977
new min fval from sgd:  -1396.4780054308865
new min fval from sgd:  -1396.4783369435556
new min fval from sgd:  -1396.478638571864
new min fval from sgd:  -1396.4791953911931
new min fval from sgd:  -1396.4818518776092
new min fval from sgd:  -1396.4874346983586
new min fval from sgd:  -1396.49138647877
new min fval from sgd:  -1396.4949422506816
new min fval from sgd:  -1396.4978344192457
new min fval from sgd:  -1396.4986843517443
new min fval from sgd:  -1396.500722222172
new min fval from sgd:  -1396.5028769470514
new min fval from sgd:  -1396.5036128858908
new min fval from sgd:  -1396.5040988400392
new min fval from sgd:  -1396.5044118362498
new min fval from sgd:  -1396.5048570448837
new min fval from sgd:  -1396.5084405210735
new min fval from sgd:  -1396.5111584579413
new min fval from sgd:  -1396.5156306625206
new min fval from sgd:  -1396.5226509074066
new min fval from sgd:  -1396.529150471204
new min fval from sgd:  -1396.53339010973
new min fval from sgd:  -1396.5352769535061
new min fval from sgd:  -1396.535588582959
new min fval from sgd:  -1396.5392318494864
new min fval from sgd:  -1396.5431181437048
new min fval from sgd:  -1396.5464257180802
new min fval from sgd:  -1396.5484702028073
new min fval from sgd:  -1396.5496496549051
new min fval from sgd:  -1396.5528168755677
new min fval from sgd:  -1396.5545774274235
new min fval from sgd:  -1396.5572207465252
new min fval from sgd:  -1396.5594038469205
new min fval from sgd:  -1396.560658194304
new min fval from sgd:  -1396.5634797483638
new min fval from sgd:  -1396.5661955188648
new min fval from sgd:  -1396.5688628737284
new min fval from sgd:  -1396.5700320860556
new min fval from sgd:  -1396.5704674016783
new min fval from sgd:  -1396.5706367980406
new min fval from sgd:  -1396.5706857174114
new min fval from sgd:  -1396.5707561568688
new min fval from sgd:  -1396.5746602218828
new min fval from sgd:  -1396.5758883167127
new min fval from sgd:  -1396.5773457990801
new min fval from sgd:  -1396.5774069495883
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.98597]
objective value function right now is: -1396.5545362314504
new min fval from sgd:  -1396.5783881775792
new min fval from sgd:  -1396.5785461732924
new min fval from sgd:  -1396.5788874482187
new min fval from sgd:  -1396.579703405775
new min fval from sgd:  -1396.5824260869674
new min fval from sgd:  -1396.5852214445135
new min fval from sgd:  -1396.5871892432654
new min fval from sgd:  -1396.5901410360718
new min fval from sgd:  -1396.592008259069
new min fval from sgd:  -1396.594193460254
new min fval from sgd:  -1396.5953245247044
new min fval from sgd:  -1396.5960999936979
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.990376]
objective value function right now is: -1396.5159035475092
min fval:  -1396.5960999936979
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-13.0642,   1.5323],
        [ 12.6431,  -1.1906],
        [  8.9200,  -7.9015],
        [  9.3468,   2.8194],
        [ 13.0982,  -4.7256],
        [ -1.1368,   0.2529],
        [ -1.1368,   0.2529],
        [-13.4527, -11.3754],
        [  4.8189,  -9.9287],
        [  7.8612,  -9.7289]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 10.1841, -10.7213,  -7.1193, -11.5018,  -4.1533,  -2.2075,  -2.2075,
         -7.9212,  -9.4853,  -9.2265], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.9944e-01, -3.6083e-01, -2.7668e-01, -1.5632e-01, -9.7704e-01,
         -1.4244e-02, -1.4244e-02,  2.2084e-03, -4.6983e-02, -1.2098e-01],
        [-3.9944e-01, -3.6083e-01, -2.7668e-01, -1.5632e-01, -9.7704e-01,
         -1.4244e-02, -1.4244e-02,  2.2084e-03, -4.6983e-02, -1.2098e-01],
        [-3.9944e-01, -3.6083e-01, -2.7668e-01, -1.5632e-01, -9.7704e-01,
         -1.4244e-02, -1.4244e-02,  2.2084e-03, -4.6983e-02, -1.2098e-01],
        [-3.9944e-01, -3.6083e-01, -2.7668e-01, -1.5632e-01, -9.7704e-01,
         -1.4244e-02, -1.4244e-02,  2.2085e-03, -4.6983e-02, -1.2098e-01],
        [-1.2169e+01,  7.9889e+00,  4.1903e+00,  3.7985e+00,  1.5319e-01,
          2.8441e-03,  2.8441e-03,  5.4058e+00,  8.9079e+00,  7.5658e+00],
        [ 1.3218e+01, -7.8270e+00, -6.3138e+00, -3.8408e+00, -4.7952e+00,
          4.6172e-02,  4.6171e-02, -6.1982e+00, -8.4926e+00, -7.4810e+00],
        [-3.9944e-01, -3.6083e-01, -2.7668e-01, -1.5632e-01, -9.7704e-01,
         -1.4244e-02, -1.4244e-02,  2.2084e-03, -4.6983e-02, -1.2098e-01],
        [-3.9944e-01, -3.6083e-01, -2.7668e-01, -1.5632e-01, -9.7704e-01,
         -1.4244e-02, -1.4244e-02,  2.2084e-03, -4.6983e-02, -1.2098e-01],
        [-8.9339e+00,  4.6124e+00,  2.9492e+00,  1.5555e+00,  1.3048e-01,
          1.9217e-02,  1.9217e-02,  4.4625e+00,  6.0922e+00,  5.6428e+00],
        [-3.9945e-01, -3.6084e-01, -2.7668e-01, -1.5633e-01, -9.7691e-01,
         -1.4241e-02, -1.4241e-02,  2.2105e-03, -4.6979e-02, -1.2098e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.9976, -0.9976, -0.9976, -0.9976, -3.8370,  4.9134, -0.9976, -0.9976,
        -3.4187, -0.9981], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0184,   0.0184,   0.0184,   0.0184, -10.5320,  14.0892,   0.0184,
           0.0184,  -5.3386,   0.0184]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 1.1044e+01,  9.5937e+00],
        [-1.6936e+01, -4.2164e+00],
        [-1.8136e+01, -4.5055e+00],
        [-1.4413e+01,  1.1752e+01],
        [-2.9041e+00,  5.5249e+00],
        [ 1.4404e+01, -2.4874e-03],
        [ 1.1987e+01,  1.2122e+00],
        [-1.9580e+00, -1.0238e-01],
        [ 2.0744e+00,  1.3468e+01],
        [-1.5706e+01, -5.2833e+00]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.3581,  -3.1712,  -2.4809,  11.1388,  -2.4996, -13.6870,  -7.8456,
         -4.5653,  12.2015,   0.0654], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-7.2597e-01,  1.1541e-03, -2.8803e-03, -2.2145e+00,  2.2668e+00,
          4.0204e+00,  1.2547e+00, -6.9057e-01, -1.7899e+00, -2.1420e+01],
        [ 4.9172e-01,  4.9069e-02,  6.7483e-02, -8.6906e-01,  2.2066e-01,
          4.7230e-01, -5.2333e-01, -8.1344e-04,  4.9630e-01, -2.8814e+00],
        [-5.1549e-01,  5.5848e+00,  6.1929e+00, -8.2516e+00, -2.2938e-01,
         -1.4812e+00, -4.1515e+00,  9.3438e-02,  1.1577e+01,  3.7180e+00],
        [ 9.6384e-01, -2.5930e+00, -4.2764e+00,  6.1344e+00,  3.3988e+00,
         -1.6516e+01, -5.2681e+00,  3.1101e-01,  3.4987e+00,  8.9152e-01],
        [-2.4888e+01,  9.2113e+00,  1.2279e+01,  2.0346e+00, -2.3485e-03,
         -1.1275e+01, -4.4131e+00, -6.5048e-01, -1.6144e+01,  5.4345e+00],
        [-6.7528e+00,  6.1765e+00,  7.7311e+00, -1.5078e+01, -1.4373e-02,
         -1.5416e+01, -7.1383e+00,  9.1411e-01, -1.4361e+01,  7.2360e+00],
        [-1.3674e-02,  4.7080e-03,  1.0665e-02, -4.9970e-01,  2.1409e-01,
          9.4456e-01, -3.1433e-01,  7.1149e-02, -3.0943e-01, -3.6817e+00],
        [-8.1926e-01,  5.8479e+00,  1.1164e+01, -1.2991e+01, -1.6185e-02,
         -7.6220e+00, -3.4096e+00,  9.7717e-01, -1.5286e+01,  3.2028e+00],
        [ 3.3245e+00,  9.4192e+00,  1.0729e+01, -1.4101e+01, -5.1100e-01,
          4.5775e-01, -1.3282e+00,  5.4061e-01,  7.5734e-01,  4.9870e-01],
        [-2.1960e+00, -9.8204e-02, -2.0610e-01, -1.0153e+00, -7.9997e-01,
         -1.4720e-03, -2.6184e-01,  7.5785e-02, -2.3987e+00, -1.5081e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -5.0396,  -6.1471, -12.7174,  -5.6280,  -6.2336,  -1.7459,  -6.6468,
         -1.9269,  -5.5974,  -3.8198], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  4.1267,   2.8141,  -2.9308,   0.2552,  16.2933,  -5.3410,   2.4158,
          -4.3208,   1.9190,  -2.0864],
        [ -3.8077,  -2.7910,   2.9557,  -0.4210, -16.2818,   5.3882,  -2.3964,
           4.3868,  -2.1340,   2.0864]], device='cuda:0'))])
xi:  [-34.99156]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 451.621256907541
W_T_median: 176.76051761315472
W_T_pctile_5: -34.99058975501751
W_T_CVAR_5_pct: -140.82456864320088
Average q (qsum/M+1):  51.86556514616935
Optimal xi:  [-34.99156]
Expected(across Rb) median(across samples) p_equity:  0.381582477192084
obj fun:  tensor(-1396.5961, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -9.0259,   1.6200],
        [  9.0716,   0.0578],
        [  7.1793,  -6.1605],
        [  7.5381,   3.2505],
        [  9.6693,  -5.0357],
        [ -1.3449,   0.3001],
        [  1.4948,  -2.5906],
        [ -6.5573, -10.0162],
        [  4.3191,  -8.2380],
        [  5.3129,  -8.5238]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 7.3420, -8.4146, -6.0056, -8.9126, -4.1491, -2.2581, -5.7163, -7.4441,
        -7.8151, -8.4529], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-9.2020e+00,  5.4065e+00,  2.8772e+00,  4.8714e+00,  1.6951e+00,
          9.5674e-02,  2.6977e-01,  6.2889e+00,  6.2490e+00,  3.6956e+00],
        [ 1.0659e+01, -5.1375e+00, -5.5630e+00, -5.5762e+00, -4.6171e+00,
          1.2856e-01, -1.1658e+00, -6.2914e+00, -7.4374e+00, -5.2700e+00],
        [-5.2413e-01, -1.7724e-01, -5.6925e-01, -4.3312e-02, -8.8777e-01,
         -1.0492e-02, -8.0194e-03,  1.3355e-01, -1.9131e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-8.7807e+00,  4.7062e+00,  2.8038e+00,  3.8880e+00,  1.8033e+00,
          1.4201e-01,  1.9636e-01,  6.0533e+00,  5.7920e+00,  3.4498e+00],
        [-8.5582e+00,  4.6390e+00,  2.6593e+00,  3.6119e+00,  1.8101e+00,
          1.4993e-01,  1.7458e-01,  5.8599e+00,  5.7528e+00,  3.3968e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.9148, -0.9148, -0.9148, -0.9148, -3.4273,  4.5581, -0.9151, -0.9148,
        -3.4455, -3.4434], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 6.4029e-03,  6.4029e-03,  6.4029e-03,  6.4029e-03, -6.4204e+00,
          1.6896e+01,  6.4026e-03,  6.4029e-03, -5.7940e+00, -5.5530e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  7.7801,   8.5117],
        [-12.8039,  -3.3197],
        [-14.0960,  -3.3781],
        [-10.3438,  13.1059],
        [ -6.7274,   3.2029],
        [ 11.3494,  -0.2092],
        [ 10.9206,   1.8935],
        [  6.0962,  -1.4135],
        [ -0.2963,  10.7227],
        [-11.1179,  -4.1613]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.3157,  -1.9104,  -2.0167,  11.5895,   3.0902, -10.3992,  -3.2258,
        -11.9670,   9.7899,  -0.1907], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.1204e+00, -4.8217e-01, -1.2984e-01, -3.9602e+00,  2.8329e+00,
          4.0120e+00, -1.4919e-01,  2.1667e+00, -5.0218e-01, -1.3871e+01],
        [-1.3321e+00, -5.5194e-01, -4.4665e-01,  2.4663e+00, -5.7744e-01,
          8.3623e-01, -2.1840e+00,  1.8294e-02,  1.2065e+00, -2.0505e+00],
        [ 7.6812e-01,  3.7889e+00,  4.6433e+00, -6.0243e+00,  2.2376e-01,
         -6.6894e+00, -4.5433e+00, -4.5007e-02,  5.9901e+00, -4.0033e-01],
        [ 8.0559e-01, -3.0883e+00, -2.2335e+00,  6.0347e+00,  3.1022e+00,
         -1.0734e+01, -5.7229e+00, -2.3178e-02,  3.6155e+00, -7.1234e-01],
        [-1.8985e+01,  7.2784e+00,  1.0297e+01, -4.8164e+00,  8.6606e-01,
         -5.7802e+00, -6.1329e+00, -2.3282e+00, -4.0583e+00,  3.9973e+00],
        [-8.3172e+00,  7.8510e+00,  8.0024e+00, -1.6984e+01, -2.4242e+00,
         -8.1970e+00, -4.3623e+00, -4.1766e+00, -1.4006e+01,  5.7935e+00],
        [-1.0779e-01,  4.5688e-01,  3.0178e-01,  2.5827e+00,  2.1453e+00,
         -5.3619e+00, -2.7556e+00,  4.3528e-03,  5.2351e-01,  8.5108e-01],
        [-1.6100e+00,  3.8398e+00,  3.2810e+00, -1.0011e+01, -1.5281e+00,
         -4.8466e+00, -2.2404e+00,  6.6782e-01, -1.1001e+01,  8.8575e-01],
        [ 3.7104e+00,  7.5672e+00,  8.9019e+00, -1.6178e+01, -8.3096e-01,
         -1.7622e+00, -1.5875e+00,  5.6997e-01, -9.7621e-02, -8.4607e-01],
        [-1.1953e+01,  4.3930e+00,  6.1183e+00, -5.1416e+00, -3.5599e-01,
         -4.5375e+00, -4.5225e+00, -1.8014e+00, -2.6516e+00,  3.8139e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.8622, -4.0911, -5.7102, -5.2697, -4.8399,  0.2404, -5.4978, -0.0417,
        -4.6828, -4.0470], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  3.1935,  -1.0137,  -2.4010,   0.3391,  11.2912,  -6.6207,   1.0781,
          -3.5835,   2.2661,   6.5807],
        [ -2.8140,   1.0718,   2.4322,  -0.5105, -11.2830,   6.6898,  -0.9991,
           3.6558,  -2.4912,  -6.5667]], device='cuda:0'))])
loaded xi:  -34.98991
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 808.1706015737759
W_T_median: 592.6450607954835
W_T_pctile_5: -246.30676296883496
W_T_CVAR_5_pct: -379.12258834538136
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1179.7549255405168
Current xi:  [-34.88339]
objective value function right now is: -1179.7549255405168
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1188.4361406329954
Current xi:  [-29.700891]
objective value function right now is: -1188.4361406329954
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1196.5630643102083
Current xi:  [-15.055701]
objective value function right now is: -1196.5630643102083
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1204.3123904895158
Current xi:  [-1.1494277]
objective value function right now is: -1204.3123904895158
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1206.5231098320269
Current xi:  [-0.01076614]
objective value function right now is: -1206.5231098320269
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1206.8802409161892
Current xi:  [-0.21183845]
objective value function right now is: -1206.8802409161892
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01058113]
objective value function right now is: -1201.6825357534165
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0491283]
objective value function right now is: -1197.4175441646432
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1207.7095600730752
Current xi:  [-0.01946752]
objective value function right now is: -1207.7095600730752
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03768852]
objective value function right now is: -1202.5610605448321
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1207.7883280001288
Current xi:  [-0.07629746]
objective value function right now is: -1207.7883280001288
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04199369]
objective value function right now is: -1204.3306408551678
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0361083]
objective value function right now is: -1202.0675056272862
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1208.1362778980845
Current xi:  [-0.0015596]
objective value function right now is: -1208.1362778980845
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.08798476]
objective value function right now is: -1206.2311882286012
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00574087]
objective value function right now is: -1204.9133590435683
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05097392]
objective value function right now is: -1201.717747347281
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1209.0127538327208
Current xi:  [-0.00368609]
objective value function right now is: -1209.0127538327208
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.09543323]
objective value function right now is: -1203.5007414968736
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01920703]
objective value function right now is: -1201.0650608356905
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02851044]
objective value function right now is: -1201.0937018900474
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03952807]
objective value function right now is: -1206.3437840795527
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01546005]
objective value function right now is: -1206.2514656526735
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02902622]
objective value function right now is: -1200.258775624332
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.026091]
objective value function right now is: -1198.545083928669
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02602276]
objective value function right now is: -1208.4075563449592
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02477466]
objective value function right now is: -1207.67590644009
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.056464]
objective value function right now is: -1207.5813044633326
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0549682]
objective value function right now is: -1203.9749155045963
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02850907]
objective value function right now is: -1208.6759509644573
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03898327]
objective value function right now is: -1208.9433606626233
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01453809]
objective value function right now is: -1206.473196528742
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03462257]
objective value function right now is: -1205.3789867367777
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00965579]
objective value function right now is: -1205.4482551990188
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00456598]
objective value function right now is: -1203.6947216658639
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1210.4933884221787
Current xi:  [-0.0043105]
objective value function right now is: -1210.4933884221787
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1211.1233610291556
Current xi:  [-0.00088123]
objective value function right now is: -1211.1233610291556
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00927156]
objective value function right now is: -1210.0575320486728
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00222421]
objective value function right now is: -1210.5036414450747
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00119091]
objective value function right now is: -1210.5658854986243
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1211.4298128423277
Current xi:  [-0.00258221]
objective value function right now is: -1211.4298128423277
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01499931]
objective value function right now is: -1211.3713877301807
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00733399]
objective value function right now is: -1210.4580456722686
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00138304]
objective value function right now is: -1210.4773300155905
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00051836]
objective value function right now is: -1209.1273735150835
new min fval from sgd:  -1211.5333184855706
new min fval from sgd:  -1211.625752235258
new min fval from sgd:  -1211.6591208663722
new min fval from sgd:  -1211.6615073454143
new min fval from sgd:  -1211.675402782012
new min fval from sgd:  -1211.7477661106439
new min fval from sgd:  -1211.7595928119852
new min fval from sgd:  -1211.7669870148422
new min fval from sgd:  -1211.7762771951939
new min fval from sgd:  -1211.780861548496
new min fval from sgd:  -1211.7876543962661
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01578996]
objective value function right now is: -1211.487835253956
new min fval from sgd:  -1211.7907188934107
new min fval from sgd:  -1211.8003805904384
new min fval from sgd:  -1211.8536687814249
new min fval from sgd:  -1211.894662168927
new min fval from sgd:  -1211.9066316671986
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01465563]
objective value function right now is: -1211.502919624137
new min fval from sgd:  -1211.9070619486386
new min fval from sgd:  -1211.9133837221395
new min fval from sgd:  -1211.9227171292341
new min fval from sgd:  -1211.9322024460193
new min fval from sgd:  -1211.9333081237135
new min fval from sgd:  -1211.95254414907
new min fval from sgd:  -1211.9575150456706
new min fval from sgd:  -1211.9708787301709
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00504065]
objective value function right now is: -1211.6245637846082
new min fval from sgd:  -1211.9733457633454
new min fval from sgd:  -1211.976962595871
new min fval from sgd:  -1211.979087579379
new min fval from sgd:  -1211.9821524562094
new min fval from sgd:  -1211.9839544629524
new min fval from sgd:  -1211.986557994505
new min fval from sgd:  -1211.9895016944936
new min fval from sgd:  -1211.9922418045733
new min fval from sgd:  -1211.9974967775927
new min fval from sgd:  -1212.0048562192735
new min fval from sgd:  -1212.012204387391
new min fval from sgd:  -1212.017070396414
new min fval from sgd:  -1212.0223247602821
new min fval from sgd:  -1212.0265808971142
new min fval from sgd:  -1212.0284633793035
new min fval from sgd:  -1212.0303608098095
new min fval from sgd:  -1212.031127385397
new min fval from sgd:  -1212.0335843563396
new min fval from sgd:  -1212.0339599162323
new min fval from sgd:  -1212.0346135508582
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00111401]
objective value function right now is: -1211.9179557948999
new min fval from sgd:  -1212.0373025551066
new min fval from sgd:  -1212.0448754629567
new min fval from sgd:  -1212.049508551441
new min fval from sgd:  -1212.0523943512883
new min fval from sgd:  -1212.0544572901383
new min fval from sgd:  -1212.0568166639907
new min fval from sgd:  -1212.0581227378077
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.627994e-05]
objective value function right now is: -1211.975088781227
min fval:  -1212.0581227378077
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.4095e+01,  1.4828e+00],
        [ 1.3241e+01, -9.6879e-01],
        [ 1.0360e+01, -5.7290e+00],
        [ 9.2642e+00,  2.4189e+00],
        [ 7.2567e+00, -3.2105e+00],
        [-1.3719e+00,  6.8569e-03],
        [-1.3719e+00,  6.8567e-03],
        [-7.7716e+00, -1.1853e+01],
        [ 5.2269e+00, -1.0569e+01],
        [ 9.0036e+00, -9.5951e+00]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  9.5501, -10.9580,  -6.7208, -12.8180,  -7.9108,  -2.6551,  -2.6551,
         -9.2646,  -8.8604,  -8.6326], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.5339e-01, -5.4769e-01, -4.9619e-01, -1.1141e-01, -2.9662e-02,
         -1.9241e-02, -1.9241e-02, -5.7592e-02, -1.3095e-01, -2.3517e-01],
        [-5.5339e-01, -5.4769e-01, -4.9619e-01, -1.1141e-01, -2.9662e-02,
         -1.9241e-02, -1.9241e-02, -5.7592e-02, -1.3095e-01, -2.3517e-01],
        [-5.5339e-01, -5.4769e-01, -4.9619e-01, -1.1141e-01, -2.9662e-02,
         -1.9241e-02, -1.9241e-02, -5.7592e-02, -1.3095e-01, -2.3517e-01],
        [-5.5339e-01, -5.4769e-01, -4.9619e-01, -1.1141e-01, -2.9662e-02,
         -1.9241e-02, -1.9241e-02, -5.7592e-02, -1.3095e-01, -2.3517e-01],
        [-1.3198e+01,  8.0784e+00,  4.2881e+00,  4.2494e+00,  5.7234e-01,
         -1.1578e-01, -1.1578e-01,  9.3484e+00,  8.9983e+00,  7.9390e+00],
        [ 1.1768e+01, -8.7466e+00, -6.3017e+00, -4.7895e+00, -2.6329e+00,
         -6.8286e-02, -6.8286e-02, -1.0707e+01, -9.8367e+00, -8.1594e+00],
        [-5.5339e-01, -5.4769e-01, -4.9619e-01, -1.1141e-01, -2.9662e-02,
         -1.9241e-02, -1.9241e-02, -5.7592e-02, -1.3095e-01, -2.3517e-01],
        [-5.5339e-01, -5.4769e-01, -4.9619e-01, -1.1141e-01, -2.9662e-02,
         -1.9241e-02, -1.9241e-02, -5.7592e-02, -1.3095e-01, -2.3517e-01],
        [-1.0861e+01,  5.8828e+00,  3.4191e+00,  2.7543e+00,  3.0645e-01,
          7.8790e-02,  7.8790e-02,  8.1406e+00,  7.3332e+00,  6.7039e+00],
        [-5.2028e+00,  2.0456e+00,  1.2368e+00,  3.4645e-01, -6.1412e-02,
         -1.2815e-02, -1.2815e-02,  4.6324e-02,  6.7535e-01,  1.9050e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.6973, -1.6973, -1.6973, -1.6973, -3.8611,  2.8324, -1.6973, -1.6973,
        -3.8084, -3.6931], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 3.9142e-03,  3.9142e-03,  3.9142e-03,  3.9142e-03, -1.0336e+01,
          1.3758e+01,  3.9142e-03,  3.9142e-03, -6.3906e+00, -1.6833e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 10.0377,  10.1784],
        [-14.4470,  -3.9341],
        [-16.4146,  -4.5078],
        [-14.5495,  13.2109],
        [ -2.5324,   0.1239],
        [ 13.7790,  -0.8557],
        [ 12.7832,   2.6089],
        [ -2.4361,   0.2228],
        [  4.8160,  12.6250],
        [-16.0793,  -5.6681]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.6410,  -3.3990,  -2.7479,  12.0901,  -4.5001, -13.9524,  -7.8478,
         -4.4505,  10.5293,   0.2127], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.0769e+00,  1.1569e-02,  7.5214e-03,  6.3525e-02, -2.6944e-01,
          2.7333e+00,  8.5426e-01, -1.3656e-01, -4.8259e-01, -2.5713e+01],
        [ 2.1604e-01, -1.4987e+00, -4.0272e-01, -1.2159e+00, -4.8228e-01,
         -1.7634e+00,  1.2862e+00, -2.4006e-01,  1.0851e+00, -5.7952e-01],
        [-1.7550e+00,  1.0221e-01,  1.3224e-01, -3.7320e-02,  1.2032e-03,
         -6.3090e-01, -7.9344e-01,  9.5926e-04, -6.8060e-01, -1.2575e-01],
        [ 9.1711e-02, -1.9103e+00, -4.9428e+00,  3.6400e+00,  5.1210e-01,
         -2.6328e+01, -1.7679e+00,  3.9185e-01,  1.6072e+00,  1.8349e+00],
        [-3.1090e+01,  7.4356e+00,  1.0517e+01,  1.0607e+00, -5.6045e-01,
         -1.1747e+01, -3.0856e-01, -5.6744e-01, -8.8143e+00,  6.3060e+00],
        [-9.7907e+00,  7.2116e+00,  1.1078e+01, -1.1065e+01,  4.5397e-01,
         -1.2956e+01, -1.0423e+01,  4.2053e-01, -1.9361e+01,  1.0729e+01],
        [-1.3808e+00,  1.8244e-02,  3.7178e-02, -7.0107e-02, -2.4507e-03,
         -5.9926e-01, -7.9087e-01, -2.6927e-03, -5.8077e-01, -5.3643e-01],
        [-8.7629e-02,  6.8676e+00,  9.6212e+00, -1.8942e+01,  6.7030e-02,
         -4.8739e+00, -3.7146e+00,  9.3540e-02, -9.8147e+00,  3.5154e+00],
        [ 3.2742e+00,  7.7088e+00,  9.6107e+00, -1.2114e+01,  4.6441e-01,
          1.0537e+00, -1.8883e+00,  2.6294e-01,  6.9994e-01,  1.2966e+00],
        [-1.8373e+00,  1.5003e-01,  8.3366e-02, -3.1708e-01,  9.4206e-03,
          1.9364e-01, -8.6022e-01,  1.0310e-02, -1.1506e+00,  2.2345e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.0850, -5.7178, -5.7348, -5.7639, -7.9029, -4.6761, -5.8401, -2.2040,
        -6.4756, -5.5844], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  4.7852,   4.3785,  -0.2455,   1.1511,  18.6943,  -6.3183,  -0.0902,
          -3.4661,   2.9100,  -0.7604],
        [ -4.4255,  -4.3313,   0.2467,  -1.3174, -18.6914,   6.3701,   0.0947,
           3.5349,  -3.1274,   0.7604]], device='cuda:0'))])
xi:  [-0.00029751]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 494.60705181379024
W_T_median: 211.33946110626306
W_T_pctile_5: 8.89198739969288e-05
W_T_CVAR_5_pct: -120.91148026295346
Average q (qsum/M+1):  50.79975349672379
Optimal xi:  [-0.00029751]
Expected(across Rb) median(across samples) p_equity:  0.3621219779054324
obj fun:  tensor(-1212.0581, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -9.0259,   1.6200],
        [  9.0716,   0.0578],
        [  7.1793,  -6.1605],
        [  7.5381,   3.2505],
        [  9.6693,  -5.0357],
        [ -1.3449,   0.3001],
        [  1.4948,  -2.5906],
        [ -6.5573, -10.0162],
        [  4.3191,  -8.2380],
        [  5.3129,  -8.5238]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 7.3420, -8.4146, -6.0056, -8.9126, -4.1491, -2.2581, -5.7163, -7.4441,
        -7.8151, -8.4529], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-9.2020e+00,  5.4065e+00,  2.8772e+00,  4.8714e+00,  1.6951e+00,
          9.5674e-02,  2.6977e-01,  6.2889e+00,  6.2490e+00,  3.6956e+00],
        [ 1.0659e+01, -5.1375e+00, -5.5630e+00, -5.5762e+00, -4.6171e+00,
          1.2856e-01, -1.1658e+00, -6.2914e+00, -7.4374e+00, -5.2700e+00],
        [-5.2413e-01, -1.7724e-01, -5.6925e-01, -4.3312e-02, -8.8777e-01,
         -1.0492e-02, -8.0194e-03,  1.3355e-01, -1.9131e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-8.7807e+00,  4.7062e+00,  2.8038e+00,  3.8880e+00,  1.8033e+00,
          1.4201e-01,  1.9636e-01,  6.0533e+00,  5.7920e+00,  3.4498e+00],
        [-8.5582e+00,  4.6390e+00,  2.6593e+00,  3.6119e+00,  1.8101e+00,
          1.4993e-01,  1.7458e-01,  5.8599e+00,  5.7528e+00,  3.3968e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.9148, -0.9148, -0.9148, -0.9148, -3.4273,  4.5581, -0.9151, -0.9148,
        -3.4455, -3.4434], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 6.4029e-03,  6.4029e-03,  6.4029e-03,  6.4029e-03, -6.4204e+00,
          1.6896e+01,  6.4026e-03,  6.4029e-03, -5.7940e+00, -5.5530e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  7.7801,   8.5117],
        [-12.8039,  -3.3197],
        [-14.0960,  -3.3781],
        [-10.3438,  13.1059],
        [ -6.7274,   3.2029],
        [ 11.3494,  -0.2092],
        [ 10.9206,   1.8935],
        [  6.0962,  -1.4135],
        [ -0.2963,  10.7227],
        [-11.1179,  -4.1613]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.3157,  -1.9104,  -2.0167,  11.5895,   3.0902, -10.3992,  -3.2258,
        -11.9670,   9.7899,  -0.1907], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.1204e+00, -4.8217e-01, -1.2984e-01, -3.9602e+00,  2.8329e+00,
          4.0120e+00, -1.4919e-01,  2.1667e+00, -5.0218e-01, -1.3871e+01],
        [-1.3321e+00, -5.5194e-01, -4.4665e-01,  2.4663e+00, -5.7744e-01,
          8.3623e-01, -2.1840e+00,  1.8294e-02,  1.2065e+00, -2.0505e+00],
        [ 7.6812e-01,  3.7889e+00,  4.6433e+00, -6.0243e+00,  2.2376e-01,
         -6.6894e+00, -4.5433e+00, -4.5007e-02,  5.9901e+00, -4.0033e-01],
        [ 8.0559e-01, -3.0883e+00, -2.2335e+00,  6.0347e+00,  3.1022e+00,
         -1.0734e+01, -5.7229e+00, -2.3178e-02,  3.6155e+00, -7.1234e-01],
        [-1.8985e+01,  7.2784e+00,  1.0297e+01, -4.8164e+00,  8.6606e-01,
         -5.7802e+00, -6.1329e+00, -2.3282e+00, -4.0583e+00,  3.9973e+00],
        [-8.3172e+00,  7.8510e+00,  8.0024e+00, -1.6984e+01, -2.4242e+00,
         -8.1970e+00, -4.3623e+00, -4.1766e+00, -1.4006e+01,  5.7935e+00],
        [-1.0779e-01,  4.5688e-01,  3.0178e-01,  2.5827e+00,  2.1453e+00,
         -5.3619e+00, -2.7556e+00,  4.3528e-03,  5.2351e-01,  8.5108e-01],
        [-1.6100e+00,  3.8398e+00,  3.2810e+00, -1.0011e+01, -1.5281e+00,
         -4.8466e+00, -2.2404e+00,  6.6782e-01, -1.1001e+01,  8.8575e-01],
        [ 3.7104e+00,  7.5672e+00,  8.9019e+00, -1.6178e+01, -8.3096e-01,
         -1.7622e+00, -1.5875e+00,  5.6997e-01, -9.7621e-02, -8.4607e-01],
        [-1.1953e+01,  4.3930e+00,  6.1183e+00, -5.1416e+00, -3.5599e-01,
         -4.5375e+00, -4.5225e+00, -1.8014e+00, -2.6516e+00,  3.8139e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.8622, -4.0911, -5.7102, -5.2697, -4.8399,  0.2404, -5.4978, -0.0417,
        -4.6828, -4.0470], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  3.1935,  -1.0137,  -2.4010,   0.3391,  11.2912,  -6.6207,   1.0781,
          -3.5835,   2.2661,   6.5807],
        [ -2.8140,   1.0718,   2.4322,  -0.5105, -11.2830,   6.6898,  -0.9991,
           3.6558,  -2.4912,  -6.5667]], device='cuda:0'))])
loaded xi:  -34.98991
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 808.1706015737759
W_T_median: 592.6450607954835
W_T_pctile_5: -246.30676296883496
W_T_CVAR_5_pct: -379.12258834538136
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -946.5209067905973
Current xi:  [-18.265892]
objective value function right now is: -946.5209067905973
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -947.3301190415608
Current xi:  [-0.34764034]
objective value function right now is: -947.3301190415608
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -958.2149133086477
Current xi:  [-0.0112822]
objective value function right now is: -958.2149133086477
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -969.4040248081486
Current xi:  [-0.04495485]
objective value function right now is: -969.4040248081486
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02414645]
objective value function right now is: -967.1430681252461
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -970.8553172691483
Current xi:  [-0.11414114]
objective value function right now is: -970.8553172691483
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01123194]
objective value function right now is: -956.1902813919887
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04510754]
objective value function right now is: -954.7165112622504
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00502454]
objective value function right now is: -959.8684384043368
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.09059326]
objective value function right now is: -967.2450224755681
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02467166]
objective value function right now is: -964.9021568494854
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06662921]
objective value function right now is: -966.0064310830913
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03060152]
objective value function right now is: -951.4165905685485
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00970825]
objective value function right now is: -968.2164351229542
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.10093775]
objective value function right now is: -956.5175122619581
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03499518]
objective value function right now is: -968.5141744240802
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00232564]
objective value function right now is: -967.9913564278581
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0793131]
objective value function right now is: -966.6055648130082
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01965885]
objective value function right now is: -958.327821482511
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02487559]
objective value function right now is: -968.2037259534544
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04797489]
objective value function right now is: -970.5376159221232
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01932514]
objective value function right now is: -968.4040359176402
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01543107]
objective value function right now is: -962.7765093545739
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00041724]
objective value function right now is: -969.5493376060106
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04221155]
objective value function right now is: -958.5703206082745
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04785714]
objective value function right now is: -964.9856838818063
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01330142]
objective value function right now is: -962.4594488951101
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -971.6317248616965
Current xi:  [-0.02838019]
objective value function right now is: -971.6317248616965
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0554606]
objective value function right now is: -961.9927676975941
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06339985]
objective value function right now is: -968.3746890946112
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02540102]
objective value function right now is: -969.0363619702943
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05686115]
objective value function right now is: -964.5784041724662
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0242364]
objective value function right now is: -965.7078332478744
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00271708]
objective value function right now is: -961.4479015091457
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.14241476]
objective value function right now is: -954.5045857824954
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -972.8825290368964
Current xi:  [0.00089513]
objective value function right now is: -972.8825290368964
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -973.6179735517386
Current xi:  [-0.00424416]
objective value function right now is: -973.6179735517386
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00164554]
objective value function right now is: -973.2040079014413
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -973.8864252705207
Current xi:  [-0.00031594]
objective value function right now is: -973.8864252705207
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00166709]
objective value function right now is: -972.6651661320077
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00298518]
objective value function right now is: -972.9109483434526
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00468945]
objective value function right now is: -972.2100673746744
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.957874e-05]
objective value function right now is: -973.3168376782064
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00035914]
objective value function right now is: -971.8463823537307
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00719615]
objective value function right now is: -973.155220969679
new min fval from sgd:  -973.9210316929978
new min fval from sgd:  -974.024011648883
new min fval from sgd:  -974.0735856427301
new min fval from sgd:  -974.1102609004027
new min fval from sgd:  -974.1319036475305
new min fval from sgd:  -974.1326776116924
new min fval from sgd:  -974.1389635880136
new min fval from sgd:  -974.2313982961405
new min fval from sgd:  -974.2356486942111
new min fval from sgd:  -974.2923700664829
new min fval from sgd:  -974.3522918245594
new min fval from sgd:  -974.4320535778464
new min fval from sgd:  -974.4986172698859
new min fval from sgd:  -974.5834046851918
new min fval from sgd:  -974.600141249542
new min fval from sgd:  -974.6188479408105
new min fval from sgd:  -974.6256244901593
new min fval from sgd:  -974.6830975699254
new min fval from sgd:  -974.719952915406
new min fval from sgd:  -974.7322898948415
new min fval from sgd:  -974.7784835694644
new min fval from sgd:  -974.7983234006232
new min fval from sgd:  -974.8094030610994
new min fval from sgd:  -974.8151555460985
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01422085]
objective value function right now is: -974.4333435771065
new min fval from sgd:  -974.8313183403362
new min fval from sgd:  -974.9142973030976
new min fval from sgd:  -974.9191763562709
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00355463]
objective value function right now is: -974.151057266396
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00351056]
objective value function right now is: -974.1122470622394
new min fval from sgd:  -974.9472908759932
new min fval from sgd:  -974.9561267065203
new min fval from sgd:  -974.9866542709946
new min fval from sgd:  -975.0116452020968
new min fval from sgd:  -975.0265157215923
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00039591]
objective value function right now is: -974.6700136890016
new min fval from sgd:  -975.0383412356601
new min fval from sgd:  -975.0519758334149
new min fval from sgd:  -975.0694385124651
new min fval from sgd:  -975.0862667558214
new min fval from sgd:  -975.0991882280978
new min fval from sgd:  -975.1084839995627
new min fval from sgd:  -975.1152037845648
new min fval from sgd:  -975.1226740485092
new min fval from sgd:  -975.1304034826984
new min fval from sgd:  -975.1317220622818
new min fval from sgd:  -975.1386420984369
new min fval from sgd:  -975.1461663508875
new min fval from sgd:  -975.148404872567
new min fval from sgd:  -975.150933204252
new min fval from sgd:  -975.153474505194
new min fval from sgd:  -975.1636941663735
new min fval from sgd:  -975.1771861681575
new min fval from sgd:  -975.1858181703012
new min fval from sgd:  -975.1979967075998
new min fval from sgd:  -975.2066297636102
new min fval from sgd:  -975.2148202754872
new min fval from sgd:  -975.220303595175
new min fval from sgd:  -975.2212745698079
new min fval from sgd:  -975.2235697485506
new min fval from sgd:  -975.2336898313479
new min fval from sgd:  -975.2419735911473
new min fval from sgd:  -975.2466296266965
new min fval from sgd:  -975.2534230238642
new min fval from sgd:  -975.2595439622899
new min fval from sgd:  -975.2622693702416
new min fval from sgd:  -975.2655379941862
new min fval from sgd:  -975.2726486735874
new min fval from sgd:  -975.2761066850032
new min fval from sgd:  -975.2787834031226
new min fval from sgd:  -975.2830551132018
new min fval from sgd:  -975.2841669215776
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0008235]
objective value function right now is: -975.2067275252574
min fval:  -975.2841669215776
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-14.4429,   1.2633],
        [ 13.9276,  -1.1060],
        [ 10.9351,  -5.4686],
        [  8.6807,   2.3756],
        [  8.3899,  -2.7201],
        [ -1.3398,  -0.0693],
        [ -1.3398,  -0.0693],
        [ -7.4402, -12.0715],
        [  5.2631, -10.7430],
        [ 10.4267,  -9.5375]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  9.7828, -10.8536,  -6.9722, -13.7791,  -8.7332,  -2.7468,  -2.7468,
         -9.4120,  -9.1233,  -8.3662], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.8640e-01, -6.2482e-01, -4.1739e-01, -3.6787e-02, -1.6538e-02,
         -1.0385e-02, -1.0385e-02,  4.8713e-03, -8.9775e-02, -2.8871e-01],
        [-5.8640e-01, -6.2482e-01, -4.1739e-01, -3.6787e-02, -1.6538e-02,
         -1.0385e-02, -1.0385e-02,  4.8714e-03, -8.9775e-02, -2.8871e-01],
        [-5.8640e-01, -6.2482e-01, -4.1739e-01, -3.6787e-02, -1.6538e-02,
         -1.0385e-02, -1.0385e-02,  4.8713e-03, -8.9775e-02, -2.8871e-01],
        [-5.8640e-01, -6.2482e-01, -4.1739e-01, -3.6787e-02, -1.6538e-02,
         -1.0385e-02, -1.0385e-02,  4.8714e-03, -8.9775e-02, -2.8871e-01],
        [-1.3642e+01,  8.4209e+00,  5.1964e+00,  4.1510e+00,  9.7715e-01,
         -2.9232e-02, -2.9232e-02,  9.1743e+00,  9.7510e+00,  7.6559e+00],
        [ 1.1964e+01, -8.9623e+00, -7.0894e+00, -6.1757e+00, -2.8531e+00,
         -1.5924e-02, -1.5924e-02, -1.0834e+01, -1.0077e+01, -8.7415e+00],
        [-5.8640e-01, -6.2482e-01, -4.1739e-01, -3.6787e-02, -1.6538e-02,
         -1.0385e-02, -1.0385e-02,  4.8714e-03, -8.9775e-02, -2.8871e-01],
        [-5.8640e-01, -6.2482e-01, -4.1739e-01, -3.6787e-02, -1.6538e-02,
         -1.0385e-02, -1.0385e-02,  4.8714e-03, -8.9775e-02, -2.8871e-01],
        [-1.1773e+01,  6.5272e+00,  4.3795e+00,  2.6887e+00,  5.2463e-01,
          5.3223e-02,  5.3223e-02,  8.1271e+00,  8.4285e+00,  6.7940e+00],
        [-9.5688e+00,  4.5576e+00,  3.2172e+00,  1.0755e+00,  1.1788e-01,
          6.9770e-02,  6.9770e-02,  5.7136e+00,  6.5038e+00,  5.4810e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.7721, -1.7721, -1.7721, -1.7721, -3.6907,  3.0627, -1.7721, -1.7721,
        -3.8140, -3.8325], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 2.4508e-03,  2.4508e-03,  2.4508e-03,  2.4508e-03, -1.0048e+01,
          1.3827e+01,  2.4508e-03,  2.4508e-03, -6.7566e+00, -3.9278e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 11.1020,   9.6284],
        [-15.4245,  -4.4816],
        [-17.9226,  -5.0852],
        [-15.2602,  13.3461],
        [ -2.5911,   7.8324],
        [ 13.3786,  -1.0574],
        [ 14.3089,   2.4379],
        [ -1.8813,   2.3253],
        [  4.9755,  13.4658],
        [-15.8926,  -5.8588]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  6.8882,  -4.5184,  -3.1627,  13.1419,   5.7201, -14.5681,  -6.8755,
         -3.7585,  11.1322,  -0.3855], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-9.8663e-01, -1.0372e-01, -1.1652e-01, -1.4970e-03, -3.9470e-01,
          4.4455e+00,  1.1562e+00, -1.6897e-01, -1.5733e+01, -2.4280e+01],
        [-1.8090e+00, -7.2845e-02, -1.2665e-01, -7.0406e-01, -6.7500e-01,
         -6.6227e-01, -1.7546e+00, -4.8182e-01, -4.0092e-01, -3.5102e-01],
        [-1.6948e-01,  1.2033e+00,  4.6523e+00, -6.5080e+00,  1.2062e+00,
          9.5186e-01, -1.1650e+00,  3.3647e+00,  4.9056e+00,  3.2208e+00],
        [ 4.6922e+00, -1.4431e+00, -5.7602e-01,  2.2340e+01,  1.5028e+01,
          4.7850e+00,  2.0023e+00, -6.4816e-02,  9.1145e+00, -5.4084e+00],
        [-3.2307e+01,  8.3791e+00,  1.3753e+01,  1.2361e+00, -3.4914e+00,
         -1.4271e+01, -7.2840e+00, -1.4929e-02, -1.6646e+01,  5.2927e+00],
        [-6.8517e+00,  3.8864e+00,  6.9228e+00, -6.9804e+00, -2.4275e+00,
         -1.3088e+01, -9.4302e+00,  1.6116e-01, -1.4783e+01,  8.1844e+00],
        [-1.7980e+00, -7.2876e-02, -1.2646e-01, -7.0364e-01, -6.8000e-01,
         -6.6859e-01, -1.7442e+00, -4.8516e-01, -3.9922e-01, -3.5323e-01],
        [-7.8295e-02,  7.5681e+00,  1.3576e+01, -9.3387e+00, -1.0893e+00,
         -5.3085e+00, -1.4285e+00,  3.0626e-01, -1.7115e+01,  4.2586e+00],
        [ 2.6413e+00,  4.9907e+00,  1.0992e+01, -6.1421e+00,  8.3531e-01,
          5.6043e+00, -1.7083e+00,  3.8218e+00,  1.6361e+00,  1.9798e+00],
        [-1.7927e+00, -7.2957e-02, -1.2658e-01, -7.0473e-01, -6.8310e-01,
         -6.6848e-01, -1.7386e+00, -4.8718e-01, -3.9842e-01, -3.5103e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-5.4189, -5.8241, -8.5517, -6.1626, -6.9906, -2.4287, -5.8253, -2.1256,
        -8.0185, -5.8274], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  4.1339,  -0.7820,   3.8608,   0.1281,  19.2846,  -6.0825,  -0.7772,
          -3.2843,   1.6990,  -0.7786],
        [ -3.7827,   0.7823,  -3.8433,  -0.2994, -19.2809,   6.1354,   0.7775,
           3.3534,  -1.9083,   0.7787]], device='cuda:0'))])
xi:  [-0.00064747]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 553.6420977696837
W_T_median: 258.56536417120356
W_T_pctile_5: 7.339654022260333e-05
W_T_CVAR_5_pct: -116.45685116966206
Average q (qsum/M+1):  50.244156376008064
Optimal xi:  [-0.00064747]
Expected(across Rb) median(across samples) p_equity:  0.39324578990538916
obj fun:  tensor(-975.2842, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -9.0259,   1.6200],
        [  9.0716,   0.0578],
        [  7.1793,  -6.1605],
        [  7.5381,   3.2505],
        [  9.6693,  -5.0357],
        [ -1.3449,   0.3001],
        [  1.4948,  -2.5906],
        [ -6.5573, -10.0162],
        [  4.3191,  -8.2380],
        [  5.3129,  -8.5238]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 7.3420, -8.4146, -6.0056, -8.9126, -4.1491, -2.2581, -5.7163, -7.4441,
        -7.8151, -8.4529], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-9.2020e+00,  5.4065e+00,  2.8772e+00,  4.8714e+00,  1.6951e+00,
          9.5674e-02,  2.6977e-01,  6.2889e+00,  6.2490e+00,  3.6956e+00],
        [ 1.0659e+01, -5.1375e+00, -5.5630e+00, -5.5762e+00, -4.6171e+00,
          1.2856e-01, -1.1658e+00, -6.2914e+00, -7.4374e+00, -5.2700e+00],
        [-5.2413e-01, -1.7724e-01, -5.6925e-01, -4.3312e-02, -8.8777e-01,
         -1.0492e-02, -8.0194e-03,  1.3355e-01, -1.9131e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-8.7807e+00,  4.7062e+00,  2.8038e+00,  3.8880e+00,  1.8033e+00,
          1.4201e-01,  1.9636e-01,  6.0533e+00,  5.7920e+00,  3.4498e+00],
        [-8.5582e+00,  4.6390e+00,  2.6593e+00,  3.6119e+00,  1.8101e+00,
          1.4993e-01,  1.7458e-01,  5.8599e+00,  5.7528e+00,  3.3968e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.9148, -0.9148, -0.9148, -0.9148, -3.4273,  4.5581, -0.9151, -0.9148,
        -3.4455, -3.4434], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 6.4029e-03,  6.4029e-03,  6.4029e-03,  6.4029e-03, -6.4204e+00,
          1.6896e+01,  6.4026e-03,  6.4029e-03, -5.7940e+00, -5.5530e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  7.7801,   8.5117],
        [-12.8039,  -3.3197],
        [-14.0960,  -3.3781],
        [-10.3438,  13.1059],
        [ -6.7274,   3.2029],
        [ 11.3494,  -0.2092],
        [ 10.9206,   1.8935],
        [  6.0962,  -1.4135],
        [ -0.2963,  10.7227],
        [-11.1179,  -4.1613]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.3157,  -1.9104,  -2.0167,  11.5895,   3.0902, -10.3992,  -3.2258,
        -11.9670,   9.7899,  -0.1907], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.1204e+00, -4.8217e-01, -1.2984e-01, -3.9602e+00,  2.8329e+00,
          4.0120e+00, -1.4919e-01,  2.1667e+00, -5.0218e-01, -1.3871e+01],
        [-1.3321e+00, -5.5194e-01, -4.4665e-01,  2.4663e+00, -5.7744e-01,
          8.3623e-01, -2.1840e+00,  1.8294e-02,  1.2065e+00, -2.0505e+00],
        [ 7.6812e-01,  3.7889e+00,  4.6433e+00, -6.0243e+00,  2.2376e-01,
         -6.6894e+00, -4.5433e+00, -4.5007e-02,  5.9901e+00, -4.0033e-01],
        [ 8.0559e-01, -3.0883e+00, -2.2335e+00,  6.0347e+00,  3.1022e+00,
         -1.0734e+01, -5.7229e+00, -2.3178e-02,  3.6155e+00, -7.1234e-01],
        [-1.8985e+01,  7.2784e+00,  1.0297e+01, -4.8164e+00,  8.6606e-01,
         -5.7802e+00, -6.1329e+00, -2.3282e+00, -4.0583e+00,  3.9973e+00],
        [-8.3172e+00,  7.8510e+00,  8.0024e+00, -1.6984e+01, -2.4242e+00,
         -8.1970e+00, -4.3623e+00, -4.1766e+00, -1.4006e+01,  5.7935e+00],
        [-1.0779e-01,  4.5688e-01,  3.0178e-01,  2.5827e+00,  2.1453e+00,
         -5.3619e+00, -2.7556e+00,  4.3528e-03,  5.2351e-01,  8.5108e-01],
        [-1.6100e+00,  3.8398e+00,  3.2810e+00, -1.0011e+01, -1.5281e+00,
         -4.8466e+00, -2.2404e+00,  6.6782e-01, -1.1001e+01,  8.8575e-01],
        [ 3.7104e+00,  7.5672e+00,  8.9019e+00, -1.6178e+01, -8.3096e-01,
         -1.7622e+00, -1.5875e+00,  5.6997e-01, -9.7621e-02, -8.4607e-01],
        [-1.1953e+01,  4.3930e+00,  6.1183e+00, -5.1416e+00, -3.5599e-01,
         -4.5375e+00, -4.5225e+00, -1.8014e+00, -2.6516e+00,  3.8139e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.8622, -4.0911, -5.7102, -5.2697, -4.8399,  0.2404, -5.4978, -0.0417,
        -4.6828, -4.0470], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  3.1935,  -1.0137,  -2.4010,   0.3391,  11.2912,  -6.6207,   1.0781,
          -3.5835,   2.2661,   6.5807],
        [ -2.8140,   1.0718,   2.4322,  -0.5105, -11.2830,   6.6898,  -0.9991,
           3.6558,  -2.4912,  -6.5667]], device='cuda:0'))])
loaded xi:  -34.98991
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 808.1706015737759
W_T_median: 592.6450607954835
W_T_pctile_5: -246.30676296883496
W_T_CVAR_5_pct: -379.12258834538136
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -342.77647975835555
Current xi:  [-15.039757]
objective value function right now is: -342.77647975835555
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -380.09636592667516
Current xi:  [-0.0195408]
objective value function right now is: -380.09636592667516
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02123867]
objective value function right now is: -373.6414119958781
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00180357]
objective value function right now is: -363.75886912012726
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -380.23223980481606
Current xi:  [-0.13928476]
objective value function right now is: -380.23223980481606
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -387.9321846601653
Current xi:  [-0.00799425]
objective value function right now is: -387.9321846601653
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0631849]
objective value function right now is: -381.70087024745663
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.21420729]
objective value function right now is: -376.32478946727963
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01702555]
objective value function right now is: -379.8626273479415
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01456013]
objective value function right now is: -376.91009438501953
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.07615824]
objective value function right now is: -376.84956367359973
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01285857]
objective value function right now is: -384.1964264147796
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06017067]
objective value function right now is: -366.97915514031274
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01115099]
objective value function right now is: -353.4438232676797
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04332624]
objective value function right now is: -385.8111876730135
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02341755]
objective value function right now is: -386.5149406632974
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06331733]
objective value function right now is: -378.0926078525501
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03686757]
objective value function right now is: -376.9188066133507
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01405062]
objective value function right now is: -383.24323338537386
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00013718]
objective value function right now is: -375.0730222732392
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00429811]
objective value function right now is: -387.0127068476048
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00237937]
objective value function right now is: -366.5325428308159
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00408172]
objective value function right now is: -352.0316304207561
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.08082163]
objective value function right now is: -377.2647800822085
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03340467]
objective value function right now is: -384.01708256466486
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00904465]
objective value function right now is: -386.20470920793684
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.10321782]
objective value function right now is: -384.11581237725625
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05258781]
objective value function right now is: -386.0280300342701
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03771824]
objective value function right now is: -372.7070737851289
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04855959]
objective value function right now is: -382.7898549332728
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.08395652]
objective value function right now is: -382.4551368678333
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -391.47571075871224
Current xi:  [-0.01528287]
objective value function right now is: -391.47571075871224
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0422096]
objective value function right now is: -383.54952672801863
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01085298]
objective value function right now is: -370.72205018033105
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02210568]
objective value function right now is: -383.24988718575327
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -393.3429772129377
Current xi:  [0.00012448]
objective value function right now is: -393.3429772129377
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -394.7470880010401
Current xi:  [-0.00151841]
objective value function right now is: -394.7470880010401
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -396.15812285488346
Current xi:  [-0.00064556]
objective value function right now is: -396.15812285488346
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00148319]
objective value function right now is: -394.7449245763761
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00081879]
objective value function right now is: -393.51219353191607
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00013042]
objective value function right now is: -395.644867375718
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00063631]
objective value function right now is: -394.5632637989219
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -397.62147288707365
Current xi:  [0.00025269]
objective value function right now is: -397.62147288707365
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00374075]
objective value function right now is: -395.182233186921
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00247022]
objective value function right now is: -396.73864071059114
new min fval from sgd:  -397.74539201735746
new min fval from sgd:  -397.79169488946405
new min fval from sgd:  -397.8589603837404
new min fval from sgd:  -397.88156097331273
new min fval from sgd:  -397.90517399267367
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0039942]
objective value function right now is: -395.9668872622901
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00254057]
objective value function right now is: -391.7906037549804
new min fval from sgd:  -397.9168082191645
new min fval from sgd:  -397.9210832273656
new min fval from sgd:  -397.9227157352852
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00146312]
objective value function right now is: -395.2424819629756
new min fval from sgd:  -397.9246962244238
new min fval from sgd:  -397.92832684716257
new min fval from sgd:  -397.9288331835636
new min fval from sgd:  -397.9522487829883
new min fval from sgd:  -397.9769766969286
new min fval from sgd:  -397.9926719012612
new min fval from sgd:  -398.004802861702
new min fval from sgd:  -398.0183213485829
new min fval from sgd:  -398.02448903178794
new min fval from sgd:  -398.03379100278283
new min fval from sgd:  -398.05093720678167
new min fval from sgd:  -398.066648087358
new min fval from sgd:  -398.07854817044733
new min fval from sgd:  -398.09521171214686
new min fval from sgd:  -398.0983949866386
new min fval from sgd:  -398.10330827696936
new min fval from sgd:  -398.10389833774684
new min fval from sgd:  -398.1050073395104
new min fval from sgd:  -398.1088999325364
new min fval from sgd:  -398.1109782124009
new min fval from sgd:  -398.11141008347033
new min fval from sgd:  -398.1132880300582
new min fval from sgd:  -398.12283634812337
new min fval from sgd:  -398.1366757399817
new min fval from sgd:  -398.1463783396084
new min fval from sgd:  -398.1469881546193
new min fval from sgd:  -398.150677653093
new min fval from sgd:  -398.15794113488124
new min fval from sgd:  -398.1662595412451
new min fval from sgd:  -398.1707751423933
new min fval from sgd:  -398.178759091774
new min fval from sgd:  -398.1820012257121
new min fval from sgd:  -398.1841833055333
new min fval from sgd:  -398.1859905290936
new min fval from sgd:  -398.18905756057967
new min fval from sgd:  -398.21415421357926
new min fval from sgd:  -398.23340082355384
new min fval from sgd:  -398.2480493384762
new min fval from sgd:  -398.2577896271386
new min fval from sgd:  -398.26154976100594
new min fval from sgd:  -398.2719185901021
new min fval from sgd:  -398.27547815645505
new min fval from sgd:  -398.28659358059934
new min fval from sgd:  -398.29652653683047
new min fval from sgd:  -398.29938723256237
new min fval from sgd:  -398.3029079240966
new min fval from sgd:  -398.3054910355848
new min fval from sgd:  -398.30599419195823
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0005912]
objective value function right now is: -398.2903710180213
new min fval from sgd:  -398.3311677913319
new min fval from sgd:  -398.35944813471764
new min fval from sgd:  -398.37294138448453
new min fval from sgd:  -398.3835036370609
new min fval from sgd:  -398.3844645025484
new min fval from sgd:  -398.3999052739173
new min fval from sgd:  -398.410027008164
new min fval from sgd:  -398.42425941921846
new min fval from sgd:  -398.4311435807674
new min fval from sgd:  -398.4324191141804
new min fval from sgd:  -398.4406474141093
new min fval from sgd:  -398.4430839998386
new min fval from sgd:  -398.44388360045764
new min fval from sgd:  -398.4454420716056
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00048235]
objective value function right now is: -398.42342129516464
min fval:  -398.4454420716056
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-14.8713,   1.4155],
        [ 14.1887,  -1.2877],
        [ 11.1221,  -5.8101],
        [  9.5308,   2.3607],
        [ 10.7342,  -4.2544],
        [ -1.5163,  -0.0786],
        [ -1.5163,  -0.0786],
        [ -8.0776, -12.2118],
        [  6.2430, -10.6194],
        [ 10.4532,  -9.6971]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  9.3319, -10.7756,  -6.6130, -14.2905,  -8.0770,  -3.1599,  -3.1599,
         -9.5040,  -9.1171,  -8.9469], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-6.8104e-01, -6.6100e-01, -6.4807e-01, -1.2160e-01, -2.0473e-01,
         -7.5314e-03, -7.5314e-03, -1.0194e-02, -1.0865e-01, -2.7439e-01],
        [-6.8104e-01, -6.6100e-01, -6.4807e-01, -1.2160e-01, -2.0473e-01,
         -7.5314e-03, -7.5314e-03, -1.0194e-02, -1.0865e-01, -2.7439e-01],
        [-6.8104e-01, -6.6100e-01, -6.4807e-01, -1.2160e-01, -2.0473e-01,
         -7.5314e-03, -7.5314e-03, -1.0194e-02, -1.0865e-01, -2.7439e-01],
        [-6.8104e-01, -6.6100e-01, -6.4807e-01, -1.2160e-01, -2.0473e-01,
         -7.5314e-03, -7.5314e-03, -1.0194e-02, -1.0865e-01, -2.7439e-01],
        [-1.3801e+01,  8.7670e+00,  4.0989e+00,  5.1417e+00,  2.1417e+00,
         -1.3125e-01, -1.3125e-01,  9.7377e+00,  9.5022e+00,  8.5834e+00],
        [ 1.1752e+01, -8.6766e+00, -7.2262e+00, -7.6193e+00, -4.9456e+00,
         -7.7586e-02, -7.7586e-02, -1.1800e+01, -9.7293e+00, -8.0544e+00],
        [-6.8104e-01, -6.6100e-01, -6.4807e-01, -1.2160e-01, -2.0473e-01,
         -7.5314e-03, -7.5314e-03, -1.0194e-02, -1.0865e-01, -2.7439e-01],
        [-6.8104e-01, -6.6100e-01, -6.4807e-01, -1.2160e-01, -2.0473e-01,
         -7.5314e-03, -7.5314e-03, -1.0194e-02, -1.0865e-01, -2.7439e-01],
        [-1.2386e+01,  6.9985e+00,  3.6679e+00,  3.8487e+00,  1.7650e+00,
          1.0081e-02,  1.0081e-02,  9.1314e+00,  8.5898e+00,  7.7864e+00],
        [-1.0986e+01,  5.6366e+00,  3.1381e+00,  2.5105e+00,  1.2089e+00,
          9.8986e-02,  9.8987e-02,  7.9549e+00,  7.5499e+00,  6.9444e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.1654, -2.1654, -2.1654, -2.1654, -3.7836,  3.0089, -2.1654, -2.1654,
        -3.9664, -4.0794], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.1068e-02, -1.1069e-02, -1.1068e-02, -1.1069e-02, -1.0130e+01,
          1.4489e+01, -1.1068e-02, -1.1068e-02, -7.2395e+00, -5.1674e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 11.9841,   9.8651],
        [-16.6293,  -3.6551],
        [-15.8524,  -4.5074],
        [-14.9160,  15.0153],
        [ -3.0853,   0.5587],
        [ 13.2388,  -1.5917],
        [ 14.8349,   2.3534],
        [ -3.0558,   0.5142],
        [  3.0097,  12.4757],
        [-15.7055,  -6.3214]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  6.7443,  -3.4393,  -2.9351,  12.9034,  -5.3391, -15.5417,  -6.9275,
         -5.3540,  10.7622,  -1.2198], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.9823e-01, -4.6384e-03, -6.2005e-01, -3.7519e-01,  9.8562e-02,
          4.0482e+00,  1.3618e+00,  5.4693e-02, -7.3184e+00, -3.0399e+01],
        [-2.4421e+00, -9.6404e-02,  8.5986e-02, -1.4692e-01,  8.8891e-03,
         -1.1559e-02, -1.4867e+00,  1.0092e-02, -1.1584e-01,  2.3582e-01],
        [-2.4375e+00, -9.6479e-02,  8.6808e-02, -1.4623e-01,  8.9207e-03,
         -1.4363e-02, -1.4913e+00,  1.0127e-02, -1.1562e-01,  2.3753e-01],
        [-2.9195e-01, -5.5328e-01, -2.4086e+00,  6.5132e+00, -1.8427e-02,
         -1.0001e+00, -6.4843e+00, -1.7699e-02,  4.9538e-01, -2.8338e-01],
        [-4.0990e+01,  1.1001e+01,  1.3816e+01, -3.4971e+00, -6.9280e-02,
         -1.4982e+01, -8.2059e+00,  6.3658e-02, -1.2283e+01,  6.1620e+00],
        [-7.6346e+00,  4.4886e+00,  5.3974e+00, -6.1124e+00,  8.6874e-02,
         -1.1867e+01, -1.0372e+01, -1.2995e-02, -1.8547e+01,  9.0443e+00],
        [ 4.0891e+00,  5.3391e+00,  1.8226e+01, -9.2601e-01,  4.4855e-01,
          3.7886e+00, -8.8196e-01,  4.9948e-01,  2.9829e+00, -2.3282e+00],
        [-3.7724e-01,  6.7252e+00,  1.3736e+01, -2.8076e+01,  1.1475e+00,
         -6.1159e+00, -1.6283e+00,  1.0035e+00, -1.5009e+01,  4.4694e+00],
        [ 3.0907e+00,  5.8529e+00,  8.0407e+00, -1.0946e+01,  1.6023e-01,
         -1.2815e+00, -1.1425e+01,  3.8021e-01,  2.3257e+00,  1.9473e+00],
        [-2.5216e+00, -8.7712e-02,  9.9922e-03, -2.2453e-01,  5.2812e-03,
         -1.0787e-01, -9.0104e-01,  6.1356e-03, -2.2764e-01,  4.1944e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-5.2872, -6.1993, -6.1988, -7.6750, -7.7866, -2.1617, -4.3589, -2.0506,
        -8.4666, -6.4482], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  4.3582,  -1.0650,  -1.0636,   1.7085,  22.7529,  -7.3347,   0.3882,
          -3.6289,   3.5281,  -0.9368],
        [ -3.9918,   1.0660,   1.0638,  -1.7515, -22.7480,   7.3897,  -0.3093,
           3.6993,  -3.7320,   0.9375]], device='cuda:0'))])
xi:  [-0.00144914]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 508.33448080546617
W_T_median: 235.3324139466049
W_T_pctile_5: 4.2301328117844154e-05
W_T_CVAR_5_pct: -114.33007289074183
Average q (qsum/M+1):  49.73384734122984
Optimal xi:  [-0.00144914]
Expected(across Rb) median(across samples) p_equity:  0.3380846271912257
obj fun:  tensor(-398.4454, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 10.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -9.0259,   1.6200],
        [  9.0716,   0.0578],
        [  7.1793,  -6.1605],
        [  7.5381,   3.2505],
        [  9.6693,  -5.0357],
        [ -1.3449,   0.3001],
        [  1.4948,  -2.5906],
        [ -6.5573, -10.0162],
        [  4.3191,  -8.2380],
        [  5.3129,  -8.5238]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 7.3420, -8.4146, -6.0056, -8.9126, -4.1491, -2.2581, -5.7163, -7.4441,
        -7.8151, -8.4529], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-9.2020e+00,  5.4065e+00,  2.8772e+00,  4.8714e+00,  1.6951e+00,
          9.5674e-02,  2.6977e-01,  6.2889e+00,  6.2490e+00,  3.6956e+00],
        [ 1.0659e+01, -5.1375e+00, -5.5630e+00, -5.5762e+00, -4.6171e+00,
          1.2856e-01, -1.1658e+00, -6.2914e+00, -7.4374e+00, -5.2700e+00],
        [-5.2413e-01, -1.7724e-01, -5.6925e-01, -4.3312e-02, -8.8777e-01,
         -1.0492e-02, -8.0194e-03,  1.3355e-01, -1.9131e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-8.7807e+00,  4.7062e+00,  2.8038e+00,  3.8880e+00,  1.8033e+00,
          1.4201e-01,  1.9636e-01,  6.0533e+00,  5.7920e+00,  3.4498e+00],
        [-8.5582e+00,  4.6390e+00,  2.6593e+00,  3.6119e+00,  1.8101e+00,
          1.4993e-01,  1.7458e-01,  5.8599e+00,  5.7528e+00,  3.3968e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.9148, -0.9148, -0.9148, -0.9148, -3.4273,  4.5581, -0.9151, -0.9148,
        -3.4455, -3.4434], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 6.4029e-03,  6.4029e-03,  6.4029e-03,  6.4029e-03, -6.4204e+00,
          1.6896e+01,  6.4026e-03,  6.4029e-03, -5.7940e+00, -5.5530e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  7.7801,   8.5117],
        [-12.8039,  -3.3197],
        [-14.0960,  -3.3781],
        [-10.3438,  13.1059],
        [ -6.7274,   3.2029],
        [ 11.3494,  -0.2092],
        [ 10.9206,   1.8935],
        [  6.0962,  -1.4135],
        [ -0.2963,  10.7227],
        [-11.1179,  -4.1613]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.3157,  -1.9104,  -2.0167,  11.5895,   3.0902, -10.3992,  -3.2258,
        -11.9670,   9.7899,  -0.1907], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.1204e+00, -4.8217e-01, -1.2984e-01, -3.9602e+00,  2.8329e+00,
          4.0120e+00, -1.4919e-01,  2.1667e+00, -5.0218e-01, -1.3871e+01],
        [-1.3321e+00, -5.5194e-01, -4.4665e-01,  2.4663e+00, -5.7744e-01,
          8.3623e-01, -2.1840e+00,  1.8294e-02,  1.2065e+00, -2.0505e+00],
        [ 7.6812e-01,  3.7889e+00,  4.6433e+00, -6.0243e+00,  2.2376e-01,
         -6.6894e+00, -4.5433e+00, -4.5007e-02,  5.9901e+00, -4.0033e-01],
        [ 8.0559e-01, -3.0883e+00, -2.2335e+00,  6.0347e+00,  3.1022e+00,
         -1.0734e+01, -5.7229e+00, -2.3178e-02,  3.6155e+00, -7.1234e-01],
        [-1.8985e+01,  7.2784e+00,  1.0297e+01, -4.8164e+00,  8.6606e-01,
         -5.7802e+00, -6.1329e+00, -2.3282e+00, -4.0583e+00,  3.9973e+00],
        [-8.3172e+00,  7.8510e+00,  8.0024e+00, -1.6984e+01, -2.4242e+00,
         -8.1970e+00, -4.3623e+00, -4.1766e+00, -1.4006e+01,  5.7935e+00],
        [-1.0779e-01,  4.5688e-01,  3.0178e-01,  2.5827e+00,  2.1453e+00,
         -5.3619e+00, -2.7556e+00,  4.3528e-03,  5.2351e-01,  8.5108e-01],
        [-1.6100e+00,  3.8398e+00,  3.2810e+00, -1.0011e+01, -1.5281e+00,
         -4.8466e+00, -2.2404e+00,  6.6782e-01, -1.1001e+01,  8.8575e-01],
        [ 3.7104e+00,  7.5672e+00,  8.9019e+00, -1.6178e+01, -8.3096e-01,
         -1.7622e+00, -1.5875e+00,  5.6997e-01, -9.7621e-02, -8.4607e-01],
        [-1.1953e+01,  4.3930e+00,  6.1183e+00, -5.1416e+00, -3.5599e-01,
         -4.5375e+00, -4.5225e+00, -1.8014e+00, -2.6516e+00,  3.8139e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.8622, -4.0911, -5.7102, -5.2697, -4.8399,  0.2404, -5.4978, -0.0417,
        -4.6828, -4.0470], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  3.1935,  -1.0137,  -2.4010,   0.3391,  11.2912,  -6.6207,   1.0781,
          -3.5835,   2.2661,   6.5807],
        [ -2.8140,   1.0718,   2.4322,  -0.5105, -11.2830,   6.6898,  -0.9991,
           3.6558,  -2.4912,  -6.5667]], device='cuda:0'))])
loaded xi:  -34.98991
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 808.1706015737759
W_T_median: 592.6450607954835
W_T_pctile_5: -246.30676296883496
W_T_CVAR_5_pct: -379.12258834538136
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  1434.813017259938
Current xi:  [-13.457468]
objective value function right now is: 1434.813017259938
4.0% of gradient descent iterations done. Method = Adam
new min fval:  1360.436203469317
Current xi:  [2.1072648]
objective value function right now is: 1360.436203469317
6.0% of gradient descent iterations done. Method = Adam
new min fval:  1297.1116312267166
Current xi:  [16.653137]
objective value function right now is: 1297.1116312267166
8.0% of gradient descent iterations done. Method = Adam
new min fval:  1189.312170643134
Current xi:  [30.232111]
objective value function right now is: 1189.312170643134
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [41.604687]
objective value function right now is: 1194.3954191396895
12.0% of gradient descent iterations done. Method = Adam
new min fval:  1160.7135960285434
Current xi:  [50.83205]
objective value function right now is: 1160.7135960285434
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [56.092804]
objective value function right now is: 1212.3183650680273
16.0% of gradient descent iterations done. Method = Adam
new min fval:  1134.7245861701886
Current xi:  [59.143394]
objective value function right now is: 1134.7245861701886
18.0% of gradient descent iterations done. Method = Adam
new min fval:  1119.523278322953
Current xi:  [63.046886]
objective value function right now is: 1119.523278322953
20.0% of gradient descent iterations done. Method = Adam
new min fval:  1110.4411427794812
Current xi:  [65.09723]
objective value function right now is: 1110.4411427794812
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [65.86596]
objective value function right now is: 1148.022908126536
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [67.01504]
objective value function right now is: 1151.1736252675225
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [67.51916]
objective value function right now is: 1154.911263980622
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [67.76176]
objective value function right now is: 1118.037555235359
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.18402]
objective value function right now is: 1110.5433578854404
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [67.15394]
objective value function right now is: 1141.3561818227674
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [67.5644]
objective value function right now is: 1135.1957133581088
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [67.37142]
objective value function right now is: 1126.7130785577292
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.14961]
objective value function right now is: 1153.741074151751
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.61254]
objective value function right now is: 1159.2425702709702
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.0365]
objective value function right now is: 1120.5130206035556
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [64.28255]
objective value function right now is: 1149.8409595911098
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [65.60603]
objective value function right now is: 1114.7131203865144
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [66.76033]
objective value function right now is: 1140.2631228922457
50.0% of gradient descent iterations done. Method = Adam
new min fval:  1094.4588395955234
Current xi:  [66.55487]
objective value function right now is: 1094.4588395955234
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [67.420135]
objective value function right now is: 1185.776187711669
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [66.89476]
objective value function right now is: 1110.831801127384
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [67.829666]
objective value function right now is: 1250.7128968410777
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [68.49171]
objective value function right now is: 1312.7174910802123
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [66.342674]
objective value function right now is: 1122.4770111946516
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.31657]
objective value function right now is: 1140.6310732635666
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.32047]
objective value function right now is: 1144.647737248737
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [66.10483]
objective value function right now is: 1140.870016488302
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [69.78]
objective value function right now is: 1126.795989370373
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [69.24285]
objective value function right now is: 1138.7293013449696
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [69.36068]
objective value function right now is: 1094.950535334771
74.0% of gradient descent iterations done. Method = Adam
new min fval:  1082.2075276898993
Current xi:  [68.95454]
objective value function right now is: 1082.2075276898993
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.855316]
objective value function right now is: 1086.2268653193753
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [69.14395]
objective value function right now is: 1083.5075438431847
80.0% of gradient descent iterations done. Method = Adam
new min fval:  1081.0349400638115
Current xi:  [69.29447]
objective value function right now is: 1081.0349400638115
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [69.0937]
objective value function right now is: 1090.2267271599123
84.0% of gradient descent iterations done. Method = Adam
new min fval:  1080.9158026157463
Current xi:  [69.04304]
objective value function right now is: 1080.9158026157463
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.96161]
objective value function right now is: 1082.648690901295
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.81262]
objective value function right now is: 1087.6051180814495
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.4768]
objective value function right now is: 1103.7143648185581
new min fval from sgd:  1080.4522664211645
new min fval from sgd:  1080.4024436518096
new min fval from sgd:  1080.304615622532
new min fval from sgd:  1079.758271433553
new min fval from sgd:  1079.2814773222906
new min fval from sgd:  1079.0396923270048
new min fval from sgd:  1078.7983238315069
new min fval from sgd:  1078.4892995581827
new min fval from sgd:  1078.3652644971185
new min fval from sgd:  1078.1734429184926
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.354225]
objective value function right now is: 1100.1202966132696
new min fval from sgd:  1077.917914045136
new min fval from sgd:  1077.4866740123953
new min fval from sgd:  1077.113765601977
new min fval from sgd:  1076.9759349641897
new min fval from sgd:  1076.9694691706748
new min fval from sgd:  1076.9279059124485
new min fval from sgd:  1076.9158939585498
new min fval from sgd:  1076.739570471628
new min fval from sgd:  1076.587846965887
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.62266]
objective value function right now is: 1108.5559308124896
new min fval from sgd:  1076.546525971132
new min fval from sgd:  1076.4510590283965
new min fval from sgd:  1076.0369993279671
new min fval from sgd:  1075.5954403548212
new min fval from sgd:  1074.9751806148274
new min fval from sgd:  1074.7329462404716
new min fval from sgd:  1074.7134168175048
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.46601]
objective value function right now is: 1077.3497627434645
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.53869]
objective value function right now is: 1077.873861197961
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.595505]
objective value function right now is: 1075.3153474862597
min fval:  1074.7134168175048
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-15.5298,   2.6695],
        [ 14.9281,  -0.6972],
        [ 13.1112,  -6.7185],
        [ 10.6522,   1.5427],
        [ 10.4553,  -5.7326],
        [ -1.2199,   0.0858],
        [ -1.2199,   0.0859],
        [-60.0730, -11.6695],
        [ -0.6389, -12.3394],
        [ 11.0520, -11.7010]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  8.1494, -10.7426,  -6.5291, -13.3650,  -4.0119,  -4.0275,  -4.0275,
         -9.0801,  -8.8120,  -8.9768], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -1.0516,  -0.6047,  -0.6983,  -0.0590,  -1.0006,  -0.0174,  -0.0174,
          -0.0225,  -0.0993,  -0.2463],
        [ -1.0516,  -0.6047,  -0.6983,  -0.0590,  -1.0006,  -0.0174,  -0.0174,
          -0.0225,  -0.0993,  -0.2463],
        [ -1.0516,  -0.6047,  -0.6983,  -0.0590,  -1.0006,  -0.0174,  -0.0174,
          -0.0225,  -0.0993,  -0.2463],
        [ -1.0516,  -0.6047,  -0.6983,  -0.0590,  -1.0006,  -0.0174,  -0.0174,
          -0.0225,  -0.0993,  -0.2463],
        [-12.8533,  11.4280,   7.2473,   8.3675,   0.4381,  -0.3197,  -0.3196,
           8.4447,   9.4340,  11.2289],
        [ 11.1649, -10.5775,  -7.6682,  -8.4219,  -2.8910,  -0.2039,  -0.2038,
         -11.3744, -10.7694,  -9.9472],
        [ -1.0516,  -0.6047,  -0.6983,  -0.0590,  -1.0006,  -0.0174,  -0.0174,
          -0.0225,  -0.0993,  -0.2463],
        [ -1.0516,  -0.6047,  -0.6983,  -0.0590,  -1.0006,  -0.0174,  -0.0174,
          -0.0225,  -0.0993,  -0.2463],
        [-12.5260,   6.0015,   6.4306,   0.5908,  -0.2268,  -0.2600,  -0.2600,
          12.0502,   9.0283,  10.1850],
        [-10.4156,   3.0812,   4.5857,  -0.0277,  -0.1022,  -0.0851,  -0.0851,
           9.2692,   7.2214,   8.0501]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.1653, -2.1653, -2.1653, -2.1653, -3.6224,  2.9723, -2.1653, -2.1653,
        -3.9116, -3.5759], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0565,   0.0565,   0.0565,   0.0565, -11.7205,  15.3971,   0.0565,
           0.0565,  -9.1430,  -4.6026]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 11.1526,   9.4122],
        [ -9.9599,  -2.1575],
        [-17.7981,  -4.7590],
        [ -4.0491,  15.6228],
        [ -9.9354,   4.3162],
        [ 14.1175,  -0.4013],
        [ 13.8438,   1.6340],
        [ -2.1307,   0.7468],
        [  6.0449,  13.2161],
        [-15.4132,  -5.9472]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  5.8843,  -6.6204,  -3.1799,  12.3728,  -2.8928, -14.5175,  -7.6384,
         -6.7420,   9.7109,  -0.5424], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-9.7173e-01, -9.8394e-02, -4.8251e-01, -4.2158e-01,  3.7264e-01,
          4.0673e+00,  1.5829e+00, -1.4962e-01, -1.0098e+01, -3.3401e+01],
        [-3.7438e+00, -8.6084e-02, -3.1110e+00, -8.4383e-01, -7.3155e-01,
          1.0458e+00, -2.1345e-01,  3.9006e-03, -1.5615e+00, -3.7089e-01],
        [-1.8929e-02,  2.1557e+00,  4.1185e+00,  1.1341e+00, -4.3494e+00,
         -1.6700e+01, -6.3784e-01,  1.8939e+00,  4.8710e+00,  2.6238e+00],
        [ 3.5593e+00, -1.1529e+00, -1.3990e+01,  3.6176e+01, -2.3681e-01,
          7.5154e+00, -1.2649e+00,  1.3253e+00,  1.3990e+00, -6.5608e+00],
        [-4.4428e+01,  2.3798e+00,  1.1302e+01,  1.0519e+00,  1.1105e-01,
         -1.7329e+01, -2.1349e+01,  1.1466e-01, -1.9517e+01,  6.0818e+00],
        [-8.8815e+00,  1.4074e+00,  5.8931e+00, -9.9259e+00, -6.5392e-01,
         -1.6158e+01, -1.0264e+01,  1.1103e+00, -1.5741e+01,  8.6929e+00],
        [-3.7445e+00, -8.6116e-02, -3.1121e+00, -8.4368e-01, -7.3163e-01,
          1.0460e+00, -2.1314e-01,  3.9099e-03, -1.5615e+00, -3.7084e-01],
        [ 5.7882e-02,  2.1268e+00,  9.9170e+00, -1.0456e+01,  4.5571e-02,
         -5.7220e+00, -1.8073e+00,  1.1818e+00, -1.2548e+01,  3.1909e+00],
        [-5.0749e-01,  2.6992e-01,  1.3098e+01, -9.7356e-01,  7.0597e-01,
          5.6313e-01, -2.2490e+00, -1.4375e-01,  7.6901e-01, -6.9456e-01],
        [-3.2596e+01,  2.5524e-01,  7.7198e+00,  1.7026e+00,  1.7561e-01,
         -8.9328e+00, -1.2339e+01,  1.1745e-01, -1.4507e+01,  5.5520e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.9016, -5.6911, -9.0656, -3.6404, -6.8410, -2.6449, -5.6906, -1.6006,
        -9.0241, -6.3926], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  4.6521,  -2.5930,   3.2831,   0.1865,  24.5065,  -9.9765,  -2.5931,
          -3.0157,   1.1098,  11.0768],
        [ -4.2784,   2.5939,  -3.2533,  -0.3580, -24.4998,  10.0361,   2.5946,
           3.0871,  -1.2775, -11.0671]], device='cuda:0'))])
xi:  [68.51868]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 611.2266657053
W_T_median: 336.96016170578525
W_T_pctile_5: 69.51950121052761
W_T_CVAR_5_pct: -102.2251951942906
Average q (qsum/M+1):  47.776831842237904
Optimal xi:  [68.51868]
Expected(across Rb) median(across samples) p_equity:  0.3092386061946551
obj fun:  tensor(1074.7134, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 25.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -9.0259,   1.6200],
        [  9.0716,   0.0578],
        [  7.1793,  -6.1605],
        [  7.5381,   3.2505],
        [  9.6693,  -5.0357],
        [ -1.3449,   0.3001],
        [  1.4948,  -2.5906],
        [ -6.5573, -10.0162],
        [  4.3191,  -8.2380],
        [  5.3129,  -8.5238]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 7.3420, -8.4146, -6.0056, -8.9126, -4.1491, -2.2581, -5.7163, -7.4441,
        -7.8151, -8.4529], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-9.2020e+00,  5.4065e+00,  2.8772e+00,  4.8714e+00,  1.6951e+00,
          9.5674e-02,  2.6977e-01,  6.2889e+00,  6.2490e+00,  3.6956e+00],
        [ 1.0659e+01, -5.1375e+00, -5.5630e+00, -5.5762e+00, -4.6171e+00,
          1.2856e-01, -1.1658e+00, -6.2914e+00, -7.4374e+00, -5.2700e+00],
        [-5.2413e-01, -1.7724e-01, -5.6925e-01, -4.3312e-02, -8.8777e-01,
         -1.0492e-02, -8.0194e-03,  1.3355e-01, -1.9131e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-8.7807e+00,  4.7062e+00,  2.8038e+00,  3.8880e+00,  1.8033e+00,
          1.4201e-01,  1.9636e-01,  6.0533e+00,  5.7920e+00,  3.4498e+00],
        [-8.5582e+00,  4.6390e+00,  2.6593e+00,  3.6119e+00,  1.8101e+00,
          1.4993e-01,  1.7458e-01,  5.8599e+00,  5.7528e+00,  3.3968e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.9148, -0.9148, -0.9148, -0.9148, -3.4273,  4.5581, -0.9151, -0.9148,
        -3.4455, -3.4434], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 6.4029e-03,  6.4029e-03,  6.4029e-03,  6.4029e-03, -6.4204e+00,
          1.6896e+01,  6.4026e-03,  6.4029e-03, -5.7940e+00, -5.5530e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  7.7801,   8.5117],
        [-12.8039,  -3.3197],
        [-14.0960,  -3.3781],
        [-10.3438,  13.1059],
        [ -6.7274,   3.2029],
        [ 11.3494,  -0.2092],
        [ 10.9206,   1.8935],
        [  6.0962,  -1.4135],
        [ -0.2963,  10.7227],
        [-11.1179,  -4.1613]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.3157,  -1.9104,  -2.0167,  11.5895,   3.0902, -10.3992,  -3.2258,
        -11.9670,   9.7899,  -0.1907], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.1204e+00, -4.8217e-01, -1.2984e-01, -3.9602e+00,  2.8329e+00,
          4.0120e+00, -1.4919e-01,  2.1667e+00, -5.0218e-01, -1.3871e+01],
        [-1.3321e+00, -5.5194e-01, -4.4665e-01,  2.4663e+00, -5.7744e-01,
          8.3623e-01, -2.1840e+00,  1.8294e-02,  1.2065e+00, -2.0505e+00],
        [ 7.6812e-01,  3.7889e+00,  4.6433e+00, -6.0243e+00,  2.2376e-01,
         -6.6894e+00, -4.5433e+00, -4.5007e-02,  5.9901e+00, -4.0033e-01],
        [ 8.0559e-01, -3.0883e+00, -2.2335e+00,  6.0347e+00,  3.1022e+00,
         -1.0734e+01, -5.7229e+00, -2.3178e-02,  3.6155e+00, -7.1234e-01],
        [-1.8985e+01,  7.2784e+00,  1.0297e+01, -4.8164e+00,  8.6606e-01,
         -5.7802e+00, -6.1329e+00, -2.3282e+00, -4.0583e+00,  3.9973e+00],
        [-8.3172e+00,  7.8510e+00,  8.0024e+00, -1.6984e+01, -2.4242e+00,
         -8.1970e+00, -4.3623e+00, -4.1766e+00, -1.4006e+01,  5.7935e+00],
        [-1.0779e-01,  4.5688e-01,  3.0178e-01,  2.5827e+00,  2.1453e+00,
         -5.3619e+00, -2.7556e+00,  4.3528e-03,  5.2351e-01,  8.5108e-01],
        [-1.6100e+00,  3.8398e+00,  3.2810e+00, -1.0011e+01, -1.5281e+00,
         -4.8466e+00, -2.2404e+00,  6.6782e-01, -1.1001e+01,  8.8575e-01],
        [ 3.7104e+00,  7.5672e+00,  8.9019e+00, -1.6178e+01, -8.3096e-01,
         -1.7622e+00, -1.5875e+00,  5.6997e-01, -9.7621e-02, -8.4607e-01],
        [-1.1953e+01,  4.3930e+00,  6.1183e+00, -5.1416e+00, -3.5599e-01,
         -4.5375e+00, -4.5225e+00, -1.8014e+00, -2.6516e+00,  3.8139e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.8622, -4.0911, -5.7102, -5.2697, -4.8399,  0.2404, -5.4978, -0.0417,
        -4.6828, -4.0470], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  3.1935,  -1.0137,  -2.4010,   0.3391,  11.2912,  -6.6207,   1.0781,
          -3.5835,   2.2661,   6.5807],
        [ -2.8140,   1.0718,   2.4322,  -0.5105, -11.2830,   6.6898,  -0.9991,
           3.6558,  -2.4912,  -6.5667]], device='cuda:0'))])
loaded xi:  -34.98991
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 808.1706015737759
W_T_median: 592.6450607954835
W_T_pctile_5: -246.30676296883496
W_T_CVAR_5_pct: -379.12258834538136
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  4527.810567875194
Current xi:  [-12.9858465]
objective value function right now is: 4527.810567875194
4.0% of gradient descent iterations done. Method = Adam
new min fval:  4161.468160671841
Current xi:  [5.500546]
objective value function right now is: 4161.468160671841
6.0% of gradient descent iterations done. Method = Adam
new min fval:  3931.2500076686842
Current xi:  [22.555117]
objective value function right now is: 3931.2500076686842
8.0% of gradient descent iterations done. Method = Adam
new min fval:  3843.186622722627
Current xi:  [35.952076]
objective value function right now is: 3843.186622722627
10.0% of gradient descent iterations done. Method = Adam
new min fval:  3732.475278247305
Current xi:  [46.468082]
objective value function right now is: 3732.475278247305
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.798195]
objective value function right now is: 3782.373411170872
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  3709.1024570596833
Current xi:  [58.6065]
objective value function right now is: 3709.1024570596833
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [64.206474]
objective value function right now is: 3745.4346417777247
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [66.50787]
objective value function right now is: 3805.8105799964574
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.809906]
objective value function right now is: 3761.788144034008
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.938065]
objective value function right now is: 3754.825947638641
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.78183]
objective value function right now is: 3859.3649049981227
26.0% of gradient descent iterations done. Method = Adam
new min fval:  3681.5163554283995
Current xi:  [71.997986]
objective value function right now is: 3681.5163554283995
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  3653.324682064947
Current xi:  [71.47582]
objective value function right now is: 3653.324682064947
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.18402]
objective value function right now is: 3732.2177245818234
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [72.17909]
objective value function right now is: 3772.502058802629
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.22052]
objective value function right now is: 3797.2597299603694
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.169685]
objective value function right now is: 3716.9183497014455
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.21288]
objective value function right now is: 3793.736466879298
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.5031]
objective value function right now is: 3817.2612313669038
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.7864]
objective value function right now is: 3683.4086049532993
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.67774]
objective value function right now is: 3661.1836323093476
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.69694]
objective value function right now is: 3706.218536746112
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [67.47218]
objective value function right now is: 3753.3405757225837
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.566765]
objective value function right now is: 3747.7804807999028
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.65215]
objective value function right now is: 3701.1727812333397
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [69.74626]
objective value function right now is: 3711.9434234241658
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [71.44695]
objective value function right now is: 3986.793461879453
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [70.26315]
objective value function right now is: 3705.734703120912
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.14855]
objective value function right now is: 3803.419298558494
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [69.96965]
objective value function right now is: 3813.4385841755975
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.93795]
objective value function right now is: 3681.6101883412034
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.225464]
objective value function right now is: 3734.0447861502926
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [69.33735]
objective value function right now is: 3725.9987938181957
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.12204]
objective value function right now is: 3683.885121612149
72.0% of gradient descent iterations done. Method = Adam
new min fval:  3637.489985346825
Current xi:  [70.44191]
objective value function right now is: 3637.489985346825
74.0% of gradient descent iterations done. Method = Adam
new min fval:  3633.182113168442
Current xi:  [70.79834]
objective value function right now is: 3633.182113168442
76.0% of gradient descent iterations done. Method = Adam
new min fval:  3625.2428845834406
Current xi:  [70.77348]
objective value function right now is: 3625.2428845834406
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.66509]
objective value function right now is: 3629.026230739254
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.93164]
objective value function right now is: 3639.042263303484
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.77727]
objective value function right now is: 3635.68402769435
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.70863]
objective value function right now is: 3640.2895466530713
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.65912]
objective value function right now is: 3656.0749317956866
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.69968]
objective value function right now is: 3626.3043100992486
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.68648]
objective value function right now is: 3625.484517741495
new min fval from sgd:  3624.7713266522865
new min fval from sgd:  3624.541965171512
new min fval from sgd:  3624.2186440178493
new min fval from sgd:  3623.5383075702307
new min fval from sgd:  3623.269228289682
new min fval from sgd:  3623.0638590270883
new min fval from sgd:  3622.445732560161
new min fval from sgd:  3621.7449760169566
new min fval from sgd:  3621.697381200215
new min fval from sgd:  3621.498254553802
new min fval from sgd:  3621.234273829127
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.12746]
objective value function right now is: 3636.423390617581
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.14111]
objective value function right now is: 3627.1852888080225
new min fval from sgd:  3621.1465981793635
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.323906]
objective value function right now is: 3634.2301017746054
new min fval from sgd:  3621.047883036332
new min fval from sgd:  3620.8199320617264
new min fval from sgd:  3620.682158702776
new min fval from sgd:  3620.6703612171023
new min fval from sgd:  3620.4951435507196
new min fval from sgd:  3620.279645657313
new min fval from sgd:  3620.052173576644
new min fval from sgd:  3619.9054293985796
new min fval from sgd:  3619.7951914529867
new min fval from sgd:  3619.7159652114374
new min fval from sgd:  3619.6864485460824
new min fval from sgd:  3619.652849908669
new min fval from sgd:  3619.5932150179324
new min fval from sgd:  3619.4777001132843
new min fval from sgd:  3619.392563407871
new min fval from sgd:  3619.2709151781614
new min fval from sgd:  3619.1968710614146
new min fval from sgd:  3619.131925562414
new min fval from sgd:  3619.0984744106563
new min fval from sgd:  3619.0198353483306
new min fval from sgd:  3618.919739985952
new min fval from sgd:  3618.7783616405486
new min fval from sgd:  3618.721119372707
new min fval from sgd:  3618.6504182912977
new min fval from sgd:  3618.5795542023093
new min fval from sgd:  3618.5328522136942
new min fval from sgd:  3618.483286244525
new min fval from sgd:  3618.4591289400814
new min fval from sgd:  3618.430962356721
new min fval from sgd:  3618.3798066242225
new min fval from sgd:  3618.332813985084
new min fval from sgd:  3618.2672119423123
new min fval from sgd:  3618.206039498589
new min fval from sgd:  3618.1716613455415
new min fval from sgd:  3618.0986324607143
new min fval from sgd:  3617.999819328955
new min fval from sgd:  3617.853303438667
new min fval from sgd:  3617.8230581957982
new min fval from sgd:  3617.7673977074255
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.26394]
objective value function right now is: 3618.8192804390314
new min fval from sgd:  3617.76278604881
new min fval from sgd:  3617.73719051398
new min fval from sgd:  3617.6009851022022
new min fval from sgd:  3617.4817080331222
new min fval from sgd:  3617.448481832378
new min fval from sgd:  3617.366877405757
new min fval from sgd:  3617.3210170847206
new min fval from sgd:  3617.29999262297
new min fval from sgd:  3617.266019781856
new min fval from sgd:  3617.2645138180824
new min fval from sgd:  3617.245464619483
new min fval from sgd:  3617.234950677217
new min fval from sgd:  3617.227274475804
new min fval from sgd:  3617.182433638653
new min fval from sgd:  3617.167332235826
new min fval from sgd:  3617.1633472657622
new min fval from sgd:  3617.155926934408
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.32441]
objective value function right now is: 3618.742517495494
min fval:  3617.155926934408
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.6022e+01,  2.8697e+00],
        [ 1.5104e+01, -5.4950e-01],
        [ 1.2535e+01, -7.1081e+00],
        [ 1.0356e+01,  1.9015e+00],
        [ 1.1559e+01, -7.0141e+00],
        [-1.2541e+00,  3.4584e-02],
        [-1.2541e+00,  3.4559e-02],
        [-5.7375e+01, -1.1743e+01],
        [ 5.0556e-01, -1.2460e+01],
        [ 1.2454e+01, -1.1179e+01]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  7.7616, -10.8461,  -6.6200, -14.2737,  -3.1714,  -4.3094,  -4.3092,
         -9.1938,  -8.9516,  -8.9440], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-8.5096e-01, -6.1549e-01, -2.8715e-01,  2.3300e-02, -1.1024e+00,
         -6.4708e-03, -6.4716e-03,  7.7043e-02,  1.0651e-02, -5.2341e-02],
        [-8.5096e-01, -6.1549e-01, -2.8715e-01,  2.3300e-02, -1.1024e+00,
         -6.4708e-03, -6.4716e-03,  7.7043e-02,  1.0651e-02, -5.2341e-02],
        [-8.5096e-01, -6.1549e-01, -2.8715e-01,  2.3301e-02, -1.1024e+00,
         -6.4708e-03, -6.4716e-03,  7.7043e-02,  1.0651e-02, -5.2341e-02],
        [-8.5096e-01, -6.1549e-01, -2.8715e-01,  2.3300e-02, -1.1024e+00,
         -6.4708e-03, -6.4716e-03,  7.7043e-02,  1.0651e-02, -5.2341e-02],
        [-1.3102e+01,  1.1913e+01,  7.0784e+00,  8.5643e+00,  9.3724e-01,
         -1.4903e-01, -1.4884e-01,  8.4977e+00,  1.0530e+01,  9.9329e+00],
        [ 1.1873e+01, -1.1110e+01, -8.0274e+00, -9.4281e+00, -3.7254e+00,
         -9.2499e-02, -9.2358e-02, -1.0693e+01, -1.0620e+01, -9.7814e+00],
        [-8.5096e-01, -6.1549e-01, -2.8715e-01,  2.3301e-02, -1.1024e+00,
         -6.4708e-03, -6.4716e-03,  7.7043e-02,  1.0651e-02, -5.2341e-02],
        [-8.5096e-01, -6.1549e-01, -2.8715e-01,  2.3301e-02, -1.1024e+00,
         -6.4708e-03, -6.4716e-03,  7.7043e-02,  1.0651e-02, -5.2340e-02],
        [-1.2844e+01,  6.9718e+00,  5.8304e+00,  1.4157e+00,  1.0153e+00,
         -1.0548e-01, -1.0542e-01,  1.1989e+01,  9.7607e+00,  9.4407e+00],
        [-1.1744e+01,  4.9321e+00,  4.7680e+00,  3.4788e-01,  8.5367e-01,
         -5.2009e-02, -5.1967e-02,  1.1188e+01,  8.9974e+00,  8.5161e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.2209, -2.2209, -2.2209, -2.2209, -2.8177,  2.8507, -2.2209, -2.2209,
        -3.4486, -3.4205], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0330,   0.0330,   0.0330,   0.0330, -11.8405,  15.8915,   0.0330,
           0.0330,  -8.9859,  -6.6425]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  9.5174,  11.0933],
        [-12.7787,  -5.6313],
        [-16.9009,  -4.9027],
        [-17.2960,  13.3743],
        [ -0.3182,  34.5333],
        [ 13.8430,  -0.5476],
        [ 13.2950,   2.0760],
        [ -4.1967,   4.7638],
        [ -0.2044,  12.8062],
        [-15.0723,  -4.2426]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.5266,  -1.9682,  -4.6933,   9.7872,   2.5066, -13.9699,  -6.6714,
         -9.9006,  10.1424,  -3.7179], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.5255e+00, -2.1295e+01, -7.5825e-01, -1.9941e+00, -4.5878e+00,
          4.1768e+00,  1.2533e+00,  1.8205e+00, -6.0508e-01, -1.5467e+01],
        [-5.0017e+00,  6.6024e-01, -2.5542e+00, -1.5271e+00,  6.3654e-01,
         -6.6766e-01, -8.6927e-01, -1.2677e+00,  2.0065e-01,  1.1998e+00],
        [ 9.8568e-01,  4.2012e+00, -4.7788e+00, -2.4477e-01,  7.9590e-01,
         -1.3462e+01, -1.0009e+00, -4.7942e+00,  4.6853e+00,  8.9052e-01],
        [ 4.1825e+00, -2.6801e+00,  5.1688e+00,  2.0457e+01, -4.7800e+00,
          2.1476e+00, -1.2473e+00,  1.9234e-02,  5.1901e+00, -4.6503e+00],
        [-4.7274e+01,  8.0060e+00,  1.5389e+01,  3.2646e+00, -5.7322e-09,
         -2.2052e+01, -2.5618e+01, -3.9993e-03, -4.9532e+00,  5.0187e+00],
        [-1.2042e+01,  1.0423e+01,  5.3648e+00, -8.6706e+00, -6.2728e-07,
         -1.4507e+01, -1.3421e+01,  4.4698e-03, -1.4543e+01,  6.0238e+00],
        [-4.8933e+00,  5.6297e-01, -2.7378e+00, -1.4091e+00,  7.7967e-01,
         -5.4637e-01, -1.0173e+00, -1.3426e+00,  9.0287e-02,  1.2320e+00],
        [-1.9505e+00,  4.4731e+00,  1.2289e+01, -9.5554e+00, -1.8919e-03,
         -6.7627e+00, -1.4384e+00,  1.0479e-02, -1.6119e+01,  5.8074e+00],
        [-3.1411e-01,  8.6445e+00,  6.7561e+00,  4.8590e+00, -6.5766e-01,
          1.4299e+00, -2.1495e+00,  1.1160e+00, -4.3590e-01, -6.6326e-01],
        [-4.8220e+01,  7.5807e+00,  1.2107e+01,  5.4221e+00, -2.4899e-09,
         -2.2513e+01, -2.3253e+01, -1.9569e-03, -4.8742e+00,  3.8734e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -4.9564,  -6.0126,  -8.1722,  -3.2337,  -8.4762,  -2.8946,  -5.8905,
         -1.4274, -10.8531,  -7.7812], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  3.8160,  -4.4685,   3.2292,   0.1746,  22.6782, -11.4217,  -4.5882,
          -2.5421,   1.5203,  14.2365],
        [ -3.4377,   4.4734,  -3.1993,  -0.3460, -22.6706,  11.4862,   4.5943,
           2.6140,  -1.7348, -14.2241]], device='cuda:0'))])
xi:  [71.32302]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 591.6276654743152
W_T_median: 351.50437314222324
W_T_pctile_5: 72.18249383531828
W_T_CVAR_5_pct: -101.62937860534633
Average q (qsum/M+1):  47.24360902847782
Optimal xi:  [71.32302]
Expected(across Rb) median(across samples) p_equity:  0.2863306554655234
obj fun:  tensor(3617.1559, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -9.0259,   1.6200],
        [  9.0716,   0.0578],
        [  7.1793,  -6.1605],
        [  7.5381,   3.2505],
        [  9.6693,  -5.0357],
        [ -1.3449,   0.3001],
        [  1.4948,  -2.5906],
        [ -6.5573, -10.0162],
        [  4.3191,  -8.2380],
        [  5.3129,  -8.5238]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 7.3420, -8.4146, -6.0056, -8.9126, -4.1491, -2.2581, -5.7163, -7.4441,
        -7.8151, -8.4529], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-9.2020e+00,  5.4065e+00,  2.8772e+00,  4.8714e+00,  1.6951e+00,
          9.5674e-02,  2.6977e-01,  6.2889e+00,  6.2490e+00,  3.6956e+00],
        [ 1.0659e+01, -5.1375e+00, -5.5630e+00, -5.5762e+00, -4.6171e+00,
          1.2856e-01, -1.1658e+00, -6.2914e+00, -7.4374e+00, -5.2700e+00],
        [-5.2413e-01, -1.7724e-01, -5.6925e-01, -4.3312e-02, -8.8777e-01,
         -1.0492e-02, -8.0194e-03,  1.3355e-01, -1.9131e-01, -2.1281e-01],
        [-5.2406e-01, -1.7716e-01, -5.6921e-01, -4.3256e-02, -8.8757e-01,
         -1.0491e-02, -8.0224e-03,  1.3355e-01, -1.9132e-01, -2.1281e-01],
        [-8.7807e+00,  4.7062e+00,  2.8038e+00,  3.8880e+00,  1.8033e+00,
          1.4201e-01,  1.9636e-01,  6.0533e+00,  5.7920e+00,  3.4498e+00],
        [-8.5582e+00,  4.6390e+00,  2.6593e+00,  3.6119e+00,  1.8101e+00,
          1.4993e-01,  1.7458e-01,  5.8599e+00,  5.7528e+00,  3.3968e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.9148, -0.9148, -0.9148, -0.9148, -3.4273,  4.5581, -0.9151, -0.9148,
        -3.4455, -3.4434], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 6.4029e-03,  6.4029e-03,  6.4029e-03,  6.4029e-03, -6.4204e+00,
          1.6896e+01,  6.4026e-03,  6.4029e-03, -5.7940e+00, -5.5530e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  7.7801,   8.5117],
        [-12.8039,  -3.3197],
        [-14.0960,  -3.3781],
        [-10.3438,  13.1059],
        [ -6.7274,   3.2029],
        [ 11.3494,  -0.2092],
        [ 10.9206,   1.8935],
        [  6.0962,  -1.4135],
        [ -0.2963,  10.7227],
        [-11.1179,  -4.1613]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.3157,  -1.9104,  -2.0167,  11.5895,   3.0902, -10.3992,  -3.2258,
        -11.9670,   9.7899,  -0.1907], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.1204e+00, -4.8217e-01, -1.2984e-01, -3.9602e+00,  2.8329e+00,
          4.0120e+00, -1.4919e-01,  2.1667e+00, -5.0218e-01, -1.3871e+01],
        [-1.3321e+00, -5.5194e-01, -4.4665e-01,  2.4663e+00, -5.7744e-01,
          8.3623e-01, -2.1840e+00,  1.8294e-02,  1.2065e+00, -2.0505e+00],
        [ 7.6812e-01,  3.7889e+00,  4.6433e+00, -6.0243e+00,  2.2376e-01,
         -6.6894e+00, -4.5433e+00, -4.5007e-02,  5.9901e+00, -4.0033e-01],
        [ 8.0559e-01, -3.0883e+00, -2.2335e+00,  6.0347e+00,  3.1022e+00,
         -1.0734e+01, -5.7229e+00, -2.3178e-02,  3.6155e+00, -7.1234e-01],
        [-1.8985e+01,  7.2784e+00,  1.0297e+01, -4.8164e+00,  8.6606e-01,
         -5.7802e+00, -6.1329e+00, -2.3282e+00, -4.0583e+00,  3.9973e+00],
        [-8.3172e+00,  7.8510e+00,  8.0024e+00, -1.6984e+01, -2.4242e+00,
         -8.1970e+00, -4.3623e+00, -4.1766e+00, -1.4006e+01,  5.7935e+00],
        [-1.0779e-01,  4.5688e-01,  3.0178e-01,  2.5827e+00,  2.1453e+00,
         -5.3619e+00, -2.7556e+00,  4.3528e-03,  5.2351e-01,  8.5108e-01],
        [-1.6100e+00,  3.8398e+00,  3.2810e+00, -1.0011e+01, -1.5281e+00,
         -4.8466e+00, -2.2404e+00,  6.6782e-01, -1.1001e+01,  8.8575e-01],
        [ 3.7104e+00,  7.5672e+00,  8.9019e+00, -1.6178e+01, -8.3096e-01,
         -1.7622e+00, -1.5875e+00,  5.6997e-01, -9.7621e-02, -8.4607e-01],
        [-1.1953e+01,  4.3930e+00,  6.1183e+00, -5.1416e+00, -3.5599e-01,
         -4.5375e+00, -4.5225e+00, -1.8014e+00, -2.6516e+00,  3.8139e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.8622, -4.0911, -5.7102, -5.2697, -4.8399,  0.2404, -5.4978, -0.0417,
        -4.6828, -4.0470], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  3.1935,  -1.0137,  -2.4010,   0.3391,  11.2912,  -6.6207,   1.0781,
          -3.5835,   2.2661,   6.5807],
        [ -2.8140,   1.0718,   2.4322,  -0.5105, -11.2830,   6.6898,  -0.9991,
           3.6558,  -2.4912,  -6.5667]], device='cuda:0'))])
loaded xi:  -34.98991
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 808.1706015737759
W_T_median: 592.6450607954835
W_T_pctile_5: -246.30676296883496
W_T_CVAR_5_pct: -379.12258834538136
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  118.29041801214358
Current xi:  [-12.856977]
objective value function right now is: 118.29041801214358
4.0% of gradient descent iterations done. Method = Adam
new min fval:  112.80888896088796
Current xi:  [6.5648127]
objective value function right now is: 112.80888896088796
6.0% of gradient descent iterations done. Method = Adam
new min fval:  109.9345911931347
Current xi:  [23.926424]
objective value function right now is: 109.9345911931347
8.0% of gradient descent iterations done. Method = Adam
new min fval:  106.86472838473242
Current xi:  [37.834248]
objective value function right now is: 106.86472838473242
10.0% of gradient descent iterations done. Method = Adam
new min fval:  104.52910779857918
Current xi:  [49.004196]
objective value function right now is: 104.52910779857918
12.0% of gradient descent iterations done. Method = Adam
new min fval:  103.42338734040712
Current xi:  [56.668884]
objective value function right now is: 103.42338734040712
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [63.803432]
objective value function right now is: 104.89639278949355
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.57918]
objective value function right now is: 103.5539288709594
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.76391]
objective value function right now is: 104.29285048077053
20.0% of gradient descent iterations done. Method = Adam
new min fval:  102.94194549764305
Current xi:  [72.71179]
objective value function right now is: 102.94194549764305
22.0% of gradient descent iterations done. Method = Adam
new min fval:  102.61451250596927
Current xi:  [72.07023]
objective value function right now is: 102.61451250596927
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [72.71645]
objective value function right now is: 102.86842265944755
26.0% of gradient descent iterations done. Method = Adam
new min fval:  102.58473034711048
Current xi:  [73.84912]
objective value function right now is: 102.58473034711048
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [73.670906]
objective value function right now is: 103.17036210290762
30.0% of gradient descent iterations done. Method = Adam
new min fval:  102.1981496521518
Current xi:  [73.86193]
objective value function right now is: 102.1981496521518
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.227745]
objective value function right now is: 103.18191614344418
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.38012]
objective value function right now is: 110.53289903352177
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [69.37328]
objective value function right now is: 106.97601800845489
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.203445]
objective value function right now is: 103.38576051484927
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.33408]
objective value function right now is: 103.18492787616232
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [72.924355]
objective value function right now is: 103.42820404427287
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [73.80427]
objective value function right now is: 103.10224582349636
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.06801]
objective value function right now is: 103.2436951641808
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.4047]
objective value function right now is: 103.02573157212144
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.273125]
objective value function right now is: 105.64496864681037
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [73.20168]
objective value function right now is: 103.10477523968899
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [75.20585]
objective value function right now is: 105.96364608133703
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [73.64492]
objective value function right now is: 104.68457718301897
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [74.52664]
objective value function right now is: 104.85648270221881
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [73.92785]
objective value function right now is: 102.38139932511814
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [72.783325]
objective value function right now is: 105.17927905581554
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [72.60879]
objective value function right now is: 107.8338714782834
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [73.286964]
objective value function right now is: 104.37542724348046
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [73.28775]
objective value function right now is: 103.77500927165084
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [73.82576]
objective value function right now is: 102.97812728156573
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.172646]
objective value function right now is: 102.28525253982167
74.0% of gradient descent iterations done. Method = Adam
new min fval:  101.69669371408322
Current xi:  [74.558525]
objective value function right now is: 101.69669371408322
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.45436]
objective value function right now is: 102.05948102356604
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.267555]
objective value function right now is: 101.73601001608047
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.096954]
objective value function right now is: 102.18222197266759
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.41192]
objective value function right now is: 101.6979970932465
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.03834]
objective value function right now is: 101.81377464250714
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [73.818825]
objective value function right now is: 102.36088737822064
88.0% of gradient descent iterations done. Method = Adam
new min fval:  101.62373894214508
Current xi:  [74.03092]
objective value function right now is: 101.62373894214508
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.27061]
objective value function right now is: 102.26068858457582
new min fval from sgd:  101.61086084249196
new min fval from sgd:  101.58663314073232
new min fval from sgd:  101.57248713648518
new min fval from sgd:  101.56623658295695
new min fval from sgd:  101.56193004835883
new min fval from sgd:  101.55830887020208
new min fval from sgd:  101.54532865517618
new min fval from sgd:  101.53931061132843
new min fval from sgd:  101.53680648394008
new min fval from sgd:  101.53599837392589
new min fval from sgd:  101.51407739423635
new min fval from sgd:  101.48587361856256
new min fval from sgd:  101.47775868350602
new min fval from sgd:  101.47674058214626
new min fval from sgd:  101.47328385081919
new min fval from sgd:  101.46711926234792
new min fval from sgd:  101.46037858162796
new min fval from sgd:  101.458090631362
new min fval from sgd:  101.45661330856703
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.289566]
objective value function right now is: 101.54767817089791
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [73.99607]
objective value function right now is: 101.63024725666855
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.043205]
objective value function right now is: 101.77361162875538
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.02937]
objective value function right now is: 101.6382053430093
new min fval from sgd:  101.45570073744642
new min fval from sgd:  101.45505764955666
new min fval from sgd:  101.45504432212631
new min fval from sgd:  101.45347280308356
new min fval from sgd:  101.44755037131085
new min fval from sgd:  101.44235689708911
new min fval from sgd:  101.43608919589708
new min fval from sgd:  101.43164426643914
new min fval from sgd:  101.42899204734488
new min fval from sgd:  101.42840517376631
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.05219]
objective value function right now is: 101.46435019502412
min fval:  101.42840517376631
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.1266, -0.2107],
        [ 0.1266, -0.2107],
        [ 0.1266, -0.2107],
        [ 0.1266, -0.2107],
        [ 0.1266, -0.2107],
        [ 0.1266, -0.2107],
        [ 0.1266, -0.2107],
        [ 0.1266, -0.2107],
        [ 0.1266, -0.2107],
        [ 0.1266, -0.2107]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([0.2255, 0.2255, 0.2255, 0.2255, 0.2255, 0.2255, 0.2255, 0.2255, 0.2255,
        0.2255], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441,
         0.2441],
        [0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441,
         0.2441],
        [0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441,
         0.2441],
        [0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441,
         0.2441],
        [0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441,
         0.2441],
        [0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441,
         0.2441],
        [0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441,
         0.2441],
        [0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441,
         0.2441],
        [0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441,
         0.2441],
        [0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441, 0.2441,
         0.2441]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([0.3929, 0.3929, 0.3929, 0.3929, 0.3929, 0.3929, 0.3929, 0.3929, 0.3929,
        0.3929], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.7600, -1.7600, -1.7600, -1.7600, -1.7600, -1.7600, -1.7600, -1.7600,
         -1.7600, -1.7600]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  8.6737,   9.9699],
        [-12.5941,  -3.5108],
        [-13.8516,  -3.7829],
        [-10.8049,   3.9877],
        [  1.0291,   3.6940],
        [ 13.0524,  -0.6537],
        [ 13.2746,   1.1444],
        [ -2.1499,   0.3764],
        [  3.4990,  12.6715],
        [-14.5553,  -5.5122]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.2202,  -3.1433,  -2.6997,   6.2471,  -4.3716, -14.2506,  -8.1413,
         -3.8783,   9.4859,  -0.1945], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-9.8210e-01,  1.3191e-02,  3.9174e-02,  3.6928e+00, -5.1761e+00,
          3.5581e+00,  2.2884e+00, -2.3969e-01, -1.4269e+00, -2.1788e+01],
        [-1.8268e+00, -1.3488e-04,  2.3844e-03, -2.2092e-01, -4.5445e-02,
         -2.6439e-01, -1.8137e+00,  6.5724e-03, -3.3526e-01, -3.8123e-01],
        [-1.8269e+00, -1.3456e-04,  2.3843e-03, -2.2092e-01, -4.5447e-02,
         -2.6439e-01, -1.8138e+00,  6.5726e-03, -3.3525e-01, -3.8125e-01],
        [ 3.2770e+00,  1.8040e+00,  2.7107e+00,  3.9916e+00, -3.7240e+00,
          6.7323e-03, -9.2841e-01,  3.6424e-02,  2.7354e+00, -2.5553e+00],
        [-1.6429e+01,  7.0531e+00,  9.2223e+00, -2.8431e+00, -4.1636e-02,
         -7.6666e+00, -9.8148e+00, -3.2593e-01, -6.8425e+00,  6.3096e+00],
        [-1.0742e+01,  2.3660e+00,  2.8555e+00, -1.0620e+00, -2.2978e-02,
         -1.1555e+01, -1.1920e+01,  1.1565e-02, -1.3422e+01,  1.0083e+01],
        [-1.8270e+00, -1.3319e-04,  2.3843e-03, -2.2095e-01, -4.5456e-02,
         -2.6437e-01, -1.8139e+00,  6.5734e-03, -3.3523e-01, -3.8133e-01],
        [-1.2956e+00,  3.7310e+00,  1.6941e+00,  1.6125e+00,  2.0791e-02,
         -5.9252e+00, -1.9589e+00, -3.7997e-04, -1.4187e+01,  3.0860e+00],
        [ 3.2386e+00,  5.7655e+00,  5.9248e+00,  3.1437e+00, -4.5989e+00,
          3.1521e+00, -1.0366e+00,  3.8667e-02, -9.2135e-01, -1.5867e+00],
        [-1.5396e+00, -2.8685e-03, -5.9037e-04, -2.7348e-01, -4.5300e-02,
         -2.6032e-01, -1.4026e+00,  5.3732e-03, -4.1666e-01,  2.3554e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.9164, -4.9003, -4.9003, -4.5321, -7.7836, -3.2629, -4.9003, -1.4450,
        -6.9955, -5.2082], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  3.6665,  -0.1919,  -0.1920,   0.3124,  16.7857,  -6.0951,  -0.1920,
          -2.4956,   1.9558,  -0.1420],
        [ -3.3341,   0.1920,   0.1920,  -0.4828, -16.7800,   6.1329,   0.1921,
           2.5607,  -2.1517,   0.1429]], device='cuda:0'))])
xi:  [74.05577]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1145.3363736381332
W_T_median: 846.9366984797718
W_T_pctile_5: 73.47021667010414
W_T_CVAR_5_pct: -101.42752581926379
Average q (qsum/M+1):  35.00000787550403
Optimal xi:  [74.05577]
Expected(across Rb) median(across samples) p_equity:  0.2919797904789448
obj fun:  tensor(101.4284, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:268: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 999.0
-----------------------------------------------
