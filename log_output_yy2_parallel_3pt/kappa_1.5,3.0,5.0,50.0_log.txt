Starting at: 
03-02-23_14:15

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 50000
batchsize: 1000
remove neg:  False


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 5000, 'itbound_SGD_algorithms': 50000, 'nit_IterateAveragingStart': 45000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1066.2831615693476
Current xi:  [147.65747]
objective value function right now is: -1066.2831615693476
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1094.0444282821734
Current xi:  [191.13019]
objective value function right now is: -1094.0444282821734
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1502.8112312898659
Current xi:  [152.75047]
objective value function right now is: -1502.8112312898659
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1517.8053200272245
Current xi:  [104.51139]
objective value function right now is: -1517.8053200272245
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1518.838777234305
Current xi:  [101.589554]
objective value function right now is: -1518.838777234305
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1519.4922115138397
Current xi:  [101.72365]
objective value function right now is: -1519.4922115138397
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [102.59012]
objective value function right now is: -1510.8001427585205
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1522.2833962308334
Current xi:  [102.83421]
objective value function right now is: -1522.2833962308334
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.85721]
objective value function right now is: -1520.0080669949975
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.9532]
objective value function right now is: -1517.325585715621
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.7231]
objective value function right now is: -1515.284997707963
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1523.9087229523097
Current xi:  [105.14915]
objective value function right now is: -1523.9087229523097
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.69596]
objective value function right now is: -1523.604274113204
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [106.20816]
objective value function right now is: -1517.8996234632752
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1524.308428861822
Current xi:  [106.47105]
objective value function right now is: -1524.308428861822
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1525.3943411727312
Current xi:  [106.17561]
objective value function right now is: -1525.3943411727312
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.129135]
objective value function right now is: -1519.9541806855482
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.00426]
objective value function right now is: -1520.0891773237167
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.1672]
objective value function right now is: -1518.6312482995247
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.301735]
objective value function right now is: -1521.6775549094937
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.03716]
objective value function right now is: -1523.879458438355
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1525.4727662136606
Current xi:  [105.99064]
objective value function right now is: -1525.4727662136606
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.627846]
objective value function right now is: -1521.5912206120165
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.99507]
objective value function right now is: -1524.1133763225578
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.02236]
objective value function right now is: -1524.512129747604
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.02136]
objective value function right now is: -1511.3046513981378
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.84076]
objective value function right now is: -1521.7764420854974
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [105.7018]
objective value function right now is: -1521.2312990848422
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1528.0455854587983
Current xi:  [105.71625]
objective value function right now is: -1528.0455854587983
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.42872]
objective value function right now is: -1523.7646550804368
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.6066]
objective value function right now is: -1526.9459390630373
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.90072]
objective value function right now is: -1525.3710229449273
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.74135]
objective value function right now is: -1519.8312637361703
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.14974]
objective value function right now is: -1524.5382028400136
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.93308]
objective value function right now is: -1520.69935330133
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.18627]
objective value function right now is: -1527.8310554404504
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1528.4884376465816
Current xi:  [105.66396]
objective value function right now is: -1528.4884376465816
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.618866]
objective value function right now is: -1527.5247486604405
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.98887]
objective value function right now is: -1527.4641357887979
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.54807]
objective value function right now is: -1527.872378224203
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1550.8873964452064
Current xi:  [105.895164]
objective value function right now is: -1550.8873964452064
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1552.21609390788
Current xi:  [106.74076]
objective value function right now is: -1552.21609390788
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1552.6392663975612
Current xi:  [107.5897]
objective value function right now is: -1552.6392663975612
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1553.7089808689577
Current xi:  [108.513]
objective value function right now is: -1553.7089808689577
new min fval from sgd:  -1553.7438624944784
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.41363]
objective value function right now is: -1553.7438624944784
new min fval from sgd:  -1553.7504966024937
new min fval from sgd:  -1553.77208866895
new min fval from sgd:  -1553.7864008507697
new min fval from sgd:  -1553.883603646575
new min fval from sgd:  -1553.9401048959094
new min fval from sgd:  -1553.9993359882963
new min fval from sgd:  -1554.0209680469018
new min fval from sgd:  -1554.0710705048075
new min fval from sgd:  -1554.1219944778036
new min fval from sgd:  -1554.124228663553
new min fval from sgd:  -1554.1662336341376
new min fval from sgd:  -1554.189736926599
new min fval from sgd:  -1554.2056137381119
new min fval from sgd:  -1554.2532676401913
new min fval from sgd:  -1554.2685673373503
new min fval from sgd:  -1554.2939978428287
new min fval from sgd:  -1554.2957330129948
new min fval from sgd:  -1554.308678552321
new min fval from sgd:  -1554.3423015112373
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [110.57074]
objective value function right now is: -1553.3100430490133
new min fval from sgd:  -1554.3487442570008
new min fval from sgd:  -1554.366287711917
new min fval from sgd:  -1554.374537976929
new min fval from sgd:  -1554.393917430194
new min fval from sgd:  -1554.440759395154
new min fval from sgd:  -1554.4939580883529
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [111.14707]
objective value function right now is: -1553.3520583951754
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [111.42451]
objective value function right now is: -1553.4653482473636
new min fval from sgd:  -1554.5390290795333
new min fval from sgd:  -1554.5399243479737
new min fval from sgd:  -1554.5534254436159
new min fval from sgd:  -1554.5809767922256
new min fval from sgd:  -1554.5949304853852
new min fval from sgd:  -1554.6055570319584
new min fval from sgd:  -1554.6151776853856
new min fval from sgd:  -1554.6291950903887
new min fval from sgd:  -1554.645108469462
new min fval from sgd:  -1554.6560243058568
new min fval from sgd:  -1554.6578757710868
new min fval from sgd:  -1554.6697244236996
new min fval from sgd:  -1554.677490900444
new min fval from sgd:  -1554.6896473135291
new min fval from sgd:  -1554.692866616693
new min fval from sgd:  -1554.7086738210473
new min fval from sgd:  -1554.71409103707
new min fval from sgd:  -1554.7153340989767
new min fval from sgd:  -1554.7158320132964
new min fval from sgd:  -1554.7185349766582
new min fval from sgd:  -1554.722265204915
new min fval from sgd:  -1554.7296542031152
new min fval from sgd:  -1554.736415938715
new min fval from sgd:  -1554.7451056361542
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [111.957275]
objective value function right now is: -1554.6107266590968
new min fval from sgd:  -1554.7618854566292
new min fval from sgd:  -1554.7634492200668
new min fval from sgd:  -1554.770187348124
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.09089]
objective value function right now is: -1554.5362884850679
min fval:  -1554.770187348124
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 11.6929,  -0.9302],
        [ -1.1215,   0.6283],
        [ -1.1264,   0.6361],
        [-46.2312,  -8.5360],
        [-11.6312,   0.7131],
        [ 10.8896,  -0.6044],
        [ 10.7863,  -5.7537],
        [  6.3301,  -9.8152],
        [ -3.3582, -10.8559],
        [ 10.2384,  -0.4889]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -9.6328,  -2.9882,  -2.9889,  -8.1091,   8.8947,  -9.9901,  -9.1216,
         -8.5901,  -8.4247, -10.3784], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.4017e-01, -4.8285e-03, -4.8277e-03, -6.9557e-03, -8.3255e-01,
         -2.4649e-01, -3.3008e-01, -3.3293e-01, -1.7448e-01, -9.5126e-02],
        [-5.1312e+00,  1.7825e-03, -1.1825e-02, -7.0696e+00,  5.9779e+00,
         -3.8654e+00, -5.1562e+00, -1.0715e+01, -1.1158e+01, -2.9277e+00],
        [-3.0939e-01, -2.3684e-02, -3.6484e-02,  6.0239e+00, -4.5072e+00,
          7.5051e-03,  1.4440e+00,  8.6501e+00,  8.7184e+00,  4.9682e-02],
        [-4.9379e+00, -3.6260e-02, -4.9407e-02, -7.2227e+00,  6.2296e+00,
         -3.4671e+00, -5.1361e+00, -1.0919e+01, -1.0925e+01, -2.6807e+00],
        [-4.4017e-01, -4.8285e-03, -4.8277e-03, -6.9557e-03, -8.3256e-01,
         -2.4649e-01, -3.3007e-01, -3.3293e-01, -1.7448e-01, -9.5126e-02],
        [-6.5242e-01, -4.4589e-02, -5.7080e-02,  5.9767e+00, -4.4793e+00,
         -1.2276e-01,  1.7172e+00,  9.1618e+00,  8.6192e+00,  1.1037e-02],
        [-4.4017e-01, -4.8285e-03, -4.8277e-03, -6.9557e-03, -8.3255e-01,
         -2.4649e-01, -3.3008e-01, -3.3293e-01, -1.7448e-01, -9.5126e-02],
        [ 6.1704e+00, -2.0263e-02, -5.0155e-02,  7.1230e+00, -5.2878e+00,
          5.2167e+00,  3.7401e+00,  9.7723e+00,  1.0116e+01,  4.2963e+00],
        [ 5.2851e+00,  1.9980e-02,  1.6800e-02,  6.6702e+00, -5.1928e+00,
          4.3106e+00,  2.6635e+00,  9.8190e+00,  9.6970e+00,  3.1719e+00],
        [-5.0621e+00, -6.1230e-03, -2.0433e-02, -7.1079e+00,  6.1952e+00,
         -3.5920e+00, -5.2340e+00, -1.0763e+01, -1.1157e+01, -2.5850e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.4506,  3.6585, -5.9171,  3.1917, -1.4506, -6.0195, -1.4506, -6.3554,
        -6.1607,  3.4511], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-5.0221e-03,  8.7239e+00, -5.1280e+00,  8.4856e+00, -5.0221e-03,
         -5.3242e+00, -5.0222e-03, -8.9201e+00, -7.4215e+00,  8.4777e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.3894,   6.0826],
        [ -0.5481,   8.4955],
        [  2.2382,  12.3756],
        [-11.6043,  -4.5554],
        [ -7.4432,   9.6988],
        [  7.0525,   8.0402],
        [  2.9343,  12.4962],
        [-10.7128,  -5.3124],
        [ -0.4267,   4.5610],
        [ 14.8735,   1.2853]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -1.5965,  -2.0654,   9.2767,  -2.1861,   9.7052,   4.0337,   8.5827,
         -3.8090,  -1.5558, -10.6808], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.7474e-01, -5.0528e-01, -4.6095e-01,  5.3169e+00, -4.7100e+00,
         -4.9148e-01,  8.7909e-01,  5.3503e+00, -4.9421e-01,  3.8699e-01],
        [-3.7464e-01, -5.0510e-01, -4.6114e-01,  5.3170e+00, -4.7099e+00,
         -4.9157e-01,  8.7894e-01,  5.3503e+00, -4.9420e-01,  3.8692e-01],
        [ 2.2402e+00,  6.7797e+00,  9.2449e-01, -1.3985e+00,  9.9266e+00,
         -8.3754e-01,  2.9096e-01,  6.9411e-01,  3.2372e-01, -1.3993e+01],
        [-3.7474e-01, -5.0528e-01, -4.6094e-01,  5.3169e+00, -4.7100e+00,
         -4.9148e-01,  8.7909e-01,  5.3503e+00, -4.9421e-01,  3.8699e-01],
        [-9.4475e-02, -7.5584e-03, -1.6316e+01,  9.2744e+00, -2.4714e+00,
         -7.7710e+00, -1.9929e+01,  5.2294e+00, -3.1216e-01, -9.2552e+00],
        [-3.7464e-01, -5.0510e-01, -4.6114e-01,  5.3170e+00, -4.7099e+00,
         -4.9157e-01,  8.7894e-01,  5.3503e+00, -4.9420e-01,  3.8693e-01],
        [-3.7450e-01, -5.0485e-01, -4.6142e-01,  5.3170e+00, -4.7099e+00,
         -4.9171e-01,  8.7873e-01,  5.3503e+00, -4.9419e-01,  3.8683e-01],
        [-3.7465e-01, -5.0512e-01, -4.6112e-01,  5.3170e+00, -4.7099e+00,
         -4.9156e-01,  8.7895e-01,  5.3503e+00, -4.9420e-01,  3.8693e-01],
        [-3.7454e-01, -5.0492e-01, -4.6134e-01,  5.3170e+00, -4.7099e+00,
         -4.9166e-01,  8.7879e-01,  5.3503e+00, -4.9419e-01,  3.8686e-01],
        [-4.6554e-03, -1.5608e-03,  1.3497e+01, -2.2570e+01,  2.0261e+01,
         -5.0165e-01,  7.7441e+00, -1.2990e+01, -6.2045e-02,  2.5905e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-6.0434, -6.0434, -8.6789, -6.0434, -1.7464, -6.0434, -6.0435, -6.0434,
        -6.0435, -0.9423], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 3.0972,  3.0971,  0.2394,  3.0971, -7.1719,  3.0972,  3.0971,  3.0971,
          3.0972,  0.3109],
        [-3.0969, -3.0971, -0.6070, -3.0971,  7.1715, -3.0969, -3.0970, -3.0971,
         -3.0969, -0.3915]], device='cuda:0'))])
xi:  [111.97136]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 529.9406533023833
W_T_median: 306.5860615901479
W_T_pctile_5: 112.03693888907064
W_T_CVAR_5_pct: -10.581799066536858
Average q (qsum/M+1):  50.666082566784276
Optimal xi:  [111.97136]
Expected(across Rb) median(across samples) p_equity:  0.28080589945117634
obj fun:  tensor(-1554.7702, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 11.6929,  -0.9302],
        [ -1.1215,   0.6283],
        [ -1.1264,   0.6361],
        [-46.2312,  -8.5360],
        [-11.6312,   0.7131],
        [ 10.8896,  -0.6044],
        [ 10.7863,  -5.7537],
        [  6.3301,  -9.8152],
        [ -3.3582, -10.8559],
        [ 10.2384,  -0.4889]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -9.6328,  -2.9882,  -2.9889,  -8.1091,   8.8947,  -9.9901,  -9.1216,
         -8.5901,  -8.4247, -10.3784], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.4017e-01, -4.8285e-03, -4.8277e-03, -6.9557e-03, -8.3255e-01,
         -2.4649e-01, -3.3008e-01, -3.3293e-01, -1.7448e-01, -9.5126e-02],
        [-5.1312e+00,  1.7825e-03, -1.1825e-02, -7.0696e+00,  5.9779e+00,
         -3.8654e+00, -5.1562e+00, -1.0715e+01, -1.1158e+01, -2.9277e+00],
        [-3.0939e-01, -2.3684e-02, -3.6484e-02,  6.0239e+00, -4.5072e+00,
          7.5051e-03,  1.4440e+00,  8.6501e+00,  8.7184e+00,  4.9682e-02],
        [-4.9379e+00, -3.6260e-02, -4.9407e-02, -7.2227e+00,  6.2296e+00,
         -3.4671e+00, -5.1361e+00, -1.0919e+01, -1.0925e+01, -2.6807e+00],
        [-4.4017e-01, -4.8285e-03, -4.8277e-03, -6.9557e-03, -8.3256e-01,
         -2.4649e-01, -3.3007e-01, -3.3293e-01, -1.7448e-01, -9.5126e-02],
        [-6.5242e-01, -4.4589e-02, -5.7080e-02,  5.9767e+00, -4.4793e+00,
         -1.2276e-01,  1.7172e+00,  9.1618e+00,  8.6192e+00,  1.1037e-02],
        [-4.4017e-01, -4.8285e-03, -4.8277e-03, -6.9557e-03, -8.3255e-01,
         -2.4649e-01, -3.3008e-01, -3.3293e-01, -1.7448e-01, -9.5126e-02],
        [ 6.1704e+00, -2.0263e-02, -5.0155e-02,  7.1230e+00, -5.2878e+00,
          5.2167e+00,  3.7401e+00,  9.7723e+00,  1.0116e+01,  4.2963e+00],
        [ 5.2851e+00,  1.9980e-02,  1.6800e-02,  6.6702e+00, -5.1928e+00,
          4.3106e+00,  2.6635e+00,  9.8190e+00,  9.6970e+00,  3.1719e+00],
        [-5.0621e+00, -6.1230e-03, -2.0433e-02, -7.1079e+00,  6.1952e+00,
         -3.5920e+00, -5.2340e+00, -1.0763e+01, -1.1157e+01, -2.5850e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.4506,  3.6585, -5.9171,  3.1917, -1.4506, -6.0195, -1.4506, -6.3554,
        -6.1607,  3.4511], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-5.0221e-03,  8.7239e+00, -5.1280e+00,  8.4856e+00, -5.0221e-03,
         -5.3242e+00, -5.0222e-03, -8.9201e+00, -7.4215e+00,  8.4777e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.3894,   6.0826],
        [ -0.5481,   8.4955],
        [  2.2382,  12.3756],
        [-11.6043,  -4.5554],
        [ -7.4432,   9.6988],
        [  7.0525,   8.0402],
        [  2.9343,  12.4962],
        [-10.7128,  -5.3124],
        [ -0.4267,   4.5610],
        [ 14.8735,   1.2853]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -1.5965,  -2.0654,   9.2767,  -2.1861,   9.7052,   4.0337,   8.5827,
         -3.8090,  -1.5558, -10.6808], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.7474e-01, -5.0528e-01, -4.6095e-01,  5.3169e+00, -4.7100e+00,
         -4.9148e-01,  8.7909e-01,  5.3503e+00, -4.9421e-01,  3.8699e-01],
        [-3.7464e-01, -5.0510e-01, -4.6114e-01,  5.3170e+00, -4.7099e+00,
         -4.9157e-01,  8.7894e-01,  5.3503e+00, -4.9420e-01,  3.8692e-01],
        [ 2.2402e+00,  6.7797e+00,  9.2449e-01, -1.3985e+00,  9.9266e+00,
         -8.3754e-01,  2.9096e-01,  6.9411e-01,  3.2372e-01, -1.3993e+01],
        [-3.7474e-01, -5.0528e-01, -4.6094e-01,  5.3169e+00, -4.7100e+00,
         -4.9148e-01,  8.7909e-01,  5.3503e+00, -4.9421e-01,  3.8699e-01],
        [-9.4475e-02, -7.5584e-03, -1.6316e+01,  9.2744e+00, -2.4714e+00,
         -7.7710e+00, -1.9929e+01,  5.2294e+00, -3.1216e-01, -9.2552e+00],
        [-3.7464e-01, -5.0510e-01, -4.6114e-01,  5.3170e+00, -4.7099e+00,
         -4.9157e-01,  8.7894e-01,  5.3503e+00, -4.9420e-01,  3.8693e-01],
        [-3.7450e-01, -5.0485e-01, -4.6142e-01,  5.3170e+00, -4.7099e+00,
         -4.9171e-01,  8.7873e-01,  5.3503e+00, -4.9419e-01,  3.8683e-01],
        [-3.7465e-01, -5.0512e-01, -4.6112e-01,  5.3170e+00, -4.7099e+00,
         -4.9156e-01,  8.7895e-01,  5.3503e+00, -4.9420e-01,  3.8693e-01],
        [-3.7454e-01, -5.0492e-01, -4.6134e-01,  5.3170e+00, -4.7099e+00,
         -4.9166e-01,  8.7879e-01,  5.3503e+00, -4.9419e-01,  3.8686e-01],
        [-4.6554e-03, -1.5608e-03,  1.3497e+01, -2.2570e+01,  2.0261e+01,
         -5.0165e-01,  7.7441e+00, -1.2990e+01, -6.2045e-02,  2.5905e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-6.0434, -6.0434, -8.6789, -6.0434, -1.7464, -6.0434, -6.0435, -6.0434,
        -6.0435, -0.9423], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 3.0972,  3.0971,  0.2394,  3.0971, -7.1719,  3.0972,  3.0971,  3.0971,
          3.0972,  0.3109],
        [-3.0969, -3.0971, -0.6070, -3.0971,  7.1715, -3.0969, -3.0970, -3.0971,
         -3.0969, -0.3915]], device='cuda:0'))])
loaded xi:  111.97136
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1556.5275044584657
Current xi:  [164.0359]
objective value function right now is: -1556.5275044584657
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.08083]
objective value function right now is: -1552.9828253414987
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [149.72481]
objective value function right now is: -1491.6282805752535
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.73198]
objective value function right now is: -1549.38485008135
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [157.45555]
objective value function right now is: -1552.4842985145829
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.29204]
objective value function right now is: -1551.7032160436604
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [164.93019]
objective value function right now is: -1547.2234259408233
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.8238]
objective value function right now is: -1554.1546692726497
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.56372]
objective value function right now is: -1552.9356119221509
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.99524]
objective value function right now is: -1554.7719540259964
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1558.4575912381592
Current xi:  [169.30737]
objective value function right now is: -1558.4575912381592
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [158.05872]
objective value function right now is: -1547.4929050547257
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.50215]
objective value function right now is: -1556.4017692587001
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [166.73312]
objective value function right now is: -1550.4534292491264
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.14441]
objective value function right now is: -1553.9696967300922
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.51762]
objective value function right now is: -1556.3872737360834
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.85487]
objective value function right now is: -1554.5843517403987
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.95496]
objective value function right now is: -1441.570602236979
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [75.41796]
objective value function right now is: -383.1550759420536
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [62.57814]
objective value function right now is: -1266.005825574049
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [65.3257]
objective value function right now is: -1302.696060084895
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [67.86471]
objective value function right now is: -1346.4155573888881
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [66.97458]
objective value function right now is: -1377.1548300205413
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [57.996387]
objective value function right now is: -1195.2887462827796
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [82.34553]
objective value function right now is: -693.8907095993403
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.51192]
objective value function right now is: -1532.885505341966
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [123.96937]
objective value function right now is: -1538.406110209469
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [138.36252]
objective value function right now is: -1547.5891176188304
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [147.0896]
objective value function right now is: -1542.6830387287014
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [145.6494]
objective value function right now is: -1549.6298825006918
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [152.58305]
objective value function right now is: -1533.2792667951378
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [156.35321]
objective value function right now is: -1553.639265126829
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [158.11006]
objective value function right now is: -1552.3535053890005
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [151.52617]
objective value function right now is: -1543.5917853835094
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [156.39561]
objective value function right now is: -1471.842232782962
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [157.0512]
objective value function right now is: -1557.5442652056038
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1558.7507535412888
Current xi:  [157.9749]
objective value function right now is: -1558.7507535412888
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [158.90765]
objective value function right now is: -1557.8711829592487
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1559.7097080303392
Current xi:  [160.42694]
objective value function right now is: -1559.7097080303392
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1560.3824126516754
Current xi:  [161.32104]
objective value function right now is: -1560.3824126516754
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.07445]
objective value function right now is: -1558.597205096226
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1561.4517996525526
Current xi:  [162.33638]
objective value function right now is: -1561.4517996525526
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.1636]
objective value function right now is: -1560.3326585152022
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.50769]
objective value function right now is: -1560.5451698672673
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.6715]
objective value function right now is: -1561.3314148496502
new min fval from sgd:  -1561.4538633618272
new min fval from sgd:  -1561.4880904389834
new min fval from sgd:  -1561.6077731605508
new min fval from sgd:  -1561.68540819054
new min fval from sgd:  -1561.6970892278014
new min fval from sgd:  -1561.826694029852
new min fval from sgd:  -1561.920974995074
new min fval from sgd:  -1562.0193370403765
new min fval from sgd:  -1562.0647968590138
new min fval from sgd:  -1562.0929031204055
new min fval from sgd:  -1562.0929864213965
new min fval from sgd:  -1562.1316934274366
new min fval from sgd:  -1562.2273104485396
new min fval from sgd:  -1562.2545387444477
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.41557]
objective value function right now is: -1559.2474460214778
new min fval from sgd:  -1562.288914815739
new min fval from sgd:  -1562.312900891674
new min fval from sgd:  -1562.40128827066
new min fval from sgd:  -1562.4678835335988
new min fval from sgd:  -1562.5126410513492
new min fval from sgd:  -1562.5249199055215
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.73302]
objective value function right now is: -1561.6450950740596
new min fval from sgd:  -1562.5274046663526
new min fval from sgd:  -1562.553190965498
new min fval from sgd:  -1562.6179925457966
new min fval from sgd:  -1562.6713865398278
new min fval from sgd:  -1562.6778217540282
new min fval from sgd:  -1562.6826676272772
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.07274]
objective value function right now is: -1562.0716782600944
new min fval from sgd:  -1562.6869699635608
new min fval from sgd:  -1562.7006844552586
new min fval from sgd:  -1562.7092511737665
new min fval from sgd:  -1562.7126954629864
new min fval from sgd:  -1562.7151643786667
new min fval from sgd:  -1562.7262494352146
new min fval from sgd:  -1562.7377235333859
new min fval from sgd:  -1562.7378007883653
new min fval from sgd:  -1562.7396448059885
new min fval from sgd:  -1562.7675783802924
new min fval from sgd:  -1562.784051192238
new min fval from sgd:  -1562.7857712538425
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.58992]
objective value function right now is: -1562.5518668132515
new min fval from sgd:  -1562.7951234409882
new min fval from sgd:  -1562.8065231828837
new min fval from sgd:  -1562.8067507325352
new min fval from sgd:  -1562.8304105558395
new min fval from sgd:  -1562.833225722989
new min fval from sgd:  -1562.8463786446823
new min fval from sgd:  -1562.8530461950888
new min fval from sgd:  -1562.8592551097624
new min fval from sgd:  -1562.8598558984522
new min fval from sgd:  -1562.8602334664163
new min fval from sgd:  -1562.8641906047874
new min fval from sgd:  -1562.8756357110146
new min fval from sgd:  -1562.8803523597335
new min fval from sgd:  -1562.8890551712666
new min fval from sgd:  -1562.894639755127
new min fval from sgd:  -1562.8963841060772
new min fval from sgd:  -1562.9032821398484
new min fval from sgd:  -1562.928026944492
new min fval from sgd:  -1562.943953024243
new min fval from sgd:  -1562.9524708703877
new min fval from sgd:  -1562.969502998212
new min fval from sgd:  -1562.9757993765759
new min fval from sgd:  -1562.988070364943
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.81046]
objective value function right now is: -1562.5791465257937
min fval:  -1562.988070364943
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 15.7553,  -1.3474],
        [ -1.1457,   0.3625],
        [ -1.1457,   0.3625],
        [-44.3826, -10.6324],
        [-16.6588,   3.5289],
        [ 12.3017,   2.5252],
        [ 14.9982,  -8.5625],
        [ -0.2695, -12.6254],
        [  4.6995, -13.6942],
        [ -1.1456,   0.3625]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-12.5189,  -2.9727,  -2.9727,  -9.3504,  10.4912, -15.6841, -12.1943,
        -10.7435, -10.6285,  -2.9726], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.4114e-01,  2.4824e-03,  2.4824e-03, -2.0760e-02, -6.9553e-01,
         -2.0030e-02, -3.8776e-01, -4.1559e-02, -2.4379e-01,  2.4818e-03],
        [-5.8236e+00,  1.2960e-01,  1.2960e-01, -6.8740e+00,  5.9730e+00,
         -6.4348e+00, -9.2099e+00, -3.4333e+00, -1.5033e+01,  1.2961e-01],
        [-3.4114e-01,  2.4824e-03,  2.4824e-03, -2.0760e-02, -6.9554e-01,
         -2.0030e-02, -3.8776e-01, -4.1559e-02, -2.4379e-01,  2.4818e-03],
        [-4.5966e+00,  8.3699e-02,  8.3699e-02, -6.7626e+00,  7.7713e+00,
         -3.1732e+00, -9.3361e+00, -6.5069e+00, -1.5768e+01,  8.3588e-02],
        [-3.4114e-01,  2.4824e-03,  2.4824e-03, -2.0760e-02, -6.9554e-01,
         -2.0030e-02, -3.8776e-01, -4.1559e-02, -2.4379e-01,  2.4818e-03],
        [-3.4113e-01,  2.4824e-03,  2.4824e-03, -2.0760e-02, -6.9553e-01,
         -2.0030e-02, -3.8775e-01, -4.1559e-02, -2.4379e-01,  2.4818e-03],
        [-3.4114e-01,  2.4824e-03,  2.4824e-03, -2.0760e-02, -6.9553e-01,
         -2.0030e-02, -3.8776e-01, -4.1559e-02, -2.4379e-01,  2.4818e-03],
        [ 9.2088e+00,  1.0201e-01,  1.0201e-01,  9.7380e+00, -8.1157e+00,
          1.0829e+01,  6.7685e+00,  8.5134e+00,  1.6516e+01,  1.0202e-01],
        [ 8.1841e+00,  1.8550e-01,  1.8550e-01,  7.8653e+00, -7.0802e+00,
          7.4929e+00, -1.1203e+00,  6.3329e+00,  1.6232e+01,  1.8547e-01],
        [-5.5147e+00,  5.8789e-02,  5.8789e-02, -7.1536e+00,  6.7264e+00,
         -6.9572e+00, -9.2662e+00, -4.5410e+00, -1.5526e+01,  5.8871e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.9873,  1.6165, -1.9873,  1.6108, -1.9873, -1.9873, -1.9873, -5.7171,
        -5.3068,  1.4314], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0610,   6.9070,  -0.0610,   6.1303,  -0.0610,  -0.0610,  -0.0610,
         -14.3846, -10.0526,   7.2089]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.1976,   0.0432],
        [  3.0497,   3.9335],
        [ -2.1244,  13.7703],
        [-13.0229,  -5.2258],
        [  1.3449,  -2.6832],
        [  0.5817,  -2.6077],
        [  8.7075,  12.3428],
        [-17.4838,  -5.2426],
        [  5.6421,   1.1253],
        [ 16.6675,  -0.1931]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -4.4880,  -0.3294,  11.3526,  -2.3251,   5.6196,   4.1156,  10.0534,
         -5.2200,   2.0365, -13.5195], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.3025e-03, -2.8780e-01, -3.6880e-01, -1.3800e-01, -1.4733e+00,
         -1.2821e+00, -1.3212e+00, -1.9970e-02, -2.0805e+00, -6.7574e-01],
        [-9.3818e-03, -3.1963e-01, -2.7292e-01, -1.5858e-01, -1.6374e+00,
         -1.3909e+00, -1.2986e+00, -2.6480e-02, -1.5389e+00, -7.7713e-01],
        [-3.4587e-01,  1.1253e+00, -1.7858e+00, -3.0032e+00,  4.7995e-01,
         -4.3546e-01,  9.3842e-02, -1.0048e+00,  2.4549e+00,  2.2991e+00],
        [ 3.4194e+00,  2.1622e+00, -2.0503e+01,  2.2841e+01, -4.0950e+00,
         -7.5235e-01, -3.3245e+01,  1.8534e+01, -2.5059e+00, -1.0128e+01],
        [ 2.1379e+00, -1.4483e+00, -2.0798e+01,  1.7950e+01,  1.5657e+00,
         -2.6393e+00, -1.3003e+01,  6.6588e+00,  2.6949e+00, -1.7530e+01],
        [-6.3848e-03, -2.6034e-01, -3.8378e-01, -1.7152e-01, -1.5281e+00,
         -1.5116e+00, -1.3133e+00, -2.7218e-02, -1.5049e+00, -1.0335e+00],
        [ 4.1376e-04,  1.9386e-01,  1.4939e+00, -1.0493e-01, -1.8649e+00,
         -9.6818e-01, -3.2734e+00, -2.6240e-02,  2.5945e-01, -2.0458e-01],
        [-5.2881e-03, -7.1724e-01,  8.0154e-02, -1.9244e-01, -1.6653e+00,
         -1.5302e+00, -1.1237e+00, -4.0945e-02, -9.2749e-01, -1.1728e+00],
        [-2.8765e-03, -3.8939e-01, -5.5632e-01, -1.5647e-01, -1.5554e+00,
         -1.5368e+00, -1.3924e+00, -2.1782e-02, -1.9623e+00, -9.7539e-01],
        [-2.3876e-01,  2.1832e-01,  2.0848e+01, -1.2428e+01,  9.2764e+00,
         -3.3324e+00,  1.3341e+00, -7.6505e+00, -1.1500e+00,  2.0084e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.6786, -2.8798,  2.3053, -5.1958, -0.7183, -2.7188, -4.6272, -3.1617,
        -2.1532, -4.6602], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.1204,   0.1724,  -1.4931,  14.8928,  -5.6547,   0.1561,   0.3908,
           0.2260,   0.1142,   1.7056],
        [ -0.1204,  -0.1724,   1.1388, -14.8893,   5.6564,  -0.1560,  -0.3907,
          -0.2259,  -0.1142,  -1.7861]], device='cuda:0'))])
xi:  [166.78674]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 680.9143749518979
W_T_median: 397.6090593650572
W_T_pctile_5: 166.810501441204
W_T_CVAR_5_pct: 14.740286404470028
Average q (qsum/M+1):  48.992526146673384
Optimal xi:  [166.78674]
Expected(across Rb) median(across samples) p_equity:  0.27628652304410933
obj fun:  tensor(-1562.9881, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 15.7553,  -1.3474],
        [ -1.1457,   0.3625],
        [ -1.1457,   0.3625],
        [-44.3826, -10.6324],
        [-16.6588,   3.5289],
        [ 12.3017,   2.5252],
        [ 14.9982,  -8.5625],
        [ -0.2695, -12.6254],
        [  4.6995, -13.6942],
        [ -1.1456,   0.3625]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-12.5189,  -2.9727,  -2.9727,  -9.3504,  10.4912, -15.6841, -12.1943,
        -10.7435, -10.6285,  -2.9726], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.4114e-01,  2.4824e-03,  2.4824e-03, -2.0760e-02, -6.9553e-01,
         -2.0030e-02, -3.8776e-01, -4.1559e-02, -2.4379e-01,  2.4818e-03],
        [-5.8236e+00,  1.2960e-01,  1.2960e-01, -6.8740e+00,  5.9730e+00,
         -6.4348e+00, -9.2099e+00, -3.4333e+00, -1.5033e+01,  1.2961e-01],
        [-3.4114e-01,  2.4824e-03,  2.4824e-03, -2.0760e-02, -6.9554e-01,
         -2.0030e-02, -3.8776e-01, -4.1559e-02, -2.4379e-01,  2.4818e-03],
        [-4.5966e+00,  8.3699e-02,  8.3699e-02, -6.7626e+00,  7.7713e+00,
         -3.1732e+00, -9.3361e+00, -6.5069e+00, -1.5768e+01,  8.3588e-02],
        [-3.4114e-01,  2.4824e-03,  2.4824e-03, -2.0760e-02, -6.9554e-01,
         -2.0030e-02, -3.8776e-01, -4.1559e-02, -2.4379e-01,  2.4818e-03],
        [-3.4113e-01,  2.4824e-03,  2.4824e-03, -2.0760e-02, -6.9553e-01,
         -2.0030e-02, -3.8775e-01, -4.1559e-02, -2.4379e-01,  2.4818e-03],
        [-3.4114e-01,  2.4824e-03,  2.4824e-03, -2.0760e-02, -6.9553e-01,
         -2.0030e-02, -3.8776e-01, -4.1559e-02, -2.4379e-01,  2.4818e-03],
        [ 9.2088e+00,  1.0201e-01,  1.0201e-01,  9.7380e+00, -8.1157e+00,
          1.0829e+01,  6.7685e+00,  8.5134e+00,  1.6516e+01,  1.0202e-01],
        [ 8.1841e+00,  1.8550e-01,  1.8550e-01,  7.8653e+00, -7.0802e+00,
          7.4929e+00, -1.1203e+00,  6.3329e+00,  1.6232e+01,  1.8547e-01],
        [-5.5147e+00,  5.8789e-02,  5.8789e-02, -7.1536e+00,  6.7264e+00,
         -6.9572e+00, -9.2662e+00, -4.5410e+00, -1.5526e+01,  5.8871e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.9873,  1.6165, -1.9873,  1.6108, -1.9873, -1.9873, -1.9873, -5.7171,
        -5.3068,  1.4314], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0610,   6.9070,  -0.0610,   6.1303,  -0.0610,  -0.0610,  -0.0610,
         -14.3846, -10.0526,   7.2089]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.1976,   0.0432],
        [  3.0497,   3.9335],
        [ -2.1244,  13.7703],
        [-13.0229,  -5.2258],
        [  1.3449,  -2.6832],
        [  0.5817,  -2.6077],
        [  8.7075,  12.3428],
        [-17.4838,  -5.2426],
        [  5.6421,   1.1253],
        [ 16.6675,  -0.1931]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -4.4880,  -0.3294,  11.3526,  -2.3251,   5.6196,   4.1156,  10.0534,
         -5.2200,   2.0365, -13.5195], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.3025e-03, -2.8780e-01, -3.6880e-01, -1.3800e-01, -1.4733e+00,
         -1.2821e+00, -1.3212e+00, -1.9970e-02, -2.0805e+00, -6.7574e-01],
        [-9.3818e-03, -3.1963e-01, -2.7292e-01, -1.5858e-01, -1.6374e+00,
         -1.3909e+00, -1.2986e+00, -2.6480e-02, -1.5389e+00, -7.7713e-01],
        [-3.4587e-01,  1.1253e+00, -1.7858e+00, -3.0032e+00,  4.7995e-01,
         -4.3546e-01,  9.3842e-02, -1.0048e+00,  2.4549e+00,  2.2991e+00],
        [ 3.4194e+00,  2.1622e+00, -2.0503e+01,  2.2841e+01, -4.0950e+00,
         -7.5235e-01, -3.3245e+01,  1.8534e+01, -2.5059e+00, -1.0128e+01],
        [ 2.1379e+00, -1.4483e+00, -2.0798e+01,  1.7950e+01,  1.5657e+00,
         -2.6393e+00, -1.3003e+01,  6.6588e+00,  2.6949e+00, -1.7530e+01],
        [-6.3848e-03, -2.6034e-01, -3.8378e-01, -1.7152e-01, -1.5281e+00,
         -1.5116e+00, -1.3133e+00, -2.7218e-02, -1.5049e+00, -1.0335e+00],
        [ 4.1376e-04,  1.9386e-01,  1.4939e+00, -1.0493e-01, -1.8649e+00,
         -9.6818e-01, -3.2734e+00, -2.6240e-02,  2.5945e-01, -2.0458e-01],
        [-5.2881e-03, -7.1724e-01,  8.0154e-02, -1.9244e-01, -1.6653e+00,
         -1.5302e+00, -1.1237e+00, -4.0945e-02, -9.2749e-01, -1.1728e+00],
        [-2.8765e-03, -3.8939e-01, -5.5632e-01, -1.5647e-01, -1.5554e+00,
         -1.5368e+00, -1.3924e+00, -2.1782e-02, -1.9623e+00, -9.7539e-01],
        [-2.3876e-01,  2.1832e-01,  2.0848e+01, -1.2428e+01,  9.2764e+00,
         -3.3324e+00,  1.3341e+00, -7.6505e+00, -1.1500e+00,  2.0084e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.6786, -2.8798,  2.3053, -5.1958, -0.7183, -2.7188, -4.6272, -3.1617,
        -2.1532, -4.6602], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.1204,   0.1724,  -1.4931,  14.8928,  -5.6547,   0.1561,   0.3908,
           0.2260,   0.1142,   1.7056],
        [ -0.1204,  -0.1724,   1.1388, -14.8893,   5.6564,  -0.1560,  -0.3907,
          -0.2259,  -0.1142,  -1.7861]], device='cuda:0'))])
loaded xi:  166.78674
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1585.1847531375815
Current xi:  [178.95659]
objective value function right now is: -1585.1847531375815
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.89825]
objective value function right now is: -1572.237555388415
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1589.6033659286313
Current xi:  [181.8727]
objective value function right now is: -1589.6033659286313
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1595.1746324509147
Current xi:  [182.50119]
objective value function right now is: -1595.1746324509147
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.62589]
objective value function right now is: -1559.3885819977681
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1599.2782863785771
Current xi:  [181.72191]
objective value function right now is: -1599.2782863785771
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [146.98233]
objective value function right now is: -1305.9039282614378
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [134.14183]
objective value function right now is: -1364.102004910251
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [125.89907]
objective value function right now is: -1386.3564700721536
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.26668]
objective value function right now is: -1416.6562695541186
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [151.24004]
objective value function right now is: -1570.6994685463292
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [171.18144]
objective value function right now is: -1585.8726236253576
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [179.52293]
objective value function right now is: -1566.3145175943264
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [182.11649]
objective value function right now is: -1586.0692308574435
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.77208]
objective value function right now is: -1583.7950183142018
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.31969]
objective value function right now is: -1587.5862394509925
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.27618]
objective value function right now is: -1582.3624814061398
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.09978]
objective value function right now is: -1528.1362629647265
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.38737]
objective value function right now is: -1592.2490034166997
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.47896]
objective value function right now is: -1597.4404658060018
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.43866]
objective value function right now is: -1590.0884943841356
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.85265]
objective value function right now is: -1590.5637278276165
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.09793]
objective value function right now is: -1560.2990976677845
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.49313]
objective value function right now is: -1595.7569817421954
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.6703]
objective value function right now is: -1596.2824914384157
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.94044]
objective value function right now is: -1586.0227445588214
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.45894]
objective value function right now is: -1587.633373201531
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [186.05006]
objective value function right now is: -1595.9248547470447
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [186.153]
objective value function right now is: -1591.7115567434423
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.77881]
objective value function right now is: -1595.624007475497
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.34549]
objective value function right now is: -1596.0036306061
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.5253]
objective value function right now is: -1574.3620264798017
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.92316]
objective value function right now is: -1594.6761480751995
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.188225]
objective value function right now is: 4124.082903722503
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.198494]
objective value function right now is: 2228.297794242665
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [67.83153]
objective value function right now is: 2080.9911043078678
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.460396]
objective value function right now is: -1212.9665975325531
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.94304]
objective value function right now is: -1388.8433251254612
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [81.53823]
objective value function right now is: -1402.6978369087933
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [88.99123]
objective value function right now is: -1422.6392550494193
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [96.88576]
objective value function right now is: -1442.9878307427728
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.04061]
objective value function right now is: -1448.8054000981354
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [111.204895]
objective value function right now is: -1468.6722971893432
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [118.611115]
objective value function right now is: -1476.544930980447
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [125.46052]
objective value function right now is: -1489.8071316585701
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [132.77405]
objective value function right now is: -1532.0944770257952
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [139.34138]
objective value function right now is: -1563.6858192535112
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [145.51067]
objective value function right now is: -1578.0262550576524
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [149.03587]
objective value function right now is: -1581.7711127630096
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [150.32927]
objective value function right now is: -1580.9021228229697
min fval:  -1524.5191712944484
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 16.4357,  -1.3710],
        [ -1.1285,   0.7399],
        [ -1.1285,   0.7399],
        [-41.0068, -11.1276],
        [-17.4244,   3.8102],
        [ 13.4742,   3.1357],
        [ 15.8937,  -8.7735],
        [ -1.3454, -13.3926],
        [  6.3465, -14.0817],
        [ -1.1285,   0.7399]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-13.3828,  -4.9361,  -4.9361,  -9.3780,  10.7608, -15.6525, -12.2615,
        -10.8362, -10.9045,  -4.9361], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.0696e-01,  8.8353e-03,  8.8353e-03, -2.4214e-01, -1.5813e+00,
          5.5985e-01, -5.1697e-01, -3.4996e-01, -9.0845e-01,  8.8355e-03],
        [-6.2932e+00, -1.5450e-01, -1.5450e-01, -7.3972e+00,  5.9182e+00,
         -5.9076e+00, -8.5589e+00, -3.9359e+00, -1.5241e+01, -1.5450e-01],
        [-2.0696e-01,  8.8352e-03,  8.8353e-03, -2.4214e-01, -1.5813e+00,
          5.5985e-01, -5.1697e-01, -3.4996e-01, -9.0845e-01,  8.8354e-03],
        [-4.0801e+00,  2.1766e-01,  2.1766e-01, -7.1120e+00,  7.9490e+00,
         -2.5612e+00, -8.7855e+00, -6.7646e+00, -1.6197e+01,  2.1766e-01],
        [-2.0696e-01,  8.8352e-03,  8.8353e-03, -2.4214e-01, -1.5813e+00,
          5.5985e-01, -5.1697e-01, -3.4996e-01, -9.0845e-01,  8.8354e-03],
        [-2.0696e-01,  8.8352e-03,  8.8352e-03, -2.4214e-01, -1.5813e+00,
          5.5985e-01, -5.1697e-01, -3.4996e-01, -9.0845e-01,  8.8354e-03],
        [-2.0696e-01,  8.8352e-03,  8.8352e-03, -2.4214e-01, -1.5813e+00,
          5.5985e-01, -5.1697e-01, -3.4996e-01, -9.0845e-01,  8.8355e-03],
        [ 1.0748e+01,  3.3599e-02,  3.3599e-02,  9.0360e+00, -8.8898e+00,
          9.2746e+00,  8.4532e+00,  8.9789e+00,  1.6988e+01,  3.3601e-02],
        [ 9.2942e+00, -1.7900e-02, -1.7900e-02,  7.4947e+00, -7.8341e+00,
          6.7407e+00,  2.4443e+00,  6.0335e+00,  1.5564e+01, -1.7902e-02],
        [-5.9673e+00, -1.2218e-01, -1.2218e-01, -7.5711e+00,  6.7943e+00,
         -6.5236e+00, -8.5747e+00, -4.9079e+00, -1.5774e+01, -1.2217e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-3.0236,  1.4606, -3.0236,  1.6892, -3.0236, -3.0236, -3.0236, -5.2791,
        -5.4329,  1.4825], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.2346e-03,  6.8801e+00, -4.2346e-03,  6.8190e+00, -4.2345e-03,
         -4.2345e-03, -4.2345e-03, -1.5987e+01, -8.8839e+00,  7.5413e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -0.2493,   4.2597],
        [  2.8596,   8.7186],
        [ -2.2844,  14.0618],
        [-13.2417,  -4.7972],
        [ -0.2132,  -1.4000],
        [  1.5465,  -4.2569],
        [  9.1632,  12.7107],
        [-15.2113,  -7.0346],
        [  6.1244,  -0.8296],
        [ 16.9258,   0.4997]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -3.8848,  -0.5109,  12.0612,  -1.4459,   2.9899,   3.7927,   9.6118,
         -5.7320,   4.3836, -13.3000], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.0305e+00, -3.0061e+00, -7.8794e+00,  3.9359e+00, -2.8287e-01,
         -4.1817e-01, -2.8749e+00,  3.9584e-01, -1.5230e+00, -1.1471e+00],
        [-4.9217e-01, -1.3078e+00, -4.9794e+00,  1.4146e+00, -5.9494e-01,
         -5.9911e-01, -1.4927e+00, -1.8825e-01, -1.6945e+00, -1.4371e+00],
        [ 1.7076e-01,  5.1525e+00, -2.9760e+00, -4.6731e+00,  1.4017e+00,
          4.7288e-01, -2.2195e+00, -5.6148e+00,  3.1022e+00,  3.9059e+00],
        [ 1.9735e-01,  2.5114e-03, -2.5492e+01,  2.1470e+01, -5.0620e+00,
         -1.7612e+00, -5.2224e+01,  1.8260e+01, -3.5040e+00, -1.2313e+01],
        [ 1.1337e-01, -2.6882e+00, -2.4351e+01,  1.8750e+01,  1.1702e+00,
         -3.0315e+00, -1.1736e+01,  7.1379e+00,  2.1972e+00, -2.1198e+01],
        [-5.1640e-01, -1.4325e+00, -5.0058e+00,  1.5705e+00, -9.5138e-01,
         -9.8225e-01, -2.4219e+00,  5.8031e-01, -2.0443e+00, -9.0541e-01],
        [-1.3809e+00, -2.8012e+00, -6.0758e+00,  3.2720e+00,  2.9193e-01,
          4.3441e-01, -3.0650e+00, -1.2160e+00,  2.1597e-02, -1.1082e+00],
        [-2.5421e-01, -1.7178e+00, -4.7647e+00,  1.5080e+00, -6.8426e-01,
         -7.0117e-01, -2.0553e+00, -9.2010e-02, -1.4859e+00, -1.1845e+00],
        [-6.4903e-01, -1.3782e+00, -5.4882e+00,  1.9970e+00, -6.9887e-01,
         -7.1216e-01, -1.4187e+00,  5.6682e-02, -1.8031e+00, -1.3019e+00],
        [ 6.5917e-02, -1.3703e-01,  1.8043e+01, -1.1337e+01,  9.0516e+00,
         -3.4637e+00,  1.2181e+00, -4.1470e+00, -1.2532e+00,  1.9307e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.7906, -2.3279,  3.1420, -6.1906, -1.1148, -2.5986, -3.8670, -2.6820,
        -1.9477, -4.7920], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -1.7576,  -1.4716,  -0.6721,  14.8193,  -6.4469,  -1.2340,  -1.1715,
          -1.3739,  -1.5503,   0.9207],
        [  1.7576,   1.4716,   0.3178, -14.8158,   6.4491,   1.2340,   1.1716,
           1.3740,   1.5503,  -1.0013]], device='cuda:0'))])
xi:  [125.46052]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 693.4316675176754
W_T_median: 425.0787737246038
W_T_pctile_5: 180.65826496592598
W_T_CVAR_5_pct: 20.23271969577739
Average q (qsum/M+1):  48.32743589339718
Optimal xi:  [125.46052]
Expected(across Rb) median(across samples) p_equity:  0.26787999322017036
obj fun:  tensor(-1524.5192, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 16.4357,  -1.3710],
        [ -1.1285,   0.7399],
        [ -1.1285,   0.7399],
        [-41.0068, -11.1276],
        [-17.4244,   3.8102],
        [ 13.4742,   3.1357],
        [ 15.8937,  -8.7735],
        [ -1.3454, -13.3926],
        [  6.3465, -14.0817],
        [ -1.1285,   0.7399]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-13.3828,  -4.9361,  -4.9361,  -9.3780,  10.7608, -15.6525, -12.2615,
        -10.8362, -10.9045,  -4.9361], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.0696e-01,  8.8353e-03,  8.8353e-03, -2.4214e-01, -1.5813e+00,
          5.5985e-01, -5.1697e-01, -3.4996e-01, -9.0845e-01,  8.8355e-03],
        [-6.2932e+00, -1.5450e-01, -1.5450e-01, -7.3972e+00,  5.9182e+00,
         -5.9076e+00, -8.5589e+00, -3.9359e+00, -1.5241e+01, -1.5450e-01],
        [-2.0696e-01,  8.8352e-03,  8.8353e-03, -2.4214e-01, -1.5813e+00,
          5.5985e-01, -5.1697e-01, -3.4996e-01, -9.0845e-01,  8.8354e-03],
        [-4.0801e+00,  2.1766e-01,  2.1766e-01, -7.1120e+00,  7.9490e+00,
         -2.5612e+00, -8.7855e+00, -6.7646e+00, -1.6197e+01,  2.1766e-01],
        [-2.0696e-01,  8.8352e-03,  8.8353e-03, -2.4214e-01, -1.5813e+00,
          5.5985e-01, -5.1697e-01, -3.4996e-01, -9.0845e-01,  8.8354e-03],
        [-2.0696e-01,  8.8352e-03,  8.8352e-03, -2.4214e-01, -1.5813e+00,
          5.5985e-01, -5.1697e-01, -3.4996e-01, -9.0845e-01,  8.8354e-03],
        [-2.0696e-01,  8.8352e-03,  8.8352e-03, -2.4214e-01, -1.5813e+00,
          5.5985e-01, -5.1697e-01, -3.4996e-01, -9.0845e-01,  8.8355e-03],
        [ 1.0748e+01,  3.3599e-02,  3.3599e-02,  9.0360e+00, -8.8898e+00,
          9.2746e+00,  8.4532e+00,  8.9789e+00,  1.6988e+01,  3.3601e-02],
        [ 9.2942e+00, -1.7900e-02, -1.7900e-02,  7.4947e+00, -7.8341e+00,
          6.7407e+00,  2.4443e+00,  6.0335e+00,  1.5564e+01, -1.7902e-02],
        [-5.9673e+00, -1.2218e-01, -1.2218e-01, -7.5711e+00,  6.7943e+00,
         -6.5236e+00, -8.5747e+00, -4.9079e+00, -1.5774e+01, -1.2217e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-3.0236,  1.4606, -3.0236,  1.6892, -3.0236, -3.0236, -3.0236, -5.2791,
        -5.4329,  1.4825], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.2346e-03,  6.8801e+00, -4.2346e-03,  6.8190e+00, -4.2345e-03,
         -4.2345e-03, -4.2345e-03, -1.5987e+01, -8.8839e+00,  7.5413e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -0.2493,   4.2597],
        [  2.8596,   8.7186],
        [ -2.2844,  14.0618],
        [-13.2417,  -4.7972],
        [ -0.2132,  -1.4000],
        [  1.5465,  -4.2569],
        [  9.1632,  12.7107],
        [-15.2113,  -7.0346],
        [  6.1244,  -0.8296],
        [ 16.9258,   0.4997]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -3.8848,  -0.5109,  12.0612,  -1.4459,   2.9899,   3.7927,   9.6118,
         -5.7320,   4.3836, -13.3000], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.0305e+00, -3.0061e+00, -7.8794e+00,  3.9359e+00, -2.8287e-01,
         -4.1817e-01, -2.8749e+00,  3.9584e-01, -1.5230e+00, -1.1471e+00],
        [-4.9217e-01, -1.3078e+00, -4.9794e+00,  1.4146e+00, -5.9494e-01,
         -5.9911e-01, -1.4927e+00, -1.8825e-01, -1.6945e+00, -1.4371e+00],
        [ 1.7076e-01,  5.1525e+00, -2.9760e+00, -4.6731e+00,  1.4017e+00,
          4.7288e-01, -2.2195e+00, -5.6148e+00,  3.1022e+00,  3.9059e+00],
        [ 1.9735e-01,  2.5114e-03, -2.5492e+01,  2.1470e+01, -5.0620e+00,
         -1.7612e+00, -5.2224e+01,  1.8260e+01, -3.5040e+00, -1.2313e+01],
        [ 1.1337e-01, -2.6882e+00, -2.4351e+01,  1.8750e+01,  1.1702e+00,
         -3.0315e+00, -1.1736e+01,  7.1379e+00,  2.1972e+00, -2.1198e+01],
        [-5.1640e-01, -1.4325e+00, -5.0058e+00,  1.5705e+00, -9.5138e-01,
         -9.8225e-01, -2.4219e+00,  5.8031e-01, -2.0443e+00, -9.0541e-01],
        [-1.3809e+00, -2.8012e+00, -6.0758e+00,  3.2720e+00,  2.9193e-01,
          4.3441e-01, -3.0650e+00, -1.2160e+00,  2.1597e-02, -1.1082e+00],
        [-2.5421e-01, -1.7178e+00, -4.7647e+00,  1.5080e+00, -6.8426e-01,
         -7.0117e-01, -2.0553e+00, -9.2010e-02, -1.4859e+00, -1.1845e+00],
        [-6.4903e-01, -1.3782e+00, -5.4882e+00,  1.9970e+00, -6.9887e-01,
         -7.1216e-01, -1.4187e+00,  5.6682e-02, -1.8031e+00, -1.3019e+00],
        [ 6.5917e-02, -1.3703e-01,  1.8043e+01, -1.1337e+01,  9.0516e+00,
         -3.4637e+00,  1.2181e+00, -4.1470e+00, -1.2532e+00,  1.9307e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.7906, -2.3279,  3.1420, -6.1906, -1.1148, -2.5986, -3.8670, -2.6820,
        -1.9477, -4.7920], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -1.7576,  -1.4716,  -0.6721,  14.8193,  -6.4469,  -1.2340,  -1.1715,
          -1.3739,  -1.5503,   0.9207],
        [  1.7576,   1.4716,   0.3178, -14.8158,   6.4491,   1.2340,   1.1716,
           1.3740,   1.5503,  -1.0013]], device='cuda:0'))])
loaded xi:  125.46052
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2101.9441582811537
Current xi:  [204.68478]
objective value function right now is: -2101.9441582811537
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -2410.353581094756
Current xi:  [205.58472]
objective value function right now is: -2410.353581094756
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [146.21341]
objective value function right now is: -1141.1638401600087
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [173.88412]
objective value function right now is: -2210.6341268115043
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -2542.9387443525056
Current xi:  [203.41858]
objective value function right now is: -2542.9387443525056
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [176.18504]
objective value function right now is: 36494.61140540794
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [111.43348]
objective value function right now is: 27302.676629702422
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.78299]
objective value function right now is: 22314.02490418356
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [41.4093]
objective value function right now is: 17269.85403001006
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.956787]
objective value function right now is: 4092.8925090183243
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.64649]
objective value function right now is: 958.9530400524097
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [94.595634]
objective value function right now is: 663.4368909460785
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [72.77324]
objective value function right now is: 2331.515651188078
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [85.85793]
objective value function right now is: 4585.4994988826065
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [89.33379]
objective value function right now is: 884.9183186552424
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [93.80183]
objective value function right now is: 30410.64402620044
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [67.72441]
objective value function right now is: 1601.4818232414164
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [87.080246]
objective value function right now is: 1473.0514979058944
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.5228]
objective value function right now is: 1255.5125319404654
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.95802]
objective value function right now is: -432.4425446699391
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [133.50653]
objective value function right now is: -761.0834033325879
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.43362]
objective value function right now is: 25362.45600872118
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.76194]
objective value function right now is: -630.2370986215307
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [62.953262]
objective value function right now is: 46720.85892421189
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.982326]
objective value function right now is: 40739.02742200458
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-41.308197]
objective value function right now is: 5717.375429225926
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-20.602667]
objective value function right now is: 2918.1274205554378
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-6.0272202]
objective value function right now is: 5983.082641128341
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [25.9551]
objective value function right now is: 1902.3359428753033
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.129142]
objective value function right now is: 8073.381074941143
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-58.019703]
objective value function right now is: 16045.028982547343
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-97.82139]
objective value function right now is: 13634.747174524327
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-126.787865]
objective value function right now is: 12560.392279246904
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-141.68803]
objective value function right now is: 8866.495002135336
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-87.2852]
objective value function right now is: 4750.126824208763
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-77.391304]
objective value function right now is: 4344.364094573311
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-67.74118]
objective value function right now is: 4002.6581046801175
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-58.14445]
objective value function right now is: 3676.4045529132477
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-48.41007]
objective value function right now is: 3179.5256199019123
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-38.5529]
objective value function right now is: 2787.772650962105
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-28.610044]
objective value function right now is: 2302.962454194216
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.671446]
objective value function right now is: 1904.6080150159337
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.962959]
objective value function right now is: 1579.7368725892513
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.1545191]
objective value function right now is: 1283.3792852437944
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.496924]
objective value function right now is: 576.0520570939074
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.283352]
objective value function right now is: 255.62732815092218
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [28.911884]
objective value function right now is: -20.415382581290423
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [38.396614]
objective value function right now is: -284.4705933298389
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [44.102886]
objective value function right now is: -454.1343688110651
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [46.027466]
objective value function right now is: -464.55687761920564
min fval:  1795.9967962877322
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 17.9606,  -1.0461],
        [ -1.1595,   0.4679],
        [ -1.1595,   0.4679],
        [-36.0865, -11.3926],
        [-18.9448,   4.3821],
        [ 13.6260,   2.4473],
        [ 18.3829, -10.0698],
        [ -1.8957, -13.7987],
        [  6.6788, -14.8869],
        [ -1.1595,   0.4679]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-13.1132,  -5.5880,  -5.5880,  -9.7259,  10.4309, -16.3849, -10.8530,
        -11.4074, -11.2050,  -5.5880], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-6.1320e-01, -1.0335e-03, -1.0335e-03,  9.8492e-01, -6.1293e-01,
          5.1053e-01, -6.3806e-01,  5.6487e-01,  7.2410e-01, -1.0335e-03],
        [-8.3818e+00, -1.8932e-01, -1.8932e-01, -6.4896e+00,  6.0886e+00,
         -5.5308e+00, -8.9695e+00, -3.7567e+00, -1.5080e+01, -1.8932e-01],
        [-6.1320e-01, -1.0335e-03, -1.0335e-03,  9.8492e-01, -6.1293e-01,
          5.1053e-01, -6.3806e-01,  5.6487e-01,  7.2410e-01, -1.0335e-03],
        [-4.4190e+00, -1.3660e-01, -1.3660e-01, -5.3492e+00,  8.5018e+00,
         -1.8353e+00, -1.0378e+01, -6.2688e+00, -1.7219e+01, -1.3660e-01],
        [-6.1320e-01, -1.0335e-03, -1.0335e-03,  9.8492e-01, -6.1293e-01,
          5.1053e-01, -6.3806e-01,  5.6487e-01,  7.2410e-01, -1.0335e-03],
        [-6.1320e-01, -1.0335e-03, -1.0335e-03,  9.8492e-01, -6.1293e-01,
          5.1053e-01, -6.3806e-01,  5.6487e-01,  7.2410e-01, -1.0335e-03],
        [-6.1320e-01, -1.0335e-03, -1.0335e-03,  9.8492e-01, -6.1293e-01,
          5.1053e-01, -6.3806e-01,  5.6487e-01,  7.2410e-01, -1.0335e-03],
        [ 1.2352e+01, -9.8771e-04, -9.8773e-04,  8.5369e+00, -8.8169e+00,
          8.0670e+00,  1.0417e+01,  9.0740e+00,  1.7957e+01, -9.8746e-04],
        [ 9.1975e+00, -1.3611e-01, -1.3611e-01,  5.8860e+00, -7.3686e+00,
          6.2411e+00,  4.0625e+00,  4.9218e+00,  1.4816e+01, -1.3611e-01],
        [-8.8913e+00, -8.0894e-02, -8.0895e-02, -5.7196e+00,  6.6829e+00,
         -5.8345e+00, -9.2529e+00, -4.1602e+00, -1.6070e+01, -8.0895e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-3.8578,  0.4215, -3.8578,  1.1648, -3.8578, -3.8578, -3.8578, -4.3137,
        -5.7483,  0.2056], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.1185,   5.9301,  -0.1185,   6.3022,  -0.1185,  -0.1185,  -0.1185,
         -17.5500,  -7.2339,   6.5363]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  6.5692,   2.1620],
        [ -3.3892,   5.8814],
        [ -2.6857,  13.6529],
        [-12.5008,  -5.0804],
        [  0.8537,  -4.2080],
        [ -2.7152,  -6.7001],
        [  8.9771,  13.9783],
        [-16.2897,  -7.1684],
        [  1.0768,  -8.0514],
        [ 15.6511,   2.3918]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -0.6677,  -6.5811,  12.7467,  -1.7849,   4.4967,  -3.0668,   8.8656,
         -5.2856,  -1.0353, -14.6772], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.4607e+01,  2.5682e-06, -1.5346e+01,  4.1905e+00,  9.2957e-03,
         -1.2900e-01, -3.0524e+00, -5.1209e+00, -1.2321e+00, -4.8740e+00],
        [ 7.1092e-01,  7.3454e-01, -3.4616e+00,  2.0456e+00, -2.5259e+00,
         -2.5029e+00, -2.7708e+00,  9.0072e-01, -3.5363e+00, -3.1634e+00],
        [-2.9130e+00,  4.6615e-01, -6.3502e+00, -9.2331e+00, -2.8895e+00,
         -3.7762e+00, -5.2817e+00, -7.5601e+00, -9.8051e-01,  2.2029e+00],
        [ 9.5599e+00,  4.2024e-02, -1.6878e+01,  2.0373e+01, -6.7231e+00,
         -3.4199e+00, -5.6797e+01,  1.8906e+01, -5.1632e+00, -1.1655e+01],
        [-4.6959e+00, -3.9313e-01, -2.6100e+01,  2.0987e+01,  8.3181e-01,
         -3.3303e+00, -1.4407e+01,  1.3980e+01,  1.8510e+00, -2.0135e+01],
        [ 2.1469e+00,  2.3312e+00, -7.3161e-01, -4.0569e-01,  1.9279e+00,
          1.1181e+00, -2.9061e+00, -9.8478e+00,  7.3365e-01,  9.1660e-01],
        [-4.3105e-01, -1.6014e-01, -7.6817e+00,  3.4454e+00, -1.6404e+00,
         -1.5203e+00, -2.5193e+00, -5.2228e-01, -1.9276e+00, -4.7491e+00],
        [ 7.6747e-01,  9.2591e-01, -3.2159e+00,  3.8305e-01, -2.6626e+00,
         -2.6316e+00, -1.8132e+00,  7.3326e-02, -3.3869e+00, -3.8192e+00],
        [ 1.3856e+00,  1.3236e+00, -3.7546e+00,  9.6895e-01, -2.7697e+00,
         -2.7533e+00, -2.8424e+00,  4.5491e-01, -3.8035e+00, -2.9615e+00],
        [ 9.7143e+00,  1.8184e-01,  2.0272e+01, -1.8914e+01,  7.6268e+00,
         -4.8688e+00,  3.8569e+00,  6.3097e+00, -2.6667e+00,  1.3882e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.5002, -4.3572, -1.0331, -7.8485, -1.4618,  0.1413, -5.7834, -4.8127,
        -4.0810, -6.2064], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -2.6420,   0.5268,  -0.0604,  14.1709,  -7.1841,  -0.9044,  -0.1893,
           0.6353,   0.7018,   1.1852],
        [  2.6420,  -0.5268,  -0.2938, -14.1672,   7.1864,   0.9045,   0.1896,
          -0.6352,  -0.7017,  -1.2658]], device='cuda:0'))])
xi:  [9.496924]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 718.2416837708298
W_T_median: 529.224634240391
W_T_pctile_5: 207.06662261584378
W_T_CVAR_5_pct: 22.410029997724468
Average q (qsum/M+1):  46.00788731728831
Optimal xi:  [9.496924]
Expected(across Rb) median(across samples) p_equity:  0.2345226320127646
obj fun:  tensor(1795.9968, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
