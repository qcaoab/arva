Starting at: 
02-02-23_23:24

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 50000
batchsize: 1000
remove neg:  False


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 5000, 'itbound_SGD_algorithms': 50000, 'nit_IterateAveragingStart': 45000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1662.2333015649233
Current xi:  [-110.419685]
objective value function right now is: -1662.2333015649233
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1670.2120446623228
Current xi:  [-195.4291]
objective value function right now is: -1670.2120446623228
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1671.484124818793
Current xi:  [-240.85573]
objective value function right now is: -1671.484124818793
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1671.9433237165074
Current xi:  [-247.37334]
objective value function right now is: -1671.9433237165074
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.3915648030002
Current xi:  [-249.27603]
objective value function right now is: -1672.3915648030002
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-249.02776]
objective value function right now is: -1672.283887910898
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-246.11543]
objective value function right now is: -1671.5998114897234
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.7763670890183
Current xi:  [-247.48166]
objective value function right now is: -1672.7763670890183
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.982623426525
Current xi:  [-247.21754]
objective value function right now is: -1672.982623426525
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.02376]
objective value function right now is: -1671.5195452283874
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.0215913407394
Current xi:  [-245.18199]
objective value function right now is: -1673.0215913407394
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.76152]
objective value function right now is: -1672.659472473921
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.2936499619689
Current xi:  [-245.19917]
objective value function right now is: -1673.2936499619689
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-245.45357]
objective value function right now is: -1673.1365819941184
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.70856]
objective value function right now is: -1673.0985880336957
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.79811]
objective value function right now is: -1673.241425098375
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.17357]
objective value function right now is: -1672.5042765853627
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.3104]
objective value function right now is: -1673.2802212786285
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.01756]
objective value function right now is: -1673.166282643468
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.97548]
objective value function right now is: -1673.1991245174509
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.05856]
objective value function right now is: -1672.2874784353269
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.52275]
objective value function right now is: -1672.966567097265
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.4112263629177
Current xi:  [-245.94528]
objective value function right now is: -1673.4112263629177
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.93408]
objective value function right now is: -1673.3926444975277
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.85509]
objective value function right now is: -1672.8515211507367
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.34937]
objective value function right now is: -1673.20812514141
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.13814]
objective value function right now is: -1673.1795414745147
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-246.04263]
objective value function right now is: -1673.092063457801
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-246.00838]
objective value function right now is: -1673.2754953184408
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.99664]
objective value function right now is: -1672.036598199228
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.42897]
objective value function right now is: -1673.1167590748257
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.474885723793
Current xi:  [-245.9281]
objective value function right now is: -1673.474885723793
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.56453]
objective value function right now is: -1672.7486325103962
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.89928]
objective value function right now is: -1673.4542869134625
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.19061]
objective value function right now is: -1673.2963096398628
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.6796304686777
Current xi:  [-244.96039]
objective value function right now is: -1673.6796304686777
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.0047]
objective value function right now is: -1673.6374778291263
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.7809]
objective value function right now is: -1673.6086372320938
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.92137]
objective value function right now is: -1673.6450501085703
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.6856109524053
Current xi:  [-245.06398]
objective value function right now is: -1673.6856109524053
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.83301]
objective value function right now is: -1673.6667870282354
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.749662874361
Current xi:  [-244.73502]
objective value function right now is: -1673.749662874361
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.06134]
objective value function right now is: -1673.6989308049856
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.93933]
objective value function right now is: -1673.6601543649488
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.75598]
objective value function right now is: -1673.6265239114514
new min fval from sgd:  -1673.764345806526
new min fval from sgd:  -1673.7723879953378
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.40709]
objective value function right now is: -1673.7222538089295
new min fval from sgd:  -1673.7726355495554
new min fval from sgd:  -1673.7757007902271
new min fval from sgd:  -1673.7773027471146
new min fval from sgd:  -1673.7792396600712
new min fval from sgd:  -1673.779745401221
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.73941]
objective value function right now is: -1673.6879521241888
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.84776]
objective value function right now is: -1673.654768352383
new min fval from sgd:  -1673.7801025090944
new min fval from sgd:  -1673.7807134775799
new min fval from sgd:  -1673.7807916755276
new min fval from sgd:  -1673.7808187725986
new min fval from sgd:  -1673.7809298553523
new min fval from sgd:  -1673.7813894240148
new min fval from sgd:  -1673.78212147906
new min fval from sgd:  -1673.782572564664
new min fval from sgd:  -1673.782609984846
new min fval from sgd:  -1673.7827474029748
new min fval from sgd:  -1673.7842898318436
new min fval from sgd:  -1673.7858446190628
new min fval from sgd:  -1673.7863954236275
new min fval from sgd:  -1673.78728687092
new min fval from sgd:  -1673.7882646931814
new min fval from sgd:  -1673.7888382420078
new min fval from sgd:  -1673.7892441653196
new min fval from sgd:  -1673.7892535455771
new min fval from sgd:  -1673.7896953526063
new min fval from sgd:  -1673.7897601083705
new min fval from sgd:  -1673.7900420216326
new min fval from sgd:  -1673.7905992235403
new min fval from sgd:  -1673.791182858782
new min fval from sgd:  -1673.7916909683147
new min fval from sgd:  -1673.7917114640577
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.8131]
objective value function right now is: -1673.780082277388
new min fval from sgd:  -1673.7919870150265
new min fval from sgd:  -1673.7926109759906
new min fval from sgd:  -1673.7930326583653
new min fval from sgd:  -1673.7934273023425
new min fval from sgd:  -1673.793494356336
new min fval from sgd:  -1673.7935099994238
new min fval from sgd:  -1673.7941336553522
new min fval from sgd:  -1673.7946691126758
new min fval from sgd:  -1673.795454768565
new min fval from sgd:  -1673.7957925405892
new min fval from sgd:  -1673.7961411467818
new min fval from sgd:  -1673.7961734685268
new min fval from sgd:  -1673.7964731383067
new min fval from sgd:  -1673.7966883429317
new min fval from sgd:  -1673.796689581161
new min fval from sgd:  -1673.7969329312457
new min fval from sgd:  -1673.797159815242
new min fval from sgd:  -1673.7972958908515
new min fval from sgd:  -1673.7974765113843
new min fval from sgd:  -1673.7976340447901
new min fval from sgd:  -1673.7976475623666
new min fval from sgd:  -1673.797694368952
new min fval from sgd:  -1673.7979459056767
new min fval from sgd:  -1673.7980899638528
new min fval from sgd:  -1673.7982322394146
new min fval from sgd:  -1673.798325083882
new min fval from sgd:  -1673.7985298325177
new min fval from sgd:  -1673.7990275310726
new min fval from sgd:  -1673.7996727530897
new min fval from sgd:  -1673.8000811911281
new min fval from sgd:  -1673.800550599627
new min fval from sgd:  -1673.8008910701303
new min fval from sgd:  -1673.8010665116465
new min fval from sgd:  -1673.8017374266922
new min fval from sgd:  -1673.8024971376408
new min fval from sgd:  -1673.8028139330004
new min fval from sgd:  -1673.8030952790007
new min fval from sgd:  -1673.803534152359
new min fval from sgd:  -1673.8036369290553
new min fval from sgd:  -1673.8037597390771
new min fval from sgd:  -1673.803878484632
new min fval from sgd:  -1673.8046583184816
new min fval from sgd:  -1673.805065150028
new min fval from sgd:  -1673.8053409697534
new min fval from sgd:  -1673.8053610123168
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.76897]
objective value function right now is: -1673.787962009566
min fval:  -1673.8053610123168
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-4.2151, -6.9391],
        [-0.4704,  1.1144],
        [-6.4598,  6.7099],
        [-9.5868,  4.2984],
        [-4.0221, -6.5910],
        [-5.9781,  6.3776],
        [-4.2737, -7.0605],
        [-1.9180,  4.1110],
        [-4.2210, -6.9503],
        [-3.8205, -6.2600]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-6.5694, -0.9540,  8.2638,  9.0729, -6.3906,  7.5210, -6.6602,  1.9218,
        -6.5771, -6.2986], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 4.1845e+00,  1.2052e-01, -5.5943e+00, -5.8702e+00,  3.5001e+00,
         -4.2210e+00,  4.4498e+00, -2.2078e-01,  4.2601e+00,  2.9469e+00],
        [-8.2923e-02, -2.3238e-02, -2.7534e-03, -9.4454e-02, -7.4507e-02,
         -1.9911e-03, -8.4070e-02, -5.9242e-03, -8.3061e-02, -6.2601e-02],
        [ 5.8757e-01, -2.5498e-02, -5.9506e-01, -1.1179e+00,  5.6625e-01,
         -4.8186e-01,  5.9515e-01,  3.5695e-03,  5.8829e-01,  5.3979e-01],
        [-4.5966e+00,  4.3515e-02,  6.6930e+00,  6.6801e+00, -3.7484e+00,
          5.1231e+00, -4.8972e+00,  4.7296e-01, -4.5779e+00, -3.1423e+00],
        [-8.2923e-02, -2.3238e-02, -2.7534e-03, -9.4454e-02, -7.4507e-02,
         -1.9911e-03, -8.4070e-02, -5.9242e-03, -8.3061e-02, -6.2601e-02],
        [-8.8640e-01,  6.4739e-02,  1.1996e+00,  7.5253e-01, -8.0216e-01,
          1.0779e+00, -9.0501e-01,  2.3155e-01, -8.8832e-01, -6.9515e-01],
        [-8.2923e-02, -2.3238e-02, -2.7534e-03, -9.4454e-02, -7.4507e-02,
         -1.9911e-03, -8.4070e-02, -5.9242e-03, -8.3061e-02, -6.2601e-02],
        [-8.2923e-02, -2.3238e-02, -2.7534e-03, -9.4454e-02, -7.4507e-02,
         -1.9911e-03, -8.4070e-02, -5.9242e-03, -8.3062e-02, -6.2601e-02],
        [-8.2923e-02, -2.3238e-02, -2.7534e-03, -9.4454e-02, -7.4507e-02,
         -1.9911e-03, -8.4070e-02, -5.9242e-03, -8.3061e-02, -6.2601e-02],
        [-8.2923e-02, -2.3238e-02, -2.7534e-03, -9.4454e-02, -7.4507e-02,
         -1.9911e-03, -8.4070e-02, -5.9242e-03, -8.3061e-02, -6.2601e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.9027, -0.7346, -1.5146,  1.2765, -0.7346, -1.1544, -0.7346, -0.7346,
        -0.7346, -0.7346], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.1137e+01,  7.2326e-03, -1.0337e+00,  1.2661e+01,  7.2327e-03,
          2.1839e+00,  7.2327e-03,  7.2327e-03,  7.2327e-03,  7.2327e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 11.9774,  -0.7922],
        [ -0.9909,   4.1104],
        [  5.4366,  -5.3003],
        [  0.6996,  13.8475],
        [ 15.8587,   7.8685],
        [-10.3301,   3.2595],
        [  9.2801,  -2.6822],
        [-15.9756,   3.5841],
        [  6.6123,  -2.0647],
        [-14.0183, -10.8188]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-12.0846,  -4.0133,  -9.6421,  11.5196,   4.7537,   6.5523, -11.6756,
          3.6631, -11.2650,  -9.2618], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -0.5377,  -0.9241,  -1.9254,  -0.4662,  -3.5873,   2.7744,  -0.7832,
           1.6637,  -0.2475,   0.7769],
        [ -0.3998,   0.0746,  -1.0551,  -0.6526,  -1.9355,  -0.0235,  -0.4914,
          -0.0232,  -0.2738,  -0.3154],
        [-10.4289,   0.0375,   7.3554,  -6.5795, -12.2435,   4.6308,  -8.4529,
          -9.9492,  -5.9516,   8.9378],
        [ -0.3998,   0.0746,  -1.0551,  -0.6526,  -1.9355,  -0.0235,  -0.4914,
          -0.0232,  -0.2738,  -0.3154],
        [ -5.5830,  -0.3791,  -4.4182,   1.5835,  -5.6524,  -1.0312,  -5.1348,
          -6.1964,  -1.4174,   7.7971],
        [ -0.0386,  -0.0351,  -4.6807, -16.3610,  -2.3357,   7.8738,   0.1871,
           2.7585,  -0.1531, -10.5303],
        [ -0.5035,   0.0688,  -0.8084,  -0.6493,  -1.9766,  -0.0254,  -0.5015,
          -0.0248,  -0.2744,  -0.3391],
        [ -2.0387,  -0.0217,  -1.0262,  -3.2549,   1.5584,   0.9422,   0.2067,
          -0.3561,   0.6551,   0.2826],
        [ -0.3998,   0.0746,  -1.0551,  -0.6526,  -1.9355,  -0.0235,  -0.4914,
          -0.0232,  -0.2738,  -0.3154],
        [ -1.7240,  -2.6205,  -6.5572,   4.7526,  -1.8201,   3.8874,   0.3444,
           2.5903,   1.2678,   7.7057]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.9365, -2.3198,  0.5352, -2.3198, -0.6538, -0.0580, -2.4248, -3.3844,
        -2.3198, -3.8843], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  2.0873,   0.0456,  -8.1383,   0.0456,  15.4230,  -7.2269,   0.0364,
          -3.1105,   0.0456,   0.7534],
        [ -2.0867,  -0.0455,   8.1420,  -0.0455, -15.4153,   7.2267,  -0.0585,
           3.2992,  -0.0455,  -0.8180]], device='cuda:0'))])
xi:  [-244.78107]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 271.3734181924309
W_T_median: 83.77546589632601
W_T_pctile_5: -244.5636424495523
W_T_CVAR_5_pct: -334.26355273331586
Average q (qsum/M+1):  56.15025674143145
Optimal xi:  [-244.78107]
Expected(across Rb) median(across samples) p_equity:  0.3042173754113416
obj fun:  tensor(-1673.8054, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
