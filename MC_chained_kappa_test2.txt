Starting at: 
2022-07-06 19:47:05

 Random seed:  1  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = basic_T30_VWD
timeseries_basket['basket_desc'] = CRSP data: T30 and VWD
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
5.0% of MC simulations done.
10.0% of MC simulations done.
15.0% of MC simulations done.
20.0% of MC simulations done.
25.0% of MC simulations done.
30.0% of MC simulations done.
35.0% of MC simulations done.
40.0% of MC simulations done.
45.0% of MC simulations done.
50.0% of MC simulations done.
55.00000000000001% of MC simulations done.
60.0% of MC simulations done.
65.0% of MC simulations done.
70.0% of MC simulations done.
75.0% of MC simulations done.
80.0% of MC simulations done.
85.0% of MC simulations done.
90.0% of MC simulations done.
95.0% of MC simulations done.
100.0% of MC simulations done.
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  hidden_layer    None       None         None   
4        obj.layers[4]        4  hidden_layer    None       None         None   
5        obj.layers[5]        5  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  
4    False        None  
5    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  hidden_layer       4  logistic_sigmoid   
4        obj.layers[4]        4  hidden_layer       4  logistic_sigmoid   
5        obj.layers[5]        5  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 4)    False        None  
4       (4, 4)    False        None  
5       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1260.9596434837733
W_T_median: 1221.9241450328454
W_T_pctile_5: 830.3340558417918
W_T_CVAR_5_pct: 742.5812257595575
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2026.446450495473
gradient value of function right now is: [ 1.66987607e+01  8.02294520e+00  1.67059361e+01  1.22582949e+00
 -1.66987607e+01 -8.02294520e+00 -1.67059361e+01 -1.22582949e+00
 -3.63316321e-02  5.73911892e+00  4.89680153e-01  1.17363439e+00
  5.80614933e-01 -5.43327932e+00  1.60496953e-01 -9.89959726e-01
 -9.39038501e-02  1.24846536e+01  1.04839674e+00  2.54993299e+00
  5.69174873e-01 -1.68554664e+00  4.92975665e-01 -2.21999370e-01
  7.05248214e-01 -2.55955848e+00  1.03782004e+00  8.92119880e-01
  2.37657111e-02  5.42858462e+00 -1.50136108e-01 -3.00098597e-01
  1.16835336e-01 -2.17635211e+00  2.35858863e-01  2.48113481e-01
 -9.77421581e-02 -4.75977533e-02 -1.29333776e-01 -1.00467922e-01
 -8.59470139e-02 -4.25338525e-01  1.42746872e-02 -4.23135454e-01
  2.65217003e-01  1.24822834e+00 -2.37160418e-02  1.24051698e+00
 -3.24466085e-01 -1.28377417e+00 -4.21096240e-02 -1.26837541e+00
  1.39503701e-01 -5.50077028e-01  3.76792076e-01 -5.54570918e-01
 -1.04597798e-01 -3.21109488e-01  3.03934144e-01  3.38478773e-01
 -4.74438807e-01 -2.99867785e-01  5.71525958e-01  4.35639031e-01
 -9.68124488e+00]
supnorm grad right now is: 16.705936079251764
Weights right now are: 
[-0.58563835  0.35219461 -0.6640498  -0.05852412  0.05416809 -0.48619005
  0.82875683 -0.507532    0.27488361 -0.82371622  0.8213932   0.58099837
 -0.47135439  0.17049292  0.93512603  0.2911358   0.46077514 -0.70283502
  0.76784678  0.0792768   0.06690084  1.29410244  0.84823651  0.94331545
  0.54863681  0.7953159  -0.9593936  -0.7377603   0.65384135 -1.08696454
  1.52623613  1.1980038  -1.06922057  0.43570075 -0.86679145  0.09637491
  0.63271608  1.35672296  0.84386674  0.34845189 -0.36110915  0.64972616
 -0.58680486  1.79839681  0.06215483 -0.9211737   1.85262833 -1.26032532
 -0.59043763  0.80699229 -1.87107853  1.95079291 -0.5071093   1.3308324
 -0.64914702  1.8028919   0.23488401  0.64353608 -0.36458645 -2.05539392
 -0.10797461  2.67767068  0.05706536 -2.44217079 27.50635696]
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2079.4940401455237
gradient value of function right now is: [ 3.22651234 -5.65762734  3.06171547 -4.61577959 -3.22651234  5.65762734
 -3.06171547  4.61577959 -2.66263056  1.54674332 -2.66958137  0.19054186
  2.2280056  -0.54201112  2.27886305 -0.03080924 -4.49108755  2.36326309
 -4.51129217  0.28141105  1.90649454 -0.21874073  1.96137446  0.01401257
  0.32968276 -0.43821795  0.32109439  0.34883533 -0.06484885  0.78732737
 -0.02395598 -0.07856395  0.04091142 -0.57772684  0.02954372  0.04353253
 -0.10322786  0.05037831 -0.10710602 -0.10714133 -0.03661537 -0.04472927
 -0.07184053 -0.03751888  0.10105159  0.04409598  0.16339497  0.0373315
 -0.12213642 -0.06487922 -0.19150794 -0.05115391 -0.07946554 -0.10755358
 -0.15001155 -0.08697308 -0.04226991  0.06307121  0.05869676 -0.28760687
  0.07627547  0.24586618 -0.04086304 -0.36960854  4.3687191 ]
supnorm grad right now is: 5.657627343403678
Weights right now are: 
[-1.00583587  0.18247826 -1.0767735  -0.03703169  0.47436561 -0.3164737
  1.24148054 -0.52902443  0.73394172 -1.80109971  1.24786297  0.45589578
 -0.4380699   1.23217681  1.0141735   0.74278467  0.81860148 -1.72710761
  1.0854339  -0.11987032  0.13780035  2.40788762  0.94056397  1.48314781
  0.04704201  1.97447018 -1.48188962 -1.39460531  1.00850257 -2.22175011
  1.928755    1.71562968 -1.80417042  1.58092366 -1.59352157 -0.74959465
  1.20476742  1.6990502   1.34412302  0.96425413 -0.68489559  1.53781407
 -1.16873214  2.77703238  0.24170735 -1.7742329   2.31091749 -2.20787511
 -0.487607    1.70746397 -2.0928494   2.91586489 -0.81144432  2.21537914
 -1.18883563  2.76527703  0.83632959  1.80240354 -0.79621414 -2.90524694
  0.45442371  3.44367123 -0.45050067 -3.22531687 27.43326903]
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2086.514955473749
gradient value of function right now is: [ 2.65583938e-01 -4.47317221e+00  1.79933200e-01 -3.21410840e+00
 -2.65583938e-01  4.47317221e+00 -1.79933200e-01  3.21410840e+00
 -1.37458880e+00  1.41055063e-01 -1.36022289e+00  3.13274135e-02
  1.67682615e+00 -1.07914222e-01  1.68546896e+00 -4.48218550e-03
 -2.14924403e+00  2.23094543e-01 -2.12031811e+00  4.84499260e-02
  1.59576238e+00 -5.48104024e-02  1.60779214e+00  8.17210951e-03
  3.26340362e-02 -6.70598653e-02 -1.12266916e-02  6.00860260e-02
  1.40294299e-01  2.74769660e-02  2.28026719e-01  1.06584251e-01
 -3.33076784e-02  5.18955086e-02 -5.23871414e-02 -3.65579627e-02
 -6.10473536e-02  3.82748046e-02 -5.87477542e-02 -6.67121474e-02
 -9.11649973e-02 -3.36760339e-02 -1.86759807e-01 -7.45101895e-03
  1.76239900e-01 -4.55344692e-02  3.49888050e-01 -8.50008114e-02
 -2.14749217e-01 -5.74088755e-02 -3.92944742e-01 -1.00026712e-02
 -2.11248160e-01 -1.52625747e-01 -3.99862453e-01 -9.55599821e-02
 -1.68850392e-01  1.64347888e-01  4.78882941e-01 -7.00359525e-01
 -1.58058226e-01  4.38742130e-01  3.88029926e-01 -7.71621531e-01
  7.72456797e+00]
supnorm grad right now is: 7.7245679730681935
Weights right now are: 
[-1.3048962   0.2264741  -1.37262738  0.04761948  0.77342593 -0.36046953
  1.53733442 -0.6136756   1.04989206 -2.43919142  1.54130082  0.31959899
 -0.53151686  1.65605714  0.93815968  0.86516668  1.11236527 -2.29730517
  1.3590632  -0.23681817  0.07244865  2.8713869   0.89258552  1.61563826
 -0.1420292   2.47824151 -1.65963106 -1.63593577  1.05163855 -2.67558415
  1.97749964  1.80391915 -2.04949981  2.16998278 -1.82590994 -1.03256689
  1.40105493  1.36281348  1.51040908  1.20975287 -0.70030152  1.67344457
 -1.13313559  2.89449511  0.31259724 -1.84649054  2.34625641 -2.26989131
 -0.44828198  1.85493223 -2.02296557  3.04623529 -0.77272982  2.45811822
 -1.10018338  2.98168065  1.22216012  1.53930562 -1.03647165 -2.55902504
  0.6485219   3.05380868 -0.64330848 -2.86980847 27.32541447]
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2082.8726646342075
gradient value of function right now is: [-2.92748559 -6.90018561 -3.27769623 -4.3462505   2.92748559  6.90018561
  3.27769623  4.3462505  -3.93184144 -1.52096834 -4.78686011 -0.46951877
  2.9083638   0.85197856  3.48379832  0.27929789 -5.84407203 -2.19755904
 -7.09870193 -0.68307016  2.37320763  0.67247318  2.83440877  0.22559753
  2.09546298  1.3479762   2.77836862  1.88616273 -4.34489201 -2.85840259
 -5.76269197 -3.9049342   1.08387033  0.90422966  1.45553967  0.94798829
 -0.34284108 -0.17861697 -0.45286901 -0.31096082  1.48964138  0.58797118
  2.6774791   0.39150048 -4.6406491  -1.97044218 -8.3486308  -1.3305981
  3.09599931  1.24612626  5.55854078  0.83322966  2.83618378  1.00802977
  5.07866572  0.65791796  0.07347169 -0.26395644 -3.32473223  9.47030227
  1.19291921 -3.14432265 -3.07324329  8.48704707 20.46141639]
supnorm grad right now is: 20.461416385786602
Weights right now are: 
[-1.42608579e+00  3.93324219e-01 -1.49120113e+00  2.12428837e-01
  8.94615526e-01 -5.27319656e-01  1.65590817e+00 -7.78484956e-01
  1.24293936e+00 -2.68747442e+00  1.70209703e+00  2.36620309e-01
 -7.27275032e-01  1.86344484e+00  7.74846377e-01  9.34515394e-01
  1.28535866e+00 -2.50833946e+00  1.50358757e+00 -3.02800019e-01
 -1.79505023e-01  3.20430968e+00  6.79088933e-01  1.72158586e+00
  9.17815858e-03  2.82279303e+00 -1.48844141e+00 -1.53163887e+00
  7.22891023e-01 -2.99119860e+00  1.65647102e+00  1.51479176e+00
 -1.86749696e+00  2.41502019e+00 -1.66254791e+00 -8.81976535e-01
  1.53305381e+00  6.60319013e-01  1.58704001e+00  1.39461379e+00
 -2.25013082e-01  2.14448393e+00 -5.97454960e-01  3.32163566e+00
 -1.33866889e-01 -2.23118156e+00  1.85960104e+00 -2.62200021e+00
  2.96519721e-02  2.27897251e+00 -1.49033525e+00  3.42558919e+00
 -2.89325714e-01  3.02824485e+00 -5.44515350e-01  3.49828920e+00
  1.89050458e+00  6.65269761e-01 -1.64655806e+00 -1.83121814e+00
  1.09726850e+00  2.31259452e+00 -1.15444566e+00 -2.14804877e+00
  2.69457072e+01]
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2090.928925416194
gradient value of function right now is: [-0.7496747  -3.68963287 -0.91630745 -2.46379759  0.7496747   3.68963287
  0.91630745  2.46379759 -1.81672961 -0.57016613 -2.18676238 -0.16920399
  1.70697428  0.37529003  2.00711965  0.12127484 -2.7090018  -0.83077786
 -3.24984128 -0.25089335  1.47535039  0.33199943  1.7288232   0.11297629
  0.96809575  0.57248907  1.3037352   0.89157541 -1.95310223 -1.19608867
 -2.63171193 -1.79494721  0.43938368  0.37397731  0.60444085  0.38690689
 -0.18211221 -0.06464129 -0.24406966 -0.16916433  0.54318533  0.24252669
  1.22070774  0.18025216 -1.5901336  -0.81707517 -3.58144046 -0.6155119
  1.12141803  0.51829831  2.519434    0.38729295  1.0454484   0.40115127
  2.34885921  0.2931354   0.05536125 -0.20585909 -1.02838688  3.80784964
  0.49595883 -1.55339287 -1.13713918  3.77529659  4.98107127]
supnorm grad right now is: 4.981071271206044
Weights right now are: 
[-1.47707020e+00  4.27889172e-01 -1.54296623e+00  2.46643577e-01
  9.45599938e-01 -5.61884609e-01  1.70767327e+00 -8.12699696e-01
  1.28993457e+00 -2.70878449e+00  1.74568537e+00  2.14996532e-01
 -7.70078868e-01  1.93475502e+00  7.39592577e-01  9.71980979e-01
  1.32422218e+00 -2.53336302e+00  1.53818436e+00 -3.21846147e-01
 -2.22505982e-01  3.30242528e+00  6.43975575e-01  1.76338042e+00
  7.89983577e-03  2.85616914e+00 -1.48315380e+00 -1.54904958e+00
  7.39269388e-01 -2.98527476e+00  1.66934339e+00  1.54601022e+00
 -1.90767974e+00  2.36857365e+00 -1.70554143e+00 -9.21955596e-01
  1.62309570e+00  4.33234741e-01  1.66829572e+00  1.50092894e+00
 -2.16487586e-01  2.26835123e+00 -5.50221970e-01  3.45251223e+00
 -1.86662131e-02 -2.23173169e+00  1.94119747e+00 -2.65343471e+00
  1.94585448e-02  2.38319610e+00 -1.45737170e+00  3.54040152e+00
 -2.93041987e-01  3.19010507e+00 -5.20285913e-01  3.67306455e+00
  2.21997593e+00  1.22633448e-01 -1.75875435e+00 -1.94396988e+00
  1.27748996e+00  2.18427849e+00 -1.28467579e+00 -2.18667576e+00
  2.63439325e+01]
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2079.442154259544
gradient value of function right now is: [ -3.14843588  -7.17350116  -3.7163474   -4.17880853   3.14843588
   7.17350116   3.7163474    4.17880853  -5.34632347  -1.54718914
  -6.68744851  -0.69472108   2.46093848   0.54317891   3.01243666
   0.25374128  -8.03568054  -2.26400672 -10.02446201  -1.02211953
   1.84520719   0.39152126   2.24996124   0.18495039   2.27841728
   1.0219202    3.13805214   2.020011    -4.39497126  -2.0735463
  -6.07077928  -3.87803388   0.99888952   0.61755492   1.40577669
   0.85320579  -0.39786005  -0.1528073   -0.54513607  -0.35543364
   0.67913991   0.44094526   2.47371388   0.30205699  -1.84097687
  -1.360055    -6.65743187  -0.94861876   1.43992022   0.97144317
   5.2394368    0.66883122   1.36960032   0.77174442   5.01263153
   0.51785129   0.1913685   -0.69040144  -2.57608771   7.16496178
   1.09899234  -2.8443285   -2.41659879   6.54783135  17.96029032]
supnorm grad right now is: 17.96029032315608
Weights right now are: 
[-1.59010172  0.2660498  -1.66165126  0.08159489  1.05863146 -0.40004524
  1.8263583  -0.64765101  1.22708745 -2.70145734  1.69608558  0.17960638
 -0.54224442  2.05045457  0.96865935  1.06778486  1.25300275 -2.53145384
  1.47603207 -0.34873772  0.04743244  3.48058723  0.91652246  1.90878266
  0.09471784  2.79588196 -1.40363261 -1.45939917  0.6886584  -2.8863285
  1.62881178  1.48986242 -2.04216576  2.2050183  -1.85745486 -1.0394658
  1.50452665  0.1290983   1.55025591  1.3873764  -0.24426612  2.21762344
 -0.54796696  3.404505    0.12028228 -2.04571479  2.09582227 -2.47861697
 -0.03058687  2.3024398  -1.48893942  3.46485723 -0.29204378  3.20411981
 -0.45915673  3.67837978  2.35468    -0.30036866 -1.67395861 -2.09091124
  1.21386781  2.32302619 -1.19189318 -2.35110524 26.64156473]
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -2090.838890415543
gradient value of function right now is: [ 1.21183269e+00 -4.19290697e-04  1.34820815e+00 -2.80030880e-01
 -1.21183269e+00  4.19290697e-04 -1.34820815e+00  2.80030880e-01
  5.82464943e-01  4.95977713e-01  8.23468943e-01  2.55162623e-01
  1.23811699e-01 -2.40476445e-01 -2.48526057e-02 -1.18681508e-01
  9.50532909e-01  7.10084719e-01  1.35268517e+00  3.64041503e-01
  2.46387177e-01 -1.30639954e-01  1.61884864e-01 -6.32345681e-02
 -3.76632753e-01 -3.14136740e-01 -5.89480522e-01 -2.66463728e-01
  6.94938659e-01  5.51755210e-01  1.08076783e+00  4.97494314e-01
 -1.73501367e-01 -1.70116549e-01 -2.58136002e-01 -1.36857643e-01
  5.30437608e-02  6.07188244e-02  8.96334083e-02  3.17668476e-02
 -1.55996012e-01 -2.41903370e-01 -6.81406708e-01 -1.47744599e-01
  3.05471587e-01  4.05593492e-01  1.35178465e+00  2.33258446e-01
 -3.44515426e-01 -5.02367474e-01 -1.48970744e+00 -3.04837264e-01
 -2.52695013e-01 -4.19901284e-01 -1.07854318e+00 -2.64337229e-01
 -1.34475644e-01  2.31386090e-01  1.63936171e+00 -2.62691691e+00
 -7.03061106e-01  1.24803151e+00  1.38594371e+00 -2.41470428e+00
 -7.55024663e+00]
supnorm grad right now is: 7.550246631452496
Weights right now are: 
[-1.55159831  0.33181003 -1.61209917  0.13199213  1.02012805 -0.46580546
  1.77680621 -0.69804825  1.507538   -2.48557126  2.01197467  0.38873638
 -0.54005525  2.11745194  0.97426781  1.11845246  1.48942816 -2.35842795
  1.74220845 -0.19037098  0.05796408  3.57136756  0.93061361  1.96844019
 -0.15599401  2.60466714 -1.65261653 -1.72744128  0.9690023  -2.64829817
  1.90740967  1.78539905 -2.4127158   1.9331022  -2.24154074 -1.39625499
  1.70509775  0.15208912  1.7422655   1.61448111 -0.47812658  2.15207235
 -0.7516882   3.29366816  0.44891566 -1.79574847  2.43432988 -2.19361215
 -0.29724652  2.18787275 -1.72199536  3.30555389 -0.54538601  3.19689977
 -0.67519896  3.61058676  2.49317181 -0.18669311 -1.76349326 -2.2180966
  1.25878649  2.48286873 -1.21987565 -2.53900201 26.40966373]
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2089.1381795399407
gradient value of function right now is: [  1.72131352   3.30699675   2.05204666   1.82919982  -1.72131352
  -3.30699675  -2.05204666  -1.82919982   1.51617624   0.81720528
   1.96725289   0.42483507  -1.63293403  -0.54087682  -2.08111178
  -0.27873094   2.56240761   1.19347938   3.31316088   0.61746394
  -1.13729504  -0.34354425  -1.44653845  -0.17701128  -0.80480239
  -0.57823826  -1.21108386  -0.59609908   1.48963367   1.03308874
   2.23766116   1.10767719  -0.27598962  -0.35206814  -0.41966873
  -0.20027218   0.19803776   0.11207923   0.29576492   0.14927883
  -0.32471879  -0.47574465  -1.3488212   -0.30185115   0.60177226
   0.87102054   2.46229123   0.55062593  -0.69365345  -1.02060981
  -2.87697533  -0.64668139  -0.51385153  -0.73989604  -2.13237776
  -0.47102887  -0.26536444   0.54367269   2.89162582  -4.83821766
  -1.34547902   2.33971032   2.52873361  -4.42267155 -12.61184278]
supnorm grad right now is: 12.611842779667619
Weights right now are: 
[-1.60912093e+00  4.25566253e-01 -1.66676362e+00  2.39838868e-01
  1.07765067e+00 -5.59561690e-01  1.83147066e+00 -8.05894987e-01
  1.57655103e+00 -2.45908627e+00  2.07990815e+00  4.07754514e-01
 -6.48520270e-01  2.23473832e+00  8.88626123e-01  1.21583585e+00
  1.54570139e+00 -2.34795138e+00  1.79204973e+00 -1.81957807e-01
 -7.56068431e-02  3.70112726e+00  8.15577195e-01  2.06892229e+00
 -1.30580759e-01  2.62049617e+00 -1.60623021e+00 -1.74274990e+00
  9.48676690e-01 -2.62998813e+00  1.86993036e+00  1.80189222e+00
 -2.43449665e+00  1.78797295e+00 -2.26701859e+00 -1.41231544e+00
  1.77889848e+00  2.95624945e-03  1.79082363e+00  1.73515706e+00
 -4.32726007e-01  2.30375571e+00 -6.52107528e-01  3.39279260e+00
  5.07655597e-01 -1.72546138e+00  2.46207551e+00 -2.07454324e+00
 -2.49599584e-01  2.32256326e+00 -1.61277644e+00  3.38518310e+00
 -4.91174615e-01  3.43084138e+00 -6.04791654e-01  3.81054466e+00
  2.63665272e+00 -2.67118965e-01 -1.97875815e+00 -2.12858422e+00
  1.45882191e+00  2.33394261e+00 -1.40718471e+00 -2.43835486e+00
  2.63049951e+01]
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2093.5970430416764
gradient value of function right now is: [ 2.36166529e-01  1.82066987e+00  3.07267984e-01  1.27081142e+00
 -2.36166529e-01 -1.82066987e+00 -3.07267984e-01 -1.27081142e+00
  4.61800873e-01 -1.07936798e-01  5.08427830e-01 -5.51274170e-02
 -1.10999384e+00 -1.67697499e-01 -1.30630885e+00 -1.01830079e-01
  8.76226842e-01 -8.70977585e-02  1.00491090e+00 -3.99668330e-02
 -1.00785265e+00 -1.44275196e-01 -1.17078798e+00 -8.67646283e-02
 -2.97995997e-01  3.33681393e-02 -4.30021043e-01 -2.59872820e-01
  5.75025285e-01 -2.67007281e-02  8.37535666e-01  4.92485121e-01
 -1.87522618e-03  1.78282412e-01  4.79433490e-03 -2.20086715e-02
  1.42431442e-01  4.87933102e-02  2.07773073e-01  1.18222660e-01
 -1.59811844e-02 -1.70540955e-02 -3.62537885e-01  1.53179842e-02
 -6.19252565e-02 -2.72375065e-01  1.75163657e-01 -2.56988770e-01
 -4.59730084e-02 -3.21255596e-02 -7.35792638e-01  4.06020149e-02
 -5.98356598e-02 -1.43690941e-01 -8.12411347e-01 -6.31124690e-02
  2.42614580e-02  3.97437339e-02  6.63528098e-01 -1.32715137e+00
  4.44651834e-02  1.94766541e-01  1.83602848e-01 -7.27694319e-01
  3.54229849e+00]
supnorm grad right now is: 3.542298486876584
Weights right now are: 
[-1.7282613   0.56931418 -1.78520361  0.42111283  1.19679104 -0.70330962
  1.94991065 -0.98716895  1.62807081 -2.54953912  2.12647476  0.28201187
 -0.84066096  2.46760614  0.74375402  1.45759487  1.58486106 -2.41127065
  1.82156106 -0.26862034 -0.33129295  3.9864195   0.60323736  2.36657252
 -0.08923645  2.66205966 -1.54228716 -1.75597639  0.90027499 -2.65043173
  1.79983902  1.80794096 -2.51297627  1.76506675 -2.36423355 -1.47787974
  1.72309666 -0.18799533  1.67709536  1.78282169 -0.32160171  2.44233703
 -0.56388126  3.5065842   0.51705509 -1.63431018  2.52484215 -1.98406139
 -0.13835173  2.43663635 -1.5030988   3.47333791 -0.36990956  3.66934917
 -0.52140335  4.02232041  2.79604648 -0.37494792 -2.09505066 -2.08745284
  1.53561732  2.28864857 -1.49302185 -2.39558788 26.66264241]
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2080.2080868374424
gradient value of function right now is: [  1.52407399  12.28015694   1.98680883   8.74321036  -1.52407399
 -12.28015694  -1.98680883  -8.74321036   2.52396052   0.65859618
   3.07814694   0.35333091  -8.08554544  -1.34479859  -9.45215819
  -0.69670978   4.34395078   1.08401135   5.29453502   0.57736023
  -7.62420705  -1.10004619  -8.81000094  -0.563688    -1.8248472
  -0.74281927  -2.68130835  -1.48030569   3.59503234   1.47798952
   5.28795866   2.91075968  -0.26729752  -0.15885532  -0.42146324
  -0.1924464    0.80352938   0.3338135    1.17657387   0.65520331
  -0.54273902  -0.80987728  -3.13750007  -0.47438479   0.73915848
   1.09224878   4.22622514   0.60956405  -1.09406655  -1.71741576
  -6.26108374  -1.00647213  -0.9540671   -1.33862878  -5.52280948
  -0.79263376  -0.16682057   0.52016049   5.35542554 -10.80950317
  -1.86989373   3.9850204    4.21115932  -8.89601083 -11.78867085]
supnorm grad right now is: 12.280156937216546
Weights right now are: 
[-1.73044699e+00  6.67589601e-01 -1.77291282e+00  5.11602354e-01
  1.19897673e+00 -8.01585038e-01  1.93761986e+00 -1.07765847e+00
  1.75383842e+00 -2.47214259e+00  2.25859502e+00  3.98742220e-01
 -9.48198628e-01  2.53376770e+00  6.48944239e-01  1.46884528e+00
  1.71004454e+00 -2.34005990e+00  1.95037405e+00 -1.60267433e-01
 -4.23045018e-01  4.11295700e+00  5.23320996e-01  2.41259069e+00
 -1.62385502e-01  2.61203985e+00 -1.60491752e+00 -1.86668569e+00
  9.80540606e-01 -2.60679750e+00  1.86534314e+00  1.93353943e+00
 -2.58817835e+00  1.64340878e+00 -2.45464833e+00 -1.52950719e+00
  1.92111392e+00 -2.22466838e-01  1.84075717e+00  2.05967275e+00
 -1.84981974e-01  2.50980777e+00 -5.73202343e-01  3.57528941e+00
  4.79443488e-01 -1.51893364e+00  2.60535830e+00 -1.86963449e+00
  1.43656739e-02  2.48345913e+00 -1.49244249e+00  3.52267172e+00
 -2.49578969e-01  3.83545569e+00 -5.73183220e-01  4.19610192e+00
  2.83953715e+00 -4.82247348e-01 -2.12211397e+00 -2.07639143e+00
  1.49971045e+00  2.27653130e+00 -1.47674038e+00 -2.37544261e+00
  2.64592490e+01]
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2093.2718672714664
gradient value of function right now is: [ 1.01123165e+00  1.00511215e+00  1.20815187e+00  2.91290155e-01
 -1.01123165e+00 -1.00511215e+00 -1.20815187e+00 -2.91290155e-01
  7.07151136e-01  6.50778486e-01  9.97868530e-01  4.28782337e-01
 -5.53446444e-01 -3.49686264e-01 -8.14727552e-01 -2.37660163e-01
  1.24090298e+00  8.96218266e-01  1.73018873e+00  5.95735771e-01
 -1.72871296e-01 -1.58083622e-01 -2.89967431e-01 -1.08100108e-01
 -3.80644467e-01 -3.06737159e-01 -6.42413177e-01 -2.31606738e-01
  6.43735294e-01  4.96033349e-01  1.09491522e+00  3.81588840e-01
 -1.44196034e-01 -3.27347979e-01 -2.40764677e-01 -9.55159441e-02
  8.86641020e-02 -6.04868892e-03  1.54765971e-01  4.69821817e-02
 -1.40414826e-01 -2.84080643e-01 -7.39819019e-01 -1.67712115e-01
  2.60960835e-01  2.69714304e-01  1.02073580e+00  1.39839438e-01
 -3.47277132e-01 -6.80828815e-01 -1.82371447e+00 -3.98564158e-01
 -1.89931032e-01 -4.19140914e-01 -9.85606946e-01 -2.59061356e-01
 -5.13307967e-02  9.37064550e-02  1.80390591e+00 -3.19224062e+00
 -5.37424980e-01  1.28947426e+00  1.41828574e+00 -2.82448902e+00
 -6.59659196e+00]
supnorm grad right now is: 6.596591961267534
Weights right now are: 
[-1.73126301  0.46942619 -1.75135033  0.28102554  1.19979275 -0.60342162
  1.91605737 -0.84708166  1.85488813 -2.26018326  2.39688269  0.68849622
 -0.63332774  2.74019975  0.9666707   1.60981294  1.79849098 -2.12589944
  2.0737452   0.13237322 -0.05135696  4.50491287  0.90102729  2.7259659
 -0.1298948   2.47642354 -1.5847188  -1.82382022  0.92112168 -2.51087135
  1.81441116  1.87193385 -2.81666311  1.41369507 -2.7096114  -1.70723901
  1.72182124 -0.42982865  1.62196962  1.91089776 -0.26231636  2.51827665
 -0.5103055   3.54427785  0.64445298 -1.27716179  2.75982237 -1.60916492
 -0.09796975  2.45097956 -1.47541566  3.45244115 -0.30300354  3.95232535
 -0.48498521  4.27653466  2.86900653 -0.48748121 -2.21999849 -2.08313011
  1.52954904  2.29088523 -1.5476352  -2.38551872 26.54747494]
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2092.716491055704
gradient value of function right now is: [ 0.61788336  2.41043924  0.77810824  1.23820491 -0.61788336 -2.41043924
 -0.77810824 -1.23820491  0.8341696   0.26480279  1.06038014  0.14226218
 -1.47616874 -0.39355664 -1.82613173 -0.22618446  1.45334985  0.45122602
  1.85015199  0.24944157 -0.91836101 -0.22019364 -1.12150124 -0.12638686
 -0.44677069 -0.18383539 -0.716293   -0.34150596  0.85953771  0.37590289
  1.37176875  0.66132122 -0.12349436 -0.03300099 -0.20237809 -0.09670821
  0.16989775  0.08199145  0.26903179  0.13126126 -0.17789109 -0.30487401
 -0.94962631 -0.19217266  0.21173457  0.13890191  0.99670714  0.04883697
 -0.39413983 -0.67688853 -2.08468635 -0.42451315 -0.28725177 -0.51116248
 -1.50860271 -0.33332625 -0.06391467  0.18162948  1.99872293 -3.73566391
 -0.6024705   1.45597884  1.61963168 -3.32723982 -4.70416941]
supnorm grad right now is: 4.704169409868301
Weights right now are: 
[-1.84096629  0.48555774 -1.85836992  0.29047347  1.30949603 -0.61955318
  2.02307696 -0.85652959  1.92135754 -2.4421842   2.45336064  0.43439083
 -0.63113703  2.80707769  0.96363806  1.68619222  1.86337023 -2.26382338
  2.13011009 -0.06219533 -0.03496785  4.49991287  0.90722017  2.70217872
 -0.19637962  2.5953923  -1.64376665 -1.94387496  1.02917253 -2.56400981
  1.91996226  2.02367873 -2.80482631  1.66811271 -2.68502682 -1.76055875
  1.94930458  0.03775378  1.86708042  2.1408395  -0.27507409  2.67051922
 -0.5464413   3.70846641  0.59867146 -1.30683351  2.81099426 -1.66428138
 -0.1101634   2.62300313 -1.49454863  3.63797234 -0.4104278   3.98096349
 -0.65181442  4.31937949  2.98164171 -0.5089972  -2.31064152 -2.20013683
  1.65423663  2.26101791 -1.68229321 -2.43055028 26.64389837]
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2092.7308566767683
gradient value of function right now is: [-2.07161864e-01 -2.32175438e+00 -2.97356574e-01 -1.54589050e+00
  2.07161864e-01  2.32175438e+00  2.97356574e-01  1.54589050e+00
 -4.91173967e-01 -2.89534527e-01 -5.69077176e-01 -1.78549316e-01
  1.22136248e+00  4.69825888e-02  1.33192306e+00  2.01537336e-02
 -7.87186964e-01 -3.56251244e-01 -8.80911175e-01 -2.17034146e-01
  9.96327585e-01  4.46875839e-02  1.09435924e+00  2.00887017e-02
  1.11621315e-01  1.78295596e-01  1.37529616e-01  1.17868244e-01
 -2.89451530e-01 -2.48537270e-01 -3.71357257e-01 -3.06899862e-01
  1.15651836e-02  2.36841870e-01  2.12973531e-02 -9.74758418e-03
 -6.98486632e-02  5.65112895e-03 -9.00558461e-02 -7.81149186e-02
  4.13429866e-02  5.33996468e-02  1.82183313e-01  5.33993722e-02
 -1.31246737e-01 -5.49800096e-01 -6.18045144e-01 -4.51962897e-01
  5.87650489e-02  1.12004217e-01  2.17437042e-01  1.17910314e-01
  6.62811523e-02 -6.34554763e-03  3.50187784e-01  7.89639198e-03
  5.06346910e-02 -9.28421493e-02  2.97563598e-02 -1.96099249e-02
  3.78443982e-01 -3.21190042e-01 -2.15411639e-01  1.59989315e-01
  7.08458598e+00]
supnorm grad right now is: 7.084585976175269
Weights right now are: 
[-1.93822515e+00  3.23791834e-01 -1.96003893e+00  1.43089533e-01
  1.40675489e+00 -4.57787270e-01  2.12474597e+00 -7.09145652e-01
  1.88931744e+00 -2.50724188e+00  2.42512267e+00  3.61244478e-01
 -4.02509841e-01  2.96929741e+00  1.18182510e+00  1.86169693e+00
  1.81723104e+00 -2.33017448e+00  2.08578444e+00 -1.37958399e-01
  1.74748188e-01  4.60684791e+00  1.10238023e+00  2.81355288e+00
 -1.44995426e-01  2.64603526e+00 -1.59100733e+00 -1.91699489e+00
  9.99237063e-01 -2.58298645e+00  1.89267635e+00  2.01124756e+00
 -2.78601207e+00  1.72970011e+00 -2.66630331e+00 -1.76422073e+00
  1.93940535e+00  1.42221234e-01  1.86550229e+00  2.13659082e+00
 -1.32406597e-01  2.80386488e+00 -4.83124782e-01  3.84910290e+00
  5.51101428e-01 -1.27276645e+00  2.85724266e+00 -1.62224377e+00
  2.47002841e-02  2.76120940e+00 -1.43064380e+00  3.77986382e+00
 -2.68933418e-01  4.07820507e+00 -6.40477527e-01  4.44635647e+00
  3.04035964e+00 -6.22830678e-01 -2.37214828e+00 -2.24488385e+00
  1.74885167e+00  2.24521007e+00 -1.77243825e+00 -2.43087673e+00
  2.68412301e+01]
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -2093.929915540436
gradient value of function right now is: [-0.05533442  0.00490226 -0.05982217 -0.09409443  0.05533442 -0.00490226
  0.05982217  0.09409443  0.04062866 -0.2723531   0.04127433 -0.18402593
  0.01506428 -0.08631156 -0.03090158 -0.05280286  0.1844195  -0.30271822
  0.22837852 -0.20694451  0.0976946  -0.05076457  0.08429667 -0.03301199
 -0.08835116  0.14739868 -0.15585189 -0.07662105  0.13201383 -0.16070729
  0.25059371  0.09574708 -0.02516308  0.23492698 -0.03765378 -0.04245639
  0.02977807  0.02915757  0.05830853  0.01611505 -0.03279076 -0.0349754
 -0.21686593 -0.00774172 -0.07674785 -0.55268341 -0.38599797 -0.45441554
 -0.09047109 -0.07136764 -0.54893263 -0.00879896 -0.07022674 -0.15585567
 -0.41085365 -0.09303402  0.02915922 -0.03035045  0.54956982 -1.22384451
  0.32901512 -0.05935127  0.18622163 -0.86391924  4.82272617]
supnorm grad right now is: 4.822726171872951
Weights right now are: 
[-1.95534484e+00  5.02125670e-01 -1.97188764e+00  3.25362753e-01
  1.42387457e+00 -6.36121106e-01  2.13659468e+00 -8.91418872e-01
  1.98440411e+00 -2.45832490e+00  2.50989501e+00  4.13499444e-01
 -5.71563022e-01  3.03622748e+00  1.01934668e+00  1.95660075e+00
  1.89892006e+00 -2.32017836e+00  2.15293584e+00 -1.33859061e-01
  2.86341500e-03  4.58701905e+00  9.29466720e-01  2.81910726e+00
 -1.83381917e-01  2.60292333e+00 -1.61244932e+00 -2.00840968e+00
  1.09653195e+00 -2.52202620e+00  1.97529362e+00  2.16215834e+00
 -2.71793094e+00  1.64435637e+00 -2.59873526e+00 -1.69197959e+00
  2.13011330e+00  2.43195318e-01  2.04616360e+00  2.37620650e+00
 -1.03243282e-01  2.90890780e+00 -4.90201663e-01  3.95995759e+00
  4.52775431e-01 -1.10872125e+00  2.95698527e+00 -1.47529503e+00
  9.19342420e-02  2.87199412e+00 -1.40126628e+00  3.89565047e+00
 -3.39712895e-01  4.18987211e+00 -7.32981896e-01  4.56097849e+00
  3.04398486e+00 -5.14115358e-01 -2.40853474e+00 -2.28042854e+00
  1.61697759e+00  2.35565290e+00 -1.77356708e+00 -2.47493373e+00
  2.68187724e+01]
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2090.9460442955165
gradient value of function right now is: [ 4.21910140e-01 -1.86733138e+00  3.54091097e-01 -1.21582464e+00
 -4.21910140e-01  1.86733138e+00 -3.54091097e-01  1.21582464e+00
 -5.20730498e-01  4.94673852e-01 -5.14546785e-01  2.84165238e-01
  8.29735117e-01  6.96434619e-02  9.51331668e-01  3.33283145e-02
 -1.07912980e+00  4.51100527e-01 -1.14855321e+00  2.66353675e-01
  6.73019813e-01  6.19878097e-02  7.73165098e-01  3.05052272e-02
  2.06221510e-01 -1.44253600e-01  2.94870855e-01  2.36667616e-01
 -3.71760925e-01  1.64923788e-01 -5.36034975e-01 -4.23241473e-01
  1.24024048e-02 -4.28926566e-01  1.12933781e-02  1.86470772e-02
 -7.46724265e-02 -1.14111247e-01 -1.09947909e-01 -8.33749076e-02
  3.37677120e-02  5.76636558e-03  2.64971031e-01  4.68498210e-03
  1.77311112e-01  1.11530440e-01 -2.53788340e-01  1.40190615e-01
  5.61940951e-02 -5.14494810e-03  4.68812001e-01  1.81349014e-03
  9.63667050e-02  3.02086466e-02  7.07937791e-01  1.73023646e-02
  7.91108300e-03 -1.13838937e-01  1.75625365e-02  4.70609388e-01
  1.43523400e-01 -1.51248204e-01 -1.85273600e-02  3.28539888e-01
 -6.40927150e+00]
supnorm grad right now is: 6.409271497828229
Weights right now are: 
[-1.91914171e+00  2.48889173e-01 -1.94050643e+00  8.50327915e-02
  1.38767144e+00 -3.82884609e-01  2.10521347e+00 -6.51088911e-01
  1.89172115e+00 -2.23217289e+00  2.42621380e+00  7.00746989e-01
 -2.06443323e-01  3.34596047e+00  1.38057070e+00  2.27864844e+00
  1.76920376e+00 -2.18590600e+00  2.02794181e+00  4.56814494e-02
  3.46854108e-01  4.88294355e+00  1.26374971e+00  3.12200116e+00
  8.53626614e-03  2.50809960e+00 -1.40474801e+00 -1.83759464e+00
  9.19209818e-01 -2.46219024e+00  1.78382221e+00  2.00592426e+00
 -2.57342548e+00  1.36694143e+00 -2.44478433e+00 -1.52525860e+00
  1.94023592e+00 -8.22679251e-02  1.83167811e+00  2.22894969e+00
  1.54196016e-02  3.19259704e+00 -2.02868670e-01  4.22038582e+00
  4.82945874e-01 -8.51090161e-01  3.00048509e+00 -1.17403540e+00
  2.31077571e-01  3.14432098e+00 -1.11510436e+00  4.14619019e+00
 -2.73130201e-01  4.56992183e+00 -5.04249512e-01  4.93098499e+00
  3.08527456e+00 -5.65458442e-01 -2.72167136e+00 -2.05904827e+00
  1.63785243e+00  2.24951967e+00 -2.02559737e+00 -2.25557678e+00
  2.59808626e+01]
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2093.61900246335
gradient value of function right now is: [ 1.42185414e-01 -1.63248916e+00  1.04957540e-01 -1.14910988e+00
 -1.42185414e-01  1.63248916e+00 -1.04957540e-01  1.14910988e+00
 -2.17863952e-01  8.60704220e-02 -1.96202254e-01  1.06028114e-02
  9.03221281e-01  3.18161155e-02  1.00568974e+00  5.28659210e-03
 -4.02160041e-01  1.49111519e-01 -3.73009524e-01  5.60257843e-02
  7.84523953e-01  4.13194795e-02  8.78202895e-01  1.34304849e-02
  4.91953551e-02  2.97418237e-03  2.48189368e-02  9.67609810e-02
 -1.99585877e-01 -4.95585713e-03 -2.26747612e-01 -2.83559380e-01
 -7.73784526e-02 -4.08199497e-02 -1.47817534e-01 -5.96751614e-02
 -6.93660279e-02 -1.65605177e-02 -9.90678948e-02 -8.15415273e-02
  2.84766482e-02 -7.55701309e-02 -2.33094085e-03 -4.06532157e-02
 -3.22207711e-02 -2.28473495e-01 -1.62700199e-01 -1.93773290e-01
  1.64415884e-02 -2.18672405e-01 -2.41753246e-01 -1.16525691e-01
  1.06617770e-01 -1.01113870e-01  2.61679130e-01 -5.99724428e-02
  1.43620605e-02 -2.03480830e-02  7.32919964e-01 -9.17839631e-01
  1.81301501e-01  2.94849201e-02  5.50225286e-01 -8.31206609e-01
 -4.59012961e+00]
supnorm grad right now is: 4.590129608936868
Weights right now are: 
[-2.0477557   0.32165699 -2.02713263  0.16275402  1.51628544 -0.45565242
  2.19183967 -0.72881014  2.04452644 -2.3543214   2.5435193   0.73388589
 -0.24566371  3.3819282   1.3406155   2.33314477  1.92301669 -2.17661148
  2.1558377   0.1902613   0.30139956  4.88357758  1.21423156  3.14642444
 -0.04116479  2.58825032 -1.45484057 -1.90041041  0.98455102 -2.48619465
  1.84993927  2.09087523 -2.58997616  1.74450371 -2.48002556 -1.47393975
  2.0313704   0.93805298  1.9143994   2.37170104  0.18177098  3.12133674
 -0.23833084  4.18602144  0.30696381 -0.67865928  3.26817284 -1.09325691
  0.43579907  3.03970323 -1.15307743  4.08014348 -0.1220327   4.60057015
 -0.55157571  5.01433579  3.01776158 -0.10452463 -2.5330642  -2.19829557
  1.35404159  2.45253394 -1.80599628 -2.38926782 26.17808988]
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2089.3152417393685
gradient value of function right now is: [-1.35864701e+00 -2.88278988e+00 -1.91165296e+00 -1.74884463e+00
  1.35864701e+00  2.88278988e+00  1.91165296e+00  1.74884463e+00
 -2.68531506e+00 -1.57686646e+00 -3.08293204e+00 -1.05057227e+00
  1.57420516e+00  3.35470594e-01  1.81993786e+00  1.71027155e-01
 -4.30384573e+00 -2.26934839e+00 -4.94605144e+00 -1.49388374e+00
  1.17837189e+00  2.29793878e-01  1.36482698e+00  1.10363381e-01
  7.03507454e-01  4.53506273e-01  1.14782322e+00  7.60000853e-01
 -1.18931874e+00 -6.36276766e-01 -1.91743946e+00 -1.32309020e+00
  4.03597548e-01  4.54262816e-01  6.85200762e-01  3.83158965e-01
 -1.33636486e-01  9.84176449e-03 -2.03514583e-01 -1.68989722e-01
  5.16382506e-01  4.23432792e-01  1.65814043e+00  2.92999150e-01
 -7.40261458e-01 -1.50457478e+00 -2.58168065e+00 -1.14858936e+00
  1.19149864e+00  1.07127788e+00  3.90315584e+00  7.42804200e-01
  1.04133699e+00  6.66628155e-01  3.23495212e+00  4.52527720e-01
 -1.00837505e-01  3.28540090e-01 -2.69046625e+00  6.84278647e+00
  1.21195692e+00 -1.73828563e+00 -2.66618057e+00  6.31293161e+00
  1.41075599e+01]
supnorm grad right now is: 14.107559936432278
Weights right now are: 
[-2.19834213e+00  3.77865754e-01 -2.17992745e+00  2.20939487e-01
  1.66687187e+00 -5.11861191e-01  2.34463449e+00 -7.86995607e-01
  2.01141400e+00 -2.55761519e+00  2.48701083e+00  5.75621397e-01
 -2.84654233e-01  3.39529568e+00  1.29370897e+00  2.38377981e+00
  1.88530574e+00 -2.37177552e+00  2.10037613e+00  1.52093801e-02
  2.63785063e-01  4.82860453e+00  1.16602454e+00  3.12537414e+00
 -5.89560857e-03  2.69403385e+00 -1.38704722e+00 -1.92813886e+00
  1.01071334e+00 -2.53550820e+00  1.85595936e+00  2.17468610e+00
 -2.35694714e+00  2.01146694e+00 -2.20444338e+00 -1.25838827e+00
  2.24119983e+00  1.53093059e+00  2.14659192e+00  2.61622754e+00
  2.20683008e-01  3.32431289e+00 -1.41250619e-01  4.38757669e+00
  1.96617883e-01 -5.46123264e-01  3.35245413e+00 -9.85459833e-01
  5.25364490e-01  3.27953371e+00 -1.00185547e+00  4.31634192e+00
 -2.18050691e-01  4.77896163e+00 -5.83507383e-01  5.20733313e+00
  2.92822362e+00  1.09344627e-01 -2.69459207e+00 -2.09313765e+00
  1.18938644e+00  2.47334727e+00 -1.95425425e+00 -2.25622779e+00
  2.67004715e+01]
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2092.7541335994392
gradient value of function right now is: [  1.65511056   3.18113013   2.11365984   1.88930497  -1.65511056
  -3.18113013  -2.11365984  -1.88930497   1.70304903   1.88016931
   1.92959267   0.97484893  -2.40832835  -0.64208344  -2.625659
  -0.39849445   2.80794785   2.54799465   3.14625978   1.43340732
  -1.74912795  -0.37268161  -1.90022851  -0.22461808  -0.40817839
  -0.751556    -0.67683836  -0.41681971   0.77699392   0.85807859
   1.26503585   0.84605075  -0.19859206  -0.94618667  -0.34759066
  -0.16628724   0.14260643  -0.16488834   0.22715888   0.17105348
  -0.37229136  -0.51940002  -1.36109589  -0.36682939   0.83459901
   2.23732062   0.91842676   2.14115724  -0.82499466  -1.24333587
  -3.14975059  -0.87049449  -0.73083101  -0.88379775  -2.56284524
  -0.62300285  -0.11364408  -0.06687431   3.14647795  -5.66478537
  -0.26120666   0.75368173   3.00762427  -5.42085048 -16.19927695]
supnorm grad right now is: 16.199276946452116
Weights right now are: 
[-2.05704246e+00  5.85841874e-01 -1.99579942e+00  4.19797617e-01
  1.52557220e+00 -7.19837311e-01  2.16050646e+00 -9.85853737e-01
  2.34773212e+00 -2.23194051e+00  2.78266700e+00  9.53412687e-01
 -4.96897471e-01  3.18694958e+00  1.07357429e+00  2.23702562e+00
  2.19715486e+00 -2.09328562e+00  2.37766575e+00  3.24756757e-01
  6.81784533e-02  4.53397559e+00  9.58260220e-01  2.92300261e+00
 -1.89697422e-01  2.47194602e+00 -1.55490353e+00 -2.22852472e+00
  1.24753509e+00 -2.33100555e+00  2.08425413e+00  2.54127585e+00
 -2.37552407e+00  1.82692574e+00 -2.19503561e+00 -1.32671709e+00
  2.53702633e+00  1.58188999e+00  2.45276741e+00  3.02908541e+00
 -7.37131203e-02  3.29372592e+00 -3.40760594e-01  4.34581930e+00
  3.06820381e-01 -1.77234922e-02  3.68148875e+00 -4.64084558e-01
  2.71560855e-01  3.26953101e+00 -1.15765727e+00  4.29146400e+00
 -6.90138114e-01  4.70152201e+00 -9.40382503e-01  5.12973943e+00
  2.94926842e+00  2.73886015e-02 -2.66521046e+00 -2.23845459e+00
  6.91400513e-01  2.71443204e+00 -1.88116714e+00 -2.38590673e+00
  2.65186501e+01]
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2095.214910570122
gradient value of function right now is: [-1.08660221 -2.3589028  -1.63299335 -1.48868346  1.08660221  2.3589028
  1.63299335  1.48868346 -2.04753239 -1.60827009 -2.22270372 -1.19081411
  1.52962776  0.31868947  1.69419209  0.18620804 -3.32844688 -2.38270384
 -3.62021846 -1.76028848  1.19009209  0.22141735  1.3214913   0.12084215
  0.38386823  0.43091149  0.6652534   0.41353215 -0.75659551 -0.50649747
 -1.26467037 -0.8855091   0.18775747  0.44820075  0.35234277  0.1538711
 -0.13051903  0.06170126 -0.20124389 -0.17763487  0.26103282  0.33332001
  1.21013985  0.20943661 -0.44351642 -2.09573455 -2.33454278 -1.68964096
  0.58252749  0.85706732  2.81333372  0.545256    0.5479756   0.53623336
  2.41154613  0.32257131  0.01690252 -0.06551626 -2.35226912  5.9516239
  1.54926055 -1.13768663 -2.28830506  5.02487848  9.6983171 ]
supnorm grad right now is: 9.698317100343411
Weights right now are: 
[-2.24581179  0.47236184 -2.19275944  0.33001246  1.71434152 -0.60635727
  2.35746648 -0.89606858  2.26064314 -2.48899268  2.69322995  0.81065622
 -0.31428888  3.59782019  1.24810721  2.71992983  2.09850452 -2.3345646
  2.27969397  0.17135664  0.21957596  4.89858435  1.10131079  3.38601511
 -0.10032884  2.80581984 -1.45076532 -2.13396551  1.169069   -2.54648561
  1.98776398  2.46589339 -2.27295223  2.22268709 -2.08794402 -1.17580249
  2.39446819  2.10168687  2.27020659  2.89379711  0.05302769  3.54956486
 -0.12148957  4.5730634   0.07822149 -0.38008681  3.72253419 -0.93751893
  0.39301112  3.48791825 -0.94293037  4.47938241 -0.55509767  5.09694444
 -0.69996346  5.49147573  3.30839091  0.02818399 -2.98457098 -1.960188
  0.77064004  2.73396088 -2.12609842 -2.15139123 26.80170045]
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2098.453434701113
gradient value of function right now is: [ 7.87309192e-01 -1.87893105e+00  9.30856141e-01 -1.38509024e+00
 -7.87309192e-01  1.87893105e+00 -9.30856141e-01  1.38509024e+00
  1.95169976e-01  1.13740533e+00  2.52654936e-01  4.73061342e-01
  1.23452673e+00  6.09055926e-02  1.32807205e+00  1.91656860e-02
  9.94719039e-02  1.45188761e+00  1.45249487e-01  7.29961746e-01
  1.15565488e+00  7.33042606e-02  1.24346648e+00  2.77639376e-02
  7.63084207e-02 -4.99957396e-01  1.08467578e-01  1.26666448e-01
 -2.94930841e-01  5.01944951e-01 -4.31827469e-01 -4.23210469e-01
 -5.70366718e-02 -6.88037536e-01 -9.00982868e-02 -5.22647460e-02
 -9.43083670e-02 -1.97359506e-01 -1.41632528e-01 -1.21107615e-01
  7.22926120e-02 -7.87439712e-03  2.88656218e-01 -2.27312502e-02
  3.70765067e-01  1.48815101e+00  8.17601393e-02  1.50531140e+00
  1.04685260e-01 -5.82240298e-02  4.78577564e-01 -7.69861466e-02
  2.32284253e-01  5.22935246e-02  7.98767581e-01  5.06367093e-03
 -8.41563969e-03 -9.67654792e-02 -2.97458013e-01  1.19986483e+00
  4.37950592e-01 -6.42866499e-02 -1.47560976e-01  6.83685511e-01
 -5.82472379e+00]
supnorm grad right now is: 5.824723789478372
Weights right now are: 
[-2.10437923  0.40709823 -2.05833973  0.27905552  1.57290896 -0.54109367
  2.22304677 -0.84511164  2.51559267 -2.0613062   2.92266028  1.26917767
 -0.21805993  3.84095032  1.33646811  3.09573884  2.22675314 -2.00877749
  2.39539898  0.49548446  0.30609774  5.06337482  1.17998732  3.66850036
 -0.10956115  2.50685414 -1.46001903 -2.13435028  1.20400016 -2.27077934
  2.02995712  2.498145   -2.31459211  1.93740095 -2.12281732 -1.18244391
  2.43595512  1.98290753  2.33211704  2.94395633  0.11305036  3.4851706
 -0.15542929  4.50650041  0.3264152   0.11572816  4.32013529 -0.50343059
  0.44307571  3.42090404 -0.9717417   4.40027444 -0.4527853   5.09672643
 -0.72502936  5.51825312  3.44561677  0.41315153 -2.87147168 -2.07629848
  0.51249854  3.12128798 -1.94950043 -2.28785571 26.58957139]
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2091.902876643791
gradient value of function right now is: [ 2.13459731e-01  2.34423905e+00  4.38899996e-01  1.79582650e+00
 -2.13459731e-01 -2.34423905e+00 -4.38899996e-01 -1.79582650e+00
  7.90189347e-01  7.83182716e-03  8.65932452e-01 -1.12909912e-02
 -1.78079316e+00 -2.38070119e-01 -1.91640444e+00 -1.24482685e-01
  1.88955954e+00  2.05229132e-01  2.06648077e+00  1.05956579e-01
 -1.77797578e+00 -2.25069717e-01 -1.91106795e+00 -1.15718497e-01
 -3.18695600e-01  2.65752157e-02 -4.97182026e-01 -3.51582111e-01
  6.96538507e-01  9.52187313e-03  1.07088093e+00  7.95148620e-01
 -1.41122696e-01  5.83455700e-02 -2.29885765e-01 -1.40031607e-01
  1.75050751e-01  3.18123411e-02  2.67705166e-01  2.02318063e-01
 -2.89975272e-01 -4.47887745e-01 -1.38822185e+00 -2.95968841e-01
  3.90123579e-02 -3.20913971e-01  3.55660204e-02 -2.78683223e-01
 -5.46808749e-01 -8.87997867e-01 -2.70514397e+00 -5.82946162e-01
 -5.70113423e-01 -8.33424510e-01 -2.63065244e+00 -5.57063452e-01
 -9.15382712e-02  2.18107249e-01  3.27767877e+00 -5.35960561e+00
  4.20236779e-03  5.94232919e-01  2.76696078e+00 -4.76822215e+00
 -7.79247051e+00]
supnorm grad right now is: 7.792470514706745
Weights right now are: 
[-2.3968064   0.50412405 -2.42274842  0.42133066  1.86533613 -0.63811948
  2.58745546 -0.98738678  2.31727002 -2.54656303  2.753555    0.69623457
 -0.30845293  4.07715828  1.24309909  3.53891005  2.05561886 -2.57586743
  2.25453949 -0.16434963  0.16318579  5.08295335  1.03371219  3.88348222
 -0.33808722  2.86186852 -1.6762118  -2.50586943  1.48123169 -2.50522959
  2.30852769  2.89915298 -2.5107531   2.29112946 -2.28981878 -1.52928441
  2.81275724  2.40105866  2.74516808  3.38352373 -0.14565145  3.36140809
 -0.50766486  4.43849586  0.10836368 -0.37096041  4.10558736 -1.00697292
  0.23633256  3.35851397 -1.25402139  4.38592495 -0.81572022  4.83489758
 -1.19717431  5.32479289  3.87552387  0.32347003 -2.56146484 -2.51597097
  1.1648715   3.1403513  -1.74916463 -2.63557392 26.70223865]
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2083.3358314923858
gradient value of function right now is: [-1.08265316e+00 -2.18881827e+00 -2.19993116e+00 -1.44793600e+00
  1.08265316e+00  2.18881827e+00  2.19993116e+00  1.44793600e+00
 -2.66973939e+00 -1.29546810e+00 -2.87374989e+00 -9.58068363e-01
  5.14600275e-01  4.77064016e-02  5.51108613e-01  2.41439112e-02
 -6.06944012e+00 -2.42509624e+00 -6.53726918e+00 -1.77584401e+00
  5.42858110e-01  4.67291856e-02  5.81159985e-01  2.26094623e-02
  6.05788445e-01  2.54053612e-01  1.00841173e+00  7.60648884e-01
 -6.72159184e-01 -3.25555491e-01 -1.13121689e+00 -8.25174732e-01
  4.71953519e-01  2.74901746e-01  8.08470598e-01  5.60846723e-01
 -3.33493899e-02  1.26104174e-02 -4.92653131e-02 -5.02030547e-02
  2.40895322e-01  3.67862052e-01  1.48509937e+00  2.57155082e-01
 -2.36989437e-01 -1.01942168e+00 -1.52781914e+00 -8.01634817e-01
  5.19304112e-01  9.15798546e-01  3.34564445e+00  6.39526843e-01
  4.82413635e-01  6.35481164e-01  2.90571310e+00  4.41597949e-01
 -3.22124072e-02  8.46018994e-02 -2.68529515e+00  6.06825560e+00
  8.78759487e-01 -1.01459991e+00 -2.62975803e+00  5.67537389e+00
  1.99381281e+01]
supnorm grad right now is: 19.938128136420907
Weights right now are: 
[-2.44749268  0.08772703 -2.50399157 -0.04509505  1.91602242 -0.22172247
  2.66869861 -0.52096107  2.17575109 -2.52571788  2.61320103  0.72638439
  0.22243176  4.34493297  1.74965566  3.81065357  1.83846046 -2.60572975
  2.04006462 -0.18782222  0.77168407  5.44889394  1.61876589  4.25670879
 -0.11345497  2.86981433 -1.45149081 -2.28758772  1.24485952 -2.49973172
  2.07693302  2.65141736 -2.32473436  2.28417628 -2.09958501 -1.3706124
  2.5101128   2.41656099  2.44733363  3.05815163  0.22831829  3.77543208
 -0.12126198  4.90564322  0.03468942 -0.32928376  4.14463441 -0.99205897
  0.55110189  3.69801217 -0.9204595   4.77268097 -0.47287437  5.34857393
 -0.83794509  5.90833688  4.30097182 -0.19396951 -2.84190128 -2.3129261
  1.16043283  3.04907077 -1.97567871 -2.40820915 26.92002286]
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2093.6643145158278
gradient value of function right now is: [ 2.08013229e-01 -1.06286582e+00  2.90899508e-01 -7.38270969e-01
 -2.08013229e-01  1.06286582e+00 -2.90899508e-01  7.38270969e-01
  9.35689537e-02  2.23026104e-01  1.18737977e-01  1.17724899e-01
  5.21511325e-01 -2.98787827e-03  5.36240253e-01 -6.92195109e-03
  2.19967944e-01  4.35466353e-01  2.69203545e-01  3.17730231e-01
  4.56545545e-01  1.07259134e-03  4.69913538e-01 -3.41005130e-03
 -6.05531715e-02 -1.92577880e-02 -1.26720674e-01 -5.64277624e-02
  2.84486100e-02  3.39533065e-02  8.54844206e-02  8.76133834e-03
 -9.11285381e-02 -4.54329273e-02 -1.69239757e-01 -9.90220436e-02
 -1.70828609e-02 -5.23473522e-03 -2.49601430e-02 -2.23999849e-02
 -4.58452098e-02 -1.14711674e-01 -2.33589311e-01 -7.83574327e-02
  1.24252066e-01 -1.61945689e-01 -4.86848141e-02 -1.32140797e-01
 -1.10247064e-01 -2.96317456e-01 -6.58915524e-01 -1.98446270e-01
 -9.74930750e-02 -2.09400855e-01 -4.33019308e-01 -1.46863583e-01
  2.42412107e-03 -1.10617241e-02  9.20145183e-01 -1.56228440e+00
  3.13872447e-01  5.77537671e-02  7.86477721e-01 -1.46796470e+00
 -7.77940511e+00]
supnorm grad right now is: 7.7794051118930065
Weights right now are: 
[-2.29257146  0.20147441 -2.27675728  0.04917599  1.7611012  -0.33546985
  2.44146432 -0.61523211  2.44130158 -2.34183684  2.85687861  0.9105491
  0.26894751  4.37878167  1.7928955   3.85964527  2.15881793 -2.45064341
  2.33755371 -0.04211441  0.821667    5.48266933  1.66515444  4.31449943
 -0.26223904  2.76418664 -1.58468432 -2.59470155  1.30783645 -2.37194973
  2.13492281  2.80267957 -2.44981337  2.17561757 -2.1978322  -1.66824823
  2.53652018  2.39684652  2.47615352  3.12155493  0.19711789  3.92291828
 -0.16905777  5.05999059  0.06300584 -0.16455325  4.29839632 -0.84236939
  0.54818574  3.83705807 -0.94265492  4.91764525 -0.54707828  5.47168969
 -0.97423693  6.0481212   4.46622212 -0.45809927 -2.94086509 -2.35365849
  1.013982    3.10610763 -2.04910012 -2.44350113 26.24927749]
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2095.2226916252357
gradient value of function right now is: [-0.34957776 -1.31188402 -0.79184629 -0.89162613  0.34957776  1.31188402
  0.79184629  0.89162613 -0.91681891 -0.59424216 -0.9475128  -0.52698631
  0.89252046  0.04322867  0.92219185  0.019908   -1.90252669 -1.15389924
 -1.97094639 -0.97144282  0.72064448  0.0325422   0.74456273  0.0138092
  0.08554419  0.12412863  0.14264998  0.10771821 -0.19038212 -0.13429172
 -0.29869698 -0.26278907  0.03678089  0.12135094  0.07229627  0.03228361
 -0.0339525   0.00365118 -0.04979905 -0.05118473  0.016329    0.07574318
  0.32104714  0.05192459  0.0028422  -0.75697024 -0.73218786 -0.63772906
  0.0322774   0.18596779  0.66986926  0.12954563  0.0252847   0.13163209
  0.66265146  0.08606641 -0.00973962  0.02348404 -0.56796373  1.31903426
  0.72733638 -0.31040181 -0.64837196  1.23065138 -1.56255737]
supnorm grad right now is: 1.9709463906378992
Weights right now are: 
[-2.35595796e+00  3.50035706e-01 -2.34877347e+00  1.93139554e-01
  1.82448769e+00 -4.84031143e-01  2.51348051e+00 -7.59195673e-01
  2.43795765e+00 -2.42501092e+00  2.85088132e+00  8.05988288e-01
  2.14877756e-01  4.39958617e+00  1.74030845e+00  3.91155491e+00
  2.16224898e+00 -2.59665564e+00  2.33811806e+00 -2.26065142e-01
  7.60632201e-01  5.47768108e+00  1.60504925e+00  4.34574196e+00
 -2.72652664e-01  2.75301625e+00 -1.56724791e+00 -2.71537762e+00
  1.33545458e+00 -2.35945594e+00  2.14578528e+00  2.90704620e+00
 -2.37206443e+00  2.17624542e+00 -2.09178401e+00 -1.70050450e+00
  2.67136052e+00  2.39672244e+00  2.61975986e+00  3.29478580e+00
  1.77495632e-01  4.18159030e+00 -1.14729931e-01  5.32642974e+00
 -2.96754463e-01 -1.44534452e-02  4.30363271e+00 -7.07188946e-01
  5.62223823e-01  4.10284407e+00 -8.48115783e-01  5.19066718e+00
 -6.30670673e-01  5.72050207e+00 -9.89925331e-01  6.29952846e+00
  4.49956567e+00 -5.71776858e-01 -3.19035667e+00 -2.23555287e+00
  6.55215196e-01  3.12916040e+00 -2.25636442e+00 -2.33094323e+00
  2.62913397e+01]
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2094.7202497865183
gradient value of function right now is: [-0.89616539 -1.63615373 -2.11190328 -1.08054746  0.89616539  1.63615373
  2.11190328  1.08054746 -2.12956787 -1.47989916 -2.17913761 -1.37561485
  1.32315131  0.15142616  1.3632026   0.08146587 -4.65213945 -3.2206395
 -4.77624366 -2.8490746   1.02164487  0.10619758  1.05248877  0.05274225
  0.24174489  0.16646059  0.42618992  0.30701932 -0.47985133 -0.18550365
 -0.80215248 -0.66426115  0.1486967   0.15325374  0.28509641  0.15724567
 -0.08692666 -0.00627125 -0.13640835 -0.13145346  0.06273014  0.26211167
  1.0153386   0.16459829  0.03564428 -1.04741442 -1.20182011 -0.87622555
  0.13431913  0.64860941  2.27719974  0.40941315  0.14226437  0.52539089
  2.29848541  0.3237469  -0.02835409  0.13120867 -2.39305262  6.13301709
  1.02894556 -0.33162816 -2.15240401  5.16135673  4.54899727]
supnorm grad right now is: 6.133017093345333
Weights right now are: 
[-2.37266666  0.47735967 -2.36279512  0.32309051  1.8411964  -0.61135511
  2.52750216 -0.88914663  2.45529486 -2.41613381  2.86616593  0.8044436
  0.10046218  4.38927268  1.62904143  3.95024028  2.16570554 -2.63018131
  2.33948922 -0.27335132  0.64942829  5.43703701  1.49592715  4.36576054
 -0.23155167  2.68706172 -1.50863889 -2.69310465  1.37498278 -2.29194784
  2.17226182  2.99312517 -2.22287888  2.12001092 -1.9338436  -1.52182743
  2.82302568  2.38451138  2.76937621  3.50678522  0.2376597   4.32533959
 -0.06043049  5.47332593 -0.74086719  0.20990537  4.3353993  -0.47609883
  0.63721888  4.23963768 -0.76791454  5.32813985 -0.58535565  5.90333711
 -0.94020897  6.48021098  4.46983159 -0.67553383 -3.34821462 -2.11164293
  0.15195388  3.11124768 -2.36179241 -2.21760897 26.48127574]
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2098.1355191467223
gradient value of function right now is: [-8.87806742e-01 -1.41985637e+00 -1.72793017e+00 -9.27415291e-01
  8.87806742e-01  1.41985637e+00  1.72793017e+00  9.27415291e-01
 -1.84525677e+00 -1.60090385e+00 -1.88432089e+00 -1.24687394e+00
  1.26517087e+00  2.15954812e-01  1.26893855e+00  1.13007549e-01
 -3.43349016e+00 -2.82185081e+00 -3.50011967e+00 -2.22957982e+00
  9.58942955e-01  1.47868430e-01  9.61104489e-01  7.13033229e-02
  2.08209688e-01  3.21274437e-01  3.77869482e-01  2.53080793e-01
 -4.56050992e-01 -3.29265324e-01 -7.75910625e-01 -6.21074431e-01
  8.84754330e-02  3.43204520e-01  1.83577126e-01  6.71313561e-02
 -7.65727148e-02  5.43118605e-02 -1.19850815e-01 -1.17250604e-01
  5.58847513e-03  2.25546161e-01  9.01190637e-01  1.38331249e-01
 -2.79148166e-01 -2.05720634e+00 -1.89152816e+00 -1.81931194e+00
  1.54061254e-02  5.79861746e-01  2.10714510e+00  3.58378866e-01
  1.05738981e-02  4.89955791e-01  2.19462826e+00  2.96290234e-01
 -2.60647579e-01  7.91727258e-02 -2.36581921e+00  6.15808375e+00
  1.46751503e+00 -2.67361805e-01 -2.18137553e+00  5.16086933e+00
  4.03608389e+00]
supnorm grad right now is: 6.1580837511369815
Weights right now are: 
[-2.41548755e+00  5.97893961e-01 -2.22462445e+00  4.44598227e-01
  1.88401729e+00 -7.31889398e-01  2.38933149e+00 -1.01065435e+00
  2.70290266e+00 -2.35185954e+00  3.07249872e+00  1.17098475e+00
 -2.29372667e-02  4.32141998e+00  1.50907859e+00  3.93880359e+00
  2.43894559e+00 -2.40920846e+00  2.58035071e+00  1.52520884e-01
  5.36120300e-01  5.30570547e+00  1.38461915e+00  4.32050047e+00
 -1.68757880e-01  3.17994050e+00 -1.43518240e+00 -2.62521821e+00
  1.45992654e+00 -2.64705397e+00  2.24938825e+00  3.14378461e+00
 -1.98677088e+00  2.67451568e+00 -1.69932063e+00 -1.18300450e+00
  3.08450563e+00  3.06633648e+00  3.02716627e+00  3.88061635e+00
  3.97846850e-01  4.45114918e+00 -2.93280163e-02  5.61888814e+00
 -2.12031177e+00 -1.88854048e-02  4.54077498e+00 -7.53279313e-01
  8.13392363e-01  4.35826041e+00 -7.08312193e-01  5.45950639e+00
 -3.98310037e-01  6.09204631e+00 -9.23386538e-01  6.71293825e+00
  3.17394899e+00 -2.38007314e+00 -3.44622413e+00 -2.03683566e+00
 -3.47792328e-01  3.54070038e+00 -2.41435118e+00 -2.13135968e+00
  2.65797039e+01]
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2099.1907302376712
gradient value of function right now is: [-0.02997741  2.26226609  0.04774393  1.60100919  0.02997741 -2.26226609
 -0.04774393 -1.60100919  0.17779457 -0.21701586  0.15554179  0.01842856
 -2.76211472 -0.11065055 -2.68149269 -0.06897054  0.52721812 -0.04273508
  0.49801642  0.21979256 -2.37392233 -0.0792766  -2.30534312 -0.04607191
 -0.06923809  0.21461727 -0.11473002 -0.08996765  0.18055825 -0.17783227
  0.29735812  0.24425816  0.13694097  0.24635436  0.17002085  0.19603129
  0.05593485  0.07061502  0.09611216  0.07870126 -0.01661168 -0.06817609
 -0.24115571 -0.05614728 -0.16771475 -1.65191669 -0.86656029 -1.52708995
 -0.04038314 -0.15507412 -0.64761173 -0.13121081 -0.03747252 -0.15236179
 -0.78470069 -0.12621237 -0.28857581 -0.01167233  0.12337938 -1.13629277
  1.77436628 -0.05423318 -0.0740902  -1.11232057 -3.22104799]
supnorm grad right now is: 3.221047985545837
Weights right now are: 
[-2.46183997  0.87393249 -2.18754984  0.72644562  1.93036971 -1.00792792
  2.35225688 -1.29250174  2.93082551 -2.32848719  3.275327    1.3888328
 -0.38907826  4.3092719   1.16203323  3.98166588  2.67442741 -2.31034294
  2.79763985  0.3794285   0.18537263  5.25528334  1.0509064   4.3861724
 -0.22396684  3.59603114 -1.5134131  -2.64940824  1.63902562 -2.97669021
  2.4272589   3.38745103 -1.79606852  3.12431289 -1.57390017 -0.78854549
  3.1280077   3.50254276  2.99612104  4.02286829  0.52298068  4.41832515
 -0.06254264  5.61285503 -2.70873925 -0.39610098  4.68786738 -1.19950084
  0.94170299  4.25149417 -0.78821408  5.3787691  -0.24455477  6.12598791
 -0.98254205  6.78815774  2.88894361 -3.41598637 -3.34474993 -2.11530643
 -0.04793004  3.97829861 -2.21125046 -2.27551271 26.79962212]
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -2088.1661248782966
gradient value of function right now is: [ 1.51053881e+00 -1.01749339e+00  2.50119740e+00 -7.13614325e-01
 -1.51053881e+00  1.01749339e+00 -2.50119740e+00  7.13614325e-01
  1.71988276e+00  2.65411628e+00  1.77927362e+00  1.71010984e+00
  9.90567654e-01 -1.06944092e-01  1.01158057e+00 -9.16381629e-02
  2.98169836e+00  4.29508256e+00  3.07383362e+00  2.95935082e+00
  7.65795775e-01 -2.70330243e-02  7.82103192e-01 -2.80175131e-02
 -4.94375218e-02 -9.43542803e-01 -1.11066015e-01 -3.83216992e-02
 -9.36063889e-02  5.67094152e-01 -1.32463799e-01 -1.45415713e-01
 -1.22679282e-01 -1.03104633e+00 -2.30005844e-01 -1.37814891e-01
 -6.28680963e-02 -3.83436604e-01 -1.13318019e-01 -7.49263910e-02
 -1.11608287e-02 -3.23375473e-02  1.29905200e-02 -3.42912345e-02
  6.61697602e-01  6.44586376e+00  4.53764599e+00  5.61018912e+00
 -3.70746722e-02 -1.65848363e-01 -1.42926658e-01 -1.41675113e-01
 -1.41906990e-02  3.77415045e-03  2.42163824e-01 -2.47345430e-02
  9.18174448e-01 -1.83261054e-01 -4.31999966e-02  4.54839273e-01
 -6.87623966e+00  6.19960579e-01  9.77073417e-01 -2.06526334e-01
 -1.61879793e+01]
supnorm grad right now is: 16.18797929253573
Weights right now are: 
[-2.14590668  0.52125415 -1.75051065  0.35750561  1.61443642 -0.65524959
  1.91521769 -0.92356173  3.47603443 -1.81384483  3.7908015   2.0372483
  0.14294752  4.16773598  1.67216941  3.6529332   3.21428453 -1.73042356
  3.31385406  1.03604691  0.74601498  5.2338525   1.58954854  4.21224681
 -0.25234304  3.47808724 -1.59475132 -2.59974463  1.61192782 -2.88501444
  2.42617339  3.31513571 -2.20054348  3.02808984 -1.96582973 -1.16240124
  2.77547228  3.42093321  2.5892556   3.64585888  0.5954071   4.34115405
 -0.06708176  5.53573763 -2.91598701 -0.19681048  5.17783142 -1.07028819
  1.01221708  4.05318524 -0.85516634  5.17897381 -0.15338001  6.14845518
 -0.86121389  6.81565681  3.00056388 -3.78557753 -3.30516946 -2.06433031
 -0.09232312  4.37658944 -1.92523426 -2.37005484 26.71083828]
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -2096.567707678497
gradient value of function right now is: [ 0.04350632 -0.94028509  0.13441453 -0.59634449 -0.04350632  0.94028509
 -0.13441453  0.59634449  0.23916991  0.04928199  0.24001107  0.15674625
  1.09477872 -0.03877186  1.08821225 -0.04047075  0.45657348  0.24632011
  0.46337954  0.32673598  0.78488106 -0.01476923  0.78085748 -0.01832538
 -0.06392133  0.07450565 -0.14312155 -0.05802647 -0.01557214 -0.05922819
  0.02258813 -0.05215645 -0.16047454  0.08643691 -0.29205454 -0.19581306
 -0.02215645  0.03446428 -0.03726833 -0.02717265 -0.01457609 -0.09406343
 -0.23371063 -0.05755202 -0.08772808 -0.66090558 -0.28442542 -0.60873193
 -0.03685399 -0.29557922 -0.64549725 -0.17297264 -0.02829671 -0.12097687
 -0.20441648 -0.08403921 -0.19229507  0.026485    0.52591509 -1.00535379
  0.66582952 -0.18022188  0.37999623 -1.05849483  6.10347985]
supnorm grad right now is: 6.103479845208917
Weights right now are: 
[-2.62958169e+00  6.97499055e-01 -2.20070648e+00  5.13498880e-01
  2.09811143e+00 -8.31494491e-01  2.36541352e+00 -1.07955500e+00
  3.17189170e+00 -2.35004016e+00  3.49304734e+00  1.60488741e+00
  1.27479547e-02  4.19435780e+00  1.53122513e+00  3.72772645e+00
  2.95868201e+00 -2.20909908e+00  3.06239531e+00  6.57213051e-01
  6.61977692e-01  5.14365518e+00  1.49496631e+00  4.15646528e+00
 -2.68033354e-01  4.04183056e+00 -1.62031220e+00 -2.61699510e+00
  1.81028461e+00 -3.26732823e+00  2.64532805e+00  3.56399242e+00
 -2.09773669e+00  3.58213931e+00 -1.85287341e+00 -1.02754828e+00
  3.16303563e+00  4.11391252e+00  3.00824684e+00  4.06781261e+00
  6.38289562e-01  4.24847414e+00 -2.55298031e-01  5.48974072e+00
 -3.72116690e+00 -7.79619058e-01  4.49914256e+00 -1.61901621e+00
  1.07010025e+00  3.98198337e+00 -1.02073389e+00  5.15685670e+00
 -1.21139483e-01  5.98255525e+00 -1.10509044e+00  6.70439932e+00
  2.22160130e+00 -3.42360062e+00 -3.19693955e+00 -2.26139257e+00
  3.01967313e-01  4.10885738e+00 -1.98541715e+00 -2.52226290e+00
  2.71404048e+01]
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2095.4009443728096
gradient value of function right now is: [-1.22297962e-01 -8.77344449e-01 -1.47215035e-01 -5.72893318e-01
  1.22297962e-01  8.77344449e-01  1.47215035e-01  5.72893318e-01
 -1.16729922e-01 -3.00889900e-01 -1.25334424e-01 -1.26088218e-01
  9.90175565e-01 -2.14918928e-02  9.77256413e-01 -1.60624114e-02
 -1.10097331e-01 -2.92510552e-01 -1.17568480e-01 -1.14385760e-01
  7.39872996e-01 -1.35958391e-02  7.30512885e-01 -1.04186776e-02
 -4.55868316e-02  1.19496765e-01 -8.99416136e-02 -5.91316130e-02
  7.07692159e-03 -1.11369625e-01  4.37363711e-02  3.79187113e-04
 -1.30881625e-01  1.32337934e-01 -2.12745671e-01 -1.85904718e-01
  7.14044679e-04  4.06236408e-02  6.74239689e-03 -1.18415170e-03
 -1.19075234e-02 -5.87987086e-02 -2.39028586e-01 -2.75427824e-02
 -1.42635464e-01 -8.44339034e-01 -1.06506208e+00 -6.44150876e-01
 -2.08403911e-02 -1.32828399e-01 -4.86617611e-01 -5.44899292e-02
 -2.87929693e-02 -1.19112393e-01 -3.67457938e-01 -6.17778885e-02
 -2.69192373e-01  2.41155859e-01  5.57161306e-01 -1.32517176e+00
  3.91815764e-01 -3.75076090e-01  1.24994392e-01 -7.55633068e-01
  1.78595656e+00]
supnorm grad right now is: 1.7859565626692073
Weights right now are: 
[-2.84192600e+00  6.18597065e-01 -2.41310207e+00  4.55868379e-01
  2.31045573e+00 -7.52592502e-01  2.57780910e+00 -1.02192450e+00
  2.93529394e+00 -2.63210529e+00  3.26326434e+00  1.34128867e+00
  1.90067052e-01  4.33883138e+00  1.70070098e+00  3.84984321e+00
  2.77491737e+00 -2.47066907e+00  2.88264485e+00  4.37924543e-01
  8.01246336e-01  5.23811922e+00  1.62731488e+00  4.23935851e+00
 -1.94322375e-01  4.33317133e+00 -1.53907528e+00 -2.54456409e+00
  1.81919375e+00 -3.51087753e+00  2.64504102e+00  3.59835356e+00
 -2.02202144e+00  3.85233575e+00 -1.74566950e+00 -9.44969429e-01
  3.26084075e+00  4.36589745e+00  3.09740016e+00  4.17782807e+00
  7.75900664e-01  4.35856605e+00 -1.93027413e-01  5.69938776e+00
 -4.19835986e+00 -1.03789940e+00  4.00783508e+00 -1.85450780e+00
  1.24660819e+00  4.08081260e+00 -9.47762071e-01  5.37640249e+00
 -2.76323067e-03  6.07828416e+00 -1.06692147e+00  6.89879616e+00
  1.92591978e+00 -2.32200380e+00 -3.16682704e+00 -2.27150420e+00
  4.09091261e-01  3.67096531e+00 -2.05525223e+00 -2.45136250e+00
  2.66393358e+01]
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2084.2676169484075
gradient value of function right now is: [ 4.70174462e-01  5.15469348e+00  7.54302192e-01  3.44940268e+00
 -4.70174462e-01 -5.15469348e+00 -7.54302192e-01 -3.44940268e+00
  1.44745688e+00  5.44754406e-01  1.28445254e+00  4.03952446e-01
 -6.99554814e+00 -5.42086879e-01 -5.87519433e+00 -2.74720405e-01
  2.20243944e+00  8.77411461e-01  1.95989870e+00  6.32928542e-01
 -5.54938709e+00 -3.79729141e-01 -4.65331797e+00 -1.85756193e-01
 -3.17212485e-01 -1.14007844e-01 -6.12693924e-01 -4.70386128e-01
  7.30165688e-01  1.96860949e-01  1.35307612e+00  1.13205328e+00
  6.02365709e-01 -2.79261154e-02  8.02584434e-01  9.60989960e-01
  1.84541153e-01  4.41340184e-02  3.36200786e-01  2.95428412e-01
 -9.16949120e-02 -4.02801272e-01 -1.31688628e+00 -2.28736059e-01
  2.96737214e-02  3.26787651e-01  1.18309115e+00  1.15096610e-01
 -2.23599191e-01 -1.04064221e+00 -3.72968945e+00 -5.86290701e-01
 -2.85025912e-01 -1.11569669e+00 -4.75350134e+00 -6.42365951e-01
  1.25392890e-01 -5.83672605e-01  5.17526002e+00 -1.11408834e+01
  1.34819195e-02  2.64466586e-01  3.95671726e+00 -8.87232752e+00
 -1.63891593e+01]
supnorm grad right now is: 16.389159261261952
Weights right now are: 
[-2.76777309  0.97790473 -2.37407608  0.82134768  2.23630283 -1.11190016
  2.53878312 -1.3874038   2.98400095 -2.58303159  3.30417355  1.32808224
 -0.16947598  4.27005164  1.35835781  3.8486639   2.80251951 -2.4771504
  2.90299507  0.40086133  0.43640086  5.12624327  1.27986313  4.21183911
 -0.13867917  4.2746636  -1.43727768 -2.5479745   1.95995031 -3.42434417
  2.74218862  3.83778444 -1.41772545  3.79448019 -1.13976576 -0.22356064
  3.52211848  4.28395604  3.35650973  4.4697422   0.92488165  4.54586068
 -0.06700562  5.95519622 -4.29871355 -0.86513023  4.09215468 -1.71613948
  1.42390573  4.28364007 -0.81231103  5.65446156  0.12663988  6.20860938
 -1.11386944  7.12695815  2.11874104 -1.94569778 -3.12096402 -2.29274449
  0.3140855   3.74318643 -1.9386155  -2.40948637 26.12027864]
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2089.75719434998
gradient value of function right now is: [-0.92207749  0.89663913 -1.67073415  0.7589242   0.92207749 -0.89663913
  1.67073415 -0.7589242  -1.87646585 -1.80181087 -1.82398681 -1.31520629
 -1.35771814  0.44645373 -1.00322799  0.39043991 -3.28351725 -2.92542349
 -3.17413339 -2.2501073  -1.41905599  0.24594763 -1.09435668  0.21650537
  0.2748727   0.44826024  0.68953409  0.39167183 -0.36507814 -0.41715667
 -0.98851639 -0.54776959  0.63283825  0.48852146  1.23487357  1.00494668
 -0.10901297  0.07901646 -0.30921168 -0.19040998  0.05413081  0.31020534
  1.58966886  0.16654174 -0.50348778 -2.74478847 -3.77666375 -2.10692526
  0.13723228  0.92784419  4.30404302  0.4894939   0.1421218   0.77215971
  3.65789081  0.41812125 -0.86943145  1.26390981 -3.61849985 13.55957852
  1.31315681 -0.84787726 -2.99790524 10.28768744 10.12948486]
supnorm grad right now is: 13.559578520135313
Weights right now are: 
[-2.63101011  1.06013253 -2.27724063  0.90246593  2.09953984 -1.19412797
  2.44194767 -1.46852205  3.16258769 -2.40039404  3.49722423  1.44309903
 -0.17792256  4.92220318  1.26048247  4.4954486   2.90337321 -2.3714546
  3.01803843  0.44309081  0.4425513   5.91164021  1.19383935  5.03515011
 -0.08480303  4.08854729 -1.36463576 -2.37152666  1.77252872 -3.27489737
  2.45581876  3.51443292 -1.21064317  3.59195814 -1.07019393  0.04584605
  2.68060453  4.03648264  2.32536138  3.43851818  1.22147061  4.94942509
  0.60289154  6.38131069 -4.26254447 -0.59535644  4.41882748 -1.48681822
  1.70767601  4.5616665  -0.31004958  5.96666392  0.56701419  6.83136321
 -0.38877593  7.80892629  2.42254914 -1.87386315 -3.45454091 -1.85962059
  0.13667716  3.96656873 -2.06147075 -1.99165513 26.41879646]
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2091.367800643855
gradient value of function right now is: [  1.36070849   2.53411875   2.73458861   1.41442477  -1.36070849
  -2.53411875  -2.73458861  -1.41442477   2.16868264   2.55320425
   2.18658128   1.91161229  -3.71117557  -0.62183873  -3.36261034
  -0.5115871    4.34706614   4.957042     4.37559666   3.8151659
  -2.40778169  -0.29762372  -2.17206685  -0.24374337  -0.09500355
  -0.82166646  -0.26259739  -0.10707719   0.37140106   0.58772985
   0.84461784   0.54135103   0.19070489  -0.85144855   0.26286091
   0.33269586   0.19499153  -0.2023932    0.41797602   0.30434081
  -0.0743634   -0.23239132  -0.98612388  -0.15719232   0.52183168
   5.96082269   4.98027915   4.98436921  -0.20271124  -0.72708148
  -2.71527724  -0.48827604  -0.18018327  -0.50481228  -2.59396876
  -0.34547073   1.29295704  -0.77415949   2.41977354  -5.22396083
  -5.90911021   0.63437168   3.0190586   -4.95641231 -22.10011981]
supnorm grad right now is: 22.10011980720546
Weights right now are: 
[-2.2970037   1.07822187 -1.93846301  0.89290344  1.76553344 -1.21221731
  2.10317005 -1.45895956  3.55074542 -1.99469922  3.87314063  1.78273866
 -0.17419723  4.66492083  1.22896222  4.06438928  3.2620472  -2.01765987
  3.36101463  0.74290109  0.49548025  5.69800284  1.2083863   4.61187556
 -0.36963925  3.67810633 -1.78633274 -2.69422964  2.06098183 -2.90257792
  2.86646157  3.86048884 -1.5963347   3.17801756 -1.53894015 -0.34603689
  3.07999782  3.65816548  2.79134752  3.91874278  1.36408009  4.73496165
  0.15504181  6.31329319 -4.17309272 -0.09240696  4.855136   -0.99211875
  1.87822401  4.26071416 -0.78851646  5.8200964   0.74725723  6.56607101
 -0.82282535  7.72626884  3.17709284 -1.96188896 -2.98504571 -2.59138774
 -0.43913495  4.26612339 -1.58791085 -2.52226544 26.80708038]
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2089.6701116984837
gradient value of function right now is: [-6.58121520e-01 -1.74529160e-01 -1.45097404e+00 -1.05063240e-01
  6.58121520e-01  1.74529160e-01  1.45097404e+00  1.05063240e-01
 -1.46424960e+00 -1.34741679e+00 -1.44611727e+00 -9.99446055e-01
  2.00727495e-02  1.58802208e-01 -6.46071548e-02  1.16568546e-01
 -3.00669560e+00 -2.56009380e+00 -2.95090077e+00 -1.95356392e+00
 -1.15316859e-02  1.00905039e-01 -7.17597279e-02  7.33078952e-02
  1.77852990e-01  3.24352178e-01  4.18520556e-01  2.42908588e-01
 -3.63979456e-01 -3.65822590e-01 -7.83470659e-01 -5.63061173e-01
  2.02407142e-01  3.38314258e-01  3.87923397e-01  2.39894946e-01
 -1.24272377e-01  1.40635581e-02 -2.35265457e-01 -2.15335707e-01
  5.38463383e-02  1.84700072e-01  1.17582138e+00  1.42915561e-01
 -2.92663238e-01 -1.77492614e+00 -3.30276682e+00 -1.35640198e+00
  1.50170492e-01  6.04362958e-01  2.85880789e+00  4.55232762e-01
  1.37084451e-01  4.06651733e-01  2.62881201e+00  3.22582626e-01
 -6.34606797e-01  1.20871482e+00 -2.41276952e+00  5.67264081e+00
  5.68547332e-01 -6.13786954e-01 -2.90747902e+00  6.50359277e+00
  1.99050233e+01]
supnorm grad right now is: 19.905023310014364
Weights right now are: 
[-2.86207910e+00  9.53918913e-01 -2.67079188e+00  7.93639134e-01
  2.33060884e+00 -1.08791435e+00  2.83549892e+00 -1.35969525e+00
  2.94147894e+00 -2.62312884e+00  3.27528646e+00  1.06562337e+00
 -2.57624241e-02  5.33269167e+00  1.37139464e+00  4.78711216e+00
  2.56076258e+00 -2.82987389e+00  2.66956085e+00 -1.00173237e-01
  6.22695722e-01  6.16663678e+00  1.32752710e+00  5.14058190e+00
 -2.79248175e-01  4.26626023e+00 -1.65081864e+00 -2.62011355e+00
  1.99337119e+00 -3.37477166e+00  2.75499087e+00  3.81103444e+00
 -1.66135525e+00  3.72893928e+00 -1.54846863e+00 -4.28600833e-01
  3.01547567e+00  4.12690080e+00  2.69804298e+00  3.87162960e+00
  1.52471082e+00  4.95032174e+00  2.97668495e-01  6.58447078e+00
 -4.66424959e+00 -7.10392008e-01  4.26367169e+00 -1.60662358e+00
  2.09728145e+00  4.52586864e+00 -5.75208739e-01  6.14562614e+00
  8.96472972e-01  6.75665937e+00 -6.92534272e-01  7.98429401e+00
  2.71834219e+00 -1.72934055e+00 -3.16817404e+00 -2.50008960e+00
  1.86084284e-01  4.14374696e+00 -1.97452645e+00 -2.32764400e+00
  2.70635602e+01]
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2094.4539983507125
gradient value of function right now is: [-0.02232704  0.02862785 -0.03109602  0.01387632  0.02232704 -0.02862785
  0.03109602 -0.01387632  0.08470516 -0.16831652  0.06547687 -0.11040893
  0.0608383  -0.07799354  0.07059997 -0.05771145  0.36305771 -0.22153577
  0.31865497 -0.1452943   0.05844739 -0.0514701   0.06452219 -0.0379743
 -0.07902157  0.05032754 -0.15018594 -0.11400576  0.15827344 -0.04580496
  0.30154582  0.23292642 -0.09213175  0.05851678 -0.15372082 -0.13400783
  0.06502685  0.01285063  0.12220736  0.09648    -0.01526497 -0.12000095
 -0.64462983 -0.07647409 -0.04431445 -0.39346883 -0.45370058 -0.31904698
 -0.02853636 -0.24549274 -1.26391139 -0.15062767 -0.03716458 -0.27183048
 -1.38245744 -0.17692473 -0.07018666 -0.01031387  1.58741196 -3.59759357
  0.17317448 -0.12042405  1.16180988 -2.97367401 -8.64935849]
supnorm grad right now is: 8.649358485772858
Weights right now are: 
[-2.86693285  0.81144425 -2.72228122  0.65985452  2.33546258 -0.94543969
  2.88698826 -1.22591064  2.92761804 -2.65047054  3.25922421  0.95872691
  0.19251471  5.27593304  1.5933589   4.75273889  2.50338914 -2.94448804
  2.60744991 -0.2858691   0.83509245  6.05653873  1.54316933  5.04303923
 -0.46497171  4.22013386 -1.80249292 -2.91081642  2.20881149 -3.2407317
  2.96497754  4.10444846 -1.99656871  3.67850321 -1.80943176 -0.77383542
  3.28559869  4.20326563  2.96701228  4.16954415  1.25840434  4.90414104
 -0.08410631  6.41779778 -4.3214445  -0.76157613  4.36899747 -1.63289576
  1.78386308  4.55126722 -0.79408203  6.0464851   0.58373327  6.64081981
 -1.04095284  7.7233017   2.92328466 -2.5814805  -3.30898764 -2.5803765
  0.26848056  4.37385991 -2.13268348 -2.49159065 26.12046279]
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2089.0357000859035
gradient value of function right now is: [-5.68073319e-01 -1.04580368e+00 -1.60414344e+00 -6.47251046e-01
  5.68073319e-01  1.04580368e+00  1.60414344e+00  6.47251046e-01
 -1.48545981e+00 -1.05338789e+00 -1.46406912e+00 -8.74211614e-01
  1.23575448e+00  1.04992843e-01  1.13442144e+00  7.74386360e-02
 -3.87991697e+00 -2.45683831e+00 -3.80210843e+00 -2.07146386e+00
  8.79541287e-01  6.86880088e-02  8.07439531e-01  5.00647317e-02
  1.98934094e-01  1.72039131e-01  4.02472764e-01  2.71015417e-01
 -3.52997361e-01 -2.12803522e-01 -6.71182732e-01 -5.11254214e-01
  1.17144900e-01  1.78870692e-01  2.61585589e-01  1.25760040e-01
 -1.06489893e-01 -1.25814095e-02 -1.84765163e-01 -1.67082757e-01
  7.88614173e-03  2.78886609e-01  1.19765485e+00  1.77879554e-01
 -5.20864691e-02 -9.99515333e-01 -1.75676535e+00 -7.32794251e-01
  2.11304707e-02  7.28415175e-01  2.77526231e+00  4.65662144e-01
  1.64895237e-02  5.80527570e-01  2.68301745e+00  3.71163885e-01
 -9.09395206e-02  1.54849083e-01 -3.24173476e+00  8.87438078e+00
  1.91329486e-01 -1.98711272e-01 -3.00859717e+00  7.92650429e+00
  9.45557559e+00]
supnorm grad right now is: 9.455575594353274
Weights right now are: 
[-2.91778328  0.75103713 -2.85079585  0.6072881   2.38631301 -0.88503256
  3.01550289 -1.17334422  2.79827437 -2.69567953  3.13716863  0.83566743
  0.28851173  5.34264083  1.69451347  4.83454536  2.2795941  -3.0966947
  2.39057072 -0.51479348  0.91994983  6.11802464  1.63262109  5.12310627
 -0.35714269  4.14704828 -1.66288848 -2.82414834  2.14437359 -3.12832019
  2.87206352  4.06947852 -1.97481372  3.6040359  -1.73759186 -0.76174371
  3.25422434  4.18810552  2.92717314  4.15977894  1.12243655  5.2159812
  0.08547869  6.57817768 -4.01293806 -0.73386621  4.45210774 -1.57097479
  1.62824197  4.86635887 -0.59069532  6.21885451  0.444785    6.90254028
 -0.91494805  7.83631984  3.06480913 -3.03024478 -3.90930494 -2.1487231
  0.24917471  4.58610911 -2.66916258 -2.12768446 26.32025497]
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2089.81886028462
gradient value of function right now is: [ 2.30702304e-01  1.75360623e-01  6.13367185e-01  1.01012114e-01
 -2.30702304e-01 -1.75360623e-01 -6.13367185e-01 -1.01012114e-01
  6.02877799e-01  3.42703770e-01  5.88595234e-01  3.13142562e-01
 -1.24238306e-01 -8.17641973e-02 -8.90969668e-02 -7.21160087e-02
  1.63218754e+00  9.48049669e-01  1.59571596e+00  8.61288752e-01
 -7.54941519e-02 -5.24362499e-02 -5.12960293e-02 -4.62354054e-02
 -1.05456765e-01 -4.56487998e-02 -2.30282557e-01 -1.23879047e-01
  1.81224314e-01  5.33051941e-02  3.68710259e-01  2.37610080e-01
 -8.57542814e-02 -4.41989575e-02 -1.87007995e-01 -8.93255406e-02
  6.61632852e-02  8.60712537e-03  1.22743132e-01  9.68218540e-02
 -2.75954707e-02 -2.46412243e-01 -8.07019084e-01 -1.64514685e-01
  1.76516394e-02  1.34477257e-01  4.80123960e-01  6.41395809e-02
 -6.34620316e-02 -6.34023969e-01 -1.90145118e+00 -4.18293854e-01
 -5.65528477e-02 -4.53979228e-01 -1.56409303e+00 -3.06700547e-01
  3.37354188e-02 -1.39964297e-01  2.49612856e+00 -4.25655039e+00
  4.95819393e-02  1.14897403e-01  2.33240693e+00 -4.05767515e+00
 -1.48450280e+01]
supnorm grad right now is: 14.84502803570668
Weights right now are: 
[-2.68004194  0.74248973 -2.48656039  0.60347122  2.14857168 -0.87648517
  2.65126743 -1.16952734  3.16642859 -2.47727111  3.49299399  1.05287828
  0.35138583  5.33333918  1.76327117  4.83999322  2.78811917 -2.80126207
  2.88441982 -0.22042394  0.97206614  6.10940218  1.69026215  5.13279379
 -0.84098145  4.00379654 -2.16511511 -3.38471975  2.47358782 -2.92583324
  3.23413148  4.43388026 -2.33047933  3.46076104 -2.14549099 -1.07195598
  3.32825337  4.18850591  2.99422341  4.257243    1.21941221  4.81330977
 -0.38101434  6.24675404 -3.83909274 -0.63640059  4.66119407 -1.46982392
  1.71266399  4.45277313 -1.07754374  5.86460349  0.55353875  6.427788
 -1.3775178   7.43928078  3.11464667 -3.13250688 -3.39430364 -2.85140342
  0.2244622   4.66583917 -2.21043535 -2.79339289 26.09158238]
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2095.938046382296
gradient value of function right now is: [-2.46375893e-01 -8.50379054e-01 -6.38170857e-01 -5.30476821e-01
  2.46375893e-01  8.50379054e-01  6.38170857e-01  5.30476821e-01
 -5.14815447e-01 -5.51975289e-01 -5.24177274e-01 -4.10153226e-01
  9.71809643e-01  4.08012219e-03  9.40696433e-01  3.57457823e-03
 -1.16128139e+00 -1.13328251e+00 -1.17687291e+00 -8.63236987e-01
  7.12128844e-01  2.79944275e-03  6.89295636e-01  2.51339910e-03
 -4.13460547e-03  1.59563531e-01 -6.26582542e-04 -1.35363634e-02
 -2.74024460e-02 -1.58602759e-01 -4.58328671e-02 -3.69018604e-02
 -5.40068167e-02  1.70449683e-01 -7.58833333e-02 -9.44735577e-02
 -1.86549490e-02  1.07357896e-02 -2.56477396e-02 -2.99175764e-02
 -1.69843995e-03  8.61494802e-03 -9.49262662e-03  7.71991718e-03
 -9.34203274e-02 -1.10316759e+00 -1.31023141e+00 -9.14390106e-01
 -2.38852311e-04  4.72821391e-02  4.46441403e-02  4.01776468e-02
 -6.06070974e-03  6.52276783e-03  1.47198987e-02  6.70868826e-03
 -1.70614149e-01  1.17374119e-01 -8.87977284e-02 -1.44427463e-02
  5.79222017e-01 -3.63710735e-01 -3.61969879e-01  2.24876077e-01
  1.32684207e+00]
supnorm grad right now is: 1.3268420654798943
Weights right now are: 
[-2.72699650e+00  5.90708770e-01 -2.54462418e+00  4.61608359e-01
  2.19552623e+00 -7.24704207e-01  2.70933121e+00 -1.02766448e+00
  3.06877321e+00 -2.47373067e+00  3.40331299e+00  1.04968581e+00
  5.25974118e-01  5.55359742e+00  1.93995668e+00  5.08907474e+00
  2.65560565e+00 -2.83106866e+00  2.75899185e+00 -2.53887522e-01
  1.13224770e+00  6.34490941e+00  1.85264690e+00  5.41166177e+00
 -6.35826954e-01  4.02768935e+00 -1.94890674e+00 -3.19100574e+00
  2.23855909e+00 -2.94607970e+00  2.99190300e+00  4.19999017e+00
 -2.28007395e+00  3.47543790e+00 -2.06213588e+00 -1.05072779e+00
  3.03357421e+00  4.16262871e+00  2.71770086e+00  3.97477434e+00
  1.59060670e+00  5.21495025e+00 -1.65138750e-02  6.74146482e+00
 -3.88429807e+00 -6.32855370e-01  4.63875885e+00 -1.47307066e+00
  2.07580771e+00  4.80835704e+00 -7.50411192e-01  6.30182421e+00
  9.22841935e-01  6.79769643e+00 -1.05551596e+00  7.90743094e+00
  3.08645659e+00 -2.92165165e+00 -3.62441066e+00 -2.68754757e+00
  2.48181002e-01  4.53919748e+00 -2.45081338e+00 -2.58681137e+00
  2.65184096e+01]
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2098.1625008877118
gradient value of function right now is: [-3.00607830e-04 -9.81352670e-01  7.52325885e-02 -6.19878167e-01
  3.00607830e-04  9.81352670e-01 -7.52325885e-02  6.19878167e-01
  8.28470508e-03 -2.10180181e-02  2.97210034e-03  7.54720757e-02
  1.22517593e+00  1.04894678e-02  1.17572114e+00  1.07254195e-02
  1.73288206e-01  2.41345788e-01  1.71769981e-01  3.54097077e-01
  9.01378484e-01  8.11287586e-03  8.65022000e-01  8.30696984e-03
 -1.86233182e-02  8.72346017e-02 -3.67787850e-02 -2.79690033e-02
 -3.88426860e-02 -7.77263940e-02 -5.71776762e-02 -6.53335762e-02
 -9.36003165e-02  9.73353299e-02 -1.57734803e-01 -1.51038545e-01
 -3.89967675e-02  1.61464587e-02 -6.38220342e-02 -6.31612518e-02
 -6.32413739e-03 -8.39204352e-03 -1.92066169e-02 -8.99566004e-03
 -6.89677658e-02 -6.47537485e-01 -3.02326147e-01 -6.11886208e-01
 -1.69884479e-02 -3.56701213e-02 -7.26223617e-02 -3.11612404e-02
 -1.38466142e-02 -5.04591766e-03  1.01685182e-01 -1.14299156e-02
 -1.43539114e-01 -1.64776660e-02 -4.22848895e-02  6.17100872e-02
  7.16929222e-01 -5.57782408e-02 -6.89120179e-02 -5.44500003e-02
 -1.83446062e+00]
supnorm grad right now is: 1.8344606235922953
Weights right now are: 
[-2.53316348e+00  6.36660710e-01 -2.29440895e+00  5.09876848e-01
  2.00169322e+00 -7.70656146e-01  2.45911599e+00 -1.07593297e+00
  3.25862452e+00 -2.21783498e+00  3.59777179e+00  1.29489374e+00
  4.98754181e-01  5.63865787e+00  1.91189739e+00  5.18716045e+00
  2.85103890e+00 -2.54861889e+00  2.96003695e+00  1.67313044e-02
  1.09920651e+00  6.44153767e+00  1.81852472e+00  5.52741028e+00
 -6.26097742e-01  3.79545637e+00 -1.96392606e+00 -3.14616070e+00
  2.22662818e+00 -2.68738153e+00  2.99000677e+00  4.17014503e+00
 -2.23937386e+00  3.24321037e+00 -2.04664586e+00 -9.79705861e-01
  2.96379152e+00  4.07298006e+00  2.62833870e+00  3.90750062e+00
  1.57498883e+00  5.22099652e+00  4.94101816e-02  6.70414333e+00
 -3.76726098e+00 -4.13847855e-01  4.89846961e+00 -1.24974633e+00
  2.01210440e+00  4.75791964e+00 -7.46363827e-01  6.19234304e+00
  9.12420640e-01  6.81487282e+00 -9.85815808e-01  7.88280076e+00
  3.32702088e+00 -3.09771952e+00 -3.68479039e+00 -2.63713453e+00
  5.85584094e-02  4.78915651e+00 -2.44405697e+00 -2.57762386e+00
  2.66609736e+01]
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2096.944197154864
gradient value of function right now is: [-7.54504534e-01 -6.36364379e-01 -2.74790981e+00 -4.01413221e-01
  7.54504534e-01  6.36364379e-01  2.74790981e+00  4.01413221e-01
 -1.64425580e+00 -1.57455643e+00 -1.63678869e+00 -1.35649070e+00
  8.88018011e-01  1.27522051e-01  7.40591656e-01  1.24551113e-01
 -5.33744017e+00 -4.82940325e+00 -5.28552508e+00 -4.31507977e+00
  6.36129828e-01  8.48595052e-02  5.27026590e-01  8.32427972e-02
  9.79600566e-02  3.13717320e-01  2.47338384e-01  1.42770435e-01
 -2.73807161e-01 -3.21563569e-01 -5.84855625e-01 -4.65840953e-01
  3.77635053e-02  3.34450199e-01  9.97803997e-02  2.98613238e-03
 -1.65507985e-01 -3.16305230e-03 -3.19628991e-01 -3.00511801e-01
  2.08571303e-03  1.78908141e-01  9.76355709e-01  1.14132232e-01
 -1.89508063e-01 -2.08708091e+00 -1.89579314e+00 -1.84158900e+00
  7.11311212e-03  5.49242555e-01  2.27596212e+00  3.48955989e-01
  4.64919643e-03  4.26340253e-01  2.41852776e+00  2.73641013e-01
 -2.86353158e-01  1.07132413e-01 -2.52876489e+00  6.22771120e+00
  1.57388870e+00 -1.99302674e-01 -2.46764579e+00  5.61855309e+00
  1.14926313e+01]
supnorm grad right now is: 11.492631261860627
Weights right now are: 
[-2.63417967  0.96486873 -2.53389142  0.83658135  2.10270941 -1.09886416
  2.69859846 -1.40263747  3.1062751  -2.2944371   3.45504877  1.08149911
  0.17051589  5.74117785  1.57092178  5.31229186  2.56283703 -2.83495647
  2.67692106 -0.39384719  0.76814158  6.54976953  1.47406652  5.66274409
 -0.49050648  3.59184869 -1.80801332 -3.00739124  2.16969343 -2.4672646
  2.88957496  4.10796165 -1.93650052  3.03487601 -1.75595857 -0.67323742
  2.96498374  3.95076262  2.58043687  3.87960911  1.63690401  5.41565384
  0.27095521  6.91700706 -3.71178682 -0.12786025  4.99691915 -0.93239408
  2.11508678  4.97071736 -0.54335594  6.4389873   0.96128019  6.9990543
 -0.82020248  8.08022566  3.56788356 -3.01801996 -3.83912423 -2.47854298
 -0.33368259  4.86590463 -2.5644395  -2.4127364  26.85498811]
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2100.023115808187
gradient value of function right now is: [ 0.29770386 -1.11409059  0.8628089  -0.70530079 -0.29770386  1.11409059
 -0.8628089   0.70530079  0.41845184  0.60551124  0.43004946  0.52825368
  1.63124373  0.06487631  1.52149178  0.06022558  1.21090533  1.72224017
  1.24753296  1.49794112  1.17580868  0.04690036  1.09684612  0.04325858
 -0.00301943 -0.12398346  0.00694265 -0.00982237 -0.158528    0.0902959
 -0.29482441 -0.26580833 -0.13157347 -0.13275409 -0.22063858 -0.23408406
 -0.1125303  -0.02032055 -0.20348711 -0.19066863 -0.00492126  0.05190712
  0.33348029  0.02568816  0.05041879  0.89595791  0.9592682   0.78099953
 -0.01630867  0.12151115  0.68237019  0.05511952 -0.00818943  0.14676944
  1.05031644  0.08023936  0.06055217 -0.10522672 -0.96701081  2.38091303
 -0.73447348  0.2386458  -0.65525549  1.80992681 -2.6969421 ]
supnorm grad right now is: 2.6969420966339834
Weights right now are: 
[-2.41504341  0.89382268 -2.0661475   0.76021734  1.88357314 -1.02781812
  2.23085454 -1.32627346  3.42803887 -1.98566594  3.77463023  1.49268906
  0.28952751  5.72339947  1.69153336  5.27699233  2.99734039 -2.32535488
  3.11522576  0.17319287  0.89534171  6.53212037  1.60318416  5.62593127
 -0.46279124  3.51551775 -1.7849545  -2.93961271  2.18169238 -2.35969257
  2.9035487   4.11827133 -2.05001406  2.97225255 -1.81901737 -0.75657377
  3.06826737  4.05248932  2.66970215  4.00823824  1.72901053  5.4357646
  0.24687936  6.98612447 -3.48177497 -0.07973574  5.19762638 -0.8982373
  2.1953293   4.93864914 -0.52852199  6.45987293  1.08901425  7.06032624
 -0.78352775  8.19820823  3.47231604 -3.13655884 -3.79417198 -2.48790316
 -0.26350595  5.28756424 -2.4699294  -2.4325582  26.95435071]
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2090.0734415241172
gradient value of function right now is: [-8.13927671e-01 -1.11606001e+00 -1.98676228e+00 -7.03465255e-01
  8.13927671e-01  1.11606001e+00  1.98676228e+00  7.03465255e-01
 -1.93505955e+00 -1.79578027e+00 -1.93628320e+00 -1.42210698e+00
  1.50775924e+00  9.95742734e-02  1.39706345e+00  8.03640389e-02
 -4.03654327e+00 -3.51963923e+00 -4.02469639e+00 -2.87257585e+00
  1.08256455e+00  6.55675300e-02  1.00306588e+00  5.26189572e-02
  1.10464695e-01  3.61929341e-01  2.73181699e-01  1.47256087e-01
 -2.69636457e-01 -3.61611284e-01 -5.49963425e-01 -4.22173826e-01
 -2.77848106e-03  3.83724991e-01  6.95970221e-02 -6.00976646e-02
 -1.06125511e-01  2.57641564e-02 -1.85087602e-01 -1.82124202e-01
  7.82010329e-03  1.77865795e-01  8.27186069e-01  1.20047498e-01
 -2.07067200e-01 -2.42605398e+00 -3.39742974e+00 -1.93011301e+00
  2.58639325e-02  5.55819553e-01  2.12591315e+00  3.74870456e-01
  1.72701751e-02  3.89943813e-01  2.05143608e+00  2.66026964e-01
 -3.38629229e-01  3.77503150e-01 -2.66479613e+00  5.78797763e+00
  8.31181279e-01 -6.00832242e-01 -2.96899211e+00  5.83849426e+00
  1.99037268e+01]
supnorm grad right now is: 19.903726760201273
Weights right now are: 
[-2.93378025  0.84513507 -2.52520023  0.70799504  2.40230999 -0.9791305
  2.68990727 -1.27405116  3.05872611 -2.49502786  3.39546102  1.07435107
  0.38206745  5.71178353  1.78717777  5.22777151  2.69066272 -2.7723441
  2.79797281 -0.17966754  0.99517416  6.49759546  1.70602264  5.55096326
 -0.43388076  4.21919654 -1.76283292 -2.8978477   2.26258575 -2.98216615
  2.98770049  4.22170338 -2.11407153  3.67791597 -1.83801444 -0.80448355
  3.26375135  4.59654402  2.86528762  4.21619245  1.85386567  5.44432845
  0.17325465  7.03277147 -3.99645996 -0.77159418  4.70519202 -1.63816729
  2.30811703  4.94205654 -0.56056376  6.47380541  1.23035596  7.0393832
 -0.84687797  8.23045742  2.95192807 -3.06427444 -3.74760743 -2.5640513
  0.52869761  5.41645785 -2.49731276 -2.45401182 27.07278156]
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2096.831321774715
gradient value of function right now is: [ 3.17212170e-02  9.43313525e-02  9.72320579e-02  5.11440845e-02
 -3.17212170e-02 -9.43313525e-02 -9.72320579e-02 -5.11440845e-02
  1.62982715e-01 -4.28513814e-02  1.41795468e-01  2.94620675e-03
 -9.79666195e-02 -7.24191565e-02 -8.77800100e-02 -6.48674712e-02
  4.98473145e-01  3.02979160e-02  4.50020585e-01  1.01008665e-01
 -4.38075129e-02 -4.83652485e-02 -3.85158527e-02 -4.32521401e-02
 -5.54676147e-02  4.59470682e-02 -1.28375813e-01 -9.02594477e-02
  1.13734097e-01 -3.55303686e-02  2.58463035e-01  1.94313353e-01
 -7.34914219e-02  5.13370685e-02 -1.46402209e-01 -1.22179340e-01
  5.79318816e-02  1.20260902e-02  1.27018421e-01  1.01961859e-01
 -1.26781324e-02 -9.82759852e-02 -5.65084505e-01 -7.03203782e-02
 -3.26549399e-02 -4.11227381e-01 -3.56856289e-01 -3.61565672e-01
 -2.91841309e-02 -2.38855669e-01 -1.24912037e+00 -1.66407223e-01
 -3.29117531e-02 -2.36522314e-01 -1.31170080e+00 -1.70719559e-01
 -5.90016050e-02 -6.61753832e-02  1.20975504e+00 -3.14569743e+00
  2.86468342e-01 -1.39971044e-01  9.42050303e-01 -2.83848391e+00
 -9.38713411e+00]
supnorm grad right now is: 9.38713410887406
Weights right now are: 
[-2.86340638  1.03133184 -2.48416763  0.89834799  2.33193612 -1.16532728
  2.64887467 -1.46440411  3.19039443 -2.42919874  3.5260656   1.11816416
  0.22667578  5.78294591  1.63468409  5.32112673  2.78244731 -2.75475997
  2.88843322 -0.16927316  0.8344186   6.56100732  1.5476364   5.64006547
 -0.53070998  4.19002427 -1.89572945 -2.95991142  2.45527373 -2.92043117
  3.16731761  4.42407127 -1.98656201  3.64840426 -1.75217843 -0.6018111
  3.39514308  4.5908015   2.92362246  4.35980692  1.91109504  5.39315636
  0.13279922  7.01096721 -3.91407267 -0.76178345  4.7788963  -1.62403355
  2.3236838   4.82327136 -0.65269328  6.37541474  1.31445559  6.98297868
 -0.91779643  8.20564844  2.92132881 -3.11598249 -3.60448862 -2.69919366
  0.53544425  5.55488572 -2.35343901 -2.58559126 26.25878484]
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2098.2336236125548
gradient value of function right now is: [-2.71163066e-01  1.70774157e-01 -7.44310978e-01  1.28039467e-01
  2.71163066e-01 -1.70774157e-01  7.44310978e-01 -1.28039467e-01
 -6.16541445e-01 -5.97484028e-01 -6.05599162e-01 -4.53681351e-01
 -4.21699101e-01  9.41913490e-02 -4.27356890e-01  1.00817717e-01
 -1.48433832e+00 -1.31524866e+00 -1.44630260e+00 -1.05673046e+00
 -3.78130452e-01  6.37830650e-02 -3.76287051e-01  6.88716179e-02
  5.91467782e-02  1.72701876e-01  1.81444439e-01  9.07535678e-02
 -1.64352940e-01 -1.58748893e-01 -4.37249564e-01 -2.99376176e-01
  1.54791445e-01  1.85219958e-01  2.73718292e-01  2.29337275e-01
 -1.17717236e-01  1.25669437e-02 -2.85072506e-01 -2.29153184e-01
 -1.36140591e-03  9.08068198e-02  8.15702486e-01  6.14343028e-02
 -1.13083746e-01 -1.19003702e+00 -1.34841347e+00 -1.01572744e+00
 -5.51957596e-03  3.14631535e-01  1.83065819e+00  2.04393283e-01
 -3.08348349e-03  2.32634324e-01  1.82409920e+00  1.60508273e-01
 -1.78541565e-01  1.04448893e-01 -1.65697974e+00  4.35100828e+00
  6.80891509e-01 -3.27104011e-01 -1.73514269e+00  4.18260132e+00
  2.36289193e+00]
supnorm grad right now is: 4.351008280878745
Weights right now are: 
[-2.76624609  1.18129028 -2.38674756  1.05449014  2.23477582 -1.31528572
  2.5514546  -1.62054626  3.29921012 -2.28640213  3.64182046  1.25115394
  0.0426572   6.24948099  1.44781869  5.85234367  2.87559005 -2.63683022
  2.98755255 -0.05393848  0.64110224  7.05788902  1.35144963  6.21157991
 -0.41818765  4.09714238 -1.85916993 -2.73556724  2.26777388 -2.8221696
  2.9608761   4.15709024 -1.69191835  3.55221912 -1.54919466 -0.23777023
  2.99605594  4.49060898  2.40193662  3.94662638  1.89711037  5.56216869
  0.58937681  7.21208298 -3.69328992 -0.74726054  4.88588197 -1.58483605
  2.19852732  4.81314754 -0.40043775  6.36648172  1.3302507   7.22785798
 -0.48237996  8.49093371  2.93319642 -3.27833491 -3.71485516 -2.48707365
  0.47916389  5.66919145 -2.47235689 -2.35984448 26.49812896]
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2094.8102195326874
gradient value of function right now is: [ 4.69058778e-01 -7.81889074e-01  1.47748770e+00 -4.83262171e-01
 -4.69058778e-01  7.81889074e-01 -1.47748770e+00  4.83262171e-01
  8.27521947e-01  9.16020127e-01  8.35446294e-01  8.03742500e-01
  1.24378391e+00  2.96906474e-03  1.19637968e+00  9.33494886e-03
  2.57571541e+00  2.78216996e+00  2.60236742e+00  2.48338971e+00
  8.62202383e-01  5.01431808e-03  8.29497104e-01  9.14435999e-03
 -4.47054529e-02 -1.76975831e-01 -1.41977621e-01 -5.76392876e-02
 -4.14322220e-02  1.27368789e-01 -3.55805991e-02 -7.79595139e-02
 -1.22356040e-01 -1.88208060e-01 -2.90067646e-01 -1.83903579e-01
 -6.89246030e-02 -2.89519702e-02 -1.36478683e-01 -1.09717368e-01
 -8.10082838e-03 -3.11500013e-02 -2.62878932e-02 -2.57575073e-02
  1.63447702e-01  1.02715610e+00  1.50020339e+00  8.00698974e-01
 -4.18900345e-02 -2.32707724e-01 -4.99475493e-01 -1.75890975e-01
 -1.26783303e-02 -2.69027243e-02  1.73587255e-01 -2.68845614e-02
  1.26542266e-01 -1.79234279e-01  3.04124243e-01 -3.80902107e-01
 -2.31703705e-01  3.22941436e-01  5.44952626e-01 -6.93539713e-01
 -1.08869167e+01]
supnorm grad right now is: 10.886916723078855
Weights right now are: 
[-2.49322389  0.86300959 -2.10670987  0.72285486  1.96175363 -0.99700503
  2.27141691 -1.28891098  3.56703991 -1.96021047  3.91835592  1.54391435
  0.46040922  6.33811805  1.84729928  5.91345992  3.1270346  -2.34719265
  3.24624486  0.21939576  1.07831671  7.14686706  1.77050302  6.2725568
 -0.59256568  3.80202755 -2.08358196 -2.88567021  2.31674592 -2.49473837
  3.0840395   4.18523751 -2.26688693  3.24671981 -2.14555299 -0.81837668
  2.93925299  4.34274048  2.39890627  3.9133214   1.83663673  5.44675956
  0.35964311  7.07566686 -3.35710389 -0.49260773  5.20024285 -1.32204269
  2.06375012  4.58410241 -0.62456592  6.11683797  1.28592101  7.15775649
 -0.5684575   8.39881645  3.15193559 -3.50588137 -3.59784008 -2.63314647
  0.28794112  5.97936633 -2.26967399 -2.57187981 26.61678347]
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2094.6030985070606
gradient value of function right now is: [ 7.38665753e-01 -8.82136809e-01  2.32937226e+00 -5.43152255e-01
 -7.38665753e-01  8.82136809e-01 -2.32937226e+00  5.43152255e-01
  1.08787311e+00  1.41393118e+00  1.11330441e+00  1.03651121e+00
  1.53282109e+00 -2.11286412e-02  1.47249739e+00 -1.58116421e-02
  3.59979420e+00  4.20066032e+00  3.66110817e+00  3.43101493e+00
  1.05509159e+00 -8.30015496e-03  1.01385792e+00 -5.43613704e-03
 -4.86882314e-02 -5.17899449e-01 -1.42788088e-01 -6.35408119e-02
 -2.61554551e-02  3.65419299e-01 -2.40833273e-03 -5.15461369e-02
 -1.48046899e-01 -5.71260111e-01 -3.19089348e-01 -2.22370019e-01
 -4.46163966e-02 -9.80875329e-02 -8.13872767e-02 -6.93771935e-02
 -8.01350528e-03 -3.85112374e-02 -1.32194723e-01 -3.18531718e-02
  2.62679187e-01  3.27124567e+00  2.28010852e+00  2.89483782e+00
 -3.59775351e-02 -2.39108880e-01 -6.15277707e-01 -1.77363126e-01
 -1.37448525e-02 -4.04789938e-02 -5.66547031e-03 -4.05837985e-02
  3.08887658e-01 -1.19407289e-01  2.58638159e-01 -4.34396221e-01
 -3.65438341e+00  3.05485316e-01  6.99738753e-01 -7.22255304e-01
 -1.45234440e+01]
supnorm grad right now is: 14.52344395288275
Weights right now are: 
[-2.51307925  1.08530442 -2.16912932  0.93219918  1.98160899 -1.21929986
  2.33383636 -1.4982553   3.66405941 -1.82566091  4.02246835  1.62538226
  0.24863416  6.26878204  1.62452344  5.77731538  3.05918734 -2.34959243
  3.18483427  0.11100535  0.88882765  7.05025352  1.56991623  6.1005688
 -0.51513591  3.38655783 -2.01304618 -2.79275218  2.51674155 -2.12254508
  3.28157527  4.41695673 -2.12532162  2.85808384 -1.97857868 -0.6737021
  3.34870575  4.03847911  2.82262996  4.32619774  1.84205266  5.43743432
  0.22909506  7.04666433 -2.50689519 -0.1465193   5.70408722 -0.97816691
  2.09643802  4.58734787 -0.68199846  6.11835532  1.28160969  7.13112816
 -0.74398687  8.34103413  3.6535832  -3.97463333 -3.55156652 -2.71103296
  0.24515136  6.61352012 -2.13984007 -2.71019315 27.15052868]
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2099.3143009333867
gradient value of function right now is: [ 4.76451513e-01 -1.15249091e+00  1.75528207e+00 -7.12836430e-01
 -4.76451513e-01  1.15249091e+00 -1.75528207e+00  7.12836430e-01
  7.38456786e-01  9.72473292e-01  7.54850517e-01  7.63595722e-01
  1.95609485e+00  3.30043565e-02  1.85840049e+00  3.74205365e-02
  2.77867923e+00  3.39121907e+00  2.83097794e+00  2.86081256e+00
  1.35305502e+00  2.51635240e-02  1.28572513e+00  2.77215247e-02
 -3.61928963e-02 -3.40210997e-01 -8.20140185e-02 -5.56124808e-02
 -1.11271171e-01  2.48109289e-01 -2.09881740e-01 -1.85289805e-01
 -1.75598048e-01 -3.71334757e-01 -3.46447294e-01 -2.87642899e-01
 -1.04558266e-01 -5.30836350e-02 -2.08627683e-01 -1.70396211e-01
 -5.40652256e-03  1.76581160e-02  1.70926455e-01  1.59321397e-03
  2.01278803e-01  2.13688678e+00  1.55066207e+00  1.90376102e+00
 -2.51143006e-02 -2.34794159e-02  1.90398042e-01 -4.59248021e-02
 -8.56020205e-03  7.67005707e-02  7.20049862e-01  2.92540823e-02
  1.39643563e-01 -7.95511884e-02 -5.14938946e-01  1.26748886e+00
 -2.52854797e+00  2.27119109e-01 -8.38715839e-02  6.88077642e-01
 -6.42649500e+00]
supnorm grad right now is: 6.4264949998824115
Weights right now are: 
[-2.52736592  1.14222122 -2.1503147   1.00209299  1.99589565 -1.27621666
  2.31502174 -1.56814911  3.68655184 -1.75030595  4.05151481  1.64922973
  0.23268655  6.40171167  1.61455167  5.93143173  3.08153485 -2.29966862
  3.20938349  0.14176137  0.85642073  7.13955428  1.54275671  6.20460083
 -0.35214607  3.27480774 -1.86722648 -2.56222008  2.63030807 -1.98237388
  3.33750562  4.56269231 -1.9542934   2.73897484 -1.75659607 -0.45227785
  3.57595671  3.93699963  2.95896875  4.58525163  2.05226114  5.54335922
  0.24710296  7.21934012 -2.37012535 -0.03533428  5.75965009 -0.86419424
  2.40128788  4.64632955 -0.59023019  6.25710717  1.47296197  7.23006678
 -0.75691561  8.49652318  3.78984235 -3.83045157 -3.52365781 -2.70573783
  0.14774864  6.63071125 -2.06612597 -2.69356982 27.26908263]
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2102.615239162173
gradient value of function right now is: [ 2.54779462e-01 -9.94900372e-01  1.12911412e+00 -6.29342126e-01
 -2.54779462e-01  9.94900372e-01 -1.12911412e+00  6.29342126e-01
  4.27462929e-01  5.01867662e-01  4.31103459e-01  4.58549993e-01
  1.78937453e+00  2.32882778e-02  1.65292528e+00  2.39632599e-02
  1.98132225e+00  2.24685643e+00  2.00509079e+00  2.07852017e+00
  1.27823274e+00  1.85325821e-02  1.18121471e+00  1.88027320e-02
 -3.08219625e-02 -8.39946551e-02 -6.76731288e-02 -5.01344213e-02
 -8.93296520e-02  7.12976517e-02 -1.62821074e-01 -1.51562982e-01
 -1.82106281e-01 -9.16826464e-02 -3.58420372e-01 -3.11905371e-01
 -8.05921473e-02 -4.24351276e-03 -1.54413481e-01 -1.34496528e-01
 -6.63723116e-03  7.22912990e-03  8.30224683e-02 -5.59567532e-03
  1.32285590e-04  5.54272956e-01  6.29550430e-01  4.70088745e-01
 -2.55937865e-02 -1.94526218e-02  6.11091861e-02 -4.31460238e-02
 -1.24426865e-02  5.23730273e-02  5.40973863e-01  1.17362887e-02
 -1.66185659e-02 -8.59270462e-02 -4.65041776e-01  9.15681400e-01
 -7.68978303e-01  1.99528698e-01 -2.17850933e-01  4.28139345e-01
 -4.45053671e+00]
supnorm grad right now is: 4.4505367072611195
Weights right now are: 
[-2.86772476e+00  1.34555590e+00 -2.76362376e+00  1.20268067e+00
  2.33625450e+00 -1.47955134e+00  2.92833080e+00 -1.76873679e+00
  3.32072428e+00 -2.05222778e+00  3.69513684e+00  1.19659666e+00
  4.24002557e-02  6.58016056e+00  1.41317586e+00  6.10871412e+00
  2.45886360e+00 -2.94937228e+00  2.58652095e+00 -5.91665955e-01
  6.75218761e-01  7.31741704e+00  1.34989466e+00  6.38031315e+00
 -2.33141219e-01  3.31283360e+00 -1.79108348e+00 -2.39184604e+00
  2.71600792e+00 -2.06178718e+00  3.37045028e+00  4.63915735e+00
 -1.73955485e+00  2.75855819e+00 -1.53089048e+00 -1.84278421e-01
  3.67019181e+00  3.70990158e+00  2.97922223e+00  4.65529656e+00
  2.36951902e+00  5.64913127e+00  3.99383125e-01  7.47386087e+00
 -2.74577678e+00 -2.15720044e-02  5.53544435e+00 -8.33590113e-01
  2.78921833e+00  4.71798194e+00 -4.81472974e-01  6.47017306e+00
  1.78427382e+00  7.30583796e+00 -6.63627872e-01  8.71784953e+00
  3.75626764e+00 -3.44834369e+00 -3.40461733e+00 -2.76528505e+00
  2.90913749e-02  6.15573746e+00 -1.99186982e+00 -2.65887365e+00
  2.73894192e+01]
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2103.2618606282485
gradient value of function right now is: [ 1.86618890e-02 -8.17960349e-01  1.93846777e-01 -5.14287261e-01
 -1.86618890e-02  8.17960349e-01 -1.93846777e-01  5.14287261e-01
  4.87810349e-02 -1.15331469e-02  4.16921272e-02  6.89827376e-02
  1.51504258e+00  1.67476176e-03  1.40423159e+00  8.12510529e-04
  4.81759890e-01  4.33920775e-01  4.77440406e-01  5.38043556e-01
  1.07930330e+00  2.41658296e-03  1.00085503e+00  1.86410429e-03
 -3.29440472e-02  9.52333556e-02 -7.38328435e-02 -5.27713274e-02
 -3.47215114e-02 -7.47000184e-02 -4.18964168e-02 -6.01619422e-02
 -1.62057434e-01  1.07677711e-01 -3.11566248e-01 -2.74797740e-01
 -3.54952941e-02  1.77411913e-02 -5.54742131e-02 -5.89509549e-02
 -7.07329027e-03 -2.40918540e-02 -1.27344200e-01 -2.03547123e-02
 -1.05587189e-01 -5.48457914e-01 -9.01071031e-02 -5.08866222e-01
 -2.38151240e-02 -7.81217286e-02 -3.30630928e-01 -6.46920839e-02
 -1.39846284e-02 -2.34020926e-02  3.16619053e-02 -2.62816013e-02
 -9.78753155e-02 -8.89942094e-02  6.05248539e-02 -2.71529808e-01
  5.05492545e-01  1.36228191e-01  5.94088478e-03 -3.50810272e-01
 -1.27328182e-01]
supnorm grad right now is: 1.5150425756736916
Weights right now are: 
[-2.87652084e+00  9.38757340e-01 -2.76450133e+00  7.97924645e-01
  2.34505058e+00 -1.07275278e+00  2.92920836e+00 -1.36398076e+00
  3.33848969e+00 -2.05285615e+00  3.70921507e+00  1.16981048e+00
  5.75896986e-01  6.35379843e+00  1.94324158e+00  5.86464252e+00
  2.48859539e+00 -2.99148773e+00  2.60560543e+00 -6.28631998e-01
  1.21718813e+00  7.07856714e+00  1.88868628e+00  6.12379305e+00
 -6.33790349e-01  3.33744330e+00 -2.19119948e+00 -2.91478235e+00
  2.99077808e+00 -2.06742826e+00  3.69903193e+00  4.98093290e+00
 -2.47150541e+00  2.77424931e+00 -2.27279015e+00 -8.94571139e-01
  3.90090462e+00  3.71617455e+00  3.25324282e+00  4.93470710e+00
  2.17131885e+00  5.54712590e+00 -6.80768446e-02  7.07904632e+00
 -3.00140722e+00 -1.07715855e-02  5.46894606e+00 -8.18740981e-01
  2.47780912e+00  4.61949100e+00 -8.99623247e-01  6.08008115e+00
  1.58010685e+00  7.12277775e+00 -1.05972306e+00  8.25233203e+00
  3.80188249e+00 -3.68116661e+00 -3.68459537e+00 -2.85554137e+00
 -3.10775484e-02  5.99555904e+00 -1.97915897e+00 -2.98665630e+00
  2.71456295e+01]
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2103.3496980254004
gradient value of function right now is: [ 0.01714657 -0.92370216  0.11585156 -0.57255258 -0.01714657  0.92370216
 -0.11585156  0.57255258  0.00905808  0.00863769  0.0064841   0.04722342
  1.71159228  0.01161757  1.6034704   0.01092547  0.17357053  0.30373674
  0.18035962  0.33304959  1.19587367  0.00849643  1.12068449  0.00801926
 -0.0277423   0.04004538 -0.05222954 -0.04748198 -0.04991563 -0.0303584
 -0.08518781 -0.08309345 -0.16302897  0.04567137 -0.29959238 -0.2802463
 -0.04313211  0.00958083 -0.07483654 -0.07176487 -0.00636157 -0.00525617
 -0.06175159 -0.00968622 -0.03896238 -0.2297264   0.02649449 -0.21305465
 -0.02090834 -0.01139639 -0.10877987 -0.02610456 -0.01283112  0.01535991
  0.18664909 -0.00638871 -0.06186926 -0.07510565 -0.20351384  0.25321052
  0.1483618   0.09773268 -0.15591612  0.03741172  0.12701688]
supnorm grad right now is: 1.7115922791941842
Weights right now are: 
[-2.53085264  1.25750378 -2.21977779  1.09584577  1.99938237 -1.39149921
  2.38448483 -1.66190189  3.67880736 -1.63917389  4.05851638  1.56568744
  0.27795563  6.51178594  1.6243534   6.04350477  2.93110922 -2.46436309
  3.05651406 -0.11374639  0.95066782  7.23776856  1.60150846  6.30264106
 -0.23373309  3.0109129  -1.85122407 -2.48273149  2.88192222 -1.64516578
  3.53887245  4.9063103  -2.15718458  2.44066453 -1.8980025  -0.5700582
  3.84222792  3.69589502  3.12300959  4.91181236  2.59778511  6.01662236
  0.29723108  7.41991206 -2.97392843  0.32784225  5.65913471 -0.48194842
  2.94712542  4.92785021 -0.56404952  6.28442241  1.98420224  7.52760475
 -0.81116791  8.55366425  4.07336829 -3.26432797 -4.14013326 -2.4927365
 -0.36417309  5.73797107 -2.16167388 -2.76501969 26.47999078]
NN weights: [-2.64225575  1.11254558 -2.39123922  0.96709515  2.11078549 -1.24654101
  2.55594626 -1.53315127  3.53908994 -1.87426273  3.9064391   1.45163032
  0.29780148  6.38559385  1.67080936  5.91062831  2.82452922 -2.58867829
  2.94881282 -0.18327552  0.93462305  7.1332507   1.61213022  6.19519954
 -0.43180395  3.29378863 -1.9618889  -2.67910284  2.70647582 -2.01196289
  3.41432657  4.650729   -2.10552335  2.7454133  -1.90983835 -0.58664688
  3.60477113  3.85078892  2.98608513  4.61274331  2.11992268  5.58612232
  0.21011999  7.20490958 -2.73657653 -0.00951328  5.61679382 -0.82792122
  2.44195515  4.68075653 -0.65551336  6.22649237  1.53754582  7.22267067
 -0.80492923  8.44024764  3.75134917 -3.69061631 -3.61772507 -2.72187502
  0.01243377  6.26512381 -2.07219734 -2.7554921 ]
Minimum obj value:-2103.3496980254004
Optimal xi: 27.096934273513142
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1461.5457530946242
W_T_median: 1242.7146852549477
W_T_pctile_5: 734.1133117114076
W_T_CVAR_5_pct: 641.8041257240027
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
F value: -2103.3496980254004
-----------------------------------------------
{'NN': [-2.642255752390676, 1.112545575898263, -2.3912392232326924, 0.9670951501366211, 2.110785489391184, -1.246541012619925, 2.5559462625104645, -1.5331512693404967, 3.5390899352768996, -1.874262728830223, 3.906439102431949, 1.4516303185037505, 0.2978014769684648, 6.385593847101037, 1.6708093582148675, 5.910628312400551, 2.82452921538039, -2.5886782913131787, 2.9488128220449137, -0.18327551843601889, 0.9346230478987114, 7.133250702569797, 1.612130224336376, 6.195199538141655, -0.43180394611075307, 3.2937886253283564, -1.9618888977822364, -2.6791028429741655, 2.706475815079903, -2.0119628852834506, 3.414326569223962, 4.650729000995014, -2.1055233538882208, 2.7454132978638635, -1.909838349363764, -0.5866468790446595, 3.6047711274296956, 3.8507889151219814, 2.9860851279590674, 4.612743307763555, 2.1199226810789553, 5.586122319891333, 0.21011998908255927, 7.204909577032132, -2.736576533881795, -0.009513284707934574, 5.616793817218672, -0.8279212184340009, 2.4419551455343216, 4.680756531598013, -0.6555133634485123, 6.226492365002125, 1.5375458179131496, 7.222670666906526, -0.8049292325152223, 8.440247640127788, 3.7513491666202174, -3.6906163084086137, -3.617725065381021, -2.7218750177590305, 0.012433769005670137, 6.265123807796204, -2.0721973364034154, -2.755492098664901]}
[-2.64225575  1.11254558 -2.39123922  0.96709515  2.11078549 -1.24654101
  2.55594626 -1.53315127  3.53908994 -1.87426273  3.9064391   1.45163032
  0.29780148  6.38559385  1.67080936  5.91062831  2.82452922 -2.58867829
  2.94881282 -0.18327552  0.93462305  7.1332507   1.61213022  6.19519954
 -0.43180395  3.29378863 -1.9618889  -2.67910284  2.70647582 -2.01196289
  3.41432657  4.650729   -2.10552335  2.7454133  -1.90983835 -0.58664688
  3.60477113  3.85078892  2.98608513  4.61274331  2.11992268  5.58612232
  0.21011999  7.20490958 -2.73657653 -0.00951328  5.61679382 -0.82792122
  2.44195515  4.68075653 -0.65551336  6.22649237  1.53754582  7.22267067
 -0.80492923  8.44024764  3.75134917 -3.69061631 -3.61772507 -2.72187502
  0.01243377  6.26512381 -2.07219734 -2.7554921 ]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1260.9596434837733
W_T_median: 1221.9241450328454
W_T_pctile_5: 830.3340558417918
W_T_CVAR_5_pct: 742.5812257595575
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2839.0255594371893
gradient value of function right now is: [-8.45411389e-02 -2.30786586e-01 -4.42318203e-01 -1.55430634e-01
  8.45411389e-02  2.30786586e-01  4.42318203e-01  1.55430634e-01
 -2.53121559e-01 -2.65036631e-01 -2.53641181e-01 -2.62835089e-01
  3.55818608e-01 -1.49376551e-02  3.03057435e-01 -1.44141841e-02
 -8.68081031e-01 -9.14718813e-01 -8.68231060e-01 -8.94849774e-01
  2.79080322e-01 -1.00800853e-02  2.40816857e-01 -9.78517318e-03
 -2.83059732e-02  1.11697505e-02 -5.63103274e-02 -5.69719019e-02
  1.15103403e-02 -1.13573288e-02  3.33813225e-02  2.57854940e-02
 -7.68803988e-02  1.07720957e-02 -1.52691324e-01 -1.60614832e-01
  4.48095719e-03 -2.72124538e-04  1.90862629e-02  8.67391697e-03
 -5.07291214e-03 -3.88815871e-02 -2.28484102e-01 -2.43467040e-02
  2.27742018e-03 -7.74160117e-02 -1.10237495e-01 -5.41202059e-02
 -1.08850506e-02 -3.87466773e-02 -4.21136356e-01 -2.14881058e-02
 -1.25774579e-02 -8.55206632e-02 -4.43009728e-01 -5.29491100e-02
 -5.98038570e-03 -4.09455051e-02  5.12999345e-01 -1.38591173e+00
  1.33312176e-02 -7.08101772e-03  1.80953229e-01 -9.15186357e-01
  1.81577972e+01]
supnorm grad right now is: 18.157797242033247
Weights right now are: 
[-3.27175724  1.03119235 -2.81581183  0.89413529  2.74028698 -1.16518778
  2.98051886 -1.46019141  3.18666682 -2.4119967   3.53987665  1.10250654
  0.46880977  6.37686608  1.84766648  5.9017109   2.47844053 -2.96488909
  2.6017839  -0.53976982  1.10054798  7.07133711  1.78390414  6.13774669
 -0.16473901  3.65405219 -1.64522847 -2.41316482  2.8544156  -2.42869215
  3.43131459  4.86198181 -1.98713721  3.16820927 -1.60790461 -0.47066123
  3.98030613  4.6828897   3.30908227  5.01255544  2.35257411  5.73010453
  0.2032932   7.51820238 -3.39185637 -0.24224645  5.64382913 -1.06038126
  2.7354328   4.91690378 -0.48008717  6.64355462  1.75584371  7.2981171
 -0.8592268   8.66791997  2.92386518 -4.34202801 -3.56497231 -2.75051299
  0.06145858  7.50571561 -2.10351539 -2.68905857 27.20976556]
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2845.7348012697657
gradient value of function right now is: [-1.78366636e-01 -4.96343855e-01 -6.50183603e-01 -3.21001211e-01
  1.78366636e-01  4.96343855e-01  6.50183603e-01  3.21001211e-01
 -6.87848910e-01 -3.48921373e-01 -6.68372969e-01 -3.20845076e-01
  6.90362661e-01  2.43202775e-02  6.35988536e-01  1.79825945e-02
 -1.91555731e+00 -8.53274140e-01 -1.85140879e+00 -7.86803481e-01
  5.03734727e-01  1.67052900e-02  4.64603423e-01  1.22465322e-02
  9.76906984e-02  2.49653169e-02  2.08803287e-01  1.71113492e-01
 -1.26881844e-01 -3.12818651e-02 -2.64653217e-01 -2.21369542e-01
  5.77146363e-02  2.55247475e-02  1.31335049e-01  8.71337336e-02
 -4.70338801e-02 -3.60541773e-03 -8.58885163e-02 -8.49329132e-02
  7.97225955e-04  6.25137779e-02  5.02972704e-01  4.60150707e-02
 -1.03438906e-02 -1.36187054e-01 -2.41117128e-01 -1.04331867e-01
  3.10717024e-03  1.98665308e-01  1.16473755e+00  1.39829540e-01
  1.59004719e-03  1.47747868e-01  1.20200075e+00  1.10539070e-01
 -2.91602124e-02  3.22999186e-02 -7.42933109e-01  2.06933993e+00
  4.06396590e-02 -2.06193040e-02 -8.60127277e-01  2.29711479e+00
  1.31671933e+01]
supnorm grad right now is: 13.167193257668853
Weights right now are: 
[-3.64369827  0.90823249 -3.10324367  0.77071068  3.11222801 -1.04222792
  3.2679507  -1.3367668   2.87993677 -2.74141774  3.23156497  0.84644317
  0.61651879  6.6315697   1.99527983  6.13547691  2.21221162 -3.22464184
  2.33637091 -0.76329277  1.24645803  7.33883169  1.92952637  6.38595854
 -0.05829306  4.00665967 -1.52189407 -2.28954425  2.59739169 -2.82804985
  3.17087228  4.59475982 -1.9895234   3.52634893 -1.56590779 -0.44999597
  3.72316656  4.86842254  3.03848145  4.7468947   2.71384475  5.89379311
  0.48577912  7.87573605 -3.5697569  -0.59286334  5.31512921 -1.40554159
  2.95022321  5.04614066 -0.24576785  6.90772074  2.19670035  7.47033256
 -0.56538515  9.0471587   2.49950947 -4.04370953 -3.5530288  -2.69239118
  0.35260982  7.65436363 -2.13902764 -2.53033262 26.5158    ]
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2848.925700739721
gradient value of function right now is: [-2.21107290e-02 -2.58360198e-01 -1.13077578e-01 -1.67398493e-01
  2.21107290e-02  2.58360198e-01  1.13077578e-01  1.67398493e-01
 -1.24765379e-01 -1.99261293e-02 -1.19715052e-01 -2.26087749e-02
  3.85287692e-01  2.41877314e-03  3.58883231e-01 -1.45353584e-05
 -4.95882461e-01 -1.08173917e-01 -4.75418664e-01 -1.07188922e-01
  2.82376536e-01  1.73886335e-03  2.63439487e-01 -1.09196581e-05
  1.90250676e-02 -1.91842259e-03  3.42941073e-02  2.99807142e-02
 -3.35216836e-02  2.52480353e-03 -5.77467036e-02 -5.43768441e-02
 -1.04984974e-02 -2.89949537e-03 -1.44863796e-02 -2.86171346e-02
 -1.11331237e-02  7.20949913e-05 -1.57415133e-02 -1.86890610e-02
 -1.97539636e-03  8.03145979e-03  7.39034620e-02  6.59782597e-03
  5.79441872e-03  2.47509971e-02  1.82580653e-02  2.09844142e-02
 -4.07016804e-03  3.59055381e-02  1.70370884e-01  2.62227937e-02
 -4.77067013e-03  1.81758422e-02  1.76693509e-01  1.57372343e-02
 -1.27322394e-03 -2.35644714e-02  3.59428997e-02  1.59500062e-02
  1.47528123e-03  2.59302868e-03 -1.57878697e-01  3.23370181e-01
 -2.82703645e-01]
supnorm grad right now is: 0.4958824606796776
Weights right now are: 
[-3.65211609  0.93069242 -3.14983823  0.79771786  3.12064583 -1.06468786
  3.31454527 -1.36377398  2.90666323 -2.76600857  3.2564942   0.81677094
  0.65046813  6.51871042  2.02770026  6.06031007  2.16042326 -3.32277213
  2.28262231 -0.86795162  1.27652884  7.20525352  1.95801564  6.28993397
 -0.14458341  4.04004559 -1.48996101 -2.44246706  2.64650258 -2.84930545
  3.10966757  4.70370718 -2.12841217  3.56095636 -1.58246861 -0.62215143
  3.92112192  4.94549892  3.18508419  4.97606013  2.894512    6.17341749
  0.44938866  8.07203542 -3.60083061 -0.60579451  5.31438141 -1.41666544
  3.16561382  5.38003586 -0.12151511  7.17882392  2.38075614  7.73609424
 -0.60515961  9.2211084   2.46323246 -3.9150937  -3.95097623 -2.45833721
  0.37380854  7.7499128  -2.45935214 -2.38596511 25.86879941]
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2848.290007416569
gradient value of function right now is: [ 9.78231944e-02 -2.15590992e-01  4.66235127e-01 -1.35483906e-01
 -9.78231944e-02  2.15590992e-01 -4.66235127e-01  1.35483906e-01
  2.56101951e-01  2.61591802e-01  2.56848616e-01  2.33048134e-01
  2.98490095e-01 -4.83964849e-03  2.89158997e-01 -4.85537644e-03
  8.42315599e-01  8.60931047e-01  8.44473123e-01  7.89627211e-01
  2.13707885e-01 -3.19521178e-03  2.07093669e-01 -3.22415944e-03
 -3.08913918e-02 -3.03014235e-02 -7.03648423e-02 -5.65043851e-02
  2.61248102e-02  3.48983859e-02  6.35502201e-02  4.85221342e-02
 -5.13621044e-02 -3.30865688e-02 -1.03429124e-01 -9.37842326e-02
  1.62958497e-03  2.47569567e-03  8.25796509e-03  3.74629775e-03
 -7.13759376e-03 -4.48960230e-02 -2.14177857e-01 -2.66793721e-02
  2.26308954e-02  2.07648343e-01  2.67892793e-01  1.58990412e-01
 -1.80738519e-02 -9.65212653e-02 -4.84872133e-01 -5.97414955e-02
 -1.69738490e-02 -1.09123019e-01 -5.12835148e-01 -6.49337580e-02
  3.05701406e-02 -1.03509119e-01  5.20915374e-01 -1.60686522e+00
 -7.21218263e-02  2.49937792e-02  2.50679833e-01 -1.02085782e+00
 -2.31221849e+00]
supnorm grad right now is: 2.312218489879237
Weights right now are: 
[-3.41344451  0.73090639 -2.94622094  0.60481044  2.88197425 -0.86490183
  3.11092798 -1.17086656  3.19907731 -2.52080749  3.54925012  1.07270361
  0.88680361  6.75742314  2.2705823   6.3194106   2.3555734  -3.12557185
  2.47851941 -0.67932269  1.50359604  7.45108332  2.19156834  6.55998038
 -0.40522549  3.98607148 -1.7277021  -2.71748515  2.80365968 -2.78386085
  3.27173946  4.87129478 -2.45490918  3.5073336  -1.86988666 -0.95289324
  3.81615065  5.01748843  3.09262164  4.88321369  3.10720358  6.19346067
  0.3407953   7.92641916 -3.63166672 -0.55595564  5.46228811 -1.37336549
  3.34989692  5.35405296 -0.25146984  7.0159655   2.5768928   7.72016875
 -0.72458944  9.04248199  2.50529145 -3.87048456 -4.09627191 -2.47963801
  0.36576856  7.82428196 -2.42492243 -2.5917078  25.95596494]
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2847.7542383463497
gradient value of function right now is: [ 1.02438233e-01 -2.25518682e-01  5.48134935e-01 -1.42814277e-01
 -1.02438233e-01  2.25518682e-01 -5.48134935e-01  1.42814277e-01
  2.27450218e-01  3.06994858e-01  2.30503522e-01  2.70731566e-01
  2.33185484e-01  2.60111021e-03  2.26909214e-01  2.42936818e-03
  7.72668727e-01  1.13262527e+00  7.84914412e-01  1.04922833e+00
  1.72569319e-01  2.03543409e-03  1.67961071e-01  1.90223769e-03
  7.34296902e-03 -3.68635330e-02  4.41983891e-04  1.24444533e-02
 -3.63779964e-03  4.17816780e-02  4.58771047e-03 -5.00507535e-03
 -3.79091064e-03 -4.05017640e-02 -1.92224412e-02 -8.79765896e-03
 -1.10807271e-02  1.10428016e-03 -1.63363811e-02 -2.01797238e-02
 -6.12217810e-03 -1.15934874e-02  9.33345665e-03 -8.59005757e-03
  2.98283349e-02  2.76073261e-01  2.85500676e-01  2.20827797e-01
 -1.71970890e-02 -3.02864368e-02 -8.02730980e-02 -2.35998578e-02
 -1.53035498e-02 -3.49133101e-02 -4.07221511e-02 -2.47259315e-02
  4.87488821e-02 -1.05390610e-01  1.63649101e-01 -4.43288454e-01
 -1.65782241e-01  3.48984612e-02  4.97904292e-02 -2.50968789e-01
 -8.53410818e-01]
supnorm grad right now is: 1.132625267962231
Weights right now are: 
[-3.33096198  0.54942333 -2.85798795  0.42648628  2.79949172 -0.68341876
  3.02269498 -0.9925424   3.26803591 -2.35019946  3.62300806  1.24725211
  1.06349001  7.05713819  2.45182003  6.62917144  2.36020051 -2.9974804
  2.48813419 -0.56357015  1.67643993  7.75858518  2.36896795  6.88108228
 -0.34483117  3.87085456 -1.69390493 -2.64413653  2.70468407 -2.62660423
  3.20630208  4.75967453 -2.4680416   3.39735038 -1.88746176 -0.94764899
  3.57241805  5.21425512  2.8650286   4.63448582  3.67126027  6.25038235
  0.47672567  8.07015929 -3.50280337 -0.41476912  5.75418629 -1.2419586
  3.79812914  5.35324835 -0.17698173  7.09590527  3.16670915  7.76811445
 -0.58530594  9.18426686  2.47710524 -3.44570881 -4.03680035 -2.50040683
  0.3869925   8.15125393 -2.34260268 -2.58816634 26.01784963]
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2843.333533820351
gradient value of function right now is: [ 3.85413576e-01 -1.42749307e-01  2.35157877e+00 -9.07502736e-02
 -3.85413576e-01  1.42749307e-01 -2.35157877e+00  9.07502736e-02
  1.01578225e+00  9.96712174e-01  1.01916685e+00  8.94856420e-01
  2.17705531e-01 -1.22030505e-02  2.13572402e-01 -1.19150189e-02
  4.77198068e+00  4.46972515e+00  4.77857636e+00  4.19080433e+00
  1.57806188e-01 -8.10569768e-03  1.54833120e-01 -7.92994542e-03
 -7.69194587e-02 -1.27690321e-01 -1.83955127e-01 -1.29919315e-01
  6.66401937e-02  1.29925915e-01  1.66832886e-01  1.12263192e-01
 -9.43916400e-02 -1.38540176e-01 -2.13418013e-01 -1.60166527e-01
  9.49715915e-03  1.58191126e-03  2.42110524e-02  1.78534806e-02
 -1.64775543e-02 -1.14328039e-01 -4.74217806e-01 -5.76975503e-02
  1.10603392e-01  9.03901735e-01  9.55052254e-01  7.06408105e-01
 -4.42960195e-02 -3.24257584e-01 -1.20193555e+00 -1.63572951e-01
 -3.82090874e-02 -2.44239379e-01 -1.05265265e+00 -1.25572114e-01
  2.23276058e-01 -3.48909616e-01  1.50048422e+00 -3.54094055e+00
 -5.57755702e-01  1.27718715e-01  1.05136435e+00 -2.45710285e+00
 -9.92027193e+00]
supnorm grad right now is: 9.92027192922604
Weights right now are: 
[-3.21399241  0.7126741  -2.71547738  0.59279081  2.68252214 -0.84666954
  2.88018442 -1.15884693  3.43540807 -2.23881738  3.79023254  1.36581035
  0.96768503  7.06567597  2.35399733  6.63634437  2.54480182 -2.90877459
  2.67148501 -0.47728649  1.57257197  7.76640819  2.26283402  6.88767872
 -0.50014722  3.83147452 -1.80762265 -2.80743008  2.80758328 -2.57597769
  3.28196457  4.86678476 -2.57388454  3.36232145 -1.96776708 -1.05712233
  3.68976312  5.3081526   2.95959896  4.74919044  4.18242402  6.2075746
  0.38129351  8.09389681 -3.45893172 -0.36574017  5.86736319 -1.20844945
  4.29961082  5.33233413 -0.2354697   7.13317496  3.71305489  7.71749036
 -0.67329386  9.2099171   2.46170185 -3.00454157 -4.00738238 -2.57313418
  0.4165885   8.26591458 -2.25658525 -2.6616746  25.91283853]
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -2846.983256455108
gradient value of function right now is: [ 6.91996618e-02 -1.54174009e-01  3.70795150e-01 -9.66437459e-02
 -6.91996618e-02  1.54174009e-01 -3.70795150e-01  9.66437459e-02
  2.16875228e-01  1.54902431e-01  2.15515292e-01  1.46727328e-01
  1.83244211e-01 -5.67291047e-03  1.80389175e-01 -5.58881496e-03
  8.75218594e-01  6.19305317e-01  8.69610950e-01  5.79017432e-01
  1.34225210e-01 -3.96386427e-03  1.32146072e-01 -3.90550597e-03
 -5.40872748e-02 -2.06134318e-02 -1.25736897e-01 -9.97117118e-02
  4.99218719e-02  2.22742718e-02  1.15560766e-01  9.17713301e-02
 -6.56057115e-02 -2.19480753e-02 -1.45994256e-01 -1.20442567e-01
  6.42234417e-03  2.94845104e-03  1.75955949e-02  1.27915872e-02
 -1.30775728e-02 -7.57444650e-02 -3.51759895e-01 -3.48446811e-02
 -2.70934767e-03  1.31914545e-01  2.45377807e-01  6.41268451e-02
 -2.89674604e-02 -1.82885251e-01 -8.26787647e-01 -8.19363991e-02
 -3.28790861e-02 -1.73325858e-01 -8.46580260e-01 -8.23165455e-02
  4.41025568e-02 -2.56407145e-01  1.29916317e+00 -2.79002895e+00
  1.79776601e-02 -4.71874529e-03  7.64888427e-01 -1.78697709e+00
  1.22868861e+00]
supnorm grad right now is: 2.7900289517708585
Weights right now are: 
[-3.45802962  0.57016032 -2.91347274  0.44997858  2.92655936 -0.70415576
  3.07817978 -1.0160347   3.28677    -2.48522852  3.63690172  1.19800015
  1.10316493  7.25912183  2.49372277  6.85623529  2.40668356 -3.11326533
  2.53034624 -0.63264269  1.70881842  7.97012083  2.40338196  7.12162338
 -0.53755251  4.40792438 -1.84808651 -2.843812    2.82776775 -3.17641375
  3.3156921   4.88032058 -2.64180564  3.93296307 -2.0297684  -1.12216786
  3.54771068  5.41108148  2.84305327  4.58509367  4.24341975  6.14017566
  0.37204344  8.124414   -3.92969462 -1.07794005  5.21847036 -1.89625309
  4.34573657  5.27507859 -0.26520697  7.17430471  3.73716281  7.62674952
 -0.68983081  9.21036506  1.43473309 -2.77929678 -3.8540043  -2.71342084
  0.96266921  7.4408857  -2.21190771 -2.73041555 26.21293422]
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2845.1394069742923
gradient value of function right now is: [-1.45994543e-01 -2.65097480e-01 -7.39186426e-01 -1.73698416e-01
  1.45994543e-01  2.65097480e-01  7.39186426e-01  1.73698416e-01
 -5.82126383e-01 -2.89649412e-01 -5.72211039e-01 -2.48484003e-01
  3.17122007e-01  1.73296048e-02  3.00992132e-01  1.39383119e-02
 -2.29389543e+00 -9.27302319e-01 -2.24239272e+00 -8.00834501e-01
  2.38945281e-01  1.26575471e-02  2.26972245e-01  1.01687412e-02
  1.26910743e-01  4.89409476e-02  2.28512603e-01  2.46640418e-01
 -1.38065912e-01 -6.17132323e-02 -2.51950098e-01 -2.64640226e-01
  1.23413128e-01  5.16705675e-02  2.23552264e-01  2.37194119e-01
 -4.60626317e-02 -9.83790548e-03 -7.76750942e-02 -8.93220481e-02
  4.36105950e-02  9.05627846e-02  7.92741929e-01  6.33781004e-02
 -5.63701341e-02 -1.59521757e-01 -5.92497683e-01 -1.05297996e-01
  8.64080848e-02  1.94283096e-01  1.44494672e+00  1.33124198e-01
  1.09098468e-01  2.25928226e-01  1.89349190e+00  1.58916908e-01
 -3.83846924e-01  1.48475397e+00 -9.81041062e-01  3.43768651e+00
  2.77227035e-03 -6.27693902e-03 -9.84989091e-01  3.39784457e+00
  1.25708743e+01]
supnorm grad right now is: 12.570874334043191
Weights right now are: 
[-3.69978462e+00  6.99466443e-01 -3.17485588e+00  5.83887469e-01
  3.16831436e+00 -8.33461879e-01  3.33956292e+00 -1.14994359e+00
  3.06868824e+00 -2.73954018e+00  3.41555578e+00  9.34544561e-01
  1.03909861e+00  7.25751821e+00  2.42971580e+00  6.85012380e+00
  2.18164983e+00 -3.40779406e+00  2.30099523e+00 -9.15559118e-01
  1.63622571e+00  7.95809480e+00  2.33066564e+00  7.10337629e+00
 -4.14082454e-01  4.68643734e+00 -1.65586441e+00 -2.73895181e+00
  2.70873809e+00 -3.48139180e+00  3.13955981e+00  4.77326399e+00
 -2.50971266e+00  4.20158881e+00 -1.82886375e+00 -1.00167634e+00
  3.68183947e+00  5.07278430e+00  2.91863740e+00  4.73031508e+00
  5.09711614e+00  6.40144192e+00  5.43836872e-01  8.50361027e+00
 -3.94383885e+00 -1.36724908e+00  4.75982509e+00 -2.12017611e+00
  5.32230711e+00  5.55252188e+00 -1.54889606e-02  7.58631398e+00
  4.60007246e+00  7.87760590e+00 -5.13564401e-01  9.58107166e+00
  7.50298612e-01 -1.97466111e+00 -4.00192711e+00 -2.61126732e+00
  1.03786510e+00  7.37160232e+00 -2.46232755e+00 -2.53436933e+00
  2.62365963e+01]
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2847.6766432279783
gradient value of function right now is: [ 2.16710937e-02 -6.58882787e-02  9.09408713e-02 -4.07252798e-02
 -2.16710937e-02  6.58882787e-02 -9.09408713e-02  4.07252798e-02
  7.51229284e-02  3.38631846e-02  7.30131164e-02  2.30183317e-02
  7.74255722e-02 -4.04281535e-03  7.71655242e-02 -3.25834151e-03
  2.65733167e-01  1.14460824e-01  2.53916186e-01  8.55220694e-02
  5.71818501e-02 -3.01335711e-03  5.70023444e-02 -2.42668907e-03
 -3.81962335e-02 -9.91416481e-03 -6.77957277e-02 -7.78170512e-02
  4.55756275e-02  1.10286221e-02  7.63223619e-02  8.96369085e-02
 -4.25039275e-02 -1.08140829e-02 -7.48598692e-02 -8.63137373e-02
  1.12823651e-02  1.69255116e-03  1.89403132e-02  2.23384972e-02
 -7.75622747e-03 -5.68070988e-02 -3.13535652e-01 -2.56022840e-02
  6.58273271e-03  4.99632712e-02  1.12412488e-01  3.10209054e-02
 -1.18373776e-02 -9.10925431e-02 -5.14330914e-01 -3.93379793e-02
 -1.92126547e-02 -1.38429157e-01 -8.03270576e-01 -6.29347080e-02
  2.54699810e-02 -1.73754389e-01  1.07738529e+00 -2.62211285e+00
 -3.68066623e-03  4.07019178e-03  5.51497966e-01 -1.53539436e+00
 -5.04239369e+00]
supnorm grad right now is: 5.042393692123755
Weights right now are: 
[-3.71442196  0.50279279 -3.24851205  0.38888293  3.1829517  -0.63678823
  3.41321909 -0.95493905  3.08912442 -2.80668466  3.42743384  0.84849267
  1.28744611  7.01938268  2.69096665  6.59259728  2.13532873 -3.54353976
  2.24406383 -1.05817314  1.88688628  7.69985075  2.59455382  6.82218766
 -0.73620423  4.6852406  -1.87633978 -3.16705991  2.95871784 -3.4857296
  3.2935146   5.12008424 -2.8740008   4.19599029 -2.07604302 -1.4630417
  4.074928    5.11291811  3.26051873  5.19872479  4.70000108  6.54226257
  0.28026722  8.56191687 -3.87981479 -1.37650105  4.7720608  -2.11067878
  5.12766707  5.74335495 -0.13266868  7.72295955  4.22980384  7.97536888
 -0.8142264   9.59705452  1.41036348 -2.81592324 -4.17352282 -2.63023987
  1.03831808  7.37195072 -2.59936021 -2.61441806 25.63259219]
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2846.1235294668822
gradient value of function right now is: [ 6.09453041e-02 -6.25700802e-03  3.48658259e-01 -2.74006658e-03
 -6.09453041e-02  6.25700802e-03 -3.48658259e-01  2.74006658e-03
  2.23926183e-01  1.04645028e-01  2.20103384e-01  8.54978970e-02
  1.17889507e-02 -3.17713881e-03  1.24381200e-02 -2.71437904e-03
  1.08169444e+00  3.89179417e-01  1.05476107e+00  3.31916344e-01
  8.59875212e-03 -2.54286061e-03  9.13517859e-03 -2.17111887e-03
 -7.93305973e-02 -2.09882982e-02 -1.48242789e-01 -1.64102576e-01
  7.10581527e-02  2.29554807e-02  1.29528981e-01  1.43182376e-01
 -8.64625660e-02 -2.28217425e-02 -1.61788256e-01 -1.79258163e-01
  1.46405768e-02  2.76542434e-03  2.64332528e-02  2.95851478e-02
 -1.38641881e-02 -1.07948566e-01 -5.38394782e-01 -4.47504296e-02
  8.76459089e-03  1.21350888e-01  2.29507414e-01  6.76688492e-02
 -2.41372806e-02 -1.97251541e-01 -9.73855290e-01 -8.01646519e-02
 -3.58545696e-02 -2.65106057e-01 -1.36556001e+00 -1.11649265e-01
  7.24654340e-02 -3.05473929e-01  1.74804200e+00 -3.97297022e+00
 -1.55737968e-02  1.21312222e-02  8.75006284e-01 -2.21106415e+00
 -6.19683219e+00]
supnorm grad right now is: 6.196832185411484
Weights right now are: 
[-3.56827304  0.27837876 -3.11638563  0.16144913  3.03680277 -0.4123742
  3.28109267 -0.72750525  3.19713987 -2.59280161  3.53827011  1.04744407
  1.4610246   7.15429226  2.8697809   6.7059624   2.15839429 -3.34981049
  2.27030438 -0.87798796  2.06878531  7.85267819  2.78189279  6.95208391
 -0.71683428  4.52346579 -1.9205415  -3.1373057   3.11621885 -3.24431899
  3.48342135  5.27878197 -2.86863232  4.03550877 -2.1274289  -1.44412483
  4.1273953   5.46471727  3.36681115  5.24631552  4.75787024  6.34842671
  0.19520337  8.32369776 -3.85492621 -1.25824053  5.05693875 -2.03375835
  5.12081941  5.55275874 -0.2798804   7.4832266   4.25999218  7.72134959
 -0.95882023  9.30532161  1.7786321  -2.96782607 -4.03166746 -2.8155948
  1.03775346  7.37019111 -2.30098316 -2.90841926 25.72816385]
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2846.558122920893
gradient value of function right now is: [-5.09855314e-02 -1.37155724e-01 -4.11007805e-01 -8.43316023e-02
  5.09855314e-02  1.37155724e-01  4.11007805e-01  8.43316023e-02
 -2.36706479e-01 -8.29845860e-02 -2.33983436e-01 -7.54471363e-02
  5.12610504e-02  1.05383036e-04  5.01091257e-02  4.61183630e-05
 -1.63284241e+00 -3.51187567e-01 -1.60705955e+00 -3.14682266e-01
  4.69798717e-02  8.27174139e-05  4.59286851e-02  3.02442660e-05
  7.49946897e-02  1.18913312e-02  1.16705678e-01  1.41979172e-01
 -2.24665429e-02 -1.47856092e-02 -4.61378419e-02 -4.16782319e-02
  9.25698499e-02  1.30419810e-02  1.42610247e-01  1.75276728e-01
 -3.82235550e-03 -2.19140323e-03 -7.40545037e-03 -7.06130821e-03
  8.66000052e-03  2.69261829e-02  2.76423280e-01  1.95198827e-02
 -1.00290240e-02 -3.05013558e-02 -1.43461172e-01 -1.76675421e-02
  1.75318879e-02  6.60449385e-02  4.60217509e-01  4.37360552e-02
  2.10945566e-02  6.37631188e-02  4.88339245e-01  4.79589481e-02
 -7.04923352e-02  2.56901926e-01 -8.20331856e-02  5.35708642e-01
 -1.08013841e-03 -2.56485824e-03 -2.39247214e-01  7.95339880e-01
  7.61083747e+00]
supnorm grad right now is: 7.610837467772406
Weights right now are: 
[-3.62719079  0.14343163 -3.17676789  0.02684494  3.09572053 -0.27742706
  3.34147493 -0.59290106  3.1085566  -2.57473054  3.45172407  1.0868316
  1.52800678  7.22281574  2.93864845  6.76789613  1.98517151 -3.32335853
  2.1004957  -0.84369447  2.14234752  7.93390016  2.85748924  7.02634013
 -0.49799491  4.6037624  -1.75145072 -2.88304108  3.01133922 -3.3087828
  3.40456214  5.14760279 -2.65053538  4.11776397 -1.9563974  -1.18731192
  4.02926556  5.5110512   3.2892417   5.12850353  5.14479454  6.41193358
  0.36635045  8.45109744 -3.93923848 -1.36622627  4.96471093 -2.14227806
  5.43952273  5.58724976 -0.15642564  7.57257827  4.62338929  7.78918762
 -0.79502797  9.43483796  1.61295797 -2.67765985 -4.01702983 -2.79449471
  1.0726265   7.19883339 -2.29026402 -2.86127546 26.18623725]
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2847.8841647338845
gradient value of function right now is: [-5.04590161e-02 -1.10850638e-01 -3.73378199e-01 -7.08216162e-02
  5.04590161e-02  1.10850638e-01  3.73378199e-01  7.08216162e-02
 -2.32040975e-01 -8.80978356e-02 -2.30460236e-01 -8.54119784e-02
  8.51965989e-02  2.19109422e-03  8.38062073e-02  1.54806309e-03
 -1.30742735e+00 -4.42752472e-01 -1.29752902e+00 -4.01473886e-01
  6.68697695e-02  1.69571246e-03  6.57853789e-02  1.19439201e-03
  6.65833030e-02  1.05601584e-02  1.10503229e-01  1.22451202e-01
 -4.69567984e-02 -1.29145340e-02 -8.52907023e-02 -8.45078630e-02
  7.08786916e-02  1.10375133e-02  1.17490787e-01  1.30133711e-01
 -1.06130652e-02 -2.99076318e-03 -1.81401053e-02 -1.92793349e-02
  8.68205506e-03  4.68489761e-02  3.52909463e-01  2.77207083e-02
 -4.96175606e-03  7.04033051e-03 -1.28975613e-01  8.17987494e-03
  1.77675699e-02  9.46220110e-02  5.98967478e-01  5.63305757e-02
  2.10240570e-02  1.07556940e-01  7.40204801e-01  6.47601063e-02
 -6.88581772e-02  2.70823712e-01 -4.04244695e-01  1.56514029e+00
 -5.16059176e-03  2.80161763e-03 -3.81566290e-01  1.35178734e+00
  9.66529575e-01]
supnorm grad right now is: 1.565140294246012
Weights right now are: 
[-3.62959133  0.36476501 -3.13787731  0.24714018  3.09812107 -0.49876044
  3.30258435 -0.8131963   3.15691468 -2.59444506  3.50127579  1.06750435
  1.46599879  7.25522098  2.87528093  6.80537047  2.14253718 -3.38405705
  2.25844531 -0.89998619  2.06335045  7.97540458  2.77674466  7.07461744
 -0.60740828  4.64951016 -1.73659032 -2.9449137   2.75589708 -3.35102483
  3.1506587   4.85064506 -2.8150298   4.16092379 -1.98098907 -1.30368878
  3.86231344  5.43245921  3.10758259  4.93346938  5.28281415  6.72598574
  0.5303824   8.75283303 -3.88774147 -1.4063129   4.90268493 -2.16651253
  5.57588295  5.84103932  0.05870649  7.8339807   4.75995131  8.13593334
 -0.51086185  9.76217603  1.2229099  -2.53025719 -4.28834144 -2.50915244
  1.07140511  7.21105801 -2.58282994 -2.58324243 25.78390889]
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2848.0260598410914
gradient value of function right now is: [-7.31036180e-02 -1.15126521e-01 -5.28448284e-01 -7.60917190e-02
  7.31036180e-02  1.15126521e-01  5.28448284e-01  7.60917190e-02
 -2.95267030e-01 -1.62394191e-01 -2.94658094e-01 -1.55179329e-01
  1.48015857e-01  9.82082588e-03  1.45419163e-01  6.99300459e-03
 -1.50395585e+00 -8.13542548e-01 -1.50133684e+00 -7.35479311e-01
  1.11871272e-01  7.26953845e-03  1.09926367e-01  5.16088189e-03
  6.22592073e-02  2.07416329e-02  1.10725578e-01  1.08266449e-01
 -7.81356222e-02 -2.57102615e-02 -1.37343082e-01 -1.36129356e-01
  5.79729179e-02  2.12137424e-02  1.05203228e-01  9.94436805e-02
 -2.33062424e-02 -5.85974662e-03 -3.79691796e-02 -4.16033869e-02
  8.49964471e-03  8.03073873e-02  4.76001411e-01  4.19842711e-02
 -7.04220518e-03 -2.03218300e-02 -2.49436885e-01  1.54924631e-03
  1.70339979e-02  1.56915404e-01  8.23482939e-01  8.28575268e-02
  1.95429677e-02  1.79159480e-01  1.05871397e+00  9.40482151e-02
 -8.04702092e-02  3.03272193e-01 -1.00781571e+00  3.23546894e+00
 -7.34778448e-03  4.25693528e-03 -7.28826186e-01  2.37408874e+00
  2.90888674e+00]
supnorm grad right now is: 3.2354689366304545
Weights right now are: 
[-3.52040645  0.71785232 -2.98102054  0.60740688  2.98893619 -0.85184775
  3.14572758 -1.173463    3.35654554 -2.55976375  3.69898909  1.10494613
  1.27832206  7.2173088   2.68204185  6.78694424  2.4388877  -3.38093391
  2.55182485 -0.89679365  1.85337996  7.92848819  2.56064231  7.04959448
 -0.87914411  4.63991837 -1.9037932  -3.23097865  2.77138681 -3.33870465
  3.13637383  4.88248406 -3.09318535  4.15020028 -2.15609945 -1.59738839
  3.94486869  5.45223416  3.1493688   5.02407355  5.2487203   6.87934787
  0.47022316  8.90993454 -3.85469845 -1.42393     4.93110982 -2.19171099
  5.57981661  5.95482919  0.06306063  7.97216607  4.69955744  8.28961256
 -0.51937735  9.91101356  1.1160567  -2.62388062 -4.4035235  -2.4113787
  1.06853783  7.24697865 -2.74858378 -2.46678019 25.84426702]
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -2847.184355970017
gradient value of function right now is: [-1.26181422e-01 -1.27021166e-01 -8.21376680e-01 -8.73810188e-02
  1.26181422e-01  1.27021166e-01  8.21376680e-01  8.73810188e-02
 -4.62124776e-01 -2.98907687e-01 -4.60914983e-01 -2.56801398e-01
  1.88843718e-01  3.00195108e-02  1.80842216e-01  1.95245489e-02
 -2.15138824e+00 -1.28193242e+00 -2.14208961e+00 -1.08861793e+00
  1.46833603e-01  2.24893833e-02  1.40679893e-01  1.45337112e-02
  9.37471500e-02  5.59284513e-02  1.62345400e-01  1.61604202e-01
 -1.51395665e-01 -6.90752659e-02 -2.50432084e-01 -2.67027972e-01
  8.18565000e-02  5.78388680e-02  1.45301133e-01  1.36846615e-01
 -4.74634371e-02 -1.29055837e-02 -7.44091842e-02 -8.50682098e-02
  1.96754171e-02  1.43151512e-01  8.59446462e-01  7.08021904e-02
 -3.30993698e-02 -2.16812786e-01 -6.61294638e-01 -1.16085507e-01
  3.47822977e-02  2.54628448e-01  1.39925477e+00  1.26660306e-01
  4.69260051e-02  3.26529684e-01  1.99139915e+00  1.62883033e-01
 -2.01902737e-01  7.82421041e-01 -2.01607690e+00  6.57211627e+00
  7.06367716e-04 -4.18784494e-03 -1.33658350e+00  4.48748753e+00
  5.94668801e+00]
supnorm grad right now is: 6.572116267622719
Weights right now are: 
[-3.53404688  1.00912879 -2.97153761  0.91467388  3.00257662 -1.14312423
  3.13624465 -1.48073     3.41240023 -2.59528588  3.75342517  1.09264812
  1.03537671  6.93791304  2.43398974  6.5866589   2.50599549 -3.4457302
  2.6174703  -0.95420125  1.59010402  7.61903819  2.29193141  6.82431288
 -0.90115937  4.74555988 -1.88488175 -3.25450846  2.82588045 -3.43336267
  3.1414582   4.93779908 -3.06486087  4.26109381 -2.09996782 -1.57029437
  4.20813354  5.4846739   3.32633544  5.28010835  5.57776462  7.01070578
  0.47870506  9.08100455 -4.08411531 -1.47861456  4.82013888 -2.29537987
  5.91376957  6.06448419  0.11790856  8.1190274   5.00516041  8.42524479
 -0.48672491 10.08631236  0.93088199 -2.37288947 -4.48610934 -2.32848648
  1.1087565   7.04929412 -2.79650745 -2.38097288 25.90860464]
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2826.4638782832812
gradient value of function right now is: [  0.4247284    1.90594779   2.62101571   1.46709875  -0.4247284
  -1.90594779  -2.62101571  -1.46709875   1.08143579   1.05531519
   1.08807242   0.80682268  -5.03310989  -0.25878801  -4.93883344
  -0.17319676   5.25248864   4.68263965   5.26782756   3.80632121
  -4.42632182  -0.21121396  -4.34342509  -0.13928953  -0.05306631
  -0.34699541  -0.1302761   -0.06723489   0.43626728   0.34859429
   0.68482395   0.70931269   0.1699875   -0.35761783   0.1752448
   0.3026921    0.19708518   0.03274207   0.30034535   0.32733281
  -0.07115034  -0.4050297   -1.8347824   -0.20276241   0.3277598
   2.2811463    3.38869383   1.49045449  -0.11778     -0.71128456
  -3.07997073  -0.35322338  -0.16650633  -0.86331155  -4.51014597
  -0.44105992   0.91648899  -2.17517646   5.69246858 -13.15982907
  -0.32488823   0.28707795   4.36586423  -9.76340437 -21.92912726]
supnorm grad right now is: 21.929127260453612
Weights right now are: 
[-3.13436157  1.56567154 -2.52948083  1.52361164  2.60289131 -1.69966698
  2.69418787 -2.08966776  3.84620803 -2.22856439  4.18908465  1.43297448
  0.36330627  6.1269267   1.75275019  5.93845394  2.9693029  -3.05815357
  3.08216918 -0.58006218  0.86117183  6.76499596  1.5540192   6.13705405
 -1.16674493  4.40258887 -2.13276604 -3.4789265   3.37152826 -3.05067291
  3.58904563  5.43670578 -3.1651822   3.92989308 -2.22952079 -1.63616475
  4.98099076  5.95045883  3.97883489  6.01201276  5.4918269   6.61878455
  0.06446106  8.7547412  -3.68273812 -1.31960265  5.21599302 -2.13209482
  5.83643932  5.71863071 -0.23290098  7.82345177  4.95095206  8.09666083
 -0.86141097  9.82533211  1.34644317 -2.68912584 -4.13214152 -2.72890029
  1.08093682  7.30429508 -2.44066788 -2.77293775 25.42163891]
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2849.4425583237385
gradient value of function right now is: [ 2.10135479e-02 -1.17142744e-01  8.75301012e-02 -8.48489524e-02
 -2.10135479e-02  1.17142744e-01 -8.75301012e-02  8.48489524e-02
  5.46831746e-02  2.50511500e-02  5.51805153e-02  1.54060186e-02
  2.38332812e-01  4.12287941e-03  2.37393071e-01  2.32186825e-03
  2.89337934e-01  1.20418691e-01  2.91636054e-01  9.64930027e-02
  1.92762082e-01  4.03422185e-03  1.92015667e-01  2.45741121e-03
 -4.31089535e-02 -9.56219309e-03 -7.45546566e-02 -7.40728865e-02
 -7.74287675e-03  7.55444978e-03  3.33294495e-03 -1.40501852e-02
 -6.10041067e-02 -1.07443092e-02 -1.00328977e-01 -1.05494569e-01
 -2.05079521e-02  4.02149294e-04 -2.58819017e-02 -3.50679371e-02
 -1.44130104e-02 -4.35778562e-02 -1.81159287e-01 -2.14919167e-02
  8.08648178e-03  2.63029481e-02  1.11810896e-01  1.06390241e-02
 -2.43581095e-02 -7.82041863e-02 -3.54784963e-01 -3.69452687e-02
 -3.23285118e-02 -9.03773106e-02 -3.47585216e-01 -4.66318354e-02
  3.09021873e-02 -3.60235718e-01  6.67548049e-01 -1.73118634e+00
  4.36243946e-03 -5.30962863e-04  3.60702464e-01 -1.12048582e+00
 -7.70188560e-01]
supnorm grad right now is: 1.7311863394965838
Weights right now are: 
[-3.38624586e+00  1.15226009e+00 -2.65699423e+00  1.06440102e+00
  2.85477559e+00 -1.28625552e+00  2.82170127e+00 -1.63045714e+00
  3.75565861e+00 -2.45200331e+00  4.09509474e+00  1.31900670e+00
  8.47742376e-01  7.74161219e+00  2.22671626e+00  7.37564489e+00
  2.94129274e+00 -3.17077249e+00  3.05319315e+00 -6.25949352e-01
  1.37525288e+00  8.50735761e+00  2.05658776e+00  7.69992866e+00
 -1.23860390e+00  4.98840168e+00 -2.20277017e+00 -3.55087583e+00
  2.84812727e+00 -3.59435302e+00  3.18953890e+00  4.93786599e+00
 -3.49251667e+00  4.50011799e+00 -2.50515583e+00 -1.94869914e+00
  3.89458609e+00  5.75705038e+00  3.02478268e+00  4.97050790e+00
  5.93311322e+00  6.83452837e+00  3.94362220e-01  9.00915188e+00
 -4.74379490e+00 -1.92132180e+00  4.70972150e+00 -2.80966574e+00
  6.16340965e+00  5.86024821e+00 -6.69185801e-03  7.98239954e+00
  5.38457629e+00  8.31386621e+00 -5.06258498e-01  1.00883067e+01
  8.02731668e-01 -2.23561351e+00 -4.29417949e+00 -2.57803389e+00
  1.33064192e+00  6.08486675e+00 -2.63562742e+00 -2.58710592e+00
  2.59533165e+01]
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2848.9672603467834
gradient value of function right now is: [-1.86822391e-02 -9.12273048e-02 -9.09652274e-02 -6.54279169e-02
  1.86822391e-02  9.12273048e-02  9.09652274e-02  6.54279169e-02
 -4.80238363e-02 -7.66420906e-02 -4.92665879e-02 -5.63776680e-02
  1.50578719e-01 -6.15545597e-04  1.50929005e-01 -6.74382594e-04
 -8.90015811e-02 -1.82280175e-01 -9.15215530e-02 -1.34251098e-01
  1.22539446e-01 -2.65124078e-04  1.22826240e-01 -3.63564893e-04
 -1.79444488e-02  2.23596158e-02 -2.65877418e-02 -3.18992752e-02
  4.08145519e-03 -2.45291793e-02  8.48019628e-03  9.05199434e-03
 -2.32941190e-02  2.37511527e-02 -3.34560443e-02 -4.09239799e-02
 -2.92967472e-03 -2.01303438e-03 -2.68031361e-03 -4.28189025e-03
 -3.22701554e-03 -2.16855439e-02 -1.10358656e-01 -3.48967494e-03
 -4.03081702e-03 -1.59309588e-01 -2.90402001e-01 -7.41522159e-02
 -3.38872434e-03 -2.63249353e-02 -1.51553618e-01 -1.20128255e-04
 -7.95684210e-03 -4.83328914e-02 -2.37965504e-01 -1.00815470e-02
 -5.16895735e-02 -8.95608318e-03  4.37691833e-01 -1.12205590e+00
  5.23324056e-03 -3.70828906e-03  3.93948065e-03 -2.01771509e-01
  1.58232901e+00]
supnorm grad right now is: 1.5823290085255288
Weights right now are: 
[-3.55904927  0.82344951 -2.85977525  0.72148013  3.02757901 -0.95744494
  3.02448229 -1.28753625  3.62810071 -2.65842747  3.96593899  1.0654748
  1.21191682  7.89420601  2.59209907  7.48281478  2.84046116 -3.43352592
  2.94874299 -0.88032982  1.73992125  8.68264988  2.42183561  7.82695731
 -1.4522305   5.20030283 -2.34243472 -3.80765855  2.98829031 -3.80584131
  3.28485099  5.13336631 -3.75773702  4.6998357  -2.68444448 -2.24494637
  4.00269251  5.3821163   3.12074326  5.11256187  5.63257043  6.90701671
  0.21283559  8.93691269 -4.9429491  -2.0126335   4.47129562 -2.87358263
  5.94829697  5.99316175 -0.10442541  7.9884445   5.05579424  8.31362008
 -0.72397035  9.9552213   0.95346352 -2.57340435 -4.51041485 -2.51893164
  1.36926613  5.97724161 -2.75024605 -2.64687269 25.98417494]
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2843.088643119332
gradient value of function right now is: [-1.26491739e-01 -5.47621948e-02 -5.33862934e-01 -3.80536157e-02
  1.26491739e-01  5.47621948e-02  5.33862934e-01  3.80536157e-02
 -4.68297887e-01 -3.20055644e-01 -4.74089103e-01 -2.20916643e-01
  7.60553588e-02  1.05189459e-02  7.51116045e-02  6.33975864e-03
 -1.47151046e+00 -8.95269580e-01 -1.48691114e+00 -6.23339014e-01
  5.94661579e-02  7.88796306e-03  5.87187265e-02  4.71926539e-03
  9.52578455e-02  1.14609907e-01  1.57058280e-01  1.62843507e-01
 -1.32909680e-01 -1.40281521e-01 -2.12433361e-01 -2.29478014e-01
  9.21270619e-02  1.19925455e-01  1.54473630e-01  1.56255845e-01
 -3.09044624e-02 -2.39888262e-02 -4.72072595e-02 -5.40003372e-02
  1.00411863e-02  2.34905206e-01  9.35504045e-01  7.06659670e-02
 -3.29514274e-02 -4.89921278e-01 -1.63305107e+00 -1.64812425e-01
  1.62167720e-02  3.70245167e-01  1.43199286e+00  1.11452540e-01
  2.37269907e-02  5.28971465e-01  2.12132031e+00  1.61164815e-01
 -1.20969463e-01  4.94793841e-01 -3.74718582e+00  1.22016159e+01
  1.72607959e-03 -9.75579304e-03 -1.56392647e+00  5.31137468e+00
  5.01210546e+00]
supnorm grad right now is: 12.201615906909666
Weights right now are: 
[-3.84966943e+00  8.81786305e-01 -3.11828407e+00  7.74802952e-01
  3.31819917e+00 -1.01578174e+00  3.28299111e+00 -1.34085907e+00
  3.38784717e+00 -3.01157698e+00  3.71984328e+00  7.22337005e-01
  1.20830963e+00  7.66446956e+00  2.58808530e+00  7.24952129e+00
  2.67305770e+00 -3.75961503e+00  2.77570664e+00 -1.17980453e+00
  1.73997914e+00  8.45193462e+00  2.42150392e+00  7.58849732e+00
 -1.40530247e+00  5.60040884e+00 -2.24336648e+00 -3.82378643e+00
  3.04745925e+00 -4.22166554e+00  3.27136001e+00  5.25516551e+00
 -3.68241536e+00  5.09911886e+00 -2.56281304e+00 -2.22783089e+00
  4.28247964e+00  4.89542904e+00  3.33477321e+00  5.42705667e+00
  5.45983071e+00  7.30220215e+00  2.50544767e-01  9.12672267e+00
 -5.04881086e+00 -2.31425080e+00  3.99387276e+00 -3.04891941e+00
  5.82583411e+00  6.39596170e+00  1.08852450e-02  8.20666058e+00
  4.84900518e+00  8.62574784e+00 -7.47145082e-01  1.00749739e+01
  8.44686153e-01 -2.72149761e+00 -5.00201145e+00 -2.14526861e+00
  1.38539144e+00  5.88975907e+00 -3.12395965e+00 -2.41357589e+00
  2.56920765e+01]
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2838.2849541568257
gradient value of function right now is: [-1.61654304e-01  8.50171993e-03 -8.12770137e-01  6.89639661e-03
  1.61654304e-01 -8.50171993e-03  8.12770137e-01 -6.89639661e-03
 -5.63060235e-01 -4.27101953e-01 -5.67585632e-01 -3.15630718e-01
 -6.91767410e-02  2.25622619e-02 -7.27106077e-02  1.44748239e-02
 -2.02424253e+00 -1.35198553e+00 -2.03489779e+00 -1.00919949e+00
 -5.50171839e-02  1.65996179e-02 -5.77220919e-02  1.05892924e-02
  1.11179738e-01  1.35617510e-01  1.87954191e-01  1.95626672e-01
 -1.70109809e-01 -1.69436536e-01 -2.76854602e-01 -3.06193743e-01
  1.06876223e-01  1.41555993e-01  1.82856370e-01  1.84866788e-01
 -4.94250656e-02 -3.31898442e-02 -7.64159295e-02 -9.05690928e-02
  8.69614954e-03  2.91463838e-01  1.15820718e+00  6.95694286e-02
 -2.60014406e-02 -6.63091629e-01 -1.87301180e+00 -1.74909734e-01
  1.40730916e-02  4.76482993e-01  1.79637636e+00  1.13140162e-01
  2.09582381e-02  6.79416554e-01  2.72608237e+00  1.63718021e-01
 -9.60207093e-02  4.46144926e-01 -4.16950085e+00  1.58324516e+01
  1.49223519e-03 -7.84638988e-03 -1.45368614e+00  5.68338270e+00
  8.33393482e+00]
supnorm grad right now is: 15.832451635883508
Weights right now are: 
[-3.77097626  1.10177581 -3.10866956  0.99472617  3.23950599 -1.23577125
  3.2733766  -1.56078229  3.47423482 -2.92197304  3.80810244  0.77272294
  1.04085718  7.6988968   2.41884201  7.28551255  2.67565907 -3.78055469
  2.77869136 -1.21727114  1.58025444  8.47350968  2.26026516  7.6132784
 -1.34293372  5.50139285 -2.18801806 -3.77795023  3.08191552 -4.08756141
  3.28374444  5.3022662  -3.5805967   4.99913363 -2.4806491  -2.14325769
  4.34600552  5.00549525  3.3737618   5.49196961  5.36451618  7.48478726
  0.31208467  9.14170514 -4.93898666 -2.21949172  4.13502192 -2.99605717
  5.73432674  6.54842989  0.0663134   8.22494355  4.73800827  8.7742317
 -0.71599733 10.05620627  0.84378072 -2.85706586 -5.25804903 -1.96098591
  1.39272356  5.9202808  -3.23872352 -2.39981319 25.65082895]
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2842.4217629018135
gradient value of function right now is: [-7.08125745e-02 -2.10828507e-02 -7.74128945e-01 -1.47590272e-02
  7.08125745e-02  2.10828507e-02  7.74128945e-01  1.47590272e-02
 -2.61487609e-01 -2.09410548e-01 -2.62705037e-01 -2.03942771e-01
  2.05719346e-02  5.81333220e-03  1.97804599e-02  4.06180318e-03
 -1.68265114e+00 -1.45695333e+00 -1.69490371e+00 -1.29340826e+00
  1.62835016e-02  4.62877554e-03  1.56328757e-02  3.21095147e-03
  3.49072634e-02  5.22208036e-02  7.20740159e-02  5.27858477e-02
 -4.80677988e-02 -5.76261854e-02 -9.24849072e-02 -7.68870458e-02
  3.29948575e-02  5.34410385e-02  7.01756451e-02  4.83361505e-02
 -1.36576724e-02 -1.26209233e-02 -2.35835437e-02 -2.33178479e-02
 -4.08007012e-03  1.40070827e-01  3.51271593e-01  9.85116880e-03
  2.24880995e-02 -3.48572739e-01 -6.44888878e-01  2.33811768e-02
 -6.70220152e-03  2.85909434e-01  6.67920458e-01  2.35312833e-02
 -1.01086576e-02  2.84118356e-01  7.26511716e-01  1.65434519e-02
  3.87455516e-04 -1.02844012e-01 -2.01402501e+00  6.66922095e+00
 -5.58208078e-03  1.90495191e-02 -4.71016643e-01  1.38718167e+00
 -6.04813306e+00]
supnorm grad right now is: 6.669220951137753
Weights right now are: 
[-3.32045028e+00  7.84725680e-01 -2.72413236e+00  6.83344976e-01
  2.78898001e+00 -9.18721117e-01  2.88883940e+00 -1.24940110e+00
  3.81309713e+00 -2.34289571e+00  4.15344873e+00  1.29990603e+00
  1.35124313e+00  7.72966886e+00  2.73299868e+00  7.31144416e+00
  2.83805877e+00 -3.38498905e+00  2.94509825e+00 -8.64486394e-01
  1.86771643e+00  8.48968904e+00  2.55092619e+00  7.62593470e+00
 -1.35330139e+00  4.94899400e+00 -2.22535164e+00 -3.78768920e+00
  3.10609884e+00 -3.45158477e+00  3.34008212e+00  5.33594520e+00
 -3.61576496e+00  4.44467174e+00 -2.53194111e+00 -2.17151415e+00
  4.38379001e+00  5.78456542e+00  3.43013532e+00  5.54472053e+00
  5.08796273e+00  7.50949817e+00  3.01698933e-01  8.97330902e+00
 -4.61910926e+00 -1.72372444e+00  4.77478712e+00 -2.70590832e+00
  5.46008210e+00  6.50204539e+00  1.70519140e-02  8.04074170e+00
  4.44391409e+00  8.79356656e+00 -7.35888931e-01  9.86932716e+00
  9.24296692e-01 -3.19316284e+00 -5.50954912e+00 -1.79567501e+00
  1.39442848e+00  6.08005582e+00 -3.25285064e+00 -2.54862714e+00
  2.51646290e+01]
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2844.0279769436374
gradient value of function right now is: [ 1.28475501e-01 -3.83975188e-02  5.70655414e-01 -2.88500906e-02
 -1.28475501e-01  3.83975188e-02 -5.70655414e-01  2.88500906e-02
  2.49639307e-01  3.15285130e-01  2.54689738e-01  1.92858621e-01
  6.49928796e-02 -2.94657334e-03  6.48131182e-02 -2.21252399e-03
  9.50341346e-01  1.01604090e+00  9.64204227e-01  7.19017976e-01
  5.53881567e-02 -2.13240599e-03  5.52342405e-02 -1.61801163e-03
 -2.84410268e-02 -1.29912505e-01 -5.50126941e-02 -4.76503874e-02
  2.09947099e-02  1.11528491e-01  4.30474741e-02  3.54445809e-02
 -3.18126449e-02 -1.38468855e-01 -6.00638974e-02 -5.32695589e-02
  1.20363323e-03 -3.47949967e-03  2.39878289e-03  2.50823812e-03
 -1.19244674e-02 -5.87371426e-02 -2.18375606e-01 -2.97563347e-02
  7.69049111e-02  8.31276667e-01  1.51418926e+00  3.56633642e-01
 -2.23541407e-02 -1.21581076e-01 -4.10679540e-01 -5.84767868e-02
 -2.63813261e-02 -1.19906183e-01 -4.62999457e-01 -6.34197378e-02
  1.19105220e-01 -5.22999432e-01  6.04100068e-01 -2.10502394e+00
 -3.10858880e-02  1.00084258e-01  7.09733531e-01 -2.07135212e+00
 -1.30101113e+01]
supnorm grad right now is: 13.010111313518541
Weights right now are: 
[-3.23387766  0.78555596 -2.38235602  0.68591398  2.7024074  -0.91955139
  2.54706306 -1.25197009  4.03553474 -2.22392325  4.37202075  1.56815635
  1.38509109  7.54546502  2.76832183  7.14603749  3.18726944 -2.99471085
  3.29504425 -0.40348975  1.8975519   8.29111138  2.58214632  7.44559828
 -1.43205619  5.06651328 -2.32965535 -3.83509201  3.29371591 -3.49278094
  3.53277434  5.50572064 -3.6626286   4.57745613 -2.60968883 -2.18395271
  4.66628579  6.30562267  3.70527872  5.82756256  5.20051624  7.24410735
  0.15571992  8.99341026 -5.17166564 -1.90490121  4.73429481 -3.26397871
  5.56830763  6.18226211 -0.14440919  8.01984294  4.55048868  8.5793926
 -0.85312231  9.91562964  0.64185616 -2.99216722 -5.20246022 -2.14912281
  1.54749543  5.11113249 -3.22821243 -2.59685924 25.1155397 ]
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2848.99281416542
gradient value of function right now is: [ 2.38104232e-02 -8.42220633e-02  1.19780957e-01 -6.27140276e-02
 -2.38104232e-02  8.42220633e-02 -1.19780957e-01  6.27140276e-02
  5.47058677e-02  1.10423405e-02  5.31851029e-02  4.24660757e-02
  1.46391366e-01  4.13371042e-03  1.44099375e-01  2.10379203e-03
  2.52975882e-01  2.38282870e-01  2.54870380e-01  2.05450667e-01
  1.23238381e-01  3.57806794e-03  1.21323986e-01  1.83331804e-03
 -2.00091894e-02 -8.06239723e-05 -3.69274000e-02 -3.46865955e-02
 -8.00603196e-03 -1.24850715e-03 -5.88678389e-03 -1.50052326e-02
 -3.11130373e-02  2.02882085e-03 -5.34102511e-02 -5.47028773e-02
 -8.43331709e-03  4.90019577e-03 -1.30760102e-02 -1.45673147e-02
 -9.55608415e-03 -1.80895293e-02 -6.71092360e-02 -1.30060112e-02
  2.69141615e-02 -2.28234836e-01  5.08949971e-02 -1.08598971e-01
 -1.82049183e-02 -4.44243893e-02 -1.43555300e-01 -2.66513143e-02
 -2.07776069e-02 -3.00933183e-02 -9.58807076e-02 -2.64184029e-02
 -6.17272012e-02 -2.28346010e-01 -5.93071308e-02 -2.33495953e-01
  7.03487039e-02 -3.70859856e-02 -2.19930952e-01 -1.15728535e-01
  6.48816200e-01]
supnorm grad right now is: 0.6488162001493578
Weights right now are: 
[-3.35893499  0.95559721 -2.2168948   0.86543561  2.82746472 -1.08959264
  2.38160184 -1.43149173  4.14085396 -2.27030305  4.47158554  1.74195354
  1.26906393  7.63274528  2.65199351  7.24835387  3.40843533 -2.73488677
  3.51684711 -0.07166196  1.77767171  8.36559156  2.46205691  7.53949444
 -1.3190014   5.27894471 -2.27363418 -3.68614935  3.32021033 -3.65367961
  3.57722102  5.50294987 -3.51148209  4.81084594 -2.52409925 -1.99809106
  4.7412551   6.81939097  3.76302135  5.89512927  5.36766402  7.1560392
  0.22002248  9.05771128 -5.79501668 -2.19036456  4.60591781 -3.84024351
  5.70582396  6.00971303 -0.13579655  8.00809146  4.72019606  8.54094767
 -0.77107657 10.01760631  0.05108251 -2.64957134 -5.03982028 -2.26792481
  1.93985347  3.88515902 -3.23005826 -2.56952568 25.91055141]
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2848.9980053091526
gradient value of function right now is: [-3.08858909e-02 -1.04289044e-01 -1.60705754e-01 -7.77177847e-02
  3.08858909e-02  1.04289044e-01  1.60705754e-01  7.77177847e-02
 -1.13606559e-01 -1.16629928e-01 -1.15551674e-01 -5.86025258e-02
  1.65018308e-01  1.72691087e-02  1.56828112e-01  9.31776729e-03
 -3.44346782e-01 -2.36257904e-01 -3.44826724e-01 -1.33840195e-01
  1.39257951e-01  1.42230777e-02  1.32371548e-01  7.61384505e-03
  6.02594861e-03  5.11229200e-02  1.58281071e-02  5.47246702e-03
 -4.98948646e-02 -4.21732328e-02 -7.84540245e-02 -9.21026935e-02
 -9.98702980e-03  5.42118745e-02 -8.10933494e-03 -2.74552169e-02
 -1.79104565e-02  6.36202596e-03 -2.61696386e-02 -3.41584480e-02
 -3.48895959e-03  4.01037626e-02  1.88408196e-01  3.43993963e-03
 -5.91482921e-02 -3.59496965e-01 -5.43366117e-01 -1.69292982e-01
 -5.46458514e-03  7.33617494e-02  3.12041296e-01  7.36654684e-03
 -7.77806986e-03  9.93926779e-02  4.89189787e-01  8.77788661e-03
 -2.10177125e-01  2.87495934e-01 -9.65507455e-01  2.71899594e+00
  9.39671528e-02 -1.65755203e-01 -4.36831109e-01  9.60117914e-01
  3.57884286e+00]
supnorm grad right now is: 3.5788428642422887
Weights right now are: 
[-3.41296594  1.02688405 -2.30331497  0.94388134  2.88149568 -1.16087949
  2.46802201 -1.50993746  4.12190981 -2.1459006   4.45810295  1.79825539
  1.22444146  7.69394801  2.60888903  7.33587957  3.2654514  -2.77396397
  3.37652026 -0.17903281  1.72887263  8.41372889  2.41481097  7.61742214
 -1.01777625  5.045823   -1.95832621 -3.3985467   3.2270343  -3.45472024
  3.44463879  5.4179222  -3.16416345  4.58176675 -2.17156504 -1.66080858
  4.85582343  6.49959748  3.87396816  5.99904089  5.71248271  7.39222109
  0.44881858  9.35300058 -6.11586644 -1.78642978  4.89501147 -3.60460618
  6.07543308  6.25591764  0.11472028  8.29544031  5.0343753   8.76382871
 -0.57099726 10.30102376  0.07969517 -2.45979535 -4.98701783 -2.22348295
  1.84895162  3.93243629 -2.96419048 -2.6563714  26.03203152]
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2847.6847373882706
gradient value of function right now is: [ 1.14849840e-01 -5.97505426e-02  6.09161386e-01 -4.49174305e-02
 -1.14849840e-01  5.97505426e-02 -6.09161386e-01  4.49174305e-02
  2.48839776e-01  3.15853740e-01  2.53055426e-01  1.98970494e-01
  7.37806193e-02 -9.17220485e-05  7.27647615e-02 -5.45285353e-04
  1.05761592e+00  1.15062051e+00  1.06967482e+00  8.49127839e-01
  6.49508424e-02  1.28124277e-04  6.40636701e-02 -3.31179954e-04
 -2.85364635e-02 -1.25333696e-01 -5.45510625e-02 -4.97016818e-02
  1.47259839e-02  9.87857367e-02  3.23156089e-02  2.49141503e-02
 -3.40811442e-02 -1.33802096e-01 -6.27267167e-02 -5.97739796e-02
 -2.26335052e-03 -9.82407956e-03 -3.48225977e-03 -3.85799787e-03
 -1.28283792e-02 -4.64111457e-02 -1.70501789e-01 -2.22451161e-02
  6.18244342e-02  8.80592097e-01  1.41153138e+00  3.38140195e-01
 -2.26111069e-02 -9.67898793e-02 -3.23542110e-01 -4.23079771e-02
 -2.99276237e-02 -9.71608172e-02 -3.71553613e-01 -4.96495553e-02
  1.87725150e-01 -6.17144471e-01  6.43272444e-01 -1.81646227e+00
 -1.26364761e-01  2.32133509e-01  8.57951367e-01 -1.97819528e+00
 -6.49840968e+00]
supnorm grad right now is: 6.498409678649823
Weights right now are: 
[-3.46419378  0.64221824 -2.34852256  0.54403319  2.93272352 -0.77621368
  2.5132296  -1.11008931  4.14775795 -2.10303294  4.48447274  1.84640529
  1.59704567  7.54879744  2.98416582  7.23430727  3.2154413  -2.77232429
  3.32771154 -0.20570528  2.10530914  8.24354291  2.79359372  7.49097433
 -1.0570811   5.00204815 -1.96079225 -3.43088647  3.32175729 -3.34791919
  3.52910952  5.52382696 -3.27852445  4.54137037 -2.21713471 -1.75671135
  5.06937381  6.67799785  4.08911908  6.2249717   5.60775732  7.37738065
  0.36506183  9.32401886 -5.9849226  -1.85683594  4.980466   -3.63240762
  5.97888333  6.25745899  0.0750011   8.2714835   4.90488736  8.73252463
 -0.64076427 10.25591639  0.17211844 -2.6934507  -4.94540031 -2.30323967
  1.92237068  4.01628997 -3.02675457 -2.70938029 25.65955768]
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2845.221215499798
gradient value of function right now is: [ 2.03124526e-01 -7.07543875e-02  1.26413633e+00 -5.26581538e-02
 -2.03124526e-01  7.07543875e-02 -1.26413633e+00  5.26581538e-02
  3.79357714e-01  5.99053526e-01  3.89011437e-01  3.73783101e-01
  5.78395879e-02  5.06253424e-03  5.65566530e-02  2.32965396e-03
  1.78190102e+00  2.51456287e+00  1.81831486e+00  1.79556025e+00
  5.23913695e-02  4.63493939e-03  5.12344490e-02  2.14638692e-03
  1.20794028e-02 -3.21680495e-01  1.55474973e-02  2.27373357e-02
 -1.57704444e-02  2.33561759e-01 -2.10453299e-02 -3.01741176e-02
  9.02594123e-03 -3.42139905e-01  1.09781993e-02  1.65526683e-02
 -5.11300677e-03 -3.54617362e-02 -7.50655607e-03 -9.79616860e-03
 -5.22143691e-03  1.07134532e-02  9.35973772e-02 -4.67173114e-03
  2.17947004e-01  2.54578820e+00  3.24312897e+00  1.20932218e+00
 -1.06199461e-02  9.00265064e-03  1.10965085e-01 -1.17980878e-02
 -1.24671786e-02  2.64262826e-02  2.14841618e-01 -1.06766454e-02
  6.50288456e-01 -1.04523125e+00  1.24992712e-01  3.37925003e-01
 -6.36530014e-01  8.57656211e-01  1.61639547e+00 -2.20031269e+00
 -6.53951700e-01]
supnorm grad right now is: 3.2431289735698505
Weights right now are: 
[-3.3555785   0.47262933 -2.22824097  0.37220487  2.82410823 -0.60662477
  2.39294801 -0.93826099  4.25834885 -1.85433395  4.60000466  2.07965576
  1.74701123  7.6588539   3.13519492  7.35644176  3.21188328 -2.57869671
  3.32895483 -0.06125131  2.25484056  8.3496527   2.94409234  7.61272543
 -0.75110163  4.6959244  -1.65641998 -3.11865292  3.1077658  -3.02279167
  3.3209949   5.3064676  -3.01293903  4.23774749 -1.93664394 -1.4809053
  5.08707386  6.50653344  4.11256043  6.24008639  5.88812014  7.65611836
  0.64701277  9.65840075 -5.7214797  -1.52985071  5.30570992 -3.37006832
  6.24565797  6.49784876  0.35133027  8.56684074  5.16129644  9.01644987
 -0.35164289 10.58833681  0.32203653 -2.79309206 -4.99300412 -2.20087565
  1.74880306  4.30121925 -2.92030294 -2.71015908 26.17318269]
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2848.5163453810733
gradient value of function right now is: [-4.94950651e-03 -5.08822617e-02 -2.26792916e-02 -3.92785367e-02
  4.94950650e-03  5.08822617e-02  2.26792916e-02  3.92785367e-02
 -4.45006314e-03 -4.45246338e-02 -6.06903908e-03 -2.43775221e-02
  9.08094379e-02 -3.58397486e-03  8.99656400e-02 -2.30157362e-03
  1.00039313e-01 -6.35304760e-02  9.46467788e-02 -2.28029995e-02
  8.02393099e-02 -2.95413235e-03  7.94929375e-02 -1.90024030e-03
 -2.38723916e-02  2.19601587e-02 -3.77290840e-02 -4.40307703e-02
  1.80049040e-02 -2.10690569e-02  2.98499998e-02  3.38400349e-02
 -2.91806745e-02  2.35399238e-02 -4.45695521e-02 -5.33646331e-02
  3.37535393e-03  6.73038092e-04  5.54995891e-03  6.50191605e-03
 -8.78577642e-03 -3.56352605e-02 -1.94220908e-01 -1.46834887e-02
 -1.58376271e-02 -1.70025465e-01 -2.51541956e-01 -8.02289457e-02
 -1.14626612e-02 -5.38867448e-02 -2.81338464e-01 -2.06616955e-02
 -2.02263918e-02 -7.86655088e-02 -4.25022371e-01 -3.31866978e-02
 -4.48568150e-02 -1.28235137e-01  5.45181468e-01 -1.44238131e+00
  7.39453506e-02 -1.07512342e-01  1.58988689e-01 -6.63704577e-01
  2.68626266e+00]
supnorm grad right now is: 2.6862626586058154
Weights right now are: 
[-3.69654708  0.81698114 -2.73913447  0.73071732  3.16507682 -0.95097658
  2.90384151 -1.29677344  4.04694224 -2.19471937  4.38606486  1.67204511
  1.60593222  7.38399671  2.99529166  7.17351294  2.92810604 -3.18405953
  3.03748277 -0.66059177  2.10067039  8.03427173  2.7913008   7.40222042
 -1.25099847  5.22139633 -2.12190003 -3.60890101  3.56195916 -3.5129003
  3.73948895  5.7584476  -3.44463085  4.75363233 -2.35512475 -1.8989535
  5.31220315  6.73160826  4.31848659  6.47057167  6.13395966  7.22448016
  0.19848594  9.58784522 -6.29601859 -1.98502222  4.84378896 -3.85058655
  6.50515495  6.1247731  -0.05506434  8.52280438  5.39746896  8.56987607
 -0.79034306 10.52267648 -0.21490513 -2.38327557 -4.53541482 -2.69124941
  2.3423637   3.71603991 -2.96821808 -2.76460637 26.05126302]
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2844.704643737091
gradient value of function right now is: [-0.07332744 -0.12307406 -0.49444099 -0.09578214  0.07332744  0.12307406
  0.49444099  0.09578214 -0.28868374 -0.17726027 -0.28482735 -0.09516649
  0.21077066  0.0224425   0.19818704  0.00950322 -1.44758673 -0.68566278
 -1.41690744 -0.3797103   0.18626376  0.01924965  0.17518653  0.00808768
  0.0854966   0.09642274  0.13293709  0.16814267 -0.11934362 -0.11029905
 -0.180145   -0.23502276  0.07473593  0.09960766  0.11870068  0.14478636
 -0.02382914 -0.01325979 -0.03486457 -0.04694018  0.06084381  0.08070828
  0.7644668   0.06358054 -0.13812885 -0.21460172 -1.24755163 -0.16294836
  0.09174933  0.12505874  1.12045577  0.09781589  0.1548794   0.20174058
  1.90706155  0.15990461 -0.9430756   3.79746421 -1.27446524  4.62344944
  0.07364285 -0.26292585 -1.38059357  5.15174338  7.55637103]
supnorm grad right now is: 7.5563710344411925
Weights right now are: 
[-3.97590394  1.10775047 -3.05687566  1.04021402  3.44443367 -1.2417459
  3.2215827  -1.60627014  3.81439208 -2.46783915  4.15253732  1.42566445
  1.42385133  7.24538288  2.81699387  7.11958364  2.67224046 -3.52037609
  2.78010736 -0.95707363  1.90606143  7.85795582  2.60079549  7.32453898
 -1.04878806  5.58970258 -1.95701661 -3.36518902  3.49316775 -3.8806555
  3.66831035  5.65797365 -3.162041    5.12145759 -2.1292584  -1.56857795
  5.39437547  6.84992539  4.36918727  6.54238903  6.95450185  7.32898103
  0.40481846  9.98419809 -6.74564696 -2.25631257  4.49904433 -4.15088951
  7.20469434  6.20459226  0.11552175  8.84752225  6.23676665  8.68101161
 -0.56524891 10.93779764 -0.50561579 -1.81575826 -4.61405916 -2.61715552
  2.63070295  3.30692376 -3.20822579 -2.43753645 25.88352577]
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -2840.441592857326
gradient value of function right now is: [ 6.74809934e-02  4.32931294e-01  4.40324572e-01  3.49018165e-01
 -6.74809934e-02 -4.32931294e-01 -4.40324572e-01 -3.49018165e-01
  2.58391104e-01  1.41194983e-01  2.54935344e-01  7.07264249e-02
 -1.15579074e+00 -8.46924768e-02 -1.11639570e+00 -3.85504788e-02
  1.32853921e+00  6.31969497e-01  1.30712362e+00  3.26403889e-01
 -1.04509580e+00 -7.40826095e-02 -1.00941476e+00 -3.35247675e-02
 -4.47796621e-02 -1.03543362e-01 -8.43265391e-02 -7.24284536e-02
  1.81537934e-01  1.37032899e-01  2.78341183e-01  3.18699432e-01
  2.09246675e-02 -9.49300040e-02  6.07365609e-03  4.69360136e-02
  5.41936958e-02  2.55611336e-02  8.14144932e-02  9.58903162e-02
 -6.46842559e-02 -1.94370643e-01 -9.31180668e-01 -1.05451027e-01
  1.21291688e-01  2.89694889e-01  1.16039327e+00  1.75426525e-01
 -9.97813017e-02 -3.08781488e-01 -1.46661557e+00 -1.65897943e-01
 -1.60264260e-01 -4.45646022e-01 -2.30578993e+00 -2.46932220e-01
  8.80849509e-01 -2.59511411e+00  3.34193314e+00 -7.36555789e+00
 -2.51246486e-01  6.33803330e-01  2.49347017e+00 -5.89816388e+00
 -1.26180135e+01]
supnorm grad right now is: 12.618013490372125
Weights right now are: 
[-3.83971861  1.58686836 -2.89104938  1.55323205  3.30824835 -1.7208638
  3.05575642 -2.11928817  4.0589383  -2.37558663  4.38929256  1.50335094
  0.95763327  6.62517212  2.35898413  6.72999538  2.98040622 -3.42635764
  3.07862847 -0.8920453   1.41790907  7.18395876  2.12148125  6.89398651
 -1.45992058  5.50297375 -2.32141947 -3.78604551  4.08738021 -3.74433752
  4.18423229  6.26591926 -3.45107273  5.03923395 -2.40378674 -1.86496392
  5.98793851  7.02597199  4.90960846  7.16639005  6.70153369  7.04229697
 -0.08466572  9.62707403 -6.72065036 -2.22752131  4.61598491 -4.1148942
  6.96642781  5.96301274 -0.31267745  8.52986412  6.0188764   8.38872408
 -1.05840202 10.59742797  0.10307791 -2.30888088 -4.23121478 -3.00269421
  2.51957855  3.41837088 -2.76003589 -2.93615652 25.55406531]
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -2848.29590995742
gradient value of function right now is: [ 8.07615987e-03  7.54562208e-02  8.45629487e-02  5.82916928e-02
 -8.07615987e-03 -7.54562208e-02 -8.45629487e-02 -5.82916928e-02
  3.66505632e-02 -6.11094356e-03  3.66471444e-02 -8.00439583e-04
 -2.42338338e-01 -2.07779870e-02 -2.42586171e-01 -1.62414448e-02
  4.24136357e-01  8.28595543e-02  4.28117328e-01  7.51331323e-02
 -2.09510975e-01 -1.72866825e-02 -2.09678452e-01 -1.35406872e-02
 -3.84991246e-02  2.28392564e-03 -6.33545702e-02 -6.81987304e-02
  4.61767589e-02  2.85877524e-03  8.10608383e-02  8.58015106e-02
 -4.39422366e-02  3.56848768e-03 -6.93211863e-02 -7.74656745e-02
  2.07536126e-02  4.99996918e-03  3.57739940e-02  4.00111186e-02
 -2.12716361e-02 -7.24381587e-02 -4.44545141e-01 -3.32023675e-02
 -8.51426268e-03 -3.41306182e-02 -2.35400560e-02 -2.29696410e-02
 -3.05455105e-02 -1.09034253e-01 -6.65997231e-01 -4.84621437e-02
 -5.08856477e-02 -1.65376981e-01 -1.02492934e+00 -7.78517585e-02
  8.97715820e-02 -6.40273644e-01  1.17287392e+00 -3.13327591e+00
  1.65976253e-02 -2.56882140e-02  5.70059756e-01 -1.85386403e+00
  1.88345269e+00]
supnorm grad right now is: 3.1332759064983087
Weights right now are: 
[-3.66815966  1.75045062 -2.72463421  1.72122175  3.1366894  -1.88444606
  2.88934125 -2.28727787  4.24264043 -2.15511362  4.57626479  1.6670054
  0.67447255  8.55230884  2.04882526  8.50849689  3.09262609 -3.23015767
  3.19540788 -0.78013981  1.13258293  9.18183567  1.80837977  8.76648479
 -1.54329414  5.18092893 -2.40677569 -3.88028764  3.41520863 -3.44312124
  3.59834344  5.59738039 -3.87348171  4.69499736 -2.79995871 -2.31882182
  4.63798946  6.8827557   3.60768143  5.8272092   6.93190685  7.3126979
  0.33578083  9.8300058  -6.30921731 -2.08694754  4.9646541  -3.89744628
  7.11111692  6.1747849   0.03380532  8.65134895  6.35930611  8.70222007
 -0.59981719 10.88565684  0.51395911 -2.51690718 -4.40912922 -2.88483702
  2.22605221  3.85513011 -2.7558411  -2.93800734 26.18500615]
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2847.8870512774843
gradient value of function right now is: [-3.51292199e-02  1.02750127e-02 -4.50031239e-01  1.13326645e-02
  3.51292199e-02 -1.02750127e-02  4.50031239e-01 -1.13326645e-02
 -1.19872454e-01 -1.23324953e-01 -1.22831977e-01 -8.63174340e-02
 -6.03997673e-02  2.14948523e-02 -5.73924625e-02  1.89529311e-02
 -9.67330161e-01 -8.14674041e-01 -9.86768971e-01 -6.41116940e-01
 -6.40853451e-02  1.89618806e-02 -6.15041933e-02  1.67223910e-02
  2.20391473e-02  6.67763446e-02  3.73440364e-02  3.64316079e-02
 -7.56210997e-02 -6.59493344e-02 -1.15666055e-01 -1.43622615e-01
  1.31689438e-02  7.00178094e-02  2.48255202e-02  1.88601250e-02
 -5.07171265e-02 -8.59505969e-03 -7.40807809e-02 -9.85076458e-02
  1.19684494e-02  4.35552012e-02  4.21951665e-01  2.11717085e-02
 -6.38302181e-02 -2.98373870e-01 -7.02470073e-01 -1.55875937e-01
  1.83214955e-02  7.13338376e-02  5.84651859e-01  3.53307870e-02
  2.80344035e-02  9.98940478e-02  1.04635671e+00  4.80027913e-02
 -3.01035793e-01  9.04596473e-01 -6.55668674e-01  2.29370967e+00
  9.14006208e-02 -1.81021249e-01 -5.67122156e-01  1.77612054e+00
  8.75939956e+00]
supnorm grad right now is: 8.759399557004162
Weights right now are: 
[-3.61058715  1.34067749 -2.66265242  1.30120753  3.07911689 -1.47467292
  2.82735946 -1.86726365  4.27729468 -2.02717309  4.61425842  1.78699498
  1.07883794  9.57638637  2.4474366   9.53110222  3.03342625 -3.15224608
  3.13970052 -0.73092324  1.53916258 10.20984906  2.20893674  9.80057638
 -1.48089533  5.04616788 -2.311645   -3.83682258  2.78842408 -3.33326015
  3.02523066  5.00040847 -4.05627443  4.54740281 -2.91476163 -2.52275107
  3.70257333  6.75197347  2.70396394  4.92352634  7.41654546  7.68609473
  0.7689905  10.25435346 -6.2737596  -1.91115687  5.11313372 -3.75501455
  7.60470103  6.50262098  0.44377239  9.04490582  6.84913445  9.08628394
 -0.15557799 11.32427653  0.52175066 -2.35187116 -4.64943591 -2.67300466
  2.04984887  3.98038933 -2.83101788 -2.78969466 26.1856672 ]
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2835.9650275268004
gradient value of function right now is: [ 2.97787972e-01 -4.94143617e-02  2.93585159e+00 -3.89292833e-02
 -2.97787972e-01  4.94143617e-02 -2.93585159e+00  3.89292833e-02
  5.04033435e-01  7.65986978e-01  5.17073660e-01  4.37210439e-01
  8.89493059e-02  4.68615335e-03  9.08167260e-02  3.90734266e-03
  4.58742904e+00  5.31675058e+00  4.65895818e+00  4.11601139e+00
  8.15935701e-02  4.48015741e-03  8.33132899e-02  3.72467578e-03
 -3.10005727e-02 -5.09874619e-01 -5.96833799e-02 -5.34128184e-02
 -1.28589653e-03  3.65494857e-01  1.09848830e-02 -6.33572657e-03
 -3.48628196e-02 -5.45033884e-01 -6.54104373e-02 -6.05096990e-02
 -1.55811681e-02 -1.97262616e-02 -2.06465995e-02 -2.95226800e-02
 -1.25100826e-02 -4.34885706e-02 -1.32705227e-01 -1.89411137e-02
  5.55215785e-01  3.08552509e+00  4.26371005e+00  1.76771006e+00
 -2.40561046e-02 -9.47502639e-02 -3.01748646e-01 -3.97920859e-02
 -2.73844715e-02 -8.46129849e-02 -2.51139256e-01 -3.85491075e-02
  1.24256137e+00 -1.84333502e+00  1.08635041e+00 -2.01760391e+00
 -1.26913707e+00  1.38801839e+00  2.13028085e+00 -2.86685064e+00
 -1.13287231e+01]
supnorm grad right now is: 11.328723086668251
Weights right now are: 
[-3.18691757  0.96781412 -2.07081938  0.92004316  2.65544731 -1.10180955
  2.23552642 -1.48609928  4.66483135 -1.4914228   5.00904867  2.27957729
  1.42213606  9.584257    2.79329039  9.46819502  3.38435205 -2.56752672
  3.49473212 -0.20791336  1.88593322 10.21465592  2.55830532  9.72956161
 -1.43574667  4.42093069 -2.24474277 -3.77928224  2.64508052 -2.68960921
  2.90918651  4.88201173 -4.04100133  3.91857804 -2.86988893 -2.4906585
  3.64341944  6.97434455  2.68668511  4.93176557  7.66730128  7.77189759
  0.87296523 10.41792803 -5.82553846 -1.2796756   5.69260356 -3.11994498
  7.87260583  6.56158073  0.55374098  9.18792029  7.09971639  9.18518308
 -0.06297781 11.502968    0.98654534 -2.50201736 -4.53450503 -2.74300028
  1.34123539  4.59773645 -2.50006758 -2.96171132 25.98780094]
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2843.731557769009
gradient value of function right now is: [ 2.55414995e-02  1.53147952e-02  1.65523045e-01  1.38808878e-02
 -2.55414995e-02 -1.53147952e-02 -1.65523045e-01 -1.38808878e-02
  1.54658170e-03  5.64762846e-02  1.06507356e-04  3.60303489e-02
 -3.52077346e-02  6.73653084e-03 -3.55762919e-02  5.75616226e-03
 -6.30093181e-01  5.16727732e-01 -6.56540085e-01  2.97903490e-01
 -3.60924680e-02  5.85627255e-03 -3.65105431e-02  5.02964991e-03
  1.04306082e-01 -7.61098075e-02  1.62651871e-01  2.01110597e-01
 -9.36200346e-02  8.29648929e-02 -1.49464629e-01 -1.82241210e-01
  1.04994390e-01 -7.97756755e-02  1.63798568e-01  2.01639469e-01
 -2.75249940e-02  2.25699542e-02 -4.35288115e-02 -5.46473736e-02
  1.28368851e-02  1.07532404e-01  7.71526400e-01  2.40669963e-02
  6.19112507e-02  5.02389941e-01  5.92544228e-01  2.65548773e-01
  1.77948838e-02  1.67927421e-01  1.10266198e+00  3.68266728e-02
  3.23137397e-02  2.60824935e-01  1.92775238e+00  5.87681984e-02
 -1.71408633e-02  5.16956583e-01 -1.64199003e+00  5.55936554e+00
 -9.60721191e-02  1.54898146e-01 -2.29589050e-01  1.56183970e+00
  7.11747451e+00]
supnorm grad right now is: 7.117474509688924
Weights right now are: 
[-3.3861906   0.86232024 -2.46471988  0.81194471  2.85472034 -0.99631568
  2.62942692 -1.37800083  4.62685205 -1.48642112  4.97596803  2.23577192
  1.51267693  9.5345609   2.88455547  9.38552654  2.83372237 -2.9328854
  2.94969083 -0.80084633  1.97826877 10.16205933  2.65139573  9.64079181
 -1.08684108  4.17579334 -1.86198729 -3.43726928  2.42033447 -2.39912436
  2.65799039  4.67501058 -3.71381691  3.68169766 -2.49936041 -2.16634182
  3.65442476  7.21066264  2.69414507  4.96815251  7.94344264  8.10439909
  1.17783377 10.70225346 -5.79898733 -0.85487131  5.95450407 -2.66424036
  8.1758116   6.90510787  0.89004107  9.48149955  7.36649908  9.51020145
  0.22499762 11.78473444  1.28112813 -2.48763251 -4.7220732  -2.52289046
  0.71877811  5.05309159 -2.26222004 -2.91476836 26.33154085]
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2847.435356600408
gradient value of function right now is: [ 4.16060600e-03 -6.82220271e-02 -3.45933773e-01 -5.41658198e-02
 -4.16060600e-03  6.82220271e-02  3.45933773e-01  5.41658198e-02
 -1.33695801e-02  2.82615407e-03 -1.32399416e-02 -9.15792373e-03
  1.14926448e-01  2.05450093e-03  1.16207756e-01  1.01777147e-03
 -8.62585202e-01 -4.59621631e-01 -8.69450046e-01 -4.24415715e-01
  1.07811247e-01  1.97721902e-03  1.09015716e-01  9.94299079e-04
  8.36552491e-03  6.15361929e-03  1.17684706e-02  1.55037239e-02
 -2.34193705e-02 -8.93101892e-03 -3.07887652e-02 -4.40531144e-02
  5.03130721e-03  5.41131103e-03  8.18268351e-03  9.18450067e-03
 -1.13486233e-02 -5.76789271e-03 -1.49701600e-02 -2.13405041e-02
  1.16674427e-02  1.00988649e-02  1.24827825e-01  1.48160009e-02
 -4.00190004e-03 -2.76337427e-02 -3.56308902e-02 -1.72980970e-02
  1.54557953e-02  1.97948963e-02  1.64150959e-01  2.09765338e-02
  3.14081467e-02  2.62124645e-02  2.97188793e-01  3.88508478e-02
 -8.81105655e-02  3.72354797e-01  3.67308589e-01 -3.43603327e-01
  1.80772457e-02 -1.01637215e-02 -1.06286301e-01  5.40747398e-01
  6.67695581e+00]
supnorm grad right now is: 6.676955809253265
Weights right now are: 
[-3.4449779   0.825557   -3.0447971   0.77197604  2.91350764 -0.95955243
  3.20950414 -1.33803216  4.629398   -1.44743655  4.98714267  2.15238373
  1.54331351  9.43247159  2.91562852  9.24491213  2.2902925  -3.53955773
  2.42390109 -1.72906261  2.01100133 10.05771698  2.6846136   9.49408706
 -1.61083683  3.8384339  -2.26404736 -4.02416201  2.89010917 -2.05324528
  3.05009036  5.20461717 -4.19463642  3.34772916 -2.87549809 -2.70724219
  4.16672379  7.2959466   3.18939351  5.53982914  8.00878664  7.85320125
  0.67537388 10.70285109 -5.35686112 -0.47183392  6.2314737  -2.195976
  8.28672919  6.78122893  0.52472963  9.53736854  7.42629414  9.18343109
 -0.33464196 11.76176012  1.67594127 -2.66119735 -4.33986166 -2.97596144
  0.25186121  5.43671469 -1.96298796 -3.09865563 26.22873427]
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2845.7030356377263
gradient value of function right now is: [ 5.44175799e-03 -4.23724038e-02  1.78309530e-01 -3.47980807e-02
 -5.44175799e-03  4.23724038e-02 -1.78309530e-01  3.47980807e-02
  9.22477725e-03  7.56474368e-03  9.53564327e-03  7.79108292e-04
  9.66185965e-02 -5.31996341e-03  9.67561877e-02 -4.26437490e-03
  9.34565212e-01  1.85375266e-01  9.51991141e-01  1.47234456e-01
  9.08881541e-02 -4.63797979e-03  9.10401755e-02 -3.72336935e-03
 -5.62947418e-02 -1.29147388e-03 -7.65400595e-02 -1.08336583e-01
  7.93681295e-02  1.38679983e-03  1.08741814e-01  1.53471681e-01
 -5.74630979e-02 -1.76400112e-03 -7.77537987e-02 -1.10126152e-01
  3.88987789e-02 -5.60346145e-04  5.30857977e-02  7.50469281e-02
 -1.44354892e-02 -9.19457256e-02 -7.06726273e-01 -3.85561007e-02
  5.65292504e-04  1.34201233e-03  4.87396862e-03  1.28289958e-03
 -1.56433402e-02 -1.13396996e-01 -8.48752478e-01 -4.58671134e-02
 -3.41265644e-02 -2.11744740e-01 -1.66437020e+00 -8.92073213e-02
  2.30684975e-01 -6.25178067e-01  2.09648844e+00 -3.73625445e+00
 -2.15936509e-03 -9.10741816e-04  1.25770464e+00 -2.41451872e+00
  2.85908952e-01]
supnorm grad right now is: 3.73625445271145
Weights right now are: 
[-3.55178079  1.0267902  -3.31970949  0.97052867  3.02031052 -1.16078564
  3.48441653 -1.53658479  4.58225322 -1.52537062  4.9406567   2.07232658
  1.43361595  9.34083834  2.80436377  9.14333936  2.24563088 -3.79476788
  2.38411911 -2.03607931  1.89830148  9.95985092  2.57031406  9.38425098
 -1.86346694  3.87471803 -2.49773354 -4.25479348  3.185961   -2.13894981
  3.33428546  5.47308815 -4.40295029  3.38799243 -3.07872236 -2.89441857
  4.49122415  6.93250077  3.53076081  5.82893015  8.39490718  7.50609958
  0.4309125  10.82882449 -5.34702505 -0.47708793  6.22971189 -2.20120019
  8.60168627  6.47502453  0.28917168  9.61967496  7.80748255  8.84431257
 -0.54673566 11.91008363  1.59181025 -2.46388089 -3.69174614 -3.44651222
  0.27427334  5.45877634 -1.76533851 -3.15907529 26.12955178]
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2844.4418052483506
gradient value of function right now is: [ 6.73721364e-03 -1.31226126e-02  3.31440310e-01 -1.15450263e-02
 -6.73721364e-03  1.31226126e-02 -3.31440310e-01  1.15450263e-02
  1.73185930e-02  1.08753876e-02  1.77177410e-02  4.13606868e-03
  3.64448235e-02 -6.33592563e-03  3.60190410e-02 -4.79005973e-03
  1.46775836e+00  3.79545276e-01  1.48956416e+00  3.24039420e-01
  3.60527104e-02 -5.67106252e-03  3.56803845e-02 -4.28212493e-03
 -6.75453650e-02 -3.91736074e-03 -9.59064042e-02 -1.33368570e-01
  9.97923393e-02  5.19883648e-03  1.41794668e-01  1.97178074e-01
 -6.76282780e-02 -4.46014670e-03 -9.60218072e-02 -1.33196740e-01
  4.86619356e-02  1.24046709e-03  6.90316547e-02  9.59507047e-02
 -2.85434483e-02 -1.06685856e-01 -8.76428834e-01 -5.74917624e-02
  3.51034826e-03  7.03947263e-03  1.94139686e-02  5.50838378e-03
 -3.46900902e-02 -1.40448739e-01 -1.11020739e+00 -7.37764158e-02
 -7.12391380e-02 -2.52815214e-01 -2.10960139e+00 -1.38342720e-01
  3.63601342e-01 -1.06539297e+00  2.19282088e+00 -3.90790192e+00
 -3.76176166e-03  3.09354943e-03  1.58586522e+00 -3.14094500e+00
 -2.72830013e+00]
supnorm grad right now is: 3.907901921724912
Weights right now are: 
[-3.61459627  0.95668012 -3.37516166  0.90235311  3.08312601 -1.09067555
  3.5398687  -1.46840923  4.54736824 -1.57585004  4.90492239  2.04162384
  1.49331093  9.44663523  2.86451291  9.27618619  2.22909658 -3.81002498
  2.36839531 -2.04184112  1.95531489 10.06327141  2.62774226  9.51648358
 -1.87757993  3.91913223 -2.53892969 -4.26556272  3.21377847 -2.20951519
  3.39755252  5.49811281 -4.42568564  3.4350671  -3.1265319  -2.91356122
  4.50927668  6.76707573  3.5879163   5.843149    8.52278694  7.46685106
  0.44680044 10.91895798 -5.38469294 -0.50954215  6.21094062 -2.24253089
  8.67180714  6.41873851  0.26037451  9.64796487  7.91552482  8.78412868
 -0.54791844 11.97795346  1.60925961 -2.46424248 -3.51085091 -3.58685021
  0.33691233  5.44064669 -1.70383544 -3.20210609 26.05193369]
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2846.6417172611154
gradient value of function right now is: [-1.96576747e-03 -6.95543424e-02 -4.44249000e-01 -5.57515471e-02
  1.96576747e-03  6.95543424e-02  4.44249000e-01  5.57515471e-02
 -1.89599433e-02 -6.96171169e-03 -1.91028273e-02 -9.30382754e-03
  1.23181543e-01  4.13394743e-03  1.24515673e-01  2.46156083e-03
 -1.43222983e+00 -4.83874231e-01 -1.44548614e+00 -4.45312145e-01
  1.15398818e-01  3.89866467e-03  1.16650712e-01  2.32481957e-03
  3.18588923e-02  4.66770302e-03  4.35132015e-02  6.42313914e-02
 -5.49371979e-02 -7.46943295e-03 -7.52830310e-02 -1.10835440e-01
  2.77270220e-02  4.66676070e-03  3.87851280e-02  5.58130753e-02
 -2.57573019e-02 -3.68598959e-03 -3.55445214e-02 -5.19733988e-02
  1.31836168e-02  2.52640891e-02  3.67273990e-01  2.37337732e-02
 -2.67001812e-03 -1.65704945e-02 -2.71877430e-02 -1.22208906e-02
  1.82271339e-02  3.85702226e-02  4.64595656e-01  3.39081579e-02
  3.56574379e-02  6.99032699e-02  8.93674212e-01  6.43976098e-02
 -1.42585983e-01  5.54583344e-01 -2.99153797e-03  6.32314322e-01
  1.29196031e-02 -4.32389823e-03 -2.95564855e-01  1.22470997e+00
  6.92670212e+00]
supnorm grad right now is: 6.926702119516798
Weights right now are: 
[-3.66308892  0.93310781 -3.3516391   0.88088152  3.13161865 -1.06710324
  3.51634614 -1.44693764  4.51605172 -1.60735257  4.87215899  2.04264
  1.51309003  9.59624077  2.88471454  9.46648454  2.09267191 -3.74801637
  2.22960902 -1.94546623  1.973149   10.21339044  2.64597059  9.70992763
 -1.54375024  3.95015354 -2.22380048 -3.93777447  2.93393854 -2.24919798
  3.12252378  5.23007329 -4.1073792   3.46828139 -2.8259317  -2.59965151
  4.23374044  6.74928631  3.30980291  5.58280544  8.44653509  7.90563549
  0.8113324  11.13309104 -5.42014747 -0.53760823  6.19599983 -2.27886333
  8.5552242   6.79071254  0.60319325  9.81288991  7.82405907  9.20260173
 -0.22045571 12.14712007  1.6393286  -2.55008499 -4.14827933 -3.17015372
  0.39651141  5.42287913 -2.18689751 -2.94184365 26.11651214]
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2848.2913864689913
gradient value of function right now is: [ 5.10832631e-03 -5.35909457e-02 -1.09959649e-01 -4.30040396e-02
 -5.10832631e-03  5.35909457e-02  1.09959649e-01  4.30040396e-02
 -2.28687047e-03  1.11395055e-02 -2.02065152e-03  1.12483303e-04
  9.45766301e-02  1.28997632e-03  9.53385749e-02  5.95433964e-04
 -4.47059398e-01 -1.65215564e-01 -4.49315389e-01 -1.79905677e-01
  8.88673843e-02  1.25535669e-03  8.95837531e-02  5.94751710e-04
  2.78590688e-03 -6.89770344e-03  5.12228899e-03  3.76882615e-03
 -1.23371837e-02  8.84289556e-03 -1.76603674e-02 -2.18332046e-02
  3.58484515e-04 -8.00444474e-03  2.47700653e-03 -9.50358980e-04
 -6.19947022e-03  1.88130986e-03 -8.86077443e-03 -1.10831815e-02
  3.74231247e-03  8.77576934e-03  5.90951324e-02  1.00285179e-02
  9.33964443e-03  3.51249100e-02  3.28454158e-02  2.64193937e-02
  5.76615443e-03  1.81379409e-02  9.29800463e-02  1.58987104e-02
  1.07830088e-02  2.18840373e-02  1.27944419e-01  2.62502373e-02
 -2.47800454e-02  1.11620299e-01  2.35626260e-01 -2.89881591e-01
 -1.43063212e-02  1.04786941e-02 -9.73919398e-02  3.80783733e-01
  4.04009251e-01]
supnorm grad right now is: 0.4493153889619291
Weights right now are: 
[-3.66602101  0.87225286 -3.0913129   0.81530222  3.13455075 -1.0062483
  3.25601994 -1.38135833  4.5581748  -1.60395409  4.91263115  2.08189142
  1.58409718  9.39770925  2.95598279  9.23536746  2.32924226 -3.60482419
  2.4635538  -1.77801336  2.04651714 10.00298411  2.7196129   9.46177751
 -1.77190953  3.95171628 -2.37893145 -4.21824512  3.13937173 -2.2441794
  3.26186676  5.48789088 -4.3225137   3.47172624 -2.96885951 -2.86377712
  4.43093326  6.84780921  3.44349478  5.83037316  8.09638148  8.1323145
  0.64075646 10.99741934 -5.43947861 -0.5430346   6.19685542 -2.28878175
  8.23479702  7.02199473  0.50385921  9.71774304  7.45757898  9.43227618
 -0.40661959 11.98822536  1.74701219 -2.91200974 -4.80154658 -2.81947162
  0.40757485  5.41484591 -2.51137191 -2.97549405 25.85658719]
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2847.098708559318
gradient value of function right now is: [ 2.87718998e-02 -4.41346006e-02  8.71488564e-01 -3.59861223e-02
 -2.87718998e-02  4.41346006e-02 -8.71488564e-01  3.59861223e-02
  6.25329990e-02  7.84167793e-02  6.38390539e-02  4.62247171e-02
  8.87953273e-02 -2.72815110e-03  8.90853305e-02 -2.27495513e-03
  1.66474655e+00  1.32841377e+00  1.67934942e+00  1.18652542e+00
  8.40114637e-02 -2.39045116e-03  8.42928973e-02 -1.99818990e-03
 -3.69154712e-02 -4.19340491e-02 -5.45323980e-02 -7.39154774e-02
  4.05368737e-02  4.85920938e-02  6.08098673e-02  8.22790754e-02
 -3.83100694e-02 -4.60558869e-02 -5.63534944e-02 -7.63988714e-02
  1.75144146e-02  1.29216504e-02  2.60997943e-02  3.55997204e-02
 -1.32921397e-02 -8.08572461e-02 -4.32357631e-01 -2.50387141e-02
  3.41886535e-02  2.06682078e-01  2.18876824e-01  1.49863908e-01
 -1.75155353e-02 -1.08164402e-01 -5.66644445e-01 -3.29418482e-02
 -3.51721764e-02 -2.00174292e-01 -1.08511066e+00 -6.44879009e-02
  1.35678767e-01 -4.39695739e-01  1.37116637e+00 -3.40952235e+00
 -1.44010443e-01  5.45533243e-02  4.67927641e-01 -1.37113519e+00
 -4.74392677e+00]
supnorm grad right now is: 4.7439267740216655
Weights right now are: 
[-3.70009828  0.91978899 -2.75457935  0.86242112  3.16862802 -1.05378443
  2.91928639 -1.42847724  4.61352022 -1.60653155  4.96577687  2.15039304
  1.56275871  9.34565039  2.93425577  9.1939642   2.63895105 -3.32433511
  2.7732411  -1.47213671  2.02423532  9.94222993  2.69692778  9.41002983
 -1.96111283  3.91237983 -2.56606807 -4.4074451   3.39815003 -2.16163458
  3.52218659  5.74710452 -4.50549667  3.43814574 -3.15214558 -3.04573605
  4.71529095  7.20538469  3.73624272  6.11351429  8.10359613  7.90881418
  0.42615262 10.80814463 -5.48168807 -0.47937469  6.22536727 -2.21149725
  8.24253406  6.77069305  0.28311123  9.51813946  7.45865204  9.20610901
 -0.62319681 11.79948655  1.86790869 -2.99539712 -4.70006242 -3.00475882
  0.28753537  5.4520184  -2.32518789 -3.22284514 25.95453591]
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2843.7697302679967
gradient value of function right now is: [-1.00907845e-02 -7.08685792e-02 -9.99342257e-01 -5.69171380e-02
  1.00907845e-02  7.08685792e-02  9.99342257e-01  5.69171380e-02
 -5.28784563e-02 -3.11387230e-02 -5.31600996e-02 -3.25791888e-02
  1.41407721e-01  6.33490596e-03  1.42678958e-01  4.02364388e-03
 -2.47224649e+00 -1.31096985e+00 -2.48998153e+00 -1.21939387e+00
  1.29801419e-01  5.75709858e-03  1.30969048e-01  3.63912950e-03
  5.82136721e-02  1.08229898e-02  8.15098977e-02  1.16248045e-01
 -1.00574984e-01 -1.61233208e-02 -1.37046178e-01 -2.00231938e-01
  5.52405901e-02  1.10828691e-02  7.88293044e-02  1.10257211e-01
 -4.96834892e-02 -8.51308545e-03 -6.78188352e-02 -9.87791851e-02
  1.79960712e-02  1.56540079e-01  8.40776674e-01  4.56937583e-02
 -2.45384799e-03 -3.41561216e-02 -4.88484169e-02 -2.09758947e-02
  2.24904599e-02  2.03468375e-01  1.03376866e+00  5.87391317e-02
  4.91923778e-02  3.96968481e-01  2.12753338e+00  1.19203546e-01
 -1.50305618e-01  6.74161247e-01 -2.11429352e+00  6.39738547e+00
  1.76304297e-02 -8.39926064e-03 -8.12112167e-01  2.70010483e+00
  1.60278034e+01]
supnorm grad right now is: 16.02780339226837
Weights right now are: 
[-3.847497    1.07940152 -3.14708733  1.0222422   3.31602673 -1.21339696
  3.31179437 -1.58829831  4.51995897 -1.71107685  4.87176313  2.05606635
  1.46596283  9.25646061  2.83631396  9.10872906  2.31435798 -3.7360009
  2.45125493 -1.99088059  1.92454782  9.847493    2.59607062  9.31845671
 -1.80623827  3.82689094 -2.36420299 -4.27209468  3.32794902 -2.02945954
  3.39746116  5.69502149 -4.33314851  3.36008486 -2.93643072 -2.8925177
  4.67473894  7.63016677  3.6407303   6.09063232  8.35905921  8.29364164
  0.58869956 11.0817212  -5.48524456 -0.36065618  6.27470244 -2.06515386
  8.50759877  7.19718668  0.50302421  9.82194182  7.71242271  9.56849801
 -0.47350007 12.06047759  1.91917328 -2.86360725 -5.05358249 -2.72070619
  0.07365614  5.54104183 -2.47546987 -3.04803746 26.35438536]
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2847.8439627797347
gradient value of function right now is: [ 3.00773127e-02 -5.98300210e-02  6.13913542e-01 -4.91273888e-02
 -3.00773127e-02  5.98300210e-02 -6.13913542e-01  4.91273888e-02
  5.32557362e-02  9.70900589e-02  5.59808330e-02  3.49530321e-02
  1.81744507e-01 -3.73543352e-03  1.82588408e-01 -3.09594127e-03
  8.02153932e-01  8.26092634e-01  8.09416673e-01  6.64859857e-01
  1.65433268e-01 -3.15683480e-03  1.66200532e-01 -2.63795354e-03
 -1.08983091e-02 -6.73213835e-02 -1.25150371e-02 -2.31423401e-02
  1.03293470e-02  7.58658995e-02  1.28578046e-02  2.57993631e-02
 -1.25455478e-02 -7.50936819e-02 -1.40919430e-02 -2.57188718e-02
  6.75647839e-03  1.62840319e-02  8.26292576e-03  1.61835272e-02
 -6.42698506e-03 -1.37215894e-02 -1.34327264e-01 -5.88624156e-03
  5.26638118e-02  3.41572391e-01  2.40108274e-01  2.86012709e-01
 -7.66197760e-03 -8.59546359e-03 -1.33748481e-01 -5.88494947e-03
 -1.77249768e-02 -4.24261071e-02 -3.64719207e-01 -1.73762206e-02
  1.63102660e-01 -2.00717204e-01  2.06365869e-01 -9.61387232e-01
 -3.99053233e-01  3.44344594e-02  4.00512804e-02 -2.29036440e-01
 -5.81367976e+00]
supnorm grad right now is: 5.813679762594867
Weights right now are: 
[-3.78233399  1.4798343  -2.91156493  1.42516946  3.25086373 -1.61382974
  3.07627197 -1.99122558  4.6541231  -1.61619931  5.00727235  2.13558877
  1.13423118  9.04716122  2.50232328  8.90338587  2.59950972 -3.57748936
  2.73515996 -1.84401746  1.5922347   9.63108228  2.2615088   9.10475654
 -1.99311477  3.70992873 -2.52507486 -4.48373596  3.60116465 -1.87946138
  3.6342801   5.98883389 -4.49306471  3.24281779 -3.07725137 -3.07840974
  4.96892903  7.81369276  3.90323641  6.40463731  8.05955095  8.22055404
  0.35722015 10.7340758  -5.36766013 -0.25130248  6.33697783 -1.92300351
  8.23221835  7.1520998   0.3036312   9.51839308  7.39973172  9.45802154
 -0.73007805 11.68840147  2.14264443 -3.13445026 -5.29651481 -2.66651177
 -0.10993442  5.59350106 -2.34975421 -3.33191564 25.66431593]
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2847.45945785669
gradient value of function right now is: [ 3.25733487e-04 -6.64130283e-02 -2.85337042e-01 -5.47152729e-02
 -3.25733487e-04  6.64130283e-02  2.85337042e-01  5.47152729e-02
 -1.38102508e-02  2.20148646e-03 -1.36703691e-02 -4.97286620e-03
  1.91344059e-01  3.35000914e-03  1.92440197e-01  1.93263996e-03
 -9.97395865e-01 -4.35969949e-01 -1.00333393e+00 -4.19060579e-01
  1.75020506e-01  2.99518722e-03  1.76018593e-01  1.70267518e-03
  2.52399329e-02 -4.48203488e-03  3.73951867e-02  5.00964332e-02
 -5.61222814e-02  5.57795836e-03 -7.56722854e-02 -1.09903556e-01
  2.12629846e-02 -5.25317057e-03  3.35361835e-02  4.25293197e-02
 -2.84831233e-02  9.05077220e-04 -3.85518746e-02 -5.55912774e-02
  7.32604022e-04  1.26202604e-01  4.48815321e-01  1.03842841e-02
  6.57904630e-03  2.46550009e-02  1.67282354e-02  2.05730345e-02
  1.42344371e-03  1.59339529e-01  5.59293890e-01  1.40691210e-02
  2.39663905e-03  3.27126375e-01  1.16511526e+00  2.74123081e-02
 -1.78541313e-02  3.59577424e-02 -2.35516543e+00  6.47522885e+00
 -1.89668370e-02  6.45690025e-03 -2.90523940e-01  7.73252064e-01
 -1.00243978e+00]
supnorm grad right now is: 6.475228854849354
Weights right now are: 
[-3.9598594   1.51123716 -3.20519874  1.46048334  3.42838914 -1.6452326
  3.36990578 -2.02653946  4.52316015 -1.77879308  4.87347559  2.02086061
  1.14082487  9.30749692  2.50861699  9.23961697  2.38648468 -3.87247638
  2.52212629 -2.14813665  1.59137214  9.88063617  2.26031109  9.43298592
 -1.88057514  3.85058642 -2.40018332 -4.38871612  3.53096156 -2.05786975
  3.54418912  5.93436847 -4.3767232   3.39102625 -2.95089613 -2.98054652
  4.88646587  7.56403886  3.80134389  6.33822566  8.13606596  8.49797389
  0.48963092 10.88874685 -5.34290667 -0.36671575  6.28943    -2.10325489
  8.31565241  7.43305578  0.45117066  9.67366019  7.45615599  9.71314346
 -0.61934429 11.82764213  1.98642214 -3.09867778 -5.5152532  -2.51047928
  0.2589125   5.5926075  -2.47428701 -3.22820721 25.69932211]
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2836.3096055403516
gradient value of function right now is: [ 2.10960282e-02  1.08574861e+00  8.44633786e-01  9.05429127e-01
 -2.10960282e-02 -1.08574861e+00 -8.44633786e-01 -9.05429127e-01
  7.69989227e-02  5.77691697e-02  7.79738419e-02  4.00853919e-02
 -4.12539866e+00 -7.04771063e-02 -4.13727336e+00 -5.07543075e-02
  2.38492366e+00  1.14653039e+00  2.40025431e+00  1.03504615e+00
 -3.78373251e+00 -6.23296624e-02 -3.79454141e+00 -4.46581549e-02
 -3.13884042e-02 -2.65913535e-02 -5.01133839e-02 -6.69972482e-02
  2.60275770e-01  3.55535432e-02  3.22183052e-01  5.27396584e-01
  8.11696335e-03 -2.81529300e-02 -4.73700391e-03  1.15662068e-02
  1.60184252e-01  1.34453873e-02  1.97838874e-01  3.25950041e-01
 -3.44216050e-02 -3.52439145e-01 -1.64680329e+00 -6.72251993e-02
  2.22394333e-02  1.18214790e-01  1.15589132e-01  7.80021178e-02
 -3.95635374e-02 -4.22257705e-01 -1.91416819e+00 -7.89444291e-02
 -9.46361617e-02 -9.16210501e-01 -4.34235020e+00 -1.80331880e-01
  2.76521672e-01 -1.10723533e+00  5.39175827e+00 -1.34983781e+01
 -8.01303373e-02  4.76319872e-02  1.11213652e+00 -3.42656105e+00
 -1.58809565e+01]
supnorm grad right now is: 15.88095653793275
Weights right now are: 
[-3.99697747  2.09374335 -3.09609144  2.05952148  3.46550721 -2.22773879
  3.26079848 -2.6255776   4.56587774 -1.82750507  4.91445799  1.99181304
  0.47347683  9.89069645  1.83994427  9.86066807  2.72044998 -3.76994578
  2.85461522 -2.03755911  0.91781048 10.48965818  1.5854196  10.09387525
 -2.31136856  3.90790069 -2.81312538 -4.84321047  3.85662225 -2.14634294
  3.87172248  6.26212769 -4.83212282  3.45065406 -3.38744683 -3.46676388
  5.16901403  7.36474851  4.10488903  6.61932946  8.12054378  7.93291236
  0.07096145 10.62440282 -5.33253072 -0.41381983  6.27078811 -2.1706982
  8.2940644   6.92951519  0.04697901  9.43326277  7.43577525  9.10025526
 -1.06372456 11.55501306  2.06712281 -3.18683785 -5.16781887 -3.00659016
  0.3654808   5.57364189 -2.21242918 -3.55973987 25.50109866]
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2847.1030139346153
gradient value of function right now is: [-5.48409748e-03 -6.92668171e-02 -3.46559148e-01 -5.77004924e-02
  5.48409748e-03  6.92668171e-02  3.46559148e-01  5.77004924e-02
 -2.76011570e-02 -1.65735070e-02 -2.78668502e-02 -1.54315405e-02
  2.14293036e-01  4.91368869e-03  2.16914819e-01  3.00194629e-03
 -1.00867236e+00 -4.82424742e-01 -1.01865816e+00 -4.53310126e-01
  1.97598121e-01  4.57686265e-03  2.00020878e-01  2.80295876e-03
  1.95050699e-02  3.81725529e-03  2.86121215e-02  4.13229340e-02
 -6.90719783e-02 -6.24538129e-03 -8.78119220e-02 -1.39957419e-01
  1.52904335e-02  3.97969135e-03  2.43523359e-02  3.33942797e-02
 -4.00893399e-02 -3.23683025e-03 -5.06922653e-02 -8.08101582e-02
  1.15770444e-02  9.78698854e-02  4.84580924e-01  2.24297876e-02
 -1.27701311e-03 -1.22778476e-02 -2.11544289e-02 -6.72761793e-03
  1.40488356e-02  1.22001114e-01  5.77458749e-01  2.76123119e-02
  3.38081119e-02  2.62452870e-01  1.27984612e+00  6.24893938e-02
 -1.12864037e-01  4.57535252e-01 -1.31836342e+00  3.93266120e+00
  7.50976757e-03 -4.52087221e-03 -4.17438693e-01  1.35678830e+00
  9.22208080e+00]
supnorm grad right now is: 9.222080801982294
Weights right now are: 
[-4.23410158  1.58617046 -3.59559958  1.54390721  3.70263132 -1.7201659
  3.76030662 -2.10996333  4.2838232  -2.06953601  4.63419102  1.69409249
  1.11302384 10.96480779  2.47985416 11.00673036  2.35572748 -4.26138579
  2.49063126 -2.54223427  1.55541192 11.56090253  2.22335828 11.24095181
 -2.18680175  4.10500901 -2.68284897 -4.70936249  3.40837336 -2.42482788
  3.47172546  5.79065126 -4.80271273  3.64759338 -3.33388949 -3.43106196
  4.59262924  6.76175502  3.5844233   6.0153481   8.71047435  8.29679412
  0.44576539 11.13252428 -5.44993205 -0.54434155  6.16965145 -2.30904068
  8.83005392  7.26784157  0.39816891  9.88629518  8.03269969  9.48259821
 -0.66647191 12.08852026  1.75273762 -2.7390734  -5.41858915 -2.78702953
  0.50418852  5.47276198 -2.43791602 -3.23392939 26.16671388]
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2848.847870146654
gradient value of function right now is: [ 2.09728675e-03 -2.95176749e-02  6.03618029e-02 -2.58443449e-02
 -2.09728675e-03  2.95176749e-02 -6.03618029e-02  2.58443449e-02
  9.44792615e-03  1.79869223e-03  9.58586063e-03  9.39053810e-04
  1.13601072e-01 -8.46255606e-03  1.13576610e-01 -6.85388529e-03
  3.61339877e-01  5.51053444e-02  3.67153162e-01  4.09343242e-02
  1.08923910e-01 -7.48365330e-03  1.08939689e-01 -6.06047476e-03
 -1.86829163e-02 -1.40855869e-03 -2.39703754e-02 -3.80006086e-02
  3.90719632e-02  2.68040348e-03  5.03751586e-02  8.48143799e-02
 -1.69133656e-02 -1.42258812e-03 -2.16451938e-02 -3.36649439e-02
  2.54996140e-02  1.62497262e-03  3.24355174e-02  5.51522791e-02
 -7.14640113e-03 -6.03565466e-02 -3.49344060e-01 -9.48219407e-03
  6.29225497e-04  3.29400618e-03  6.92671201e-03  1.26510865e-03
 -7.88141066e-03 -6.67756357e-02 -3.91003957e-01 -1.01615479e-02
 -1.87869715e-02 -1.56460964e-01 -9.11312347e-01 -2.52522056e-02
  5.51896937e-02 -2.98399738e-01  1.23631108e+00 -3.17001307e+00
  3.19025140e-03  1.69709251e-03  1.73035037e-01 -6.20952086e-01
 -1.49840594e+00]
supnorm grad right now is: 3.1700130707326775
Weights right now are: 
[-4.18848076  1.75743421 -3.34517633  1.71411143  3.6570105  -1.89142964
  3.50988337 -2.28016755  4.41464785 -2.02729736  4.7627405   1.79519336
  1.02741646 10.99619776  2.3934646  11.05202929  2.6389993  -4.07155345
  2.77364361 -2.32614155  1.46693764 11.58457293  2.13409207 11.27793975
 -2.26321935  4.13222661 -2.77362838 -4.76605802  3.56554677 -2.45145527
  3.62853513  5.92421812 -4.8753694   3.67572966 -3.42467418 -3.48449151
  4.76550777  6.7435632   3.7613216   6.16279583  8.92463693  8.11645886
  0.33356533 11.08564925 -5.50900575 -0.55626003  6.15656249 -2.32751016
  9.00190255  7.06590076  0.27114743  9.80670822  8.28973637  9.30380001
 -0.75735058 12.06781327  1.89290652 -2.65644731 -5.21520655 -2.99838753
  0.50807817  5.44888512 -2.24156576 -3.34578316 25.87539947]
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2845.590556403952
gradient value of function right now is: [ 2.93081525e-03 -2.23201178e-02  1.27092079e-01 -1.98820694e-02
 -2.93081525e-03  2.23201178e-02 -1.27092079e-01  1.98820694e-02
  1.68328757e-02 -1.42801183e-03  1.70125032e-02  7.97862585e-04
  8.91441261e-02 -9.77341912e-03  8.75892061e-02 -7.73959956e-03
  6.73351609e-01  1.64687649e-01  6.83762313e-01  1.64440315e-01
  8.66639508e-02 -8.69020389e-03  8.52675131e-02 -6.86337347e-03
 -2.93083400e-02  6.48582584e-03 -4.23917381e-02 -5.96709700e-02
  6.18655002e-02 -6.55993250e-03  8.47274974e-02  1.26979376e-01
 -2.63842891e-02  6.97379302e-03 -3.87373866e-02 -5.37624970e-02
  3.71246536e-02 -1.72871228e-03  5.01523282e-02  7.60004649e-02
 -1.69875521e-02 -1.24542102e-01 -5.70718191e-01 -2.63664414e-02
 -6.47270221e-03 -5.45159477e-02 -3.27722395e-02 -3.73486953e-02
 -2.09271749e-02 -1.53242942e-01 -7.03556778e-01 -3.22252436e-02
 -4.54390413e-02 -3.14944216e-01 -1.45029816e+00 -6.92905659e-02
  8.07243772e-02 -4.84107199e-01  1.90389689e+00 -4.50151522e+00
  5.20559537e-02 -1.06815705e-02  3.63677868e-01 -1.20840089e+00
  6.57934586e-01]
supnorm grad right now is: 4.501515223758989
Weights right now are: 
[-4.17133145e+00  1.64831848e+00 -3.16696201e+00  1.60334304e+00
  3.63986119e+00 -1.78231392e+00  3.33166905e+00 -2.16939916e+00
  4.51519848e+00 -2.03002724e+00  4.86145823e+00  1.83926885e+00
  1.16415182e+00  1.11627512e+01  2.53066097e+00  1.12126936e+01
  2.90945619e+00 -3.86127372e+00  3.04414735e+00 -2.09423420e+00
  1.60278063e+00  1.17570665e+01  2.27038857e+00  1.14453901e+01
 -2.56518991e+00  4.16765315e+00 -3.07250023e+00 -5.08463948e+00
  3.72997163e+00 -2.49870117e+00  3.81470277e+00  6.10715148e+00
 -5.20197370e+00  3.71267925e+00 -3.74500815e+00 -3.82690193e+00
  4.90018507e+00  6.68754621e+00  3.92491333e+00  6.31684031e+00
  8.49853214e+00  7.86276884e+00  7.78864978e-02  1.07130756e+01
 -5.55220542e+00 -6.02887547e-01  6.13744325e+00 -2.38465813e+00
  8.60188736e+00  6.78546675e+00  1.20281450e-03  9.43303465e+00
  7.86725779e+00  9.04351659e+00 -1.02796041e+00  1.16966537e+01
  2.10422309e+00 -3.09460823e+00 -5.08862667e+00 -3.17839569e+00
  5.69205997e-01  5.42327538e+00 -2.06227646e+00 -3.64419138e+00
  2.63184538e+01]
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2844.649824544854
gradient value of function right now is: [ 3.07464723e-03  8.07375769e-02  1.47298516e-01  6.63006113e-02
 -3.07464723e-03 -8.07375769e-02 -1.47298516e-01 -6.63006113e-02
  2.25219904e-02 -4.19095792e-03  2.27141668e-02  7.10464493e-04
 -2.80686541e-01 -1.95873673e-02 -2.84826960e-01 -1.53081596e-02
  8.10538485e-01  1.94107779e-01  8.23826218e-01  1.92006218e-01
 -2.51689211e-01 -1.74051353e-02 -2.55427714e-01 -1.35599636e-02
 -2.99301247e-02  7.81374989e-03 -4.59514674e-02 -6.03680728e-02
  9.75952491e-02 -6.70321097e-03  1.33232262e-01  1.97364789e-01
 -2.35131489e-02  8.59366451e-03 -3.79265065e-02 -4.74622160e-02
  6.19525945e-02 -6.48387750e-04  8.33737595e-02  1.25139637e-01
 -1.89919100e-02 -1.74705055e-01 -7.80543731e-01 -3.20111065e-02
 -8.78882284e-03 -6.41797648e-02 -4.32113610e-02 -4.35899419e-02
 -2.31890094e-02 -2.24337311e-01 -9.78851146e-01 -3.96026579e-02
 -5.06152053e-02 -4.37319168e-01 -1.97448413e+00 -8.35032295e-02
  1.00544263e-01 -5.64219792e-01  2.77992459e+00 -6.27357803e+00
  5.92670536e-02 -1.32152281e-02  5.07763865e-01 -1.54717622e+00
 -2.05914231e-01]
supnorm grad right now is: 6.2735780349805985
Weights right now are: 
[-4.21043478  2.23549782 -3.09969331  2.19790956  3.67896451 -2.36949326
  3.26440035 -2.76396568  4.53911029 -2.08013856  4.88326973  1.82901626
  0.56064559 11.13325126  1.92833107 11.16648118  3.02425824 -3.77137681
  3.15785947 -1.97730497  0.9954095  11.73221088  1.66421406 11.40350629
 -2.59453787  4.27475847 -3.10943241 -5.11583468  3.77233812 -2.6292695
  3.8755917   6.14031188 -5.24140788  3.82041543 -3.78938851 -3.86855465
  4.95452971  6.45921199  4.00550707  6.36175241  8.48468335  7.75707609
  0.03283241 10.67951498 -5.49818745 -0.74948362  6.06805851 -2.55518917
  8.58772477  6.65465857 -0.05913439  9.37434914  7.83349689  8.93904093
 -1.07758324 11.66171209  2.0385727  -3.10489411 -4.97290419 -3.28204128
  0.88761696  5.31086446 -2.01665638 -3.70242911 26.36291071]
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2845.386873568423
gradient value of function right now is: [-1.38277561e-03  6.85933791e-02 -4.55003199e-02  5.61614241e-02
  1.38277561e-03 -6.85933791e-02  4.55003199e-02 -5.61614241e-02
  4.30279769e-03 -1.72663734e-02  4.38504459e-03 -1.29442514e-02
 -2.48247762e-01 -1.82107862e-02 -2.51397482e-01 -1.42317544e-02
  3.85890042e-01 -1.23958475e-01  3.95316368e-01 -1.25900750e-01
 -2.21215020e-01 -1.61799265e-02 -2.24030606e-01 -1.26181817e-02
 -2.37613757e-02  6.05701540e-03 -3.34812525e-02 -5.03964207e-02
  8.44231500e-02 -5.42664624e-03  1.10612581e-01  1.78457193e-01
 -1.79764778e-02  6.74817008e-03 -2.62187205e-02 -3.83276299e-02
  5.52900065e-02 -9.28306517e-04  7.16817926e-02  1.16284999e-01
 -1.84114309e-02 -1.18338864e-01 -6.49585906e-01 -2.55554578e-02
 -7.47211651e-03 -3.59548942e-02 -4.09544441e-02 -1.97636428e-02
 -2.12072521e-02 -1.37275712e-01 -7.67393964e-01 -2.90330929e-02
 -5.09991647e-02 -3.14242910e-01 -1.72492287e+00 -7.03559806e-02
  1.05840893e-01 -6.10227843e-01  2.03366906e+00 -5.01266890e+00
  1.82265412e-02 -4.44088502e-03  3.72002473e-01 -1.33400227e+00
  4.46585418e+00]
supnorm grad right now is: 5.012668896802152
Weights right now are: 
[-4.4904887   1.85530227 -3.59439298  1.81265636  3.95901844 -1.9892977
  3.75910002 -2.37871248  4.12331136 -2.34442045  4.47067916  1.45269989
  1.01050401 11.62079749  2.37939918 11.60021345  2.52524468 -4.32924468
  2.65876884 -2.55851285  1.44526468 12.2358827   2.11528341 11.85285375
 -2.33496881  4.42731728 -2.79935053 -4.86357495  3.21451229 -2.8393175
  3.3289446   5.58358022 -5.05038922  3.96948417 -3.53319374 -3.68550626
  4.35880564  6.04153673  3.42573914  5.76650236  9.17375565  8.18026779
  0.50707534 11.37427208 -5.44584378 -0.87164181  5.94719321 -2.66691018
  9.32123289  7.11335281  0.43982064 10.11047196  8.48342052  9.35302111
 -0.61593096 12.33370655  1.5319506  -2.52972728 -5.22508337 -3.02245846
  1.1299613   5.14681373 -2.44694815 -3.22673813 26.70831027]
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2846.149594549833
gradient value of function right now is: [ 8.80607017e-05  2.32830128e-03  3.58050963e-03  4.05457449e-04
 -8.80607015e-05 -2.32830128e-03 -3.58050963e-03 -4.05457449e-04
  9.32265578e-03 -1.04472535e-02  9.50438126e-03 -7.78165616e-03
  6.68329801e-03 -1.32533921e-02  4.72995436e-03 -1.04935740e-02
  4.58934628e-01 -3.41429227e-02  4.69068902e-01 -4.33848397e-02
  1.26440240e-02 -1.17766040e-02  1.09075350e-02 -9.30862234e-03
 -2.57183236e-02  2.89582357e-03 -3.54946346e-02 -5.55934170e-02
  7.02107303e-02 -2.10323382e-03  9.23299717e-02  1.52999000e-01
 -2.18563937e-02  3.34210766e-03 -3.06653026e-02 -4.73467646e-02
  4.48508804e-02  1.30636703e-04  5.82651369e-02  9.71803978e-02
 -1.73808434e-02 -1.08314031e-01 -5.80315556e-01 -2.28697948e-02
 -5.09928857e-03 -1.81668679e-02 -2.02063085e-02 -9.43330975e-03
 -2.01010183e-02 -1.25530056e-01 -6.83645680e-01 -2.59000200e-02
 -4.97414796e-02 -2.92571558e-01 -1.57667779e+00 -6.47441812e-02
  9.49652091e-02 -5.64333689e-01  1.96846055e+00 -4.72977810e+00
  4.65770595e-03  1.90509500e-03  3.32673330e-01 -1.20341056e+00
  3.46132945e+00]
supnorm grad right now is: 4.729778104468905
Weights right now are: 
[-4.31919051e+00  1.76123950e+00 -3.21987265e+00  1.71739802e+00
  3.78772024e+00 -1.89523494e+00  3.38457969e+00 -2.28345413e+00
  4.45731695e+00 -2.19273996e+00  4.80338790e+00  1.67308278e+00
  1.16870397e+00  1.18106526e+01  2.53765049e+00  1.18035606e+01
  3.02386239e+00 -3.88058594e+00  3.15823621e+00 -2.09098316e+00
  1.60047525e+00  1.24195018e+01  2.27053044e+00  1.20502480e+01
 -2.79687141e+00  4.37606165e+00 -3.24897960e+00 -5.39069230e+00
  3.64042592e+00 -2.73487300e+00  3.77033934e+00  6.07264989e+00
 -5.54092851e+00  3.91859304e+00 -4.00876307e+00 -4.24318659e+00
  4.77391056e+00  6.27299486e+00  3.86540282e+00  6.23911501e+00
  8.38539926e+00  7.76941055e+00  2.38087145e-02  1.07073505e+01
 -5.38736938e+00 -8.96996832e-01  6.00210020e+00 -2.68700081e+00
  8.56105388e+00  6.71221660e+00 -4.28622484e-02  9.48785249e+00
  7.62025228e+00  8.86441559e+00 -1.18969052e+00  1.15670472e+01
  2.09049083e+00 -3.37128095e+00 -4.96789498e+00 -3.37525045e+00
  1.19513303e+00  5.14916171e+00 -2.01385599e+00 -3.89695745e+00
  2.58062307e+01]
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2846.0967034174764
gradient value of function right now is: [ 3.36331918e-03  1.64409890e-02  9.26767814e-02  1.22836636e-02
 -3.36331918e-03 -1.64409890e-02 -9.26767814e-02 -1.22836636e-02
  2.45090002e-02  2.44513185e-04  2.49455379e-02  5.61084649e-04
 -4.44776134e-02 -1.44767703e-02 -4.70374569e-02 -1.13636354e-02
  6.80421705e-01  9.97807260e-02  6.93443625e-01  7.87955128e-02
 -3.44744545e-02 -1.29099547e-02 -3.67743079e-02 -1.01126542e-02
 -2.97704252e-02 -1.10431735e-03 -4.18046200e-02 -6.51774424e-02
  8.21510133e-02  2.81106956e-03  1.08202880e-01  1.79569277e-01
 -2.54942365e-02 -8.97168216e-04 -3.65551666e-02 -5.60460486e-02
  5.15698226e-02  2.04568241e-03  6.70822148e-02  1.12200489e-01
 -1.80245502e-02 -1.33924528e-01 -6.74971966e-01 -2.71134359e-02
 -2.29928381e-03  4.59253660e-03  6.61039480e-03  2.49344584e-03
 -2.09936907e-02 -1.59617068e-01 -8.03302957e-01 -3.16827591e-02
 -5.12851897e-02 -3.63002332e-01 -1.83794364e+00 -7.61029052e-02
  1.22241468e-01 -5.92559821e-01  2.29478230e+00 -5.48304947e+00
 -1.09986003e-02  1.40035354e-02  4.32514585e-01 -1.40384284e+00
  6.97536165e-01]
supnorm grad right now is: 5.483049472755046
Weights right now are: 
[-4.68213502  1.97874997 -3.54408725  1.93841612  4.15066476 -2.1127454
  3.70879429 -2.50447224  3.95401945 -2.55232124  4.29750382  1.27338467
  0.97910286 12.07552236  2.34879839 12.09629856  2.68975952 -4.21964903
  2.82160745 -2.41273052  1.40630363 12.67508879  2.07708848 12.33333118
 -2.43746992  4.68935141 -2.87445109 -5.02307199  3.34910034 -3.10657537
  3.46081549  5.76299274 -5.21040333  4.22746967 -3.65945619 -3.90659566
  4.50148539  5.72203326  3.57489499  5.94808492  8.46588931  8.28342238
  0.39496843 10.87654644 -5.61106579 -1.14674922  5.74771933 -2.89687796
  8.65283524  7.24300782  0.34600861  9.66790446  7.68094588  9.38156539
 -0.82750255 11.71970756  2.0248818  -3.36220416 -5.52169949 -2.94859346
  1.47137475  4.78898316 -2.24465353 -3.75990858 25.97373176]
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2846.8040609174227
gradient value of function right now is: [ 2.16478357e-03 -6.44546462e-03  3.01732442e-02 -6.88726435e-03
 -2.16478357e-03  6.44546462e-03 -3.01732442e-02  6.88726435e-03
  1.83053679e-02 -3.17015449e-03  1.87394770e-02 -4.12966554e-03
  3.76211650e-02 -1.15970979e-02  3.60221520e-02 -9.24497014e-03
  5.14450070e-01 -3.81933635e-03  5.25470824e-01 -2.16875836e-02
  4.07314815e-02 -1.03094651e-02  3.93130137e-02 -8.20621807e-03
 -2.79099697e-02 -1.23898678e-03 -3.73238790e-02 -6.31377820e-02
  6.70133180e-02  2.39705455e-03  8.69679326e-02  1.52292253e-01
 -2.47266899e-02 -1.11472288e-03 -3.33502298e-02 -5.61457889e-02
  4.17701872e-02  1.28997338e-03  5.36518588e-02  9.42638717e-02
 -1.67125076e-02 -1.04750698e-01 -5.71176988e-01 -2.10185477e-02
 -1.20505047e-03  9.62937794e-03  6.80797398e-03  6.48424731e-03
 -1.90737241e-02 -1.17572433e-01 -6.59656756e-01 -2.29080658e-02
 -4.91759062e-02 -2.95269621e-01 -1.61110600e+00 -6.18668776e-02
  9.50997595e-02 -5.44072938e-01  1.85756928e+00 -4.78069299e+00
 -1.61110955e-02  1.54968792e-02  2.49968851e-01 -1.07216521e+00
  9.82469793e-01]
supnorm grad right now is: 4.780692990757045
Weights right now are: 
[-4.63275643  1.79992872 -3.47638251  1.75521304  4.10128617 -1.93392416
  3.64108955 -2.32126916  4.01968537 -2.45320613  4.36172727  1.44860603
  1.17856834 12.15502273  2.54871958 12.14950603  2.67218361 -4.15135763
  2.80360967 -2.33285135  1.60848032 12.76093516  2.27973693 12.39160791
 -2.40343633  4.72954855 -2.82697974 -4.98293574  3.22264059 -3.14285816
  3.35535571  5.63846586 -5.18940558  4.26747388 -3.62228794 -3.878913
  4.38233539  5.70290474  3.47692494  5.83187861  8.59453163  8.38928055
  0.49656557 10.97236622 -5.65914954 -1.17964885  5.72379865 -2.944038
  8.77998132  7.34162365  0.44511249  9.75959264  7.80801616  9.4885443
 -0.73672268 11.8166623   2.00500499 -3.2893532  -5.67581197 -2.87696992
  1.54844811  4.68077306 -2.29273162 -3.718221   26.00338339]
NN weights: [-4.4288039   1.89075846 -3.36214615  1.84850175  3.89733363 -2.0247539
  3.52685319 -2.41455787  4.25933188 -2.29117077  4.60448027  1.56220494
  1.00308421 11.67681035  2.37181839 11.68684594  2.80916842 -4.04726183
  2.94245319 -2.25606492  1.43572463 12.28155518  2.10556703 11.92924541
 -2.53456008  4.45073461 -3.00005782 -5.09169106  3.48907413 -2.83479664
  3.60285735  5.88668032 -5.25465407  3.99255642 -3.7403221  -3.91958112
  4.64330937  6.12267123  3.71219715  6.07692504  8.59224195  8.02589333
  0.24793779 10.88514061 -5.50717218 -0.92038311  5.93326058 -2.70135924
  8.74437494  6.96140178  0.18075232  9.63795228  7.86939172  9.16123056
 -0.9222437  11.79383672  1.95422263 -3.12990743 -5.21996206 -3.13059293
  1.16228985  5.08203789 -2.1804026  -3.66254084]
Minimum obj value:-2846.8040609174227
Optimal xi: 26.199194469794758
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1509.7240615129099
W_T_median: 1348.7279456546546
W_T_pctile_5: 684.003131228039
W_T_CVAR_5_pct: 582.235505138669
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
F value: -2846.8040609174227
-----------------------------------------------
{'NN': [-4.42880389706552, 1.890758461597573, -3.3621461493549907, 1.8485017503034185, 3.8973336340661358, -2.024753898319239, 3.526853188632597, -2.414557869507293, 4.259331882156808, -2.291170771856312, 4.604480268116718, 1.5622049365699477, 1.0030842096704375, 11.67681035399136, 2.3718183942689013, 11.686845942371612, 2.8091684170978173, -4.047261828575196, 2.9424531914964196, -2.2560649186661266, 1.435724632307658, 12.281555183661098, 2.1055670279282896, 11.929245409032697, -2.5345600834577073, 4.4507346083031125, -3.000057821635321, -5.091691058727052, 3.4890741264251615, -2.834796638414224, 3.6028573502708277, 5.886680319960685, -5.2546540675820435, 3.9925564176796846, -3.740322100400303, -3.919581123882351, 4.643309372558133, 6.122671226547048, 3.7121971533056066, 6.076925041019994, 8.592241954769785, 8.025893327347205, 0.24793778638081979, 10.885140611859235, -5.507172183286799, -0.9203831129489247, 5.933260580555136, -2.701359237882245, 8.744374937491477, 6.961401778622313, 0.18075232435987387, 9.637952277242158, 7.869391720254348, 9.16123055996921, -0.9222436996924839, 11.793836720243696, 1.954222628532045, -3.1299074251622425, -5.2199620575028, -3.1305929335285523, 1.1622898478704033, 5.082037886478624, -2.1804026035517987, -3.662540842466836]}
[-4.4288039   1.89075846 -3.36214615  1.84850175  3.89733363 -2.0247539
  3.52685319 -2.41455787  4.25933188 -2.29117077  4.60448027  1.56220494
  1.00308421 11.67681035  2.37181839 11.68684594  2.80916842 -4.04726183
  2.94245319 -2.25606492  1.43572463 12.28155518  2.10556703 11.92924541
 -2.53456008  4.45073461 -3.00005782 -5.09169106  3.48907413 -2.83479664
  3.60285735  5.88668032 -5.25465407  3.99255642 -3.7403221  -3.91958112
  4.64330937  6.12267123  3.71219715  6.07692504  8.59224195  8.02589333
  0.24793779 10.88514061 -5.50717218 -0.92038311  5.93326058 -2.70135924
  8.74437494  6.96140178  0.18075232  9.63795228  7.86939172  9.16123056
 -0.9222437  11.79383672  1.95422263 -3.12990743 -5.21996206 -3.13059293
  1.16228985  5.08203789 -2.1804026  -3.66254084]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1260.9596434837733
W_T_median: 1221.9241450328454
W_T_pctile_5: 830.3340558417918
W_T_CVAR_5_pct: 742.5812257595575
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5120.956534342089
gradient value of function right now is: [ 7.46702891e-02  7.57522144e-02  9.85393904e-01  6.35994424e-02
 -7.46702891e-02 -7.57522144e-02 -9.85393904e-01 -6.35994424e-02
  4.90014319e-01  1.98369418e-01  4.97566810e-01  1.75512626e-01
 -1.44136986e-01 -2.53192284e-03 -1.44810952e-01 -1.99546470e-03
  4.30558288e+00  1.50335087e+00  4.37521604e+00  1.35667251e+00
 -1.42953869e-01 -2.44085548e-03 -1.43613981e-01 -1.91918912e-03
 -1.39538466e-01 -3.61316041e-02 -2.03175322e-01 -3.15423809e-01
  1.99361588e-01  4.96683441e-02  2.89156106e-01  4.49502249e-01
 -1.39554206e-01 -3.76012613e-02 -2.04089283e-01 -3.15659549e-01
  9.58544018e-02  2.20632024e-02  1.38936269e-01  2.15910206e-01
 -4.95285494e-02 -5.46578307e-01 -2.07142194e+00 -8.25341041e-02
  1.21986437e-02  1.40531750e-01  3.39433075e-01  2.99066993e-02
 -6.23326571e-02 -7.29042728e-01 -2.68404888e+00 -1.06671607e-01
 -1.46307521e-01 -1.52787757e+00 -5.79756996e+00 -2.38871920e-01
  3.92490722e-01 -1.60703127e+00  8.64598613e+00 -2.03927612e+01
 -5.24874015e-02  1.26683075e-01  1.37904984e+00 -4.15481777e+00
  1.42595263e+00]
supnorm grad right now is: 20.39276122892324
Weights right now are: 
[-5.03977521  0.9312492  -3.69157592  0.89128109  4.50830494 -1.06524464
  3.85628296 -1.45733721  3.7201501  -2.75914905  4.07051384  1.0187688
  1.93460059 12.88325264  3.30354975 12.89725285  2.63974287 -4.12522269
  2.77658952 -2.33480141  2.36756121 13.49425615  3.0376043  13.14923046
 -2.70780067  4.73640729 -3.17926421 -5.27788842  3.40407608 -3.13475206
  3.55944364  5.8460912  -5.48335818  4.28018284 -3.9657497  -4.16183315
  4.47004336  5.81139489  3.58710092  5.94576804  8.41522129  7.99626649
  0.19747443 10.70789034 -5.34157963 -1.71977594  5.64926295 -3.21658886
  8.55963529  6.9517404   0.10946745  9.45436014  7.67228169  9.09161553
 -1.02301885 11.58806469  1.92994481 -3.30171508 -5.3611542  -3.09393529
  1.98021621  4.85250879 -2.16834701 -3.85650787 26.13865024]
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5146.58898334826
gradient value of function right now is: [ 5.64367012e-02  1.47179484e-02  2.46465163e-01  1.20570940e-02
 -5.64367012e-02 -1.47179484e-02 -2.46465163e-01 -1.20570940e-02
  6.13376666e-01  3.71331265e-02  6.19057629e-01  2.92784508e-02
 -1.44549567e-02 -1.90964139e-04 -1.45185662e-02 -1.40281187e-04
  1.94016869e+00  1.19667191e-01  1.95848186e+00  9.43375522e-02
 -1.61155474e-02 -2.07079407e-04 -1.61856091e-02 -1.51820838e-04
 -4.21393911e-02 -3.61833179e-03 -5.13937964e-02 -1.01566961e-01
  6.99282676e-02  6.39941136e-03  8.57713932e-02  1.70663204e-01
 -3.92203448e-02 -3.58879469e-03 -4.81716849e-02 -9.58709625e-02
  2.62600824e-02  2.48500160e-03  3.23018573e-02  6.45597007e-02
 -1.35244225e-02 -4.51697813e-02 -5.06798611e-01 -1.57043871e-02
  1.55145984e-03  4.27641966e-03  4.38883295e-02  1.81295734e-03
 -1.54616125e-02 -4.89213931e-02 -5.55562154e-01 -1.79605005e-02
 -4.17380716e-02 -1.30724340e-01 -1.42493683e+00 -4.82987649e-02
  6.42190664e-02 -5.57315422e-01 -5.22193989e-02 -1.51441524e+00
 -8.01039112e-04  6.54815808e-03  1.01815890e-01 -8.81766358e-01
  3.47400384e+00]
supnorm grad right now is: 3.4740038380564364
Weights right now are: 
[-6.10511697  0.45150738 -4.33855759  0.415707    5.57364671 -0.58550282
  4.50326463 -0.98176312  2.56111662 -3.57799072  2.91700513  0.19706745
  2.24946141 13.3632867   3.61881326 13.36745783  1.92267769 -4.46487012
  2.06377875 -2.66764025  2.69515834 14.00298555  3.3656263  13.64861962
 -2.27728773  5.14763739 -2.7693     -4.853361    2.91096359 -3.54535074
  3.09928832  5.36172989 -5.08704373  4.68745066 -3.58427271 -3.77094594
  3.99908683  5.45003033  3.15044804  5.48315546  8.39539514  8.39177906
  0.62621807 10.83372909 -5.33403479 -2.38505074  5.19411323 -3.4643025
  8.51925439  7.34788634  0.51600603  9.56942477  7.63198985  9.46591663
 -0.63072999 11.68062063  1.8290097  -3.28311828 -5.7545695  -2.69343518
  2.36516414  4.58474001 -2.46914809 -3.63995934 25.55760834]
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5148.790726327763
gradient value of function right now is: [-5.66760082e-02 -3.38396050e-04 -2.15702873e-01 -2.26093700e-04
  5.66760082e-02  3.38396050e-04  2.15702873e-01  2.26093700e-04
 -6.30521245e-01 -5.91919205e-02 -6.43016096e-01 -4.40334768e-02
  1.11915174e-04  7.11794692e-05  1.26142738e-04  5.16642993e-05
 -1.58819717e+00 -1.46542897e-01 -1.61981361e+00 -1.08576295e-01
  9.87004254e-05  8.92482070e-05  1.16333382e-04  6.48083434e-05
  4.89837509e-02  3.17055716e-03  6.01967081e-02  1.27061118e-01
 -1.17090898e-01 -7.59596836e-03 -1.43651118e-01 -3.01435018e-01
  4.93131119e-02  3.15123760e-03  6.04010659e-02  1.26728809e-01
 -4.05569293e-02 -2.64179870e-03 -4.97342917e-02 -1.04048541e-01
  6.20602560e-03  3.87692758e-02  6.41514288e-01  8.01306442e-03
 -4.29535143e-04 -3.10576472e-03 -4.63069412e-02 -5.95015614e-04
  6.73871118e-03  4.34330770e-02  7.08265123e-01  8.81552761e-03
  1.68154239e-02  1.14760893e-01  1.86564883e+00  2.26122995e-02
 -4.15560650e-02  3.85105861e-01 -4.45410392e-01  2.60814636e+00
 -2.29343801e-04  1.86821185e-03 -9.96605706e-02  7.06027121e-01
  3.46219454e+00]
supnorm grad right now is: 3.462194542890295
Weights right now are: 
[-6.78808585e+00  2.55692529e-01 -4.83733528e+00  2.24016645e-01
  6.25661559e+00 -3.89687966e-01  5.00204232e+00 -7.90072765e-01
  1.81377235e+00 -3.76757922e+00  2.17303245e+00  2.09558904e-02
  2.34249292e+00  1.35388116e+01  3.71205174e+00  1.35368932e+01
  1.23360707e+00 -4.62864350e+00  1.37836863e+00 -2.81938756e+00
  2.79603047e+00  1.42005626e+01  3.46673119e+00  1.38396290e+01
 -1.80961546e+00  5.36310250e+00 -2.35512344e+00 -4.38606728e+00
  2.50011108e+00 -3.72051051e+00  2.74157642e+00  4.95690471e+00
 -4.66601402e+00  4.88811378e+00 -3.21268289e+00 -3.34963476e+00
  3.63512817e+00  5.28956380e+00  2.83617092e+00  5.12348119e+00
  8.68727001e+00  8.56778258e+00  1.01179423e+00  1.11642881e+01
 -5.44016134e+00 -2.56503050e+00  4.97816441e+00 -3.58415229e+00
  8.78308372e+00  7.50922790e+00  8.53060794e-01  9.86415112e+00
  7.90881709e+00  9.60655363e+00 -3.03645282e-01  1.19972296e+01
  1.58120859e+00 -2.92569192e+00 -5.69002537e+00 -2.72163256e+00
  2.50933593e+00  4.35599161e+00 -2.66391577e+00 -3.27807046e+00
  2.51990272e+01]
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.68924905159
gradient value of function right now is: [-3.06023265e-02 -1.93234961e-04 -1.04831560e-01 -1.45748667e-04
  3.06023265e-02  1.93234961e-04  1.04831560e-01  1.45748667e-04
 -3.48617327e-01 -2.86670421e-02 -3.56162582e-01 -2.04167918e-02
  7.88177699e-05  1.16781892e-05  8.12423530e-05  8.45195615e-06
 -7.02502216e-01 -5.86034147e-02 -7.17881576e-01 -4.17072149e-02
  1.03485044e-04  1.59590654e-05  1.06751771e-04  1.15618705e-05
  1.57039896e-02  9.33060558e-04  1.93159192e-02  4.26629199e-02
 -5.38435311e-02 -3.13528978e-03 -6.60211743e-02 -1.45288246e-01
  1.67961574e-02  9.78073921e-04  2.05966309e-02  4.53355649e-02
 -1.73571509e-02 -1.00023837e-03 -2.12498818e-02 -4.66804199e-02
  1.48074484e-03  8.95559908e-03  2.33781642e-01  1.88077160e-03
 -9.67345032e-05 -6.02993144e-04 -1.63522836e-02 -1.26659312e-04
  1.63527819e-03  9.94315424e-03  2.61652107e-01  2.08705463e-03
  4.36993898e-03  2.68188651e-02  7.34264131e-01  5.63771518e-03
 -8.41736967e-03  1.06671896e-01 -6.61011295e-02  6.87650557e-01
 -7.59581556e-05  8.28915732e-04 -1.90670644e-02  1.92040395e-01
 -3.50289138e-01]
supnorm grad right now is: 0.73426413119377
Weights right now are: 
[-7.28826992  0.18239094 -5.25508149  0.15346364  6.75679966 -0.31638638
  5.41978853 -0.71951976  1.27893628 -3.85722706  1.63987883 -0.05009968
  2.36491901 13.58941075  3.73457166 13.58354895  0.68234987 -4.69590498
  0.82891286 -2.87303008  2.82243898 14.26310023  3.49325831 13.89711858
 -1.44334625  5.44324733 -2.02925758 -4.0394962   2.18875865 -3.78231395
  2.46097727  4.67537469 -4.35395809  4.95231166 -2.9374959  -3.0556685
  3.36719145  5.23961639  2.60016893  4.87783345  8.66697488  8.65691867
  1.28160159 11.21329215 -5.43022129 -2.59998778  4.89198945 -3.59744141
  8.76549833  7.58366851  1.08844841  9.90768685  7.88192535  9.682591
 -0.09563011 12.0378449   1.50257991 -2.96930284 -5.76489211 -2.61453119
  2.53018909  4.3160553  -2.76589471 -3.20673303 24.93234451]
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.204610176114
gradient value of function right now is: [-1.13355410e-02 -2.60129132e-06 -3.60736630e-02  1.00587179e-06
  1.13355410e-02  2.60129132e-06  3.60736630e-02 -1.00587179e-06
 -1.22723560e-01 -1.30750594e-02 -1.25812806e-01 -9.40622888e-03
 -3.00825899e-06  3.11418629e-06 -2.67320257e-06  2.33932234e-06
 -2.06284588e-01 -2.33590361e-02 -2.11661895e-01 -1.68268547e-02
 -6.69153472e-06  4.32094185e-06 -6.25873669e-06  3.25422211e-06
  4.86949921e-03  3.28999632e-04  6.11774731e-03  1.39146080e-02
 -2.03082612e-02 -1.31337372e-03 -2.53418299e-02 -5.72399857e-02
  5.65388788e-03  3.66573864e-04  7.05817114e-03  1.59503869e-02
 -6.42645045e-03 -4.07252114e-04 -7.99525153e-03 -1.80040679e-02
  5.93459475e-04  3.80719333e-03  8.41622470e-02  7.69994596e-04
 -3.94833727e-05 -2.59985962e-04 -6.36118246e-03 -5.25792503e-05
  6.63208829e-04  4.27714593e-03  9.62974207e-02  8.64179096e-04
  1.81851347e-03  1.18424768e-02  2.86481284e-01  2.39237285e-03
 -3.30879157e-03  4.35716580e-02 -3.04443250e-02  3.03844231e-01
 -3.35954994e-05  3.81288835e-04 -7.73926787e-03  8.05100415e-02
  2.39323260e+00]
supnorm grad right now is: 2.3932325976913718
Weights right now are: 
[-7.40614685  0.16788769 -5.35970019  0.13954146  6.87467658 -0.30188312
  5.52440723 -0.70559758  1.14532443 -3.83728837  1.50697462 -0.03048092
  2.36873225 13.59721632  3.73840396 13.59055093  0.54163861 -4.69689074
  0.68878814 -2.87314394  2.82711353 14.27299041  3.4979585  13.9059032
 -1.37993671  5.44962692 -1.97533222 -3.9863336   2.15583769 -3.77319453
  2.43549548  4.65910793 -4.31039644  4.95455261 -2.90148229 -3.02232295
  3.34342299  5.24620791  2.58240162  4.86730103  8.64572477  8.66106546
  1.31265383 11.20550603 -5.42505106 -2.6034756   4.89212719 -3.59756098
  8.74687181  7.58746645  1.11322037  9.90139211  7.86076309  9.68569644
 -0.08758562 12.02926473  1.50128254 -3.01482795 -5.77695402 -2.61182432
  2.53387195  4.3046611  -2.77631492 -3.22553807 25.12687917]
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.5123797469505
gradient value of function right now is: [-2.11575051e-02 -1.84544227e-04 -7.34648806e-02 -1.42889649e-04
  2.11575051e-02  1.84544227e-04  7.34648806e-02  1.42889649e-04
 -2.44000114e-01 -1.66444051e-02 -2.49375677e-01 -1.18009502e-02
  7.44786600e-05  6.75000280e-06  7.64253159e-05  4.81381918e-06
 -4.81817294e-01 -3.31919716e-02 -4.92537070e-01 -2.35059406e-02
  1.04184395e-04  9.58651350e-06  1.06931805e-04  6.83996601e-06
  8.79674076e-03  4.84204758e-04  1.08295752e-02  2.41853601e-02
 -3.16162625e-02 -1.71114394e-03 -3.88160724e-02 -8.63821393e-02
  9.45031526e-03  5.11697526e-04  1.16043436e-02  2.58337842e-02
 -9.88659920e-03 -5.30934476e-04 -1.21237919e-02 -2.69418771e-02
  7.22021392e-04  5.04977148e-03  1.34629076e-01  9.72788518e-04
 -4.38212279e-05 -3.17788199e-04 -8.63207699e-03 -6.10340600e-05
  8.03264919e-04  5.65304937e-03  1.51538289e-01  1.08788889e-03
  2.13145577e-03  1.51880319e-02  4.19973760e-01  2.92382508e-03
 -4.04934326e-03  5.13196237e-02 -3.99975195e-02  3.89473153e-01
 -4.13742760e-05  4.49386716e-04 -9.88205862e-03  9.84381207e-02
  6.63721047e-01]
supnorm grad right now is: 0.6637210466820719
Weights right now are: 
[-7.35013711  0.16507724 -5.32824395  0.13682171  6.81866685 -0.29907267
  5.49295099 -0.70287782  1.19218277 -3.73081076  1.55493543  0.06327405
  2.36939137 13.59757643  3.73906614 13.59078909  0.55528142 -4.66683591
  0.70316458 -2.84891891  2.8279605  14.27324263  3.49880974 13.90597664
 -1.4107217   5.43255597 -2.00655751 -4.02923546  2.24945401 -3.72226653
  2.52863877  4.77386386 -4.35561552  4.93526231 -2.94528402 -3.08049483
  3.42325594  5.28295887  2.65907379  4.9648768   8.62132674  8.64781229
  1.25876166 11.18412055 -5.41993169 -2.60018989  4.92882343 -3.59495923
  8.7251757   7.57629153  1.06074987  9.88293418  7.83672994  9.67112899
 -0.15927629 12.00705809  1.51456389 -3.06736757 -5.77692954 -2.63197321
  2.53552453  4.29490988 -2.76876825 -3.26886439 24.96776838]
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.598436646247
gradient value of function right now is: [-2.57382045e-02 -3.22947404e-04 -9.30601815e-02 -2.53458011e-04
  2.57382045e-02  3.22947404e-04  9.30601815e-02  2.53458011e-04
 -3.02480862e-01 -1.68323812e-02 -3.08842442e-01 -1.18915098e-02
  1.33384913e-04  8.38944219e-06  1.36462348e-04  5.90879412e-06
 -6.32590661e-01 -3.51627743e-02 -6.45973910e-01 -2.47874937e-02
  1.90170088e-04  1.20381486e-05  1.94574918e-04  8.47996449e-06
  1.02994187e-02  5.21901772e-04  1.26239842e-02  2.80738722e-02
 -3.44708929e-02 -1.73099693e-03 -4.21736860e-02 -9.35111313e-02
  1.07363956e-02  5.38415479e-04  1.31355700e-02  2.91340911e-02
 -1.07087116e-02 -5.35380709e-04 -1.30917326e-02 -2.89958213e-02
  7.47700771e-04  5.53029517e-03  1.50460793e-01  1.03194187e-03
 -4.25739362e-05 -3.32995899e-04 -8.91182344e-03 -6.12538785e-05
  8.31058582e-04  6.20100533e-03  1.68749885e-01  1.15423448e-03
  2.15297111e-03  1.63829593e-02  4.52669701e-01  3.04041064e-03
 -4.29267256e-03  5.17285365e-02 -4.60025671e-02  4.16014913e-01
 -4.28144061e-05  4.43346170e-04 -1.07769464e-02  1.01654968e-01
  7.80958386e-01]
supnorm grad right now is: 0.780958385912595
Weights right now are: 
[-7.34223806  0.16269999 -5.32834285  0.13454041  6.8107678  -0.29669543
  5.49304989 -0.70059653  1.18821824 -3.65377421  1.55183385  0.1334804
  2.36975412 13.59745859  3.73943177 13.59054407  0.53636397 -4.64453783
  0.68483175 -2.83018342  2.82850263 14.27304565  3.49935637 13.905572
 -1.42483106  5.41996401 -2.0229199  -4.05371287  2.30426286 -3.68485619
  2.58617452  4.84734997 -4.38287579  4.920527   -2.97330075 -3.11907278
  3.4740618   5.31069986  2.70995135  5.03133155  8.60373233  8.63194979
  1.22294984 11.16423476 -5.41655607 -2.59622381  4.95535942 -3.59255905
  8.70949939  7.56283783  1.02458826  9.86567489  7.81969692  9.65404625
 -0.21251171 11.98660795  1.53022945 -3.10596532 -5.77336894 -2.65579048
  2.53708761  4.28706231 -2.75684016 -3.30921497 25.01268863]
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.919103763192
gradient value of function right now is: [-3.33829448e-02 -4.89933365e-04 -1.29265259e-01 -3.85106303e-04
  3.33829448e-02  4.89933365e-04  1.29265259e-01  3.85106303e-04
 -3.92296644e-01 -2.11889249e-02 -4.00638944e-01 -1.52245788e-02
  2.01213587e-04  1.24696206e-05  2.06086784e-04  8.88326096e-06
 -8.90568477e-01 -4.78512353e-02 -9.09659049e-01 -3.42687929e-02
  2.87606835e-04  1.79583993e-05  2.94609505e-04  1.27942626e-05
  1.45738950e-02  7.47386109e-04  1.79724000e-02  4.00835068e-02
 -4.47704631e-02 -2.27521986e-03 -5.50809579e-02 -1.22307832e-01
  1.49980142e-02  7.59437530e-04  1.84498411e-02  4.09895318e-02
 -1.42805397e-02 -7.22572161e-04 -1.75544406e-02 -3.89241739e-02
  1.20620163e-03  8.88900698e-03  2.07822377e-01  1.69591091e-03
 -6.71464465e-05 -5.34724525e-04 -1.20419700e-02 -9.90628556e-05
  1.34162366e-03  1.00241013e-02  2.33693598e-01  1.90102320e-03
  3.42265031e-03  2.64007381e-02  6.21375954e-01  4.95018185e-03
 -7.19602031e-03  7.96842349e-02 -8.44635804e-02  6.47389256e-01
 -6.21501844e-05  6.05198165e-04 -1.84942051e-02  1.59893272e-01
  5.21703006e+00]
supnorm grad right now is: 5.217030059411842
Weights right now are: 
[-7.24408308  0.16271515 -5.25888502  0.13451815  6.71261282 -0.29671058
  5.42359206 -0.70057427  1.28108662 -3.54300535  1.64586458  0.23149732
  2.3697741  13.59629434  3.73945025 13.58944209  0.59452363 -4.61046902
  0.74380465 -2.80276863  2.82853544 14.27120785  3.49938692 13.90382312
 -1.47646798  5.40025461 -2.07318665 -4.11656252  2.41605973 -3.63343973
  2.69682762  4.97755657 -4.44512634  4.89956666 -3.0326697  -3.19325012
  3.56823916  5.3478048   2.80018455  5.14138013  8.5989129   8.60984295
  1.1535023  11.15355162 -5.415603   -2.59044217  4.99370096 -3.59119926
  8.70523343  7.54378568  0.95768726  9.85631027  7.81528811  9.62949663
 -0.29501332 11.97558012  1.53914761 -3.11985142 -5.75337724 -2.69792555
  2.5382679   4.28353153 -2.74142096 -3.33653564 25.27398862]
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5146.9161848449985
gradient value of function right now is: [-6.17781843e-02 -2.28232571e-03 -2.73818276e-01 -1.82679022e-03
  6.17781843e-02  2.28232571e-03  2.73818276e-01  1.82679022e-03
 -7.60196112e-01 -1.89473186e-02 -7.72193181e-01 -1.34284860e-02
  9.92364602e-04  2.15801763e-05  1.00784904e-03  1.49258046e-05
 -2.13417325e+00 -5.18896545e-02 -2.16776416e+00 -3.66244179e-02
  1.44616934e-03  3.13976543e-05  1.46873324e-03  2.17090174e-05
  2.53982511e-02  9.84420099e-04  3.06217660e-02  6.56046536e-02
 -5.39771794e-02 -2.17488601e-03 -6.52531139e-02 -1.39863089e-01
  2.24549406e-02  8.86452339e-04  2.71018769e-02  5.80405009e-02
 -1.72141956e-02 -7.02132922e-04 -2.08293139e-02 -4.46549403e-02
  1.35060466e-03  1.05605286e-02  2.81518794e-01  2.10524670e-03
 -6.95911465e-05 -5.68507903e-04 -1.33311562e-02 -1.15830441e-04
  1.49660251e-03  1.18568350e-02  3.10130930e-01  2.36343603e-03
  3.53263552e-03  2.93236987e-02  7.25865240e-01  5.82766159e-03
 -9.86120784e-03  8.43315611e-02 -9.67802274e-02  6.86276687e-01
 -2.43049004e-05  2.58382042e-04 -2.59922012e-02  1.83500901e-01
 -1.20613430e+00]
supnorm grad right now is: 2.167764159972675
Weights right now are: 
[-7.14043113  0.16038259 -5.17761825  0.13222246  6.60896087 -0.29437803
  5.34232529 -0.69827858  1.36934631 -3.37515533  1.73639558  0.38419128
  2.37013307 13.59384204  3.73980727 13.58702156  0.64517902 -4.55132641
  0.79637319 -2.75399156  2.8291032  14.26728418  3.49995177 13.89993038
 -1.55716767  5.36483967 -2.1554497  -4.22261889  2.58639245 -3.5506495
  2.86969462  5.18255272 -4.54981501  4.8621884  -3.13537361 -3.32347582
  3.72087415  5.40959331  2.94960954  5.32453407  8.58467395  8.55892245
  1.0332182  11.12612963 -5.41409583 -2.57808353  5.05705387 -3.58853805
  8.69298218  7.49982757  0.83995493  9.8326153   7.80481708  9.57501074
 -0.44038284 11.95029814  1.56213115 -3.15268015 -5.70601034 -2.78883762
  2.54038841  4.27687579 -2.70520757 -3.39840734 24.94790156]
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5148.569044616895
gradient value of function right now is: [ 4.47333459e-02  3.32541855e-03  2.34504206e-01  2.65202325e-03
 -4.47333459e-02 -3.32541855e-03 -2.34504206e-01 -2.65202325e-03
  4.89928523e-01  3.15561719e-02  4.98008073e-01  2.53880083e-02
 -1.68862475e-03 -5.56697097e-05 -1.70723036e-03 -4.41375613e-05
  1.75954319e+00  1.10907060e-01  1.78853368e+00  8.91523831e-02
 -2.27613911e-03 -7.39713691e-05 -2.30097774e-03 -5.86281323e-05
 -3.85481803e-02 -3.27398491e-03 -4.90524966e-02 -1.03975306e-01
  6.51379065e-02  5.79132960e-03  8.32529536e-02  1.77400875e-01
 -3.67002228e-02 -3.20665021e-03 -4.68581397e-02 -9.97192359e-02
  2.56709966e-02  2.28330403e-03  3.28333407e-02  7.00233781e-02
 -6.97275811e-03 -4.87535426e-02 -4.79373631e-01 -1.12015620e-02
  7.73467313e-04  4.10889629e-03  4.04364162e-02  1.17126237e-03
 -8.13399558e-03 -5.58896894e-02 -5.49774738e-01 -1.29907406e-02
 -2.30883456e-02 -1.51734917e-01 -1.48756502e+00 -3.65161004e-02
  3.94993791e-02 -3.34382725e-01  7.58328283e-01 -3.25939535e+00
 -6.51063342e-04  5.46464815e-03  1.10971738e-01 -7.86667299e-01
 -6.21617341e+00]
supnorm grad right now is: 6.216173414676319
Weights right now are: 
[-6.43495642  0.20647713 -4.51049267  0.1757346   5.90348615 -0.34047257
  4.67519971 -0.74179072  2.1174182  -3.24142621  2.48508261  0.46859484
  2.36428875 13.57713914  3.73388496 13.57206274  1.39425893 -4.4988479
  1.54577695 -2.7276013   2.81995024 14.24031553  3.49067818 13.87561336
 -1.9539671   5.33138059 -2.51136458 -4.63277839  3.01006988 -3.51001607
  3.25791162  5.61736451 -4.90148927  4.8479549  -3.44452313 -3.68661963
  4.07144998  5.41746398  3.25814057  5.68493822  8.53170793  8.58047018
  0.72680143 11.13900966 -5.41954174 -2.61655699  5.10854165 -3.60670053
  8.65133315  7.53353524  0.57340354  9.85634886  7.76480057  9.6385176
 -0.68163927 11.99059474  1.50476431 -3.23105367 -5.77858087 -2.7727327
  2.57669385  4.19981149 -2.80271747 -3.37998046 24.84617499]
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5147.5187191223895
gradient value of function right now is: [-3.03611274e-02  4.97359218e-03 -1.62954547e-01  4.06798227e-03
  3.03611274e-02 -4.97359218e-03  1.62954547e-01 -4.06798227e-03
 -2.62068552e-01 -6.59939812e-02 -2.75467539e-01 -5.35173305e-02
 -2.46079296e-03  6.16590959e-05 -2.46436096e-03  5.11016016e-05
 -8.26103905e-01 -2.25015615e-01 -8.72464330e-01 -1.82144189e-01
 -3.56684987e-03  8.50238860e-05 -3.57295512e-03  7.05832045e-05
  6.14407226e-02  7.51978485e-03  7.56884154e-02  1.60315545e-01
 -1.38794596e-01 -1.57810723e-02 -1.68396386e-01 -3.51522668e-01
  6.55387203e-02  7.51516374e-03  7.97440482e-02  1.67235988e-01
 -6.00355528e-02 -6.75142101e-03 -7.26841777e-02 -1.51392136e-01
  5.40038048e-03  9.05098169e-02  9.43322042e-01  1.55714580e-02
 -5.32972132e-04 -1.05533603e-02 -1.09852461e-01 -1.68834477e-03
  5.62921866e-03  9.69897956e-02  1.01334336e+00  1.64543019e-02
  1.48263372e-02  2.66043512e-01  2.88274177e+00  4.41548030e-02
 -3.49799427e-02  3.68598504e-01 -1.10197872e+00  7.24768828e+00
 -1.65278604e-04  1.81342079e-03 -2.06803197e-01  1.60884395e+00
  4.19407766e+00]
supnorm grad right now is: 7.247688282855202
Weights right now are: 
[-6.56324588  0.17300326 -4.68008768  0.14323496  6.03177562 -0.3069987
  4.84479472 -0.70929107  2.04120065 -3.77051591  2.40885938 -0.08997696
  2.37088919 13.56376082  3.74044602 13.55953841  1.29191784 -4.85407373
  1.44349094 -3.07242279  2.82909439 14.21723582  3.49975851 13.85370058
 -1.79122841  5.69866032 -2.30609124 -4.43225518  2.79103261 -4.03533599
  2.98606944  5.34577334 -4.7014915   5.23773387 -3.19827924 -3.43644205
  3.80266851  4.89352921  2.93866736  5.35599931  8.74734829  9.04380371
  1.00716284 11.56286076 -5.56419752 -2.87665719  4.64660914 -3.75039391
  8.88184843  7.98160935  0.90238664 10.28274457  8.06745236 10.22208811
 -0.25932485 12.54996662  1.13093103 -2.92598232 -6.30262357 -2.19500417
  2.68366429  3.89868801 -3.3240883  -2.78496459 25.11061682]
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5147.3879115290365
gradient value of function right now is: [-2.67212887e-02  2.72061427e-03 -1.40731480e-01  2.19309386e-03
  2.67212887e-02 -2.72061427e-03  1.40731480e-01 -2.19309386e-03
 -2.52294802e-01 -4.54842250e-02 -2.64907692e-01 -3.62960979e-02
 -7.66557766e-04  3.10620930e-05 -7.60964047e-04  2.47879292e-05
 -8.01617664e-01 -1.56833029e-01 -8.45970343e-01 -1.24853924e-01
 -1.50525488e-03  5.88460627e-05 -1.49489958e-03  4.69775163e-05
  4.97682392e-02  5.12749893e-03  5.87458861e-02  1.21864695e-01
 -1.13615736e-01 -1.11461860e-02 -1.32716523e-01 -2.71944507e-01
  5.05623944e-02  4.95548606e-03  5.91342710e-02  1.21555314e-01
 -4.88681874e-02 -4.75797522e-03 -5.69954119e-02 -1.16565135e-01
  3.23004478e-03  6.91103364e-02  7.73847006e-01  1.38886913e-02
 -2.86197468e-04 -6.89049541e-03 -7.55856765e-02 -1.30928578e-03
  3.29160259e-03  7.16748532e-02  8.01818779e-01  1.42832211e-02
  8.47745535e-03  1.89664547e-01  2.18014300e+00  3.72349061e-02
 -1.91593545e-02  2.13786909e-01 -6.88362470e-01  5.26893366e+00
 -3.01179928e-05  4.78333254e-04 -1.68961117e-01  1.40359259e+00
 -1.16073819e+00]
supnorm grad right now is: 5.268933660484997
Weights right now are: 
[-6.60353485  0.07706266 -4.7322549   0.05113278  6.07206458 -0.2110581
  4.89696194 -0.6171889   1.9984606  -3.79319647  2.36973386 -0.13301078
  2.38424373 13.52711032  3.75375516 13.52217906  1.23680391 -4.86996865
  1.39257571 -3.10234277  2.85027743 14.15975007  3.52087499 13.79475939
 -1.99219069  5.6428574  -2.48577183 -4.6483307   3.00871459 -4.02646923
  3.17120651  5.55812338 -4.92488872  5.20874977 -3.39029254 -3.66056921
  4.02595481  4.88418355  3.12468368  5.57102004  8.66420112  9.12823646
  0.82258208 11.585898   -5.54760425 -2.97922457  4.7221073  -3.77758226
  8.82371093  8.08288363  0.75744366 10.32918619  8.02567751 10.36408812
 -0.4033606  12.63517354  1.06222982 -3.06855403 -6.41058911 -2.18834136
  2.7142651   3.7991995  -3.57033131 -2.72910055 24.77392142]
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5148.961262821122
gradient value of function right now is: [-1.36404721e-02 -1.65642833e-03 -7.57358116e-02 -1.29116853e-03
  1.36404721e-02  1.65642833e-03  7.57358116e-02  1.29116853e-03
 -1.67754267e-01 -1.17980470e-03 -1.69920397e-01 -8.29301552e-04
  4.12705212e-05  4.86730140e-07  4.20369862e-05  3.16721747e-07
 -6.52517884e-01 -4.49501528e-04 -6.60941584e-01  3.98952472e-04
  5.19837037e-04  6.20078991e-06  5.29492045e-04  4.04824185e-06
  8.25863056e-03  1.25327973e-04  8.48496337e-03  1.61934504e-02
 -6.82211691e-03  2.09761091e-05 -6.37760291e-03 -1.12729462e-02
  4.90991927e-03  2.90395959e-05  4.84886485e-03  8.91989351e-03
 -1.98614049e-03  3.88467658e-05 -1.72088873e-03 -2.86699149e-03
  1.09883095e-04 -5.35436066e-03  4.53625823e-02  1.18502612e-03
 -7.90098609e-06  1.86697348e-04 -9.30289710e-04 -8.49765559e-05
  9.68412785e-05 -5.68186745e-03  3.89554061e-02  1.17736033e-03
 -7.26952994e-05 -1.76881744e-02  2.71716757e-02  1.85211996e-03
 -2.03453448e-03  1.17563395e-02  3.63644841e-01 -9.62577503e-01
  5.82393833e-05 -1.24915597e-04 -8.74485123e-03  9.06547602e-02
 -4.48343923e+00]
supnorm grad right now is: 4.483439233210092
Weights right now are: 
[-6.74717729 -0.06162664 -4.88692398 -0.08002648  6.21570703 -0.07236879
  5.05163102 -0.48602964  1.82742166 -3.62225489  2.20470277  0.03002744
  2.39849591 13.52244821  3.76803195 13.51664527  1.01883276 -4.73483609
  1.18126465 -2.98130715  2.87600234 14.14345578  3.54662181 13.77681332
 -2.11566087  5.4974546  -2.61323524 -4.79746272  3.25411892 -3.82275066
  3.40557848  5.80539818 -5.10688569  5.06762359 -3.56450646 -3.85136984
  4.29024012  5.08482794  3.37447753  5.83429993  8.59249782  8.94312976
  0.59040108 11.49394336 -5.53144615 -2.91069535  4.91452339 -3.78015927
  8.76828956  7.93167226  0.54229424 10.26273833  7.97500174 10.2077423
 -0.64204046 12.58811858  1.00988464 -3.15607022 -6.19823642 -2.49069142
  2.72226541  3.7800848  -3.5062592  -2.88699183 24.74524805]
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -5148.742988739787
gradient value of function right now is: [-3.64693365e-02 -1.17262387e-03 -2.09975209e-01 -8.91745239e-04
  3.64693365e-02  1.17262387e-03  2.09975209e-01  8.91745239e-04
 -4.08718349e-01 -2.51754779e-02 -4.20782683e-01 -1.98269424e-02
 -1.46458369e-04 -1.35472184e-05 -1.55647895e-04 -1.02738971e-05
 -1.55007472e+00 -9.38684147e-02 -1.59734138e+00 -7.35986024e-02
  1.87153331e-04  1.73193743e-05  1.98937409e-04  1.31296216e-05
  4.15288278e-02  2.69689972e-03  4.68198425e-02  9.18498876e-02
 -7.83296036e-02 -5.44579785e-03 -8.85473943e-02 -1.72204678e-01
  3.65286778e-02  2.42184983e-03  4.11481681e-02  8.01506448e-02
 -3.21949224e-02 -2.26566171e-03 -3.64247003e-02 -7.07511188e-02
  2.08627687e-03  5.51150026e-02  5.68726683e-01  9.73025800e-03
 -1.64436556e-04 -3.75660651e-03 -3.53803421e-02 -7.19601426e-04
  2.12873369e-03  5.63939126e-02  5.74880016e-01  9.98698315e-03
  5.06418058e-03  1.37568552e-01  1.37919666e+00  2.42721135e-02
 -1.45851117e-02  1.19187723e-01 -5.11778329e-01  3.43762279e+00
  6.69524889e-04 -4.37970015e-03 -1.34824879e-01  8.59855944e-01
  4.21144334e+00]
supnorm grad right now is: 4.211443337087511
Weights right now are: 
[-6.73698849e+00 -1.41397343e-01 -4.88563339e+00 -1.54990164e-01
  6.20551823e+00  7.40190657e-03  5.05034043e+00 -4.11065955e-01
  1.84916383e+00 -3.61691137e+00  2.22824628e+00  2.51463337e-02
  2.40472981e+00  1.35240903e+01  3.77428520e+00  1.35179154e+01
  1.01590003e+00 -4.74911575e+00  1.18028575e+00 -3.00320686e+00
  2.88906891e+00  1.41406121e+01  3.55966273e+00  1.37742775e+01
 -2.16952881e+00  5.49775865e+00 -2.65296429e+00 -4.85723944e+00
  3.34299321e+00 -3.80239835e+00  3.47527306e+00  5.89217930e+00
 -5.15581471e+00  5.07582264e+00 -3.59756656e+00 -3.90185747e+00
  4.37090764e+00  5.09900202e+00  3.43568763e+00  5.91343918e+00
  8.55676200e+00  9.11478057e+00  5.53337824e-01  1.14659520e+01
 -5.52001981e+00 -2.97715847e+00  4.93965176e+00 -3.77483541e+00
  8.73963206e+00  8.09352874e+00  5.21914299e-01  1.02485456e+01
  7.94420133e+00  1.03748328e+01 -6.61806006e-01  1.25742048e+01
  1.00717619e+00 -3.22965814e+00 -6.45618387e+00 -2.32976697e+00
  2.80040801e+00  3.61718435e+00 -3.68625882e+00 -2.91796020e+00
  2.51659636e+01]
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.409535203008
gradient value of function right now is: [ 1.12899830e-02  8.19627865e-03  6.68259651e-02  6.38021276e-03
 -1.12899830e-02 -8.19627865e-03 -6.68259651e-02 -6.38021276e-03
  1.98768634e-01 -3.86114505e-02  1.92710585e-01 -3.28411752e-02
  8.57349883e-05 -5.21165840e-07  8.56758274e-05 -4.24726506e-07
  9.95753198e-01 -1.29943072e-01  9.73850927e-01 -1.10085218e-01
 -2.44067654e-03  1.45997928e-05 -2.43905588e-03  1.18952329e-05
  2.33170173e-02  3.62892491e-03  2.86977444e-02  5.38299588e-02
 -5.28160531e-02 -7.89545454e-03 -6.38359096e-02 -1.17120186e-01
  2.55149370e-02  3.72242245e-03  3.08386275e-02  5.70269299e-02
 -2.37767327e-02 -3.53840016e-03 -2.86968402e-02 -5.25326106e-02
  4.49108116e-04  9.09495424e-02  4.50637422e-01  2.62660567e-03
 -7.41782272e-05 -1.03516046e-02 -4.81386209e-02 -3.13426599e-04
  4.83629960e-04  9.78045669e-02  4.80852909e-01  2.78243906e-03
  1.14016247e-03  2.46567389e-01  1.23360569e+00  6.75127125e-03
 -3.12819005e-03  2.59700918e-02 -7.48573074e-01  5.49426253e+00
  4.49205656e-04 -3.36835827e-03 -3.26776643e-02  2.41298028e-01
  6.90363419e+00]
supnorm grad right now is: 6.903634189882232
Weights right now are: 
[-6.41923366 -0.07258226 -4.54586211 -0.09349145  5.8877634  -0.06141317
  4.71056915 -0.47256467  2.18785347 -3.56610878  2.56669872  0.05663743
  2.41135386 13.53422263  3.78095483 13.52757047  1.37604772 -4.7180766
  1.54029813 -2.99163911  2.88757071 14.13611982  3.55812877 13.77051682
 -2.37504708  5.41382391 -2.82430752 -5.05505492  3.55810717 -3.69867485
  3.65575595  6.10129048 -5.32898244  5.0136302  -3.73756968 -4.0702178
  4.57406043  5.18690866  3.60351538  6.11308613  8.02141176  9.37907706
  0.38954967 10.87205364 -5.36646274 -3.1027308   5.04998194 -3.63119239
  8.28007019  8.35373864  0.39075356  9.72553043  7.477539   10.62893822
 -0.79575392 12.04140595  1.20303064 -3.8648895  -7.14940114 -1.83356946
  2.8274099   3.85658148 -3.71268138 -3.44216343 25.23655589]
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5149.553386069218
gradient value of function right now is: [ 3.02039611e-02  3.72487448e-03  1.94359482e-01  2.82884040e-03
 -3.02039611e-02 -3.72487448e-03 -1.94359482e-01 -2.82884040e-03
  3.34419026e-01  1.82623924e-02  3.39270610e-01  1.51283240e-02
  7.28490050e-04  8.76995296e-06  7.30176518e-04  7.34781572e-06
  1.48782196e+00  8.10312903e-02  1.50913562e+00  6.73812634e-02
 -5.62775193e-04 -6.64432407e-06 -5.64036167e-04 -5.56810865e-06
 -2.14337522e-02 -1.96042140e-03 -2.57302082e-02 -4.55005440e-02
  4.01528510e-02  3.63569934e-03  4.80987915e-02  8.52118963e-02
 -2.08727437e-02 -1.89862617e-03 -2.50372634e-02 -4.43830606e-02
  1.70896793e-02  1.53735072e-03  2.04666332e-02  3.62513801e-02
 -3.66550268e-03 -6.83246419e-02 -3.59464720e-01 -1.02795053e-02
  2.64380964e-04  3.93810398e-03  2.12520540e-02  7.36665303e-04
 -3.89734284e-03 -7.26025113e-02 -3.82629252e-01 -1.08877209e-02
 -9.21337810e-03 -1.74080277e-01 -9.19137955e-01 -2.58880324e-02
  1.50200423e-02 -1.34786118e-01  1.10825764e+00 -4.74082890e+00
 -2.52521869e-03  2.34408910e-02  5.80679397e-02 -5.32712144e-01
 -1.60610882e+00]
supnorm grad right now is: 4.740828897173907
Weights right now are: 
[-6.35347402 -0.16771933 -4.47300374 -0.18258033  5.82200376  0.03372389
  4.63771078 -0.38347579  2.22576995 -3.16823018  2.60806524  0.46997559
  2.41414706 13.53647014  3.78377228 13.52940233  1.39419402 -4.39361006
  1.56205976 -2.67075849  2.89972594 14.12184396  3.57020546 13.75626816
 -2.63685188  5.06600576 -3.07988161 -5.30379907  3.83414547 -3.29500397
  3.92637932  6.35830072 -5.60425222  4.65876019 -4.00403125 -4.32652036
  4.87461488  5.61676465  3.89836908  6.39363063  8.01971662  8.95830094
  0.0745895  10.9602185  -5.33774418 -2.73113607  5.42874952 -3.62049482
  8.27830599  7.9593483   0.08582872  9.80844198  7.46914919 10.21599049
 -1.10102749 12.10985398  1.13021369 -3.86648516 -6.82184221 -2.36490749
  2.9795457   3.52403458 -3.81865712 -3.38512275 25.06429578]
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5150.024606740468
gradient value of function right now is: [ 1.65275118e-02  3.00323237e-03  1.07879461e-01  2.30004516e-03
 -1.65275118e-02 -3.00323237e-03 -1.07879461e-01 -2.30004516e-03
  1.93801388e-01  3.71706852e-03  1.94694537e-01  2.96749072e-03
  5.17101682e-04  1.04621901e-06  5.15325703e-04  1.13596384e-06
  8.99775325e-01  1.87982591e-02  9.03991871e-01  1.53495069e-02
 -5.14315279e-04 -9.54988007e-07 -5.12514003e-04 -1.06019679e-06
 -4.27378892e-03 -4.77714883e-04 -5.28070363e-03 -9.85722570e-03
  1.06205625e-02  8.51139654e-04  1.26277564e-02  2.41675889e-02
 -4.85946563e-03 -4.67135166e-04 -5.91304516e-03 -1.13017375e-02
  4.71850443e-03  3.49620759e-04  5.57856925e-03  1.06887040e-02
 -1.84480328e-03 -1.58519583e-02 -8.97113609e-02 -2.94134616e-03
  1.34635858e-04  6.60955599e-04  5.11774530e-03  2.12310804e-04
 -1.89933391e-03 -1.62981747e-02 -9.46456627e-02 -2.89787792e-03
 -4.62908126e-03 -4.32077414e-02 -2.59233100e-01 -7.47919468e-03
  4.86477883e-03 -7.07385999e-02  5.23138784e-01 -1.88971182e+00
  5.86187155e-04  5.06442882e-03 -1.87032689e-02 -9.79691956e-02
 -1.56903702e-01]
supnorm grad right now is: 1.889711819448073
Weights right now are: 
[-6.37189858e+00 -1.55832483e-01 -4.49609328e+00 -1.73943653e-01
  5.84042832e+00  2.18370462e-02  4.66080032e+00 -3.92112466e-01
  2.20999738e+00 -3.13871079e+00  2.59107157e+00  5.09237681e-01
  2.42386033e+00  1.35334180e+01  3.79351005e+00  1.35252213e+01
  1.36530956e+00 -4.44097741e+00  1.53136138e+00 -2.71083143e+00
  2.90585529e+00  1.41275181e+01  3.57632600e+00  1.37624712e+01
 -2.50742810e+00  5.09472320e+00 -2.94848774e+00 -5.18328941e+00
  3.70275432e+00 -3.27004755e+00  3.79722575e+00  6.23773130e+00
 -5.46062222e+00  4.66617311e+00 -3.86165948e+00 -4.19407220e+00
  4.74234381e+00  5.65146586e+00  3.76941617e+00  6.27200701e+00
  8.28796266e+00  9.10048054e+00  2.36272421e-01  1.14148476e+01
 -5.31042818e+00 -2.67988064e+00  5.43041608e+00 -3.62795373e+00
  8.51545050e+00  8.08787669e+00  2.43051455e-01  1.02259676e+01
  7.68870318e+00  1.03451942e+01 -9.47675189e-01  1.25070572e+01
  8.36951945e-01 -3.57912378e+00 -6.78589289e+00 -2.37792399e+00
  3.25573617e+00  3.01907892e+00 -4.02779245e+00 -3.02477782e+00
  2.51170157e+01]
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5147.688114237161
gradient value of function right now is: [-2.65195216e-02  3.54092649e-04 -1.73755012e-01  2.66218926e-04
  2.65195216e-02 -3.54092649e-04  1.73755012e-01 -2.66218926e-04
 -2.77832086e-01 -2.75923444e-02 -2.89280272e-01 -2.19201420e-02
  2.90655141e-04 -5.09665456e-05  2.63296688e-04 -3.91834785e-05
 -1.12577713e+00 -1.16773115e-01 -1.17653782e+00 -9.22914660e-02
  4.11555539e-05 -7.10743650e-06  3.73316296e-05 -5.46274961e-06
  4.42822707e-02  2.75487617e-03  4.90032835e-02  9.65112215e-02
 -8.94257289e-02 -5.72388448e-03 -9.88843176e-02 -1.92617126e-01
  4.14663875e-02  2.56391616e-03  4.57566855e-02  8.94355370e-02
 -3.69439054e-02 -2.37991797e-03 -4.08625037e-02 -7.94743319e-02
  4.58266143e-03  5.28001829e-02  6.36628681e-01  2.01026046e-02
 -2.23561203e-04 -2.94234732e-03 -3.22148163e-02 -1.05708919e-03
  4.47576874e-03  5.25796986e-02  6.26207784e-01  1.98587498e-02
  1.07606767e-02  1.30520874e-01  1.54928327e+00  4.86537129e-02
 -2.94801243e-02  2.91215049e-01 -5.11303446e-01  3.74513414e+00
  5.22411924e-03 -3.75551348e-02 -2.56336415e-01  1.94581790e+00
  2.38557373e+00]
supnorm grad right now is: 3.74513414334555
Weights right now are: 
[-6.67103384 -0.31554485 -4.84285961 -0.32228361  6.13956358  0.18154942
  5.00756664 -0.24377251  1.91558614 -3.26304828  2.29694798  0.39710234
  2.41749998 13.52920453  3.78719491 13.5198663   1.0112491  -4.65142489
  1.17722878 -2.90883383  2.91759573 14.14419022  3.58808713 13.77978321
 -2.23068839  5.31373259 -2.67653381 -4.92411905  3.43895558 -3.46185306
  3.538945    5.98881456 -5.17703228  4.86896325 -3.58474991 -3.92636877
  4.47898613  5.46685735  3.51230875  6.02207298  8.97808537  9.33093164
  0.53339961 12.15758007 -5.48985907 -2.7388534   5.25485705 -3.80851775
  9.1416556   8.30185188  0.5317555  10.91852402  8.30497698 10.57005254
 -0.65605465 13.20519857  0.32348882 -2.86542996 -6.8220388  -2.275988
  3.47774761  2.24493744 -4.15723554 -2.4976486  25.0381319 ]
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5147.218395536985
gradient value of function right now is: [ 5.84458829e-03 -4.79797789e-04  3.97923161e-02 -3.44405708e-04
 -5.84458829e-03  4.79797789e-04 -3.97923161e-02  3.44405708e-04
  5.58570586e-02  8.77127828e-03  5.90168922e-02  6.94520515e-03
 -3.90496869e-04  2.53876065e-05 -3.78813268e-04  1.98345916e-05
  2.21874380e-01  3.71744337e-02  2.36620147e-01  2.92534620e-02
 -1.08286117e-04  7.04894748e-06 -1.05037992e-04  5.50688855e-06
 -1.23357135e-02 -9.87810617e-04 -1.37445843e-02 -2.60832804e-02
  2.39728945e-02  2.02665788e-03  2.64491770e-02  4.99193346e-02
 -1.18426572e-02 -9.54428969e-04 -1.30960419e-02 -2.47439172e-02
  9.48207650e-03  7.93671504e-04  1.04416286e-02  1.97009077e-02
 -6.31023083e-03 -2.29337659e-02 -1.95663365e-01 -6.89106501e-03
  5.04276971e-04  1.23970030e-03  1.05986800e-02  5.10361459e-04
 -6.29362790e-03 -2.23798733e-02 -1.92359104e-01 -6.87144358e-03
 -1.57153236e-02 -5.19698157e-02 -4.60230691e-01 -1.68994570e-02
  3.79786996e-02 -2.89515025e-01  2.61192903e-01 -1.28423643e+00
 -5.41371226e-03  3.38138493e-02  7.51656253e-02 -4.67477814e-01
 -8.76532674e+00]
supnorm grad right now is: 8.765326739119743
Weights right now are: 
[-6.5978766  -0.41947407 -4.76422484 -0.41732422  6.06640634  0.28547864
  4.92893188 -0.1487319   1.96387228 -2.97397428  2.35148628  0.6774096
  2.4046716  13.59768164  3.7746486  13.58815296  1.03447801 -4.3741569
  1.20759976 -2.64820365  2.91947535 14.15944163  3.59001325 13.79527367
 -2.63121707  5.09641264 -3.05320356 -5.30743427  3.89768464 -3.2213689
  3.9667189   6.41350523 -5.58940112  4.66901287 -3.96684517 -4.31138025
  4.94227005  5.70277769  3.94331421  6.45134464  9.33766846  9.01588798
  0.10479317 12.01106432 -5.62819683 -2.58410013  5.48055157 -3.79777118
  9.46810771  8.01572497  0.13659763 10.79273743  8.65734771 10.28129911
 -1.04271683 13.1084092   0.31698784 -2.59381426 -6.4473873  -2.68021474
  2.80191781  2.61128493 -3.41871265 -3.02289595 24.50188692]
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5148.451347252529
gradient value of function right now is: [ 1.08808527e-02  5.77755519e-04  7.62906102e-02  4.21500476e-04
 -1.08808527e-02 -5.77755519e-04 -7.62906102e-02 -4.21500476e-04
  1.20806184e-01  6.90467946e-03  1.23047404e-01  5.35788040e-03
  2.93766437e-04  1.84314221e-05  2.97983015e-04  1.46423374e-05
  5.73040667e-01  3.08575493e-02  5.83477225e-01  2.39427622e-02
  5.66424422e-05  3.59275494e-06  5.74605284e-05  2.85473528e-06
 -9.65391114e-03 -1.00387573e-03 -1.09419586e-02 -2.19451734e-02
  1.89813913e-02  1.82912446e-03  2.09676589e-02  4.23621298e-02
 -9.88214117e-03 -9.77767986e-04 -1.10348154e-02 -2.22731936e-02
  7.48678728e-03  7.00785445e-04  8.23692772e-03  1.66587313e-02
 -8.17623554e-03 -1.68347527e-02 -1.54906083e-01 -5.00180913e-03
  7.15830517e-04  9.69817122e-04  1.03413403e-02  4.06375769e-04
 -8.08878029e-03 -1.61702240e-02 -1.52337123e-01 -4.93403898e-03
 -2.22312869e-02 -3.94336540e-02 -3.99409552e-01 -1.30572807e-02
  7.10077089e-02 -4.52784645e-01  2.22817430e-01 -1.07320111e+00
 -7.25190279e-03  4.16136753e-02  6.63264005e-02 -3.93858045e-01
 -6.89515452e+00]
supnorm grad right now is: 6.895154523815586
Weights right now are: 
[-6.52469883 -0.35276915 -4.67054463 -0.36128457  5.99322856  0.21877371
  4.83525167 -0.20477155  2.04459038 -2.95326833  2.42993748  0.70759724
  2.42680682 13.54212712  3.7965612  13.53325622  1.13411354 -4.37736884
  1.30430624 -2.64240902  2.92794424 14.1443466   3.59842579 13.78015882
 -2.51081296  5.13152025 -2.94369384 -5.20047476  3.82201367 -3.21602631
  3.90362657  6.35201985 -5.47496881  4.68699614 -3.86615966 -4.21340624
  4.88227544  5.72310933  3.89545129  6.40436581  9.96899211  9.05670897
  0.20983154 12.29566954 -5.78765232 -2.5409683   5.47212902 -3.89630412
 10.02874412  8.04026485  0.22671939 11.04959043  9.20425992 10.29321228
 -0.96848027 13.36342223 -0.17793378 -1.97975318 -6.39304064 -2.71640896
  3.00815757  2.16932002 -3.36789356 -2.92222272 24.63109073]
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5148.3578864515175
gradient value of function right now is: [ 5.33701354e-02  6.11504576e-03  3.90451420e-01  4.54608730e-03
 -5.33701354e-02 -6.11504576e-03 -3.90451420e-01 -4.54608730e-03
  5.82953522e-01  3.71932267e-02  5.92217607e-01  3.12921729e-02
  3.42734759e-03  5.11271633e-05  3.45876094e-03  4.02207435e-05
  2.86946517e+00  1.68443535e-01  2.91417633e+00  1.41401393e-01
  7.48498125e-04  1.10703523e-05  7.55343683e-04  8.70282230e-06
 -4.22341883e-02 -3.71638954e-03 -4.80744743e-02 -8.93118640e-02
  6.72925483e-02  6.88475229e-03  7.76527749e-02  1.43468340e-01
 -3.89579683e-02 -3.69943345e-03 -4.46103689e-02 -8.27404782e-02
  2.65661063e-02  2.73826816e-03  3.07377609e-02  5.66987390e-02
 -2.95576031e-02 -9.50215260e-02 -6.22201462e-01 -2.71002730e-02
  1.89877525e-03  5.60825711e-03  3.35923238e-02  1.76590837e-03
 -3.02417092e-02 -9.98121910e-02 -6.41239789e-01 -2.79693367e-02
 -7.54656485e-02 -2.38517876e-01 -1.53603093e+00 -6.91534080e-02
  1.89683224e-01 -1.19559293e+00  1.12301584e+00 -4.37089368e+00
 -2.97836625e-02  1.54718658e-01  3.03686960e-01 -1.60567413e+00
 -5.45484738e+00]
supnorm grad right now is: 5.454847381507529
Weights right now are: 
[-6.41078699 -0.39355062 -4.54382736 -0.40079202  5.87931673  0.25955518
  4.70853439 -0.1652641   2.14130195 -2.70125566  2.5293877   0.94309464
  2.42592476 13.57030042  3.79574739 13.56206464  1.22793386 -4.11847662
  1.40110573 -2.40521475  2.93110759 14.15300439  3.60161485 13.78891258
 -2.75461563  4.77328979 -3.17509233 -5.44854724  4.10873393 -2.82927795
  4.17543788  6.63666953 -5.74294732  4.32549833 -4.11849301 -4.48275444
  5.16451967  6.09608877  4.16253676  6.68459789  9.3680653   8.86251524
 -0.0676437  11.99093164 -5.34781016 -2.31644058  5.81868007 -3.7292687
  9.44521338  7.86298266 -0.03496937 10.7702242   8.61480898 10.09526414
 -1.25338244 13.06494555  0.37730791 -2.67223867 -6.23128481 -2.90848903
  2.42310637  2.82253553 -2.97469279 -3.33508964 25.13961853]
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5148.557829772481
gradient value of function right now is: [-4.04855080e-03  2.11446959e-03 -2.25073566e-02  1.65407687e-03
  4.04855080e-03 -2.11446959e-03  2.25073566e-02 -1.65407687e-03
 -9.92707505e-03 -1.99877176e-02 -1.60637233e-02 -1.59899386e-02
  1.14271016e-03 -3.57635022e-05  1.11723883e-03 -2.61150593e-05
  8.45934117e-02 -7.53059106e-02  5.77320095e-02 -5.93853167e-02
  2.72699678e-04 -8.52235052e-06  2.66619152e-04 -6.22279909e-06
  3.02976376e-02  2.88443164e-03  3.15935288e-02  6.99165774e-02
 -4.27210411e-02 -5.09026676e-03 -4.48418167e-02 -9.64408649e-02
  2.83000122e-02  2.77964791e-03  2.94870472e-02  6.37751703e-02
 -1.69630912e-02 -2.11490796e-03 -1.78127693e-02 -3.82756684e-02
  2.50955504e-02  1.09531245e-02  3.61151631e-01  1.30775796e-02
 -2.01917724e-03 -2.30514348e-03 -3.12248133e-02 -1.24388187e-03
  2.40464787e-02  1.05661068e-02  3.38468176e-01  1.26852276e-02
  6.11829633e-02  2.99658641e-02  8.69361423e-01  3.32039761e-02
 -1.85817568e-01  1.62671791e+00  3.81941198e-01 -2.06109991e-01
  5.64036404e-03 -4.99272009e-02 -1.21714462e-01  1.09685486e+00
  6.49280972e+00]
supnorm grad right now is: 6.492809716742236
Weights right now are: 
[-6.39423218 -0.28201654 -4.51280572 -0.32571537  5.86276191  0.1480211
  4.67751276 -0.24034075  2.20851779 -3.1071974   2.59141759  0.53864382
  2.51061921 13.54957054  3.88033196 13.53876517  1.33116076 -4.55817605
  1.49870925 -2.83791525  2.97927559 14.17939397  3.64983554 13.81498698
 -2.33840469  5.26929072 -2.76924784 -5.02899367  3.74372299 -3.26382368
  3.82226079  6.27234056 -5.30164724  4.82193464 -3.69231006 -4.04271627
  4.80683813  5.68030916  3.81855843  6.32648999 10.27029229  9.21205047
  0.36686176 12.76647495 -6.07610031 -2.56512742  5.42184147 -4.2069398
 10.36071078  8.19323409  0.38437074 11.51872968  9.52540438 10.43927582
 -0.81780901 13.84951701 -0.02939268 -2.01532309 -6.4418655  -2.67389399
  2.70116735  2.26745226 -3.26462936 -2.79952353 25.30739038]
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.666722549769
gradient value of function right now is: [ 1.58136450e-05 -1.31493619e-06  1.35929179e-05 -9.22215171e-07
 -1.58136521e-05  1.31493619e-06 -1.35929252e-05  9.22215171e-07
  6.55661736e-06  8.42121921e-05  8.86634282e-06  6.62736741e-05
 -1.77953489e-06  6.57672865e-09 -1.82804941e-06  8.37677710e-09
 -7.25846140e-04  1.21823924e-04 -7.41652166e-04  9.70565319e-05
 -9.24422056e-07  2.42462621e-09 -9.49665191e-07  3.57354456e-09
 -2.19295684e-06 -3.61794763e-06 -2.39748721e-06 -2.26401864e-05
  5.62753959e-05  1.60503386e-05  5.89099187e-05  1.94156500e-04
 -1.09386669e-05 -4.79092580e-06 -1.15072581e-05 -4.60848703e-05
  2.18122190e-05  5.92488141e-06  2.28247224e-05  7.46384821e-05
 -4.33748280e-05 -9.95996308e-07 -3.05635016e-04 -8.06179735e-07
  7.54190510e-06  2.20758538e-07  8.30099229e-05  1.99194956e-07
 -4.07344214e-05 -9.39348897e-07 -2.89806561e-04 -7.60461957e-07
 -1.47267199e-04 -3.58667673e-06 -1.42985337e-03 -2.94200988e-06
  1.79041920e-03 -1.29387306e-02  4.22874378e-05 -2.95325369e-04
  2.61347250e-06 -1.87725271e-05  4.23248544e-05 -3.17970262e-04
  4.26008699e+00]
supnorm grad right now is: 4.260086986805352
Weights right now are: 
[-7.35225031 -0.76362137 -5.49037526 -0.79732649  6.82078005  0.62962594
  5.6550823   0.23127037  1.27521493 -3.76517736  1.65365507 -0.08984821
  2.42875613 13.38714085  3.79768419 13.3826231   0.38917405 -5.25568486
  0.55132129 -3.49805472  2.9660976  14.14512673  3.63640333 13.78384313
 -1.21019429  6.16631003 -1.7152528  -3.91773414  2.83994204 -4.01337471
  2.98119752  5.37930099 -4.22981639  5.67583373 -2.69447946 -2.98488587
  3.91247029  4.93887219  2.98766362  5.43955604 11.39668916  9.74003711
  1.31300862 13.67662834 -6.98700363 -2.98724587  4.59370872 -4.7987835
 11.47049372  8.67829028  1.26640559 12.37760637 10.62900986 10.95655728
  0.0782852  14.75898754 -1.15361392 -0.70419928 -6.84189615 -2.19087527
  3.18967223  1.56153848 -4.00285209 -1.84683644 25.27463804]
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.2449285969205
gradient value of function right now is: [-7.07286705e-06 -6.88083679e-08 -2.83423502e-05 -4.60404283e-08
  7.07287342e-06  6.88083680e-08  2.83423567e-05  4.60404283e-08
 -7.68757727e-05 -5.15790111e-06 -7.97047825e-05 -3.84395294e-06
 -9.17745655e-08 -6.32123646e-09 -9.51479836e-08 -4.71547379e-09
 -1.39627224e-04 -9.45836900e-06 -1.44763466e-04 -7.04895246e-06
 -4.69603463e-08 -3.23631101e-09 -4.86864549e-08 -2.41427851e-09
  2.49062968e-06  2.19717658e-07  2.62112004e-06  6.36535157e-06
 -1.01538189e-05 -8.91167010e-07 -1.06857365e-05 -2.59301729e-05
  3.02233051e-06  2.65985882e-07  3.18066858e-06  7.72138605e-06
 -3.80577183e-06 -3.34396466e-07 -4.00514485e-06 -9.72006991e-06
  9.15289313e-07 -8.71107566e-13  4.72481194e-05 -2.41079504e-12
 -9.17156810e-08 -1.25364133e-12 -4.96167578e-06  8.50389917e-12
  8.65764797e-07 -8.54772423e-13  4.47117718e-05 -2.35906698e-12
  2.69881027e-06 -5.57921765e-12  1.41314077e-04 -1.43039548e-11
 -4.75715058e-05  6.80445616e-04  2.03815096e-11 -3.26576279e-10
 -2.68647884e-11  4.74370518e-10  7.84689678e-11 -1.37748763e-09
 -1.24347189e+00]
supnorm grad right now is: 1.243471889513526
Weights right now are: 
[-7.55669352 -0.85335823 -5.69279692 -0.88390835  7.02522326  0.71936279
  5.85750396  0.31785224  1.07205445 -3.8868176   1.44936838 -0.20389765
  2.40927753 13.3469536   3.77799225 13.34466606  0.19233881 -5.37688093
  0.35326524 -3.61079495  2.96029247 14.13107787  3.63051125 13.77088432
 -0.98389172  6.32301848 -1.50703354 -3.6927982   2.65270508 -4.14555236
  2.8101098   5.19184301 -4.0163464   5.82268333 -2.49897777 -2.77229028
  3.7289592   4.80969358  2.82026857  5.25532052 11.59247543  9.81616711
  1.49558271 13.80871793 -7.12569285 -3.04823995  4.43912144 -4.88134087
 11.66068708  8.74808622  1.43405505 12.50192574 10.81439264 11.03081539
  0.24855134 14.88933571 -1.40763695 -0.394515   -6.90039777 -2.11826141
  3.25115799  1.47761625 -4.12082307 -1.69624415 24.84768668]
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.174573483179
gradient value of function right now is: [-2.78032654e-06 -1.51596216e-08 -1.03180748e-05 -9.96492237e-09
  2.78032467e-06  1.51596216e-08  1.03180729e-05  9.96492237e-09
 -2.81690695e-05 -2.83958335e-06 -2.92041609e-05 -2.15756853e-06
 -1.96371480e-08 -2.03186586e-09 -2.03563179e-08 -1.54441399e-09
 -4.23783132e-05 -4.32142826e-06 -4.39333763e-05 -3.28259340e-06
 -9.90552208e-09 -1.02542068e-09 -1.02682677e-08 -7.79434764e-10
  8.63229717e-07  1.03697516e-07  9.10447155e-07  2.33190753e-06
 -4.00662890e-06 -4.80096310e-07 -4.22578130e-06 -1.08201698e-05
  1.11273840e-06  1.33482496e-07  1.17360283e-06  3.00538149e-06
 -1.46814108e-06 -1.76387184e-07 -1.54844642e-06 -3.96594766e-06
  6.55677736e-07 -2.64004892e-13  1.85415875e-05 -5.71515317e-13
 -8.77360250e-08 -2.35955651e-12 -2.55312368e-06  2.36638668e-12
  6.23754442e-07 -2.59416642e-13  1.76428208e-05 -5.66472159e-13
  2.16087344e-06 -1.78400205e-12  6.14929880e-05 -4.11185506e-12
 -3.57917649e-05  5.03046091e-04  7.73383817e-12 -1.12312953e-10
 -1.70770629e-11  2.87488083e-10  1.86201154e-11 -3.36769744e-10
  1.64551369e+00]
supnorm grad right now is: 1.6455136899121874
Weights right now are: 
[-7.58434724 -0.86543969 -5.72013072 -0.89556409  7.05287697  0.73144426
  5.88483776  0.32950797  1.044582   -3.90335801  1.42174139 -0.21940001
  2.40665276 13.34152048  3.77533865 13.33953899  0.16579003 -5.39327667
  0.3265507  -3.62604362  2.95950921 14.12917182  3.62971622 13.76912758
 -0.95334971  6.34419322 -1.47893367 -3.66242324  2.62732945 -4.16349546
  2.78692693  5.16640751 -3.98750788  5.84254507 -2.47256916 -2.74354943
  3.70409578  4.79216485  2.7975927   5.23033083 11.61894549  9.82641004
  1.52026418 13.82649027 -7.14447605 -3.05644649  4.41816027 -4.89244882
 11.68639613  8.75747693  1.45671634 12.5186526  10.8394805  11.0408065
  0.27159599 14.90687387 -1.44234538 -0.35204803 -6.90826898 -2.10849131
  3.25942902  1.46632818 -4.13669683 -1.67598016 25.09048092]
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.102033088611
gradient value of function right now is: [-2.33182502e-06 -1.13928351e-08 -8.53708700e-06 -7.46314068e-09
  2.33181841e-06  1.13928351e-08  8.53708027e-06  7.46314067e-09
 -2.33192809e-05 -2.50776362e-06 -2.41721111e-05 -1.91099025e-06
 -1.46735425e-08 -1.61769494e-09 -1.52083093e-08 -1.23289862e-09
 -3.38602722e-05 -3.68311196e-06 -3.50967454e-05 -2.80550312e-06
 -7.37966676e-09 -8.13932772e-10 -7.64859583e-09 -6.20335454e-10
  7.06783868e-07  8.94879741e-08  7.45662527e-07  1.92652365e-06
 -3.33359338e-06 -4.21213724e-07 -3.51696503e-06 -9.08454461e-06
  9.18262722e-07  1.16122248e-07  9.68773954e-07  2.50258668e-06
 -1.21428923e-06 -1.53867723e-07 -1.28108510e-06 -3.31011339e-06
  5.87718139e-07 -2.06286713e-13  1.54174008e-05 -4.36872918e-13
 -8.28455372e-08 -3.35221745e-12 -2.23019871e-06  2.83562866e-14
  5.59614299e-07 -2.02105676e-13  1.46830491e-05 -4.33000580e-13
  1.97108750e-06 -1.40646342e-12  5.19869433e-05 -3.25363177e-12
 -3.23581924e-05  4.53899962e-04  5.08303433e-12 -7.98422248e-11
 -1.23902700e-11  2.23603239e-10  1.13822200e-11 -2.34528482e-10
 -2.32369490e+00]
supnorm grad right now is: 2.32369489687875
Weights right now are: 
[-7.58805738 -0.86706489 -5.72380316 -0.89713203  7.05658712  0.73306946
  5.8885102   0.33107591  1.04089318 -3.90555452  1.41803191 -0.22145862
  2.40629968 13.34079156  3.7749817  13.33885193  0.16221989 -5.39547659
  0.32295829 -3.62808965  2.95940385 14.12891631  3.62960928 13.76889244
 -0.94924416  6.34703364 -1.47515638 -3.65834133  2.62392412 -4.16588965
  2.78381577  5.16299714 -3.9836329   5.84520608 -2.46902063 -2.73968925
  3.70075743  4.78982159  2.79454799  5.22697781 11.6224948   9.82778796
  1.52357955 13.82888107 -7.14697107 -3.05755046  4.41535965 -4.89394307
 11.68984357  8.75874021  1.45976035 12.52090276 10.84283789 11.04215054
  0.2746884  14.90923316 -1.4469478  -0.34647174 -6.90932785 -2.107177
  3.26054167  1.46480967 -4.13883223 -1.67325417 24.77524388]
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.25985547268
gradient value of function right now is: [-2.27442272e-06 -1.09403806e-08 -8.31084560e-06 -7.16323603e-09
  2.27441663e-06  1.09403806e-08  8.31083941e-06  7.16323603e-09
 -2.27025720e-05 -2.46334521e-06 -2.35322211e-05 -1.87787353e-06
 -1.40793273e-08 -1.56579602e-09 -1.45920441e-08 -1.19377677e-09
 -3.27992587e-05 -3.59958424e-06 -3.39960675e-05 -2.74290747e-06
 -7.07778457e-09 -7.87477531e-10 -7.33551435e-09 -6.00389680e-10
  6.87011165e-07  8.76242494e-08  7.24830668e-07  1.87495873e-06
 -3.24716943e-06 -4.13339084e-07 -3.42592201e-06 -8.86012652e-06
  8.93508599e-07  1.13825714e-07  9.42695260e-07  2.43816338e-06
 -1.18176553e-06 -1.50862232e-07 -1.24682138e-06 -3.22549994e-06
  5.78219986e-07 -2.07401651e-13  1.50164635e-05 -4.34879066e-13
 -8.20940432e-08 -3.24503592e-12 -2.18732874e-06  3.18152573e-13
  5.50639723e-07 -2.03335733e-13  1.43029567e-05 -4.31283570e-13
  1.94391144e-06 -1.41673285e-12  5.07511215e-05 -3.24655360e-12
 -3.18731678e-05  4.46983704e-04  5.27456618e-12 -8.19849140e-11
 -1.32112906e-11  2.31550988e-10  1.19580238e-11 -2.39063112e-10
 -9.33108988e-01]
supnorm grad right now is: 0.933108988388894
Weights right now are: 
[-7.58855674 -0.86728349 -5.72429723 -0.89734293  7.05708648  0.73328806
  5.88900427  0.33128681  1.04039688 -3.90585084  1.41753284 -0.22173629
  2.40625219 13.34069373  3.77493369 13.33875985  0.16173967 -5.39577258
  0.32247507 -3.62836494  2.95938968 14.12888204  3.6295949  13.76886096
 -0.94869189  6.34741591 -1.47464827 -3.65779222  2.62346597 -4.1662122
  2.78339719  5.16253825 -3.98311163  5.84556429 -2.46854327 -2.73916994
  3.70030834  4.78950612  2.7941384   5.22652671 11.62297242  9.8279733
  1.52402555 13.82920265 -7.14730794 -3.05769895  4.41498242 -4.89414406
 11.69030748  8.75891013  1.46016985 12.52120542 10.84328986 11.04233132
  0.27510447 14.90955051 -1.44756819 -0.34571914 -6.90947027 -2.10700022
  3.26069133  1.46460542 -4.13911945 -1.67288751 24.91165098]
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.166208580593
gradient value of function right now is: [-2.26629022e-06 -1.08773224e-08 -8.27913536e-06 -7.12145005e-09
  2.26629268e-06  1.08773224e-08  8.27913786e-06  7.12145006e-09
 -2.26164753e-05 -2.45736050e-06 -2.34428804e-05 -1.87344143e-06
 -1.39965172e-08 -1.55852825e-09 -1.45061601e-08 -1.18829747e-09
 -3.26509832e-05 -3.58789046e-06 -3.38422446e-05 -2.73415088e-06
 -7.03572066e-09 -7.83773372e-10 -7.29188946e-09 -5.97596466e-10
  6.84248312e-07  8.73756568e-08  7.21919581e-07  1.86775450e-06
 -3.23506404e-06 -4.12281383e-07 -3.41316852e-06 -8.82869503e-06
  8.90046630e-07  1.13520826e-07  9.39047738e-07  2.42915518e-06
 -1.17720787e-06 -1.50446231e-07 -1.24201947e-06 -3.21363929e-06
  5.76899697e-07 -1.81335066e-13  1.49603368e-05 -3.84915993e-13
 -8.20784451e-08 -3.95737455e-12 -2.18165531e-06 -1.58298826e-12
  5.49392477e-07 -1.77108438e-13  1.42497434e-05 -3.80920055e-13
  1.94017341e-06 -1.24053679e-12  5.05781395e-05 -2.91179027e-12
 -3.18061084e-05  4.46021955e-04  3.80459724e-12 -6.32126383e-11
 -9.83834872e-12  1.84999637e-10  7.78373403e-12 -1.84186127e-10
 -5.67743120e+00]
supnorm grad right now is: 5.677431203344575
Weights right now are: 
[-7.58862459 -0.86731289 -5.72436391 -0.8973713   7.05715433  0.73331746
  5.88907095  0.33131518  1.04032983 -3.90589289  1.41746541 -0.22177566
  2.40624581 13.34068061  3.77492723 13.33874752  0.16167506 -5.39581261
  0.32241006 -3.62840217  2.95938778 14.12887744  3.62959296 13.76885675
 -0.94861751  6.34746788 -1.47457984 -3.65771821  2.62340408 -4.16625687
  2.78334065  5.16247613 -3.98304136  5.84561323 -2.46847893 -2.73909986
  3.70024778  4.78946296  2.79408317  5.2264658  11.62303721  9.82799823
  1.5240857  13.8292459  -7.14735665 -3.05771892  4.41493032 -4.89417109
 11.69037041  8.75893298  1.46022507 12.52124612 10.84335161 11.04235563
  0.27516071 14.90959319 -1.44765508 -0.34561139 -6.90948942 -2.10697644
  3.26071146  1.46457794 -4.13915808 -1.6728382  24.54804353]
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.7585513254135
gradient value of function right now is: [-2.26510236e-06 -1.08681469e-08 -8.27445909e-06 -7.11537018e-09
  2.26510649e-06  1.08681469e-08  8.27446329e-06  7.11537018e-09
 -2.26036343e-05 -2.45624345e-06 -2.34295608e-05 -1.87259575e-06
 -1.39844614e-08 -1.55747133e-09 -1.44936565e-08 -1.18750038e-09
 -3.26294046e-05 -3.58616523e-06 -3.38198587e-05 -2.73285455e-06
 -7.02959645e-09 -7.83234608e-10 -7.28553781e-09 -5.97190109e-10
  6.83843872e-07  8.73298794e-08  7.21493418e-07  1.86669601e-06
 -3.23329109e-06 -4.12089070e-07 -3.41130052e-06 -8.82407486e-06
  8.89539254e-07  1.13463228e-07  9.38513137e-07  2.42782953e-06
 -1.17654420e-06 -1.50380871e-07 -1.24132019e-06 -3.21191031e-06
  5.76696519e-07 -1.92374818e-13  1.49521244e-05 -4.06457198e-13
 -8.20126402e-08 -3.71690595e-12 -2.18056353e-06 -8.66919035e-13
  5.49200109e-07 -1.88179530e-13  1.42419549e-05 -4.02562054e-13
  1.93955917e-06 -1.31481629e-12  5.05526516e-05 -3.05480692e-12
 -3.17954258e-05  4.45874521e-04  4.41791879e-12 -7.09684194e-11
 -1.11341759e-11  2.03310431e-10  9.56882589e-12 -2.07294968e-10
 -3.89377816e+00]
supnorm grad right now is: 3.8937781584232254
Weights right now are: 
[-7.58863398 -0.86731685 -5.72437294 -0.89737511  7.05716372  0.73332141
  5.88907998  0.33131899  1.04032074 -3.90589919  1.41745627 -0.22178151
  2.40624495 13.34067885  3.77492636 13.33874586  0.16166636 -5.39581805
  0.3224013  -3.62840722  2.95938752 14.12887682  3.6295927  13.76885618
 -0.94860748  6.34747503 -1.47457061 -3.65770822  2.6233957  -4.16626325
  2.78333299  5.16246769 -3.98303188  5.84562003 -2.46847024 -2.73909038
  3.7002396   4.78945697  2.79407571  5.22645756 11.62304605  9.82800158
  1.52409382 13.82925172 -7.14736435 -3.0577216   4.41492292 -4.89417472
 11.69037899  8.75893605  1.46023253 12.5212516  10.84336015 11.0423589
  0.27516834 14.90959893 -1.4476676  -0.34559536 -6.909492   -2.10697324
  3.26071417  1.46457425 -4.13916328 -1.67283157 24.67390652]
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.907512576206
gradient value of function right now is: [-2.26519454e-06 -1.08687239e-08 -8.27473559e-06 -7.11575253e-09
  2.26519707e-06  1.08687239e-08  8.27473817e-06  7.11575253e-09
 -2.26043254e-05 -2.45628330e-06 -2.34302793e-05 -1.87261940e-06
 -1.39852361e-08 -1.55753444e-09 -1.44944608e-08 -1.18754771e-09
 -3.26306099e-05 -3.58624443e-06 -3.38211113e-05 -2.73291189e-06
 -7.02999159e-09 -7.83266977e-10 -7.28594805e-09 -5.97214401e-10
  6.83864723e-07  8.73309089e-08  7.21515456e-07  1.86674880e-06
 -3.23338430e-06 -4.12094145e-07 -3.41139905e-06 -8.82430956e-06
  8.89565508e-07  1.13464424e-07  9.38540887e-07  2.42789579e-06
 -1.17657925e-06 -1.50383176e-07 -1.24135724e-06 -3.21199883e-06
  5.76699908e-07 -1.94605717e-13  1.49525412e-05 -4.10368544e-13
 -8.20077215e-08 -3.65468620e-12 -2.18058572e-06 -6.84970208e-13
  5.49203293e-07 -1.90446730e-13  1.42423509e-05 -4.06545603e-13
  1.93956253e-06 -1.33011903e-12  5.05538666e-05 -3.08152454e-12
 -3.17955571e-05  4.45876592e-04  4.52848656e-12 -7.26177333e-11
 -1.15429662e-11  2.08180759e-10  9.97288456e-12 -2.12248843e-10
 -3.20525552e+00]
supnorm grad right now is: 3.2052555192068484
Weights right now are: 
[-7.58863517 -0.86731738 -5.7243741  -0.89737562  7.05716491  0.73332194
  5.88908114  0.3313195   1.04031963 -3.90589937  1.41745515 -0.2217816
  2.40624483 13.34067861  3.77492624 13.33874565  0.1616652  -5.3958187
  0.32240014 -3.62840782  2.95938749 14.12887674  3.62959267 13.76885611
 -0.94860617  6.34747581 -1.47456941 -3.65770694  2.62339468 -4.1662637
  2.78333206  5.16246671 -3.98303066  5.8456207  -2.46846913 -2.73908919
  3.70023856  4.78945643  2.79407476  5.22645654 11.62304703  9.82800203
  1.52409485 13.8292525  -7.14736445 -3.05772196  4.41492237 -4.89417521
 11.69037995  8.75893646  1.46023348 12.52125233 10.84336094 11.04235934
  0.27516926 14.9095997  -1.44766797 -0.34559568 -6.90949235 -2.10697281
  3.26071453  1.46457375 -4.13916398 -1.67283067 24.70963475]
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5143.938737672815
gradient value of function right now is: [-2.26981678e-06 -1.09068547e-08 -8.29301203e-06 -7.14102088e-09
  2.26981085e-06  1.09068547e-08  8.29300600e-06  7.14102087e-09
 -2.26547654e-05 -2.45985200e-06 -2.34826456e-05 -1.87528653e-06
 -1.40356932e-08 -1.56185288e-09 -1.45468100e-08 -1.19079958e-09
 -3.27169784e-05 -3.59270998e-06 -3.39107583e-05 -2.73774584e-06
 -7.05565703e-09 -7.85472789e-10 -7.31257550e-09 -5.98875755e-10
  6.85442150e-07  8.74779082e-08  7.23179031e-07  1.87083273e-06
 -3.24033706e-06 -4.12714272e-07 -3.41873128e-06 -8.84222579e-06
  8.91545618e-07  1.13646628e-07  9.40629101e-07  2.43301081e-06
 -1.17919097e-06 -1.50611818e-07 -1.24411155e-06 -3.21873967e-06
  5.77324848e-07 -1.80209821e-13  1.49844346e-05 -3.83564440e-13
 -8.20963566e-08 -4.05834127e-12 -2.18401902e-06 -1.76490756e-12
  5.49793891e-07 -1.75943999e-13  1.42726112e-05 -3.79507255e-13
  1.94125079e-06 -1.23261754e-12  5.06508885e-05 -2.90207489e-12
 -3.18269580e-05  4.46323337e-04  3.67815542e-12 -6.18089018e-11
 -9.44525380e-12  1.81329170e-10  7.42120124e-12 -1.80820925e-10
 -5.89813337e+00]
supnorm grad right now is: 5.898133373252303
Weights right now are: 
[-7.5886303  -0.86731744 -5.72437204 -0.89737568  7.05716004  0.733322
  5.88907908  0.33131956  1.0403228  -3.90588342  1.41745838 -0.22176652
  2.40624482 13.34067878  3.77492623 13.33874581  0.16166557 -5.3958161
  0.32240052 -3.6284054   2.95938749 14.12887683  3.62959267 13.7688562
 -0.94860739  6.34747158 -1.47457053 -3.65770874  2.62339829 -4.1662533
  2.78333531  5.16247181 -3.98303255  5.84561492 -2.46847084 -2.73909192
  3.70024113  4.7894638   2.79407707  5.22646019 11.62304137  9.82800209
  1.5240928  13.8292526  -7.14734547 -3.05772201  4.41493275 -4.89417527
 11.69037453  8.75893652  1.46023159 12.52125243 10.84335189 11.0423594
  0.2751658  14.9095998  -1.4476338  -0.34566604 -6.90949239 -2.10697275
  3.26071458  1.46457368 -4.13916407 -1.67283055 24.5049136 ]
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.087702610727
gradient value of function right now is: [-2.27228829e-06 -1.09273673e-08 -8.30252728e-06 -7.15461492e-09
  2.27228710e-06  1.09273673e-08  8.30252607e-06  7.15461492e-09
 -2.26804094e-05 -2.46082123e-06 -2.35092945e-05 -1.87593337e-06
 -1.40628393e-08 -1.56417352e-09 -1.45749741e-08 -1.19254591e-09
 -3.27631428e-05 -3.59602418e-06 -3.39586801e-05 -2.74020324e-06
 -7.06946591e-09 -7.86658229e-10 -7.32690203e-09 -5.99768171e-10
  6.86270326e-07  8.75208235e-08  7.24052472e-07  1.87295576e-06
 -3.24398799e-06 -4.12901327e-07 -3.42258161e-06 -8.85154118e-06
  8.92582724e-07  1.13694931e-07  9.41722875e-07  2.43566071e-06
 -1.18058056e-06 -1.50713101e-07 -1.24557705e-06 -3.22231095e-06
  5.77594338e-07 -2.21482085e-13  1.50011837e-05 -4.63333897e-13
 -8.19242769e-08 -2.34162704e-12 -2.18487010e-06  1.95096763e-12
  5.50046676e-07 -2.17689769e-13  1.42884929e-05 -4.60180766e-13
  1.94179341e-06 -1.50954766e-12  5.07008737e-05 -3.43017430e-12
 -3.18388355e-05  4.46517320e-04  6.70195361e-12 -9.62672951e-11
 -1.56724860e-11  2.63052655e-10  1.50599851e-11 -2.76620783e-10
  2.35488835e+00]
supnorm grad right now is: 2.3548883483378718
Weights right now are: 
[-7.58862731 -0.86731744 -5.72437071 -0.89737568  7.05715705  0.733322
  5.88907775  0.33131957  1.04032484 -3.90587364  1.41746046 -0.22175724
  2.40624482 13.34067889  3.77492623 13.33874592  0.16166588 -5.39581443
  0.32240083 -3.62840385  2.95938749 14.12887689  3.62959267 13.76885626
 -0.94860824  6.34746893 -1.4745713  -3.65770995  2.6234006  -4.16624688
  2.7833374   5.16247504 -3.98303381  5.84561132 -2.46847197 -2.73909369
  3.7002428   4.78946841  2.79407857  5.22646253 11.62303779  9.8280021
  1.52409145 13.82925262 -7.14733401 -3.05772202  4.41493916 -4.89417528
 11.6903711   8.75893653  1.46023036 12.52125245 10.84334622 11.04235941
  0.27516359 14.90959982 -1.44761257 -0.34570974 -6.9094924  -2.10697275
  3.26071459  1.46457367 -4.13916408 -1.67283054 25.1064215 ]
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.134255431585
gradient value of function right now is: [-2.26649617e-06 -1.08778002e-08 -8.27928179e-06 -7.12176679e-09
  2.26649999e-06  1.08778002e-08  8.27928567e-06  7.12176679e-09
 -2.26163020e-05 -2.45710060e-06 -2.34427112e-05 -1.87319790e-06
 -1.39972535e-08 -1.55856040e-09 -1.45069286e-08 -1.18832052e-09
 -3.26511430e-05 -3.58778729e-06 -3.38424229e-05 -2.73405792e-06
 -7.03610415e-09 -7.83790897e-10 -7.29228962e-09 -5.97609212e-10
  6.84239165e-07  8.73588006e-08  7.21910449e-07  1.86771790e-06
 -3.23503787e-06 -4.12216962e-07 -3.41314335e-06 -8.82856927e-06
  8.90035886e-07  1.13498125e-07  9.39037077e-07  2.42911055e-06
 -1.17719964e-06 -1.50434385e-07 -1.24201168e-06 -3.21359906e-06
  5.76844693e-07 -2.16869483e-13  1.49601069e-05 -4.51647689e-13
 -8.19671080e-08 -2.50982621e-12 -2.18121460e-06  1.63394063e-12
  5.49340179e-07 -2.13089662e-13  1.42495314e-05 -4.48587765e-13
  1.93995136e-06 -1.48123830e-12  5.05768814e-05 -3.36099162e-12
 -3.18027861e-05  4.45979005e-04  6.31756818e-12 -9.24358075e-11
 -1.52824867e-11  2.57261319e-10  1.39492841e-11 -2.64069622e-10
  1.99991975e+00]
supnorm grad right now is: 1.9999197463478304
Weights right now are: 
[-7.58863366 -0.86731751 -5.72437399 -0.89737575  7.0571634   0.73332208
  5.88908103  0.33131963  1.04031989 -3.90588651  1.41745539 -0.22176905
  2.40624479 13.34067817  3.7749262  13.33874523  0.16166455 -5.39581833
  0.32239947 -3.62840741  2.95938747 14.1288765   3.62959265 13.76885588
 -0.94860578  6.34747295 -1.47456905 -3.65770676  2.6233939  -4.1662568
  2.78333136  5.16246648 -3.98303027  5.84561662 -2.46846877 -2.73908914
  3.70023708  4.7894599   2.79407343  5.2264552  11.62304415  9.8280021
  1.52409536 13.82925262 -7.14734332 -3.05772202  4.41492995 -4.89417529
 11.69037715  8.75893653  1.46023393 12.52125245 10.84335473 11.04235941
  0.27516908 14.90959982 -1.44764699 -0.34562825 -6.9094924  -2.10697275
  3.26071459  1.46457366 -4.13916408 -1.67283054 25.07702628]
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.25352505678
gradient value of function right now is: [-2.22373553e-06 -1.05303107e-08 -8.11176256e-06 -6.89153988e-09
  2.22374208e-06  1.05303107e-08  8.11176921e-06  6.89153988e-09
 -2.21560389e-05 -2.42601206e-06 -2.29648323e-05 -1.85015863e-06
 -1.35377181e-08 -1.51900554e-09 -1.40301675e-08 -1.15853421e-09
 -3.18607416e-05 -3.52840481e-06 -3.30220214e-05 -2.68970693e-06
 -6.80240505e-09 -7.63591718e-10 -7.04983386e-09 -5.82394922e-10
  6.69789803e-07  8.60828211e-08  7.06672186e-07  1.83030625e-06
 -3.17126212e-06 -4.06797126e-07 -3.34588791e-06 -8.66420329e-06
  8.71889446e-07  1.11926140e-07  9.19900220e-07  2.38223435e-06
 -1.15322127e-06 -1.48358970e-07 -1.21672456e-06 -3.15166763e-06
  5.71097009e-07 -2.01537728e-13  1.46674254e-05 -4.21740185e-13
 -8.16972221e-08 -3.32887026e-12 -2.15171185e-06  1.65907053e-13
  5.43910864e-07 -1.97551530e-13  1.39718420e-05 -4.18314609e-13
  1.92465275e-06 -1.37839063e-12  4.96863723e-05 -3.16149817e-12
 -3.15162339e-05  4.41861886e-04  4.89751134e-12 -7.83807006e-11
 -1.26824700e-11  2.26270323e-10  1.10461967e-11 -2.29038489e-10
 -1.12722705e+00]
supnorm grad right now is: 1.1272270539718556
Weights right now are: 
[-7.58868656 -0.86731791 -5.72439989 -0.89737606  7.0572163   0.73332248
  5.88910693  0.33131994  1.04028088 -3.90601725  1.41741552 -0.22189089
  2.40624459 13.34067379  3.77492599 13.33874102  0.16165566 -5.39584981
  0.32239037 -3.62843645  2.95938736 14.12887416  3.62959254 13.76885364
 -0.94858738  6.34751121 -1.47455224 -3.65768219  2.62334369 -4.16635084
  2.78328606  5.16240032 -3.98300359  5.8456678  -2.46844462 -2.73905377
  3.70019619  4.78938472  2.79403666  5.22640125 11.62310171  9.8280021
  1.52412466 13.82925262 -7.14746614 -3.05772202  4.41483998 -4.8941753
 11.69043206  8.75893653  1.46026076 12.52125245 10.84343722 11.04235941
  0.27521251 14.90959982 -1.4479696  -0.34490778 -6.9094924  -2.10697275
  3.26071459  1.46457366 -4.13916409 -1.67283053 24.88558673]
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.249046322454
gradient value of function right now is: [-2.23620012e-06 -1.06306772e-08 -8.16043497e-06 -6.95802775e-09
  2.23619896e-06  1.06306772e-08  8.16043380e-06  6.95802775e-09
 -2.22895922e-05 -2.43508541e-06 -2.31034967e-05 -1.85687405e-06
 -1.36703887e-08 -1.53047172e-09 -1.41678101e-08 -1.16716956e-09
 -3.20898055e-05 -3.54570114e-06 -3.32597778e-05 -2.70262500e-06
 -6.86986596e-09 -7.69446099e-10 -7.11982190e-09 -5.86804988e-10
  6.73984312e-07  8.64548304e-08  7.11095596e-07  1.84117393e-06
 -3.18979045e-06 -4.08380296e-07 -3.36542665e-06 -8.71199053e-06
  8.77159362e-07  1.12384700e-07  9.25457621e-07  2.39585729e-06
 -1.16018701e-06 -1.48965296e-07 -1.22407032e-06 -3.16967238e-06
  5.72790716e-07 -2.07925223e-13  1.47524934e-05 -4.33699714e-13
 -8.17761382e-08 -2.97242335e-12 -2.16031895e-06  7.51671554e-13
  5.45510852e-07 -2.04044276e-13  1.40525531e-05 -4.30451726e-13
  1.92918111e-06 -1.42175848e-12  4.99455361e-05 -3.24182807e-12
 -3.16008258e-05  4.43076856e-04  5.68060625e-12 -8.51204991e-11
 -1.40731976e-11  2.40605234e-10  1.24904674e-11 -2.44437230e-10
  8.99152238e-01]
supnorm grad right now is: 0.8991522376213564
Weights right now are: 
[-7.58866919 -0.86731765 -5.72439051 -0.89737585  7.05719893  0.73332222
  5.88909755  0.33131973  1.04029524 -3.90598265  1.41743023 -0.22185885
  2.40624472 13.34067607  3.77492613 13.33874319  0.16165972 -5.39583899
  0.32239453 -3.62842656  2.95938743 14.1288754   3.62959262 13.76885481
 -0.9485946   6.34750018 -1.47455884 -3.65769142  2.62336313 -4.16632412
  2.7833036   5.16242485 -3.98301389  5.84565333 -2.46845394 -2.73906685
  3.70021296  4.78940774  2.79405173  5.22642245 11.62308488  9.8280021
  1.52411336 13.82925262 -7.1474426  -3.05772202  4.41486512 -4.8941753
 11.69041606  8.75893653  1.46025044 12.52125245 10.84341513 11.04235941
  0.27519698 14.90959982 -1.44787927 -0.34512708 -6.9094924  -2.10697275
  3.26071459  1.46457366 -4.13916409 -1.67283053 24.99394641]
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.507284611489
gradient value of function right now is: [-2.23720248e-06 -1.06405194e-08 -8.16498883e-06 -6.96454830e-09
  2.23720342e-06  1.06405194e-08  8.16498979e-06  6.96454830e-09
 -2.23027169e-05 -2.43598320e-06 -2.31171244e-05 -1.85757919e-06
 -1.36834002e-08 -1.53159430e-09 -1.41813093e-08 -1.16801530e-09
 -3.21122466e-05 -3.54738559e-06 -3.32830708e-05 -2.70389246e-06
 -6.87648257e-09 -7.70019431e-10 -7.12668645e-09 -5.87236936e-10
  6.74395422e-07  8.64962400e-08  7.11529123e-07  1.84223926e-06
 -3.19160500e-06 -4.08550993e-07 -3.36734006e-06 -8.71667120e-06
  8.77675746e-07  1.12436398e-07  9.26002151e-07  2.39719241e-06
 -1.16086895e-06 -1.49026486e-07 -1.22478942e-06 -3.17143498e-06
  5.72957768e-07 -1.87859312e-13  1.47608273e-05 -3.96628388e-13
 -8.18290010e-08 -3.78436468e-12 -2.16129578e-06 -9.89183999e-13
  5.45668699e-07 -1.83685353e-13  1.40604600e-05 -3.92775520e-13
  1.92963162e-06 -1.28470284e-12  4.99709313e-05 -2.99015911e-12
 -3.16092130e-05  4.43197011e-04  4.02361657e-12 -6.77012525e-11
 -1.05590009e-11  1.97496851e-10  8.82053347e-12 -1.99032507e-10
 -4.69244761e+00]
supnorm grad right now is: 4.692447607561644
Weights right now are: 
[-7.58866779 -0.86731762 -5.72438952 -0.89737582  7.05719753  0.73332218
  5.88909656  0.3313197   1.04029683 -3.90598115  1.41743187 -0.22185744
  2.40624474 13.3406764   3.77492615 13.33874349  0.16166027 -5.3958378
  0.3223951  -3.62842547  2.95938745 14.12887558  3.62959263 13.76885498
 -0.94859548  6.34749947 -1.47455964 -3.6576925   2.62336551 -4.16632234
  2.78330575  5.16242774 -3.98301513  5.84565245 -2.46845506 -2.73906835
  3.70021515  4.78940983  2.7940537   5.22642515 11.6230834   9.8280021
  1.52411197 13.82925262 -7.14744433 -3.05772202  4.41486659 -4.8941753
 11.69041466  8.75893653  1.46024918 12.52125245 10.8434136  11.04235941
  0.27519521 14.90959982 -1.44787209 -0.34514762 -6.9094924  -2.10697275
  3.26071459  1.46457365 -4.13916409 -1.67283053 24.65485953]
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.578331569541
gradient value of function right now is: [-2.23821106e-06 -1.06486889e-08 -8.16893512e-06 -6.96996065e-09
  2.23821338e-06  1.06486889e-08  8.16893747e-06  6.96996065e-09
 -2.23135591e-05 -2.43671751e-06 -2.31283817e-05 -1.85812255e-06
 -1.36942016e-08 -1.53252578e-09 -1.41925156e-08 -1.16871664e-09
 -3.21308526e-05 -3.54878632e-06 -3.33023830e-05 -2.70493830e-06
 -6.88197535e-09 -7.70495075e-10 -7.13238504e-09 -5.87595149e-10
  6.74735911e-07  8.65264661e-08  7.11888194e-07  1.84312109e-06
 -3.19310823e-06 -4.08679531e-07 -3.36892528e-06 -8.72054655e-06
  8.78103416e-07  1.12473657e-07  9.26453152e-07  2.39829748e-06
 -1.16143404e-06 -1.49075630e-07 -1.22538534e-06 -3.17289496e-06
  5.73094117e-07 -1.88050475e-13  1.47677276e-05 -3.97116074e-13
 -8.18360236e-08 -3.79715561e-12 -2.16199525e-06 -1.00007503e-12
  5.45797496e-07 -1.83872585e-13  1.40670068e-05 -3.93258941e-13
  1.92999506e-06 -1.28598163e-12  4.99919366e-05 -2.99339378e-12
 -3.16160156e-05  4.43294760e-04  4.02146711e-12 -6.77198005e-11
 -1.05506211e-11  1.97569287e-10  8.81885237e-12 -1.99214391e-10
 -4.61839386e+00]
supnorm grad right now is: 4.618393863388291
Weights right now are: 
[-7.58866696 -0.8673176  -5.72438891 -0.89737581  7.05719669  0.73332216
  5.88909595  0.33131969  1.04029785 -3.90597943  1.41743291 -0.22185573
  2.40624475 13.34067655  3.77492616 13.33874363  0.16166056 -5.39583696
  0.32239539 -3.6284247   2.95938745 14.12887566  3.62959263 13.76885505
 -0.948596    6.34749884 -1.47456012 -3.65769317  2.62336697 -4.1663207
  2.78330706  5.16242959 -3.98301589  5.84565162 -2.46845575 -2.73906932
  3.70021642  4.78941154  2.79405484  5.22642678 11.62308208  9.8280021
  1.52411112 13.82925262 -7.1474444  -3.05772202  4.41486801 -4.89417529
 11.69041341  8.75893653  1.4602484  12.52125245 10.84341194 11.04235941
  0.27519403 14.90959982 -1.4478652  -0.3451647  -6.9094924  -2.10697275
  3.26071459  1.46457365 -4.13916409 -1.67283053 24.63352548]
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.568285816757
gradient value of function right now is: [-2.21190666e-06 -1.04356395e-08 -8.06572891e-06 -6.82883178e-09
  2.21190949e-06  1.04356395e-08  8.06573179e-06  6.82883178e-09
 -2.20298149e-05 -2.41747203e-06 -2.28337787e-05 -1.84384452e-06
 -1.34125858e-08 -1.50817437e-09 -1.39003472e-08 -1.15037434e-09
 -3.16443810e-05 -3.51205775e-06 -3.27974526e-05 -2.67749507e-06
 -6.73878432e-09 -7.58062144e-10 -6.98383061e-09 -5.78228114e-10
  6.65829360e-07  8.57359768e-08  7.02495246e-07  1.82004605e-06
 -3.15374322e-06 -4.05317230e-07 -3.32741189e-06 -8.61902259e-06
  8.66911248e-07  1.11498870e-07  9.14649969e-07  2.36936683e-06
 -1.14663492e-06 -1.47789169e-07 -1.20977827e-06 -3.13464391e-06
  5.69507483e-07 -1.85912022e-13  1.45870564e-05 -3.91577665e-13
 -8.16500091e-08 -3.80274163e-12 -2.14369680e-06 -1.03192105e-12
  5.42409160e-07 -1.81767702e-13  1.38955813e-05 -3.87793232e-13
  1.92041949e-06 -1.27223697e-12  4.94415226e-05 -2.95779490e-12
 -3.14369869e-05  4.40722766e-04  3.96008297e-12 -6.68326550e-11
 -1.04708717e-11  1.96215897e-10  8.61085641e-12 -1.95957402e-10
 -4.61798601e+00]
supnorm grad right now is: 4.617986010608772
Weights right now are: 
[-7.5886959  -0.86731779 -5.72440271 -0.89737595  7.05722564  0.73332235
  5.88910975  0.33131983  1.04027716 -3.90605385  1.41741178 -0.22192512
  2.40624466 13.34067456  3.77492606 13.33874178  0.16165609 -5.39585375
  0.32239083 -3.62844021  2.9593874  14.12887461  3.62959258 13.76885409
 -0.94858642  6.34752024 -1.47455136 -3.65768025  2.62334084 -4.16637326
  2.7832835   5.16239484 -3.98300195  5.84568033 -2.46844314 -2.73905067
  3.7001955   4.7893704   2.79403604  5.22639894 11.6231137   9.8280021
  1.52412637 13.82925262 -7.14751818 -3.05772202  4.4148174  -4.89417529
 11.6904436   8.75893653  1.46026237 12.52125245 10.84345812 11.04235941
  0.27521701 14.90959982 -1.44804415 -0.34477057 -6.9094924  -2.10697275
  3.26071459  1.46457365 -4.13916409 -1.67283053 24.5957073 ]
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.015817817414
gradient value of function right now is: [-2.22261310e-06 -1.03360247e-08 -8.07990026e-06 -6.76222734e-09
  2.22260584e-06  1.03360247e-08  8.07989288e-06  6.76222734e-09
 -2.20604910e-05 -2.45611379e-06 -2.28638697e-05 -1.87450558e-06
 -1.32786179e-08 -1.49896414e-09 -1.37612436e-08 -1.14354246e-09
 -3.15151770e-05 -3.52689015e-06 -3.26621624e-05 -2.68980608e-06
 -6.67013804e-09 -7.53206925e-10 -6.91255919e-09 -5.74619287e-10
  6.65567462e-07  8.67069953e-08  7.02220906e-07  1.82174561e-06
 -3.15461453e-06 -4.10336289e-07 -3.32834061e-06 -8.63313571e-06
  8.67068807e-07  1.12882251e-07  9.14818786e-07  2.37306237e-06
 -1.14523753e-06 -1.49128910e-07 -1.20830738e-06 -3.13450107e-06
  5.76675211e-07 -2.30173752e-13  1.45927852e-05 -4.10455587e-13
 -8.48504275e-08 -7.26305181e-13 -2.17172055e-06  5.00237865e-12
  5.49336319e-07 -2.27349056e-13  1.39023281e-05 -4.11745814e-13
  1.95422713e-06 -1.60238668e-12  4.95836194e-05 -3.30076054e-12
 -3.19068867e-05  4.45722956e-04  7.77314938e-12 -1.08795139e-10
 -2.05621884e-11  3.29421216e-10  1.09607901e-11 -2.42709026e-10
  6.38165519e+00]
supnorm grad right now is: 6.381655187308862
Weights right now are: 
[-7.5887142  -0.8673179  -5.72441137 -0.89737604  7.05724394  0.73332246
  5.88911841  0.33131992  1.04026422 -3.90610059  1.41739856 -0.22196862
  2.4062446  13.34067334  3.77492601 13.33874065  0.16165331 -5.39586426
  0.32238798 -3.62844991  2.95938737 14.12887398  3.62959255 13.7688535
 -0.94858043  6.34753366 -1.47454589 -3.65767217  2.62332451 -4.16640624
  2.78326877  5.16237311 -3.98299324  5.84569833 -2.46843525 -2.739039
  3.70018244  4.78934463  2.79402429  5.22638153 11.62313351  9.8280021
  1.5241359  13.82925262 -7.14756463 -3.05772202  4.41478567 -4.89417529
 11.69046252  8.75893653  1.46027111 12.52125245 10.84348707 11.04235941
  0.27523138 14.90959982 -1.4481563  -0.3445237  -6.9094924  -2.10697275
  3.26071459  1.46457365 -4.13916409 -1.67283053 25.35592944]
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.270842690797
gradient value of function right now is: [-2.20867370e-06 -1.04080721e-08 -8.05249597e-06 -6.81057244e-09
  2.20867077e-06  1.04080721e-08  8.05249300e-06  6.81057244e-09
 -2.19928667e-05 -2.41493888e-06 -2.27954157e-05 -1.84193093e-06
 -1.33761656e-08 -1.50501155e-09 -1.38625621e-08 -1.14799254e-09
 -3.15812370e-05 -3.50726984e-06 -3.27319120e-05 -2.67391130e-06
 -6.72026807e-09 -7.56447436e-10 -6.96462077e-09 -5.77011936e-10
  6.64669813e-07  8.56273328e-08  7.01272473e-07  1.81703891e-06
 -3.14861634e-06 -4.04859271e-07 -3.32200567e-06 -8.60578643e-06
  8.65453647e-07  1.11364180e-07  9.13112921e-07  2.36559520e-06
 -1.14470860e-06 -1.47617942e-07 -1.20774696e-06 -3.12966023e-06
  5.69029422e-07 -2.04091059e-13  1.45635055e-05 -4.24939688e-13
 -8.15827497e-08 -3.09614569e-12 -2.14115968e-06  4.66604720e-13
  5.41957457e-07 -2.00188703e-13  1.38732361e-05 -4.21659933e-13
  1.91913018e-06 -1.39672003e-12  4.93696470e-05 -3.18484743e-12
 -3.14130085e-05  4.40378971e-04  5.51375040e-12 -8.26830869e-11
 -1.37562934e-11  2.35047679e-10  1.20413992e-11 -2.36930258e-10
  5.07623687e-01]
supnorm grad right now is: 0.5076236873100263
Weights right now are: 
[-7.58870273 -0.86731789 -5.72440664 -0.89737603  7.05723247  0.73332245
  5.88911368  0.33131991  1.04027124 -3.90606105  1.4174057  -0.22193122
  2.40624461 13.34067358  3.77492601 13.33874088  0.16165422 -5.39585871
  0.32238891 -3.62844473  2.95938738 14.12887409  3.62959255 13.76885361
 -0.94858321  6.34752316 -1.47454843 -3.6576762   2.62333185 -4.16638136
  2.78327538  5.16238362 -3.98299734  5.84568407 -2.46843896 -2.7390449
  3.70018737  4.78936108  2.79402873  5.22638862 11.62312077  9.8280021
  1.52413162 13.82925262 -7.14751208 -3.05772202  4.4148112  -4.89417529
 11.69045029  8.75893653  1.46026716 12.52125245 10.84346611 11.04235941
  0.27522407 14.90959982 -1.44807916 -0.34467744 -6.9094924  -2.10697275
  3.2607146   1.46457364 -4.13916409 -1.67283053 25.00209195]
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.010365705379
gradient value of function right now is: [-2.21497976e-06 -1.04492043e-08 -8.07522993e-06 -6.83780613e-09
  2.21498588e-06  1.04492043e-08  8.07523614e-06  6.83780614e-09
 -2.20546779e-05 -2.42144494e-06 -2.28594842e-05 -1.84688339e-06
 -1.34304870e-08 -1.50975952e-09 -1.39189168e-08 -1.15157107e-09
 -3.16779144e-05 -3.51553347e-06 -3.28322149e-05 -2.68012825e-06
 -6.74787734e-09 -7.58867923e-10 -6.99326314e-09 -5.78836598e-10
  6.66532459e-07  8.58516295e-08  7.03236837e-07  1.82197518e-06
 -3.15687948e-06 -4.05842049e-07 -3.33071980e-06 -8.62761227e-06
  8.67811853e-07  1.11650773e-07  9.15599865e-07  2.37184378e-06
 -1.14772033e-06 -1.47948321e-07 -1.21092314e-06 -3.13756440e-06
  5.70093998e-07 -2.25446060e-13  1.46014554e-05 -4.59822160e-13
 -8.18177776e-08 -1.74688887e-12 -2.14667640e-06  3.08512040e-12
  5.42971564e-07 -2.21883756e-13  1.39093021e-05 -4.57470451e-13
  1.92275815e-06 -1.54594544e-12  4.94906821e-05 -3.44486547e-12
 -3.14723311e-05  4.41130089e-04  7.05134931e-12 -1.01124649e-10
 -1.71085594e-11  2.84081782e-10  1.46727440e-11 -2.79399091e-10
  2.75080359e+00]
supnorm grad right now is: 2.7508035876662844
Weights right now are: 
[-7.58869633 -0.86731787 -5.7244038  -0.89737602  7.05722607  0.73332243
  5.88911084  0.3313199   1.04027556 -3.90603957  1.41741009 -0.2219108
  2.40624461 13.34067378  3.77492602 13.33874106  0.16165487 -5.39585524
  0.32238958 -3.6284415   2.95938738 14.12887419  3.62959256 13.7688537
 -0.94858499  6.34751734 -1.47455006 -3.65767875  2.62333663 -4.16636741
  2.7832797   5.16239033 -3.98299997  5.84567619 -2.46844134 -2.73904862
  3.70019077  4.78937085  2.79403178  5.2263934  11.62311317  9.8280021
  1.52412883 13.82925262 -7.14748566 -3.05772202  4.41482528 -4.8941753
 11.690443    8.75893653  1.46026459 12.52125245 10.84345398 11.04235941
  0.27521946 14.90959982 -1.44803361 -0.34477008 -6.9094924  -2.10697275
  3.2607146   1.46457363 -4.13916409 -1.67283053 25.14358566]
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.190551025196
gradient value of function right now is: [-2.13040919e-06 -9.78691382e-09 -7.74636683e-06 -6.39931437e-09
  2.13040599e-06  9.78691381e-09  7.74636357e-06  6.39931437e-09
 -2.11517235e-05 -2.35685788e-06 -2.19221081e-05 -1.79881561e-06
 -1.25562145e-08 -1.43316490e-09 -1.30119168e-08 -1.09384245e-09
 -3.01468662e-05 -3.39727352e-06 -3.12431628e-05 -2.59166183e-06
 -6.30356920e-09 -7.19786169e-10 -6.53232962e-09 -5.49374495e-10
  6.38277102e-07  8.32539630e-08  6.73438104e-07  1.74851777e-06
 -3.03153955e-06 -3.94709727e-07 -3.19853776e-06 -8.30315563e-06
  8.32231463e-07  1.08430635e-07  8.78076615e-07  2.27953305e-06
 -1.10071905e-06 -1.43737030e-07 -1.16135523e-06 -3.01570382e-06
  5.57962452e-07 -1.89602484e-13  1.40259049e-05 -3.94241816e-13
 -8.09779806e-08 -3.55484962e-12 -2.08604536e-06 -4.68365071e-13
  5.31498430e-07 -1.85661975e-13  1.33630772e-05 -3.90894240e-13
  1.88920462e-06 -1.29952177e-12  4.77250225e-05 -2.98003691e-12
 -3.08577730e-05  4.32418182e-04  4.62473342e-12 -7.25963535e-11
 -1.18412533e-11  2.11861645e-10  9.83722201e-12 -2.08463829e-10
 -1.62955755e+00]
supnorm grad right now is: 1.6295575466945698
Weights right now are: 
[-7.58879164 -0.86731871 -5.72445114 -0.89737669  7.05732138  0.73332328
  5.88915817  0.33132057  1.04020414 -3.90626641  1.41733707 -0.22212179
  2.40624419 13.34066646  3.77492559 13.33873454  0.16163782 -5.39591205
  0.32237212 -3.62849379  2.95938716 14.12887037  3.62959233 13.76885033
 -0.9485509   6.3475845  -1.47451891 -3.65763361  2.62324381 -4.16653209
  2.78319598  5.16226914 -3.98295067  5.84576574 -2.46839672 -2.73898386
  3.70011427  4.78923806  2.79396299  5.2262934  11.62321409  9.8280021
  1.52418293 13.82925262 -7.14769101 -3.05772201  4.41466799 -4.8941753
 11.69053923  8.75893653  1.46031411 12.52125245 10.84359664 11.04235941
  0.27529844 14.90959982 -1.44859529 -0.34349567 -6.9094924  -2.10697275
  3.2607146   1.46457363 -4.13916409 -1.67283053 24.81053122]
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.849300346415
gradient value of function right now is: [-2.09726993e-06 -9.52888551e-09 -7.61703468e-06 -6.22856874e-09
  2.09726857e-06  9.52888551e-09  7.61703331e-06  6.22856874e-09
 -2.07964911e-05 -2.33199295e-06 -2.15533019e-05 -1.78034358e-06
 -1.22160688e-08 -1.40296443e-09 -1.26590518e-08 -1.07106541e-09
 -2.95442226e-05 -3.35035661e-06 -3.06176944e-05 -2.55655036e-06
 -6.13079916e-09 -7.04384698e-10 -6.35310179e-09 -5.37756212e-10
  6.27135430e-07  8.22432320e-08  6.61687520e-07  1.71953404e-06
 -2.98192959e-06 -3.90363577e-07 -3.14621812e-06 -8.17464187e-06
  8.18182616e-07  1.07178911e-07  8.63260206e-07  2.24306463e-06
 -1.08208856e-06 -1.42072627e-07 -1.14170680e-06 -2.96733522e-06
  5.53116275e-07 -1.79780506e-13  1.37980261e-05 -3.73633409e-13
 -8.07213303e-08 -3.70836208e-12 -2.06251786e-06 -9.70670531e-13
  5.26916897e-07 -1.75819938e-13  1.31467980e-05 -3.70218083e-13
  1.87596531e-06 -1.23406527e-12  4.70251151e-05 -2.84290784e-12
 -3.06137019e-05  4.28922679e-04  4.10198917e-12 -6.62214165e-11
 -1.08105691e-11  1.96959800e-10  8.45892282e-12 -1.89401951e-10
 -3.47313151e+00]
supnorm grad right now is: 3.4731315064139094
Weights right now are: 
[-7.58883717 -0.86731911 -5.72447354 -0.897377    7.0573669   0.73332367
  5.88918058  0.33132088  1.04017042 -3.90637516  1.4173026  -0.22222284
  2.40624399 13.34066309  3.77492538 13.33873156  0.1616298  -5.39593877
  0.32236391 -3.62851837  2.95938705 14.12886862  3.62959222 13.7688488
 -0.94853484  6.34761657 -1.47450424 -3.65761235  2.62320016 -4.16661052
  2.78315661  5.16221214 -3.98292747  5.84580851 -2.46837572 -2.73895336
  3.70007836  4.78917539  2.79393069  5.22624646 11.62326165  9.8280021
  1.52420837 13.82925262 -7.14779177 -3.05772201  4.41459283 -4.89417529
 11.69058458  8.75893653  1.4603374  12.52125245 10.84366408 11.04235941
  0.2753356  14.90959982 -1.44886051 -0.34289545 -6.9094924  -2.10697275
  3.2607146   1.46457363 -4.13916409 -1.67283052 24.73043986]
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.263353505653
gradient value of function right now is: [-2.09249243e-06 -9.49069750e-09 -7.59793072e-06 -6.20330318e-09
  2.09249713e-06  9.49069751e-09  7.59793550e-06  6.20330318e-09
 -2.07435625e-05 -2.32825118e-06 -2.14983514e-05 -1.77753125e-06
 -1.21657501e-08 -1.39847718e-09 -1.26068523e-08 -1.06767970e-09
 -2.94546694e-05 -3.34335401e-06 -3.05247507e-05 -2.55129997e-06
 -6.10524562e-09 -7.02096692e-10 -6.32659351e-09 -5.36029568e-10
  6.25477125e-07  8.20881134e-08  6.59938559e-07  1.71521693e-06
 -2.97453587e-06 -3.89699818e-07 -3.13842043e-06 -8.15547310e-06
  8.16090319e-07  1.06986111e-07  8.61053560e-07  2.23762915e-06
 -1.07931254e-06 -1.41822164e-07 -1.13877907e-06 -2.96012258e-06
  5.52385242e-07 -1.94651890e-13  1.37640612e-05 -4.01128040e-13
 -8.06442510e-08 -3.08857136e-12 -2.05888826e-06  3.93821974e-13
  5.26225650e-07 -1.90908517e-13  1.31145601e-05 -3.98163018e-13
  1.87395709e-06 -1.33588510e-12  4.69206387e-05 -3.03003135e-12
 -3.05767898e-05  4.28394709e-04  5.25284543e-12 -7.90246632e-11
 -1.34300911e-11  2.29849291e-10  1.11704051e-11 -2.23718919e-10
  6.64138973e-01]
supnorm grad right now is: 0.6641389733529779
Weights right now are: 
[-7.58884439 -0.86731916 -5.72447681 -0.89737704  7.05737412  0.73332373
  5.88918385  0.33132092  1.04016566 -3.9063926   1.41729773 -0.22223885
  2.40624397 13.34066267  3.77492535 13.33873119  0.16162873 -5.39594252
  0.32236281 -3.62852182  2.95938704 14.1288684   3.62959221 13.76884861
 -0.94853263  6.34762155 -1.47450222 -3.65760939  2.62319418 -4.16662262
  2.7831512   5.16220424 -3.98292426  5.84581519 -2.46837282 -2.7389491
  3.70007352  4.78916626  2.79392634  5.2262401  11.62326854  9.8280021
  1.52421186 13.82925262 -7.14781044 -3.05772201  4.41458107 -4.8941753
 11.69059116  8.75893653  1.46034059 12.52125245 10.84367413 11.04235941
  0.27534077 14.90959982 -1.44889939 -0.34280911 -6.9094924  -2.10697275
  3.2607146   1.46457362 -4.13916409 -1.67283052 24.99224391]
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.285972017993
gradient value of function right now is: [-2.09241851e-06 -9.49031846e-09 -7.59770986e-06 -6.20305258e-09
  2.09242188e-06  9.49031847e-09  7.59771328e-06  6.20305259e-09
 -2.07430342e-05 -2.32820954e-06 -2.14978031e-05 -1.77750430e-06
 -1.21652509e-08 -1.39843224e-09 -1.26063345e-08 -1.06764559e-09
 -2.94537646e-05 -3.34328066e-06 -3.05238120e-05 -2.55124544e-06
 -6.10499228e-09 -7.02073814e-10 -6.32633076e-09 -5.36012190e-10
  6.25460530e-07  8.20870135e-08  6.59921055e-07  1.71517360e-06
 -2.97446166e-06 -3.89694697e-07 -3.13834216e-06 -8.15528010e-06
  8.16069341e-07  1.06984800e-07  8.61031434e-07  2.23757449e-06
 -1.07928447e-06 -1.41819911e-07 -1.13874946e-06 -2.96004939e-06
  5.52377586e-07 -1.92873152e-13  1.37637194e-05 -3.97707137e-13
 -8.06472720e-08 -3.27479514e-12 -2.05886434e-06  6.52133924e-14
  5.26218413e-07 -1.89059613e-13  1.31142358e-05 -3.94615634e-13
  1.87393565e-06 -1.32395581e-12  4.69195817e-05 -3.00719559e-12
 -3.05763999e-05  4.28389180e-04  5.05432905e-12 -7.68805280e-11
 -1.30588901e-11  2.24465641e-10  1.08195956e-11 -2.18499350e-10
  1.16956051e-01]
supnorm grad right now is: 0.11695605081026819
Weights right now are: 
[-7.5888458  -0.86731917 -5.72447715 -0.89737704  7.05737554  0.73332373
  5.88918419  0.33132093  1.04016535 -3.9063958   1.41729741 -0.22224152
  2.40624397 13.34066265  3.77492535 13.33873117  0.16162869 -5.39594268
  0.32236277 -3.62852195  2.95938704 14.12886839  3.62959221 13.7688486
 -0.94853253  6.34762234 -1.47450213 -3.65760925  2.62319395 -4.16662437
  2.78315101  5.16220393 -3.98292413  5.84581626 -2.4683727  -2.7389489
  3.7000734   4.7891656   2.79392623  5.22623994 11.62326893  9.8280021
  1.52421199 13.82925262 -7.14781699 -3.05772201  4.41457914 -4.89417532
 11.69059154  8.75893653  1.46034071 12.52125245 10.84367492 11.04235941
  0.27534099 14.90959982 -1.44890163 -0.34280478 -6.9094924  -2.10697275
  3.2607146   1.46457361 -4.13916409 -1.67283052 24.94918826]
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.286540607268
gradient value of function right now is: [-2.72619099e-06 -1.49250375e-08 -1.01016998e-05 -9.80896300e-09
  2.72618553e-06  1.49250375e-08  1.01016943e-05  9.80896299e-09
 -2.76259902e-05 -2.77291419e-06 -2.86446606e-05 -2.10639367e-06
 -1.93789361e-08 -1.99667679e-09 -2.00911306e-08 -1.51728811e-09
 -4.14497258e-05 -4.20866485e-06 -4.29760229e-05 -3.19612899e-06
 -9.77886460e-09 -1.00803637e-09 -1.01382238e-08 -7.66029683e-10
  8.42025426e-07  1.00685364e-07  8.88278552e-07  2.27295136e-06
 -3.91939703e-06 -4.67473291e-07 -4.13468788e-06 -1.05767597e-05
  1.08665122e-06  1.29753338e-07  1.14634102e-06  2.93275681e-06
 -1.43506814e-06 -1.71614202e-07 -1.51389764e-06 -3.87373636e-06
  6.30093582e-07 -2.51469481e-13  1.80992156e-05 -5.51454707e-13
 -8.40549863e-08 -2.98603567e-12 -2.48521798e-06  1.10036034e-12
  5.99544125e-07 -2.46849554e-13  1.72256764e-05 -5.46299921e-13
  2.07439377e-06 -1.69907190e-12  5.99711218e-05 -3.97203235e-12
 -3.44042616e-05  4.83685437e-04  6.88475480e-12 -1.02069110e-10
 -1.53329740e-11  2.60882504e-10  1.70323233e-11 -3.12367550e-10
  1.16933454e-01]
supnorm grad right now is: 0.11693345435018791
Weights right now are: 
[-7.58690844 -0.86727866 -5.72341662 -0.89734418  7.05543818  0.73328323
  5.88812366  0.33128806  1.04178781 -3.90295096  1.41896091 -0.21908117
  2.40626533 13.34087223  3.77494744 13.3389069   0.16212998 -5.39482046
  0.32287734 -3.62750401  2.95939859 14.12897835  3.62960415 13.76893983
 -0.94936044  6.34649633 -1.47525839 -3.6586423   2.6253555  -4.16397177
  2.78510036  5.16486455 -3.98408697  5.84435751 -2.46942503 -2.74038902
  3.70196232  4.79143773  2.79562457  5.22856818 11.62171644  9.8280021
  1.52296085 13.82925262 -7.14564946 -3.05772201  4.41710673 -4.89417535
 11.68911721  8.75893653  1.45920021 12.52125245 10.84169494 11.04235941
  0.27369328 14.90959982 -1.44067678 -0.36352599 -6.9094924  -2.10697274
  3.26071461  1.46457356 -4.13916409 -1.67283052 24.87195382]
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.283868376708
gradient value of function right now is: [-3.21467137e-06 -1.98289881e-08 -1.20676828e-05 -1.30764013e-08
  3.21467035e-06  1.98289881e-08  1.20676818e-05  1.30764013e-08
 -3.30374410e-05 -3.07174798e-06 -3.42644237e-05 -2.32510966e-06
 -2.59634731e-08 -2.48065682e-09 -2.69247974e-08 -1.87874904e-09
 -5.13122176e-05 -4.82608753e-06 -5.32156060e-05 -3.65235124e-06
 -1.31464910e-08 -1.25673101e-09 -1.36332230e-08 -9.51820951e-10
  1.01273692e-06  1.13810647e-07  1.06823734e-06  2.70460323e-06
 -4.63638199e-06 -5.19412919e-07 -4.89045730e-06 -1.23769443e-05
  1.29629738e-06  1.45440064e-07  1.36733610e-06  3.46111874e-06
 -1.70593665e-06 -1.91569751e-07 -1.79942506e-06 -4.55524256e-06
  6.68142523e-07 -3.06650557e-13  2.13783441e-05 -7.09497043e-13
 -8.40147324e-08 -2.66051714e-12 -2.77684608e-06  2.11621238e-12
  6.35235374e-07 -3.01308232e-13  2.03309996e-05 -7.01724055e-13
  2.15454612e-06 -2.04754227e-12  6.94743948e-05 -4.90625789e-12
 -3.61491209e-05  5.09688094e-04  8.56154834e-12 -1.25510300e-10
 -1.70380242e-11  2.89279479e-10  2.35457593e-11 -4.11309507e-10
  2.73005376e-01]
supnorm grad right now is: 0.27300537553210014
Weights right now are: 
[-7.58635691 -0.86726724 -5.72311514 -0.89733491  7.05488665  0.7332718
  5.88782218  0.33127879  1.04224905 -3.90196172  1.41943378 -0.21817295
  2.40627136 13.34092888  3.77495367 13.338954    0.1622719  -5.39450089
  0.32302302 -3.62721404  2.95940185 14.12900795  3.62960752 13.76896419
 -0.94959546  6.34617404 -1.47547307 -3.65893583  2.6259693  -4.16321199
  2.78565391  5.16562081 -3.98441719  5.84393969 -2.46972387 -2.74079839
  3.70249807  4.7920867   2.79610626  5.22922909 11.62127187  9.8280021
  1.52260554 13.82925262 -7.14501801 -3.05772202  4.41783131 -4.89417538
 11.68869497  8.75893653  1.4588763  12.52125245 10.84112597 11.04235941
  0.27322452 14.90959982 -1.43831764 -0.36944963 -6.9094924  -2.10697274
  3.26071463  1.46457352 -4.13916409 -1.67283053 24.99376865]
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.284810731115
gradient value of function right now is: [-3.42216679e-06 -2.20960360e-08 -1.29113862e-05 -1.45902369e-08
  3.42216963e-06  2.20960360e-08  1.29113891e-05  1.45902369e-08
 -3.53610852e-05 -3.18855498e-06 -3.66775804e-05 -2.41003076e-06
 -2.90269868e-08 -2.69037732e-09 -3.01045247e-08 -2.03476744e-09
 -5.56497775e-05 -5.07572098e-06 -5.77192720e-05 -3.83582890e-06
 -1.47167839e-08 -1.36476772e-09 -1.52630663e-08 -1.03221882e-09
  1.08605739e-06  1.19097875e-07  1.14551611e-06  2.88801803e-06
 -4.93701520e-06 -5.39591724e-07 -5.20729133e-06 -1.31226672e-05
  1.38537514e-06  1.51661452e-07  1.46121885e-06  3.68309523e-06
 -1.81964383e-06 -1.99328460e-07 -1.91926271e-06 -4.83786357e-06
  6.79007862e-07 -3.32803491e-13  2.27506490e-05 -7.87821447e-13
 -8.34337864e-08 -2.52191665e-12 -2.89201839e-06  2.54889093e-12
  6.45357285e-07 -3.27112198e-13  2.16294945e-05 -7.78599992e-13
  2.17140762e-06 -2.21152282e-12  7.33621182e-05 -5.35582980e-12
 -3.66049911e-05  5.16777529e-04  9.33279916e-12 -1.36342125e-10
 -1.77232415e-11  3.00575902e-10  2.67478917e-11 -4.60032098e-10
  2.72967688e-01]
supnorm grad right now is: 0.2729676879096597
Weights right now are: 
[-7.58643867 -0.86726797 -5.72315683 -0.89733548  7.05496841  0.73327254
  5.88786387  0.33127937  1.0421864  -3.90214547  1.41936969 -0.21834338
  2.406271   13.34092402  3.7749533  13.33894997  0.16225621 -5.39455148
  0.32300695 -3.62726059  2.95940166 14.12900551  3.62960733 13.76896218
 -0.94956486  6.34622968 -1.47544512 -3.65889551  2.62588543 -4.16334908
  2.78557828  5.16551192 -3.98437291  5.84401362 -2.4696838  -2.74074053
  3.70242758  4.79197294  2.79604289  5.22913733 11.62135989  9.8280021
  1.5226546  13.82925262 -7.14516887 -3.05772202  4.41770204 -4.8941754
 11.68877877  8.75893653  1.45892116 12.52125245 10.84124689 11.04235941
  0.27329524 14.90959982 -1.43879949 -0.36847166 -6.9094924  -2.10697274
  3.26071463  1.46457352 -4.13916409 -1.67283052 24.79008682]
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.253025844304
gradient value of function right now is: [-3.48628065e-06 -2.28187414e-08 -1.31732784e-05 -1.50732106e-08
  3.48627894e-06  2.28187414e-08  1.31732767e-05  1.50732106e-08
 -3.60824910e-05 -3.22356817e-06 -3.74267770e-05 -2.43542974e-06
 -3.00058661e-08 -2.75561750e-09 -3.11205664e-08 -2.08322671e-09
 -5.70082126e-05 -5.15150324e-06 -5.91297455e-05 -3.89141396e-06
 -1.52189371e-08 -1.39840609e-09 -1.57842779e-08 -1.05721304e-09
  1.10882687e-06  1.20704189e-07  1.16951274e-06  2.94475294e-06
 -5.02949144e-06 -5.45634801e-07 -5.30474145e-06 -1.33510267e-05
  1.41292087e-06  1.53540268e-07  1.49024777e-06  3.75145230e-06
 -1.85463176e-06 -2.01651919e-07 -1.95613341e-06 -4.92443414e-06
  6.81793294e-07 -3.36232786e-13  2.31725672e-05 -8.05555705e-13
 -8.32064725e-08 -2.70880821e-12 -2.92674922e-06  2.39581233e-12
  6.47941160e-07 -3.30364116e-13  2.20285881e-05 -7.95742208e-13
  2.17481393e-06 -2.22967082e-12  7.45473757e-05 -5.44736777e-12
 -3.67154940e-05  5.18545840e-04  8.83558765e-12 -1.34468437e-10
 -1.67317177e-11  2.94464937e-10  2.65298609e-11 -4.65541582e-10
 -1.12720658e+00]
supnorm grad right now is: 1.1272065821777009
Weights right now are: 
[-7.5866052  -0.86727017 -5.72324279 -0.89733724  7.05513494  0.73327473
  5.88794983  0.33128112  1.04205647 -3.9025031   1.41923668 -0.21867446
  2.40626988 13.34091256  3.77495214 13.33894056  0.16222133 -5.39464973
  0.32297119 -3.62735055  2.95940106 14.12899965  3.62960671 13.76895741
 -0.9495009   6.34633903 -1.4753867  -3.65881282  2.62571421 -4.16361327
  2.78542387  5.16529358 -3.98428152  5.84415797 -2.46960109 -2.74062323
  3.70228256  4.79175648  2.7959125   5.22895233 11.62152043  9.8280021
  1.52275428 13.82925263 -7.14546281 -3.05772201  4.41744956 -4.89417542
 11.68893159  8.75893653  1.45901222 12.52125245 10.8414661  11.04235941
  0.27343481 14.90959983 -1.43967763 -0.36656459 -6.9094924  -2.10697274
  3.26071462  1.46457352 -4.1391641  -1.6728305  24.51471686]
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5145.220805301257
gradient value of function right now is: [-3.51595660e-06 -2.31569519e-08 -1.32947306e-05 -1.52992919e-08
  3.51595706e-06  2.31569519e-08  1.32947310e-05  1.52992919e-08
 -3.64170248e-05 -3.23951819e-06 -3.77742013e-05 -2.44698138e-06
 -3.04643059e-08 -2.78589984e-09 -3.15964152e-08 -2.10570715e-09
 -5.76402456e-05 -5.18639521e-06 -5.97859930e-05 -3.91698636e-06
 -1.54541740e-08 -1.41402452e-09 -1.60284452e-08 -1.06881144e-09
  1.11939180e-06  1.21441339e-07  1.18064661e-06  2.97104213e-06
 -5.07225687e-06 -5.48393673e-07 -5.34980497e-06 -1.34564684e-05
  1.42568289e-06  1.54400035e-07  1.50369639e-06  3.78307693e-06
 -1.87081490e-06 -2.02716255e-07 -1.97318662e-06 -4.96441613e-06
  6.82996913e-07 -3.38769810e-13  2.33676761e-05 -8.14915666e-13
 -8.30748588e-08 -2.82035393e-12 -2.94262590e-06  2.03727715e-12
  6.49055516e-07 -3.32706178e-13  2.22131189e-05 -8.04574308e-13
  2.17610701e-06 -2.24248631e-12  7.50938932e-05 -5.49284970e-12
 -3.67619303e-05  5.19301874e-04  9.30396717e-12 -1.36396586e-10
 -1.65827535e-11  2.89401774e-10  2.73223894e-11 -4.70368098e-10
 -1.39788665e+00]
supnorm grad right now is: 1.3978866484196981
Weights right now are: 
[-7.58662672 -0.86727039 -5.72325277 -0.89733742  7.05515646  0.73327495
  5.88795981  0.3312813   1.04204176 -3.90255503  1.41922165 -0.21872237
  2.40626976 13.3409115   3.77495202 13.3389397   0.16221778 -5.39466059
  0.32296756 -3.6273605   2.959401   14.12899912  3.62960664 13.76895698
 -0.94949402  6.3463539  -1.47538042 -3.65880376  2.62569607 -4.16364857
  2.78540751  5.16527003 -3.98427167  5.84417778 -2.46959218 -2.74061035
  3.70226782  4.79173054  2.79589925  5.22893332 11.62153959  9.8280021
  1.52276483 13.82925263 -7.14551873 -3.05772201  4.41741458 -4.89417545
 11.68894988  8.75893653  1.45902187 12.52125245 10.84149416 11.04235941
  0.27345006 14.90959983 -1.4397851  -0.36634221 -6.9094924  -2.10697274
  3.26071462  1.4645735  -4.1391641  -1.67283049 25.07712596]
NN weights: [-7.58680001 -0.86727556 -5.7233542  -0.89734165  7.05532975  0.73328013
  5.88806124  0.33128553  1.04188443 -3.90279197  1.41906009 -0.21893672
  2.40626699 13.34088599  3.77494916 13.33891824  0.16216299 -5.39475947
  0.32291125 -3.62744922  2.95939949 14.12898559  3.62960508 13.76894573
 -0.94941099  6.3464403  -1.47530457 -3.65870357  2.62548436 -4.1638444
  2.78521658  5.16501839 -3.98415692  5.84428653 -2.46948833 -2.74047309
  3.70207745  4.79155101  2.79572808  5.22870604 11.62164735  9.8280021
  1.52288668 13.82925262 -7.1455851  -3.05772201  4.41722875 -4.89417539
 11.68905179  8.75893653  1.45913271 12.52125245 10.84161469 11.04235941
  0.27360076 14.90959982 -1.4403252  -0.36461371 -6.9094924  -2.10697274
  3.26071462  1.46457353 -4.13916409 -1.67283051]
Minimum obj value:-5145.220805301257
Optimal xi: 24.851318194127877
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1553.2825347708456
W_T_median: 1404.876255228885
W_T_pctile_5: 621.9833554099082
W_T_CVAR_5_pct: 485.4393902631574
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
F value: -5145.220805301257
-----------------------------------------------
{'NN': [-7.5868000095218795, -0.8672755641371089, -5.723354199845702, -0.8973416473560798, 7.055329746003976, 0.7332801274154365, 5.888061239041404, 0.33128552815219886, 1.0418844275773862, -3.902791972128975, 1.4190600893987515, -0.21893671764611738, 2.4062669923633, 13.340885985027722, 3.774949158596164, 13.338918239073642, 0.162162986537129, -5.394759470970027, 0.3229112527765158, -3.6274492200005284, 2.959399491683203, 14.12898558515753, 3.629605083590105, 13.76894572734121, -0.9494109898545864, 6.346440300699791, -1.4753045677258148, -3.6587035659224703, 2.6254843575700417, -4.163844401162288, 2.7852165765745425, 5.165018394284534, -3.9841569150091423, 5.844286532420956, -2.469488330832115, -2.740473086664376, 3.702077445783124, 4.791551012569241, 2.7957280798768216, 5.228706040743885, 11.621647349600542, 9.82800209821849, 1.5228866783706956, 13.829252623691394, -7.145585099492885, -3.0577220146550057, 4.417228750869506, -4.89417538820832, 11.689051791769117, 8.758936529079968, 1.459132711172519, 12.521252449696037, 10.841614694530476, 11.042359409755392, 0.2736007599853121, 14.90959982317116, -1.4403252009813698, -0.36461371032549644, -6.9094924008240834, -2.1069727446029236, 3.260714620774335, 1.4645735327498677, -4.13916409051742, -1.6728305138721449]}
[-7.58680001 -0.86727556 -5.7233542  -0.89734165  7.05532975  0.73328013
  5.88806124  0.33128553  1.04188443 -3.90279197  1.41906009 -0.21893672
  2.40626699 13.34088599  3.77494916 13.33891824  0.16216299 -5.39475947
  0.32291125 -3.62744922  2.95939949 14.12898559  3.62960508 13.76894573
 -0.94941099  6.3464403  -1.47530457 -3.65870357  2.62548436 -4.1638444
  2.78521658  5.16501839 -3.98415692  5.84428653 -2.46948833 -2.74047309
  3.70207745  4.79155101  2.79572808  5.22870604 11.62164735  9.8280021
  1.52288668 13.82925262 -7.1455851  -3.05772201  4.41722875 -4.89417539
 11.68905179  8.75893653  1.45913271 12.52125245 10.84161469 11.04235941
  0.27360076 14.90959982 -1.4403252  -0.36461371 -6.9094924  -2.10697274
  3.26071462  1.46457353 -4.13916409 -1.67283051]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1260.9596434837733
W_T_median: 1221.9241450328454
W_T_pctile_5: 830.3340558417918
W_T_CVAR_5_pct: 742.5812257595575
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.577039411799
gradient value of function right now is: [ 3.04320971e-08  5.41034390e-14  3.09332579e-08  2.48056487e-14
 -3.04108170e-08 -5.40656408e-14 -3.09116294e-08 -2.47883243e-14
  2.06503732e-11  9.14723445e-09  3.24363614e-11  8.56872962e-11
  1.79239133e-16  8.51648210e-14  2.85756651e-16  7.72826982e-16
  3.66984533e-12  1.67826378e-09  5.80271592e-12  1.54924044e-11
  6.33311914e-17  2.99766893e-14  1.00886852e-16  2.72504571e-16
 -7.62800002e-16 -1.86479883e-11 -8.12135549e-16 -3.52024955e-15
  8.96994778e-15  7.11052846e-10  9.57231955e-15  4.14372886e-14
 -1.32826200e-15 -4.18123132e-11 -1.41468515e-15 -6.13126762e-15
  1.14378754e-15  7.15073420e-11  1.21995828e-15  5.28392075e-15
 -1.94274730e-14 -3.65451915e-16 -3.26259072e-14 -7.39206470e-16
  4.29458434e-11  2.21838918e-12  6.78854320e-11  2.75766008e-12
 -1.99892308e-14 -3.92349195e-16 -3.35318979e-14 -7.76101079e-16
 -1.55884639e-13 -2.85570310e-15 -2.61451101e-13 -5.82978660e-15
  2.04093382e-11 -3.14906810e-10  1.46170240e-12 -1.15075989e-11
 -2.70539266e-12  3.05585281e-11  2.49409258e-12 -2.55577889e-11
  2.20957231e+01]
supnorm grad right now is: 22.095723055977437
Weights right now are: 
[-7.59438086 -0.86727823 -5.74316275 -0.89734326  7.06291065  0.73328279
  5.90786985  0.33128714  0.99183471 -3.91741457  1.36764351 -0.23025798
  2.40626405 13.34088512  3.77494614 13.33891756  0.12935226 -5.40434579
  0.28920112 -3.63494677  2.95939813 14.12898519  3.62960368 13.76894541
 -0.94819616  6.3468518  -1.47402309 -3.65486053  2.61959979 -4.16585672
  2.77900927  5.14641696 -3.98249387  5.84485023 -2.46773405 -2.73521241
  3.70038324  4.79097044  2.79394095  5.22334084 11.62424766  9.82800212
  1.55032607 13.82925273 -7.14663598 -3.0577228   4.40606305 -4.89417672
 11.69155434  8.75893655  1.48554801 12.52125255 10.85292892 11.04235955
  0.38823562 14.90960036 -1.59081068  0.04111459 -6.90949381 -2.10695546
  3.26071543  1.46455902 -4.13917208 -1.67273434 26.36332248]
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.869417200527
gradient value of function right now is: [ 3.09243089e-08  5.49744112e-14  3.14309914e-08  2.52016211e-14
 -3.09380177e-08 -5.49987584e-14 -3.14449242e-08 -2.52127804e-14
  1.80089054e-11  9.26815502e-09  2.92523724e-11  8.13552916e-11
  1.65976249e-16  8.66200677e-14  2.70362503e-16  7.55269979e-16
  3.28826161e-12  1.70384801e-09  5.34846767e-12  1.49068977e-11
  5.84721194e-17  3.04787019e-14  9.52244182e-17  2.65914808e-16
 -1.34059969e-16 -1.75170690e-11 -1.41898428e-16 -6.46358892e-16
  2.03647166e-15  7.09461506e-10  2.17654325e-15  9.75724925e-15
 -2.46616494e-16 -4.00478498e-11 -2.61463533e-16 -1.18792581e-15
  2.53362200e-16  7.10388822e-11  2.70121711e-16  1.21621644e-15
 -4.67110086e-15 -9.41876913e-17 -7.14673627e-15 -1.61410628e-16
  2.06320629e-11  1.76610502e-12  2.93672036e-11  1.93545158e-12
 -4.84492318e-15 -1.15775838e-16 -7.39324695e-15 -1.86822194e-16
 -3.94179580e-14 -6.71854691e-16 -6.03109777e-14 -1.22414304e-15
  9.04758458e-12 -1.25276646e-10  1.01520094e-12 -7.03169268e-12
 -1.64277409e-12  1.50422867e-11  1.59947382e-12 -1.34295759e-11
  3.69952539e+00]
supnorm grad right now is: 3.6995253917660964
Weights right now are: 
[-7.59577235 -0.86727854 -5.7459965  -0.89734344  7.06430215  0.73328311
  5.9107036   0.33128733  0.98593563 -3.91927718  1.36158096 -0.23158398
  2.40626371 13.34088502  3.77494578 13.33891748  0.12549781 -5.40549672
  0.28524035 -3.63582535  2.95939797 14.12898514  3.62960352 13.76894538
 -0.94805385  6.3469002  -1.47387297 -3.65441034  2.61891038 -4.16610382
  2.77828205  5.14423604 -3.98229905  5.84491681 -2.46752854 -2.73459614
  3.70018475  4.79090128  2.79373159  5.22271226 11.62455222  9.82800213
  1.55354667 13.82925274 -7.14675915 -3.0577229   4.40475647 -4.89417688
 11.69184746  8.75893656  1.48864797 12.52125256 10.85425436 11.04235956
  0.40213222 14.90960041 -1.60952507  0.12592525 -6.90949398 -2.10695358
  3.26071552  1.46455743 -4.13917299 -1.6727238  25.2258353 ]
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.248153105644
gradient value of function right now is: [ 3.09707707e-08  5.50550749e-14  3.14778598e-08  2.52382268e-14
 -3.09501831e-08 -5.50185112e-14 -3.14569358e-08 -2.52214683e-14
  1.77932213e-11  9.27311546e-09  2.89811579e-11  8.09405513e-11
  1.64649579e-16  8.67071294e-14  2.68732960e-16  7.52997877e-16
  3.25524629e-12  1.70528920e-09  5.30742073e-12  1.48467708e-11
  5.80017250e-17  3.05152951e-14  9.46504272e-17  2.65136038e-16
 -9.98163281e-17 -1.74045132e-11 -1.05800617e-16 -4.83889391e-16
  1.58773975e-15  7.08802581e-10  1.70420416e-15  7.62189374e-15
 -1.85408708e-16 -3.98588628e-11 -1.96954994e-16 -8.97388135e-16
  1.96171418e-16  7.09465803e-11  2.09898217e-16  9.44269951e-16
 -3.66957297e-15 -8.24713248e-17 -5.52850650e-15 -1.33503796e-16
  1.84354032e-11  1.75883477e-12  2.58097525e-11  1.90614437e-12
 -3.82478471e-15 -1.04493239e-16 -5.74476089e-15 -1.59400096e-16
 -3.11184295e-14 -5.66660306e-16 -4.69082858e-14 -9.81917665e-16
  8.00836742e-12 -1.07467741e-10  9.81914805e-13 -6.74846761e-12
 -1.56386081e-12  1.38774735e-11  1.53626588e-12 -1.25876577e-11
  5.07640174e-01]
supnorm grad right now is: 0.5076401744891609
Weights right now are: 
[-7.5964794  -0.86727858 -5.74687652 -0.89734347  7.0650092   0.73328314
  5.91158363  0.33128735  0.98526383 -3.91965352  1.36089031 -0.23173623
  2.40626367 13.34088501  3.77494574 13.33891747  0.12505927 -5.40565784
  0.28478967 -3.63592552  2.95939795 14.12898513  3.6296035  13.76894537
 -0.94803766  6.34690602 -1.4738559  -3.65435915  2.61883199 -4.16614457
  2.77819935  5.14398801 -3.9822769   5.8449251  -2.46750518 -2.73452607
  3.70016219  4.79089214  2.79370778  5.22264079 11.62458686  9.82800213
  1.553913   13.82925274 -7.14677363 -3.05772294  4.40460719 -4.89417694
 11.69188079  8.75893656  1.48900056 12.52125257 10.85440508 11.04235956
  0.40372152 14.90960041 -1.61167567  0.13846642 -6.90949401 -2.10695324
  3.26071556  1.46455691 -4.13917312 -1.67272231 24.98171493]
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.130470954795
gradient value of function right now is: [ 3.08305610e-08  5.48055469e-14  3.13352399e-08  2.51238739e-14
 -3.08350496e-08 -5.48135182e-14 -3.13398018e-08 -2.51275274e-14
  1.77366168e-11  9.23382333e-09  2.88829331e-11  8.06391566e-11
  1.64112076e-16  8.63457801e-14  2.67805840e-16  7.50180057e-16
  3.24524130e-12  1.69838573e-09  5.29007013e-12  1.47936078e-11
  5.78053515e-17  3.03835210e-14  9.43119458e-17  2.64108141e-16
 -1.01183965e-16 -1.73397560e-11 -1.07288869e-16 -4.89862335e-16
  1.61366478e-15  7.05954231e-10  1.73124226e-15  7.74357992e-15
 -1.88085441e-16 -3.97060552e-11 -1.99840831e-16 -9.09281601e-16
  1.99398679e-16  7.06735533e-11  2.13293108e-16  9.59238235e-16
 -3.72898176e-15 -8.27927266e-17 -5.61606453e-15 -1.35901853e-16
  1.86093687e-11  1.72818815e-12  2.60706600e-11  1.87948917e-12
 -3.88579429e-15 -1.04075850e-16 -5.83527355e-15 -1.61000804e-16
 -3.16103261e-14 -5.75218797e-16 -4.76242841e-14 -1.00847132e-15
  8.01830343e-12 -1.08532233e-10  9.67953152e-13 -6.67544306e-12
 -1.54875766e-12  1.38874594e-11  1.51787804e-12 -1.25429880e-11
  1.80314164e+00]
supnorm grad right now is: 1.8031416404286826
Weights right now are: 
[-7.59712503 -0.86727859 -5.74755215 -0.89734347  7.06565478  0.73328315
  5.9122592   0.33128735  0.98518244 -3.91986667  1.36080644 -0.23175605
  2.40626366 13.340885    3.77494574 13.33891747  0.1250063  -5.40570813
  0.2847352  -3.63593787  2.95939795 14.12898513  3.6296035  13.76894537
 -0.94803571  6.34690704 -1.47385384 -3.65435298  2.61882253 -4.16616226
  2.77818938  5.14395809 -3.98227423  5.84492682 -2.46750236 -2.73451761
  3.70015946  4.79088977  2.79370491  5.22263216 11.62459103  9.82800213
  1.55395719 13.82925274 -7.14677564 -3.05772297  4.40458882 -4.89417697
 11.69188481  8.75893656  1.48904311 12.52125257 10.85442327 11.04235956
  0.40391367 14.90960042 -1.61193627  0.14022949 -6.90949403 -2.1069531
  3.26071559  1.46455666 -4.13917316 -1.67272196 25.0693523 ]
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.257209278916
gradient value of function right now is: [ 3.08193122e-08  5.47849029e-14  3.13236728e-08  2.51143314e-14
 -3.08223023e-08 -5.47902137e-14 -3.13267118e-08 -2.51167656e-14
  1.76877933e-11  9.22909020e-09  2.88167647e-11  8.05132723e-11
  1.63760664e-16  8.63121420e-14  2.67330619e-16  7.49281191e-16
  3.23761874e-12  1.69780435e-09  5.27984841e-12  1.47747356e-11
  5.76795079e-17  3.03718741e-14  9.41419898e-17  2.63787888e-16
 -9.61874072e-17 -1.73100458e-11 -1.01963831e-16 -4.66644730e-16
  1.53956411e-15  7.05414447e-10  1.65291694e-15  7.39470220e-15
 -1.78906164e-16 -3.96492768e-11 -1.90071994e-16 -8.66527189e-16
  1.90074398e-16  7.06150804e-11  2.03417849e-16  9.15469934e-16
 -3.56226703e-15 -8.05615361e-17 -5.35574402e-15 -1.29937534e-16
  1.81524316e-11  1.74108277e-12  2.53636128e-11  1.88499312e-12
 -3.71437253e-15 -1.02281944e-16 -5.56727044e-15 -1.55462807e-16
 -3.02353559e-14 -5.52498957e-16 -4.54841580e-14 -9.54339620e-16
  7.86531086e-12 -1.05342116e-10  9.70807454e-13 -6.66359709e-12
 -1.54438054e-12  1.36717870e-11  1.51796881e-12 -1.24113223e-11
  4.29217984e-01]
supnorm grad right now is: 0.42921798399862426
Weights right now are: 
[-7.59770316 -0.86727859 -5.74814208 -0.89734347  7.06623287  0.73328315
  5.91284909  0.33128735  0.98517227 -3.92004213  1.3607958  -0.23175974
  2.40626366 13.340885    3.77494574 13.33891747  0.12499981 -5.40574176
  0.2847285  -3.63593961  2.95939795 14.12898513  3.6296035  13.76894537
 -0.94803548  6.34690744 -1.47385359 -3.65435223  2.61882138 -4.16617579
  2.77818817  5.14395445 -3.9822739   5.84492766 -2.46750201 -2.73451659
  3.70015913  4.79088834  2.79370456  5.22263112 11.62459154  9.82800213
  1.55396256 13.82925274 -7.14677608 -3.05772301  4.40458632 -4.89417701
 11.6918853   8.75893656  1.48904827 12.52125257 10.85442548 11.04235956
  0.40393704 14.90960042 -1.61196808  0.1404676  -6.90949405 -2.10695299
  3.26071561  1.46455648 -4.13917319 -1.67272177 24.99811808]
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.257946432695
gradient value of function right now is: [ 3.07474150e-08  5.46566888e-14  3.12504884e-08  2.50555450e-14
 -3.07593149e-08 -5.46778224e-14 -3.12625825e-08 -2.50652313e-14
  1.76443854e-11  9.20798447e-09  2.87469460e-11  8.03224697e-11
  1.63376960e-16  8.61216615e-14  2.66710984e-16  7.47576890e-16
  3.23025425e-12  1.69420264e-09  5.26798925e-12  1.47422682e-11
  5.75412450e-17  3.03031784e-14  9.39186925e-17  2.63173584e-16
 -9.56472424e-17 -1.72688354e-11 -1.01384586e-16 -4.64121497e-16
  1.53132005e-15  7.03820610e-10  1.64402818e-15  7.35628562e-15
 -1.77908214e-16 -3.95557602e-11 -1.89000856e-16 -8.61867140e-16
  1.89056551e-16  7.04595656e-11  2.02321875e-16  9.10725427e-16
 -3.54367675e-15 -8.01153393e-17 -5.32710382e-15 -1.29104795e-16
  1.80761607e-11  1.73636040e-12  2.52533548e-11  1.87913291e-12
 -3.69491484e-15 -1.01769643e-16 -5.53733100e-15 -1.54540359e-16
 -3.00826024e-14 -5.49167963e-16 -4.52492515e-14 -9.47941288e-16
  7.83574101e-12 -1.04888629e-10  9.68141370e-13 -6.64155449e-12
 -1.53978930e-12  1.36177153e-11  1.51367559e-12 -1.23649402e-11
  3.90169055e-01]
supnorm grad right now is: 0.3901690549496176
Weights right now are: 
[-7.59829144 -0.86727859 -5.74874024 -0.89734347  7.06682113  0.73328315
  5.91344724  0.33128735  0.98517074 -3.92021859  1.36079402 -0.23176155
  2.40626366 13.340885    3.77494574 13.33891747  0.12499897 -5.40577438
  0.2847276  -3.63594007  2.95939795 14.12898513  3.6296035  13.76894537
 -0.94803545  6.34690778 -1.47385356 -3.65435214  2.61882124 -4.16618931
  2.77818802  5.14395401 -3.98227386  5.84492843 -2.46750197 -2.73451646
  3.70015909  4.79088698  2.79370452  5.22263099 11.6245916   9.82800213
  1.55396322 13.82925274 -7.14677647 -3.05772304  4.40458556 -4.89417705
 11.69188536  8.75893656  1.4890489  12.52125257 10.85442575 11.04235956
  0.40393988 14.90960042 -1.6119721   0.14050024 -6.90949407 -2.10695284
  3.26071565  1.4645562  -4.13917322 -1.6727215  24.96883343]
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.166187455472
gradient value of function right now is: [ 3.07629934e-08  5.46842404e-14  3.12662065e-08  2.50681700e-14
 -3.07473350e-08 -5.46564322e-14 -3.12502925e-08 -2.50554245e-14
  1.76354130e-11  9.20778416e-09  2.87360975e-11  8.03075353e-11
  1.63345351e-16  8.61298092e-14  2.66683007e-16  7.47592266e-16
  3.22936753e-12  1.69440587e-09  5.26712997e-12  1.47422339e-11
  5.75384629e-17  3.03111591e-14  9.39229032e-17  2.63220327e-16
 -9.39547557e-17 -1.72653719e-11 -9.98442341e-17 -4.54190766e-16
  1.51081445e-15  7.03839637e-10  1.62739859e-15  7.22052157e-15
 -1.74913183e-16 -3.95504371e-11 -1.86323517e-16 -8.43921956e-16
  1.86388087e-16  7.04643515e-11  2.00102880e-16  8.93469540e-16
 -3.48677066e-15 -8.45427048e-17 -5.24329143e-15 -1.35200736e-16
  1.79600264e-11  1.77318310e-12  2.51010994e-11  1.92717747e-12
 -3.64661099e-15 -1.07483999e-16 -5.46693811e-15 -1.62574988e-16
 -2.94734539e-14 -5.73053259e-16 -4.43457705e-14 -9.78464658e-16
  7.70479620e-12 -1.03450363e-10  9.73815583e-13 -6.74490135e-12
 -1.54304483e-12  1.37743372e-11  1.52053383e-12 -1.25534082e-11
 -1.62947351e+00]
supnorm grad right now is: 1.6294735136073615
Weights right now are: 
[-7.59890051 -0.86727859 -5.74935929 -0.89734347  7.06743021  0.73328315
  5.91406629  0.33128735  0.98517025 -3.92040091  1.36079331 -0.23176315
  2.40626366 13.340885    3.77494574 13.33891747  0.12499881 -5.40580796
  0.2847274  -3.63594038  2.95939795 14.12898513  3.6296035  13.76894537
 -0.94803544  6.34690812 -1.47385356 -3.65435213  2.61882123 -4.16620319
  2.778188    5.14395396 -3.98227386  5.84492921 -2.46750197 -2.73451645
  3.70015909  4.79088559  2.79370451  5.22263097 11.62459161  9.82800213
  1.5539633  13.82925274 -7.14677674 -3.05772307  4.40458517 -4.89417707
 11.69188536  8.75893656  1.48904898 12.52125257 10.85442578 11.04235956
  0.40394022 14.90960042 -1.61197271  0.14050551 -6.90949409 -2.10695273
  3.26071568  1.46455601 -4.13917325 -1.67272134 24.8736609 ]
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.675699268479
gradient value of function right now is: [ 3.08010213e-08  5.47523597e-14  3.13047525e-08  2.50994237e-14
 -3.08184474e-08 -5.47833074e-14 -3.13224629e-08 -2.51136082e-14
  1.76541055e-11  9.22315141e-09  2.87726217e-11  8.04328109e-11
  1.63590417e-16  8.62794205e-14  2.67117939e-16  7.48934617e-16
  3.23390759e-12  1.69754065e-09  5.27545631e-12  1.47689779e-11
  5.76126620e-17  3.03576968e-14  9.40565838e-17  2.63639444e-16
 -9.08740857e-17 -1.72923378e-11 -9.68496586e-17 -4.37879058e-16
  1.47464086e-15  7.05124610e-10  1.59506238e-15  7.00999289e-15
 -1.69486659e-16 -3.96158335e-11 -1.81114331e-16 -8.14740946e-16
  1.81626778e-16  7.05987094e-11  1.95759520e-16  8.66329500e-16
 -3.39611895e-15 -8.96264028e-17 -5.10084315e-15 -1.41002580e-16
  1.79140550e-11  1.83700260e-12  2.50153404e-11  2.00374340e-12
 -3.56577373e-15 -1.14366004e-16 -5.33908220e-15 -1.70894779e-16
 -2.85879035e-14 -5.99004433e-16 -4.29657558e-14 -1.00284212e-15
  7.65189731e-12 -1.02143111e-10  9.92571632e-13 -6.94601718e-12
 -1.56961199e-12  1.40747561e-11  1.55052633e-12 -1.28971088e-11
 -4.12257510e+00]
supnorm grad right now is: 4.122575096563384
Weights right now are: 
[-7.59946427 -0.86727859 -5.74993227 -0.89734347  7.06799394  0.73328315
  5.91463924  0.33128735  0.98516991 -3.9205697   1.36079276 -0.23176463
  2.40626366 13.340885    3.77494574 13.33891747  0.12499874 -5.40583902
  0.28472729 -3.63594065  2.95939795 14.12898513  3.6296035  13.76894537
 -0.94803544  6.34690843 -1.47385356 -3.65435213  2.61882122 -4.16621609
  2.778188    5.14395395 -3.98227386  5.84492994 -2.46750197 -2.73451644
  3.70015909  4.7908843   2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14677708 -3.0577231   4.40458468 -4.8941771
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442578 11.04235956
  0.40394027 14.90960042 -1.61197291  0.14050775 -6.90949411 -2.10695262
  3.2607157   1.46455578 -4.13917327 -1.67272112 24.68816284]
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16012.173994192732
gradient value of function right now is: [ 3.10826423e-08  5.52534095e-14  3.15908215e-08  2.53290126e-14
 -3.10735282e-08 -5.52372239e-14 -3.15815587e-08 -2.53215942e-14
  1.77112247e-11  9.30223600e-09  2.89017312e-11  8.09455367e-11
  1.64414618e-16  8.70360606e-14  2.68709216e-16  7.54418442e-16
  3.24734492e-12  1.71238313e-09  5.30316203e-12  1.48708117e-11
  5.79048793e-17  3.06287765e-14  9.46225342e-17  2.65596414e-16
 -7.68924427e-17 -1.73979076e-11 -8.29221899e-17 -3.65136751e-16
  1.28816829e-15  7.10903507e-10  1.41606450e-15  5.98963474e-15
 -1.44336452e-16 -3.98848645e-11 -1.56219130e-16 -6.82698488e-16
  1.57715041e-16  7.11644815e-11  1.72620051e-16  7.36817249e-16
 -2.93995952e-15 -1.02705641e-16 -4.40042856e-15 -1.54138988e-16
  1.70101753e-11  2.00915018e-12  2.36367842e-11  2.19715962e-12
 -3.13491392e-15 -1.32297304e-16 -4.67666368e-15 -1.90872987e-16
 -2.43095004e-14 -6.61275382e-16 -3.64205382e-14 -1.04225852e-15
  7.20448135e-12 -9.31626546e-11  1.05555227e-12 -7.49186013e-12
 -1.63843966e-12  1.46231945e-11  1.63643991e-12 -1.36696043e-11
 -1.12375676e+01]
supnorm grad right now is: 11.237567578343242
Weights right now are: 
[-7.60003534 -0.86727859 -5.75051266 -0.89734347  7.06856501  0.73328316
  5.91521963  0.33128735  0.98516957 -3.92074062  1.36079222 -0.23176612
  2.40626366 13.34088499  3.77494574 13.33891747  0.12499868 -5.40587048
  0.28472719 -3.63594092  2.95939795 14.12898513  3.6296035  13.76894537
 -0.94803544  6.34690875 -1.47385356 -3.65435212  2.61882122 -4.1662291
  2.778188    5.14395395 -3.98227386  5.84493067 -2.46750197 -2.73451644
  3.70015909  4.790883    2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14677738 -3.05772312  4.40458423 -4.89417712
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442579 11.04235956
  0.40394027 14.90960042 -1.611973    0.14050977 -6.90949411 -2.10695259
  3.26071571  1.46455568 -4.13917328 -1.67272105 24.00315897]
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16015.137587895091
gradient value of function right now is: [ 3.08779460e-08  5.48884414e-14  3.13826765e-08  2.51616888e-14
 -3.08663771e-08 -5.48678961e-14 -3.13709189e-08 -2.51522722e-14
  1.76103354e-11  9.23983913e-09  2.87286753e-11  8.04275648e-11
  1.63415371e-16  8.64579188e-14  2.67023553e-16  7.49487074e-16
  3.22865710e-12  1.70114673e-09  5.27133185e-12  1.47764505e-11
  5.75553986e-17  3.04257658e-14  9.40323431e-17  2.63867290e-16
 -8.02377385e-17 -1.72871090e-11 -8.60782950e-17 -3.83868792e-16
  1.32806351e-15  7.06150157e-10  1.45043697e-15  6.23637348e-15
 -1.50238297e-16 -3.96253805e-11 -1.61710627e-16 -7.16303122e-16
  1.62964912e-16  7.06944295e-11  1.77245971e-16  7.68578863e-16
 -3.04427461e-15 -9.57908308e-17 -4.56023590e-15 -1.45425524e-16
  1.70709070e-11  1.92863672e-12  2.37493984e-11  2.10348502e-12
 -3.22566262e-15 -1.23360307e-16 -4.81603167e-15 -1.79300198e-16
 -2.53806817e-14 -6.22463580e-16 -3.80560242e-14 -9.97953854e-16
  7.26724722e-12 -9.49149307e-11  1.02353065e-12 -7.20822837e-12
 -1.59517996e-12  1.42011231e-11  1.58808685e-12 -1.31950592e-11
 -8.64662298e+00]
supnorm grad right now is: 8.646622979845402
Weights right now are: 
[-7.60061443 -0.86727859 -5.75110121 -0.89734347  7.06914412  0.73328316
  5.9158082   0.33128735  0.98516925 -3.92091388  1.36079169 -0.23176762
  2.40626366 13.34088499  3.77494574 13.33891747  0.12499862 -5.40590237
  0.28472709 -3.6359412   2.95939795 14.12898513  3.6296035  13.76894537
 -0.94803544  6.34690907 -1.47385356 -3.65435212  2.61882122 -4.16624226
  2.778188    5.14395395 -3.98227386  5.8449314  -2.46750197 -2.73451644
  3.70015909  4.79088168  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14677761 -3.05772314  4.4045839  -4.89417714
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442579 11.04235956
  0.40394028 14.90960042 -1.61197306  0.14051119 -6.90949413 -2.10695251
  3.26071573  1.46455555 -4.1391733  -1.67272093 24.27092489]
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.651694404612
gradient value of function right now is: [ 3.05887960e-08  5.43740275e-14  3.10887316e-08  2.49259758e-14
 -3.05726186e-08 -5.43452980e-14 -3.10722904e-08 -2.49128081e-14
  1.75183982e-11  9.15228332e-09  2.85514915e-11  7.98153415e-11
  1.62379484e-16  8.56403694e-14  2.65140493e-16  7.43394439e-16
  3.21049287e-12  1.68524694e-09  5.23726185e-12  1.46621509e-11
  5.71971934e-17  3.01390444e-14  9.33784447e-17  2.61741040e-16
 -9.00931750e-17 -1.71596932e-11 -9.60429074e-17 -4.33949235e-16
  1.46304575e-15  6.99822547e-10  1.58307525e-15  6.95095639e-15
 -1.68051580e-16 -3.93121441e-11 -1.79631927e-16 -8.07501443e-16
  1.80210102e-16  7.00803372e-11  1.94298236e-16  8.59114520e-16
 -3.36841010e-15 -8.94865487e-17 -5.05936653e-15 -1.40700122e-16
  1.77720836e-11  1.82747501e-12  2.48235700e-11  1.99480327e-12
 -3.53784475e-15 -1.14189684e-16 -5.29742502e-15 -1.70588605e-16
 -2.83409487e-14 -5.97517758e-16 -4.25964832e-14 -9.99514158e-16
  7.57608932e-12 -1.01283278e-10  9.84425898e-13 -6.90075943e-12
 -1.55727569e-12  1.39897303e-11  1.53862275e-12 -1.28221829e-11
 -4.31421891e+00]
supnorm grad right now is: 4.314218913691569
Weights right now are: 
[-7.6012407  -0.86727859 -5.7517377  -0.89734347  7.06977031  0.73328316
  5.91644461  0.33128735  0.98516889 -3.92110122  1.36079111 -0.23176923
  2.40626366 13.34088499  3.77494574 13.33891747  0.12499855 -5.40593687
  0.28472698 -3.63594149  2.95939795 14.12898513  3.6296035  13.76894537
 -0.94803544  6.34690942 -1.47385356 -3.65435212  2.61882122 -4.16625649
  2.778188    5.14395395 -3.98227386  5.8449322  -2.46750197 -2.73451644
  3.70015909  4.79088026  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14677787 -3.05772316  4.40458353 -4.89417716
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442579 11.04235956
  0.40394028 14.90960042 -1.61197317  0.14051274 -6.90949414 -2.10695245
  3.26071574  1.46455541 -4.13917332 -1.67272082 24.63563075]
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.22167511351
gradient value of function right now is: [ 3.02803166e-08  5.38243020e-14  3.07751121e-08  2.46739613e-14
 -3.02849955e-08 -5.38326107e-14 -3.07798673e-08 -2.46777695e-14
  1.73777994e-11  9.06223782e-09  2.83081788e-11  7.90783315e-11
  1.60962518e-16  8.48009268e-14  2.62736570e-16  7.36305496e-16
  3.18417214e-12  1.66894730e-09  5.19210337e-12  1.45269297e-11
  5.66939771e-17  2.98397588e-14  9.25240035e-17  2.59218307e-16
 -9.57379228e-17 -1.70023208e-11 -1.01500497e-16 -4.64130245e-16
  1.53187370e-15  6.92976591e-10  1.64433099e-15  7.35519797e-15
 -1.78055289e-16 -3.89420793e-11 -1.89181224e-16 -8.61841664e-16
  1.89238946e-16  6.94032168e-11  2.02499496e-16  9.11042463e-16
 -3.54302152e-15 -7.97222756e-17 -5.32923918e-15 -1.29318011e-16
  1.79370787e-11  1.70146719e-12  2.50834971e-11  1.84483651e-12
 -3.69375264e-15 -1.00847795e-16 -5.53923187e-15 -1.54181551e-16
 -3.00589574e-14 -5.49294835e-16 -4.52363226e-14 -9.53195672e-16
  7.75583369e-12 -1.04250204e-10  9.50699206e-13 -6.53501089e-12
 -1.51527873e-12  1.34705207e-11  1.48789660e-12 -1.22074305e-11
  9.77550055e-01]
supnorm grad right now is: 0.9775500552563511
Weights right now are: 
[-7.60185469 -0.8672786  -5.75236173 -0.89734347  7.07038427  0.73328316
  5.91706861  0.33128735  0.98516853 -3.92128508  1.36079052 -0.23177086
  2.40626366 13.34088499  3.77494574 13.33891747  0.12499848 -5.40597072
  0.28472688 -3.63594179  2.95939795 14.12898513  3.6296035  13.76894537
 -0.94803544  6.34690977 -1.47385356 -3.65435212  2.61882122 -4.16627062
  2.778188    5.14395395 -3.98227386  5.844933   -2.46750197 -2.73451644
  3.70015909  4.79087884  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14677837 -3.05772319  4.40458281 -4.8941772
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442579 11.04235956
  0.40394028 14.90960042 -1.61197336  0.14051551 -6.90949416 -2.10695231
  3.26071578  1.46455506 -4.13917335 -1.67272051 25.03091239]
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.083892234761
gradient value of function right now is: [ 3.01580266e-08  5.36067827e-14  3.06507400e-08  2.45742969e-14
 -3.01499743e-08 -5.35924830e-14 -3.06425565e-08 -2.45677428e-14
  1.73327636e-11  9.02324379e-09  2.82247513e-11  7.88013387e-11
  1.60494317e-16  8.44413635e-14  2.61898318e-16  7.33635154e-16
  3.17575397e-12  1.66198043e-09  5.17671252e-12  1.44766210e-11
  5.65351619e-17  2.97156514e-14  9.22382517e-17  2.58302421e-16
 -9.90657279e-17 -1.69446417e-11 -1.05042877e-16 -4.79537338e-16
  1.58080132e-15  6.90191547e-10  1.69570971e-15  7.58633337e-15
 -1.84160816e-16 -3.88017212e-11 -1.95665070e-16 -8.90211514e-16
  1.95441566e-16  6.91348454e-11  2.09034152e-16  9.40212438e-16
 -3.65295491e-15 -8.07608319e-17 -5.50170170e-15 -1.32957238e-16
  1.81983013e-11  1.68291999e-12  2.54962352e-11  1.83024818e-12
 -3.80607057e-15 -1.01403464e-16 -5.71587833e-15 -1.57306806e-16
 -3.09677569e-14 -5.62065996e-16 -4.66553286e-14 -9.88088772e-16
  7.82931939e-12 -1.06156708e-10  9.44160273e-13 -6.50658899e-12
 -1.50993253e-12  1.35525918e-11  1.47968896e-12 -1.22328169e-11
  2.15794639e+00]
supnorm grad right now is: 2.157946392165104
Weights right now are: 
[-7.60242966 -0.8672786  -5.7529461  -0.89734347  7.07095924  0.73328316
  5.91765299  0.33128736  0.98516819 -3.92145724  1.36078997 -0.23177239
  2.40626366 13.34088499  3.77494574 13.33891747  0.12499842 -5.40600241
  0.28472677 -3.63594207  2.95939795 14.12898513  3.6296035  13.76894537
 -0.94803544  6.3469101  -1.47385356 -3.65435212  2.61882122 -4.16628387
  2.778188    5.14395395 -3.98227386  5.84493375 -2.46750197 -2.73451644
  3.70015909  4.79087751  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14677884 -3.05772323  4.40458214 -4.89417725
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442579 11.04235956
  0.40394028 14.90960042 -1.61197353  0.14051812 -6.90949417 -2.10695216
  3.26071581  1.46455472 -4.13917338 -1.67272021 25.11216623]
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.25030259706
gradient value of function right now is: [ 3.02072862e-08  5.36941275e-14  3.07006781e-08  2.46142855e-14
 -3.02070673e-08 -5.36937385e-14 -3.07004556e-08 -2.46141072e-14
  1.73245408e-11  9.03805966e-09  2.82247027e-11  7.88585615e-11
  1.60526203e-16  8.45896062e-14  2.62044990e-16  7.34449194e-16
  3.17564354e-12  1.66500359e-09  5.17870921e-12  1.44915289e-11
  5.65412272e-17  2.97663064e-14  9.22822529e-17  2.58571019e-16
 -9.39131722e-17 -1.69549406e-11 -9.96979010e-17 -4.54595647e-16
  1.50806816e-15  6.91217555e-10  1.62198319e-15  7.22268581e-15
 -1.74780590e-16 -3.88358563e-11 -1.85973032e-16 -8.44541371e-16
  1.86187737e-16  6.92350869e-11  1.99604130e-16  8.94269640e-16
 -3.48405069e-15 -8.18171695e-17 -5.23884893e-15 -1.31636241e-16
  1.78208318e-11  1.72822258e-12  2.49130811e-11  1.87720448e-12
 -3.63879050e-15 -1.03833475e-16 -5.45497172e-15 -1.57764140e-16
 -2.95019056e-14 -5.58195884e-16 -4.43843918e-14 -9.59914646e-16
  7.67298171e-12 -1.03058950e-10  9.54695091e-13 -6.59698590e-12
 -1.51776174e-12  1.35374503e-11  1.49314663e-12 -1.23062610e-11
 -6.22472184e-01]
supnorm grad right now is: 0.6224721841645077
Weights right now are: 
[-7.6030107  -0.8672786  -5.75353663 -0.89734347  7.07154029  0.73328316
  5.91824352  0.33128736  0.98516786 -3.92163109  1.36078943 -0.2317739
  2.40626366 13.34088499  3.77494574 13.33891747  0.12499836 -5.40603444
  0.28472668 -3.63594235  2.95939795 14.12898513  3.6296035  13.76894537
 -0.94803544  6.34691043 -1.47385356 -3.65435212  2.61882122 -4.16629714
  2.778188    5.14395395 -3.98227386  5.8449345  -2.46750197 -2.73451644
  3.70015909  4.79087618  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14677916 -3.05772326  4.40458168 -4.89417728
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442579 11.04235956
  0.40394028 14.90960042 -1.61197363  0.14052003 -6.90949419 -2.10695205
  3.26071583  1.46455451 -4.1391734  -1.67272002 24.88634259]
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.467421399007
gradient value of function right now is: [ 2.98907024e-08  5.31312496e-14  3.03788847e-08  2.43564222e-14
 -2.98764484e-08 -5.31059370e-14 -3.03643985e-08 -2.43448206e-14
  1.72499148e-11  8.94129057e-09  2.80640164e-11  7.82408898e-11
  1.59576490e-16  8.36829223e-14  2.60215133e-16  7.28123163e-16
  3.15988991e-12  1.64734308e-09  5.14662030e-12  1.43742106e-11
  5.62187507e-17  2.94500438e-14  9.16552401e-17  2.56382620e-16
 -1.08100437e-16 -1.68285638e-11 -1.14500575e-16 -5.22730719e-16
  1.70900097e-15  6.84375798e-10  1.82779065e-15  8.21339171e-15
 -2.00609584e-16 -3.85152253e-11 -2.12836992e-16 -9.69168794e-16
  2.11761779e-16  6.85767418e-11  2.25912537e-16  1.01963079e-15
 -3.94645434e-15 -8.09923115e-17 -5.96148735e-15 -1.38143004e-16
  1.88758207e-11  1.62735042e-12  2.65541061e-11  1.77702251e-12
 -4.10051918e-15 -1.00023298e-16 -6.17812920e-15 -1.60484974e-16
 -3.34702061e-14 -5.78365948e-16 -5.05589450e-14 -1.05017244e-15
  8.05170528e-12 -1.11465206e-10  9.32360016e-13 -6.41931483e-12
 -1.50047369e-12  1.36804211e-11  1.46326674e-12 -1.22227470e-11
  4.89868940e+00]
supnorm grad right now is: 4.898689402286299
Weights right now are: 
[-7.6036175  -0.8672786  -5.75415335 -0.89734348  7.07214712  0.73328316
  5.91886026  0.33128736  0.9851675  -3.92181269  1.36078886 -0.2317755
  2.40626366 13.34088498  3.77494574 13.33891747  0.1249983  -5.40606789
  0.28472657 -3.63594265  2.95939795 14.12898512  3.6296035  13.76894537
 -0.94803544  6.34691077 -1.47385356 -3.65435212  2.61882122 -4.16631106
  2.778188    5.14395395 -3.98227386  5.84493528 -2.46750197 -2.73451644
  3.70015909  4.79087479  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14677957 -3.05772331  4.4045811  -4.89417733
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442579 11.04235956
  0.40394028 14.90960042 -1.61197379  0.14052218 -6.90949421 -2.10695189
  3.26071586  1.46455416 -4.13917343 -1.6727197  25.26917783]
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16014.313363625502
gradient value of function right now is: [ 2.95485045e-08  5.25239123e-14  3.00311192e-08  2.40783533e-14
 -2.95290465e-08 -5.24893580e-14 -3.00113442e-08 -2.40625159e-14
  1.72452956e-11  8.83883336e-09  2.79912443e-11  7.77525669e-11
  1.59112884e-16  8.27157571e-14  2.58982747e-16  7.22601143e-16
  3.15544896e-12  1.62857947e-09  5.12862370e-12  1.42769674e-11
  5.60695247e-17  2.91110566e-14  9.12404607e-17  2.54475630e-16
 -1.31690655e-16 -1.67353173e-11 -1.39586468e-16 -6.32268976e-16
  2.05267399e-15  6.77603864e-10  2.18940966e-15  9.83448398e-15
 -2.43794468e-16 -3.82482311e-11 -2.58675070e-16 -1.17025973e-15
  2.55278214e-16  6.79449894e-11  2.71813965e-16  1.22418243e-15
 -4.71851994e-15 -8.94022833e-17 -7.17459803e-15 -1.63758499e-16
  2.08032813e-11  1.55089027e-12  2.95782655e-11  1.72815141e-12
 -4.89096982e-15 -1.05767385e-16 -7.42183396e-15 -1.83121316e-16
 -3.98340269e-14 -6.72881493e-16 -6.05289752e-14 -1.29245966e-15
  8.60487548e-12 -1.25210773e-10  9.15409351e-13 -6.40965241e-12
 -1.50958047e-12  1.45314566e-11  1.45467602e-12 -1.26990402e-11
  1.11542650e+01]
supnorm grad right now is: 11.154264987747776
Weights right now are: 
[-7.604231   -0.8672786  -5.75477686 -0.89734348  7.07276064  0.73328316
  5.91948381  0.33128736  0.98516715 -3.92199626  1.36078828 -0.2317771
  2.40626366 13.34088498  3.77494574 13.33891747  0.12499823 -5.40610171
  0.28472646 -3.63594294  2.95939795 14.12898512  3.6296035  13.76894537
 -0.94803544  6.34691112 -1.47385356 -3.65435212  2.61882122 -4.16632512
  2.778188    5.14395395 -3.98227386  5.84493607 -2.46750197 -2.73451644
  3.70015909  4.79087338  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14677994 -3.05772335  4.40458057 -4.89417737
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442579 11.04235956
  0.40394028 14.90960042 -1.61197396  0.1405242  -6.90949424 -2.10695172
  3.2607159   1.46455383 -4.13917347 -1.67271939 25.67875071]
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16013.555349952258
gradient value of function right now is: [ 2.94393683e-08  5.23296653e-14  2.99201010e-08  2.39893362e-14
 -2.94336635e-08 -5.23195345e-14 -2.99143033e-08 -2.39846929e-14
  1.72029771e-11  8.80744394e-09  2.79162285e-11  7.75165389e-11
  1.58694569e-16  8.24270391e-14  2.58254713e-16  7.20365533e-16
  3.14788838e-12  1.62307689e-09  5.11528501e-12  1.42352629e-11
  5.59188158e-17  2.90069371e-14  9.09781432e-17  2.53669953e-16
 -1.33591331e-16 -1.66857327e-11 -1.41611977e-16 -6.40995819e-16
  2.08104218e-15  6.75339269e-10  2.21907156e-15  9.96836528e-15
 -2.47293464e-16 -3.81297051e-11 -2.62391319e-16 -1.18640621e-15
  2.58883587e-16  6.77272130e-11  2.75600726e-16  1.24109687e-15
 -4.78234546e-15 -8.99143159e-17 -7.27442196e-15 -1.65810477e-16
  2.09464133e-11  1.53529304e-12  2.98077941e-11  1.71487993e-12
 -4.95610833e-15 -1.05854850e-16 -7.52395974e-15 -1.84677172e-16
 -4.03594451e-14 -6.80921058e-16 -6.13472257e-14 -1.31410799e-15
  8.65124425e-12 -1.26297074e-10  9.09809055e-13 -6.38314403e-12
 -1.50591583e-12  1.45735281e-11  1.44861209e-12 -1.27036656e-11
  1.20678261e+01]
supnorm grad right now is: 12.067826053108538
Weights right now are: 
[-7.60487132 -0.8672786  -5.75542763 -0.89734348  7.07340092  0.73328316
  5.92013454  0.33128736  0.98516679 -3.9221878   1.36078769 -0.23177877
  2.40626366 13.34088498  3.77494574 13.33891747  0.12499817 -5.40613701
  0.28472636 -3.63594325  2.95939795 14.12898512  3.6296035  13.76894537
 -0.94803544  6.34691147 -1.47385356 -3.65435212  2.61882122 -4.16633977
  2.778188    5.14395395 -3.98227386  5.84493689 -2.46750197 -2.73451644
  3.70015909  4.79087191  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14678031 -3.05772339  4.40458007 -4.89417742
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442579 11.04235956
  0.40394028 14.90960042 -1.61197413  0.1405261  -6.90949426 -2.10695154
  3.26071594  1.46455348 -4.13917351 -1.67271908 25.78338064]
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.822093044066
gradient value of function right now is: [ 3.00341033e-08  5.33852822e-14  3.05242132e-08  2.44726735e-14
 -3.00496851e-08 -5.34129523e-14 -3.05400488e-08 -2.44853556e-14
  1.71963089e-11  8.98498985e-09  2.80263772e-11  7.83492288e-11
  1.59477934e-16  8.41236637e-14  2.60404463e-16  7.30146688e-16
  3.15501756e-12  1.65634989e-09  5.14676555e-12  1.44092699e-11
  5.61647284e-17  2.95993778e-14  9.16932586e-17  2.57027314e-16
 -8.87834787e-17 -1.68443741e-11 -9.45476835e-17 -4.28277408e-16
  1.43889321e-15  6.87263201e-10  1.55484078e-15  6.85053836e-15
 -1.65527029e-16 -3.85906301e-11 -1.76736050e-16 -7.96658531e-16
  1.77369977e-16  6.88537756e-11  1.90987239e-16  8.47261781e-16
 -3.31616268e-15 -8.58236261e-17 -4.98106760e-15 -1.35318277e-16
  1.74276491e-11  1.77521874e-12  2.43339860e-11  1.93412683e-12
 -3.47854359e-15 -1.09529527e-16 -5.20876320e-15 -1.63863293e-16
 -2.79508630e-14 -5.74874495e-16 -4.20109194e-14 -9.65565241e-16
  7.45865675e-12 -9.95754288e-11  9.62104966e-13 -6.71527364e-12
 -1.52179067e-12  1.36173745e-11  1.50249610e-12 -1.24661211e-11
 -3.55010965e+00]
supnorm grad right now is: 3.5501096471371785
Weights right now are: 
[-7.60547358 -0.8672786  -5.75603973 -0.89734348  7.0740031   0.73328317
  5.92074654  0.33128736  0.98516644 -3.92236795  1.36078712 -0.23178035
  2.40626366 13.34088498  3.77494574 13.33891747  0.1249981  -5.40617021
  0.28472625 -3.63594354  2.95939795 14.12898512  3.6296035  13.76894537
 -0.94803544  6.34691181 -1.47385356 -3.65435212  2.61882122 -4.16635356
  2.778188    5.14395395 -3.98227386  5.84493767 -2.46750197 -2.73451644
  3.70015909  4.79087053  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14678069 -3.05772342  4.40457953 -4.89417745
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442579 11.04235956
  0.40394028 14.90960042 -1.61197428  0.14052835 -6.90949428 -2.10695143
  3.26071597  1.46455323 -4.13917354 -1.67271885 24.74652288]
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.489888810918
gradient value of function right now is: [ 3.00145101e-08  5.33505075e-14  3.05042015e-08  2.44567490e-14
 -2.99955646e-08 -5.33168642e-14 -3.04849476e-08 -2.44413292e-14
  1.71745112e-11  8.97341901e-09  2.79918803e-11  7.82557066e-11
  1.59311526e-16  8.40244766e-14  2.60135290e-16  7.29389645e-16
  3.15156380e-12  1.65441732e-09  5.14125734e-12  1.43941212e-11
  5.61172616e-17  2.95709074e-14  9.16169971e-17  2.56814213e-16
 -8.77535161e-17 -1.68246510e-11 -9.35856555e-17 -4.22561924e-16
  1.42853883e-15  6.86470782e-10  1.54668246e-15  6.78260475e-15
 -1.63755107e-16 -3.85456070e-11 -1.75116899e-16 -7.86575817e-16
  1.75965773e-16  6.87790147e-11  1.89831538e-16  8.38389953e-16
 -3.28849547e-15 -8.84379320e-17 -4.93719641e-15 -1.38629800e-16
  1.74285668e-11  1.80012103e-12  2.43385796e-11  1.96624264e-12
 -3.45598818e-15 -1.12905433e-16 -5.17256507e-15 -1.68282456e-16
 -2.76564668e-14 -5.89480958e-16 -4.15509056e-14 -9.82507913e-16
  7.43081904e-12 -9.91853394e-11  9.67111790e-13 -6.79218241e-12
 -1.53104818e-12  1.37625935e-11  1.51285161e-12 -1.26220600e-11
 -4.69264593e+00]
supnorm grad right now is: 4.69264592733353
Weights right now are: 
[-7.60603953 -0.8672786  -5.7566149  -0.89734348  7.07456905  0.73328317
  5.92132173  0.33128736  0.98516611 -3.92253721  1.36078659 -0.23178183
  2.40626366 13.34088498  3.77494574 13.33891747  0.12499804 -5.40620141
  0.28472615 -3.63594381  2.95939795 14.12898512  3.6296035  13.76894537
 -0.94803544  6.34691213 -1.47385356 -3.65435212  2.61882122 -4.16636649
  2.778188    5.14395395 -3.98227386  5.8449384  -2.46750197 -2.73451644
  3.70015909  4.79086923  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14678102 -3.05772345  4.40457906 -4.89417748
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442579 11.04235956
  0.40394029 14.90960042 -1.6119744   0.14053028 -6.90949429 -2.10695134
  3.26071599  1.46455301 -4.13917356 -1.67271866 24.60987221]
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16016.513998869226
gradient value of function right now is: [ 3.00371599e-08  5.33902177e-14  3.05270817e-08  2.44748770e-14
 -3.00520102e-08 -5.34165889e-14 -3.05421737e-08 -2.44869639e-14
  1.71522188e-11  8.98387845e-09  2.79701110e-11  7.82585545e-11
  1.59213268e-16  8.41311677e-14  2.60080206e-16  7.29695033e-16
  3.14900361e-12  1.65669971e-09  5.13947789e-12  1.43996139e-11
  5.60687717e-17  2.96020909e-14  9.15755768e-17  2.56862803e-16
 -8.24996430e-17 -1.68227296e-11 -8.82368674e-17 -3.96003966e-16
  1.35303340e-15  6.87099789e-10  1.47139467e-15  6.38821502e-15
 -1.54171586e-16 -3.85531674e-11 -1.65396939e-16 -7.37830327e-16
  1.66420401e-16  6.88384683e-11  1.80273278e-16  7.88842654e-16
 -3.10748607e-15 -9.07020317e-17 -4.66186693e-15 -1.39716275e-16
  1.69373658e-11  1.84161101e-12  2.36047419e-11  2.00768437e-12
 -3.27913896e-15 -1.16448278e-16 -4.90354840e-15 -1.71157979e-16
 -2.60182206e-14 -5.95412227e-16 -3.90658605e-14 -9.72112406e-16
  7.20502508e-12 -9.51838623e-11  9.85854866e-13 -6.91506992e-12
 -1.54108235e-12  1.37598999e-11  1.53027485e-12 -1.27159042e-11
 -6.79983947e+00]
supnorm grad right now is: 6.799839467483622
Weights right now are: 
[-7.60666666 -0.8672786  -5.75725227 -0.89734348  7.07519634  0.73328317
  5.92195924  0.33128736  0.98516576 -3.92272471  1.36078601 -0.23178345
  2.40626366 13.34088498  3.77494574 13.33891747  0.12499798 -5.40623598
  0.28472605 -3.63594411  2.95939795 14.12898512  3.6296035  13.76894537
 -0.94803544  6.34691248 -1.47385356 -3.65435212  2.61882122 -4.16638076
  2.778188    5.14395395 -3.98227386  5.84493919 -2.46750197 -2.73451644
  3.70015909  4.7908678   2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14678128 -3.05772348  4.40457868 -4.89417751
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442579 11.04235956
  0.40394029 14.90960042 -1.61197451  0.14053194 -6.90949431 -2.10695124
  3.26071601  1.46455282 -4.13917358 -1.67271849 24.44836299]
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.247266197013
gradient value of function right now is: [ 2.96921122e-08  5.27758949e-14  3.01763353e-08  2.41933187e-14
 -2.97092746e-08 -5.28063719e-14 -3.01937769e-08 -2.42072874e-14
  1.70203335e-11  8.88061338e-09  2.77297646e-11  7.74802241e-11
  1.57804970e-16  8.31654778e-14  2.57607866e-16  7.22049680e-16
  3.12348818e-12  1.63787640e-09  5.09377088e-12  1.42546368e-11
  5.55767968e-17  2.92619012e-14  9.07099022e-17  2.54177973e-16
 -9.20099121e-17 -1.66586337e-11 -9.76736948e-17 -4.45452246e-16
  1.47837296e-15  6.79436119e-10  1.59006454e-15  7.08112878e-15
 -1.71245745e-16 -3.81582879e-11 -1.82206610e-16 -8.27580976e-16
  1.82578475e-16  6.80862009e-11  1.95735243e-16  8.77028714e-16
 -3.41574732e-15 -8.02529771e-17 -5.13559636e-15 -1.28993318e-16
  1.74882210e-11  1.69856358e-12  2.44449451e-11  1.84448045e-12
 -3.56751441e-15 -1.01896725e-16 -5.34748897e-15 -1.54668959e-16
 -2.89268633e-14 -5.47236082e-16 -4.35153958e-14 -9.40281469e-16
  7.53298034e-12 -1.01119302e-10  9.38233865e-13 -6.48120226e-12
 -1.49145800e-12  1.32914833e-11  1.46744372e-12 -1.20856000e-11
 -7.00154234e-01]
supnorm grad right now is: 0.70015423407904
Weights right now are: 
[-7.60726639 -0.86727861 -5.75786178 -0.89734348  7.07579612  0.73328317
  5.92256882  0.33128736  0.98516541 -3.92290411  1.36078544 -0.23178502
  2.40626366 13.34088497  3.77494574 13.33891747  0.12499791 -5.40626906
  0.28472594 -3.6359444   2.95939795 14.12898512  3.6296035  13.76894537
 -0.94803544  6.34691282 -1.47385356 -3.65435212  2.61882122 -4.16639453
  2.778188    5.14395395 -3.98227386  5.84493997 -2.46750197 -2.73451644
  3.70015909  4.79086642  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.1467817  -3.05772352  4.40457808 -4.89417756
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442579 11.04235956
  0.40394029 14.90960042 -1.61197465  0.14053426 -6.90949433 -2.10695107
  3.26071605  1.46455248 -4.13917362 -1.67271818 24.84240944]
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.879372586654
gradient value of function right now is: [ 2.94395618e-08  5.23269096e-14  2.99196157e-08  2.39876248e-14
 -2.94268630e-08 -5.23043588e-14 -2.99067103e-08 -2.39772889e-14
  1.69484519e-11  8.80061680e-09  2.75842321e-11  7.69505095e-11
  1.56959387e-16  8.24184092e-14  2.56022911e-16  7.16726685e-16
  3.10887359e-12  1.62327333e-09  5.06527006e-12  1.41547187e-11
  5.52942036e-17  2.90047007e-14  9.01751149e-17  2.52362793e-16
 -1.01995572e-16 -1.65495705e-11 -1.08049164e-16 -4.93878535e-16
  1.62082623e-15  6.73746930e-10  1.73530159e-15  7.79086878e-15
 -1.89446870e-16 -3.78856190e-11 -2.01056798e-16 -9.16275748e-16
  2.00690162e-16  6.75366516e-11  2.14283644e-16  9.66727834e-16
 -3.74609922e-15 -7.90766658e-17 -5.64883932e-15 -1.32479567e-16
  1.82698914e-11  1.62086619e-12  2.56499914e-11  1.76641984e-12
 -3.89616893e-15 -9.84800786e-17 -5.85909658e-15 -1.55290221e-16
 -3.17909327e-14 -5.57833117e-16 -4.79462086e-14 -9.96930127e-16
  7.83487506e-12 -1.07400870e-10  9.20708326e-13 -6.34049224e-12
 -1.47852499e-12  1.33732992e-11  1.44460865e-12 -1.20018135e-11
  3.46300186e+00]
supnorm grad right now is: 3.4630018569653074
Weights right now are: 
[-7.60785343 -0.86727861 -5.75845839 -0.89734348  7.07638317  0.73328317
  5.92316543  0.33128736  0.98516507 -3.92307964  1.3607849  -0.23178655
  2.40626366 13.34088497  3.77494574 13.33891747  0.12499785 -5.40630143
  0.28472584 -3.63594468  2.95939795 14.12898512  3.6296035  13.76894537
 -0.94803544  6.34691315 -1.47385356 -3.65435212  2.61882122 -4.16640795
  2.778188    5.14395395 -3.98227386  5.84494072 -2.46750197 -2.73451644
  3.70015909  4.79086508  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14678204 -3.05772355  4.40457761 -4.89417759
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442579 11.04235956
  0.40394029 14.90960042 -1.6119748   0.14053619 -6.90949435 -2.10695094
  3.26071608  1.46455222 -4.13917365 -1.67271793 25.18604893]
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.544437171548
gradient value of function right now is: [ 2.93210140e-08  5.21157109e-14  2.97990399e-08  2.38908259e-14
 -2.93465603e-08 -5.21610753e-14 -2.98250017e-08 -2.39116180e-14
  1.69088468e-11  8.76998398e-09  2.75124447e-11  7.67194283e-11
  1.56546815e-16  8.21344553e-14  2.55298667e-16  7.14487172e-16
  3.10169906e-12  1.61792464e-09  5.05239301e-12  1.41137745e-11
  5.51376588e-17  2.88976961e-14  8.99010539e-17  2.51517974e-16
 -1.04713834e-16 -1.65009100e-11 -1.10912136e-16 -5.06577179e-16
  1.65837030e-15  6.71508208e-10  1.77398525e-15  7.97126302e-15
 -1.94375307e-16 -3.77687119e-11 -2.06228616e-16 -9.39411957e-16
  2.05516266e-16  6.73212620e-11  2.19283213e-16  9.89768576e-16
 -3.83055552e-15 -7.90403746e-17 -5.78362400e-15 -1.34270095e-16
  1.83969474e-11  1.59623792e-12  2.58706956e-11  1.74219832e-12
 -3.98084280e-15 -9.78069884e-17 -5.99476758e-15 -1.56312718e-16
 -3.24960769e-14 -5.62839866e-16 -4.90662461e-14 -1.01844132e-15
  7.85424914e-12 -1.08558400e-10  9.12237713e-13 -6.28044675e-12
 -1.46736052e-12  1.33576066e-11  1.43162328e-12 -1.19453661e-11
  4.61917449e+00]
supnorm grad right now is: 4.61917449491572
Weights right now are: 
[-7.60846131 -0.86727861 -5.75907619 -0.89734348  7.07699103  0.73328317
  5.92378321  0.33128736  0.98516471 -3.92326144  1.36078432 -0.23178816
  2.40626366 13.34088497  3.77494574 13.33891747  0.12499779 -5.40633496
  0.28472574 -3.63594497  2.95939795 14.12898512  3.6296035  13.76894537
 -0.94803544  6.34691349 -1.47385356 -3.65435212  2.61882122 -4.16642191
  2.778188    5.14395395 -3.98227386  5.84494151 -2.46750197 -2.73451644
  3.70015909  4.79086368  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14678248 -3.05772359  4.40457698 -4.89417764
 11.69188537  8.75893656  1.48904899 12.52125257 10.8544258  11.04235956
  0.40394029 14.90960042 -1.61197496  0.14053873 -6.90949437 -2.10695079
  3.26071611  1.46455189 -4.13917368 -1.67271764 25.25528489]
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.138550837519
gradient value of function right now is: [ 2.93685375e-08  5.21996936e-14  2.98471887e-08  2.39292172e-14
 -2.93515669e-08 -5.21695583e-14 -2.98299421e-08 -2.39154052e-14
  1.68570348e-11  8.77676478e-09  2.74509413e-11  7.66466952e-11
  1.56245732e-16  8.22134226e-14  2.54971072e-16  7.14273758e-16
  3.09408893e-12  1.61942971e-09  5.04373219e-12  1.41057181e-11
  5.50412498e-17  2.89332609e-14  8.98030460e-17  2.51498793e-16
 -9.58845794e-17 -1.64815528e-11 -1.01681411e-16 -4.64159407e-16
  1.53230745e-15  6.71761003e-10  1.64415983e-15  7.35222100e-15
 -1.78278188e-16 -3.77427061e-11 -1.89442936e-16 -8.61776105e-16
  1.89515994e-16  6.73364728e-11  2.02747438e-16  9.11579046e-16
 -3.54092793e-15 -7.88232181e-17 -5.33164661e-15 -1.29390241e-16
  1.76919953e-11  1.64410068e-12  2.47817232e-11  1.78839624e-12
 -3.69036666e-15 -9.90890040e-17 -5.54060268e-15 -1.53308857e-16
 -3.00155912e-14 -5.47462018e-16 -4.52111434e-14 -9.59770329e-16
  7.61268817e-12 -1.03105753e-10  9.20023700e-13 -6.34716520e-12
 -1.47169547e-12  1.32060381e-11  1.44252591e-12 -1.19282013e-11
  1.76362421e+00]
supnorm grad right now is: 1.7636242093346834
Weights right now are: 
[-7.60908041 -0.86727861 -5.75970538 -0.89734348  7.07761011  0.73328317
  5.92441238  0.33128736  0.98516437 -3.92344647  1.36078375 -0.23178976
  2.40626366 13.34088497  3.77494574 13.33891747  0.12499772 -5.4063691
  0.28472563 -3.63594527  2.95939795 14.12898512  3.6296035  13.76894537
 -0.94803544  6.34691384 -1.47385356 -3.65435212  2.61882122 -4.16643603
  2.778188    5.14395395 -3.98227386  5.8449423  -2.46750197 -2.73451644
  3.70015909  4.79086226  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14678278 -3.05772362  4.40457658 -4.89417767
 11.69188537  8.75893656  1.48904899 12.52125257 10.8544258  11.04235956
  0.40394029 14.90960042 -1.61197513  0.14054042 -6.90949439 -2.10695067
  3.26071614  1.46455167 -4.13917371 -1.67271744 25.11421025]
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.934658828757
gradient value of function right now is: [ 2.95020925e-08  5.24370942e-14  2.99827792e-08  2.40379753e-14
 -2.94812665e-08 -5.24001122e-14 -2.99616146e-08 -2.40210251e-14
  1.68720904e-11  8.81492658e-09  2.74972245e-11  7.68684769e-11
  1.56562606e-16  8.25833474e-14  2.55639626e-16  7.16783294e-16
  3.09885240e-12  1.62677280e-09  5.05502753e-12  1.41522515e-11
  5.51499643e-17  2.90640760e-14  9.00351493e-17  2.52379088e-16
 -8.74092523e-17 -1.65262333e-11 -9.30325979e-17 -4.22010514e-16
  1.41596688e-15  6.74515218e-10  1.52898061e-15  6.74914463e-15
 -1.62937674e-16 -3.78618002e-11 -1.73868998e-16 -7.84918049e-16
  1.74632495e-16  6.76063442e-11  1.87911435e-16  8.35102390e-16
 -3.26525796e-15 -8.33285148e-17 -4.90410384e-15 -1.31588471e-16
  1.71211813e-11  1.73300779e-12  2.39048693e-11  1.88708912e-12
 -3.42288464e-15 -1.06344902e-16 -5.12490033e-15 -1.59243159e-16
 -2.75494305e-14 -5.59143213e-16 -4.14031483e-14 -9.41243318e-16
  7.34202912e-12 -9.79889978e-11  9.41306592e-13 -6.56075227e-12
 -1.49020644e-12  1.33204007e-11  1.47046096e-12 -1.21837993e-11
 -3.09135304e+00]
supnorm grad right now is: 3.091353036782235
Weights right now are: 
[-7.6096975  -0.86727861 -5.76033253 -0.89734348  7.0782272   0.73328317
  5.92503953  0.33128736  0.98516401 -3.92363098  1.36078317 -0.23179138
  2.40626366 13.34088497  3.77494574 13.33891747  0.12499766 -5.40640314
  0.28472553 -3.63594557  2.95939795 14.12898512  3.6296035  13.76894537
 -0.94803544  6.34691418 -1.47385356 -3.65435212  2.61882122 -4.16645019
  2.778188    5.14395395 -3.98227386  5.8449431  -2.46750197 -2.73451644
  3.70015909  4.79086084  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14678321 -3.05772366  4.40457598 -4.89417771
 11.69188537  8.75893656  1.48904899 12.52125257 10.8544258  11.04235956
  0.40394029 14.90960042 -1.61197532  0.14054296 -6.90949441 -2.10695053
  3.26071617  1.46455138 -4.13917374 -1.67271718 24.75997542]
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.263692768132
gradient value of function right now is: [ 2.93133159e-08  5.21007193e-14  2.97908329e-08  2.38837687e-14
 -2.92928451e-08 -5.20643682e-14 -2.97700294e-08 -2.38671076e-14
  1.67794194e-11  8.75784750e-09  2.73388318e-11  7.63962309e-11
  1.55649740e-16  8.20538048e-14  2.54102391e-16  7.12293265e-16
  3.08171361e-12  1.61647579e-09  5.02591413e-12  1.40661825e-11
  5.48297271e-17  2.88776572e-14  8.94953842e-17  2.50800530e-16
 -9.02088649e-17 -1.64252709e-11 -9.56803224e-17 -4.37458367e-16
  1.44928942e-15  6.70185440e-10  1.55742801e-15  6.95438245e-15
 -1.67876551e-16 -3.76255715e-11 -1.78468156e-16 -8.12676894e-16
  1.79030821e-16  6.71782412e-11  1.91763707e-16  8.61533764e-16
 -3.35243827e-15 -7.74068199e-17 -5.03761171e-15 -1.24187650e-16
  1.71962766e-11  1.66525709e-12  2.40188141e-11  1.80451490e-12
 -3.49865794e-15 -9.84238869e-17 -5.24108198e-15 -1.48979756e-16
 -2.84366373e-14 -5.28597279e-16 -4.27571675e-14 -9.07675688e-16
  7.44562571e-12 -9.95505183e-11  9.23894775e-13 -6.35832778e-12
 -1.46928539e-12  1.30203391e-11  1.44497914e-12 -1.18363593e-11
 -2.33707374e-01]
supnorm grad right now is: 0.23370737389169796
Weights right now are: 
[-7.61027216 -0.86727861 -5.76091655 -0.89734348  7.07880185  0.73328317
  5.92562354  0.33128736  0.98516369 -3.9238027   1.36078264 -0.23179287
  2.40626366 13.34088497  3.77494574 13.33891747  0.1249976  -5.40643484
  0.28472543 -3.63594584  2.95939795 14.12898512  3.6296035  13.76894537
 -0.94803544  6.3469145  -1.47385356 -3.65435212  2.61882122 -4.16646331
  2.778188    5.14395395 -3.98227386  5.84494383 -2.46750197 -2.73451644
  3.70015909  4.79085953  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.1467835  -3.05772369  4.4045756  -4.89417774
 11.69188537  8.75893656  1.48904899 12.52125257 10.8544258  11.04235956
  0.40394029 14.90960042 -1.61197546  0.14054437 -6.90949443 -2.1069504
  3.2607162   1.46455114 -4.13917377 -1.67271695 24.92227885]
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.945504548825
gradient value of function right now is: [ 2.93596196e-08  5.21832424e-14  2.98377864e-08  2.39216062e-14
 -2.93522026e-08 -5.21700710e-14 -2.98302489e-08 -2.39155692e-14
  1.67950612e-11  8.77275605e-09  2.73704544e-11  7.65089786e-11
  1.55864765e-16  8.22005671e-14  2.54490634e-16  7.13522868e-16
  3.08561219e-12  1.61950196e-09  5.03322275e-12  1.40903573e-11
  5.48999765e-17  2.89268896e-14  8.96235290e-17  2.51210936e-16
 -8.73876026e-17 -1.64492585e-11 -9.30106313e-17 -4.21846974e-16
  1.41574277e-15  6.71384060e-10  1.52862227e-15  6.74772245e-15
 -1.62899858e-16 -3.76845358e-11 -1.73827987e-16 -7.84637629e-16
  1.74629414e-16  6.73021719e-11  1.87898031e-16  8.35024279e-16
 -3.26452688e-15 -8.31789354e-17 -4.90332259e-15 -1.31530893e-16
  1.70841820e-11  1.72333539e-12  2.38602849e-11  1.87772661e-12
 -3.42194198e-15 -1.06050434e-16 -5.12389794e-15 -1.59024657e-16
 -2.75408177e-14 -5.58926380e-16 -4.13926532e-14 -9.41966174e-16
  7.32096176e-12 -9.78260429e-11  9.36158962e-13 -6.53009862e-12
 -1.48320066e-12  1.32798711e-11  1.46301019e-12 -1.21398521e-11
 -2.97585597e+00]
supnorm grad right now is: 2.975855965266121
Weights right now are: 
[-7.61085175 -0.86727861 -5.76150557 -0.89734348  7.0793814   0.73328318
  5.92621252  0.33128736  0.98516337 -3.92397579  1.36078211 -0.23179434
  2.40626366 13.34088496  3.77494574 13.33891747  0.12499754 -5.4064668
  0.28472533 -3.63594611  2.95939795 14.12898512  3.6296035  13.76894537
 -0.94803544  6.34691482 -1.47385356 -3.65435212  2.61882122 -4.16647645
  2.778188    5.14395395 -3.98227386  5.84494456 -2.46750197 -2.73451644
  3.70015909  4.79085822  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14678369 -3.05772371  4.40457536 -4.89417776
 11.69188537  8.75893656  1.48904899 12.52125257 10.8544258  11.04235956
  0.40394029 14.90960042 -1.61197558  0.14054544 -6.90949445 -2.10695032
  3.26071622  1.46455102 -4.13917379 -1.67271684 24.73794821]
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.199013831818
gradient value of function right now is: [ 2.91081903e-08  5.17354569e-14  2.95821838e-08  2.37163550e-14
 -2.90895185e-08 -5.17023016e-14 -2.95632087e-08 -2.37011588e-14
  1.66851446e-11  8.69546002e-09  2.71761145e-11  7.59027519e-11
  1.54735388e-16  8.14802864e-14  2.52544712e-16  7.07653800e-16
  3.06472786e-12  1.60543113e-09  4.99672105e-12  1.39781660e-11
  5.45087917e-17  2.86755375e-14  8.89480973e-17  2.49168207e-16
 -9.31667512e-17 -1.63206512e-11 -9.87751969e-17 -4.51454699e-16
  1.49053784e-15  6.65591565e-10  1.59951184e-15  7.15605741e-15
 -1.73247732e-16 -3.73788597e-11 -1.84064394e-16 -8.38238434e-16
  1.84331746e-16  6.67310391e-11  1.97206257e-16  8.87258285e-16
 -3.44645421e-15 -7.70075979e-17 -5.18622792e-15 -1.25548953e-16
  1.73511205e-11  1.63171922e-12  2.42777692e-11  1.77059590e-12
 -3.59222035e-15 -9.71696677e-17 -5.38954663e-15 -1.49283844e-16
 -2.92345793e-14 -5.32524819e-16 -4.40119413e-14 -9.28305578e-16
  7.49174872e-12 -1.00969174e-10  9.13781561e-13 -6.28359847e-12
 -1.45779913e-12  1.29938226e-11  1.43056023e-12 -1.17600426e-11
  1.36957402e+00]
supnorm grad right now is: 1.369574015508799
Weights right now are: 
[-7.61141931 -0.86727861 -5.76208238 -0.89734348  7.07994899  0.73328318
  5.92678936  0.33128736  0.98516304 -3.92414547  1.36078158 -0.23179584
  2.40626366 13.34088496  3.77494574 13.33891747  0.12499748 -5.40649812
  0.28472523 -3.63594639  2.95939795 14.12898512  3.6296035  13.76894537
 -0.94803544  6.34691514 -1.47385356 -3.65435212  2.61882122 -4.1664895
  2.778188    5.14395395 -3.98227386  5.8449453  -2.46750197 -2.73451644
  3.70015909  4.79085691  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.1467841  -3.05772375  4.40457478 -4.89417781
 11.69188537  8.75893656  1.48904899 12.52125257 10.8544258  11.04235956
  0.40394029 14.90960042 -1.61197576  0.14054773 -6.90949447 -2.10695017
  3.26071626  1.46455069 -4.13917383 -1.67271654 25.07916029]
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.996014938159
gradient value of function right now is: [ 2.92153664e-08  5.19261978e-14  2.96910018e-08  2.38037793e-14
 -2.92333432e-08 -5.19581195e-14 -2.97092706e-08 -2.38184101e-14
  1.67268346e-11  8.73191995e-09  2.72555445e-11  7.61723274e-11
  1.55224945e-16  8.18280440e-14  2.53419855e-16  7.10414640e-16
  3.07376880e-12  1.61245578e-09  5.01330023e-12  1.40321022e-11
  5.46669306e-17  2.87910928e-14  8.92335161e-17  2.50078697e-16
 -8.85149236e-17 -1.63774608e-11 -9.42037828e-17 -4.27020586e-16
  1.43085735e-15  6.68367493e-10  1.54422491e-15  6.81902395e-15
 -1.64931625e-16 -3.75173306e-11 -1.75969804e-16 -7.94004413e-16
  1.76601727e-16  6.70102349e-11  1.89947913e-16  8.44268482e-16
 -3.29786216e-15 -8.31481442e-17 -4.95770991e-15 -1.32371172e-16
  1.71030786e-11  1.70944090e-12  2.39093342e-11  1.86396819e-12
 -3.45535288e-15 -1.05718604e-16 -5.17873883e-15 -1.59517775e-16
 -2.78133973e-14 -5.60899708e-16 -4.18354492e-14 -9.51556181e-16
  7.30488178e-12 -9.81157753e-11  9.30410252e-13 -6.49044273e-12
 -1.47450309e-12  1.32480178e-11  1.45382638e-12 -1.20924809e-11
 -2.62996701e+00]
supnorm grad right now is: 2.629967011380338
Weights right now are: 
[-7.61193765 -0.86727861 -5.76260916 -0.89734348  7.08046733  0.73328318
  5.92731615  0.33128736  0.98516273 -3.92430044  1.36078108 -0.23179721
  2.40626366 13.34088496  3.77494574 13.33891747  0.12499742 -5.40652672
  0.28472514 -3.63594664  2.95939795 14.12898512  3.6296035  13.76894537
 -0.94803544  6.34691544 -1.47385356 -3.65435212  2.61882122 -4.16650143
  2.778188    5.14395395 -3.98227386  5.84494597 -2.46750197 -2.73451644
  3.70015909  4.79085571  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14678452 -3.05772378  4.4045742  -4.89417784
 11.69188537  8.75893656  1.48904899 12.52125257 10.8544258  11.04235956
  0.40394029 14.90960042 -1.61197595  0.1405502  -6.90949448 -2.10695004
  3.26071629  1.4645504  -4.13917385 -1.67271628 24.75092605]
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.49982107298
gradient value of function right now is: [ 2.92329330e-08  5.19575241e-14  2.97087479e-08  2.38181364e-14
 -2.92397872e-08 -5.19696952e-14 -2.97157134e-08 -2.38237149e-14
  1.67170794e-11  8.73473298e-09  2.72462540e-11  7.61732926e-11
  1.55206260e-16  8.18632480e-14  2.53431884e-16  7.10616768e-16
  3.07293899e-12  1.61321681e-09  5.01298902e-12  1.40354360e-11
  5.46627232e-17  2.88055454e-14  8.92421601e-17  2.50164535e-16
 -8.54692017e-17 -1.63771653e-11 -9.11459307e-17 -4.11559431e-16
  1.39167174e-15  6.68612630e-10  1.50666440e-15  6.60787968e-15
 -1.59482661e-16 -3.75212584e-11 -1.70540159e-16 -7.66057751e-16
  1.71530647e-16  6.70369980e-11  1.85035728e-16  8.17288670e-16
 -3.20351128e-15 -8.60282013e-17 -4.81009397e-15 -1.34924712e-16
  1.69625643e-11  1.75098679e-12  2.36888782e-11  1.91252792e-12
 -3.36645313e-15 -1.09826262e-16 -5.03908355e-15 -1.63763568e-16
 -2.69429566e-14 -5.73463977e-16 -4.04826955e-14 -9.56421436e-16
  7.22953519e-12 -9.65468736e-11  9.40956846e-13 -6.60757779e-12
 -1.48937198e-12  1.33894585e-11  1.47171238e-12 -1.22790338e-11
 -4.65458469e+00]
supnorm grad right now is: 4.654584686842398
Weights right now are: 
[-7.61251314 -0.86727861 -5.76319403 -0.89734348  7.08104281  0.73328318
  5.927901    0.33128736  0.9851624  -3.92447243  1.36078054 -0.23179872
  2.40626366 13.34088496  3.77494574 13.33891747  0.12499736 -5.40655848
  0.28472504 -3.63594692  2.95939795 14.12898512  3.6296035  13.76894537
 -0.94803544  6.34691576 -1.47385356 -3.65435212  2.61882122 -4.16651462
  2.778188    5.14395395 -3.98227386  5.84494672 -2.46750197 -2.73451644
  3.70015909  4.79085438  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14678491 -3.05772381  4.40457364 -4.89417788
 11.69188537  8.75893656  1.48904899 12.52125257 10.8544258  11.04235956
  0.4039403  14.90960042 -1.6119761   0.14055244 -6.9094945  -2.10694992
  3.26071632  1.4645501  -4.13917388 -1.67271602 24.60298259]
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.26454280983
gradient value of function right now is: [ 2.89623199e-08  5.14751296e-14  2.94336282e-08  2.35969643e-14
 -2.89472063e-08 -5.14482922e-14 -2.94182693e-08 -2.35846638e-14
  1.65734075e-11  8.64993504e-09  2.70025211e-11  7.54550335e-11
  1.53793329e-16  8.10752261e-14  2.51068074e-16  7.03785035e-16
  3.04612880e-12  1.59777384e-09  4.96778227e-12  1.39033298e-11
  5.41741787e-17  2.85322812e-14  8.84239906e-17  2.47796811e-16
 -8.94437744e-17 -1.62230020e-11 -9.48419827e-17 -4.33866032e-16
  1.43598371e-15  6.62092480e-10  1.54242020e-15  6.89417411e-15
 -1.66423572e-16 -3.71622455e-11 -1.76867098e-16 -8.05902816e-16
  1.77467573e-16  6.63873591e-11  1.90008718e-16  8.54417599e-16
 -3.32223578e-15 -7.59324522e-17 -4.99307479e-15 -1.22123290e-16
  1.69895078e-11  1.63750780e-12  2.37327660e-11  1.77331332e-12
 -3.46566977e-15 -9.64722065e-17 -5.19260986e-15 -1.46310575e-16
 -2.81918074e-14 -5.19638426e-16 -4.23955670e-14 -8.94718462e-16
  7.35965309e-12 -9.84721384e-11  9.11150796e-13 -6.25993262e-12
 -1.44908299e-12  1.28294214e-11  1.42479101e-12 -1.16551162e-11
  7.79464799e-02]
supnorm grad right now is: 0.07794647990327533
Weights right now are: 
[-7.61312547 -0.86727862 -5.76381632 -0.89734348  7.08165512  0.73328318
  5.92852328  0.33128736  0.98516204 -3.9246554   1.36077997 -0.23180032
  2.40626366 13.34088496  3.77494574 13.33891747  0.1249973  -5.40659227
  0.28472494 -3.63594721  2.95939795 14.12898512  3.6296035  13.76894537
 -0.94803544  6.34691611 -1.47385356 -3.65435212  2.61882122 -4.16652864
  2.778188    5.14395395 -3.98227386  5.84494751 -2.46750197 -2.73451644
  3.70015909  4.79085298  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.1467853  -3.05772385  4.40457309 -4.89417792
 11.69188537  8.75893656  1.48904899 12.52125257 10.8544258  11.04235956
  0.4039403  14.90960042 -1.61197625  0.14055463 -6.90949452 -2.10694978
  3.26071635  1.46454979 -4.13917391 -1.67271574 24.90665839]
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.144990231296
gradient value of function right now is: [ 2.88250131e-08  5.12310419e-14  2.92940152e-08  2.34851396e-14
 -2.88177594e-08 -5.12181617e-14 -2.92866437e-08 -2.34792362e-14
  1.65350362e-11  8.60959859e-09  2.69268285e-11  7.51859155e-11
  1.53362631e-16  8.07000136e-14  2.50267934e-16  7.01120130e-16
  3.03874183e-12  1.59054663e-09  4.95355203e-12  1.38539806e-11
  5.40223672e-17  2.83988645e-14  8.81412363e-17  2.46852266e-16
 -9.39121124e-17 -1.61675999e-11 -9.95935539e-17 -4.54609936e-16
  1.50183816e-15  6.59251601e-10  1.61159642e-15  7.20553474e-15
 -1.74622287e-16 -3.70245165e-11 -1.85566845e-16 -8.44090940e-16
  1.85811054e-16  6.61155565e-11  1.98798519e-16  8.93708100e-16
 -3.47046790e-15 -7.74068358e-17 -5.22520820e-15 -1.26970759e-16
  1.73522257e-11  1.61450583e-12  2.43042592e-11  1.75611330e-12
 -3.61723460e-15 -9.73302318e-17 -5.43041653e-15 -1.50488731e-16
 -2.94174088e-14 -5.37369874e-16 -4.43076749e-14 -9.41396551e-16
  7.46639491e-12 -1.01097594e-10  9.03511165e-13 -6.23300923e-12
 -1.44485090e-12  1.29618487e-11  1.41639908e-12 -1.17101930e-11
  1.68499018e+00]
supnorm grad right now is: 1.684990175303908
Weights right now are: 
[-7.61370047 -0.86727862 -5.76440069 -0.89734348  7.08223011  0.73328318
  5.92910763  0.33128736  0.98516171 -3.92482724  1.36077942 -0.23180184
  2.40626366 13.34088496  3.77494574 13.33891747  0.12499723 -5.40662401
  0.28472484 -3.63594749  2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.34691644 -1.47385356 -3.65435212  2.61882122 -4.16654185
  2.778188    5.14395395 -3.98227386  5.84494825 -2.46750197 -2.73451644
  3.70015909  4.79085165  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14678573 -3.05772389  4.40457247 -4.89417797
 11.69188537  8.75893656  1.48904899 12.52125257 10.8544258  11.04235956
  0.4039403  14.90960042 -1.61197641  0.14055707 -6.90949453 -2.10694964
  3.26071638  1.46454945 -4.13917394 -1.67271544 25.08934677]
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.03048706464
gradient value of function right now is: [ 2.89342932e-08  5.14253187e-14  2.94049575e-08  2.35741641e-14
 -2.89507604e-08 -5.14545585e-14 -2.94216920e-08 -2.35875656e-14
  1.65673576e-11  8.64469516e-09  2.69929058e-11  7.54272026e-11
  1.53775752e-16  8.10369575e-14  2.51034410e-16  7.03652014e-16
  3.04614956e-12  1.59732779e-09  4.96780157e-12  1.39029688e-11
  5.41574900e-17  2.85129914e-14  8.83949218e-17  2.47701233e-16
 -8.87487281e-17 -1.62177765e-11 -9.44564747e-17 -4.27908457e-16
  1.43259847e-15  6.61870866e-10  1.54584043e-15  6.82532043e-15
 -1.65316325e-16 -3.71496382e-11 -1.76380124e-16 -7.95450647e-16
  1.76912285e-16  6.63770102e-11  1.90258558e-16  8.45453637e-16
 -3.30036544e-15 -8.28421800e-17 -4.96481559e-15 -1.32445185e-16
  1.70112527e-11  1.69016414e-12  2.37979568e-11  1.84482820e-12
 -3.45734499e-15 -1.05150620e-16 -5.18542547e-15 -1.59309636e-16
 -2.78237783e-14 -5.59828668e-16 -4.18768236e-14 -9.53616265e-16
  7.24152694e-12 -9.76879227e-11  9.19934678e-13 -6.42484898e-12
 -1.45847485e-12  1.31497867e-11  1.43769270e-12 -1.19917306e-11
 -2.51529232e+00]
supnorm grad right now is: 2.515292315875993
Weights right now are: 
[-7.61426565 -0.86727862 -5.76497505 -0.89734348  7.08279531  0.73328318
  5.92968202  0.33128736  0.98516139 -3.92499601  1.3607789  -0.2318033
  2.40626366 13.34088495  3.77494574 13.33891747  0.12499717 -5.40665519
  0.28472474 -3.63594776  2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.34691675 -1.47385356 -3.65435212  2.61882122 -4.16655471
  2.778188    5.14395395 -3.98227386  5.84494897 -2.46750197 -2.73451644
  3.70015909  4.79085036  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14678599 -3.05772391  4.4045721  -4.89417799
 11.69188537  8.75893656  1.48904899 12.52125257 10.8544258  11.04235956
  0.4039403  14.90960042 -1.6119765   0.14055879 -6.90949454 -2.10694958
  3.26071639  1.46454933 -4.13917395 -1.67271534 24.76743537]
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.669179178065
gradient value of function right now is: [ 2.89372443e-08  5.14305464e-14  2.94078486e-08  2.35765389e-14
 -2.89388191e-08 -5.14333430e-14 -2.94094490e-08 -2.35778207e-14
  1.65407716e-11  8.64256421e-09  2.69584449e-11  7.53680752e-11
  1.53613076e-16  8.10263917e-14  2.50828951e-16  7.03324524e-16
  3.04231754e-12  1.59716620e-09  4.96297610e-12  1.38954055e-11
  5.41033747e-17  2.85120171e-14  8.83284347e-17  2.47605350e-16
 -8.49025875e-17 -1.62040273e-11 -9.04843451e-17 -4.09168358e-16
  1.38038034e-15  6.61684449e-10  1.49315952e-15  6.56243979e-15
 -1.58370974e-16 -3.71247595e-11 -1.69235681e-16 -7.61412955e-16
  1.70225057e-16  6.63582608e-11  1.83476828e-16  8.12018418e-16
 -3.17928626e-15 -8.39818022e-17 -4.77431806e-15 -1.32007876e-16
  1.67866157e-11  1.72362354e-12  2.34374250e-11  1.87964815e-12
 -3.33829498e-15 -1.07204218e-16 -4.99755042e-15 -1.60062421e-16
 -2.67668069e-14 -5.61027401e-16 -4.02222363e-14 -9.38513273e-16
  7.17189348e-12 -9.56899983e-11  9.31161878e-13 -6.51431495e-12
 -1.47225454e-12  1.31936122e-11  1.45453592e-12 -1.20921486e-11
 -4.16083724e+00]
supnorm grad right now is: 4.160837238506475
Weights right now are: 
[-7.61482682 -0.86727862 -5.76554535 -0.89734348  7.08335644  0.73328318
  5.93025228  0.33128737  0.98516107 -3.92516355  1.36077838 -0.23180475
  2.40626366 13.34088495  3.77494574 13.33891747  0.12499712 -5.40668615
  0.28472465 -3.63594803  2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.34691706 -1.47385356 -3.65435212  2.61882122 -4.16656746
  2.778188    5.14395394 -3.98227386  5.84494968 -2.46750197 -2.73451644
  3.70015909  4.79084909  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14678624 -3.05772393  4.40457175 -4.894178
 11.69188537  8.75893656  1.48904899 12.52125257 10.8544258  11.04235956
  0.4039403  14.90960042 -1.6119766   0.14056037 -6.90949456 -2.10694951
  3.26071641  1.46454919 -4.13917397 -1.67271522 24.65133284]
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.245533005662
gradient value of function right now is: [ 2.87299164e-08  5.10611392e-14  2.91970616e-08  2.34071990e-14
 -2.87476505e-08 -5.10926281e-14 -2.92150833e-08 -2.34216314e-14
  1.64506917e-11  8.58232635e-09  2.68009515e-11  7.48843120e-11
  1.52696344e-16  8.04649477e-14  2.49262300e-16  6.98655849e-16
  3.02543171e-12  1.58628372e-09  4.93373578e-12  1.38067169e-11
  5.37772127e-17  2.83114476e-14  8.77705554e-17  2.45940779e-16
 -8.90868911e-17 -1.61010320e-11 -9.46030245e-17 -4.31029069e-16
  1.43301877e-15  6.57137097e-10  1.54183761e-15  6.85908731e-15
 -1.65823064e-16 -3.68813556e-11 -1.76498141e-16 -8.00856057e-16
  1.77092545e-16  6.59100262e-11  1.89923054e-16  8.50084707e-16
 -3.30950387e-15 -7.83255280e-17 -4.97655781e-15 -1.25950269e-16
  1.69342852e-11  1.64548437e-12  2.36774137e-11  1.78904791e-12
 -3.45772536e-15 -9.93921788e-17 -5.18375150e-15 -1.51000162e-16
 -2.80107213e-14 -5.33828315e-16 -4.21420903e-14 -9.17161010e-16
  7.27892944e-12 -9.78647787e-11  9.06432324e-13 -6.27644648e-12
 -1.44177566e-12  1.28861454e-11  1.41848102e-12 -1.17166071e-11
 -8.16769030e-01]
supnorm grad right now is: 0.8167690296845365
Weights right now are: 
[-7.61540747 -0.86727862 -5.76613544 -0.89734348  7.08393709  0.73328318
  5.93084237  0.33128737  0.98516074 -3.92533697  1.36077784 -0.23180626
  2.40626366 13.34088495  3.77494574 13.33891747  0.12499706 -5.4067182
  0.28472455 -3.63594831  2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.34691739 -1.47385356 -3.65435212  2.61882122 -4.16658073
  2.778188    5.14395394 -3.98227386  5.84495043 -2.46750197 -2.73451644
  3.70015909  4.79084776  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14678658 -3.05772396  4.40457125 -4.89417804
 11.69188537  8.75893656  1.48904899 12.52125257 10.8544258  11.04235956
  0.4039403  14.90960042 -1.61197674  0.14056261 -6.90949457 -2.10694942
  3.26071643  1.46454898 -4.13917399 -1.67271504 24.89862284]
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.539036720167
gradient value of function right now is: [ 2.88223658e-08  5.12258364e-14  2.92909059e-08  2.34826952e-14
 -2.88112211e-08 -5.12060474e-14 -2.92795804e-08 -2.34736251e-14
  1.64686191e-11  8.60492185e-09  2.68412403e-11  7.50418888e-11
  1.52976477e-16  8.06875712e-14  2.49790521e-16  7.00415751e-16
  3.02999600e-12  1.59067728e-09  4.94292481e-12  1.38394438e-11
  5.38832614e-17  2.83951498e-14  8.79695693e-17  2.46600945e-16
 -8.41906778e-17 -1.61340634e-11 -8.97798017e-17 -4.05431770e-16
  1.37134929e-15  6.58889822e-10  1.48462006e-15  6.51185538e-15
 -1.57099770e-16 -3.69646641e-11 -1.67987046e-16 -7.54665694e-16
  1.69074967e-16  6.60862748e-11  1.82381622e-16  8.05645387e-16
 -3.15685948e-15 -8.47310233e-17 -4.73988593e-15 -1.32880939e-16
  1.67162802e-11  1.72538450e-12  2.33442175e-11  1.88447153e-12
 -3.31735083e-15 -1.08179919e-16 -4.96541165e-15 -1.61292694e-16
 -2.65527131e-14 -5.64800899e-16 -3.98950998e-14 -9.41958701e-16
  7.12525520e-12 -9.51473213e-11  9.27316167e-13 -6.51117681e-12
 -1.46772273e-12  1.31930903e-11  1.45031275e-12 -1.20989636e-11
 -4.61770294e+00]
supnorm grad right now is: 4.617702943432105
Weights right now are: 
[-7.61597683 -0.86727862 -5.76671408 -0.89734349  7.08450648  0.73328318
  5.93142103  0.33128737  0.98516039 -3.92550721  1.36077727 -0.2318078
  2.40626366 13.34088495  3.77494574 13.33891747  0.12499699 -5.40674964
  0.28472444 -3.63594859  2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.34691772 -1.47385356 -3.65435212  2.61882122 -4.16659392
  2.778188    5.14395394 -3.98227386  5.84495118 -2.46750197 -2.73451644
  3.70015909  4.79084643  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14678718 -3.057724    4.40457037 -4.89417809
 11.69188537  8.75893656  1.48904899 12.52125257 10.8544258  11.04235956
  0.4039403  14.90960042 -1.61197694  0.14056622 -6.90949459 -2.10694925
  3.26071647  1.46454853 -4.13917403 -1.67271465 24.63542194]
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.520951333674
gradient value of function right now is: [ 2.87567988e-08  5.11089889e-14  2.92241776e-08  2.34291267e-14
 -2.87634521e-08 -5.11208022e-14 -2.92309388e-08 -2.34345411e-14
  1.64342657e-11  8.58716463e-09  2.67853338e-11  7.48861839e-11
  1.52667803e-16  8.05263244e-14  2.49287145e-16  6.99009224e-16
  3.02417162e-12  1.58765545e-09  4.93343881e-12  1.38129946e-11
  5.37686404e-17  2.83350994e-14  8.77825734e-17  2.46078035e-16
 -8.39910139e-17 -1.61005885e-11 -8.95669175e-17 -4.04470030e-16
  1.36817412e-15  6.57563268e-10  1.48118279e-15  6.49677692e-15
 -1.56727575e-16 -3.68881304e-11 -1.67589114e-16 -7.52877384e-16
  1.68691612e-16  6.59576356e-11  1.81968172e-16  8.03818267e-16
 -3.14954678e-15 -8.45349159e-17 -4.72890200e-15 -1.32574047e-16
  1.66774353e-11  1.72137995e-12  2.32899326e-11  1.88009676e-12
 -3.30966891e-15 -1.07929450e-16 -4.95390922e-15 -1.60919997e-16
 -2.64912502e-14 -5.63494963e-16 -3.98027136e-14 -9.39785603e-16
  7.10862685e-12 -9.49257019e-11  9.25162677e-13 -6.49604405e-12
 -1.46430924e-12  1.31624225e-11  1.44694105e-12 -1.20708393e-11
 -4.61696854e+00]
supnorm grad right now is: 4.616968535070193
Weights right now are: 
[-7.61653733 -0.86727862 -5.7672837  -0.89734349  7.08506699  0.73328319
  5.93199066  0.33128737  0.98516005 -3.92567468  1.36077674 -0.23180929
  2.40626366 13.34088495  3.77494574 13.33891747  0.12499693 -5.40678059
  0.28472434 -3.63594886  2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.34691804 -1.47385356 -3.65435212  2.61882122 -4.16660682
  2.778188    5.14395394 -3.98227386  5.84495192 -2.46750197 -2.73451644
  3.70015909  4.79084513  2.79370451  5.22263097 11.62459161  9.82800213
  1.55396331 13.82925274 -7.14678764 -3.05772403  4.40456971 -4.89417813
 11.69188537  8.75893656  1.48904899 12.52125257 10.8544258  11.04235956
  0.4039403  14.90960042 -1.61197711  0.14056887 -6.90949461 -2.1069491
  3.2607165   1.46454818 -4.13917405 -1.67271434 24.60611065]
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.4939713223
gradient value of function right now is: [ 2.82951982e-08  5.02876993e-14  2.87550444e-08  2.30527858e-14
 -2.83210596e-08 -5.03336187e-14 -2.87813251e-08 -2.30738322e-14
  1.63010826e-11  8.45213819e-09  2.65217776e-11  7.39519861e-11
  1.51094278e-16  7.92551859e-14  2.46393825e-16  6.89536529e-16
  2.99699181e-12  1.56288397e-09  4.88154577e-12  1.36357771e-11
  5.32169482e-17  2.78843913e-14  8.67647827e-17  2.42732873e-16
 -1.01523561e-16 -1.59063824e-11 -1.07531657e-16 -4.91049545e-16
  1.60828078e-15  6.47738468e-10  1.72016087e-15  7.73051164e-15
 -1.88439010e-16 -3.64074831e-11 -1.99922722e-16 -9.10580931e-16
  1.99479993e-16  6.50003325e-11  2.12818589e-16  9.60658374e-16
 -3.71450947e-15 -7.63748077e-17 -5.60939004e-15 -1.30015818e-16
  1.78091309e-11  1.53919009e-12  2.50445081e-11  1.67993064e-12
 -3.85981609e-15 -9.43956600e-17 -5.81360088e-15 -1.51165992e-16
 -3.15107471e-14 -5.44931973e-16 -4.75853039e-14 -9.87756811e-16
  7.60630751e-12 -1.05114219e-10  8.81942978e-13 -6.06879376e-12
 -1.41894465e-12  1.29166639e-11  1.38397513e-12 -1.15454920e-11
  4.77913483e+00]
supnorm grad right now is: 4.779134827975414
Weights right now are: 
[-7.61715185 -0.86727862 -5.76790821 -0.89734349  7.08568147  0.73328319
  5.93261513  0.33128737  0.9851597  -3.92585821  1.36077616 -0.2318109
  2.40626366 13.34088495  3.77494574 13.33891747  0.12499686 -5.40681452
  0.28472424 -3.63594916  2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.34691839 -1.47385356 -3.65435212  2.61882122 -4.16662091
  2.778188    5.14395394 -3.98227386  5.84495271 -2.46750197 -2.73451644
  3.70015909  4.79084371  2.79370451  5.22263097 11.62459162  9.82800213
  1.55396331 13.82925274 -7.14678806 -3.05772407  4.4045691  -4.89417818
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442581 11.04235956
  0.4039403  14.90960042 -1.61197728  0.14057119 -6.90949463 -2.10694893
  3.26071653  1.4645478  -4.13917409 -1.672714   25.23223813]
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.15375543148
gradient value of function right now is: [ 2.81859609e-08  5.00931233e-14  2.86439381e-08  2.29635963e-14
 -2.81956557e-08 -5.01103373e-14 -2.86537901e-08 -2.29714862e-14
  1.62447351e-11  8.41650062e-09  2.64253948e-11  7.36636445e-11
  1.50549623e-16  7.89277848e-14  2.45473672e-16  6.86830923e-16
  2.98672773e-12  1.55651119e-09  4.86406831e-12  1.35838183e-11
  5.30312906e-17  2.77721906e-14  8.64506348e-17  2.41807242e-16
 -1.03089490e-16 -1.58449793e-11 -1.09091978e-16 -4.99045715e-16
  1.62831573e-15  6.45083885e-10  1.73904630e-15  7.84007424e-15
 -1.91236104e-16 -3.62634734e-11 -2.02684733e-16 -9.25034335e-16
  2.02097373e-16  6.47397591e-11  2.15319356e-16  9.74753354e-16
 -3.76323962e-15 -7.46162550e-17 -5.68546974e-15 -1.28283608e-16
  1.78727812e-11  1.51469306e-12  2.51465427e-11  1.65155305e-12
 -3.90526271e-15 -9.18756178e-17 -5.88477264e-15 -1.48354185e-16
 -3.19636762e-14 -5.37508378e-16 -4.82887561e-14 -9.83909362e-16
  7.64949365e-12 -1.05912802e-10  8.74625014e-13 -5.99808025e-12
 -1.41194111e-12  1.28380286e-11  1.37478179e-12 -1.14407950e-11
  5.78116016e+00]
supnorm grad right now is: 5.781160155308669
Weights right now are: 
[-7.61775698 -0.86727862 -5.76852318 -0.89734349  7.08628667  0.73328319
  5.93323016  0.33128737  0.98515935 -3.92603891  1.36077559 -0.23181248
  2.40626366 13.34088494  3.77494574 13.33891747  0.1249968  -5.40684793
  0.28472413 -3.63594945  2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.34691873 -1.47385356 -3.65435212  2.61882122 -4.16663479
  2.778188    5.14395394 -3.98227386  5.84495349 -2.46750197 -2.73451644
  3.70015909  4.79084232  2.79370451  5.22263097 11.62459162  9.82800213
  1.55396331 13.82925274 -7.14678845 -3.05772412  4.40456857 -4.89417823
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442581 11.04235956
  0.40394031 14.90960042 -1.61197746  0.14057325 -6.90949465 -2.10694877
  3.26071657  1.46454746 -4.13917413 -1.67271368 25.38562232]
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.258567258683
gradient value of function right now is: [ 2.83450052e-08  5.03750466e-14  2.88053993e-08  2.30926343e-14
 -2.83244134e-08 -5.03384841e-14 -2.87844738e-08 -2.30758765e-14
  1.62071461e-11  8.45804305e-09  2.64049961e-11  7.37839686e-11
  1.50498029e-16  7.93352684e-14  2.45683905e-16  6.88691550e-16
  2.98281000e-12  1.56446408e-09  4.86439961e-12  1.36138541e-11
  5.30153336e-17  2.79209829e-14  8.65308813e-17  2.42491329e-16
 -8.78222462e-17 -1.58639588e-11 -9.30906717e-17 -4.26163292e-16
  1.40903043e-15  6.47718769e-10  1.51274822e-15  6.76898878e-15
 -1.63371000e-16 -3.63399104e-11 -1.73558241e-16 -7.91463896e-16
  1.74248857e-16  6.49822209e-11  1.86477842e-16  8.39409960e-16
 -3.26065480e-15 -7.37342405e-17 -4.90125502e-15 -1.18811336e-16
  1.66326917e-11  1.59789894e-12  2.32351288e-11  1.72926746e-12
 -3.39992580e-15 -9.36684986e-17 -5.09484697e-15 -1.42227369e-16
 -2.76827414e-14 -5.05427700e-16 -4.16362148e-14 -8.72380253e-16
  7.20900616e-12 -9.64969501e-11  8.90887855e-13 -6.11149995e-12
 -1.41689368e-12  1.25304193e-11  1.39288545e-12 -1.13779097e-11
  3.90144192e-01]
supnorm grad right now is: 0.3901441921981456
Weights right now are: 
[-7.6182998  -0.86727862 -5.76907481 -0.89734349  7.08682944  0.73328319
  5.93378176  0.33128737  0.98515904 -3.92620096  1.36077509 -0.2318139
  2.40626366 13.34088494  3.77494574 13.33891747  0.12499674 -5.4068779
  0.28472404 -3.63594971  2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.34691903 -1.47385356 -3.65435212  2.61882122 -4.16664721
  2.778188    5.14395394 -3.98227386  5.84495419 -2.46750197 -2.73451644
  3.70015909  4.79084108  2.79370451  5.22263097 11.62459162  9.82800213
  1.55396331 13.82925274 -7.14678878 -3.05772415  4.40456812 -4.89417826
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442581 11.04235956
  0.40394031 14.90960042 -1.61197761  0.14057519 -6.90949467 -2.10694864
  3.2607166   1.46454722 -4.13917416 -1.67271346 24.95093665]
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.259739298499
gradient value of function right now is: [ 2.82806916e-08  5.02603901e-14  2.87399435e-08  2.30400649e-14
 -2.82609313e-08 -5.02253043e-14 -2.87198630e-08 -2.30239839e-14
  1.61669365e-11  8.43820654e-09  2.63401442e-11  7.36059376e-11
  1.50140403e-16  7.91557721e-14  2.45105216e-16  6.87094517e-16
  2.97589830e-12  1.56103477e-09  4.85323982e-12  1.35831658e-11
  5.28889640e-17  2.78576550e-14  8.63264568e-17  2.41927433e-16
 -8.73967311e-17 -1.58255500e-11 -9.26327627e-17 -4.24183087e-16
  1.40230109e-15  6.46219061e-10  1.50546966e-15  6.73774525e-15
 -1.62579032e-16 -3.62525850e-11 -1.72705225e-16 -7.87779505e-16
  1.73421570e-16  6.48352020e-11  1.85584076e-16  8.35568150e-16
 -3.24545205e-15 -7.33416128e-17 -4.87804676e-15 -1.18089315e-16
  1.65676785e-11  1.59359461e-12  2.31411492e-11  1.72398371e-12
 -3.38394397e-15 -9.32215782e-17 -5.07047770e-15 -1.41429570e-16
 -2.75581282e-14 -5.02454864e-16 -4.14462712e-14 -8.66794289e-16
  7.18374044e-12 -9.61123102e-11  8.88575872e-13 -6.09241084e-12
 -1.41268706e-12  1.24821137e-11  1.38894525e-12 -1.13364272e-11
  3.51085517e-01]
supnorm grad right now is: 0.3510855170101215
Weights right now are: 
[-7.61886993 -0.86727863 -5.7696542  -0.89734349  7.08739952  0.73328319
  5.93436109  0.33128737  0.98515873 -3.92637106  1.36077457 -0.23181536
  2.40626366 13.34088494  3.77494574 13.33891747  0.12499669 -5.40690937
  0.28472394 -3.63594998  2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.34691934 -1.47385356 -3.65435212  2.61882122 -4.16666019
  2.778188    5.14395394 -3.98227386  5.84495491 -2.46750197 -2.73451644
  3.70015909  4.79083978  2.79370451  5.22263097 11.62459162  9.82800213
  1.55396331 13.82925274 -7.14678901 -3.05772419  4.40456784 -4.8941783
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442581 11.04235956
  0.40394031 14.90960042 -1.61197775  0.14057639 -6.9094947  -2.1069485
  3.26071663  1.46454703 -4.13917419 -1.67271327 24.98223719]
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.88806320722
gradient value of function right now is: [ 2.80832629e-08  4.99098708e-14  2.85392645e-08  2.28795370e-14
 -2.81063313e-08 -4.99508304e-14 -2.85627065e-08 -2.28983102e-14
  1.61476675e-11  8.38531769e-09  2.62811626e-11  7.33200809e-11
  1.49789122e-16  7.86568218e-14  2.44328420e-16  6.84028065e-16
  2.97122428e-12  1.55149645e-09  4.84104055e-12  1.35289774e-11
  5.27563227e-17  2.76743008e-14  8.60365360e-17  2.40793750e-16
 -9.69368725e-17 -1.57692840e-11 -1.02698501e-16 -4.69385487e-16
  1.54308982e-15  6.42664430e-10  1.65235715e-15  7.41631116e-15
 -1.80078643e-16 -3.61009017e-11 -1.91133279e-16 -8.70948341e-16
  1.91229487e-16  6.45026719e-11  2.04213971e-16  9.21066374e-16
 -3.56637508e-15 -7.55990217e-17 -5.37690409e-15 -1.26462721e-16
  1.74188834e-11  1.54820811e-12  2.44525810e-11  1.68792656e-12
 -3.70988882e-15 -9.42094967e-17 -5.57796270e-15 -1.48360415e-16
 -3.02649701e-14 -5.32683159e-16 -4.56376596e-14 -9.50616517e-16
  7.46919667e-12 -1.02339091e-10  8.78154527e-13 -6.05380809e-12
 -1.41049163e-12  1.27639967e-11  1.37814986e-12 -1.14583015e-11
  3.38395488e+00]
supnorm grad right now is: 3.383954883652095
Weights right now are: 
[-7.61947936 -0.86727863 -5.77027352 -0.89734349  7.08800895  0.73328319
  5.93498041  0.33128737  0.98515838 -3.92655297  1.360774   -0.23181695
  2.40626366 13.34088494  3.77494574 13.33891747  0.12499662 -5.40694303
  0.28472384 -3.63595028  2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.34691969 -1.47385356 -3.65435212  2.61882122 -4.16667413
  2.778188    5.14395394 -3.98227386  5.84495569 -2.46750197 -2.73451644
  3.70015909  4.79083838  2.79370451  5.22263097 11.62459162  9.82800213
  1.55396331 13.82925274 -7.14678938 -3.05772422  4.40456732 -4.89417833
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442581 11.04235956
  0.40394031 14.90960042 -1.61197791  0.14057856 -6.90949471 -2.10694837
  3.26071666  1.46454676 -4.13917422 -1.67271302 25.21831454]
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.479142759643
gradient value of function right now is: [ 2.83547657e-08  5.03925624e-14  2.88150244e-08  2.31006848e-14
 -2.83745105e-08 -5.04276201e-14 -2.88350890e-08 -2.31167529e-14
  1.61991496e-11  8.46449182e-09  2.64022521e-11  7.38167343e-11
  1.50558615e-16  7.94145276e-14  2.45843525e-16  6.89364885e-16
  2.98370972e-12  1.56644201e-09  4.86745002e-12  1.36285074e-11
  5.30213577e-17  2.79414138e-14  8.65626494e-17  2.42662082e-16
 -8.26709475e-17 -1.58708218e-11 -8.81657657e-17 -3.98094375e-16
  1.34779457e-15  6.48386522e-10  1.45926967e-15  6.39933036e-15
 -1.54282243e-16 -3.63621855e-11 -1.64987750e-16 -7.41086053e-16
  1.66214455e-16  6.50620406e-11  1.79314076e-16  7.91936945e-16
 -3.10260703e-15 -8.34530778e-17 -4.65785971e-15 -1.30809002e-16
  1.64433828e-11  1.69852058e-12  2.29617014e-11  1.85524959e-12
 -3.26070970e-15 -1.06544594e-16 -4.88003074e-15 -1.58794512e-16
 -2.60948382e-14 -5.56256788e-16 -3.92026376e-14 -9.27091590e-16
  7.01014503e-12 -9.35682770e-11  9.12484327e-13 -6.40845703e-12
 -1.44455198e-12  1.29847412e-11  1.42739738e-12 -1.19088391e-11
 -4.69220938e+00]
supnorm grad right now is: 4.69220938009359
Weights right now are: 
[-7.62003006 -0.86727863 -5.77083317 -0.89734349  7.08855964  0.73328319
  5.93554005  0.33128737  0.98515806 -3.92671736  1.36077348 -0.23181839
  2.40626366 13.34088494  3.77494574 13.33891747  0.12499656 -5.40697344
  0.28472374 -3.63595054  2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.34691999 -1.47385356 -3.65435212  2.61882122 -4.16668676
  2.778188    5.14395394 -3.98227386  5.8449564  -2.46750197 -2.73451644
  3.70015909  4.79083711  2.79370451  5.22263097 11.62459162  9.82800213
  1.55396331 13.82925274 -7.14678974 -3.05772426  4.40456682 -4.89417838
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442581 11.04235956
  0.40394031 14.90960042 -1.61197809  0.14058066 -6.90949474 -2.10694822
  3.2607167   1.46454647 -4.13917425 -1.67271276 24.62609884]
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16016.510993480519
gradient value of function right now is: [ 2.83861939e-08  5.04480148e-14  2.88468471e-08  2.31260420e-14
 -2.83903929e-08 -5.04554709e-14 -2.88511140e-08 -2.31294594e-14
  1.61747665e-11  8.47065492e-09  2.63752114e-11  7.37963318e-11
  1.50433847e-16  7.94826311e-14  2.45730975e-16  6.89444201e-16
  2.98044275e-12  1.56781657e-09  4.86420784e-12  1.36284868e-11
  5.29801210e-17  2.79681956e-14  8.65282751e-17  2.42708521e-16
 -7.79752173e-17 -1.58643193e-11 -8.34192937e-17 -3.74100661e-16
  1.28102793e-15  6.48718816e-10  1.39341409e-15  6.04517906e-15
 -1.45736652e-16 -3.63575533e-11 -1.56387815e-16 -6.97112125e-16
  1.57746083e-16  6.50911387e-11  1.70919513e-16  7.47337072e-16
 -2.94116412e-15 -8.61899193e-17 -4.41265337e-15 -1.32862231e-16
  1.60189833e-11  1.74081382e-12  2.23293767e-11  1.89926877e-12
 -3.10438801e-15 -1.10581777e-16 -4.64263419e-15 -1.62688793e-16
 -2.46155009e-14 -5.65979730e-16 -3.69615134e-14 -9.24395020e-16
  6.80495345e-12 -8.99930627e-11  9.30853502e-13 -6.53837352e-12
 -1.45571776e-12  1.30225888e-11  1.44537899e-12 -1.20328437e-11
 -6.83791781e+00]
supnorm grad right now is: 6.8379178105735345
Weights right now are: 
[-7.6205829  -0.86727863 -5.77139497 -0.89734349  7.0891125   0.73328319
  5.93610188  0.33128737  0.98515775 -3.92688224  1.36077298 -0.2318198
  2.40626366 13.34088494  3.77494574 13.33891747  0.12499651 -5.40700396
  0.28472365 -3.6359508   2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.3469203  -1.47385356 -3.65435212  2.61882122 -4.16669931
  2.778188    5.14395394 -3.98227386  5.8449571  -2.46750197 -2.73451644
  3.70015909  4.79083585  2.79370451  5.22263097 11.62459162  9.82800213
  1.55396331 13.82925274 -7.14678995 -3.05772429  4.40456655 -4.8941784
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442581 11.04235956
  0.40394031 14.90960042 -1.6119782   0.14058186 -6.90949475 -2.10694814
  3.26071672  1.46454634 -4.13917428 -1.67271263 24.43150725]
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16017.653967914039
gradient value of function right now is: [ 2.82119305e-08  5.01379812e-14  2.86696892e-08  2.29839669e-14
 -2.82177906e-08 -5.01483847e-14 -2.86756442e-08 -2.29887350e-14
  1.61119349e-11  8.41846843e-09  2.62593755e-11  7.34151353e-11
  1.49762883e-16  7.89957326e-14  2.44540990e-16  6.85707837e-16
  2.96837916e-12  1.55835306e-09  4.84233728e-12  1.35579303e-11
  5.27458080e-17  2.77966052e-14  8.61116661e-17  2.41396290e-16
 -8.27103624e-17 -1.57844311e-11 -8.81639727e-17 -3.98471230e-16
  1.34572248e-15  6.44911231e-10  1.45596845e-15  6.39517814e-15
 -1.54290021e-16 -3.61640726e-11 -1.64905545e-16 -7.41537067e-16
  1.66038838e-16  6.47200937e-11  1.79001253e-16  7.91745113e-16
 -3.09867804e-15 -8.21586481e-17 -4.65372132e-15 -1.29160326e-16
  1.63567391e-11  1.68192997e-12  2.28402037e-11  1.83492077e-12
 -3.25428352e-15 -1.04868237e-16 -4.87228542e-15 -1.56619207e-16
 -2.60795666e-14 -5.48604919e-16 -3.91931456e-14 -9.17718042e-16
  6.97872811e-12 -9.32010797e-11  9.07279410e-13 -6.35279342e-12
 -1.43434481e-12  1.28684765e-11  1.41729351e-12 -1.17956915e-11
 -4.27579085e+00]
supnorm grad right now is: 4.275790846823077
Weights right now are: 
[-7.62111661 -0.86727863 -5.77193734 -0.89734349  7.0896462   0.73328319
  5.93664423  0.33128737  0.98515746 -3.92704141  1.3607725  -0.23182117
  2.40626366 13.34088494  3.77494574 13.33891747  0.12499645 -5.40703343
  0.28472356 -3.63595106  2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.34692059 -1.47385356 -3.65435212  2.61882122 -4.16671144
  2.778188    5.14395394 -3.98227386  5.84495777 -2.46750197 -2.73451644
  3.70015909  4.79083464  2.79370451  5.22263097 11.62459162  9.82800213
  1.55396331 13.82925274 -7.14679016 -3.05772431  4.40456629 -4.89417842
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442581 11.04235956
  0.40394031 14.90960042 -1.61197834  0.14058301 -6.90949477 -2.10694804
  3.26071675  1.4645462  -4.1391743  -1.67271249 24.6396959 ]
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.070925981003
gradient value of function right now is: [ 2.81045229e-08  4.99466104e-14  2.85605021e-08  2.28962508e-14
 -2.81059412e-08 -4.99491281e-14 -2.85619433e-08 -2.28974047e-14
  1.60695987e-11  8.38553390e-09  2.61820909e-11  7.31647101e-11
  1.49309293e-16  7.86883295e-14  2.43744534e-16  6.83247699e-16
  2.96018158e-12  1.55235265e-09  4.82763317e-12  1.35112812e-11
  5.25890771e-17  2.76892675e-14  8.58356226e-17  2.40540375e-16
 -8.59754496e-17 -1.57315435e-11 -9.14946838e-17 -4.14643121e-16
  1.38888127e-15  6.42451140e-10  1.49851852e-15  6.61869833e-15
 -1.60156027e-16 -3.60368625e-11 -1.70855836e-16 -7.70818179e-16
  1.71614408e-16  6.44782304e-11  1.84542550e-16  8.20341682e-16
 -3.20014848e-15 -8.01767660e-17 -4.81351251e-15 -1.28136443e-16
  1.65001938e-11  1.63940963e-12  2.30801872e-11  1.78886737e-12
 -3.35207799e-15 -1.01799560e-16 -5.02694225e-15 -1.54160251e-16
 -2.69858647e-14 -5.41785117e-16 -4.06116101e-14 -9.22711650e-16
  7.02748236e-12 -9.47601299e-11  8.92794778e-13 -6.23155071e-12
 -1.41505423e-12  1.27489870e-11  1.39491222e-12 -1.16265802e-11
 -2.43951127e+00]
supnorm grad right now is: 2.4395112742790004
Weights right now are: 
[-7.62168725 -0.86727863 -5.77251724 -0.89734349  7.09021684  0.73328319
  5.93722413  0.33128737  0.98515714 -3.92721168  1.36077197 -0.23182265
  2.40626366 13.34088493  3.77494574 13.33891747  0.12499639 -5.40706495
  0.28472346 -3.63595133  2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.34692091 -1.47385356 -3.65435212  2.61882122 -4.1667245
  2.778188    5.14395394 -3.98227386  5.8449585  -2.46750197 -2.73451644
  3.70015909  4.79083333  2.79370451  5.22263097 11.62459162  9.82800213
  1.55396331 13.82925274 -7.1467905  -3.05772435  4.40456583 -4.89417846
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442581 11.04235956
  0.40394031 14.90960042 -1.61197852  0.14058495 -6.90949479 -2.1069479
  3.26071678  1.46454594 -4.13917434 -1.67271225 24.80837037]
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.18630038037
gradient value of function right now is: [ 2.80342021e-08  4.98211565e-14  2.84889890e-08  2.28387188e-14
 -2.80556483e-08 -4.98592347e-14 -2.85107823e-08 -2.28561713e-14
  1.60319179e-11  8.36701282e-09  2.61205401e-11  7.29934629e-11
  1.48952982e-16  7.85165284e-14  2.43165399e-16  6.81647868e-16
  2.95343728e-12  1.54907272e-09  4.81664068e-12  1.34808009e-11
  5.24570097e-17  2.76250673e-14  8.56207763e-17  2.39945816e-16
 -8.62105221e-17 -1.56943503e-11 -9.16253471e-17 -4.16544912e-16
  1.38851927e-15  6.40994700e-10  1.49559617e-15  6.63397140e-15
 -1.60493544e-16 -3.59521327e-11 -1.70978108e-16 -7.73987556e-16
  1.71662214e-16  6.43331260e-11  1.84293413e-16  8.22578461e-16
 -3.20331855e-15 -7.75467369e-17 -4.81867157e-15 -1.24406892e-16
  1.64294120e-11  1.61390916e-12  2.29734681e-11  1.75604559e-12
 -3.35010617e-15 -9.84370578e-17 -5.02429654e-15 -1.49364153e-16
 -2.70697763e-14 -5.26538895e-16 -4.07412523e-14 -9.01614413e-16
  7.03089320e-12 -9.46812112e-11  8.86082581e-13 -6.14667606e-12
 -1.40520266e-12  1.25841496e-11  1.38421340e-12 -1.14596246e-11
 -1.51385327e+00]
supnorm grad right now is: 1.513853272629903
Weights right now are: 
[-7.6222714  -0.86727863 -5.77311086 -0.89734349  7.09080097  0.7332832
  5.93781773  0.33128737  0.9851568  -3.92738596  1.36077143 -0.23182417
  2.40626366 13.34088493  3.77494574 13.33891747  0.12499633 -5.40709721
  0.28472336 -3.63595161  2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.34692124 -1.47385356 -3.65435212  2.61882122 -4.16673785
  2.778188    5.14395394 -3.98227386  5.84495925 -2.46750197 -2.73451644
  3.70015909  4.79083199  2.79370451  5.22263097 11.62459162  9.82800213
  1.55396331 13.82925274 -7.14679084 -3.05772438  4.40456535 -4.8941785
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442581 11.04235956
  0.40394031 14.90960042 -1.61197866  0.14058684 -6.90949481 -2.10694777
  3.26071681  1.46454567 -4.13917437 -1.672712   25.05264376]
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.261734873264
gradient value of function right now is: [ 2.79283522e-08  4.96326017e-14  2.83813718e-08  2.27522663e-14
 -2.79350375e-08 -4.96444714e-14 -2.83881653e-08 -2.27577066e-14
  1.59675382e-11  8.33291514e-09  2.60145429e-11  7.26936876e-11
  1.48347514e-16  7.82006569e-14  2.42172302e-16  6.78854438e-16
  2.94163939e-12  1.54285955e-09  4.79724497e-12  1.34260617e-11
  5.22488305e-17  2.75166364e-14  8.52793174e-17  2.38985519e-16
 -8.65035435e-17 -1.56297414e-11 -9.17132636e-17 -4.19608739e-16
  1.38879513e-15  6.38364342e-10  1.49140499e-15  6.66878664e-15
 -1.60930846e-16 -3.58037339e-11 -1.71005387e-16 -7.79339942e-16
  1.71792523e-16  6.40700178e-11  1.83895789e-16  8.27213762e-16
 -3.21298013e-15 -7.30550061e-17 -4.82988374e-15 -1.17702350e-16
  1.63918447e-11  1.57655096e-12  2.29013050e-11  1.70712776e-12
 -3.35101453e-15 -9.27847648e-17 -5.02190414e-15 -1.40914048e-16
 -2.72679326e-14 -5.00451714e-16 -4.10143104e-14 -8.63398195e-16
  7.09684220e-12 -9.50567737e-11  8.77890716e-13 -6.02887829e-12
 -1.39619509e-12  1.23633286e-11  1.37267365e-12 -1.12277913e-11
  2.72999287e-01]
supnorm grad right now is: 0.27299928748450675
Weights right now are: 
[-7.62286907 -0.86727863 -5.77371823 -0.89734349  7.09139856  0.7332832
  5.93842502  0.33128737  0.98515645 -3.92756429  1.36077086 -0.23182574
  2.40626366 13.34088493  3.77494574 13.33891747  0.12499627 -5.40713023
  0.28472326 -3.6359519   2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.34692157 -1.47385356 -3.65435212  2.61882122 -4.16675154
  2.778188    5.14395394 -3.98227386  5.84496002 -2.46750197 -2.73451644
  3.70015909  4.79083061  2.79370451  5.22263097 11.62459162  9.82800213
  1.55396331 13.82925274 -7.14679125 -3.05772442  4.40456476 -4.89417853
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442581 11.04235956
  0.40394031 14.90960042 -1.6119788   0.14058926 -6.90949483 -2.10694763
  3.26071684  1.46454536 -4.1391744  -1.67271173 25.27300876]
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.14323590314
gradient value of function right now is: [ 2.78355097e-08  4.94677375e-14  2.82869987e-08  2.26767573e-14
 -2.78472099e-08 -4.94885107e-14 -2.82988882e-08 -2.26862783e-14
  1.59511411e-11  8.30584570e-09  2.59759789e-11  7.25337489e-11
  1.48125327e-16  7.79466434e-14  2.41721292e-16  6.77202984e-16
  2.93814211e-12  1.53794858e-09  4.78956587e-12  1.33958870e-11
  5.21711677e-17  2.74263955e-14  8.51207666e-17  2.38401723e-16
 -9.05157072e-17 -1.55976453e-11 -9.59881492e-17 -4.38215807e-16
  1.44902636e-15  6.36507104e-10  1.55486902e-15  6.95293062e-15
 -1.68318698e-16 -3.57203265e-11 -1.78861687e-16 -8.13708824e-16
  1.79399689e-16  6.38939797e-11  1.91932685e-16  8.62965024e-16
 -3.34870982e-15 -7.46471456e-17 -5.04130601e-15 -1.22412829e-16
  1.67490938e-11  1.55749267e-12  2.34576639e-11  1.69417884e-12
 -3.49027785e-15 -9.38601939e-17 -5.23920235e-15 -1.45087116e-16
 -2.83892515e-14 -5.18347075e-16 -4.27543916e-14 -9.07835529e-16
  7.21033999e-12 -9.75843978e-11  8.71521753e-13 -6.01333212e-12
 -1.39448360e-12  1.25082059e-11  1.36679947e-12 -1.12994200e-11
  1.72424704e+00]
supnorm grad right now is: 1.7242470439315667
Weights right now are: 
[-7.62345676 -0.86727863 -5.77431547 -0.89734349  7.09198618  0.7332832
  5.93902219  0.33128737  0.98515609 -3.9277398   1.36077028 -0.23182733
  2.40626366 13.34088493  3.77494574 13.33891747  0.1249962  -5.40716271
  0.28472315 -3.63595219  2.95939795 14.1289851   3.6296035  13.76894537
 -0.94803544  6.34692192 -1.47385356 -3.65435212  2.61882122 -4.16676515
  2.778188    5.14395394 -3.98227386  5.8449608  -2.46750197 -2.73451644
  3.70015909  4.79082924  2.79370451  5.22263097 11.62459162  9.82800213
  1.55396331 13.82925274 -7.14679187 -3.05772446  4.40456384 -4.89417859
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442581 11.04235956
  0.40394031 14.90960042 -1.61197901  0.14059296 -6.90949485 -2.10694744
  3.26071688  1.46454487 -4.13917443 -1.6727113  25.37945736]
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16018.11576985965
gradient value of function right now is: [ 2.77957516e-08  4.93968659e-14  2.82465473e-08  2.26442628e-14
 -2.77763937e-08 -4.93624966e-14 -2.82268762e-08 -2.26285103e-14
  1.59191180e-11  8.28914840e-09  2.59237141e-11  7.23875449e-11
  1.47835907e-16  7.77952140e-14  2.41248660e-16  6.75879585e-16
  2.93237708e-12  1.53493187e-09  4.78015117e-12  1.33695245e-11
  5.20794619e-17  2.73788686e-14  8.49712061e-17  2.37983969e-16
 -9.04173002e-17 -1.55661822e-11 -9.58683871e-17 -4.37832350e-16
  1.44693648e-15  6.35236873e-10  1.55226783e-15  6.94517252e-15
 -1.68122493e-16 -3.56482598e-11 -1.78621936e-16 -8.12951461e-16
  1.79157004e-16  6.37674225e-11  1.91631427e-16  8.62060230e-16
 -3.34442149e-15 -7.41796798e-17 -5.03488826e-15 -1.21734592e-16
  1.67179072e-11  1.55116609e-12  2.34116006e-11  1.68618157e-12
 -3.48508245e-15 -9.32372367e-17 -5.23144338e-15 -1.44198919e-16
 -2.83605105e-14 -5.15698000e-16 -4.27111684e-14 -9.03878213e-16
  7.20590012e-12 -9.74524597e-11  8.70403310e-13 -5.99640208e-12
 -1.39244120e-12  1.24710096e-11  1.36467468e-12 -1.12634136e-11
  1.92134421e+00]
supnorm grad right now is: 1.921344205066993
Weights right now are: 
[-7.62400539 -0.86727864 -5.77487301 -0.89734349  7.09253483  0.7332832
  5.93957974  0.33128737  0.98515576 -3.92790359  1.36076975 -0.23182879
  2.40626366 13.34088493  3.77494574 13.33891747  0.12499614 -5.40719304
  0.28472306 -3.63595246  2.95939795 14.1289851   3.6296035  13.76894537
 -0.94803544  6.34692223 -1.47385356 -3.65435212  2.61882122 -4.16677782
  2.778188    5.14395394 -3.98227386  5.84496152 -2.46750197 -2.73451644
  3.70015909  4.79082796  2.79370451  5.22263097 11.62459162  9.82800213
  1.55396331 13.82925274 -7.14679239 -3.0577245   4.4045631  -4.89417864
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442581 11.04235956
  0.40394031 14.90960042 -1.61197921  0.14059606 -6.90949487 -2.10694728
  3.26071691  1.4645445  -4.13917447 -1.67271098 25.0279405 ]
NN weights: [-7.62259585 -0.86727863 -5.77344058 -0.89734349  7.09112537  0.7332832
  5.9381474   0.33128737  0.9851566  -3.92748283  1.36077111 -0.23182505
  2.40626366 13.34088493  3.77494574 13.33891747  0.12499629 -5.40711515
  0.28472331 -3.63595177  2.95939795 14.12898511  3.6296035  13.76894537
 -0.94803544  6.34692143 -1.47385356 -3.65435212  2.61882122 -4.16674535
  2.778188    5.14395394 -3.98227386  5.84495968 -2.46750197 -2.73451644
  3.70015909  4.79083123  2.79370451  5.22263097 11.62459162  9.82800213
  1.55396331 13.82925274 -7.14679117 -3.0577244   4.40456487 -4.89417852
 11.69188537  8.75893656  1.48904899 12.52125257 10.85442581 11.04235956
  0.40394031 14.90960042 -1.61197876  0.14058883 -6.90949482 -2.10694767
  3.26071683  1.46454542 -4.13917439 -1.67271179]
Minimum obj value:-16018.11576985965
Optimal xi: 25.095107984548406
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1553.2825337587535
W_T_median: 1404.8762552293338
W_T_pctile_5: 621.9833554095644
W_T_CVAR_5_pct: 485.4393690216443
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 10.0
F value: -16018.11576985965
-----------------------------------------------
