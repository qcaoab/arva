/home/marcchen/Documents/constrain_factor/researchcode/exp_config_json_files/multi_portfolio_exp3_split_1927.json
Starting at: 
13-07-23_19:43

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.005383     0.031411
192608                    0.0319              0.0561  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0355  ...     0.005323    -0.028996
192611                   -0.0038              0.0294  ...     0.005303     0.028554

[5 rows x 6 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.043289    -0.036240
202209                   -0.0955             -0.0871  ...    -0.050056    -0.091324
202210                    0.0883              0.1486  ...    -0.014968     0.077403
202211                   -0.0076              0.0462  ...     0.040789     0.052365
202212                   -0.0457             -0.0499  ...    -0.018566    -0.057116

[5 rows x 6 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'CPI_nom_ret_ind', 'T30_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Value_Hi30_real_ret
T30_real_ret             1.000000  ...             0.018239
B10_real_ret             0.351722  ...             0.031301
VWD_real_ret             0.068448  ...             0.909335
Size_Lo30_real_ret       0.014412  ...             0.908542
Value_Hi30_real_ret      0.018239  ...             1.000000

[5 rows x 5 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 192707
End: 199112
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       5       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       5           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 5)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 1500, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.6, 0.1, 0.15, 0.15]
W_T_mean: 879.1071857337951
W_T_median: 554.6081909537522
W_T_pctile_5: -319.2190231086564
W_T_CVAR_5_pct: -441.58650468055305
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1091.1488222694015
Current xi:  [113.630486]
objective value function right now is: -1091.1488222694015
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1501.6631863804998
Current xi:  [125.67682]
objective value function right now is: -1501.6631863804998
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1564.869908449044
Current xi:  [114.137344]
objective value function right now is: -1564.869908449044
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1581.7246704407544
Current xi:  [113.95925]
objective value function right now is: -1581.7246704407544
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1598.5996600499745
Current xi:  [117.63417]
objective value function right now is: -1598.5996600499745
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [120.56849]
objective value function right now is: -1595.2975018814975
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [123.036255]
objective value function right now is: -1595.7890841695757
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [125.38409]
objective value function right now is: -1587.148176779538
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [125.69441]
objective value function right now is: -1596.681072067028
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [127.315765]
objective value function right now is: -1591.8576317670352
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [129.42307]
objective value function right now is: -1593.3925895098025
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1599.081777976778
Current xi:  [131.16248]
objective value function right now is: -1599.081777976778
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.942872532718
Current xi:  [132.13083]
objective value function right now is: -1602.942872532718
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [133.33922]
objective value function right now is: -1596.8852563229416
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [135.22333]
objective value function right now is: -1600.0655033016474
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [136.84915]
objective value function right now is: -1600.4772178838255
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1603.7288874540752
Current xi:  [138.12697]
objective value function right now is: -1603.7288874540752
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [137.58614]
objective value function right now is: -1595.7567215311085
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [138.90686]
objective value function right now is: -1599.402037942658
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [140.07773]
objective value function right now is: -1599.0329793612384
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1603.7740254795528
Current xi:  [141.06703]
objective value function right now is: -1603.7740254795528
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [141.65665]
objective value function right now is: -1599.5194732064156
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [142.52861]
objective value function right now is: -1599.9470262475102
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [142.93365]
objective value function right now is: -1602.2067738009757
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [140.79517]
objective value function right now is: -1599.839766655935
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [141.68224]
objective value function right now is: -1601.3300166216063
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [142.83887]
objective value function right now is: -1599.2524090266998
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [142.901]
objective value function right now is: -1601.0456714336422
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1603.8049217041819
Current xi:  [143.04341]
objective value function right now is: -1603.8049217041819
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [144.33557]
objective value function right now is: -1592.176101047192
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [145.55936]
objective value function right now is: -1600.0449664509683
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [145.31993]
objective value function right now is: -1603.400584710046
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [146.00545]
objective value function right now is: -1600.455002667424
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [146.88141]
objective value function right now is: -1601.618544424851
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [146.98172]
objective value function right now is: -1603.0532910109903
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.002945135408
Current xi:  [147.09386]
objective value function right now is: -1605.002945135408
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.9006209644283
Current xi:  [147.20969]
objective value function right now is: -1605.9006209644283
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1606.1667070162655
Current xi:  [147.4051]
objective value function right now is: -1606.1667070162655
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1606.1931224474897
Current xi:  [147.38846]
objective value function right now is: -1606.1931224474897
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [147.54994]
objective value function right now is: -1605.5455725722445
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1606.4870151241566
Current xi:  [147.60725]
objective value function right now is: -1606.4870151241566
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [147.78824]
objective value function right now is: -1606.3499163198928
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [147.84738]
objective value function right now is: -1605.2090883176463
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [147.94864]
objective value function right now is: -1605.9675266739605
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.1737]
objective value function right now is: -1606.2622577087102
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -1606.6557855185047
Current xi:  [148.35933]
objective value function right now is: -1606.6557855185047
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.49127]
objective value function right now is: -1604.9619299780684
new min fval from sgd:  -1606.656088409581
new min fval from sgd:  -1606.7460687034602
new min fval from sgd:  -1606.7883108599099
new min fval from sgd:  -1606.824681826688
new min fval from sgd:  -1606.8449751715427
new min fval from sgd:  -1606.8472165988037
new min fval from sgd:  -1606.8527028419926
new min fval from sgd:  -1606.865686221135
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.62712]
objective value function right now is: -1606.0367184441375
new min fval from sgd:  -1606.8753812502098
new min fval from sgd:  -1606.8805824412452
new min fval from sgd:  -1606.8866740021854
new min fval from sgd:  -1606.8950616828497
new min fval from sgd:  -1606.8983983268276
new min fval from sgd:  -1606.9011949414178
new min fval from sgd:  -1606.906585286752
new min fval from sgd:  -1606.9107217403612
new min fval from sgd:  -1606.923381292841
new min fval from sgd:  -1606.9377667179406
new min fval from sgd:  -1606.940777312773
new min fval from sgd:  -1606.961939592801
new min fval from sgd:  -1606.9622062644237
new min fval from sgd:  -1606.9704823029579
new min fval from sgd:  -1606.9709927565164
new min fval from sgd:  -1606.9745817857997
new min fval from sgd:  -1606.9795450753245
new min fval from sgd:  -1606.9838106737393
new min fval from sgd:  -1606.9918953426388
new min fval from sgd:  -1606.9942377877212
new min fval from sgd:  -1606.9974085581339
new min fval from sgd:  -1607.0024053421673
new min fval from sgd:  -1607.007571953132
new min fval from sgd:  -1607.0142771789963
new min fval from sgd:  -1607.0171737862254
new min fval from sgd:  -1607.0213083835024
new min fval from sgd:  -1607.0273191942042
new min fval from sgd:  -1607.0281937084617
new min fval from sgd:  -1607.031058532083
new min fval from sgd:  -1607.0407739574011
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.78862]
objective value function right now is: -1606.957170136973
new min fval from sgd:  -1607.0484579248432
new min fval from sgd:  -1607.0510117967754
new min fval from sgd:  -1607.0571253038222
new min fval from sgd:  -1607.0694401038038
new min fval from sgd:  -1607.0770139545402
new min fval from sgd:  -1607.085813959377
new min fval from sgd:  -1607.0909342023335
new min fval from sgd:  -1607.096870385484
new min fval from sgd:  -1607.1033036110823
new min fval from sgd:  -1607.1066853043721
new min fval from sgd:  -1607.1098442598245
new min fval from sgd:  -1607.1185744677969
new min fval from sgd:  -1607.1245548654492
new min fval from sgd:  -1607.1248792123079
new min fval from sgd:  -1607.133058855027
new min fval from sgd:  -1607.141526448285
new min fval from sgd:  -1607.1486665988016
new min fval from sgd:  -1607.1592686488402
new min fval from sgd:  -1607.1630207437943
new min fval from sgd:  -1607.1648354981785
new min fval from sgd:  -1607.1728469310453
new min fval from sgd:  -1607.184681065451
new min fval from sgd:  -1607.1896956449493
new min fval from sgd:  -1607.1961329323926
new min fval from sgd:  -1607.1989974098724
new min fval from sgd:  -1607.200138177243
new min fval from sgd:  -1607.2037543606903
new min fval from sgd:  -1607.2084897357975
new min fval from sgd:  -1607.2109111920727
new min fval from sgd:  -1607.211246683119
new min fval from sgd:  -1607.2144383648183
new min fval from sgd:  -1607.2154444526561
new min fval from sgd:  -1607.2163193750916
new min fval from sgd:  -1607.2167819161327
new min fval from sgd:  -1607.2174524487832
new min fval from sgd:  -1607.218108007737
new min fval from sgd:  -1607.2328799424927
new min fval from sgd:  -1607.2518047486737
new min fval from sgd:  -1607.2620305594733
new min fval from sgd:  -1607.263390540194
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.86449]
objective value function right now is: -1607.1899679342544
min fval:  -1607.263390540194
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-3.3954e+00,  8.6443e+00],
        [-8.1211e-01,  8.5642e-01],
        [-8.2278e+00,  7.0593e+00],
        [-4.9807e+00, -1.0988e+01],
        [-8.3060e-01,  8.4491e-01],
        [-8.6242e-01,  8.2901e-01],
        [-8.1359e-01,  8.5522e-01],
        [-4.8868e+00,  5.0977e+00],
        [-8.1737e-01,  8.5223e-01],
        [-8.1206e-01,  8.5645e-01],
        [-8.1228e-01,  8.5627e-01],
        [-4.8733e+01, -6.0771e+00],
        [ 1.2000e+01,  2.8881e-02]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  4.8708,  -2.6626,   3.5370,  -4.4603,  -2.7050,  -2.6916,  -2.6633,
         -1.1008,  -2.6657,  -2.6626,  -2.6627,  -4.6073, -10.5900],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-7.6965e+00,  1.1547e-01, -5.7539e+00,  6.0051e+00,  1.0272e-02,
          1.7547e-01,  1.1644e-01, -1.0329e-01,  1.1617e-01,  1.1533e-01,
          1.1571e-01,  6.6806e+00,  1.2391e+01],
        [-4.5514e-01, -1.8029e-02, -1.1595e-01, -1.1097e+00, -1.7703e-02,
         -1.7740e-02, -1.8023e-02, -1.9190e-02, -1.8004e-02, -1.8029e-02,
         -1.8028e-02, -8.6311e-02, -5.4186e-01],
        [ 5.3964e+00,  7.0179e-02,  4.4883e+00, -7.1008e+00,  6.7310e-02,
          1.7334e-02,  6.9757e-02,  2.2534e-01,  6.8640e-02,  7.0192e-02,
          7.0153e-02, -6.6219e+00, -1.1366e+01],
        [ 6.4647e+00,  6.0083e-02,  5.0555e+00, -7.2397e+00,  4.5245e-03,
          8.9837e-03,  5.7719e-02,  5.4569e-01,  5.3004e-02,  6.0056e-02,
          5.9762e-02, -8.2769e+00, -1.2530e+01],
        [-4.7682e-01, -1.2767e-02, -1.5417e-01, -4.5057e-01, -1.2121e-02,
         -1.2265e-02, -1.2752e-02, -6.3139e-03, -1.2708e-02, -1.2767e-02,
         -1.2765e-02, -4.1747e-02, -1.4777e-01],
        [-8.7659e+00,  3.1386e-02, -8.1808e+00,  7.5002e+00,  7.8537e-03,
         -9.4257e-02,  2.7233e-02, -1.3971e+00,  1.7783e-02,  3.1548e-02,
          3.0835e-02,  8.5756e+00,  1.5221e+01],
        [ 6.4682e+00,  5.7568e-02,  5.3874e+00, -7.6090e+00, -2.5201e-02,
          1.7975e-02,  5.4919e-02,  6.9134e-01,  4.6520e-02,  5.7704e-02,
          5.7289e-02, -8.3168e+00, -1.2807e+01],
        [-4.7681e-01, -1.2767e-02, -1.5417e-01, -4.5056e-01, -1.2121e-02,
         -1.2265e-02, -1.2752e-02, -6.3138e-03, -1.2708e-02, -1.2767e-02,
         -1.2765e-02, -4.1747e-02, -1.4777e-01],
        [-1.4431e+00,  7.9548e-02,  7.5180e-01,  9.4146e-01,  7.6937e-02,
          7.6186e-02,  7.9447e-02, -5.6400e-03,  7.9136e-02,  7.9548e-02,
          7.9538e-02, -1.1462e-01, -5.5186e+00],
        [-4.7681e-01, -1.2767e-02, -1.5417e-01, -4.5056e-01, -1.2121e-02,
         -1.2265e-02, -1.2752e-02, -6.3138e-03, -1.2708e-02, -1.2767e-02,
         -1.2765e-02, -4.1747e-02, -1.4777e-01],
        [-7.7758e+00,  6.7079e-02, -6.6529e+00,  6.4055e+00,  3.5642e-02,
          8.1893e-02,  6.7034e-02, -9.9431e-02,  6.7358e-02,  6.7096e-02,
          6.7051e-02,  5.6107e+00,  1.0844e+01],
        [-4.7080e-01, -1.2704e-02, -1.5306e-01, -4.4667e-01, -1.2059e-02,
         -1.2202e-02, -1.2689e-02, -6.2631e-03, -1.2644e-02, -1.2704e-02,
         -1.2702e-02, -4.1830e-02, -1.4712e-01],
        [-4.7681e-01, -1.2767e-02, -1.5417e-01, -4.5056e-01, -1.2121e-02,
         -1.2265e-02, -1.2752e-02, -6.3138e-03, -1.2708e-02, -1.2767e-02,
         -1.2765e-02, -4.1747e-02, -1.4777e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.3989, -1.9578,  0.6367,  0.7951, -1.4081, -1.0707,  0.9341, -1.4082,
         2.9244, -1.4082, -1.4237, -1.4236, -1.4082], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-7.9790e+00,  1.9374e-01,  5.4335e+00,  6.6469e+00,  8.9620e-03,
         -1.0567e+01,  7.1263e+00,  8.9619e-03,  1.4048e+00,  8.9619e-03,
         -6.9559e+00,  8.9630e-03,  8.9620e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -9.3022,  -4.1122],
        [  2.4556,   8.7648],
        [ -6.0552,   1.8754],
        [  5.0574,   7.5305],
        [ 10.4851,   4.7988],
        [  7.6625,   5.2914],
        [ -5.3755,   5.7250],
        [-10.0780,  -0.0371],
        [ -2.5851,  12.8759],
        [ -5.8126,  -3.5935],
        [ 10.3101,   4.6857],
        [  1.2458,  11.6111],
        [ -3.5009,  -1.4684]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-1.3317,  4.8126, -2.9673,  5.6882,  2.2951, -0.7293,  2.3200,  8.7566,
         6.6358, -8.0956,  2.3796,  4.8125, -5.6960], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 6.1919e-01,  4.1844e-01,  1.9825e-01,  3.6159e-01, -2.2805e+00,
         -5.1066e-01, -3.4822e-01, -8.2061e-01, -1.4515e+00, -2.3982e-02,
         -2.2853e+00, -1.1612e+00, -4.2512e-02],
        [ 1.4206e+00,  1.1507e+00,  6.5238e-01,  1.1794e+00, -1.5559e+00,
         -5.8109e-01, -9.0184e-01,  1.6623e-01, -3.6930e+00,  1.5768e-01,
         -1.5472e+00, -2.6664e+00,  3.0222e-01],
        [ 1.4308e+00,  8.1343e-01,  3.8743e-01,  7.7582e-01, -1.9747e+00,
         -9.0247e-03, -6.4252e-01, -8.5474e-01, -3.3075e+00, -1.3997e-02,
         -1.9776e+00, -1.8026e+00,  3.5731e-02],
        [ 5.0974e+00, -9.3367e+00, -1.1980e-02, -2.7732e+00, -4.6002e+00,
         -1.5228e+00, -2.6435e+00,  6.6470e+00, -1.6350e+01,  1.7568e+00,
         -4.4318e+00, -1.4731e+01,  2.4888e+00],
        [ 1.1530e+00,  7.3528e-01,  3.0151e-01,  6.6180e-01, -2.0943e+00,
         -1.7919e-01, -5.5967e-01, -8.1549e-01, -2.6700e+00, -3.5242e-02,
         -2.0984e+00, -1.7623e+00,  3.7213e-03],
        [ 8.4641e-01,  5.5989e-01,  1.7440e-01,  5.0284e-01, -2.2150e+00,
         -4.6013e-01, -4.7104e-01, -6.9305e-01, -1.9602e+00, -3.9354e-02,
         -2.2221e+00, -1.5475e+00, -3.1443e-02],
        [-2.3345e+00, -9.3045e-01, -2.3350e-01, -1.2432e+00,  4.0555e-01,
          8.2025e-01,  1.0681e-01,  5.4140e-01,  4.7658e-01,  1.0025e-01,
          4.1036e-01, -3.3867e-01,  2.7883e-01],
        [ 1.4208e+00,  8.1110e-01,  3.8458e-01,  7.7200e-01, -1.9790e+00,
         -1.3916e-02, -6.3937e-01, -8.5493e-01, -3.2857e+00, -1.5023e-02,
         -1.9820e+00, -1.8015e+00,  3.4576e-02],
        [ 9.2239e+00, -1.4781e+01, -3.1228e-01, -1.8606e+01, -9.9472e+00,
         -2.1938e+00, -4.1510e+00,  9.7370e+00, -1.7999e+00,  1.8431e+00,
         -9.0697e+00, -1.2445e+00,  2.2034e+00],
        [ 1.3844e+00,  8.0285e-01,  3.7074e-01,  7.5672e-01, -1.9952e+00,
         -3.7484e-02, -6.3059e-01, -8.4753e-01, -3.1998e+00, -1.8737e-02,
         -1.9984e+00, -1.8030e+00,  3.0410e-02],
        [-4.8041e+00,  1.3556e+00, -1.9576e+00, -6.5222e-02, -4.1658e+00,
         -2.7811e+00,  4.2127e+00,  1.0408e+01,  3.4271e+00, -1.7447e-01,
         -4.7432e+00,  4.0342e+00, -2.2076e-01],
        [ 3.0233e-01, -3.7507e-02,  6.9318e-01,  3.4726e+00, -2.1322e+00,
         -3.6081e-01, -6.8796e-01, -4.8710e+00, -2.5259e+00,  2.2720e-02,
         -1.5374e+00, -2.3572e+00,  3.2547e-03],
        [ 3.8767e+00, -9.1682e-01,  2.7584e-01, -4.6174e+00, -4.6874e+00,
         -3.2885e-01,  6.1048e-01,  3.7965e+00,  1.9562e-01, -5.7429e-02,
         -4.7620e+00,  8.3134e-02,  7.8790e-02]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.3050, -1.6556, -1.9990,  2.2320, -2.1067, -2.2098, -0.7880, -2.0035,
         1.5333, -2.0192, -5.2033, -1.5496, -0.0816], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.5952e+00,  3.2594e+00,  2.6687e+00, -6.3041e+01,  2.3613e+00,
          1.9220e+00, -2.0096e+00,  2.6588e+00, -5.5416e-04,  2.6222e+00,
          2.7343e+00,  1.2454e+00,  2.8869e+00],
        [-7.3922e-02,  7.1269e-01,  1.3305e-01, -4.6719e-01,  2.2060e-02,
         -9.7303e-02,  1.8590e+00,  1.2952e-01,  1.1907e+01,  1.1493e-01,
         -6.1540e-01,  3.3182e+00, -1.8908e+00],
        [-1.7124e-01, -4.8724e+00, -5.4561e-01, -1.7148e+00, -3.7832e-01,
         -2.6758e-01, -3.2607e+01, -5.3716e-01, -2.9285e-03, -5.1037e-01,
         -1.0380e+01, -1.7677e-01, -3.5517e-01],
        [-1.7855e-01, -5.4262e+00, -6.1753e-01, -2.3253e+00, -3.9701e-01,
         -2.7627e-01, -3.2223e+01, -6.0588e-01, -1.5344e-03, -5.6857e-01,
         -1.1162e+01, -1.9264e-01, -3.9180e-01],
        [-2.9507e-01, -1.1955e+00, -7.3893e-01,  8.5164e+00, -5.9776e-01,
         -4.2844e-01,  1.2163e+00, -7.3393e-01, -1.1244e+01, -7.1424e-01,
          7.7466e-02, -1.9028e+00,  3.4675e-01]], device='cuda:0'))])
xi:  [148.86137]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 516.9184150588266
W_T_median: 326.534392895614
W_T_pctile_5: 148.85828809062875
W_T_CVAR_5_pct: -14.778055692695133
Average q (qsum/M+1):  52.323907667590724
Optimal xi:  [148.86137]
Expected(across Rb) median(across samples) p_equity:  0.21983456456412873
obj fun:  tensor(-1607.2634, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
