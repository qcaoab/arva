/home/marcchen/Documents/constrain_factor/researchcode/exp_config_json_files/multi_portfolio_exp3_split_1927.json
Starting at: 
14-07-23_15:22

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = 3factor_mc
timeseries_basket['basket_desc'] = 3 factors : Basic, size, value, mom
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret', 'Mom_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = 3factor_mc
timeseries_basket['basket_desc'] = 3 factors : Basic, size, value, mom
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret', 'Mom_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.005383     0.031411
192608                    0.0319              0.0561  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0355  ...     0.005323    -0.028996
192611                   -0.0038              0.0294  ...     0.005303     0.028554

[5 rows x 7 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.043289    -0.036240
202209                   -0.0955             -0.0871  ...    -0.050056    -0.091324
202210                    0.0883              0.1486  ...    -0.014968     0.077403
202211                   -0.0076              0.0462  ...     0.040789     0.052365
202212                   -0.0457             -0.0499  ...    -0.018566    -0.057116

[5 rows x 7 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'Mom_Hi30_nom_ret_ind', 'CPI_nom_ret_ind', 'T30_nom_ret_ind',
       'B10_nom_ret_ind', 'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
Mom_Hi30_real_ret      0.011386
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
Mom_Hi30_real_ret      0.061421
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Mom_Hi30_real_ret
T30_real_ret             1.000000  ...           0.055142
B10_real_ret             0.351722  ...           0.066570
VWD_real_ret             0.068448  ...           0.936115
Size_Lo30_real_ret       0.014412  ...           0.903222
Value_Hi30_real_ret      0.018239  ...           0.869469
Mom_Hi30_real_ret        0.055142  ...           1.000000

[6 rows x 6 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 192707
End: 199112
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      14  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      14  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 14)     True          14  
2     (14, 14)     True          14  
3      (14, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       6       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      14  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      14  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       6           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 14)     True          14  
2     (14, 14)     True          14  
3      (14, 6)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        6           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 6)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 1500, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        6           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 6)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.6, 0.1, 0.1, 0.1, 0.1]
W_T_mean: 924.4024154136672
W_T_median: 613.3748362942811
W_T_pctile_5: -280.23183268243423
W_T_CVAR_5_pct: -405.44527023207644
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1783.6448684487289
Current xi:  [120.58369]
objective value function right now is: -1783.6448684487289
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1809.940374745803
Current xi:  [144.81313]
objective value function right now is: -1809.940374745803
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1827.5720595294945
Current xi:  [168.43153]
objective value function right now is: -1827.5720595294945
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1844.1113234229986
Current xi:  [191.2802]
objective value function right now is: -1844.1113234229986
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1850.8029088179596
Current xi:  [213.55788]
objective value function right now is: -1850.8029088179596
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1865.9316723502006
Current xi:  [235.73688]
objective value function right now is: -1865.9316723502006
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1876.2963927649414
Current xi:  [257.59888]
objective value function right now is: -1876.2963927649414
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1884.753705283883
Current xi:  [279.67868]
objective value function right now is: -1884.753705283883
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1894.2149866463155
Current xi:  [301.75177]
objective value function right now is: -1894.2149866463155
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1905.867463161706
Current xi:  [323.85165]
objective value function right now is: -1905.867463161706
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1917.9343418972355
Current xi:  [345.75165]
objective value function right now is: -1917.9343418972355
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1931.2470481759685
Current xi:  [367.89075]
objective value function right now is: -1931.2470481759685
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1938.7478501511557
Current xi:  [389.01212]
objective value function right now is: -1938.7478501511557
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1945.4920247045748
Current xi:  [410.33932]
objective value function right now is: -1945.4920247045748
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1951.930220893244
Current xi:  [431.71423]
objective value function right now is: -1951.930220893244
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1962.5026447331531
Current xi:  [452.14557]
objective value function right now is: -1962.5026447331531
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1983.6219886608792
Current xi:  [474.87308]
objective value function right now is: -1983.6219886608792
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1987.0077596733804
Current xi:  [498.63034]
objective value function right now is: -1987.0077596733804
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -2001.019410964204
Current xi:  [521.07654]
objective value function right now is: -2001.019410964204
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2013.9299886802773
Current xi:  [542.9128]
objective value function right now is: -2013.9299886802773
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -2016.7038255872453
Current xi:  [564.2095]
objective value function right now is: -2016.7038255872453
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -2027.828438474455
Current xi:  [585.17676]
objective value function right now is: -2027.828438474455
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2031.3706952402144
Current xi:  [606.54614]
objective value function right now is: -2031.3706952402144
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [626.851]
objective value function right now is: -2027.5298012719602
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -2049.5238514371367
Current xi:  [648.1221]
objective value function right now is: -2049.5238514371367
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [668.66534]
objective value function right now is: -2031.5761275091795
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -2061.6935736179735
Current xi:  [689.02216]
objective value function right now is: -2061.6935736179735
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -2067.5433730007603
Current xi:  [708.5907]
objective value function right now is: -2067.5433730007603
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -2070.1890896641326
Current xi:  [728.01184]
objective value function right now is: -2070.1890896641326
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -2074.479176956233
Current xi:  [747.59875]
objective value function right now is: -2074.479176956233
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -2082.457668160346
Current xi:  [766.96576]
objective value function right now is: -2082.457668160346
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -2083.3299733335157
Current xi:  [785.84064]
objective value function right now is: -2083.3299733335157
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -2083.6645393746976
Current xi:  [804.97437]
objective value function right now is: -2083.6645393746976
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -2091.302773002171
Current xi:  [822.92096]
objective value function right now is: -2091.302773002171
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [840.0811]
objective value function right now is: -2075.126901239286
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2099.78934084116
Current xi:  [843.8581]
objective value function right now is: -2099.78934084116
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -2101.486826546068
Current xi:  [847.6664]
objective value function right now is: -2101.486826546068
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -2102.8893324768087
Current xi:  [851.31555]
objective value function right now is: -2102.8893324768087
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2104.035766776731
Current xi:  [855.0883]
objective value function right now is: -2104.035766776731
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [858.895]
objective value function right now is: -2103.141425926078
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -2104.4246848975013
Current xi:  [862.7012]
objective value function right now is: -2104.4246848975013
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -2105.611920455037
Current xi:  [866.2731]
objective value function right now is: -2105.611920455037
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -2106.0534282271365
Current xi:  [870.0212]
objective value function right now is: -2106.0534282271365
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -2106.4451866583054
Current xi:  [873.8612]
objective value function right now is: -2106.4451866583054
90.0% of gradient descent iterations done. Method = Adam
new min fval:  -2107.624598283886
Current xi:  [877.4263]
objective value function right now is: -2107.624598283886
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -2108.227250012421
Current xi:  [881.1218]
objective value function right now is: -2108.227250012421
94.0% of gradient descent iterations done. Method = Adam
new min fval:  -2109.2072517189717
Current xi:  [884.5398]
objective value function right now is: -2109.2072517189717
new min fval from sgd:  -2109.239047176693
new min fval from sgd:  -2109.287140544335
new min fval from sgd:  -2109.296305507612
new min fval from sgd:  -2109.299504668173
new min fval from sgd:  -2109.2996616882683
new min fval from sgd:  -2109.3437566989846
new min fval from sgd:  -2109.406161820818
new min fval from sgd:  -2109.4627156730444
new min fval from sgd:  -2109.5642571812923
new min fval from sgd:  -2109.646305022508
new min fval from sgd:  -2109.6764523955053
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [888.06793]
objective value function right now is: -2103.033425927672
new min fval from sgd:  -2109.7091062221907
new min fval from sgd:  -2109.7351564907376
new min fval from sgd:  -2109.7591517238307
new min fval from sgd:  -2109.773866799453
new min fval from sgd:  -2109.7793000708593
new min fval from sgd:  -2109.781145060544
new min fval from sgd:  -2109.807715263168
new min fval from sgd:  -2109.8096534885226
new min fval from sgd:  -2109.8255959843527
new min fval from sgd:  -2109.868495560845
new min fval from sgd:  -2109.939120542046
new min fval from sgd:  -2110.047590298714
new min fval from sgd:  -2110.121228848968
new min fval from sgd:  -2110.1418767008045
new min fval from sgd:  -2110.1792180681628
new min fval from sgd:  -2110.2200822754953
new min fval from sgd:  -2110.2495980089598
new min fval from sgd:  -2110.270084412955
new min fval from sgd:  -2110.289344945798
new min fval from sgd:  -2110.3022981881827
new min fval from sgd:  -2110.312432020924
new min fval from sgd:  -2110.316483318
new min fval from sgd:  -2110.317387153956
new min fval from sgd:  -2110.321452796962
new min fval from sgd:  -2110.3241122798904
new min fval from sgd:  -2110.326824438723
new min fval from sgd:  -2110.32787766203
new min fval from sgd:  -2110.330707444281
new min fval from sgd:  -2110.3326731714665
new min fval from sgd:  -2110.338423126723
new min fval from sgd:  -2110.343741127778
new min fval from sgd:  -2110.3470220518984
new min fval from sgd:  -2110.359710843054
new min fval from sgd:  -2110.3686401842965
new min fval from sgd:  -2110.37551464072
new min fval from sgd:  -2110.3775789842157
new min fval from sgd:  -2110.378949761237
new min fval from sgd:  -2110.378958384211
new min fval from sgd:  -2110.3811695487825
new min fval from sgd:  -2110.3826507599338
new min fval from sgd:  -2110.3889238709494
new min fval from sgd:  -2110.392167408849
new min fval from sgd:  -2110.3921771495957
new min fval from sgd:  -2110.3942330309565
new min fval from sgd:  -2110.3960276067473
new min fval from sgd:  -2110.397092872407
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [890.28375]
objective value function right now is: -2109.8834359397842
new min fval from sgd:  -2110.414823649867
new min fval from sgd:  -2110.4325556508697
new min fval from sgd:  -2110.449513636135
new min fval from sgd:  -2110.4623246131932
new min fval from sgd:  -2110.471402461264
new min fval from sgd:  -2110.477850850309
new min fval from sgd:  -2110.4811657206665
new min fval from sgd:  -2110.484237354768
new min fval from sgd:  -2110.4866677602918
new min fval from sgd:  -2110.48914959095
new min fval from sgd:  -2110.490424051767
new min fval from sgd:  -2110.4926708458697
new min fval from sgd:  -2110.49546111908
new min fval from sgd:  -2110.4981543372405
new min fval from sgd:  -2110.5001970087887
new min fval from sgd:  -2110.5007943208648
new min fval from sgd:  -2110.502696242462
new min fval from sgd:  -2110.503856459198
new min fval from sgd:  -2110.506166462293
new min fval from sgd:  -2110.5075132194056
new min fval from sgd:  -2110.5093432968456
new min fval from sgd:  -2110.5133578535765
new min fval from sgd:  -2110.517901824078
new min fval from sgd:  -2110.5225839044547
new min fval from sgd:  -2110.523667926095
new min fval from sgd:  -2110.552905520183
new min fval from sgd:  -2110.5783284191257
new min fval from sgd:  -2110.5926343793635
new min fval from sgd:  -2110.60324930734
new min fval from sgd:  -2110.6110285228747
new min fval from sgd:  -2110.6189532273456
new min fval from sgd:  -2110.6265774272233
new min fval from sgd:  -2110.6333786021714
new min fval from sgd:  -2110.6352365895136
new min fval from sgd:  -2110.6352971767747
new min fval from sgd:  -2110.636109505159
new min fval from sgd:  -2110.640262480937
new min fval from sgd:  -2110.6404858901137
new min fval from sgd:  -2110.641232674135
new min fval from sgd:  -2110.6470323920416
new min fval from sgd:  -2110.6534469118865
new min fval from sgd:  -2110.657893592282
new min fval from sgd:  -2110.668970970161
new min fval from sgd:  -2110.6757429379495
new min fval from sgd:  -2110.6772550529113
new min fval from sgd:  -2110.680519167223
new min fval from sgd:  -2110.682926724068
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [891.04456]
objective value function right now is: -2110.670318128468
min fval:  -2110.682926724068
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -9.7254,   4.8540],
        [ -1.3516,  -0.6530],
        [ -1.3656,  -0.6647],
        [-15.2196, -16.8238],
        [  8.7156,  -3.0943],
        [  9.1819,   0.4126],
        [-12.2732,   6.5627],
        [  8.7603,  -6.5582],
        [ -1.3516,  -0.6530],
        [  8.7827,  -1.4460],
        [ -7.6862, -14.4481],
        [ -1.3516,  -0.6530],
        [ -7.5404,   2.1648],
        [ -9.4877,   4.6089]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-0.3985, -2.9165, -2.9158,  5.6141, -7.7928, -9.2247,  0.4964, -6.5937,
        -2.9165, -8.3073, -0.7100, -2.9165,  1.0259, -0.2537], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.2003e+00, -8.0207e-02, -1.5604e-01,  9.5012e+00,  5.5892e+00,
          7.9595e+00, -4.6534e+00,  4.7566e+00, -8.0193e-02,  4.3946e+00,
          6.7543e+00, -8.0236e-02, -1.4580e+00, -2.0205e+00],
        [-2.2091e+00,  2.7695e-01,  2.3533e-01,  8.8371e+00,  4.3879e+00,
          6.6005e+00, -4.7679e+00,  3.4492e+00,  2.7695e-01,  3.4474e+00,
          5.8333e+00,  2.7695e-01, -2.5724e+00, -2.3601e+00],
        [-3.8530e-02, -1.1263e-02, -1.1175e-02, -1.8290e-01, -1.8508e-01,
         -4.4805e-01, -9.7264e-02, -2.9460e-01, -1.1263e-02, -2.2348e-01,
         -4.5240e-03, -1.1263e-02, -1.5247e-01, -4.8053e-02],
        [-3.8530e-02, -1.1263e-02, -1.1175e-02, -1.8290e-01, -1.8508e-01,
         -4.4805e-01, -9.7264e-02, -2.9460e-01, -1.1263e-02, -2.2348e-01,
         -4.5240e-03, -1.1263e-02, -1.5247e-01, -4.8053e-02],
        [-3.8530e-02, -1.1263e-02, -1.1175e-02, -1.8290e-01, -1.8508e-01,
         -4.4805e-01, -9.7264e-02, -2.9460e-01, -1.1263e-02, -2.2348e-01,
         -4.5240e-03, -1.1263e-02, -1.5247e-01, -4.8053e-02],
        [-3.8530e-02, -1.1263e-02, -1.1175e-02, -1.8290e-01, -1.8508e-01,
         -4.4805e-01, -9.7264e-02, -2.9460e-01, -1.1263e-02, -2.2348e-01,
         -4.5240e-03, -1.1263e-02, -1.5247e-01, -4.8053e-02],
        [-3.8530e-02, -1.1263e-02, -1.1175e-02, -1.8290e-01, -1.8508e-01,
         -4.4805e-01, -9.7264e-02, -2.9460e-01, -1.1263e-02, -2.2348e-01,
         -4.5240e-03, -1.1263e-02, -1.5247e-01, -4.8053e-02],
        [-3.8530e-02, -1.1263e-02, -1.1175e-02, -1.8290e-01, -1.8508e-01,
         -4.4805e-01, -9.7264e-02, -2.9460e-01, -1.1263e-02, -2.2348e-01,
         -4.5239e-03, -1.1263e-02, -1.5247e-01, -4.8053e-02],
        [-3.8530e-02, -1.1263e-02, -1.1175e-02, -1.8290e-01, -1.8508e-01,
         -4.4805e-01, -9.7264e-02, -2.9460e-01, -1.1263e-02, -2.2348e-01,
         -4.5239e-03, -1.1263e-02, -1.5247e-01, -4.8053e-02],
        [-1.2198e+00,  1.9246e-01,  1.5242e-01,  9.1028e+00,  4.9360e+00,
          7.2668e+00, -2.9827e+00,  4.1251e+00,  1.9246e-01,  3.7378e+00,
          2.9582e+00,  1.9246e-01, -2.0875e-01, -1.1778e+00],
        [-3.8526e-02, -1.1263e-02, -1.1174e-02, -1.8289e-01, -1.8508e-01,
         -4.4802e-01, -9.7259e-02, -2.9459e-01, -1.1263e-02, -2.2347e-01,
         -4.5243e-03, -1.1263e-02, -1.5246e-01, -4.8050e-02],
        [-3.8530e-02, -1.1263e-02, -1.1175e-02, -1.8290e-01, -1.8508e-01,
         -4.4805e-01, -9.7264e-02, -2.9460e-01, -1.1263e-02, -2.2348e-01,
         -4.5240e-03, -1.1263e-02, -1.5247e-01, -4.8053e-02],
        [ 3.8337e+00,  1.3571e-02, -4.2235e-02, -1.1184e+01, -6.0762e+00,
         -8.9310e+00,  6.8498e+00, -5.3308e+00,  1.3577e-02, -4.8125e+00,
         -1.1640e+01,  1.3558e-02,  3.0407e+00,  4.1836e+00],
        [-3.8530e-02, -1.1263e-02, -1.1175e-02, -1.8290e-01, -1.8508e-01,
         -4.4805e-01, -9.7264e-02, -2.9460e-01, -1.1263e-02, -2.2348e-01,
         -4.5240e-03, -1.1263e-02, -1.5247e-01, -4.8053e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-4.6493, -4.2334, -1.9407, -1.9407, -1.9407, -1.9407, -1.9407, -1.9407,
        -1.9407, -4.2083, -1.9408, -1.9407,  4.7925, -1.9407], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-6.9503e+00, -5.1736e+00, -6.7320e-03, -6.7320e-03, -6.7320e-03,
         -6.7320e-03, -6.7320e-03, -6.7320e-03, -6.7320e-03, -4.8810e+00,
         -6.7325e-03, -6.7320e-03,  1.8189e+01, -6.7320e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  6.0145,  -4.6858],
        [ 10.5591,   6.9843],
        [ -8.5693,   0.8683],
        [ -2.0855,  -0.0307],
        [ -3.0989,  12.5859],
        [ 13.3927,   4.8484],
        [  3.0796,  -0.0419],
        [  9.1939,   8.8042],
        [ 11.4103,   5.5233],
        [  7.5390,  -1.8369],
        [-19.3766,   3.4862],
        [ -3.4803,  -3.3253],
        [ 10.4115,  -0.0344],
        [  9.8218,   8.4243]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -2.7176,   1.7011,   9.2422,  -4.4832,   1.5985,  -0.9114,  -8.4963,
         -1.7514,  -2.1678, -10.8165,   1.5402,  -3.1618,  -9.1856,   4.8670],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.7203e+00, -6.4714e-01, -1.9168e+00,  1.3175e-02, -1.1842e+00,
         -1.7483e+00,  2.5850e-02, -1.4146e-01, -1.7068e-01,  2.1062e-01,
         -2.5245e-02, -2.7779e-01, -9.7455e-02, -8.4873e-01],
        [-1.7392e+00, -8.3613e-01, -1.9283e+00,  1.6576e-02, -1.0369e+00,
         -1.5708e+00,  2.3624e-02, -1.2572e-01, -4.1491e-01,  2.8863e-01,
         -3.0246e-02, -1.9161e-01, -5.4880e-02, -1.0289e+00],
        [-1.0836e-01, -1.0283e+01,  5.7382e+00,  1.9289e-02, -1.2004e-02,
         -1.9043e+01,  6.0640e-02, -2.9874e-02, -1.6120e+00, -6.9386e+00,
         -2.1016e+00,  6.0484e+00, -8.4770e+00, -2.3837e+01],
        [-1.5441e+00, -8.1028e-01, -1.4244e+00, -3.6292e-03, -5.0616e-01,
         -1.4666e+00, -6.4050e-03, -3.4472e-01, -4.3923e-01, -2.8264e-01,
         -1.3258e-01, -5.0111e-01, -6.6721e-01, -8.9131e-01],
        [ 2.4843e-01, -1.7998e+00,  2.9634e+00,  2.1725e-01, -7.8231e+00,
          1.1328e+00, -1.6680e-01, -1.1419e+01, -1.7422e+00, -4.0033e-01,
          3.6412e+00,  1.0809e+00, -3.6631e+00, -8.7070e-01],
        [-4.2963e+00,  7.6567e-01,  1.6785e+00, -3.0335e-01,  6.0313e+00,
         -4.0972e-01,  3.3334e-01, -3.9411e-01, -7.2668e-01,  1.2574e-01,
         -6.7357e+00, -1.3728e+00, -8.6175e+00,  1.7932e+00],
        [ 1.1990e+00, -2.1702e+00,  4.8215e+00,  1.8179e-01, -1.4863e+01,
         -4.4410e+00, -9.0209e-01, -1.8138e+00, -2.3036e+00, -3.3179e+00,
          6.8540e+00,  1.8530e+00, -6.6367e+00, -3.7015e+00],
        [-1.6811e+00, -6.2060e-01, -1.9153e+00,  1.5560e-02, -1.2525e+00,
         -1.7265e+00,  2.6816e-02, -1.0507e-01, -1.3418e-01,  2.1008e-01,
         -2.6873e-02, -2.1691e-01, -5.4511e-02, -8.3558e-01],
        [-1.6231e+00, -8.5066e-01, -1.7057e+00, -3.1133e-03, -5.4108e-01,
         -1.5579e+00,  1.1632e-02, -2.7713e-01, -5.3182e-01, -5.6097e-03,
         -4.8729e-02, -2.9322e-01, -4.6899e-01, -9.7830e-01],
        [-1.6425e+00, -8.8789e-01, -1.6809e+00, -4.3088e-03, -5.4148e-01,
         -1.4847e+00,  9.4480e-03, -2.9569e-01, -4.9256e-01, -1.0615e-01,
         -5.4828e-02, -3.4066e-01, -4.7517e-01, -1.0086e+00],
        [-1.8134e+00, -8.0764e-01, -1.7350e+00, -4.7855e-03, -5.5053e-01,
         -1.5650e+00,  9.9649e-03, -2.9394e-01, -4.7705e-01,  2.6795e-02,
         -5.8218e-02, -1.3428e-01, -4.6979e-01, -8.9623e-01],
        [-1.2505e+01, -7.3336e-01, -3.2735e+00, -5.9842e-02,  6.7509e+00,
         -4.0654e+00, -4.3504e-02,  1.9758e+00, -1.5629e+00, -1.3951e-02,
          9.6854e+00, -1.6159e-01, -5.9386e+00, -2.3652e+00],
        [-1.5676e+00, -8.0439e-01, -1.4532e+00, -3.6076e-03, -5.0693e-01,
         -1.4712e+00, -5.2878e-03, -3.4130e-01, -4.3575e-01, -2.6125e-01,
         -1.2130e-01, -4.6800e-01, -6.5256e-01, -8.8736e-01],
        [ 8.2695e+00, -5.7112e-01,  2.4714e-01,  6.1821e-01, -4.7719e+00,
          2.1515e+00, -2.9506e-01, -1.4226e+00,  3.7089e-01,  2.3451e-01,
         -4.7753e+00,  7.4271e-01,  8.6121e+00, -1.4871e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.2371, -2.3080, -2.8345, -2.2145, -1.0015, -2.1158,  0.2103, -2.2887,
        -2.3199, -2.2597, -2.3316, -4.8971, -2.2380,  3.0149], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-2.2650e-01, -1.5952e-01, -3.9398e-04, -8.5181e-02, -3.1301e+01,
          2.4178e+00, -4.8784e+00, -2.3147e-01, -1.7025e-01, -1.6760e-01,
         -1.4995e-01,  4.1207e+00, -8.9199e-02, -4.2727e-01],
        [ 5.2916e-01,  5.8734e-01,  1.4194e+01,  1.0760e-01,  1.4706e+00,
          8.4789e-02, -1.7324e+00,  5.3269e-01,  2.5748e-01,  2.3577e-01,
          2.3771e-01, -5.0574e+00,  1.3028e-01,  1.1685e+00],
        [-9.5763e-03, -8.0848e-03,  4.2148e-07, -1.3203e-02, -8.5676e-01,
         -8.2983e+00, -5.4498e-02, -9.6173e-03, -9.8609e-03, -1.0068e-02,
         -9.9653e-03, -4.4063e-01, -1.2795e-02, -1.1247e+01],
        [-9.1697e-03, -7.6634e-03, -2.6184e-06, -1.6489e-02, -9.1185e-01,
         -8.7290e+00, -6.9024e-02, -9.1482e-03, -1.0077e-02, -1.0468e-02,
         -1.0432e-02,  1.9131e-01, -1.5779e-02, -1.1503e+01],
        [-1.0604e-02, -4.4684e-03,  1.2164e-06, -3.1556e-02, -1.0329e+00,
         -6.7832e+00, -9.1445e-02, -9.8920e-03, -1.5737e-02, -1.6870e-02,
         -1.7234e-02,  5.1637e-01, -3.0080e-02, -1.0560e+01],
        [-3.4831e-01, -4.4832e-01, -1.3862e+01, -5.8370e-02,  2.3018e+00,
          6.4279e-01,  1.0704e+01, -3.6832e-01, -1.8108e-01, -1.5711e-01,
         -1.8296e-01, -1.0793e+00, -4.8276e-02,  1.5008e+00]], device='cuda:0'))])
xi:  [891.036]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1605.6549652627962
W_T_median: 1516.6834135661059
W_T_pctile_5: 895.4397533232669
W_T_CVAR_5_pct: 482.34570712426927
Average q (qsum/M+1):  52.536719537550404
Optimal xi:  [891.036]
Expected(across Rb) median(across samples) p_equity:  0.16038096243234273
obj fun:  tensor(-2110.6829, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: 3factor_mc
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
