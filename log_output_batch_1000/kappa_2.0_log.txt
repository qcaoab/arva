Starting at: 
2022-06-17 22:26:16
tracing parameter entered from terminal:  2.0


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = basic_T30_VWD
timeseries_basket['basket_desc'] = CRSP data: T30 and VWD
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
5.0% of MC simulations done.
10.0% of MC simulations done.
15.0% of MC simulations done.
20.0% of MC simulations done.
25.0% of MC simulations done.
30.0% of MC simulations done.
35.0% of MC simulations done.
40.0% of MC simulations done.
45.0% of MC simulations done.
50.0% of MC simulations done.
55.00000000000001% of MC simulations done.
60.0% of MC simulations done.
65.0% of MC simulations done.
70.0% of MC simulations done.
75.0% of MC simulations done.
80.0% of MC simulations done.
85.0% of MC simulations done.
90.0% of MC simulations done.
95.0% of MC simulations done.
100.0% of MC simulations done.
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  hidden_layer    None       None         None   
4        obj.layers[4]        4  hidden_layer    None       None         None   
5        obj.layers[5]        5  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  
4    False        None  
5    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  hidden_layer       4  logistic_sigmoid   
4        obj.layers[4]        4  hidden_layer       4  logistic_sigmoid   
5        obj.layers[5]        5  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 4)    False        None  
4       (4, 4)    False        None  
5       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
[-0.62277812  0.75294104  0.37108154 -0.23457045  0.87734683 -0.86163273
  0.02785573 -0.48145794 -0.54600176  0.39576202 -0.65000431 -0.36885567
  0.66919174 -0.62014212 -0.6576227  -0.09547811  0.44289714 -0.62745997
 -0.69254554 -0.71497204 -0.09160633 -0.70209214 -0.08649162 -0.03995664
  0.43014226  0.54196156 -0.18869499  0.3803182  -0.4213674   0.51582093
  0.719146   -0.41932646  0.60494673  0.54081414  0.80135845 -0.24869297
 -0.78078131  0.14937982  0.57187137  0.48684314 -0.81297698 -0.8032804
 -0.30872557  0.20747554 -0.27489946  0.56429979 -0.5811595  -0.16646221
 -0.44248831  0.58917257  0.54882857 -0.57567892 -0.30430046  0.43734894
 -0.41036775  0.8237008   0.69471977 -0.63290457 -0.75257106 -0.88717124
  0.98028969  0.25945333  0.60467509  0.43128316] (64,)
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.0022788489478
gradient value of function right now is: [ 2.93035467e-04  1.59288836e-05  2.92469351e-04  2.92708571e-04
 -2.93035467e-04 -1.59288836e-05 -2.92469351e-04 -2.92708571e-04
  5.53810963e-06  6.79829227e-07  5.38285267e-06  5.85345209e-06
  1.27386377e-05  1.56219021e-06  1.23813017e-05  1.34625952e-05
  5.62820876e-06  6.90836491e-07  5.47041950e-06  5.94863307e-06
  5.14758148e-06  6.31831058e-07  5.00325428e-06  5.44063153e-06
  4.77044571e-07  1.31585061e-07  5.55129879e-07  8.84933267e-08
  4.11813536e-07  1.14763810e-07  4.81671619e-07  7.56850647e-08
  2.52527082e-06  6.94795035e-07  2.93934563e-06  4.65241676e-07
  4.64134573e-07  1.23966895e-07  5.33959737e-07  8.62061540e-08
  5.96661391e-07  8.40919089e-07  3.48948415e-08  2.69381108e-07
  1.79097159e-07  2.57059859e-07 -9.17066432e-09  6.79812007e-08
  4.08179411e-07  5.68409674e-07  2.51097181e-08  1.70997041e-07
  3.06160604e-07  4.15702578e-07  1.78806258e-08  1.15331994e-07
  1.11412600e-07  1.89705166e-07  5.34932416e-08  2.07655834e-08
 -2.55172821e-08 -7.49528242e-09 -4.55184229e-08 -5.27234256e-08
 -2.15642805e+00]
supnorm grad right now is: 2.1564280525574837
Weights right now are: 
[-3.04396203 -0.51651044 -1.82964143 -2.55764762  3.29853074  0.40781875
  2.2285787   1.84161924 -2.14523863 -0.83653579 -2.08584258 -1.87565743
  1.76306409  0.45109722  0.26236724  0.9968872  -1.32367183 -0.39705488
 -2.2979522  -2.02570131 -2.07599739 -0.77931308 -2.05182689 -1.67335914
 -0.91773231 -0.74436846 -1.66220256 -0.8345892   0.47499437  1.45967105
  1.57007168  0.50788818 -0.96111022 -0.38390806 -1.34966451 -0.96954352
 -2.50395278 -1.3375399  -1.29544066 -0.97646349 -0.43495615 -1.07028069
  0.58013442  0.55160873  0.45929152  1.15637003 -0.38300078  0.40693133
 -1.47228767 -0.59982791 -0.42710548 -1.5883508   0.65371907  1.20859554
  0.55613055  1.7182019   1.22513062 -0.82578677 -0.10009583 -1.87412385
  2.03984557  1.72735083 -0.13782916  1.04060483 24.84963265]
(65,)
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.165009670872
gradient value of function right now is: [ 6.10221736e-07  5.93967036e-08  6.10082050e-07  6.10324416e-07
 -6.10221737e-07 -5.93967038e-08 -6.10082052e-07 -6.10324417e-07
  1.17287827e-08  1.41216596e-09  1.17045621e-08  1.24020709e-08
  5.38745547e-08  6.47782933e-09  5.37600173e-08  5.69587119e-08
  9.23086745e-09  1.11145282e-09  9.21182172e-09  9.76080028e-09
  8.60858567e-09  1.03651249e-09  8.59080247e-09  9.10277856e-09
 -1.47879167e-09 -4.09366401e-10 -1.69752684e-09 -2.69784248e-10
 -1.89926005e-10 -5.24007025e-11 -2.18654577e-10 -3.38713282e-11
  4.52792039e-09  1.22022039e-09  5.13930218e-09  8.32749141e-10
  3.60992153e-10  8.80800013e-11  3.95413321e-10  6.66493741e-11
  6.75479599e-10  9.57857880e-10  4.96723244e-12  2.90404560e-10
  2.15958149e-10  3.08152494e-10 -2.38472444e-11  6.92619378e-11
  4.83436551e-10  6.73499632e-10  1.21668082e-11  1.94087825e-10
  3.78761169e-10  5.12416960e-10  1.12438509e-11  1.35442715e-10
  1.43649019e-10  2.32412313e-10  7.60295307e-11  2.60330585e-11
 -3.20081636e-11 -9.86319135e-12 -5.06794421e-11 -6.20135613e-11
 -6.58499455e-01]
supnorm grad right now is: 0.6584994546815472
Weights right now are: 
[-4.03830985e+00 -6.01822317e-01 -2.80661485e+00 -3.63343332e+00
  4.29287865e+00  4.93130636e-01  3.20555221e+00  2.91740503e+00
 -2.20984158e+00 -8.43767451e-01 -2.17685118e+00 -1.92808838e+00
  1.53980040e+00  4.27921870e-01 -2.40382054e-02  8.16982270e-01
 -1.51078134e+00 -4.19828096e-01 -2.55778254e+00 -2.18813174e+00
 -2.25106928e+00 -8.03843636e-01 -2.31832701e+00 -1.83152425e+00
 -9.16570090e-01 -7.44070715e-01 -1.66119847e+00 -8.34544209e-01
  4.71111208e-01  1.45825069e+00  1.56354221e+00  5.06705596e-01
 -1.09798654e+00 -4.28377502e-01 -1.52002244e+00 -1.00903460e+00
 -2.51960846e+00 -1.34227413e+00 -1.31476706e+00 -9.81408303e-01
 -4.88338997e-01 -1.12990450e+00  5.78816826e-01  5.28565171e-01
  4.35442497e-01  1.12752889e+00 -3.80252487e-01  3.98312728e-01
 -1.49860551e+00 -6.28372141e-01 -4.28090705e-01 -1.59919983e+00
  6.32095560e-01  1.18559629e+00  5.55280910e-01  1.71018619e+00
  1.20616607e+00 -8.59538079e-01 -1.08256872e-01 -1.87782320e+00
  2.04308610e+00  1.72875143e+00 -1.33157327e-01  1.04727079e+00
  2.49499835e+01]
(65,)
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.180487526947
gradient value of function right now is: [ 1.06582615e-07  1.11363403e-08  1.06557757e-07  1.06599192e-07
 -1.06582998e-07 -1.11363811e-08 -1.06558139e-07 -1.06599574e-07
  2.15098966e-09  2.59015998e-10  2.14772549e-09  2.27485779e-09
  1.05548549e-08  1.26915616e-09  1.05380836e-08  1.11608901e-08
  1.74026479e-09  2.09565265e-10  1.73762724e-09  1.84048766e-09
  1.61371811e-09  1.94323631e-10  1.61126824e-09  1.70665006e-09
 -3.07655822e-10 -8.45916334e-11 -3.52316232e-10 -5.60706955e-11
 -5.12646342e-11 -1.41070465e-11 -5.88871121e-11 -9.20360740e-12
  8.63246970e-10  2.30248521e-10  9.76487545e-10  1.58376763e-10
  6.50656528e-11  1.54519705e-11  7.06718863e-11  1.19586971e-11
  1.25298676e-10  1.77978431e-10 -6.53358744e-13  5.23997562e-11
  4.09967047e-11  5.85080322e-11 -5.00167946e-12  1.25473945e-11
  9.02516985e-11  1.25838649e-10  1.47486631e-12  3.54142726e-11
  7.06409791e-11  9.56383815e-11  1.55386200e-12  2.47068618e-11
  2.66584487e-11  4.36404482e-11  1.40084563e-11  4.89875439e-12
 -5.87753009e-12 -1.96139299e-12 -9.36360717e-12 -1.17268669e-11
 -1.87370769e-02]
supnorm grad right now is: 0.018737076907602186
Weights right now are: 
[-4.32630617 -0.6310285  -3.09429019 -3.92256657  4.58087497  0.52233682
  3.49322755  3.20653828 -2.2156493  -0.84446671 -2.18265395 -1.93422169
  1.51224292  0.42460956 -0.05156303  0.78788223 -1.51542224 -0.42038707
 -2.56241547 -2.19303936 -2.2553855  -0.80436351 -2.32263582 -1.83608859
 -0.91578935 -0.74385598 -1.66030408 -0.83440196  0.47122657  1.45828246
  1.56367485  0.50672625 -1.10027678 -0.42898842 -1.5226134  -1.00945457
 -2.51978602 -1.34231651 -1.31496026 -0.98144092 -0.48867596 -1.13038343
  0.57881828  0.52842427  0.43533361  1.12737326 -0.38023931  0.39827921
 -1.49884768 -0.62871005 -0.42809478 -1.59929489  0.63190593  1.18533935
  0.5552767   1.71011988  1.20609447 -0.85965581 -0.10829458 -1.87783671
  2.04310175  1.72875704 -0.13313225  1.04730253 24.99051754]
(65,)
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.9071886709003
gradient value of function right now is: [ 5.64509152e-08  6.04763532e-09  5.64377295e-08  5.64594321e-08
 -5.64507655e-08 -6.04761912e-09 -5.64375798e-08 -5.64592824e-08
  1.15411118e-09  1.39209544e-10  1.15295073e-09  1.22090487e-09
  5.82157990e-09  7.00996712e-10  5.81506763e-09  6.15731088e-09
  9.41835059e-10  1.13609293e-10  9.40890538e-10  9.96347676e-10
  8.71768483e-10  1.05155885e-10  8.70891823e-10  9.22223928e-10
 -1.75499112e-10 -4.69938875e-11 -1.99264333e-10 -3.17738748e-11
 -3.17800207e-11 -8.65949785e-12 -3.63788001e-11 -5.70030528e-12
  4.72731448e-10  1.20294837e-10  5.26879406e-10  8.57542672e-11
  3.42671125e-11  7.27391755e-12  3.60358171e-11  6.16281216e-12
  6.76190514e-11  9.66178572e-11 -3.89090832e-12  2.47402932e-11
  2.22691598e-11  3.20052761e-11 -4.13512672e-12  5.38655643e-12
  4.90878892e-11  6.86672878e-11 -1.03660258e-12  1.73193314e-11
  3.85134170e-11  5.22683728e-11 -3.99087215e-13  1.21303364e-11
  1.42391455e-11  2.46292813e-11  7.43164336e-12  2.94732084e-12
 -3.03738232e-12 -1.41483624e-12 -4.93215636e-12 -6.83917854e-12
  2.78201416e+00]
supnorm grad right now is: 2.7820141573475103
Weights right now are: 
[-4.42605552 -0.64158475 -3.19401621 -4.02233124  4.68062432  0.53289307
  3.59295357  3.30630295 -2.21767909 -0.84471106 -2.18468076 -1.93636836
  1.50216364  0.42339795 -0.06162684  0.77722422 -1.51707156 -0.42058562
 -2.56406237 -2.19478366 -2.2569135  -0.80454745 -2.32416158 -1.83770458
 -0.91549267 -0.7437742  -1.65996404 -0.83434785  0.47127791  1.4582966
  1.56373384  0.50673548 -1.10109612 -0.42920793 -1.52354158 -1.00960504
 -2.51984714 -1.34233114 -1.31502681 -0.98145217 -0.48879432 -1.13055134
  0.5788183   0.52837419  0.43529471  1.12731784 -0.3802348   0.39826709
 -1.49893292 -0.62882878 -0.4280965  -1.59932865  0.63183923  1.18524913
  0.55527501  1.71009633  1.20606934 -0.85969682 -0.10830778 -1.87784124
  2.04310735  1.7287588  -0.13312339  1.04731353 25.14129703]
(65,)
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1738972884523
gradient value of function right now is: [ 4.04001868e-08  4.40478136e-09  4.03906742e-08  4.04061984e-08
 -4.03999942e-08 -4.40475970e-09 -4.03904816e-08 -4.04060057e-08
  8.36996729e-10  1.00743227e-10  8.35918686e-10  8.85233708e-10
  4.27711613e-09  5.14075062e-10  4.27132332e-09  4.52290041e-09
  6.86619438e-10  8.26462805e-11  6.85736311e-10  7.26192663e-10
  6.34866721e-10  7.64159067e-11  6.34048549e-10  6.71455998e-10
 -1.29148393e-10 -3.56211377e-11 -1.48027082e-10 -2.35683273e-11
 -2.42711064e-11 -6.68110956e-12 -2.78665417e-11 -4.37080130e-12
  3.42926370e-10  9.21020089e-11  3.88752575e-10  6.30259177e-11
  2.48436985e-11  5.96835998e-12  2.70721760e-11  4.58116179e-12
  4.87859129e-11  6.92660096e-11  8.40540602e-14  2.07880384e-11
  1.62014131e-11  2.30749318e-11 -1.78889309e-12  5.11482948e-12
  3.52319117e-11  4.91084194e-11  7.61734885e-13  1.40368181e-11
  2.75497827e-11  3.72932947e-11  7.30320504e-13  9.78388970e-12
  1.04201151e-11  1.69011225e-11  5.46002837e-12  1.86001353e-12
 -2.30442402e-12 -7.23248019e-13 -3.66727616e-12 -4.52070974e-12
 -4.35516523e-01]
supnorm grad right now is: 0.4355165225064732
Weights right now are: 
[-4.48770334 -0.64824933 -3.25564954 -4.08398831  4.74227214  0.53955765
  3.65458689  3.36796002 -2.21894974 -0.84486406 -2.18594974 -1.93771226
  1.4957223   0.42262347 -0.0680593   0.77041262 -1.5181112  -0.42071081
 -2.56510065 -2.19588324 -2.25787529 -0.80466326 -2.32512211 -1.83872182
 -0.9152994  -0.74372101 -1.65974269 -0.8343126   0.47131344  1.45830637
  1.56377463  0.50674187 -1.10161492 -0.42934664 -1.5241288  -1.00970033
 -2.51988502 -1.34234015 -1.31506795 -0.98145915 -0.48886835 -1.13065666
  0.57881868  0.52834293  0.43527021  1.12728285 -0.38023186  0.39825949
 -1.49898638 -0.62890342 -0.42809741 -1.59934984  0.63179743  1.18519246
  0.55527407  1.71008156  1.20605346 -0.85972264 -0.10831611 -1.87784411
  2.04311087  1.72875991 -0.1331178   1.04732045 24.94744864]
(65,)
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1804754328705
gradient value of function right now is: [ 3.06287948e-08  3.38018712e-09  3.06215719e-08  3.06332929e-08
 -3.06289726e-08 -3.38020748e-09 -3.06217496e-08 -3.06334708e-08
  6.38620529e-10  7.68845819e-11  6.37885783e-10  6.75465588e-10
  3.30465502e-09  3.97268882e-10  3.30061017e-09  3.49474658e-09
  5.25802167e-10  6.33044371e-11  5.25198224e-10  5.56140267e-10
  4.85814776e-10  5.84892913e-11  4.85255515e-10  5.13844730e-10
 -1.01063764e-10 -2.77490998e-11 -1.15663666e-10 -1.84236984e-11
 -1.96195632e-11 -5.39001405e-12 -2.25085885e-11 -3.53369748e-12
  2.63608634e-10  7.02604644e-11  2.98102044e-10  4.83579145e-11
  1.87954270e-11  4.43219660e-12  2.03666519e-11  3.45355692e-12
  3.72473525e-11  5.29365069e-11 -2.64219462e-13  1.55451154e-11
  1.24184425e-11  1.77031849e-11 -1.49750966e-12  3.78693486e-12
  2.69520461e-11  3.75880538e-11  4.11392739e-13  1.05570094e-11
  2.10820432e-11  2.85503077e-11  4.43212978e-13  7.36336598e-12
  7.94488680e-12  1.30059145e-11  4.15446613e-12  1.44439917e-12
 -1.74701884e-12 -5.83865320e-13 -2.78965359e-12 -3.49912679e-12
  2.84991996e-02]
supnorm grad right now is: 0.02849919964591364
Weights right now are: 
[-4.5325902  -0.65317226 -3.30052583 -4.1288818   4.78715899  0.54448058
  3.69946317  3.4128535  -2.21988245 -0.84497635 -2.18688134 -1.93869877
  1.49092584  0.42204684 -0.07284965  0.76534031 -1.51887772 -0.4208031
 -2.56586626 -2.19669398 -2.25858378 -0.80474856 -2.32582975 -1.83947118
 -0.91515356 -0.74368093 -1.65957573 -0.83428601  0.4713413   1.45831404
  1.5638066   0.50674689 -1.10199869 -0.42944898 -1.52456289 -1.00977072
 -2.51991259 -1.34234665 -1.31509783 -0.98146421 -0.48892279 -1.13073406
  0.578819    0.52832018  0.43525211  1.12725703 -0.38022969  0.39825396
 -1.49902572 -0.62895832 -0.42809802 -1.59936526  0.63176666  1.18515078
  0.5552734   1.7100708   1.20604186 -0.85974162 -0.1083222  -1.87784625
  2.0431134   1.7287608  -0.13311373  1.04732558 24.96912125]
(65,)
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.895073977005
gradient value of function right now is: [ 2.39495020e-08  2.66664547e-09  2.39438684e-08  2.39529740e-08
 -2.39497012e-08 -2.66666773e-09 -2.39440676e-08 -2.39531732e-08
  5.00430028e-10  6.03541987e-11  5.00058446e-10  5.29432632e-10
  2.62212255e-09  3.15688779e-10  2.61987237e-09  2.77354363e-09
  4.13165637e-10  4.98317048e-11  4.12859936e-10  4.37112674e-10
  3.81534954e-10  4.60160356e-11  3.81251596e-10  4.03647890e-10
 -8.17955239e-11 -2.18623378e-11 -9.28069743e-11 -1.48086112e-11
 -1.63589287e-11 -4.44028662e-12 -1.86943125e-11 -2.93806862e-12
  2.09100850e-10  5.31494249e-11  2.32960429e-10  3.79228137e-11
  1.45227483e-11  3.05373682e-12  1.52295307e-11  2.60970851e-12
  2.93101176e-11  4.19015228e-11 -1.75358991e-12  1.06797968e-11
  9.78579219e-12  1.40530385e-11 -1.81034069e-12  2.35537704e-12
  2.13516328e-11  2.98747921e-11 -4.79998401e-13  7.50985327e-12
  1.67464120e-11  2.27331270e-11 -1.94064830e-13  5.25964896e-12
  6.18290597e-12  1.07030121e-11  3.21521537e-12  1.27273778e-12
 -1.31582969e-12 -6.16447201e-13 -2.14099230e-12 -2.97629115e-12
  2.85372362e+00]
supnorm grad right now is: 2.8537236241553923
Weights right now are: 
[-4.56774076 -0.65707003 -3.33566809 -4.1640375   4.82230954  0.54837835
  3.73460542  3.44800919 -2.22061721 -0.84506481 -2.18761528 -1.93947594
  1.48710525  0.42158753 -0.07666573  0.76129989 -1.51948352 -0.42087604
 -2.56647138 -2.19733474 -2.25914335 -0.80481594 -2.3263887  -1.84006304
 -0.91503618 -0.74364873 -1.65944143 -0.83426461  0.47136436  1.45832037
  1.56383305  0.50675104 -1.10230278 -0.42952994 -1.52490662 -1.0098265
 -2.51993414 -1.34235172 -1.31512117 -0.98146817 -0.48896562 -1.13079494
  0.57881939  0.52830235  0.4352378   1.12723663 -0.38022794  0.39824962
 -1.49905675 -0.62900159 -0.42809846 -1.59937739  0.6317424   1.18511791
  0.55527292  1.71006234  1.20603271 -0.8597566  -0.10832698 -1.8778479
  2.04311541  1.72876147 -0.13311052  1.0473296  25.17181523]
(65,)
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.963981366168
gradient value of function right now is: [ 2.01739714e-08  2.26505422e-09  2.01692167e-08  2.01768731e-08
 -2.01742822e-08 -2.26509002e-09 -2.01695274e-08 -2.01771839e-08
  4.23574323e-10  5.10697061e-11  4.23261885e-10  4.48114849e-10
  2.23678968e-09  2.69225596e-10  2.23489427e-09  2.36592748e-09
  3.50518599e-10  4.22631503e-11  3.50260930e-10  3.70828066e-10
  3.23538475e-10  3.90094746e-11  3.23299751e-10  3.42284017e-10
 -7.01709727e-11 -1.88222691e-11 -7.97057741e-11 -1.27168482e-11
 -1.42945882e-11 -3.88400371e-12 -1.63391757e-11 -2.56917137e-12
  1.77548321e-10  4.54419795e-11  1.98229684e-10  3.22529947e-11
  1.22374201e-11  2.61421345e-12  1.28884272e-11  2.20602700e-12
  2.47825535e-11  3.54029920e-11 -1.30210467e-12  9.21590308e-12
  8.30138857e-12  1.19060105e-11 -1.45412043e-12  2.07515183e-12
  1.80520714e-11  2.52474200e-11 -3.10787185e-13  6.45108155e-12
  1.41521071e-11  1.92058235e-11 -1.00105556e-13  4.51503960e-12
  5.23859722e-12  8.99689378e-12  2.72388285e-12  1.05837785e-12
 -1.11950967e-12 -5.02706093e-13 -1.81766232e-12 -2.49142156e-12
  2.45880277e+00]
supnorm grad right now is: 2.4588027683396296
Weights right now are: 
[-4.59686462 -0.66032909 -3.36478506 -4.19316558  4.85143339  0.5516374
  3.76372239  3.47713726 -2.22122921 -0.84513847 -2.18822661 -1.94012324
  1.48389551  0.4212018  -0.07987171  0.75790554 -1.5199894  -0.42093692
 -2.56697671 -2.19786981 -2.25961039 -0.80487215 -2.32685523 -1.84055703
 -0.91493693 -0.74362141 -1.65932776 -0.83424651  0.47138428  1.45832584
  1.5638559   0.50675463 -1.10255695 -0.42959805 -1.52519455 -1.00987319
 -2.51995201 -1.34235598 -1.31514059 -0.98147146 -0.48900126 -1.13084556
  0.57881947  0.52828725  0.43522586  1.12721963 -0.38022659  0.39824588
 -1.49908255 -0.62903755 -0.42809896 -1.59938763  0.6317222   1.18509059
  0.55527243  1.7100552   1.20602511 -0.85976896 -0.10833097 -1.87784926
  2.04311711  1.72876201 -0.13310784  1.04733293 25.20068732]
(65,)
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.114548972153
gradient value of function right now is: [ 1.76173559e-08  1.99259951e-09  1.76131922e-08  1.76198750e-08
 -1.76175220e-08 -1.99261882e-09 -1.76133582e-08 -1.76200411e-08
  3.71716164e-10  4.47832800e-11  3.71408110e-10  3.93221620e-10
  1.97455354e-09  2.37507116e-10  1.97273327e-09  2.08841583e-09
  3.08199808e-10  3.71323995e-11  3.07945077e-10  3.26031843e-10
  2.84371276e-10  3.42609925e-11  2.84135480e-10  3.00824057e-10
 -6.20519719e-11 -1.68217569e-11 -7.07204211e-11 -1.12776287e-11
 -1.28146554e-11 -3.49799062e-12 -1.46678466e-11 -2.30698093e-12
  1.55920476e-10  4.06755596e-11  1.75121950e-10  2.84554886e-11
  1.07205730e-11  2.39638710e-12  1.14353134e-11  1.95013228e-12
  2.17008380e-11  3.09303249e-11 -6.89390602e-13  8.52979912e-12
  7.29368834e-12  1.04271616e-11 -1.08738295e-12  2.00990530e-12
  1.57806116e-11  2.20422462e-11 -3.67046048e-14  5.89090840e-12
  1.23571141e-11  1.67543519e-11  7.15443051e-14  4.11616179e-12
  4.61082178e-12  7.74165121e-12  2.40070639e-12  8.83343638e-13
 -9.98711991e-13 -3.92410409e-13 -1.60923180e-12 -2.11628612e-12
  1.34114576e+00]
supnorm grad right now is: 1.3411457577916688
Weights right now are: 
[-4.62143922 -0.66310009 -3.38935384 -4.21774371  4.87600799  0.5544084
  3.78829117  3.50171539 -2.22174769 -0.84520087 -2.18874455 -1.94067164
  1.4811555   0.42087252 -0.08260866  0.75500787 -1.52041888 -0.42098861
 -2.56740574 -2.19832408 -2.26000675 -0.80491985 -2.32725116 -1.84097625
 -0.91485164 -0.74359793 -1.65923006 -0.83423095  0.47140169  1.45833063
  1.56387588  0.50675777 -1.10277315 -0.42965596 -1.52543945 -1.00991289
 -2.51996707 -1.34235957 -1.31515695 -0.98147423 -0.48903148 -1.13088847
  0.57881951  0.52827446  0.43521571  1.12720518 -0.38022545  0.39824271
 -1.49910443 -0.62906805 -0.42809939 -1.5993963   0.6317051   1.18506743
  0.55527201  1.71004916  1.20601867 -0.85977948 -0.10833433 -1.87785041
  2.04311853  1.72876247 -0.13310557  1.04733576 25.11258691]
(65,)
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.178342810853
gradient value of function right now is: [ 1.56711503e-08  1.78406280e-09  1.56674372e-08  1.56733799e-08
 -1.56710606e-08 -1.78405267e-09 -1.56673475e-08 -1.56732902e-08
  3.32102701e-10  3.99816619e-11  3.31797196e-10  3.51289697e-10
  1.77321778e-09  2.13156153e-10  1.77144947e-09  1.87535091e-09
  2.75816830e-10  3.32066125e-11  2.75563641e-10  2.91753036e-10
  2.54410846e-10  3.06290264e-11  2.54176646e-10  2.69109760e-10
 -5.57905362e-11 -1.52800778e-11 -6.37929420e-11 -1.01673612e-11
 -1.16558545e-11 -3.19701764e-12 -1.33610619e-11 -2.10177072e-12
  1.39358786e-10  3.70182602e-11  1.57418818e-10  2.55445339e-11
  9.56479888e-12  2.22922130e-12  1.03267207e-11  1.75471259e-12
  1.93481770e-11  2.75168215e-11 -2.24505288e-13  7.99947737e-12
  6.52245604e-12  9.29604694e-12 -8.08032560e-13  1.95765303e-12
  1.40453227e-11  1.95944932e-11  1.70921304e-13  5.45935445e-12
  1.09858291e-11  1.48821524e-11  2.01432683e-13  3.80903616e-12
  4.13024744e-12  6.78508562e-12  2.15287106e-12  7.51813535e-13
 -9.05176935e-13 -3.10132641e-13 -1.44914146e-12 -1.83130744e-12
  2.33611297e-01]
supnorm grad right now is: 0.23361129689568957
Weights right now are: 
[-4.64294387 -0.66554059 -3.41085339 -4.23925142  4.89751264  0.55684891
  3.80979073  3.52322311 -2.22220269 -0.84525564 -2.18919913 -1.94115294
  1.47873377  0.42058141 -0.08502792  0.75244667 -1.52079645 -0.42103407
 -2.56778296 -2.19872346 -2.26035507 -0.80496178 -2.32759916 -1.8413447
 -0.91477567 -0.74357711 -1.65914317 -0.8342171   0.47141745  1.45833496
  1.56389395  0.50676062 -1.10296377 -0.42970661 -1.52565478 -1.00994785
 -2.51998021 -1.34236262 -1.31517113 -0.98147665 -0.489058   -1.13092618
  0.57881984  0.52826348  0.43520678  1.12719245 -0.38022432  0.39824004
 -1.49912368 -0.62909487 -0.42809962 -1.59940379  0.63169004  1.18504705
  0.55527173  1.71004393  1.20601301 -0.85978874 -0.10833729 -1.87785143
  2.04311979  1.72876287 -0.13310357  1.04733825 25.00085693]
(65,)
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.180473794628
gradient value of function right now is: [ 1.40064842e-08  1.60350494e-09  1.40031615e-08  1.40084666e-08
 -1.40063517e-08 -1.60348937e-09 -1.40030291e-08 -1.40083341e-08
  2.97739148e-10  3.58376471e-11  2.97466577e-10  3.14937363e-10
  1.59820245e-09  1.92084691e-10  1.59662186e-09  1.69024143e-09
  2.47638317e-10  2.98082540e-11  2.47412077e-10  2.61943531e-10
  2.28355999e-10  2.74868477e-11  2.28146785e-10  2.41546901e-10
 -5.04772306e-11 -1.38578335e-11 -5.77609045e-11 -9.20535437e-12
 -1.06679572e-11 -2.92921174e-12 -1.22321804e-11 -1.92470750e-12
  1.25208230e-10  3.34046699e-11  1.41630170e-10  2.29751997e-11
  8.54616702e-12  2.01086693e-12  9.25264673e-12  1.57109737e-12
  1.73308706e-11  2.46367341e-11 -1.17932920e-13  7.25158972e-12
  5.85529401e-12  8.33852834e-12 -6.88145659e-13  1.79280119e-12
  1.25810766e-11  1.75472105e-11  1.97403687e-13  4.93781234e-12
  9.83769065e-12  1.33245107e-11  2.10136774e-13  3.44375000e-12
  3.70514968e-12  6.05418030e-12  1.93074921e-12  6.65450211e-13
 -8.14149618e-13 -2.69007132e-13 -1.30161212e-12 -1.62895344e-12
 -2.45917712e-02]
supnorm grad right now is: 0.024591771214276595
Weights right now are: 
[-4.66199728 -0.66771571 -3.42990229 -4.25830755  4.91656605  0.55902403
  3.82883962  3.54227923 -2.22260711 -0.84530432 -2.18960317 -1.94158071
  1.47656891  0.42032124 -0.08719062  0.75015714 -1.52113256 -0.42107453
 -2.56811876 -2.19907899 -2.26066505 -0.80499909 -2.32790886 -1.84167259
 -0.91470746 -0.74355838 -1.65906511 -0.83420466  0.47143178  1.45833889
  1.56391039  0.5067632  -1.10313362 -0.42975186 -1.52584684 -1.009979
 -2.51999184 -1.34236535 -1.3151837  -0.98147878 -0.48908159 -1.13095966
  0.57882     0.52825366  0.43519881  1.12718113 -0.38022337  0.39823763
 -1.49914078 -0.6291187  -0.42809987 -1.59941046  0.63167666  1.18502895
  0.55527145  1.71003927  1.20600799 -0.85979693 -0.10833991 -1.87785232
  2.04312089  1.72876323 -0.13310181  1.04734046 24.99293657]
(65,)
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1501810321847
gradient value of function right now is: [ 1.27722113e-08  1.46985692e-09  1.27691753e-08  1.27740117e-08
 -1.27724231e-08 -1.46988120e-09 -1.27693870e-08 -1.27742236e-08
  2.72444821e-10  3.27739969e-11  2.72175256e-10  2.88164408e-10
  1.46847920e-09  1.76405038e-10  1.46693685e-09  1.55296812e-09
  2.26894987e-10  2.72954496e-11  2.26670867e-10  2.39987263e-10
  2.09176222e-10  2.51635349e-11  2.08969078e-10  2.21245700e-10
 -4.64212837e-11 -1.28513247e-11 -5.32628780e-11 -8.48495362e-12
 -9.89807877e-12 -2.72936638e-12 -1.13642730e-11 -1.78831664e-12
  1.14604409e-10  3.10198218e-11  1.30236032e-10  2.11051194e-11
  7.81075441e-12  1.89819690e-12  8.53854277e-12  1.44582678e-12
  1.58292648e-11  2.24644268e-11  1.48786141e-13  6.88804132e-12
  5.36075059e-12  7.61619575e-12 -5.22106313e-13  1.74850538e-12
  1.14752390e-11  1.59899339e-11  3.14945717e-13  4.65024393e-12
  8.96492740e-12  1.21343262e-11  2.82779692e-13  3.23937082e-12
  3.39795981e-12  5.45101665e-12  1.77336729e-12  5.83938407e-13
 -7.54198028e-13 -2.19391751e-13 -1.19919105e-12 -1.45104965e-12
 -9.09514192e-01]
supnorm grad right now is: 0.9095141923099003
Weights right now are: 
[-4.67896548 -0.66966288 -3.44686647 -4.27527814  4.93353424  0.56097119
  3.84580379  3.55924981 -2.22296815 -0.84534778 -2.1899639  -1.94196262
  1.47462568  0.42008766 -0.089132    0.74810196 -1.52143304 -0.4211107
 -2.56841898 -2.19939684 -2.26094211 -0.80503245 -2.32818567 -1.84196566
 -0.9146459  -0.74354152 -1.65899472 -0.83419344  0.47144486  1.45834248
  1.56392539  0.50676556 -1.10328573 -0.42979229 -1.52601869 -1.0100069
 -2.52000218 -1.34236776 -1.31519486 -0.98148068 -0.48910261 -1.13098955
  0.57882026  0.52824495  0.43519171  1.127171   -0.38022248  0.39823549
 -1.49915605 -0.62914    -0.42810005 -1.59941642  0.63166472  1.18501278
  0.55527123  1.71003512  1.2060035  -0.85980429 -0.10834226 -1.87785313
  2.04312188  1.72876357 -0.13310023  1.04734244 24.88410861]
(65,)
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1729995869523
gradient value of function right now is: [ 1.14441642e-08  1.32246280e-09  1.14414477e-08  1.14457679e-08
 -1.14439516e-08 -1.32243701e-09 -1.14412352e-08 -1.14455553e-08
  2.44280408e-10  2.94106096e-11  2.44086439e-10  2.58405507e-10
  1.32456381e-09  1.59229511e-10  1.32340479e-09  1.40091578e-09
  2.03671090e-10  2.45222448e-11  2.03509775e-10  2.15448831e-10
  1.87726201e-10  2.26021344e-11  1.87577019e-10  1.98581526e-10
 -4.22640958e-11 -1.15475873e-11 -4.82869427e-11 -7.69865853e-12
 -9.12153808e-12 -2.49828685e-12 -1.04497693e-11 -1.64523385e-12
  1.03357416e-10  2.73551132e-11  1.16614914e-10  1.89287461e-11
  6.95078252e-12  1.60277654e-12  7.48033108e-12  1.27294103e-12
  1.42206741e-11  2.02357931e-11 -2.28348045e-13  5.81966366e-12
  4.81999120e-12  6.87077193e-12 -6.17724898e-13  1.42235457e-12
  1.03425609e-11  1.44326705e-11  9.36339672e-14  3.98686728e-12
  8.09071657e-12  1.09626819e-11  1.26446740e-13  2.78263007e-12
  3.03500800e-12  5.00723790e-12  1.57900621e-12  5.55910337e-13
 -6.63267806e-13 -2.33610506e-13 -1.06390214e-12 -1.35559276e-12
  4.56175379e-01]
supnorm grad right now is: 0.4561753785267665
Weights right now are: 
[-4.69443115 -0.67144625 -3.46232846 -4.29074598  4.9489999   0.56275457
  3.86126577  3.57471765 -2.22329812 -0.84538749 -2.19029357 -1.94231165
  1.47284173  0.41987328 -0.09091426  0.74621526 -1.521708   -0.42114379
 -2.56869369 -2.19968767 -2.26119556 -0.80506295 -2.3284389  -1.84223375
 -0.91458922 -0.74352594 -1.65892984 -0.8341831   0.47145702  1.45834582
  1.56393933  0.50676775 -1.10342496 -0.4298295  -1.52617628 -1.01003245
 -2.5200116  -1.34236998 -1.31520507 -0.98148241 -0.48912181 -1.13101683
  0.57882034  0.52823688  0.4351852   1.12716175 -0.38022174  0.39823349
 -1.49916999 -0.62915943 -0.4281003  -1.5994219   0.63165383  1.18499802
  0.55527098  1.71003129  1.2059994  -0.85981099 -0.10834438 -1.87785387
  2.04312278  1.72876386 -0.13309879  1.04734424 25.01374814]
(65,)
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.156349535105
gradient value of function right now is: [ 1.04601826e-08  1.21370221e-09  1.04576995e-08  1.04616418e-08
 -1.04602421e-08 -1.21370930e-09 -1.04577590e-08 -1.04617014e-08
  2.23647955e-10  2.69316639e-11  2.23485771e-10  2.36588345e-10
  1.21827793e-09  1.46475481e-10  1.21728956e-09  1.28854321e-09
  1.86664938e-10  2.24789842e-11  1.86529962e-10  1.97466222e-10
  1.72017267e-10  2.07147447e-11  1.71892428e-10  1.81970630e-10
 -3.90666559e-11 -1.06381254e-11 -4.45852230e-11 -7.11024016e-12
 -8.50977515e-12 -2.32648322e-12 -9.74293678e-12 -1.53438260e-12
  9.49140054e-11  2.49790198e-11  1.06896827e-10  1.73584761e-11
  6.33608737e-12  1.44058206e-12  6.79073222e-12  1.15723236e-12
  1.30224698e-11  1.85432128e-11 -2.92333862e-13  5.24517865e-12
  4.41992501e-12  6.30504354e-12 -5.99447857e-13  1.27014830e-12
  9.48155999e-12  1.32357983e-11  4.23925492e-14  3.60833488e-12
  7.41953876e-12  1.00558295e-11  8.65655000e-14  2.51973457e-12
  2.77550523e-12  4.61090647e-12  1.44243482e-12  5.15707645e-13
 -6.04157437e-13 -2.22135248e-13 -9.71339409e-13 -1.25342571e-12
  8.00622780e-01]
supnorm grad right now is: 0.8006227799988732
Weights right now are: 
[-4.70860585 -0.67308779 -3.4764998  -4.30492267  4.96317461  0.5643961
  3.87543712  3.58889434 -2.22360111 -0.84542396 -2.19059631 -1.94263214
  1.47119595  0.41967549 -0.09255855  0.74447467 -1.52196075 -0.42117421
 -2.56894623 -2.19995503 -2.2614285  -0.80509099 -2.32867165 -1.84248015
 -0.91453669 -0.74351154 -1.65886976 -0.83417352  0.47146841  1.45834894
  1.56395237  0.50676981 -1.10355317 -0.42986363 -1.52632121 -1.01005596
 -2.52002021 -1.342372   -1.31521438 -0.981484   -0.48913944 -1.13104189
  0.57882049  0.52822956  0.43517922  1.12715324 -0.38022103  0.39823167
 -1.49918281 -0.6291773  -0.42810048 -1.5994269   0.6316438   1.18498445
  0.55527078  1.7100278   1.20599564 -0.85981715 -0.10834634 -1.87785455
  2.04312361  1.72876414 -0.13309747  1.0473459  25.0407818 ]
(65,)
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.3798668818986
gradient value of function right now is: [ 9.21283246e-09  1.07178297e-09  9.21065761e-09  9.21410730e-09
 -9.21283533e-09 -1.07178266e-09 -9.21066048e-09 -9.21411016e-09
  1.96432216e-10  2.37160908e-11  1.96392907e-10  2.07867931e-10
  1.07869959e-09  1.29979817e-10  1.07832341e-09  1.14124340e-09
  1.64097234e-10  1.98130003e-11  1.64064906e-10  1.73651301e-10
  1.51194867e-10  1.82548770e-11  1.51164634e-10  1.59997399e-10
 -3.52619657e-11 -9.22493383e-12 -3.97352700e-11 -6.35177851e-12
 -7.79925046e-12 -2.08795782e-12 -8.87021451e-12 -1.39842428e-12
  8.44095281e-11  2.07086896e-11  9.30273287e-11  1.51808202e-11
  5.47849251e-12  1.03835957e-12  5.58863565e-12  9.67258849e-13
  1.15047677e-11  1.65226639e-11 -1.13785128e-12  3.74409251e-12
  3.90070943e-12  5.62442356e-12 -8.88431797e-13  7.62052361e-13
  8.45338414e-12  1.18561158e-11 -4.20491530e-13  2.72793322e-12
  6.64147741e-12  9.03234870e-12 -2.33727908e-13  1.91695478e-12
  2.41069671e-12  4.34398791e-12  1.24261216e-12  5.36651107e-13
 -4.98675323e-13 -2.87930708e-13 -8.25758984e-13 -1.23574007e-12
  4.84126925e+00]
supnorm grad right now is: 4.8412692518095675
Weights right now are: 
[-4.72165237 -0.67460483 -3.48954322 -4.317971    4.97622113  0.56591314
  3.88848053  3.60194267 -2.22388055 -0.84545759 -2.19087553 -1.94292774
  1.46967196  0.41949234 -0.0940812   0.74286286 -1.5221941  -0.4212023
 -2.5691794  -2.20020187 -2.26164352 -0.80511687 -2.32888649 -1.8427076
 -0.91448788 -0.74349816 -1.65881393 -0.83416463  0.47147907  1.45835187
  1.56396459  0.50677173 -1.10367163 -0.42989523 -1.52645519 -1.01007768
 -2.52002813 -1.34237386 -1.31522296 -0.98148545 -0.48915567 -1.13106497
  0.57882061  0.52822278  0.4351737   1.12714538 -0.38022039  0.39822998
 -1.49919461 -0.62919377 -0.42810067 -1.59943153  0.63163457  1.18497194
  0.55527058  1.71002457  1.20599217 -0.85982284 -0.10834814 -1.87785518
  2.04312437  1.72876439 -0.13309625  1.04734744 25.28750086]
(65,)
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1314451580256
gradient value of function right now is: [ 9.19402643e-09  1.07543332e-09  9.19183546e-09  9.19530231e-09
 -9.19369426e-09 -1.07539356e-09 -9.19150338e-09 -9.19497009e-09
  1.97709399e-10  2.37779128e-11  1.97529145e-10  2.09119331e-10
  1.08310909e-09  1.30082251e-10  1.08205908e-09  1.14544147e-09
  1.65327618e-10  1.98841061e-11  1.65177152e-10  1.74869375e-10
  1.52301623e-10  1.83172050e-11  1.52162628e-10  1.61091309e-10
 -3.46950767e-11 -9.62525367e-12 -3.98340576e-11 -6.34615643e-12
 -7.64461799e-12 -2.11056072e-12 -8.77925847e-12 -1.38256740e-12
  8.38112559e-11  2.27743317e-11  9.53625748e-11  1.54493424e-11
  5.59947179e-12  1.37043333e-12  6.13382205e-12  1.03848902e-12
  1.14666586e-11  1.62671549e-11  1.56275974e-13  5.04207179e-12
  3.90740483e-12  5.54551337e-12 -3.56100862e-13  1.29529400e-12
  8.32123927e-12  1.15922770e-11  2.54522216e-13  3.40076850e-12
  6.49872928e-12  8.79501445e-12  2.22665164e-13  2.36841139e-12
  2.46576263e-12  3.93516822e-12  1.28521818e-12  4.16894911e-13
 -5.48425250e-13 -1.53537446e-13 -8.71345945e-13 -1.04482680e-12
 -1.15357171e+00]
supnorm grad right now is: 1.153571706583874
Weights right now are: 
[-4.73370854 -0.67601204 -3.50159652 -4.33002885  4.9882773   0.56732036
  3.90053383  3.61400052 -2.2241393  -0.84548873 -2.19113407 -1.94320144
  1.46825571  0.41932215 -0.09549621  0.74136501 -1.52241036 -0.42122833
 -2.56939549 -2.20043063 -2.26184276 -0.80514085 -2.32908557 -1.84291836
 -0.91444241 -0.74348567 -1.65876189 -0.83415633  0.47148907  1.45835461
  1.56397606  0.50677354 -1.10378149 -0.4299246  -1.52657953 -1.01009787
 -2.52003545 -1.34237558 -1.31523088 -0.9814868  -0.4891707  -1.13108633
  0.5788207   0.52821645  0.43516859  1.1271381  -0.38021981  0.3982284
 -1.49920555 -0.62920902 -0.42810085 -1.59943584  0.63162602  1.18496037
  0.55527039  1.71002157  1.20598894 -0.85982808 -0.10834982 -1.87785574
  2.04312508  1.72876461 -0.13309511  1.04734884 24.89322982]
(65,)
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.0672902186593
gradient value of function right now is: [ 8.29878706e-09  9.73123591e-10  8.29681711e-09  8.29993139e-09
 -8.29872611e-09 -9.73116270e-10 -8.29675618e-09 -8.29987044e-09
  1.78156473e-10  2.14660477e-11  1.78061131e-10  1.88483687e-10
  9.82223093e-10  1.18150950e-10  9.81598129e-10  1.03896572e-09
  1.49092452e-10  1.79648031e-11  1.49013010e-10  1.57735538e-10
  1.37325632e-10  1.65467162e-11  1.37252085e-10  1.45286299e-10
 -3.19145078e-11 -8.60707392e-12 -3.63090484e-11 -5.79477811e-12
 -7.11606133e-12 -1.93510468e-12 -8.13267753e-12 -1.28180239e-12
  7.62227619e-11  1.97353474e-11  8.54044325e-11  1.38857354e-11
  4.98783096e-12  1.08743280e-12  5.28165547e-12  9.03855905e-13
  1.03782454e-11  1.48111874e-11 -4.26188357e-13  3.98957084e-12
  3.53488780e-12  5.05464315e-12 -5.55920881e-13  9.38408130e-13
  7.58003600e-12  1.05947808e-11 -6.63977576e-14  2.78010140e-12
  5.93669777e-12  8.05390867e-12  1.22769466e-15  1.94404162e-12
  2.20552211e-12  3.73384141e-12  1.14314403e-12  4.27290303e-13
 -4.74424270e-13 -1.96439842e-13 -7.68187370e-13 -1.02700597e-12
  1.77283344e+00]
supnorm grad right now is: 1.7728334355855422
Weights right now are: 
[-4.7449107  -0.67732409 -3.51279601 -4.34123256  4.99947946  0.5686324
  3.91173333  3.62520423 -2.22438008 -0.84551771 -2.19137466 -1.94345614
  1.46693292  0.41916321 -0.09681788  0.739966   -1.52261179 -0.42125257
 -2.56959676 -2.2006437  -2.2620283  -0.80516317 -2.32927097 -1.84311462
 -0.9143998  -0.74347397 -1.65871313 -0.83414856  0.47149851  1.45835721
  1.56398688  0.50677524 -1.10388392 -0.42995195 -1.52669544 -1.01011666
 -2.52004223 -1.34237718 -1.31523823 -0.98148805 -0.4891847  -1.13110621
  0.57882077  0.52821059  0.43516382  1.12713132 -0.38021926  0.39822694
 -1.49921573 -0.6292232  -0.42810102 -1.59943983  0.63161806  1.18494959
  0.55527021  1.71001878  1.20598596 -0.85983296 -0.10835137 -1.87785628
  2.04312573  1.72876483 -0.13309407  1.04735015 25.10526732]
(65,)
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.038133120989
gradient value of function right now is: [ 7.76396020e-09  9.13260632e-10  7.76211703e-09  7.76502724e-09
 -7.76351819e-09 -9.13207838e-10 -7.76167513e-09 -7.76458517e-09
  1.66880581e-10  2.01096481e-11  1.66798898e-10  1.76558084e-10
  9.23256082e-10  1.11067852e-10  9.22707955e-10  9.76611450e-10
  1.39763359e-10  1.68425730e-11  1.39695280e-10  1.47868913e-10
  1.28715164e-10  1.55109426e-11  1.28652111e-10  1.36179714e-10
 -3.01048604e-11 -8.10208176e-12 -3.42271925e-11 -5.46334263e-12
 -6.75666087e-12 -1.83507690e-12 -7.71868656e-12 -1.21679036e-12
  7.15532856e-11  1.84629816e-11  8.00866675e-11  1.30241062e-11
  4.65640291e-12  1.00597186e-12  4.91805647e-12  8.42376581e-13
  9.72101047e-12  1.38794317e-11 -4.36742584e-13  3.69929732e-12
  3.31472015e-12  4.74190377e-12 -5.35566119e-13  8.65066163e-13
  7.10546027e-12  9.93378603e-12 -8.15302541e-14  2.58543814e-12
  5.56606103e-12  7.55244364e-12 -1.20061632e-14  1.80842203e-12
  2.06423653e-12  3.50943450e-12  1.06905100e-12  4.03487273e-13
 -4.42882118e-13 -1.87899233e-13 -7.18235159e-13 -9.67674800e-13
  1.98398771e+00]
supnorm grad right now is: 1.9839877093390907
Weights right now are: 
[-4.75548477 -0.67856657 -3.52336757 -4.35180809  5.01005353  0.56987489
  3.92230489  3.63577976 -2.22460766 -0.8455451  -2.19160208 -1.94369688
  1.46567822  0.4190124  -0.09807158  0.73863895 -1.52280232 -0.4212755
 -2.56978715 -2.20084526 -2.26220379 -0.8051843  -2.32944633 -1.84330026
 -0.91435923 -0.74346285 -1.65866674 -0.83414116  0.47150755  1.45835969
  1.56399725  0.50677688 -1.10398099 -0.42997775 -1.52680509 -1.01013446
 -2.52004863 -1.34237866 -1.31524512 -0.98148922 -0.48919792 -1.13112503
  0.57882093  0.52820511  0.4351593   1.12712489 -0.3802187   0.39822559
 -1.49922535 -0.62923664 -0.42810113 -1.59944358  0.63161053  1.18493939
  0.55527008  1.71001616  1.20598312 -0.85983758 -0.10835285 -1.87785678
  2.04312636  1.72876504 -0.13309307  1.0473514  25.10670225]
(65,)
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1318972335293
gradient value of function right now is: [ 7.38378030e-09  8.71372419e-10  7.38202455e-09  7.38479304e-09
 -7.38342955e-09 -8.71330371e-10 -7.38167389e-09 -7.38444224e-09
  1.59082009e-10  1.91592218e-11  1.58990960e-10  1.68296791e-10
  8.82070420e-10  1.06062692e-10  8.81484817e-10  9.32996727e-10
  1.33329848e-10  1.60583161e-11  1.33253826e-10  1.41053486e-10
  1.22773748e-10  1.47867134e-11  1.22703414e-10  1.29885637e-10
 -2.87424119e-11 -7.79868124e-12 -3.27629748e-11 -5.22739780e-12
 -6.47842680e-12 -1.76709778e-12 -7.41075295e-12 -1.16820810e-12
  6.81676924e-11  1.78412051e-11  7.66377542e-11  1.24508526e-11
  4.43880652e-12  9.92939597e-13  4.73448488e-12  8.08577121e-13
  9.25123490e-12  1.31861613e-11 -2.70562343e-13  3.66846060e-12
  3.15980314e-12  4.50996547e-12 -4.49887523e-13  8.84749798e-13
  6.75232270e-12  9.43101304e-12 -1.55586683e-15  2.53826010e-12
  5.28504238e-12  7.16612635e-12  3.99563192e-14  1.77322705e-12
  1.97193170e-12  3.29601872e-12  1.02245090e-12  3.70264817e-13
 -4.27460425e-13 -1.63355755e-13 -6.89154944e-13 -8.99923703e-13
  1.15277777e+00]
supnorm grad right now is: 1.1527777705704418
Weights right now are: 
[-4.76539328 -0.67973455 -3.53327372 -4.36171796  5.01996204  0.57104286
  3.93221103  3.64568963 -2.2248213  -0.84557081 -2.19181556 -1.94392287
  1.46449702  0.41887047 -0.09925183  0.73738967 -1.52298131 -0.42129704
 -2.56996601 -2.20103459 -2.26236861 -0.80520413 -2.32961103 -1.84347461
 -0.91432097 -0.74345234 -1.65862297 -0.83413418  0.47151613  1.45836204
  1.56400708  0.50677843 -1.10407216 -0.43000212 -1.52690827 -1.01015121
 -2.52005462 -1.34238007 -1.3152516  -0.98149032 -0.48921032 -1.13114265
  0.578821    0.52819989  0.43515507  1.12711887 -0.38021822  0.39822428
 -1.49923438 -0.62924922 -0.42810128 -1.59944714  0.63160347  1.18492983
  0.55526992  1.71001369  1.20598046 -0.8598419  -0.10835423 -1.87785725
  2.04312694  1.72876522 -0.13309213  1.04735256 25.08528602]
(65,)
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.0034375668206
gradient value of function right now is: [ 7.25831757e-09  8.59829040e-10  7.25658274e-09  7.25931380e-09
 -7.25847101e-09 -8.59847989e-10 -7.25673614e-09 -7.25946727e-09
  1.57164344e-10  1.88884488e-11  1.57015178e-10  1.66225173e-10
  8.71097472e-10  1.04555448e-10  8.70232592e-10  9.21187455e-10
  1.31818357e-10  1.58427908e-11  1.31693427e-10  1.39418389e-10
  1.21365403e-10  1.45862893e-11  1.21250087e-10  1.28362546e-10
 -2.80960406e-11 -7.86776399e-12 -3.23551982e-11 -5.15254425e-12
 -6.33003823e-12 -1.75693055e-12 -7.28126899e-12 -1.14705193e-12
  6.68810661e-11  1.84646854e-11  7.64923649e-11  1.23776802e-11
  4.41815050e-12  1.11904118e-12  4.89097205e-12  8.25709508e-13
  9.08972206e-12  1.28696621e-11  2.90127865e-13  4.16693192e-12
  3.11386286e-12  4.40624498e-12 -2.12376786e-13  1.10100460e-12
  6.59073685e-12  9.17095698e-12  2.89194708e-13  2.78716101e-12
  5.14198459e-12  6.95309507e-12  2.35564865e-13  1.93888690e-12
  1.96354303e-12  3.06920240e-12  1.02397071e-12  3.14246537e-13
 -4.41275059e-13 -1.04734674e-13 -6.97253453e-13 -8.04659881e-13
 -2.15219654e+00]
supnorm grad right now is: 2.1521965371349174
Weights right now are: 
[-4.77467833 -0.68083212 -3.54255655 -4.37100428  5.02924709  0.57214044
  3.94149387  3.65497595 -2.22502169 -0.84559493 -2.19201581 -1.94413485
  1.46338538  0.41873688 -0.10036263  0.73621392 -1.52314932 -0.42131726
 -2.57013391 -2.20121232 -2.26252331 -0.80522275 -2.32976563 -1.84363826
 -0.91428484 -0.74344245 -1.65858165 -0.83412759  0.47152428  1.45836428
  1.56401642  0.5067799  -1.10415788 -0.43002491 -1.52700513 -1.01016692
 -2.52006022 -1.34238137 -1.31525764 -0.98149135 -0.48922197 -1.13115921
  0.57882113  0.52819507  0.43515109  1.1271132  -0.38021774  0.39822308
 -1.49924286 -0.62926105 -0.42810139 -1.59945043  0.63159685  1.18492086
  0.5552698   1.71001138  1.20597797 -0.85984598 -0.10835552 -1.87785769
  2.04312749  1.72876541 -0.13309126  1.04735366 24.81143008]
(65,)
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.580215946988
gradient value of function right now is: [ 7.02433911e-09  8.34798179e-10  7.02265528e-09  7.02530251e-09
 -7.02449580e-09 -8.34817467e-10 -7.02281194e-09 -7.02545923e-09
  1.52588759e-10  1.83173713e-11  1.52413934e-10  1.61363206e-10
  8.46575749e-10  1.01511883e-10  8.45590473e-10  8.95148733e-10
  1.28065997e-10  1.53739935e-11  1.27919388e-10  1.35430677e-10
  1.17896328e-10  1.41529723e-11  1.17761093e-10  1.24675981e-10
 -2.71808192e-11 -7.74119122e-12 -3.14757003e-11 -5.00722037e-12
 -6.13615138e-12 -1.71924876e-12 -7.07960625e-12 -1.11492724e-12
  6.47260697e-11  1.83804647e-11  7.47196923e-11  1.20640640e-11
  4.30177181e-12  1.15867424e-12  4.85649478e-12  8.14916552e-13
  8.79424714e-12  1.24070697e-11  5.75414165e-13  4.32930270e-12
  3.01899832e-12  4.25204484e-12 -8.20410496e-14  1.18987366e-12
  6.35594326e-12  8.82623889e-12  4.34943808e-13  2.85328780e-12
  4.95030953e-12  6.68388496e-12  3.32151556e-13  1.98048746e-12
  1.91371466e-12  2.88181053e-12  1.00043308e-12  2.78753417e-13
 -4.37851977e-13 -7.21984111e-14 -6.85105866e-13 -7.37106334e-13
 -3.91650554e+00]
supnorm grad right now is: 3.9165055375643774
Weights right now are: 
[-4.78351614 -0.68187982 -3.55139225 -4.3798433   5.0380849   0.57318814
  3.95032957  3.66381497 -2.22521273 -0.84561792 -2.19220673 -1.94433695
  1.46232291  0.4186092  -0.10142429  0.73509018 -1.5233096  -0.42133656
 -2.57029408 -2.20138187 -2.26267088 -0.80524051 -2.32991309 -1.84379436
 -0.91425026 -0.74343296 -1.65854209 -0.83412128  0.47153212  1.45836643
  1.5640254   0.50678131 -1.10423967 -0.43004676 -1.52709767 -1.01018194
 -2.52006556 -1.34238263 -1.31526341 -0.98149233 -0.48923303 -1.13117497
  0.57882123  0.52819041  0.4351473   1.12710781 -0.3802173   0.39822191
 -1.49925092 -0.62927231 -0.42810152 -1.59945363  0.63159054  1.18491231
  0.55526967  1.71000916  1.20597559 -0.85984986 -0.10835676 -1.87785812
  2.04312801  1.72876558 -0.13309042  1.04735471 24.70191524]
(65,)
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1632801094056
gradient value of function right now is: [ 6.42605488e-09  7.64920478e-10  6.42452130e-09  6.42693107e-09
 -6.42647326e-09 -7.64971015e-10 -6.42493957e-09 -6.42734951e-09
  1.39253185e-10  1.67510567e-11  1.39150287e-10  1.47300235e-10
  7.77133794e-10  9.33490406e-11  7.76509179e-10  8.21911890e-10
  1.16937731e-10  1.40671537e-11  1.16851523e-10  1.23695665e-10
  1.07640786e-10  1.29485841e-11  1.07561160e-10  1.13861237e-10
 -2.53211121e-11 -6.99133559e-12 -2.90252622e-11 -4.62661031e-12
 -5.77625862e-12 -1.59070501e-12 -6.62728528e-12 -1.04464499e-12
  5.96434744e-11  1.60848542e-11  6.76967567e-11  1.09739230e-11
  3.87975758e-12  9.31223844e-13  4.22448836e-12  7.17002410e-13
  8.06730864e-12  1.14582605e-11  3.62948989e-14  3.47638568e-12
  2.76685416e-12  3.93007825e-12 -2.79108686e-13  8.87927244e-13
  5.87062220e-12  8.18362539e-12  1.41504719e-13  2.35986594e-12
  4.58648010e-12  6.21034030e-12  1.31237808e-13  1.64448200e-12
  1.73361336e-12  2.79224984e-12  9.01127285e-13  2.98494636e-13
 -3.83247865e-13 -1.15000975e-13 -6.11309005e-13 -7.46145018e-13
 -6.84145265e-01]
supnorm grad right now is: 0.6841452649400686
Weights right now are: 
[-4.79189606 -0.68287576 -3.55977017 -4.38822436  5.04646482  0.57418407
  3.95870749  3.67219603 -2.22539404 -0.84563974 -2.19238791 -1.94452874
  1.46131158  0.41848767 -0.10243489  0.73402052 -1.5234618  -0.42135487
 -2.57044618 -2.20154288 -2.26281098 -0.80525737 -2.33005311 -1.84394257
 -0.91421724 -0.74342391 -1.65850434 -0.83411526  0.47153965  1.45836849
  1.56403403  0.50678267 -1.10431744 -0.43006744 -1.52718554 -1.0101962
 -2.5200706  -1.3423838  -1.31526885 -0.98149326 -0.48924355 -1.13118993
  0.57882135  0.52818605  0.43514369  1.12710268 -0.38021687  0.39822083
 -1.49925859 -0.62928301 -0.42810161 -1.59945661  0.63158454  1.18490418
  0.55526955  1.71000708  1.20597334 -0.85985355 -0.10835793 -1.87785852
  2.04312851  1.72876574 -0.13308963  1.0473557  24.94744682]
(65,)
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.17098933881
gradient value of function right now is: [ 6.11113132e-09  7.29181577e-10  6.10967282e-09  6.11196247e-09
 -6.11073063e-09 -7.29133136e-10 -6.10927224e-09 -6.11156172e-09
  1.32542287e-10  1.59451534e-11  1.32449031e-10  1.40203962e-10
  7.41671548e-10  8.90955430e-11  7.41099894e-10  7.84418413e-10
  1.11365180e-10  1.33979469e-11  1.11287018e-10  1.17803115e-10
  1.02501813e-10  1.23314521e-11  1.02429609e-10  1.08427167e-10
 -2.42313669e-11 -6.67983547e-12 -2.77616191e-11 -4.42575780e-12
 -5.55434150e-12 -1.52820899e-12 -6.37070795e-12 -1.00435126e-12
  5.68644177e-11  1.52958433e-11  6.44887142e-11  1.04559338e-11
  3.68286199e-12  8.78410789e-13  4.00244434e-12  6.79782166e-13
  7.67816635e-12  1.09091199e-11  1.14664752e-14  3.28550032e-12
  2.63568500e-12  3.74500120e-12 -2.74914527e-13  8.36432336e-13
  5.59085953e-12  7.79492367e-12  1.22811185e-13  2.23453028e-12
  4.36858609e-12  5.91602687e-12  1.16863912e-13  1.55745910e-12
  1.64911336e-12  2.66501381e-12  8.56535000e-13  2.85899562e-13
 -3.63961618e-13 -1.11548830e-13 -5.81065922e-13 -7.13523206e-13
 -5.20388187e-01]
supnorm grad right now is: 0.520388186782932
Weights right now are: 
[-4.79986284 -0.68382504 -3.56773506 -4.39619223  5.0544316   0.57513335
  3.96667237  3.6801639  -2.22556663 -0.84566052 -2.1925604  -1.94471132
  1.46034651  0.41837168 -0.10339926  0.73299978 -1.52360678 -0.42137232
 -2.57059107 -2.20169625 -2.26294443 -0.80527344 -2.33018647 -1.84408374
 -0.91418568 -0.74341526 -1.65846824 -0.8341095   0.47154687  1.45837048
  1.5640423   0.50678398 -1.10439156 -0.43008719 -1.52726935 -1.0102098
 -2.5200754  -1.34238491 -1.31527403 -0.98149414 -0.48925356 -1.13120418
  0.57882145  0.52818187  0.43514026  1.12709779 -0.38021646  0.39821978
 -1.49926589 -0.6292932  -0.42810171 -1.59945947  0.63157885  1.18489645
  0.55526944  1.71000508  1.20597118 -0.85985706 -0.10835905 -1.8778589
  2.04312898  1.72876591 -0.13308887  1.04735665 24.97022821]
(65,)
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.111826584702
gradient value of function right now is: [ 5.70797319e-09  6.82285716e-10  5.70661432e-09  5.70874624e-09
 -5.70768642e-09 -6.82250901e-10 -5.70632762e-09 -5.70845943e-09
  1.23667332e-10  1.48950649e-11  1.23610310e-10  1.30836350e-10
  6.94985692e-10  8.35707035e-11  6.94599083e-10  7.35139870e-10
  1.03961079e-10  1.25220113e-11  1.03913373e-10  1.09988148e-10
  9.56781387e-11  1.15241656e-11  9.56339737e-11  1.01224816e-10
 -2.29153617e-11 -6.20262332e-12 -2.60997118e-11 -4.16557524e-12
 -5.29205483e-12 -1.44115721e-12 -6.04992432e-12 -9.54314976e-13
  5.33772559e-11  1.39196333e-11  5.99403552e-11  9.74087984e-12
  3.40924645e-12  7.54022622e-13  3.62427997e-12  6.19903951e-13
  7.18386999e-12  1.02455540e-11 -2.41862856e-13  2.81877648e-12
  2.46578438e-12  3.52017321e-12 -3.61269981e-13  6.78531637e-13
  5.25276231e-12  7.33879434e-12 -1.73133060e-14  1.95807968e-12
  4.11191531e-12  5.57688637e-12  2.00991596e-14  1.36836884e-12
  1.53106110e-12  2.56977361e-12  7.92467273e-13  2.89433986e-13
 -3.30865686e-13 -1.29879392e-13 -5.34567558e-13 -7.03717285e-13
  1.36908155e+00]
supnorm grad right now is: 1.369081550438877
Weights right now are: 
[-4.80743035 -0.68472885 -3.57530076 -4.40376076  5.0619991   0.57603716
  3.97423807  3.68773243 -2.22573073 -0.84568027 -2.1927244  -1.94488491
  1.45942657  0.41826113 -0.10431856  0.73202678 -1.52374469 -0.42138892
 -2.57072889 -2.20184214 -2.26307136 -0.80528871 -2.33031332 -1.84421802
 -0.91415551 -0.74340699 -1.65843375 -0.834104    0.47155381  1.45837238
  1.56405025  0.50678523 -1.10446212 -0.43010597 -1.52734909 -1.01022275
 -2.52007994 -1.34238597 -1.31527893 -0.98149498 -0.48926306 -1.13121772
  0.57882157  0.52817791  0.435137    1.12709314 -0.38021606  0.39821879
 -1.49927282 -0.62930287 -0.4281018  -1.59946219  0.63157342  1.1848891
  0.55526934  1.71000319  1.20596914 -0.8598604  -0.10836012 -1.87785926
  2.04312943  1.72876605 -0.13308815  1.04735755 25.0845545 ]
(65,)
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.16342796254
gradient value of function right now is: [ 5.50720879e-09  6.59900763e-10  5.50589608e-09  5.50795357e-09
 -5.50703478e-09 -6.59879741e-10 -5.50572211e-09 -5.50777954e-09
  1.19529300e-10  1.43901552e-11  1.19466144e-10  1.26452085e-10
  6.72828202e-10  8.08749968e-11  6.72415384e-10  7.11672014e-10
  1.00535416e-10  1.21039078e-11  1.00482502e-10  1.06358530e-10
  9.25165976e-11  1.11383214e-11  9.24676578e-11  9.78750686e-11
 -2.21693196e-11 -6.04034298e-12 -2.53033963e-11 -4.03688450e-12
 -5.13464487e-12 -1.40346006e-12 -5.87679741e-12 -9.26920685e-13
  5.15619961e-11  1.35985581e-11  5.81085673e-11  9.43514009e-12
  3.29545721e-12  7.49023201e-13  3.53085365e-12  6.02461016e-13
  6.93528422e-12  9.87648304e-12 -1.45252635e-13  2.80946136e-12
  2.38344388e-12  3.39609257e-12 -3.12443847e-13  6.91698426e-13
  5.06452927e-12  7.06992100e-12  2.92132659e-14  1.93620156e-12
  3.96186280e-12  5.37009482e-12  5.05532778e-14  1.35194948e-12
  1.48185059e-12  2.45388605e-12  7.67705491e-13  2.71271676e-13
 -3.22642293e-13 -1.16452204e-13 -5.19121054e-13 -6.66703193e-13
  6.77882959e-01]
supnorm grad right now is: 0.6778829589942368
Weights right now are: 
[-4.81469504 -0.68559851 -3.58256372 -4.41102644  5.06926379  0.57690682
  3.98150103  3.6949981  -2.22588844 -0.84569925 -2.19288201 -1.94505175
  1.45854045  0.41815465 -0.10520408  0.73108953 -1.52387731 -0.42140488
 -2.57086143 -2.20198243 -2.2631934  -0.8053034  -2.33043529 -1.84434712
 -0.91412641 -0.74339902 -1.65840047 -0.83409869  0.47156052  1.45837422
  1.56405795  0.50678645 -1.10453001 -0.43012405 -1.52742584 -1.0102352
 -2.5200843  -1.34238699 -1.31528364 -0.98149578 -0.48927221 -1.13123072
  0.57882165  0.5281741   0.43513386  1.12708867 -0.3802157   0.39821783
 -1.49927949 -0.62931218 -0.42810189 -1.5994648   0.63156821  1.18488204
  0.55526924  1.71000137  1.20596718 -0.8598636  -0.10836113 -1.87785962
  2.04312986  1.7287662  -0.13308747  1.04735842 25.06502596]
(65,)
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.179356811715
gradient value of function right now is: [ 5.33206406e-09  6.40463273e-10  5.33079118e-09  5.33278431e-09
 -5.33198689e-09 -6.40453375e-10 -5.33071404e-09 -5.33270713e-09
  1.15951900e-10  1.39517795e-11  1.15880285e-10  1.22659577e-10
  6.53586689e-10  7.85253095e-11  6.53135696e-10  6.91282002e-10
  9.75761748e-11  1.17411413e-11  9.75160884e-11  1.03221208e-10
  8.97851553e-11  1.08035094e-11  8.97296331e-11  9.49792837e-11
 -2.15044844e-11 -5.90781017e-12 -2.46097307e-11 -3.92440398e-12
 -4.99288189e-12 -1.37106119e-12 -5.72288912e-12 -9.02554700e-13
  4.99655812e-11  1.33648989e-11  5.65628113e-11  9.17463826e-12
  3.19899329e-12  7.52185355e-13  3.46168979e-12  5.88857278e-13
  6.71602131e-12  9.54883346e-12 -3.36710055e-14  2.82974421e-12
  2.31110819e-12  3.28591981e-12 -2.58004104e-13  7.15376033e-13
  4.89707759e-12  6.83017528e-12  8.46238310e-14  1.93260819e-12
  3.82760039e-12  5.18489090e-12  8.68811299e-14  1.34773984e-12
  1.44075843e-12  2.34459935e-12  7.47226684e-13  2.53392942e-13
 -3.16604066e-13 -1.01908103e-13 -5.06796503e-13 -6.30493101e-13
 -1.77530964e-01]
supnorm grad right now is: 0.17753096410153665
Weights right now are: 
[-4.82168534 -0.68643713 -3.58955236 -4.41801769  5.07625409  0.57774544
  3.98848966  3.70198934 -2.22604031 -0.84571753 -2.19303379 -1.94521241
  1.45768503  0.41805184 -0.10605894  0.73018475 -1.52400507 -0.42142026
 -2.57098912 -2.20211759 -2.26331097 -0.80531755 -2.33055279 -1.8444715
 -0.91409825 -0.74339131 -1.65836828 -0.83409356  0.47156705  1.45837601
  1.56406543  0.50678763 -1.10459549 -0.43014144 -1.5274998  -1.0102472
 -2.52008849 -1.34238795 -1.31528814 -0.98149654 -0.48928102 -1.13124327
  0.57882176  0.52817047  0.43513083  1.12708436 -0.38021533  0.39821693
 -1.49928591 -0.62932115 -0.42810197 -1.59946729  0.63156319  1.18487523
  0.55526915  1.70999963  1.20596529 -0.85986671 -0.10836211 -1.87785996
  2.04313027  1.72876634 -0.1330868   1.04735925 24.98261366]
(65,)
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3589.4633794870465
gradient value of function right now is: [ 4.69241269e-09  5.63541837e-10  4.69130464e-09  4.69304028e-09
 -4.69254763e-09 -5.63557751e-10 -4.69143955e-09 -4.69317524e-09
  1.01273545e-10  1.22435682e-11  1.01304765e-10  1.07196675e-10
  5.76446742e-10  6.95348927e-11  5.76515882e-10  6.10005604e-10
  8.52536828e-11  1.03073077e-11  8.52802786e-11  9.02403226e-11
  7.84411929e-11  9.48350052e-12  7.84654176e-11  8.30291738e-11
 -1.95480827e-11 -4.98538353e-12 -2.18533972e-11 -3.49952202e-12
 -4.61259233e-12 -1.21462424e-12 -5.21769267e-12 -8.24380864e-13
  4.45308268e-11  1.04650431e-11  4.84546066e-11  7.92920960e-12
  2.72133523e-12  4.48122284e-13  2.68320407e-12  4.69939777e-13
  5.93090371e-12  8.56212380e-12 -8.57590646e-13  1.65747455e-12
  2.03470698e-12  2.94870530e-12 -5.65774684e-13  2.90556038e-13
  4.39545885e-12  6.18113909e-12 -3.57161366e-13  1.26981453e-12
  3.46101696e-12  4.71629415e-12 -2.16076126e-13  8.96313548e-13
  1.23081274e-12  2.32528723e-12  6.28892111e-13  3.00527503e-13
 -2.46019187e-13 -1.76972344e-13 -4.16075154e-13 -6.77994864e-13
  7.18312817e+00]
supnorm grad right now is: 7.183128174840691
Weights right now are: 
[-4.82834943 -0.68723837 -3.59621486 -4.42468268  5.08291819  0.57854669
  3.99515217  3.70865434 -2.22618527 -0.84573497 -2.19317867 -1.94536576
  1.45686695  0.41795353 -0.10687648  0.72931947 -1.52412709 -0.42143495
 -2.57111107 -2.20224667 -2.26342324 -0.80533107 -2.33066499 -1.84459027
 -0.91407129 -0.74338391 -1.65833745 -0.83408864  0.47157333  1.45837773
  1.56407261  0.50678876 -1.10465803 -0.43015812 -1.52757052 -1.01025868
 -2.52009249 -1.34238888 -1.31529246 -0.98149728 -0.48928941 -1.13125522
  0.57882184  0.52816695  0.43512794  1.12708024 -0.38021499  0.39821605
 -1.49929204 -0.6293297  -0.42810205 -1.59946969  0.6315584   1.18486874
  0.55526905  1.70999795  1.20596349 -0.85986965 -0.10836304 -1.87786028
  2.04313067  1.72876647 -0.13308617  1.04736005 25.47725749]
(65,)
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.125013233486
gradient value of function right now is: [ 4.98602100e-09  6.01530997e-10  4.98482839e-09  4.98669262e-09
 -4.98597020e-09 -6.01525148e-10 -4.98477760e-09 -4.98664181e-09
  1.08748869e-10  1.30762028e-11  1.08671008e-10  1.15031261e-10
  6.14928148e-10  7.38376625e-11  6.14452607e-10  6.50352870e-10
  9.16007840e-11  1.10146363e-11  9.15353452e-11  9.68928623e-11
  8.42727822e-11  1.01333309e-11  8.42123678e-11  8.91413389e-11
 -2.02215924e-11 -5.61156124e-12 -2.32170063e-11 -3.70024663e-12
 -4.72084581e-12 -1.30393053e-12 -5.42103202e-12 -8.54862842e-13
  4.68368426e-11  1.27413562e-11  5.33096928e-11  8.63628494e-12
  2.99886701e-12  7.33112353e-13  3.28324994e-12  5.56570492e-13
  6.28536450e-12  8.91836224e-12  8.96334430e-14  2.77239652e-12
  2.16754742e-12  3.07336973e-12 -1.91212384e-13  7.21320080e-13
  4.57486293e-12  6.37358897e-12  1.42766148e-13  1.87417855e-12
  3.57200166e-12  4.83474138e-12  1.24266841e-13  1.30528681e-12
  1.35455690e-12  2.15705748e-12  7.03803161e-13  2.26063394e-13
 -3.01131364e-13 -8.31174108e-14 -4.78994753e-13 -5.72725841e-13
 -1.22294834e+00]
supnorm grad right now is: 1.2229483439047337
Weights right now are: 
[-4.83479001 -0.68801437 -3.6026539  -4.43112412  5.08935878  0.57932269
  4.00159122  3.7150958  -2.22632553 -0.84575185 -2.19331884 -1.94551413
  1.45607391  0.41785825 -0.10766898  0.72848068 -1.5242452  -0.42144916
 -2.57122911 -2.20237162 -2.26353191 -0.80534414 -2.3307736  -1.84470522
 -0.91404514 -0.74337673 -1.65830751 -0.83408386  0.47157943  1.45837941
  1.56407961  0.50678986 -1.10471856 -0.43017432 -1.52763906 -1.0102698
 -2.52009635 -1.34238979 -1.31529663 -0.98149799 -0.48929752 -1.13126677
  0.57882189  0.52816352  0.43512515  1.12707626 -0.38021468  0.39821518
 -1.49929796 -0.62933796 -0.42810215 -1.59947204  0.63155378  1.18486247
  0.55526895  1.70999632  1.20596174 -0.85987248 -0.10836395 -1.87786059
  2.04313105  1.7287666  -0.13308555  1.04736081 24.914675  ]
(65,)
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.148761685199
gradient value of function right now is: [ 4.68255033e-09  5.65652842e-10  4.68143359e-09  4.68317850e-09
 -4.68242859e-09 -5.65637879e-10 -4.68131188e-09 -4.68305675e-09
  1.01969927e-10  1.22775231e-11  1.01924483e-10  1.07879553e-10
  5.78992894e-10  6.96016111e-11  5.78683378e-10  6.12439184e-10
  8.59249144e-11  1.03460198e-11  8.58868013e-11  9.09050020e-11
  7.90455123e-11  9.51754497e-12  7.90102387e-11  8.36267204e-11
 -1.92269863e-11 -5.22554507e-12 -2.19269343e-11 -3.49911046e-12
 -4.52002997e-12 -1.23354075e-12 -5.17046393e-12 -8.15854249e-13
  4.42024103e-11  1.16118494e-11  4.97521205e-11  8.08071139e-12
  2.78809170e-12  6.26726474e-13  2.97754191e-12  5.08723891e-13
  5.91252607e-12  8.42525133e-12 -1.51507505e-13  2.36852646e-12
  2.03830962e-12  2.90560679e-12 -2.77073988e-13  5.80870014e-13
  4.32365601e-12  6.03773673e-12  1.08283136e-14  1.63837084e-12
  3.38293238e-12  4.58664488e-12  3.35122929e-14  1.14432587e-12
  1.26286278e-12  2.10107124e-12  6.53293258e-13  2.33360537e-13
 -2.74106379e-13 -1.01918569e-13 -4.41902269e-13 -5.72610713e-13
  9.25776855e-01]
supnorm grad right now is: 0.9257768551626303
Weights right now are: 
[-4.84095468 -0.68875861 -3.60881709 -4.43728962  5.09552345  0.58006693
  4.00775442  3.7212613  -2.22645991 -0.84576802 -2.19345314 -1.94565629
  1.45531264  0.41776679 -0.10842974  0.7276755  -1.52435842 -0.42146278
 -2.57134225 -2.20249138 -2.26363606 -0.80535667 -2.33087769 -1.8448154
 -0.91401999 -0.74336981 -1.65827873 -0.83407927  0.47158532  1.45838103
  1.56408636  0.50679093 -1.10477659 -0.43018989 -1.52770481 -1.01028046
 -2.52010004 -1.34239067 -1.31530064 -0.98149867 -0.48930529 -1.13127781
  0.57882191  0.52816022  0.43512247  1.12707246 -0.38021439  0.39821433
 -1.49930363 -0.62934587 -0.42810227 -1.5994743   0.63154935  1.18485647
  0.55526884  1.70999475  1.20596007 -0.85987519 -0.10836482 -1.87786088
  2.04313142  1.72876671 -0.13308497  1.04736154 25.05747102]
(65,)
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1665992768026
gradient value of function right now is: [ 4.60048947e-09  5.57026463e-10  4.59938961e-09  4.60110654e-09
 -4.60010465e-09 -5.56979220e-10 -4.59900488e-09 -4.60072166e-09
  1.00434119e-10  1.20807653e-11  1.00371988e-10  1.06242070e-10
  5.70488284e-10  6.85221936e-11  5.70097631e-10  6.03382174e-10
  8.46679971e-11  1.01846689e-11  8.46157659e-11  8.95645288e-11
  7.78835215e-11  9.36843510e-12  7.78352749e-11  8.23875428e-11
 -1.88636832e-11 -5.20389923e-12 -2.16161981e-11 -3.44652182e-12
 -4.43771888e-12 -1.22153942e-12 -5.09018030e-12 -8.02941993e-13
  4.34082351e-11  1.16944011e-11  4.92522906e-11  7.98475034e-12
  2.75460957e-12  6.58021033e-13  2.99474623e-12  5.08838527e-13
  5.80745468e-12  8.25073625e-12  1.71838445e-14  2.49541212e-12
  2.00512112e-12  2.84721548e-12 -2.03405736e-13  6.40357890e-13
  4.23416856e-12  5.90304528e-12  9.77924763e-14  1.69790811e-12
  3.30789243e-12  4.47959905e-12  9.17226196e-14  1.18338862e-12
  1.24880018e-12  2.01363153e-12  6.47699999e-13  2.14565297e-13
 -2.75720244e-13 -8.32952325e-14 -4.40296022e-13 -5.38745957e-13
 -6.23493463e-01]
supnorm grad right now is: 0.6234934633378592
Weights right now are: 
[-4.84688946 -0.68947649 -3.61475045 -4.44322519  5.10145821  0.58078481
  4.01368777  3.72719686 -2.22658941 -0.8457836  -2.19358256 -1.94579328
  1.45457769  0.41767852 -0.10916419  0.72689818 -1.52446757 -0.42147591
 -2.57145134 -2.20260684 -2.26373647 -0.80536875 -2.33097803 -1.84492161
 -0.9139957  -0.74336312 -1.65825091 -0.83407484  0.47159103  1.45838259
  1.56409291  0.50679196 -1.10483254 -0.43020495 -1.52776827 -1.01029075
 -2.52010359 -1.34239152 -1.3153045  -0.98149933 -0.48931278 -1.13128845
  0.5788219   0.52815701  0.43511988  1.12706879 -0.38021413  0.39821351
 -1.49930909 -0.62935348 -0.42810239 -1.59947648  0.63154508  1.18485069
  0.55526873  1.70999322  1.20595846 -0.85987779 -0.10836565 -1.87786116
  2.04313177  1.72876682 -0.1330844   1.04736224 24.92377346]
(65,)
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.7269439992315
gradient value of function right now is: [ 4.22425036e-09  5.11759688e-10  4.22324656e-09  4.22481344e-09
 -4.22471718e-09 -5.11817053e-10 -4.22371327e-09 -4.22528033e-09
  9.18606562e-11  1.10794940e-11  9.18522893e-11  9.72064876e-11
  5.25020611e-10  6.32057009e-11  5.24904186e-10  5.55457054e-10
  7.74664704e-11  9.34375868e-12  7.74596282e-11  8.19749853e-11
  7.12538125e-11  8.59426839e-12  7.12473163e-11  7.54005992e-11
 -1.76695120e-11 -4.67485016e-12 -1.99789954e-11 -3.19356326e-12
 -4.19893882e-12 -1.12832424e-12 -4.77956581e-12 -7.54821954e-13
  4.01761839e-11  1.00790973e-11  4.45766254e-11  7.26428623e-12
  2.48121084e-12  4.94067236e-13  2.56276179e-12  4.42594095e-13
  5.34576649e-12  7.66189945e-12 -4.10374842e-13  1.86683166e-12
  1.84307099e-12  2.64591969e-12 -3.62513321e-13  4.13719513e-13
  3.93341224e-12  5.51052129e-12 -1.32882257e-13  1.33937009e-12
  3.08562643e-12  4.19353242e-12 -6.62990139e-14  9.39226667e-13
  1.12967124e-12  1.98304649e-12  5.81443759e-13  2.35676966e-13
 -2.37298431e-13 -1.20287319e-13 -3.89894381e-13 -5.57365263e-13
  3.61956726e+00]
supnorm grad right now is: 3.6195672572483346
Weights right now are: 
[-4.85266789 -0.69017673 -3.6205275  -4.4490044   5.10723664  0.58148504
  4.01946481  3.73297607 -2.22671559 -0.84579877 -2.19370866 -1.94592675
  1.45386017  0.41759234 -0.10988123  0.72613929 -1.52457396 -0.42148871
 -2.57155767 -2.20271939 -2.26383433 -0.80538052 -2.33107584 -1.84502513
 -0.91397194 -0.74335658 -1.6582237  -0.8340705   0.47159663  1.45838413
  1.56409933  0.50679297 -1.10488711 -0.43021962 -1.52783015 -1.01030078
 -2.52010705 -1.34239234 -1.31530825 -0.98149996 -0.48932007 -1.13129881
  0.57882189  0.5281539   0.43511736  1.12706521 -0.38021387  0.39821271
 -1.49931441 -0.6293609  -0.42810251 -1.5994786   0.63154092  1.18484506
  0.55526862  1.70999175  1.2059569  -0.85988033 -0.10836646 -1.87786143
  2.04313212  1.72876693 -0.13308385  1.04736292 25.2595428 ]
(65,)
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.777151540805
gradient value of function right now is: [ 4.42825424e-09  5.38492244e-10  4.42719115e-09  4.42884781e-09
 -4.42791667e-09 -5.38450770e-10 -4.42685366e-09 -4.42851019e-09
  9.71040288e-11  1.16606627e-11  9.70160046e-11  1.02698659e-10
  5.52171778e-10  6.62274240e-11  5.51655418e-10  5.83908998e-10
  8.19287918e-11  9.83864271e-12  8.18546149e-11  8.66493618e-11
  7.53527411e-11  9.04882245e-12  7.52843399e-11  7.96942800e-11
 -1.81322965e-11 -5.12875693e-12 -2.09484711e-11 -3.33497708e-12
 -4.27372583e-12 -1.19378490e-12 -4.92516957e-12 -7.76485834e-13
  4.17679764e-11  1.17282190e-11  4.80358134e-11  7.76292837e-12
  2.67578199e-12  7.02175565e-13  2.99506098e-12  5.04316124e-13
  5.58862838e-12  7.89784512e-12  2.88178173e-13  2.67590645e-12
  1.93494918e-12  2.72887591e-12 -8.28143305e-14  7.30246561e-13
  4.05444725e-12  5.63550967e-12  2.36606764e-13  1.77770510e-12
  3.15953827e-12  4.26920594e-12  1.84372293e-13  1.23512284e-12
  1.21444379e-12  1.85535490e-12  6.32505537e-13  1.82455563e-13
 -2.75571045e-13 -5.31144384e-14 -4.33452664e-13 -4.79750186e-13
 -3.20671470e+00]
supnorm grad right now is: 3.206714695667658
Weights right now are: 
[-4.85821509 -0.69085007 -3.62607338 -4.45455235  5.11278385  0.58215838
  4.0250107   3.73852402 -2.22683676 -0.84581335 -2.19382977 -1.94605494
  1.45316959  0.41750938 -0.11057139  0.72540887 -1.52467617 -0.42150101
 -2.57165982 -2.20282751 -2.26392834 -0.80539183 -2.33116979 -1.84512458
 -0.91394901 -0.74335028 -1.65819747 -0.83406632  0.47160205  1.45838562
  1.56410554  0.50679395 -1.10493962 -0.43023365 -1.52788957 -1.01031042
 -2.52011035 -1.34239312 -1.31531183 -0.98150057 -0.48932708 -1.13130876
  0.57882193  0.52815096  0.43511494  1.12706177 -0.3802136   0.39821196
 -1.49931953 -0.62936803 -0.42810259 -1.59948062  0.63153692  1.18483964
  0.55526853  1.70999034  1.2059554  -0.85988277 -0.10836724 -1.87786169
  2.04313245  1.72876703 -0.13308332  1.04736357 24.73895288]
(65,)
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.159197225415
gradient value of function right now is: [ 4.16382807e-09  5.06835272e-10  4.16283185e-09  4.16438387e-09
 -4.16387133e-09 -5.06841011e-10 -4.16287509e-09 -4.16442713e-09
  9.11267728e-11  1.09599244e-11  9.10719705e-11  9.63964199e-11
  5.20368243e-10  6.24954728e-11  5.20021924e-10  5.50372596e-10
  7.69114058e-11  9.25053102e-12  7.68652820e-11  8.13592920e-11
  7.07335189e-11  8.50736337e-12  7.06909212e-11  7.48239944e-11
 -1.72726560e-11 -4.77141988e-12 -1.98013945e-11 -3.15713873e-12
 -4.09940511e-12 -1.12933305e-12 -4.70318161e-12 -7.42025014e-13
  3.94795627e-11  1.06611321e-11  4.48284642e-11  7.26643236e-12
  2.48822201e-12  5.97236798e-13  2.70892777e-12  4.60172929e-13
  5.26461430e-12  7.47788465e-12  2.90783431e-14  2.27685137e-12
  1.82164609e-12  2.58543086e-12 -1.78497557e-13  5.87632824e-13
  3.83932218e-12  5.35191701e-12  9.58741763e-14  1.54770367e-12
  2.99889495e-12  4.06084963e-12  8.80311069e-14  1.07851485e-12
  1.13318848e-12  1.82138063e-12  5.87649895e-13  1.92950471e-13
 -2.50562490e-13 -7.40688443e-14 -3.99865376e-13 -4.86530197e-13
 -7.60724791e-01]
supnorm grad right now is: 0.7607247914723667
Weights right now are: 
[-4.86359284 -0.69150405 -3.63144985 -4.45993081  5.11816161  0.58281237
  4.03038717  3.74390249 -2.22695437 -0.8458275  -2.19394732 -1.94617935
  1.45249839  0.41742875 -0.11124216  0.72469896 -1.52477542 -0.42151295
 -2.57175901 -2.2029325  -2.26401962 -0.80540281 -2.33126102 -1.84522114
 -0.91392673 -0.74334415 -1.65817194 -0.83406225  0.47160734  1.45838707
  1.5641116   0.50679491 -1.10499059 -0.43024735 -1.52794736 -1.01031979
 -2.52011357 -1.34239388 -1.31531532 -0.98150117 -0.48933387 -1.13131843
  0.57882194  0.52814805  0.43511259  1.12705843 -0.38021335  0.39821122
 -1.49932448 -0.62937495 -0.42810269 -1.5994826   0.63153304  1.18483439
  0.55526843  1.70998896  1.20595394 -0.85988514 -0.108368   -1.87786195
  2.04313277  1.72876713 -0.1330828   1.04736421 24.92858107]
(65,)
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.179893091079
gradient value of function right now is: [ 4.00461198e-09  4.88166811e-10  4.00365458e-09  4.00514534e-09
 -4.00471583e-09 -4.88179789e-10 -4.00375840e-09 -4.00524921e-09
  8.76426078e-11  1.05449364e-11  8.75974717e-11  9.27157143e-11
  5.01643606e-10  6.02662239e-11  5.01348445e-10  5.30592674e-10
  7.39966374e-11  8.90339056e-12  7.39586647e-11  7.82801393e-11
  6.80486547e-11  8.18760108e-12  6.80135586e-11  7.19877081e-11
 -1.67138251e-11 -4.58871882e-12 -1.91225302e-11 -3.05009799e-12
 -3.98200091e-12 -1.09301469e-12 -4.56312650e-12 -7.20095185e-13
  3.80638332e-11  1.01744480e-11  4.30795136e-11  6.98820280e-12
  2.38454761e-12  5.58472786e-13  2.57707699e-12  4.38806936e-13
  5.06744467e-12  7.20669057e-12 -3.12617755e-14  2.13111590e-12
  1.75407800e-12  2.49325287e-12 -1.96186713e-13  5.41204677e-13
  3.70110088e-12  5.16268130e-12  6.12308933e-14  1.45833078e-12
  2.89268327e-12  3.91893256e-12  6.37778965e-14  1.01710103e-12
  1.08795623e-12  1.77139643e-12  5.63286512e-13  1.90804940e-13
 -2.38834883e-13 -7.71880763e-14 -3.82694642e-13 -4.76794588e-13
 -1.28776743e-01]
supnorm grad right now is: 0.12877674311801193
Weights right now are: 
[-4.86878701 -0.69213671 -3.63664277 -4.46512567  5.12335577  0.58344502
  4.03558009  3.74909735 -2.22706802 -0.84584117 -2.1940609  -1.94629957
  1.45184853  0.41735068 -0.11189162  0.7240116  -1.52487135 -0.42152449
 -2.57185489 -2.20303399 -2.26410785 -0.80541343 -2.3313492  -1.84531447
 -0.91390509 -0.7433382  -1.65814719 -0.8340583   0.47161248  1.45838849
  1.5641175   0.50679584 -1.10503991 -0.43026055 -1.52800321 -1.01032885
 -2.52011666 -1.34239461 -1.31531866 -0.98150174 -0.48934044 -1.13132777
  0.57882197  0.52814527  0.43511032  1.1270552  -0.3802131   0.39821051
 -1.49932928 -0.62938164 -0.42810278 -1.5994845   0.63152929  1.18482931
  0.55526834  1.70998763  1.20595253 -0.85988743 -0.10836873 -1.87786219
  2.04313308  1.72876723 -0.13308231  1.04736483 25.01498024]
(65,)
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.001652821359
gradient value of function right now is: [ 3.97856464e-09  4.86053337e-10  3.97761051e-09  3.97909485e-09
 -3.97852722e-09 -4.86049125e-10 -3.97757309e-09 -3.97905743e-09
  8.73271569e-11  1.04933847e-11  8.72619466e-11  9.23673565e-11
  4.99653891e-10  5.99609502e-11  4.99258740e-10  5.28416585e-10
  7.37583985e-11  8.86321207e-12  7.37034209e-11  7.80157078e-11
  6.78252313e-11  8.15013913e-12  6.77745115e-11  7.17399558e-11
 -1.65422088e-11 -4.63157375e-12 -1.90473619e-11 -3.03447328e-12
 -3.93877108e-12 -1.09380340e-12 -4.53045640e-12 -7.14587452e-13
  3.77629802e-11  1.04272742e-11  4.31907019e-11  6.98914874e-12
  2.38754379e-12  6.03056447e-13  2.64026301e-12  4.46366147e-13
  5.03241366e-12  7.12702251e-12  1.58771765e-13  2.30844130e-12
  1.74480522e-12  2.46700318e-12 -1.16376541e-13  6.16603539e-13
  3.66043536e-12  5.09393382e-12  1.60180123e-13  1.54843743e-12
  2.85530609e-12  3.86155510e-12  1.30488409e-13  1.07730845e-12
  1.08908674e-12  1.70092111e-12  5.65971739e-13  1.72463008e-13
 -2.44495606e-13 -5.76308201e-14 -3.86884824e-13 -4.46266236e-13
 -2.15988158e+00]
supnorm grad right now is: 2.1598815807144245
Weights right now are: 
[-4.87383387 -0.69275239 -3.64168842 -4.4701732   5.12840263  0.5840607
  4.04062574  3.75414487 -2.22717849 -0.84585446 -2.19417132 -1.94641644
  1.45121562  0.41727465 -0.11252418  0.72334216 -1.52496464 -0.42153571
 -2.57194814 -2.20313268 -2.26419363 -0.80542375 -2.33143494 -1.84540523
 -0.91388398 -0.74333242 -1.65812305 -0.83405445  0.47161752  1.45838987
  1.56412327  0.50679675 -1.10508791 -0.43027337 -1.52805752 -1.01033766
 -2.52011966 -1.34239531 -1.31532191 -0.98150229 -0.48934682 -1.13133684
  0.57882202  0.5281426   0.43510811  1.12705206 -0.38021285  0.39820983
 -1.49933395 -0.62938814 -0.42810286 -1.59948634  0.63152565  1.18482437
  0.55526827  1.70998636  1.20595116 -0.85988967 -0.10836944 -1.87786243
  2.04313338  1.72876733 -0.13308183  1.04736543 24.81895349]
(65,)
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.892937626607
gradient value of function right now is: [ 3.63877518e-09  4.44584941e-10  3.63790881e-09  3.63925685e-09
 -3.63832707e-09 -4.44529260e-10 -3.63746080e-09 -3.63880867e-09
  7.94673676e-11  9.57953844e-12  7.94570263e-11  8.40881608e-11
  4.57740999e-10  5.50803390e-11  4.57626915e-10  4.84259434e-10
  6.71370054e-11  8.09346673e-12  6.71284417e-11  7.10411191e-11
  6.17342246e-11  7.44203552e-12  6.17261729e-11  6.53240266e-11
 -1.54660903e-11 -4.12217951e-12 -1.75279625e-11 -2.80086141e-12
 -3.72230051e-12 -1.00416604e-12 -4.24200515e-12 -6.70031014e-13
  3.48383438e-11  8.85613329e-12  3.88111133e-11  6.31891931e-12
  2.13458096e-12  4.39565936e-13  2.22442747e-12  3.83213761e-13
  4.61424884e-12  6.60332683e-12 -2.89102543e-13  1.67800119e-12
  1.59690517e-12  2.28733039e-12 -2.85787168e-13  3.85869229e-13
  3.39267550e-12  4.74884263e-12 -8.05022181e-14  1.19177297e-12
  2.65934194e-12  3.61196307e-12 -3.40794451e-14  8.34749469e-13
  9.78634175e-13  1.69210490e-12  5.03873947e-13  1.96924197e-13
 -2.07503009e-13 -9.68458405e-14 -3.39163538e-13 -4.71743149e-13
  2.86443479e+00]
supnorm grad right now is: 2.864434791886564
Weights right now are: 
[-4.87874118 -0.69335201 -3.64659456 -4.47508117  5.13330994  0.58466033
  4.04553187  3.75905284 -2.227286   -0.8458674  -2.19427877 -1.94653017
  1.45059876  0.41720056 -0.11314067  0.72268972 -1.52505545 -0.42154664
 -2.5720389  -2.20322875 -2.26427714 -0.8054338  -2.33151841 -1.84549357
 -0.91386339 -0.74332676 -1.65809949 -0.83405069  0.47162245  1.45839122
  1.56412892  0.50679764 -1.10513464 -0.43028588 -1.52811043 -1.01034623
 -2.52012258 -1.342396   -1.31532507 -0.98150283 -0.48935303 -1.13134566
  0.57882204  0.52813998  0.43510596  1.127049   -0.38021262  0.39820916
 -1.49933849 -0.62939447 -0.42810294 -1.59948812  0.63152209  1.18481957
  0.55526818  1.70998511  1.20594983 -0.85989183 -0.10837012 -1.87786266
  2.04313368  1.72876742 -0.13308136  1.04736601 25.16995192]
(65,)
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.08238644265
gradient value of function right now is: [ 3.72747608e-09  4.56707196e-10  3.72658260e-09  3.72797111e-09
 -3.72769660e-09 -4.56734146e-10 -3.72680307e-09 -3.72819165e-09
  8.18753121e-11  9.84157150e-12  8.18211896e-11  8.66051961e-11
  4.70212118e-10  5.64436932e-11  4.69876844e-10  4.97301780e-10
  6.91991682e-11  8.31813741e-12  6.91535282e-11  7.31969996e-11
  6.36252012e-11  7.64800929e-12  6.35830795e-11  6.73008903e-11
 -1.56397653e-11 -4.35561389e-12 -1.79766728e-11 -2.86498132e-12
 -3.74650944e-12 -1.03714822e-12 -4.30485600e-12 -6.79186920e-13
  3.55123321e-11  9.72097351e-12  4.05012550e-11  6.55837227e-12
  2.22811092e-12  5.51563231e-13  2.44854249e-12  4.14824612e-13
  4.72014759e-12  6.69293220e-12  1.00196312e-13  2.11666904e-12
  1.63812085e-12  2.31936815e-12 -1.29059029e-13  5.59046694e-13
  3.43849361e-12  4.78839409e-12  1.24924823e-13  1.42769287e-12
  2.68353654e-12  3.63116906e-12  1.05275185e-13  9.93918154e-13
  1.01972640e-12  1.61040714e-12  5.29178145e-13  1.66003145e-13
 -2.27536975e-13 -5.87933236e-14 -3.61317939e-13 -4.25583846e-13
 -1.60803616e+00]
supnorm grad right now is: 1.6080361578962015
Weights right now are: 
[-4.88349601 -0.69393386 -3.65134826 -4.47983663  5.13806477  0.58524218
  4.05028557  3.7638083  -2.22739021 -0.84587994 -2.19438293 -1.94664041
  1.44999976  0.41712858 -0.11373936  0.72205613 -1.52514351 -0.42155724
 -2.57212692 -2.2033219  -2.26435811 -0.80544354 -2.33159934 -1.84557922
 -0.91384335 -0.74332127 -1.65807658 -0.83404703  0.47162726  1.45839254
  1.56413442  0.50679851 -1.10518003 -0.43029796 -1.52816173 -1.01035456
 -2.5201254  -1.34239665 -1.31532811 -0.98150334 -0.48935905 -1.13135424
  0.57882211  0.52813748  0.43510387  1.12704603 -0.38021238  0.39820853
 -1.49934289 -0.62940061 -0.428103   -1.59948985  0.63151865  1.18481491
  0.55526812  1.70998391  1.20594854 -0.85989395 -0.10837079 -1.87786289
  2.04313396  1.72876752 -0.1330809   1.04736658 24.87939077]
(65,)
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.952664062983
gradient value of function right now is: [ 3.66040996e-09  4.49260668e-10  3.65953135e-09  3.66089584e-09
 -3.66080660e-09 -4.49309394e-10 -3.65992789e-09 -3.66129253e-09
  8.05300108e-11  9.67454657e-12  8.04693957e-11  8.51766254e-11
  4.62799533e-10  5.55277884e-11  4.62432762e-10  4.89434974e-10
  6.80847192e-11  8.17966766e-12  6.80335594e-11  7.20134597e-11
  6.25967124e-11  7.52024082e-12  6.25495257e-11  6.62086619e-11
 -1.53624821e-11 -4.31355688e-12 -1.77052998e-11 -2.82033946e-12
 -3.68408538e-12 -1.02490001e-12 -4.23981682e-12 -6.68807897e-13
  3.48799591e-11  9.67724771e-12  3.99554615e-11  6.46332777e-12
  2.19424957e-12  5.60036333e-13  2.43436089e-12  4.11205942e-13
  4.63546851e-12  6.56141023e-12  1.71848134e-13  2.15296486e-12
  1.61035446e-12  2.27497480e-12 -9.61992483e-14  5.79911484e-13
  3.37136904e-12  4.69024162e-12  1.61202337e-13  1.44096611e-12
  2.62903591e-12  3.55477506e-12  1.29360836e-13  1.00213414e-12
  1.00476663e-12  1.55911219e-12  5.22211707e-13  1.56463741e-13
 -2.26213746e-13 -5.05101080e-14 -3.57453648e-13 -4.07516014e-13
 -2.44362287e+00]
supnorm grad right now is: 2.4436228667984667
Weights right now are: 
[-4.88811445 -0.69449992 -3.65596559 -4.48445568  5.14268321  0.58580823
  4.0549029   3.76842735 -2.22749152 -0.84589213 -2.1944842  -1.9467476
  1.44941665  0.41705852 -0.11432214  0.72143936 -1.52522915 -0.42156754
 -2.57221252 -2.20341251 -2.26443685 -0.80545301 -2.33167804 -1.84566253
 -0.91382384 -0.74331591 -1.65805425 -0.83404347  0.47163195  1.45839383
  1.5641398   0.50679936 -1.10522416 -0.43030976 -1.52821168 -1.01036267
 -2.52012815 -1.34239729 -1.31533107 -0.98150385 -0.4893649  -1.13136257
  0.57882216  0.528135    0.43510184  1.12704314 -0.38021215  0.3982079
 -1.49934716 -0.62940658 -0.42810306 -1.59949154  0.63151532  1.18481037
  0.55526805  1.70998273  1.20594727 -0.859896   -0.10837145 -1.87786312
  2.04313424  1.72876761 -0.13308046  1.04736713 24.77686726]
(65,)
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.0456203862436
gradient value of function right now is: [ 3.53819845e-09  4.34813410e-10  3.53734978e-09  3.53866719e-09
 -3.53837624e-09 -4.34834956e-10 -3.53752752e-09 -3.53884500e-09
  7.78357194e-11  9.35420876e-12  7.77832549e-11  8.23309300e-11
  4.48246242e-10  5.37979285e-11  4.47922512e-10  4.74064287e-10
  6.58264447e-11  7.91119420e-12  6.57821690e-11  6.96283137e-11
  6.05176042e-11  7.27306518e-12  6.04767498e-11  6.40127447e-11
 -1.49299031e-11 -4.16868593e-12 -1.71751606e-11 -2.73688235e-12
 -3.59225847e-12 -9.96032787e-13 -4.12966405e-12 -6.51566527e-13
  3.37886059e-11  9.28886786e-12  3.85892794e-11  6.24667511e-12
  2.11402161e-12  5.28315322e-13  2.32996666e-12  3.94404825e-13
  4.48387664e-12  6.35422171e-12  1.17759006e-13  2.03336823e-12
  1.55817058e-12  2.20434632e-12 -1.13094777e-13  5.40945968e-13
  3.26549458e-12  4.54591699e-12  1.30488914e-13  1.36828786e-12
  2.54784647e-12  3.44667670e-12  1.08002626e-13  9.52293217e-13
  9.69709418e-13  1.52287279e-12  5.03319194e-13  1.55512633e-13
 -2.16935290e-13 -5.36360422e-14 -3.44027900e-13 -4.01087577e-13
 -1.88072889e+00]
supnorm grad right now is: 1.880728892468904
Weights right now are: 
[-4.89261487 -0.69505231 -3.66046493 -4.48895669  5.14718363  0.58636062
  4.05940224  3.77292836 -2.22759031 -0.84590401 -2.19458294 -1.9468521
  1.44884723  0.41699011 -0.11489125  0.72083707 -1.52531268 -0.42157759
 -2.57229601 -2.20350087 -2.26451364 -0.80546225 -2.3317548  -1.84574377
 -0.91380477 -0.74331067 -1.65803242 -0.83403999  0.47163654  1.45839509
  1.56414506  0.50680019 -1.10526721 -0.43032128 -1.52826042 -1.01037057
 -2.52013082 -1.34239792 -1.31533396 -0.98150434 -0.48937061 -1.13137069
  0.57882218  0.5281326   0.43509986  1.12704033 -0.38021193  0.39820728
 -1.49935133 -0.6294124  -0.42810314 -1.59949319  0.63151206  1.18480596
  0.55526797  1.70998158  1.20594605 -0.85989799 -0.10837209 -1.87786333
  2.0431345   1.7287677  -0.13308003  1.04736767 24.82988966]
(65,)
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1506446988897
gradient value of function right now is: [ 3.40534217e-09  4.18958481e-10  3.40452641e-09  3.40579226e-09
 -3.40511545e-09 -4.18929617e-10 -3.40429975e-09 -3.40556550e-09
  7.48712430e-11  9.00349301e-12  7.48302115e-11  7.92016858e-11
  4.32230674e-10  5.19029192e-11  4.31966844e-10  4.57158242e-10
  6.33373096e-11  7.61675148e-12  6.33027043e-11  6.70008750e-11
  5.82268548e-11  7.00208582e-12  5.81948914e-11  6.15947095e-11
 -1.44665799e-11 -4.00122293e-12 -1.65907207e-11 -2.64538761e-12
 -3.49431793e-12 -9.63403000e-13 -4.00971266e-12 -6.32849169e-13
  3.26072596e-11  8.82538581e-12  3.70518506e-11  6.00500876e-12
  2.02441355e-12  4.87773879e-13  2.20635976e-12  3.74833990e-13
  4.31865548e-12  6.13288439e-12  3.42353463e-14  1.87916994e-12
  1.50089195e-12  2.12872062e-12 -1.41592184e-13  4.88672609e-13
  3.15215286e-12  4.39333510e-12  8.43570768e-14  1.27691665e-12
  2.46167476e-12  3.33308024e-12  7.60918000e-14  8.89727937e-13
  9.30661859e-13  1.49114490e-12  4.82098402e-13  1.56735220e-13
 -2.06058195e-13 -5.94458569e-14 -3.28676285e-13 -3.97683592e-13
 -9.01745767e-01]
supnorm grad right now is: 0.90174576694318
Weights right now are: 
[-4.89700936 -0.69559248 -3.66485837 -4.49335177  5.15157812  0.5869008
  4.06379569  3.77732344 -2.22768684 -0.84591563 -2.19467942 -1.94695422
  1.44829005  0.41692318 -0.11544812  0.72024773 -1.52539432 -0.42158741
 -2.57237762 -2.20358724 -2.2645887  -0.80547128 -2.33182982 -1.84582317
 -0.91378608 -0.74330554 -1.65801104 -0.83403658  0.47164105  1.45839633
  1.56415024  0.50680101 -1.1053093  -0.43033256 -1.5283081  -1.0103783
 -2.52013342 -1.34239854 -1.31533678 -0.98150482 -0.48937618 -1.13137861
  0.5788222   0.52813024  0.43509793  1.12703758 -0.38021173  0.39820668
 -1.4993554  -0.62941808 -0.42810322 -1.5994948   0.63150888  1.18480165
  0.55526789  1.70998045  1.20594485 -0.85989993 -0.10837271 -1.87786354
  2.04313476  1.72876778 -0.13307961  1.0473682  24.89667083]
(65,)
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.9454974571
gradient value of function right now is: [ 3.38150992e-09  4.16782519e-10  3.38069789e-09  3.38195702e-09
 -3.38127351e-09 -4.16753165e-10 -3.38046154e-09 -3.38172058e-09
  7.45210327e-11  8.95225150e-12  7.44667005e-11  7.88213943e-11
  4.30131261e-10  5.16059433e-11  4.29800711e-10  4.54889338e-10
  6.30609173e-11  7.57577052e-12  6.30150217e-11  6.67001611e-11
  5.79695976e-11  6.96403552e-12  5.79272669e-11  6.13149161e-11
 -1.43261033e-11 -4.02384368e-12 -1.65124239e-11 -2.63040245e-12
 -3.45932500e-12 -9.62655593e-13 -3.98142279e-12 -6.28135416e-13
  3.23461445e-11  8.97918031e-12  3.70594693e-11  5.99463841e-12
  2.02246773e-12  5.16592402e-13  2.24426604e-12  3.79128648e-13
  4.28701195e-12  6.06800337e-12  1.61410127e-13  1.99400004e-12
  1.49184128e-12  2.10714252e-12 -8.77154441e-14  5.38267211e-13
  3.11909728e-12  4.33919752e-12  1.50499827e-13  1.33469105e-12
  2.43214235e-12  3.28855777e-12  1.20604683e-13  9.28222077e-13
  9.29616257e-13  1.44136752e-12  4.82917790e-13  1.44283777e-13
 -2.09326603e-13 -4.64151206e-14 -3.30781991e-13 -3.76605394e-13
 -2.48246977e+00]
supnorm grad right now is: 2.4824697676974976
Weights right now are: 
[-4.90124971 -0.69611442 -3.66909771 -4.49759268  5.15581846  0.58742273
  4.06803501  3.78156434 -2.22778003 -0.84592684 -2.19477257 -1.9470528
  1.44775133  0.41685847 -0.11598655  0.71967792 -1.52547317 -0.4215969
 -2.57245642 -2.20367065 -2.26466118 -0.80548    -2.33190227 -1.84589985
 -0.913768   -0.74330057 -1.65799034 -0.83403328  0.47164543  1.45839753
  1.56415525  0.5068018  -1.10534997 -0.43034345 -1.52835415 -1.01038577
 -2.52013593 -1.34239913 -1.3153395  -0.98150528 -0.48938156 -1.13138627
  0.57882222  0.52812796  0.43509606  1.12703492 -0.38021152  0.39820609
 -1.49935933 -0.62942357 -0.42810328 -1.59949636  0.6315058   1.18479748
  0.55526782  1.70997937  1.20594369 -0.85990181 -0.1083733  -1.87786375
  2.04313502  1.72876787 -0.1330792   1.0473687  24.81056097]
(65,)
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1501593774237
gradient value of function right now is: [ 3.16777430e-09  3.90579794e-10  3.16701725e-09  3.16819113e-09
 -3.16782375e-09 -3.90586432e-10 -3.16706669e-09 -3.16824059e-09
  6.95885933e-11  8.37760333e-12  6.95667357e-11  7.36244737e-11
  4.03594668e-10  4.85100287e-11  4.03431954e-10  4.26925343e-10
  5.89005711e-11  7.09115017e-12  5.88821930e-11  6.23168197e-11
  5.41426861e-11  6.51824051e-12  5.41256487e-11  5.72828662e-11
 -1.36301394e-11 -3.70391595e-12 -1.55426236e-11 -2.48097325e-12
 -3.31625346e-12 -9.04696161e-13 -3.79249714e-12 -5.98912588e-13
  3.04895969e-11  8.01346447e-12  3.43222432e-11  5.57461574e-12
  1.86566699e-12  4.18224332e-13  1.99056885e-12  3.40477902e-13
  4.02414983e-12  5.73534479e-12 -1.03209455e-13  1.61401694e-12
  1.39894385e-12  1.99291065e-12 -1.87606401e-13  3.99603898e-13
  2.94908087e-12  4.11846560e-12  7.76620080e-15  1.11854771e-12
  2.30707550e-12  3.12829553e-12  2.30614912e-14  7.81272390e-13
  8.60748550e-13  1.43066937e-12  4.44259806e-13  1.57855916e-13
 -1.86731490e-13 -6.90574780e-14 -3.01315661e-13 -3.90005843e-13
  9.01451908e-01]
supnorm grad right now is: 0.9014519082713052
Weights right now are: 
[-4.90541245 -0.69662747 -3.67325945 -4.50175596  5.15998121  0.58793578
  4.07219676  3.78572764 -2.22787154 -0.84593785 -2.19486404 -1.94714962
  1.44722145  0.41679481 -0.11651617  0.71911744 -1.52555062 -0.42160622
 -2.57253384 -2.20375259 -2.26473238 -0.80548857 -2.33197343 -1.84597517
 -0.91375018 -0.74329568 -1.65796996 -0.83403003  0.47164976  1.45839872
  1.56416021  0.50680258 -1.10538996 -0.43035412 -1.52839938 -1.01039311
 -2.5201384  -1.3423997  -1.31534216 -0.98150574 -0.48938684 -1.13139379
  0.57882228  0.52812575  0.43509422  1.12703231 -0.38021131  0.39820553
 -1.4993632  -0.62942896 -0.42810334 -1.59949788  0.63150278  1.18479339
  0.55526777  1.70997831  1.20594256 -0.85990366 -0.1083739  -1.87786395
  2.04313527  1.72876795 -0.1330788   1.0473692  25.05445575]
(65,)
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.092977127016
gradient value of function right now is: [ 3.06741703e-09  3.78627043e-10  3.06668458e-09  3.06781989e-09
 -3.06708629e-09 -3.78585454e-10 -3.06635392e-09 -3.06748910e-09
  6.73661444e-11  8.11341743e-12  6.73509176e-11  7.12771017e-11
  3.91498130e-10  4.70725704e-11  3.91370845e-10  4.14149334e-10
  5.70344373e-11  6.86934241e-12  5.70216740e-11  6.03458190e-11
  5.24254112e-11  6.31412565e-12  5.24135340e-11  5.54690874e-11
 -1.32685673e-11 -3.58229996e-12 -1.50987536e-11 -2.41115706e-12
 -3.23841892e-12 -8.80035213e-13 -3.69886529e-12 -5.84270234e-13
  2.95860814e-11  7.69146447e-12  3.31904548e-11  5.39523185e-12
  1.79980013e-12  3.92242018e-13  1.90493782e-12  3.26691432e-13
  3.89868788e-12  5.56435236e-12 -1.48412818e-13  1.51526923e-12
  1.35566369e-12  1.93448994e-12 -2.01620327e-13  3.67512612e-13
  2.86162044e-12  3.99943578e-12 -1.77466604e-14  1.05869728e-12
  2.24006525e-12  3.03918437e-12  5.23963370e-15  7.40139297e-13
  8.31888463e-13  1.40073606e-12  4.28804117e-13  1.57193755e-13
 -1.79123870e-13 -7.17848035e-14 -2.90277828e-13 -3.84785206e-13
  1.55017781e+00]
supnorm grad right now is: 1.5501778070511354
Weights right now are: 
[-4.90945223 -0.69712598 -3.67729826 -4.50579627  5.16402098  0.5884343
  4.07623557  3.78976794 -2.22796038 -0.84594854 -2.19495284 -1.9472436
  1.44670624  0.41673291 -0.11703113  0.71857247 -1.52562582 -0.42161527
 -2.57260901 -2.20383215 -2.26480151 -0.80549689 -2.33204253 -1.8460483
 -0.91373281 -0.74329093 -1.65795012 -0.83402686  0.47165398  1.45839987
  1.56416504  0.50680335 -1.10542882 -0.43036446 -1.52844329 -1.01040024
 -2.52014078 -1.34240025 -1.31534472 -0.98150617 -0.48939197 -1.13140109
  0.57882234  0.52812362  0.43509244  1.12702977 -0.3802111   0.39820499
 -1.49936695 -0.6294342  -0.42810338 -1.59949934  0.63149985  1.18478941
  0.55526771  1.70997729  1.20594146 -0.85990546 -0.10837446 -1.87786414
  2.04313551  1.72876803 -0.13307841  1.04736969 25.06122995]
(65,)
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.178694325838
gradient value of function right now is: [ 3.04221443e-09  3.76141474e-10  3.04148649e-09  3.04261407e-09
 -3.04231909e-09 -3.76155167e-10 -3.04159112e-09 -3.04271874e-09
  6.69537663e-11  8.05682613e-12  6.69283463e-11  7.08333506e-11
  3.89085749e-10  4.67484304e-11  3.88907335e-10  4.11560723e-10
  5.67019058e-11  6.82341198e-12  5.66804864e-11  5.99876710e-11
  5.21165965e-11  6.27153008e-12  5.20967741e-11  5.51365482e-11
 -1.31339134e-11 -3.59305600e-12 -1.50089118e-11 -2.39490695e-12
 -3.20534147e-12 -8.77941798e-13 -3.67026162e-12 -5.79561763e-13
  2.93238336e-11  7.79458327e-12  3.31282666e-11  5.37629391e-12
  1.79448857e-12  4.13544873e-13  1.92998579e-12  3.29315984e-13
  3.86620883e-12  5.50305665e-12 -4.99692690e-14  1.60090464e-12
  1.34587474e-12  1.91395432e-12 -1.59807293e-13  4.04934636e-13
  2.83006575e-12  3.94948590e-12  3.33005418e-14  1.10119835e-12
  2.21241818e-12  2.99846219e-12  3.96132941e-14  7.68429552e-13
  8.29628510e-13  1.35969511e-12  4.28609165e-13  1.47236072e-13
 -1.81313297e-13 -6.13576665e-14 -2.91395701e-13 -3.67689318e-13
  2.10549315e-01]
supnorm grad right now is: 0.2105493146902325
Weights right now are: 
[-4.91341335 -0.69761546 -3.68125843 -4.50975792  5.16798211  0.58892377
  4.08019575  3.79372959 -2.22804756 -0.84595903 -2.19503998 -1.94733583
  1.4462001   0.4166721  -0.11753702  0.71803711 -1.52569964 -0.42162415
 -2.5726828  -2.20391024 -2.26486936 -0.80550505 -2.33211036 -1.84612008
 -0.91371575 -0.74328625 -1.65793061 -0.83402375  0.47165814  1.45840101
  1.56416981  0.5068041  -1.10546697 -0.43037464 -1.52848643 -1.01040724
 -2.52014312 -1.34240079 -1.31534725 -0.98150661 -0.48939699 -1.13140824
  0.57882239  0.52812151  0.43509069  1.12702729 -0.3802109   0.39820446
 -1.49937063 -0.62943934 -0.42810344 -1.59950079  0.63149697  1.18478551
  0.55526765  1.70997628  1.20594037 -0.85990722 -0.10837502 -1.87786433
  2.04313575  1.7287681  -0.13307803  1.04737016 24.98445114]
(65,)
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.084577689603
gradient value of function right now is: [ 3.17045136e-09  3.93096164e-10  3.16968659e-09  3.17086974e-09
 -3.17076115e-09 -3.93135567e-10 -3.16999630e-09 -3.17117958e-09
  7.02402788e-11  8.42292090e-12  7.01682462e-11  7.42777441e-11
  4.06469448e-10  4.86924992e-11  4.06052680e-10  4.29787357e-10
  5.95059560e-11  7.13589013e-12  5.94449692e-11  6.29265663e-11
  5.46902861e-11  6.55832007e-12  5.46341179e-11  5.78339855e-11
 -1.34536669e-11 -3.88028231e-12 -1.56432703e-11 -2.48788262e-12
 -3.26175266e-12 -9.22495340e-13 -3.77369438e-12 -5.95002088e-13
  3.03533701e-11  8.79673422e-12  3.52783512e-11  5.68739227e-12
  1.91312888e-12  5.36883832e-13  2.18873714e-12  3.66305286e-13
  4.01814649e-12  5.65586684e-12  3.60558096e-13  2.08072395e-12
  1.40316705e-12  1.96767447e-12  5.53492360e-15  5.93401185e-13
  2.90930095e-12  4.03438881e-12  2.51501391e-13  1.36282653e-12
  2.26254014e-12  3.05201092e-12  1.87319447e-13  9.44757156e-13
  8.81452531e-13  1.28856528e-12  4.59601021e-13  1.16986079e-13
 -2.04069494e-13 -2.25966246e-14 -3.17619068e-13 -3.23442512e-13
 -5.23673553e+00]
supnorm grad right now is: 5.23673552746088
Weights right now are: 
[-4.91724406 -0.69808934 -3.68508823 -4.51358913  5.17181283  0.58939766
  4.08402556  3.79756081 -2.22813186 -0.84596917 -2.19512426 -1.94742502
  1.44570976  0.41661318 -0.11802716  0.71751843 -1.52577104 -0.42163274
 -2.57275418 -2.20398579 -2.26493498 -0.80551295 -2.33217596 -1.84618951
 -0.91369918 -0.74328173 -1.65791168 -0.83402073  0.47166219  1.45840212
  1.56417444  0.50680483 -1.10550393 -0.43038441 -1.52852813 -1.01041401
 -2.52014538 -1.3424013  -1.31534966 -0.98150702 -0.48940188 -1.13141519
  0.57882248  0.52811952  0.43508899  1.12702487 -0.38021068  0.39820396
 -1.4993742  -0.62944432 -0.42810346 -1.59950216  0.63149418  1.18478173
  0.55526761  1.70997532  1.20593933 -0.85990894 -0.10837557 -1.87786452
  2.04313598  1.72876818 -0.13307766  1.04737063 24.5615665 ]
(65,)
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1787540106116
gradient value of function right now is: [ 2.95361878e-09  3.65908488e-10  2.95291147e-09  2.95400629e-09
 -2.95319513e-09 -3.65855148e-10 -2.95248792e-09 -2.95358257e-09
  6.50787619e-11  7.82892394e-12  6.50514483e-11  6.88475236e-11
  3.78756179e-10  4.54960380e-11  3.78569785e-10  4.00623874e-10
  5.51355928e-11  6.63299172e-12  5.51125525e-11  5.83287436e-11
  5.06742012e-11  6.09618397e-12  5.06528917e-11  5.36088735e-11
 -1.27838540e-11 -3.51200141e-12 -1.46286372e-11 -2.33365354e-12
 -3.12709021e-12 -8.58653093e-13 -3.58348376e-12 -5.65827093e-13
  2.84987196e-11  7.62916191e-12  3.22691533e-11  5.23401078e-12
  1.74336976e-12  4.08735980e-13  1.88451593e-12  3.21053237e-13
  3.75427596e-12  5.33898284e-12 -1.80643521e-14  1.58522335e-12
  1.30817496e-12  1.85815053e-12 -1.42415452e-13  4.06228133e-13
  2.74629754e-12  3.83059492e-12  4.84228609e-14  1.08555878e-12
  2.14604125e-12  2.90740052e-12  4.92981591e-14  7.57068765e-13
  8.07020848e-13  1.31135306e-12  4.17048244e-13  1.40279042e-13
 -1.77223695e-13 -5.64888100e-14 -2.84045689e-13 -3.52761518e-13
 -2.23544995e-01]
supnorm grad right now is: 0.22354499457887
Weights right now are: 
[-4.92101029 -0.69855584 -3.68885356 -4.51735585  5.17557906  0.58986415
  4.08779088  3.80132753 -2.22821479 -0.84597915 -2.19520716 -1.94751276
  1.4452268   0.41655516 -0.1185099   0.71700757 -1.52584131 -0.4216412
 -2.57282442 -2.20406012 -2.26499956 -0.80552072 -2.33224052 -1.84625783
 -0.91368285 -0.74327726 -1.65789302 -0.83401775  0.47166619  1.45840322
  1.56417902  0.50680555 -1.10554029 -0.43039407 -1.5285692  -1.01042067
 -2.5201476  -1.34240181 -1.31535204 -0.98150742 -0.48940667 -1.131422
  0.57882254  0.52811755  0.43508732  1.1270225  -0.38021048  0.39820346
 -1.49937771 -0.62944921 -0.42810351 -1.59950352  0.63149144  1.18477802
  0.55526756  1.70997437  1.20593831 -0.85991062 -0.10837609 -1.8778647
  2.0431362   1.72876826 -0.13307731  1.04737108 25.32741504]
(65,)
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.180497943011
gradient value of function right now is: [ 2.91259485e-09  3.61020952e-10  2.91189755e-09  2.91297667e-09
 -2.91259570e-09 -3.61020116e-10 -2.91189841e-09 -2.91297752e-09
  6.41791038e-11  7.72178062e-12  6.41541799e-11  6.78970921e-11
  3.73838775e-10  4.49106737e-11  3.73665245e-10  3.95429211e-10
  5.43801606e-11  6.54303217e-12  5.43591440e-11  5.75306873e-11
  4.99783637e-11  6.01332074e-12  4.99589156e-11  5.28737734e-11
 -1.26347717e-11 -3.46330731e-12 -1.44475884e-11 -2.30511381e-12
 -3.09467917e-12 -8.48611503e-13 -3.54480123e-12 -5.59768461e-13
  2.81300844e-11  7.50260550e-12  3.18139397e-11  5.16164045e-12
  1.71699166e-12  3.98862962e-13  1.85095160e-12  3.15619728e-13
  3.70348699e-12  5.26913359e-12 -3.35809197e-14  1.54780246e-12
  1.29067952e-12  1.83428962e-12 -1.47013580e-13  3.94240927e-13
  2.71060113e-12  3.78175134e-12  3.94841727e-14  1.06257295e-12
  2.11860753e-12  2.87075242e-12  4.30517145e-14  7.41273750e-13
  7.95407239e-13  1.29833970e-12  4.10822122e-13  1.39705641e-13
 -1.74227043e-13 -5.72477651e-14 -2.79642713e-13 -3.50200922e-13
  1.17112465e-03]
supnorm grad right now is: 0.0011711246451609543
Weights right now are: 
[-4.9246991  -0.69901339 -3.69254148 -4.52104515  5.17926787  0.5903217
  4.09147881  3.80501683 -2.22829613 -0.84598894 -2.19528846 -1.9475988
  1.4447529   0.41649824 -0.11898356  0.71650632 -1.52591023 -0.42164949
 -2.57289332 -2.20413304 -2.26506291 -0.80552834 -2.33230383 -1.84632484
 -0.91366684 -0.74327286 -1.6578747  -0.83401483  0.47167011  1.45840429
  1.56418352  0.50680626 -1.10557592 -0.43040363 -1.52860957 -1.01042721
 -2.52014977 -1.34240233 -1.3153544  -0.98150783 -0.48941136 -1.13142867
  0.57882255  0.52811556  0.43508568  1.12702018 -0.38021031  0.39820295
 -1.49938114 -0.62945399 -0.42810357 -1.59950488  0.63148876  1.18477439
  0.5552675   1.70997342  1.2059373  -0.85991226 -0.10837661 -1.87786487
  2.04313642  1.72876833 -0.13307695  1.04737152 24.8808815 ]
(65,)
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1804640825667
gradient value of function right now is: [ 2.88135254e-09  3.57364496e-10  2.88066265e-09  2.88173007e-09
 -2.88084618e-09 -3.57300818e-10 -2.88015642e-09 -2.88122364e-09
  6.35026805e-11  7.64021328e-12  6.34780230e-11  6.71813917e-11
  3.70119992e-10  4.44629840e-11  3.69948379e-10  3.91495259e-10
  5.38136038e-11  6.47470898e-12  5.37928089e-11  5.69312297e-11
  4.94572135e-11  5.95047309e-12  4.94379705e-11  5.23223593e-11
 -1.25137909e-11 -3.43119675e-12 -1.43106449e-11 -2.28323264e-12
 -3.06782796e-12 -8.41402504e-13 -3.51423670e-12 -5.54948339e-13
  2.78401799e-11  7.42918723e-12  3.14913506e-11  5.10908545e-12
  1.69801020e-12  3.94918212e-13  1.83111799e-12  3.12209131e-13
  3.66396247e-12  5.21257397e-12 -3.10148685e-14  1.53347871e-12
  1.27723657e-12  1.81500273e-12 -1.44507131e-13  3.91032261e-13
  2.68166952e-12  3.74125466e-12  4.02286583e-14  1.05242866e-12
  2.09590853e-12  2.83993604e-12  4.33814380e-14  7.34175971e-13
  7.87048236e-13  1.28392095e-12  4.06468359e-13  1.37994987e-13
 -1.72442736e-13 -5.64112616e-14 -2.76741892e-13 -3.46174573e-13
 -2.96658343e-02]
supnorm grad right now is: 0.02966583432403786
Weights right now are: 
[-4.92829623 -0.69946006 -3.69613775 -4.52464274  5.182865    0.59076838
  4.09507507  3.80861443 -2.22837546 -0.84599848 -2.19536776 -1.94768273
  1.44429001  0.41644264 -0.11944624  0.7160167  -1.52597748 -0.42165758
 -2.57296053 -2.20420418 -2.2651247  -0.80553578 -2.33236561 -1.84639022
 -0.91365118 -0.74326857 -1.65785679 -0.83401197  0.47167396  1.45840535
  1.56418792  0.50680696 -1.10561071 -0.43041293 -1.52864895 -1.0104336
 -2.52015189 -1.34240283 -1.31535669 -0.98150822 -0.48941593 -1.13143517
  0.57882257  0.52811364  0.43508409  1.12701791 -0.38021014  0.39820245
 -1.49938449 -0.62945866 -0.42810363 -1.5995062   0.63148614  1.18477084
  0.55526744  1.7099725   1.20593632 -0.85991386 -0.10837712 -1.87786505
  2.04313664  1.7287684  -0.13307661  1.04737196 24.95802652]
(65,)
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.180313340045
gradient value of function right now is: [ 2.85151020e-09  3.53875993e-10  2.85082737e-09  2.85188363e-09
 -2.85149512e-09 -3.53874570e-10 -2.85081229e-09 -2.85186855e-09
  6.28695798e-11  7.56379366e-12  6.28449890e-11  6.65114252e-11
  3.66629151e-10  4.40423701e-11  3.66458428e-10  3.87801935e-10
  5.32837855e-11  6.41075109e-12  5.32630421e-11  5.63705552e-11
  4.89686423e-11  5.89149503e-12  4.89494524e-11  5.18053360e-11
 -1.23989351e-11 -3.40153373e-12 -1.41816928e-11 -2.26264105e-12
 -3.04220093e-12 -8.34643011e-13 -3.48521999e-12 -5.50375834e-13
  2.75672590e-11  7.36312841e-12  3.11916300e-11  5.06025572e-12
  1.68047886e-12  3.91688014e-13  1.81334510e-12  3.09138197e-13
  3.62666102e-12  5.15932663e-12 -2.72373703e-14  1.52198526e-12
  1.26456226e-12  1.79684522e-12 -1.41573549e-13  3.88800386e-13
  2.65427006e-12  3.70301786e-12  4.16945113e-14  1.04403722e-12
  2.07436283e-12  2.81076016e-12  4.41891332e-14  7.28233284e-13
  7.79440520e-13  1.26979213e-12  4.02649775e-13  1.36233902e-13
 -1.70895024e-13 -5.54674148e-14 -2.74166562e-13 -3.42159977e-13
 -7.72786301e-02]
supnorm grad right now is: 0.07727863006366477
Weights right now are: 
[-4.93181641 -0.69989764 -3.69965709 -4.52816339  5.18638518  0.59120596
  4.09859442  3.81213507 -2.22845309 -0.84600782 -2.19544537 -1.94776487
  1.44383628  0.41638811 -0.11989979  0.71553674 -1.5260433  -0.4216655
 -2.57302633 -2.20427381 -2.26518519 -0.80554306 -2.33242607 -1.84645421
 -0.91363579 -0.74326436 -1.6578392  -0.83400917  0.47167774  1.45840639
  1.56419226  0.50680764 -1.10564483 -0.43042197 -1.52868745 -1.01043985
 -2.52015396 -1.3424033  -1.31535891 -0.9815086  -0.48942042 -1.13144156
  0.57882265  0.52811179  0.43508252  1.12701568 -0.38020994  0.39820199
 -1.49938777 -0.62946325 -0.42810366 -1.59950747  0.63148357  1.18476736
  0.5552674   1.70997162  1.20593536 -0.85991544 -0.10837762 -1.87786522
  2.04313685  1.72876847 -0.13307627  1.04737238 24.98118713]
(65,)
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.180472564529
gradient value of function right now is: [ 2.81909165e-09  3.50049556e-10  2.81841660e-09  2.81946061e-09
 -2.81896707e-09 -3.50034147e-10 -2.81829205e-09 -2.81933602e-09
  6.21641805e-11  7.47915661e-12  6.21405290e-11  6.57655331e-11
  3.62756357e-10  4.35782365e-11  3.62590991e-10  3.83707379e-10
  5.26921905e-11  6.33976886e-12  5.26722406e-11  5.57450017e-11
  4.84241188e-11  5.82616161e-12  4.84056586e-11  5.12295565e-11
 -1.22764077e-11 -3.36599894e-12 -1.40389652e-11 -2.23994039e-12
 -3.01517690e-12 -8.26943181e-13 -3.45387021e-12 -5.45441743e-13
  2.72702898e-11  7.27700156e-12  3.08464624e-11  5.00453091e-12
  1.66031708e-12  3.86064171e-13  1.79032797e-12  3.05281169e-13
  3.58609292e-12  5.10210179e-12 -3.06765812e-14  1.50095580e-12
  1.25068804e-12  1.77730563e-12 -1.41502491e-13  3.82918597e-13
  2.62503130e-12  3.66240264e-12  3.92492931e-14  1.03028240e-12
  2.05161887e-12  2.78003793e-12  4.23652268e-14  7.18708693e-13
  7.70483064e-13  1.25674693e-12  3.97913523e-13  1.35054671e-13
 -1.68796201e-13 -5.52475381e-14 -2.70924757e-13 -3.38885775e-13
 -2.49820671e-02]
supnorm grad right now is: 0.024982067098111033
Weights right now are: 
[-4.9352674  -0.70032718 -3.70310725 -4.53161483  5.18983617  0.5916355
  4.10204458  3.81558651 -2.2285293  -0.84601699 -2.19552154 -1.94784548
  1.44339069  0.41633461 -0.12034516  0.71506544 -1.52610792 -0.42167327
 -2.57309093 -2.20434218 -2.26524457 -0.8055502  -2.33248543 -1.84651703
 -0.91362071 -0.74326021 -1.65782193 -0.83400641  0.47168146  1.45840741
  1.56419652  0.50680832 -1.10567825 -0.43043095 -1.52872535 -1.010446
 -2.52015599 -1.34240378 -1.31536112 -0.98150897 -0.4894248  -1.13144779
  0.57882265  0.52810992  0.43508099  1.12701351 -0.38020979  0.39820151
 -1.49939099 -0.62946772 -0.42810373 -1.59950875  0.63148106  1.18476396
  0.55526733  1.70997072  1.20593442 -0.85991697 -0.1083781  -1.87786538
  2.04313706  1.72876854 -0.13307593  1.0473728  24.98752265]
(65,)
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1551.6347824796605
W_T_median: 1406.0687944584558
W_T_pctile_5: 624.1985510172708
W_T_CVAR_5_pct: 487.91093300852606
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.0
F value: -3591.180472564529
-----------------------------------------------
+00 -9.46071286e-01 -3.26009043e-01 -3.02989542e+00
  1.53745187e+00  1.49353367e+00  2.31669880e-01  1.65489672e+00
 -1.93716988e+00 -2.02956057e+00 -2.74948608e-01 -1.83361156e+00
 -1.59681561e+00 -1.18917236e+00 -1.29787194e+00 -9.79960562e-01
 -1.17294590e+00 -1.01091079e+00 -1.79774264e+00 -7.67943040e-01
  1.78377685e+00  8.28406420e-01  8.69748859e-01  1.73147345e+00
 -1.37458628e+00 -1.19892954e+00 -1.78530629e+00 -2.24135612e+00
 -1.16771892e+00 -1.35370634e+00 -2.03231881e+00 -1.56951628e+00
  3.96885960e-01 -3.93966847e-02  2.06449262e-01 -4.28488981e-03
 -1.19710751e+00 -8.57811416e-01  2.61564722e-01 -1.04527428e+00
 -2.28232910e+00 -1.30354486e+00 -1.20095565e+00 -1.36145077e+00
 -4.84173719e-02 -1.47034524e+00 -8.95328498e-02 -9.72288214e-01
 -4.28022033e-01  8.28512377e-01 -1.18447160e+00 -1.34795579e+00
  2.48777819e+01]
(65,)
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.7531169722342
gradient value of function right now is: [ 2.98550325e-09  2.98819247e-09  2.87924962e-11  2.98874085e-09
 -2.98530907e-09 -2.98799811e-09 -2.87906016e-11 -2.98854645e-09
  1.22529163e-10  1.22099927e-10  1.16765923e-12  1.23865472e-10
  9.14239529e-11  9.11035643e-11  8.71393661e-13  9.24211464e-11
  2.91494305e-11  2.90472869e-11  2.77821271e-13  2.94673655e-11
  7.15149580e-11  7.12652021e-11  6.80541375e-13  7.22941195e-11
  8.05643299e-12  3.32519724e-12  7.78328337e-12  8.18078462e-12
  5.56406888e-12  2.29045194e-12  5.39782274e-12  5.65717016e-12
  1.58788379e-12  6.55125094e-13  1.53520785e-12  1.61254070e-12
  1.69016610e-12  6.88240083e-13  1.66841893e-12  1.72635235e-12
  2.97556249e-13  2.58742832e-13 -7.10934152e-14  3.29153620e-13
  3.50214617e-12  3.29089505e-12  8.90611887e-13  3.79761011e-12
  1.39132104e-12  1.26451753e-12  1.70659338e-13  1.60410218e-12
  1.34673624e-13  9.81852530e-14 -1.60059460e-13  1.56894449e-13
  2.35421281e-13  2.79701063e-13  2.93568083e-13  6.29671454e-13
 -4.20803072e-14  1.15581233e-13  2.80543041e-13  4.15804917e-13
 -1.95057101e-01]
supnorm grad right now is: 0.19505710088094452
Weights right now are: 
[-5.21088178e+00 -4.58497250e+00 -2.71115681e-01 -4.53997825e+00
  4.47658308e+00  4.54172480e+00  7.65432635e-01  3.00408993e+00
 -2.74543738e+00 -1.12702697e+00 -1.03412009e+00 -1.62563503e+00
 -1.75089604e+00 -9.46187660e-01 -3.26010154e-01 -3.03001348e+00
  1.53741464e+00  1.49349657e+00  2.31669526e-01  1.65485908e+00
 -1.93726122e+00 -2.02965160e+00 -2.74949476e-01 -1.83370390e+00
 -1.59682588e+00 -1.18917660e+00 -1.29788188e+00 -9.79970998e-01
 -1.17295299e+00 -1.01091370e+00 -1.79774953e+00 -7.67950253e-01
  1.78377482e+00  8.28405585e-01  8.69746899e-01  1.73147139e+00
 -1.37458843e+00 -1.19893042e+00 -1.78530842e+00 -2.24135832e+00
 -1.16771930e+00 -1.35370667e+00 -2.03231872e+00 -1.56951670e+00
  3.96881478e-01 -3.94008932e-02  2.06448141e-01 -4.28974447e-03
 -1.19710929e+00 -8.57813026e-01  2.61564528e-01 -1.04527632e+00
 -2.28232927e+00 -1.30354498e+00 -1.20095544e+00 -1.36145097e+00
 -4.84176671e-02 -1.47034561e+00 -8.95332177e-02 -9.72289035e-01
 -4.28021978e-01  8.28512226e-01 -1.18447195e+00 -1.34795634e+00
  2.48767831e+01]
(65,)
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.753159010065
gradient value of function right now is: [ 2.95182186e-09  2.95447954e-09  2.84691359e-11  2.95502128e-09
 -2.95199918e-09 -2.95465702e-09 -2.84708844e-11 -2.95519880e-09
  1.21173173e-10  1.20748684e-10  1.15473559e-12  1.22494664e-10
  9.04200357e-11  9.01031622e-11  8.61824854e-13  9.14062582e-11
  2.88257704e-11  2.87247576e-11  2.74738588e-13  2.91401707e-11
  7.07375228e-11  7.04904818e-11  6.73141020e-13  7.15081954e-11
  7.96795391e-12  3.28868428e-12  7.69780995e-12  8.09094312e-12
  5.50336016e-12  2.26546369e-12  5.33893846e-12  5.59544899e-12
  1.57037435e-12  6.47902425e-13  1.51827950e-12  1.59475982e-12
  1.67165592e-12  6.80703345e-13  1.65015006e-12  1.70744677e-12
  2.94293845e-13  2.55900812e-13 -7.03259500e-14  3.25547793e-13
  3.46377723e-12  3.25482882e-12  8.80934337e-13  3.75606575e-12
  1.37612982e-12  1.25069741e-12  1.68789292e-13  1.58661525e-12
  1.33199614e-13  9.71051371e-14 -1.58323496e-13  1.55177257e-13
  2.32885123e-13  2.76698315e-13  2.90396897e-13  6.22852604e-13
 -4.16321257e-14  1.14335072e-13  2.77492456e-13  4.11341901e-13
 -1.91547741e-01]
supnorm grad right now is: 0.1915477407782688
Weights right now are: 
[-5.21461427e+00 -4.58870834e+00 -2.71151679e-01 -4.54371477e+00
  4.48031556e+00  4.54546064e+00  7.65468632e-01  3.00782645e+00
 -2.74559061e+00 -1.12717966e+00 -1.03412155e+00 -1.62578993e+00
 -1.75101040e+00 -9.46301611e-01 -3.26011243e-01 -3.03012907e+00
  1.53737819e+00  1.49346024e+00  2.31669179e-01  1.65482224e+00
 -1.93735069e+00 -2.02974076e+00 -2.74950326e-01 -1.83379434e+00
 -1.59683595e+00 -1.18918075e+00 -1.29789161e+00 -9.79981220e-01
 -1.17295995e+00 -1.01091656e+00 -1.79775628e+00 -7.67957321e-01
  1.78377284e+00  8.28404767e-01  8.69744979e-01  1.73146937e+00
 -1.37459054e+00 -1.19893128e+00 -1.78531051e+00 -2.24136047e+00
 -1.16771967e+00 -1.35370699e+00 -2.03231863e+00 -1.56951711e+00
  3.96877089e-01 -3.94050146e-02  2.06447039e-01 -4.29449736e-03
 -1.19711102e+00 -8.57814600e-01  2.61564330e-01 -1.04527832e+00
 -2.28232943e+00 -1.30354510e+00 -1.20095524e+00 -1.36145117e+00
 -4.84179554e-02 -1.47034597e+00 -8.95335787e-02 -9.72289830e-01
 -4.28021925e-01  8.28512081e-01 -1.18447229e+00 -1.34795687e+00
  2.50463808e+01]
(65,)
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.753918536388
gradient value of function right now is: [ 2.91702215e-09  2.91964726e-09  2.81344619e-11  2.92018223e-09
 -2.91712315e-09 -2.91974835e-09 -2.81354430e-11 -2.92028334e-09
  1.19760320e-10  1.19341086e-10  1.14092909e-12  1.21066061e-10
  8.93730077e-11  8.90600315e-11  8.51589904e-13  9.03475549e-11
  2.84874546e-11  2.83877013e-11  2.71431638e-13  2.87980825e-11
  6.99266922e-11  6.96826631e-11  6.65223097e-13  7.06883294e-11
  7.87368593e-12  3.24942331e-12  7.60823648e-12  7.99553424e-12
  5.43826910e-12  2.23839431e-12  5.27694885e-12  5.52951384e-12
  1.55174147e-12  6.40146195e-13  1.50055674e-12  1.57589859e-12
  1.65152378e-12  6.72395180e-13  1.63073423e-12  1.68698161e-12
  2.90853394e-13  2.52793105e-13 -7.02656724e-14  3.21644375e-13
  3.42479592e-12  3.21791086e-12  8.68688415e-13  3.71339149e-12
  1.36003545e-12  1.23564406e-12  1.63909689e-13  1.56768825e-12
  1.31526089e-13  9.57250345e-14 -1.57348688e-13  1.53133057e-13
  2.29816443e-13  2.74671888e-13  2.86528603e-13  6.17272933e-13
 -4.12912915e-14  1.13419622e-13  2.73676278e-13  4.08416691e-13
 -1.32664828e-01]
supnorm grad right now is: 0.13266482801334378
Weights right now are: 
[-5.21826061e+00 -4.59235797e+00 -2.71186849e-01 -4.54736507e+00
  4.48396191e+00  4.54911027e+00  7.65503803e-01  3.01147675e+00
 -2.74574035e+00 -1.12732888e+00 -1.03412298e+00 -1.62594130e+00
 -1.75112216e+00 -9.46412985e-01 -3.26012307e-01 -3.03024206e+00
  1.53734258e+00  1.49342476e+00  2.31668840e-01  1.65478623e+00
 -1.93743816e+00 -2.02982792e+00 -2.74951157e-01 -1.83388276e+00
 -1.59684579e+00 -1.18918481e+00 -1.29790112e+00 -9.79991214e-01
 -1.17296674e+00 -1.01091936e+00 -1.79776288e+00 -7.67964233e-01
  1.78377090e+00  8.28403967e-01  8.69743103e-01  1.73146740e+00
 -1.37459260e+00 -1.19893212e+00 -1.78531254e+00 -2.24136258e+00
 -1.16772004e+00 -1.35370731e+00 -2.03231853e+00 -1.56951751e+00
  3.96872804e-01 -3.94090405e-02  2.06445959e-01 -4.29914354e-03
 -1.19711273e+00 -8.57816147e-01  2.61564136e-01 -1.04528028e+00
 -2.28232960e+00 -1.30354522e+00 -1.20095504e+00 -1.36145136e+00
 -4.84182426e-02 -1.47034631e+00 -8.95339361e-02 -9.72290608e-01
 -4.28021872e-01  8.28511936e-01 -1.18447263e+00 -1.34795739e+00
  2.48678674e+01]
(65,)
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.7534345400236
gradient value of function right now is: [ 2.88660222e-09  2.88919885e-09  2.78428209e-11  2.88972779e-09
 -2.88663100e-09 -2.88922766e-09 -2.78430647e-11 -2.88975661e-09
  1.18529430e-10  1.18114315e-10  1.12941869e-12  1.19821917e-10
  8.84613542e-11  8.81514291e-11  8.43063427e-13  8.94260838e-11
  2.81924869e-11  2.80937229e-11  2.68671033e-13  2.84999364e-11
  6.92221468e-11  6.89804622e-11  6.58649538e-13  6.99762102e-11
  7.79471744e-12  3.21707396e-12  7.53101912e-12  7.91514729e-12
  5.38438090e-12  2.21640071e-12  5.22394709e-12  5.47456715e-12
  1.53610012e-12  6.33740504e-13  1.48525395e-12  1.55997556e-12
  1.63526860e-12  6.65851082e-13  1.61440074e-12  1.67031565e-12
  2.87938057e-13  2.50335508e-13 -6.90814844e-14  3.18473368e-13
  3.38949695e-12  3.18493221e-12  8.61027540e-13  3.67526769e-12
  1.34632728e-12  1.22346005e-12  1.64018888e-13  1.55206306e-12
  1.30276526e-13  9.49186197e-14 -1.55212333e-13  1.51733777e-13
  2.27652734e-13  2.71122917e-13  2.83870016e-13  6.09966295e-13
 -4.07722970e-14  1.12008819e-13  2.71239456e-13  4.03066238e-13
 -1.72832627e-01]
supnorm grad right now is: 0.1728326273096566
Weights right now are: 
[-5.22182156e+00 -4.59592211e+00 -2.71221202e-01 -4.55092986e+00
  4.48752286e+00  4.55267441e+00  7.65538156e-01  3.01504154e+00
 -2.74588664e+00 -1.12747466e+00 -1.03412437e+00 -1.62608919e+00
 -1.75123136e+00 -9.46521806e-01 -3.26013348e-01 -3.03035245e+00
  1.53730779e+00  1.49339009e+00  2.31668508e-01  1.65475107e+00
 -1.93752364e+00 -2.02991310e+00 -2.74951971e-01 -1.83396918e+00
 -1.59685542e+00 -1.18918878e+00 -1.29791042e+00 -9.80000988e-01
 -1.17297339e+00 -1.01092210e+00 -1.79776933e+00 -7.67970995e-01
  1.78376900e+00  8.28403185e-01  8.69741270e-01  1.73146548e+00
 -1.37459462e+00 -1.19893294e+00 -1.78531454e+00 -2.24136464e+00
 -1.16772039e+00 -1.35370762e+00 -2.03231845e+00 -1.56951791e+00
  3.96868622e-01 -3.94129704e-02  2.06444892e-01 -4.30368103e-03
 -1.19711439e+00 -8.57817661e-01  2.61563930e-01 -1.04528220e+00
 -2.28232976e+00 -1.30354534e+00 -1.20095485e+00 -1.36145155e+00
 -4.84185270e-02 -1.47034664e+00 -8.95342906e-02 -9.72291360e-01
 -4.28021823e-01  8.28511797e-01 -1.18447297e+00 -1.34795788e+00
  2.49536353e+01]
(65,)
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3588.7538963020256
gradient value of function right now is: [ 2.85425826e-09  2.85682465e-09  2.75319064e-11  2.85734728e-09
 -2.85435234e-09 -2.85691882e-09 -2.75328398e-11 -2.85744147e-09
  1.17219407e-10  1.16809052e-10  1.11673616e-12  1.18497404e-10
  8.74906506e-11  8.71842535e-11  8.33664534e-13  8.84446424e-11
  2.78791845e-11  2.77815563e-11  2.65640155e-13  2.81831711e-11
  6.84702332e-11  6.82312774e-11  6.51375329e-13  6.92159829e-11
  7.70804124e-12  3.18109586e-12  7.44813136e-12  7.82732268e-12
  5.32466472e-12  2.19165810e-12  5.16668283e-12  5.41400111e-12
  1.51895928e-12  6.26629067e-13  1.46884657e-12  1.54260525e-12
  1.61689130e-12  6.58302526e-13  1.59652765e-12  1.65160520e-12
  2.84757629e-13  2.47498262e-13 -6.87798340e-14  3.14909354e-13
  3.35290154e-12  3.15036856e-12  8.50574702e-13  3.63548332e-12
  1.33156413e-12  1.20979323e-12  1.60570365e-13  1.53489877e-12
  1.28780173e-13  9.37320024e-14 -1.54044689e-13  1.49940138e-13
  2.25040025e-13  2.68879350e-13  2.80563036e-13  6.04306609e-13
 -4.04238307e-14  1.11045313e-13  2.67980016e-13  3.99815337e-13
 -1.35005078e-01]
supnorm grad right now is: 0.13500507815008253
Weights right now are: 
[-5.22532546e+00 -4.59942916e+00 -2.71255008e-01 -4.55443755e+00
  4.49102677e+00  4.55618147e+00  7.65571962e-01  3.01854924e+00
 -2.74603062e+00 -1.12761814e+00 -1.03412574e+00 -1.62623474e+00
 -1.75133887e+00 -9.46628932e-01 -3.26014373e-01 -3.03046113e+00
  1.53727355e+00  1.49335597e+00  2.31668182e-01  1.65471646e+00
 -1.93760781e+00 -2.02999698e+00 -2.74952772e-01 -1.83405427e+00
 -1.59686489e+00 -1.18919269e+00 -1.29791957e+00 -9.80010607e-01
 -1.17297994e+00 -1.01092479e+00 -1.79777568e+00 -7.67977651e-01
  1.78376714e+00  8.28402415e-01  8.69739465e-01  1.73146358e+00
 -1.37459661e+00 -1.19893375e+00 -1.78531650e+00 -2.24136667e+00
 -1.16772074e+00 -1.35370792e+00 -2.03231837e+00 -1.56951829e+00
  3.96864502e-01 -3.94168413e-02  2.06443843e-01 -4.30814902e-03
 -1.19711603e+00 -8.57819145e-01  2.61563727e-01 -1.04528409e+00
 -2.28232992e+00 -1.30354545e+00 -1.20095466e+00 -1.36145173e+00
 -4.84188038e-02 -1.47034698e+00 -8.95346366e-02 -9.72292099e-01
 -4.28021773e-01  8.28511663e-01 -1.18447330e+00 -1.34795837e+00
  2.45922780e+01]
(65,)
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1550.4132917549532
W_T_median: 1404.2325046774995
W_T_pctile_5: 624.0906801710605
W_T_CVAR_5_pct: 487.92797957183916
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.0
F value: -3588.7538963020256
-----------------------------------------------
