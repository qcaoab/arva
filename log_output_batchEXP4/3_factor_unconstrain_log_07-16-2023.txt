/home/marcchen/Documents/constrain_factor/researchcode/exp_config_json_files/multi_portfolio_exp4_kappa3.json
Starting at: 
16-07-23_09:59

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = 3factor_mc
timeseries_basket['basket_desc'] = 3 factors : Basic, size, value, mom
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret', 'Mom_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = 3factor_mc
timeseries_basket['basket_desc'] = 3 factors : Basic, size, value, mom
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret', 'Mom_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.005383     0.031411
192608                    0.0319              0.0561  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0355  ...     0.005323    -0.028996
192611                   -0.0038              0.0294  ...     0.005303     0.028554

[5 rows x 7 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.043289    -0.036240
202209                   -0.0955             -0.0871  ...    -0.050056    -0.091324
202210                    0.0883              0.1486  ...    -0.014968     0.077403
202211                   -0.0076              0.0462  ...     0.040789     0.052365
202212                   -0.0457             -0.0499  ...    -0.018566    -0.057116

[5 rows x 7 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'Mom_Hi30_nom_ret_ind', 'CPI_nom_ret_ind', 'T30_nom_ret_ind',
       'B10_nom_ret_ind', 'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
Mom_Hi30_real_ret      0.011386
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
Mom_Hi30_real_ret      0.061421
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Mom_Hi30_real_ret
T30_real_ret             1.000000  ...           0.055142
B10_real_ret             0.351722  ...           0.066570
VWD_real_ret             0.068448  ...           0.936115
Size_Lo30_real_ret       0.014412  ...           0.903222
Value_Hi30_real_ret      0.018239  ...           0.869469
Mom_Hi30_real_ret        0.055142  ...           1.000000

[6 rows x 6 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 192707
End: 199112
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      14  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      14  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 14)     True          14  
2     (14, 14)     True          14  
3      (14, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       6       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      14  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      14  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       6           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 14)     True          14  
2     (14, 14)     True          14  
3      (14, 6)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        6           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 6)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 1500, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        6           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 6)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.6, 0.1, 0.1, 0.1, 0.1]
W_T_mean: 924.4024154136672
W_T_median: 613.3748362942811
W_T_pctile_5: -280.23183268243423
W_T_CVAR_5_pct: -405.44527023207644
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1336.6323262226663
Current xi:  [123.54778]
objective value function right now is: -1336.6323262226663
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1897.155237127031
Current xi:  [144.83527]
objective value function right now is: -1897.155237127031
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -2007.7540909540116
Current xi:  [167.48479]
objective value function right now is: -2007.7540909540116
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -2091.8745987572606
Current xi:  [191.93507]
objective value function right now is: -2091.8745987572606
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -2137.516256816558
Current xi:  [215.58379]
objective value function right now is: -2137.516256816558
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -2190.3423506830395
Current xi:  [238.94185]
objective value function right now is: -2190.3423506830395
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -2235.2874483690625
Current xi:  [262.07138]
objective value function right now is: -2235.2874483690625
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -2277.1221039318934
Current xi:  [285.0775]
objective value function right now is: -2277.1221039318934
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -2327.961125052401
Current xi:  [308.164]
objective value function right now is: -2327.961125052401
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -2374.4818824950617
Current xi:  [330.73694]
objective value function right now is: -2374.4818824950617
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -2407.8039385869906
Current xi:  [353.40808]
objective value function right now is: -2407.8039385869906
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -2456.564916455209
Current xi:  [376.35318]
objective value function right now is: -2456.564916455209
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -2493.114702234089
Current xi:  [398.95685]
objective value function right now is: -2493.114702234089
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -2519.0425758367655
Current xi:  [421.39557]
objective value function right now is: -2519.0425758367655
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -2579.630875217964
Current xi:  [444.0527]
objective value function right now is: -2579.630875217964
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [466.4918]
objective value function right now is: -2577.4057806188034
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -2646.664428147518
Current xi:  [488.6983]
objective value function right now is: -2646.664428147518
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -2676.0815273008684
Current xi:  [511.46555]
objective value function right now is: -2676.0815273008684
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [533.705]
objective value function right now is: -2622.174769317884
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2752.870775042677
Current xi:  [555.92163]
objective value function right now is: -2752.870775042677
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -2793.1895422070484
Current xi:  [577.7202]
objective value function right now is: -2793.1895422070484
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -2822.292721156885
Current xi:  [599.581]
objective value function right now is: -2822.292721156885
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2845.967593303686
Current xi:  [621.67236]
objective value function right now is: -2845.967593303686
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -2879.3264508409757
Current xi:  [642.93945]
objective value function right now is: -2879.3264508409757
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -2909.8718576481974
Current xi:  [664.83685]
objective value function right now is: -2909.8718576481974
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -2915.4667842969598
Current xi:  [686.24274]
objective value function right now is: -2915.4667842969598
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -2930.8749205917925
Current xi:  [708.0445]
objective value function right now is: -2930.8749205917925
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -2984.6491333050776
Current xi:  [728.55164]
objective value function right now is: -2984.6491333050776
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -2997.1675795139686
Current xi:  [749.92413]
objective value function right now is: -2997.1675795139686
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -3011.278955721474
Current xi:  [770.6396]
objective value function right now is: -3011.278955721474
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -3043.7733130598995
Current xi:  [791.47327]
objective value function right now is: -3043.7733130598995
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -3048.964194383346
Current xi:  [812.22626]
objective value function right now is: -3048.964194383346
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -3076.338324858359
Current xi:  [832.26294]
objective value function right now is: -3076.338324858359
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -3084.417132940896
Current xi:  [851.65753]
objective value function right now is: -3084.417132940896
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -3114.428133210421
Current xi:  [871.0694]
objective value function right now is: -3114.428133210421
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -3123.1089860989323
Current xi:  [875.24603]
objective value function right now is: -3123.1089860989323
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -3130.1096730350537
Current xi:  [879.42926]
objective value function right now is: -3130.1096730350537
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [883.5414]
objective value function right now is: -3117.2467639994998
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -3135.208119844231
Current xi:  [887.67474]
objective value function right now is: -3135.208119844231
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -3138.9540517890873
Current xi:  [891.861]
objective value function right now is: -3138.9540517890873
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -3141.808181529984
Current xi:  [896.1444]
objective value function right now is: -3141.808181529984
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -3146.972298845835
Current xi:  [900.161]
objective value function right now is: -3146.972298845835
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [904.32666]
objective value function right now is: -3146.225528269588
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -3153.4465200669933
Current xi:  [908.5325]
objective value function right now is: -3153.4465200669933
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [912.5933]
objective value function right now is: -3149.4496503321725
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -3157.1175722431094
Current xi:  [916.70337]
objective value function right now is: -3157.1175722431094
94.0% of gradient descent iterations done. Method = Adam
new min fval:  -3163.9251404982692
Current xi:  [920.6919]
objective value function right now is: -3163.9251404982692
new min fval from sgd:  -3164.1217810542043
new min fval from sgd:  -3164.326566810805
new min fval from sgd:  -3164.4775754407956
new min fval from sgd:  -3164.595962573087
new min fval from sgd:  -3164.809761826557
new min fval from sgd:  -3165.06649996396
new min fval from sgd:  -3165.198649050599
new min fval from sgd:  -3165.268062376697
new min fval from sgd:  -3165.2894524301178
new min fval from sgd:  -3165.317250489968
new min fval from sgd:  -3165.364089046835
new min fval from sgd:  -3165.4690129654978
new min fval from sgd:  -3165.5933350887485
new min fval from sgd:  -3165.7292529635524
new min fval from sgd:  -3165.8250180627992
new min fval from sgd:  -3165.8918582175893
new min fval from sgd:  -3165.892802511585
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [924.7157]
objective value function right now is: -3151.428650189205
new min fval from sgd:  -3166.1836785388746
new min fval from sgd:  -3166.3302078080615
new min fval from sgd:  -3166.341176031543
new min fval from sgd:  -3166.527266833661
new min fval from sgd:  -3166.630367594816
new min fval from sgd:  -3166.668184488007
new min fval from sgd:  -3166.6863579402416
new min fval from sgd:  -3166.695666927574
new min fval from sgd:  -3166.877198819874
new min fval from sgd:  -3167.0597681605145
new min fval from sgd:  -3167.2043691829754
new min fval from sgd:  -3167.37037435803
new min fval from sgd:  -3167.5256089482236
new min fval from sgd:  -3167.7588819960524
new min fval from sgd:  -3168.070903090326
new min fval from sgd:  -3168.0833100600153
new min fval from sgd:  -3168.144059064424
new min fval from sgd:  -3168.1971161887714
new min fval from sgd:  -3168.22969640575
new min fval from sgd:  -3168.2940958839804
new min fval from sgd:  -3168.3279962859515
new min fval from sgd:  -3168.3734927753676
new min fval from sgd:  -3168.422983988899
new min fval from sgd:  -3168.4668816223443
new min fval from sgd:  -3168.522265351185
new min fval from sgd:  -3168.545936330904
new min fval from sgd:  -3168.5719553496356
new min fval from sgd:  -3168.6119710977027
new min fval from sgd:  -3168.6538240344134
new min fval from sgd:  -3168.677755760483
new min fval from sgd:  -3168.692226940558
new min fval from sgd:  -3168.7188631747385
new min fval from sgd:  -3168.7363810699244
new min fval from sgd:  -3168.7469640374097
new min fval from sgd:  -3168.760064868896
new min fval from sgd:  -3168.765835865453
new min fval from sgd:  -3168.766865246933
new min fval from sgd:  -3168.77170682194
new min fval from sgd:  -3168.8220689684863
new min fval from sgd:  -3168.8538598169553
new min fval from sgd:  -3168.8757986408073
new min fval from sgd:  -3168.8840464829136
new min fval from sgd:  -3168.8867093731146
new min fval from sgd:  -3168.892304500082
new min fval from sgd:  -3168.908770327331
new min fval from sgd:  -3168.9346667980376
new min fval from sgd:  -3168.949370858048
new min fval from sgd:  -3168.950598981547
new min fval from sgd:  -3168.9537579346147
new min fval from sgd:  -3168.9554456943783
new min fval from sgd:  -3168.9584607670768
new min fval from sgd:  -3168.9600657731576
new min fval from sgd:  -3168.9622339252587
new min fval from sgd:  -3168.989306128312
new min fval from sgd:  -3169.0130934162826
new min fval from sgd:  -3169.0314137469536
new min fval from sgd:  -3169.045280667285
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [927.23206]
objective value function right now is: -3167.7091124207955
new min fval from sgd:  -3169.0982136199978
new min fval from sgd:  -3169.17757632204
new min fval from sgd:  -3169.2635137796765
new min fval from sgd:  -3169.328487482539
new min fval from sgd:  -3169.3595414643514
new min fval from sgd:  -3169.3675787758257
new min fval from sgd:  -3169.4681377725733
new min fval from sgd:  -3169.543691142039
new min fval from sgd:  -3169.5539841227333
new min fval from sgd:  -3169.7728125654876
new min fval from sgd:  -3169.786593925099
new min fval from sgd:  -3169.9819209452244
new min fval from sgd:  -3169.9913993794694
new min fval from sgd:  -3170.015993564755
new min fval from sgd:  -3170.030924443718
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [928.0718]
objective value function right now is: -3169.752265262278
min fval:  -3170.030924443718
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-13.9670,   5.1608],
        [ -0.8990,  -0.6985],
        [ -0.8981,  -0.6981],
        [ -9.4562, -18.4832],
        [  1.4935, -16.0743],
        [ -0.8972,  -0.6955],
        [ -7.3080,  10.0471],
        [ -0.9010,  -0.7036],
        [ -0.9014,  -0.7044],
        [ -0.9010,  -0.7036],
        [ -0.9010,  -0.7037],
        [  1.1233, -11.7108],
        [-13.2518,   4.9097],
        [ 12.9653,  -1.0677]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -0.3164,  -3.3825,  -3.3841,   7.0560,  -1.4360,  -3.3852,   0.0207,
         -3.3798,  -3.3792,  -3.3798,  -3.3798,  -2.5700,  -0.7817, -10.2598],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-7.7044e+00, -9.7833e-02, -1.0397e-01,  1.1185e+01,  6.1641e+00,
         -9.2521e-02, -6.5595e+00, -1.2923e-01, -1.3295e-01, -1.2913e-01,
         -1.2924e-01,  3.9755e+00, -3.5840e+00,  2.0208e+01],
        [-4.4939e+00, -1.4115e-01, -1.2600e-01,  9.2918e+00,  3.9883e+00,
         -1.2093e-01, -5.2094e+00, -1.3201e-01, -1.3220e-01, -1.3200e-01,
         -1.3201e-01,  1.6816e+00, -2.2534e+00,  1.5355e+01],
        [-4.6250e-02, -2.0941e-02, -2.0923e-02, -3.8947e-01, -1.4972e-02,
         -2.0921e-02, -8.9713e-01, -2.0952e-02, -2.0956e-02, -2.0952e-02,
         -2.0952e-02, -9.2571e-03, -2.6590e-02, -3.6900e-01],
        [-4.6250e-02, -2.0941e-02, -2.0923e-02, -3.8947e-01, -1.4972e-02,
         -2.0921e-02, -8.9713e-01, -2.0952e-02, -2.0956e-02, -2.0952e-02,
         -2.0952e-02, -9.2571e-03, -2.6590e-02, -3.6900e-01],
        [-4.6250e-02, -2.0941e-02, -2.0923e-02, -3.8947e-01, -1.4972e-02,
         -2.0921e-02, -8.9713e-01, -2.0952e-02, -2.0956e-02, -2.0952e-02,
         -2.0952e-02, -9.2571e-03, -2.6590e-02, -3.6900e-01],
        [-4.6250e-02, -2.0941e-02, -2.0923e-02, -3.8947e-01, -1.4972e-02,
         -2.0921e-02, -8.9713e-01, -2.0952e-02, -2.0956e-02, -2.0952e-02,
         -2.0952e-02, -9.2571e-03, -2.6590e-02, -3.6900e-01],
        [-4.6250e-02, -2.0941e-02, -2.0923e-02, -3.8947e-01, -1.4972e-02,
         -2.0921e-02, -8.9713e-01, -2.0952e-02, -2.0956e-02, -2.0952e-02,
         -2.0952e-02, -9.2571e-03, -2.6590e-02, -3.6900e-01],
        [-4.6250e-02, -2.0941e-02, -2.0923e-02, -3.8947e-01, -1.4972e-02,
         -2.0921e-02, -8.9713e-01, -2.0952e-02, -2.0956e-02, -2.0952e-02,
         -2.0952e-02, -9.2571e-03, -2.6590e-02, -3.6900e-01],
        [-4.6250e-02, -2.0941e-02, -2.0923e-02, -3.8947e-01, -1.4972e-02,
         -2.0921e-02, -8.9713e-01, -2.0952e-02, -2.0956e-02, -2.0952e-02,
         -2.0952e-02, -9.2571e-03, -2.6590e-02, -3.6900e-01],
        [-1.1638e+00,  2.7207e-02,  2.6849e-02,  5.1340e+00,  7.0923e-01,
          2.7934e-02, -4.5200e+00,  2.6407e-02,  2.6388e-02,  2.6408e-02,
          2.6407e-02,  2.4429e-01, -1.6604e-01,  1.1900e+01],
        [-4.6250e-02, -2.0941e-02, -2.0923e-02, -3.8947e-01, -1.4972e-02,
         -2.0921e-02, -8.9713e-01, -2.0952e-02, -2.0956e-02, -2.0952e-02,
         -2.0952e-02, -9.2571e-03, -2.6590e-02, -3.6900e-01],
        [-4.6250e-02, -2.0941e-02, -2.0923e-02, -3.8947e-01, -1.4972e-02,
         -2.0921e-02, -8.9713e-01, -2.0952e-02, -2.0956e-02, -2.0952e-02,
         -2.0952e-02, -9.2571e-03, -2.6590e-02, -3.6900e-01],
        [ 7.8238e+00, -1.8236e-01, -1.8196e-01, -1.0291e+01, -7.0615e+00,
         -1.7240e-01,  3.6720e+00, -2.0129e-01, -2.0392e-01, -2.0122e-01,
         -2.0130e-01, -5.0000e+00,  7.7073e+00, -1.5965e+01],
        [-4.6250e-02, -2.0941e-02, -2.0923e-02, -3.8947e-01, -1.4972e-02,
         -2.0921e-02, -8.9713e-01, -2.0952e-02, -2.0956e-02, -2.0952e-02,
         -2.0952e-02, -9.2571e-03, -2.6590e-02, -3.6900e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.8873, -2.7502, -2.1943, -2.1943, -2.1943, -2.1943, -2.1943, -2.1943,
        -2.1943, -3.9373, -2.1943, -2.1943,  2.6530, -2.1943], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-10.8522,  -5.0674,   0.0271,   0.0271,   0.0271,   0.0271,   0.0271,
           0.0271,   0.0271,  -3.3349,   0.0271,   0.0271,  15.6759,   0.0271]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-13.4503,  -4.7178],
        [ -1.2040,   0.2477],
        [ -9.9933,   0.0887],
        [-11.4659,   8.9327],
        [  6.0430,  11.1368],
        [ 11.3065,   3.9951],
        [ -8.0258,  -6.6275],
        [  7.4533,   0.6144],
        [ 10.0492,   1.2798],
        [ -2.1431,   0.5750],
        [  6.0206,   8.5448],
        [-13.4375,  -4.7207],
        [  1.3047,  -5.7865],
        [  2.7199,   8.7591]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-2.0027, -5.8892,  9.0965,  0.7773,  0.6496, -0.5980, -6.6628, -8.4269,
        -5.0548, -5.7107,  6.5395, -1.8844, -4.7526, -9.8004], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-5.5607e-01, -1.2097e-02, -1.9528e+00, -9.4785e-01,  3.3710e-01,
         -1.4039e+00,  4.2784e-01,  2.3468e-01, -2.5047e-01,  3.0678e-03,
         -3.7015e+00, -5.3358e-01, -1.2836e-01, -2.6477e-01],
        [-1.8252e+00, -4.5698e-02,  1.6742e+00,  8.0469e+00,  1.7005e+00,
         -6.3532e+00, -5.9374e-01, -5.2485e+00, -3.0819e+00,  3.0732e-01,
         -1.0092e+00, -1.7065e+00, -8.3187e+00,  2.0904e+00],
        [ 7.4079e+00, -2.5511e-01,  8.0507e+00,  2.3686e-01,  7.0073e-02,
         -8.8589e+00,  3.5134e+00,  1.1182e-02, -1.3096e+01, -1.9860e-01,
         -3.2501e+01,  7.2980e+00, -2.4362e+00,  1.6001e-04],
        [-6.2485e-01,  4.8290e-02, -1.4758e+00, -1.4018e+00,  7.8856e-01,
         -9.7484e-01,  4.9768e-01,  6.7628e-01,  1.9387e-01, -9.6229e-02,
         -3.7565e+00, -5.8552e-01,  2.6600e-01, -2.9706e+00],
        [-6.2873e-01, -1.3600e-02, -1.6351e+00, -1.1620e+00,  5.5051e-01,
         -1.1532e+00,  4.1827e-01,  5.1746e-01,  1.3057e-02, -3.8368e-02,
         -3.7477e+00, -5.9381e-01,  9.5599e-02, -2.4315e+00],
        [-1.0540e+01,  4.3691e-01, -8.4759e+00,  1.7756e+01,  9.1046e+00,
          9.8260e-01,  8.5599e-02, -4.1873e-01, -2.5209e+00,  1.8076e-01,
          4.1046e+00, -1.0811e+01, -5.9520e+00, -2.3278e-01],
        [ 8.8235e+00, -4.3419e-03,  3.9457e+00, -9.5178e+00, -5.1945e+00,
         -4.2501e+00, -1.1324e+00,  8.1508e-01, -1.8482e+00, -2.1975e-01,
         -2.8860e+00,  8.7862e+00,  1.0935e+00, -1.3905e+00],
        [ 7.0018e-01,  2.5177e-02,  1.6371e-01, -1.3083e+00, -1.4076e+00,
         -2.3136e+00, -2.6536e-01, -4.9485e-01, -2.1191e+00,  2.2722e-02,
         -1.0335e+00,  7.0349e-01, -1.1051e+00, -5.3430e-01],
        [-2.7867e+00,  1.7465e-02,  8.1355e-01,  8.7341e-02,  2.5747e+00,
         -2.5186e+00, -7.4240e-01, -1.7194e+00, -2.9116e+00, -1.0631e-01,
         -6.8628e-01, -2.6549e+00,  1.1742e+00,  3.0185e+00],
        [-5.5350e-01, -1.5208e-02, -1.9571e+00, -1.0144e+00,  3.8305e-01,
         -1.3887e+00,  4.3693e-01,  2.3066e-01, -2.4608e-01, -4.0449e-03,
         -3.6969e+00, -5.3021e-01, -1.1836e-01, -1.3017e-01],
        [-4.5528e-02, -4.2775e-02, -2.7886e+00, -1.3313e+00,  7.5390e-01,
         -1.8444e+00,  3.2564e-01, -1.0218e-01, -8.1399e-01, -2.5553e-01,
         -2.0011e+00, -6.7963e-02, -8.3381e-01,  1.6729e+00],
        [-2.9192e-02,  9.1482e-05, -1.5927e+00, -2.0395e-01, -7.1569e-01,
         -1.9870e+00, -1.2955e-01, -1.2600e-01, -1.3999e+00,  1.5926e-03,
         -1.5004e+00, -3.2764e-02, -1.2222e+00, -5.9804e-02],
        [-5.9575e-01, -2.4096e-02, -1.7816e+00, -1.0033e+00,  4.0740e-01,
         -1.2622e+00,  4.2578e-01,  3.6798e-01, -9.9651e-02, -7.5356e-03,
         -3.7759e+00, -5.6483e-01, -2.2136e-03, -1.3115e+00],
        [ 1.5252e+01,  1.0277e-01, -6.9112e+00, -5.7471e+00, -2.4502e+00,
          1.8058e+00,  7.0074e-01,  1.1774e+00,  6.3844e-01,  1.6047e-01,
          2.2683e+00,  1.5713e+01,  2.0993e+01,  5.0049e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.1104, -4.8720, -4.0972, -1.5819, -1.7693, -3.4951, -0.0402, -2.9413,
        -2.0263, -2.1130, -2.8115, -2.5866, -1.9145,  4.7467], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-2.7100e+00,  4.1893e+00, -1.8113e-07, -5.0477e+00, -4.5647e+00,
          1.4324e+00, -9.9105e+01, -1.6651e-01,  3.7341e+00, -2.6019e+00,
          6.7371e-01, -1.3706e-02, -3.7110e+00,  3.0588e-01],
        [ 1.4089e+00,  7.5548e-01,  1.2173e+01,  1.7084e+00,  1.5549e+00,
          5.5470e-01,  1.7601e-01, -7.2921e-01, -5.1085e+00,  1.4240e+00,
          4.6555e-01, -8.5500e-03,  1.5151e+00,  1.2906e+00],
        [-7.8081e-03, -3.7062e-01, -2.9624e-05, -4.3842e-02, -2.3153e-02,
         -8.7580e+00, -2.9026e-01, -1.2438e-02, -1.1362e+00, -8.0176e-03,
         -1.8248e-03, -4.9728e-03, -1.4340e-02, -1.2073e+01],
        [-9.4607e-03, -3.3872e-01, -3.2743e-05, -3.8129e-02, -2.0699e-02,
         -8.9765e+00, -2.8689e-01, -1.2044e-02, -1.1885e+00, -9.5777e-03,
         -1.3856e-03, -6.0713e-03, -1.3017e-02, -1.2366e+01],
        [ 8.9473e-06, -1.6325e-01, -6.0617e-05, -1.4221e-02, -4.6820e-03,
         -7.5144e+00, -2.5267e-01, -9.8918e-03, -9.5366e-01, -6.7909e-05,
         -2.7806e-03, -6.1233e-03, -1.6285e-03, -1.1303e+01],
        [-1.1090e+00, -6.8937e-01, -1.1328e+01, -1.1786e+00, -1.0993e+00,
          7.9742e-01,  8.8736e+00,  7.9542e-01,  4.4177e-01, -1.1296e+00,
         -8.6646e-01, -2.5032e-02, -1.1169e+00,  1.5594e+00]], device='cuda:0'))])
xi:  [928.05524]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1671.362795229817
W_T_median: 1497.6324696338515
W_T_pctile_5: 939.2676378015518
W_T_CVAR_5_pct: 545.4516762468841
Average q (qsum/M+1):  49.603003717237904
Optimal xi:  [928.05524]
Expected(across Rb) median(across samples) p_equity:  0.1779600273662557
obj fun:  tensor(-3170.0309, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: 3factor_mc
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
