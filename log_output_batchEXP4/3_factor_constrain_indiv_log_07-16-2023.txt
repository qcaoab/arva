/home/marcchen/Documents/constrain_factor/researchcode/exp_config_json_files/multi_portfolio_exp4_kappa3.json
Starting at: 
16-07-23_09:59

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = 3factor_mc
timeseries_basket['basket_desc'] = 3 factors : Basic, size, value, mom
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret', 'Mom_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = 3factor_mc
timeseries_basket['basket_desc'] = 3 factors : Basic, size, value, mom
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret', 'Mom_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.005383     0.031411
192608                    0.0319              0.0561  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0355  ...     0.005323    -0.028996
192611                   -0.0038              0.0294  ...     0.005303     0.028554

[5 rows x 7 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.043289    -0.036240
202209                   -0.0955             -0.0871  ...    -0.050056    -0.091324
202210                    0.0883              0.1486  ...    -0.014968     0.077403
202211                   -0.0076              0.0462  ...     0.040789     0.052365
202212                   -0.0457             -0.0499  ...    -0.018566    -0.057116

[5 rows x 7 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'Mom_Hi30_nom_ret_ind', 'CPI_nom_ret_ind', 'T30_nom_ret_ind',
       'B10_nom_ret_ind', 'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
Mom_Hi30_real_ret      0.011386
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
Mom_Hi30_real_ret      0.061421
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Mom_Hi30_real_ret
T30_real_ret             1.000000  ...           0.055142
B10_real_ret             0.351722  ...           0.066570
VWD_real_ret             0.068448  ...           0.936115
Size_Lo30_real_ret       0.014412  ...           0.903222
Value_Hi30_real_ret      0.018239  ...           0.869469
Mom_Hi30_real_ret        0.055142  ...           1.000000

[6 rows x 6 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 192707
End: 199112
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      14  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      14  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 14)     True          14  
2     (14, 14)     True          14  
3      (14, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       6       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      14  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      14  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       6              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 14)     True          14  
2     (14, 14)     True          14  
3      (14, 6)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        6              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 6)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 1500, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        6              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 6)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.6, 0.1, 0.1, 0.1, 0.1]
W_T_mean: 924.4024154136672
W_T_median: 613.3748362942811
W_T_pctile_5: -280.23183268243423
W_T_CVAR_5_pct: -405.44527023207644
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1227.1650042551173
Current xi:  [120.89699]
objective value function right now is: -1227.1650042551173
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1262.2882777934549
Current xi:  [143.0258]
objective value function right now is: -1262.2882777934549
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1326.7244064881588
Current xi:  [166.18768]
objective value function right now is: -1326.7244064881588
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1368.641092787255
Current xi:  [189.04475]
objective value function right now is: -1368.641092787255
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1394.9458235235707
Current xi:  [209.9948]
objective value function right now is: -1394.9458235235707
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1426.8969386145825
Current xi:  [230.69838]
objective value function right now is: -1426.8969386145825
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1459.5551488291048
Current xi:  [251.21458]
objective value function right now is: -1459.5551488291048
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1481.7803636077238
Current xi:  [271.82428]
objective value function right now is: -1481.7803636077238
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1504.4602486826118
Current xi:  [290.36554]
objective value function right now is: -1504.4602486826118
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1526.5545707426843
Current xi:  [307.9618]
objective value function right now is: -1526.5545707426843
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1540.0087360447833
Current xi:  [325.2825]
objective value function right now is: -1540.0087360447833
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1541.3617983139682
Current xi:  [339.65585]
objective value function right now is: -1541.3617983139682
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1550.3270992955077
Current xi:  [352.377]
objective value function right now is: -1550.3270992955077
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1556.5179589302638
Current xi:  [362.95993]
objective value function right now is: -1556.5179589302638
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [373.58505]
objective value function right now is: -1546.8570582947782
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1563.622013311618
Current xi:  [383.37827]
objective value function right now is: -1563.622013311618
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1568.7882187555783
Current xi:  [388.75275]
objective value function right now is: -1568.7882187555783
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1570.8773760050083
Current xi:  [395.11005]
objective value function right now is: -1570.8773760050083
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [398.35126]
objective value function right now is: -1566.8554211843266
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [401.878]
objective value function right now is: -1568.9958999996734
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [403.7771]
objective value function right now is: -1568.365507561444
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1573.4321993796495
Current xi:  [404.16226]
objective value function right now is: -1573.4321993796495
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [405.31476]
objective value function right now is: -1572.6774585425728
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [405.49426]
objective value function right now is: -1570.5347059433382
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1575.7850501415162
Current xi:  [404.96542]
objective value function right now is: -1575.7850501415162
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [406.56638]
objective value function right now is: -1571.6223281159755
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [408.3403]
objective value function right now is: -1568.4369087745351
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [408.8588]
objective value function right now is: -1573.7236400802078
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [410.70724]
objective value function right now is: -1568.6935498747612
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [410.415]
objective value function right now is: -1567.981527695926
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [412.2421]
objective value function right now is: -1563.5418601492477
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [411.5835]
objective value function right now is: -1573.7092685710923
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [411.2804]
objective value function right now is: -1574.2114956924174
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [411.3417]
objective value function right now is: -1568.6039517554836
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [412.2201]
objective value function right now is: -1561.168276176049
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1580.735188978676
Current xi:  [412.26324]
objective value function right now is: -1580.735188978676
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [412.28638]
objective value function right now is: -1580.3055866217167
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1581.6311382293804
Current xi:  [412.62592]
objective value function right now is: -1581.6311382293804
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [412.30487]
objective value function right now is: -1579.5462054651837
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [412.38925]
objective value function right now is: -1581.3741511135877
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [412.1845]
objective value function right now is: -1579.6098045813703
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1582.1804158794469
Current xi:  [412.15192]
objective value function right now is: -1582.1804158794469
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [412.53415]
objective value function right now is: -1580.029018895421
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [411.77878]
objective value function right now is: -1581.0048876587439
90.0% of gradient descent iterations done. Method = Adam
new min fval:  -1582.4022780074436
Current xi:  [411.685]
objective value function right now is: -1582.4022780074436
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [411.77722]
objective value function right now is: -1579.7223086645236
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [411.71753]
objective value function right now is: -1581.6596247243708
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [411.82657]
objective value function right now is: -1581.8521376387191
new min fval from sgd:  -1582.4700855534898
new min fval from sgd:  -1582.5053295264383
new min fval from sgd:  -1582.526957097763
new min fval from sgd:  -1582.5394241236806
new min fval from sgd:  -1582.5499823401258
new min fval from sgd:  -1582.5569131520806
new min fval from sgd:  -1582.597153716845
new min fval from sgd:  -1582.6704845588813
new min fval from sgd:  -1582.7368144257414
new min fval from sgd:  -1582.809979222362
new min fval from sgd:  -1582.8592135817637
new min fval from sgd:  -1582.8806180862896
new min fval from sgd:  -1582.8922649729557
new min fval from sgd:  -1582.9085781000913
new min fval from sgd:  -1582.9190298216706
new min fval from sgd:  -1582.9392148911038
new min fval from sgd:  -1582.9543724993578
new min fval from sgd:  -1582.9704146501101
new min fval from sgd:  -1582.9846245111103
new min fval from sgd:  -1582.9978517517598
new min fval from sgd:  -1583.013516683699
new min fval from sgd:  -1583.0283927502926
new min fval from sgd:  -1583.037087545715
new min fval from sgd:  -1583.04858126665
new min fval from sgd:  -1583.062744067533
new min fval from sgd:  -1583.079405010235
new min fval from sgd:  -1583.0890100093538
new min fval from sgd:  -1583.0992607178362
new min fval from sgd:  -1583.1145353618417
new min fval from sgd:  -1583.1228825447945
new min fval from sgd:  -1583.1246422879533
new min fval from sgd:  -1583.1274608585281
new min fval from sgd:  -1583.1308315054355
new min fval from sgd:  -1583.1351749385713
new min fval from sgd:  -1583.1485787857032
new min fval from sgd:  -1583.156811850324
new min fval from sgd:  -1583.1633433910195
new min fval from sgd:  -1583.1722565813302
new min fval from sgd:  -1583.1843131895635
new min fval from sgd:  -1583.19244455234
new min fval from sgd:  -1583.1948874774944
new min fval from sgd:  -1583.2019415410925
new min fval from sgd:  -1583.2151227433076
new min fval from sgd:  -1583.2229279590297
new min fval from sgd:  -1583.2275737666357
new min fval from sgd:  -1583.2321972239654
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [411.97256]
objective value function right now is: -1583.2029868437207
new min fval from sgd:  -1583.233586947431
new min fval from sgd:  -1583.2439714929942
new min fval from sgd:  -1583.249610527895
new min fval from sgd:  -1583.2512801083374
new min fval from sgd:  -1583.2520912223044
new min fval from sgd:  -1583.2600509225042
new min fval from sgd:  -1583.265096564789
new min fval from sgd:  -1583.2736315574944
new min fval from sgd:  -1583.2842487609464
new min fval from sgd:  -1583.2859800203196
new min fval from sgd:  -1583.2872524618158
new min fval from sgd:  -1583.289411524093
new min fval from sgd:  -1583.2941587083412
new min fval from sgd:  -1583.2985773811567
new min fval from sgd:  -1583.301463488217
new min fval from sgd:  -1583.301718333494
new min fval from sgd:  -1583.3027272772742
new min fval from sgd:  -1583.303521074834
new min fval from sgd:  -1583.3040287140925
new min fval from sgd:  -1583.3179973576314
new min fval from sgd:  -1583.3283942128066
new min fval from sgd:  -1583.3395101858866
new min fval from sgd:  -1583.3498374129329
new min fval from sgd:  -1583.360317040244
new min fval from sgd:  -1583.3682031419114
new min fval from sgd:  -1583.3733123376492
new min fval from sgd:  -1583.376640655408
new min fval from sgd:  -1583.3799414489438
new min fval from sgd:  -1583.3857751202477
new min fval from sgd:  -1583.3912568836868
new min fval from sgd:  -1583.398398094585
new min fval from sgd:  -1583.4047434819536
new min fval from sgd:  -1583.4080698418734
new min fval from sgd:  -1583.412882439042
new min fval from sgd:  -1583.418269571097
new min fval from sgd:  -1583.4193107032079
new min fval from sgd:  -1583.4273291694205
new min fval from sgd:  -1583.4319775825143
new min fval from sgd:  -1583.4389140061708
new min fval from sgd:  -1583.44702468429
new min fval from sgd:  -1583.4523162065354
new min fval from sgd:  -1583.4569495101919
new min fval from sgd:  -1583.4608566358497
new min fval from sgd:  -1583.4622323195533
new min fval from sgd:  -1583.4642649056455
new min fval from sgd:  -1583.4753528467015
new min fval from sgd:  -1583.5075556666484
new min fval from sgd:  -1583.5262788798334
new min fval from sgd:  -1583.5518777794737
new min fval from sgd:  -1583.5686793616699
new min fval from sgd:  -1583.5838587366054
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [412.08194]
objective value function right now is: -1583.5838587366054
min fval:  -1583.5838587366054
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.1066, -0.1746],
        [ 0.1066, -0.1746],
        [ 0.1066, -0.1746],
        [ 0.1066, -0.1746],
        [ 0.1066, -0.1746],
        [ 0.1066, -0.1746],
        [ 0.1066, -0.1746],
        [ 0.1066, -0.1746],
        [ 0.1066, -0.1746],
        [ 0.1066, -0.1746],
        [ 0.1066, -0.1746],
        [ 0.1066, -0.1746],
        [ 0.1066, -0.1746],
        [ 0.1066, -0.1746]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([0.1750, 0.1750, 0.1750, 0.1750, 0.1750, 0.1750, 0.1750, 0.1750, 0.1750,
        0.1750, 0.1750, 0.1750, 0.1750, 0.1750], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787,
         0.1787, 0.1787, 0.1787, 0.1787, 0.1787],
        [0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787,
         0.1787, 0.1787, 0.1787, 0.1787, 0.1787],
        [0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787,
         0.1787, 0.1787, 0.1787, 0.1787, 0.1787],
        [0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787,
         0.1787, 0.1787, 0.1787, 0.1787, 0.1787],
        [0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787,
         0.1787, 0.1787, 0.1787, 0.1787, 0.1787],
        [0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787,
         0.1787, 0.1787, 0.1787, 0.1787, 0.1787],
        [0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787,
         0.1787, 0.1787, 0.1787, 0.1787, 0.1787],
        [0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787,
         0.1787, 0.1787, 0.1787, 0.1787, 0.1787],
        [0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787,
         0.1787, 0.1787, 0.1787, 0.1787, 0.1787],
        [0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787,
         0.1787, 0.1787, 0.1787, 0.1787, 0.1787],
        [0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787,
         0.1787, 0.1787, 0.1787, 0.1787, 0.1787],
        [0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787,
         0.1787, 0.1787, 0.1787, 0.1787, 0.1787],
        [0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787,
         0.1787, 0.1787, 0.1787, 0.1787, 0.1787],
        [0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787, 0.1787,
         0.1787, 0.1787, 0.1787, 0.1787, 0.1787]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([0.2959, 0.2959, 0.2959, 0.2959, 0.2959, 0.2959, 0.2959, 0.2959, 0.2959,
        0.2959, 0.2959, 0.2959, 0.2959, 0.2959], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.4047, -1.4047, -1.4047, -1.4047, -1.4047, -1.4047, -1.4047, -1.4047,
         -1.4047, -1.4047, -1.4047, -1.4047, -1.4047, -1.4047]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.7728,   0.7975],
        [ -5.5140,   8.1758],
        [-16.2090,   4.8230],
        [ -1.8241,   0.7897],
        [ -3.9328,  13.2128],
        [ 12.4547,   9.6399],
        [ -2.8827,  -8.9084],
        [ 13.8561,   4.3678],
        [ 13.4274,   2.1302],
        [ -3.5819, -10.0139],
        [ 12.1916,   9.5775],
        [ -5.1179, -10.5117],
        [ -1.8209,   0.7677],
        [  6.6416,   5.3924]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-4.9524,  3.8302, -5.3262, -4.9760,  6.3899,  2.6699, -6.9222, -1.7050,
        -6.9928, -6.0557,  2.4041, -4.8134, -4.9475, -5.2863], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 5.9801e-02, -1.4165e+00, -3.4300e-02,  5.5614e-02, -4.1217e-01,
         -6.8685e-01, -1.0713e+00, -2.5486e+00, -2.7347e+00, -2.9555e-01,
         -6.1997e-01, -5.6904e-01,  5.4597e-02, -3.0832e-02],
        [-4.3858e-03,  1.1108e+00, -8.0131e-02,  8.5826e-02,  3.0948e+00,
          7.1203e+00, -3.1932e+00,  1.0113e+01,  1.4832e+01, -3.8629e+00,
          6.3310e+00, -5.6818e+00,  1.0514e-01,  4.1169e-01],
        [-9.1358e-02,  5.8995e+00, -7.8113e+00, -4.2376e-02,  4.5504e+00,
          8.8125e-01, -5.5868e+00, -9.9771e-01, -1.9880e+00, -6.9268e+00,
          3.3523e+00, -1.0456e+01,  1.6264e-02, -8.5472e+00],
        [ 2.2819e-01,  2.9663e+00, -1.3433e-01,  2.9415e-01, -3.5949e+00,
         -2.0395e+01,  1.6020e+00, -9.5112e+00, -1.2041e+01,  2.2836e+00,
         -1.9451e+01,  4.4466e+00,  2.8545e-01, -1.1026e-01],
        [ 5.9386e-01,  1.0022e+00,  4.4992e-01,  5.9177e-01,  3.2627e-01,
          7.8972e-01, -1.2378e+00, -5.1186e-01, -5.1618e+00, -5.1608e-01,
          6.4983e-01, -5.6247e-01,  6.4573e-01,  7.7730e-01],
        [-1.4956e-01,  1.5921e+01,  2.2328e-03, -1.7056e-01,  2.1174e+01,
          5.3980e-01, -6.5752e+00,  1.4707e+00, -2.9082e+00, -7.0944e+00,
          2.1600e+00, -5.7955e+00, -1.9423e-01,  6.0092e+00],
        [-2.4749e-01,  1.3614e+00, -8.9976e-01, -2.0724e-01,  1.6458e+00,
         -1.4536e+00, -4.8971e-01, -2.6739e+00, -1.4552e+00, -4.1783e-01,
         -1.1967e+00, -4.4092e-01, -2.4172e-01,  1.7780e-01],
        [ 1.9134e-01,  1.5619e+00, -2.0777e-01,  3.2968e-01,  1.3727e+00,
         -4.9293e-01, -6.7141e-01, -2.9204e+00, -4.0350e+00,  5.3547e-02,
         -7.5242e-01,  5.9783e-01,  3.6584e-01, -9.7145e-02],
        [-1.5116e-01, -5.5435e+00,  6.8408e-02, -2.2763e-01, -1.3784e+01,
         -1.5645e+00,  3.3012e+00, -5.8619e+00, -6.0805e+00,  3.9386e+00,
         -8.1429e-01,  3.8661e+00, -2.4584e-01, -3.4673e+00],
        [ 1.0152e-01, -6.8257e-01, -3.4259e-02,  2.2039e-01, -8.2736e+00,
         -6.9083e+00,  4.6845e+00, -5.7398e+00, -4.7890e+00,  5.1365e+00,
         -1.1385e+01,  6.7097e+00,  2.4682e-01, -1.2969e-01],
        [ 3.0231e-01, -4.4385e+00, -1.2447e-01,  5.7207e-01, -6.4543e+00,
          9.8632e-01, -1.3526e+00, -6.9025e-02, -3.1539e+00, -3.6628e-01,
          1.2174e+00, -3.7198e-01,  6.2608e-01,  2.0753e+00],
        [ 5.3935e-03, -5.4315e-01,  1.1378e-01,  5.6583e-03, -6.0247e-01,
         -1.1224e+00, -8.5014e-01, -1.4889e+00, -1.9123e+00, -6.2507e-01,
         -7.7131e-01, -9.1228e-01,  6.0964e-03, -3.4765e-02],
        [ 2.0075e-01, -7.3075e-01,  1.4338e-01,  1.9675e-01, -1.9702e+00,
          5.1868e-01, -1.4007e+00, -1.0878e+00, -5.0948e+00, -5.4266e-01,
          1.7047e-01,  1.2066e-01,  2.1168e-01, -3.9544e-01],
        [ 6.8451e-02,  5.3139e+00, -5.6607e+00,  8.1388e-02,  1.0860e+01,
         -5.3715e-01, -1.3366e+01, -3.8555e+00, -1.6845e+00, -1.0885e+01,
          3.1987e-01, -6.2174e+00,  1.2420e-01, -8.6248e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.2768, -3.3686, -2.2550,  1.4418, -2.2682, -1.3271, -2.7834, -2.2977,
         3.5578,  3.0590, -2.1755, -3.6024, -2.3729, -4.0575], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 6.5971e-01,  1.8667e+00,  1.0769e+00, -3.3984e+00, -5.4596e-01,
          1.0460e+00,  3.2034e+00,  3.8298e+00, -7.5692e-01, -6.9381e+00,
          1.3601e+00, -3.4377e-01,  2.1778e-01,  1.9353e+00],
        [-1.5164e+00,  1.7243e+00,  3.1761e-01, -4.7676e+00,  9.8895e-02,
          1.8470e-01, -1.7695e+00, -2.3707e+00,  1.2712e+00,  1.7713e+00,
          2.8224e-02, -5.7744e-01, -2.1294e-01,  9.8694e-01],
        [ 3.0681e-01, -1.2259e+01, -3.0431e+00,  7.5701e+00,  2.3820e-01,
          3.3661e-01, -6.9619e-01, -1.4509e+00,  1.1150e+00,  1.5560e+00,
         -1.9023e+00, -3.9344e-02,  1.6599e-01, -4.3583e+00],
        [ 3.9929e+00, -9.0996e+00, -2.1676e+00,  2.9759e+00,  4.2078e+00,
         -4.7982e+00,  1.2576e+00,  6.3168e+00,  8.2276e+00,  7.7285e-01,
          6.3428e+00,  1.9345e+00,  6.2963e+00, -3.0381e+00],
        [ 1.8301e+00,  1.4998e+00, -2.0122e+00,  5.5404e-01,  3.1621e+00,
         -1.0153e+00,  7.0609e-01,  1.8195e+00,  3.7147e+01,  1.6099e+01,
         -3.5326e+00,  7.9553e-01,  2.1283e+00, -7.5544e+00],
        [ 5.1555e-02,  1.1088e+01,  3.2661e-01,  1.1732e+00,  7.8358e-01,
          4.4581e+00,  3.7398e-01,  1.0758e+00,  5.4373e+00,  5.9289e+00,
          7.3006e-01,  3.5677e-02,  2.5000e-01,  5.6632e-01]], device='cuda:0'))])
xi:  [412.08194]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1132.0630978159948
W_T_median: 999.3952846835399
W_T_pctile_5: 412.28661333269133
W_T_CVAR_5_pct: 166.1944608370772
Average q (qsum/M+1):  35.0
Optimal xi:  [412.08194]
Expected(across Rb) median(across samples) p_equity:  0.1010473884952565
obj fun:  tensor(-1583.5839, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: 3factor_mc
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
