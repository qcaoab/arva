Starting at: 
05-02-23_11:25

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 50000
batchsize: 1000
remove neg:  False


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 5000, 'itbound_SGD_algorithms': 50000, 'nit_IterateAveragingStart': 45000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1654.0485718439586
Current xi:  [-45.279724]
objective value function right now is: -1654.0485718439586
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1666.320967282115
Current xi:  [-131.93613]
objective value function right now is: -1666.320967282115
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1671.6656650857906
Current xi:  [-196.03442]
objective value function right now is: -1671.6656650857906
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.3989758197301
Current xi:  [-229.99532]
objective value function right now is: -1672.3989758197301
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.615701365646
Current xi:  [-245.23643]
objective value function right now is: -1672.615701365646
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.0377618668974
Current xi:  [-245.29758]
objective value function right now is: -1673.0377618668974
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-244.87985]
objective value function right now is: -1673.0025769208216
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.72484]
objective value function right now is: -1672.9014198387822
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.076612672246
Current xi:  [-245.30276]
objective value function right now is: -1673.076612672246
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.299383199348
Current xi:  [-245.21564]
objective value function right now is: -1673.299383199348
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.325395449637
Current xi:  [-245.15663]
objective value function right now is: -1673.325395449637
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.51819]
objective value function right now is: -1673.1849237288206
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.47981]
objective value function right now is: -1673.0331498263006
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-245.30539]
objective value function right now is: -1673.2268882613566
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.0552]
objective value function right now is: -1672.8705516600073
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.12697]
objective value function right now is: -1673.2139672597052
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.386050511468
Current xi:  [-245.34093]
objective value function right now is: -1673.386050511468
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.0007]
objective value function right now is: -1672.935687985782
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.95015]
objective value function right now is: -1673.034369134334
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.20775]
objective value function right now is: -1673.0584196095363
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.92891]
objective value function right now is: -1672.6093192443614
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.82718]
objective value function right now is: -1672.8462328246371
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.97586]
objective value function right now is: -1673.2924915801739
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.15245]
objective value function right now is: -1672.7549826284242
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.2941]
objective value function right now is: -1673.035043945234
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.05107]
objective value function right now is: -1673.2685691395993
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.57216]
objective value function right now is: -1673.2959312028481
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-245.63068]
objective value function right now is: -1673.1342044898406
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1673.4334695699001
Current xi:  [-244.22536]
objective value function right now is: -1673.4334695699001
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.51332]
objective value function right now is: -1673.1317737406623
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-250.47037]
objective value function right now is: -1661.0766137489634
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.71573]
objective value function right now is: -1672.9903035624275
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.60753]
objective value function right now is: -1673.1859682818144
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.93185]
objective value function right now is: -1673.1248683811443
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.4839686722767
Current xi:  [-244.27808]
objective value function right now is: -1673.4839686722767
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.6162093109488
Current xi:  [-244.3732]
objective value function right now is: -1673.6162093109488
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.58913]
objective value function right now is: -1673.6064628690015
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.6356567768344
Current xi:  [-244.57948]
objective value function right now is: -1673.6356567768344
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.6695793124497
Current xi:  [-244.47337]
objective value function right now is: -1673.6695793124497
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.45833]
objective value function right now is: -1673.6360427553275
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.29565]
objective value function right now is: -1673.6123644247823
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.06885]
objective value function right now is: -1673.5769137990815
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.6719349883483
Current xi:  [-244.16283]
objective value function right now is: -1673.6719349883483
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.44984]
objective value function right now is: -1673.5643507832829
new min fval from sgd:  -1673.6864851756955
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.39761]
objective value function right now is: -1673.6864851756955
new min fval from sgd:  -1673.6922307469638
new min fval from sgd:  -1673.6984692449096
new min fval from sgd:  -1673.703736180245
new min fval from sgd:  -1673.7069904429525
new min fval from sgd:  -1673.709572677877
new min fval from sgd:  -1673.7097401572678
new min fval from sgd:  -1673.7133567626997
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.25137]
objective value function right now is: -1673.6803964700493
new min fval from sgd:  -1673.717920408662
new min fval from sgd:  -1673.7200188224638
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.55356]
objective value function right now is: -1673.5529869517236
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.45338]
objective value function right now is: -1673.6949366331746
new min fval from sgd:  -1673.7212983801685
new min fval from sgd:  -1673.7232191219769
new min fval from sgd:  -1673.7242929858453
new min fval from sgd:  -1673.7244142115867
new min fval from sgd:  -1673.724758499013
new min fval from sgd:  -1673.7252816099326
new min fval from sgd:  -1673.7255960503671
new min fval from sgd:  -1673.725784207928
new min fval from sgd:  -1673.7265861607095
new min fval from sgd:  -1673.726912142739
new min fval from sgd:  -1673.727363444468
new min fval from sgd:  -1673.7279435435241
new min fval from sgd:  -1673.7281413495964
new min fval from sgd:  -1673.7284001042322
new min fval from sgd:  -1673.7286101067216
new min fval from sgd:  -1673.7298203416162
new min fval from sgd:  -1673.7307881417653
new min fval from sgd:  -1673.7309378844348
new min fval from sgd:  -1673.7309761929218
new min fval from sgd:  -1673.7314822280553
new min fval from sgd:  -1673.7327730486672
new min fval from sgd:  -1673.7335910079441
new min fval from sgd:  -1673.7341166808005
new min fval from sgd:  -1673.7344430077787
new min fval from sgd:  -1673.7346876218944
new min fval from sgd:  -1673.7349692954108
new min fval from sgd:  -1673.7349854927863
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.44675]
objective value function right now is: -1673.7241787248718
new min fval from sgd:  -1673.7353109867033
new min fval from sgd:  -1673.7360859324062
new min fval from sgd:  -1673.7369659041372
new min fval from sgd:  -1673.7370903765811
new min fval from sgd:  -1673.7371281491396
new min fval from sgd:  -1673.737289095048
new min fval from sgd:  -1673.7378415276712
new min fval from sgd:  -1673.7380935651772
new min fval from sgd:  -1673.7382210596008
new min fval from sgd:  -1673.7388932871095
new min fval from sgd:  -1673.739448235586
new min fval from sgd:  -1673.739636458647
new min fval from sgd:  -1673.7397650872274
new min fval from sgd:  -1673.7399315744126
new min fval from sgd:  -1673.7399934127084
new min fval from sgd:  -1673.7403506074907
new min fval from sgd:  -1673.7404164776578
new min fval from sgd:  -1673.740426745032
new min fval from sgd:  -1673.7408558421232
new min fval from sgd:  -1673.7416011763532
new min fval from sgd:  -1673.7420253500845
new min fval from sgd:  -1673.7425732169286
new min fval from sgd:  -1673.7428261937355
new min fval from sgd:  -1673.742971531605
new min fval from sgd:  -1673.7434530423968
new min fval from sgd:  -1673.74385316588
new min fval from sgd:  -1673.7443997345977
new min fval from sgd:  -1673.7449720922648
new min fval from sgd:  -1673.7451851333403
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.45529]
objective value function right now is: -1673.7352307239585
min fval:  -1673.7451851333403
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 7.4661, -5.5887],
        [-4.8502,  7.7651],
        [-0.3248,  1.0944],
        [ 9.3794,  6.5263],
        [-4.1714, -7.9173],
        [-4.1275, -7.7873],
        [-4.1144, -7.8362],
        [-0.3248,  1.0944],
        [-0.3248,  1.0944],
        [-0.3248,  1.0944]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-9.7922,  9.4538, -0.9346, -1.5323, -7.9540, -7.8360, -7.9092, -0.9346,
        -0.9346, -0.9346], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.2696, -0.0462, -0.0138, -0.0414, -0.0492, -0.0489, -0.0482, -0.0138,
         -0.0138, -0.0138],
        [ 2.5271, -5.2816,  0.0680, -3.6298,  3.6647,  3.5392,  3.4644,  0.0680,
          0.0680,  0.0680],
        [ 2.6169, -5.3742,  0.0733, -3.7207,  3.6986,  3.5843,  3.5097,  0.0733,
          0.0733,  0.0733],
        [-4.9414,  7.8950, -0.0363,  4.9073, -4.4155, -4.2794, -4.2335, -0.0363,
         -0.0363, -0.0363],
        [-0.2696, -0.0462, -0.0138, -0.0414, -0.0492, -0.0489, -0.0482, -0.0138,
         -0.0138, -0.0138],
        [-4.7588,  7.3411,  0.1749,  1.3854, -4.0536, -3.9010, -3.8448,  0.1749,
          0.1749,  0.1749],
        [-0.2696, -0.0462, -0.0138, -0.0414, -0.0492, -0.0489, -0.0482, -0.0138,
         -0.0138, -0.0138],
        [-0.2696, -0.0462, -0.0138, -0.0414, -0.0492, -0.0489, -0.0482, -0.0138,
         -0.0138, -0.0138],
        [-0.2696, -0.0462, -0.0138, -0.0414, -0.0492, -0.0489, -0.0482, -0.0138,
         -0.0138, -0.0138],
        [ 0.2079, -0.8832, -0.0090, -0.6980,  0.6395,  0.6125,  0.6068, -0.0090,
         -0.0090, -0.0090]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5384, -3.0302, -3.0633,  3.2658, -0.5384,  2.8830, -0.5384, -0.5384,
        -0.5384, -1.9597], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-0.0202, -6.5863, -6.7271,  8.9067, -0.0202,  7.4812, -0.0202, -0.0202,
         -0.0202, -1.1724]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -7.3479, -11.0127],
        [  0.1459,  -8.9404],
        [ -1.8929,   0.7994],
        [ 10.5976,  -0.3376],
        [-14.8446,   0.2036],
        [ -1.8886,   0.7994],
        [  4.4954,  -2.1212],
        [ 12.8013,   6.0473],
        [-14.4036, -13.2632],
        [-19.0901,   2.9345]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-10.9202,  -8.7089,  -2.2367, -10.6757,   4.7110,  -2.2395,  -6.4017,
          3.2160, -10.8911,   4.0274], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 6.1798e-01,  9.8268e-01,  1.0187e+00, -2.5400e+00,  6.0138e+00,
          1.0131e+00, -2.0725e+00, -1.4872e+00, -4.9651e+00, -8.6337e-01],
        [-4.1023e-01, -7.7812e+00,  1.7924e-01, -1.9741e-01,  1.6405e+00,
          1.7799e-01,  2.3791e+00, -3.1037e+00,  5.3426e-01,  3.7163e+00],
        [ 4.5091e+00, -1.7762e+00, -1.4985e-01, -1.0308e+01,  3.3767e+00,
         -1.4954e-01, -3.5849e+00, -5.1003e+00,  6.5002e+00, -6.1150e+00],
        [-3.1802e-01, -8.0501e-01, -8.3878e-03, -5.5809e-01,  3.4125e-02,
         -8.4102e-03, -6.3075e-01, -1.9350e+00, -3.0454e-01,  6.7556e-03],
        [-3.1802e-01, -8.0501e-01, -8.3836e-03, -5.5806e-01,  3.4101e-02,
         -8.4060e-03, -6.3074e-01, -1.9351e+00, -3.0453e-01,  6.7614e-03],
        [-3.1803e-01, -8.0502e-01, -8.3791e-03, -5.5803e-01,  3.4075e-02,
         -8.4015e-03, -6.3072e-01, -1.9353e+00, -3.0451e-01,  6.7678e-03],
        [-3.1803e-01, -8.0502e-01, -8.3810e-03, -5.5805e-01,  3.4087e-02,
         -8.4035e-03, -6.3073e-01, -1.9352e+00, -3.0452e-01,  6.7650e-03],
        [-3.1803e-01, -8.0502e-01, -8.3812e-03, -5.5805e-01,  3.4088e-02,
         -8.4036e-03, -6.3073e-01, -1.9352e+00, -3.0452e-01,  6.7647e-03],
        [-8.5955e+00, -1.6427e+00,  6.3677e-01, -2.6163e+00,  4.9162e+00,
          6.3491e-01, -7.2293e+00, -8.8432e-01, -1.4798e+01,  9.0016e+00],
        [ 4.8118e+00, -7.0312e-02, -6.7055e-02, -4.9992e+00,  4.7061e+00,
         -6.6560e-02, -6.8500e+00, -1.3299e+01,  9.9506e+00, -6.3056e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-0.8377, -1.1799, -3.1239, -2.3215, -2.3214, -2.3211, -2.3212, -2.3212,
        -0.3203, -8.5668], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -3.1689,   4.0475,  -5.4350,   0.0265,   0.0265,   0.0266,   0.0265,
           0.0265,   3.7839,  13.2819],
        [  3.2583,  -4.0235,   5.4228,  -0.0265,  -0.0265,  -0.0266,  -0.0266,
          -0.0266,  -3.9637, -13.2836]], device='cuda:0'))])
xi:  [-244.46162]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 230.4495672849907
W_T_median: 80.07699979796166
W_T_pctile_5: -244.21380076002714
W_T_CVAR_5_pct: -333.8507404814066
Average q (qsum/M+1):  56.145657447076616
Optimal xi:  [-244.46162]
Observed VAR:  80.07699979796166
Expected(across Rb) median(across samples) p_equity:  0.28769417648242473
obj fun:  tensor(-1673.7452, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
