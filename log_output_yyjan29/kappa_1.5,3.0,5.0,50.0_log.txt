Starting at: 
06-02-23_11:39

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 50000
batchsize: 1000
remove neg:  False


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 5000, 'itbound_SGD_algorithms': 50000, 'nit_IterateAveragingStart': 45000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1070.8159049933502
Current xi:  [126.469765]
objective value function right now is: -1070.8159049933502
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1314.9626216851957
Current xi:  [145.41684]
objective value function right now is: -1314.9626216851957
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1512.4111367570401
Current xi:  [125.89416]
objective value function right now is: -1512.4111367570401
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1518.083375028534
Current xi:  [116.19737]
objective value function right now is: -1518.083375028534
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1519.7258275214388
Current xi:  [110.393936]
objective value function right now is: -1519.7258275214388
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1522.936901880974
Current xi:  [106.07374]
objective value function right now is: -1522.936901880974
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [104.87207]
objective value function right now is: -1511.6315928515955
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.45157]
objective value function right now is: -1522.8240264959054
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.34694]
objective value function right now is: -1519.7265413485798
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [102.953705]
objective value function right now is: -1521.6095526207705
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [102.94793]
objective value function right now is: -1511.751304988329
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.47788]
objective value function right now is: -1522.0474676200913
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.34164]
objective value function right now is: -1520.3526107678858
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [103.529686]
objective value function right now is: -1518.9719028332108
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.32614]
objective value function right now is: -1522.7317247675396
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1526.3864264950794
Current xi:  [103.82085]
objective value function right now is: -1526.3864264950794
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.30031]
objective value function right now is: -1521.8577138711687
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.83342]
objective value function right now is: -1523.9511354333958
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.91592]
objective value function right now is: -1525.6920525585506
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.6303]
objective value function right now is: -1524.1254966161266
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.4832]
objective value function right now is: -1516.7557601652438
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.62882]
objective value function right now is: -1524.2257155693546
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.56033]
objective value function right now is: -1522.8413165029904
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.75893]
objective value function right now is: -1522.900928047785
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.51366]
objective value function right now is: -1524.517375529196
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [102.88285]
objective value function right now is: -1518.6070884256233
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.74664]
objective value function right now is: -1521.323545429715
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [104.66498]
objective value function right now is: -1516.0585246410535
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [104.87408]
objective value function right now is: -1523.0265108240812
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [102.774635]
objective value function right now is: -1522.111638532005
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.54277]
objective value function right now is: -1524.876005921284
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.513466]
objective value function right now is: -1525.5714396411904
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.401405]
objective value function right now is: -1525.7620745279175
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.08748]
objective value function right now is: -1522.589759710226
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.69034]
objective value function right now is: -1516.8861711807158
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1528.5540713784965
Current xi:  [106.014404]
objective value function right now is: -1528.5540713784965
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.41603]
objective value function right now is: -1528.549575871995
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.40203]
objective value function right now is: -1528.1548969492023
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.77636]
objective value function right now is: -1527.8843524412325
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.4565]
objective value function right now is: -1527.5677603926397
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.74473]
objective value function right now is: -1528.4485498439956
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1528.912583177598
Current xi:  [105.47618]
objective value function right now is: -1528.912583177598
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.0307]
objective value function right now is: -1528.6383108086955
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.9762]
objective value function right now is: -1526.0357028701246
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.27857]
objective value function right now is: -1528.857984366226
new min fval from sgd:  -1528.9409197997275
new min fval from sgd:  -1529.1165157106952
new min fval from sgd:  -1529.2001469804545
new min fval from sgd:  -1529.2118079326326
new min fval from sgd:  -1529.240504204542
new min fval from sgd:  -1529.2892881734035
new min fval from sgd:  -1529.339003904928
new min fval from sgd:  -1529.376105311405
new min fval from sgd:  -1529.3832173259912
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.655815]
objective value function right now is: -1528.9325184753673
new min fval from sgd:  -1529.4139615697125
new min fval from sgd:  -1529.4391600452152
new min fval from sgd:  -1529.4571944343961
new min fval from sgd:  -1529.4760204109011
new min fval from sgd:  -1529.4787156689088
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.60018]
objective value function right now is: -1528.7630832196241
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.18251]
objective value function right now is: -1529.0573815773005
new min fval from sgd:  -1529.4817974368925
new min fval from sgd:  -1529.4868395903572
new min fval from sgd:  -1529.4889987775568
new min fval from sgd:  -1529.5076346602543
new min fval from sgd:  -1529.5317907012432
new min fval from sgd:  -1529.545550327275
new min fval from sgd:  -1529.5519358657884
new min fval from sgd:  -1529.564962054777
new min fval from sgd:  -1529.5721626821714
new min fval from sgd:  -1529.5754993796986
new min fval from sgd:  -1529.5779287807195
new min fval from sgd:  -1529.5813392109224
new min fval from sgd:  -1529.5825034135828
new min fval from sgd:  -1529.5974291379116
new min fval from sgd:  -1529.6039597286847
new min fval from sgd:  -1529.6054298293884
new min fval from sgd:  -1529.6116554141438
new min fval from sgd:  -1529.613830182788
new min fval from sgd:  -1529.6198294723197
new min fval from sgd:  -1529.6243575070407
new min fval from sgd:  -1529.6250049034861
new min fval from sgd:  -1529.6304316510227
new min fval from sgd:  -1529.6364924818263
new min fval from sgd:  -1529.6420100118694
new min fval from sgd:  -1529.6438838746546
new min fval from sgd:  -1529.6451184020216
new min fval from sgd:  -1529.6513728494524
new min fval from sgd:  -1529.654275403106
new min fval from sgd:  -1529.6550793594824
new min fval from sgd:  -1529.6551523010246
new min fval from sgd:  -1529.658700303609
new min fval from sgd:  -1529.665044391494
new min fval from sgd:  -1529.666025381043
new min fval from sgd:  -1529.6683712019428
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.41786]
objective value function right now is: -1529.6025633843276
new min fval from sgd:  -1529.6775012583123
new min fval from sgd:  -1529.6779597904188
new min fval from sgd:  -1529.6815002067142
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.430824]
objective value function right now is: -1529.521730210416
min fval:  -1529.6815002067142
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 13.4534,  -1.2940],
        [-48.4227, -10.4654],
        [  8.0911,  -9.0404],
        [-11.5126,   4.0582],
        [ -0.7732,   0.6998],
        [ -3.8718, -11.8813],
        [ -0.7628,   0.6826],
        [ -0.7627,   0.6824],
        [ -7.0075,  -3.7037],
        [  3.8840, -10.1478]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-11.1243,  -9.8810,  -8.3802,   8.2056,  -2.5763,  -9.1737,  -2.5598,
         -2.5591,   9.9968,  -8.8441], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.8585e-01, -4.1923e-02, -1.8334e-01, -4.9996e-01,  2.0895e-03,
         -1.1631e-01,  1.8967e-03,  1.8941e-03, -7.9580e-01, -9.9634e-02],
        [-1.8585e-01, -4.1923e-02, -1.8334e-01, -4.9996e-01,  2.0895e-03,
         -1.1631e-01,  1.8967e-03,  1.8941e-03, -7.9580e-01, -9.9634e-02],
        [ 4.0518e-01, -1.8577e-01,  3.1856e-01,  1.1872e+00,  9.1001e-02,
         -1.1631e-03,  8.9312e-02,  8.9287e-02,  1.9772e+00,  1.8160e-01],
        [-1.8585e-01, -4.1923e-02, -1.8334e-01, -4.9996e-01,  2.0894e-03,
         -1.1631e-01,  1.8967e-03,  1.8941e-03, -7.9580e-01, -9.9634e-02],
        [ 1.7745e+01,  1.2631e+01,  8.3274e+00, -6.2428e+00, -5.0756e-02,
          1.4436e+01, -3.1419e-02, -3.0556e-02, -7.0213e+00,  9.7696e+00],
        [ 4.9968e-01, -1.9335e-01,  3.5393e-01,  1.3174e+00,  1.0722e-01,
         -3.5527e-03,  1.0481e-01,  1.0477e-01,  2.1949e+00,  1.9679e-01],
        [-1.8585e-01, -4.1923e-02, -1.8334e-01, -4.9996e-01,  2.0894e-03,
         -1.1631e-01,  1.8967e-03,  1.8941e-03, -7.9580e-01, -9.9634e-02],
        [ 1.1337e+01,  7.9761e+00,  4.3700e+00, -5.2883e+00,  3.2859e-01,
          1.1723e+01,  2.1690e-01,  2.1205e-01, -5.7671e+00,  7.6176e+00],
        [-1.8585e-01, -4.1923e-02, -1.8334e-01, -4.9996e-01,  2.0894e-03,
         -1.1631e-01,  1.8967e-03,  1.8941e-03, -7.9580e-01, -9.9634e-02],
        [-1.8585e-01, -4.1923e-02, -1.8334e-01, -4.9996e-01,  2.0894e-03,
         -1.1631e-01,  1.8967e-03,  1.8941e-03, -7.9580e-01, -9.9634e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.9206, -0.9206,  2.2575, -0.9206, -2.2857,  2.5672, -0.9206, -2.4970,
        -0.9206, -0.9206], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0235,   0.0235,   3.6561,   0.0235, -17.0736,   7.2800,   0.0235,
          -9.3621,   0.0235,   0.0235]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 13.5101,  -4.7556],
        [ 13.4134,   4.8936],
        [  4.8158,   9.6671],
        [-13.0585,  -4.0793],
        [-13.6817,  -0.3679],
        [ -1.0167,  12.4438],
        [  6.2583,   8.6616],
        [  2.3834,  13.2838],
        [-16.7434,  -4.9825],
        [  8.0188,   4.9912]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-8.7844, -1.7400,  5.0177, -0.6295, 11.2033,  8.0785,  5.5554, 10.6320,
        -3.4407, -8.1330], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.3969e+00, -1.8130e+00, -4.8718e-01, -5.4177e-01, -1.5054e+00,
         -6.3569e-01, -7.7324e-01, -3.5671e-01, -2.7414e-02, -2.5221e-01],
        [ 1.6251e-01, -1.1107e+00,  5.7689e-01,  5.2911e-01, -2.5394e+00,
         -2.3398e+00,  6.2152e-01,  9.4969e-01, -4.6377e-02, -1.2714e+00],
        [-8.9319e-01, -1.5523e+00, -2.9452e-01, -4.2245e-01, -1.5429e+00,
         -2.3788e+00, -3.9774e-01,  4.2718e-01, -3.0014e-01, -6.3488e-01],
        [-1.6639e+00,  5.4186e+00, -1.0824e+00, -9.9652e+00, -3.6055e+00,
          1.1156e+01, -3.0154e+00,  2.7041e+01,  4.0214e+00, -5.8005e-01],
        [-1.2045e+00, -1.4895e+00, -4.1427e-01, -1.3129e-01, -1.0048e+00,
         -7.2411e-01, -4.3779e-01, -4.6996e-01, -2.3090e-02, -1.7901e-01],
        [-5.6536e-01, -1.8640e+00, -2.7168e-01, -4.6256e-01, -1.4012e+00,
         -2.7581e+00,  5.7649e-02,  5.4921e-01, -5.8567e-01, -7.4037e-01],
        [-6.3470e-01, -9.8841e-01, -2.8261e-01,  2.6437e-01, -2.2369e+00,
         -1.6621e+00, -1.3848e+00,  5.4283e-01,  1.5322e-01, -7.8807e-01],
        [-8.5213e-01, -1.5062e+00,  5.4082e-01,  1.3744e+00, -1.2684e+00,
         -2.5780e+00,  6.4542e-01, -8.8204e-01,  4.1358e+00, -6.8007e-01],
        [-1.5593e+00, -3.3029e+00, -9.5914e+00,  6.3933e+00,  4.6638e+00,
         -1.0899e+01, -9.8794e+00, -2.3181e+01,  6.2791e+00, -3.1555e-02],
        [-1.0593e+01, -1.0752e+00,  3.5897e+00, -9.5032e-01,  3.4683e+00,
          3.7040e+00,  1.1219e+00,  1.0997e+00,  6.8117e+00, -3.2015e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.6258, -2.9430, -3.4694,  1.2415, -4.7652, -3.2521, -3.8394, -3.6011,
        -3.5767, -6.7155], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 0.2429,  1.9345,  1.2912,  0.4347,  0.3612,  1.5968,  1.1934,  2.1978,
         -6.0359,  0.3075],
        [-0.2429, -1.9387, -1.2912, -0.3092, -0.3589, -1.5968, -1.1932, -2.1970,
          6.1767, -0.3967]], device='cuda:0'))])
xi:  [105.4326]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 498.69355336028076
W_T_median: 279.3246026179525
W_T_pctile_5: 105.42802906270698
W_T_CVAR_5_pct: -25.44436268425058
Average q (qsum/M+1):  50.57573478452621
Optimal xi:  [105.4326]
Observed VAR:  279.3246026179525
Expected(across Rb) median(across samples) p_equity:  0.2745115098853906
obj fun:  tensor(-1529.6815, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:158: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1020.6694549296891
Current xi:  [127.85857]
objective value function right now is: -1020.6694549296891
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1137.2912195897097
Current xi:  [162.11067]
objective value function right now is: -1137.2912195897097
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1151.078450653266
Current xi:  [184.09544]
objective value function right now is: -1151.078450653266
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1155.9056870722245
Current xi:  [199.34456]
objective value function right now is: -1155.9056870722245
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1163.2148692312744
Current xi:  [205.70973]
objective value function right now is: -1163.2148692312744
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.41808]
objective value function right now is: -1156.1098782449621
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1504.4015174072858
Current xi:  [190.4598]
objective value function right now is: -1504.4015174072858
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1549.4812657747796
Current xi:  [176.6071]
objective value function right now is: -1549.4812657747796
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1552.0806056578044
Current xi:  [170.93938]
objective value function right now is: -1552.0806056578044
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.31436]
objective value function right now is: -1549.0269150291056
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.88655]
objective value function right now is: -1522.5750372080804
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.25864]
objective value function right now is: -1549.7137358531436
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.89369]
objective value function right now is: -1546.8883720468475
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [166.82854]
objective value function right now is: -1549.7909494122655
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [82.86787]
objective value function right now is: 756.3575818686295
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [32.131027]
objective value function right now is: 290.80625640007247
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.058507]
objective value function right now is: -21.352617900390083
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-48.09305]
objective value function right now is: -282.15377063423034
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-82.73457]
objective value function right now is: -429.78913089492767
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-113.491714]
objective value function right now is: -601.7004085264349
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-131.06018]
objective value function right now is: -694.2236123463165
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-100.50059]
objective value function right now is: -1219.8856340978853
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-57.783493]
objective value function right now is: -1290.2382826888202
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.630898]
objective value function right now is: -1340.3427476536735
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04341219]
objective value function right now is: -1420.9247716250607
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00718402]
objective value function right now is: -1429.5196440799841
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02138705]
objective value function right now is: -1410.4781443759757
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-13.541229]
objective value function right now is: -1353.7258922381284
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04097785]
objective value function right now is: -1427.331439809483
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00084101]
objective value function right now is: -1427.537431291663
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0762807]
objective value function right now is: -1393.489420609553
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03192917]
objective value function right now is: -1430.7988567570662
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01011401]
objective value function right now is: -1433.6442523615378
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.04905223]
objective value function right now is: -1377.5649388423096
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.03464371]
objective value function right now is: -1436.1269353723192
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.91082394]
objective value function right now is: -1437.6419941383303
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.5212474]
objective value function right now is: -1442.9368306968897
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.98848]
objective value function right now is: -1447.478049428827
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [21.153276]
objective value function right now is: -1463.4096183899953
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [29.138702]
objective value function right now is: -1470.4494065044944
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [37.074074]
objective value function right now is: -1482.698585208018
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.024982]
objective value function right now is: -1491.0478053605334
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.059284]
objective value function right now is: -1497.036402270578
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.723335]
objective value function right now is: -1505.585127999596
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.24876]
objective value function right now is: -1514.1462961404973
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [75.902596]
objective value function right now is: -1520.0866349684889
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [83.35056]
objective value function right now is: -1525.2663437604406
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [90.50232]
objective value function right now is: -1530.394856812622
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [94.846565]
objective value function right now is: -1535.908808026044
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [96.39045]
objective value function right now is: -1537.1340357559802
min fval:  -1435.729600398711
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -6.3837,  -0.9585],
        [  3.9948,  -4.4687],
        [  5.2762,  -3.1815],
        [-12.5537,  -5.6711],
        [ -2.1326,  -6.0824],
        [ -0.9924,  -5.4452],
        [  4.8360,  -3.7335],
        [  6.5244,  -1.4714],
        [ -5.9452,  -1.7669],
        [  0.9041,  -5.2429]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 4.8527, -3.8961, -3.5504, -4.9222, -4.3025, -4.1238, -3.7009, -3.4871,
         5.1776, -4.1884], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 5.3527,  1.2062, -0.0370,  1.1366,  1.0854,  0.9507,  0.6306, -2.8418,
          5.1732,  0.9392],
        [-4.1747,  3.2641,  3.1416,  4.8949,  4.0486,  3.5047,  3.1655,  3.3889,
         -3.8053,  3.4215],
        [-5.1404,  3.5767,  3.4174,  5.4409,  4.5871,  3.7088,  3.6251,  3.9672,
         -5.0686,  3.8237],
        [-5.5364,  3.7375,  3.3771,  5.6375,  4.9934,  3.7761,  3.8772,  4.1526,
         -5.4424,  3.9605],
        [-4.6789,  3.2485,  3.2880,  5.3995,  4.2911,  3.6259,  3.3414,  3.8274,
         -4.4519,  3.4744],
        [-5.4405,  3.8079,  3.3294,  5.4675,  4.9457,  3.8766,  3.5178,  4.2878,
         -5.4015,  4.0380],
        [-3.4246,  3.2246,  3.1066,  4.2865,  3.9582,  3.4259,  3.1102,  2.8018,
         -3.1300,  3.2821],
        [-5.4574,  3.7363,  3.5883,  5.5608,  5.0683,  3.9216,  3.8502,  4.1468,
         -5.5636,  3.9363],
        [-5.2672,  3.9008,  3.3575,  5.3635,  4.6883,  3.9283,  3.6479,  4.0114,
         -5.2361,  3.8069],
        [-4.5609,  3.3952,  3.0773,  5.0686,  4.3148,  3.5460,  3.3504,  3.8170,
         -4.3349,  3.5779]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 2.4502, -4.1929, -2.6325, -2.2774, -3.3561, -2.3552, -5.3822, -2.3341,
        -2.5352, -3.4757], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[13.4660, -5.3445, -7.0923, -7.0768, -5.5078, -6.8738, -5.0723, -6.8802,
         -6.6804, -6.3292]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-5.2435,  7.8828],
        [ 2.4409,  4.4028],
        [-7.6239, -2.2717],
        [ 6.8178,  2.1294],
        [-6.8887, -1.9628],
        [-2.7000,  7.7053],
        [-0.3910,  7.5588],
        [-7.2662, -2.0578],
        [-7.0734, -2.1107],
        [ 6.4887,  1.7152]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 7.0175, -0.2617, -0.8082,  0.8175, -0.7893,  6.4881,  6.8490, -0.5320,
        -1.0029, -1.9153], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-8.8275e-01,  6.0656e-02, -1.0258e+00, -3.1418e+00, -9.8188e-01,
         -1.0023e+00, -1.4953e+00, -8.5076e-01, -8.3327e-01, -1.8773e+00],
        [ 3.0252e+00, -1.1861e+01,  6.1485e+00, -7.8601e+00,  4.4935e+00,
         -9.3948e+00, -1.9261e+01,  5.2439e+00,  5.7112e+00, -6.8924e+00],
        [-3.3324e-01, -1.4870e+00, -6.5828e-01, -4.5273e+00, -6.5687e-01,
         -4.1420e-01, -1.7229e+00, -7.5429e-01, -5.9243e-01, -3.2160e+00],
        [-2.1176e-01, -1.1958e+00, -4.0085e-01, -4.6810e+00, -4.2136e-01,
         -4.3738e-01, -1.7438e+00, -5.3112e-01, -3.2756e-01, -3.2370e+00],
        [-7.2510e+00, -5.4858e-01,  8.9764e+00, -3.8068e+00,  6.2150e+00,
         -1.2957e+01, -1.2807e+01,  7.8299e+00,  7.0506e+00, -2.0711e+00],
        [ 7.9131e-01, -1.5653e+00,  3.3588e+00, -5.7293e+00,  2.3624e+00,
         -3.6916e+00, -1.0996e+01,  2.7940e+00,  2.5345e+00, -5.3463e+00],
        [ 2.2398e+01, -7.0914e-02, -7.1361e+00,  7.6085e-01, -6.8800e+00,
          1.0970e+01, -7.1050e-02, -7.9726e+00, -7.0855e+00,  1.6221e+00],
        [ 2.2933e+00, -5.8300e+00,  1.2395e-01, -3.2910e+00,  2.9600e-01,
          1.6888e+00,  1.0011e+00, -6.6719e-01,  1.1844e+00, -3.6531e+00],
        [-7.2432e-01,  6.1548e-02, -9.9498e-01, -3.1481e+00, -9.5573e-01,
         -8.9876e-01, -1.4847e+00, -8.3893e-01, -8.1483e-01, -1.7001e+00],
        [-8.9741e-01,  1.1229e-03, -9.6738e-01, -3.1479e+00, -9.3961e-01,
         -1.0256e+00, -1.5164e+00, -8.7053e-01, -8.0989e-01, -1.9626e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.0040, -3.3012, -5.0606, -4.9231, -0.7834, -3.8916, -1.0304, -3.7435,
        -4.1697, -4.0035], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.3584,  12.6406,  -0.1922,  -0.3308,  -6.6449,   1.6856,   0.4858,
           0.9307,  -0.2255,  -0.2292],
        [  0.2379, -12.1936,   0.3290,   0.4083,   6.6222,  -1.6572,  -0.3636,
          -1.0009,   0.2906,   0.2646]], device='cuda:0'))])
xi:  [68.24876]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 741.16910782283
W_T_median: 445.9885565430496
W_T_pctile_5: 175.2774781754364
W_T_CVAR_5_pct: 13.413439987368916
Average q (qsum/M+1):  48.786798292590724
Optimal xi:  [68.24876]
Observed VAR:  445.9885565430496
Expected(across Rb) median(across samples) p_equity:  0.29127056946357094
obj fun:  tensor(-1435.7296, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:196: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:158: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -990.4028558001028
Current xi:  [121.86506]
objective value function right now is: -990.4028558001028
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1137.9268682434126
Current xi:  [158.12549]
objective value function right now is: -1137.9268682434126
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.99231]
objective value function right now is: -1123.5224095262047
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1205.2072620020215
Current xi:  [197.89056]
objective value function right now is: -1205.2072620020215
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1208.2732886390372
Current xi:  [205.77573]
objective value function right now is: -1208.2732886390372
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.38632]
objective value function right now is: -1193.263862183153
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1220.0488942841844
Current xi:  [213.11972]
objective value function right now is: -1220.0488942841844
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1224.9242713460196
Current xi:  [214.84338]
objective value function right now is: -1224.9242713460196
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.76161]
objective value function right now is: -1222.1195645756982
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.12053]
objective value function right now is: 8717.248300683506
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [123.589615]
objective value function right now is: 409.2295528378603
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.99516]
objective value function right now is: -861.6752413961107
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [119.50613]
objective value function right now is: -890.4010891890063
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [73.65223]
objective value function right now is: 1653.809185585295
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [50.842762]
objective value function right now is: -481.8565548056604
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.417168]
objective value function right now is: -1116.053388448473
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1440.6108605552872
Current xi:  [83.02298]
objective value function right now is: -1440.6108605552872
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1476.023717209975
Current xi:  [106.440285]
objective value function right now is: -1476.023717209975
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1505.7124438582837
Current xi:  [120.89014]
objective value function right now is: -1505.7124438582837
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1522.0018519457142
Current xi:  [147.42392]
objective value function right now is: -1522.0018519457142
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1565.50087538424
Current xi:  [164.49438]
objective value function right now is: -1565.50087538424
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [176.08788]
objective value function right now is: -1465.398233391723
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1582.0212366109035
Current xi:  [178.4737]
objective value function right now is: -1582.0212366109035
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1584.5392777408301
Current xi:  [184.4863]
objective value function right now is: -1584.5392777408301
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1592.0151843588887
Current xi:  [187.00528]
objective value function right now is: -1592.0151843588887
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.47377]
objective value function right now is: -1565.1516360502712
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.58531]
objective value function right now is: -1576.9407075903166
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [186.06432]
objective value function right now is: -1575.8855741122095
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [182.60498]
objective value function right now is: -1574.095957432486
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.61485]
objective value function right now is: -1569.0342247716492
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.44911]
objective value function right now is: -1576.9026685662081
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.1616]
objective value function right now is: -1566.5748617400473
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.21408]
objective value function right now is: -1581.123380203107
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.26686]
objective value function right now is: -1491.066474513852
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.19864]
objective value function right now is: -1584.691820989276
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1592.7273674211538
Current xi:  [184.32854]
objective value function right now is: -1592.7273674211538
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1597.3540606696242
Current xi:  [185.1448]
objective value function right now is: -1597.3540606696242
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1597.6190156617517
Current xi:  [185.49367]
objective value function right now is: -1597.6190156617517
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.75874]
objective value function right now is: -1591.2378298798103
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.3774]
objective value function right now is: -1594.3490262580453
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.58499]
objective value function right now is: -1596.050145662733
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.25691]
objective value function right now is: -1585.715347686783
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1598.161670066108
Current xi:  [186.27289]
objective value function right now is: -1598.161670066108
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.77953]
objective value function right now is: -1595.4546035996948
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.23917]
objective value function right now is: -1581.7492431792036
new min fval from sgd:  -1598.213902121148
new min fval from sgd:  -1598.3793018818335
new min fval from sgd:  -1598.6086325323934
new min fval from sgd:  -1598.9858439773147
new min fval from sgd:  -1599.1663047127327
new min fval from sgd:  -1599.328956877611
new min fval from sgd:  -1599.454841325486
new min fval from sgd:  -1599.6334986453198
new min fval from sgd:  -1599.72705493062
new min fval from sgd:  -1599.95963569789
new min fval from sgd:  -1600.135183041717
new min fval from sgd:  -1600.1624505522732
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.61354]
objective value function right now is: -1597.2676822924307
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.35973]
objective value function right now is: -1588.3281580324556
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.27362]
objective value function right now is: -1597.9666600832581
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.38025]
objective value function right now is: -1599.6507820082372
new min fval from sgd:  -1600.1863271483464
new min fval from sgd:  -1600.1999233559611
new min fval from sgd:  -1600.2119760841917
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.49529]
objective value function right now is: -1600.0651335953123
min fval:  -1600.2119760841917
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-31.8650, -12.5954],
        [ 10.8188,  -0.2009],
        [ 10.7428,  -0.0339],
        [  9.9534,  -3.9428],
        [  8.1868,  -7.8705],
        [  8.9248,  -1.3810],
        [ 10.2648,  -0.9589],
        [  2.1270, -12.5119],
        [  9.7552,  -1.0603],
        [  9.9122,  -4.2953]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-11.3819,  -9.3991,  -9.4762,  -6.8419,  -7.0460,  -8.8824,  -8.4267,
         -9.4776,  -8.5947,  -6.8327], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.3277e-02, -1.1041e-01, -1.0828e-01, -2.4386e-01, -3.7082e-01,
         -3.4433e-02, -1.1906e-01, -3.5483e-01, -7.4278e-02, -2.4994e-01],
        [-9.3863e-02,  4.7575e-04, -9.4657e-03,  3.8606e-01,  5.6564e-01,
          5.4093e-02,  5.3340e-02,  1.4587e+00,  4.6126e-02,  3.7708e-01],
        [-2.3277e-02, -1.1041e-01, -1.0828e-01, -2.4386e-01, -3.7082e-01,
         -3.4433e-02, -1.1906e-01, -3.5483e-01, -7.4278e-02, -2.4994e-01],
        [ 1.0819e+01,  6.6459e+00,  6.6781e+00,  2.8835e+00,  4.9268e+00,
          2.1035e+00,  3.9963e+00,  1.2636e+01,  3.1067e+00,  3.1225e+00],
        [ 1.3696e+01,  7.2598e+00,  7.2852e+00,  4.4799e+00,  7.2349e+00,
          3.2382e+00,  4.9237e+00,  1.3874e+01,  4.1249e+00,  4.9159e+00],
        [ 1.3376e+01,  7.5163e+00,  7.5556e+00,  5.0102e+00,  8.2438e+00,
          2.9858e+00,  4.6516e+00,  1.5425e+01,  3.8549e+00,  5.5851e+00],
        [-2.3277e-02, -1.1041e-01, -1.0828e-01, -2.4386e-01, -3.7082e-01,
         -3.4433e-02, -1.1906e-01, -3.5483e-01, -7.4278e-02, -2.4994e-01],
        [ 1.0909e-01,  4.3416e-01,  4.3571e-01,  5.7780e-01,  6.7370e-01,
          1.1003e-01,  4.6512e-01,  9.6543e-01,  2.9250e-01,  6.1963e-01],
        [-2.3277e-02, -1.1041e-01, -1.0828e-01, -2.4386e-01, -3.7082e-01,
         -3.4433e-02, -1.1906e-01, -3.5483e-01, -7.4278e-02, -2.4994e-01],
        [-7.5167e-01,  2.9683e+00,  3.0922e+00,  8.9047e+00,  7.7577e+00,
          2.9537e-01,  2.4507e+00, -8.4559e+00,  1.4208e+00,  1.0341e+01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ -1.7218,  -4.8756,  -1.7218, -10.5217, -11.5146, -11.6845,  -1.7218,
          4.9962,  -1.7218,  -1.3411], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0171,  -0.9369,   0.0171,  -7.6107, -11.6791, -14.1452,   0.0171,
          10.3738,   0.0171,   6.8074]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -6.6052,  -0.5782],
        [ -2.3921,   0.1622],
        [ -2.3424,   0.1502],
        [-14.7715,  -5.8714],
        [-14.1136,  -2.8486],
        [  1.9563,   0.3428],
        [ -2.3716,   0.1571],
        [ -5.2854, -10.3677],
        [ -2.2987,   0.1396],
        [ -2.1835,   0.5314]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 3.1707, -6.0885, -6.0879, -2.0550,  6.0155,  8.9252, -6.0872, -8.2212,
        -6.0863, -7.6145], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 3.4007e+00, -1.3585e-02, -1.0026e-02, -1.9286e-01, -3.3463e+00,
         -6.9390e+00, -1.2105e-02,  1.9453e-01, -6.5103e-03, -6.8778e-02],
        [ 2.0948e+00, -4.1855e-01, -3.9381e-01,  1.6598e+01,  3.9353e-01,
         -2.8008e+01, -4.1337e-01,  2.1664e+01, -3.3393e-01, -1.7412e-01],
        [-4.0818e-01, -1.1542e-02, -1.1580e-02,  3.6305e-02, -1.5256e+00,
         -3.9147e+00, -1.1557e-02,  2.1069e-01, -1.1625e-02, -1.8349e-02],
        [-4.4568e-01, -1.2099e-02, -1.2136e-02,  6.9570e-02, -1.5323e+00,
         -3.9252e+00, -1.2114e-02,  2.2931e-01, -1.2180e-02, -1.9085e-02],
        [-7.6339e+00, -1.1937e-02, -6.5113e-03,  1.5464e+01,  6.6438e+00,
         -4.7851e+00, -1.3908e-02, -4.8531e+00, -1.5358e-02, -2.7850e-02],
        [ 7.4072e-01,  2.1681e-01,  1.5885e-01,  2.6117e+00,  1.2150e+00,
          1.0134e+00,  1.9113e-01, -6.4811e+00,  1.1224e-01,  4.3619e-01],
        [-2.3983e+00, -7.4619e-02, -8.4596e-02, -5.0664e+00, -7.7803e+00,
         -1.9181e+00, -7.8302e-02,  1.5109e+00, -9.5687e-02, -5.8563e-01],
        [ 1.0190e+00, -2.0980e-01, -1.3571e-01, -2.1553e+01, -6.9306e+00,
          3.9534e-01, -1.7687e-01,  1.5803e+00, -7.9346e-02, -6.5673e-01],
        [-3.7843e-01, -1.3101e-02, -1.3116e-02,  1.3152e-01, -1.6044e+00,
         -3.9208e+00, -1.3107e-02,  2.5854e-01, -1.3139e-02, -2.0825e-02],
        [ 2.4170e+00, -3.2309e-02, -3.2800e-02,  2.3253e+00,  9.4527e+00,
         -1.6118e+01, -3.4186e-02,  1.1570e+01, -4.9375e-02, -1.8571e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-0.3577, -7.3389, -3.9459, -3.9326, -5.4378, -1.3920, -3.9339, -3.0844,
        -3.9213, -4.7659], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  3.0833,  21.7782,   0.9780,   1.0104,   1.6602,   0.8077,   3.0017,
           2.0052,   1.0763,  -9.3218],
        [ -3.0825, -21.9946,  -0.9801,  -1.0098,  -1.3407,  -0.7261,  -3.1649,
          -2.0701,  -1.0744,   9.3649]], device='cuda:0'))])
xi:  [187.47363]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 660.2330655627388
W_T_median: 405.3701638497341
W_T_pctile_5: 187.8270822304984
W_T_CVAR_5_pct: 21.327080862994627
Average q (qsum/M+1):  48.180404170866936
Optimal xi:  [187.47363]
Observed VAR:  405.3701638497341
Expected(across Rb) median(across samples) p_equity:  0.24841728148361047
obj fun:  tensor(-1600.2120, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:196: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:158: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -417.42609043096087
Current xi:  [123.5035]
objective value function right now is: -417.42609043096087
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1145.8652016654266
Current xi:  [156.83752]
objective value function right now is: -1145.8652016654266
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1351.0403291524783
Current xi:  [176.7463]
objective value function right now is: -1351.0403291524783
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1359.4837257546806
Current xi:  [190.47481]
objective value function right now is: -1359.4837257546806
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1455.9567337545952
Current xi:  [193.89032]
objective value function right now is: -1455.9567337545952
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.67174]
objective value function right now is: -1387.1915971075236
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [200.85837]
objective value function right now is: -1410.6122769575165
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.63068]
objective value function right now is: -1387.3234539615862
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.07077]
objective value function right now is: -1405.5479251342958
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1503.477306890384
Current xi:  [201.55585]
objective value function right now is: -1503.477306890384
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1523.907677273186
Current xi:  [202.00693]
objective value function right now is: -1523.907677273186
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.08534]
objective value function right now is: -1468.0612933205064
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.73747]
objective value function right now is: -1493.155943530668
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [200.35011]
objective value function right now is: -1401.9528490139303
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1561.2556348617297
Current xi:  [199.16948]
objective value function right now is: -1561.2556348617297
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.93625]
objective value function right now is: -1178.9368595070953
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.22609]
objective value function right now is: -1495.5839745741687
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.85222]
objective value function right now is: -1439.0747474425675
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.50717]
objective value function right now is: -1321.2959023595208
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1569.1684527738746
Current xi:  [203.92119]
objective value function right now is: -1569.1684527738746
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1569.654716998663
Current xi:  [203.82027]
objective value function right now is: -1569.654716998663
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.45389]
objective value function right now is: -1505.6592514968354
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.72627]
objective value function right now is: -1530.9353084830539
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [202.7477]
objective value function right now is: -1514.255942044077
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.31343]
objective value function right now is: -1305.5126087405404
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [202.06824]
objective value function right now is: -1483.3756351283837
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [202.8008]
objective value function right now is: -1513.812235164124
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [203.78038]
objective value function right now is: -1549.8395391785436
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1624.7744641045663
Current xi:  [204.16028]
objective value function right now is: -1624.7744641045663
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.58655]
objective value function right now is: -1381.8378582793293
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [202.36879]
objective value function right now is: -1608.8114468855083
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.10199]
objective value function right now is: -1433.3266536405465
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [202.37134]
objective value function right now is: -1399.6913313905281
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.65764]
objective value function right now is: -1533.8250166374207
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [202.54645]
objective value function right now is: -1492.0120681040082
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1627.4504690720644
Current xi:  [203.7]
objective value function right now is: -1627.4504690720644
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1644.625863801918
Current xi:  [204.23299]
objective value function right now is: -1644.625863801918
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.1078]
objective value function right now is: -1611.1655892793306
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.06102]
objective value function right now is: -1626.1497301689117
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.42992]
objective value function right now is: -1621.1621422735789
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.69398]
objective value function right now is: -1634.7079243835249
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1645.6432693349395
Current xi:  [206.30576]
objective value function right now is: -1645.6432693349395
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1647.8165329379206
Current xi:  [206.34148]
objective value function right now is: -1647.8165329379206
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.86252]
objective value function right now is: -1646.916626051126
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.67403]
objective value function right now is: -1643.4079281792256
new min fval from sgd:  -1648.0308559394837
new min fval from sgd:  -1648.7530252327738
new min fval from sgd:  -1650.3962296678462
new min fval from sgd:  -1652.0629641439464
new min fval from sgd:  -1653.3761962667704
new min fval from sgd:  -1654.925216786199
new min fval from sgd:  -1656.609338741194
new min fval from sgd:  -1657.36394595097
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.93077]
objective value function right now is: -1630.9579966624071
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.93774]
objective value function right now is: -1618.271593881797
new min fval from sgd:  -1658.193979005803
new min fval from sgd:  -1658.2507519248684
new min fval from sgd:  -1658.3127525321424
new min fval from sgd:  -1658.4956457706373
new min fval from sgd:  -1658.6347106521232
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.58723]
objective value function right now is: -1519.8817072434258
new min fval from sgd:  -1658.851092965778
new min fval from sgd:  -1659.2387358529654
new min fval from sgd:  -1659.4776125210083
new min fval from sgd:  -1659.598346536795
new min fval from sgd:  -1659.682165070945
new min fval from sgd:  -1659.718821976293
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.66493]
objective value function right now is: -1655.6600251540933
new min fval from sgd:  -1659.7416000654484
new min fval from sgd:  -1659.8223836610016
new min fval from sgd:  -1659.8297328072458
new min fval from sgd:  -1659.9492913572003
new min fval from sgd:  -1660.0837172074998
new min fval from sgd:  -1660.1823821574742
new min fval from sgd:  -1660.251512693711
new min fval from sgd:  -1660.2548910773696
new min fval from sgd:  -1660.28582100573
new min fval from sgd:  -1660.4373013402792
new min fval from sgd:  -1660.498636792261
new min fval from sgd:  -1660.50591301745
new min fval from sgd:  -1660.6316130895962
new min fval from sgd:  -1660.799455222405
new min fval from sgd:  -1660.9388512743903
new min fval from sgd:  -1661.0177033905418
new min fval from sgd:  -1661.05773527628
new min fval from sgd:  -1661.064226372992
new min fval from sgd:  -1661.103278243964
new min fval from sgd:  -1661.1451399936673
new min fval from sgd:  -1661.1793753675
new min fval from sgd:  -1661.2236226018456
new min fval from sgd:  -1661.2705212199864
new min fval from sgd:  -1661.3117323113718
new min fval from sgd:  -1661.3483387549506
new min fval from sgd:  -1661.3700885522944
new min fval from sgd:  -1661.3840805626414
new min fval from sgd:  -1661.3854440134012
new min fval from sgd:  -1661.412116985674
new min fval from sgd:  -1661.4137900345102
new min fval from sgd:  -1661.43851510007
new min fval from sgd:  -1661.5577423280033
new min fval from sgd:  -1661.6377159012688
new min fval from sgd:  -1661.7009533856076
new min fval from sgd:  -1661.7187571171164
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.74806]
objective value function right now is: -1656.2693625921113
min fval:  -1661.7187571171164
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.1591, -0.2487],
        [ 0.1591, -0.2487],
        [ 0.1591, -0.2487],
        [ 0.1591, -0.2487],
        [ 0.1591, -0.2487],
        [ 0.1591, -0.2487],
        [ 0.1591, -0.2487],
        [ 0.1591, -0.2487],
        [ 0.1591, -0.2487],
        [ 0.1591, -0.2487]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822,
        0.2822], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822,
         0.2822],
        [0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822,
         0.2822],
        [0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822,
         0.2822],
        [0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822,
         0.2822],
        [0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822,
         0.2822],
        [0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822,
         0.2822],
        [0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822,
         0.2822],
        [0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822,
         0.2822],
        [0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822,
         0.2822],
        [0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822, 0.2822,
         0.2822]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,
        0.4382], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-2.1369, -2.1369, -2.1369, -2.1369, -2.1369, -2.1369, -2.1369, -2.1369,
         -2.1369, -2.1369]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  5.3006,  10.0499],
        [ 12.1980,   6.3611],
        [ 12.9874,  -0.1872],
        [  3.2522,  12.3502],
        [  0.6737,   2.9267],
        [ -8.7039,  -3.6813],
        [-12.6991,  -5.5930],
        [ -5.9228,  10.2440],
        [-11.7891,  -4.8717],
        [ -2.8415,   0.5833]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  1.6732,   0.9919, -12.6991,   8.7161, -10.1386, -10.6844,  -3.0349,
          7.9707,  -5.1964,  -7.1009], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.0250e-01, -3.0112e+00, -7.4709e-01, -6.1786e-01,  1.6147e-01,
          1.1779e-03, -6.9065e-01, -6.3668e-02, -4.7535e-02,  3.9055e-04],
        [-2.0250e-01, -3.0112e+00, -7.4711e-01, -6.1788e-01,  1.6146e-01,
          1.1780e-03, -6.9063e-01, -6.3674e-02, -4.7535e-02,  3.9063e-04],
        [-2.0251e-01, -3.0112e+00, -7.4708e-01, -6.1786e-01,  1.6147e-01,
          1.1778e-03, -6.9064e-01, -6.3667e-02, -4.7535e-02,  3.9054e-04],
        [-9.2529e-01, -1.1022e+00, -4.0521e+00,  1.7863e+00, -8.6082e-01,
         -7.4088e-01,  9.9892e-01,  3.6837e+00,  5.5670e+00,  6.0847e-02],
        [-2.3433e-01, -3.1877e+00, -2.5395e+00,  1.4782e-01, -1.2758e-01,
         -4.7023e-03, -3.5767e-01,  2.1206e+00,  2.9344e-03, -8.0258e-03],
        [-1.0367e+00,  1.5944e+00,  2.6868e+00,  1.1284e+00,  7.2215e-02,
          3.7859e-02, -1.3293e+01,  2.8365e+01, -8.1372e+00, -3.6631e-01],
        [-2.0250e-01, -3.0113e+00, -7.4706e-01, -6.1784e-01,  1.6148e-01,
          1.1777e-03, -6.9066e-01, -6.3662e-02, -4.7535e-02,  3.9050e-04],
        [-2.2909e+00, -3.5805e-01, -8.3266e+00,  2.3979e+00, -1.0963e+00,
         -2.2222e+00,  5.3644e-01,  1.5004e+00,  8.7911e+00, -2.4046e-01],
        [-2.8075e+01, -6.6733e+00, -9.5778e+00, -2.2895e+01, -6.3691e-02,
          2.2538e+00,  7.9973e+00, -3.4281e+00,  4.7100e+00, -7.9702e-01],
        [-7.0395e+00, -1.0057e+00, -1.0471e+01, -1.4993e+01, -1.7503e-02,
         -2.6638e-01,  7.0148e+00, -7.0932e+00,  8.6995e+00, -2.0182e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-7.8235, -7.8235, -7.8235, -6.6434, -8.9766, -1.4029, -7.8235, -5.3611,
        -1.3123, -2.1188], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-0.0212, -0.0228, -0.0218,  3.9355,  0.7470,  0.4762, -0.0234,  2.6255,
         -9.4101, -5.3672],
        [ 0.0242,  0.0226,  0.0236, -3.9783, -0.7485, -0.5684,  0.0220, -2.7688,
          9.3002,  5.4310]], device='cuda:0'))])
xi:  [206.68324]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1099.431562777883
W_T_median: 868.2247987140527
W_T_pctile_5: 206.5978727967703
W_T_CVAR_5_pct: 11.534390615621005
Average q (qsum/M+1):  35.0
Optimal xi:  [206.68324]
Observed VAR:  868.2247987140527
Expected(across Rb) median(across samples) p_equity:  0.19768950616319975
obj fun:  tensor(-1661.7188, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:196: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:158: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
