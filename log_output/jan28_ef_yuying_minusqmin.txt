Starting at: 
28-01-23_09:52

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 40000
batchsize: 1000
remove neg:  False


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 4000, 'itbound_SGD_algorithms': 40000, 'nit_IterateAveragingStart': 36000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.02, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1692.1244823651152
Current xi:  [-14.665979]
objective value function right now is: -1692.1244823651152
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1704.8871371568675
Current xi:  [-30.02141]
objective value function right now is: -1704.8871371568675
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1706.6324700060752
Current xi:  [-44.777546]
objective value function right now is: -1706.6324700060752
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1707.1051327292134
Current xi:  [-59.480003]
objective value function right now is: -1707.1051327292134
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1713.9014819946042
Current xi:  [-74.001854]
objective value function right now is: -1713.9014819946042
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-87.66359]
objective value function right now is: -1712.3487823372982
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1718.7237882185357
Current xi:  [-102.47197]
objective value function right now is: -1718.7237882185357
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-116.984825]
objective value function right now is: -1714.3286709464398
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-132.05185]
objective value function right now is: -1691.4101530050073
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1723.999109381324
Current xi:  [-146.36365]
objective value function right now is: -1723.999109381324
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-159.25424]
objective value function right now is: -1723.8930612695763
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.883584098063
Current xi:  [-173.7468]
objective value function right now is: -1725.883584098063
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-187.67332]
objective value function right now is: -1713.1030830481457
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-201.28166]
objective value function right now is: -1723.1021298209787
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1729.2133643947723
Current xi:  [-214.99]
objective value function right now is: -1729.2133643947723
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-229.56018]
objective value function right now is: -1724.0198371260788
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1732.5385104333404
Current xi:  [-243.94518]
objective value function right now is: -1732.5385104333404
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-257.66476]
objective value function right now is: -1730.9324651178042
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-271.65195]
objective value function right now is: -1726.6537958531833
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-285.64517]
objective value function right now is: -1732.0818589298335
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1732.979362048705
Current xi:  [-297.16714]
objective value function right now is: -1732.979362048705
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1733.893768997399
Current xi:  [-308.19473]
objective value function right now is: -1733.893768997399
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-314.53275]
objective value function right now is: -1732.6381600550553
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-334.55783]
objective value function right now is: -1733.2109701658871
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-346.34778]
objective value function right now is: -1733.667808987728
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1737.8074928736137
Current xi:  [-358.8636]
objective value function right now is: -1737.8074928736137
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-372.852]
objective value function right now is: -1730.086528395276
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1738.0564287803224
Current xi:  [-383.59235]
objective value function right now is: -1738.0564287803224
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-394.55954]
objective value function right now is: -1727.6216594608193
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-413.85486]
objective value function right now is: -1728.832531757838
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-424.61487]
objective value function right now is: -1733.8432654215092
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-432.21738]
objective value function right now is: -1737.3633125329798
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-442.08865]
objective value function right now is: -1726.7486293716013
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-457.49756]
objective value function right now is: -1734.308996683878
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-464.42136]
objective value function right now is: -1733.8290729745454
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1738.9776660984335
Current xi:  [-462.98264]
objective value function right now is: -1738.9776660984335
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-463.493]
objective value function right now is: -1738.3006445838082
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1739.8065898226234
Current xi:  [-466.86826]
objective value function right now is: -1739.8065898226234
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1740.1288664470917
Current xi:  [-469.51367]
objective value function right now is: -1740.1288664470917
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1740.9551717151242
Current xi:  [-471.66718]
objective value function right now is: -1740.9551717151242
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-472.86606]
objective value function right now is: -1740.296082526875
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-475.97433]
objective value function right now is: -1740.3436573770477
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-478.83942]
objective value function right now is: -1740.1557499274356
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-480.70386]
objective value function right now is: -1740.063270884801
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-480.53235]
objective value function right now is: -1740.8322235664364
new min fval from sgd:  -1740.96552859607
new min fval from sgd:  -1740.9795538688052
new min fval from sgd:  -1740.993601251612
new min fval from sgd:  -1741.0012456366328
new min fval from sgd:  -1741.0072485312992
new min fval from sgd:  -1741.0160120471005
new min fval from sgd:  -1741.0211591966688
new min fval from sgd:  -1741.0299142873728
new min fval from sgd:  -1741.0377938529693
new min fval from sgd:  -1741.0418533454022
new min fval from sgd:  -1741.044339413385
new min fval from sgd:  -1741.0491846211874
new min fval from sgd:  -1741.0502009355791
new min fval from sgd:  -1741.0514533815488
new min fval from sgd:  -1741.0554497581663
new min fval from sgd:  -1741.056661072346
new min fval from sgd:  -1741.0572843598272
new min fval from sgd:  -1741.057670868814
new min fval from sgd:  -1741.0596115522735
new min fval from sgd:  -1741.063539017531
new min fval from sgd:  -1741.0653653279062
new min fval from sgd:  -1741.0678117909565
new min fval from sgd:  -1741.071787767437
new min fval from sgd:  -1741.074744152194
new min fval from sgd:  -1741.0774039722755
new min fval from sgd:  -1741.0791398084878
new min fval from sgd:  -1741.0802270915724
new min fval from sgd:  -1741.0832602585722
new min fval from sgd:  -1741.085308888985
new min fval from sgd:  -1741.0858072171955
new min fval from sgd:  -1741.0858993152012
new min fval from sgd:  -1741.0877266006037
new min fval from sgd:  -1741.0880748782008
new min fval from sgd:  -1741.1000751730287
new min fval from sgd:  -1741.1046117189583
new min fval from sgd:  -1741.1095635397094
new min fval from sgd:  -1741.1108632270596
new min fval from sgd:  -1741.1120903979333
new min fval from sgd:  -1741.1124964970757
new min fval from sgd:  -1741.1497630417296
new min fval from sgd:  -1741.177004406103
new min fval from sgd:  -1741.1866864695717
new min fval from sgd:  -1741.2030154137599
new min fval from sgd:  -1741.2128358570642
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-483.11658]
objective value function right now is: -1740.2644588603257
new min fval from sgd:  -1741.2144460565974
new min fval from sgd:  -1741.2168842634571
new min fval from sgd:  -1741.2209070028048
new min fval from sgd:  -1741.2234374888312
new min fval from sgd:  -1741.227270705439
new min fval from sgd:  -1741.2295658513037
new min fval from sgd:  -1741.2301543157907
new min fval from sgd:  -1741.232406293288
new min fval from sgd:  -1741.2333148424407
new min fval from sgd:  -1741.235535000567
new min fval from sgd:  -1741.2373753173579
new min fval from sgd:  -1741.23951090944
new min fval from sgd:  -1741.2422896766539
new min fval from sgd:  -1741.2437616317798
new min fval from sgd:  -1741.2443176263268
new min fval from sgd:  -1741.2444495703382
new min fval from sgd:  -1741.2446661661681
new min fval from sgd:  -1741.2448869772807
new min fval from sgd:  -1741.2458407080385
new min fval from sgd:  -1741.2511426713393
new min fval from sgd:  -1741.2552006803346
new min fval from sgd:  -1741.2606520883571
new min fval from sgd:  -1741.2652977078867
new min fval from sgd:  -1741.2693347817724
new min fval from sgd:  -1741.2731528375755
new min fval from sgd:  -1741.276679535871
new min fval from sgd:  -1741.2772759459103
new min fval from sgd:  -1741.2806388051604
new min fval from sgd:  -1741.2843043242276
new min fval from sgd:  -1741.2870223381049
new min fval from sgd:  -1741.290233627471
new min fval from sgd:  -1741.2927155002326
new min fval from sgd:  -1741.2945006122434
new min fval from sgd:  -1741.2964046492625
new min fval from sgd:  -1741.2989783226471
new min fval from sgd:  -1741.3023560801657
new min fval from sgd:  -1741.309395184721
new min fval from sgd:  -1741.3144404479474
new min fval from sgd:  -1741.326672727441
new min fval from sgd:  -1741.3342005967468
new min fval from sgd:  -1741.340312037018
new min fval from sgd:  -1741.3470735394133
new min fval from sgd:  -1741.3512435848852
new min fval from sgd:  -1741.362525724318
new min fval from sgd:  -1741.3706378957652
new min fval from sgd:  -1741.3713640860406
new min fval from sgd:  -1741.3724215736893
new min fval from sgd:  -1741.3732299787134
new min fval from sgd:  -1741.374045014488
new min fval from sgd:  -1741.375004377734
new min fval from sgd:  -1741.3777342907906
new min fval from sgd:  -1741.378739248687
new min fval from sgd:  -1741.3825498632561
new min fval from sgd:  -1741.410617493575
new min fval from sgd:  -1741.4292831560886
new min fval from sgd:  -1741.4410753390073
new min fval from sgd:  -1741.441582578192
new min fval from sgd:  -1741.444354268431
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-485.04352]
objective value function right now is: -1739.8225085875724
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-485.79953]
objective value function right now is: -1741.2206408660597
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-487.72226]
objective value function right now is: -1741.0249096978466
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-488.18872]
objective value function right now is: -1741.1427359551453
min fval:  -1741.444354268431
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-3.8450, -4.3620],
        [ 2.9779,  4.2212],
        [-3.7975, -4.0846],
        [-3.5673, -3.5340],
        [ 9.7729, -3.2263],
        [ 1.9048,  3.9846],
        [-2.0030,  1.1459],
        [-3.4490, -3.1787],
        [-3.4637, -3.8259],
        [ 8.0592,  2.5966]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-5.3997,  3.7731, -5.1873, -5.4459, -9.2191,  1.9773, -0.7738, -5.7702,
        -5.4755, -2.0457], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 2.1094e+00, -6.2087e+00,  2.0207e+00,  1.3386e+00,  5.4698e+00,
         -1.7284e+00, -1.7132e+00,  1.0468e+00,  2.1004e+00, -5.9818e+00],
        [ 1.9026e-01, -2.7478e+00, -5.5605e-02, -2.0218e-01,  2.7867e+00,
         -1.1863e-01, -6.5186e-05, -9.7379e-02, -6.2835e-03, -2.9567e+00],
        [ 2.7510e+00, -6.0717e+00,  2.3646e+00,  1.8679e+00,  5.9678e+00,
         -1.7091e+00, -1.8120e+00,  1.1810e+00,  2.0889e+00, -6.1560e+00],
        [ 2.4910e+00, -6.1629e+00,  2.5627e+00,  1.7912e+00,  5.7162e+00,
         -2.0923e+00, -1.9224e+00,  1.3367e+00,  2.2450e+00, -6.0517e+00],
        [ 2.5130e+00, -6.2692e+00,  2.3485e+00,  1.7775e+00,  5.8452e+00,
         -2.0091e+00, -1.8597e+00,  1.4628e+00,  2.2683e+00, -6.1006e+00],
        [ 2.5882e+00, -6.3313e+00,  2.5831e+00,  1.4736e+00,  6.1023e+00,
         -1.9188e+00, -2.0423e+00,  1.3553e+00,  2.1132e+00, -5.6513e+00],
        [ 1.2831e+00,  1.1822e+00,  1.2914e+00,  7.6431e-01,  5.3280e-01,
          2.0568e-01,  1.9899e-01,  3.4185e-01,  9.8875e-01,  9.6001e-01],
        [ 2.3773e+00, -6.2607e+00,  2.5075e+00,  1.5327e+00,  5.7202e+00,
         -1.5720e+00, -1.8572e+00,  1.1960e+00,  2.1798e+00, -5.7864e+00],
        [ 2.8079e+00, -6.3293e+00,  2.3878e+00,  1.8216e+00,  6.1306e+00,
         -1.9365e+00, -2.2484e+00,  1.3458e+00,  2.4173e+00, -5.9751e+00],
        [ 2.5207e+00, -6.2840e+00,  2.2977e+00,  1.8414e+00,  5.9856e+00,
         -1.7213e+00, -1.7253e+00,  1.0094e+00,  2.1053e+00, -5.9193e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-3.1926, -3.6704, -3.4129, -3.3600, -3.2851, -3.2380,  3.1445, -3.3231,
        -3.2846, -3.2722], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-2.8466, -0.7578, -3.6151, -3.4197, -3.5606, -3.4919,  5.6976, -3.3341,
         -3.8554, -3.6202]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  5.2697,   5.2468],
        [ -1.9907,  -4.4607],
        [ -0.4719,   5.0765],
        [ -3.5802,   0.2627],
        [ -5.9119,  -5.8882],
        [ -8.2985,  -7.3100],
        [-13.0353,  -3.0462],
        [  5.2018,  -0.3760],
        [ -3.8702,  -7.4603],
        [ -6.3989,   1.8153]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 3.3749, -3.3156,  3.8724, -3.9470, -6.5091, -3.7513,  0.2435, -5.0771,
        -7.9607,  2.8165], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.0294e+01, -2.4453e+00, -1.1521e+01, -1.4204e-01,  1.3441e+00,
         -2.4368e+00,  1.1679e+01, -7.0178e+00,  8.5055e+00, -2.5655e+00],
        [-7.4021e-01, -1.5690e+00,  2.1136e+00,  1.0097e+00, -3.1369e+00,
         -2.2199e+00,  4.6969e+00, -3.4556e+00, -7.4427e+00,  6.6300e+00],
        [-2.1494e+00,  1.0151e+00, -7.6784e+00, -1.6795e-02,  1.5620e+00,
          7.7320e+00, -1.2708e+00, -1.3213e+00,  7.9639e-01, -4.1179e+00],
        [-8.7042e-01,  1.4632e+00, -3.3871e+00,  1.7059e+00, -2.5590e+00,
          1.5191e+00, -1.3786e+00, -3.3778e-01, -9.5920e-01,  9.3226e-01],
        [-4.6801e+00, -1.0642e+00, -1.1377e+01,  3.4188e+00,  2.5273e+00,
         -1.6047e+00,  1.0976e+00, -9.2126e+00,  4.6129e+00,  4.8055e+00],
        [-2.3159e+00, -2.9349e-01, -2.1370e+00, -1.8210e-03, -9.6074e-01,
          5.6636e-02, -7.6344e-01, -1.2668e+00, -5.1588e-01, -3.8382e-01],
        [-1.8830e+00, -4.0910e-01, -5.2198e+00,  1.5254e-01,  3.2268e-01,
          5.6311e+00, -8.0270e+00, -2.0642e+00, -9.6746e-01, -2.2747e+00],
        [-3.2230e+00, -8.4563e-01, -8.4846e-01, -7.2869e-06, -4.9621e-01,
         -3.4971e-01,  7.7628e-01, -3.0654e+00, -2.1797e-01,  9.2496e-01],
        [-2.2799e+00, -7.5823e-01, -4.5546e+00,  1.0047e-02, -9.5969e-02,
          4.8669e+00, -7.6202e+00, -2.4278e+00,  3.6156e-01, -1.7094e+00],
        [-2.0510e+00, -4.8742e-01, -5.1123e+00,  9.0980e-02, -9.8075e-02,
          5.4799e+00, -7.4853e+00, -2.3602e+00, -6.9063e-01, -2.0557e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.4744, -1.5859, -1.4141, -1.8169, -2.3739, -4.0977, -2.4502, -3.4484,
        -2.4264, -2.2911], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 2.8722e+00,  8.7642e-01, -2.7626e+00, -1.7980e+00,  8.1723e+00,
         -9.8384e-02, -2.4247e+00,  2.6450e-03, -2.1818e+00, -2.2342e+00],
        [-2.7423e+00, -8.9059e-01,  2.7728e+00,  1.9670e+00, -8.4976e+00,
         -3.4896e-02,  2.4962e+00, -6.0730e-02,  2.2660e+00,  1.9356e+00]],
       device='cuda:0'))])
xi:  [-483.73126]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 264.80542773346605
W_T_median: 133.48107592348964
W_T_pctile_5: -490.05872096921905
W_T_CVAR_5_pct: -607.458472185204
Average q (qsum/M+1):  57.15572627898185
Optimal xi:  [-483.73126]
Expected(across Rb) median(across samples) p_equity:  0.3210451178252697
obj fun:  tensor(-1741.4444, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1650.263589429409
Current xi:  [-10.336477]
objective value function right now is: -1650.263589429409
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1652.572137085659
Current xi:  [-18.956211]
objective value function right now is: -1652.572137085659
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1653.3728038652148
Current xi:  [-24.646795]
objective value function right now is: -1653.3728038652148
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1654.3030998601032
Current xi:  [-31.883064]
objective value function right now is: -1654.3030998601032
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-49.475994]
objective value function right now is: -1645.5005210265952
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-63.062004]
objective value function right now is: -1653.152937121728
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1656.0384656515998
Current xi:  [-66.956314]
objective value function right now is: -1656.0384656515998
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-80.93742]
objective value function right now is: -1642.5800095031225
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1663.6827524907947
Current xi:  [-94.28406]
objective value function right now is: -1663.6827524907947
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-109.19372]
objective value function right now is: -1661.7859043714845
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-119.64429]
objective value function right now is: -1633.8082909234524
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-127.392654]
objective value function right now is: -1650.2645060786576
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-136.19572]
objective value function right now is: -1645.4259362983742
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-146.14165]
objective value function right now is: -1655.5658132041294
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.82623]
objective value function right now is: -1661.848505090559
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-172.37222]
objective value function right now is: -1663.4227554885522
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1667.4083287502383
Current xi:  [-180.15756]
objective value function right now is: -1667.4083287502383
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1668.6166274150196
Current xi:  [-188.72963]
objective value function right now is: -1668.6166274150196
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-199.71544]
objective value function right now is: -1661.6616455454823
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-210.0725]
objective value function right now is: -1663.787077276005
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-220.64737]
objective value function right now is: -1661.5723255756227
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-232.97719]
objective value function right now is: -1662.0226053696442
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.82864]
objective value function right now is: -1651.8810397672005
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-251.43102]
objective value function right now is: -1651.1020019965829
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-253.82375]
objective value function right now is: -1656.1274611753895
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-265.90717]
objective value function right now is: -1646.5707650525223
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-270.0829]
objective value function right now is: -1642.9202926882003
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-270.47568]
objective value function right now is: -1643.0023940320575
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-271.53598]
objective value function right now is: -1666.3100754435145
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-278.5405]
objective value function right now is: -1665.1092670554654
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-283.75558]
objective value function right now is: -1665.3372532196386
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-290.03522]
objective value function right now is: -1665.706078849049
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-294.30408]
objective value function right now is: -1661.3399933686148
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-294.77295]
objective value function right now is: -1658.5414149224573
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-298.68976]
objective value function right now is: -1661.3440726412446
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-297.35782]
objective value function right now is: -1662.2327163441687
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-296.65784]
objective value function right now is: -1658.0112913402334
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-293.74207]
objective value function right now is: -1659.8483085650705
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-291.29065]
objective value function right now is: -1662.8271322765495
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-291.3155]
objective value function right now is: -1663.1995048866486
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-291.3465]
objective value function right now is: -1661.3058186619007
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-291.49338]
objective value function right now is: -1659.9109207322354
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-291.34448]
objective value function right now is: -1663.6791547473506
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-290.0976]
objective value function right now is: -1661.9895907312134
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-291.03998]
objective value function right now is: -1663.7537290559355
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-291.5509]
objective value function right now is: -1661.2132580148832
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-290.8636]
objective value function right now is: -1665.9709189372868
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-291.71732]
objective value function right now is: -1664.695048579704
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-292.29944]
objective value function right now is: -1664.6982301706785
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-292.29776]
objective value function right now is: -1664.8924797076122
min fval:  -1660.61131679558
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.8142, -5.6220],
        [ 1.3145,  5.0063],
        [-0.7997, -5.3957],
        [-0.8851, -5.3418],
        [ 8.4222, -4.4820],
        [-0.8323,  4.0891],
        [-2.0390, -2.8296],
        [-0.3667, -5.1507],
        [-1.0543, -5.5223],
        [ 6.4496,  4.0410]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-5.3786,  4.3155, -5.1545, -5.2557, -8.9535,  3.3564,  5.2972, -5.4655,
        -5.2997, -3.4568], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 3.3213, -7.2254,  3.1522,  2.9247,  5.0578, -4.0112, -5.1328,  2.8883,
          3.6701, -2.8975],
        [-0.2659, -0.9533, -0.3482, -0.3181,  1.8512, -0.1239, -2.9071, -0.2651,
         -0.2919,  0.1094],
        [ 4.4288, -7.0898,  3.9460,  3.8008,  5.8432, -4.0798, -4.7206,  3.2020,
          4.1054, -3.6480],
        [ 4.3491, -7.1789,  4.3140,  3.9491,  5.6659, -4.4653, -5.3871,  3.6969,
          4.4423, -3.5976],
        [ 4.2662, -7.3067,  4.0016,  3.8167,  5.7340, -4.3957, -4.9480,  3.6220,
          4.3594, -3.6588],
        [ 1.7214, -5.0853,  1.6232,  0.7305,  5.6761, -1.4786, -7.9272,  0.7462,
          1.1778, -0.0664],
        [ 1.1946,  0.6323,  1.2201,  0.9615, -0.6750,  0.4312,  2.3102,  0.8314,
          1.0126, -0.1225],
        [ 4.0713, -7.1634,  4.0853,  3.5145,  5.5087, -3.7622, -4.7186,  3.2404,
          4.2209, -3.3907],
        [ 3.9454, -6.9653,  3.3951,  3.0708,  6.1116, -3.5799, -5.7022,  2.7199,
          3.7174, -3.4456],
        [ 3.8461, -7.0609,  3.5049,  3.2548,  5.7862, -3.5984, -5.1679,  2.5367,
          3.6158, -3.4320]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-3.2745, -2.3214, -3.4074, -3.2410, -3.2583, -3.3091,  2.6159, -3.2300,
        -3.2831, -3.2953], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-3.5340, -0.6694, -4.9919, -5.2827, -5.2456, -2.3887,  8.9024, -4.5266,
         -4.5743, -4.2315]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 11.2603,   5.2196],
        [ -1.5187,  -4.2894],
        [ -2.1950,   5.8518],
        [ -5.7153,   1.8723],
        [-14.0326,  -5.5577],
        [-11.2835,  -6.1841],
        [-12.9269,  -2.4581],
        [  6.6139,  -1.1370],
        [ -3.5334,  -7.9871],
        [ -6.9866,   2.3503]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 3.2606, -4.7785,  4.1586, -1.7239, -3.5549, -3.5577, -0.5815, -7.2708,
        -8.0244,  2.2740], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-12.6946,  -0.9790,  -9.6644,   1.3740,   4.0492,   0.3989,  12.0294,
          -4.0491,   9.8929,  -1.9546],
        [  0.1555,  -1.6460,   3.8101,   2.0292,  -2.7247,  -3.2884,   5.4042,
          -4.2197, -13.5413,   8.7542],
        [ -5.4329,   0.9590,  -9.8026,   2.3767,   3.4281,   7.6399,   3.2030,
          -8.3887,   0.1531,  -4.2199],
        [ -0.4572,   0.4991,  -1.9136,   1.7811,  -1.1778,   1.5559,   0.1953,
          -2.6008,  -1.5026,   0.2629],
        [ -9.4968,  -0.0671, -12.2802,   2.4666,   3.7003,  -0.1327,  -0.9282,
         -13.7640,   6.2808,   6.2571],
        [ -0.9081,   0.4268,  -1.7026,   0.7067,  -1.1071,  -3.3815,  -2.6547,
          -1.7931,   0.3816,   0.1629],
        [ -0.7161,   0.3798,  -3.8430,  -0.5853,   0.1222,   2.4517,  -2.6121,
          -2.6938,  -0.6314,   0.0348],
        [ -2.7928,  -1.6801,   1.6427,   1.0070,  -1.5536,  -1.0335,   1.4553,
          -3.0925,  -5.5896,   4.1704],
        [ -1.9866,  -0.2485,  -7.8033,   2.3736,   0.4034,   4.2319,  -5.2812,
          -6.9830,   2.8240,  -4.3227],
        [ -2.3364,  -1.3726,  -3.8797,   0.6471,  -2.2517,   2.6524,  -3.1273,
          -2.7551,  -0.3657,  -0.6509]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.6060, -0.9971, -1.5626, -1.9121, -2.4479, -2.8028, -2.8769, -3.3608,
        -2.3446, -2.7501], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  3.0847,   0.4728,  -2.5739,  -0.9743,  12.9107,  -1.6023,  -1.3665,
           1.1647,  -3.3339,  -1.8057],
        [ -2.9549,  -0.4870,   2.5841,   1.1391, -13.1546,   1.4767,   1.4124,
          -1.2178,   3.4106,   1.5899]], device='cuda:0'))])
xi:  [-291.03998]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 317.85860314855205
W_T_median: 119.72299735690507
W_T_pctile_5: -197.3005841392578
W_T_CVAR_5_pct: -293.52733157994066
Average q (qsum/M+1):  55.7275390625
Optimal xi:  [-291.03998]
Expected(across Rb) median(across samples) p_equity:  0.37960387505590915
obj fun:  tensor(-1660.6113, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1599.874206033763
Current xi:  [-6.3096075]
objective value function right now is: -1599.874206033763
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1601.2112969260725
Current xi:  [-9.829889]
objective value function right now is: -1601.2112969260725
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1601.5319661106164
Current xi:  [-14.280313]
objective value function right now is: -1601.5319661106164
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.7597967091021
Current xi:  [-18.261112]
objective value function right now is: -1602.7597967091021
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1603.1468108383856
Current xi:  [-20.030785]
objective value function right now is: -1603.1468108383856
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1603.526133750488
Current xi:  [-21.455158]
objective value function right now is: -1603.526133750488
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-21.894356]
objective value function right now is: -1603.0977756538837
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1603.7664708064144
Current xi:  [-21.573324]
objective value function right now is: -1603.7664708064144
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-21.900608]
objective value function right now is: -1602.2241931416515
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.458637]
objective value function right now is: -1603.1477038069188
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.280981]
objective value function right now is: -1602.1342046824773
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.515585]
objective value function right now is: -1602.9234486145976
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1603.7762893485256
Current xi:  [-22.340693]
objective value function right now is: -1603.7762893485256
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-22.778168]
objective value function right now is: -1603.5704675768068
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.629269]
objective value function right now is: -1602.6237654466368
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.566395]
objective value function right now is: -1603.5704983935925
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.3229442343318
Current xi:  [-22.524866]
objective value function right now is: -1604.3229442343318
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.4286214676156
Current xi:  [-22.547142]
objective value function right now is: -1604.4286214676156
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.563753]
objective value function right now is: -1603.9180202592393
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.783812]
objective value function right now is: -1603.8995936826689
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.720598]
objective value function right now is: -1602.9802742808545
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.5022002910646
Current xi:  [-22.673935]
objective value function right now is: -1604.5022002910646
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.674438]
objective value function right now is: -1604.3517672670162
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.831402]
objective value function right now is: -1603.8043140897084
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.67948]
objective value function right now is: -1604.160273937712
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.844055]
objective value function right now is: -1604.2636135808855
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.79564]
objective value function right now is: -1604.371296125884
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-22.93477]
objective value function right now is: -1604.2188166396757
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-22.834534]
objective value function right now is: -1604.0181288891397
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.92077]
objective value function right now is: -1603.3497880011714
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.683304]
objective value function right now is: -1603.9359897714119
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-23.694084]
objective value function right now is: -1604.2674004672017
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.7305485091265
Current xi:  [-22.774456]
objective value function right now is: -1604.7305485091265
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.995834]
objective value function right now is: -1604.4871103524997
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.727037]
objective value function right now is: -1604.3384990013542
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.0833925309378
Current xi:  [-22.775702]
objective value function right now is: -1605.0833925309378
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.23613331341
Current xi:  [-22.789087]
objective value function right now is: -1605.23613331341
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.903006]
objective value function right now is: -1605.1992391821068
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.83577]
objective value function right now is: -1605.2293834113707
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.781836]
objective value function right now is: -1605.1496997204097
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.3036142206117
Current xi:  [-22.778591]
objective value function right now is: -1605.3036142206117
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.3492476211293
Current xi:  [-22.804163]
objective value function right now is: -1605.3492476211293
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.79322]
objective value function right now is: -1605.184833998282
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.910856]
objective value function right now is: -1605.014732147629
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.91474]
objective value function right now is: -1605.1549597273427
new min fval from sgd:  -1605.3551766071873
new min fval from sgd:  -1605.3693728315056
new min fval from sgd:  -1605.3853505905927
new min fval from sgd:  -1605.4003574703315
new min fval from sgd:  -1605.4015009911407
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.858116]
objective value function right now is: -1605.2929343279586
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.878445]
objective value function right now is: -1605.1175142125655
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.91203]
objective value function right now is: -1605.2152347676233
new min fval from sgd:  -1605.4015426384697
new min fval from sgd:  -1605.4023562337961
new min fval from sgd:  -1605.4037338204996
new min fval from sgd:  -1605.4052843516417
new min fval from sgd:  -1605.4056067200024
new min fval from sgd:  -1605.4058536397017
new min fval from sgd:  -1605.405988180906
new min fval from sgd:  -1605.4061322934149
new min fval from sgd:  -1605.4062041284762
new min fval from sgd:  -1605.4075794880625
new min fval from sgd:  -1605.4092683673725
new min fval from sgd:  -1605.4101733303914
new min fval from sgd:  -1605.4116962887722
new min fval from sgd:  -1605.4159540612638
new min fval from sgd:  -1605.4187424681227
new min fval from sgd:  -1605.4187943322959
new min fval from sgd:  -1605.4198375753197
new min fval from sgd:  -1605.4225295161098
new min fval from sgd:  -1605.4234512970775
new min fval from sgd:  -1605.4239344639236
new min fval from sgd:  -1605.4241714396408
new min fval from sgd:  -1605.425642421003
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.86516]
objective value function right now is: -1605.425642421003
new min fval from sgd:  -1605.4273096624813
new min fval from sgd:  -1605.4287454425562
new min fval from sgd:  -1605.4312800479806
new min fval from sgd:  -1605.4346700186397
new min fval from sgd:  -1605.4367993499623
new min fval from sgd:  -1605.4381134436935
new min fval from sgd:  -1605.4386988573517
new min fval from sgd:  -1605.4393035090804
new min fval from sgd:  -1605.4404142437759
new min fval from sgd:  -1605.4405274732842
new min fval from sgd:  -1605.4440183631461
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.863327]
objective value function right now is: -1605.421213977795
min fval:  -1605.4440183631461
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.7599, -6.6320],
        [-5.9795,  6.2838],
        [-1.4426, -6.3164],
        [-1.8042, -6.3844],
        [ 9.0638, -5.4163],
        [-5.2263,  5.9465],
        [-4.7234, -0.7575],
        [ 2.6328, -6.0483],
        [-2.8107, -6.6335],
        [ 6.7862,  3.1812]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-5.9515,  3.9997, -5.7836, -5.7852, -9.6731,  4.0091,  4.2356, -5.5169,
        -5.7883, -5.4359], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 4.0134e+00, -3.3126e+00,  3.5282e+00,  3.6144e+00,  5.5600e+00,
         -3.1179e+00, -5.0946e+00,  2.1366e+00,  4.6168e+00,  2.3925e+00],
        [-1.8025e-01, -3.5473e-02, -1.7390e-01, -1.7327e-01, -2.3331e-01,
         -5.7830e-02, -6.6207e-01, -3.7414e-01, -1.8002e-01, -8.6296e-02],
        [ 2.5264e+00, -1.7659e+00,  2.0674e+00,  2.1747e+00,  7.8512e+00,
         -9.7976e-01, -3.9859e+00,  1.5780e+00,  2.7621e+00, -6.5943e-01],
        [ 5.2991e+00, -6.8598e+00,  5.0543e+00,  4.7958e+00,  8.7269e+00,
         -6.2553e+00, -4.9482e+00,  4.6231e+00,  5.3644e+00,  3.7933e+00],
        [ 5.2243e+00, -6.7605e+00,  4.7562e+00,  4.6837e+00,  8.5458e+00,
         -5.9578e+00, -4.5882e+00,  4.3639e+00,  5.3547e+00,  3.7881e+00],
        [ 8.1660e-02, -5.8691e-02,  9.2140e-02,  7.9943e-02, -1.5404e+00,
          5.4853e-03,  2.7965e+00,  1.5917e+00,  5.5334e-02, -1.0284e+00],
        [ 6.1248e-02, -7.5729e-02,  7.0671e-02,  6.0390e-02, -2.2982e+00,
          3.1348e-01,  3.6929e+00,  1.6791e+00,  3.6261e-02, -8.2924e-01],
        [ 4.0675e+00, -2.8832e+00,  3.7873e+00,  3.4964e+00,  6.1719e+00,
         -2.9913e+00, -5.4010e+00,  2.0777e+00,  4.4074e+00,  1.3656e+00],
        [-1.8025e-01, -3.5474e-02, -1.7390e-01, -1.7327e-01, -2.3332e-01,
         -5.7830e-02, -6.6207e-01, -3.7414e-01, -1.8002e-01, -8.6296e-02],
        [ 4.7096e+00, -4.0859e+00,  4.0034e+00,  4.1331e+00,  7.2558e+00,
         -3.0971e+00, -5.5034e+00,  1.4174e+00,  5.0805e+00,  3.4956e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-3.7461, -0.8085, -0.8124, -2.7886, -2.9062,  2.2195,  2.8936, -3.5349,
        -0.8085, -3.8891], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-5.3422e+00,  6.3130e-03, -3.9503e+00, -6.6683e+00, -6.4253e+00,
          3.5364e+00,  9.7841e+00, -5.1203e+00,  6.3129e-03, -6.7999e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 14.3041,   5.4897],
        [ -2.1094,  -0.7967],
        [ -6.0700,   9.2529],
        [ -6.6945,   0.9235],
        [-13.6037,  -4.6404],
        [-12.0925,  -7.4029],
        [-15.7007,  -0.0750],
        [  8.7134,  -0.5788],
        [ -4.0383, -10.3880],
        [-13.0222,   4.7024]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  3.4111,  -6.3340,   9.2333,   2.8933,  -2.4017,  -5.0757,   0.4044,
         -8.6250, -10.5707,   5.7301], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.1043e+01,  8.6582e-03, -4.2391e+00, -2.0844e-01,  5.9029e+00,
          1.7808e+00,  5.7894e-01, -2.8421e+00,  9.2699e+00, -2.7363e+00],
        [-2.6719e+00,  6.3620e-02,  4.1499e+00,  1.4923e+00, -1.1870e+01,
         -1.2887e+01, -4.0927e+00,  2.0235e+00, -2.8661e+00,  1.8153e+01],
        [-6.8882e+00,  6.6224e-01, -1.6351e+01,  2.3036e+00,  6.2994e+00,
          7.4493e+00,  4.7776e+00, -9.2838e+00,  4.2270e+00, -3.4788e+00],
        [-9.4016e-01, -6.6471e+00, -5.9359e+00,  4.3432e+00,  3.9342e+00,
          3.1156e+00, -4.5635e+00,  2.5191e-01, -2.8114e+00, -4.6531e+00],
        [-1.3660e+01, -2.4096e+00, -1.3701e+01,  4.2189e+00,  3.6047e+00,
          3.3383e+00,  6.3155e+00, -2.0801e+01,  1.1753e+01,  6.0380e-01],
        [-2.3017e+00, -3.1653e-01, -3.9216e-01, -2.2724e-01, -3.5668e-01,
         -4.2875e-01,  3.7366e-03, -7.7905e-01, -7.3199e-01,  8.5817e-03],
        [-2.3017e+00, -3.1653e-01, -3.9216e-01, -2.2724e-01, -3.5668e-01,
         -4.2875e-01,  3.7368e-03, -7.7905e-01, -7.3199e-01,  8.5819e-03],
        [-6.5711e-01, -1.5057e-01,  1.5798e+00,  3.7654e+00, -7.1778e+00,
         -1.3798e+01,  8.9055e+00, -6.2696e-01,  1.5436e+00,  1.2750e+01],
        [-2.6927e+00, -4.9248e+00, -1.7439e+01,  3.7515e+00,  2.6647e+00,
          6.4006e+00,  2.1843e+00, -1.1962e+01,  4.3529e+00, -1.1874e+00],
        [-1.1906e+00, -5.5561e-02, -4.8136e+00,  7.5600e+00,  1.0239e+00,
         -2.5507e-01,  1.5771e+00, -3.7840e+00, -1.2782e+01, -2.7665e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.3168, -4.0809, -1.5611, -1.7657, -4.8052, -2.7615, -2.7615, -2.0031,
        -3.1686, -1.5929], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 2.7960e+00, -8.5262e-01, -2.6041e+00,  2.5957e+00,  1.3197e+01,
         -2.0143e-03,  6.7191e-03,  1.3631e+00, -6.2974e+00, -2.8594e+00],
        [-2.6668e+00,  8.3943e-01,  2.6144e+00, -2.4485e+00, -1.3197e+01,
         -1.0620e-02, -1.8863e-03, -1.4151e+00,  6.3621e+00,  2.7753e+00]],
       device='cuda:0'))])
xi:  [-22.898876]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 412.54919803043634
W_T_median: 118.29168468237899
W_T_pctile_5: -22.841555387390645
W_T_CVAR_5_pct: -131.3693578138758
Average q (qsum/M+1):  53.90739588583669
Optimal xi:  [-22.898876]
Expected(across Rb) median(across samples) p_equity:  0.32975623855988184
obj fun:  tensor(-1605.4440, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1561.0400039660867
Current xi:  [0.06921296]
objective value function right now is: -1561.0400039660867
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.1710534]
objective value function right now is: -1560.7071729272493
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.7230186]
objective value function right now is: -1557.6169248952572
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.2007596]
objective value function right now is: -1554.0453482274477
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.8589883]
objective value function right now is: -1559.8142032526941
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.671204]
objective value function right now is: -1560.4853018827896
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1561.603752139016
Current xi:  [5.544254]
objective value function right now is: -1561.603752139016
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.700266]
objective value function right now is: -1560.4661472783202
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.394611]
objective value function right now is: -1561.4212240595002
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.055555]
objective value function right now is: -1559.2915420977868
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.769177]
objective value function right now is: -1560.8804098139935
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.4152169539989
Current xi:  [13.608047]
objective value function right now is: -1562.4152169539989
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.798396]
objective value function right now is: -1558.6262438558824
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1562.587655693334
Current xi:  [17.929071]
objective value function right now is: -1562.587655693334
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.6838561963818
Current xi:  [19.839676]
objective value function right now is: -1562.6838561963818
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [21.95781]
objective value function right now is: -1561.3281281670988
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1563.2385448640696
Current xi:  [24.19294]
objective value function right now is: -1563.2385448640696
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [26.014996]
objective value function right now is: -1562.9025559455
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [28.101112]
objective value function right now is: -1562.7249968550332
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [30.15418]
objective value function right now is: -1562.673001612688
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [32.211197]
objective value function right now is: -1561.388870628586
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1563.6921505502787
Current xi:  [34.061108]
objective value function right now is: -1563.6921505502787
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [35.66138]
objective value function right now is: -1563.3622883619853
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1563.7509785245827
Current xi:  [37.249832]
objective value function right now is: -1563.7509785245827
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [38.855118]
objective value function right now is: -1561.7036280186246
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1563.9965715253693
Current xi:  [40.458576]
objective value function right now is: -1563.9965715253693
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1564.5745385776017
Current xi:  [42.054188]
objective value function right now is: -1564.5745385776017
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [43.23881]
objective value function right now is: -1562.4876332618765
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [44.09854]
objective value function right now is: -1564.424998995775
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1565.0365330227585
Current xi:  [45.281452]
objective value function right now is: -1565.0365330227585
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1565.4369036546118
Current xi:  [46.312412]
objective value function right now is: -1565.4369036546118
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [47.39718]
objective value function right now is: -1564.8478696444615
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [48.165085]
objective value function right now is: -1564.2844934438272
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [48.992622]
objective value function right now is: -1564.7864679149498
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [49.744926]
objective value function right now is: -1564.8883737400724
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1566.8066573241065
Current xi:  [49.875877]
objective value function right now is: -1566.8066573241065
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1566.9837379793512
Current xi:  [50.138805]
objective value function right now is: -1566.9837379793512
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1567.0596563576535
Current xi:  [50.387054]
objective value function right now is: -1567.0596563576535
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1567.2182299006759
Current xi:  [50.59664]
objective value function right now is: -1567.2182299006759
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1567.2374895902701
Current xi:  [50.91912]
objective value function right now is: -1567.2374895902701
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [51.170513]
objective value function right now is: -1567.163219899162
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1567.2522090596929
Current xi:  [51.5072]
objective value function right now is: -1567.2522090596929
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [51.694576]
objective value function right now is: -1567.167854015383
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [51.939434]
objective value function right now is: -1567.2365224896544
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.13738]
objective value function right now is: -1567.166389731715
new min fval from sgd:  -1567.2559489988807
new min fval from sgd:  -1567.270555654188
new min fval from sgd:  -1567.2841704365937
new min fval from sgd:  -1567.2954139311962
new min fval from sgd:  -1567.3243545733715
new min fval from sgd:  -1567.3386961201722
new min fval from sgd:  -1567.339943329331
new min fval from sgd:  -1567.3404689403014
new min fval from sgd:  -1567.3445260003236
new min fval from sgd:  -1567.3553307498973
new min fval from sgd:  -1567.3557320426657
new min fval from sgd:  -1567.4106076807236
new min fval from sgd:  -1567.4423966425447
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.42996]
objective value function right now is: -1567.3009184316393
new min fval from sgd:  -1567.4455749358642
new min fval from sgd:  -1567.4464805623395
new min fval from sgd:  -1567.4731512774663
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.710335]
objective value function right now is: -1567.019958276182
new min fval from sgd:  -1567.474211457129
new min fval from sgd:  -1567.4971071481452
new min fval from sgd:  -1567.5125688781882
new min fval from sgd:  -1567.5141577564575
new min fval from sgd:  -1567.5197007153265
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.893738]
objective value function right now is: -1566.945276489229
new min fval from sgd:  -1567.5200473277985
new min fval from sgd:  -1567.5208350892456
new min fval from sgd:  -1567.521003396795
new min fval from sgd:  -1567.5213752934583
new min fval from sgd:  -1567.521528770094
new min fval from sgd:  -1567.522631056624
new min fval from sgd:  -1567.5241035198583
new min fval from sgd:  -1567.524646604624
new min fval from sgd:  -1567.5257129574402
new min fval from sgd:  -1567.5271669484196
new min fval from sgd:  -1567.533463514573
new min fval from sgd:  -1567.5381450770647
new min fval from sgd:  -1567.5391676523216
new min fval from sgd:  -1567.5394965068338
new min fval from sgd:  -1567.5419317223989
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.004364]
objective value function right now is: -1567.4030861547385
new min fval from sgd:  -1567.544076639304
new min fval from sgd:  -1567.5449386744258
new min fval from sgd:  -1567.5464537076111
new min fval from sgd:  -1567.550767301497
new min fval from sgd:  -1567.550947230728
new min fval from sgd:  -1567.5528525598668
new min fval from sgd:  -1567.5546100605204
new min fval from sgd:  -1567.5559889812498
new min fval from sgd:  -1567.5577136033305
new min fval from sgd:  -1567.5587274137563
new min fval from sgd:  -1567.5601433495938
new min fval from sgd:  -1567.5601669166551
new min fval from sgd:  -1567.5610580108118
new min fval from sgd:  -1567.561067647124
new min fval from sgd:  -1567.5613950805437
new min fval from sgd:  -1567.5631558899327
new min fval from sgd:  -1567.5633362374372
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.059036]
objective value function right now is: -1567.537827872621
min fval:  -1567.5633362374372
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -5.9797,  -7.2239],
        [ -7.8038,   7.0875],
        [ -6.1739,  -6.3824],
        [-39.1859,  -6.1981],
        [ 10.1887,  -8.4049],
        [ -7.4972,   6.4179],
        [ -5.5644,  -1.6838],
        [  5.3500,  -5.4508],
        [ -6.7260,  -7.2625],
        [  6.2941,   7.9158]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-6.0804,  0.5100, -6.3246, -6.5116, -8.0269,  0.9759,  5.4839, -6.9364,
        -5.9019, -7.1574], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 4.3333e+00, -1.3950e+00,  3.7053e+00,  4.4322e+00,  8.1815e+00,
         -1.5609e+00, -5.9177e+00,  4.1480e+00,  4.7984e+00,  5.6683e+00],
        [-1.6619e-01, -9.0951e-03, -6.9784e-02, -5.2900e-02, -3.9007e-01,
         -8.0506e-03, -7.8743e-01, -1.2648e-01, -1.8893e-01, -2.6895e-01],
        [ 1.9207e+00, -1.7686e-01,  1.1571e+00,  1.3269e+00,  4.3886e+00,
         -1.8403e-01, -5.2548e+00,  1.3373e+00,  2.1801e+00,  9.8722e-01],
        [ 6.3810e+00, -2.1987e+00,  5.2711e+00,  4.5929e+00,  6.3221e+00,
         -2.5488e+00, -5.0275e+00,  4.0601e+00,  6.6964e+00,  5.5314e+00],
        [ 6.2356e+00, -2.1051e+00,  4.9719e+00,  4.4941e+00,  6.2326e+00,
         -2.3251e+00, -4.6897e+00,  3.9123e+00,  6.6062e+00,  5.5264e+00],
        [ 2.4338e-01,  2.0561e-01,  1.4221e-01,  2.8050e-03,  1.9797e-01,
          2.3315e-01,  2.7433e+00, -5.0023e-02,  2.6856e-01, -3.1473e-01],
        [ 3.7179e-01,  1.1573e-01,  2.1592e-01,  2.3654e-02,  4.0840e-01,
          1.5784e-01,  2.7390e+00, -2.5839e-02,  4.0794e-01,  5.2798e-01],
        [ 4.4319e+00, -9.8102e-01,  4.0956e+00,  4.4801e+00,  8.2474e+00,
         -1.3484e+00, -6.1770e+00,  4.0858e+00,  4.6864e+00,  4.9770e+00],
        [-1.6619e-01, -9.0951e-03, -6.9784e-02, -5.2900e-02, -3.9007e-01,
         -8.0506e-03, -7.8743e-01, -1.2648e-01, -1.8893e-01, -2.6895e-01],
        [ 4.8387e+00, -1.8580e+00,  4.2350e+00,  5.1115e+00,  9.2409e+00,
         -1.1491e+00, -6.4620e+00,  2.8380e+00,  5.0885e+00,  6.2356e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-4.3147, -1.0605, -2.4242, -2.7559, -2.8800,  3.1259,  3.6031, -4.1593,
        -1.0605, -4.5040], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -8.3692,   0.0142,  -1.8513,  -6.3706,  -6.1214,   3.6249,   9.8232,
          -8.1584,   0.0142, -10.2716]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 16.1023,   4.8929],
        [ -1.6619,   0.7691],
        [ -9.7420,  10.6148],
        [ -6.9832,   1.1620],
        [-11.3752,  -6.5268],
        [-17.0911,  -5.4888],
        [-16.1392,  -1.4286],
        [  9.5242,  -0.6572],
        [ -4.6181, -11.3333],
        [-14.1853,   3.6306]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  3.5535,  -3.6102,   9.9497,   3.0638,  -3.9236,  -4.1498,  -1.3310,
         -9.8641, -10.5111,   3.9951], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.5001e+00, -1.7429e-02, -1.0586e-01, -3.0583e-01,  6.5325e-01,
          8.7135e-01,  8.7018e-01,  1.8314e+00,  1.7384e+00,  1.7780e-01],
        [-3.0069e+00,  1.4975e-01,  5.1919e+00, -1.2649e+00, -4.2474e+00,
         -2.2588e+00, -1.2979e+00,  3.5107e+00, -1.1738e+00,  8.7700e+00],
        [-8.0530e+00, -2.8804e-01, -2.1423e+01,  2.8093e+00,  8.4372e+00,
          1.0790e+01,  4.6270e+00, -1.1858e+01,  6.1163e+00, -2.3493e+00],
        [-7.1593e-01, -2.5163e-03, -9.2160e+00,  1.3015e+00,  5.5292e+00,
          7.9647e+00,  2.4635e+00, -4.3521e-01, -4.1941e+00, -4.8848e+00],
        [-1.3845e+01,  1.5572e-01, -9.6621e+00,  1.5189e+00,  6.6257e+00,
          3.5634e+00,  2.8234e+00, -1.4643e+01,  1.4312e+01, -1.7862e+00],
        [-3.1001e+00, -1.3121e-02, -1.0166e+00, -6.7457e-01,  6.5549e-02,
         -4.8432e-02, -2.1316e-02,  1.4784e-01, -1.6683e+00, -2.7824e-01],
        [-3.1001e+00, -1.3121e-02, -1.0166e+00, -6.7457e-01,  6.5548e-02,
         -4.8433e-02, -2.1316e-02,  1.4784e-01, -1.6683e+00, -2.7824e-01],
        [-2.2826e-01, -3.3657e-02,  1.1640e+00,  6.1066e+00, -1.4981e+01,
         -4.3053e+00,  7.4647e+00,  5.7619e-01,  1.2338e+00,  1.1265e+01],
        [-2.6861e+00,  1.4348e-01, -1.7413e+01,  1.1376e+00,  5.8255e+00,
          4.0711e+00,  6.4377e-02, -1.1948e+01,  3.8698e+00, -3.5225e+00],
        [-1.4967e+00, -2.0267e-01, -9.3988e+00,  8.5272e+00,  4.2024e-01,
          2.5008e+00,  1.9103e+00, -3.9606e+00, -2.2293e+01, -2.0810e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.8276, -4.5384, -1.7749, -1.2501, -5.9068, -3.5517, -3.5517, -1.6520,
        -3.0308, -2.0140], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.0995,  -0.6459,  -7.3602,   2.1033,  17.3922,   0.9084,   0.9132,
           0.8950,  -4.1786,  -3.5759],
        [ -1.0864,   0.6328,   7.3684,  -1.9597, -17.3977,  -0.9153,  -0.9105,
          -0.9468,   4.2361,   3.5006]], device='cuda:0'))])
xi:  [53.053913]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 503.2336721999012
W_T_median: 217.3218702351471
W_T_pctile_5: 53.05343178742604
W_T_CVAR_5_pct: -46.558647405980544
Average q (qsum/M+1):  52.06843813004032
Optimal xi:  [53.053913]
Expected(across Rb) median(across samples) p_equity:  0.3058855727314949
obj fun:  tensor(-1567.5633, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
loaded xi:  53.053913
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1546.4529632342849
Current xi:  [58.89641]
objective value function right now is: -1546.4529632342849
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1546.830606790163
Current xi:  [64.46531]
objective value function right now is: -1546.830606790163
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1549.081411134894
Current xi:  [69.63206]
objective value function right now is: -1549.081411134894
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1549.896546039346
Current xi:  [74.22665]
objective value function right now is: -1549.896546039346
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1550.1485018016194
Current xi:  [78.70228]
objective value function right now is: -1550.1485018016194
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1550.3063188045835
Current xi:  [82.91034]
objective value function right now is: -1550.3063188045835
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1551.7195717441593
Current xi:  [86.687706]
objective value function right now is: -1551.7195717441593
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [90.37828]
objective value function right now is: -1550.3096133588992
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1552.7173667455795
Current xi:  [93.74006]
objective value function right now is: -1552.7173667455795
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [96.82214]
objective value function right now is: -1550.7834399992573
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1553.6135522416128
Current xi:  [99.124435]
objective value function right now is: -1553.6135522416128
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [102.05902]
objective value function right now is: -1553.443536077221
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.0072]
objective value function right now is: -1551.3312366857995
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [105.80006]
objective value function right now is: -1552.7697256485687
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1554.2706974402415
Current xi:  [107.48045]
objective value function right now is: -1554.2706974402415
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.13313]
objective value function right now is: -1553.7740316518205
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.668396]
objective value function right now is: -1552.7401029826174
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [111.06098]
objective value function right now is: -1552.6196326669856
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [111.633804]
objective value function right now is: -1553.1706018840512
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.727196]
objective value function right now is: -1552.8637349031637
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [113.250725]
objective value function right now is: -1551.667888384976
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [113.79472]
objective value function right now is: -1549.4607434618738
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [114.70496]
objective value function right now is: -1553.378168472289
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.35455]
objective value function right now is: -1550.2740857868605
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.4776]
objective value function right now is: -1553.1410255794465
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.54312]
objective value function right now is: -1552.2541320762123
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.9106]
objective value function right now is: -1550.763441282823
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [115.756226]
objective value function right now is: -1552.5568158405913
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [115.6179]
objective value function right now is: -1552.8692341651126
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.1209]
objective value function right now is: -1554.2020628638786
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.668]
objective value function right now is: -1553.6549134256297
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.917336]
objective value function right now is: -1553.4438783169915
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.99474]
objective value function right now is: -1552.400932837976
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.38298]
objective value function right now is: -1553.210543894024
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.96335]
objective value function right now is: -1554.0345842380448
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.3743081082177
Current xi:  [116.92148]
objective value function right now is: -1555.3743081082177
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.84725]
objective value function right now is: -1555.2307326794235
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.922714]
objective value function right now is: -1554.7286861824405
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.01139]
objective value function right now is: -1555.2120000980822
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.500143365155
Current xi:  [117.17395]
objective value function right now is: -1555.500143365155
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.30487]
objective value function right now is: -1555.286985492873
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.502527833649
Current xi:  [117.181435]
objective value function right now is: -1555.502527833649
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.5653896673577
Current xi:  [117.30078]
objective value function right now is: -1555.5653896673577
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.807357538483
Current xi:  [117.32822]
objective value function right now is: -1555.807357538483
new min fval from sgd:  -1555.8288013894792
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.377754]
objective value function right now is: -1555.8288013894792
new min fval from sgd:  -1555.8303759448222
new min fval from sgd:  -1555.8456101493803
new min fval from sgd:  -1555.8625366263632
new min fval from sgd:  -1555.877795739908
new min fval from sgd:  -1555.90723584146
new min fval from sgd:  -1555.9330750189397
new min fval from sgd:  -1555.9341950700484
new min fval from sgd:  -1555.9443826789206
new min fval from sgd:  -1555.949214116281
new min fval from sgd:  -1555.9691319886242
new min fval from sgd:  -1555.9821116196972
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.408615]
objective value function right now is: -1555.269239487162
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.508934]
objective value function right now is: -1555.4356268643583
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.539345]
objective value function right now is: -1555.74420837898
new min fval from sgd:  -1555.9855326280656
new min fval from sgd:  -1555.9968204080642
new min fval from sgd:  -1556.0034959577108
new min fval from sgd:  -1556.0062279569531
new min fval from sgd:  -1556.0092772966527
new min fval from sgd:  -1556.0097400435739
new min fval from sgd:  -1556.018297487387
new min fval from sgd:  -1556.0204608884585
new min fval from sgd:  -1556.0214744874104
new min fval from sgd:  -1556.0261871817804
new min fval from sgd:  -1556.030477839415
new min fval from sgd:  -1556.0348646286782
new min fval from sgd:  -1556.0377950943728
new min fval from sgd:  -1556.0385106930585
new min fval from sgd:  -1556.038597322656
new min fval from sgd:  -1556.0427076490284
new min fval from sgd:  -1556.0465300605697
new min fval from sgd:  -1556.0488663436365
new min fval from sgd:  -1556.0525393756602
new min fval from sgd:  -1556.057128205433
new min fval from sgd:  -1556.059174534258
new min fval from sgd:  -1556.062755085404
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.54513]
objective value function right now is: -1556.0403728770045
new min fval from sgd:  -1556.065283831598
new min fval from sgd:  -1556.071637061765
new min fval from sgd:  -1556.0730805698745
new min fval from sgd:  -1556.077004026259
new min fval from sgd:  -1556.0811211429568
new min fval from sgd:  -1556.0825604635668
new min fval from sgd:  -1556.0861571310302
new min fval from sgd:  -1556.0886180172522
new min fval from sgd:  -1556.0918595938608
new min fval from sgd:  -1556.093348083998
new min fval from sgd:  -1556.0971429376818
new min fval from sgd:  -1556.097487996724
new min fval from sgd:  -1556.0985997926146
new min fval from sgd:  -1556.1007958111736
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.56206]
objective value function right now is: -1556.096376922848
min fval:  -1556.1007958111736
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -6.1948,  -7.7934],
        [ -9.3463,   4.5374],
        [ -2.9469,  -6.9977],
        [-41.7990,  -6.7189],
        [ 11.1263,  -8.3008],
        [ -9.0385,   4.6822],
        [ -7.2942,  -0.5222],
        [  5.8551,  -6.5071],
        [ -8.0261,  -7.9496],
        [  6.5500,   6.7714]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-6.3172,  0.6926, -6.4853, -6.6084, -7.4120,  1.6533,  6.4826, -7.3492,
        -6.0343, -7.8644], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 4.5971e+00, -2.1458e+00,  3.7610e+00,  6.1255e+00,  8.3268e+00,
         -2.8601e+00, -6.9083e+00,  5.4364e+00,  5.0364e+00,  6.1042e+00],
        [-1.4499e-01, -1.8023e-02, -8.3384e-02, -5.5895e-02, -4.2278e-01,
         -6.6838e-02, -7.4344e-01, -1.0739e-01, -1.7897e-01, -3.6071e-01],
        [-1.4499e-01, -1.8023e-02, -8.3383e-02, -5.5895e-02, -4.2278e-01,
         -6.6838e-02, -7.4344e-01, -1.0739e-01, -1.7897e-01, -3.6071e-01],
        [ 7.6056e+00, -6.7779e-01,  5.3623e+00,  1.9246e+00,  5.4477e+00,
         -1.9349e+00, -5.6548e+00,  4.2082e+00,  8.4247e+00,  4.8042e+00],
        [ 4.5473e+00,  1.3623e-01,  2.4420e+00,  8.3740e-01,  6.5962e+00,
         -3.4827e-01, -2.6951e+00,  2.9778e+00,  5.2976e+00,  4.8918e+00],
        [ 4.4590e-01, -6.1812e-04,  2.2094e-01,  2.1857e-01,  5.3272e-01,
         -8.1739e-02,  1.9638e+00,  4.1191e-02,  5.4688e-01,  9.9431e-01],
        [ 4.9985e-01, -2.2784e-02,  2.5026e-01,  2.3763e-01,  6.7760e-01,
         -1.0167e-01,  2.2896e+00,  5.2552e-02,  6.1201e-01,  1.2403e+00],
        [ 4.6179e+00, -1.6668e+00,  4.0514e+00,  6.1312e+00,  8.3687e+00,
         -2.4289e+00, -7.1682e+00,  5.3753e+00,  4.8533e+00,  5.9016e+00],
        [-1.4499e-01, -1.8023e-02, -8.3383e-02, -5.5895e-02, -4.2278e-01,
         -6.6838e-02, -7.4344e-01, -1.0739e-01, -1.7897e-01, -3.6071e-01],
        [ 5.0280e+00, -2.6665e+00,  4.2188e+00,  6.7926e+00,  9.5445e+00,
         -2.6710e+00, -7.4788e+00,  4.4111e+00,  5.2548e+00,  6.6308e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-4.6906, -1.0968, -1.0968, -2.7471, -0.5667,  2.7565,  3.2661, -4.4815,
        -1.0968, -4.9079], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.0543e+01,  8.3677e-03,  8.3677e-03, -6.0946e+00, -6.0087e+00,
          3.3464e+00,  9.4302e+00, -1.0383e+01,  8.3677e-03, -1.2911e+01]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 14.4620,   5.3373],
        [ -1.7169,   0.4874],
        [-10.2262,  11.5305],
        [ -8.1868,   1.4671],
        [-10.8912,  -7.3763],
        [-18.8340,  -5.7569],
        [ -1.9605,   0.4490],
        [ 10.2776,  -0.8529],
        [ -4.7939, -12.4927],
        [-12.6273,   3.3574]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  3.6773,  -3.9055,  10.0079,   3.5045,  -5.1083,  -4.6688,  -3.9921,
        -10.4108, -10.9266,   3.7165], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-5.4401e+00,  2.0864e-02,  2.5336e-01,  5.4274e-01,  7.5930e-01,
          5.7349e+00,  9.5570e-02,  4.3734e+00,  2.3123e+00,  4.6176e-01],
        [-3.5554e+00, -1.3999e-03,  5.6659e+00, -2.9410e+00, -1.6880e+00,
          1.5773e-01, -5.4623e-02,  4.2715e+00, -5.3134e-01,  8.5862e+00],
        [-8.3883e+00, -2.2526e-01, -1.9095e+01,  9.2883e-01,  9.5732e+00,
          1.0224e+01, -1.4321e-02, -1.2728e+01,  6.6512e+00,  1.6318e+00],
        [-9.3973e-01,  2.6623e-01, -1.4493e+01, -5.8069e-02,  5.7188e+00,
          1.8876e+00,  4.4329e-02, -2.3408e+00, -4.6365e+00, -5.4419e+00],
        [-1.2859e+01,  3.0085e-01, -3.4660e+00, -1.5726e-01,  5.9917e+00,
          1.7265e+00,  2.5585e-01, -1.4516e+01,  1.4296e+01, -2.8747e+00],
        [-2.9770e+00, -1.3465e-02, -5.2755e-01, -3.9374e-01, -5.7600e-02,
         -3.9035e-04, -1.1318e-02, -4.2707e-01, -5.7657e-01, -2.2070e-01],
        [-2.9770e+00, -1.3465e-02, -5.2755e-01, -3.9374e-01, -5.7600e-02,
         -3.9036e-04, -1.1318e-02, -4.2707e-01, -5.7657e-01, -2.2070e-01],
        [ 2.3822e-01, -1.6816e-01,  4.2396e-01,  6.3133e+00, -1.2639e+01,
          2.4319e+00,  3.1341e-02,  1.4873e+00, -3.7187e-01,  7.8205e+00],
        [-2.6668e+00,  6.2253e-02, -2.2427e+01,  1.2979e+00,  8.5230e+00,
          4.6557e-01,  2.0554e-01, -1.1777e+01,  3.4886e+00, -7.4084e+00],
        [-1.8613e+00,  3.5020e-02, -9.1313e+00,  8.1337e+00,  2.5548e+00,
          3.7335e+00,  1.8036e-01, -4.2358e+00, -2.0720e+01, -9.0970e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.2327, -5.1236, -2.6421, -1.3788, -7.8450, -3.0327, -3.0327, -1.0870,
        -3.0079, -2.2002], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 2.1269e+00, -3.6322e-01, -6.9394e+00,  2.5497e+00,  1.7264e+01,
         -1.0156e-02, -9.5673e-03,  6.8567e-01, -3.5646e+00, -3.6785e+00],
        [-2.1164e+00,  3.5013e-01,  6.9464e+00, -2.4091e+00, -1.7263e+01,
          9.3143e-03,  9.9032e-03, -7.3740e-01,  3.6192e+00,  3.6126e+00]],
       device='cuda:0'))])
xi:  [117.56161]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 585.1864257327769
W_T_median: 312.0855554322063
W_T_pctile_5: 117.58275031624319
W_T_CVAR_5_pct: -8.817343346687261
Average q (qsum/M+1):  50.623476089969756
Optimal xi:  [117.56161]
Expected(across Rb) median(across samples) p_equity:  0.2919597161312898
obj fun:  tensor(-1556.1008, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
loaded xi:  117.56161
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1551.2691253355442
Current xi:  [126.72676]
objective value function right now is: -1551.2691253355442
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1557.764989626831
Current xi:  [134.61668]
objective value function right now is: -1557.764989626831
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [142.4052]
objective value function right now is: -1555.984572366384
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1560.4046367185351
Current xi:  [147.75098]
objective value function right now is: -1560.4046367185351
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1561.406920158444
Current xi:  [152.93843]
objective value function right now is: -1561.406920158444
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [157.00665]
objective value function right now is: -1560.3064529135618
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [159.91762]
objective value function right now is: -1560.7844053552217
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.37538]
objective value function right now is: -1560.6894553454172
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1561.5159514695388
Current xi:  [164.25038]
objective value function right now is: -1561.5159514695388
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1561.8407791709512
Current xi:  [164.96927]
objective value function right now is: -1561.8407791709512
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1564.329303502597
Current xi:  [166.93661]
objective value function right now is: -1564.329303502597
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.69867]
objective value function right now is: -1563.0149402079933
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.1924]
objective value function right now is: -1563.2891654103998
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [168.34221]
objective value function right now is: -1563.3899807031466
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.50397]
objective value function right now is: -1557.4928589028273
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.82265]
objective value function right now is: -1555.3737288486063
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1564.4304084398277
Current xi:  [167.36342]
objective value function right now is: -1564.4304084398277
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.91055]
objective value function right now is: -1560.9748512081437
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.6742]
objective value function right now is: -1558.9046588030806
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.39177]
objective value function right now is: -1563.5672651471275
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.99634]
objective value function right now is: -1561.7820853731391
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.05222]
objective value function right now is: -1558.0165507616493
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.97168]
objective value function right now is: -1555.3393976449981
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.70758]
objective value function right now is: -1557.2930301074218
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.19528]
objective value function right now is: -1560.242156500975
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.8132]
objective value function right now is: -1563.758687441808
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.433]
objective value function right now is: -1559.8023397193745
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [169.09195]
objective value function right now is: -1564.1822285614676
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [169.70482]
objective value function right now is: -1563.5921861755817
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.76186]
objective value function right now is: -1563.166942210825
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.48729]
objective value function right now is: -1562.3946108521457
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.24442]
objective value function right now is: -1562.8409448425064
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.91475]
objective value function right now is: -1560.7730829223385
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.8717]
objective value function right now is: -1554.9841956362136
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.8223]
objective value function right now is: -1561.3275524668963
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1566.4628109178539
Current xi:  [169.58145]
objective value function right now is: -1566.4628109178539
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.76125]
objective value function right now is: -1565.9334633539236
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1566.5680503811016
Current xi:  [169.70769]
objective value function right now is: -1566.5680503811016
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.78232]
objective value function right now is: -1566.1084071802672
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.81741]
objective value function right now is: -1566.2976102155858
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.76286]
objective value function right now is: -1565.9219313980393
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1566.8331010797103
Current xi:  [169.63501]
objective value function right now is: -1566.8331010797103
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.61586]
objective value function right now is: -1566.6928443597253
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.73552]
objective value function right now is: -1566.3960873556816
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.87033]
objective value function right now is: -1566.0357489353903
new min fval from sgd:  -1566.856459601381
new min fval from sgd:  -1566.8871442739378
new min fval from sgd:  -1566.894818472591
new min fval from sgd:  -1566.904627774391
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.08435]
objective value function right now is: -1566.2572390054636
new min fval from sgd:  -1566.9104357563417
new min fval from sgd:  -1566.9135492245211
new min fval from sgd:  -1566.9301138478277
new min fval from sgd:  -1566.930401678701
new min fval from sgd:  -1566.9688144726806
new min fval from sgd:  -1566.974129845819
new min fval from sgd:  -1566.9806166567635
new min fval from sgd:  -1566.985326996237
new min fval from sgd:  -1566.9884114131612
new min fval from sgd:  -1567.0378491029508
new min fval from sgd:  -1567.0686953811821
new min fval from sgd:  -1567.0864578210046
new min fval from sgd:  -1567.115123179621
new min fval from sgd:  -1567.1357521735313
new min fval from sgd:  -1567.1663335753483
new min fval from sgd:  -1567.2015436009644
new min fval from sgd:  -1567.220815374683
new min fval from sgd:  -1567.2219016175816
new min fval from sgd:  -1567.2288552800678
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.05959]
objective value function right now is: -1565.8351013378615
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.02939]
objective value function right now is: -1566.5438231500061
new min fval from sgd:  -1567.2297150839158
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.08919]
objective value function right now is: -1567.1788615719483
new min fval from sgd:  -1567.2332362957925
new min fval from sgd:  -1567.2366511964399
new min fval from sgd:  -1567.2393981388204
new min fval from sgd:  -1567.2413554428197
new min fval from sgd:  -1567.2437218034386
new min fval from sgd:  -1567.2482787527879
new min fval from sgd:  -1567.2531246082872
new min fval from sgd:  -1567.2539056043925
new min fval from sgd:  -1567.2601046664843
new min fval from sgd:  -1567.2782293379241
new min fval from sgd:  -1567.2786183970218
new min fval from sgd:  -1567.2786556861843
new min fval from sgd:  -1567.2792937225847
new min fval from sgd:  -1567.2793111950318
new min fval from sgd:  -1567.280178578323
new min fval from sgd:  -1567.2834498085826
new min fval from sgd:  -1567.2861626029787
new min fval from sgd:  -1567.2880672025212
new min fval from sgd:  -1567.2885593380063
new min fval from sgd:  -1567.2901187687783
new min fval from sgd:  -1567.2920124230498
new min fval from sgd:  -1567.2945934434706
new min fval from sgd:  -1567.2976883637994
new min fval from sgd:  -1567.301405072045
new min fval from sgd:  -1567.304346534376
new min fval from sgd:  -1567.3052261901491
new min fval from sgd:  -1567.3092633251279
new min fval from sgd:  -1567.310048388551
new min fval from sgd:  -1567.3106135054668
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.08281]
objective value function right now is: -1567.0559686501024
min fval:  -1567.3106135054668
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -5.0098,  -8.4912],
        [ -4.6020,   1.5774],
        [ -2.0051,  -7.6074],
        [-36.9980,  -7.2728],
        [ 11.7680,  -7.9176],
        [ -9.7495,   3.2815],
        [ -9.1810,   0.2892],
        [  5.9486,  -7.9361],
        [ -6.7557,  -8.7360],
        [  6.7789,   6.4752]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-6.6662, -2.3903, -6.9092, -6.9534, -7.0111, -0.9241,  7.1895, -7.2496,
        -6.2997, -8.3597], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 4.8603e+00,  1.2305e-01,  3.8680e+00,  7.4689e+00,  8.5755e+00,
         -8.9307e-01, -8.3805e+00,  5.4186e+00,  5.2223e+00,  5.9882e+00],
        [-1.0379e-01, -3.1260e-03, -5.2058e-02, -3.8113e-02, -4.9603e-01,
         -3.8536e-03, -8.1386e-01, -1.9854e-01, -1.4094e-01, -5.7583e-01],
        [-1.0379e-01, -3.1260e-03, -5.2058e-02, -3.8113e-02, -4.9603e-01,
         -3.8536e-03, -8.1386e-01, -1.9854e-01, -1.4094e-01, -5.7583e-01],
        [ 8.2397e+00,  4.0924e-01,  5.2575e+00, -2.3450e+00,  5.2304e+00,
          1.4258e-01, -6.6698e+00,  2.9152e+00,  9.4829e+00,  3.4085e+00],
        [-3.7856e-03, -3.2125e-03, -1.0018e-01, -1.7793e-01,  1.4834e+00,
         -3.7518e-02,  1.4177e+00,  3.3538e-01,  1.3190e-01,  1.4877e+00],
        [ 4.3358e-01,  1.3736e-02,  2.1695e-01,  2.5286e-01,  5.9293e-01,
          5.5419e-02,  1.9948e+00,  1.6108e-01,  5.6010e-01,  1.1860e+00],
        [ 4.8277e-01,  1.4188e-02,  2.4286e-01,  2.7150e-01,  7.5607e-01,
          6.4785e-02,  2.3646e+00,  2.0355e-01,  6.2323e-01,  1.4536e+00],
        [ 4.8764e+00,  2.5831e-01,  4.1289e+00,  7.4508e+00,  8.6391e+00,
         -5.7404e-01, -8.6191e+00,  5.3744e+00,  5.0465e+00,  5.8860e+00],
        [-1.0379e-01, -3.1260e-03, -5.2058e-02, -3.8113e-02, -4.9603e-01,
         -3.8536e-03, -8.1386e-01, -1.9854e-01, -1.4094e-01, -5.7583e-01],
        [ 5.0827e+00, -5.3554e-01,  4.2226e+00,  7.2541e+00,  1.0456e+01,
         -1.2438e+00, -8.6529e+00,  5.0431e+00,  5.2250e+00,  6.4305e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-4.6329, -1.3943, -1.3943, -2.4957,  3.3432,  2.7255,  3.2527, -4.3723,
        -1.3943, -4.6278], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.1363e+01,  1.6768e-03,  1.6768e-03, -5.2128e+00, -6.1123e+00,
          2.9467e+00,  8.8920e+00, -1.1425e+01,  1.6767e-03, -1.4222e+01]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 14.6484,   5.4503],
        [ -1.8186,   0.6115],
        [-12.0648,  11.9035],
        [ -9.7840,   1.3382],
        [-11.5205,  -8.1515],
        [-20.8327,  -5.8138],
        [ -1.8186,   0.6115],
        [ 11.4852,  -0.5912],
        [ -2.7542, -12.6181],
        [-16.2409,   3.8699]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  3.3292,  -4.3280,  10.3029,   3.6072,  -5.6960,  -5.3371,  -4.3280,
        -10.7170, -11.2383,   4.5094], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-6.2664e+00, -2.1623e-01,  1.3279e+00,  2.4312e+00,  7.1765e-01,
          3.4338e+00, -2.1623e-01,  4.7688e+00,  3.0127e+00, -2.6697e-01],
        [-3.8084e+00, -4.8592e-02,  5.7119e+00, -1.8640e+00, -4.0541e+00,
          1.0127e+00, -4.8592e-02,  3.6932e-01, -9.5412e-01,  8.0666e+00],
        [-7.7639e+00, -1.0353e-01, -1.4947e+01,  4.8224e-01,  9.6665e+00,
          9.4241e+00, -1.0353e-01, -1.3805e+01,  7.0409e+00,  3.4580e+00],
        [-1.0987e+00,  3.3363e-01, -1.4863e+01,  4.9251e-01,  5.9280e+00,
         -4.8803e-01,  3.3363e-01, -7.5402e+00, -8.0763e+00, -6.5025e+00],
        [-1.2998e+01,  1.1582e-01, -2.6599e+00, -2.1632e+00,  6.9502e+00,
          5.1359e+00,  1.1582e-01, -1.5492e+01,  1.4074e+01, -1.8777e+00],
        [-3.3441e+00,  4.6950e-05, -3.0566e-01, -4.4654e-01, -1.0239e-01,
         -2.4082e-03,  4.6950e-05, -6.1845e-01, -7.0098e-01, -1.7028e-01],
        [-3.3441e+00,  4.6911e-05, -3.0566e-01, -4.4654e-01, -1.0239e-01,
         -2.4082e-03,  4.6912e-05, -6.1845e-01, -7.0098e-01, -1.7028e-01],
        [ 7.0539e-01, -1.1479e-01,  2.4639e-01,  5.7106e+00, -1.8057e+01,
          2.8935e+00, -1.1479e-01,  1.0198e+00, -1.1316e+00,  1.0389e+01],
        [-3.1675e+00,  6.3292e-02, -2.4960e+01,  2.0842e+00,  9.1014e+00,
         -2.5068e+00,  6.3293e-02, -1.1474e+01,  3.7501e+00, -7.6056e+00],
        [-1.7365e+00,  1.8926e-01, -4.8706e+00,  7.4078e+00, -1.1084e+01,
          2.7863e-01,  1.8926e-01, -1.3792e+01, -1.8725e+01, -7.8028e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.7617, -5.4992, -2.9071, -1.5773, -9.3328, -3.4514, -3.4514, -0.6094,
        -3.4531, -2.4630], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 2.5114e+00, -2.6011e-01, -6.6572e+00,  3.7165e+00,  1.9222e+01,
         -5.3888e-04, -4.7432e-04,  6.3267e-01, -3.9650e+00, -4.1755e+00],
        [-2.5013e+00,  2.4704e-01,  6.6628e+00, -3.5812e+00, -1.9219e+01,
          4.5177e-04,  5.1628e-04, -6.8436e-01,  4.0175e+00,  4.1219e+00]],
       device='cuda:0'))])
xi:  [170.09268]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 652.8660505681802
W_T_median: 391.86678161508974
W_T_pctile_5: 170.1598520637165
W_T_CVAR_5_pct: 16.07939902759959
Average q (qsum/M+1):  49.002378402217744
Optimal xi:  [170.09268]
Expected(across Rb) median(across samples) p_equity:  0.2728838374217351
obj fun:  tensor(-1567.3106, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
loaded xi:  170.09268
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1598.9915372980322
Current xi:  [174.8777]
objective value function right now is: -1598.9915372980322
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [179.3318]
objective value function right now is: -1598.0024107117526
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1600.4461211469065
Current xi:  [182.68073]
objective value function right now is: -1600.4461211469065
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.40466]
objective value function right now is: -1594.4795873778885
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.8145]
objective value function right now is: -1599.209389350224
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.66501]
objective value function right now is: -1600.2403288490436
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [186.5224]
objective value function right now is: -1599.4348685116959
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.44939]
objective value function right now is: -1592.9732066350116
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1603.0527291770854
Current xi:  [188.4092]
objective value function right now is: -1603.0527291770854
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.77156]
objective value function right now is: -1592.2535429807288
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.38579]
objective value function right now is: -1600.455130501149
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.98825]
objective value function right now is: -1589.7417588661144
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.49333]
objective value function right now is: -1600.3634280203544
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [190.56944]
objective value function right now is: -1600.370155751511
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1603.869169559143
Current xi:  [190.61559]
objective value function right now is: -1603.869169559143
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.82674]
objective value function right now is: -1600.011750185467
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.4218232195067
Current xi:  [190.03337]
objective value function right now is: -1605.4218232195067
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.42531]
objective value function right now is: -1586.772613330613
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.70303]
objective value function right now is: -1591.89735068197
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.72208]
objective value function right now is: -1598.0728611153593
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.04579]
objective value function right now is: -1599.4647424870293
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.74048]
objective value function right now is: -1604.566919462204
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.2988]
objective value function right now is: -1602.4818145574457
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.26393]
objective value function right now is: -1602.9532023379045
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.49835]
objective value function right now is: -1599.8627751483914
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.81226]
objective value function right now is: -1603.0845016467788
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.7044]
objective value function right now is: -1601.7770159842635
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [189.27492]
objective value function right now is: -1604.661405802141
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [189.38835]
objective value function right now is: -1602.1025999415065
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.76782]
objective value function right now is: -1603.5171419706746
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.20706]
objective value function right now is: -1598.2700841517005
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.32805]
objective value function right now is: -1603.1115932071023
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.92549]
objective value function right now is: -1599.9521161916605
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.91025]
objective value function right now is: -1598.2969227660321
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.80367]
objective value function right now is: -1602.4042828265765
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1606.8780285180178
Current xi:  [189.83282]
objective value function right now is: -1606.8780285180178
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1607.0351022547527
Current xi:  [189.96313]
objective value function right now is: -1607.0351022547527
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1607.1227755229365
Current xi:  [189.98727]
objective value function right now is: -1607.1227755229365
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.06316]
objective value function right now is: -1606.1644165463815
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.01779]
objective value function right now is: -1604.4869735820569
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.13065]
objective value function right now is: -1606.9153221285997
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.31949]
objective value function right now is: -1607.0118966041923
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.35182]
objective value function right now is: -1605.9535931914018
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.36691]
objective value function right now is: -1607.1209558889366
new min fval from sgd:  -1607.4405066073757
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.65923]
objective value function right now is: -1607.4405066073757
new min fval from sgd:  -1607.4833416891925
new min fval from sgd:  -1607.4910181106166
new min fval from sgd:  -1607.4972529750903
new min fval from sgd:  -1607.5083996118374
new min fval from sgd:  -1607.5275004667017
new min fval from sgd:  -1607.5299431854571
new min fval from sgd:  -1607.5828854707327
new min fval from sgd:  -1607.66336121235
new min fval from sgd:  -1607.690209738698
new min fval from sgd:  -1607.7311425994155
new min fval from sgd:  -1607.8018504194956
new min fval from sgd:  -1607.8107882758393
new min fval from sgd:  -1607.8476668126198
new min fval from sgd:  -1607.8681018139673
new min fval from sgd:  -1607.8967909244266
new min fval from sgd:  -1607.920654453375
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.76645]
objective value function right now is: -1606.928608703694
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.84608]
objective value function right now is: -1606.4444930103823
new min fval from sgd:  -1607.962304748229
new min fval from sgd:  -1608.0050727177668
new min fval from sgd:  -1608.0567601660389
new min fval from sgd:  -1608.0728263885644
new min fval from sgd:  -1608.138765405085
new min fval from sgd:  -1608.2183418602665
new min fval from sgd:  -1608.2191506836741
new min fval from sgd:  -1608.2408911991351
new min fval from sgd:  -1608.3003181702227
new min fval from sgd:  -1608.3330593472892
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.74857]
objective value function right now is: -1607.4183363412997
new min fval from sgd:  -1608.3425432319916
new min fval from sgd:  -1608.3581143203617
new min fval from sgd:  -1608.3703471723938
new min fval from sgd:  -1608.3786674266635
new min fval from sgd:  -1608.3874272107446
new min fval from sgd:  -1608.3917450344627
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.68108]
objective value function right now is: -1608.1798100812205
new min fval from sgd:  -1608.3959803029215
new min fval from sgd:  -1608.4025067138034
new min fval from sgd:  -1608.4104221750722
new min fval from sgd:  -1608.4159662006095
new min fval from sgd:  -1608.4203835729327
new min fval from sgd:  -1608.4219669269053
new min fval from sgd:  -1608.4240205471328
new min fval from sgd:  -1608.4260698121077
new min fval from sgd:  -1608.4307613313117
new min fval from sgd:  -1608.4336415213556
new min fval from sgd:  -1608.4350524688364
new min fval from sgd:  -1608.4362166397134
new min fval from sgd:  -1608.4385373712073
new min fval from sgd:  -1608.4406327399981
new min fval from sgd:  -1608.4419100582286
new min fval from sgd:  -1608.4433445159202
new min fval from sgd:  -1608.4445092590843
new min fval from sgd:  -1608.4461091543847
new min fval from sgd:  -1608.447651842873
new min fval from sgd:  -1608.4487059983314
new min fval from sgd:  -1608.4498839980147
new min fval from sgd:  -1608.4499334189704
new min fval from sgd:  -1608.4525532086766
new min fval from sgd:  -1608.4547992606456
new min fval from sgd:  -1608.4580788354538
new min fval from sgd:  -1608.4633508238776
new min fval from sgd:  -1608.4676491566354
new min fval from sgd:  -1608.474382779969
new min fval from sgd:  -1608.4787161024365
new min fval from sgd:  -1608.4821797870877
new min fval from sgd:  -1608.4835518037617
new min fval from sgd:  -1608.4837567025702
new min fval from sgd:  -1608.4847295220732
new min fval from sgd:  -1608.4859663736377
new min fval from sgd:  -1608.4888488568427
new min fval from sgd:  -1608.490724088701
new min fval from sgd:  -1608.4944575974152
new min fval from sgd:  -1608.4964676247798
new min fval from sgd:  -1608.4985503473968
new min fval from sgd:  -1608.5004810729722
new min fval from sgd:  -1608.501526298381
new min fval from sgd:  -1608.5052250686042
new min fval from sgd:  -1608.506100874086
new min fval from sgd:  -1608.507657969215
new min fval from sgd:  -1608.5084353598927
new min fval from sgd:  -1608.5112068766891
new min fval from sgd:  -1608.5145831675688
new min fval from sgd:  -1608.5184785723338
new min fval from sgd:  -1608.5215297581485
new min fval from sgd:  -1608.5236568608168
new min fval from sgd:  -1608.525483459623
new min fval from sgd:  -1608.5268331542718
new min fval from sgd:  -1608.5286275250344
new min fval from sgd:  -1608.5314248730663
new min fval from sgd:  -1608.5353874597595
new min fval from sgd:  -1608.538057685537
new min fval from sgd:  -1608.541277011948
new min fval from sgd:  -1608.5470227644466
new min fval from sgd:  -1608.552704779208
new min fval from sgd:  -1608.5575989272186
new min fval from sgd:  -1608.5579822166544
new min fval from sgd:  -1608.5592411367138
new min fval from sgd:  -1608.5616946622572
new min fval from sgd:  -1608.5633848768707
new min fval from sgd:  -1608.56578116077
new min fval from sgd:  -1608.5676291147158
new min fval from sgd:  -1608.5681681266774
new min fval from sgd:  -1608.5685220591174
new min fval from sgd:  -1608.5735984927608
new min fval from sgd:  -1608.5779396750745
new min fval from sgd:  -1608.5822371068268
new min fval from sgd:  -1608.5854731731167
new min fval from sgd:  -1608.5884179027305
new min fval from sgd:  -1608.5894098234062
new min fval from sgd:  -1608.5896015999654
new min fval from sgd:  -1608.589958956261
new min fval from sgd:  -1608.590223086446
new min fval from sgd:  -1608.595725704046
new min fval from sgd:  -1608.5967078057358
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.64357]
objective value function right now is: -1608.0369943080939
min fval:  -1608.5967078057358
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -4.4148,  -9.2736],
        [ -1.2256,   0.2886],
        [ -1.9738,  -8.1342],
        [-32.9275,  -7.8861],
        [ 12.0473,  -7.5309],
        [ -7.6407,   2.6965],
        [-10.6208,   0.4012],
        [  6.7749,  -8.9879],
        [ -6.0683,  -9.5202],
        [  7.1764,   5.9880]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-7.0334, -3.1582, -7.4670, -7.3285, -6.8701, -3.0164,  7.9960, -7.1923,
        -6.6681, -8.6148], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 5.1442e+00,  3.0161e-02,  3.7087e+00,  8.7548e+00,  8.6308e+00,
         -6.3371e-01, -1.0082e+01,  5.6520e+00,  5.4299e+00,  5.6552e+00],
        [-9.1321e-02, -1.3078e-02, -2.6434e-02, -2.7096e-02, -5.4837e-01,
         -1.3614e-02, -8.1973e-01, -2.9139e-01, -1.2399e-01, -5.2388e-01],
        [-9.1321e-02, -1.3078e-02, -2.6434e-02, -2.7096e-02, -5.4837e-01,
         -1.3614e-02, -8.1974e-01, -2.9139e-01, -1.2400e-01, -5.2388e-01],
        [ 8.1934e+00,  5.3333e-02,  4.2194e+00, -3.4951e+00,  4.3934e+00,
          3.5541e-02, -7.3152e+00,  2.3195e+00,  9.7802e+00,  2.0522e+00],
        [-5.9253e-02,  2.9132e-03, -1.2059e-01, -1.5573e-01,  1.3458e+00,
          1.0620e-01,  1.5101e+00,  4.7238e-01,  1.4636e-02,  1.1185e+00],
        [ 3.7214e-01,  3.0388e-02,  1.4302e-01,  2.1575e-01,  5.0754e-01,
         -9.9157e-02,  1.9089e+00,  4.0330e-01,  4.8733e-01,  1.3100e+00],
        [ 4.2801e-01,  3.2151e-02,  1.6397e-01,  2.3712e-01,  6.7391e-01,
         -1.0546e-01,  2.2879e+00,  4.5764e-01,  5.5918e-01,  1.6177e+00],
        [ 5.2014e+00,  4.0997e-02,  3.9767e+00,  8.7893e+00,  8.6644e+00,
         -6.6316e-01, -1.0342e+01,  5.6109e+00,  5.3058e+00,  5.5464e+00],
        [-9.1321e-02, -1.3078e-02, -2.6434e-02, -2.7096e-02, -5.4837e-01,
         -1.3614e-02, -8.1973e-01, -2.9139e-01, -1.2399e-01, -5.2388e-01],
        [ 5.1433e+00, -4.2975e-02,  3.9259e+00,  6.1865e+00,  1.1122e+01,
         -9.8986e-01, -9.6753e+00,  5.6568e+00,  5.2464e+00,  5.7897e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-4.3138, -1.4444, -1.4444, -1.9016,  3.5744,  2.6594,  3.2249, -4.0466,
        -1.4444, -4.1536], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.2027e+01,  9.1978e-03,  9.1978e-03, -4.3448e+00, -6.2515e+00,
          2.5196e+00,  8.3345e+00, -1.2290e+01,  9.1979e-03, -1.4318e+01]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 14.9211,   5.3707],
        [ -1.4656,   3.0377],
        [-10.6708,  12.3614],
        [ -9.6180,   2.2080],
        [-11.6604,  -8.4104],
        [-19.1339,  -5.9761],
        [ -1.4656,   3.0377],
        [ 12.4624,  -0.4710],
        [ -2.0160, -12.6855],
        [-18.0869,   4.9827]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  2.8943,  -5.2356,  10.6534,   4.8814,  -5.7116,  -6.2092,  -5.2357,
        -11.1257, -11.2236,   5.3951], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-6.3768e+00, -1.4543e+00,  3.4965e+00,  2.3997e+00,  4.6774e-01,
          2.6559e+00, -1.4544e+00,  5.3074e+00,  3.1425e+00, -2.2371e+00],
        [-3.4585e+00, -4.7187e-01,  4.5846e+00, -2.9413e+00,  1.4811e-01,
          8.0780e-01, -4.7187e-01, -1.8808e+00, -1.2185e-01,  4.2329e+00],
        [-7.6861e+00, -2.1051e-02, -1.3413e+01, -7.9886e-02,  9.9482e+00,
          8.1711e+00, -2.1049e-02, -1.3686e+01,  7.2974e+00,  3.7214e+00],
        [-1.1025e+00, -9.6689e-01, -7.4362e+00,  1.9657e+00,  8.0096e+00,
          3.0959e+00, -9.6690e-01, -6.3196e+00, -1.7671e+01,  8.1129e-01],
        [-1.3241e+01, -3.7397e-02, -3.3652e+00, -3.3688e+00,  8.0464e+00,
          6.2160e+00, -3.7397e-02, -1.7151e+01,  1.3826e+01,  3.6588e-01],
        [-2.4149e+00, -3.4794e+00,  6.3628e-01,  3.6609e+00, -2.4389e+00,
         -1.8873e+00, -3.4794e+00,  4.4776e-01, -2.7300e+00, -3.0574e+00],
        [-2.4149e+00, -3.4794e+00,  6.3627e-01,  3.6609e+00, -2.4389e+00,
         -1.8872e+00, -3.4794e+00,  4.4776e-01, -2.7300e+00, -3.0574e+00],
        [ 1.4962e+00,  8.1069e-01,  3.9852e+00,  6.2566e+00, -2.9289e+01,
          4.8518e+00,  8.1067e-01,  1.9502e+00, -2.3370e+00,  1.4888e+01],
        [-3.8619e+00, -4.6168e-02, -2.7571e+01,  2.0160e+00,  9.2120e+00,
          1.5027e-01, -4.6168e-02, -1.0553e+01,  3.8211e+00, -8.6850e+00],
        [-1.7727e+00, -3.7580e-01, -2.8990e+00,  4.9583e+00, -1.1708e+01,
          7.5807e-02, -3.7580e-01, -1.1161e+01, -2.1413e+01, -1.2052e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -4.3049,  -5.3694,  -3.1162,  -1.7030, -10.5405,  -3.3017,  -3.3017,
          0.1673,  -3.8210,  -2.8335], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  2.5640,  -0.7718,  -7.4095,   4.5657,  21.5741,   2.0915,   2.0916,
           0.4194,  -4.4592,  -4.3454],
        [ -2.5541,   0.7592,   7.4146,  -4.4359, -21.5707,  -2.0916,  -2.0915,
          -0.4710,   4.5100,   4.3025]], device='cuda:0'))])
xi:  [190.65733]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 668.2334646586933
W_T_median: 422.8929981854813
W_T_pctile_5: 190.98083761997603
W_T_CVAR_5_pct: 23.472003639489408
Average q (qsum/M+1):  48.104594569052416
Optimal xi:  [190.65733]
Expected(across Rb) median(across samples) p_equity:  0.26045606657862663
obj fun:  tensor(-1608.5967, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
loaded xi:  190.65733
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2827.946747617161
Current xi:  [198.06691]
objective value function right now is: -2827.946747617161
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [202.50671]
objective value function right now is: -2804.263364099936
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -2856.6699802867847
Current xi:  [205.63837]
objective value function right now is: -2856.6699802867847
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.67006]
objective value function right now is: -2838.2965140922247
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -2867.4138593945277
Current xi:  [210.47343]
objective value function right now is: -2867.4138593945277
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.086]
objective value function right now is: -2853.819138236118
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [212.80167]
objective value function right now is: -2806.4385264287066
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.4685]
objective value function right now is: -2866.6731731872515
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.23965]
objective value function right now is: -2821.8283502240256
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.1938]
objective value function right now is: -2847.4400552578913
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.38846]
objective value function right now is: -2861.4270340241055
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -2878.779638471455
Current xi:  [213.61018]
objective value function right now is: -2878.779638471455
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.17607]
objective value function right now is: -2823.4991674908074
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [214.71567]
objective value function right now is: -2804.70748363293
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.79659]
objective value function right now is: -2788.680645340805
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [216.56415]
objective value function right now is: -2864.3584118442004
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.87746]
objective value function right now is: -2858.1076092725175
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -2888.204062321758
Current xi:  [215.4328]
objective value function right now is: -2888.204062321758
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.32524]
objective value function right now is: -2808.060452041845
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.49953]
objective value function right now is: -2859.677235170589
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.83235]
objective value function right now is: -2861.052697161808
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.41606]
objective value function right now is: -2814.7089879271825
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.80423]
objective value function right now is: -2873.947192312182
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -2889.6352853063227
Current xi:  [213.87924]
objective value function right now is: -2889.6352853063227
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.90787]
objective value function right now is: -2871.7017544053374
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.0114]
objective value function right now is: -2853.839563268914
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.62253]
objective value function right now is: -2855.6452297387914
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [215.34749]
objective value function right now is: -2869.322887143199
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [211.47762]
objective value function right now is: -2815.0379844943027
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.19089]
objective value function right now is: -2848.467915948853
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.46968]
objective value function right now is: -2877.1884883008406
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -2899.773349005182
Current xi:  [213.27448]
objective value function right now is: -2899.773349005182
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.50473]
objective value function right now is: -2864.462913996237
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.22922]
objective value function right now is: -2895.516810289979
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.94012]
objective value function right now is: -2753.9338275268587
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2902.837934270509
Current xi:  [214.88089]
objective value function right now is: -2902.837934270509
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.95273]
objective value function right now is: -2901.0626714250975
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.41444]
objective value function right now is: -2890.6391419194238
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.24814]
objective value function right now is: -2886.928066707047
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.36476]
objective value function right now is: -2899.7272952059084
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -2904.788305149963
Current xi:  [215.33347]
objective value function right now is: -2904.788305149963
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.45062]
objective value function right now is: -2899.6886869473597
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -2912.9330902044367
Current xi:  [215.24776]
objective value function right now is: -2912.9330902044367
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.41377]
objective value function right now is: -2893.3845484393805
new min fval from sgd:  -2915.5365549576386
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.56473]
objective value function right now is: -2915.5365549576386
new min fval from sgd:  -2916.1800796769744
new min fval from sgd:  -2917.8333505576898
new min fval from sgd:  -2918.6018164717407
new min fval from sgd:  -2918.8092239270345
new min fval from sgd:  -2919.0083242335945
new min fval from sgd:  -2919.5074881450914
new min fval from sgd:  -2919.679113198268
new min fval from sgd:  -2919.709172512564
new min fval from sgd:  -2919.787707435282
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.63295]
objective value function right now is: -2909.2275143161164
new min fval from sgd:  -2919.9446349471395
new min fval from sgd:  -2920.3177233140536
new min fval from sgd:  -2920.4831803023594
new min fval from sgd:  -2920.500487203357
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.69669]
objective value function right now is: -2920.3864014449623
new min fval from sgd:  -2920.584213304217
new min fval from sgd:  -2920.719353069679
new min fval from sgd:  -2920.7194596629806
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.72374]
objective value function right now is: -2892.81480379275
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.79575]
objective value function right now is: -2920.0254334413166
new min fval from sgd:  -2920.738944550347
new min fval from sgd:  -2920.773462136693
new min fval from sgd:  -2920.8073983704885
new min fval from sgd:  -2920.8377221421674
new min fval from sgd:  -2920.8541439685628
new min fval from sgd:  -2920.8640244359544
new min fval from sgd:  -2920.8762642928623
new min fval from sgd:  -2920.8975561647567
new min fval from sgd:  -2920.9264792356184
new min fval from sgd:  -2920.959545103509
new min fval from sgd:  -2920.9811196137434
new min fval from sgd:  -2920.9977277519647
new min fval from sgd:  -2921.0013547365015
new min fval from sgd:  -2921.0177580252316
new min fval from sgd:  -2921.0311650021436
new min fval from sgd:  -2921.041147683787
new min fval from sgd:  -2921.04792000641
new min fval from sgd:  -2921.060876169265
new min fval from sgd:  -2921.0879415103477
new min fval from sgd:  -2921.1533320825074
new min fval from sgd:  -2921.252945363056
new min fval from sgd:  -2921.524596172873
new min fval from sgd:  -2921.746837531178
new min fval from sgd:  -2921.828205525901
new min fval from sgd:  -2921.8657899148234
new min fval from sgd:  -2921.9011301714563
new min fval from sgd:  -2921.9219230184835
new min fval from sgd:  -2921.948010955542
new min fval from sgd:  -2921.959458272703
new min fval from sgd:  -2921.960881919329
new min fval from sgd:  -2921.965267334224
new min fval from sgd:  -2921.9722565208635
new min fval from sgd:  -2921.9789861008207
new min fval from sgd:  -2921.992387308621
new min fval from sgd:  -2921.995749131819
new min fval from sgd:  -2921.9969027506727
new min fval from sgd:  -2921.998274566034
new min fval from sgd:  -2922.034711337741
new min fval from sgd:  -2922.051639226695
new min fval from sgd:  -2922.090519505918
new min fval from sgd:  -2922.1155868915775
new min fval from sgd:  -2922.146787560806
new min fval from sgd:  -2922.3377567782263
new min fval from sgd:  -2922.4513060298523
new min fval from sgd:  -2922.514570412582
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.76878]
objective value function right now is: -2921.033363023817
min fval:  -2922.514570412582
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.6958e+00, -1.0312e+01],
        [-1.4545e+00,  1.1622e-01],
        [-1.9565e-03, -9.2461e+00],
        [-1.7962e+01, -8.1415e+00],
        [ 1.3269e+01, -7.1156e+00],
        [-4.8882e+00,  6.8117e+00],
        [-1.2407e+01,  7.9535e-01],
        [ 6.5519e+00, -9.3819e+00],
        [-3.4293e+00, -1.0100e+01],
        [ 7.1960e+00,  6.4208e+00]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-7.3050, -4.1114, -7.7211, -8.2923, -6.6692,  0.9060,  7.8596, -7.8033,
        -7.3721, -9.4686], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 5.7421e+00,  9.7363e-02,  4.1686e+00,  6.1392e+00,  1.0817e+01,
         -9.3341e-01, -1.0691e+01,  6.8242e+00,  5.7064e+00,  4.4666e+00],
        [-1.4587e-01, -4.9133e-03, -6.5495e-02,  2.6584e-03, -6.5700e-01,
         -7.5311e-01, -1.1272e+00, -1.8673e-01, -1.1304e-01, -6.2769e-01],
        [-1.4587e-01, -4.9133e-03, -6.5495e-02,  2.6584e-03, -6.5700e-01,
         -7.5311e-01, -1.1272e+00, -1.8673e-01, -1.1304e-01, -6.2769e-01],
        [ 6.4379e+00, -1.4866e-01,  2.6765e+00, -4.8588e+00,  6.2090e+00,
         -4.9483e-01, -7.1957e+00,  3.5510e+00,  7.1608e+00,  4.0361e-01],
        [ 5.2791e-03,  9.4348e-03, -9.9278e-02, -1.6579e-01,  1.2587e+00,
          1.2652e+00,  2.0284e+00,  2.8241e-01, -1.0479e-01,  7.6837e-01],
        [ 4.6444e-01,  1.3432e-02,  2.0728e-01,  6.8459e-02,  8.6814e-01,
          1.0713e+00,  2.0846e+00,  3.0329e-01,  4.3981e-01,  1.0113e+00],
        [ 5.4206e-01,  1.1200e-02,  2.4303e-01,  7.8214e-02,  1.1230e+00,
          1.3409e+00,  2.5257e+00,  3.6189e-01,  5.0948e-01,  1.2021e+00],
        [ 5.7677e+00,  9.6778e-02,  4.4009e+00,  6.1323e+00,  1.0871e+01,
         -9.0991e-01, -1.0936e+01,  6.8262e+00,  5.5428e+00,  4.4010e+00],
        [-1.4587e-01, -4.9133e-03, -6.5494e-02,  2.6584e-03, -6.5700e-01,
         -7.5311e-01, -1.1272e+00, -1.8673e-01, -1.1304e-01, -6.2769e-01],
        [ 6.0459e+00, -1.2681e-01,  4.7862e+00,  4.4391e+00,  1.2248e+01,
         -7.4924e-01, -1.0407e+01,  5.8796e+00,  5.7644e+00,  4.8050e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.8726, -1.9331, -1.9331, -0.8487,  3.8427,  3.0075,  3.7426, -2.5768,
        -1.9331, -3.1058], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.3243e+01,  7.5839e-03,  7.5838e-03, -1.5495e+00, -6.8189e+00,
          1.7919e+00,  7.5544e+00, -1.3734e+01,  7.5838e-03, -1.5503e+01]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 15.0806,   5.2988],
        [ -3.9169,   3.0739],
        [ -7.3126,  13.2742],
        [ -7.3891,   3.5004],
        [-11.5780,  -8.6780],
        [-20.1507,  -6.3833],
        [ -3.9169,   3.0739],
        [ 13.1232,  -0.5126],
        [ -2.5521, -12.7979],
        [-17.5107,   6.0880]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  2.0889,  -5.8184,  10.9301,   5.2473,  -6.2478,  -6.4641,  -5.8184,
        -11.8874, -11.3270,   5.3785], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -6.0190,  -1.5189,   2.9902,   4.9270,   0.2866,  -3.3689,  -1.5189,
           5.5069,   5.0952,  -6.7891],
        [ -2.0299,  -3.6586,   0.6657,   0.1981,  -0.1317,   0.5999,  -3.6586,
          -2.5005,  -1.0835,   1.2202],
        [ -7.6601,  -0.4257, -13.6476,   1.3013,   9.4166,   8.2689,  -0.4257,
         -15.1957,   7.1788,   2.0558],
        [ -1.3112,  -3.0774,  -2.5898,   1.8884,   7.0996,   4.2718,  -3.0774,
         -13.9869, -23.0468,  -1.5422],
        [-13.1292,  -0.0546,  -7.9402,  -3.6630,   8.9992,   9.8730,  -0.0546,
         -17.4944,  12.5253,   4.2096],
        [ -2.3831,  -3.7032,   1.6196,   0.0699,  -0.1492,  -0.4492,  -3.7032,
          -2.5682,  -4.3284,  -2.9400],
        [ -2.3830,  -3.7032,   1.6197,   0.0699,  -0.1492,  -0.4492,  -3.7032,
          -2.5682,  -4.3284,  -2.9400],
        [  1.1465,   0.5087,  15.8676,   3.9285, -38.1096,   9.6124,   0.5087,
           2.2812,  -1.6630,  13.3636],
        [ -4.3088,  -0.2748, -29.8973,   2.9870,   9.8019,  -8.6312,  -0.2748,
         -12.0055,   4.2562, -11.7507],
        [ -2.0422,  -3.6646,   0.6964,   0.2072,  -0.1107,   0.6255,  -3.6646,
          -2.5303,  -1.0563,   1.2555]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -6.3267,  -4.1670,  -3.2895,  -2.1788, -12.3096,  -2.9501,  -2.9501,
         -0.2771,  -3.4044,  -4.1028], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  2.0973,   1.8897,  -9.2636,   4.6334,  26.8377,   3.5576,   3.5576,
           0.4404,  -5.1300,   1.8877],
        [ -2.0874,  -1.8962,   9.2685,  -4.5045, -26.8341,  -3.5576,  -3.5576,
          -0.4921,   5.1803,  -1.9008]], device='cuda:0'))])
xi:  [215.76927]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 726.9036184893503
W_T_median: 507.8364091723897
W_T_pctile_5: 215.97368127993892
W_T_CVAR_5_pct: 30.112656158511268
Average q (qsum/M+1):  45.70630276587702
Optimal xi:  [215.76927]
Expected(across Rb) median(across samples) p_equity:  0.2265891619026661
obj fun:  tensor(-2922.5146, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
