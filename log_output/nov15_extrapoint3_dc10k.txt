Starting at: 
16-11-22_14:42

 Random seed:  2  


extra point pasted:
############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1  logistic_sigmoid   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer        4  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer        4  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0       (2, 4)     False        None  
0       (4, 4)     False        None  
0       (4, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer        4  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer        4  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0       (2, 4)     False        None  
0       (4, 4)     False        None  
0       (4, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.871969]
objective value function right now is: -1086.2681689177375
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.172177]
objective value function right now is: -1095.0684921254567
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.82119]
objective value function right now is: -1139.8241888415344
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.2436]
objective value function right now is: -1144.0616157797647
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.128517]
objective value function right now is: -1166.3219958013383
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.24541]
objective value function right now is: -1178.4688409759713
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [14.318214]
objective value function right now is: -1168.355913505624
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.755195]
objective value function right now is: -1145.5022568852125
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.086935]
objective value function right now is: -1231.000141314223
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.761175]
objective value function right now is: -1289.1369750757851
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.519324]
objective value function right now is: -1356.0244074472153
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.433351]
objective value function right now is: -1366.0360285026927
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.760876]
objective value function right now is: -1375.6483027919442
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [13.263622]
objective value function right now is: -1358.3213432294574
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.011749]
objective value function right now is: -1358.6840924155144
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.774003]
objective value function right now is: -1389.4003675998617
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.747355]
objective value function right now is: -1390.141466763421
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.153421]
objective value function right now is: -1379.019810714814
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.93243]
objective value function right now is: -1375.514583527728
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.445069]
objective value function right now is: -1395.0866717523604
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.831944]
objective value function right now is: -1371.146547576981
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.871484]
objective value function right now is: -1397.3983464312419
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.5992155]
objective value function right now is: -1378.6365464877938
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.772982]
objective value function right now is: -1399.7262873454324
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.796402]
objective value function right now is: -1408.5829340805658
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.772748]
objective value function right now is: -1407.7583141488083
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.611494]
objective value function right now is: -1404.6501655090674
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [13.838604]
objective value function right now is: -1407.535469239514
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [14.020751]
objective value function right now is: -1412.7257337159142
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.062371]
objective value function right now is: -1417.2587203143296
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.626132]
objective value function right now is: -1402.3260675882993
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.122809]
objective value function right now is: -1405.5694851907358
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.928609]
objective value function right now is: -1422.6290452917851
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.180489]
objective value function right now is: -1415.131748139413
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.076198]
objective value function right now is: -1420.7384240549175
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.710197]
objective value function right now is: -1420.8769402427586
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.647585]
objective value function right now is: -1424.2759448905917
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.953872]
objective value function right now is: -1426.372440034822
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.190853]
objective value function right now is: -1432.7246494334308
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.428659]
objective value function right now is: -1415.9706402495372
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.247484]
objective value function right now is: -1428.289741045083
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.090892]
objective value function right now is: -1423.7491952705573
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.106925]
objective value function right now is: -1429.3040789323381
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.295643]
objective value function right now is: -1432.9736385992132
new min fval:  9580.813866912466
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.157429]
objective value function right now is: -1369.6487154470337
new min fval:  -1394.1294557358697
new min fval:  -1405.0640900250517
new min fval:  -1414.0372114496647
new min fval:  -1420.9905581751098
new min fval:  -1424.0926482036878
new min fval:  -1425.1568848994846
new min fval:  -1426.2370497304016
new min fval:  -1426.8772990427933
new min fval:  -1427.2978273599988
new min fval:  -1427.6329772466165
new min fval:  -1427.8965489609734
new min fval:  -1428.0375772756006
new min fval:  -1428.085362726052
new min fval:  -1428.5382441689383
new min fval:  -1429.076231041691
new min fval:  -1429.6356135083395
new min fval:  -1430.1409290991585
new min fval:  -1430.5546141176042
new min fval:  -1430.8942042244014
new min fval:  -1431.1550990144872
new min fval:  -1431.3456965383725
new min fval:  -1431.5031464095316
new min fval:  -1431.65277463131
new min fval:  -1431.8227937545928
new min fval:  -1432.0245673588308
new min fval:  -1432.2670830949392
new min fval:  -1432.5629302676655
new min fval:  -1432.908393624532
new min fval:  -1433.2574117770787
new min fval:  -1433.5861762924567
new min fval:  -1433.8797032534387
new min fval:  -1434.1247546976465
new min fval:  -1434.3418733886815
new min fval:  -1434.524454836451
new min fval:  -1434.6961921907896
new min fval:  -1434.8618279810364
new min fval:  -1435.036979116467
new min fval:  -1435.218151059138
new min fval:  -1435.3847632381492
new min fval:  -1435.523822895712
new min fval:  -1435.635532307535
new min fval:  -1435.7357579798145
new min fval:  -1435.8254302674081
new min fval:  -1435.9120008795508
new min fval:  -1435.9997506465631
new min fval:  -1436.097045152361
new min fval:  -1436.203496463526
new min fval:  -1436.320461324936
new min fval:  -1436.442052899974
new min fval:  -1436.5649206106789
new min fval:  -1436.6798667990283
new min fval:  -1436.7769987334027
new min fval:  -1436.8555388067655
new min fval:  -1436.9235357146274
new min fval:  -1436.9908277312727
new min fval:  -1437.0588325891142
new min fval:  -1437.1315932876753
new min fval:  -1437.2064482844378
new min fval:  -1437.2850893357015
new min fval:  -1437.3592825133242
new min fval:  -1437.420753046816
new min fval:  -1437.469770522832
new min fval:  -1437.50876376598
new min fval:  -1437.5407825715345
new min fval:  -1437.5782099713254
new min fval:  -1437.6214199895933
new min fval:  -1437.674193002298
new min fval:  -1437.7351806607587
new min fval:  -1437.7993266709022
new min fval:  -1437.8670292289353
new min fval:  -1437.9367651910247
new min fval:  -1438.001812378854
new min fval:  -1438.058869494271
new min fval:  -1438.108038332861
new min fval:  -1438.1497855231
new min fval:  -1438.185229725223
new min fval:  -1438.2175693365655
new min fval:  -1438.244719056318
new min fval:  -1438.268387960063
new min fval:  -1438.2926951180852
new min fval:  -1438.3235814353013
new min fval:  -1438.3571802538368
new min fval:  -1438.3854665634522
new min fval:  -1438.4138552513327
new min fval:  -1438.443329729149
new min fval:  -1438.474872409661
new min fval:  -1438.507270851556
new min fval:  -1438.5380527292555
new min fval:  -1438.5641084204015
new min fval:  -1438.5851334727233
new min fval:  -1438.6003871769863
new min fval:  -1438.6133782040001
new min fval:  -1438.628914547154
new min fval:  -1438.6464651436906
new min fval:  -1438.667563277902
new min fval:  -1438.6847371403446
new min fval:  -1438.697562632647
new min fval:  -1438.7035635321042
new min fval:  -1438.7044367731175
new min fval:  -1438.706204109386
new min fval:  -1438.7178467879764
new min fval:  -1438.734043148235
new min fval:  -1438.7509641784122
new min fval:  -1438.7692651875254
new min fval:  -1438.7907264316636
new min fval:  -1438.8132706670472
new min fval:  -1438.836233564084
new min fval:  -1438.8595637499195
new min fval:  -1438.8810441342819
new min fval:  -1438.9034801916278
new min fval:  -1438.9278831625363
new min fval:  -1438.9531095337707
new min fval:  -1438.973869584758
new min fval:  -1438.9899910190925
new min fval:  -1438.9984937392703
new min fval:  -1439.0023679219616
new min fval:  -1439.011923096403
new min fval:  -1439.021306783964
new min fval:  -1439.0255914209122
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.773211]
objective value function right now is: -1424.4939015638247
new min fval:  -1439.0490237762626
new min fval:  -1439.0778826612575
new min fval:  -1439.1090481157644
new min fval:  -1439.1414595150468
new min fval:  -1439.1709704790553
new min fval:  -1439.1965662144885
new min fval:  -1439.221005369896
new min fval:  -1439.2447396262353
new min fval:  -1439.268390402764
new min fval:  -1439.2917262767642
new min fval:  -1439.3173883921409
new min fval:  -1439.3452138053574
new min fval:  -1439.3744719919055
new min fval:  -1439.4038373300498
new min fval:  -1439.4325755225739
new min fval:  -1439.4572023147982
new min fval:  -1439.4768539849683
new min fval:  -1439.4905778650148
new min fval:  -1439.4994658447708
new min fval:  -1439.5016774646658
new min fval:  -1439.5033493118342
new min fval:  -1439.5047037563227
new min fval:  -1439.505447207954
new min fval:  -1439.5055372748077
new min fval:  -1439.5058765701335
new min fval:  -1439.5074268340013
new min fval:  -1439.5121140716658
new min fval:  -1439.519187670584
new min fval:  -1439.526815593794
new min fval:  -1439.5316728197174
new min fval:  -1439.5348035587292
new min fval:  -1439.5371839685888
new min fval:  -1439.5404913658529
new min fval:  -1439.5449427349731
new min fval:  -1439.5526421403829
new min fval:  -1439.5611967996135
new min fval:  -1439.5693858604134
new min fval:  -1439.57822311818
new min fval:  -1439.5869766699002
new min fval:  -1439.5950733342956
new min fval:  -1439.6045452655221
new min fval:  -1439.6145590828955
new min fval:  -1439.625386352935
new min fval:  -1439.636721181186
new min fval:  -1439.6471945245537
new min fval:  -1439.6554090981647
new min fval:  -1439.6570059591588
new min fval:  -1439.6575500707
new min fval:  -1439.6582563335253
new min fval:  -1439.6609763031015
new min fval:  -1439.6655297187417
new min fval:  -1439.6708692952573
new min fval:  -1439.6771198486979
new min fval:  -1439.6833129613235
new min fval:  -1439.6882960530636
new min fval:  -1439.6909791742676
new min fval:  -1439.6934310895513
new min fval:  -1439.694561257139
new min fval:  -1439.7029629932683
new min fval:  -1439.7201008539614
new min fval:  -1439.741103441008
new min fval:  -1439.7643344393125
new min fval:  -1439.7890739294828
new min fval:  -1439.814306322434
new min fval:  -1439.8389228452297
new min fval:  -1439.862834709351
new min fval:  -1439.8852856336307
new min fval:  -1439.9052509512087
new min fval:  -1439.922621642048
new min fval:  -1439.9372688973979
new min fval:  -1439.9509881969118
new min fval:  -1439.9624525716933
new min fval:  -1439.9722041891496
new min fval:  -1439.981731078775
new min fval:  -1439.9915895714885
new min fval:  -1440.0027025650325
new min fval:  -1440.0149996674825
new min fval:  -1440.0284223861472
new min fval:  -1440.041028740002
new min fval:  -1440.0514810891298
new min fval:  -1440.0583331255466
new min fval:  -1440.0631735770392
new min fval:  -1440.0649182295854
new min fval:  -1440.0657662693882
new min fval:  -1440.0668637203194
new min fval:  -1440.069196696539
new min fval:  -1440.0719249105161
new min fval:  -1440.0743020122388
new min fval:  -1440.0769409910479
new min fval:  -1440.07905636499
new min fval:  -1440.0794779190292
new min fval:  -1440.0837677265358
new min fval:  -1440.093499615206
new min fval:  -1440.106720312792
new min fval:  -1440.12077576348
new min fval:  -1440.1330967725355
new min fval:  -1440.1432066763984
new min fval:  -1440.1515288300209
new min fval:  -1440.160024535182
new min fval:  -1440.16820400446
new min fval:  -1440.1772521189866
new min fval:  -1440.185707694957
new min fval:  -1440.194302364878
new min fval:  -1440.2019933442655
new min fval:  -1440.2091619608002
new min fval:  -1440.2162489476227
new min fval:  -1440.224846074898
new min fval:  -1440.2336578650834
new min fval:  -1440.2406906140222
new min fval:  -1440.246066722795
new min fval:  -1440.2502346747028
new min fval:  -1440.2540190622715
new min fval:  -1440.2583535116767
new min fval:  -1440.2635449302763
new min fval:  -1440.269424656278
new min fval:  -1440.2770987727922
new min fval:  -1440.2838930780438
new min fval:  -1440.289320187248
new min fval:  -1440.292950235439
new min fval:  -1440.2949129709114
new min fval:  -1440.2963190813095
new min fval:  -1440.2983181096397
new min fval:  -1440.3009561754359
new min fval:  -1440.3062836552783
new min fval:  -1440.3125354015465
new min fval:  -1440.3211616746883
new min fval:  -1440.3310508410846
new min fval:  -1440.3415674175565
new min fval:  -1440.3519193167585
new min fval:  -1440.3605210613352
new min fval:  -1440.3677114934571
new min fval:  -1440.3726165436283
new min fval:  -1440.3764267972206
new min fval:  -1440.3792661277862
new min fval:  -1440.381754590768
new min fval:  -1440.3850059096123
new min fval:  -1440.390705687186
new min fval:  -1440.399293682268
new min fval:  -1440.410294246465
new min fval:  -1440.4227930375725
new min fval:  -1440.43744387045
new min fval:  -1440.452292137321
new min fval:  -1440.4662025088096
new min fval:  -1440.47812556422
new min fval:  -1440.4875478345934
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.150777]
objective value function right now is: -1438.7141184497768
new min fval:  -1440.4950500945304
new min fval:  -1440.5014261316755
new min fval:  -1440.507266193573
new min fval:  -1440.511872786163
new min fval:  -1440.5157409889673
new min fval:  -1440.520581167299
new min fval:  -1440.5265633366366
new min fval:  -1440.533134903642
new min fval:  -1440.5386504054661
new min fval:  -1440.5441525751917
new min fval:  -1440.5497019134407
new min fval:  -1440.5551698940703
new min fval:  -1440.561744214954
new min fval:  -1440.5692976274422
new min fval:  -1440.576118733309
new min fval:  -1440.58216040202
new min fval:  -1440.5874747876403
new min fval:  -1440.5927235045008
new min fval:  -1440.5987338958128
new min fval:  -1440.6045937320453
new min fval:  -1440.610970445327
new min fval:  -1440.6170684272686
new min fval:  -1440.6226002108638
new min fval:  -1440.629587784517
new min fval:  -1440.6358089079415
new min fval:  -1440.6416711963902
new min fval:  -1440.6465683129838
new min fval:  -1440.6505744644169
new min fval:  -1440.6551211474998
new min fval:  -1440.6592685493054
new min fval:  -1440.6641177053768
new min fval:  -1440.6691731904727
new min fval:  -1440.6752347835163
new min fval:  -1440.6809840349313
new min fval:  -1440.6826798382992
new min fval:  -1440.683439800228
new min fval:  -1440.6869663804182
new min fval:  -1440.6914812144316
new min fval:  -1440.6944737865263
new min fval:  -1440.6964467011169
new min fval:  -1440.6969770926637
new min fval:  -1440.6976576623863
new min fval:  -1440.699882691084
new min fval:  -1440.7053581089694
new min fval:  -1440.7128547030588
new min fval:  -1440.7224049769175
new min fval:  -1440.7316677794508
new min fval:  -1440.7408323147092
new min fval:  -1440.7499289973111
new min fval:  -1440.7587952906977
new min fval:  -1440.767737187899
new min fval:  -1440.7765162889748
new min fval:  -1440.7842251465804
new min fval:  -1440.7931544674818
new min fval:  -1440.802597770433
new min fval:  -1440.8120447269134
new min fval:  -1440.8208254028261
new min fval:  -1440.8298196465817
new min fval:  -1440.838494731103
new min fval:  -1440.8474384950641
new min fval:  -1440.856126155339
new min fval:  -1440.8648598868
new min fval:  -1440.8734858995106
new min fval:  -1440.8815270430412
new min fval:  -1440.8885141274486
new min fval:  -1440.8961059044168
new min fval:  -1440.9041652641088
new min fval:  -1440.9134213546038
new min fval:  -1440.9225552725072
new min fval:  -1440.9301454291985
new min fval:  -1440.9356027687863
new min fval:  -1440.938753410115
new min fval:  -1440.93966443855
new min fval:  -1440.9404968769531
new min fval:  -1440.9425772773916
new min fval:  -1440.9446115660078
new min fval:  -1440.9470167285394
new min fval:  -1440.9494210037763
new min fval:  -1440.9526350671435
new min fval:  -1440.9559144358332
new min fval:  -1440.9593651738892
new min fval:  -1440.963259212354
new min fval:  -1440.9676289128056
new min fval:  -1440.9709237731001
new min fval:  -1440.9730613362503
new min fval:  -1440.9745508417184
new min fval:  -1440.9753881212189
new min fval:  -1440.9771152912938
new min fval:  -1440.9792907167416
new min fval:  -1440.980374272963
new min fval:  -1440.98145287625
new min fval:  -1440.9827361026476
new min fval:  -1440.9835450602225
new min fval:  -1440.9851758702523
new min fval:  -1440.9858888078272
new min fval:  -1440.9873354905205
new min fval:  -1440.988822274672
new min fval:  -1440.9896445745328
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.258727]
objective value function right now is: -1430.3950711355724
new min fval:  -1440.9927224681564
new min fval:  -1440.9994112658057
new min fval:  -1441.006764220734
new min fval:  -1441.0146657974678
new min fval:  -1441.0226608399366
new min fval:  -1441.0295976686862
new min fval:  -1441.0355049314246
new min fval:  -1441.0407441566797
new min fval:  -1441.0456877832282
new min fval:  -1441.0506703726483
new min fval:  -1441.0557046842791
new min fval:  -1441.0600297024787
new min fval:  -1441.0641203955477
new min fval:  -1441.0678201008895
new min fval:  -1441.0709570998904
new min fval:  -1441.0745164014809
new min fval:  -1441.0783381849635
new min fval:  -1441.0828455301175
new min fval:  -1441.0877762070759
new min fval:  -1441.0932591634278
new min fval:  -1441.098399145297
new min fval:  -1441.1035299076948
new min fval:  -1441.1080367258155
new min fval:  -1441.112572014035
new min fval:  -1441.116904494673
new min fval:  -1441.1199077700255
new min fval:  -1441.1225700468178
new min fval:  -1441.1251189796124
new min fval:  -1441.1275643022298
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.6264925]
objective value function right now is: -1429.0978595773975
new min fval:  -1441.1293203362204
new min fval:  -1441.1313058277326
new min fval:  -1441.1337955623694
new min fval:  -1441.1357122720276
new min fval:  -1441.1378499446043
new min fval:  -1441.1396005035783
new min fval:  -1441.1418979557063
new min fval:  -1441.145618413785
new min fval:  -1441.1501300880504
new min fval:  -1441.1554028568823
new min fval:  -1441.1613714551336
new min fval:  -1441.167194144084
new min fval:  -1441.1719714123578
new min fval:  -1441.1766418066845
new min fval:  -1441.180299116029
new min fval:  -1441.1842420151881
new min fval:  -1441.1868557270277
new min fval:  -1441.1898774457757
new min fval:  -1441.1931109020472
new min fval:  -1441.1962259978527
new min fval:  -1441.1990955352658
new min fval:  -1441.2021777929729
new min fval:  -1441.2044732668987
new min fval:  -1441.206526648857
new min fval:  -1441.2085183391648
new min fval:  -1441.2101487626453
new min fval:  -1441.2120203710574
new min fval:  -1441.2138350871512
new min fval:  -1441.2162396130266
new min fval:  -1441.2197679777253
new min fval:  -1441.223542660457
new min fval:  -1441.227568050231
new min fval:  -1441.2322287623965
new min fval:  -1441.2371354692748
new min fval:  -1441.2415614376764
new min fval:  -1441.2449985105764
new min fval:  -1441.2481969943242
new min fval:  -1441.2513840844329
new min fval:  -1441.2544321057521
new min fval:  -1441.2577613383985
new min fval:  -1441.261009387571
new min fval:  -1441.2646923793234
new min fval:  -1441.2680766483038
new min fval:  -1441.2716220863833
new min fval:  -1441.2748265757098
new min fval:  -1441.2778563660631
new min fval:  -1441.2809740218424
new min fval:  -1441.283340210965
new min fval:  -1441.2864832607556
new min fval:  -1441.2893646737655
new min fval:  -1441.2934949154421
new min fval:  -1441.29766547223
new min fval:  -1441.3026551865141
new min fval:  -1441.3065593289357
new min fval:  -1441.3091958694617
new min fval:  -1441.3102894122483
new min fval:  -1441.31082165043
new min fval:  -1441.3118609702003
new min fval:  -1441.3142294785816
new min fval:  -1441.3184155509737
new min fval:  -1441.3232396285753
new min fval:  -1441.328882967423
new min fval:  -1441.3339969634428
new min fval:  -1441.3390362305227
new min fval:  -1441.3436494738125
new min fval:  -1441.3477845665723
new min fval:  -1441.351763978703
new min fval:  -1441.3552897704128
new min fval:  -1441.3589217082585
new min fval:  -1441.3628567180463
new min fval:  -1441.3662653655324
new min fval:  -1441.368504971246
new min fval:  -1441.3705897218651
new min fval:  -1441.371810336821
new min fval:  -1441.3722984373658
new min fval:  -1441.3723962435076
new min fval:  -1441.3734765062104
new min fval:  -1441.373774384699
new min fval:  -1441.375147043696
new min fval:  -1441.3771030510359
new min fval:  -1441.3777698357642
new min fval:  -1441.378558382995
new min fval:  -1441.3787552205906
new min fval:  -1441.3802236485603
new min fval:  -1441.3820904725976
new min fval:  -1441.3848700773663
new min fval:  -1441.387954313971
new min fval:  -1441.3910884350864
new min fval:  -1441.3942238900274
new min fval:  -1441.396487854045
new min fval:  -1441.3981082354
new min fval:  -1441.398944368023
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.471555]
objective value function right now is: -1433.306430232388
min fval:  -1441.398944368023
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 874.5532538307029
W_T_median: 662.9378950845289
W_T_pctile_5: 206.94574634488708
W_T_CVAR_5_pct: 41.39256980416537
Average q (qsum/M+1):  42.50146484375
Optimal xi:  [14.212213]
Expected(across Rb) median(across samples) p_equity:  0.2314179728428523
obj fun:  tensor(-1441.3989, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
