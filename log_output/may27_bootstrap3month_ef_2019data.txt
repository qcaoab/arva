Starting at: 
27-05-23_17:49

 numpy seed:  3  


 pytorch seed:  3  

Key parameters-------
paths: 256000
iterations: 50000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
               CPI  CPI_nom_ret  ...  VWD_real_ret  EWD_real_ret
date                             ...                            
192512  100.000000          NaN  ...           NaN           NaN
192601  100.000000     0.000000  ...      0.000561      0.023174
192602  100.000000     0.000000  ...     -0.033046     -0.053510
192603   99.441303    -0.005587  ...     -0.058743     -0.091750
192604  100.000000     0.005618  ...      0.031235      0.027204

[5 rows x 12 columns]
                CPI  CPI_nom_ret  ...  VWD_real_ret  EWD_real_ret
date                              ...                            
202008  1452.055956     0.003153  ...      0.065084      0.035618
202009  1454.078149     0.001393  ...     -0.036399     -0.028709
202010  1454.681696     0.000415  ...     -0.020584      0.000169
202011  1453.793232    -0.000611  ...      0.124393      0.175130
202012  1455.162018     0.000942  ...      0.044065      0.071843

[5 rows x 12 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_nom_ret', 'VWD_nom_ret']
############# End: defined asset  basket #################
        CPI_nom_ret  B10_nom_ret  VWD_nom_ret
date                                         
192601     0.000000     0.004350     0.000561
192602     0.000000     0.004338    -0.033046
192603    -0.005587     0.004327    -0.064002
192604     0.005618     0.004316     0.037029
192605    -0.005587     0.004304     0.012095
        CPI_nom_ret  B10_nom_ret  VWD_nom_ret
date                                         
202008     0.003153    -0.014709     0.068443
202009     0.001393     0.002560    -0.035057
202010     0.000415    -0.017311    -0.020178
202011    -0.000611     0.004016     0.123706
202012     0.000942    -0.004965     0.045048
Indices constructed with column names:
Index(['date_for_plt', 'CPI_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
B10_real_ret    0.001986
VWD_real_ret    0.006974
dtype: float64


timeseries_basket['data_df_stdev'] = 
B10_real_ret    0.018939
VWD_real_ret    0.053569
dtype: float64


timeseries_basket['data_df_corr'] = 
              B10_real_ret  VWD_real_ret
B10_real_ret      1.000000      0.075353
VWD_real_ret      0.075353      1.000000


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192601
End: 202012
-----------------------------------------------
Bootstrap block size: 3
-----------------------------------------------
Dates USED bootstrapping:
Start: 192601
End: 201912
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 5000, 'itbound_SGD_algorithms': 50000, 'nit_IterateAveragingStart': 45000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1439.2724125859495
W_T_median: 1086.6311344566764
W_T_pctile_5: -114.4532091189134
W_T_CVAR_5_pct: -274.67719995288854
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1694.902628251822
Current xi:  [65.924934]
objective value function right now is: -1694.902628251822
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1703.26602139515
Current xi:  [28.635141]
objective value function right now is: -1703.26602139515
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1710.9824908416856
Current xi:  [-7.2222967]
objective value function right now is: -1710.9824908416856
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1716.3705155628697
Current xi:  [-41.095196]
objective value function right now is: -1716.3705155628697
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1720.827746607608
Current xi:  [-74.983955]
objective value function right now is: -1720.827746607608
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1724.821787856665
Current xi:  [-108.85536]
objective value function right now is: -1724.821787856665
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1728.5443175738587
Current xi:  [-142.5098]
objective value function right now is: -1728.5443175738587
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1731.6026463012556
Current xi:  [-176.0073]
objective value function right now is: -1731.6026463012556
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1733.985059914604
Current xi:  [-209.32776]
objective value function right now is: -1733.985059914604
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1736.2921241179276
Current xi:  [-242.06459]
objective value function right now is: -1736.2921241179276
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1737.791760823402
Current xi:  [-274.24225]
objective value function right now is: -1737.791760823402
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1739.6289945478375
Current xi:  [-305.36823]
objective value function right now is: -1739.6289945478375
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-335.40442]
objective value function right now is: -1739.392107957192
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1741.2399363509762
Current xi:  [-364.6274]
objective value function right now is: -1741.2399363509762
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.8576971299167
Current xi:  [-389.8863]
objective value function right now is: -1741.8576971299167
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-413.75397]
objective value function right now is: -1741.7951101895562
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1742.585541242172
Current xi:  [-434.2562]
objective value function right now is: -1742.585541242172
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-450.72937]
objective value function right now is: -1741.872136559376
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1742.853055198529
Current xi:  [-464.19357]
objective value function right now is: -1742.853055198529
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1742.8641728044756
Current xi:  [-470.22476]
objective value function right now is: -1742.8641728044756
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-472.81726]
objective value function right now is: -1742.8140926316155
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.1815]
objective value function right now is: -1741.884302447121
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-475.97778]
objective value function right now is: -1742.8613113231445
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-484.10764]
objective value function right now is: -1733.8263113315902
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-493.68808]
objective value function right now is: -1734.018265445009
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-499.66318]
objective value function right now is: -1734.1628105525908
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-502.87772]
objective value function right now is: -1734.1401008714383
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-502.4548]
objective value function right now is: -1733.7877586473503
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-503.64383]
objective value function right now is: -1734.2020755140982
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-502.7708]
objective value function right now is: -1733.9663649733727
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-501.58124]
objective value function right now is: -1734.2528043538566
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-504.16138]
objective value function right now is: -1733.9099979959435
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-504.14624]
objective value function right now is: -1734.1185549570826
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-501.6794]
objective value function right now is: -1734.297982712282
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-504.19315]
objective value function right now is: -1734.082606022112
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-504.38025]
objective value function right now is: -1734.4769246037426
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-503.9571]
objective value function right now is: -1734.4667829273885
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-503.43872]
objective value function right now is: -1734.4190167916631
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-503.67242]
objective value function right now is: -1734.439914672174
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-503.15054]
objective value function right now is: -1734.4392221969463
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-503.20724]
objective value function right now is: -1734.4051837337897
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-503.55292]
objective value function right now is: -1734.5157872047755
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-503.348]
objective value function right now is: -1734.4762901606246
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-503.0827]
objective value function right now is: -1734.5268822153544
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-503.19113]
objective value function right now is: -1734.442901269691
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-502.63904]
objective value function right now is: -1734.4512093616925
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-503.0444]
objective value function right now is: -1734.5108053737279
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-502.1713]
objective value function right now is: -1734.4838052760233
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-502.26657]
objective value function right now is: -1734.534706806265
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-502.22137]
objective value function right now is: -1734.5306592132054
min fval:  -1742.6976105621202
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.5617,  1.7671],
        [-0.5775,  1.7756],
        [-2.4710,  5.9977],
        [12.9912,  1.6838],
        [-1.3795,  3.9679],
        [-2.4951,  5.7569],
        [-6.7090,  3.9122],
        [-0.5613,  1.7665],
        [-2.6693,  6.1097],
        [-0.5653,  1.7210]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-0.9975, -1.0068,  9.8773, -7.8028,  3.3290,  8.9841,  7.9236, -0.9971,
        10.4591, -0.9857], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 3.0119e-02,  2.4798e-02, -1.7249e-01, -3.2871e-01, -8.4524e-03,
         -1.0834e-01, -4.4549e-02,  3.0014e-02, -2.1420e-01,  2.5730e-02],
        [ 3.0119e-02,  2.4798e-02, -1.7249e-01, -3.2871e-01, -8.4524e-03,
         -1.0834e-01, -4.4549e-02,  3.0014e-02, -2.1420e-01,  2.5730e-02],
        [-3.4550e-02, -6.6303e-02, -6.6986e+00, -9.5870e+00, -1.2072e+00,
         -5.2025e+00, -4.4059e+00, -3.4031e-02, -7.6779e+00, -3.7301e-02],
        [ 1.6229e-02,  2.6595e-02,  2.0733e+00,  4.4657e+00,  1.8555e-01,
          1.3973e+00,  1.2239e+00,  1.6159e-02,  2.6237e+00,  2.7598e-02],
        [ 3.0119e-02,  2.4798e-02, -1.7249e-01, -3.2871e-01, -8.4524e-03,
         -1.0834e-01, -4.4549e-02,  3.0014e-02, -2.1420e-01,  2.5730e-02],
        [ 3.0119e-02,  2.4798e-02, -1.7249e-01, -3.2871e-01, -8.4524e-03,
         -1.0834e-01, -4.4549e-02,  3.0014e-02, -2.1420e-01,  2.5730e-02],
        [ 3.0894e-02,  4.4798e-02,  3.2173e+00,  5.1939e+00,  3.4282e-01,
          2.3282e+00,  2.2169e+00,  3.0920e-02,  3.9216e+00,  6.0602e-02],
        [ 4.0366e-02,  5.4379e-02,  3.9384e+00,  5.9008e+00,  4.7366e-01,
          2.9152e+00,  2.7346e+00,  4.0452e-02,  4.7487e+00,  8.1186e-02],
        [ 3.0119e-02,  2.4798e-02, -1.7249e-01, -3.2871e-01, -8.4524e-03,
         -1.0834e-01, -4.4549e-02,  3.0014e-02, -2.1420e-01,  2.5730e-02],
        [ 2.2806e-02,  2.5030e-02, -1.7275e-01, -3.2990e-01, -1.0514e-02,
         -1.0864e-01, -4.4866e-02,  2.9501e-02, -2.1447e-01, -2.3663e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.5948, -1.5948,  5.0924, -3.1258, -1.5948, -1.5948, -3.0655, -3.2801,
        -1.5948, -1.5973], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0565,  -0.0565, -13.6937,   3.7647,  -0.0565,  -0.0565,   5.0105,
           6.3726,  -0.0565,  -0.0564]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  8.0304,   2.8892],
        [ -3.8788, -10.0199],
        [ -1.4494,  -0.8182],
        [-15.5368,  -1.8157],
        [-14.9599,  -1.4921],
        [ -7.1002,   0.7678],
        [ -4.5311,  -6.9696],
        [ -9.7366,  -8.8746],
        [  0.7730,  -5.7221],
        [ 10.5007,   3.5761]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -3.4100, -12.6924,  -6.1159,   3.3756,   0.6089,   4.3345,  -4.5487,
         -6.6006, -11.5009,  -1.0802], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -0.9482,   0.5190,  -0.8485,  -1.6185,  -4.6641,  -0.2067,   0.8558,
          -0.8862,  -0.0322,   1.3892],
        [ -1.2496,  -0.4177,  -0.4702,  -0.7591,  -0.5306,  -0.7715,  -0.9017,
          -0.5198,  -0.4627,  -2.1559],
        [ -5.2879,  -1.3670,   2.1139,  -5.7780,  -0.5325,  -1.9552,   1.9956,
           1.8522,  -2.9532,  -2.2649],
        [ -4.9239,  -8.8275,   4.8898,  -7.5059,  -3.9290,  -4.8633,   6.3644,
          10.5595,  -7.1728,  -6.8163],
        [ -1.2445,  -0.4074,  -0.4599,  -0.7513,  -0.5229,  -0.7728,  -0.8887,
          -0.5122,  -0.4530,  -2.1431],
        [ -0.4648,   9.4653, -10.2793,   4.3563,   0.0703,   4.1850,  -4.2320,
           0.2734,  -0.1410,  -9.6471],
        [ -2.7861,  -1.2361,   0.3685,  -2.7041,  -1.2860,  -2.2169,   1.1705,
           1.5889,  -2.1377,  -2.5434],
        [ -1.2429,  -0.4041,  -0.4556,  -0.7464,  -0.5195,  -0.7730,  -0.8865,
          -0.5093,  -0.4496,  -2.1380],
        [ -1.2428,  -0.4040,  -0.4551,  -0.7455,  -0.5190,  -0.7730,  -0.8866,
          -0.5092,  -0.4492,  -2.1374],
        [  2.2350,  -2.0344,  -0.2626,   1.4174,   7.4433,   8.4738,  -6.9391,
          -6.2205,  -0.7340,  -1.0320]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 1.9093, -3.0908, -0.8713, -0.2740, -3.0887, -7.6301, -2.1789, -3.0894,
        -3.0897, -2.7640], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.4421,   0.0717,  -1.8093, -10.1434,   0.0676,   7.8503,  -0.9233,
           0.0666,   0.0678,   0.8170],
        [  0.2084,  -0.0715,   1.8727,  10.1416,  -0.0677,  -7.8431,   0.9588,
          -0.0663,  -0.0649,  -0.9635]], device='cuda:0'))])
xi:  [-503.19113]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 297.7085282377334
W_T_median: 135.78672676253382
W_T_pctile_5: -476.19342195720736
W_T_CVAR_5_pct: -570.3991188925148
Average q (qsum/M+1):  57.14171969506048
Optimal xi:  [-503.19113]
Expected(across Rb) median(across samples) p_equity:  0.33017126539101205
obj fun:  tensor(-1742.6976, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1439.2724125859495
W_T_median: 1086.6311344566764
W_T_pctile_5: -114.4532091189134
W_T_CVAR_5_pct: -274.67719995288854
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1631.2090683783176
Current xi:  [63.570843]
objective value function right now is: -1631.2090683783176
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1645.6805561042643
Current xi:  [27.733374]
objective value function right now is: -1645.6805561042643
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1656.537405247839
Current xi:  [-5.7805815]
objective value function right now is: -1656.537405247839
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1662.7295408409955
Current xi:  [-32.94615]
objective value function right now is: -1662.7295408409955
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1666.1107371429414
Current xi:  [-54.881268]
objective value function right now is: -1666.1107371429414
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1669.948654376293
Current xi:  [-80.81963]
objective value function right now is: -1669.948654376293
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-111.70215]
objective value function right now is: -1657.4912390357506
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-135.27267]
objective value function right now is: -1661.6957415915836
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-161.22755]
objective value function right now is: -1664.793031540749
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1675.5342908781618
Current xi:  [-176.68213]
objective value function right now is: -1675.5342908781618
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.1284396950798
Current xi:  [-197.67346]
objective value function right now is: -1678.1284396950798
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-207.50882]
objective value function right now is: -1676.6327210983272
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.1995105869953
Current xi:  [-212.09799]
objective value function right now is: -1678.1995105869953
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-214.8069]
objective value function right now is: -1678.1871086272795
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.6009806061713
Current xi:  [-220.14117]
objective value function right now is: -1678.6009806061713
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-227.9163]
objective value function right now is: -1678.1483084642016
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-237.45494]
objective value function right now is: -1677.7683086612367
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.770311511294
Current xi:  [-239.86087]
objective value function right now is: -1678.770311511294
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.10979]
objective value function right now is: -1678.4223065681367
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-240.7493]
objective value function right now is: -1677.6787038232142
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-249.49744]
objective value function right now is: -1665.8443047313608
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-254.7594]
objective value function right now is: -1664.8693019602604
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-254.37358]
objective value function right now is: -1665.9783466155463
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-253.79306]
objective value function right now is: -1666.1121169026928
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-253.94865]
objective value function right now is: -1666.1718805796938
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-253.74344]
objective value function right now is: -1665.8933189650695
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-253.29419]
objective value function right now is: -1666.2339578680378
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-253.38597]
objective value function right now is: -1665.784372355402
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-252.88042]
objective value function right now is: -1666.268495050393
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-253.3646]
objective value function right now is: -1666.086109032413
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-252.71774]
objective value function right now is: -1666.200944372336
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-252.85793]
objective value function right now is: -1666.0925517760822
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-252.75012]
objective value function right now is: -1666.279607850042
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-253.38046]
objective value function right now is: -1665.8964218650126
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-253.40297]
objective value function right now is: -1666.0946971382607
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-253.44513]
objective value function right now is: -1666.5870985639249
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-253.24388]
objective value function right now is: -1666.5116394365843
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-253.10295]
objective value function right now is: -1666.421381612506
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-252.87206]
objective value function right now is: -1666.5594165988634
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-252.68631]
objective value function right now is: -1666.617679313147
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-252.57413]
objective value function right now is: -1666.5401317525618
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-252.53856]
objective value function right now is: -1666.5787638769873
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-252.42372]
objective value function right now is: -1666.5788749915375
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-252.57379]
objective value function right now is: -1666.503979407379
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-252.27637]
objective value function right now is: -1666.561461164486
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-252.18735]
objective value function right now is: -1666.5867333899027
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-251.83537]
objective value function right now is: -1666.5157808972708
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-252.17026]
objective value function right now is: -1666.5697390051125
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-252.44595]
objective value function right now is: -1666.65547878896
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-252.53557]
objective value function right now is: -1666.6503174280608
min fval:  -1678.7377701987762
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.7206, -4.5816],
        [-1.7592, -4.7563],
        [-1.3490, -3.2032],
        [ 1.7147,  5.7886],
        [-3.8491,  5.1388],
        [-5.4251,  4.3418],
        [-1.9807, -5.2780],
        [-2.0293, -5.2453],
        [-1.0476, -1.7315],
        [-4.5809,  4.7649]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-5.9093, -5.9175, -6.0040,  5.4616,  6.0442,  6.1683, -5.9977, -5.9111,
        -5.3881,  6.0145], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.4609, -0.5188, -0.1337, -0.5043, -0.2475, -0.3732, -0.6750, -0.6847,
         -0.0539, -0.2896],
        [ 2.0889,  2.2268,  0.9646, -4.0938, -4.0910, -4.2471,  3.1948,  3.2539,
          0.3566, -4.0375],
        [-2.4207, -3.0567, -1.4723,  4.6348,  5.6440,  5.6527, -4.0062, -3.6407,
         -0.6229,  5.4559],
        [-2.2528, -2.2279, -0.9142,  3.4500,  4.3434,  4.6966, -3.5131, -3.3128,
         -0.2712,  4.3506],
        [ 2.1044,  2.1736,  0.8382, -3.9962, -3.9872, -4.0908,  2.8996,  3.1492,
          0.3006, -3.9545],
        [ 0.6645,  0.6265,  0.0448, -2.5928, -1.8847, -2.3295,  1.4045,  1.4851,
         -0.0824, -1.9651],
        [-1.3442, -1.6788, -0.3763,  0.8442,  1.2793,  2.0775, -2.4676, -2.5307,
         -0.0907,  1.5307],
        [ 1.6772,  1.6129,  0.5460, -3.5106, -3.3053, -3.4749,  2.4661,  2.7536,
          0.1296, -3.2671],
        [-1.9556, -2.2533, -0.7177,  2.8300,  3.7304,  4.1727, -3.2345, -3.2037,
         -0.2375,  3.7309],
        [-0.6526, -0.7734, -0.1381, -0.4534, -0.1187, -0.0910, -1.2425, -1.2507,
         -0.0450, -0.1091]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.5839, -1.3007,  0.5721,  0.5765, -1.1900, -1.7642, -0.0816, -1.2918,
         0.5955, -0.9075], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 0.0179, -4.9806,  6.8240,  4.5529, -4.6516, -1.7828,  1.6248, -3.4551,
          3.6540,  0.2138]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -5.6203,   6.7713],
        [  9.3170,  -0.5736],
        [  3.5146,   7.5289],
        [ -0.9611,  11.1787],
        [ -2.4566,   0.8585],
        [ 12.5786,   4.4666],
        [ -7.8544, -10.7738],
        [ -9.3078,  -1.5682],
        [ 10.5246,   1.7448],
        [  2.5657,  -0.6703]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  5.3477,  -9.3063,   9.2222,  10.5159,  -3.6060,   1.5339, -12.3890,
          0.8048,  -2.7802,  -9.0642], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.2443e-02, -1.0303e+00, -1.2103e+00, -3.0751e-01, -1.4835e-02,
         -1.5098e+00, -1.1969e+00, -9.5688e-01, -1.2115e+00, -8.8826e-01],
        [-8.9963e-01, -9.8320e+00, -7.5909e+00, -4.3827e+00,  1.1766e-01,
         -5.8268e+00,  8.4660e+00,  9.7204e+00, -5.1259e+00,  2.6312e-01],
        [-2.0458e-02, -1.0285e+00, -1.2057e+00, -3.0699e-01, -1.0540e-02,
         -1.5045e+00, -1.1995e+00, -9.4953e-01, -1.2117e+00, -8.7292e-01],
        [-6.8210e+00, -9.5249e+00, -2.4810e+00, -1.2514e+01, -1.3245e-01,
         -1.6308e+01,  5.5618e+00,  6.8111e+00, -7.5959e+00, -9.9383e+00],
        [ 4.4558e-01,  9.3053e-03, -3.0439e+00,  6.7503e+00,  6.5643e-02,
         -4.5564e+00,  4.9999e+00,  4.2628e+00, -7.4349e+00,  2.3524e+00],
        [-7.1876e+00, -8.4804e-01,  2.2043e-01, -1.5816e+00, -2.8002e-01,
         -5.1658e-01, -1.6811e-01, -1.9760e+00, -6.4226e-01, -2.8940e+00],
        [-7.0790e+00, -3.0708e+00,  1.8653e+00, -1.2185e+01,  1.1262e-01,
         -1.9453e+00, -1.1687e+01,  4.0702e+00, -3.4092e+00,  4.2379e-01],
        [ 1.5558e+00, -8.2451e+00,  1.4978e+00,  6.1811e+00,  4.7201e-01,
         -2.4404e+00,  1.4108e+00, -3.7250e-01, -3.1026e+00, -3.7410e-02],
        [-2.0427e-02, -1.0285e+00, -1.2058e+00, -3.0699e-01, -1.0549e-02,
         -1.5045e+00, -1.1995e+00, -9.4953e-01, -1.2117e+00, -8.7297e-01],
        [-2.0429e-02, -1.0285e+00, -1.2057e+00, -3.0697e-01, -1.0525e-02,
         -1.5045e+00, -1.1995e+00, -9.4950e-01, -1.2117e+00, -8.7291e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.7118, -0.3967, -2.7048, -1.4941, -4.0466, -0.7796,  0.0787, -2.5905,
        -2.7048, -2.7047], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.0225,  -5.8364,  -0.0222,  13.1696,   5.7976,  -1.4774,  -7.5406,
           0.7129,  -0.0221,  -0.0221],
        [  0.0231,   5.7754,   0.0222, -13.1648,  -5.8200,   1.7148,   7.5573,
          -0.9039,   0.0223,   0.0223]], device='cuda:0'))])
xi:  [-252.27637]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 270.0017198179415
W_T_median: 87.20625427377597
W_T_pctile_5: -245.2360814349185
W_T_CVAR_5_pct: -323.07707031556095
Average q (qsum/M+1):  56.240624212449596
Optimal xi:  [-252.27637]
Expected(across Rb) median(across samples) p_equity:  0.3147431176053942
obj fun:  tensor(-1678.7378, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.7206, -4.5816],
        [-1.7592, -4.7563],
        [-1.3490, -3.2032],
        [ 1.7147,  5.7886],
        [-3.8491,  5.1388],
        [-5.4251,  4.3418],
        [-1.9807, -5.2780],
        [-2.0293, -5.2453],
        [-1.0476, -1.7315],
        [-4.5809,  4.7649]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-5.9093, -5.9175, -6.0040,  5.4616,  6.0442,  6.1683, -5.9977, -5.9111,
        -5.3881,  6.0145], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.4609, -0.5188, -0.1337, -0.5043, -0.2475, -0.3732, -0.6750, -0.6847,
         -0.0539, -0.2896],
        [ 2.0889,  2.2268,  0.9646, -4.0938, -4.0910, -4.2471,  3.1948,  3.2539,
          0.3566, -4.0375],
        [-2.4207, -3.0567, -1.4723,  4.6348,  5.6440,  5.6527, -4.0062, -3.6407,
         -0.6229,  5.4559],
        [-2.2528, -2.2279, -0.9142,  3.4500,  4.3434,  4.6966, -3.5131, -3.3128,
         -0.2712,  4.3506],
        [ 2.1044,  2.1736,  0.8382, -3.9962, -3.9872, -4.0908,  2.8996,  3.1492,
          0.3006, -3.9545],
        [ 0.6645,  0.6265,  0.0448, -2.5928, -1.8847, -2.3295,  1.4045,  1.4851,
         -0.0824, -1.9651],
        [-1.3442, -1.6788, -0.3763,  0.8442,  1.2793,  2.0775, -2.4676, -2.5307,
         -0.0907,  1.5307],
        [ 1.6772,  1.6129,  0.5460, -3.5106, -3.3053, -3.4749,  2.4661,  2.7536,
          0.1296, -3.2671],
        [-1.9556, -2.2533, -0.7177,  2.8300,  3.7304,  4.1727, -3.2345, -3.2037,
         -0.2375,  3.7309],
        [-0.6526, -0.7734, -0.1381, -0.4534, -0.1187, -0.0910, -1.2425, -1.2507,
         -0.0450, -0.1091]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.5839, -1.3007,  0.5721,  0.5765, -1.1900, -1.7642, -0.0816, -1.2918,
         0.5955, -0.9075], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 0.0179, -4.9806,  6.8240,  4.5529, -4.6516, -1.7828,  1.6248, -3.4551,
          3.6540,  0.2138]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -5.6203,   6.7713],
        [  9.3170,  -0.5736],
        [  3.5146,   7.5289],
        [ -0.9611,  11.1787],
        [ -2.4566,   0.8585],
        [ 12.5786,   4.4666],
        [ -7.8544, -10.7738],
        [ -9.3078,  -1.5682],
        [ 10.5246,   1.7448],
        [  2.5657,  -0.6703]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  5.3477,  -9.3063,   9.2222,  10.5159,  -3.6060,   1.5339, -12.3890,
          0.8048,  -2.7802,  -9.0642], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.2443e-02, -1.0303e+00, -1.2103e+00, -3.0751e-01, -1.4835e-02,
         -1.5098e+00, -1.1969e+00, -9.5688e-01, -1.2115e+00, -8.8826e-01],
        [-8.9963e-01, -9.8320e+00, -7.5909e+00, -4.3827e+00,  1.1766e-01,
         -5.8268e+00,  8.4660e+00,  9.7204e+00, -5.1259e+00,  2.6312e-01],
        [-2.0458e-02, -1.0285e+00, -1.2057e+00, -3.0699e-01, -1.0540e-02,
         -1.5045e+00, -1.1995e+00, -9.4953e-01, -1.2117e+00, -8.7292e-01],
        [-6.8210e+00, -9.5249e+00, -2.4810e+00, -1.2514e+01, -1.3245e-01,
         -1.6308e+01,  5.5618e+00,  6.8111e+00, -7.5959e+00, -9.9383e+00],
        [ 4.4558e-01,  9.3053e-03, -3.0439e+00,  6.7503e+00,  6.5643e-02,
         -4.5564e+00,  4.9999e+00,  4.2628e+00, -7.4349e+00,  2.3524e+00],
        [-7.1876e+00, -8.4804e-01,  2.2043e-01, -1.5816e+00, -2.8002e-01,
         -5.1658e-01, -1.6811e-01, -1.9760e+00, -6.4226e-01, -2.8940e+00],
        [-7.0790e+00, -3.0708e+00,  1.8653e+00, -1.2185e+01,  1.1262e-01,
         -1.9453e+00, -1.1687e+01,  4.0702e+00, -3.4092e+00,  4.2379e-01],
        [ 1.5558e+00, -8.2451e+00,  1.4978e+00,  6.1811e+00,  4.7201e-01,
         -2.4404e+00,  1.4108e+00, -3.7250e-01, -3.1026e+00, -3.7410e-02],
        [-2.0427e-02, -1.0285e+00, -1.2058e+00, -3.0699e-01, -1.0549e-02,
         -1.5045e+00, -1.1995e+00, -9.4953e-01, -1.2117e+00, -8.7297e-01],
        [-2.0429e-02, -1.0285e+00, -1.2057e+00, -3.0697e-01, -1.0525e-02,
         -1.5045e+00, -1.1995e+00, -9.4950e-01, -1.2117e+00, -8.7291e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.7118, -0.3967, -2.7048, -1.4941, -4.0466, -0.7796,  0.0787, -2.5905,
        -2.7048, -2.7047], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.0225,  -5.8364,  -0.0222,  13.1696,   5.7976,  -1.4774,  -7.5406,
           0.7129,  -0.0221,  -0.0221],
        [  0.0231,   5.7754,   0.0222, -13.1648,  -5.8200,   1.7148,   7.5573,
          -0.9039,   0.0223,   0.0223]], device='cuda:0'))])
loaded xi:  -252.27637
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1439.2724125859495
W_T_median: 1086.6311344566764
W_T_pctile_5: -114.4532091189134
W_T_CVAR_5_pct: -274.67719995288854
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1596.36980860201
Current xi:  [-214.153]
objective value function right now is: -1596.36980860201
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1600.128643821571
Current xi:  [-182.03545]
objective value function right now is: -1600.128643821571
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1607.552870400909
Current xi:  [-151.76338]
objective value function right now is: -1607.552870400909
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1611.0476805739834
Current xi:  [-124.777954]
objective value function right now is: -1611.0476805739834
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-111.543274]
objective value function right now is: -1593.9618155283679
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-112.25241]
objective value function right now is: -1594.6919132100593
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-111.50496]
objective value function right now is: -1594.767574558306
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-111.7506]
objective value function right now is: -1594.2540385782474
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-111.15911]
objective value function right now is: -1594.998297980259
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1612.6788972187462
Current xi:  [-106.98322]
objective value function right now is: -1612.6788972187462
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1612.9675715381625
Current xi:  [-96.0835]
objective value function right now is: -1612.9675715381625
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1614.1527238652063
Current xi:  [-79.92044]
objective value function right now is: -1614.1527238652063
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.26803]
objective value function right now is: -1595.3166680004467
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-73.904106]
objective value function right now is: -1613.6323119904002
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.50627]
objective value function right now is: -1594.3691181621684
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.86548]
objective value function right now is: -1613.8350896989439
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.11339]
objective value function right now is: -1613.171306842008
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.56573]
objective value function right now is: -1595.375563836824
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.76414]
objective value function right now is: -1595.991151997837
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.808464]
objective value function right now is: -1595.8393463159562
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.81709]
objective value function right now is: -1595.299773874514
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.05227]
objective value function right now is: -1595.640089899855
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.08338]
objective value function right now is: -1613.6980654604993
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.33383]
objective value function right now is: -1594.535917485315
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.9891]
objective value function right now is: -1611.4431559644256
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1614.217900500269
Current xi:  [-73.789314]
objective value function right now is: -1614.217900500269
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.60331]
objective value function right now is: -1613.704479487101
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1614.2482415261632
Current xi:  [-73.64146]
objective value function right now is: -1614.2482415261632
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1614.6698691697263
Current xi:  [-73.71609]
objective value function right now is: -1614.6698691697263
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.98777]
objective value function right now is: -1614.2458540738212
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.70574]
objective value function right now is: -1614.2144945683372
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.90899]
objective value function right now is: -1614.4365849069209
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.91188]
objective value function right now is: -1614.585505868574
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.661995]
objective value function right now is: -1597.9656702648722
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.02557]
objective value function right now is: -1609.4252618340427
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.127335]
objective value function right now is: -1610.7478600885786
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.2282]
objective value function right now is: -1610.9479235187157
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.21558]
objective value function right now is: -1611.9725009824415
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.011185]
objective value function right now is: -1612.6305272447066
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.96677]
objective value function right now is: -1613.1153457466564
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.85847]
objective value function right now is: -1612.858900416015
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.99236]
objective value function right now is: -1613.1463324201643
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.02521]
objective value function right now is: -1613.466568941114
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.13364]
objective value function right now is: -1612.972182711612
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.99243]
objective value function right now is: -1613.7748248279759
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.05044]
objective value function right now is: -1613.8586877549765
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.22068]
objective value function right now is: -1613.1079552654667
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.05843]
objective value function right now is: -1613.827216826558
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.994286]
objective value function right now is: -1614.131437754091
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.97718]
objective value function right now is: -1614.1521537950043
min fval:  -1614.6827327967785
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.8523, -7.2196],
        [-0.5981, -7.4422],
        [-1.1661,  1.9276],
        [-1.1661,  1.9276],
        [-6.6993,  6.9318],
        [-9.6541,  2.9969],
        [-0.7743, -7.9554],
        [-6.2018, -8.2409],
        [-1.1661,  1.9276],
        [-6.4032,  5.6469]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-8.1140, -8.2654, -2.1880, -2.1880,  6.8186,  9.2488, -8.5384, -7.8832,
        -2.1880,  7.4077], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.0485e-01, -2.3342e-01, -9.6592e-02, -9.6583e-02, -1.5587e-01,
         -9.0193e-01, -2.5544e-01, -8.8704e-02, -9.6592e-02, -2.7946e-01],
        [ 3.4979e+00,  3.7707e+00, -3.6684e-03, -3.6087e-03, -4.7950e+00,
         -5.5528e+00,  5.2136e+00,  5.9504e+00, -3.6683e-03, -5.0447e+00],
        [-3.9511e+00, -4.6445e+00,  4.0495e-04,  4.9020e-04,  6.7727e+00,
          6.1731e+00, -6.1115e+00, -5.3778e+00,  4.0508e-04,  6.4020e+00],
        [-2.7881e+00, -3.1790e+00,  1.4269e-02,  1.4249e-02,  4.2385e+00,
          4.4953e+00, -4.2805e+00, -3.5497e+00,  1.4269e-02,  4.4364e+00],
        [ 3.3861e+00,  3.6145e+00, -3.5326e-03, -3.4730e-03, -4.4289e+00,
         -5.4771e+00,  4.9214e+00,  5.7661e+00, -3.5323e-03, -4.7324e+00],
        [-2.0485e-01, -2.3342e-01, -9.6593e-02, -9.6584e-02, -1.5586e-01,
         -9.0193e-01, -2.5544e-01, -8.8703e-02, -9.6593e-02, -2.7946e-01],
        [-2.0485e-01, -2.3342e-01, -9.6592e-02, -9.6583e-02, -1.5587e-01,
         -9.0193e-01, -2.5544e-01, -8.8704e-02, -9.6592e-02, -2.7946e-01],
        [-2.0485e-01, -2.3342e-01, -9.6593e-02, -9.6583e-02, -1.5587e-01,
         -9.0193e-01, -2.5544e-01, -8.8703e-02, -9.6593e-02, -2.7946e-01],
        [-2.0485e-01, -2.3342e-01, -9.6592e-02, -9.6583e-02, -1.5587e-01,
         -9.0193e-01, -2.5544e-01, -8.8704e-02, -9.6592e-02, -2.7946e-01],
        [-2.0485e-01, -2.3342e-01, -9.6592e-02, -9.6583e-02, -1.5587e-01,
         -9.0193e-01, -2.5544e-01, -8.8704e-02, -9.6592e-02, -2.7946e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.2661,  0.8604, -3.1996, -3.1611,  0.7479, -2.2661, -2.2661, -2.2661,
        -2.2661, -2.2661], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-0.1012, -8.0947, 10.1928,  4.6190, -7.3616, -0.1012, -0.1012, -0.1012,
         -0.1012, -0.1012]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -7.2814,   7.0060],
        [ 12.6515,  -1.0540],
        [ -0.4561,  12.5915],
        [ -4.5252,  16.3156],
        [ -4.6125,   2.0295],
        [ 16.4523,   6.2576],
        [-10.1996, -13.0888],
        [-12.2411,  -4.4124],
        [ 10.9053,   1.9724],
        [ -2.7940,  -0.6280]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.1954, -12.0873,  14.0533,  16.0063,  -2.8934,   4.3394, -13.5474,
         -1.7900,  -7.8231,  -4.7894], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -1.0826,   1.6832,   0.1021,  -0.9852,  -0.2426,  -0.7294,   0.6497,
          -8.0625,  -0.1858,   2.4897],
        [ -4.9312,  -6.9836, -13.3881, -16.2183,  -0.0729,  -7.3269,   5.1938,
          11.5589,  -6.4988,   3.7612],
        [ -0.4022,   0.2084,  -1.7490,  -1.0752,  -0.1133,  -0.4468,  -1.1325,
          -7.7441,  -0.5107,   0.4595],
        [ -0.1328,  -3.7493,  -7.1167,  -2.6456,  -0.4120, -19.6065,  14.6899,
           9.6597,  -4.5826,  -1.0625],
        [  1.9640,  -0.9667,   2.5057,   3.5939,   0.1622,  -5.8310, -15.0095,
           4.6297,  -7.5156,   0.2038],
        [ -1.2239, -12.0558,  -0.6189, -14.2641,   0.0949,  -4.1965,  -0.8740,
           5.7794,   2.1505,  -0.1001],
        [  1.5719,  -8.1936,   5.6554, -10.9248,   0.2348,  -5.8835, -10.8110,
           3.9319, -10.5567,  -0.1393],
        [  3.2533,  -6.2327,  -2.3062,   3.1026,   1.0887,   0.1647,  -8.2021,
           2.5279,  -2.0919,   1.9789],
        [ -0.4011,   0.2242,  -1.7394,  -1.0670,  -0.1131,  -0.4575,  -1.1308,
          -7.6827,  -0.5237,   0.4184],
        [ -0.4011,   0.2228,  -1.7400,  -1.0686,  -0.1131,  -0.4566,  -1.1303,
          -7.6897,  -0.5225,   0.4227]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.6717,  0.1823, -3.8886, -7.2181, -5.0330, -1.1905, -0.4570, -1.2931,
        -3.9010, -3.8999], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 3.6364, -6.2491,  4.1920,  6.2056,  4.9551,  5.6126, -3.6036,  0.5539,
          4.1755,  4.1771],
        [-3.6362,  6.1940, -4.1920, -6.2074, -4.9553, -5.5491,  3.6081, -0.7289,
         -4.1754, -4.1770]], device='cuda:0'))])
xi:  [-73.99243]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 298.6864917884297
W_T_median: 98.3552598101138
W_T_pctile_5: -74.44036619175495
W_T_CVAR_5_pct: -155.89049484998887
Average q (qsum/M+1):  54.601137222782256
Optimal xi:  [-73.99243]
Expected(across Rb) median(across samples) p_equity:  0.3000514875166118
obj fun:  tensor(-1614.6827, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.8523, -7.2196],
        [-0.5981, -7.4422],
        [-1.1661,  1.9276],
        [-1.1661,  1.9276],
        [-6.6993,  6.9318],
        [-9.6541,  2.9969],
        [-0.7743, -7.9554],
        [-6.2018, -8.2409],
        [-1.1661,  1.9276],
        [-6.4032,  5.6469]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-8.1140, -8.2654, -2.1880, -2.1880,  6.8186,  9.2488, -8.5384, -7.8832,
        -2.1880,  7.4077], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.0485e-01, -2.3342e-01, -9.6592e-02, -9.6583e-02, -1.5587e-01,
         -9.0193e-01, -2.5544e-01, -8.8704e-02, -9.6592e-02, -2.7946e-01],
        [ 3.4979e+00,  3.7707e+00, -3.6684e-03, -3.6087e-03, -4.7950e+00,
         -5.5528e+00,  5.2136e+00,  5.9504e+00, -3.6683e-03, -5.0447e+00],
        [-3.9511e+00, -4.6445e+00,  4.0495e-04,  4.9020e-04,  6.7727e+00,
          6.1731e+00, -6.1115e+00, -5.3778e+00,  4.0508e-04,  6.4020e+00],
        [-2.7881e+00, -3.1790e+00,  1.4269e-02,  1.4249e-02,  4.2385e+00,
          4.4953e+00, -4.2805e+00, -3.5497e+00,  1.4269e-02,  4.4364e+00],
        [ 3.3861e+00,  3.6145e+00, -3.5326e-03, -3.4730e-03, -4.4289e+00,
         -5.4771e+00,  4.9214e+00,  5.7661e+00, -3.5323e-03, -4.7324e+00],
        [-2.0485e-01, -2.3342e-01, -9.6593e-02, -9.6584e-02, -1.5586e-01,
         -9.0193e-01, -2.5544e-01, -8.8703e-02, -9.6593e-02, -2.7946e-01],
        [-2.0485e-01, -2.3342e-01, -9.6592e-02, -9.6583e-02, -1.5587e-01,
         -9.0193e-01, -2.5544e-01, -8.8704e-02, -9.6592e-02, -2.7946e-01],
        [-2.0485e-01, -2.3342e-01, -9.6593e-02, -9.6583e-02, -1.5587e-01,
         -9.0193e-01, -2.5544e-01, -8.8703e-02, -9.6593e-02, -2.7946e-01],
        [-2.0485e-01, -2.3342e-01, -9.6592e-02, -9.6583e-02, -1.5587e-01,
         -9.0193e-01, -2.5544e-01, -8.8704e-02, -9.6592e-02, -2.7946e-01],
        [-2.0485e-01, -2.3342e-01, -9.6592e-02, -9.6583e-02, -1.5587e-01,
         -9.0193e-01, -2.5544e-01, -8.8704e-02, -9.6592e-02, -2.7946e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.2661,  0.8604, -3.1996, -3.1611,  0.7479, -2.2661, -2.2661, -2.2661,
        -2.2661, -2.2661], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-0.1012, -8.0947, 10.1928,  4.6190, -7.3616, -0.1012, -0.1012, -0.1012,
         -0.1012, -0.1012]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -7.2814,   7.0060],
        [ 12.6515,  -1.0540],
        [ -0.4561,  12.5915],
        [ -4.5252,  16.3156],
        [ -4.6125,   2.0295],
        [ 16.4523,   6.2576],
        [-10.1996, -13.0888],
        [-12.2411,  -4.4124],
        [ 10.9053,   1.9724],
        [ -2.7940,  -0.6280]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.1954, -12.0873,  14.0533,  16.0063,  -2.8934,   4.3394, -13.5474,
         -1.7900,  -7.8231,  -4.7894], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -1.0826,   1.6832,   0.1021,  -0.9852,  -0.2426,  -0.7294,   0.6497,
          -8.0625,  -0.1858,   2.4897],
        [ -4.9312,  -6.9836, -13.3881, -16.2183,  -0.0729,  -7.3269,   5.1938,
          11.5589,  -6.4988,   3.7612],
        [ -0.4022,   0.2084,  -1.7490,  -1.0752,  -0.1133,  -0.4468,  -1.1325,
          -7.7441,  -0.5107,   0.4595],
        [ -0.1328,  -3.7493,  -7.1167,  -2.6456,  -0.4120, -19.6065,  14.6899,
           9.6597,  -4.5826,  -1.0625],
        [  1.9640,  -0.9667,   2.5057,   3.5939,   0.1622,  -5.8310, -15.0095,
           4.6297,  -7.5156,   0.2038],
        [ -1.2239, -12.0558,  -0.6189, -14.2641,   0.0949,  -4.1965,  -0.8740,
           5.7794,   2.1505,  -0.1001],
        [  1.5719,  -8.1936,   5.6554, -10.9248,   0.2348,  -5.8835, -10.8110,
           3.9319, -10.5567,  -0.1393],
        [  3.2533,  -6.2327,  -2.3062,   3.1026,   1.0887,   0.1647,  -8.2021,
           2.5279,  -2.0919,   1.9789],
        [ -0.4011,   0.2242,  -1.7394,  -1.0670,  -0.1131,  -0.4575,  -1.1308,
          -7.6827,  -0.5237,   0.4184],
        [ -0.4011,   0.2228,  -1.7400,  -1.0686,  -0.1131,  -0.4566,  -1.1303,
          -7.6897,  -0.5225,   0.4227]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.6717,  0.1823, -3.8886, -7.2181, -5.0330, -1.1905, -0.4570, -1.2931,
        -3.9010, -3.8999], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 3.6364, -6.2491,  4.1920,  6.2056,  4.9551,  5.6126, -3.6036,  0.5539,
          4.1755,  4.1771],
        [-3.6362,  6.1940, -4.1920, -6.2074, -4.9553, -5.5491,  3.6081, -0.7289,
         -4.1754, -4.1770]], device='cuda:0'))])
loaded xi:  -73.99243
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1439.2724125859495
W_T_median: 1086.6311344566764
W_T_pctile_5: -114.4532091189134
W_T_CVAR_5_pct: -274.67719995288854
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1549.3386660279846
Current xi:  [-47.11077]
objective value function right now is: -1549.3386660279846
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1561.6539334922456
Current xi:  [-26.96144]
objective value function right now is: -1561.6539334922456
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1570.5674337672936
Current xi:  [-3.006479]
objective value function right now is: -1570.5674337672936
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1570.589112783796
Current xi:  [-0.02458309]
objective value function right now is: -1570.589112783796
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00458308]
objective value function right now is: -1569.493152718266
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1570.597083840801
Current xi:  [-0.01325011]
objective value function right now is: -1570.597083840801
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02973734]
objective value function right now is: -1568.1169143488728
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0201794]
objective value function right now is: -1570.3437413849767
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1571.3906724407748
Current xi:  [-0.01860862]
objective value function right now is: -1571.3906724407748
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1571.5357950094397
Current xi:  [-0.02023236]
objective value function right now is: -1571.5357950094397
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04320291]
objective value function right now is: -1565.0529466880892
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01967911]
objective value function right now is: -1570.4052210345783
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01268889]
objective value function right now is: -1571.0830501256908
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04633725]
objective value function right now is: -1570.0935952196285
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01357608]
objective value function right now is: -1571.1162800786074
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.8996468]
objective value function right now is: -1470.7272965977752
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01663274]
objective value function right now is: -1548.2127424399275
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00320353]
objective value function right now is: -1551.6698278884119
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02932102]
objective value function right now is: -1568.0985540984125
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04032764]
objective value function right now is: -1569.6696752802743
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01417945]
objective value function right now is: -1568.6530183131645
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02453814]
objective value function right now is: -1570.729405634529
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01748936]
objective value function right now is: -1570.6381286706155
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00402973]
objective value function right now is: -1570.6230023197272
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0004787]
objective value function right now is: -1569.6587071785932
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00327784]
objective value function right now is: -1567.4349372699694
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00081496]
objective value function right now is: -1560.022880046384
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [0.00696051]
objective value function right now is: -1571.2731096294974
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [0.0082298]
objective value function right now is: -1569.8037254858623
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00010457]
objective value function right now is: -1570.7194637792404
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03977543]
objective value function right now is: -1571.0658437634286
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00121119]
objective value function right now is: -1570.578091249785
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00364844]
objective value function right now is: -1568.9637871227628
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04353401]
objective value function right now is: -1553.2342271733755
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00442123]
objective value function right now is: -1571.0622245510813
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1572.1629218392463
Current xi:  [0.00220223]
objective value function right now is: -1572.1629218392463
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0041295]
objective value function right now is: -1570.6210444077692
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0018619]
objective value function right now is: -1571.9028375751022
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1572.6169646167093
Current xi:  [0.0027201]
objective value function right now is: -1572.6169646167093
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00620636]
objective value function right now is: -1572.546529633128
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00165437]
objective value function right now is: -1572.3239570157689
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1572.6712469057093
Current xi:  [0.00018933]
objective value function right now is: -1572.6712469057093
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00661923]
objective value function right now is: -1572.5940091998134
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00310065]
objective value function right now is: -1572.588895688651
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01128127]
objective value function right now is: -1572.2558256314064
new min fval from sgd:  -1572.6828300773307
new min fval from sgd:  -1572.7142738827254
new min fval from sgd:  -1572.7443141756078
new min fval from sgd:  -1572.7621836437957
new min fval from sgd:  -1572.8120663209395
new min fval from sgd:  -1572.8498249491245
new min fval from sgd:  -1572.8854025311437
new min fval from sgd:  -1572.9078032966042
new min fval from sgd:  -1572.913458894765
new min fval from sgd:  -1572.9198320122273
new min fval from sgd:  -1572.92064964698
new min fval from sgd:  -1572.9234457143202
new min fval from sgd:  -1572.927731667517
new min fval from sgd:  -1572.9316765360013
new min fval from sgd:  -1572.9372203792418
new min fval from sgd:  -1572.9458429179806
new min fval from sgd:  -1572.9596188388018
new min fval from sgd:  -1572.960292244611
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00573974]
objective value function right now is: -1572.489240979646
new min fval from sgd:  -1572.9656336460635
new min fval from sgd:  -1572.9690390180424
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00045048]
objective value function right now is: -1572.6763502630533
new min fval from sgd:  -1572.9720063212771
new min fval from sgd:  -1572.9765181198443
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00860812]
objective value function right now is: -1572.8137966685022
new min fval from sgd:  -1572.9781375662046
new min fval from sgd:  -1572.981298637521
new min fval from sgd:  -1572.9846308987549
new min fval from sgd:  -1572.9867808532617
new min fval from sgd:  -1572.9887186183726
new min fval from sgd:  -1572.989555291616
new min fval from sgd:  -1572.9903169316137
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00360372]
objective value function right now is: -1572.9703347914624
new min fval from sgd:  -1572.991208440274
new min fval from sgd:  -1572.9936333836372
new min fval from sgd:  -1572.995618525721
new min fval from sgd:  -1572.9967519045506
new min fval from sgd:  -1572.9971261569383
new min fval from sgd:  -1572.9988424027085
new min fval from sgd:  -1573.0002664389997
new min fval from sgd:  -1573.0021334609496
new min fval from sgd:  -1573.0057396259517
new min fval from sgd:  -1573.0094214319824
new min fval from sgd:  -1573.012403024707
new min fval from sgd:  -1573.0147479396392
new min fval from sgd:  -1573.01521374204
new min fval from sgd:  -1573.0156316385503
new min fval from sgd:  -1573.0158915738637
new min fval from sgd:  -1573.0166080071072
new min fval from sgd:  -1573.0189657974352
new min fval from sgd:  -1573.0215062584225
new min fval from sgd:  -1573.0222461624448
new min fval from sgd:  -1573.0223785109094
new min fval from sgd:  -1573.025357958964
new min fval from sgd:  -1573.0295346382989
new min fval from sgd:  -1573.0301270021077
new min fval from sgd:  -1573.0301822753834
new min fval from sgd:  -1573.0318118529212
new min fval from sgd:  -1573.0332979172524
new min fval from sgd:  -1573.034217190727
new min fval from sgd:  -1573.0344046436999
new min fval from sgd:  -1573.0361958896801
new min fval from sgd:  -1573.0403892867964
new min fval from sgd:  -1573.042668028747
new min fval from sgd:  -1573.043454657504
new min fval from sgd:  -1573.0440903623532
new min fval from sgd:  -1573.0449300254093
new min fval from sgd:  -1573.0467204818415
new min fval from sgd:  -1573.0483037355614
new min fval from sgd:  -1573.0489325147016
new min fval from sgd:  -1573.0501615703604
new min fval from sgd:  -1573.0509847432768
new min fval from sgd:  -1573.0535955387895
new min fval from sgd:  -1573.056292548438
new min fval from sgd:  -1573.0584647043738
new min fval from sgd:  -1573.058861047189
new min fval from sgd:  -1573.0599688765626
new min fval from sgd:  -1573.0621532203024
new min fval from sgd:  -1573.0649372690666
new min fval from sgd:  -1573.0677831716794
new min fval from sgd:  -1573.0700451878004
new min fval from sgd:  -1573.0722048876355
new min fval from sgd:  -1573.0748384170518
new min fval from sgd:  -1573.07576649171
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00344396]
objective value function right now is: -1573.0100647309073
min fval:  -1573.07576649171
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  9.3645,  -7.5347],
        [  6.8578,  -9.1295],
        [ -1.1386,   0.6525],
        [ -1.1386,   0.6525],
        [ -1.1386,   0.6524],
        [-14.1843,   1.3401],
        [  6.8516,  -8.8836],
        [ -4.9454, -10.7550],
        [ -1.1386,   0.6525],
        [-11.4145,  -1.2170]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.3575,  -6.8378,  -1.9703,  -1.9703,  -1.9703,  12.7133, -10.6468,
        -10.1670,  -1.9703,   9.9973], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-8.5235e-01, -2.7924e-01, -5.9734e-03, -5.9734e-03, -5.9732e-03,
         -3.5520e-01, -4.6707e-02, -2.2622e-02, -5.9734e-03, -3.0731e-01],
        [ 3.1225e+00,  1.9266e+00,  1.1708e-01,  1.1708e-01,  1.1716e-01,
         -9.1235e+00,  1.0803e+01,  1.2675e+01,  1.1708e-01, -5.0506e+00],
        [-4.1811e+00, -3.3911e+00,  1.6479e-01,  1.6479e-01,  1.6484e-01,
          1.0401e+01, -1.2602e+01, -1.1174e+01,  1.6479e-01,  7.0587e+00],
        [-8.5235e-01, -2.7924e-01, -5.9734e-03, -5.9734e-03, -5.9732e-03,
         -3.5520e-01, -4.6707e-02, -2.2622e-02, -5.9734e-03, -3.0731e-01],
        [ 3.0620e+00,  1.8168e+00,  1.2743e-01,  1.2743e-01,  1.2747e-01,
         -8.7884e+00,  1.0186e+01,  1.2042e+01,  1.2743e-01, -4.6057e+00],
        [-8.5235e-01, -2.7924e-01, -5.9734e-03, -5.9734e-03, -5.9732e-03,
         -3.5520e-01, -4.6707e-02, -2.2622e-02, -5.9734e-03, -3.0731e-01],
        [-8.5235e-01, -2.7924e-01, -5.9734e-03, -5.9734e-03, -5.9732e-03,
         -3.5520e-01, -4.6707e-02, -2.2622e-02, -5.9734e-03, -3.0731e-01],
        [-8.5235e-01, -2.7924e-01, -5.9734e-03, -5.9734e-03, -5.9732e-03,
         -3.5520e-01, -4.6707e-02, -2.2622e-02, -5.9734e-03, -3.0731e-01],
        [-8.5235e-01, -2.7924e-01, -5.9734e-03, -5.9734e-03, -5.9732e-03,
         -3.5520e-01, -4.6707e-02, -2.2622e-02, -5.9734e-03, -3.0731e-01],
        [-8.5235e-01, -2.7924e-01, -5.9734e-03, -5.9734e-03, -5.9732e-03,
         -3.5520e-01, -4.6707e-02, -2.2622e-02, -5.9734e-03, -3.0731e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8804, -0.0467, -2.5173, -0.8804, -0.2444, -0.8804, -0.8804, -0.8804,
        -0.8804, -0.8804], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 0.0233, -8.8396, 14.0643,  0.0233, -7.6469,  0.0233,  0.0233,  0.0233,
          0.0233,  0.0233]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.6737,   0.7999],
        [ -4.1992,  -3.6677],
        [  8.2441,  15.5914],
        [ -7.1856,  18.3144],
        [-10.0756,   5.6687],
        [ 19.2967,   5.8635],
        [-17.3578, -11.8486],
        [-12.9156,  -4.5415],
        [ 12.5885,   0.1007],
        [ -2.0953,   0.2987]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.7646,  -1.1888,  14.7651,  19.2006,   8.2751,   3.7539,  -9.8614,
         -3.9745, -10.8284,  -3.9123], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.1970e+00, -5.8569e-01,  1.3803e+00, -8.0239e-02, -1.5694e+00,
         -1.5989e+00, -2.5425e-01, -2.8924e-01, -2.6645e-01, -2.0347e-02],
        [-2.1753e+00,  3.1874e-01, -2.2238e+00, -3.7313e+00, -4.9470e+00,
         -4.9097e+00,  6.1914e+00,  4.9558e+00,  1.5007e+00, -1.7088e-01],
        [-2.3722e+00, -1.7907e-01, -1.9660e-01, -5.8506e-01, -6.3981e-01,
         -1.4176e+00, -1.7795e-01, -1.8825e-01, -2.7469e-01, -1.7096e-02],
        [-3.5488e+00,  5.1315e+00, -1.6391e+01, -3.9452e+00, -8.4709e+00,
         -1.4932e+01,  1.6265e+01,  1.2396e+01, -1.0356e+01,  6.5247e-01],
        [ 2.2088e+00,  3.2900e+00, -5.5379e+00, -1.9713e+01, -1.3284e+00,
         -4.0049e+00,  3.9627e+00,  1.7561e+01, -1.2202e+01,  2.6551e-01],
        [-7.4664e-01,  8.9035e-02,  1.4238e-01, -6.9841e-02, -1.7524e+00,
         -2.8330e+00, -8.1234e+00, -9.9589e+00,  4.0199e-01, -5.6963e-02],
        [-2.0443e+00,  1.8793e+00, -1.1013e+00, -2.8650e+00, -1.2482e+00,
         -1.8545e+00,  1.3659e+00,  1.8028e-01,  1.9866e+00, -1.0606e-02],
        [-1.3178e-01,  2.2324e+00, -4.0280e+00,  8.8983e+00,  5.8777e+00,
         -2.3496e+00, -1.2493e+01, -2.4238e+00, -5.4874e+00,  8.8007e-02],
        [-1.5675e+00,  1.8739e+00,  1.9660e+00, -1.0703e+01,  4.2812e+00,
         -9.5338e-01,  4.0024e-01,  6.0843e+00, -4.9924e-01,  7.4379e-02],
        [-1.9570e+00,  2.0130e+00,  1.3125e+00, -9.1126e+00,  2.3130e+00,
         -6.3665e-01, -4.8949e+00, -4.7822e-01, -6.6926e-01, -1.6394e-02]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.6344, -2.7191, -3.6789, -6.1528, -3.0934,  0.4610, -2.5189, -4.7733,
        -3.6706, -4.0488], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.2901,   3.6259,   0.3586,  11.0728,  -5.6211,   3.6653,   1.4760,
           0.4986,   3.5802,   3.4743],
        [ -1.2901,  -3.6258,  -0.3585, -11.0725,   5.6220,  -3.6137,  -1.4760,
          -0.6636,  -3.5802,  -3.4742]], device='cuda:0'))])
xi:  [0.00403484]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 412.0558895374377
W_T_median: 182.31512688102396
W_T_pctile_5: 0.004141909511407605
W_T_CVAR_5_pct: -71.42698411004352
Average q (qsum/M+1):  53.04846585181452
Optimal xi:  [0.00403484]
Expected(across Rb) median(across samples) p_equity:  0.3062920855979125
obj fun:  tensor(-1573.0758, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  9.3645,  -7.5347],
        [  6.8578,  -9.1295],
        [ -1.1386,   0.6525],
        [ -1.1386,   0.6525],
        [ -1.1386,   0.6524],
        [-14.1843,   1.3401],
        [  6.8516,  -8.8836],
        [ -4.9454, -10.7550],
        [ -1.1386,   0.6525],
        [-11.4145,  -1.2170]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.3575,  -6.8378,  -1.9703,  -1.9703,  -1.9703,  12.7133, -10.6468,
        -10.1670,  -1.9703,   9.9973], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-8.5235e-01, -2.7924e-01, -5.9734e-03, -5.9734e-03, -5.9732e-03,
         -3.5520e-01, -4.6707e-02, -2.2622e-02, -5.9734e-03, -3.0731e-01],
        [ 3.1225e+00,  1.9266e+00,  1.1708e-01,  1.1708e-01,  1.1716e-01,
         -9.1235e+00,  1.0803e+01,  1.2675e+01,  1.1708e-01, -5.0506e+00],
        [-4.1811e+00, -3.3911e+00,  1.6479e-01,  1.6479e-01,  1.6484e-01,
          1.0401e+01, -1.2602e+01, -1.1174e+01,  1.6479e-01,  7.0587e+00],
        [-8.5235e-01, -2.7924e-01, -5.9734e-03, -5.9734e-03, -5.9732e-03,
         -3.5520e-01, -4.6707e-02, -2.2622e-02, -5.9734e-03, -3.0731e-01],
        [ 3.0620e+00,  1.8168e+00,  1.2743e-01,  1.2743e-01,  1.2747e-01,
         -8.7884e+00,  1.0186e+01,  1.2042e+01,  1.2743e-01, -4.6057e+00],
        [-8.5235e-01, -2.7924e-01, -5.9734e-03, -5.9734e-03, -5.9732e-03,
         -3.5520e-01, -4.6707e-02, -2.2622e-02, -5.9734e-03, -3.0731e-01],
        [-8.5235e-01, -2.7924e-01, -5.9734e-03, -5.9734e-03, -5.9732e-03,
         -3.5520e-01, -4.6707e-02, -2.2622e-02, -5.9734e-03, -3.0731e-01],
        [-8.5235e-01, -2.7924e-01, -5.9734e-03, -5.9734e-03, -5.9732e-03,
         -3.5520e-01, -4.6707e-02, -2.2622e-02, -5.9734e-03, -3.0731e-01],
        [-8.5235e-01, -2.7924e-01, -5.9734e-03, -5.9734e-03, -5.9732e-03,
         -3.5520e-01, -4.6707e-02, -2.2622e-02, -5.9734e-03, -3.0731e-01],
        [-8.5235e-01, -2.7924e-01, -5.9734e-03, -5.9734e-03, -5.9732e-03,
         -3.5520e-01, -4.6707e-02, -2.2622e-02, -5.9734e-03, -3.0731e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8804, -0.0467, -2.5173, -0.8804, -0.2444, -0.8804, -0.8804, -0.8804,
        -0.8804, -0.8804], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 0.0233, -8.8396, 14.0643,  0.0233, -7.6469,  0.0233,  0.0233,  0.0233,
          0.0233,  0.0233]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.6737,   0.7999],
        [ -4.1992,  -3.6677],
        [  8.2441,  15.5914],
        [ -7.1856,  18.3144],
        [-10.0756,   5.6687],
        [ 19.2967,   5.8635],
        [-17.3578, -11.8486],
        [-12.9156,  -4.5415],
        [ 12.5885,   0.1007],
        [ -2.0953,   0.2987]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.7646,  -1.1888,  14.7651,  19.2006,   8.2751,   3.7539,  -9.8614,
         -3.9745, -10.8284,  -3.9123], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.1970e+00, -5.8569e-01,  1.3803e+00, -8.0239e-02, -1.5694e+00,
         -1.5989e+00, -2.5425e-01, -2.8924e-01, -2.6645e-01, -2.0347e-02],
        [-2.1753e+00,  3.1874e-01, -2.2238e+00, -3.7313e+00, -4.9470e+00,
         -4.9097e+00,  6.1914e+00,  4.9558e+00,  1.5007e+00, -1.7088e-01],
        [-2.3722e+00, -1.7907e-01, -1.9660e-01, -5.8506e-01, -6.3981e-01,
         -1.4176e+00, -1.7795e-01, -1.8825e-01, -2.7469e-01, -1.7096e-02],
        [-3.5488e+00,  5.1315e+00, -1.6391e+01, -3.9452e+00, -8.4709e+00,
         -1.4932e+01,  1.6265e+01,  1.2396e+01, -1.0356e+01,  6.5247e-01],
        [ 2.2088e+00,  3.2900e+00, -5.5379e+00, -1.9713e+01, -1.3284e+00,
         -4.0049e+00,  3.9627e+00,  1.7561e+01, -1.2202e+01,  2.6551e-01],
        [-7.4664e-01,  8.9035e-02,  1.4238e-01, -6.9841e-02, -1.7524e+00,
         -2.8330e+00, -8.1234e+00, -9.9589e+00,  4.0199e-01, -5.6963e-02],
        [-2.0443e+00,  1.8793e+00, -1.1013e+00, -2.8650e+00, -1.2482e+00,
         -1.8545e+00,  1.3659e+00,  1.8028e-01,  1.9866e+00, -1.0606e-02],
        [-1.3178e-01,  2.2324e+00, -4.0280e+00,  8.8983e+00,  5.8777e+00,
         -2.3496e+00, -1.2493e+01, -2.4238e+00, -5.4874e+00,  8.8007e-02],
        [-1.5675e+00,  1.8739e+00,  1.9660e+00, -1.0703e+01,  4.2812e+00,
         -9.5338e-01,  4.0024e-01,  6.0843e+00, -4.9924e-01,  7.4379e-02],
        [-1.9570e+00,  2.0130e+00,  1.3125e+00, -9.1126e+00,  2.3130e+00,
         -6.3665e-01, -4.8949e+00, -4.7822e-01, -6.6926e-01, -1.6394e-02]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.6344, -2.7191, -3.6789, -6.1528, -3.0934,  0.4610, -2.5189, -4.7733,
        -3.6706, -4.0488], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.2901,   3.6259,   0.3586,  11.0728,  -5.6211,   3.6653,   1.4760,
           0.4986,   3.5802,   3.4743],
        [ -1.2901,  -3.6258,  -0.3585, -11.0725,   5.6220,  -3.6137,  -1.4760,
          -0.6636,  -3.5802,  -3.4742]], device='cuda:0'))])
loaded xi:  0.00403484
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1439.2724125859495
W_T_median: 1086.6311344566764
W_T_pctile_5: -114.4532091189134
W_T_CVAR_5_pct: -274.67719995288854
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1522.6332618994074
Current xi:  [0.00064093]
objective value function right now is: -1522.6332618994074
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.1537466]
objective value function right now is: -1226.3361461971733
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.5350099]
objective value function right now is: -1410.2266694120076
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06761904]
objective value function right now is: -1498.5256501214212
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03839378]
objective value function right now is: -1509.478045397155
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00968445]
objective value function right now is: -1512.4205542785746
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01076079]
objective value function right now is: -1449.7923512553155
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.05233138]
objective value function right now is: -1458.620084688411
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01135701]
objective value function right now is: -1451.9325852212228
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01821055]
objective value function right now is: -1511.7248899829965
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00784298]
objective value function right now is: -1520.4126090531092
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1524.234360734269
Current xi:  [-0.00341543]
objective value function right now is: -1524.234360734269
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1534.1810376487017
Current xi:  [-0.00486721]
objective value function right now is: -1534.1810376487017
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00559308]
objective value function right now is: -1533.178024638231
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00884465]
objective value function right now is: -1526.8080709703063
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.08600238]
objective value function right now is: -1533.7195686416458
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00744009]
objective value function right now is: -1530.9367071948727
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01523858]
objective value function right now is: -1531.380489353079
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00049917]
objective value function right now is: -1534.0582466939782
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1534.5945702601628
Current xi:  [-0.01401661]
objective value function right now is: -1534.5945702601628
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06827553]
objective value function right now is: -1533.3926410799415
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02809312]
objective value function right now is: -1534.0895891306532
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0384936]
objective value function right now is: -1534.0594669652542
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1534.7809859440952
Current xi:  [-0.03458818]
objective value function right now is: -1534.7809859440952
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03979648]
objective value function right now is: -1533.7505726731001
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.08768585]
objective value function right now is: -1533.3107203880807
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06429689]
objective value function right now is: -1534.1982598518575
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1534.7844431728802
Current xi:  [0.00541828]
objective value function right now is: -1534.7844431728802
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1534.9381643701706
Current xi:  [0.00629458]
objective value function right now is: -1534.9381643701706
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00593612]
objective value function right now is: -1518.394886507064
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1535.6507207292766
Current xi:  [-0.00023593]
objective value function right now is: -1535.6507207292766
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00094563]
objective value function right now is: -1531.8743107977732
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02022867]
objective value function right now is: -1528.2759645899798
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03580648]
objective value function right now is: -1534.017660857364
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.001545]
objective value function right now is: -1530.4233357189603
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1536.3424399949447
Current xi:  [0.00490582]
objective value function right now is: -1536.3424399949447
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00701831]
objective value function right now is: -1535.953252458176
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1536.859342937255
Current xi:  [0.01011005]
objective value function right now is: -1536.859342937255
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00573032]
objective value function right now is: -1536.68764258687
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00558955]
objective value function right now is: -1536.3470101069831
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01484429]
objective value function right now is: -1536.5884468674055
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00776628]
objective value function right now is: -1536.2745858271421
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1537.0145567270363
Current xi:  [-0.00751675]
objective value function right now is: -1537.0145567270363
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1537.1874370925343
Current xi:  [0.00429755]
objective value function right now is: -1537.1874370925343
new min fval from sgd:  -1537.3218951162407
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00586702]
objective value function right now is: -1537.3218951162407
new min fval from sgd:  -1537.3385932250796
new min fval from sgd:  -1537.3718338600909
new min fval from sgd:  -1537.42025588599
new min fval from sgd:  -1537.427042969829
new min fval from sgd:  -1537.472283248583
new min fval from sgd:  -1537.495455658617
new min fval from sgd:  -1537.4975183718184
new min fval from sgd:  -1537.5275616381289
new min fval from sgd:  -1537.554265106334
new min fval from sgd:  -1537.555825453101
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00884631]
objective value function right now is: -1537.0325231914196
new min fval from sgd:  -1537.5694827542607
new min fval from sgd:  -1537.5832572898062
new min fval from sgd:  -1537.5989819493386
new min fval from sgd:  -1537.6156977555252
new min fval from sgd:  -1537.6325460289813
new min fval from sgd:  -1537.6388476439627
new min fval from sgd:  -1537.657782067192
new min fval from sgd:  -1537.6731907880894
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0035261]
objective value function right now is: -1537.076453355487
new min fval from sgd:  -1537.6851149677448
new min fval from sgd:  -1537.6979660088257
new min fval from sgd:  -1537.7329767584806
new min fval from sgd:  -1537.7651061285474
new min fval from sgd:  -1537.784057907933
new min fval from sgd:  -1537.7985341889384
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01808052]
objective value function right now is: -1537.294752943275
new min fval from sgd:  -1537.7999865511256
new min fval from sgd:  -1537.803035088776
new min fval from sgd:  -1537.806713604427
new min fval from sgd:  -1537.809455172672
new min fval from sgd:  -1537.8103051393937
new min fval from sgd:  -1537.8104844973213
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00661084]
objective value function right now is: -1537.783700262026
new min fval from sgd:  -1537.8126067711012
new min fval from sgd:  -1537.8193512242556
new min fval from sgd:  -1537.8249630551911
new min fval from sgd:  -1537.8308009968937
new min fval from sgd:  -1537.8361697998569
new min fval from sgd:  -1537.83902765679
new min fval from sgd:  -1537.8396369114816
new min fval from sgd:  -1537.8410188784806
new min fval from sgd:  -1537.8410661929327
new min fval from sgd:  -1537.841736819922
new min fval from sgd:  -1537.8423121318838
new min fval from sgd:  -1537.8440928654088
new min fval from sgd:  -1537.8441218109438
new min fval from sgd:  -1537.8483615130062
new min fval from sgd:  -1537.8532626442932
new min fval from sgd:  -1537.864894427815
new min fval from sgd:  -1537.8762754539016
new min fval from sgd:  -1537.8797056141343
new min fval from sgd:  -1537.8807423643586
new min fval from sgd:  -1537.8808140783738
new min fval from sgd:  -1537.8825593917713
new min fval from sgd:  -1537.8877005455508
new min fval from sgd:  -1537.8918528597521
new min fval from sgd:  -1537.8949280356417
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00938538]
objective value function right now is: -1537.8949280356417
min fval:  -1537.8949280356417
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  2.4235,  -1.0921],
        [  3.2637,  -1.6524],
        [ -1.1181,   0.5197],
        [ -1.1181,   0.5197],
        [ -1.1181,   0.5197],
        [-17.7066,   2.0801],
        [  8.5407,  -9.3993],
        [ -4.5438, -12.9611],
        [ -1.1181,   0.5197],
        [-14.1467,   0.5270]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  3.1713,   2.1736,  -2.0104,  -2.0104,  -2.0104,  15.4307, -11.5119,
        -12.2199,  -2.0104,  12.3369], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-6.5075e-01, -6.4857e-01, -2.1882e-02, -2.1882e-02, -2.1882e-02,
         -3.1052e-01, -1.1155e-01,  1.3993e-02, -2.1882e-02, -3.0039e-01],
        [ 2.0843e+00,  2.2786e+00, -5.0251e-03, -5.0250e-03, -5.0247e-03,
         -1.0906e+01,  1.3163e+01,  1.5235e+01, -5.0251e-03, -4.8042e+00],
        [-3.6918e+00, -1.5173e+00, -1.0161e-02, -1.0160e-02, -1.0160e-02,
          1.1712e+01, -1.4705e+01, -1.4879e+01, -1.0160e-02,  5.4096e+00],
        [-6.5075e-01, -6.4857e-01, -2.1882e-02, -2.1882e-02, -2.1882e-02,
         -3.1052e-01, -1.1155e-01,  1.3993e-02, -2.1882e-02, -3.0039e-01],
        [ 1.5326e+00,  1.7105e+00,  7.1651e-02,  7.1651e-02,  7.1651e-02,
         -8.7499e+00,  1.0004e+01,  1.2117e+01,  7.1651e-02, -3.3101e+00],
        [-6.5075e-01, -6.4857e-01, -2.1882e-02, -2.1882e-02, -2.1882e-02,
         -3.1052e-01, -1.1155e-01,  1.3993e-02, -2.1882e-02, -3.0039e-01],
        [-6.5075e-01, -6.4857e-01, -2.1882e-02, -2.1882e-02, -2.1882e-02,
         -3.1052e-01, -1.1155e-01,  1.3993e-02, -2.1882e-02, -3.0039e-01],
        [-6.5075e-01, -6.4857e-01, -2.1882e-02, -2.1882e-02, -2.1882e-02,
         -3.1052e-01, -1.1155e-01,  1.3993e-02, -2.1882e-02, -3.0039e-01],
        [-6.5075e-01, -6.4857e-01, -2.1882e-02, -2.1882e-02, -2.1882e-02,
         -3.1052e-01, -1.1155e-01,  1.3993e-02, -2.1882e-02, -3.0039e-01],
        [-6.5075e-01, -6.4857e-01, -2.1882e-02, -2.1882e-02, -2.1882e-02,
         -3.1052e-01, -1.1155e-01,  1.3993e-02, -2.1882e-02, -3.0039e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.6553, -0.3403, -2.5267, -0.6553, -0.6960, -0.6553, -0.6553, -0.6553,
        -0.6553, -0.6553], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0153, -11.0796,  12.8151,  -0.0153,  -6.1410,  -0.0153,  -0.0153,
          -0.0153,  -0.0153,  -0.0153]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  1.2650,   0.3512],
        [  0.0918,   1.5418],
        [  6.3455,   9.8287],
        [ -4.5230,   9.7384],
        [-12.8493,  -1.5793],
        [ 17.8691,   8.4248],
        [-16.4041, -12.3591],
        [-15.5023,  -3.5395],
        [ 13.7700,  -3.5361],
        [-12.2830,   4.6490]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  4.2318,  -3.0703,   9.2314,  24.9200,   7.8249,   5.8896, -11.2169,
         -0.0595, -15.7862,   6.7795], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.2997e+00, -2.0676e-01,  2.3689e-03, -2.8640e+00, -1.1867e+00,
         -3.1973e+00,  1.2972e+00,  7.9184e-02,  1.5376e+00, -3.7973e-02],
        [-8.9949e+00, -1.1770e-01, -9.8855e+00,  3.3281e+00, -1.9963e+00,
         -1.2640e+01,  1.6831e+01,  1.3555e+01, -4.6470e+00, -5.5358e+00],
        [ 1.2315e-01, -2.4542e-01, -3.9543e+00,  3.3090e+00,  4.0371e+00,
         -5.3906e+00,  1.5524e+01,  1.8635e+00, -1.3849e+01, -1.4950e+01],
        [-1.5520e+00,  2.3557e+00, -8.4267e-01, -8.6163e-01,  9.4663e-01,
         -6.5782e-01, -5.2780e+00,  2.3616e+00,  9.4731e-01,  9.4729e-01],
        [-1.6136e+00, -1.8862e-01, -8.8050e-01, -1.3917e+00, -6.8947e-01,
         -1.5015e+00, -2.1649e-01, -1.2937e-02, -5.1213e-01, -2.0258e-01],
        [-4.0808e-01, -4.6569e-01, -3.6844e-03, -5.2848e+00, -1.3247e+00,
         -3.6420e+00,  1.2695e+00, -5.3845e-01,  1.3329e+00, -1.5792e-02],
        [-1.8272e+00,  6.8210e-01, -3.7448e+00,  9.6519e-01,  2.8391e+00,
         -1.0153e+00,  6.9068e-01, -2.9290e+00,  5.1922e-01, -5.1157e+00],
        [-1.9680e+00, -2.6551e-01,  3.6174e-01, -3.2592e+00, -3.2525e-01,
         -3.8291e+00,  1.8011e+00,  4.2829e-01,  1.6015e+00,  3.6723e-01],
        [-2.0762e+00, -1.6770e+00,  6.1840e-01, -3.9203e+00, -1.2041e+00,
          1.3016e+00,  1.1952e+00, -1.1646e+00,  6.1830e+00,  1.0647e-01],
        [ 6.4841e-02, -4.1762e+00,  5.8507e+00, -1.9681e+00,  4.3556e+00,
         -2.7493e-02, -3.4297e+01,  4.7068e+00,  1.4538e+01,  1.9973e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.6266, -8.8154, -5.4064, -1.6762, -1.6384, -1.7271, -2.1005, -2.0354,
        -2.9538, -4.2311], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.8406,  12.8334,  -5.4653,  -1.2602,  -0.0129,   1.6505,  -1.3998,
           0.4322,   0.4573,   0.3519],
        [ -0.8405, -12.8271,   5.4701,   1.2602,   0.0129,  -1.6502,   1.3998,
          -0.4327,  -0.4573,  -0.3519]], device='cuda:0'))])
xi:  [0.00938538]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 538.7495453328944
W_T_median: 222.16838085662897
W_T_pctile_5: 0.008255281246701784
W_T_CVAR_5_pct: -61.25679603581099
Average q (qsum/M+1):  52.573545394405244
Optimal xi:  [0.00938538]
Expected(across Rb) median(across samples) p_equity:  0.35545980284611384
obj fun:  tensor(-1537.8949, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  2.4235,  -1.0921],
        [  3.2637,  -1.6524],
        [ -1.1181,   0.5197],
        [ -1.1181,   0.5197],
        [ -1.1181,   0.5197],
        [-17.7066,   2.0801],
        [  8.5407,  -9.3993],
        [ -4.5438, -12.9611],
        [ -1.1181,   0.5197],
        [-14.1467,   0.5270]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  3.1713,   2.1736,  -2.0104,  -2.0104,  -2.0104,  15.4307, -11.5119,
        -12.2199,  -2.0104,  12.3369], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-6.5075e-01, -6.4857e-01, -2.1882e-02, -2.1882e-02, -2.1882e-02,
         -3.1052e-01, -1.1155e-01,  1.3993e-02, -2.1882e-02, -3.0039e-01],
        [ 2.0843e+00,  2.2786e+00, -5.0251e-03, -5.0250e-03, -5.0247e-03,
         -1.0906e+01,  1.3163e+01,  1.5235e+01, -5.0251e-03, -4.8042e+00],
        [-3.6918e+00, -1.5173e+00, -1.0161e-02, -1.0160e-02, -1.0160e-02,
          1.1712e+01, -1.4705e+01, -1.4879e+01, -1.0160e-02,  5.4096e+00],
        [-6.5075e-01, -6.4857e-01, -2.1882e-02, -2.1882e-02, -2.1882e-02,
         -3.1052e-01, -1.1155e-01,  1.3993e-02, -2.1882e-02, -3.0039e-01],
        [ 1.5326e+00,  1.7105e+00,  7.1651e-02,  7.1651e-02,  7.1651e-02,
         -8.7499e+00,  1.0004e+01,  1.2117e+01,  7.1651e-02, -3.3101e+00],
        [-6.5075e-01, -6.4857e-01, -2.1882e-02, -2.1882e-02, -2.1882e-02,
         -3.1052e-01, -1.1155e-01,  1.3993e-02, -2.1882e-02, -3.0039e-01],
        [-6.5075e-01, -6.4857e-01, -2.1882e-02, -2.1882e-02, -2.1882e-02,
         -3.1052e-01, -1.1155e-01,  1.3993e-02, -2.1882e-02, -3.0039e-01],
        [-6.5075e-01, -6.4857e-01, -2.1882e-02, -2.1882e-02, -2.1882e-02,
         -3.1052e-01, -1.1155e-01,  1.3993e-02, -2.1882e-02, -3.0039e-01],
        [-6.5075e-01, -6.4857e-01, -2.1882e-02, -2.1882e-02, -2.1882e-02,
         -3.1052e-01, -1.1155e-01,  1.3993e-02, -2.1882e-02, -3.0039e-01],
        [-6.5075e-01, -6.4857e-01, -2.1882e-02, -2.1882e-02, -2.1882e-02,
         -3.1052e-01, -1.1155e-01,  1.3993e-02, -2.1882e-02, -3.0039e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.6553, -0.3403, -2.5267, -0.6553, -0.6960, -0.6553, -0.6553, -0.6553,
        -0.6553, -0.6553], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0153, -11.0796,  12.8151,  -0.0153,  -6.1410,  -0.0153,  -0.0153,
          -0.0153,  -0.0153,  -0.0153]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  1.2650,   0.3512],
        [  0.0918,   1.5418],
        [  6.3455,   9.8287],
        [ -4.5230,   9.7384],
        [-12.8493,  -1.5793],
        [ 17.8691,   8.4248],
        [-16.4041, -12.3591],
        [-15.5023,  -3.5395],
        [ 13.7700,  -3.5361],
        [-12.2830,   4.6490]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  4.2318,  -3.0703,   9.2314,  24.9200,   7.8249,   5.8896, -11.2169,
         -0.0595, -15.7862,   6.7795], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.2997e+00, -2.0676e-01,  2.3689e-03, -2.8640e+00, -1.1867e+00,
         -3.1973e+00,  1.2972e+00,  7.9184e-02,  1.5376e+00, -3.7973e-02],
        [-8.9949e+00, -1.1770e-01, -9.8855e+00,  3.3281e+00, -1.9963e+00,
         -1.2640e+01,  1.6831e+01,  1.3555e+01, -4.6470e+00, -5.5358e+00],
        [ 1.2315e-01, -2.4542e-01, -3.9543e+00,  3.3090e+00,  4.0371e+00,
         -5.3906e+00,  1.5524e+01,  1.8635e+00, -1.3849e+01, -1.4950e+01],
        [-1.5520e+00,  2.3557e+00, -8.4267e-01, -8.6163e-01,  9.4663e-01,
         -6.5782e-01, -5.2780e+00,  2.3616e+00,  9.4731e-01,  9.4729e-01],
        [-1.6136e+00, -1.8862e-01, -8.8050e-01, -1.3917e+00, -6.8947e-01,
         -1.5015e+00, -2.1649e-01, -1.2937e-02, -5.1213e-01, -2.0258e-01],
        [-4.0808e-01, -4.6569e-01, -3.6844e-03, -5.2848e+00, -1.3247e+00,
         -3.6420e+00,  1.2695e+00, -5.3845e-01,  1.3329e+00, -1.5792e-02],
        [-1.8272e+00,  6.8210e-01, -3.7448e+00,  9.6519e-01,  2.8391e+00,
         -1.0153e+00,  6.9068e-01, -2.9290e+00,  5.1922e-01, -5.1157e+00],
        [-1.9680e+00, -2.6551e-01,  3.6174e-01, -3.2592e+00, -3.2525e-01,
         -3.8291e+00,  1.8011e+00,  4.2829e-01,  1.6015e+00,  3.6723e-01],
        [-2.0762e+00, -1.6770e+00,  6.1840e-01, -3.9203e+00, -1.2041e+00,
          1.3016e+00,  1.1952e+00, -1.1646e+00,  6.1830e+00,  1.0647e-01],
        [ 6.4841e-02, -4.1762e+00,  5.8507e+00, -1.9681e+00,  4.3556e+00,
         -2.7493e-02, -3.4297e+01,  4.7068e+00,  1.4538e+01,  1.9973e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.6266, -8.8154, -5.4064, -1.6762, -1.6384, -1.7271, -2.1005, -2.0354,
        -2.9538, -4.2311], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.8406,  12.8334,  -5.4653,  -1.2602,  -0.0129,   1.6505,  -1.3998,
           0.4322,   0.4573,   0.3519],
        [ -0.8405, -12.8271,   5.4701,   1.2602,   0.0129,  -1.6502,   1.3998,
          -0.4327,  -0.4573,  -0.3519]], device='cuda:0'))])
loaded xi:  0.00938538
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1439.2724125859495
W_T_median: 1086.6311344566764
W_T_pctile_5: -114.4532091189134
W_T_CVAR_5_pct: -274.67719995288854
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1447.3355911098286
Current xi:  [-0.03668321]
objective value function right now is: -1447.3355911098286
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03628243]
objective value function right now is: -1445.6365920360947
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00805248]
objective value function right now is: -1444.6200351763716
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1449.628386994539
Current xi:  [-0.01973733]
objective value function right now is: -1449.628386994539
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1450.8461493311436
Current xi:  [0.02260866]
objective value function right now is: -1450.8461493311436
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06119167]
objective value function right now is: -1446.7203996866695
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01072664]
objective value function right now is: -1446.6552109793242
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.10441855]
objective value function right now is: -1450.5167790203882
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.09439731]
objective value function right now is: -1445.8759099080462
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.7948124]
objective value function right now is: -1450.8049443235234
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1469.7653824446197
Current xi:  [17.30767]
objective value function right now is: -1469.7653824446197
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1490.5915159962121
Current xi:  [42.509945]
objective value function right now is: -1490.5915159962121
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1522.2795909558236
Current xi:  [67.43098]
objective value function right now is: -1522.2795909558236
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1549.1201195057943
Current xi:  [93.6296]
objective value function right now is: -1549.1201195057943
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1558.3055560992552
Current xi:  [116.80389]
objective value function right now is: -1558.3055560992552
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [135.8326]
objective value function right now is: -1554.6344212934453
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1576.2824385531333
Current xi:  [151.12018]
objective value function right now is: -1576.2824385531333
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1576.7264149120665
Current xi:  [161.67975]
objective value function right now is: -1576.7264149120665
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.58212]
objective value function right now is: -1573.8034880194584
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1580.2841420125399
Current xi:  [167.97179]
objective value function right now is: -1580.2841420125399
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.63599]
objective value function right now is: -1570.8574404914389
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [172.13574]
objective value function right now is: -1578.830804256333
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.94101]
objective value function right now is: -1580.150947288226
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1581.6621848990158
Current xi:  [171.55478]
objective value function right now is: -1581.6621848990158
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [171.63965]
objective value function right now is: -1580.0709528654083
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [171.76949]
objective value function right now is: -1577.7638384744278
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [172.16187]
objective value function right now is: -1579.7232945522521
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [173.05957]
objective value function right now is: -1574.593206568654
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [172.1071]
objective value function right now is: -1575.1634239632442
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [172.52406]
objective value function right now is: -1578.0399076154804
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1581.920936347738
Current xi:  [172.19539]
objective value function right now is: -1581.920936347738
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [171.90459]
objective value function right now is: -1575.6351676744032
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [172.55774]
objective value function right now is: -1579.2204497137955
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [171.35474]
objective value function right now is: -1579.670078825851
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.59302]
objective value function right now is: -1567.4762623238466
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.70966]
objective value function right now is: -1577.5682506549454
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1584.5484775907712
Current xi:  [168.93652]
objective value function right now is: -1584.5484775907712
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1585.5607326767565
Current xi:  [169.52014]
objective value function right now is: -1585.5607326767565
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1586.145457508031
Current xi:  [170.09715]
objective value function right now is: -1586.145457508031
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.86156]
objective value function right now is: -1585.6894692916499
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1586.4319799772177
Current xi:  [171.49097]
objective value function right now is: -1586.4319799772177
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [171.617]
objective value function right now is: -1585.5875734075894
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [171.85667]
objective value function right now is: -1585.8605695686244
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [171.58687]
objective value function right now is: -1586.142752110218
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [171.85638]
objective value function right now is: -1586.0408636424845
new min fval from sgd:  -1586.5336069690416
new min fval from sgd:  -1586.656301536016
new min fval from sgd:  -1586.7874425855814
new min fval from sgd:  -1586.9111591365167
new min fval from sgd:  -1587.0079201469825
new min fval from sgd:  -1587.0496352323216
new min fval from sgd:  -1587.0816986064954
new min fval from sgd:  -1587.0847520300686
new min fval from sgd:  -1587.0956768441738
new min fval from sgd:  -1587.1220847879272
new min fval from sgd:  -1587.1513062087497
new min fval from sgd:  -1587.1628933989484
new min fval from sgd:  -1587.1640308834264
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [172.01521]
objective value function right now is: -1586.6416656172107
new min fval from sgd:  -1587.1697388493542
new min fval from sgd:  -1587.19841037397
new min fval from sgd:  -1587.2262043789265
new min fval from sgd:  -1587.2364807283986
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [172.19325]
objective value function right now is: -1587.099903269676
new min fval from sgd:  -1587.2446870298222
new min fval from sgd:  -1587.2466779889523
new min fval from sgd:  -1587.2499276124681
new min fval from sgd:  -1587.2582522189914
new min fval from sgd:  -1587.2974722740628
new min fval from sgd:  -1587.3495587953307
new min fval from sgd:  -1587.3543220988563
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [172.40472]
objective value function right now is: -1585.4860548085346
new min fval from sgd:  -1587.3647075918943
new min fval from sgd:  -1587.3889812526768
new min fval from sgd:  -1587.456298696557
new min fval from sgd:  -1587.4605208043074
new min fval from sgd:  -1587.4611193352007
new min fval from sgd:  -1587.4666682669713
new min fval from sgd:  -1587.4697725890935
new min fval from sgd:  -1587.4757605040977
new min fval from sgd:  -1587.4787246597211
new min fval from sgd:  -1587.481454031281
new min fval from sgd:  -1587.4921959468948
new min fval from sgd:  -1587.5032583549284
new min fval from sgd:  -1587.5111565518275
new min fval from sgd:  -1587.5171675723
new min fval from sgd:  -1587.5253094584966
new min fval from sgd:  -1587.5339108293917
new min fval from sgd:  -1587.5406461586906
new min fval from sgd:  -1587.54838632557
new min fval from sgd:  -1587.5549378868118
new min fval from sgd:  -1587.5645354200283
new min fval from sgd:  -1587.567875309532
new min fval from sgd:  -1587.5711673683102
new min fval from sgd:  -1587.581014987593
new min fval from sgd:  -1587.5917000178842
new min fval from sgd:  -1587.6014154574266
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [173.1925]
objective value function right now is: -1587.4833184766694
new min fval from sgd:  -1587.6217732867424
new min fval from sgd:  -1587.6260213444766
new min fval from sgd:  -1587.6281351324944
new min fval from sgd:  -1587.6325827781782
new min fval from sgd:  -1587.6413211303743
new min fval from sgd:  -1587.6500955496988
new min fval from sgd:  -1587.6577000306775
new min fval from sgd:  -1587.664139938727
new min fval from sgd:  -1587.667525470311
new min fval from sgd:  -1587.6724393727834
new min fval from sgd:  -1587.6743134508315
new min fval from sgd:  -1587.6773790435473
new min fval from sgd:  -1587.6774468412639
new min fval from sgd:  -1587.6797346055728
new min fval from sgd:  -1587.6810697315457
new min fval from sgd:  -1587.6826356067788
new min fval from sgd:  -1587.6832084394307
new min fval from sgd:  -1587.6832943398604
new min fval from sgd:  -1587.6869045891822
new min fval from sgd:  -1587.6880870938958
new min fval from sgd:  -1587.6932590844856
new min fval from sgd:  -1587.6982070482593
new min fval from sgd:  -1587.6994850238948
new min fval from sgd:  -1587.7040771249756
new min fval from sgd:  -1587.7114258556846
new min fval from sgd:  -1587.720768264787
new min fval from sgd:  -1587.7288406147045
new min fval from sgd:  -1587.7337856992453
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [173.19722]
objective value function right now is: -1587.5730456332512
min fval:  -1587.7337856992453
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  1.4375,  -0.3778],
        [  1.7405,  -0.3984],
        [ -1.2847,   0.5117],
        [ -1.2847,   0.5117],
        [ -1.2847,   0.5117],
        [-23.2145,   1.9576],
        [ 14.3943, -11.6583],
        [ -4.3903, -14.9793],
        [ -1.2847,   0.5117],
        [-18.1982,   1.2017]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  4.5428,   3.6631,  -3.1070,  -3.1070,  -3.1070,  17.5939, -11.3304,
        -11.9495,  -3.1070,  13.9510], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-8.0069e-01, -7.9631e-01, -9.7078e-03, -9.7078e-03, -9.7078e-03,
         -4.3679e-01, -1.8447e-01, -4.2241e-02, -9.7078e-03, -4.3351e-01],
        [ 2.0926e+00,  2.1998e+00,  2.1658e-01,  2.1658e-01,  2.1658e-01,
         -1.6208e+01,  1.7288e+01,  1.5616e+01,  2.1658e-01, -4.8332e+00],
        [-2.6691e+00, -5.1832e-01,  1.7965e-01,  1.7965e-01,  1.7965e-01,
          1.3726e+01, -1.5488e+01, -1.6681e+01,  1.7965e-01,  4.3313e+00],
        [-8.0069e-01, -7.9631e-01, -9.7078e-03, -9.7078e-03, -9.7078e-03,
         -4.3679e-01, -1.8447e-01, -4.2241e-02, -9.7078e-03, -4.3351e-01],
        [-1.0730e-01, -6.1970e-02,  1.9040e-01,  1.9040e-01,  1.9040e-01,
         -5.7292e+00, -2.5064e+01,  2.1733e+01,  1.9040e-01, -1.6837e+00],
        [-8.0069e-01, -7.9631e-01, -9.7078e-03, -9.7078e-03, -9.7078e-03,
         -4.3679e-01, -1.8447e-01, -4.2241e-02, -9.7078e-03, -4.3351e-01],
        [-8.0069e-01, -7.9631e-01, -9.7078e-03, -9.7078e-03, -9.7078e-03,
         -4.3679e-01, -1.8447e-01, -4.2241e-02, -9.7078e-03, -4.3351e-01],
        [-8.0069e-01, -7.9631e-01, -9.7078e-03, -9.7078e-03, -9.7078e-03,
         -4.3679e-01, -1.8447e-01, -4.2241e-02, -9.7078e-03, -4.3351e-01],
        [-8.0069e-01, -7.9631e-01, -9.7078e-03, -9.7078e-03, -9.7078e-03,
         -4.3679e-01, -1.8447e-01, -4.2241e-02, -9.7078e-03, -4.3351e-01],
        [-8.0069e-01, -7.9631e-01, -9.7078e-03, -9.7078e-03, -9.7078e-03,
         -4.3679e-01, -1.8447e-01, -4.2241e-02, -9.7078e-03, -4.3351e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8048, -0.0888, -1.6513, -0.8048, -1.6477, -0.8048, -0.8048, -0.8048,
        -0.8048, -0.8048], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0236, -15.0327,  14.6943,  -0.0236, -13.1863,  -0.0236,  -0.0236,
          -0.0236,  -0.0236,  -0.0236]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -6.0957,  -8.4957],
        [  0.8002,   3.7843],
        [  2.3106,  13.9821],
        [ -2.4631,   6.7792],
        [-18.6799,  -1.6292],
        [ 16.5741,  11.4748],
        [-19.9886, -11.9226],
        [-22.3845,  -4.7535],
        [ 16.3362,  -2.4055],
        [-16.5295,  10.6751]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -2.8243,  -2.1988,  12.1489,  24.6994,  12.3345,   5.8143,  -9.5439,
          2.1475, -19.3408,   9.9517], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -0.5771,  -1.9307,  -0.6448,  -2.2382,  -3.4194,  -2.4017,   0.5134,
           0.6573,   3.0723,  -0.5028],
        [-12.6125,  -0.3165, -26.9235,   4.3783,  -3.2663,  -1.7385,  20.4554,
           8.2010,  -6.3963,   3.3470],
        [ -2.2096,  -1.1753, -14.0666,   3.2536,   6.0292,  -4.3005,  12.0483,
           0.3537, -14.8268,  -6.0557],
        [  0.1451,  -5.6654,   1.9069,  -2.4348,   0.4352,   0.7165,   1.8654,
           1.5748,   1.1857,   2.9741],
        [ -0.8749,  -1.4515,  -0.7423,  -2.6328,  -3.3632,  -2.0387,   0.6077,
           0.4411,   2.6180,  -0.5688],
        [ -1.4958,  -0.7834,  -0.8132,  -3.0602,  -1.6849,  -1.4325,   0.3082,
           0.0390,   1.3887,  -0.4431],
        [  0.1370,  -2.9217,   3.1690,  -3.4979,  -1.0010,  -2.0207,  -0.0846,
          -0.5013,   0.4737,   0.5754],
        [ -0.4155,  -2.1773,  -0.6580,  -1.9773,  -3.5275,  -2.5522,   0.4462,
           0.5151,   3.2764,  -0.4759],
        [ -0.5836,   0.0565,  -0.5413,  -2.9593,  -0.2518,  -0.7318,   0.1858,
          -0.1330,  -0.1275,  -0.0858],
        [  1.4487,  -3.3775,   7.6952,  -1.1672,  -3.4194,   5.8575, -24.7937,
           4.1733,   0.5762,  26.6797]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -2.2041, -12.5530,  -7.7057,  -2.8691,  -2.5203,  -2.9194,  -4.7343,
         -2.0243,  -3.8041,  -3.4467], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.6905,  14.9191,  -6.8886,   0.8938,   1.2509,   0.2170,   0.4034,
           1.9017,  -0.0989,   0.3362],
        [ -1.6905, -14.9088,   6.9009,  -0.8938,  -1.2509,  -0.2170,  -0.4032,
          -1.9017,   0.0989,  -0.3361]], device='cuda:0'))])
xi:  [173.1952]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 604.574686163255
W_T_median: 364.4113804960638
W_T_pctile_5: 173.1989191069671
W_T_CVAR_5_pct: 20.899757188068207
Average q (qsum/M+1):  49.194647019909276
Optimal xi:  [173.1952]
Expected(across Rb) median(across samples) p_equity:  0.27486174429456395
obj fun:  tensor(-1587.7338, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  1.4375,  -0.3778],
        [  1.7405,  -0.3984],
        [ -1.2847,   0.5117],
        [ -1.2847,   0.5117],
        [ -1.2847,   0.5117],
        [-23.2145,   1.9576],
        [ 14.3943, -11.6583],
        [ -4.3903, -14.9793],
        [ -1.2847,   0.5117],
        [-18.1982,   1.2017]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  4.5428,   3.6631,  -3.1070,  -3.1070,  -3.1070,  17.5939, -11.3304,
        -11.9495,  -3.1070,  13.9510], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-8.0069e-01, -7.9631e-01, -9.7078e-03, -9.7078e-03, -9.7078e-03,
         -4.3679e-01, -1.8447e-01, -4.2241e-02, -9.7078e-03, -4.3351e-01],
        [ 2.0926e+00,  2.1998e+00,  2.1658e-01,  2.1658e-01,  2.1658e-01,
         -1.6208e+01,  1.7288e+01,  1.5616e+01,  2.1658e-01, -4.8332e+00],
        [-2.6691e+00, -5.1832e-01,  1.7965e-01,  1.7965e-01,  1.7965e-01,
          1.3726e+01, -1.5488e+01, -1.6681e+01,  1.7965e-01,  4.3313e+00],
        [-8.0069e-01, -7.9631e-01, -9.7078e-03, -9.7078e-03, -9.7078e-03,
         -4.3679e-01, -1.8447e-01, -4.2241e-02, -9.7078e-03, -4.3351e-01],
        [-1.0730e-01, -6.1970e-02,  1.9040e-01,  1.9040e-01,  1.9040e-01,
         -5.7292e+00, -2.5064e+01,  2.1733e+01,  1.9040e-01, -1.6837e+00],
        [-8.0069e-01, -7.9631e-01, -9.7078e-03, -9.7078e-03, -9.7078e-03,
         -4.3679e-01, -1.8447e-01, -4.2241e-02, -9.7078e-03, -4.3351e-01],
        [-8.0069e-01, -7.9631e-01, -9.7078e-03, -9.7078e-03, -9.7078e-03,
         -4.3679e-01, -1.8447e-01, -4.2241e-02, -9.7078e-03, -4.3351e-01],
        [-8.0069e-01, -7.9631e-01, -9.7078e-03, -9.7078e-03, -9.7078e-03,
         -4.3679e-01, -1.8447e-01, -4.2241e-02, -9.7078e-03, -4.3351e-01],
        [-8.0069e-01, -7.9631e-01, -9.7078e-03, -9.7078e-03, -9.7078e-03,
         -4.3679e-01, -1.8447e-01, -4.2241e-02, -9.7078e-03, -4.3351e-01],
        [-8.0069e-01, -7.9631e-01, -9.7078e-03, -9.7078e-03, -9.7078e-03,
         -4.3679e-01, -1.8447e-01, -4.2241e-02, -9.7078e-03, -4.3351e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8048, -0.0888, -1.6513, -0.8048, -1.6477, -0.8048, -0.8048, -0.8048,
        -0.8048, -0.8048], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0236, -15.0327,  14.6943,  -0.0236, -13.1863,  -0.0236,  -0.0236,
          -0.0236,  -0.0236,  -0.0236]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -6.0957,  -8.4957],
        [  0.8002,   3.7843],
        [  2.3106,  13.9821],
        [ -2.4631,   6.7792],
        [-18.6799,  -1.6292],
        [ 16.5741,  11.4748],
        [-19.9886, -11.9226],
        [-22.3845,  -4.7535],
        [ 16.3362,  -2.4055],
        [-16.5295,  10.6751]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -2.8243,  -2.1988,  12.1489,  24.6994,  12.3345,   5.8143,  -9.5439,
          2.1475, -19.3408,   9.9517], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -0.5771,  -1.9307,  -0.6448,  -2.2382,  -3.4194,  -2.4017,   0.5134,
           0.6573,   3.0723,  -0.5028],
        [-12.6125,  -0.3165, -26.9235,   4.3783,  -3.2663,  -1.7385,  20.4554,
           8.2010,  -6.3963,   3.3470],
        [ -2.2096,  -1.1753, -14.0666,   3.2536,   6.0292,  -4.3005,  12.0483,
           0.3537, -14.8268,  -6.0557],
        [  0.1451,  -5.6654,   1.9069,  -2.4348,   0.4352,   0.7165,   1.8654,
           1.5748,   1.1857,   2.9741],
        [ -0.8749,  -1.4515,  -0.7423,  -2.6328,  -3.3632,  -2.0387,   0.6077,
           0.4411,   2.6180,  -0.5688],
        [ -1.4958,  -0.7834,  -0.8132,  -3.0602,  -1.6849,  -1.4325,   0.3082,
           0.0390,   1.3887,  -0.4431],
        [  0.1370,  -2.9217,   3.1690,  -3.4979,  -1.0010,  -2.0207,  -0.0846,
          -0.5013,   0.4737,   0.5754],
        [ -0.4155,  -2.1773,  -0.6580,  -1.9773,  -3.5275,  -2.5522,   0.4462,
           0.5151,   3.2764,  -0.4759],
        [ -0.5836,   0.0565,  -0.5413,  -2.9593,  -0.2518,  -0.7318,   0.1858,
          -0.1330,  -0.1275,  -0.0858],
        [  1.4487,  -3.3775,   7.6952,  -1.1672,  -3.4194,   5.8575, -24.7937,
           4.1733,   0.5762,  26.6797]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -2.2041, -12.5530,  -7.7057,  -2.8691,  -2.5203,  -2.9194,  -4.7343,
         -2.0243,  -3.8041,  -3.4467], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.6905,  14.9191,  -6.8886,   0.8938,   1.2509,   0.2170,   0.4034,
           1.9017,  -0.0989,   0.3362],
        [ -1.6905, -14.9088,   6.9009,  -0.8938,  -1.2509,  -0.2170,  -0.4032,
          -1.9017,   0.0989,  -0.3361]], device='cuda:0'))])
loaded xi:  173.1952
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1439.2724125859495
W_T_median: 1086.6311344566764
W_T_pctile_5: -114.4532091189134
W_T_CVAR_5_pct: -274.67719995288854
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1625.8629455316554
Current xi:  [182.1079]
objective value function right now is: -1625.8629455316554
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.36484]
objective value function right now is: -1623.7232520090436
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1633.6245000060217
Current xi:  [188.48564]
objective value function right now is: -1633.6245000060217
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.6571]
objective value function right now is: -1620.0847375657975
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.91968]
objective value function right now is: -1621.7163720892524
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.10059]
objective value function right now is: -1625.9411356452413
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [191.79877]
objective value function right now is: -1622.7266594315442
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.4236]
objective value function right now is: -1626.6724013210346
