Starting at: 
10-01-23_11:57

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1  logistic_sigmoid   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1720.643270841434
Current xi:  [-101.933334]
objective value function right now is: -1720.643270841434
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1742.6353273150823
Current xi:  [-207.81201]
objective value function right now is: -1742.6353273150823
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1759.409868632432
Current xi:  [-309.0661]
objective value function right now is: -1759.409868632432
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1772.427376459845
Current xi:  [-404.7304]
objective value function right now is: -1772.427376459845
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1782.547947300817
Current xi:  [-493.8874]
objective value function right now is: -1782.547947300817
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1789.3754666070315
Current xi:  [-574.77313]
objective value function right now is: -1789.3754666070315
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1794.6043589120316
Current xi:  [-650.6228]
objective value function right now is: -1794.6043589120316
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1798.0239505416448
Current xi:  [-718.40857]
objective value function right now is: -1798.0239505416448
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1799.8846945115304
Current xi:  [-776.7895]
objective value function right now is: -1799.8846945115304
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1801.6916768924689
Current xi:  [-827.68066]
objective value function right now is: -1801.6916768924689
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.2133389596884
Current xi:  [-867.4703]
objective value function right now is: -1802.2133389596884
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.271010322717
Current xi:  [-896.9046]
objective value function right now is: -1802.271010322717
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-916.6961]
objective value function right now is: -1801.9680773207085
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1802.28087867745
Current xi:  [-926.2767]
objective value function right now is: -1802.28087867745
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.5237517279638
Current xi:  [-928.7072]
objective value function right now is: -1802.5237517279638
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.6137533726278
Current xi:  [-930.9213]
objective value function right now is: -1802.6137533726278
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-928.1423]
objective value function right now is: -1802.5227263472607
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-928.1632]
objective value function right now is: -1802.532428299855
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-931.3787]
objective value function right now is: -1802.4645903234127
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-928.2226]
objective value function right now is: -1802.2696841381378
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-929.1545]
objective value function right now is: -1802.55122894197
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-928.3108]
objective value function right now is: -1802.5486686016834
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.68668083009
Current xi:  [-925.25653]
objective value function right now is: -1802.68668083009
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-927.65967]
objective value function right now is: -1802.5561582728835
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-929.7063]
objective value function right now is: -1802.6182090558823
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-937.4587]
objective value function right now is: -1801.8178058923127
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.717209349069
Current xi:  [-929.4559]
objective value function right now is: -1802.717209349069
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-926.8325]
objective value function right now is: -1802.5280785733294
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-927.59265]
objective value function right now is: -1802.566395964745
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-927.0871]
objective value function right now is: -1801.3783158658855
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-930.3712]
objective value function right now is: -1802.5279947140002
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-926.8231]
objective value function right now is: -1802.5256834144366
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-934.7459]
objective value function right now is: -1802.2044811303401
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-926.949]
objective value function right now is: -1802.437326266875
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-933.7971]
objective value function right now is: -1802.6451743201287
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-931.3112]
objective value function right now is: -1802.6834349738651
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-929.55756]
objective value function right now is: -1802.676179469594
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-927.4143]
objective value function right now is: -1802.466301722935
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-928.6684]
objective value function right now is: -1802.4781948607974
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-921.5725]
objective value function right now is: -1802.4405273565467
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-928.04065]
objective value function right now is: -1802.5483501599874
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-927.6218]
objective value function right now is: -1802.581689765962
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-931.3908]
objective value function right now is: -1802.4905661498972
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-924.31024]
objective value function right now is: -1802.3705974805061
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-928.20447]
objective value function right now is: -1802.5123568141605
new min fval from sgd:  -1802.730461868269
new min fval from sgd:  -1802.7559910300747
new min fval from sgd:  -1802.7709841729454
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-954.53326]
objective value function right now is: -1801.1333578095291
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-957.1182]
objective value function right now is: -1800.983663928942
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-934.27576]
objective value function right now is: -1801.8250540135975
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-925.3714]
objective value function right now is: -1802.5658575034574
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-951.5507]
objective value function right now is: -1801.015745767859
min fval:  -1802.7709841729454
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-3.2185,  1.9863],
        [13.7141,  6.7760],
        [-2.8778,  2.0886],
        [-2.9463,  2.0677],
        [18.4812,  7.8737],
        [-3.2092,  1.9893],
        [-2.8471,  2.0979],
        [ 2.5044, -1.8827],
        [-2.6451,  2.1596],
        [ 2.2508, -1.8061]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-62.9553, -23.0400, -64.9122, -64.5493, -11.7498, -63.3970, -65.0364,
           1.1845, -64.8143,   2.5681],
        [ 66.5224,  21.5585,  67.8582,  68.1965,  13.8203,  66.0288,  67.3561,
          -3.7968,  66.2025,  -1.9074],
        [ 15.6133,   4.8303,  19.4304,  19.4832,   3.5082,  14.2455,  19.3873,
         -10.1354,  19.6914,  -8.3310],
        [ -3.0375,  -2.6315,  -2.1832,  -2.2432,  -2.3369,  -3.7373,  -2.3942,
          -9.7831,  -2.0821,  -7.9567],
        [ 55.7229,  24.3736,  58.2158,  58.4234,  10.9556,  54.6578,  57.7324,
          -7.5823,  58.1434,  -6.1164],
        [  1.4582,   2.4261,  -2.7569,  -2.4366,   2.1701,   1.1868,  -2.9525,
           7.4337,  -3.3890,  10.5257],
        [ 61.6467,  24.5905,  63.1373,  63.5955,  12.0159,  60.7571,  63.1306,
          -5.0023,  62.8254,  -3.6919],
        [ -1.8764,  -1.6291,  -1.0378,  -1.1654,  -1.8577,  -2.3219,  -1.1969,
          -9.2762,  -1.1629,  -8.7005],
        [ 66.4686,  21.4784,  68.2718,  68.5156,  15.0030,  66.2508,  68.1535,
          -3.7498,  66.6498,  -1.6378],
        [ 66.5548,  21.0872,  67.4571,  68.1362,  13.7737,  65.4085,  67.2533,
          -3.7833,  66.1409,  -1.9948]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-18.4953,  20.6154,   3.4575,  -0.1478,  12.6364,  -7.8214,  16.2437,
           0.2207,  23.6254,  20.6368]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 22.8517,   9.0493],
        [ -4.3213,   1.8833],
        [ -4.4929,   1.8049],
        [ -4.5005,   1.7713],
        [ -4.3185,   1.8839],
        [ 14.5493,   8.0390],
        [ -3.5585,   2.4024],
        [ -6.5953,   0.5817],
        [-13.1995,  -1.3001],
        [ -3.3965,   2.4638]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -27.3744,    0.3583,    5.0904,    6.3473,    0.1999,  -94.0961,
          -19.3406,   22.2169,  -11.6317,  -29.8275],
        [ -68.6457,  -14.0223,  -11.4480,  -17.6215,  -13.7614, -153.8385,
          -14.7476,   -4.6459,  -13.4574,  -23.4450],
        [  -4.8486,  -40.5830,  -55.8809,  -32.8324,  -40.8277,   -4.7909,
          -74.5342,  -27.8186,  -92.4156,  -80.1821],
        [ -11.4057,  -89.5561,  -96.3384,  -82.0795,  -89.7029,  -30.1740,
         -115.2626,  -67.3422, -101.1935, -113.1874],
        [  -7.2449,  -78.1298,  -93.0024,  -66.5627,  -78.4205,   -3.1797,
         -101.2267,  -55.6299, -106.1041,  -95.0784],
        [-115.2879,   -0.3262,   12.9479,    7.9866,   -0.6004, -180.3434,
          -31.3910,   35.5662,  -10.4365,  -39.4238],
        [  -3.4237,  -22.3161,  -35.7068,  -14.8330,  -22.3003,   -6.4309,
          -57.7415,  -12.4679,  -68.0064,  -62.9853],
        [ -10.2642,  -82.0906,  -87.8578,  -76.0172,  -82.4009,  -54.6160,
          -99.6696,  -64.7446,  -88.3262,  -97.0247],
        [  -8.1951,  -50.8827,  -55.6826,  -46.0832,  -51.3421,  -68.9128,
          -56.9513,  -48.5731,  -74.9951,  -49.6989],
        [  -2.5569,   78.9777,  128.1208,   72.0232,   79.1672,    2.6060,
           46.3047,   70.1753,    1.2288,   40.1234]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-22.4303,  30.3426,  -6.1467, -11.8827, -13.5702,  40.5707,  -6.7858,
         -14.8649, -17.5505,   0.6992],
        [ 22.1020, -30.2276,   6.1636,  11.8649,  14.0560, -40.5071,   6.7530,
          15.0312,  17.5267,  -0.6293]], device='cuda:0'))])
xi:  [-927.8088]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: -260.0238742724366
W_T_median: -393.5029117552348
W_T_pctile_5: -924.665315372758
W_T_CVAR_5_pct: -1050.4255280640675
Average q (qsum/M+1):  59.84818784652218
Optimal xi:  [-927.8088]
Expected(across Rb) median(across samples) p_equity:  0.16533439599754532
obj fun:  tensor(-1802.7710, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1658.2295206211852
Current xi:  [-61.899086]
objective value function right now is: -1658.2295206211852
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1674.7414333238323
Current xi:  [-129.08305]
objective value function right now is: -1674.7414333238323
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1684.096140602002
Current xi:  [-191.86478]
objective value function right now is: -1684.096140602002
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1688.5715881213644
Current xi:  [-252.16997]
objective value function right now is: -1688.5715881213644
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1693.1770901902466
Current xi:  [-305.90967]
objective value function right now is: -1693.1770901902466
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1695.8413269540022
Current xi:  [-345.3105]
objective value function right now is: -1695.8413269540022
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1697.8851336083326
Current xi:  [-373.5725]
objective value function right now is: -1697.8851336083326
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-405.49637]
objective value function right now is: -1691.1374782494236
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-426.9394]
objective value function right now is: -1692.6386200852776
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-437.5837]
objective value function right now is: -1682.5379723540689
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-448.39444]
objective value function right now is: -1680.837528798084
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-454.56268]
objective value function right now is: -1683.223684785384
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-452.65802]
objective value function right now is: -1682.6409470365807
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-452.21542]
objective value function right now is: -1682.6196586079086
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-501.29834]
objective value function right now is: -1555.5830354553443
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-585.1608]
objective value function right now is: -1620.5592964404686
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-602.98773]
objective value function right now is: -1678.4242129316128
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-578.5432]
objective value function right now is: -1679.4225330437696
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-545.37585]
objective value function right now is: -1685.6630733740828
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-519.53656]
objective value function right now is: -1687.1662230323175
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-510.47342]
objective value function right now is: -1687.4734519115243
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-506.2469]
objective value function right now is: -1651.679005184963
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-504.7404]
objective value function right now is: -1683.7401278276423
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-484.84622]
objective value function right now is: -1684.8705055906867
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-479.05743]
objective value function right now is: -1685.7574794657091
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.59833]
objective value function right now is: -1687.2956144905227
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-481.5834]
objective value function right now is: -1681.7092963254418
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-484.742]
objective value function right now is: -1687.4571181867022
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-496.8632]
objective value function right now is: -1676.4079816244412
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.80072]
objective value function right now is: -1685.5775300598248
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-477.82294]
objective value function right now is: -1687.6796967824785
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-477.44418]
objective value function right now is: -1681.9171919691637
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-468.8763]
objective value function right now is: -1684.6801882558505
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-481.57132]
objective value function right now is: -1682.8522508395383
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-471.21793]
objective value function right now is: -1686.0106496170217
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.6041]
objective value function right now is: -1687.4381831831258
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-484.39825]
objective value function right now is: -1684.3252236404023
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.59406]
objective value function right now is: -1687.9496864725575
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-466.7724]
objective value function right now is: -1687.9560334106652
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-465.73416]
objective value function right now is: -1686.682326398054
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-469.82776]
objective value function right now is: -1685.1757378125933
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-487.40915]
objective value function right now is: -1688.0954200471933
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.45444]
objective value function right now is: -1680.9972635576294
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-477.97122]
objective value function right now is: -1683.8734348529447
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.08694]
objective value function right now is: -1684.1309660403456
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-475.69043]
objective value function right now is: -1688.6552739011877
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-477.74393]
objective value function right now is: -1687.8883047995416
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-471.52286]
objective value function right now is: -1686.726652722641
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-470.77487]
objective value function right now is: -1688.6768373197522
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-468.88416]
objective value function right now is: -1685.0261700505093
min fval:  -1684.6461768071197
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.5933,  3.6025],
        [13.2820,  9.2231],
        [ 0.5860,  3.5786],
        [ 0.5865,  3.5830],
        [15.0829, 10.1053],
        [ 0.5946,  3.6018],
        [ 0.5861,  3.5767],
        [-0.4288, -3.1814],
        [ 0.5832,  3.5650],
        [-0.4582, -3.0660]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-71.8880, -23.7981, -74.1194, -73.7010,  -5.1545, -72.3390, -74.2689,
           3.0422, -74.2045,   4.4210],
        [ 74.7528,  16.1609,  76.3898,  76.6686,  -0.7517,  74.2658,  75.9139,
          -6.2274,  74.9359,  -4.3329],
        [ 15.4283,   4.6569,  19.2497,  19.3016,   0.9469,  14.0606,  19.2070,
         -12.0560,  19.5139, -10.2483],
        [ -3.6725,  -2.7737,  -2.8485,  -2.9024,   0.2092,  -4.3731,  -3.0622,
         -10.4675,  -2.7680,  -8.6336],
        [ 59.4148,  17.8997,  62.6877,  62.7441,  -2.5731,  58.3682,  62.2717,
         -11.4369,  63.1243,  -9.9801],
        [  1.4582,   2.4261,  -2.7569,  -2.4366,   2.1701,   1.1868,  -2.9525,
           7.4337,  -3.3890,  10.5257],
        [ 67.5941,  21.7325,  69.8254,  70.1375,  -2.0472,  66.7217,  69.8845,
          -8.0441,  70.0133,  -6.7443],
        [ -1.9635,  -1.6609,  -1.1301,  -1.2566,  -1.3994,  -2.4092,  -1.2896,
         -10.3085,  -1.2586,  -9.7270],
        [ 73.4668,  13.4777,  75.3525,  75.5789,  -1.2539,  73.2505,  75.2426,
          -7.8396,  73.7937,  -5.7185],
        [ 75.1301,  20.6191,  76.3502,  76.9661,   3.3691,  73.9917,  76.1745,
          -5.7019,  75.2476,  -3.9088]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-21.4724,  18.7172,   1.8225,  -0.0557,  10.0176,  -9.6094,  14.1593,
          -0.3504,  18.6034,  18.4176]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 26.1333,   9.4888],
        [ -6.9076,   1.8027],
        [ -7.0330,   1.6884],
        [ -3.7647,   1.1760],
        [ -6.9072,   1.8020],
        [ 12.9424,   7.0584],
        [ -7.7630,   2.2177],
        [-11.9744,   0.1612],
        [-12.6712,   0.5084],
        [ -7.8365,   2.2568]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[  -9.9864,  -15.4570,   -0.3133,    2.5296,  -15.7045, -104.1071,
          -38.0178,   30.2657,   -4.2053,  -48.1040],
        [ -16.5900,  -20.2756,  -20.9654,    7.3024,  -20.2086, -162.2636,
          -53.8842,   -1.1463,  -20.7386,  -63.4876],
        [ -25.1856, -114.3861, -128.4910,  -89.2675, -114.5179,  -28.2983,
         -136.3384,  -63.9314, -166.0140, -138.9005],
        [ -31.1534, -168.4042, -167.5181, -132.9889, -168.5613,  -63.1970,
         -186.1058,  -98.0532, -147.5336, -183.2952],
        [ -20.0529, -128.5892, -143.0593,  -92.8970, -128.8242,   -5.4561,
         -159.0041,  -89.6755, -170.3212, -152.0743],
        [-102.5589,  -14.0500,   -2.2851,   -3.5599,  -14.3305, -244.0849,
          -49.5951,   44.3298,    1.0947,  -59.0020],
        [  -3.2735,  -13.5560,  -28.6904,   22.0567,  -13.5689,   -0.9556,
          -54.9331,  -52.4159,  -79.6897,  -60.2840],
        [ -32.4525, -162.4952, -161.3516, -129.3607, -162.8070,  -86.7450,
         -169.8768,  -98.0303, -137.0535, -166.7642],
        [ -34.7404, -126.2641, -126.4544,  -98.3113, -126.7034, -108.1003,
         -115.9465,  -95.0493, -142.0401, -106.5240],
        [  -7.3668,  105.1220,  152.2096,   85.4407,  105.3156,    5.5092,
           79.5755,    0.2782,  -74.5388,   72.8807]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-18.1106,  36.0924,  -5.0764, -10.4973, -10.8974,  43.7114,  -8.1890,
         -13.2408, -14.3212,   0.8546],
        [ 17.7817, -35.9782,   5.0928,  10.4785,  11.3824, -43.6485,   8.1560,
          13.4062,  14.2965,  -0.7848]], device='cuda:0'))])
xi:  [-489.08694]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 47.42369143108811
W_T_median: -142.88947887737902
W_T_pctile_5: -387.5372292638521
W_T_CVAR_5_pct: -435.7216127597089
Average q (qsum/M+1):  57.61233618951613
Optimal xi:  [-489.08694]
Expected(across Rb) median(across samples) p_equity:  0.23533290458552228
obj fun:  tensor(-1684.6462, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1567.5904860194325
Current xi:  [-54.06719]
objective value function right now is: -1567.5904860194325
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1579.5930141325211
Current xi:  [-92.09875]
objective value function right now is: -1579.5930141325211
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1581.865669808997
Current xi:  [-122.19605]
objective value function right now is: -1581.865669808997
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1583.0582627554854
Current xi:  [-143.6954]
objective value function right now is: -1583.0582627554854
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1583.651587043886
Current xi:  [-157.66798]
objective value function right now is: -1583.651587043886
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-168.53687]
objective value function right now is: -1583.29758840786
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-197.50423]
objective value function right now is: -1572.1544613141791
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1584.817444235421
Current xi:  [-199.83908]
objective value function right now is: -1584.817444235421
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-192.97229]
objective value function right now is: -1579.683966538611
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-188.10062]
objective value function right now is: -1584.2007540632176
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1584.929648192571
Current xi:  [-188.14592]
objective value function right now is: -1584.929648192571
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-186.42259]
objective value function right now is: -1583.6572300824485
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1585.304011327458
Current xi:  [-184.26254]
objective value function right now is: -1585.304011327458
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1585.3320221117492
Current xi:  [-182.93134]
objective value function right now is: -1585.3320221117492
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-185.62126]
objective value function right now is: -1584.2618738856359
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-191.37543]
objective value function right now is: -1585.273321538181
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1585.4174884692625
Current xi:  [-190.07555]
objective value function right now is: -1585.4174884692625
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-184.38922]
objective value function right now is: -1585.0670291852732
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-184.61859]
objective value function right now is: -1584.870198937815
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-192.36803]
objective value function right now is: -1582.9845610146021
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-190.23854]
objective value function right now is: -1583.054939877856
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-190.15613]
objective value function right now is: -1584.9451984241596
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-185.51367]
objective value function right now is: -1583.4990573539633
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-184.78642]
objective value function right now is: -1584.9217967194347
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-186.29475]
objective value function right now is: -1585.262128313626
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1588.6104360973989
Current xi:  [-194.35469]
objective value function right now is: -1588.6104360973989
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-191.00697]
objective value function right now is: -1586.0187210709164
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-192.38293]
objective value function right now is: -1587.2256572216777
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-186.84264]
objective value function right now is: -1586.5724753002996
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-190.89288]
objective value function right now is: -1587.1938661929591
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1588.7803008637236
Current xi:  [-187.43431]
objective value function right now is: -1588.7803008637236
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-188.3791]
objective value function right now is: -1585.7934367123196
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-186.51274]
objective value function right now is: -1584.143841404488
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-182.18233]
objective value function right now is: -1586.5928082962498
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-182.52968]
objective value function right now is: -1585.14999040394
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-181.57004]
objective value function right now is: -1585.1713867786748
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-183.38708]
objective value function right now is: -1583.0468272252233
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-186.54341]
objective value function right now is: -1584.9139986595153
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-185.41231]
objective value function right now is: -1583.967672757676
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-184.97284]
objective value function right now is: -1586.2069347435151
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-183.76358]
objective value function right now is: -1586.3907383460744
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-182.40529]
objective value function right now is: -1586.1364100960293
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-182.29538]
objective value function right now is: -1586.402335070825
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.99164]
objective value function right now is: -1586.9882115561386
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-182.12825]
objective value function right now is: -1585.991380657667
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.95732]
objective value function right now is: -1586.2559117792239
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-181.71951]
objective value function right now is: -1585.16323614746
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-180.43788]
objective value function right now is: -1584.6784697260375
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-181.61497]
objective value function right now is: -1586.1860544808824
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-181.14847]
objective value function right now is: -1586.3742942599115
min fval:  -1585.7835569872643
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.6856,  4.9059],
        [15.9452, 19.8345],
        [ 0.6853,  4.9068],
        [ 0.6855,  4.9054],
        [ 9.7147,  7.3281],
        [ 0.6855,  4.9078],
        [ 0.6848,  4.9081],
        [-0.4496, -3.8087],
        [ 0.6844,  4.9101],
        [-0.3292, -3.7873]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -99.3882,    0.8553, -101.5865, -101.1664,   13.9189,  -99.8494,
         -101.7386,    1.6414, -101.6702,    2.9525],
        [ 104.2983,  -12.5883,  105.8934,  106.1784,   -9.3695,  103.8077,
          105.4189,   -3.2613,  104.4307,   -1.3005],
        [  13.2397,    4.2365,   17.0605,   17.1126,   -1.5704,   11.8721,
           17.0174,  -15.0639,   17.3235,  -13.2343],
        [  -3.7004,   -2.6377,   -2.8765,   -2.9305,    0.3017,   -4.4010,
           -3.0902,  -11.6224,   -2.7960,   -9.7868],
        [  60.2580,    7.1605,   63.5012,   63.5683,   -6.6993,   59.1990,
           63.0838,  -20.2165,   63.9224,  -18.7633],
        [   1.4582,    2.4261,   -2.7569,   -2.4366,    2.1701,    1.1868,
           -2.9525,    7.4337,   -3.3890,   10.5257],
        [  71.0577,    4.5517,   73.2372,   73.5684,   -7.8610,   70.1642,
           73.2946,  -19.2162,   73.4012,  -17.9103],
        [  -1.9750,   -1.7572,   -1.1415,   -1.2681,   -1.3943,   -2.4206,
           -1.3010,  -11.2527,   -1.2701,  -10.6690],
        [  89.5356,  -11.4442,   91.3602,   91.6160,  -58.2206,   89.2805,
           91.2464,   -9.0339,   89.7716,   -6.8334],
        [  85.2981,   20.9907,   86.4626,   87.1009,   -2.5425,   84.1334,
           86.2837,  -13.2505,   85.3344,  -11.2938]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-23.8584,  29.0605,   0.4305,  -0.5556,   3.5338, -10.2909,   2.6119,
          -0.1473,  20.0490,  15.5642]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 17.8615,  10.8245],
        [ -7.2350,   2.0964],
        [ -7.5281,   1.9935],
        [-50.9382,  -5.8247],
        [ -7.2348,   2.0969],
        [ 13.6381,   5.7532],
        [ -7.0308,   2.2034],
        [ -8.0383,  -2.2068],
        [-13.3261,   1.5312],
        [ -6.7980,   2.3999]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -36.0148, -164.4561, -148.2125,   40.5835, -164.7253,  -77.9937,
         -182.9053,   42.3059,  -42.0054, -196.5087],
        [  -7.0541,  -51.4361,  -52.3464,   31.4085,  -51.4128, -200.1064,
          -89.9752,  -29.9364,  -51.2083,  -99.0009],
        [ -17.6839, -154.8593, -165.9420,  -55.8524, -155.0709,  -45.1827,
         -174.5535,  -91.6300, -206.8698, -177.8219],
        [ -18.8436, -179.1180, -176.2203,  -94.5493, -179.3611,  -61.1370,
         -193.5834,  -99.0309, -158.9218, -191.2558],
        [ -11.1109, -164.7083, -176.4519,  -58.4731, -165.0222,  -25.2268,
         -192.2836, -115.2693, -206.8582, -185.9345],
        [-259.6004, -125.1121, -120.8037,   37.5915, -125.4982, -371.9305,
         -129.9861,   27.5230,   -3.0815, -132.0053],
        [ -18.4634,  -70.2437,  -83.2477,   42.9435,  -70.2752,   -7.1928,
         -107.5607,  -80.6798, -105.5435, -111.3808],
        [ -16.2830, -174.2794, -171.0174,  -91.3962, -174.6748,  -86.5753,
         -178.5169, -100.3799, -149.8127, -175.7112],
        [ -13.0557, -155.9441, -153.0846,  -64.1210, -156.4659, -120.6565,
         -143.0310, -112.0201, -172.0151, -133.9557],
        [  -1.7410,  179.4623,  244.7209, -318.2033,  179.5875,    0.4642,
          141.3764,  -31.7661,   32.7180,   93.4042]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -2.9094,  26.2494, -14.9015, -19.0610, -20.1215,  28.7158,  -6.9557,
         -22.0774, -25.5472,   0.7197],
        [  2.5830, -26.0633,  14.9173,  19.0416,  20.6060, -28.6709,   6.9225,
          22.2423,  25.5221,  -0.6498]], device='cuda:0'))])
xi:  [-182.12825]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 164.15000401729975
W_T_median: -49.83751736805654
W_T_pctile_5: -193.85592752536346
W_T_CVAR_5_pct: -251.84524248794165
Average q (qsum/M+1):  55.340761246219756
Optimal xi:  [-182.12825]
Expected(across Rb) median(across samples) p_equity:  0.28874557710490384
obj fun:  tensor(-1585.7836, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1498.0978082701397
Current xi:  [-26.080593]
objective value function right now is: -1498.0978082701397
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1499.5624262025283
Current xi:  [-32.852554]
objective value function right now is: -1499.5624262025283
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1529.071365394394
Current xi:  [-24.333038]
objective value function right now is: -1529.071365394394
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1532.5660659091673
Current xi:  [-20.42282]
objective value function right now is: -1532.5660659091673
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.1563363]
objective value function right now is: -1524.5408756678355
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1536.0894725266633
Current xi:  [0.44500622]
objective value function right now is: -1536.0894725266633
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1537.822470367421
Current xi:  [3.3458147]
objective value function right now is: -1537.822470367421
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.930764]
objective value function right now is: -1530.898111557888
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.579219]
objective value function right now is: -1520.033464562895
Traceback (most recent call last):
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/decumulation_driver.py", line 889, in <module>
    fun_RUN__wrapper.RUN__wrapper_ONE_stage_optimization(
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_RUN__wrapper.py", line 116, in RUN__wrapper_ONE_stage_optimization
    RUN__wrapper_training_testing_NN(
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_RUN__wrapper.py", line 216, in RUN__wrapper_training_testing_NN
    res_adam = fun_train_NN.train_NN( theta0 = theta0,
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_train_NN.py", line 196, in train_NN
    result_pyt_adam = run_Gradient_Descent_pytorch(NN_list= NN_list,
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_train_NN_SGD_algorithms.py", line 148, in run_Gradient_Descent_pytorch
    params_it = copy.deepcopy(params)  # Create a copy of input data for this iteration
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
KeyboardInterrupt
