Starting at: 
25-01-23_17:37

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 40000
batchsize: 1000
remove neg:  False


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 4000, 'itbound_SGD_algorithms': 40000, 'nit_IterateAveragingStart': 36000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1647.8145227560676
Current xi:  [-36.332245]
objective value function right now is: -1647.8145227560676
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1660.3716811132188
Current xi:  [-71.81778]
objective value function right now is: -1660.3716811132188
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1669.249776036106
Current xi:  [-106.4471]
objective value function right now is: -1669.249776036106
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1683.4190715055327
Current xi:  [-141.44734]
objective value function right now is: -1683.4190715055327
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-182.7875]
objective value function right now is: -1651.7786995340516
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1698.2489728993523
Current xi:  [-214.2157]
objective value function right now is: -1698.2489728993523
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1704.2349583841963
Current xi:  [-245.80461]
objective value function right now is: -1704.2349583841963
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1709.4472636304017
Current xi:  [-278.60147]
objective value function right now is: -1709.4472636304017
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-312.25174]
objective value function right now is: -1684.6191083152494
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1717.7831570281023
Current xi:  [-345.0161]
objective value function right now is: -1717.7831570281023
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1722.6541073222813
Current xi:  [-377.05463]
objective value function right now is: -1722.6541073222813
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1726.1036330132858
Current xi:  [-408.8892]
objective value function right now is: -1726.1036330132858
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1728.4152118210307
Current xi:  [-440.1952]
objective value function right now is: -1728.4152118210307
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1729.1264086299063
Current xi:  [-470.47]
objective value function right now is: -1729.1264086299063
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1732.35975836367
Current xi:  [-501.0958]
objective value function right now is: -1732.35975836367
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-530.4595]
objective value function right now is: -1727.4876431146597
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1733.3813956739828
Current xi:  [-555.76483]
objective value function right now is: -1733.3813956739828
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1734.237985155165
Current xi:  [-579.13934]
objective value function right now is: -1734.237985155165
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1735.0690002598305
Current xi:  [-603.6307]
objective value function right now is: -1735.0690002598305
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-619.42084]
objective value function right now is: -1733.8582030361913
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-631.5002]
objective value function right now is: -1730.5333649620172
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1735.1924310900672
Current xi:  [-638.9407]
objective value function right now is: -1735.1924310900672
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1735.3703411773072
Current xi:  [-641.3626]
objective value function right now is: -1735.3703411773072
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-620.5017]
objective value function right now is: -1721.5034898829854
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1739.512915406679
Current xi:  [-582.8501]
objective value function right now is: -1739.512915406679
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1740.3787559587868
Current xi:  [-552.638]
objective value function right now is: -1740.3787559587868
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-532.22473]
objective value function right now is: -1740.3047644597973
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-514.3016]
objective value function right now is: -1740.1666214356962
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-503.17117]
objective value function right now is: -1740.1281319369473
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1740.4512057625298
Current xi:  [-497.1454]
objective value function right now is: -1740.4512057625298
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-493.9083]
objective value function right now is: -1740.3291433758
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-497.1715]
objective value function right now is: -1739.3564606703665
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-497.7494]
objective value function right now is: -1740.3174202450207
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-500.8998]
objective value function right now is: -1739.8244107169085
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-495.27957]
objective value function right now is: -1740.0071451456827
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1740.9336997743305
Current xi:  [-494.82968]
objective value function right now is: -1740.9336997743305
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1740.9683406832298
Current xi:  [-493.94284]
objective value function right now is: -1740.9683406832298
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-493.01202]
objective value function right now is: -1740.7981460188132
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.0378783093886
Current xi:  [-491.02878]
objective value function right now is: -1741.0378783093886
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.0893926302176
Current xi:  [-490.01294]
objective value function right now is: -1741.0893926302176
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.79788]
objective value function right now is: -1741.0782605077397
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.43378]
objective value function right now is: -1741.0250387108993
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.7138]
objective value function right now is: -1741.0525257427018
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.4865]
objective value function right now is: -1741.0031120015374
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.0216]
objective value function right now is: -1740.9045796976588
new min fval from sgd:  -1741.116687450812
new min fval from sgd:  -1741.120994841809
new min fval from sgd:  -1741.1346426764587
new min fval from sgd:  -1741.1438545292078
new min fval from sgd:  -1741.1511731796154
new min fval from sgd:  -1741.1561142490502
new min fval from sgd:  -1741.1586558695049
new min fval from sgd:  -1741.162428623984
new min fval from sgd:  -1741.1631235365332
new min fval from sgd:  -1741.1688764890657
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.0476]
objective value function right now is: -1741.01035380523
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.34384]
objective value function right now is: -1741.028781982774
new min fval from sgd:  -1741.17077098253
new min fval from sgd:  -1741.1752332620933
new min fval from sgd:  -1741.1791036663185
new min fval from sgd:  -1741.17975248527
new min fval from sgd:  -1741.181175574177
new min fval from sgd:  -1741.1838485465325
new min fval from sgd:  -1741.1852883145964
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-488.89932]
objective value function right now is: -1741.082772077743
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-488.57062]
objective value function right now is: -1741.1828810148343
new min fval from sgd:  -1741.1865573249302
new min fval from sgd:  -1741.1865747213774
new min fval from sgd:  -1741.1866579276475
new min fval from sgd:  -1741.1868967572893
new min fval from sgd:  -1741.1871033802852
new min fval from sgd:  -1741.1871819690973
new min fval from sgd:  -1741.187390928629
new min fval from sgd:  -1741.1889267554377
new min fval from sgd:  -1741.1904728853474
new min fval from sgd:  -1741.1917409227603
new min fval from sgd:  -1741.1927436021524
new min fval from sgd:  -1741.193344651596
new min fval from sgd:  -1741.1942453818056
new min fval from sgd:  -1741.1943026818535
new min fval from sgd:  -1741.1946144316225
new min fval from sgd:  -1741.1954000925455
new min fval from sgd:  -1741.1967813344072
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-488.3229]
objective value function right now is: -1741.1631790196277
min fval:  -1741.1967813344072
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  7.3817,   5.5474],
        [  0.3164,   3.7864],
        [-13.1586,   3.6886],
        [ -6.9198,  -5.5163],
        [  0.0293,   1.2964],
        [ -7.4977,  -5.9859],
        [-13.5290,   0.2492],
        [ -0.9113,   4.1094],
        [ -7.7430,  -6.2064],
        [  8.0681,   5.9437]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 4.2214,  2.0021, 11.6166, -4.5039, -1.2171, -4.8344,  4.5909,  4.4933,
        -5.0403,  4.2524], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.0207e-01,  5.5269e-02, -2.4687e-01, -1.9883e-01, -1.1761e-04,
         -2.0250e-01,  2.9422e-02,  9.0426e-03, -1.9935e-01, -2.8617e-01],
        [-3.3104e+00, -5.2729e-01, -1.0395e+01,  5.6269e+00,  3.7297e-01,
          6.9856e+00,  1.1855e+00, -1.3986e+00,  7.6280e+00, -5.3115e+00],
        [-3.0297e-01,  5.5624e-02, -2.4756e-01, -1.9883e-01,  2.3322e-05,
         -2.0244e-01,  2.9469e-02,  9.8929e-03, -1.9927e-01, -2.8735e-01],
        [-3.0207e-01,  5.5269e-02, -2.4687e-01, -1.9883e-01, -1.1757e-04,
         -2.0250e-01,  2.9423e-02,  9.0431e-03, -1.9935e-01, -2.8616e-01],
        [ 4.1802e-01,  3.5005e-01,  9.6452e+00, -1.2759e+00,  1.2079e-02,
         -1.3592e+00,  1.7710e+00,  1.0664e+00, -1.4691e+00, -1.4591e-01],
        [ 4.2356e-01,  3.5126e-01,  9.6665e+00, -1.2829e+00,  1.2553e-02,
         -1.3665e+00,  1.7791e+00,  1.0700e+00, -1.4766e+00, -1.4230e-01],
        [ 3.4095e-01,  3.3348e-01,  9.3507e+00, -1.1791e+00,  5.4740e-03,
         -1.2589e+00,  1.6592e+00,  1.0181e+00, -1.3650e+00, -1.9627e-01],
        [-4.1181e+00, -1.1951e+00, -7.8526e+00,  7.0705e+00, -2.3633e-01,
          8.4169e+00, -3.0372e+00, -2.8055e+00,  9.1225e+00, -5.2858e+00],
        [ 1.1109e+01,  8.1001e-01, -1.1501e+00, -2.4430e+00,  4.0681e-01,
         -3.0020e+00, -1.0024e+01,  6.8088e-01, -3.1624e+00,  1.2513e+01],
        [-3.0208e-01,  5.5268e-02, -2.4687e-01, -1.9883e-01, -1.1802e-04,
         -2.0250e-01,  2.9419e-02,  9.0401e-03, -1.9935e-01, -2.8617e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5795,  1.6993, -0.5674, -0.5795, -1.3874, -1.3896, -1.3578,  2.1560,
         4.2984, -0.5796], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0965, -14.0227,  -0.0964,  -0.0965,   4.8947,   4.9111,   4.6718,
         -15.0390,  15.3808,  -0.0965]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -7.5063,  -5.4637],
        [ -1.0116,   7.9144],
        [-11.1723, -14.5741],
        [-17.0989,  -4.0509],
        [ -1.5651,  -5.0288],
        [ -5.6653,   1.0624],
        [-12.3944,  -9.4617],
        [  3.3518,  -0.0847],
        [  7.9366,  -1.4695],
        [ -2.3731,   3.2951]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -6.2215,   6.2170, -15.0939,   0.5591, -11.9865,   3.4411,  -5.1864,
          1.7008, -10.2763,  -4.3024], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-8.7892e-02, -8.2790e-01, -1.2486e-01, -7.9374e-02, -8.1298e-02,
         -1.7773e-01, -1.2020e-01, -2.4670e+00, -3.5355e-01, -1.9912e-03],
        [-5.5132e-02,  1.6801e+01,  1.0292e+01,  8.4060e+00, -6.0867e+00,
         -2.4162e+00, -1.3833e+01,  3.3874e+00,  4.9976e+00,  2.8544e-02],
        [-8.7899e-02, -8.2803e-01, -1.2490e-01, -7.9353e-02, -8.1302e-02,
         -1.7777e-01, -1.2027e-01, -2.4761e+00, -3.5341e-01, -1.9871e-03],
        [-8.6783e-02, -8.2663e-01, -1.2434e-01, -7.8303e-02, -8.0211e-02,
         -1.7790e-01, -1.1928e-01, -2.4705e+00, -3.5054e-01, -1.7132e-03],
        [-7.3919e-01, -2.4412e+00, -2.7512e+00, -1.5571e+00, -1.1430e+00,
         -3.9915e-01,  3.3695e-01, -2.9444e+00,  1.7753e+00, -7.8931e-02],
        [ 1.5333e+00, -4.3316e+00,  1.2398e+00, -9.2325e+00,  3.0303e+00,
          1.3167e+00,  1.5022e+00,  1.0117e+00, -5.1683e+00, -3.1113e-01],
        [-8.7792e-02, -8.2767e-01, -1.2477e-01, -7.9310e-02, -8.1212e-02,
         -1.7773e-01, -1.2008e-01, -2.4754e+00, -3.5347e-01, -1.9761e-03],
        [-8.7812e-02, -8.2745e-01, -1.2491e-01, -7.9301e-02, -8.1229e-02,
         -1.7773e-01, -1.2016e-01, -2.4716e+00, -3.5258e-01, -1.9597e-03],
        [-8.6675e-02, -8.2652e-01, -1.2430e-01, -7.8221e-02, -8.0105e-02,
         -1.7794e-01, -1.1921e-01, -2.4712e+00, -3.5002e-01, -1.6794e-03],
        [-6.5272e+00, -2.1096e+00, -7.9437e+00, -5.4815e+00, -6.9472e-02,
         -2.5963e+00,  6.1576e+00,  2.0440e+00,  1.5196e+01,  9.3881e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.5113, -0.9923, -2.5020, -2.5157, -1.5439, -2.8432, -2.5038, -2.5081,
        -2.5159,  3.0329], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-0.0057,  4.2156, -0.0057, -0.0049, -2.1289, -3.2999, -0.0057, -0.0057,
         -0.0052, -4.3417],
        [ 0.0058, -4.5072,  0.0058,  0.0056,  2.0538,  3.3077,  0.0058,  0.0057,
          0.0052,  4.3342]], device='cuda:0'))])
xi:  [-488.3145]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 207.2392766263641
W_T_median: 107.62265532056244
W_T_pctile_5: -489.3306217452008
W_T_CVAR_5_pct: -597.2166321858888
Average q (qsum/M+1):  57.13089875252016
Optimal xi:  [-488.3145]
Expected(across Rb) median(across samples) p_equity:  0.273121524322778
obj fun:  tensor(-1741.1968, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1564.5386918068798
Current xi:  [-13.292315]
objective value function right now is: -1564.5386918068798
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1652.5798805371217
Current xi:  [-37.168655]
objective value function right now is: -1652.5798805371217
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1660.117651373381
Current xi:  [-76.630714]
objective value function right now is: -1660.117651373381
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1665.0881454279977
Current xi:  [-109.01771]
objective value function right now is: -1665.0881454279977
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1667.864765806303
Current xi:  [-135.73976]
objective value function right now is: -1667.864765806303
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.12889]
objective value function right now is: -1659.693563423449
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1670.311881924164
Current xi:  [-186.29002]
objective value function right now is: -1670.311881924164
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-202.11662]
objective value function right now is: -1670.2247948619627
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-216.75249]
objective value function right now is: -1670.001805366878
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.3202602486492
Current xi:  [-229.41327]
objective value function right now is: -1672.3202602486492
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-236.66809]
objective value function right now is: -1668.2507096325874
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-240.52861]
objective value function right now is: -1671.9732106581337
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.83168]
objective value function right now is: -1670.7443161460692
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-245.75935]
objective value function right now is: -1670.3558226357186
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.432834034747
Current xi:  [-245.08223]
objective value function right now is: -1672.432834034747
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-243.43689]
objective value function right now is: -1672.0326755996753
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-243.80997]
objective value function right now is: -1671.359257196791
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.4498898879294
Current xi:  [-244.59978]
objective value function right now is: -1672.4498898879294
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.6318797187237
Current xi:  [-244.89148]
objective value function right now is: -1672.6318797187237
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.61107]
objective value function right now is: -1671.8138217000385
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.08928]
objective value function right now is: -1670.9049299663343
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.9490377322313
Current xi:  [-245.09418]
objective value function right now is: -1672.9490377322313
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.6211]
objective value function right now is: -1672.7348122100655
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.0125370408875
Current xi:  [-246.25714]
objective value function right now is: -1673.0125370408875
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.20445]
objective value function right now is: -1671.4450438205176
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.51329]
objective value function right now is: -1672.8758074943116
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.29024]
objective value function right now is: -1671.6490989209753
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-245.215]
objective value function right now is: -1672.8445200271135
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-244.32962]
objective value function right now is: -1672.3344879965578
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.998]
objective value function right now is: -1672.8356133357631
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.56001]
objective value function right now is: -1672.7266760581726
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.52185]
objective value function right now is: -1672.9280373412696
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.35103]
objective value function right now is: -1672.605937249421
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.81601]
objective value function right now is: -1672.544099873836
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.18965]
objective value function right now is: -1672.7993874800159
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.3196874216321
Current xi:  [-244.81212]
objective value function right now is: -1673.3196874216321
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.98856]
objective value function right now is: -1672.958765662038
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.80193]
objective value function right now is: -1673.2428626255974
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.321196762942
Current xi:  [-244.55963]
objective value function right now is: -1673.321196762942
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.3347392213514
Current xi:  [-244.70518]
objective value function right now is: -1673.3347392213514
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.68399]
objective value function right now is: -1673.2298374890215
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.3641910619526
Current xi:  [-244.92834]
objective value function right now is: -1673.3641910619526
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.4084941409371
Current xi:  [-244.88261]
objective value function right now is: -1673.4084941409371
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.58376]
objective value function right now is: -1673.257147872935
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.49191]
objective value function right now is: -1673.3540281282874
new min fval from sgd:  -1673.415909418833
new min fval from sgd:  -1673.4230622156272
new min fval from sgd:  -1673.4418790981942
new min fval from sgd:  -1673.441888027657
new min fval from sgd:  -1673.444906225088
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.53238]
objective value function right now is: -1673.2086040816941
new min fval from sgd:  -1673.4602863401399
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.5248]
objective value function right now is: -1673.3182602379766
new min fval from sgd:  -1673.4608870166355
new min fval from sgd:  -1673.463979698526
new min fval from sgd:  -1673.4646768103823
new min fval from sgd:  -1673.4676892261723
new min fval from sgd:  -1673.472509706081
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.47308]
objective value function right now is: -1673.3602950277111
new min fval from sgd:  -1673.4727865653788
new min fval from sgd:  -1673.4739573836107
new min fval from sgd:  -1673.4752293320892
new min fval from sgd:  -1673.4759793084804
new min fval from sgd:  -1673.4760921913196
new min fval from sgd:  -1673.4775428260293
new min fval from sgd:  -1673.4799429458603
new min fval from sgd:  -1673.48200617712
new min fval from sgd:  -1673.4833904970105
new min fval from sgd:  -1673.4844234195118
new min fval from sgd:  -1673.4847443274407
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.36064]
objective value function right now is: -1673.4624438939684
new min fval from sgd:  -1673.4850981319798
new min fval from sgd:  -1673.4856338108052
new min fval from sgd:  -1673.485799663642
new min fval from sgd:  -1673.4859035318802
new min fval from sgd:  -1673.4875430698028
new min fval from sgd:  -1673.488518198254
new min fval from sgd:  -1673.4890024645492
new min fval from sgd:  -1673.4894386718304
new min fval from sgd:  -1673.489826837304
new min fval from sgd:  -1673.49061178636
new min fval from sgd:  -1673.4906666538
new min fval from sgd:  -1673.4909116925965
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.45892]
objective value function right now is: -1673.4745209085222
min fval:  -1673.4909116925965
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -0.3598,   1.2730],
        [ -0.9865,  -2.1502],
        [-11.0343,   6.8660],
        [ -3.0428,  -8.6238],
        [ 15.4049,   4.1757],
        [ -3.1271,  -9.0695],
        [  0.0436,  -1.7976],
        [-14.6997,   6.3518],
        [ -3.1645,  -9.3387],
        [ -0.3598,   1.2730]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-1.1645,  2.4948, 12.7735, -7.9235,  3.5026, -8.3750,  1.7544,  9.7501,
        -8.6496, -1.1645], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.0104e-02, -4.3539e-01, -8.4915e-02, -1.5273e-01, -4.5857e-01,
         -1.5270e-01, -4.3150e-01,  1.3771e-02, -1.5351e-01, -1.0104e-02],
        [ 2.3462e-02,  1.3264e+00, -6.5322e-01,  1.2100e-01,  9.4635e-01,
          1.0669e-01,  1.3161e+00,  4.9204e-01,  1.0013e-01,  2.3462e-02],
        [ 5.2473e-02,  4.6660e+00, -8.2288e+00,  5.4543e+00, -1.2852e+01,
          6.0919e+00, -2.4983e-01,  3.1764e+00,  6.5622e+00,  5.2473e-02],
        [-1.0104e-02, -4.3539e-01, -8.4915e-02, -1.5273e-01, -4.5857e-01,
         -1.5270e-01, -4.3150e-01,  1.3771e-02, -1.5351e-01, -1.0104e-02],
        [-1.6140e-01,  4.4868e-01,  1.5889e+00,  6.2851e-01,  8.1908e-01,
          6.8145e-01,  4.5141e-01, -2.9076e-01,  7.1996e-01, -1.6140e-01],
        [-1.6151e-01,  4.4884e-01,  1.5887e+00,  6.2873e-01,  8.1968e-01,
          6.8169e-01,  4.5157e-01, -2.9138e-01,  7.2022e-01, -1.6151e-01],
        [-1.5437e-01,  4.3875e-01,  1.5962e+00,  6.1559e-01,  7.8383e-01,
          6.6720e-01,  4.4115e-01, -2.5609e-01,  7.0475e-01, -1.5437e-01],
        [-4.6940e-02, -7.4428e+00, -1.0428e+01,  7.6150e+00,  6.7759e+00,
          8.5347e+00, -4.5520e+00, -1.4712e+01,  9.1407e+00, -4.6940e-02],
        [-1.0104e-02, -4.3539e-01, -8.4915e-02, -1.5273e-01, -4.5857e-01,
         -1.5270e-01, -4.3150e-01,  1.3771e-02, -1.5351e-01, -1.0104e-02],
        [-1.0104e-02, -4.3539e-01, -8.4915e-02, -1.5273e-01, -4.5857e-01,
         -1.5270e-01, -4.3150e-01,  1.3771e-02, -1.5351e-01, -1.0104e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.4419,  1.3425, -0.3530, -0.4419,  0.4460,  0.4461,  0.4362,  1.1467,
        -0.4419, -0.4419], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 7.5861e-03, -6.7396e+00, -2.3386e+01,  7.5862e-03,  4.9464e+00,
          4.9548e+00,  4.4575e+00, -1.2199e+01,  7.5861e-03,  7.5861e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.3887,   2.3792],
        [ -4.6079,  13.0995],
        [-17.4690, -17.3094],
        [-21.6537,  -8.1147],
        [ -1.3960,   2.1656],
        [-13.3934,  -0.2087],
        [-17.9680, -12.0406],
        [  4.3479,  -1.1372],
        [ 10.1383,  -1.7699],
        [  1.2606,   7.2412]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -3.0478,  11.3641, -16.7723,  -4.6032,  -2.9570,   4.5949,  -7.7447,
         -1.6807, -11.7318,   5.4664], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 8.0066e-03, -1.4734e+00,  1.5456e+00, -1.6760e+00, -1.5749e-03,
         -7.4153e-01, -1.6420e+00, -1.6810e+00,  2.4630e-01, -2.3021e+00],
        [ 1.7769e-01, -3.6683e+00,  4.9477e+00,  3.8724e+00,  1.9052e-01,
          5.8030e+00, -1.8903e+01,  4.9854e+00,  7.4278e+00,  3.5783e+00],
        [ 8.8223e-03, -1.4805e+00,  1.5496e+00, -1.7120e+00, -7.0182e-04,
         -7.3933e-01, -1.6562e+00, -1.6602e+00,  2.6881e-01, -2.3406e+00],
        [-6.4786e-03, -1.1000e+00,  8.9967e-01, -6.8983e-01, -1.3914e-02,
         -6.7181e-01, -1.0093e+00, -2.2436e+00, -2.5651e-01, -1.3770e+00],
        [ 9.9269e-02, -1.5921e+01, -2.2906e+01,  8.3546e+00,  1.1109e-01,
          4.3123e+00, -1.2898e+00, -2.3342e+00, -2.4736e+00, -1.1006e+00],
        [-1.1728e-02, -3.8168e+00,  8.1641e-01, -5.8106e+00, -3.3875e-02,
         -4.7612e+00,  3.0974e+00,  2.2623e-01, -6.6054e+00, -5.9865e-01],
        [ 6.3282e-03, -1.4575e+00,  1.5335e+00, -1.6011e+00, -3.3472e-03,
         -7.4592e-01, -1.6107e+00, -1.7242e+00,  2.0039e-01, -2.2242e+00],
        [ 4.3808e-03, -1.4365e+00,  1.5121e+00, -1.5126e+00, -5.3572e-03,
         -7.5033e-01, -1.5703e+00, -1.7753e+00,  1.4790e-01, -2.1349e+00],
        [-6.5183e-03, -1.1074e+00,  9.1503e-01, -7.0177e-01, -1.4039e-02,
         -6.7571e-01, -1.0201e+00, -2.2373e+00, -2.5187e-01, -1.3879e+00],
        [ 4.5800e-01, -7.2013e-01, -1.1292e+01, -1.1477e+01,  2.8369e-01,
         -1.8242e+00,  6.1947e+00,  1.6152e+00,  1.6623e+01, -2.6347e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.9216,  6.0983, -2.9135, -3.0903, -1.0798, -3.6649, -2.9380, -2.9570,
        -3.0902,  4.2624], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.9597,  4.7696, -2.0014, -0.7542, -8.2817, -3.5945, -1.8732, -1.7705,
         -0.7704, -4.8473],
        [ 1.9597, -5.0583,  2.0014,  0.7542,  8.2401,  3.5967,  1.8732,  1.7705,
          0.7704,  4.8399]], device='cuda:0'))])
xi:  [-244.45065]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 234.0722157464843
W_T_median: 83.28421806765255
W_T_pctile_5: -245.98879079788634
W_T_CVAR_5_pct: -335.3447118854077
Average q (qsum/M+1):  56.147244361139116
Optimal xi:  [-244.45065]
Expected(across Rb) median(across samples) p_equity:  0.29662722504872363
obj fun:  tensor(-1673.4909, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1599.3341696871319
Current xi:  [-0.08193671]
objective value function right now is: -1599.3341696871319
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.36613038]
objective value function right now is: -1593.1345550258434
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0503622]
objective value function right now is: -1590.2667936304892
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-11.615784]
objective value function right now is: -1599.0833307444025
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.4712949]
objective value function right now is: -1595.4555890433785
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04957385]
objective value function right now is: -1584.484919154834
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-28.389719]
objective value function right now is: -1181.9436351274107
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-67.13129]
objective value function right now is: -1446.4490750799528
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-83.58375]
objective value function right now is: -1563.867263160301
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-78.57248]
objective value function right now is: -1584.8916441879517
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-77.33449]
objective value function right now is: -1588.5777347389126
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-84.327705]
objective value function right now is: -1594.7755886383522
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-77.19511]
objective value function right now is: -1596.3972967362256
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-76.57518]
objective value function right now is: -1593.8830938130566
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-76.856064]
objective value function right now is: -1595.0288190085112
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-76.79471]
objective value function right now is: -1580.3213807009988
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-117.31745]
objective value function right now is: -1560.2114430990098
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-114.66457]
objective value function right now is: -1576.587163056497
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-111.29554]
objective value function right now is: -1579.5728715918078
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-107.12253]
objective value function right now is: -1579.1304832166425
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-104.65634]
objective value function right now is: -1577.3360660078963
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-107.12625]
objective value function right now is: -1579.0563553476538
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.21503]
objective value function right now is: -1581.0247241477127
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.86163]
objective value function right now is: -1581.4328639958671
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-111.53903]
objective value function right now is: -1580.1514778252367
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-109.84905]
objective value function right now is: -1582.517227047376
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-108.86181]
objective value function right now is: -1576.4522837193322
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-110.95933]
objective value function right now is: -1532.3143360653098
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-103.437744]
objective value function right now is: -1581.1675513724545
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1601.2553547632322
Current xi:  [-85.84721]
objective value function right now is: -1601.2553547632322
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-80.08545]
objective value function right now is: -1578.1591795034963
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-77.87227]
objective value function right now is: -1578.623823295441
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-77.041435]
objective value function right now is: -1582.2816778006843
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-101.21509]
objective value function right now is: -1233.7912959708306
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-102.57531]
objective value function right now is: -1009.5773545515333
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-99.45145]
objective value function right now is: -1402.0357666702523
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-96.78596]
objective value function right now is: -1536.9824737387037
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-92.5031]
objective value function right now is: -1561.2579936970094
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-88.7533]
objective value function right now is: -1571.4220935845724
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-85.9701]
objective value function right now is: -1574.020019141673
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-83.94905]
objective value function right now is: -1576.8698251032092
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-78.35882]
objective value function right now is: -1593.2528875079906
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-76.9768]
objective value function right now is: -1597.58584218946
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.737656]
objective value function right now is: -1597.1737201144747
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.795166]
objective value function right now is: -1592.5047454013998
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.59108]
objective value function right now is: -1597.3814226391776
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-72.91036]
objective value function right now is: -1596.3854643755808
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.86232]
objective value function right now is: -1593.1426428311952
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-76.38827]
objective value function right now is: -1597.3931628452083
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-76.40056]
objective value function right now is: -1597.7602616745212
min fval:  -1597.9709574140413
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -0.9679,   1.9367],
        [  2.9081,  -1.6336],
        [-14.2347,   4.8087],
        [  1.4298, -13.1783],
        [ -0.2917,   1.7105],
        [  1.5212, -10.8449],
        [ -0.9681,   1.9290],
        [-11.7383,   2.6215],
        [  1.6037, -11.0389],
        [ -0.9679,   1.9367]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -2.3141,   3.8662,  14.9332,  -7.4932,  -3.1075, -10.6333,  -2.3118,
         12.6757, -10.8100,  -2.3141], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.8000e-02,  1.8745e+00,  8.5111e-01,  1.6626e+00, -4.0372e-02,
          4.0241e-01, -4.8884e-02,  1.0487e+00,  4.1355e-01, -4.8000e-02],
        [ 4.6937e-01,  1.4749e+00,  1.4516e+00,  1.5719e+00,  4.1098e-01,
          4.7981e-01,  4.6990e-01,  1.5758e+00,  4.7236e-01,  4.6937e-01],
        [ 4.9087e-01,  1.9081e+00,  1.5464e+00,  2.2251e+00,  4.1132e-01,
          6.1474e-01,  4.8919e-01,  1.6838e+00,  6.2686e-01,  4.9087e-01],
        [-4.8002e-02,  1.8745e+00,  8.5110e-01,  1.6626e+00, -4.0373e-02,
          4.0241e-01, -4.8887e-02,  1.0487e+00,  4.1356e-01, -4.8002e-02],
        [-3.8653e-02,  2.1040e+00,  9.5371e-01,  1.8406e+00, -2.5727e-02,
          3.8118e-01, -3.9962e-02,  1.1751e+00,  3.9186e-01, -3.8653e-02],
        [-1.4435e-01, -1.1438e+00, -8.7608e-01, -1.2421e+00, -1.0499e-01,
         -3.0104e-01, -1.4872e-01, -8.9207e-01, -2.7926e-01, -1.4435e-01],
        [-1.5874e-02, -1.1769e+00, -1.1498e+00, -1.1529e+00,  3.6751e-02,
         -4.8117e-01, -1.3085e-02, -1.1042e+00, -4.7134e-01, -1.5874e-02],
        [-1.9648e-01, -6.8735e+00, -1.3609e+01,  1.4881e+01, -3.8258e-01,
          1.3608e+01, -2.0170e-01, -1.0780e+01,  1.4206e+01, -1.9648e-01],
        [-4.7998e-02,  1.8745e+00,  8.5110e-01,  1.6626e+00, -4.0370e-02,
          4.0242e-01, -4.8883e-02,  1.0487e+00,  4.1356e-01, -4.7998e-02],
        [-4.8003e-02,  1.8745e+00,  8.5108e-01,  1.6626e+00, -4.0375e-02,
          4.0241e-01, -4.8887e-02,  1.0487e+00,  4.1356e-01, -4.8003e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 1.8722,  1.4912,  1.5465,  1.8722,  2.1035, -1.1465, -1.1814,  0.2577,
         1.8722,  1.8722], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  3.0876,  -3.8907, -17.3249,   3.0873,   6.6383,   0.1557,   0.1119,
         -15.2382,   3.0876,   3.0874]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.2740,   0.6187],
        [ -0.5477,   1.6161],
        [-23.2776, -19.3659],
        [-18.0518,  -6.8945],
        [ -0.8134,  -1.1185],
        [-17.2772,  -0.0812],
        [-18.8352, -12.0222],
        [  0.2832,  -6.6408],
        [ 13.3500,  -1.3340],
        [  2.8360, -11.3167]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -4.3749,  14.6914, -18.0694,  -8.5023,  -6.8345,   6.2525,  -6.7343,
          0.3725, -12.2535,  -8.9812], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-8.5801e-02,  1.5198e+00, -1.0345e-01, -8.9286e-03, -2.8600e-01,
          4.8908e+00, -9.7488e+00, -9.4949e-01, -1.1042e+01, -9.4947e+00],
        [ 4.6685e-01, -2.0170e+00,  8.1969e+00,  2.0198e+00, -3.2581e-01,
          1.2099e+01, -2.0224e+01,  3.7950e+00,  9.9720e+00,  3.8274e-01],
        [-2.8676e-01,  6.6304e-01, -3.1585e+00,  5.6490e-02,  6.2345e-01,
          3.0439e+00,  8.1550e+00, -8.4978e-01, -6.6567e+00,  1.8135e-01],
        [-1.5595e-01, -2.2899e+00, -3.2815e-01, -2.7373e-01, -2.4317e-01,
         -4.0038e-01, -5.7976e-01, -3.1875e+00, -8.4186e-01, -1.1378e+00],
        [-7.5788e-02, -2.0053e+00, -9.9452e-02, -4.3005e-02, -3.9947e-02,
         -2.9342e-01, -3.4324e-01, -3.2548e+00, -1.5091e+00, -1.5385e+00],
        [ 3.4224e-03, -4.7873e+00, -1.9944e+00,  8.1528e-01,  4.1358e+00,
          7.4550e-01,  6.7465e-01, -3.9419e+00, -3.3202e-01, -3.9707e+00],
        [ 6.5731e-02, -1.0968e+00, -6.6603e+00,  2.1730e+00, -5.7085e-01,
          6.2086e+00,  5.7869e+00, -3.8878e+00, -1.8324e+00, -1.5866e+01],
        [-2.4891e-01, -2.0667e+00, -2.1812e-01, -1.4428e-01, -1.3023e-01,
         -4.8945e-01, -5.3133e-01, -3.3698e+00, -1.3256e+00, -1.2775e+00],
        [-2.7647e-02, -3.0804e+00, -2.1608e-01, -1.8764e-01, -1.8937e-01,
         -1.9926e-01, -2.6012e-01, -1.4795e+00, -8.1076e-01, -9.6269e-01],
        [-1.0554e-01,  1.6396e+00, -2.4740e+01, -4.5916e+00,  1.6685e+00,
         -1.9394e+01,  4.6019e+00,  6.6370e+00,  9.1840e+00,  3.5269e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-0.4130,  5.3880, -4.0439, -3.3501, -3.2572, -6.0624, -2.3438, -3.2751,
        -3.3254, 10.0710], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 0.8721,  4.6651, -0.2534,  0.7782,  0.8903, -3.8607,  4.1603,  0.8168,
          0.0971, -4.6599],
        [-0.8721, -4.9533,  0.2537, -0.7782, -0.8908,  3.8622, -4.1603, -0.8168,
         -0.0970,  4.6525]], device='cuda:0'))])
xi:  [-74.795166]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 279.32953625288354
W_T_median: 157.4227752829042
W_T_pctile_5: -84.86846465694586
W_T_CVAR_5_pct: -175.78774811947704
Average q (qsum/M+1):  54.48920662172379
Optimal xi:  [-74.795166]
Expected(across Rb) median(across samples) p_equity:  0.39128624290848774
obj fun:  tensor(-1597.9710, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -945.5426446048848
Current xi:  [-7.202938]
objective value function right now is: -945.5426446048848
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1526.3531177811335
Current xi:  [-2.8484113]
objective value function right now is: -1526.3531177811335
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1528.5361981359426
Current xi:  [-0.01579028]
objective value function right now is: -1528.5361981359426
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.11196837]
objective value function right now is: -1527.9873837972973
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.0591224]
objective value function right now is: -1333.740331121871
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-45.347633]
objective value function right now is: -742.0518867014099
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1530.9541405828427
Current xi:  [-60.34213]
objective value function right now is: -1530.9541405828427
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-49.57102]
objective value function right now is: -1457.8875250003266
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1538.7152635064583
Current xi:  [-39.12544]
objective value function right now is: -1538.7152635064583
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1542.7720771184868
Current xi:  [-35.394238]
objective value function right now is: -1542.7720771184868
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-53.433113]
objective value function right now is: -1435.3383642418064
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-86.77336]
objective value function right now is: -1470.9847609710948
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-72.78014]
objective value function right now is: -1506.7991191122494
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-85.786995]
objective value function right now is: -1435.7547676033853
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-72.63794]
objective value function right now is: -1520.9498145752834
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-81.75668]
objective value function right now is: -1080.0216948689751
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-117.97799]
objective value function right now is: -1135.206336327307
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-157.4768]
objective value function right now is: -1094.0371000039395
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-166.20607]
objective value function right now is: -1151.2736428515805
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-171.74658]
objective value function right now is: -1378.0985413018805
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-178.08067]
objective value function right now is: -1439.9126431240024
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-177.94812]
objective value function right now is: -1443.0420838927398
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-215.0461]
objective value function right now is: -1137.1526748102806
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-229.82661]
objective value function right now is: -1154.69478098552
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-207.82475]
objective value function right now is: -1441.4534044688787
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-181.17606]
objective value function right now is: -1457.4048028680247
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-152.95348]
objective value function right now is: -1471.5109945595282
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-151.41023]
objective value function right now is: -851.0046421902944
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-199.58214]
objective value function right now is: -928.7208127001168
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-237.14201]
objective value function right now is: -959.5934206153297
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-211.19678]
objective value function right now is: -1437.4224699912374
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-181.07773]
objective value function right now is: -1452.3862079216074
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-149.74529]
objective value function right now is: -1473.551524407308
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-116.712265]
objective value function right now is: -1500.4077114083718
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-85.558914]
objective value function right now is: -1525.30813553708
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-77.8444]
objective value function right now is: -1527.235480589207
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-71.26585]
objective value function right now is: -1533.6229100467847
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-64.57201]
objective value function right now is: -1534.8236843167153
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-57.25574]
objective value function right now is: -1538.9292255317382
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-49.598827]
objective value function right now is: -1541.6956915757128
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-42.29519]
objective value function right now is: -1542.5027501148193
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1547.1236250450233
Current xi:  [-36.228893]
objective value function right now is: -1547.1236250450233
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1547.5028898596215
Current xi:  [-31.531616]
objective value function right now is: -1547.5028898596215
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-25.443136]
objective value function right now is: -1546.8309310544694
new min fval from sgd:  -1551.3689460663293
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.632381]
objective value function right now is: -1551.3689460663293
new min fval from sgd:  -1551.8117992840364
new min fval from sgd:  -1551.9247759436987
new min fval from sgd:  -1551.947527212122
new min fval from sgd:  -1551.9946282880796
new min fval from sgd:  -1552.0916059637275
new min fval from sgd:  -1552.2514653363933
new min fval from sgd:  -1552.3166644020775
new min fval from sgd:  -1552.3275740069298
new min fval from sgd:  -1552.3314847388165
new min fval from sgd:  -1552.3433702593982
new min fval from sgd:  -1552.425772650558
new min fval from sgd:  -1552.4391789968201
new min fval from sgd:  -1552.5567097289377
new min fval from sgd:  -1552.6871268162895
new min fval from sgd:  -1552.7150989495904
new min fval from sgd:  -1552.8174418137683
new min fval from sgd:  -1552.9263039008729
new min fval from sgd:  -1552.9593000817067
new min fval from sgd:  -1553.0490852418548
new min fval from sgd:  -1553.0544133491835
new min fval from sgd:  -1553.0934662563684
new min fval from sgd:  -1553.1731084188148
new min fval from sgd:  -1553.178625733138
new min fval from sgd:  -1553.2863496942236
new min fval from sgd:  -1553.3293609057414
new min fval from sgd:  -1553.3596259946708
new min fval from sgd:  -1553.3656295794503
new min fval from sgd:  -1553.4061696427236
new min fval from sgd:  -1553.451376426916
new min fval from sgd:  -1553.5431015545073
new min fval from sgd:  -1553.6346888817736
new min fval from sgd:  -1553.6393600861343
new min fval from sgd:  -1553.6419721208579
new min fval from sgd:  -1553.6644844215796
new min fval from sgd:  -1553.6896286344095
new min fval from sgd:  -1553.7979560480514
new min fval from sgd:  -1553.8670086397126
new min fval from sgd:  -1553.9140063091359
new min fval from sgd:  -1553.9287479637644
new min fval from sgd:  -1553.9366367300727
new min fval from sgd:  -1554.0225326405666
new min fval from sgd:  -1554.1396464054794
new min fval from sgd:  -1554.146102902545
new min fval from sgd:  -1554.1609106613096
new min fval from sgd:  -1554.1976573986674
new min fval from sgd:  -1554.2662884452693
new min fval from sgd:  -1554.2757807526718
new min fval from sgd:  -1554.290599161082
new min fval from sgd:  -1554.388316045717
new min fval from sgd:  -1554.4169188135859
new min fval from sgd:  -1554.5824404789712
new min fval from sgd:  -1554.622282657118
new min fval from sgd:  -1554.6769946415047
new min fval from sgd:  -1554.8664735596305
new min fval from sgd:  -1554.8760945666222
new min fval from sgd:  -1554.9219611768704
new min fval from sgd:  -1554.9603516908264
new min fval from sgd:  -1555.0791743392792
new min fval from sgd:  -1555.1170133502549
new min fval from sgd:  -1555.1657982457855
new min fval from sgd:  -1555.176232941159
new min fval from sgd:  -1555.2061509097618
new min fval from sgd:  -1555.2430102718756
new min fval from sgd:  -1555.3444117378378
new min fval from sgd:  -1555.5397196354538
new min fval from sgd:  -1555.684355259545
new min fval from sgd:  -1555.7876226761728
new min fval from sgd:  -1555.8071530436846
new min fval from sgd:  -1555.8111288522582
new min fval from sgd:  -1555.828282890795
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.506785]
objective value function right now is: -1555.3508402994564
new min fval from sgd:  -1555.8914638782667
new min fval from sgd:  -1556.0588319959352
new min fval from sgd:  -1556.1594390638345
new min fval from sgd:  -1556.205103072682
new min fval from sgd:  -1556.3686662840983
new min fval from sgd:  -1556.4759129627166
new min fval from sgd:  -1556.6188346060676
new min fval from sgd:  -1556.820014598016
new min fval from sgd:  -1556.855246525834
new min fval from sgd:  -1556.876369433175
new min fval from sgd:  -1556.9563229775008
new min fval from sgd:  -1556.973692714282
new min fval from sgd:  -1556.9844240772104
new min fval from sgd:  -1557.049470005838
new min fval from sgd:  -1557.102284614329
new min fval from sgd:  -1557.289555200972
new min fval from sgd:  -1557.3890455333744
new min fval from sgd:  -1557.453281332282
new min fval from sgd:  -1557.4563445707681
new min fval from sgd:  -1557.4575685986074
new min fval from sgd:  -1557.5742406091024
new min fval from sgd:  -1557.6850918062748
new min fval from sgd:  -1557.7336858775811
new min fval from sgd:  -1557.7462347684686
new min fval from sgd:  -1557.9996822991836
new min fval from sgd:  -1558.1168180688016
new min fval from sgd:  -1558.1432092870862
new min fval from sgd:  -1558.1690903008162
new min fval from sgd:  -1558.3738065424236
new min fval from sgd:  -1558.3927616375609
new min fval from sgd:  -1558.4539100950915
new min fval from sgd:  -1558.876660724943
new min fval from sgd:  -1559.1242591069667
new min fval from sgd:  -1559.1892864026752
new min fval from sgd:  -1559.3288653604593
new min fval from sgd:  -1559.366857899707
new min fval from sgd:  -1559.3915578866881
new min fval from sgd:  -1559.54613372285
new min fval from sgd:  -1559.6218528608265
new min fval from sgd:  -1559.9552568770357
new min fval from sgd:  -1560.133291565361
new min fval from sgd:  -1560.1633708924383
new min fval from sgd:  -1560.2476226646486
new min fval from sgd:  -1560.584329924545
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.9893244]
objective value function right now is: -1555.2630997706253
new min fval from sgd:  -1561.0184264849936
new min fval from sgd:  -1561.2046473477205
new min fval from sgd:  -1561.321586978626
new min fval from sgd:  -1561.5759582989747
new min fval from sgd:  -1561.578087115371
new min fval from sgd:  -1561.6172109219563
new min fval from sgd:  -1561.8190320180042
new min fval from sgd:  -1561.8691234016596
new min fval from sgd:  -1561.9208900566089
new min fval from sgd:  -1561.9392800248547
new min fval from sgd:  -1561.9452212971482
new min fval from sgd:  -1561.9622796646283
new min fval from sgd:  -1561.971415093693
new min fval from sgd:  -1561.9750160277601
new min fval from sgd:  -1561.9979692520205
new min fval from sgd:  -1562.0063878555509
new min fval from sgd:  -1562.0155236029495
new min fval from sgd:  -1562.0410333661857
new min fval from sgd:  -1562.0451825991759
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00043115]
objective value function right now is: -1560.9271053021835
new min fval from sgd:  -1562.0781296855412
new min fval from sgd:  -1562.1589484978078
new min fval from sgd:  -1562.1992333785188
new min fval from sgd:  -1562.2151874208441
new min fval from sgd:  -1562.232034337256
new min fval from sgd:  -1562.2402385133341
new min fval from sgd:  -1562.2428057424268
new min fval from sgd:  -1562.255917003547
new min fval from sgd:  -1562.2743355896546
new min fval from sgd:  -1562.2822446302594
new min fval from sgd:  -1562.3214341621097
new min fval from sgd:  -1562.347566715374
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00058637]
objective value function right now is: -1562.325012907329
new min fval from sgd:  -1562.350407557936
new min fval from sgd:  -1562.3514291962595
new min fval from sgd:  -1562.3619131747762
new min fval from sgd:  -1562.3802961247773
new min fval from sgd:  -1562.3903640620815
new min fval from sgd:  -1562.417886759881
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00025296]
objective value function right now is: -1562.311318221202
min fval:  -1562.417886759881
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -9.5627,   0.8444],
        [  1.9087,  -1.8940],
        [-16.5840,   3.8018],
        [  8.5009, -10.6019],
        [ -2.4482,   0.5561],
        [  4.3512, -13.3490],
        [  2.2446,  -1.3589],
        [-14.8032,   3.4276],
        [ -1.7584, -13.4157],
        [ -6.1172,  -4.1337]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  7.0398,   3.4204,  15.3087,  -3.3674,  -2.0751, -11.4538,   2.6671,
         13.5310, -12.4285,   8.3909], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  0.6580,   0.3175,   2.3217,   1.2157,   0.0515,  -0.1486,   0.6021,
           1.7237,  -0.1501,   3.2715],
        [ -0.9314,   1.0791,   0.0378,   1.1652,  -0.8973,  -0.0532,   1.0722,
          -0.0528,  -0.8218,   1.0098],
        [ -0.4907,   0.9185,  -0.2135,   0.8512,  -0.9018,  -1.6864,   0.9299,
          -0.3553,  -1.8309,   0.9173],
        [  0.6566,   0.1677,   2.6336,   1.1302,   0.0674,  -0.5464,   0.4453,
           1.9414,  -0.5932,   3.4761],
        [ -0.0897,  -0.9530,  -0.5463,  -0.9449,  -0.0815,  -0.2083,  -0.9520,
          -0.4755,  -0.0487,  -0.9305],
        [-13.6219,   3.4839,  -2.2305,  16.5936,   0.1627,  15.9844,  -1.6721,
          -7.0830,  16.9324, -14.7498],
        [ -0.0897,  -0.9530,  -0.5463,  -0.9449,  -0.0815,  -0.2083,  -0.9520,
          -0.4755,  -0.0487,  -0.9305],
        [ -0.7737,  -3.4791,  -6.7128,   9.8681,  -0.1645,   7.3721,  -3.2474,
          -6.3179,   6.7943,  -0.1627],
        [  0.6582,   0.3159,   2.3260,   1.2142,   0.0516,  -0.1510,   0.5996,
           1.7262,  -0.1524,   3.2756],
        [  0.6601,   0.2545,   2.4634,   1.1754,   0.0555,  -0.2628,   0.5274,
           1.8162,  -0.2625,   3.3864]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.6259,  1.1766,  0.9280, -0.8885, -0.9558,  1.6667, -0.9558,  1.8374,
        -0.6297, -0.7482], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  5.4576,  -6.5447, -13.9947,   5.4794,  -0.0705, -15.3839,  -0.0705,
          -8.2862,   5.4583,   5.4753]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-14.3377,  -0.8521],
        [  1.0000,  -6.3175],
        [-26.1479, -20.2095],
        [-22.4214,  -6.2274],
        [  0.2564,   5.1515],
        [ -6.7717,  -2.8989],
        [-15.4535,  -7.3642],
        [  7.8833,   3.7614],
        [  3.1423,   5.2775],
        [  7.5975,   0.7859]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  8.1741,   2.0356, -14.9283,  -1.7397,   3.2918,   7.1819, -11.6855,
          5.1522,   2.5352,  -8.1446], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-7.8076e-01, -1.4159e+00,  2.0103e-01, -3.2716e-02, -1.1902e+00,
         -1.3224e+00, -6.1940e-04, -2.3696e+00, -1.4394e+00, -8.5209e-01],
        [ 8.2781e-01,  2.0939e+00,  2.7632e-01,  8.0893e-02,  6.7542e-01,
          2.0266e+00,  7.6816e-03,  2.1326e+00,  5.7752e-01, -3.0148e-01],
        [-6.9673e-01, -1.5808e+00,  2.9195e-04, -4.6366e-02,  8.1715e-02,
         -1.5194e+00, -4.7787e-03, -1.8397e+00,  6.7127e-02,  3.8254e-01],
        [ 1.3303e+01,  5.6540e+00, -1.1348e+01,  4.8719e+00,  4.2585e+00,
          1.6968e+00, -1.3917e+00, -1.1493e+00,  5.3951e+00,  8.6095e-01],
        [ 5.0098e+00, -1.1104e+00, -8.5350e-01, -2.2110e+00,  4.1171e+00,
         -1.4336e+00,  1.8379e-03, -4.4728e-01,  3.8155e+00, -5.0494e+00],
        [-7.3513e-01, -5.5106e-01, -1.5953e-01, -8.1770e-02, -2.3587e-02,
         -2.5100e+00, -2.5433e-02, -3.4268e+00, -1.5423e-01, -1.4186e-01],
        [ 8.6551e-01,  2.4773e+00,  5.4771e-01,  1.3374e-01,  1.4063e+00,
          2.1770e+00,  5.9489e-02,  3.7480e+00,  1.3954e+00, -5.9080e-01],
        [-1.0549e-01, -4.4065e+00,  2.8495e+00,  2.6007e+00, -1.2855e-01,
          4.1174e+00,  1.9896e-02,  1.9541e+00, -1.3328e+00,  1.2147e+00],
        [-3.9045e+00,  9.8846e-01, -7.3241e+00,  4.8408e+00,  8.2287e+00,
         -2.1667e+00, -2.8706e+00,  4.5735e+00,  1.4348e+01,  3.9331e-01],
        [-1.5388e+01,  5.5749e+00, -2.9377e+01, -2.1912e+00,  1.0607e+01,
          1.6372e+01, -3.7262e-01,  7.0065e+00,  1.8533e+01,  2.2223e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.5990,  4.9697, -5.7820, -1.0195, -7.6084, -3.9629,  2.0731,  3.4354,
         1.0236, 14.0761], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 0.0199,  4.0587, -1.2783, -4.6393,  0.9627, -1.0795,  2.8638,  0.4884,
          5.0391, -7.6486],
        [-0.0202, -4.3422,  1.2785,  4.6393, -0.9624,  1.0796, -2.8640, -0.4883,
         -5.0389,  7.6411]], device='cuda:0'))])
xi:  [7.025728e-05]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 381.3908159630494
W_T_median: 178.92642910295262
W_T_pctile_5: 0.0
W_T_CVAR_5_pct: -13.571964618975336
Average q (qsum/M+1):  52.777895035282256
Optimal xi:  [7.025728e-05]
Expected(across Rb) median(across samples) p_equity:  0.32640152672926587
obj fun:  tensor(-1562.4179, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
loaded xi:  7.025728e-05
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1089.6740911163445
Current xi:  [-0.84888595]
objective value function right now is: -1089.6740911163445
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1467.1548916783815
Current xi:  [-0.00994929]
objective value function right now is: -1467.1548916783815
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.6365253]
objective value function right now is: -894.3431516716411
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.428534]
objective value function right now is: -28.338002897312
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-26.225458]
objective value function right now is: -748.7757453814123
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-71.05505]
objective value function right now is: -878.5861755543547
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-71.41027]
objective value function right now is: -1438.4712316861055
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-104.54201]
objective value function right now is: -730.0878738869739
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-131.26299]
objective value function right now is: -951.269752839074
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-154.54701]
objective value function right now is: -1015.9264579048644
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-140.23407]
objective value function right now is: -1037.067469304738
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-120.916595]
objective value function right now is: -1416.0138927231128
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-95.08605]
objective value function right now is: -1408.5731300400976
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1476.6248046328642
Current xi:  [-64.20712]
objective value function right now is: -1476.6248046328642
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1495.86292913859
Current xi:  [-33.44507]
objective value function right now is: -1495.86292913859
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.2058252]
objective value function right now is: -937.8543157355455
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.0762873]
objective value function right now is: -1357.0913635902969
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-61.533436]
objective value function right now is: -840.7000839387794
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-94.71666]
objective value function right now is: -933.5195442332351
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-123.300285]
objective value function right now is: -980.9797215775294
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-140.3678]
objective value function right now is: -988.7653638718145
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-143.71982]
objective value function right now is: -1387.6745673053751
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.01578]
objective value function right now is: -914.6801160083182
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-186.32661]
objective value function right now is: -961.8877229342121
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-189.31783]
objective value function right now is: -542.5482248491692
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-213.56926]
objective value function right now is: -1296.9125294621265
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-214.05646]
objective value function right now is: -935.3943340146618
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-244.91081]
objective value function right now is: -613.1619020817249
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-278.76953]
objective value function right now is: -644.3085552268033
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-309.6859]
objective value function right now is: -666.2486308725242
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-320.8759]
objective value function right now is: -1196.0833332029433
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-273.8683]
objective value function right now is: -1241.7920140050358
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-232.88318]
objective value function right now is: -1293.8709243553988
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-201.22003]
objective value function right now is: -489.34808714533375
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-201.21822]
objective value function right now is: -1339.1080097861977
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-193.4436]
objective value function right now is: -1358.5750789771578
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-185.62006]
objective value function right now is: -1367.5474548337506
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-177.8093]
objective value function right now is: -1374.9861554257093
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-170.03384]
objective value function right now is: -1383.7635530597192
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-162.36441]
objective value function right now is: -1391.4539093919516
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-154.7367]
objective value function right now is: -1397.352912877118
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-147.03094]
objective value function right now is: -1404.7645640028127
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-139.15123]
objective value function right now is: -1414.2570172126213
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-131.20523]
objective value function right now is: -1422.483758119987
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-123.53817]
objective value function right now is: -1430.081661254316
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-116.09326]
objective value function right now is: -1436.3182467514962
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-108.55655]
objective value function right now is: -1443.360571966499
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-100.72039]
objective value function right now is: -1450.5685766493502
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-95.94821]
objective value function right now is: -1455.2723616191643
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-94.38201]
objective value function right now is: -1456.8156345718996
min fval:  -1408.9522062972537
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -9.8071,   3.0896],
        [ -1.4757,   3.5169],
        [-16.9842,   4.5713],
        [  7.4072,  -7.3850],
        [  5.0582,  10.0758],
        [  5.7225, -13.8235],
        [ -5.6951,  -3.5036],
        [-15.2118,   4.9011],
        [ -3.1110, -14.6440],
        [ -4.6047,  -2.8679]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  9.1466,  -2.8875,  15.7025,  -9.1723,  -1.6724, -12.0354,   5.7754,
         14.0298, -12.3586,  10.1222], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  1.1066,   1.5078,   1.5896,   1.0370,  -0.8426,   0.4800,   2.3657,
           1.5525,   0.2790,   1.1895],
        [  1.6261,   0.4586,   1.1912,   1.0206,   0.1208,   0.8721,   1.6675,
           1.2453,   0.3431,   2.1759],
        [  1.7322,   0.5026,   1.4902,   1.0993,   0.3082,   1.0995,   1.8628,
           1.5029,   0.5187,   2.4303],
        [  0.3729,   0.3185,   0.6615,   3.0585,   0.5984,  -0.0693,   2.4281,
           0.8802,  -3.7548,   2.5643],
        [ -1.9087,  -0.5057,  -1.3988,  -1.0761,   0.1626,  -2.1998,  -1.1134,
          -1.4747,  -2.3022,  -2.0514],
        [-12.0607,   3.2770,  -2.2766,  15.9463,   2.0731,  19.2341,  -1.0441,
          -6.9811,  20.7683, -13.4112],
        [ -1.9253,  -0.5228,  -1.3815,  -1.0859,   0.1836,  -2.2522,  -1.0869,
          -1.4639,  -2.3650,  -2.0372],
        [ -8.2203,   0.2844, -13.2040,  11.8539,   6.4004,   4.8120,  -1.7268,
          -9.0672,   3.2185,   2.6067],
        [  1.1050,   1.5072,   1.5930,   1.0256,  -0.8484,   0.4770,   2.3574,
           1.5572,   0.2759,   1.1968],
        [  1.0338,   1.5230,   1.5774,   1.0102,  -0.8705,   0.5929,   2.2533,
           1.6057,   0.2096,   1.2869]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 1.7885,  2.0986,  2.3552,  2.6454, -1.1790,  2.2313, -1.1549,  4.5950,
         1.7927,  1.8197], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  4.6708,  -7.1911, -14.2808,   4.6494,   1.8964, -17.8316,   1.8982,
         -18.7308,   4.6716,   4.6887]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-15.1804,  -0.1546],
        [  6.1415,   4.8827],
        [-19.3358, -24.3164],
        [-16.3914, -11.7735],
        [  2.2229,  -0.4605],
        [  1.6088,   0.1050],
        [ 14.6613,   2.3723],
        [  6.0601,   6.4819],
        [ -3.3019,  -7.0298],
        [ 12.8761,   2.7573]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 11.0144,   2.4849, -10.4296,   3.8150,   5.1400,   5.7272,  -0.2400,
          4.7266,  -6.1737,  -4.1390], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -2.1309,  -0.8090,  -1.2874,   2.4896,  -2.8187,  -2.5273,  -0.9715,
          -1.8968,   0.4328,  -1.4074],
        [ -2.7812,   1.1252,  -1.6659,   1.0120,  -2.0191,   2.3475,  -0.6080,
           5.0518,  -5.3466,   3.0250],
        [ -0.9741,  -1.0537,  -0.7536,  -0.8505,  -1.5874,  -1.5857,  -1.4385,
          -1.1102,  -0.4256,  -1.0070],
        [  8.3279,   0.3148, -11.1469,   2.0709,  -3.1549,  -4.5162,  -0.4394,
          -4.2428,   0.1376,  -3.0903],
        [  5.8182,   0.6412,   2.2164,   1.2254,   5.3627,   0.9059,   7.7526,
           1.5331,   2.7464,   0.5256],
        [  2.3242,   2.2240,   1.2414,   1.2529,   2.9133,   2.7973,   2.4372,
           2.3486,   0.6994,   1.7088],
        [ -0.2808,   0.9163,  -4.4958,   5.3965,  -3.3008,   2.7419,   2.4792,
           6.0468,   0.6150,   0.6426],
        [ -5.4525,  -2.2682,  -3.7300,  -3.3686,  -2.5685,  -0.5696,  -0.1037,
          -1.2365,   0.0415,  -3.1677],
        [ -1.8978,  -1.1803,  -0.8084,  -0.7915,  -0.7653,  -3.8258,  -0.6434,
          -0.9332,   2.2765,  -3.6641],
        [-12.9705,  13.5150, -28.3295,  -3.4657,  10.3104,  18.7955,   6.3111,
          20.7246, -12.3289,  25.9453]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.9372,  7.2662, -1.5908, -7.5479, -3.3988,  2.7476,  4.4842, -0.7688,
        -2.7647, 16.5622], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 0.1238,  5.8103,  0.0701, -3.5798, -1.9481, -2.5858,  4.8626,  1.9067,
          3.5304, -5.9074],
        [-0.1239, -6.0937, -0.0701,  3.5798,  1.9483,  2.5855, -4.8628, -1.9066,
         -3.5302,  5.9000]], device='cuda:0'))])
xi:  [-123.53817]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 490.8423260474943
W_T_median: 165.69873851196508
W_T_pctile_5: -32.02758818203269
W_T_CVAR_5_pct: -89.18847193684651
Average q (qsum/M+1):  52.57881016885081
Optimal xi:  [-123.53817]
Expected(across Rb) median(across samples) p_equity:  0.3208399633566538
obj fun:  tensor(-1408.9522, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
loaded xi:  -123.53817
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1281.8193181428505
Current xi:  [-84.16909]
objective value function right now is: -1281.8193181428505
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1317.7750595187758
Current xi:  [-47.592648]
objective value function right now is: -1317.7750595187758
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1391.926538766258
Current xi:  [-13.656406]
objective value function right now is: -1391.926538766258
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1436.0618318960053
Current xi:  [-0.02513186]
objective value function right now is: -1436.0618318960053
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02422935]
objective value function right now is: -1317.3601352825162
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.10131474]
objective value function right now is: -1355.9255127696936
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [0.08319736]
objective value function right now is: -1345.831831803308
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.538633]
objective value function right now is: -1413.155162520048
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1446.8461401416544
Current xi:  [33.86469]
objective value function right now is: -1446.8461401416544
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1484.271167558952
Current xi:  [58.648724]
objective value function right now is: -1484.271167558952
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1518.4669085832572
Current xi:  [84.57883]
objective value function right now is: -1518.4669085832572
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.47888]
objective value function right now is: -1508.6020047257482
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1540.2822436830243
Current xi:  [125.0204]
objective value function right now is: -1540.2822436830243
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1551.7466580409334
Current xi:  [138.01125]
objective value function right now is: -1551.7466580409334
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.71298]
objective value function right now is: -1550.5305061495997
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [153.11293]
objective value function right now is: -1533.2450426363805
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [156.2857]
objective value function right now is: -1516.8835110810076
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.963690103875
Current xi:  [159.00621]
objective value function right now is: -1555.963690103875
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.06555]
objective value function right now is: -1553.7643811886219
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [160.92961]
objective value function right now is: -1551.2916066240734
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1556.8004395871367
Current xi:  [160.46686]
objective value function right now is: -1556.8004395871367
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [159.30731]
objective value function right now is: -1523.2284572413196
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.64697]
objective value function right now is: -1530.2124427587573
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.74976]
objective value function right now is: -1550.2662067706083
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.00291]
objective value function right now is: -1546.281151684174
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [160.1169]
objective value function right now is: -1555.6178181320033
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [160.23434]
objective value function right now is: -1520.673625194907
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [160.3999]
objective value function right now is: -1550.99070564509
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [160.3983]
objective value function right now is: -1524.6653905617372
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.22826]
objective value function right now is: -1541.6226524417498
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1559.0088523534955
Current xi:  [161.08307]
objective value function right now is: -1559.0088523534955
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.27449]
objective value function right now is: -1522.863066540848
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [160.55042]
objective value function right now is: -1538.6373794479346
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1560.7198588221142
Current xi:  [160.28366]
objective value function right now is: -1560.7198588221142
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.3685]
objective value function right now is: -1549.7028584283591
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.5067351211762
Current xi:  [162.83429]
objective value function right now is: -1562.5067351211762
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.6452442236828
Current xi:  [164.0074]
objective value function right now is: -1562.6452442236828
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.11452]
objective value function right now is: -1562.011341586907
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.12433]
objective value function right now is: -1548.6267380494255
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1564.5582685532147
Current xi:  [165.49197]
objective value function right now is: -1564.5582685532147
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.01479]
objective value function right now is: -1562.9791094145685
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.83192]
objective value function right now is: -1554.2389228853508
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.18123]
objective value function right now is: -1546.241023159429
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1565.178885271031
Current xi:  [166.65314]
objective value function right now is: -1565.178885271031
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.93901]
objective value function right now is: -1561.0731146071603
new min fval from sgd:  -1565.20161850811
new min fval from sgd:  -1565.249819500258
new min fval from sgd:  -1565.285068365544
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.74652]
objective value function right now is: -1562.3525756366312
new min fval from sgd:  -1565.4082932939148
new min fval from sgd:  -1565.4498211645982
new min fval from sgd:  -1565.5282587999698
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.93494]
objective value function right now is: -1555.312722829397
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.07527]
objective value function right now is: -1562.5720137661417
new min fval from sgd:  -1565.5406038364815
new min fval from sgd:  -1565.554858647961
new min fval from sgd:  -1565.5647580198074
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.19456]
objective value function right now is: -1565.4124286876222
new min fval from sgd:  -1565.5761078911842
new min fval from sgd:  -1565.6321478821887
new min fval from sgd:  -1565.6645420752207
new min fval from sgd:  -1565.6688719668748
new min fval from sgd:  -1565.674807585618
new min fval from sgd:  -1565.6955642365608
new min fval from sgd:  -1565.7111510275245
new min fval from sgd:  -1565.721717962929
new min fval from sgd:  -1565.7432214912014
new min fval from sgd:  -1565.7477874375818
new min fval from sgd:  -1565.7506137044022
new min fval from sgd:  -1565.7586526224316
new min fval from sgd:  -1565.783240332146
new min fval from sgd:  -1565.797198816532
new min fval from sgd:  -1565.8016708935393
new min fval from sgd:  -1565.8154031607933
new min fval from sgd:  -1565.8205692094944
new min fval from sgd:  -1565.82205427171
new min fval from sgd:  -1565.824172910852
new min fval from sgd:  -1565.8480008834572
new min fval from sgd:  -1565.8551254981662
new min fval from sgd:  -1565.929250924341
new min fval from sgd:  -1565.9765387325358
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.23195]
objective value function right now is: -1561.3154778088015
min fval:  -1565.9765387325358
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-17.9409,   2.6908],
        [  3.0718,   5.1444],
        [-30.7674,  12.9550],
        [ -1.3207,   0.1627],
        [  2.0232,   7.2667],
        [-14.8650, -16.7718],
        [ -1.3207,   0.1627],
        [-20.1534,   5.2518],
        [  6.1430, -13.4208],
        [  1.3066,  -0.3039]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 12.5842, -10.4431,  12.2911,  -3.2255,  -7.4270, -12.9241,  -3.2255,
         13.8909,  -9.0329,   5.4023], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.7101e-01, -6.5131e-02, -1.8950e-01, -6.1640e-03, -1.6126e-01,
         -2.6144e-02, -6.1640e-03, -6.1798e-01, -2.2737e-01, -1.3288e+00],
        [-1.1670e+01,  5.3083e+00,  1.3625e+00,  2.9899e-02,  4.4389e+00,
         -1.3937e-03,  2.9899e-02, -6.8321e+00,  1.3868e+00,  8.6458e-01],
        [-5.7101e-01, -6.5131e-02, -1.8950e-01, -6.1640e-03, -1.6126e-01,
         -2.6144e-02, -6.1640e-03, -6.1798e-01, -2.2737e-01, -1.3288e+00],
        [ 8.8487e+00, -1.0077e+01,  8.5963e-01,  4.3756e-03, -6.8119e+00,
         -3.0217e-01,  4.3757e-03,  1.4948e+01, -1.3312e+01, -3.3810e+00],
        [ 9.9186e-02,  1.7280e-01,  3.4380e+00, -1.8358e-01,  8.6792e-01,
          2.7166e+00, -1.8358e-01, -6.5020e-01, -3.3026e-01,  1.8562e+00],
        [-1.1131e+01,  1.7696e-02, -1.1018e+01,  5.9841e-02,  1.5890e-01,
          2.6020e+01,  5.9841e-02, -8.1687e+00,  2.3317e+01, -8.4076e+00],
        [ 1.0505e-01,  1.7343e-01,  3.4313e+00, -1.8142e-01,  8.7047e-01,
          2.7088e+00, -1.8142e-01, -6.4646e-01, -3.2670e-01,  1.8683e+00],
        [-5.7101e-01, -6.5131e-02, -1.8950e-01, -6.1640e-03, -1.6126e-01,
         -2.6144e-02, -6.1640e-03, -6.1798e-01, -2.2737e-01, -1.3288e+00],
        [-5.7101e-01, -6.5131e-02, -1.8950e-01, -6.1640e-03, -1.6126e-01,
         -2.6144e-02, -6.1640e-03, -6.1798e-01, -2.2737e-01, -1.3288e+00],
        [-5.7101e-01, -6.5131e-02, -1.8950e-01, -6.1640e-03, -1.6126e-01,
         -2.6144e-02, -6.1640e-03, -6.1798e-01, -2.2737e-01, -1.3288e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.3314,  0.9054, -1.3314, -1.4704,  1.8576,  5.2844,  1.8698, -1.3314,
        -1.3314, -1.3314], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-2.3820e-03, -5.3635e+00, -2.3819e-03,  9.9074e+00, -6.6492e+00,
         -1.9310e+01, -6.8033e+00, -2.3820e-03, -2.3819e-03, -2.3820e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-14.2246,   2.4438],
        [  0.8953,  13.6488],
        [  8.2507,  -4.6762],
        [-13.9372,  -4.5084],
        [ -2.3038,   5.6845],
        [ -5.3101,  -7.5494],
        [ -0.3419,   4.8062],
        [ 16.0586,  12.2259],
        [ -1.2033,   2.3884],
        [ 15.3836,   0.2548]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 13.9545,  10.3769,  -9.7658,   0.7189,  -0.5296,  -5.8945,  -2.9545,
          8.8083,  -3.6450, -12.6067], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 8.7358e-01,  2.9860e+00,  1.1239e+00,  3.2385e+00,  2.9914e-01,
          3.3428e+00,  4.9676e-01,  9.3750e-01, -1.1959e-01,  9.7352e-01],
        [-5.9173e+00,  4.5829e-01, -1.4919e+00,  1.5005e+00,  2.8268e+00,
         -1.8006e+00,  1.2911e+00,  8.9843e-01, -1.2674e-01, -1.5206e+01],
        [-2.5874e+00, -2.8957e-01, -1.2893e+00, -2.1917e-01, -1.4454e-01,
         -3.5933e-01, -1.1267e-01, -3.3208e+00, -3.2734e-01, -1.3173e-01],
        [ 3.5147e+00, -1.9962e+01,  2.3063e+00,  2.9539e+00, -4.8904e-01,
          1.4588e+00,  1.1889e-02, -1.1420e+01,  1.4162e-01, -1.5796e+01],
        [ 1.7456e-01, -1.6355e+01,  1.0039e+00,  4.8260e+00, -1.1259e+00,
          2.1046e+00, -4.6758e-01, -8.9468e-01, -1.3005e-01, -2.8076e+00],
        [ 2.7262e+00, -2.9390e+00,  1.9757e+00, -5.4081e+00, -1.0386e-02,
         -2.6669e-01, -2.8462e-01,  3.7479e+00, -9.0393e-03,  4.5705e+00],
        [ 2.9449e+00, -2.5214e+01, -3.0951e+00,  1.5695e+01, -5.6586e-01,
         -4.0406e+00, -1.3004e-01, -3.6502e+01, -5.4281e-01, -6.9436e+00],
        [-4.9830e-01,  2.6317e+00,  4.4585e-01,  4.5274e+00,  1.5017e-01,
          2.4158e+00,  2.0713e+00, -1.2353e-01,  4.8383e-01,  3.0333e-01],
        [-2.5437e+00, -3.0551e-01, -1.3138e+00, -2.3540e-01, -1.3839e-01,
         -3.9716e-01, -1.1139e-01, -3.2919e+00, -3.2188e-01, -1.6763e-01],
        [ 2.1333e+00, -1.4047e+00,  7.3767e-01, -2.2770e+00,  8.9504e-01,
         -1.3877e-01,  2.4156e-01,  2.8373e+00,  6.2897e-01, -2.8277e-02]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 3.8671, -0.0712, -2.9628,  1.7812, -0.8027,  3.8729, -7.8277,  2.7394,
        -2.9823,  3.8574], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  4.7344,   5.8306,   0.4626,  -8.5214,  -0.6657,  -2.4452,  16.8448,
           1.2263,   0.4340,  -3.1003],
        [ -4.7339,  -6.0025,  -0.4627,   8.5159,   0.6659,   2.4443, -16.8351,
          -1.2650,  -0.4340,   3.0931]], device='cuda:0'))])
xi:  [168.25294]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 585.7380719462438
W_T_median: 377.07375063901304
W_T_pctile_5: 168.46497090608833
W_T_CVAR_5_pct: 15.62101911061916
Average q (qsum/M+1):  49.004051946824596
Optimal xi:  [168.25294]
Expected(across Rb) median(across samples) p_equity:  0.26424398546417555
obj fun:  tensor(-1565.9765, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
loaded xi:  168.25294
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1594.1471914812703
Current xi:  [173.47614]
objective value function right now is: -1594.1471914812703
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [177.43115]
objective value function right now is: -1548.8908013102223
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [178.32864]
objective value function right now is: -1592.8006662303844
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1595.424365481822
Current xi:  [180.30464]
objective value function right now is: -1595.424365481822
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.1696]
objective value function right now is: -1500.191725697737
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.58522]
objective value function right now is: -1590.1246624432151
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [182.43546]
objective value function right now is: -1543.068240290556
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.49063]
objective value function right now is: -1584.530055727129
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.92033]
objective value function right now is: -314.6424646520528
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.04097]
objective value function right now is: -1577.9886198547383
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1596.1811848414536
Current xi:  [181.89146]
objective value function right now is: -1596.1811848414536
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.31357]
objective value function right now is: -1577.338668537707
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.99146]
objective value function right now is: -1584.3896422668925
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [184.49641]
objective value function right now is: -1586.021594767477
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [176.81949]
objective value function right now is: -1362.6582813569382
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.98242]
objective value function right now is: -1568.7226368175484
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [156.93193]
objective value function right now is: -1539.657400608868
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.29402]
objective value function right now is: -1556.7164781477657
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.8766]
objective value function right now is: -1542.775751439451
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [172.97607]
objective value function right now is: -1557.1172853113749
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [175.58315]
objective value function right now is: -1481.9506271799758
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [176.84929]
objective value function right now is: -1571.4227323999132
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [179.27654]
objective value function right now is: -1587.0163363958582
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.666]
objective value function right now is: -1578.645128714437
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.0113]
objective value function right now is: -1571.7335934777495
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.21968]
objective value function right now is: -1487.1440187229382
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.45035]
objective value function right now is: -1566.5541321046674
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [164.40562]
objective value function right now is: -1480.305305936343
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [164.99258]
objective value function right now is: -1556.4997335844507
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.22827]
objective value function right now is: -1563.9183909813544
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [171.76283]
objective value function right now is: -1586.6991935908814
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [172.65817]
objective value function right now is: -1345.7742249198682
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.92883]
objective value function right now is: -1542.7951019793743
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [171.12431]
objective value function right now is: -1581.1970254814848
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [175.85489]
objective value function right now is: -1581.129327235195
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [177.33229]
objective value function right now is: -1592.9087992340117
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1597.400054025315
Current xi:  [178.31712]
objective value function right now is: -1597.400054025315
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1598.3025701993085
Current xi:  [179.6267]
objective value function right now is: -1598.3025701993085
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1600.1471489563287
Current xi:  [180.72643]
objective value function right now is: -1600.1471489563287
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.8888]
objective value function right now is: -1590.274203798023
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.99522]
objective value function right now is: -1598.5559456169233
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1600.6905669684857
Current xi:  [183.60829]
objective value function right now is: -1600.6905669684857
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.4165122968132
Current xi:  [184.31133]
objective value function right now is: -1602.4165122968132
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.42938]
objective value function right now is: -1591.7250993160114
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.41917]
objective value function right now is: -1600.673267573571
new min fval from sgd:  -1602.4299311635427
new min fval from sgd:  -1602.459158549549
new min fval from sgd:  -1602.5744876471304
new min fval from sgd:  -1602.6398861564903
new min fval from sgd:  -1602.7141502096877
new min fval from sgd:  -1602.73322030675
new min fval from sgd:  -1602.7955829651526
new min fval from sgd:  -1602.8349154654486
new min fval from sgd:  -1602.85036829582
new min fval from sgd:  -1603.0031812543734
new min fval from sgd:  -1603.1454386713156
new min fval from sgd:  -1603.2177718881605
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.22911]
objective value function right now is: -1600.247186664983
new min fval from sgd:  -1603.2862361217426
new min fval from sgd:  -1603.3923391020876
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.5189]
objective value function right now is: -1598.519162811422
new min fval from sgd:  -1603.460905820441
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.18762]
objective value function right now is: -1599.2040394956582
new min fval from sgd:  -1603.4699826538874
new min fval from sgd:  -1603.5281778981202
new min fval from sgd:  -1603.5843911304364
new min fval from sgd:  -1603.634371611679
new min fval from sgd:  -1603.6593012687015
new min fval from sgd:  -1603.6935044773843
new min fval from sgd:  -1603.7150621468702
new min fval from sgd:  -1603.743212398186
new min fval from sgd:  -1603.7481142870213
new min fval from sgd:  -1603.748874297915
new min fval from sgd:  -1603.753791570702
new min fval from sgd:  -1603.756897247838
new min fval from sgd:  -1603.7654585315577
new min fval from sgd:  -1603.7676687740561
new min fval from sgd:  -1603.7696645068784
new min fval from sgd:  -1603.796774150917
new min fval from sgd:  -1603.8248554722238
new min fval from sgd:  -1603.8430336700858
new min fval from sgd:  -1603.8471845386346
new min fval from sgd:  -1603.854828535759
new min fval from sgd:  -1603.8735053574112
new min fval from sgd:  -1603.8780532295498
new min fval from sgd:  -1603.8838775522859
new min fval from sgd:  -1603.891043618477
new min fval from sgd:  -1603.9049434647957
new min fval from sgd:  -1603.9366425169824
new min fval from sgd:  -1603.9852490606643
new min fval from sgd:  -1604.0081318287862
new min fval from sgd:  -1604.0147287562738
new min fval from sgd:  -1604.01745238202
new min fval from sgd:  -1604.0583433856802
new min fval from sgd:  -1604.0747219918906
new min fval from sgd:  -1604.0926044155476
new min fval from sgd:  -1604.1033898976136
new min fval from sgd:  -1604.131889952022
new min fval from sgd:  -1604.1666595964816
new min fval from sgd:  -1604.191778257803
new min fval from sgd:  -1604.236343361504
new min fval from sgd:  -1604.262120296433
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.13605]
objective value function right now is: -1604.2353607025846
new min fval from sgd:  -1604.2645592199276
new min fval from sgd:  -1604.2753820381022
new min fval from sgd:  -1604.2879506180623
new min fval from sgd:  -1604.3021209644116
new min fval from sgd:  -1604.3143601131676
new min fval from sgd:  -1604.3226457193896
new min fval from sgd:  -1604.331119894894
new min fval from sgd:  -1604.3402763470808
new min fval from sgd:  -1604.345823879499
new min fval from sgd:  -1604.3748443168565
new min fval from sgd:  -1604.3939157197792
new min fval from sgd:  -1604.413520075059
new min fval from sgd:  -1604.4351129423399
new min fval from sgd:  -1604.4508513831702
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.10423]
objective value function right now is: -1603.654015327633
min fval:  -1604.4508513831702
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-21.5366,   3.3781],
        [  5.5161,   5.8282],
        [-38.8803,   9.9385],
        [ -1.3420,   0.3915],
        [  0.7416,   5.7135],
        [-10.6274, -18.1632],
        [ -1.3420,   0.3915],
        [-23.0573,   3.8223],
        [ 10.4639, -11.9650],
        [  2.2518,   0.3156]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 14.6507, -12.9995,  11.1291,  -2.9954, -11.9538, -13.0814,  -2.9954,
         15.9897, -10.1900,   4.7463], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-6.5380e-01, -5.4707e-02, -9.4254e-02, -1.7629e-03, -2.1814e-02,
         -6.5813e-02, -1.7629e-03, -7.1910e-01, -1.8007e-01, -1.4790e+00],
        [-6.5380e-01, -5.4707e-02, -9.4254e-02, -1.7629e-03, -2.1814e-02,
         -6.5813e-02, -1.7629e-03, -7.1910e-01, -1.8007e-01, -1.4790e+00],
        [-6.5380e-01, -5.4707e-02, -9.4254e-02, -1.7629e-03, -2.1814e-02,
         -6.5813e-02, -1.7629e-03, -7.1910e-01, -1.8007e-01, -1.4790e+00],
        [ 3.7046e+00, -9.0574e-02,  1.2621e-02,  6.7061e-02,  3.3291e-02,
         -3.9550e-01,  6.7061e-02,  5.7603e+00, -1.8962e+00,  1.0195e+00],
        [ 1.2051e+00,  3.5504e-01,  1.3112e-01, -2.7385e-02,  2.4405e-01,
          3.3148e-01, -2.7384e-02,  1.1579e+00,  1.1290e+00,  2.1570e+00],
        [-1.0478e+01,  8.9576e+00, -1.0001e+01,  5.0249e-02,  4.9386e+00,
          2.9911e+01,  5.0249e-02, -8.9274e+00,  2.1501e+01, -5.3105e+00],
        [ 1.2065e+00,  3.5653e-01,  1.3198e-01, -2.7655e-02,  2.4454e-01,
          3.3213e-01, -2.7655e-02,  1.1590e+00,  1.1317e+00,  2.1656e+00],
        [-6.5380e-01, -5.4707e-02, -9.4254e-02, -1.7629e-03, -2.1814e-02,
         -6.5813e-02, -1.7629e-03, -7.1910e-01, -1.8007e-01, -1.4790e+00],
        [-6.5380e-01, -5.4707e-02, -9.4254e-02, -1.7629e-03, -2.1814e-02,
         -6.5813e-02, -1.7629e-03, -7.1910e-01, -1.8007e-01, -1.4790e+00],
        [-6.5380e-01, -5.4707e-02, -9.4254e-02, -1.7629e-03, -2.1814e-02,
         -6.5813e-02, -1.7629e-03, -7.1910e-01, -1.8007e-01, -1.4790e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.4825, -1.4825, -1.4825,  2.2354,  2.1581,  7.9095,  2.1667, -1.4825,
        -1.4825, -1.4825], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0308,   0.0308,   0.0308,   9.6346,  -6.5119, -17.3311,  -6.6606,
           0.0308,   0.0308,   0.0308]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-11.2446,   0.1221],
        [  3.5513,  14.8058],
        [-14.4488,  -1.0982],
        [-12.9775,  -3.9077],
        [  4.6171,   1.0245],
        [ -4.3308,  -4.1637],
        [  9.4019,   3.2273],
        [ 20.1562,  11.7829],
        [  4.4421,   6.1930],
        [ 17.4077,   0.3720]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 11.6599,  11.7740,  -1.5135,   2.4383,   4.9442, -10.4871,   1.6747,
          5.7014,  -0.2789, -14.7841], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-5.7088e-01,  1.1510e+00,  1.1133e-01,  6.7204e-01,  8.2950e+00,
         -1.1539e-02,  4.4011e-01, -2.4538e-01,  1.1274e+00,  5.1754e-01],
        [-3.3262e+00,  5.4274e+00,  1.0080e+01,  3.0749e+00,  1.0708e+00,
          1.0533e+00,  2.7367e-01, -6.3432e-01,  5.7855e-01, -1.4802e+00],
        [-1.4319e+00, -5.6139e-01,  4.0187e-03, -3.9565e-01, -2.0631e+00,
         -3.7784e-03, -1.7889e+00, -1.2859e+00, -1.6537e-01, -6.1254e-01],
        [ 6.3790e+00, -2.5415e+01,  8.1017e+00,  1.6417e+00, -2.8411e+00,
          1.8630e+00, -3.0280e+00, -1.0054e+01, -9.8056e+00, -2.0001e+01],
        [ 2.0725e+00, -5.2008e+00, -5.3761e+00,  1.7121e+00, -6.0917e-01,
          2.3958e+00,  1.0511e-01, -1.3741e+00, -4.8263e+00, -1.3293e+00],
        [ 4.8169e+00, -3.5670e+00, -2.4628e-02,  9.3826e-01,  1.9443e+00,
          1.1304e-03, -5.7378e-01,  3.0039e-01,  9.6569e-01,  2.1249e+00],
        [ 2.2051e+00, -3.6953e+01,  1.3156e+01,  1.4312e+01, -3.1326e+00,
         -3.2081e+00, -1.7278e+00, -4.4701e+01, -1.4670e+00, -8.4820e+00],
        [-1.6036e+00, -5.5750e-01,  1.3583e-02, -4.0983e-01, -1.8541e+00,
         -5.0877e-03, -1.6905e+00, -1.3933e+00, -1.6967e-01, -6.4882e-01],
        [-1.0680e+00, -4.9027e-01, -1.1411e-01, -1.1938e+00, -4.9444e+00,
          1.0884e-02,  1.7161e-01, -5.4185e-01, -1.4193e+00, -9.5861e-01],
        [-6.6989e-01, -5.0463e+00, -7.8625e-01,  8.6168e-01, -4.2479e+00,
         -1.6523e-01, -2.3631e+00, -1.3265e+00, -1.1644e-01, -2.7034e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  1.9955,  -4.5657,  -1.7583,   5.0130,  -1.2043,   7.2930, -10.7510,
         -1.8066,  -2.3732,   1.0311], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 3.7557e+00,  4.6185e-01, -6.9556e-04, -7.9330e+00, -1.3138e+00,
         -3.3643e+00,  2.0351e+01, -1.8122e-03, -9.3737e-01, -1.0405e+00],
        [-3.7552e+00, -4.9070e-01,  6.7106e-04,  7.9238e+00,  1.3139e+00,
          3.3633e+00, -2.0334e+01, -3.8818e-03,  9.3738e-01,  1.0335e+00]],
       device='cuda:0'))])
xi:  [186.12494]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 650.0622886427277
W_T_median: 398.7349399671035
W_T_pctile_5: 186.15438753985143
W_T_CVAR_5_pct: 22.055995430214235
Average q (qsum/M+1):  48.199041551159276
Optimal xi:  [186.12494]
Expected(across Rb) median(across samples) p_equity:  0.24654656847318013
obj fun:  tensor(-1604.4509, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
loaded xi:  186.12494
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2561.7088009585973
Current xi:  [197.72983]
objective value function right now is: -2561.7088009585973
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.66966]
objective value function right now is: -2511.905051738771
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -2663.3210824843745
Current xi:  [203.28276]
objective value function right now is: -2663.3210824843745
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -2778.157446181658
Current xi:  [203.83632]
objective value function right now is: -2778.157446181658
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.97119]
objective value function right now is: -2643.524103301879
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.93086]
objective value function right now is: -2736.258608704354
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [206.35715]
objective value function right now is: -2444.2220955761077
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.2004]
objective value function right now is: -2422.5566470049603
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.82474]
objective value function right now is: -2715.296120264771
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.667]
objective value function right now is: -1556.9200062812936
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.31369]
objective value function right now is: -2352.608906203138
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.33711]
objective value function right now is: 42083.512402259934
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [94.52125]
objective value function right now is: 890.138871491439
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [92.39718]
objective value function right now is: 916.5954183282781
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [96.13372]
objective value function right now is: 1608.856319974579
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [98.11507]
objective value function right now is: 30740.67514857804
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [88.13137]
objective value function right now is: 3342.035708814586
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [83.71925]
objective value function right now is: 1881.3462637372263
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [82.7642]
objective value function right now is: 1692.9130185967265
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.802284]
objective value function right now is: 23597.53704511142
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.465286]
objective value function right now is: 60948.38467566424
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-22.098984]
objective value function right now is: 62095.60487045914
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-57.367268]
objective value function right now is: 44490.22602844831
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-91.24833]
objective value function right now is: 48709.49657888855
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-131.1656]
objective value function right now is: 47074.13958554823
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-165.96667]
objective value function right now is: 39995.10979582854
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-197.15422]
objective value function right now is: 42833.574368563466
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-229.05544]
objective value function right now is: 35450.62992136839
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-232.97743]
objective value function right now is: 13274.499260092274
