Starting at: 
03-12-22_10:09

 Random seed:  2  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1  logistic_sigmoid   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.2032969334232
Current xi:  [3.0941186]
objective value function right now is: -1686.2032969334232
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1694.879997605078
Current xi:  [0.01240494]
objective value function right now is: -1694.879997605078
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1696.3559861910985
Current xi:  [1.5035059e-07]
objective value function right now is: -1696.3559861910985
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1698.334591540047
Current xi:  [-1.2267866e-11]
objective value function right now is: -1698.334591540047
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1698.8475014331914
Current xi:  [1.06689616e-16]
objective value function right now is: -1698.8475014331914
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1700.3285993608088
Current xi:  [-3.942768e-20]
objective value function right now is: -1700.3285993608088
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1701.212344361474
Current xi:  [1.194874e-24]
objective value function right now is: -1701.212344361474
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1702.2971659548855
Current xi:  [1.0604425e-28]
objective value function right now is: -1702.2971659548855
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.4332994e-31]
objective value function right now is: -1701.8794625414723
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00048711]
objective value function right now is: -1701.827612364895
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.76015e-07]
objective value function right now is: -1700.5899359723317
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.9077013e-05]
objective value function right now is: -1700.8656803913284
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00026737]
objective value function right now is: -1702.056092865395
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [0.00099388]
objective value function right now is: -1701.8692021228978
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1703.9923843828728
Current xi:  [-2.282117e-05]
objective value function right now is: -1703.9923843828728
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00011162]
objective value function right now is: -1702.6493986690743
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00048282]
objective value function right now is: -1703.095478501039
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00050892]
objective value function right now is: -1701.5566653263759
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01221892]
objective value function right now is: -1702.3702093060851
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.235857e-05]
objective value function right now is: -1700.9148117750394
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00114099]
objective value function right now is: -1703.3971310465458
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00017956]
objective value function right now is: -1703.2857449941728
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.49422e-05]
objective value function right now is: -1703.6324496821412
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1704.4861683941008
Current xi:  [0.00073666]
objective value function right now is: -1704.4861683941008
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00312057]
objective value function right now is: -1702.780925536409
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00040504]
objective value function right now is: -1703.8063096338212
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00779396]
objective value function right now is: -1702.1539652291813
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [0.00052286]
objective value function right now is: -1704.3960146205545
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1704.8907264697798
Current xi:  [1.2202172e-05]
objective value function right now is: -1704.8907264697798
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00065782]
objective value function right now is: -1702.119258763835
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00087942]
objective value function right now is: -1701.5385473798071
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00037492]
objective value function right now is: -1704.641323270387
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.6316711e-05]
objective value function right now is: -1700.7526427763557
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0001498]
objective value function right now is: -1703.1072740821298
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01097332]
objective value function right now is: -1701.345654745731
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00350454]
objective value function right now is: -1701.762761427789
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00014048]
objective value function right now is: -1703.519217461099
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00027837]
objective value function right now is: -1703.2740651352644
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1704.9899527377836
Current xi:  [1.3740577e-05]
objective value function right now is: -1704.9899527377836
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.0022963e-05]
objective value function right now is: -1704.793005712441
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.3921602e-05]
objective value function right now is: -1704.6177345374479
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00015368]
objective value function right now is: -1702.3372596239149
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00397979]
objective value function right now is: -1703.9996079979135
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00781947]
objective value function right now is: -1704.2451816325138
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00025028]
objective value function right now is: -1704.4615776651972
new min fval:  -1704.99125477636
new min fval:  -1705.1300791447288
new min fval:  -1705.2408538994464
new min fval:  -1705.338207774268
new min fval:  -1705.4336235270816
new min fval:  -1705.5569685207442
new min fval:  -1705.650244350446
new min fval:  -1705.6879990461396
new min fval:  -1705.6991529934742
new min fval:  -1705.7048562905632
new min fval:  -1705.7168171568578
new min fval:  -1705.7243766491201
new min fval:  -1705.7296438179235
new min fval:  -1705.735081577261
new min fval:  -1705.742572251098
new min fval:  -1705.7439879002081
new min fval:  -1705.748484820634
new min fval:  -1705.754564562437
new min fval:  -1705.7565762089487
new min fval:  -1705.7602171016993
new min fval:  -1705.7673394709052
new min fval:  -1705.7692010414619
new min fval:  -1705.770389431213
new min fval:  -1705.7763095838936
new min fval:  -1705.7812790495643
new min fval:  -1705.7826595865643
new min fval:  -1705.7847970804905
new min fval:  -1705.7895166562114
new min fval:  -1705.7972154841366
new min fval:  -1705.803678540423
new min fval:  -1705.803816039587
new min fval:  -1705.8065449759254
new min fval:  -1705.8078233360197
new min fval:  -1705.8086676059759
new min fval:  -1705.811063426547
new min fval:  -1705.811520184108
new min fval:  -1705.812252967361
new min fval:  -1705.8146413725392
new min fval:  -1705.8178767566963
new min fval:  -1705.821195742478
new min fval:  -1705.8236259296243
new min fval:  -1705.8272258148695
new min fval:  -1705.8272606940293
new min fval:  -1705.8275187351753
new min fval:  -1705.82898288918
new min fval:  -1705.8316076932686
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00550501]
objective value function right now is: -1704.7498884451627
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00410217]
objective value function right now is: -1698.86329962238
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00463272]
objective value function right now is: -1703.2376720367786
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00332657]
objective value function right now is: -1705.246277891725
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00065517]
objective value function right now is: -1704.0363767121844
min fval:  -1705.8316076932686
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 323.33119782659094
W_T_median: 165.49195041350032
W_T_pctile_5: -278.7692823356869
W_T_CVAR_5_pct: -485.1214072795298
Average q (qsum/M+1):  56.22769657258065
Optimal xi:  [-5.2918294e-05]
Expected(across Rb) median(across samples) p_equity:  0.28072492285476375
obj fun:  tensor(-1705.8316, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1633.9783228921958
Current xi:  [4.327865]
objective value function right now is: -1633.9783228921958
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1639.107532381854
Current xi:  [0.1285625]
objective value function right now is: -1639.107532381854
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.44866e-06]
objective value function right now is: -1636.4493982439712
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.8927167e-10]
objective value function right now is: -1638.2178683363138
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.669369e-15]
objective value function right now is: -1637.2144537118934
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1640.0312452824153
Current xi:  [-3.666632e-19]
objective value function right now is: -1640.0312452824153
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [1.5019484e-20]
objective value function right now is: -1637.1591881494533
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1641.9768574518819
Current xi:  [7.5450426e-17]
objective value function right now is: -1641.9768574518819
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1644.5152281544447
Current xi:  [3.0487072e-08]
objective value function right now is: -1644.5152281544447
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.7228769e-05]
objective value function right now is: -1640.376935779478
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00140647]
objective value function right now is: -1640.8867634483968
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01281561]
objective value function right now is: -1641.7695004943334
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00121903]
objective value function right now is: -1641.7476154177273
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00066728]
objective value function right now is: -1642.3681400162475
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02135037]
objective value function right now is: -1643.7214426055925
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.5384895e-05]
objective value function right now is: -1641.921977596504
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.1359875e-06]
objective value function right now is: -1626.4148734354042
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00455292]
objective value function right now is: -1641.0429723889354
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00587473]
objective value function right now is: -1642.476484331995
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0056414]
objective value function right now is: -1638.8040042926432
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.13336071]
objective value function right now is: -1642.3476802382004
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0014475]
objective value function right now is: -1640.5961758457008
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04633048]
objective value function right now is: -1641.9133829249722
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00025544]
objective value function right now is: -1639.4696453047768
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00011253]
objective value function right now is: -1641.127667511886
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.260071e-07]
objective value function right now is: -1644.1886887487015
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.04142545]
objective value function right now is: -1644.446121304451
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [0.07351214]
objective value function right now is: -1641.5343259602785
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00017775]
objective value function right now is: -1643.250387804926
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00934133]
objective value function right now is: -1641.1623468028586
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02022579]
objective value function right now is: -1640.4644415358696
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01038278]
objective value function right now is: -1638.8614625776697
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0041979]
objective value function right now is: -1635.2367610602817
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01853675]
objective value function right now is: -1642.903857141347
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00291467]
objective value function right now is: -1640.628244747805
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00195204]
objective value function right now is: -1638.3528268728605
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01617029]
objective value function right now is: -1643.061525848468
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.29445198]
objective value function right now is: -1643.21062093933
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.594252e-05]
objective value function right now is: -1642.5506789514202
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0023211]
objective value function right now is: -1635.2342620691177
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02032764]
objective value function right now is: -1638.7888184147357
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00366504]
objective value function right now is: -1641.9100940220621
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00238786]
objective value function right now is: -1644.159942030804
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00125898]
objective value function right now is: -1640.711337970213
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00183967]
objective value function right now is: -1635.1506343892097
new min fval:  -1644.5187170779416
new min fval:  -1644.5309006920536
new min fval:  -1644.5743335384295
new min fval:  -1644.6419265225777
new min fval:  -1644.6948621599483
new min fval:  -1644.7087595655737
new min fval:  -1644.7348025704061
new min fval:  -1644.74911687813
new min fval:  -1644.765037000614
new min fval:  -1644.79360380385
new min fval:  -1644.8032880397561
new min fval:  -1644.803806472208
new min fval:  -1644.8048393646964
new min fval:  -1644.804949872747
new min fval:  -1644.8197546180047
new min fval:  -1644.8466487796659
new min fval:  -1644.8630330952583
new min fval:  -1644.8642983930822
new min fval:  -1644.866337462038
new min fval:  -1644.876034324918
new min fval:  -1644.8831053141053
new min fval:  -1644.891817518991
new min fval:  -1644.9048156123872
new min fval:  -1644.9238569437969
new min fval:  -1644.9314216315845
new min fval:  -1644.9371310710721
new min fval:  -1644.9569230788477
new min fval:  -1644.975973605424
new min fval:  -1644.9852759815492
new min fval:  -1644.988471781663
new min fval:  -1644.9894256690109
new min fval:  -1644.9921643696289
new min fval:  -1645.0038467094803
new min fval:  -1645.0273537572668
new min fval:  -1645.0351690227624
new min fval:  -1645.0359170962447
new min fval:  -1645.060678334728
new min fval:  -1645.0831591501203
new min fval:  -1645.0925379156108
new min fval:  -1645.104063596102
new min fval:  -1645.116782113201
new min fval:  -1645.1232180140348
new min fval:  -1645.1378098081789
new min fval:  -1645.1654251816185
new min fval:  -1645.1757910885703
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00944471]
objective value function right now is: -1642.2284972260356
new min fval:  -1645.1844256829309
new min fval:  -1645.190461335334
new min fval:  -1645.1950822333347
new min fval:  -1645.202623773796
new min fval:  -1645.2049945559052
new min fval:  -1645.2055699406508
new min fval:  -1645.2103564960287
new min fval:  -1645.2190790077034
new min fval:  -1645.2221910703597
new min fval:  -1645.2303211473838
new min fval:  -1645.241952995861
new min fval:  -1645.2492164636749
new min fval:  -1645.2534172704316
new min fval:  -1645.2588595197415
new min fval:  -1645.265905562951
new min fval:  -1645.2731288772716
new min fval:  -1645.276701000947
new min fval:  -1645.2808303434467
new min fval:  -1645.2871013651522
new min fval:  -1645.294304469284
new min fval:  -1645.3032593755634
new min fval:  -1645.306491797004
new min fval:  -1645.3068999179873
new min fval:  -1645.3086275451494
new min fval:  -1645.3162183118084
new min fval:  -1645.3223054359562
new min fval:  -1645.3238825511876
new min fval:  -1645.3304355266653
new min fval:  -1645.3326425942253
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00618997]
objective value function right now is: -1634.2900192922182
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01131509]
objective value function right now is: -1637.6223619802058
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00431719]
objective value function right now is: -1640.7862588101357
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03849751]
objective value function right now is: -1642.9897544463645
min fval:  -1645.3326425942253
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 331.0131541434522
W_T_median: 183.01088038509553
W_T_pctile_5: -53.412251387903844
W_T_CVAR_5_pct: -245.44704941432803
Average q (qsum/M+1):  54.732205298639116
Optimal xi:  [0.00061686]
Expected(across Rb) median(across samples) p_equity:  0.33377928578605254
obj fun:  tensor(-1645.3326, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1569.2352699561934
Current xi:  [6.9834332]
objective value function right now is: -1569.2352699561934
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1584.9580500507977
Current xi:  [2.326115]
objective value function right now is: -1584.9580500507977
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.14050207]
objective value function right now is: -1580.764935001099
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.7703408e-06]
objective value function right now is: -1570.0248980926908
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.910531e-10]
objective value function right now is: -1582.8740549890726
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.9753223e-14]
objective value function right now is: -1572.062548357939
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [8.002538e-16]
objective value function right now is: -1476.1432845591464
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.30258e-06]
objective value function right now is: -1579.3139266728294
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.05928316]
objective value function right now is: -1572.7689505485894
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.5372275e-05]
objective value function right now is: -1573.342133119138
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.07257842]
objective value function right now is: -1568.9910882002082
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01902583]
objective value function right now is: -1577.09738578211
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00019912]
objective value function right now is: -1583.3543598594788
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00051523]
objective value function right now is: -1579.2873944464545
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.21683249]
objective value function right now is: -1583.7052363406422
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.04078611]
objective value function right now is: -1584.284597384769
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00794427]
objective value function right now is: -1584.563109383641
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1586.6972533464527
Current xi:  [-0.00075902]
objective value function right now is: -1586.6972533464527
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0335017]
objective value function right now is: -1576.5703798238296
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.923966e-05]
objective value function right now is: -1584.3702382302386
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1586.9244303264738
Current xi:  [0.18136017]
objective value function right now is: -1586.9244303264738
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.6603585e-05]
objective value function right now is: -1576.9602146481004
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00142935]
objective value function right now is: -1582.8620074744626
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1587.456872929768
Current xi:  [0.0002846]
objective value function right now is: -1587.456872929768
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00011619]
objective value function right now is: -1584.4615407423155
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.8097864e-05]
objective value function right now is: -1581.7451641254722
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1589.6660356084155
Current xi:  [-0.00090478]
objective value function right now is: -1589.6660356084155
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-7.7290904e-05]
objective value function right now is: -1585.0881405115108
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00100341]
objective value function right now is: -1553.747811711982
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01633187]
objective value function right now is: -1575.427928923375
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.4383356e-05]
objective value function right now is: -1585.7942999412362
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.2256086e-06]
objective value function right now is: -1579.1216282036924
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.07508517]
objective value function right now is: -1577.8958534005442
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.6294128e-07]
objective value function right now is: -1587.2505705554272
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.17339902]
objective value function right now is: -1586.6501285241438
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00024628]
objective value function right now is: -1589.4778015104832
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.09104384]
objective value function right now is: -1589.363477518882
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00111594]
objective value function right now is: -1582.0389441341601
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.22540183]
objective value function right now is: -1585.1143277926924
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1590.1731374081592
Current xi:  [1.1469863e-05]
objective value function right now is: -1590.1731374081592
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00243098]
objective value function right now is: -1588.744276109179
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1590.4275361292898
Current xi:  [-0.010179]
objective value function right now is: -1590.4275361292898
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00437323]
objective value function right now is: -1571.9454936403945
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02641955]
objective value function right now is: -1585.9427091207435
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00230129]
objective value function right now is: -1584.9243765003546
new min fval:  -1590.5166213349105
new min fval:  -1591.5277711671458
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01353926]
objective value function right now is: -1586.4545257063005
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.15159532]
objective value function right now is: -1575.6540303755448
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00010638]
objective value function right now is: -1589.197552710824
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.03536361]
objective value function right now is: -1585.952291945214
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0068987]
objective value function right now is: -1581.2007472918142
min fval:  -1591.5277711671458
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 611.6944894256245
W_T_median: 203.5833437125018
W_T_pctile_5: -0.4335707199480953
W_T_CVAR_5_pct: -127.62695212807725
Average q (qsum/M+1):  53.39815398185484
Optimal xi:  [-0.00233356]
Expected(across Rb) median(across samples) p_equity:  0.3855330154299736
obj fun:  tensor(-1591.5278, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1534.1677808752488
Current xi:  [9.569176]
objective value function right now is: -1534.1677808752488
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1549.2355573820328
Current xi:  [7.3690085]
objective value function right now is: -1549.2355573820328
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1551.0282760514697
Current xi:  [7.209587]
objective value function right now is: -1551.0282760514697
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.68431]
objective value function right now is: -1533.290456396062
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.784392]
objective value function right now is: -1545.908274005627
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.1565533]
objective value function right now is: -1547.80131056164
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [6.6466618]
objective value function right now is: -1546.6594061980838
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.642706]
objective value function right now is: -1394.2802626285375
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.476095]
objective value function right now is: -1542.8694918451124
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.265485]
objective value function right now is: -1484.5976404375076
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.5375953]
objective value function right now is: -1508.698158692557
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.584093]
objective value function right now is: -1547.2587712214115
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.779575]
objective value function right now is: -1550.6527227343838
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1554.2349061421976
Current xi:  [7.168765]
objective value function right now is: -1554.2349061421976
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.675519]
objective value function right now is: -1541.6592653099215
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.410177]
objective value function right now is: -1537.0244268966492
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.7541766]
objective value function right now is: -1539.7722901001632
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.8305116]
objective value function right now is: -1544.4552398280023
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.5165005]
objective value function right now is: -1533.5828940667186
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.827482476722
Current xi:  [6.9538107]
objective value function right now is: -1555.827482476722
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.783634]
objective value function right now is: -1554.256312234285
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.2538004]
objective value function right now is: -1510.288982107981
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.763589]
objective value function right now is: -1545.3662236690127
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.2478423]
objective value function right now is: -1555.0087455919627
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.0315166]
objective value function right now is: -1546.345538163215
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.247517]
objective value function right now is: -1538.1714433889267
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.1683493]
objective value function right now is: -1541.2188496187655
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [7.0214562]
objective value function right now is: -1548.3013563954955
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [6.392429]
objective value function right now is: -1546.947571841339
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.14583]
objective value function right now is: -1554.3400150479383
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.754938]
objective value function right now is: -1544.6529823036135
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.6843185]
objective value function right now is: -1523.0370852832966
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.2787457]
objective value function right now is: -1533.1297679010772
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.8007736]
objective value function right now is: -1550.3140558405687
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.6545653]
objective value function right now is: -1553.0558917447622
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.6063237]
objective value function right now is: -1544.6839398228935
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.8974276]
objective value function right now is: -1551.9896459827635
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.031726]
objective value function right now is: -1538.397360067842
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.0071573]
objective value function right now is: -1553.1664525370775
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.086585]
objective value function right now is: -1548.8032682830562
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.1016936]
objective value function right now is: -1476.0048668839038
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.617942]
objective value function right now is: -1537.7339451556197
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.1853857]
objective value function right now is: -1551.3534715063292
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.1903634]
objective value function right now is: -1530.915620866359
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.6235857]
objective value function right now is: -1553.878296706565
new min fval:  -1556.1026084509701
new min fval:  -1557.3457642792648
new min fval:  -1558.1856612926936
new min fval:  -1558.6258037978969
new min fval:  -1558.8781413105037
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.1252747]
objective value function right now is: -1544.7279688220285
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.3243065]
objective value function right now is: -1545.8540678784927
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.037025]
objective value function right now is: -1537.31271750471
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.302108]
objective value function right now is: -1552.4260334181113
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.9340353]
objective value function right now is: -1548.1614156744527
min fval:  -1558.8781413105037
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 647.1936811289913
W_T_median: 292.7392057902889
W_T_pctile_5: 59.16996796025434
W_T_CVAR_5_pct: -48.48876124685087
Average q (qsum/M+1):  51.8546142578125
Optimal xi:  [7.569059]
Expected(across Rb) median(across samples) p_equity:  0.3435829967260361
obj fun:  tensor(-1558.8781, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1531.6693985441452
Current xi:  [11.807902]
objective value function right now is: -1531.6693985441452
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.219512]
objective value function right now is: -1525.0028361207992
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1535.4686392920214
Current xi:  [10.14174]
objective value function right now is: -1535.4686392920214
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.410917]
objective value function right now is: -1527.7332610746216
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.642883]
objective value function right now is: -1527.6450162437345
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.583855]
objective value function right now is: -1423.2417760839087
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1538.937722903139
Current xi:  [10.581356]
objective value function right now is: -1538.937722903139
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.122367]
objective value function right now is: -1537.514963454157
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.654595]
objective value function right now is: -1532.631369141196
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.309843]
objective value function right now is: -1531.2574187652003
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1542.0140369245746
Current xi:  [10.37585]
objective value function right now is: -1542.0140369245746
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.767189]
objective value function right now is: -1532.5092451503483
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.367172]
objective value function right now is: -1529.237520574385
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1545.2850425501763
Current xi:  [10.356266]
objective value function right now is: -1545.2850425501763
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.976695]
objective value function right now is: -1527.4083625125804
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.155979]
objective value function right now is: -1530.045246730579
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.497866]
objective value function right now is: -1539.5961585114314
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.317956]
objective value function right now is: -1542.9169388846165
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.902534]
objective value function right now is: -1543.9072435447843
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.3936405]
objective value function right now is: -1518.7027233263498
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.661958]
objective value function right now is: -1538.5840236745153
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.433659]
objective value function right now is: -1520.1553353375991
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.642561]
objective value function right now is: -1538.1448198628568
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.482493]
objective value function right now is: -1544.0906542408331
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.90124]
objective value function right now is: -1535.7168380254316
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.223805]
objective value function right now is: -1525.6608930343139
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.544931]
objective value function right now is: -1523.6849286541078
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [10.92994]
objective value function right now is: -1534.2490076175566
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [10.526213]
objective value function right now is: -1538.9297348293264
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.186454]
objective value function right now is: -1536.1208417239836
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.583579]
objective value function right now is: -1535.260729813888
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.443173]
objective value function right now is: -1517.054723668413
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.219024]
objective value function right now is: -1520.945311344233
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.363155]
objective value function right now is: -1533.4543479109307
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.513668]
objective value function right now is: -1542.760933415128
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.882729]
objective value function right now is: -1526.1626153737554
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.828038]
objective value function right now is: -1523.871259631549
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.322977]
objective value function right now is: -1535.5867210946192
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.698138]
objective value function right now is: -1541.3840159287927
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.862583]
objective value function right now is: -1500.600318144142
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.740204]
objective value function right now is: -1533.0112958971713
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.648086]
objective value function right now is: -1527.0731897229207
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.913816]
objective value function right now is: -1519.1102541540233
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.620627]
objective value function right now is: -1524.5040774047236
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.159115]
objective value function right now is: -1518.918350485025
new min fval:  -1545.2973030901114
new min fval:  -1545.393998528964
new min fval:  -1545.4797434585955
new min fval:  -1545.5541210015294
new min fval:  -1545.5993639123728
new min fval:  -1545.6061029452947
new min fval:  -1545.733678436044
new min fval:  -1545.9275564869438
new min fval:  -1546.0419151149256
new min fval:  -1546.0918959559847
new min fval:  -1546.1073102732735
new min fval:  -1546.1210863438093
new min fval:  -1546.1514039144008
new min fval:  -1546.2078903432662
new min fval:  -1546.2914452582947
new min fval:  -1546.3890923724507
new min fval:  -1546.4935109930113
new min fval:  -1546.6029449228947
new min fval:  -1546.7286904727112
new min fval:  -1546.8581267898717
new min fval:  -1546.9022078256705
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.564166]
objective value function right now is: -1543.7147427513646
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.48872]
objective value function right now is: -1540.4050459301827
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.924126]
objective value function right now is: -1540.5642260427576
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.0339575]
objective value function right now is: -1541.336520819047
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.428004]
objective value function right now is: -1533.2298229329504
min fval:  -1546.9022078256705
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 650.7269742025461
W_T_median: 335.7541542425596
W_T_pctile_5: 110.45844458243458
W_T_CVAR_5_pct: -12.52954112540685
Average q (qsum/M+1):  50.53617612777218
Optimal xi:  [10.12462]
Expected(across Rb) median(across samples) p_equity:  0.3139331365625064
obj fun:  tensor(-1546.9022, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1500.2848047111058
Current xi:  [12.94874]
objective value function right now is: -1500.2848047111058
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1534.3224931245772
Current xi:  [12.58881]
objective value function right now is: -1534.3224931245772
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.538496]
objective value function right now is: -1532.6155230873505
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1544.3657492301602
Current xi:  [12.822392]
objective value function right now is: -1544.3657492301602
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.127166]
objective value function right now is: -1539.9683111369018
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.56646]
objective value function right now is: -1535.0663701750184
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [12.335515]
objective value function right now is: -1515.3719092263302
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.296496]
objective value function right now is: -1535.5597706521448
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.26304]
objective value function right now is: -1543.0610900914028
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.403847]
objective value function right now is: -1537.7008711422309
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.979426]
objective value function right now is: -1512.831340099045
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.009577]
objective value function right now is: -1494.047984318749
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.475886]
objective value function right now is: -1499.1998553746873
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [12.73586]
objective value function right now is: -1542.2558748424576
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.718837]
objective value function right now is: -1473.9205368757002
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1544.488566075573
Current xi:  [12.966683]
objective value function right now is: -1544.488566075573
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.26991]
objective value function right now is: -1525.4159001170854
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.894079]
objective value function right now is: -1509.7695718396797
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.626039]
objective value function right now is: -1318.3342151900574
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.532057]
objective value function right now is: -1490.9526175847593
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.45083]
objective value function right now is: -1543.680636782201
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1549.2602344503305
Current xi:  [12.388412]
objective value function right now is: -1549.2602344503305
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1549.5954641701924
Current xi:  [12.664487]
objective value function right now is: -1549.5954641701924
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.4602375]
objective value function right now is: -1534.808880778201
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.355638]
objective value function right now is: -1527.90080876945
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.94374]
objective value function right now is: -1541.6700197215243
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.344574]
objective value function right now is: -1506.8334529262497
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [13.00139]
objective value function right now is: -1545.5985423605712
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [12.220016]
objective value function right now is: -1546.8601738888133
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.440553]
objective value function right now is: -1518.799113023985
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.922112]
objective value function right now is: -1523.238135793739
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.714375]
objective value function right now is: -1524.9092563149286
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.249451]
objective value function right now is: -1543.1394373287437
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.873709]
objective value function right now is: -1505.9921089227425
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.631809]
objective value function right now is: -1545.5891488523375
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.856791]
objective value function right now is: -1534.8016137294721
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.440603]
objective value function right now is: -1540.2674832572927
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.28315]
objective value function right now is: -1525.2676918189015
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.276158]
objective value function right now is: -1546.8652276985563
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.305066]
objective value function right now is: -1544.1785032576859
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.9342985]
objective value function right now is: -1488.5742547115713
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.735337]
objective value function right now is: -1528.1383990081972
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.654764]
objective value function right now is: -1547.6758848525692
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.635201]
objective value function right now is: -1498.2286710479727
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.697289]
objective value function right now is: -1534.401350425401
new min fval:  -1550.090498724333
new min fval:  -1550.6928587347124
new min fval:  -1550.9176663999808
new min fval:  -1551.0001558118445
new min fval:  -1551.031244955842
new min fval:  -1551.1195896189035
new min fval:  -1551.3851428901921
new min fval:  -1552.0255775373141
new min fval:  -1553.076670010503
new min fval:  -1554.0810248244359
new min fval:  -1554.9788920194796
new min fval:  -1555.6832652989863
new min fval:  -1556.145606732841
new min fval:  -1556.3912656943066
new min fval:  -1556.442406260572
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.545595]
objective value function right now is: -1549.2660253539586
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.6818905]
objective value function right now is: -1545.82232921417
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.7063875]
objective value function right now is: -1542.9485616847794
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.5753565]
objective value function right now is: -1480.7262301965925
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.517375]
objective value function right now is: -1537.0938167197744
min fval:  -1556.442406260572
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 718.9079916765843
W_T_median: 433.8386535184822
W_T_pctile_5: 164.389371619001
W_T_CVAR_5_pct: 11.091866837740524
Average q (qsum/M+1):  49.135064894153224
Optimal xi:  [12.790526]
Expected(across Rb) median(across samples) p_equity:  0.3005488673845927
obj fun:  tensor(-1556.4424, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.9054560178058
Current xi:  [13.616015]
objective value function right now is: -1562.9054560178058
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1565.6734777711686
Current xi:  [13.394719]
objective value function right now is: -1565.6734777711686
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.625453]
objective value function right now is: -1560.3325322282217
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1584.0410388807702
Current xi:  [13.911432]
objective value function right now is: -1584.0410388807702
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.845656]
objective value function right now is: -1578.783263559269
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.486604]
objective value function right now is: -1577.6954303655257
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [13.401604]
objective value function right now is: -1557.9342874843137
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.454069]
objective value function right now is: -1487.4727128291424
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.282888]
objective value function right now is: -1572.4741618786031
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.296082]
objective value function right now is: -1557.6654222905515
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.520922]
objective value function right now is: -1578.6513248426916
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1586.3481784138442
Current xi:  [13.654638]
objective value function right now is: -1586.3481784138442
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.442128]
objective value function right now is: -1576.0982240158664
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1588.2045811480677
Current xi:  [13.443415]
objective value function right now is: -1588.2045811480677
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.260494]
objective value function right now is: -1562.3656635570148
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.95189]
objective value function right now is: -1565.6965200746022
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.017928]
objective value function right now is: -1568.9966055310815
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.986937]
objective value function right now is: -1554.4360459414338
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.516185]
objective value function right now is: -1549.9429965236745
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.572363]
objective value function right now is: -1577.8050423497375
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.8836975]
objective value function right now is: -1529.0845648800298
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.822306]
objective value function right now is: -1582.3895968414802
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.731449]
objective value function right now is: -1564.838291054396
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.020898]
objective value function right now is: -1504.1134607050064
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.619228]
objective value function right now is: -1579.4900254180607
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.316975]
objective value function right now is: -1573.7890970361243
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.6522665]
objective value function right now is: -1572.7719097837776
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [12.869167]
objective value function right now is: -1556.3467534920183
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [13.738898]
objective value function right now is: -1568.6560962071285
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.306642]
objective value function right now is: -1491.0668553998685
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.559286]
objective value function right now is: -1551.0400755565972
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.618276]
objective value function right now is: -1574.3597455279712
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.677541]
objective value function right now is: -1526.915974776673
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.589013]
objective value function right now is: -1550.1571018741624
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.766851]
objective value function right now is: -1572.0596602566618
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.20009]
objective value function right now is: -1577.1269981196258
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.522633]
objective value function right now is: -1577.9329950687559
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.649251]
objective value function right now is: -1572.0614423896388
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.61602]
objective value function right now is: -1569.3307759739535
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.440037]
objective value function right now is: -1560.2267167441066
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.667429]
objective value function right now is: -1582.0905050667543
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.969493]
objective value function right now is: -1572.863868637273
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.316482]
objective value function right now is: -1529.1790179826703
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.944802]
objective value function right now is: -1552.7519382237447
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.3089905]
objective value function right now is: -1537.3824921788066
new min fval:  -1588.2757160267574
new min fval:  -1588.3543781778044
new min fval:  -1588.4293847466904
new min fval:  -1588.4752125118146
new min fval:  -1588.4955731802675
new min fval:  -1588.6098440628898
new min fval:  -1588.7728408717117
new min fval:  -1588.9589257229763
new min fval:  -1589.1314648799842
new min fval:  -1589.2659029162862
new min fval:  -1589.3581678607288
new min fval:  -1589.4052322964978
new min fval:  -1589.4205209591305
new min fval:  -1589.5041496440067
new min fval:  -1589.5870902521683
new min fval:  -1589.656674033126
new min fval:  -1589.703797275548
new min fval:  -1589.7329575989577
new min fval:  -1589.7502136701733
new min fval:  -1589.7503373825134
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.198621]
objective value function right now is: -1544.9238281858172
new min fval:  -1589.7943628248806
new min fval:  -1589.885743314378
new min fval:  -1590.007131679904
new min fval:  -1590.1333816166061
new min fval:  -1590.2436247818916
new min fval:  -1590.3314388544736
new min fval:  -1590.3988185598027
new min fval:  -1590.412217639122
new min fval:  -1590.4270542629438
new min fval:  -1590.4507569343239
new min fval:  -1590.5009919943564
new min fval:  -1590.539652378744
new min fval:  -1590.5740549389361
new min fval:  -1590.6063439603556
new min fval:  -1590.621412525439
new min fval:  -1590.6382870592001
new min fval:  -1590.6580016796752
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.375738]
objective value function right now is: -1561.3843528206532
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.970224]
objective value function right now is: -1510.4772504469418
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.2695675]
objective value function right now is: -1561.947365981125
new min fval:  -1590.6699394535099
new min fval:  -1590.6840481779209
new min fval:  -1590.6960409098158
new min fval:  -1590.706424769095
new min fval:  -1590.724850841268
new min fval:  -1590.7538322301061
new min fval:  -1590.8004002448413
new min fval:  -1590.8088466106412
new min fval:  -1590.8108739388451
new min fval:  -1590.8125681110546
new min fval:  -1590.8165521217566
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.676192]
objective value function right now is: -1576.8770142625185
min fval:  -1590.8165521217566
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 687.9121832727443
W_T_median: 439.8283132006453
W_T_pctile_5: 184.30898044391083
W_T_CVAR_5_pct: 21.033128626846988
Average q (qsum/M+1):  47.95392830141129
Optimal xi:  [13.365681]
Expected(across Rb) median(across samples) p_equity:  0.2692837844292323
obj fun:  tensor(-1590.8166, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2475.6323394403275
Current xi:  [14.104589]
objective value function right now is: -2475.6323394403275
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -2656.1669368579783
Current xi:  [14.375447]
objective value function right now is: -2656.1669368579783
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.571092]
objective value function right now is: -2128.046241086044
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.76204]
objective value function right now is: -2383.8122859812643
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.553528]
objective value function right now is: -2034.3809323572207
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.297514]
objective value function right now is: -2249.2614414213676
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [14.069767]
objective value function right now is: -2135.696863626119
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.645347]
objective value function right now is: -2375.134638096167
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.140376]
objective value function right now is: -2143.1431237473353
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.168842]
objective value function right now is: -2393.17377686443
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.081098]
objective value function right now is: -2301.1146380029727
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.058932]
objective value function right now is: -2536.2990096369526
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.679388]
objective value function right now is: -2035.8243217360741
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [14.132575]
objective value function right now is: -1966.5842267778091
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.959409]
objective value function right now is: -2579.3993088129337
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.713178]
objective value function right now is: -1834.882141644758
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.831071]
objective value function right now is: -2567.4700186560126
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.598998]
objective value function right now is: -1800.5817676033666
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.324567]
objective value function right now is: -2565.8827133551226
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.607314]
objective value function right now is: 462.52080033135786
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.55141]
objective value function right now is: -2518.5778905210054
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.298856]
objective value function right now is: -2332.665424044036
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.522147]
objective value function right now is: -1880.8252287731996
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.7479105]
objective value function right now is: -2340.5614646963254
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.183667]
objective value function right now is: 4183.7070501595545
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.836629]
objective value function right now is: -1566.7142461640742
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.571575]
objective value function right now is: -2547.9078222151875
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [14.212167]
objective value function right now is: -2405.636079297385
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -2716.2208973209504
Current xi:  [14.83582]
objective value function right now is: -2716.2208973209504
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.854761]
objective value function right now is: -2454.6531261031664
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.1041565]
objective value function right now is: -2010.5235604804616
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.436443]
objective value function right now is: -2542.948462918129
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.471465]
objective value function right now is: -2611.4480068616494
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.158082]
objective value function right now is: -2339.386492988158
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.246223]
objective value function right now is: -2472.3144249807697
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.728768]
objective value function right now is: -2485.9185882594866
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.925108]
objective value function right now is: -2014.0031095393883
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.093849]
objective value function right now is: -2562.6308224754753
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.760811]
objective value function right now is: -2570.7265876666834
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.196254]
objective value function right now is: -2242.1972520980794
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.554485]
objective value function right now is: -2669.9748154191434
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.442729]
objective value function right now is: -2670.645119883265
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.582826]
objective value function right now is: -2635.1703744195465
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.312106]
objective value function right now is: -2631.339781172185
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.872367]
objective value function right now is: -2601.032381873918
new min fval:  -2720.7621543096875
new min fval:  -2724.2061255553167
new min fval:  -2726.044809803984
new min fval:  -2727.030132535499
new min fval:  -2731.0929843426625
new min fval:  -2734.7533434993243
new min fval:  -2737.9514218826484
new min fval:  -2741.085743737013
new min fval:  -2744.4103579667153
new min fval:  -2747.8611633235632
new min fval:  -2750.8439042276937
new min fval:  -2752.733586496099
new min fval:  -2753.5831581640655
new min fval:  -2753.6852058615277
new min fval:  -2754.880608982129
new min fval:  -2756.923286186365
new min fval:  -2758.728110561086
new min fval:  -2760.3377719605987
new min fval:  -2761.192616812467
new min fval:  -2761.3102577992186
new min fval:  -2761.8897857089464
new min fval:  -2762.4374290746673
new min fval:  -2762.801805060009
new min fval:  -2763.1710693717096
new min fval:  -2763.329914181514
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.869544]
objective value function right now is: -2701.142184605845
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.622138]
objective value function right now is: -2465.1490527600527
new min fval:  -2763.3410619376273
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.059648]
objective value function right now is: -2674.9639272674585
new min fval:  -2763.5072112452863
new min fval:  -2763.6180125737214
new min fval:  -2763.6637039305747
new min fval:  -2763.78035933852
new min fval:  -2763.9670074214037
new min fval:  -2764.1603282256983
new min fval:  -2764.2125911750345
new min fval:  -2764.3046459296784
new min fval:  -2764.444464909843
new min fval:  -2764.579155935184
new min fval:  -2764.607923484172
new min fval:  -2764.60967761264
new min fval:  -2764.6695181978935
new min fval:  -2764.790132882676
new min fval:  -2764.923027942287
new min fval:  -2765.044325204207
new min fval:  -2765.1699874177352
new min fval:  -2765.280038731711
new min fval:  -2765.3789589686617
new min fval:  -2765.464339217393
new min fval:  -2765.501205615795
new min fval:  -2765.5151418135197
new min fval:  -2765.543265197519
new min fval:  -2765.671844828544
new min fval:  -2765.790235280017
new min fval:  -2765.894024687198
new min fval:  -2766.01245212634
new min fval:  -2766.0899585480834
new min fval:  -2766.162774574824
new min fval:  -2766.2288591201977
new min fval:  -2766.2726204813875
new min fval:  -2766.313693474085
new min fval:  -2766.3315372968077
new min fval:  -2766.3352657537175
new min fval:  -2766.346098052531
new min fval:  -2766.378226727945
new min fval:  -2766.4266810528097
new min fval:  -2766.5022354738203
new min fval:  -2766.6019702915382
new min fval:  -2766.715391444657
new min fval:  -2766.8134635844326
new min fval:  -2766.8939552086144
new min fval:  -2766.9106174663552
new min fval:  -2766.935304730086
new min fval:  -2766.9901175905025
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.115707]
objective value function right now is: -2574.323430511685
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.494945]
objective value function right now is: -2499.481434721939
min fval:  -2766.9901175905025
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 774.1812098027485
W_T_median: 553.2495424429146
W_T_pctile_5: 206.63909627534
W_T_CVAR_5_pct: 27.319476702241555
Average q (qsum/M+1):  45.48804104712702
Optimal xi:  [14.160448]
Expected(across Rb) median(across samples) p_equity:  0.24620502740144729
obj fun:  tensor(-2766.9901, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -117527.9077650703
Current xi:  [14.89024]
objective value function right now is: -117527.9077650703
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -130999.78584018958
Current xi:  [14.525586]
objective value function right now is: -130999.78584018958
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.685987]
objective value function right now is: -116869.62912520698
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.877865]
objective value function right now is: -86945.62925136619
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.605481]
objective value function right now is: -44346.56766445455
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.179095]
objective value function right now is: -80514.50773918255
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [14.01165]
objective value function right now is: 137909.78666979022
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.99054]
objective value function right now is: 30320.099976435195
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.4147625]
objective value function right now is: -60164.98505018069
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.574745]
objective value function right now is: -26260.871036869965
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.453901]
objective value function right now is: -121320.69457603175
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.61536]
objective value function right now is: -106339.5878168861
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.134632]
objective value function right now is: -114959.75494991339
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [14.354305]
objective value function right now is: -95374.08732772722
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.959083]
objective value function right now is: -110619.30440201625
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.950501]
objective value function right now is: -78447.86640650395
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.987062]
objective value function right now is: 241952.7479834287
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.441857]
objective value function right now is: -102355.89485908806
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.242744]
objective value function right now is: -117388.23691651388
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.8885975]
objective value function right now is: -79848.57416802303
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.437088]
objective value function right now is: -111226.78826173428
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.202687]
objective value function right now is: -124550.22030502546
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.328932]
objective value function right now is: -125924.06572677642
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.400506]
objective value function right now is: -116067.19459144477
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.457312]
objective value function right now is: -113278.31736964911
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.950332]
objective value function right now is: -99828.30639864472
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -135859.2913438262
Current xi:  [14.7367115]
objective value function right now is: -135859.2913438262
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [14.410422]
objective value function right now is: -124934.91853113829
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [14.26587]
objective value function right now is: -115574.42122197492
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.071721]
objective value function right now is: 134125.20046369403
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.978161]
objective value function right now is: -118174.78867902738
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.333386]
objective value function right now is: 111124.47794063586
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.473531]
objective value function right now is: -107931.10275230098
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.898664]
objective value function right now is: -69884.13408434553
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.060878]
objective value function right now is: -95847.31274793808
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.000639]
objective value function right now is: 2741771.6053440007
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.5483556]
objective value function right now is: 287872.43470995134
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.929262]
objective value function right now is: 108957.4506616932
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.325187]
objective value function right now is: 384331.0784382864
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.524811]
objective value function right now is: -104009.60961812873
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.5441265]
objective value function right now is: -107511.49836689372
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.081781]
objective value function right now is: -102394.07762406909
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.587667]
objective value function right now is: -114036.39848411691
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.570899]
objective value function right now is: -73457.90432542187
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.177727]
objective value function right now is: -58637.94065037675
new min fval:  -135862.9932280742
new min fval:  -135955.1479628513
new min fval:  -136043.9300471921
new min fval:  -136118.00299546742
new min fval:  -136170.0207646966
new min fval:  -136190.14972691043
new min fval:  -136195.82551366408
new min fval:  -136219.44942802688
new min fval:  -136224.14868795747
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.445454]
objective value function right now is: -48024.16059148607
new min fval:  -136251.5461438972
new min fval:  -136281.2938097246
new min fval:  -136293.23373599537
new min fval:  -136298.43855373524
new min fval:  -136307.99918308808
new min fval:  -136352.04473197574
new min fval:  -136408.08528257773
new min fval:  -136468.06155063835
new min fval:  -136519.89695996285
new min fval:  -136569.45708547934
new min fval:  -136616.1491522527
new min fval:  -136653.03931363334
new min fval:  -136678.8189687048
new min fval:  -136693.74763872882
new min fval:  -136699.32754581602
new min fval:  -136701.0002268902
new min fval:  -136703.5588840186
new min fval:  -136714.66457249984
new min fval:  -136736.11108511247
new min fval:  -136767.1418982072
new min fval:  -136796.95381657578
new min fval:  -136817.9783665718
new min fval:  -136830.17229159796
new min fval:  -136832.22025665097
new min fval:  -136841.01927397365
new min fval:  -136869.86015602035
new min fval:  -136903.40644335732
new min fval:  -136930.87501848824
new min fval:  -136951.1707536106
new min fval:  -136967.2101244247
new min fval:  -136972.37760535855
new min fval:  -136972.55495683345
new min fval:  -136972.62852550187
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.642127]
objective value function right now is: -118738.62410794706
new min fval:  -136979.5943922737
new min fval:  -137004.50774261434
new min fval:  -137034.22348053634
new min fval:  -137059.3613992005
new min fval:  -137078.43254733298
new min fval:  -137101.38369214558
new min fval:  -137121.46127619196
new min fval:  -137137.39932914282
new min fval:  -137146.59547470204
new min fval:  -137152.52203246029
new min fval:  -137157.0474934531
new min fval:  -137159.78183970266
new min fval:  -137161.1822040731
new min fval:  -137162.3753360039
new min fval:  -137165.15758696903
new min fval:  -137168.90344971247
new min fval:  -137176.10001673957
new min fval:  -137191.82249058568
new min fval:  -137213.37125307432
new min fval:  -137243.08666756685
new min fval:  -137277.5934535265
new min fval:  -137313.1889029382
new min fval:  -137346.39076060712
new min fval:  -137371.04426448775
new min fval:  -137398.12345916068
new min fval:  -137421.81460708202
new min fval:  -137442.0693979712
new min fval:  -137454.50657168718
new min fval:  -137460.0828656432
new min fval:  -137463.19294157147
new min fval:  -137463.6434993014
new min fval:  -137466.57461179022
new min fval:  -137480.67499560374
new min fval:  -137499.3889258167
new min fval:  -137516.20394949114
new min fval:  -137531.9258081895
new min fval:  -137543.02292948222
new min fval:  -137550.65747984272
new min fval:  -137557.38351773005
new min fval:  -137566.85764682564
new min fval:  -137583.30317917414
new min fval:  -137603.68057296553
new min fval:  -137624.9399380916
new min fval:  -137648.19337645156
new min fval:  -137674.24620037075
new min fval:  -137696.85085153917
new min fval:  -137714.8904589401
new min fval:  -137729.15288864225
new min fval:  -137737.2142795648
new min fval:  -137753.73400498476
new min fval:  -137814.88093961935
new min fval:  -137865.82324006475
new min fval:  -137901.50477784267
new min fval:  -137919.0268259909
new min fval:  -137922.8904851784
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.394023]
objective value function right now is: -103942.0259299216
new min fval:  -137927.55607818638
new min fval:  -137946.75929199904
new min fval:  -137958.6229192019
new min fval:  -137963.3562837779
new min fval:  -137964.2329352041
new min fval:  -137965.8180141672
new min fval:  -137970.93725170367
new min fval:  -137977.5233547344
new min fval:  -137986.36385026178
new min fval:  -137998.65623568953
new min fval:  -138013.91073360102
new min fval:  -138032.37532649026
new min fval:  -138043.73312825937
new min fval:  -138050.1636066265
new min fval:  -138074.33766838917
new min fval:  -138112.5828204719
new min fval:  -138148.63066526275
new min fval:  -138165.0533708474
new min fval:  -138179.21785031556
new min fval:  -138199.0025297294
new min fval:  -138216.20077814933
new min fval:  -138226.55133937614
new min fval:  -138231.36924788254
new min fval:  -138241.36770682895
new min fval:  -138264.5217004492
new min fval:  -138272.74634456832
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.3589525]
objective value function right now is: -130086.51168115698
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.06496]
objective value function right now is: -102172.52621382117
min fval:  -138272.74634456832
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 985.010643932255
W_T_median: 757.2292114962838
W_T_pctile_5: 213.17786691609254
W_T_CVAR_5_pct: 27.64555019430804
Average q (qsum/M+1):  41.94958102318548
Optimal xi:  [14.356119]
Expected(across Rb) median(across samples) p_equity:  0.25123194555441536
obj fun:  tensor(-138272.7463, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5000.0
-----------------------------------------------
