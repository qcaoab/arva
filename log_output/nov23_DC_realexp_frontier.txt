Starting at: 
24-11-22_02:16

 Random seed:  2  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1  logistic_sigmoid   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer        4  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer        4  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0       (2, 4)     False        None  
0       (4, 4)     False        None  
0       (4, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer        4  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer        4  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0       (2, 4)     False        None  
0       (4, 4)     False        None  
0       (4, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1579.8088921591966
W_T_median: 1137.3149081751399
W_T_pctile_5: -129.57910035187072
W_T_CVAR_5_pct: -296.09225597438984
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.154988]
objective value function right now is: -1085.8919455121038
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.611255]
objective value function right now is: -1086.006415749281
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.0807905]
objective value function right now is: -1086.7866464041854
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.056944]
objective value function right now is: -1086.8196655732052
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.396065]
objective value function right now is: -1086.734677876857
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.480334]
objective value function right now is: -1086.160843264267
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [14.997642]
objective value function right now is: -1086.3593381719745
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.396455]
objective value function right now is: -1087.014585216073
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.335815]
objective value function right now is: -1085.7008228210204
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.638368]
objective value function right now is: -1086.7673268308783
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.946015]
objective value function right now is: -1086.3681843753395
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.0946045]
objective value function right now is: -1087.025045756421
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.817471]
objective value function right now is: -1087.0151081347149
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [15.500027]
objective value function right now is: -1086.9640204153416
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.644035]
objective value function right now is: -1086.8635893728156
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.327218]
objective value function right now is: -1086.821787867757
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.1029625]
objective value function right now is: -1087.1233793289562
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.313484]
objective value function right now is: -1087.075171456999
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.251654]
objective value function right now is: -1087.1049031499892
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.055839]
objective value function right now is: -1087.1406325360128
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.26724]
objective value function right now is: -1086.680052696205
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.913639]
objective value function right now is: -1086.674086748163
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.859705]
objective value function right now is: -1086.798400765523
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.204314]
objective value function right now is: -1087.1133743899957
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.455056]
objective value function right now is: -1087.009215208738
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.277763]
objective value function right now is: -1087.14216544625
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.623822]
objective value function right now is: -1086.9879611227968
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [15.119067]
objective value function right now is: -1087.187156894621
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [15.157916]
objective value function right now is: -1087.0790001057728
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.791101]
objective value function right now is: -1087.0379792593565
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.869849]
objective value function right now is: -1087.0056228545768
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.043265]
objective value function right now is: -1086.623001685286
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.059297]
objective value function right now is: -1086.8288329394254
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.800211]
objective value function right now is: -1087.0978259112821
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.664047]
objective value function right now is: -1086.9637531563774
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.073072]
objective value function right now is: -1087.0469867807917
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.494516]
objective value function right now is: -1087.2050693080444
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.341353]
objective value function right now is: -1087.009516624335
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.87055]
objective value function right now is: -1087.051055889546
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.715225]
objective value function right now is: -1086.969570443281
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.4746065]
objective value function right now is: -1087.0731903930052
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.410429]
objective value function right now is: -1087.1311920611724
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.076712]
objective value function right now is: -1086.7260626263758
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.666128]
objective value function right now is: -1086.9323399878294
new min fval:  -1070.8287448651333
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.2538]
objective value function right now is: -1087.1886281548468
new min fval:  -1087.1890223297087
new min fval:  -1087.1901193093433
new min fval:  -1087.1913941714045
new min fval:  -1087.1924791162307
new min fval:  -1087.193233319226
new min fval:  -1087.1936042057162
new min fval:  -1087.193656850209
new min fval:  -1087.1938279086296
new min fval:  -1087.1945365847685
new min fval:  -1087.1952050857983
new min fval:  -1087.1958339488792
new min fval:  -1087.1963408228567
new min fval:  -1087.1967163033723
new min fval:  -1087.1970232412652
new min fval:  -1087.1973218273417
new min fval:  -1087.1976264075774
new min fval:  -1087.197904458337
new min fval:  -1087.1981617365502
new min fval:  -1087.1983163413497
new min fval:  -1087.1983923619719
new min fval:  -1087.1984211709193
new min fval:  -1087.1984269555824
new min fval:  -1087.1984869566952
new min fval:  -1087.198717227481
new min fval:  -1087.1990819559583
new min fval:  -1087.1995819784631
new min fval:  -1087.2001231212705
new min fval:  -1087.2006992654071
new min fval:  -1087.2012781003216
new min fval:  -1087.2017993747681
new min fval:  -1087.2022930412595
new min fval:  -1087.2027427920136
new min fval:  -1087.203141805525
new min fval:  -1087.2034783578188
new min fval:  -1087.203760542837
new min fval:  -1087.2040252503923
new min fval:  -1087.2043321149938
new min fval:  -1087.2047349080017
new min fval:  -1087.2052336606978
new min fval:  -1087.2057958526823
new min fval:  -1087.2064665827975
new min fval:  -1087.2071991422863
new min fval:  -1087.2079037907326
new min fval:  -1087.208551889687
new min fval:  -1087.2090972231788
new min fval:  -1087.20951371513
new min fval:  -1087.2097809186525
new min fval:  -1087.2099272326027
new min fval:  -1087.2100314918375
new min fval:  -1087.2101838869294
new min fval:  -1087.2104619194183
new min fval:  -1087.2108394026566
new min fval:  -1087.2112802567083
new min fval:  -1087.2117904605263
new min fval:  -1087.2123190083616
new min fval:  -1087.2128736428656
new min fval:  -1087.2134093368577
new min fval:  -1087.2138851294067
new min fval:  -1087.2142922667258
new min fval:  -1087.2146188433599
new min fval:  -1087.2149120659672
new min fval:  -1087.215222090701
new min fval:  -1087.2156099066497
new min fval:  -1087.2161282440404
new min fval:  -1087.2167756741183
new min fval:  -1087.2175279394646
new min fval:  -1087.2183679829288
new min fval:  -1087.2191517890517
new min fval:  -1087.219831236108
new min fval:  -1087.2203861073904
new min fval:  -1087.220834662676
new min fval:  -1087.221195860672
new min fval:  -1087.2214663902478
new min fval:  -1087.2216832020515
new min fval:  -1087.2218969465923
new min fval:  -1087.2220913757992
new min fval:  -1087.2222617791945
new min fval:  -1087.2224201126837
new min fval:  -1087.2225727374187
new min fval:  -1087.2227194580862
new min fval:  -1087.222877082569
new min fval:  -1087.223059836926
new min fval:  -1087.2232858440177
new min fval:  -1087.223541958613
new min fval:  -1087.2238093412739
new min fval:  -1087.2240488617074
new min fval:  -1087.224272608425
new min fval:  -1087.224489980093
new min fval:  -1087.224701524311
new min fval:  -1087.2249182290104
new min fval:  -1087.2251716936403
new min fval:  -1087.2254338409673
new min fval:  -1087.225690158812
new min fval:  -1087.2258875232094
new min fval:  -1087.226052172308
new min fval:  -1087.2262122253244
new min fval:  -1087.2263110590595
new min fval:  -1087.2264018032376
new min fval:  -1087.226467321872
new min fval:  -1087.2266006611023
new min fval:  -1087.226751825805
new min fval:  -1087.2269167149038
new min fval:  -1087.2270835654076
new min fval:  -1087.2272206970995
new min fval:  -1087.2273617953522
new min fval:  -1087.227484356616
new min fval:  -1087.22761190734
new min fval:  -1087.2277269077877
new min fval:  -1087.227822940507
new min fval:  -1087.227893982045
new min fval:  -1087.2280082415573
new min fval:  -1087.228212483352
new min fval:  -1087.2285518945087
new min fval:  -1087.229038892533
new min fval:  -1087.2295849371899
new min fval:  -1087.2300637304913
new min fval:  -1087.2304301679433
new min fval:  -1087.2307127689082
new min fval:  -1087.2309008683187
new min fval:  -1087.2310110281906
new min fval:  -1087.2310679854736
new min fval:  -1087.2311072339767
new min fval:  -1087.2311427268016
new min fval:  -1087.2311995329142
new min fval:  -1087.2312769002222
new min fval:  -1087.231382584576
new min fval:  -1087.2315493997212
new min fval:  -1087.2318011924172
new min fval:  -1087.2321065802084
new min fval:  -1087.2324239004997
new min fval:  -1087.2327279184592
new min fval:  -1087.2329993288529
new min fval:  -1087.2332366942753
new min fval:  -1087.233509068008
new min fval:  -1087.2338354632818
new min fval:  -1087.234232314661
new min fval:  -1087.2346611328194
new min fval:  -1087.2350708567853
new min fval:  -1087.2354200507793
new min fval:  -1087.2357101750135
new min fval:  -1087.2359448935117
new min fval:  -1087.2361451825966
new min fval:  -1087.236310588586
new min fval:  -1087.2364895954056
new min fval:  -1087.2367026610298
new min fval:  -1087.2369367320075
new min fval:  -1087.237188199088
new min fval:  -1087.237468660013
new min fval:  -1087.237770264493
new min fval:  -1087.238071417776
new min fval:  -1087.2383137610275
new min fval:  -1087.2384955904004
new min fval:  -1087.2386088914252
new min fval:  -1087.238671859319
new min fval:  -1087.2387093454365
new min fval:  -1087.2387515918665
new min fval:  -1087.2387970530071
new min fval:  -1087.2388772457214
new min fval:  -1087.2390308000263
new min fval:  -1087.2392667699994
new min fval:  -1087.239589306439
new min fval:  -1087.2400187948965
new min fval:  -1087.2404661690846
new min fval:  -1087.2408513193961
new min fval:  -1087.2411823212674
new min fval:  -1087.2414311763116
new min fval:  -1087.2415653691096
new min fval:  -1087.2415655998545
new min fval:  -1087.241615645941
new min fval:  -1087.2419823658809
new min fval:  -1087.2422452806718
new min fval:  -1087.2424249355197
new min fval:  -1087.2425285017205
new min fval:  -1087.2425547414277
new min fval:  -1087.2425862257273
new min fval:  -1087.242931251696
new min fval:  -1087.2432775084876
new min fval:  -1087.2435927359495
new min fval:  -1087.2438399548444
new min fval:  -1087.2440210099821
new min fval:  -1087.2441135794459
new min fval:  -1087.2441293686727
new min fval:  -1087.2442307798467
new min fval:  -1087.2445484854154
new min fval:  -1087.2448759938459
new min fval:  -1087.2451791529759
new min fval:  -1087.2454213467752
new min fval:  -1087.245581964658
new min fval:  -1087.2456565071545
new min fval:  -1087.2456739691065
new min fval:  -1087.2457523343503
new min fval:  -1087.2459063452336
new min fval:  -1087.2460666744173
new min fval:  -1087.24622704815
new min fval:  -1087.246351697072
new min fval:  -1087.2464136803735
new min fval:  -1087.2464212992813
new min fval:  -1087.2464459874914
new min fval:  -1087.246684712268
new min fval:  -1087.246973027236
new min fval:  -1087.2472409780833
new min fval:  -1087.2474287827333
new min fval:  -1087.2474936768003
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.523702]
objective value function right now is: -1087.1447632402906
new min fval:  -1087.247496934077
new min fval:  -1087.2476457656776
new min fval:  -1087.2477574445893
new min fval:  -1087.247829904103
new min fval:  -1087.247863384507
new min fval:  -1087.2478678896323
new min fval:  -1087.2478963280041
new min fval:  -1087.2479427043097
new min fval:  -1087.2480087313304
new min fval:  -1087.2480781857855
new min fval:  -1087.248150954813
new min fval:  -1087.248199691005
new min fval:  -1087.2482096323542
new min fval:  -1087.2482113251904
new min fval:  -1087.2482619523466
new min fval:  -1087.2482867641468
new min fval:  -1087.2482905067488
new min fval:  -1087.2483300815106
new min fval:  -1087.248403955463
new min fval:  -1087.248475399136
new min fval:  -1087.2485396797083
new min fval:  -1087.2485615675823
new min fval:  -1087.2486048254866
new min fval:  -1087.2486808969945
new min fval:  -1087.2487480997997
new min fval:  -1087.2487807140153
new min fval:  -1087.2487978689394
new min fval:  -1087.2488190333975
new min fval:  -1087.2488716407624
new min fval:  -1087.2489309342354
new min fval:  -1087.2489802901414
new min fval:  -1087.2490291337328
new min fval:  -1087.2490683526792
new min fval:  -1087.2490914438574
new min fval:  -1087.249099715725
new min fval:  -1087.2491008701027
new min fval:  -1087.2491113632125
new min fval:  -1087.2491355374025
new min fval:  -1087.249170741165
new min fval:  -1087.249233100901
new min fval:  -1087.2493398603867
new min fval:  -1087.2494603233752
new min fval:  -1087.2495744481903
new min fval:  -1087.2496516229276
new min fval:  -1087.2496953595821
new min fval:  -1087.24976520344
new min fval:  -1087.2498848378223
new min fval:  -1087.2500247881371
new min fval:  -1087.2501798663263
new min fval:  -1087.250339198595
new min fval:  -1087.2504749361092
new min fval:  -1087.2505799345292
new min fval:  -1087.2506403157563
new min fval:  -1087.250667881009
new min fval:  -1087.2506987734907
new min fval:  -1087.2507002700825
new min fval:  -1087.250702564384
new min fval:  -1087.2507637148806
new min fval:  -1087.250855457331
new min fval:  -1087.2509682680761
new min fval:  -1087.2510943559953
new min fval:  -1087.2512282333314
new min fval:  -1087.2513570591439
new min fval:  -1087.2514855198208
new min fval:  -1087.2516056940883
new min fval:  -1087.2517336962203
new min fval:  -1087.2518623185963
new min fval:  -1087.2519814624436
new min fval:  -1087.2520947661158
new min fval:  -1087.2522170152654
new min fval:  -1087.2523304699023
new min fval:  -1087.2524434787856
new min fval:  -1087.252569094168
new min fval:  -1087.2526921292026
new min fval:  -1087.2528094322877
new min fval:  -1087.2529326557212
new min fval:  -1087.253043787478
new min fval:  -1087.2531205436333
new min fval:  -1087.2531704401233
new min fval:  -1087.2531751267015
new min fval:  -1087.2532583997486
new min fval:  -1087.253396700299
new min fval:  -1087.253520407531
new min fval:  -1087.2536104011149
new min fval:  -1087.253662047258
new min fval:  -1087.2536783053727
new min fval:  -1087.2537088047814
new min fval:  -1087.2537372044958
new min fval:  -1087.253740954505
new min fval:  -1087.2537892211044
new min fval:  -1087.2538780822306
new min fval:  -1087.2539355358856
new min fval:  -1087.2539780559678
new min fval:  -1087.2539794327474
Traceback (most recent call last):
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/decumulation_driver.py", line 857, in <module>
    fun_RUN__wrapper.RUN__wrapper_ONE_stage_optimization(
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_RUN__wrapper.py", line 115, in RUN__wrapper_ONE_stage_optimization
    RUN__wrapper_training_testing_NN(
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_RUN__wrapper.py", line 207, in RUN__wrapper_training_testing_NN
    res_adam = fun_train_NN.train_NN( theta0 = theta0,
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_train_NN.py", line 196, in train_NN
    result_pyt_adam = run_Gradient_Descent_pytorch(NN_list= NN_list,
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_train_NN_SGD_algorithms.py", line 197, in run_Gradient_Descent_pytorch
    new_fval, _ = objfun_pyt(swa_both_nns.module, params, xi_avg)
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_eval_objfun_NN_strategy.py", line 31, in eval_obj_NN_strategy_pyt
    params, g, qsum_T_vector = fun_invest_NN_strategy.withdraw_invest_NN_strategy(NN_list, params)
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_invest_NN_strategy.py", line 182, in withdraw_invest_NN_strategy
    a_t_n_output = NN_list[1].forward(phi_2)
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_NN_Pytorch.py", line 83, in forward
    return self.model(input_tensor)
  File "/home/marcchen/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/marcchen/.local/lib/python3.10/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/marcchen/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/marcchen/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.69 GiB total capacity; 17.98 GiB already allocated; 46.75 MiB free; 18.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
