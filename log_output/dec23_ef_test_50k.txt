Starting at: 
23-12-22_11:31

 Random seed:  2  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1  logistic_sigmoid   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  1981.7968070882316
Current xi:  [-5.9097]
objective value function right now is: 1981.7968070882316
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -601.1395219478209
Current xi:  [87.62968]
objective value function right now is: -601.1395219478209
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1657.2562425203585
Current xi:  [167.02132]
objective value function right now is: -1657.2562425203585
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.97055]
objective value function right now is: -1544.660430979499
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -2388.917358165434
Current xi:  [203.43163]
objective value function right now is: -2388.917358165434
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -2587.2846628450666
Current xi:  [204.82613]
objective value function right now is: -2587.2846628450666
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [205.93375]
objective value function right now is: -2562.4527189448772
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.72095]
objective value function right now is: -2289.746873356201
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.9202]
objective value function right now is: -2394.8842595618116
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -2637.8683039976863
Current xi:  [205.23853]
objective value function right now is: -2637.8683039976863
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.85555]
objective value function right now is: -2611.854204665555
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -2684.73021137893
Current xi:  [207.62753]
objective value function right now is: -2684.73021137893
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.4236]
objective value function right now is: -2665.578954822035
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [208.00682]
objective value function right now is: -1983.9389617296554
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -2734.202054037915
Current xi:  [206.1968]
objective value function right now is: -2734.202054037915
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.94252]
objective value function right now is: -2464.1635823947677
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.15807]
objective value function right now is: -2639.092576081628
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.77977]
objective value function right now is: -2329.3290529222695
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.57814]
objective value function right now is: -2432.6429466464797
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.1991]
objective value function right now is: -2483.066584129689
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.15733]
objective value function right now is: -2336.8850211502163
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.68054]
objective value function right now is: -2625.0899394443754
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.63159]
objective value function right now is: -2013.0802812174704
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.05675]
objective value function right now is: -2582.489103221512
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.36948]
objective value function right now is: -2620.615154019445
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.882]
objective value function right now is: -2499.7024888282126
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.22766]
objective value function right now is: -2630.3308599507063
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [206.8636]
objective value function right now is: -2473.0339998234276
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [208.98456]
objective value function right now is: -1936.046600473691
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.41574]
objective value function right now is: -2167.7776620940726
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.21028]
objective value function right now is: -2722.8152493312564
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.4205]
objective value function right now is: -2292.767835041732
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.9992]
objective value function right now is: -2671.2639911619426
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.5733]
objective value function right now is: -2495.0617399459024
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.53668]
objective value function right now is: -2465.801155852694
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.13365]
objective value function right now is: -2691.420588562481
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.2495]
objective value function right now is: 4808.4262227769195
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.76082]
objective value function right now is: -2672.825837395506
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2740.607150078162
Current xi:  [210.31917]
objective value function right now is: -2740.607150078162
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.91725]
objective value function right now is: -2592.094818473783
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.35951]
objective value function right now is: -2677.481483135377
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.11046]
objective value function right now is: -2581.79608243261
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.91754]
objective value function right now is: -2442.2392627100203
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.24013]
objective value function right now is: -2451.8546139639543
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.27339]
objective value function right now is: -2610.1160794653833
new min fval from sgd:  -2757.992591350698
new min fval from sgd:  -2766.416378286339
new min fval from sgd:  -2782.002769049286
new min fval from sgd:  -2784.4190435918217
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.91423]
objective value function right now is: -2408.918868638129
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.36308]
objective value function right now is: -2541.4718995988082
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.59839]
objective value function right now is: -2316.728621377692
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.89166]
objective value function right now is: -2315.89390878482
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.25827]
objective value function right now is: -2664.3002287180416
min fval:  -2784.4190435918217
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-111.6343,   -3.8611],
        [ -13.4390,   -1.7714],
        [ -12.7634,    7.0357],
        [ -12.4246,    3.8021],
        [ -12.4052,    3.7915],
        [ -12.3717,    3.7841],
        [ -12.3720,    3.7849],
        [ -12.4378,    3.7941],
        [ -12.3363,    3.7732],
        [ -12.4692,    3.8007]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  5.8532,  12.5888,   2.4605, -25.4302, -27.6076, -30.8728, -30.9055,
         -25.3457, -35.1578, -23.3423],
        [ 11.0290,   4.7489,   4.7227, -42.4441, -44.1558, -47.4480, -46.6577,
         -41.6693, -51.9402, -39.9282],
        [ 10.4221,   4.0246,   4.9428, -37.3694, -41.5546, -44.4214, -44.0720,
         -40.7974, -49.4982, -38.3173],
        [  4.6751,  10.1955,   2.6150, -26.2011, -28.0524, -31.9747, -31.7478,
         -25.5402, -36.1391, -23.4489],
        [ -9.0986,  -2.5528,  -4.4028,  33.1796,  35.6880,  37.9122,  37.7145,
          36.8009,  43.4202,  34.5754],
        [  5.8451,  12.6133,   2.4684, -25.7727, -27.5101, -30.8846, -30.7398,
         -25.2621, -35.4009, -23.3103],
        [ -5.7123, -11.3343,  -4.1154,  20.6898,  24.6258,  26.6490,  26.4539,
          24.1955,  31.2880,  21.6790],
        [ 10.5198,   4.1687,   5.0267, -37.8536, -42.0370, -45.1475, -44.8304,
         -40.3778, -50.0884, -38.6628],
        [  5.8279,  12.6933,   2.5122, -26.0935, -27.8313, -30.7907, -30.6346,
         -25.8096, -35.9454, -23.2198],
        [ -9.9368,  -3.6797,  -5.1671,  36.2792,  39.4288,  41.2270,  40.9929,
          40.4712,  45.8888,  39.9284]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-24.0144, -12.1305, -21.7212,  -4.6462,  28.6801, -24.3942,  51.9021,
         -23.1446, -26.6008,  28.3029]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -4.5995,   3.5894],
        [ -4.2945,   3.5088],
        [ -9.4842,  -2.4856],
        [ -4.6088,   3.5958],
        [-23.9897,  -5.9040],
        [ -9.5764,  -2.5374],
        [  7.5550,   3.2916],
        [ -1.1383,   2.5640],
        [ -4.0351,   3.4437],
        [-37.4416,  -3.4185]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[  71.1921,   85.1628,  -19.7225,   80.5284,    0.2014,  -20.3353,
           76.0320,   79.8442,   86.0762,    2.3572],
        [  -6.7912,   -4.2657,  -17.5555,   -7.8437,   16.8572,  -17.7387,
          -18.0859,  -29.8142,   -2.3335,  -41.9385],
        [ -12.8917,   -8.7626,  -15.8770,  -14.9916,   16.1709,  -16.1295,
          -17.3374,  -24.5246,   -6.0453,  -93.3360],
        [  -7.5673,   -9.8921,  -11.6134,   -8.1279,   11.1212,  -11.1030,
          -17.6976,  -24.4148,  -10.0203,  -49.9801],
        [-173.1420, -192.9839,   -2.6225, -182.4577,   19.7628,   -2.5085,
           -4.6424, -138.7256, -196.3271,   24.7247],
        [ -31.4162,  -25.4295,   -5.8967,  -32.5617,   18.2312,   -5.7700,
          -13.4386,  -26.1136,  -24.4545, -149.4508],
        [  -7.8266,   -8.7667,  -11.4814,   -8.5346,   11.1367,  -11.1246,
          -17.6602,  -24.2186,   -9.1890,  -59.0271],
        [  -6.0485,   -7.7453,  -14.9985,   -7.2226,   14.0603,  -13.9917,
          -18.3631,  -26.4854,   -8.5204,  -43.7675],
        [   2.0495,   -0.7437,   -4.6591,    0.6906,    4.2081,   -3.9789,
          -18.9086,  -13.1346,   -3.2147,  -49.3207],
        [ -77.8017,  -64.9101,    1.7594,  -83.7871,    3.5213,    1.7736,
           -3.6646,   -6.7129,  -54.1701,    6.5911]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.3671, -13.6997, -13.3155, -15.6513,  18.5792,  -2.6520, -13.5021,
         -13.0184, -13.6413, -11.2851],
        [ -0.7805,  14.0650,  13.1401,  16.0493, -18.7223,   2.5205,  13.5221,
          13.1956,  13.7347,  10.7535]], device='cuda:0'))])
xi:  [208.62328]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 758.8112051345574
W_T_median: 537.8223478078448
W_T_pctile_5: 210.0614996622156
W_T_CVAR_5_pct: 27.457065288886795
Average q (qsum/M+1):  45.55118290070565
Optimal xi:  [208.62328]
Expected(across Rb) median(across samples) p_equity:  0.24107056284944217
obj fun:  tensor(-2784.4190, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1266.6320068508132
Current xi:  [-14.163458]
objective value function right now is: -1266.6320068508132
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1423.378346981421
Current xi:  [71.386505]
objective value function right now is: -1423.378346981421
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1518.2711011759673
Current xi:  [138.50381]
objective value function right now is: -1518.2711011759673
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1556.4945951680993
Current xi:  [170.42203]
objective value function right now is: -1556.4945951680993
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [178.28003]
objective value function right now is: -1528.881854410455
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.31192]
objective value function right now is: -1556.2368369189314
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [180.02917]
objective value function right now is: -1552.349776490187
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1568.0197858509082
Current xi:  [182.3758]
objective value function right now is: -1568.0197858509082
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.69156]
objective value function right now is: -1564.9851752936547
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1576.5984673376347
Current xi:  [183.34319]
objective value function right now is: -1576.5984673376347
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1580.3835710697006
Current xi:  [183.6943]
objective value function right now is: -1580.3835710697006
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.13417]
objective value function right now is: -1557.156133185606
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.19]
objective value function right now is: -1570.8337966829968
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [183.47813]
objective value function right now is: -1565.1540943564041
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.09616]
objective value function right now is: -1553.8703422135177
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.34824]
objective value function right now is: -1535.3772810159787
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1584.1216516150287
Current xi:  [185.05713]
objective value function right now is: -1584.1216516150287
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.46454]
objective value function right now is: -1566.1054366620183
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.66385]
objective value function right now is: -1576.7184318101945
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.65338]
objective value function right now is: -1551.504144996855
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.58163]
objective value function right now is: -1568.3272536400855
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.37988]
objective value function right now is: -1583.9841133964014
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.83875]
objective value function right now is: -1574.4382740002309
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.88622]
objective value function right now is: -1560.253392987995
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.01239]
objective value function right now is: -1573.1217025803294
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.5572]
objective value function right now is: -1580.482067553697
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.56482]
objective value function right now is: -1566.719694427638
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [183.19957]
objective value function right now is: -1573.8651984088037
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [184.92102]
objective value function right now is: -1578.4127600542456
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.26141]
objective value function right now is: -1555.4633085236549
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.01639]
objective value function right now is: -1550.595325411968
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [179.46468]
objective value function right now is: -1574.8243396325302
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.7122]
objective value function right now is: -1582.6239445934993
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.50072]
objective value function right now is: -1553.6956583597769
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.2454]
objective value function right now is: -1583.3499426456765
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.01288]
objective value function right now is: -1540.492524215351
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.84201]
objective value function right now is: -1556.7168201738634
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.33833]
objective value function right now is: -1581.2996411772124
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.31569]
objective value function right now is: -1499.9365174616964
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [176.49101]
objective value function right now is: -1556.8078661141083
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [171.00343]
objective value function right now is: -1535.8418697048892
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [175.30865]
objective value function right now is: -1547.7715429066686
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [179.56137]
objective value function right now is: -1485.0832287656028
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.71861]
objective value function right now is: -1575.1246273356778
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.4285]
objective value function right now is: -1567.7734164510025
new min fval from sgd:  -1585.1207897677798
new min fval from sgd:  -1585.1374424853866
new min fval from sgd:  -1586.4752156997752
new min fval from sgd:  -1588.422524599561
new min fval from sgd:  -1591.5202488140008
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.17001]
objective value function right now is: -1566.8490239327396
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.1575]
objective value function right now is: -1583.6020398775897
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.5064]
objective value function right now is: -1567.0706774993905
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.78651]
objective value function right now is: -1560.3497509153076
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.57832]
objective value function right now is: -1526.099774193634
min fval:  -1591.5202488140008
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-90.5651,  -2.9698],
        [-10.8607,  -1.8626],
        [-11.0673,   2.4062],
        [-11.2573,   2.5039],
        [-11.5016,   2.5911],
        [-15.2278,   3.9883],
        [-15.2626,   4.0018],
        [-11.6933,   2.6722],
        [-16.2002,   4.4198],
        [-12.0431,   2.8030]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[   9.2376,   15.3081,  -19.2637,  -22.8190,  -25.5478,  -35.6887,
          -35.9706,  -22.9337,  -45.2318,  -21.7236],
        [  87.3243,   31.9668, -166.4152, -143.3990, -160.9625, -194.1296,
         -194.1578, -135.2900, -200.8354, -124.5105],
        [  87.0809,   31.6502, -174.2117, -138.4841, -158.7604, -192.3135,
         -192.8148, -134.4017, -199.9429, -122.6862],
        [-382.2328,   19.1322,  -30.6910,  -24.5396,  -26.4058,  -30.8985,
          -30.9218,  -22.6020,  -38.9956,  -18.4023],
        [ -84.9194,  -30.8526,  189.0089,  131.1039,  150.2556,  184.6987,
          185.4128,  126.8471,  193.6570,  114.9728],
        [   9.2462,   15.3319,  -19.2730,  -23.1918,  -25.4808,  -35.7240,
          -35.8281,  -22.8789,  -45.4970,  -21.7170],
        [  -7.9300,  -13.6050,   16.0849,   16.7539,   21.2247,   30.3369,
           30.3836,   20.5590,   40.1799,   19.0108],
        [  86.9144,   31.5982, -172.8557, -138.5810, -158.8587, -192.6288,
         -193.1617, -133.5983, -200.1338, -122.6483],
        [   9.2889,   15.4546,  -19.3125,  -23.6144,  -25.9060,  -35.6933,
          -35.7865,  -23.5194,  -46.0956,  -21.7026],
        [ -84.0637,  -31.1762,  176.1083,  132.2185,  151.7889,  184.6912,
          185.3384,  128.6422,  192.5952,  118.5580]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-36.8208, -29.0566, -37.6427,  18.6271,  41.7941, -37.1778,  92.5167,
         -39.1529, -39.2641,  42.0192]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.7212,   3.9993],
        [ -3.6790,   3.8987],
        [ -6.9042,  -1.6702],
        [ -3.7222,   4.0028],
        [-26.5447,  -6.2384],
        [ -6.9260,  -1.6738],
        [  8.1107,   3.2080],
        [  1.8107,   4.0915],
        [ -3.6184,   3.8201],
        [-35.1703,  -3.1612]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 123.3852,  136.8902,  -26.0148,  132.1450,   -6.2351,  -25.9300,
          120.4468,  125.8381,  138.7153,   12.5978],
        [ -10.6837,   -7.5015,  -51.1062,  -11.7923,    6.6324,  -51.5883,
          -23.7206,  -47.7093,   -5.6062,  -34.0749],
        [ -10.0437,   -5.5743,  -33.5842,  -12.1896,   24.4310,  -33.3977,
          -34.6230,  -36.1768,   -3.0527,  -76.4191],
        [ -16.8516,  -17.8488,  -47.8452,  -17.5034,   -5.0631,  -47.6438,
          -47.8466,  -35.2423,  -16.5459,  -55.1048],
        [-297.5830, -316.7604,   -1.6407, -307.2939,   16.5426,   -1.4484,
           -4.6818, -210.9223, -321.2692,   21.7841],
        [ -29.3884,  -25.9139,  -10.4699,  -30.4006,   26.5699,  -10.6221,
           -9.5471,  -59.1421,  -27.3285, -184.1714],
        [ -18.5898,  -18.8932,  -47.4170,  -19.3566,   -6.7018,  -47.3325,
          -41.5498,  -40.3604,  -18.6456,  -56.1305],
        [ -38.8378,  -39.1542,  -14.8153,  -40.1219,   28.1413,  -14.2752,
          -58.2999,  -52.8203,  -38.8383,  -27.2182],
        [ -20.8098,  -15.7339,   -3.9303,  -22.5997,    5.1495,   -3.6860,
           -9.7387,   -7.3549,  -10.6506, -114.8961],
        [-143.0730, -123.4303,    2.8912, -149.2007,    4.5603,    3.0984,
           -4.0701,   -2.7857, -105.1842,    7.9373]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.2703, -10.5815,  -4.0383,  -2.3137,  24.9690,  -4.3092,  -4.3097,
           5.7886, -18.4876, -11.9784],
        [ -0.6837,  10.9470,   3.8632,   2.7119, -25.1092,   4.1746,   4.3299,
          -5.6108,  18.5813,  11.4479]], device='cuda:0'))])
xi:  [183.70161]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 697.9528856032218
W_T_median: 440.8705791151063
W_T_pctile_5: 181.56580896276762
W_T_CVAR_5_pct: 18.910558714703413
Average q (qsum/M+1):  48.29499275453629
Optimal xi:  [183.70161]
Expected(across Rb) median(across samples) p_equity:  0.2781015545129776
obj fun:  tensor(-1591.5202, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1386.0319544811327
Current xi:  [-17.65186]
objective value function right now is: -1386.0319544811327
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1476.837399688965
Current xi:  [62.273975]
objective value function right now is: -1476.837399688965
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1491.7800166794302
Current xi:  [85.82346]
objective value function right now is: -1491.7800166794302
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1505.516714106847
Current xi:  [129.26709]
objective value function right now is: -1505.516714106847
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [149.52228]
objective value function right now is: -1418.5288352215189
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1519.4357171750385
Current xi:  [158.78897]
objective value function right now is: -1519.4357171750385
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1540.8623663551634
Current xi:  [160.7635]
objective value function right now is: -1540.8623663551634
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.58928]
objective value function right now is: -1520.3912942639126
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1548.472464205835
Current xi:  [164.02223]
objective value function right now is: -1548.472464205835
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [158.42596]
objective value function right now is: -1542.527643392058
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.59953]
objective value function right now is: -1524.4227830039858
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.80322]
objective value function right now is: -1511.2659239653965
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.4644]
objective value function right now is: -1544.670813538652
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [165.36151]
objective value function right now is: -1533.1821511450762
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.97241]
objective value function right now is: -1480.7956511773943
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.39105]
objective value function right now is: -1543.4978677976799
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.23967]
objective value function right now is: -1533.6412194100008
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.18475]
objective value function right now is: -1529.6003200979042
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.00247]
objective value function right now is: -1540.406399309215
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.95477]
objective value function right now is: -1508.560812251458
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.9391]
objective value function right now is: -1506.587523842222
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.50327]
objective value function right now is: -1541.0232650551156
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.81935]
objective value function right now is: -1542.0923438792238
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.25911]
objective value function right now is: -1537.2005484297633
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.61156]
objective value function right now is: -1547.970704319735
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.68182]
objective value function right now is: -1536.3835544783842
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.37498]
objective value function right now is: -1540.6570410387567
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [163.58832]
objective value function right now is: -1548.2159017791344
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [161.23286]
objective value function right now is: -1541.6896890396702
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.23064]
objective value function right now is: -1548.4445721953398
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.53981]
objective value function right now is: -1523.6761591570796
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.54517]
objective value function right now is: -1540.3983031270607
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.11621]
objective value function right now is: -1539.7461297314978
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1550.1958953632693
Current xi:  [166.04117]
objective value function right now is: -1550.1958953632693
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.87537]
objective value function right now is: -1530.3848624051764
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.37112]
objective value function right now is: -1547.2865344072807
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.37724]
objective value function right now is: -1532.9202317278675
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1554.307010565632
Current xi:  [162.32071]
objective value function right now is: -1554.307010565632
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.64865]
objective value function right now is: -1534.431783585989
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.35359]
objective value function right now is: -1542.7868652897246
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.95078]
objective value function right now is: -1543.264251355543
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.61314]
objective value function right now is: -1517.3865194575233
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [159.32286]
objective value function right now is: -1548.1817400141108
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.13419]
objective value function right now is: -1540.5939997962741
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.31366]
objective value function right now is: -1541.666495570745
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.75505]
objective value function right now is: -1549.076533257712
new min fval from sgd:  -1555.5111759345546
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.62048]
objective value function right now is: -1551.4668146710005
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.05956]
objective value function right now is: -1521.3041811069593
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [159.45366]
objective value function right now is: -1541.2426328485394
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.26778]
objective value function right now is: -1532.4718737004978
min fval:  -1555.5111759345546
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-113.6183,   -4.3203],
        [ -10.8398,   -1.8172],
        [ -10.5857,    2.5003],
        [ -11.8093,    3.0133],
        [ -11.5803,    2.9271],
        [ -15.6473,    4.7175],
        [ -15.6484,    4.7183],
        [ -15.4476,    4.5901],
        [ -15.6670,    4.7641],
        [ -15.7771,    4.7601]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 1.0007e+01,  1.1972e+01, -2.3226e+01, -2.6995e+01, -2.9216e+01,
         -5.6529e+01, -5.6877e+01, -3.5262e+01, -6.7808e+01, -3.7948e+01],
        [ 8.1438e+01,  2.2466e+01, -2.2809e+02, -2.1942e+02, -2.3732e+02,
         -2.7425e+02, -2.7428e+02, -2.1167e+02, -2.7989e+02, -2.0092e+02],
        [ 8.1718e+01,  2.2747e+01, -2.3575e+02, -2.1491e+02, -2.3554e+02,
         -2.7532e+02, -2.7583e+02, -2.1184e+02, -2.8197e+02, -2.0058e+02],
        [-4.8915e+02,  1.2682e+01, -2.7376e+01, -1.5595e+01, -1.8169e+01,
         -1.6766e+01, -1.6786e+01, -1.1038e+01, -2.7303e+01, -5.9413e+00],
        [-8.1916e+01, -2.3190e+01,  2.5046e+02,  2.1043e+02,  2.2961e+02,
          2.7407e+02,  2.7480e+02,  2.0956e+02,  2.8166e+02,  1.9861e+02],
        [ 3.7989e-01,  1.3648e+01, -2.3844e+01, -2.6401e+01, -2.8846e+01,
         -5.8098e+01, -5.8287e+01, -3.5166e+01, -7.0090e+01, -3.8486e+01],
        [-8.6408e+00, -1.1041e+01,  1.9154e+01,  2.0250e+01,  2.4142e+01,
          5.0177e+01,  5.0289e+01,  3.2022e+01,  6.1572e+01,  3.4302e+01],
        [ 8.1554e+01,  2.2646e+01, -2.3427e+02, -2.1491e+02, -2.3553e+02,
         -2.7513e+02, -2.7567e+02, -2.1091e+02, -2.8161e+02, -2.0033e+02],
        [ 2.0871e-01,  1.3905e+01, -2.3802e+01, -2.6671e+01, -2.9148e+01,
         -5.7968e+01, -5.8147e+01, -3.5616e+01, -7.0601e+01, -3.8289e+01],
        [-7.2035e+01, -2.3328e+01,  1.9623e+02,  1.5947e+02,  1.7935e+02,
          2.2645e+02,  2.2714e+02,  1.6230e+02,  2.3683e+02,  1.5567e+02]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-45.8434, -37.2252, -45.9045,  15.4001,  45.2390, -40.0172, 119.5487,
         -47.3360, -41.9656,  54.0715]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.8540,   4.5173],
        [ -3.8334,   4.4757],
        [ -6.6824,  -1.6073],
        [ -3.8550,   4.5182],
        [-81.4492,  -5.7469],
        [-43.1133,  -6.2368],
        [  9.5565,   3.7338],
        [  4.6776,   4.9653],
        [ -3.8110,   4.4432],
        [-31.7757,  -8.1983]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.2314e+02,  2.3723e+02, -1.9348e+01,  2.3184e+02,  1.2369e+01,
          8.5833e+01,  1.2553e+02,  1.8930e+02,  2.3952e+02,  1.3779e+01],
        [ 5.5188e+00,  1.0160e+01, -8.3417e+00,  4.3656e+00, -4.1691e+01,
         -5.9728e+00, -1.4865e+01, -3.9156e+01,  1.3455e+01, -5.2856e+00],
        [-5.9629e+00, -1.3209e-01, -1.7937e+01, -8.1944e+00,  2.2737e+01,
         -3.3047e+01, -2.0369e+01, -4.2001e+01,  3.2958e+00, -5.0799e+01],
        [ 2.9336e+00,  2.1815e+00, -7.5211e+00,  2.2821e+00, -1.1710e+00,
         -2.5622e+01, -2.7184e+01, -1.0130e+01,  3.6450e+00, -2.7989e+01],
        [-4.1477e+02, -4.2755e+02, -1.4895e+00, -4.2469e+02,  1.0775e+01,
          1.0573e+01, -4.1377e+00, -3.0355e+02, -4.2676e+02,  9.7094e+00],
        [-2.3253e+01, -1.9347e+01, -1.8709e+01, -2.4278e+01,  1.9948e+01,
         -1.0925e+01, -2.5852e+01, -7.5796e+01, -2.0195e+01, -1.7567e+02],
        [-9.3767e+00, -8.6588e+00, -1.9690e+00, -1.0168e+01, -1.8007e+00,
         -1.0821e+01, -2.0728e+01, -2.4571e+01, -7.4610e+00, -2.5626e+01],
        [-6.1372e+01, -6.1067e+01, -3.2389e+01, -6.2677e+01,  2.3047e+01,
         -2.1761e+01, -8.9786e+01, -6.4670e+01, -6.0209e+01, -3.3226e+01],
        [-6.5175e+01, -5.8415e+01,  5.6096e+00, -6.7076e+01,  3.5611e+00,
         -9.8262e+00, -2.5094e+01, -4.1564e+01, -5.2074e+01, -1.0600e+02],
        [-2.1383e+02, -1.9228e+02,  3.5245e+00, -2.1997e+02,  4.3832e+00,
          5.5634e+00, -4.3302e+00, -1.9543e+00, -1.7241e+02,  3.1575e+00]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.2704, -35.4898, -19.7049, -30.9898,  25.1772,  -6.7806, -34.7494,
          -5.2151, -22.6064, -18.4381],
        [ -0.6837,  35.8558,  19.5293,  31.3911, -25.3177,   6.6461,  34.7753,
           5.3942,  22.6860,  17.9076]], device='cuda:0'))])
xi:  [162.50865]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 657.950685241796
W_T_median: 400.81917416968224
W_T_pctile_5: 164.17178663328502
W_T_CVAR_5_pct: 13.993035586997912
Average q (qsum/M+1):  48.82774697580645
Optimal xi:  [162.50865]
Expected(across Rb) median(across samples) p_equity:  0.27805301249027253
obj fun:  tensor(-1555.5112, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1490.7199620353913
Current xi:  [-36.264008]
objective value function right now is: -1490.7199620353913
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1518.0723514739611
Current xi:  [13.546086]
objective value function right now is: -1518.0723514739611
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1530.0902759552162
Current xi:  [50.877026]
objective value function right now is: -1530.0902759552162
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [77.293495]
objective value function right now is: -1527.8258761294767
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1540.6730991245765
Current xi:  [93.45018]
objective value function right now is: -1540.6730991245765
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [102.87618]
objective value function right now is: -1524.1060628557354
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1546.2626409311877
Current xi:  [108.4255]
objective value function right now is: -1546.2626409311877
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.89137]
objective value function right now is: -1538.6081499050724
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [110.071526]
objective value function right now is: -1543.856432363904
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [110.94319]
objective value function right now is: -1529.9380564166668
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.13478]
objective value function right now is: -1543.829512752869
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [99.36362]
objective value function right now is: -1500.1407850282435
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [99.56957]
objective value function right now is: -1540.649904934051
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [104.71275]
objective value function right now is: -1544.485418200924
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.29902]
objective value function right now is: -1534.7058333747107
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [99.725174]
objective value function right now is: -1469.765255686015
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [85.93452]
objective value function right now is: -1514.655279057483
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [86.61326]
objective value function right now is: -1505.7420427597087
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [87.399216]
objective value function right now is: -1513.2817866602438
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [91.082306]
objective value function right now is: -1530.9303494859637
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [96.49212]
objective value function right now is: -1520.286466512045
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [99.46267]
objective value function right now is: -1523.1204275803398
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [98.65242]
objective value function right now is: -1537.3324499757066
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.94463]
objective value function right now is: -1523.4720955695645
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.01809]
objective value function right now is: -1520.6798463250939
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.32824]
objective value function right now is: -1537.9977818778348
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.22535]
objective value function right now is: -1528.324288247272
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [109.71433]
objective value function right now is: -1541.9792921331739
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [109.96588]
objective value function right now is: -1533.0365628485656
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.051476]
objective value function right now is: -1537.0700137820486
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.55098]
objective value function right now is: -1540.7396356920206
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.02334]
objective value function right now is: -1524.0834952790674
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.55788]
objective value function right now is: -1535.0347259045218
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.842964]
objective value function right now is: -1535.0830354218797
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.71199]
objective value function right now is: -1534.374347387153
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.06008]
objective value function right now is: -1523.0285348598802
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.22453]
objective value function right now is: -1542.191368849743
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.552055]
objective value function right now is: -1540.4238228240733
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.19546]
objective value function right now is: -1531.6097943577101
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.39848]
objective value function right now is: -1495.990309664778
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [98.70938]
objective value function right now is: -1534.4578889445884
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.798294]
objective value function right now is: -1538.0889411666221
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.93525]
objective value function right now is: -1536.523139161797
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.79857]
objective value function right now is: -1542.3180923077937
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.30472]
objective value function right now is: -1542.3931743361738
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.120476]
objective value function right now is: -1529.0807389085812
new min fval from sgd:  -1547.0554289587708
new min fval from sgd:  -1547.0677374180104
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.13621]
objective value function right now is: -1531.3338856657033
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.14188]
objective value function right now is: -1526.2275046978484
new min fval from sgd:  -1547.4197129031595
new min fval from sgd:  -1547.5517720501734
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.167496]
objective value function right now is: -1538.5671146126879
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [111.764046]
objective value function right now is: -1541.637232795145
min fval:  -1547.5517720501734
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-120.3327,   -4.2798],
        [ -11.0869,   -1.7918],
        [ -10.0700,    2.9522],
        [ -11.3588,    3.5741],
        [ -11.4568,    3.6236],
        [ -13.2210,    4.5580],
        [ -13.2228,    4.5591],
        [ -13.0441,    4.4460],
        [ -13.3439,    4.6446],
        [ -13.2344,    4.5595]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[   9.7684,   11.6964,  -26.0512,  -32.1808,  -34.3730,  -71.3828,
          -71.7476,  -48.6855,  -83.7574,  -52.8875],
        [  75.6592,   18.4634, -281.4156, -276.3535, -294.3290, -334.5904,
         -334.6243, -270.9713, -340.8503, -260.8749],
        [  75.8749,   18.6432, -288.7554, -271.6308, -292.3415, -335.6116,
         -336.1238, -271.0739, -342.8795, -260.4855],
        [-666.0516,   11.4048,  -29.3909,  -16.9018,  -19.9888,  -21.6710,
          -21.7020,  -14.1938,  -33.9850,  -10.4304],
        [ -76.2105,  -18.9980,  303.2041,  267.6924,  286.9031,  335.3702,
          336.1024,  269.8571,  343.5322,  259.5605],
        [  -0.9875,   12.8881,  -23.8817,  -27.8701,  -30.5580,  -69.8061,
          -70.0130,  -45.6359,  -82.9070,  -50.5153],
        [  -8.7327,  -11.0848,   21.9535,   25.3878,   29.2340,   64.5640,
           64.6913,   45.0521,   76.9792,   48.7682],
        [  75.7661,   18.5807, -287.3309, -271.7194, -292.4062, -335.4748,
         -336.0209, -270.2113, -342.5775, -260.3043],
        [  -2.5551,   14.8702,  -23.6751,  -28.1705,  -30.9019,  -70.0794,
          -70.2782,  -46.3772,  -83.9209,  -50.6962],
        [ -60.5099,  -17.3622,  206.4218,  174.7010,  194.5916,  254.6456,
          255.3459,  187.9531,  267.3750,  183.7695]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-49.3366, -44.8569, -53.6488,  13.5637,  48.5775, -42.2752, 145.7147,
         -55.0252, -44.2108,  60.7086]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.9795,   3.8313],
        [ -3.1100,   4.0033],
        [ -5.1564,   3.9802],
        [ -3.9345,   3.8381],
        [-60.7253,   7.3054],
        [-39.8955,  -3.1069],
        [ 13.6576,   5.5869],
        [ -5.0753,   3.7794],
        [  2.7423,   4.9141],
        [-23.9835,  -4.8354]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 3.3358e+02,  3.4658e+02,  4.0322e+01,  3.4233e+02,  1.2414e+01,
          1.2593e+02,  1.8027e+02,  2.9122e+02,  3.3908e+02,  3.1443e+00],
        [-9.7442e+01, -7.7625e+01, -7.1637e+01, -9.7785e+01, -1.0624e+02,
          7.7870e+00, -5.0056e+00, -1.7780e+02, -2.7824e+01,  6.9903e+00],
        [ 2.7369e+00,  8.6454e+00, -2.6547e+01,  4.5279e-01,  2.2733e+01,
         -2.6356e+01, -4.6910e+01, -5.0176e+01, -5.3771e+00, -4.7312e+01],
        [ 1.9186e+00, -1.6803e+00,  3.4984e+01,  1.3016e+00, -8.1704e+00,
          1.2153e+01, -3.9992e+01, -2.6322e+00, -2.0321e+01, -3.4349e+01],
        [-4.7633e+02, -4.9361e+02, -5.5467e+01, -4.8655e+02,  4.9499e+01,
          3.3043e+01, -5.5981e+00, -3.7207e+02, -4.9026e+02,  2.5179e+01],
        [ 4.1040e+00,  7.6901e+00,  6.9962e+00,  3.0334e+00,  2.1880e+02,
          1.5028e+02, -3.0741e+01, -3.6755e+01,  5.6084e+00, -1.3539e+02],
        [-9.6110e+01, -9.5606e+01, -3.5045e+01, -9.6760e+01, -1.1587e+01,
         -2.6865e+01, -3.2712e+01, -1.0456e+02, -1.1164e+02, -5.4777e+01],
        [-1.6361e+02, -1.6064e+02, -1.2825e+02, -1.6478e+02,  2.3047e+01,
         -9.5411e+01, -1.4682e+02, -1.7737e+02, -1.5403e+02, -2.4088e+01],
        [-8.6603e+01, -7.9030e+01,  3.5102e+00, -8.8328e+01,  3.5601e+00,
         -8.8077e+01, -3.1770e+01, -4.6576e+01, -1.0826e+02, -9.0510e+01],
        [-2.3225e+02, -2.1021e+02, -1.2716e+00, -2.3799e+02, -1.0258e+01,
         -1.3948e+02, -1.3936e+01, -4.9630e+00, -2.1659e+02, -3.0956e+01]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.1872, -41.4397,  -6.8588, -30.4390,  45.2196,   6.1316, -33.9774,
         -18.1517, -26.0290, -11.3766],
        [ -0.6005,  41.8056,   6.6827,  30.8399, -45.3584,  -6.2661,  34.0026,
          18.3329,  26.1096,  10.8464]], device='cuda:0'))])
xi:  [111.94411]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 615.9860883760564
W_T_median: 330.430707760402
W_T_pctile_5: 112.46719561156243
W_T_CVAR_5_pct: -14.230523939413683
Average q (qsum/M+1):  50.609890845514116
Optimal xi:  [111.94411]
Expected(across Rb) median(across samples) p_equity:  0.3126564145088196
obj fun:  tensor(-1547.5518, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1530.57003956998
Current xi:  [-57.078716]
objective value function right now is: -1530.57003956998
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1541.2700087412977
Current xi:  [-25.702549]
objective value function right now is: -1541.2700087412977
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.301476]
objective value function right now is: -1536.5303129930935
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1551.7184073488354
Current xi:  [14.130512]
objective value function right now is: -1551.7184073488354
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1553.1207549328356
Current xi:  [24.640844]
objective value function right now is: -1553.1207549328356
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [34.659634]
objective value function right now is: -1543.0106870595305
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [42.459957]
objective value function right now is: -1539.2444302557055
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1556.506024415478
Current xi:  [45.37094]
objective value function right now is: -1556.506024415478
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [47.739098]
objective value function right now is: -1554.8665287452252
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [49.447914]
objective value function right now is: -1542.976780285386
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [51.15072]
objective value function right now is: -1547.9078045709641
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.21519]
objective value function right now is: -1538.763849846431
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.73738]
objective value function right now is: -1544.203218275629
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [52.098194]
objective value function right now is: -1556.5051101666518
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.97861]
objective value function right now is: -1544.5473123194618
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.09967]
objective value function right now is: -1553.9055463911884
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.659054]
objective value function right now is: -1554.3895948748197
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1557.3661655252004
Current xi:  [53.752525]
objective value function right now is: -1557.3661655252004
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.343475]
objective value function right now is: -1556.9872108268594
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.29673]
objective value function right now is: -1551.7470029361941
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.11431]
objective value function right now is: -1537.8934503196658
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [51.83041]
objective value function right now is: -1547.1967656369727
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.87528]
objective value function right now is: -1549.909151225611
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.874855]
objective value function right now is: -1547.7793243818633
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.88644]
objective value function right now is: -1553.4231188509889
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.774582]
objective value function right now is: -1542.430405985689
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.092205]
objective value function right now is: -1549.4110790417567
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1559.340049073477
Current xi:  [53.91632]
objective value function right now is: -1559.340049073477
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [53.770046]
objective value function right now is: -1548.0228188808221
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.076824]
objective value function right now is: -1536.6216632184555
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [51.574913]
objective value function right now is: -1547.2897761817833
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [49.625862]
objective value function right now is: -1543.449403553383
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [50.318645]
objective value function right now is: -1546.3768759083114
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [51.356953]
objective value function right now is: -1554.0366545757174
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [50.53023]
objective value function right now is: -1549.5690671128211
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [50.901096]
objective value function right now is: -1546.6894472668298
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [51.559006]
objective value function right now is: -1553.984465049279
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [50.998325]
objective value function right now is: -1551.864873594356
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.53467]
objective value function right now is: -1553.5386953653135
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.726265]
objective value function right now is: -1552.819620946914
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [50.619343]
objective value function right now is: -1553.818082637676
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.120052]
objective value function right now is: -1546.4225265957828
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.87476]
objective value function right now is: -1546.320468909878
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.26909]
objective value function right now is: -1552.2099302444885
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [51.088814]
objective value function right now is: -1550.4778720727882
new min fval from sgd:  -1559.6039436957587
new min fval from sgd:  -1559.9457314715314
new min fval from sgd:  -1559.966999237393
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.346893]
objective value function right now is: -1541.7223560553714
new min fval from sgd:  -1560.082316437603
new min fval from sgd:  -1560.4839914384736
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.556343]
objective value function right now is: -1555.7611495668111
new min fval from sgd:  -1560.5168617664146
new min fval from sgd:  -1560.5359613770659
new min fval from sgd:  -1560.9950569168514
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.22255]
objective value function right now is: -1555.8735033666296
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [47.638096]
objective value function right now is: -1539.2606202102556
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [44.850254]
objective value function right now is: -1556.383219423681
min fval:  -1560.9950569168514
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-127.4441,   -4.2746],
        [ -11.6120,   -1.9486],
        [ -10.7713,    4.6815],
        [ -10.6921,    4.6398],
        [ -10.6805,    4.6363],
        [ -10.5927,    4.6412],
        [ -10.5927,    4.6415],
        [ -10.6082,    4.6312],
        [ -10.5979,    4.6607],
        [ -10.6127,    4.6466]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[   8.1908,    9.7693,  -40.9964,  -46.7248,  -48.9893,  -86.1535,
          -86.5214,  -63.4423,  -98.5204,  -67.6941],
        [  45.6007,    7.8535, -320.6657, -318.6737, -337.0958, -386.2513,
         -386.3042, -320.3877, -394.4395, -312.0779],
        [  45.6707,    7.8960, -327.7174, -313.7669, -334.9302, -387.2288,
         -387.7606, -320.4387, -396.4389, -311.6504],
        [-855.1952,    8.3506,  -29.9534,  -21.1636,  -24.5046,  -35.2717,
          -35.3168,  -25.7627,  -49.8334,  -23.9043],
        [ -45.6481,   -7.9578,  341.2958,  309.1761,  328.8473,  386.5912,
          387.3442,  318.8183,  396.7082,  310.3452],
        [  -1.8982,    9.5020,  -37.5398,  -40.3981,  -43.3033,  -81.6936,
          -81.9054,  -57.8125,  -94.5205,  -62.4885],
        [  -7.5492,   -9.6146,   36.8507,   39.7434,   43.6452,   78.8613,
           78.9923,   59.3746,   91.2183,   63.0953],
        [  45.6188,    7.8761, -326.3258, -313.8669, -335.0010, -387.0760,
         -387.6420, -319.5594, -396.1177, -311.4529],
        [ -12.3638,   19.9833,  -37.2244,  -42.1076,  -45.0893,  -87.2793,
          -87.4879,  -62.9364, -101.8178,  -67.9074],
        [ -40.1947,   -6.6701,  223.1436,  194.4067,  214.3466,  281.4034,
          282.1136,  213.3455,  295.8362,  210.7142]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-50.7857, -50.3425, -59.1918,  12.8533,  49.4549, -41.2039, 160.5028,
         -60.5424, -47.3713,  62.0655]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.6423,   4.1932],
        [ -0.2709,   3.7366],
        [ -4.8561,   4.6917],
        [ -3.3770,   4.1832],
        [ -5.0534,   5.3938],
        [-42.7138,  -3.6580],
        [ 16.9667,   7.2764],
        [ -4.8711,   4.4178],
        [  2.8309,   5.8532],
        [-23.5453,  -4.8879]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 619.3093,  604.0071,  310.5077,  626.8311,   16.4926,  329.1080,
          360.5307,  576.6046,  564.9880,   -2.1722],
        [-167.1382,  -81.7150, -164.1196, -162.5143, -198.5696,    6.9957,
           -5.5785, -266.7141,  -34.2034,    8.9351],
        [ -55.2628,  -23.9034, -137.6702,  -55.2291,   -1.5624, -229.2652,
          -79.3534, -141.5204,  -44.3066,  -23.0590],
        [  -6.0115,  -22.1362,   49.3730,   -7.7821,  -20.6052,   39.2459,
          -97.8500,    3.5356,  -54.7930,  -68.4578],
        [-560.3471, -598.7714, -182.4387, -573.4938,  -78.6582,   26.4108,
           -5.9523, -465.3250, -569.7595,   27.8743],
        [ -57.8271,  -42.9436,  -51.0043,  -59.0809,  226.3828,  149.7224,
          -66.1776,  -92.6487,  -39.5615, -185.1940],
        [ -81.1398,  -66.2041,  -58.9512,  -79.4052,   -5.5038, -174.5229,
          -51.1303, -120.1112, -104.9377,  -49.9402],
        [-429.1378, -396.7692, -402.4267, -428.7993,   12.4693, -337.9308,
         -325.8456, -455.8180, -371.3420,   -3.0019],
        [ -44.7689,  -23.2940,   -2.1436,  -44.3825,    9.7627, -184.4932,
          -52.7568,  -37.8460,  -90.8916,  -55.9531],
        [-160.9489, -152.7540,   -2.7979, -163.5402,   -7.5197, -187.1782,
          -15.7965,   17.4495, -234.3679,   -9.1593]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.1281, -34.8056,  32.3403,   4.0021,  42.0678,  11.2483, -15.2461,
         -41.1156,  -8.9795,  -9.3430],
        [ -0.5412,  35.1709, -32.5172,  -3.6034, -42.2076, -11.4205,  15.2702,
          41.2969,   9.0589,   8.8127]], device='cuda:0'))])
xi:  [53.970985]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 614.085401761983
W_T_median: 264.5293833866615
W_T_pctile_5: 53.86535112363856
W_T_CVAR_5_pct: -49.08052967727928
Average q (qsum/M+1):  51.937964654737904
Optimal xi:  [53.970985]
Expected(across Rb) median(across samples) p_equity:  0.33864912390708923
obj fun:  tensor(-1560.9951, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1585.5733683012259
Current xi:  [-99.82525]
objective value function right now is: -1585.5733683012259
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [-109.43269]
objective value function right now is: -1582.440177233899
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-118.31078]
objective value function right now is: -1583.0860339766532
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1603.5641109807557
Current xi:  [-124.13337]
objective value function right now is: -1603.5641109807557
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-126.21273]
objective value function right now is: -1587.2436656379673
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-130.39641]
objective value function right now is: -1583.885421680435
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-134.99089]
objective value function right now is: -1572.7523475688627
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-138.00293]
objective value function right now is: -1599.876207200097
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-140.49338]
objective value function right now is: -1581.9260326045953
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.950822154618
Current xi:  [-142.29779]
objective value function right now is: -1604.950822154618
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-145.2804]
objective value function right now is: -1590.7000314208378
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-144.00748]
objective value function right now is: -1598.93908437248
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-143.54025]
objective value function right now is: -1599.749256503548
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1605.226069904453
Current xi:  [-143.09854]
objective value function right now is: -1605.226069904453
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-141.91353]
objective value function right now is: -1603.5190934658158
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-144.56425]
objective value function right now is: -1585.8374789155396
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-145.52202]
objective value function right now is: -1594.0713171667182
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-145.57646]
objective value function right now is: -1602.4944483282156
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-146.49168]
objective value function right now is: -1591.536069069677
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-149.4176]
objective value function right now is: -1600.4621425590678
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-149.42453]
objective value function right now is: -1592.553684502999
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-147.53018]
objective value function right now is: -1593.630673726774
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-146.57922]
objective value function right now is: -1589.9768921597522
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-150.72911]
objective value function right now is: -1579.0523074446885
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-153.96384]
objective value function right now is: -1584.3605537258572
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-157.37567]
objective value function right now is: -1599.8363473301176
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-154.91444]
objective value function right now is: -1595.6890351940128
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-152.03627]
objective value function right now is: -1601.2375975152295
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-153.44159]
objective value function right now is: -1590.766911183609
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-152.23964]
objective value function right now is: -1605.0717156214787
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.9713938244622
Current xi:  [-148.97455]
objective value function right now is: -1605.9713938244622
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-149.03209]
objective value function right now is: -1602.09278502869
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-147.79419]
objective value function right now is: -1605.6946949365106
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-146.98058]
objective value function right now is: -1602.3427933320636
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-149.71242]
objective value function right now is: -1587.771885418698
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-152.8075]
objective value function right now is: -1587.789200292817
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-154.11668]
objective value function right now is: -1585.9282671899825
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-157.971]
objective value function right now is: -1562.9862430243325
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-157.96977]
objective value function right now is: -1581.3212943988387
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-165.31291]
objective value function right now is: -1581.2373426621514
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-166.3724]
objective value function right now is: -1602.91232770035
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-165.88548]
objective value function right now is: -1588.4530044760152
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-164.36636]
objective value function right now is: -1592.1370352654876
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.8565]
objective value function right now is: -1588.867229454569
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-165.59035]
objective value function right now is: -1597.8825993075632
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-166.51576]
objective value function right now is: -1578.5190588912703
new min fval from sgd:  -1606.0754405223618
new min fval from sgd:  -1606.110916513467
new min fval from sgd:  -1606.8416101626092
new min fval from sgd:  -1606.9109580522677
new min fval from sgd:  -1606.9306520563377
new min fval from sgd:  -1606.9657502969512
new min fval from sgd:  -1607.1581633445614
new min fval from sgd:  -1607.3467337299144
new min fval from sgd:  -1607.5325452987668
new min fval from sgd:  -1607.5997481528455
new min fval from sgd:  -1607.8664204035806
new min fval from sgd:  -1607.97646830807
new min fval from sgd:  -1608.0284108502758
new min fval from sgd:  -1608.1987758196208
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-162.75365]
objective value function right now is: -1595.4974209490042
new min fval from sgd:  -1608.42641974663
new min fval from sgd:  -1608.6172969035
new min fval from sgd:  -1608.8890289288113
new min fval from sgd:  -1608.9998830457698
new min fval from sgd:  -1609.3627699963372
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-158.86266]
objective value function right now is: -1491.7601975809384
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.69823]
objective value function right now is: -1591.9493871065997
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-165.41853]
objective value function right now is: -1585.751792330851
min fval:  -1609.3627699963372
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-119.3466,   -2.7870],
        [  -9.6754,   -1.8739],
        [  -6.7939,    4.8296],
        [  -5.9329,    5.0536],
        [  -5.9226,    5.0505],
        [  -5.4530,    5.1278],
        [  -5.4517,    5.1280],
        [  -5.5312,    5.1215],
        [  -5.3457,    5.1545],
        [  -5.4372,    5.1429]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[    3.9159,     5.7198,   -31.4207,   -54.4912,   -57.0814,  -105.5169,
          -105.9093,   -80.8127,  -122.1293,   -87.5995],
        [   17.8097,     3.3616,  -343.7974,  -337.0167,  -355.4612,  -402.6838,
          -402.7277,  -337.3572,  -409.2539,  -328.3093],
        [   17.5362,     3.6505,  -350.6848,  -332.1594,  -353.3422,  -403.5292,
          -404.0529,  -337.3168,  -411.0598,  -327.7396],
        [-1006.0016,   -16.1474,  -245.9748,   -91.0488,   -93.4304,   -56.7758,
           -56.7375,   -54.6884,   -57.6442,   -43.5591],
        [  -17.3642,    -3.8330,   363.9653,   327.3962,   347.0745,   402.6635,
           403.4119,   335.4553,   411.1819,   326.2095],
        [   -3.6321,     5.4397,   -44.1714,   -56.8436,   -60.0106,  -102.4992,
          -102.7200,   -78.0301,  -117.1434,   -83.6240],
        [ -106.8959,    -6.7811,    -4.9520,    28.0462,    32.4441,    87.1948,
            87.3650,    64.5293,   105.6214,    72.2460],
        [   17.5658,     3.6190,  -349.2722,  -332.2440,  -353.3972,  -403.4186,
          -403.9787,  -336.4628,  -410.8068,  -327.5917],
        [  -78.0069,    77.3737,  -113.8809,   -79.3526,   -81.5541,   -75.6681,
           -75.7575,   -61.5559,   -71.8208,   -54.2432],
        [  -27.3240,    -3.4395,   232.3826,   210.7136,   230.5721,   298.9171,
           299.6264,   230.8269,   314.0804,   228.6766]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-48.5276, -62.9231, -71.8177,  17.6947,  51.6216, -37.5192, 147.6322,
         -73.1878, -53.3264,  62.4587]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  1.4638,   4.2325],
        [ -2.8208,   5.3510],
        [ -2.7352,   5.1024],
        [  1.4049,   4.3836],
        [ -3.1566,   4.4486],
        [-54.1532,  -1.5406],
        [ 19.0541,   7.6027],
        [  2.4669,   4.8964],
        [ -2.8682,   5.2271],
        [-20.4629,  -3.8133]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.0057e+03,  9.7657e+02,  5.9518e+02,  1.0149e+03,  1.7012e+02,
          3.8920e+02,  6.5820e+02,  8.6980e+02,  9.0101e+02, -4.6093e+00],
        [-1.1809e+02, -4.2653e+01, -5.6180e+01, -1.0702e+02, -9.8403e+01,
         -4.1572e+01, -7.4802e+00, -1.3105e+02, -5.9587e+00,  6.1306e+00],
        [-5.3550e+01, -2.1037e+01, -1.1155e+02, -5.0837e+01, -1.7541e+00,
         -2.0117e+02, -9.6766e+01, -1.2853e+02, -5.7792e+01, -3.6397e+01],
        [-7.4814e+01, -1.0007e+02, -1.5758e+01, -8.1462e+01, -4.8677e+01,
          3.7548e+01, -1.2924e+02, -4.0345e+01, -1.2190e+02, -1.1055e+02],
        [-6.3995e+02, -6.6552e+02, -3.0914e+02, -6.5514e+02, -2.7930e+02,
          5.3003e+01, -6.5062e+00, -6.4642e+02, -6.6200e+02,  3.8586e+01],
        [-1.0495e+02, -1.1020e+02, -9.7691e+01, -1.1142e+02,  2.0125e+02,
          1.4265e+02, -4.1796e+01, -1.3813e+02, -9.2017e+01, -2.3434e+02],
        [-9.1919e+01, -9.0293e+01, -1.2432e+02, -1.0045e+02, -3.7184e+01,
         -2.2157e+02, -4.8433e+01, -1.7361e+02, -1.1965e+02, -2.9790e+01],
        [-5.3970e+02, -5.1908e+02, -5.4377e+02, -5.4945e+02, -6.0691e+01,
         -3.6656e+02, -3.3177e+02, -5.6425e+02, -4.7415e+02, -8.3052e+00],
        [ 9.1538e-01, -2.3795e+01, -1.2170e+01,  8.0636e-01, -5.2413e+01,
         -2.1445e+02, -1.8551e+00,  3.8602e-01, -2.6853e+01, -3.9491e+00],
        [-2.8798e+02, -3.0550e+02, -1.6828e+02, -3.0010e+02, -1.0578e+02,
         -3.8028e+02, -3.5150e+01, -1.3186e+02, -3.6484e+02, -1.3734e+01]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.1768, -34.9055,  22.3488,  10.3641,  35.1576,  27.9312, -24.0487,
         -42.1081,  -1.3632, -11.1678],
        [ -0.5899,  35.2710, -22.5249,  -9.9644, -35.2970, -28.1042,  24.0740,
          42.2601,   1.4433,  10.6382]], device='cuda:0'))])
xi:  [-158.52019]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 364.49395547011574
W_T_median: 49.34532205378874
W_T_pctile_5: -158.1905301272887
W_T_CVAR_5_pct: -211.99598021607912
Average q (qsum/M+1):  55.33630764868952
Optimal xi:  [-158.52019]
Expected(across Rb) median(across samples) p_equity:  0.37708319574594495
obj fun:  tensor(-1609.3628, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1667.5962579808029
Current xi:  [-163.9334]
objective value function right now is: -1667.5962579808029
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1675.4104368877452
Current xi:  [-220.51558]
objective value function right now is: -1675.4104368877452
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1683.5065290593209
Current xi:  [-270.71704]
objective value function right now is: -1683.5065290593209
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1688.7685896890982
Current xi:  [-315.9585]
objective value function right now is: -1688.7685896890982
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1688.8886645474495
Current xi:  [-355.32806]
objective value function right now is: -1688.8886645474495
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1699.5386462975262
Current xi:  [-380.6978]
objective value function right now is: -1699.5386462975262
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-397.3038]
objective value function right now is: -1698.5806738760198
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1700.5319362799014
Current xi:  [-409.19275]
objective value function right now is: -1700.5319362799014
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-417.52386]
objective value function right now is: -1699.4536321354747
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-419.4214]
objective value function right now is: -1688.0688592247168
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-423.87668]
objective value function right now is: -1696.7244934657392
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-434.6697]
objective value function right now is: -1691.9093363681307
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-435.85736]
objective value function right now is: -1695.2443170350737
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-444.87338]
objective value function right now is: -1692.426312137491
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-453.55923]
objective value function right now is: -1692.7980089098326
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-453.95663]
objective value function right now is: -1692.6712561289207
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-458.91342]
objective value function right now is: -1688.9257919251716
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-462.08447]
objective value function right now is: -1693.7405178564215
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-461.67368]
objective value function right now is: -1692.5415353551502
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-463.75986]
objective value function right now is: -1692.6297655151734
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-467.65497]
objective value function right now is: -1693.8259075750918
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-469.81598]
objective value function right now is: -1692.1106021033988
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.02307]
objective value function right now is: -1684.2575050463997
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-496.47537]
objective value function right now is: -1667.210773896285
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-471.85294]
objective value function right now is: -1656.2683733968754
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-493.39606]
objective value function right now is: -1494.0715938535122
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-464.19028]
objective value function right now is: -1612.5986905045443
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-513.68066]
objective value function right now is: -1525.398408648957
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-546.9031]
objective value function right now is: -1243.2871247732728
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-581.8635]
objective value function right now is: -1502.50036110807
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-521.48663]
objective value function right now is: -1534.4009964477652
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-479.67157]
objective value function right now is: -1672.8965675682664
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-442.12964]
objective value function right now is: -1674.0032232287394
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-420.3693]
objective value function right now is: -1675.531300813719
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-404.62628]
objective value function right now is: -1675.614119636344
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-396.819]
objective value function right now is: -1675.4132765910986
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-392.5921]
objective value function right now is: -1675.601001299057
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-390.29236]
objective value function right now is: -1672.1305829819978
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-384.46848]
objective value function right now is: -1675.926586764578
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-383.42154]
objective value function right now is: -1673.3434816033423
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-381.39026]
objective value function right now is: -1675.913518215058
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-386.08115]
objective value function right now is: -1675.7028077137386
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-380.18814]
objective value function right now is: -1675.6461883998047
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-383.19693]
objective value function right now is: -1675.831171006592
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-388.43005]
objective value function right now is: -1673.9273856707555
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-385.75323]
objective value function right now is: -1675.4313945396286
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-386.8762]
objective value function right now is: -1674.7714599756202
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-386.66565]
objective value function right now is: -1672.8182963112286
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-387.17947]
objective value function right now is: -1674.9568104388757
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-385.4364]
objective value function right now is: -1675.6901784088575
min fval:  -1699.181334739384
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-110.4281,   -3.0490],
        [  -8.9796,   -2.6375],
        [  -4.4205,    4.9236],
        [  -3.8629,    4.5310],
        [  -3.8807,    4.5278],
        [  -3.4881,    4.3550],
        [  -3.4866,    4.3549],
        [  -3.5409,    4.3755],
        [  -3.3637,    4.3433],
        [  -3.4390,    4.3623]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 4.5044e+00,  4.7125e+00, -4.0665e+01, -6.3420e+01, -6.5969e+01,
         -1.1410e+02, -1.1450e+02, -8.9366e+01, -1.3082e+02, -9.6187e+01],
        [ 1.4892e+01,  4.2043e+00, -3.4287e+02, -3.3617e+02, -3.5463e+02,
         -4.0185e+02, -4.0189e+02, -3.3660e+02, -4.0836e+02, -3.2754e+02],
        [ 1.4625e+01,  4.6303e+00, -3.4956e+02, -3.3112e+02, -3.5231e+02,
         -4.0245e+02, -4.0297e+02, -3.3633e+02, -4.0989e+02, -3.2672e+02],
        [-1.0070e+03, -3.8743e+01, -2.6436e+02, -1.0544e+02, -1.0782e+02,
         -6.7731e+01, -6.7682e+01, -6.6179e+01, -6.7654e+01, -5.4267e+01],
        [-1.4379e+01, -4.9790e+00,  3.6276e+02,  3.2627e+02,  3.4596e+02,
          4.0147e+02,  4.0221e+02,  3.3436e+02,  4.0989e+02,  3.2507e+02],
        [-2.9932e+00,  5.7626e+00, -5.0752e+01, -6.3113e+01, -6.6267e+01,
         -1.0833e+02, -1.0856e+02, -8.3911e+01, -1.2301e+02, -8.9485e+01],
        [-2.0660e+02, -5.8882e+00,  1.2181e+01,  4.3376e+01,  4.7788e+01,
          9.9032e+01,  9.9190e+01,  7.6982e+01,  1.1624e+02,  8.3772e+01],
        [ 1.4654e+01,  4.5687e+00, -3.4820e+02, -3.3126e+02, -3.5242e+02,
         -4.0242e+02, -4.0298e+02, -3.3555e+02, -4.0973e+02, -3.2665e+02],
        [-7.7482e+01,  7.9974e+01, -1.1565e+02, -8.3929e+01, -8.6112e+01,
         -8.4658e+01, -8.4765e+01, -6.9706e+01, -8.2203e+01, -6.3554e+01],
        [-2.1330e+01,  2.0343e-01,  2.3835e+02,  2.1666e+02,  2.3650e+02,
          3.0360e+02,  3.0431e+02,  2.3596e+02,  3.1829e+02,  2.3346e+02]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-50.5021, -37.9605, -46.9769,  54.7478,  49.8779, -38.8583, 164.2292,
         -48.3454, -55.0920,  62.5895]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -5.8106,   3.1669],
        [ -2.8343,   4.1337],
        [ -3.2821,   3.8938],
        [ -6.0642,   3.0248],
        [ -4.1415,   3.7475],
        [-47.3501,  -2.9714],
        [ 12.3214,   4.5401],
        [ -4.5451,   3.8500],
        [ -2.3578,   4.2362],
        [-24.2209,  -4.7716]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.0451e+03,  1.0088e+03,  6.3066e+02,  1.0565e+03,  2.0715e+02,
          4.1700e+02,  6.8564e+02,  9.1435e+02,  9.3251e+02,  2.9505e-03],
        [-1.4133e+02, -3.4046e+01, -4.6949e+01, -1.2885e+02, -1.0916e+02,
          2.4878e+00, -5.4540e+00, -1.5925e+02, -8.1180e+00,  4.6172e+00],
        [-2.8864e+01,  1.7439e+01, -8.2789e+01, -2.7301e+01,  2.9688e+01,
         -1.7584e+02, -1.0200e+02, -1.0875e+02, -2.3409e+01, -3.0511e+01],
        [-8.3986e+01, -1.0896e+02, -2.6409e+01, -9.1045e+01, -5.5288e+01,
          3.8433e+01, -1.4054e+02, -5.3834e+01, -1.3085e+02, -1.0739e+02],
        [-6.4551e+02, -6.6695e+02, -3.2387e+02, -6.5835e+02, -2.9410e+02,
          9.8202e+01, -4.6935e+00, -6.4544e+02, -6.6068e+02,  4.6664e+01],
        [-2.0116e+02, -2.1432e+02, -1.9131e+02, -2.0762e+02,  1.1086e+02,
          1.4474e+02, -2.9419e+01, -2.3311e+02, -1.9629e+02, -2.5356e+02],
        [-1.0057e+02, -1.1261e+02, -1.3744e+02, -1.0715e+02, -5.1352e+01,
         -2.4985e+02, -4.9002e+01, -1.7636e+02, -1.3377e+02, -4.9350e+01],
        [-5.6684e+02, -5.5656e+02, -5.7737e+02, -5.7980e+02, -9.9674e+01,
         -4.0112e+02, -3.4010e+02, -5.8857e+02, -5.0986e+02, -2.7931e+00],
        [-5.2823e+01, -6.3792e+01, -4.8256e+01, -5.3163e+01, -1.0033e+02,
         -3.8079e+02, -2.1640e+00, -3.8482e+01, -6.9461e+01, -3.8694e+01],
        [-2.9506e+02, -3.2709e+02, -1.8180e+02, -3.0585e+02, -1.1933e+02,
         -4.2511e+02, -3.3893e+01, -1.3129e+02, -3.7996e+02, -1.8258e+01]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.2525, -36.2986,  22.0117,   9.1623,  35.9413,  25.7790, -25.7075,
         -39.7716,  -5.2961,  -9.0922],
        [ -0.6656,  36.6641, -22.1877,  -8.7625, -36.0808, -25.9518,  25.7329,
          39.9264,   5.3762,   8.5628]], device='cuda:0'))])
xi:  [-388.43005]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 170.26631821229734
W_T_median: -104.34484794895971
W_T_pctile_5: -399.99963988287834
W_T_CVAR_5_pct: -441.5707987813145
Average q (qsum/M+1):  57.727027154737904
Optimal xi:  [-388.43005]
Expected(across Rb) median(across samples) p_equity:  0.30067865600188576
obj fun:  tensor(-1699.1813, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1714.8588725431905
Current xi:  [-196.36386]
objective value function right now is: -1714.8588725431905
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1730.3749179851409
Current xi:  [-293.25143]
objective value function right now is: -1730.3749179851409
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1744.7823316313852
Current xi:  [-389.76318]
objective value function right now is: -1744.7823316313852
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1756.1859028744173
Current xi:  [-486.2477]
objective value function right now is: -1756.1859028744173
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1765.4515638468508
Current xi:  [-582.8174]
objective value function right now is: -1765.4515638468508
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1773.7565496647785
Current xi:  [-678.89746]
objective value function right now is: -1773.7565496647785
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1780.2921982465218
Current xi:  [-774.72565]
objective value function right now is: -1780.2921982465218
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1785.5761702826348
Current xi:  [-869.4216]
objective value function right now is: -1785.5761702826348
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1789.612233391092
Current xi:  [-961.8579]
objective value function right now is: -1789.612233391092
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1791.960939339893
Current xi:  [-1048.625]
objective value function right now is: -1791.960939339893
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1792.6577543207882
Current xi:  [-1115.627]
objective value function right now is: -1792.6577543207882
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1792.924452524168
Current xi:  [-1135.9353]
objective value function right now is: -1792.924452524168
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1141.7709]
objective value function right now is: -1792.6976183852992
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-1171.3347]
objective value function right now is: -1792.1034626984888
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1175.8936]
objective value function right now is: -1792.0616253533794
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1177.2513]
objective value function right now is: -1791.9955764961016
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1179.2946]
objective value function right now is: -1792.0789811336422
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1178.5648]
objective value function right now is: -1792.0801070390794
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1179.7452]
objective value function right now is: -1792.0812285435593
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1176.4567]
objective value function right now is: -1792.1241506524939
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1179.149]
objective value function right now is: -1792.052905537054
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1170.2985]
objective value function right now is: -1792.3383673524559
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1177.125]
objective value function right now is: -1791.946562589664
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1179.3207]
objective value function right now is: -1791.8809593860942
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1177.7689]
objective value function right now is: -1791.980962650673
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1175.7185]
objective value function right now is: -1792.084235102445
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1178.2839]
objective value function right now is: -1792.017896756448
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-1179.5372]
objective value function right now is: -1792.0658509783889
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-1177.4359]
objective value function right now is: -1792.0927461416481
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1171.5105]
objective value function right now is: -1791.9547233000594
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1174.7516]
objective value function right now is: -1792.0791548383686
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1177.6279]
objective value function right now is: -1792.0719653397427
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1176.2234]
objective value function right now is: -1791.957336030801
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1178.09]
objective value function right now is: -1792.041638249326
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1178.709]
objective value function right now is: -1792.0802841297368
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1176.129]
objective value function right now is: -1792.0457347267331
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1179.1285]
objective value function right now is: -1792.0639973601014
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1177.6277]
objective value function right now is: -1792.0064918418932
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1177.1338]
objective value function right now is: -1792.0807211452902
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1178.8389]
objective value function right now is: -1792.0715042015452
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1179.4996]
objective value function right now is: -1792.026495671578
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1177.8392]
objective value function right now is: -1792.0031421708866
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1178.4397]
objective value function right now is: -1792.0664801609723
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1179.207]
objective value function right now is: -1792.0647985326686
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1178.7737]
objective value function right now is: -1792.0587338292619
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1176.9453]
objective value function right now is: -1792.0782031284543
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1178.2091]
objective value function right now is: -1792.0115297290324
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1177.5099]
objective value function right now is: -1792.0807108571353
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1178.0258]
objective value function right now is: -1792.0367360446248
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1178.2853]
objective value function right now is: -1792.0805462797098
min fval:  -1792.5738595317016
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-55.6266, -38.3786],
        [-24.2346,  -9.9428],
        [  0.4666,   4.0903],
        [  0.4780,   4.0967],
        [  0.1076,   3.8781],
        [ -0.1947,   3.7153],
        [ -0.1945,   3.7155],
        [  0.8702,   4.3259],
        [ -0.1934,   3.7147],
        [  1.0977,   4.4624]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-7.4229e+00,  1.7135e+01, -6.1648e+01, -7.9487e+01, -7.6177e+01,
         -1.2278e+02, -1.2363e+02, -1.0527e+02, -1.3879e+02, -1.1256e+02],
        [-2.1203e+01,  9.7060e+00, -3.4484e+02, -3.3833e+02, -3.5662e+02,
         -4.0348e+02, -4.0352e+02, -3.3833e+02, -4.1005e+02, -3.2926e+02],
        [-2.1515e+01,  9.8041e+00, -3.5016e+02, -3.3228e+02, -3.5332e+02,
         -4.0336e+02, -4.0387e+02, -3.3730e+02, -4.1093e+02, -3.2774e+02],
        [-1.0115e+03, -2.4584e+01, -2.4736e+02, -8.8456e+01, -9.0913e+01,
         -5.0715e+01, -5.0666e+01, -4.9058e+01, -5.0514e+01, -3.7061e+01],
        [ 5.8541e+00, -1.0004e+01,  3.7569e+02,  3.4644e+02,  3.6708e+02,
          4.2433e+02,  4.2501e+02,  3.5586e+02,  4.3317e+02,  3.4664e+02],
        [ 1.5846e+01,  3.2350e+00, -8.3224e+01, -9.0549e+01, -9.3637e+01,
         -1.3251e+02, -1.3272e+02, -1.0880e+02, -1.4668e+02, -1.1389e+02],
        [-1.8829e+02, -2.7390e+01,  3.6372e-01,  2.9069e+01,  3.3322e+01,
          8.3253e+01,  8.3413e+01,  6.1365e+01,  1.0019e+02,  6.7951e+01],
        [-2.1550e+01,  9.8033e+00, -3.4867e+02, -3.3232e+02, -3.5333e+02,
         -4.0324e+02, -4.0379e+02, -3.3643e+02, -4.1068e+02, -3.2759e+02],
        [-9.2537e+01,  6.6883e+01, -1.2609e+02, -9.0088e+01, -9.1472e+01,
         -8.9500e+01, -8.9607e+01, -7.4648e+01, -8.7652e+01, -6.8861e+01],
        [-1.1821e+01,  7.3779e+00,  2.8101e+02,  2.6933e+02,  2.8893e+02,
          3.6885e+02,  3.6990e+02,  2.9732e+02,  3.8542e+02,  2.9721e+02]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-25.8160, -44.3918, -54.4822,  82.3449,  52.3077, -24.7793, 159.7167,
         -55.8734, -51.7684,  72.5294]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  0.7924,   2.8698],
        [  1.0737,   1.2857],
        [  0.2572,   2.5383],
        [  0.7758,   2.8232],
        [  0.2205,   0.8883],
        [-44.6293,  -8.6039],
        [ 11.6276,   2.4308],
        [  2.4558,   1.3494],
        [  1.5538,  -0.3226],
        [-25.1439,  -8.0562]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[1082.1948, 1042.8564,  668.8418, 1093.3458,  247.4759,  441.9165,
          710.4810,  949.8062,  968.0580,    6.7318],
        [-168.4731,  -90.5555,  -64.3031, -158.9295, -115.8820,  -55.3983,
          -12.3000, -190.1684,  -70.8506,   20.6377],
        [ -20.5069,   20.7040,  -83.1888,  -19.5086,   34.6606, -139.4974,
         -107.5866, -100.9101,  -21.6797,  -33.4404],
        [-102.5223, -130.7122,  -48.6830, -110.5544,  -70.5008,   35.0051,
         -158.2989,  -69.8430, -151.9008, -121.9222],
        [-654.6216, -662.7900, -342.8839, -665.1368, -330.0060,  126.6527,
           -8.9529, -654.5291, -656.5305,   21.3199],
        [-295.5348, -301.6072, -286.8349, -302.2567,   32.7688,  108.7081,
          -27.8051, -322.7883, -278.5217, -293.9798],
        [ -91.5725,  -93.3294, -118.3472,  -97.2961,  -48.3109, -321.6693,
          -53.5494, -167.1772, -113.0582,  -46.4591],
        [-608.1016, -596.5869, -615.1636, -621.4156, -139.3204, -424.0744,
         -338.1449, -628.7457, -548.2408,   -5.2096],
        [-112.7242, -108.7060,  -92.1221, -113.1950, -166.4568, -664.5838,
           -5.7369,  -96.0009, -114.5578,    4.7119],
        [-288.1528, -309.2503, -160.2355, -298.3835, -109.1017, -563.8043,
          -17.9902, -127.7102, -362.0739,  -11.2911]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.6455, -29.1950,  18.0555,   5.4174,  33.1388,  30.4812, -23.9243,
         -36.6172,  -5.2457, -10.9313],
        [ -1.0584,  29.5606, -18.2312,  -5.0174, -33.2781, -30.6540,  23.9499,
          36.7718,   5.3258,  10.4019]], device='cuda:0'))])
xi:  [-1178.7737]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: -462.64685123834306
W_T_median: -550.9143929507235
W_T_pctile_5: -1130.4613726325508
W_T_CVAR_5_pct: -1260.3122695647812
Average q (qsum/M+1):  59.86918000252016
Optimal xi:  [-1178.7737]
Expected(across Rb) median(across samples) p_equity:  0.15395890176296234
obj fun:  tensor(-1792.5739, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
