Starting at: 
29-01-23_23:01

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 40000
batchsize: 1000
remove neg:  False


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 4000, 'itbound_SGD_algorithms': 40000, 'nit_IterateAveragingStart': 36000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.02, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1698.852805716046
Current xi:  [-13.867779]
objective value function right now is: -1698.852805716046
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1705.3498438138395
Current xi:  [-28.27779]
objective value function right now is: -1705.3498438138395
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1708.88590740756
Current xi:  [-41.847794]
objective value function right now is: -1708.88590740756
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1712.1391441864603
Current xi:  [-56.27821]
objective value function right now is: -1712.1391441864603
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1715.2792098058937
Current xi:  [-70.91539]
objective value function right now is: -1715.2792098058937
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1718.4857263349572
Current xi:  [-84.42831]
objective value function right now is: -1718.4857263349572
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1720.6999887943782
Current xi:  [-99.41681]
objective value function right now is: -1720.6999887943782
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1722.517648538739
Current xi:  [-113.39131]
objective value function right now is: -1722.517648538739
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1724.4543272570097
Current xi:  [-127.94907]
objective value function right now is: -1724.4543272570097
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.2386196273608
Current xi:  [-142.89363]
objective value function right now is: -1725.2386196273608
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1726.7661607562593
Current xi:  [-157.10966]
objective value function right now is: -1726.7661607562593
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1728.8525529734884
Current xi:  [-171.92525]
objective value function right now is: -1728.8525529734884
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1730.0502888141723
Current xi:  [-186.32175]
objective value function right now is: -1730.0502888141723
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1731.0494725800743
Current xi:  [-200.9686]
objective value function right now is: -1731.0494725800743
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1731.909049036403
Current xi:  [-215.4362]
objective value function right now is: -1731.909049036403
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1733.2239354637209
Current xi:  [-229.60016]
objective value function right now is: -1733.2239354637209
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1734.4511740401354
Current xi:  [-244.20709]
objective value function right now is: -1734.4511740401354
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-258.38812]
objective value function right now is: -1732.533244675751
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1736.17251212662
Current xi:  [-272.42264]
objective value function right now is: -1736.17251212662
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1736.7577495821756
Current xi:  [-286.68417]
objective value function right now is: -1736.7577495821756
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-300.33487]
objective value function right now is: -1736.1614575173166
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1737.750723677075
Current xi:  [-314.0259]
objective value function right now is: -1737.750723677075
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1738.2421685989152
Current xi:  [-328.05795]
objective value function right now is: -1738.2421685989152
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1739.1619324659082
Current xi:  [-341.2649]
objective value function right now is: -1739.1619324659082
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1739.574587266443
Current xi:  [-354.23294]
objective value function right now is: -1739.574587266443
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-367.85907]
objective value function right now is: -1739.2978716293662
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-380.50696]
objective value function right now is: -1739.4434144039628
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1740.4635370073365
Current xi:  [-392.97205]
objective value function right now is: -1740.4635370073365
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-404.72583]
objective value function right now is: -1740.4633976393504
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1740.6152308852054
Current xi:  [-417.02454]
objective value function right now is: -1740.6152308852054
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.089439979385
Current xi:  [-427.61224]
objective value function right now is: -1741.089439979385
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-437.4289]
objective value function right now is: -1740.7037677563942
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.2284656630943
Current xi:  [-447.758]
objective value function right now is: -1741.2284656630943
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-456.9]
objective value function right now is: -1740.7146051655025
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.4195963029135
Current xi:  [-463.78845]
objective value function right now is: -1741.4195963029135
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.4917591163708
Current xi:  [-465.1255]
objective value function right now is: -1741.4917591163708
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.6209055358004
Current xi:  [-466.45618]
objective value function right now is: -1741.6209055358004
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-468.12607]
objective value function right now is: -1741.5270533061116
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-469.4726]
objective value function right now is: -1741.5904464877199
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.6411695183153
Current xi:  [-470.7519]
objective value function right now is: -1741.6411695183153
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-472.083]
objective value function right now is: -1741.6089426554483
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.6508678384446
Current xi:  [-473.20053]
objective value function right now is: -1741.6508678384446
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.53494]
objective value function right now is: -1741.630965114573
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-475.4758]
objective value function right now is: -1741.6260245725941
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-475.9457]
objective value function right now is: -1740.9713653894878
new min fval from sgd:  -1741.6605878126813
new min fval from sgd:  -1741.6660650435178
new min fval from sgd:  -1741.6679430567167
new min fval from sgd:  -1741.679283865516
new min fval from sgd:  -1741.6802802479299
new min fval from sgd:  -1741.680713492175
new min fval from sgd:  -1741.6821371803064
new min fval from sgd:  -1741.6900955593849
new min fval from sgd:  -1741.6927602498424
new min fval from sgd:  -1741.694416002406
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-477.09583]
objective value function right now is: -1741.6337957366366
new min fval from sgd:  -1741.6989504781602
new min fval from sgd:  -1741.7026371166257
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-477.91556]
objective value function right now is: -1741.5308719572752
new min fval from sgd:  -1741.7041320841777
new min fval from sgd:  -1741.7044692066586
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-478.7084]
objective value function right now is: -1741.6060539397636
new min fval from sgd:  -1741.7045881048698
new min fval from sgd:  -1741.705309413334
new min fval from sgd:  -1741.7055239994588
new min fval from sgd:  -1741.7056053149722
new min fval from sgd:  -1741.7058376979735
new min fval from sgd:  -1741.7065078273179
new min fval from sgd:  -1741.7069804327382
new min fval from sgd:  -1741.707365394046
new min fval from sgd:  -1741.707518803867
new min fval from sgd:  -1741.707813685479
new min fval from sgd:  -1741.7081730869488
new min fval from sgd:  -1741.7086490589106
new min fval from sgd:  -1741.708943783065
new min fval from sgd:  -1741.7091783839953
new min fval from sgd:  -1741.709294976057
new min fval from sgd:  -1741.7094587810598
new min fval from sgd:  -1741.709460097297
new min fval from sgd:  -1741.709621114942
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-478.94638]
objective value function right now is: -1741.6979964147883
new min fval from sgd:  -1741.7098498837352
new min fval from sgd:  -1741.7103738789372
new min fval from sgd:  -1741.7106793531325
new min fval from sgd:  -1741.7107515595123
new min fval from sgd:  -1741.7108351329075
new min fval from sgd:  -1741.711063744848
new min fval from sgd:  -1741.7112433514485
new min fval from sgd:  -1741.7112987113633
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-479.00134]
objective value function right now is: -1741.6949497557557
min fval:  -1741.7112987113633
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 1.6943, -0.1310],
        [-4.4451,  5.8265],
        [ 0.0695,  1.0425],
        [ 0.0694,  1.0424],
        [ 0.4230,  1.3274],
        [-5.5552,  5.1397],
        [ 0.0694,  1.0424],
        [ 7.6219,  2.2305],
        [10.3979,  1.9653],
        [-3.6969,  5.7143]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 2.4677,  9.8550, -0.2975, -0.2975,  0.1869,  9.4937, -0.2975, -0.5374,
        -4.7758,  8.9391], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.3303, -0.0133, -0.0162, -0.0162, -0.0230, -0.0141, -0.0162, -0.2605,
         -0.1363, -0.0087],
        [-1.9841, -4.9552,  0.6208,  0.6216,  0.8661, -5.1293,  0.6217, -0.5649,
         -5.7548, -3.4962],
        [-0.3861,  0.0525, -0.0808, -0.0808, -0.1079,  0.0652, -0.0808, -0.4964,
          0.1663,  0.0322],
        [ 0.7530,  2.0048, -0.3591, -0.3590, -0.4165,  2.1048, -0.3590, -0.5566,
          3.4292,  1.3500],
        [-0.3303, -0.0133, -0.0162, -0.0162, -0.0230, -0.0141, -0.0162, -0.2605,
         -0.1363, -0.0087],
        [ 1.6381,  4.2451, -0.5379, -0.5373, -0.6477,  4.4401, -0.5373,  0.1834,
          5.1931,  2.9400],
        [-1.7229, -4.7512,  0.6092,  0.6095,  0.8229, -4.9584,  0.6095, -0.4735,
         -5.6117, -3.3436],
        [ 0.4133,  0.4047, -0.4354, -0.4355, -0.5292,  0.4225, -0.4355, -0.4543,
          3.1741,  0.2149],
        [-1.2772, -3.4857,  0.5261,  0.5258,  0.5991, -3.6420,  0.5258, -0.1654,
         -4.5017, -2.3764],
        [ 1.5748,  4.5359, -0.5593, -0.5587, -0.6982,  4.7358, -0.5587,  0.2270,
          5.4987,  3.1856]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.3372,  5.8920, -0.9135, -2.3587, -0.3372, -4.9150,  5.4560, -2.2340,
         3.9477, -5.0880], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 8.3458e-04, -7.8138e+00,  3.3734e-01,  3.0829e+00,  8.3458e-04,
          6.6912e+00, -7.4539e+00,  2.1805e+00, -4.8950e+00,  7.3435e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-10.4414,   1.3927],
        [ -1.8060,  -0.5810],
        [-11.3008,  -7.2811],
        [  4.8435,  -1.7689],
        [  5.7000,  -0.2788],
        [  5.4166,  -1.1892],
        [ -7.9621,  -2.7736],
        [ -3.3114,  -5.5044],
        [ -7.4157,  -6.2002],
        [ -0.9481,   6.1967]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 2.1740, -4.3910, -5.7944, -5.3477, -5.6451, -7.3929,  0.3959, -4.5215,
        -7.0620,  5.4132], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 4.0105e+00,  2.4480e+00, -5.1741e+00,  1.2047e+00,  5.3377e+00,
          7.7698e+00, -5.3622e+00, -6.1323e+00,  3.4817e+00,  1.3629e+01],
        [ 3.6794e+00,  2.4321e+00, -1.3074e+00,  3.8974e-02,  1.9326e+00,
          4.0458e+00, -2.0727e+00, -1.0296e+00,  1.0629e+00,  3.4493e+00],
        [-6.0808e-03, -3.1791e-02, -4.2702e-02, -7.7812e-01, -5.3114e-01,
         -2.0895e-01, -2.5964e-01, -3.3716e-01, -3.5119e-02, -1.4108e+00],
        [ 1.6984e-01,  2.7118e+00, -1.8502e-01, -4.5501e-01,  1.8364e+00,
          3.2134e+00, -1.4303e+00, -1.4139e+00,  1.0447e+00,  2.0292e+00],
        [-5.2140e+00, -8.1630e-01,  9.1575e+00, -1.1017e+01, -5.2402e+00,
         -4.7379e+00,  3.5078e+00,  2.6263e-01,  6.4553e+00, -8.7235e+00],
        [ 2.9577e+00,  2.4998e+00, -3.6202e+00,  1.4444e+00,  4.5593e+00,
          6.7244e+00, -4.2291e+00, -5.4043e+00,  1.8244e+00,  1.1349e+01],
        [ 6.1477e+00, -3.1375e-01,  4.9027e-01, -1.9851e+00, -9.7765e-01,
         -6.6806e-01,  2.0339e+00, -9.4935e-01, -4.7928e-01, -1.0688e+00],
        [ 9.9075e-02,  9.7694e-02,  7.0804e-02,  1.4157e+00,  7.4757e-01,
          4.5543e-01,  5.9174e-01,  9.6238e-01,  7.7147e-02,  1.5166e+00],
        [-3.1820e+00,  2.7013e-01,  2.3762e+00,  3.0353e+00,  2.6043e+00,
          4.3480e+00, -2.4642e+00,  2.8754e+00,  2.0806e+00, -1.2946e+00],
        [ 1.2590e-01,  5.5738e-02,  4.9805e-02,  1.8238e+00,  9.9302e-01,
          4.1528e-01,  6.7908e-01,  1.1573e+00,  4.8228e-02,  1.7152e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 1.7508, -0.4864, -2.5210, -1.5886, -6.1271,  1.5343, -3.9117,  4.2350,
         1.2592,  3.9236], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 2.7904e+00,  1.0086e+00, -4.2935e-02,  1.8149e-01,  1.1670e+01,
          1.9727e+00,  1.7060e+00, -1.9064e+00, -1.6247e+00, -2.0780e+00],
        [-2.5625e+00, -1.0373e+00, -1.1607e-02, -2.2831e-01, -1.1670e+01,
         -1.9334e+00, -1.7377e+00,  1.9277e+00,  1.8454e+00,  2.2399e+00]],
       device='cuda:0'))])
xi:  [-478.9525]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 224.28287885815942
W_T_median: 104.64981723295594
W_T_pctile_5: -480.55720831443637
W_T_CVAR_5_pct: -590.9204426365114
Average q (qsum/M+1):  57.137348790322584
Optimal xi:  [-478.9525]
Expected(across Rb) median(across samples) p_equity:  0.2730988861915345
obj fun:  tensor(-1741.7113, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1650.4613237812957
Current xi:  [-10.3220215]
objective value function right now is: -1650.4613237812957
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1653.6729492916359
Current xi:  [-24.90774]
objective value function right now is: -1653.6729492916359
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1656.893921777845
Current xi:  [-36.972748]
objective value function right now is: -1656.893921777845
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1657.618427788527
Current xi:  [-46.12964]
objective value function right now is: -1657.618427788527
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1660.611888003794
Current xi:  [-60.427933]
objective value function right now is: -1660.611888003794
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1660.8604983694174
Current xi:  [-74.45189]
objective value function right now is: -1660.8604983694174
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1663.4799376202848
Current xi:  [-83.79237]
objective value function right now is: -1663.4799376202848
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1664.3550632679394
Current xi:  [-97.12617]
objective value function right now is: -1664.3550632679394
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1666.968461393435
Current xi:  [-111.78875]
objective value function right now is: -1666.968461393435
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1667.455666420292
Current xi:  [-121.232895]
objective value function right now is: -1667.455666420292
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1668.6684411684653
Current xi:  [-132.28557]
objective value function right now is: -1668.6684411684653
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1668.7666024253429
Current xi:  [-145.92834]
objective value function right now is: -1668.7666024253429
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1669.9694200893093
Current xi:  [-157.47827]
objective value function right now is: -1669.9694200893093
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1670.3836457324187
Current xi:  [-165.49927]
objective value function right now is: -1670.3836457324187
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1671.525156506551
Current xi:  [-175.51956]
objective value function right now is: -1671.525156506551
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-186.78003]
objective value function right now is: -1670.230873051903
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.5922876109594
Current xi:  [-197.74196]
objective value function right now is: -1672.5922876109594
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.767694450194
Current xi:  [-204.468]
objective value function right now is: -1672.767694450194
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-209.62863]
objective value function right now is: -1671.0624232000675
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.9532640568045
Current xi:  [-213.68481]
objective value function right now is: -1672.9532640568045
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-218.84435]
objective value function right now is: -1672.2941791987369
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-224.80046]
objective value function right now is: -1672.7926867385136
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.0788590957568
Current xi:  [-231.91408]
objective value function right now is: -1673.0788590957568
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.524867785125
Current xi:  [-238.03368]
objective value function right now is: -1673.524867785125
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-242.10019]
objective value function right now is: -1672.615037073162
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-243.60835]
objective value function right now is: -1672.845689469252
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.11284]
objective value function right now is: -1673.3864831532146
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-244.63107]
objective value function right now is: -1673.4859585444206
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1673.5503034213173
Current xi:  [-244.8851]
objective value function right now is: -1673.5503034213173
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.39317]
objective value function right now is: -1673.5393133761459
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.93895]
objective value function right now is: -1671.62794024362
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.89322]
objective value function right now is: -1673.5311765255528
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.83621]
objective value function right now is: -1673.2737099720155
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.84836]
objective value function right now is: -1673.2931370121012
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.3659]
objective value function right now is: -1673.0289692065262
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.6618493651845
Current xi:  [-244.4328]
objective value function right now is: -1673.6618493651845
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.40445]
objective value function right now is: -1673.3459023063879
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.687497209982
Current xi:  [-244.40805]
objective value function right now is: -1673.687497209982
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.38599]
objective value function right now is: -1673.555272777144
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.7262123697126
Current xi:  [-244.44775]
objective value function right now is: -1673.7262123697126
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.56862]
objective value function right now is: -1673.6020015724416
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.61809]
objective value function right now is: -1673.716862109547
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.6659]
objective value function right now is: -1673.6319434961456
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.51059]
objective value function right now is: -1673.6578151667645
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.4605]
objective value function right now is: -1673.6716953732305
new min fval from sgd:  -1673.727833428945
new min fval from sgd:  -1673.7285039411313
new min fval from sgd:  -1673.7310695667366
new min fval from sgd:  -1673.7328708349407
new min fval from sgd:  -1673.7345592656982
new min fval from sgd:  -1673.7375157262893
new min fval from sgd:  -1673.7384225864523
new min fval from sgd:  -1673.7391540460503
new min fval from sgd:  -1673.7519968807878
new min fval from sgd:  -1673.7533138368199
new min fval from sgd:  -1673.7553504382722
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.49118]
objective value function right now is: -1673.6837880566623
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.46487]
objective value function right now is: -1673.701115453242
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.42128]
objective value function right now is: -1673.714197327183
new min fval from sgd:  -1673.7580956306506
new min fval from sgd:  -1673.7594896079497
new min fval from sgd:  -1673.7600814323653
new min fval from sgd:  -1673.760821701881
new min fval from sgd:  -1673.761142325236
new min fval from sgd:  -1673.7612989167624
new min fval from sgd:  -1673.76157370618
new min fval from sgd:  -1673.7621673970903
new min fval from sgd:  -1673.7622588040033
new min fval from sgd:  -1673.76229425703
new min fval from sgd:  -1673.762623932231
new min fval from sgd:  -1673.7628310154917
new min fval from sgd:  -1673.762908835754
new min fval from sgd:  -1673.7629746005423
new min fval from sgd:  -1673.763294796085
new min fval from sgd:  -1673.7648461655676
new min fval from sgd:  -1673.7657295883134
new min fval from sgd:  -1673.766318971878
new min fval from sgd:  -1673.7667384696206
new min fval from sgd:  -1673.7672672223175
new min fval from sgd:  -1673.7672872233866
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.28215]
objective value function right now is: -1673.6602542273677
new min fval from sgd:  -1673.7674909532232
new min fval from sgd:  -1673.7684899241342
new min fval from sgd:  -1673.7687583429185
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.30858]
objective value function right now is: -1673.7465840529496
min fval:  -1673.7687583429185
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 5.8858,  4.0423],
        [-4.0673,  8.0763],
        [-2.8462, -0.9545],
        [-2.8467, -0.9558],
        [-3.3467, -1.2364],
        [-4.6988,  7.3571],
        [-2.8467, -0.9558],
        [11.9797,  3.5688],
        [ 9.4325,  1.2375],
        [-3.2699,  7.9832]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-1.0059,  9.4312, -0.4097, -0.4082,  1.3256,  9.0854, -0.4081,  0.8825,
        -0.9467,  8.6326], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.0255, -0.0234, -0.0532, -0.0533, -0.1764, -0.0287, -0.0533, -0.2613,
         -0.2591, -0.0156],
        [-4.8444, -6.3749,  0.9421,  0.9435,  1.4729, -5.6440,  0.9436, -3.0083,
         -2.1193, -5.2022],
        [-0.0255, -0.0234, -0.0532, -0.0533, -0.1764, -0.0287, -0.0533, -0.2613,
         -0.2591, -0.0156],
        [-0.0255, -0.0234, -0.0532, -0.0533, -0.1764, -0.0287, -0.0533, -0.2613,
         -0.2591, -0.0156],
        [-0.0255, -0.0234, -0.0532, -0.0533, -0.1764, -0.0287, -0.0533, -0.2613,
         -0.2591, -0.0156],
        [ 4.2112,  5.6306, -0.7393, -0.7397, -1.2755,  5.0920, -0.7397,  2.1669,
          1.3170,  4.3642],
        [-4.6263, -6.0796,  0.9503,  0.9514,  1.4372, -5.4318,  0.9514, -2.8111,
         -1.9611, -4.8896],
        [-0.0255, -0.0234, -0.0532, -0.0533, -0.1764, -0.0287, -0.0533, -0.2613,
         -0.2591, -0.0156],
        [-0.0255, -0.0234, -0.0532, -0.0533, -0.1764, -0.0287, -0.0533, -0.2613,
         -0.2591, -0.0156],
        [ 3.5942,  5.9514, -0.7236, -0.7240, -1.3085,  5.3052, -0.7240,  2.2504,
          1.2778,  4.7699]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.3131,  5.8344, -0.3131, -0.3131, -0.3131, -4.7818,  5.3867, -0.3131,
        -0.3131, -5.0950], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.3455e-03, -8.8571e+00,  1.3451e-03,  1.3451e-03,  1.3455e-03,
          7.5108e+00, -8.2538e+00,  1.3451e-03,  1.3451e-03,  8.0152e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-16.2782,   2.2736],
        [ -1.5525,  -0.6007],
        [-15.3160,  -7.7092],
        [  2.8627,  -2.9704],
        [  6.8744,  -0.4305],
        [  7.4246,  -0.8718],
        [-11.6833,  -2.0507],
        [ -6.3850,  -8.2481],
        [-11.5940, -10.1886],
        [ -0.2993,   6.1236]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 3.5833, -3.5600, -4.5222, -5.0831, -7.1232, -7.8968,  1.4277, -8.2770,
        -8.7100,  6.8979], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[  1.7213,   0.7153,  -7.2284,   1.3998,   8.1615,   9.6834,  -1.6795,
          -5.2961,  -1.7527,  14.0383],
        [ -6.3008,  -0.3542,  -2.2102,   0.3339,   3.5301,   3.3634,  -8.4170,
           5.6214,   8.1176,   1.0308],
        [ -0.1064,  -0.2011,  -0.4352,  -1.1117,  -0.8309,  -0.8356,  -0.0340,
          -0.5043,  -0.4375,  -1.9216],
        [  9.2259,   0.7784,   1.6645,  -1.0999,   1.5707,   3.6485,  -1.8177,
          -2.1669,   0.7919,   1.3179],
        [ -5.1271,  -1.6402,  12.2555,  -9.7544,  -5.4914,  -5.3296,   4.8393,
           2.1949,   7.6632,  -8.5223],
        [  5.3304,   4.1374,  -6.2058,   3.2618,   5.4903,   6.4350,  -5.2337,
          -4.3361,  -4.1676,   2.8141],
        [ -0.1061,  -0.1919,  -0.3836,  -1.1782,  -0.7758,  -0.7914,  -0.0263,
          -0.4780,  -0.4236,  -1.8278],
        [ -0.1477,   0.0491,  -0.1063,   2.1160,   0.5554,   0.5661,   0.3068,
           0.3303,   0.0339,   1.9338],
        [-18.2466,   0.5916,  11.7589,   2.3747,   2.2370,   3.6175,  -6.5702,
           7.8387,  13.9525,  -1.7152],
        [ -0.1491,   0.0499,  -0.1090,   2.1561,   0.5651,   0.5767,   0.3094,
           0.3369,   0.0311,   1.9786]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  3.3259,   0.2248,  -2.9375,  -4.8664, -11.3244,   6.7773,  -2.9520,
          4.4293,   2.0220,   4.4901], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  2.9164,   2.0938,  -0.5390,   0.8448,  11.7468,   2.1182,  -0.4408,
          -1.9415,  -2.8691,  -2.1130],
        [ -2.6897,  -2.1223,   0.5292,  -0.8899, -11.7477,  -2.0790,   0.4368,
           1.9626,   3.0884,   2.2740]], device='cuda:0'))])
xi:  [-244.30014]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 242.25295985831596
W_T_median: 89.0533579502173
W_T_pctile_5: -244.5170990491851
W_T_CVAR_5_pct: -334.81939425546204
Average q (qsum/M+1):  56.15266270791331
Optimal xi:  [-244.30014]
Expected(across Rb) median(across samples) p_equity:  0.30586431132008635
obj fun:  tensor(-1673.7688, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1598.9693120320676
Current xi:  [-3.3002446]
objective value function right now is: -1598.9693120320676
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.2297]
objective value function right now is: -1598.889893493536
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1600.4098184283869
Current xi:  [-3.9860985]
objective value function right now is: -1600.4098184283869
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.028998352018
Current xi:  [-3.7193112]
objective value function right now is: -1602.028998352018
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.5102217]
objective value function right now is: -1598.6199359362004
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.2045332]
objective value function right now is: -1593.237041135754
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1602.4158295716745
Current xi:  [-0.03764327]
objective value function right now is: -1602.4158295716745
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.18654442]
objective value function right now is: -1593.1008419223829
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.5931023360117
Current xi:  [-0.08984636]
objective value function right now is: -1602.5931023360117
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.09348765]
objective value function right now is: -1600.4239323120196
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.111047228294
Current xi:  [0.00684511]
objective value function right now is: -1604.111047228294
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.199891287665
Current xi:  [0.00588781]
objective value function right now is: -1605.199891287665
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03026573]
objective value function right now is: -1604.6213591941219
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00745351]
objective value function right now is: -1604.4409806458177
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00543134]
objective value function right now is: -1604.7692749363273
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00134848]
objective value function right now is: -1605.123665141216
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01504447]
objective value function right now is: -1605.0969706824865
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.6554424791852
Current xi:  [-0.01858569]
objective value function right now is: -1605.6554424791852
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00917695]
objective value function right now is: -1604.6188746068622
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.20396146]
objective value function right now is: -1601.5283555543829
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02215387]
objective value function right now is: -1604.4549129392058
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00241619]
objective value function right now is: -1604.2853511304156
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0518395]
objective value function right now is: -1604.1763835290917
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00749827]
objective value function right now is: -1605.3537363140088
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01501608]
objective value function right now is: -1604.864056004896
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.07681453]
objective value function right now is: -1605.6274160332443
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01001548]
objective value function right now is: -1603.119381045959
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0981453]
objective value function right now is: -1599.4775468276455
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00124899]
objective value function right now is: -1603.9917949336539
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01607829]
objective value function right now is: -1601.0536815656294
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04844321]
objective value function right now is: -1604.8112450293638
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05647938]
objective value function right now is: -1605.2981276911037
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00063584]
objective value function right now is: -1605.4653268393577
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.7529348570868
Current xi:  [-0.00889127]
objective value function right now is: -1605.7529348570868
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0101232]
objective value function right now is: -1603.304598752158
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1606.031447103211
Current xi:  [-0.00287072]
objective value function right now is: -1606.031447103211
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1606.0703472891128
Current xi:  [0.00308415]
objective value function right now is: -1606.0703472891128
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00228049]
objective value function right now is: -1605.893065213712
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00454621]
objective value function right now is: -1606.0655689371638
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00312017]
objective value function right now is: -1605.872353950957
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00674625]
objective value function right now is: -1605.55800629484
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00874861]
objective value function right now is: -1606.040868687172
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00533981]
objective value function right now is: -1604.502407049518
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00369087]
objective value function right now is: -1606.0639809931536
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00370274]
objective value function right now is: -1606.045431295141
new min fval from sgd:  -1606.0718554330097
new min fval from sgd:  -1606.0914223052373
new min fval from sgd:  -1606.1049037145403
new min fval from sgd:  -1606.1130998105984
new min fval from sgd:  -1606.1459700952205
new min fval from sgd:  -1606.1553830768164
new min fval from sgd:  -1606.1620498139541
new min fval from sgd:  -1606.162165908876
new min fval from sgd:  -1606.1661220976666
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00092703]
objective value function right now is: -1602.7290877981081
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00258085]
objective value function right now is: -1606.0448421559017
new min fval from sgd:  -1606.1869354037183
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00556286]
objective value function right now is: -1605.6599169053263
new min fval from sgd:  -1606.191303784897
new min fval from sgd:  -1606.1920382941473
new min fval from sgd:  -1606.1936068431582
new min fval from sgd:  -1606.1939465446696
new min fval from sgd:  -1606.1944993545847
new min fval from sgd:  -1606.1945066112253
new min fval from sgd:  -1606.1950557383655
new min fval from sgd:  -1606.1964463709025
new min fval from sgd:  -1606.196894091737
new min fval from sgd:  -1606.1988429775424
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00486584]
objective value function right now is: -1606.186293561929
new min fval from sgd:  -1606.201619952279
new min fval from sgd:  -1606.2041217746842
new min fval from sgd:  -1606.2051516284412
new min fval from sgd:  -1606.2060024046436
new min fval from sgd:  -1606.2062949280728
new min fval from sgd:  -1606.2067679316992
new min fval from sgd:  -1606.207307977542
new min fval from sgd:  -1606.2078585035044
new min fval from sgd:  -1606.2083444498169
new min fval from sgd:  -1606.2095936108076
new min fval from sgd:  -1606.2111291125495
new min fval from sgd:  -1606.2123990016294
new min fval from sgd:  -1606.2125043974258
new min fval from sgd:  -1606.213720997389
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0052881]
objective value function right now is: -1606.163401152643
min fval:  -1606.213720997389
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 4.7127, 10.8438],
        [-8.3191,  7.6549],
        [-5.7738, -2.8568],
        [-5.7717, -2.8551],
        [ 1.7412, -1.5362],
        [-9.3025,  3.4626],
        [-5.7716, -2.8550],
        [12.6330,  4.2481],
        [ 8.8634,  5.4822],
        [-8.0091,  7.8873]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-3.1436,  8.1828, -1.6150, -1.6150,  2.7939,  9.1650, -1.6149,  2.6430,
        -1.2645,  7.3594], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.0666, -0.0826, -0.0113, -0.0113, -0.5022, -0.2314, -0.0113, -0.5031,
         -0.3805, -0.0502],
        [ 6.1615, -6.8753,  2.9098,  2.9068,  2.4894, -5.1283,  2.9066, -4.8361,
         -2.1094, -6.0992],
        [-0.0666, -0.0826, -0.0113, -0.0113, -0.5022, -0.2314, -0.0113, -0.5031,
         -0.3805, -0.0502],
        [-0.0666, -0.0826, -0.0113, -0.0113, -0.5022, -0.2314, -0.0113, -0.5031,
         -0.3805, -0.0502],
        [-0.0666, -0.0826, -0.0113, -0.0113, -0.5022, -0.2314, -0.0113, -0.5031,
         -0.3805, -0.0502],
        [-6.3495,  6.0382, -2.5026, -2.4983, -2.4298,  4.7951, -2.4981,  3.6069,
          0.7102,  5.0438],
        [ 6.1917, -6.2934,  2.8078,  2.8044,  2.2330, -4.8940,  2.8043, -4.8216,
         -2.4950, -5.4524],
        [-0.0666, -0.0826, -0.0113, -0.0113, -0.5022, -0.2314, -0.0113, -0.5031,
         -0.3805, -0.0502],
        [-0.0666, -0.0826, -0.0113, -0.0113, -0.5022, -0.2314, -0.0113, -0.5031,
         -0.3805, -0.0502],
        [-7.0350,  6.1805, -2.7237, -2.7194, -2.2758,  4.9701, -2.7191,  4.0299,
          1.3466,  5.2231]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5063,  6.7594, -0.5063, -0.5063, -0.5063, -5.8878,  6.1148, -0.5063,
        -0.5063, -6.0217], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.2043e-03, -1.4259e+01, -1.2043e-03, -1.2043e-03, -1.2042e-03,
          1.1814e+01, -1.3050e+01, -1.2043e-03, -1.2043e-03,  1.2855e+01]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-16.9970,   1.7346],
        [ -1.2537,   0.2910],
        [-15.0039,  -9.2593],
        [  5.9383,  -2.3255],
        [  5.1586,   1.1572],
        [  8.7699,  -0.7821],
        [-11.4687,  -2.8029],
        [ -4.1188, -11.1032],
        [-11.9252, -12.4937],
        [ -3.0947,   9.3530]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  3.1473,  -3.2957,  -7.1455,  -6.7949, -10.0665,  -9.3270,   0.3524,
         -9.9751, -10.3655,   8.4461], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.0099e+00, -1.3459e-01, -6.5986e+00,  9.2363e-01, -1.0335e-01,
          1.2972e+01, -3.1910e+00, -4.0326e+00, -2.1867e+00,  1.6838e+01],
        [-1.0248e+01,  1.2500e-01,  2.7852e+00,  2.9655e+00,  2.7322e+00,
          5.2027e+00, -8.0307e+00,  9.1623e+00,  1.2451e+01,  1.2007e+00],
        [-1.1097e-02, -2.8139e-02, -3.1496e-02, -1.0195e+00, -1.0911e-02,
         -2.9652e-01, -3.0417e-01, -6.0341e-01, -8.6412e-02, -1.1667e+00],
        [-1.1093e-02, -2.8136e-02, -3.1492e-02, -1.0193e+00, -1.0920e-02,
         -2.9654e-01, -3.0414e-01, -6.0343e-01, -8.6417e-02, -1.1666e+00],
        [ 3.1714e+00,  5.4550e-01,  1.4557e+01, -9.5830e+00,  4.2534e-03,
         -5.4129e+00,  2.6654e+00,  3.9358e-01,  9.2423e+00, -1.3763e+01],
        [ 2.1970e+00,  5.4057e-01, -5.5517e+00,  4.2221e+00,  3.2366e-02,
          9.4876e+00, -6.6211e+00, -4.9695e+00, -7.2360e+00,  1.3553e+01],
        [-7.5170e-02,  1.7854e-02, -3.4644e-01, -2.6530e-01, -1.6877e-01,
          1.1617e+00, -2.2202e-01, -3.4495e+00, -5.7867e-01, -2.3541e+00],
        [-1.5927e+00,  2.9465e-01, -4.9304e+00,  1.3116e+00,  3.7799e-01,
         -7.4890e-01, -3.7145e+00,  8.0404e-01,  2.2297e+00,  5.0543e+00],
        [-1.2734e+01, -1.4446e-03,  8.4155e+00,  5.8666e+00,  5.5003e+00,
          9.1537e+00, -8.1552e+00,  1.1858e+01,  1.4572e+01, -1.2426e+00],
        [ 2.6383e-01,  2.2435e-02, -3.3030e-01,  2.0856e+00,  1.0589e-01,
          8.3363e-01, -2.4793e+00,  1.1529e+00,  3.7137e+00,  5.5325e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  5.1922,   1.0142,  -4.2266,  -4.2267, -16.7243,   9.1776,  -4.3646,
          2.6042,   1.8041,   3.2946], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 2.9841e+00,  1.7756e+00, -5.0376e-03, -9.0461e-03,  1.1053e+01,
          2.1445e+00,  8.9224e-01, -2.0084e+00, -2.1805e+00, -2.2438e+00],
        [-2.7579e+00, -1.8039e+00,  1.0271e-03, -2.9831e-03, -1.1048e+01,
         -2.1054e+00, -8.9459e-01,  2.0295e+00,  2.3995e+00,  2.4045e+00]],
       device='cuda:0'))])
xi:  [0.003664]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 380.9040489813508
W_T_median: 150.9560149128173
W_T_pctile_5: 0.004207418533341922
W_T_CVAR_5_pct: -105.18533209846042
Average q (qsum/M+1):  53.509871944304436
Optimal xi:  [0.003664]
Expected(across Rb) median(across samples) p_equity:  0.2844400448103746
obj fun:  tensor(-1606.2137, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1554.6261564784427
Current xi:  [0.01213477]
objective value function right now is: -1554.6261564784427
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1561.4659769225511
Current xi:  [-0.00649741]
objective value function right now is: -1561.4659769225511
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00742229]
objective value function right now is: -1559.418458306693
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1561.6650870539158
Current xi:  [0.00591365]
objective value function right now is: -1561.6650870539158
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01581097]
objective value function right now is: -1560.2388111294135
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00348146]
objective value function right now is: -1559.8160478633656
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01094951]
objective value function right now is: -1561.3614283185464
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04054438]
objective value function right now is: -1560.7638233693317
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03125393]
objective value function right now is: -1559.9335714886447
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01848356]
objective value function right now is: -1557.8267402466397
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.445715832442
Current xi:  [0.02016769]
objective value function right now is: -1562.445715832442
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00057259]
objective value function right now is: -1562.213244071682
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00083435]
objective value function right now is: -1544.533000120158
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04875628]
objective value function right now is: -1560.6925242187488
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.6746656814646
Current xi:  [0.00112903]
objective value function right now is: -1562.6746656814646
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01041481]
objective value function right now is: -1558.0997326504084
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02450796]
objective value function right now is: -1562.1795929248365
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0435322]
objective value function right now is: -1562.466811420305
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05025977]
objective value function right now is: -1562.1750202942635
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0029271]
objective value function right now is: -1562.248897769876
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02094386]
objective value function right now is: -1562.448021787315
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01361944]
objective value function right now is: -1562.1114489656632
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01897427]
objective value function right now is: -1561.6119220911019
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00186259]
objective value function right now is: -1558.9825402196566
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01429282]
objective value function right now is: -1561.4429309087918
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02795003]
objective value function right now is: -1560.261996766385
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0266473]
objective value function right now is: -1552.2588893864352
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [0.01376014]
objective value function right now is: -1561.8572482393936
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [0.00846467]
objective value function right now is: -1562.0834821449562
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00141608]
objective value function right now is: -1562.4165981459535
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0097433]
objective value function right now is: -1561.540196405022
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06409245]
objective value function right now is: -1561.4397981986642
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00442458]
objective value function right now is: -1562.2460028322475
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00663784]
objective value function right now is: -1559.9546060522039
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00451361]
objective value function right now is: -1562.3547327025274
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1563.180293566403
Current xi:  [0.00724969]
objective value function right now is: -1563.180293566403
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00553502]
objective value function right now is: -1562.8549682497649
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1563.2049167867908
Current xi:  [0.0070084]
objective value function right now is: -1563.2049167867908
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1563.2172302064278
Current xi:  [0.00961639]
objective value function right now is: -1563.2172302064278
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00558366]
objective value function right now is: -1562.549923402344
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00356319]
objective value function right now is: -1562.832455070459
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0064917]
objective value function right now is: -1563.1401662030332
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00711663]
objective value function right now is: -1562.9573857933067
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1563.2510482614543
Current xi:  [-0.00857799]
objective value function right now is: -1563.2510482614543
new min fval from sgd:  -1563.3602991004373
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01078565]
objective value function right now is: -1563.3602991004373
new min fval from sgd:  -1563.3712342681029
new min fval from sgd:  -1563.3770433138222
new min fval from sgd:  -1563.377277359576
new min fval from sgd:  -1563.4093912142225
new min fval from sgd:  -1563.4206176691785
new min fval from sgd:  -1563.4228413234136
new min fval from sgd:  -1563.434521130093
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00347006]
objective value function right now is: -1562.9195683669075
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00274455]
objective value function right now is: -1562.466659262519
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01265221]
objective value function right now is: -1563.2171856664118
new min fval from sgd:  -1563.4348650053175
new min fval from sgd:  -1563.437088865876
new min fval from sgd:  -1563.4377015961102
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0075396]
objective value function right now is: -1563.388887655727
new min fval from sgd:  -1563.438008262599
new min fval from sgd:  -1563.4384250919409
new min fval from sgd:  -1563.4408747809473
new min fval from sgd:  -1563.4473590268772
new min fval from sgd:  -1563.4538338534546
new min fval from sgd:  -1563.4595295232896
new min fval from sgd:  -1563.460976105333
new min fval from sgd:  -1563.4620716324237
new min fval from sgd:  -1563.4625405869467
new min fval from sgd:  -1563.463768669608
new min fval from sgd:  -1563.464507505762
new min fval from sgd:  -1563.4645788224718
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00891644]
objective value function right now is: -1563.4213157735933
min fval:  -1563.4645788224718
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  4.8133,   6.9183],
        [ -7.6343,   7.4300],
        [ -5.1862,  -3.6668],
        [ -5.1829,  -3.6655],
        [  2.5673,  -1.8259],
        [-10.7697,   0.9631],
        [ -5.1828,  -3.6654],
        [ 11.4683,   5.1620],
        [  8.6424,   7.7584],
        [ -7.9994,   7.7367]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-4.2705,  8.3177, -2.6574, -2.6570,  2.9408,  9.6017, -2.6569,  3.2123,
        -0.6253,  7.5134], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.0343, -0.1276, -0.0392, -0.0392, -0.4886, -0.3682, -0.0392, -0.4944,
         -0.2316, -0.0598],
        [10.5168, -6.9844,  3.1576,  3.1541,  3.0032, -5.2082,  3.1539, -4.7941,
         -1.6088, -5.9793],
        [-0.0343, -0.1276, -0.0392, -0.0392, -0.4886, -0.3682, -0.0392, -0.4944,
         -0.2316, -0.0598],
        [-0.0343, -0.1276, -0.0392, -0.0392, -0.4886, -0.3682, -0.0392, -0.4944,
         -0.2316, -0.0598],
        [-0.0343, -0.1276, -0.0392, -0.0392, -0.4886, -0.3682, -0.0392, -0.4944,
         -0.2316, -0.0598],
        [-8.7787,  6.6727, -3.0365, -3.0320, -3.0069,  4.9893, -3.0317,  3.5818,
         -0.5335,  5.7528],
        [ 9.9877, -6.6510,  2.9524,  2.9487,  2.8867, -4.8411,  2.9485, -4.5396,
         -1.7865, -5.5201],
        [-0.0343, -0.1276, -0.0392, -0.0392, -0.4886, -0.3682, -0.0392, -0.4944,
         -0.2316, -0.0598],
        [-0.0343, -0.1276, -0.0392, -0.0392, -0.4886, -0.3682, -0.0392, -0.4944,
         -0.2316, -0.0598],
        [-9.2684,  6.8140, -3.1164, -3.1117, -3.0387,  5.2135, -3.1115,  3.6576,
         -0.1632,  5.9767]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.4905,  7.1239, -0.4905, -0.4905, -0.4905, -6.3110,  6.6097, -0.4905,
        -0.4905, -6.6382], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-3.5228e-03, -1.3851e+01, -3.5228e-03, -3.5228e-03, -3.5228e-03,
          1.0908e+01, -1.2402e+01, -3.5228e-03, -3.5228e-03,  1.1792e+01]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-15.8084,   1.3730],
        [ -1.4952,   0.7046],
        [-15.0147, -10.0993],
        [  7.3959,  -2.6391],
        [  2.0881,   1.6953],
        [  9.9575,  -0.8530],
        [ -9.7305,  -1.9367],
        [ -5.1472, -12.6353],
        [-13.3659, -13.2787],
        [ -6.1587,  10.3637]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  2.7428,  -3.6208,  -8.2977,  -8.1683,  -8.9764, -10.5453,   0.3294,
        -11.1168, -11.0092,   9.9176], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.9917e+00, -1.8986e-01, -6.2752e+00,  3.9108e-01, -1.4071e-02,
          1.2736e+01, -3.8608e+00, -4.2546e+00, -1.3781e+00,  1.5402e+01],
        [-1.1996e+01,  7.7849e-02,  1.9611e+00,  4.1372e+00,  4.0909e+00,
          4.3667e+00, -9.2673e+00,  1.1278e+01,  1.3930e+01,  1.0255e+00],
        [-4.5923e-02, -9.6111e-02, -2.6784e-01, -7.2557e-01, -3.4139e-01,
         -2.0841e-01, -3.7027e-01, -5.2308e-01, -5.2185e-01, -1.4491e+00],
        [-4.5923e-02, -9.6111e-02, -2.6784e-01, -7.2557e-01, -3.4139e-01,
         -2.0842e-01, -3.7027e-01, -5.2308e-01, -5.2185e-01, -1.4491e+00],
        [ 6.4316e+00,  8.2871e-02,  1.5755e+01, -8.6885e+00,  2.5831e-03,
         -3.0570e+00,  5.6837e+00,  7.0544e-01,  8.8976e+00, -1.5776e+01],
        [-3.6353e-01,  2.6301e-01, -6.4833e+00,  7.1168e+00, -1.9675e-03,
          1.1934e+01, -6.9584e+00, -6.2942e+00, -1.0998e+01,  1.5103e+01],
        [-2.7223e-02, -8.5083e-02, -3.3306e-01,  3.3194e-02, -2.5343e-01,
          7.3269e-01, -3.3504e-01, -2.3668e+00, -7.3747e-01, -1.7956e+00],
        [-4.0503e+00, -4.8673e-02, -3.9513e+00,  1.1842e+00,  9.7229e-02,
         -5.1863e-01, -4.9660e+00,  8.7921e-01,  3.6430e+00,  5.9583e+00],
        [-1.2077e+01,  1.9024e-02,  7.6374e+00,  8.2139e+00,  5.1416e+00,
          1.1958e+01, -9.3092e+00,  1.6298e+01,  1.3354e+01, -1.0655e+00],
        [-2.9932e-01,  5.2726e-02,  3.5600e+00,  1.6984e+00,  6.8730e-02,
          1.4465e+00, -2.2583e+00, -4.5378e-01,  6.9712e+00,  6.0526e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  5.3888,   1.0411,  -5.0032,  -5.0032, -18.6775,  11.3608,  -5.5782,
          2.5180,   1.6739,   3.3051], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  2.8943,   1.4412,   0.1493,   0.1484,  11.8029,   2.3721,   0.8849,
          -1.9694,  -1.7968,  -2.3627],
        [ -2.6684,  -1.4695,  -0.1503,  -0.1512, -11.7870,  -2.3331,  -0.8864,
           1.9905,   2.0155,   2.5232]], device='cuda:0'))])
xi:  [0.01005519]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 368.80831264929924
W_T_median: 144.7304948397616
W_T_pctile_5: 0.008250373825642754
W_T_CVAR_5_pct: -74.68896461634353
Average q (qsum/M+1):  52.84366336945565
Optimal xi:  [0.01005519]
Expected(across Rb) median(across samples) p_equity:  0.26454657862583797
obj fun:  tensor(-1563.4646, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
loaded xi:  0.01005519
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1525.772635238689
Current xi:  [-0.01222266]
objective value function right now is: -1525.772635238689
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1526.6381887322582
Current xi:  [0.00804071]
objective value function right now is: -1526.6381887322582
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00545939]
objective value function right now is: -1524.7471353351398
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1527.5942728515138
Current xi:  [0.00061662]
objective value function right now is: -1527.5942728515138
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00092743]
objective value function right now is: -1525.9992148066553
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1527.8059224916208
Current xi:  [-0.00937099]
objective value function right now is: -1527.8059224916208
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1528.2388016648408
Current xi:  [0.02519877]
objective value function right now is: -1528.2388016648408
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01035104]
objective value function right now is: -1526.1822033824444
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0390353]
objective value function right now is: -1525.6573162747716
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0062336]
objective value function right now is: -1526.9104177831925
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00480385]
objective value function right now is: -1527.0081130180472
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00856809]
objective value function right now is: -1492.1728805696093
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01731426]
objective value function right now is: -1525.5632573043938
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01540306]
objective value function right now is: -1524.9445872077097
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00151184]
objective value function right now is: -1526.1035156190285
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03832484]
objective value function right now is: -1528.0181795758756
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00452769]
objective value function right now is: -1527.6200586014072
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1528.4380635914188
Current xi:  [0.00382798]
objective value function right now is: -1528.4380635914188
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01514389]
objective value function right now is: -1527.8889115420234
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04039196]
objective value function right now is: -1526.5835416119883
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00613295]
objective value function right now is: -1527.5878481890793
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00387656]
objective value function right now is: -1528.264966923391
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00786973]
objective value function right now is: -1526.6476138430876
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00629906]
objective value function right now is: -1523.0120627599358
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00676461]
objective value function right now is: -1526.7446744118595
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1528.7882993236358
Current xi:  [0.00581303]
objective value function right now is: -1528.7882993236358
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00279227]
objective value function right now is: -1526.839300950443
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1529.0335963944315
Current xi:  [0.01478435]
objective value function right now is: -1529.0335963944315
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00046131]
objective value function right now is: -1527.5822799817286
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00869346]
objective value function right now is: -1526.787768482735
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01870761]
objective value function right now is: -1526.676415344483
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02293761]
objective value function right now is: -1527.6621437231531
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.1140556]
objective value function right now is: -1500.0637319733025
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01836915]
objective value function right now is: -1527.6160430941707
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00497429]
objective value function right now is: -1517.8564185743126
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01251085]
objective value function right now is: -1528.847793261828
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01097049]
objective value function right now is: -1528.0227318061923
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0091703]
objective value function right now is: -1528.6681344380709
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01202217]
objective value function right now is: -1528.7425200896012
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01193945]
objective value function right now is: -1528.9442458675883
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1529.1537869162485
Current xi:  [0.01628818]
objective value function right now is: -1529.1537869162485
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01085567]
objective value function right now is: -1524.830884744635
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02390767]
objective value function right now is: -1528.9381638837633
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0098723]
objective value function right now is: -1529.0147537268015
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01892171]
objective value function right now is: -1528.6719806545973
new min fval from sgd:  -1529.1786716417423
new min fval from sgd:  -1529.2731493822123
new min fval from sgd:  -1529.3021811141614
new min fval from sgd:  -1529.3047185457665
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01086775]
objective value function right now is: -1528.639607333329
new min fval from sgd:  -1529.3105814261571
new min fval from sgd:  -1529.3210588532725
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00759738]
objective value function right now is: -1528.7480828655005
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0051614]
objective value function right now is: -1529.203172332539
new min fval from sgd:  -1529.3254968522303
new min fval from sgd:  -1529.327720070253
new min fval from sgd:  -1529.3337355547715
new min fval from sgd:  -1529.3360263554748
new min fval from sgd:  -1529.3363364337324
new min fval from sgd:  -1529.3390613728627
new min fval from sgd:  -1529.3401697993777
new min fval from sgd:  -1529.3446205314333
new min fval from sgd:  -1529.3452802803936
new min fval from sgd:  -1529.3458318960988
new min fval from sgd:  -1529.3479410157568
new min fval from sgd:  -1529.348066260975
new min fval from sgd:  -1529.354355371953
new min fval from sgd:  -1529.3549863885971
new min fval from sgd:  -1529.3555132942224
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01603649]
objective value function right now is: -1529.27190840377
new min fval from sgd:  -1529.3608940087151
new min fval from sgd:  -1529.3706454129374
new min fval from sgd:  -1529.3713317046975
new min fval from sgd:  -1529.372777048829
new min fval from sgd:  -1529.3814579421062
new min fval from sgd:  -1529.3861259230255
new min fval from sgd:  -1529.3877395820548
new min fval from sgd:  -1529.3877851150373
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01829751]
objective value function right now is: -1529.2871738092927
min fval:  -1529.3877851150373
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  5.4661,   4.5038],
        [ -7.2499,   7.5604],
        [ -5.1475,  -4.2982],
        [ -5.1357,  -4.2970],
        [  3.5200,  -4.0783],
        [-11.8569,   0.9122],
        [ -5.1352,  -4.2969],
        [ 12.2711,   5.9272],
        [  7.7011,   7.8576],
        [ -8.6977,   7.1059]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-5.0924,  8.2647, -3.3547, -3.3541,  2.3477, 10.5751, -3.3540,  3.6567,
         0.1710,  7.4063], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.0935, -0.2286, -0.0412, -0.0413, -0.5236, -0.4151, -0.0413, -0.5070,
         -0.2397, -0.1378],
        [11.0576, -7.2199,  3.3753,  3.3713,  3.6139, -6.1747,  3.3710, -4.7296,
         -1.7407, -5.9769],
        [-0.0935, -0.2286, -0.0412, -0.0413, -0.5236, -0.4151, -0.0413, -0.5070,
         -0.2397, -0.1378],
        [-0.0935, -0.2286, -0.0412, -0.0413, -0.5236, -0.4151, -0.0413, -0.5070,
         -0.2397, -0.1378],
        [-0.0935, -0.2286, -0.0412, -0.0413, -0.5236, -0.4151, -0.0413, -0.5070,
         -0.2397, -0.1378],
        [-8.3717,  6.9424, -3.5317, -3.5268, -3.7201,  5.9254, -3.5266,  3.4661,
         -1.0340,  6.3697],
        [10.4251, -6.9436,  3.1243,  3.1201,  3.4622, -5.5925,  3.1198, -4.5298,
         -1.8698, -5.5253],
        [-0.0935, -0.2286, -0.0412, -0.0413, -0.5236, -0.4151, -0.0413, -0.5070,
         -0.2397, -0.1378],
        [-0.0935, -0.2286, -0.0412, -0.0413, -0.5236, -0.4151, -0.0413, -0.5070,
         -0.2397, -0.1378],
        [-8.8152,  7.0939, -3.6752, -3.6702, -3.7804,  6.2211, -3.6699,  3.5323,
         -0.7873,  6.6799]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5275,  7.6038, -0.5275, -0.5275, -0.5275, -6.8792,  7.0410, -0.5275,
        -0.5275, -7.2380], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 5.3727e-03, -1.3402e+01,  5.3727e-03,  5.3727e-03,  5.3727e-03,
          9.2166e+00, -1.1582e+01,  5.3727e-03,  5.3727e-03,  1.0316e+01]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-14.7352,   0.6604],
        [ -1.6466,   0.5724],
        [-14.7669, -10.7379],
        [  8.2757,  -2.2009],
        [ -0.1437,   2.1576],
        [ 10.6586,  -0.8580],
        [ -9.4135,  -2.1873],
        [ -5.6966, -13.6392],
        [-14.2167, -13.9364],
        [ -8.3082,  11.0404]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  2.0069,  -3.9066,  -8.6838,  -8.7009,  -7.0891, -11.2705,   0.0472,
        -11.7952, -11.8843,  10.9514], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.7597e+00, -1.1478e-01, -5.5150e+00,  4.7609e-01, -3.3264e-02,
          1.0935e+01, -4.0876e+00, -4.0939e+00, -1.4199e+00,  1.4366e+01],
        [-1.2431e+01,  4.2208e-02,  2.5258e+00,  4.0959e+00,  3.7532e+00,
          4.7731e+00, -1.0199e+01,  1.3255e+01,  1.4823e+01,  1.2694e+00],
        [-5.7121e-02, -3.2562e-02, -1.3897e-01, -9.0758e-01, -7.4921e-02,
         -4.5407e-01, -3.8969e-01, -8.4136e-01, -3.2931e-01, -9.9824e-01],
        [-5.7121e-02, -3.2562e-02, -1.3897e-01, -9.0758e-01, -7.4921e-02,
         -4.5407e-01, -3.8969e-01, -8.4136e-01, -3.2931e-01, -9.9824e-01],
        [ 6.5890e+00,  5.2929e-02,  1.4873e+01, -7.6289e+00,  2.1679e-03,
         -1.7223e+00,  1.0620e+01,  1.9464e-01,  8.0973e+00, -1.3528e+01],
        [-1.7793e+00,  1.2937e-01, -8.0085e+00,  1.1142e+01,  2.2213e-03,
          1.3508e+01, -7.2653e+00, -7.2141e+00, -1.3500e+01,  1.5151e+01],
        [-5.7121e-02, -3.2563e-02, -1.3897e-01, -9.0757e-01, -7.4920e-02,
         -4.5407e-01, -3.8969e-01, -8.4136e-01, -3.2931e-01, -9.9824e-01],
        [-6.2991e+00,  2.9405e-02, -1.3943e+00,  8.5630e-01, -2.6394e-01,
          4.2402e-01, -5.9825e+00,  2.2408e+00,  4.6019e+00,  7.3914e+00],
        [-1.1032e+01, -5.1242e-03,  6.1596e+00,  1.1369e+01,  3.4910e+00,
          1.1679e+01, -1.0269e+01,  2.0036e+01,  1.1495e+01, -1.1595e+00],
        [ 2.5119e-01, -4.1374e-02,  3.4057e+00,  2.0763e+00, -5.3053e-02,
          2.2307e+00, -2.6148e+00,  1.4529e-01,  6.5020e+00,  6.4801e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  5.6705,   0.9627,  -5.2160,  -5.2160, -21.7650,  13.6901,  -5.2160,
          2.2368,   1.8525,   3.9483], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  2.7211,   1.1430,   0.0898,   0.0896,  12.5669,   2.6009,   0.0896,
          -1.9292,  -1.4687,  -2.4228],
        [ -2.4954,  -1.1712,  -0.0900,  -0.0902, -12.5571,  -2.5619,  -0.0901,
           1.9504,   1.6872,   2.5832]], device='cuda:0'))])
xi:  [0.01879877]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 372.3420648875538
W_T_median: 151.19026714262023
W_T_pctile_5: 0.018449079638590507
W_T_CVAR_5_pct: -63.776617668951765
Average q (qsum/M+1):  52.421048072076616
Optimal xi:  [0.01879877]
Expected(across Rb) median(across samples) p_equity:  0.25357865691185
obj fun:  tensor(-1529.3878, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
loaded xi:  0.018798774
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1440.086941021623
Current xi:  [0.0095815]
objective value function right now is: -1440.086941021623
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01301354]
objective value function right now is: -1437.6619481983003
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01537457]
objective value function right now is: -1439.4271973404002
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.04167961]
objective value function right now is: -1428.2708129049986
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1441.0251279423628
Current xi:  [0.1445704]
objective value function right now is: -1441.0251279423628
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1442.3657923380583
Current xi:  [3.7490003]
objective value function right now is: -1442.3657923380583
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1445.2975710745902
Current xi:  [10.929088]
objective value function right now is: -1445.2975710745902
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1458.196030714687
Current xi:  [20.507214]
objective value function right now is: -1458.196030714687
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1470.866372519761
Current xi:  [31.006844]
objective value function right now is: -1470.866372519761
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1488.5780760816108
Current xi:  [41.71743]
objective value function right now is: -1488.5780760816108
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.99964]
objective value function right now is: -1468.2101527349735
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1490.6321582515818
Current xi:  [63.756905]
objective value function right now is: -1490.6321582515818
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1517.933233741751
Current xi:  [74.06506]
objective value function right now is: -1517.933233741751
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1527.2893038673196
Current xi:  [83.9263]
objective value function right now is: -1527.2893038673196
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1536.9008047509244
Current xi:  [93.75879]
objective value function right now is: -1536.9008047509244
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1539.2216151228747
Current xi:  [102.72178]
objective value function right now is: -1539.2216151228747
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [111.26152]
objective value function right now is: -1538.6262835225878
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1548.1373025181001
Current xi:  [119.5924]
objective value function right now is: -1548.1373025181001
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [127.37316]
objective value function right now is: -1538.013639115703
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [133.91116]
objective value function right now is: -1528.5589008523657
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1558.0701716605975
Current xi:  [139.19888]
objective value function right now is: -1558.0701716605975
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1560.303388522359
Current xi:  [144.08247]
objective value function right now is: -1560.303388522359
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [149.00789]
objective value function right now is: -1554.1473463810278
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [152.34999]
objective value function right now is: -1547.0170781597258
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [154.71567]
objective value function right now is: -1544.5118711140788
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1561.7613643656669
Current xi:  [156.43945]
objective value function right now is: -1561.7613643656669
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [158.4219]
objective value function right now is: -1551.417557443378
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [159.57434]
objective value function right now is: -1559.3221662833168
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [160.92503]
objective value function right now is: -1561.604211262709
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.8757181505273
Current xi:  [162.43468]
objective value function right now is: -1562.8757181505273
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.86833]
objective value function right now is: -1562.2561057108683
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.0954]
objective value function right now is: -1558.5280984706744
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.09804]
objective value function right now is: -1555.2243506452244
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.65714]
objective value function right now is: -1559.5498302335275
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.55032]
objective value function right now is: -1560.0849348121264
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1565.391955216979
Current xi:  [164.73997]
objective value function right now is: -1565.391955216979
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.11172]
objective value function right now is: -1565.1690264972751
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.28851]
objective value function right now is: -1564.783142858642
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.61865]
objective value function right now is: -1560.3715067262253
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1565.7870546843799
Current xi:  [165.85167]
objective value function right now is: -1565.7870546843799
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.14635]
objective value function right now is: -1565.1368984206092
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.20537]
objective value function right now is: -1565.3683260002438
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.29895]
objective value function right now is: -1562.1241680517737
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.55116]
objective value function right now is: -1565.7514879168107
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.71086]
objective value function right now is: -1561.6455942871783
new min fval from sgd:  -1565.8034110439082
new min fval from sgd:  -1565.849857405062
new min fval from sgd:  -1565.892521594768
new min fval from sgd:  -1565.9210829473943
new min fval from sgd:  -1565.9522423473488
new min fval from sgd:  -1565.9878265640273
new min fval from sgd:  -1566.0156471850917
new min fval from sgd:  -1566.0522140643425
new min fval from sgd:  -1566.0696501588657
new min fval from sgd:  -1566.0797181942808
new min fval from sgd:  -1566.1767807878084
new min fval from sgd:  -1566.2324281857973
new min fval from sgd:  -1566.2911161097554
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.97086]
objective value function right now is: -1565.0342650545163
new min fval from sgd:  -1566.3166080768383
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.98926]
objective value function right now is: -1558.5599372168904
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.14867]
objective value function right now is: -1564.9005783676148
new min fval from sgd:  -1566.323154336319
new min fval from sgd:  -1566.3322736067694
new min fval from sgd:  -1566.3375108761443
new min fval from sgd:  -1566.3399766032128
new min fval from sgd:  -1566.3549893818956
new min fval from sgd:  -1566.3841551376483
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.21701]
objective value function right now is: -1566.3245480085664
new min fval from sgd:  -1566.3848178840908
new min fval from sgd:  -1566.4012213807202
new min fval from sgd:  -1566.4034929411528
new min fval from sgd:  -1566.4129288856327
new min fval from sgd:  -1566.4168877871857
new min fval from sgd:  -1566.41731770556
new min fval from sgd:  -1566.4263340169493
new min fval from sgd:  -1566.4302342541328
new min fval from sgd:  -1566.442470312153
new min fval from sgd:  -1566.4432662877289
new min fval from sgd:  -1566.4510686726308
new min fval from sgd:  -1566.4518854472249
new min fval from sgd:  -1566.4621898006296
new min fval from sgd:  -1566.4666072692457
new min fval from sgd:  -1566.4670938456263
new min fval from sgd:  -1566.4712483152869
new min fval from sgd:  -1566.4726361655826
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.22916]
objective value function right now is: -1565.4456701492777
min fval:  -1566.4726361655826
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  6.7393,   3.7634],
        [ -7.9753,   9.3664],
        [ -7.0257,  -5.2947],
        [ -7.0210,  -5.2925],
        [  4.5798,  -1.3369],
        [-13.7088,   1.2440],
        [ -7.0208,  -5.2924],
        [ 43.4666,   5.4780],
        [  0.0910,   3.2105],
        [-10.1629,   8.1559]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-5.1331,  7.5040, -3.7062, -3.7067,  4.3021, 10.7060, -3.7067,  5.0945,
        -5.9629,  6.9269], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.5302e-01, -2.8654e-01, -7.4058e-02, -7.3967e-02, -5.8206e-01,
         -4.6446e-01, -7.3963e-02, -5.6591e-01, -7.1828e-03, -2.6324e-01],
        [ 9.9300e+00, -8.2230e+00,  3.6676e+00,  3.6623e+00,  4.2533e+00,
         -6.7421e+00,  3.6620e+00, -5.0365e+00,  4.6656e-01, -6.8555e+00],
        [-1.5302e-01, -2.8654e-01, -7.4058e-02, -7.3967e-02, -5.8206e-01,
         -4.6446e-01, -7.3963e-02, -5.6591e-01, -7.1828e-03, -2.6324e-01],
        [-1.5302e-01, -2.8654e-01, -7.4058e-02, -7.3967e-02, -5.8206e-01,
         -4.6446e-01, -7.3963e-02, -5.6591e-01, -7.1828e-03, -2.6324e-01],
        [-1.5302e-01, -2.8654e-01, -7.4058e-02, -7.3968e-02, -5.8206e-01,
         -4.6446e-01, -7.3963e-02, -5.6591e-01, -7.1828e-03, -2.6324e-01],
        [-7.7662e+00,  7.4740e+00, -3.9314e+00, -3.9256e+00, -4.3403e+00,
          6.5347e+00, -3.9253e+00,  3.4042e+00, -3.1371e+00,  6.9127e+00],
        [ 9.6213e+00, -7.9217e+00,  3.5435e+00,  3.5381e+00,  4.0140e+00,
         -6.1707e+00,  3.5378e+00, -5.1246e+00,  5.7603e-01, -6.2588e+00],
        [-1.5302e-01, -2.8654e-01, -7.4058e-02, -7.3968e-02, -5.8206e-01,
         -4.6446e-01, -7.3963e-02, -5.6591e-01, -7.1828e-03, -2.6324e-01],
        [-1.5302e-01, -2.8654e-01, -7.4058e-02, -7.3967e-02, -5.8206e-01,
         -4.6446e-01, -7.3963e-02, -5.6591e-01, -7.1828e-03, -2.6324e-01],
        [-8.4216e+00,  7.7451e+00, -3.8787e+00, -3.8728e+00, -4.3436e+00,
          6.9196e+00, -3.8725e+00,  3.4834e+00, -3.0462e+00,  7.3560e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5831,  8.2240, -0.5831, -0.5831, -0.5831, -7.4672,  7.5617, -0.5831,
        -0.5831, -7.7727], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.3557e-02, -1.4591e+01,  1.3557e-02,  1.3557e-02,  1.3557e-02,
          8.6712e+00, -1.2273e+01,  1.3557e-02,  1.3557e-02,  1.0011e+01]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-18.2970,   1.4658],
        [ -1.6091,   0.5053],
        [-14.8232, -11.2121],
        [ 10.3754,  -1.1845],
        [ -1.3363,   2.5020],
        [ 10.2655,  -1.0565],
        [ -9.6586,  -3.3603],
        [ -2.0728, -14.0946],
        [-12.2013, -15.4218],
        [ -8.6195,  12.5640]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  1.0554,  -4.5440,  -8.6679,  -8.5087,  -6.7558, -12.6802,  -1.0435,
        -11.9603, -12.0113,   9.6595], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-6.8200e+00, -1.0720e-01, -1.4451e+00,  1.1215e+00, -4.2959e-02,
          7.5861e+00, -3.6381e+00, -4.0501e+00, -9.2108e-01,  1.2491e+01],
        [-1.2591e+01,  1.8751e-01,  2.0671e+00,  1.0076e+01,  3.3223e+00,
          6.5084e-01, -1.0285e+01,  1.6908e+01,  1.3298e+01,  1.8794e+00],
        [-1.0555e-01,  7.8880e-02, -1.4673e-01, -1.2503e+00, -1.5533e-02,
         -2.3549e-01, -6.1426e-01, -4.1173e-01,  5.3939e-01, -1.2784e+00],
        [-1.0555e-01,  7.8880e-02, -1.4673e-01, -1.2503e+00, -1.5533e-02,
         -2.3549e-01, -6.1426e-01, -4.1173e-01,  5.3939e-01, -1.2784e+00],
        [-2.5422e+00, -4.3744e-01,  1.3640e+01, -7.0698e+00, -3.6075e-02,
         -4.2320e+00,  1.0459e+01,  3.0718e-01,  8.0671e+00, -1.0800e+01],
        [-1.7198e+00,  3.5218e-01, -9.0392e+00,  1.2403e+01,  1.2306e-02,
          1.6422e+01, -5.7586e+00, -7.2167e+00, -1.3571e+01,  1.2102e+01],
        [-1.0555e-01,  7.8880e-02, -1.4673e-01, -1.2503e+00, -1.5533e-02,
         -2.3549e-01, -6.1426e-01, -4.1173e-01,  5.3939e-01, -1.2784e+00],
        [-7.2786e+00, -7.2885e-02, -5.8692e-01,  2.0546e+00, -1.8870e+00,
         -4.1011e+00, -7.3430e+00,  7.5139e+00,  4.6938e+00,  6.0463e+00],
        [-4.2438e+00,  9.7590e-02,  6.7135e+00,  1.7018e+01,  3.3188e+00,
          6.5558e-01, -9.8319e+00,  1.8165e+01,  1.1263e+01, -2.4890e+00],
        [-6.0490e-01,  2.3072e-02,  5.7530e+00,  8.7630e+00,  7.0121e-02,
          1.2671e+00, -3.9530e+00,  5.6577e-02,  9.5402e+00,  8.8615e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  5.3124,   1.7093,  -5.8522,  -5.8522, -25.2819,  16.4752,  -5.8522,
          2.3212,   3.3252,   3.2566], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  2.5695,   0.9731,  -0.7222,  -0.7223,  15.2327,   2.7974,  -0.7222,
          -1.8364,  -1.3375,  -2.4318],
        [ -2.3439,  -1.0013,   0.7221,   0.7221, -15.2207,  -2.7585,   0.7221,
           1.8576,   1.5559,   2.5922]], device='cuda:0'))])
xi:  [167.25404]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 568.2813510267628
W_T_median: 356.62097097143317
W_T_pctile_5: 167.63587587568023
W_T_CVAR_5_pct: 15.565870091962882
Average q (qsum/M+1):  49.025634765625
Optimal xi:  [167.25404]
Expected(across Rb) median(across samples) p_equity:  0.2454637236893177
obj fun:  tensor(-1566.4726, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
loaded xi:  167.25404
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1580.8140056640148
Current xi:  [171.76494]
objective value function right now is: -1580.8140056640148
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [175.68088]
objective value function right now is: -1535.315595051881
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1581.1128386279634
Current xi:  [178.52162]
objective value function right now is: -1581.1128386279634
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.45882]
objective value function right now is: -1575.7186037171425
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1599.7629308396597
Current xi:  [181.40134]
objective value function right now is: -1599.7629308396597
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.30054]
objective value function right now is: -1598.3779777650684
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [183.58893]
objective value function right now is: -1497.6699472266055
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.70247]
objective value function right now is: -1585.9920250276216
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1601.18087660384
Current xi:  [184.76932]
objective value function right now is: -1601.18087660384
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.26662]
objective value function right now is: -1587.658654963159
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.57655]
objective value function right now is: -1596.776389091798
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.23581]
objective value function right now is: -1600.7527438269294
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.75111]
objective value function right now is: -1502.9693763885161
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1601.3476098636036
Current xi:  [187.56274]
objective value function right now is: -1601.3476098636036
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.71472]
objective value function right now is: -1588.4019082933376
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.49464]
objective value function right now is: -1599.195464821674
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.86996]
objective value function right now is: -1594.341161941862
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.50523]
objective value function right now is: -1585.53086629028
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.55013]
objective value function right now is: -1598.3092771347392
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.4135]
objective value function right now is: -1577.4906268088473
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.59512]
objective value function right now is: -1563.9237528491335
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.5727]
objective value function right now is: -1586.7791930638166
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1603.2633837994024
Current xi:  [187.2904]
objective value function right now is: -1603.2633837994024
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.99216]
objective value function right now is: -1601.630950238784
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.87543]
objective value function right now is: -1556.3607850614358
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.202128622464
Current xi:  [187.30031]
objective value function right now is: -1604.202128622464
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.23927]
objective value function right now is: -1576.2954593831398
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [186.97502]
objective value function right now is: -1599.7965328553369
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [186.84698]
objective value function right now is: -1583.7978725017215
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.87035]
objective value function right now is: -1602.6753626491707
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.6812285787685
Current xi:  [186.53932]
objective value function right now is: -1604.6812285787685
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.54385]
objective value function right now is: -1588.3108563492272
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.1198847084734
Current xi:  [185.84499]
objective value function right now is: -1605.1198847084734
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.89978]
objective value function right now is: -1603.3833717830119
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.70184]
objective value function right now is: -1601.7123474538132
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.9151]
objective value function right now is: -1605.0105318570352
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.1327]
objective value function right now is: -1602.7432269148353
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1607.0905439894752
Current xi:  [187.37459]
objective value function right now is: -1607.0905439894752
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.60783]
objective value function right now is: -1605.6025152870882
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.66795]
objective value function right now is: -1601.9723670075223
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.9114]
objective value function right now is: -1601.7248441646643
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1607.2236823899673
Current xi:  [188.07834]
objective value function right now is: -1607.2236823899673
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1607.8605535317877
Current xi:  [188.217]
objective value function right now is: -1607.8605535317877
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.19229]
objective value function right now is: -1602.641162879095
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.47304]
objective value function right now is: -1607.7328078358512
new min fval from sgd:  -1607.8698442061125
new min fval from sgd:  -1607.8755881358059
new min fval from sgd:  -1607.9464105475015
new min fval from sgd:  -1608.1346825806202
new min fval from sgd:  -1608.243802792163
new min fval from sgd:  -1608.2706692563645
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.65883]
objective value function right now is: -1606.997337624982
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.7215]
objective value function right now is: -1606.1649129902844
new min fval from sgd:  -1608.2706738121035
new min fval from sgd:  -1608.3205461163657
new min fval from sgd:  -1608.3221766822573
new min fval from sgd:  -1608.3281755355285
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.62976]
objective value function right now is: -1606.9991057811876
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.63776]
objective value function right now is: -1607.7495803492259
new min fval from sgd:  -1608.3442174819606
new min fval from sgd:  -1608.3610782721735
new min fval from sgd:  -1608.407197201041
new min fval from sgd:  -1608.4148520423585
new min fval from sgd:  -1608.4191726661825
new min fval from sgd:  -1608.421339699208
new min fval from sgd:  -1608.425952216452
new min fval from sgd:  -1608.428335525521
new min fval from sgd:  -1608.4740658020764
new min fval from sgd:  -1608.4760871625087
new min fval from sgd:  -1608.4784395656982
new min fval from sgd:  -1608.4939806355048
new min fval from sgd:  -1608.5055492922133
new min fval from sgd:  -1608.523827454373
new min fval from sgd:  -1608.536309344125
new min fval from sgd:  -1608.5522914021383
new min fval from sgd:  -1608.564176609133
new min fval from sgd:  -1608.5691589890432
new min fval from sgd:  -1608.5706858694177
new min fval from sgd:  -1608.5742758734818
new min fval from sgd:  -1608.578198100178
new min fval from sgd:  -1608.5789762259622
new min fval from sgd:  -1608.6079508344503
new min fval from sgd:  -1608.6228723401475
new min fval from sgd:  -1608.628812508721
new min fval from sgd:  -1608.6337860387694
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.63274]
objective value function right now is: -1607.4471274209436
min fval:  -1608.6337860387694
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  7.0291,   4.2367],
        [ -8.9652,  10.0543],
        [ -7.2303,  -6.1152],
        [ -7.2219,  -6.1101],
        [  2.4524,  -0.6084],
        [-15.1161,   1.5758],
        [ -7.2214,  -6.1099],
        [ 40.8510,   6.1302],
        [  0.5286,   4.0227],
        [-11.0949,   8.2294]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-5.8024,  7.7150, -4.2070, -4.2095,  5.5694, 11.0992, -4.2096,  5.2793,
        -7.7942,  6.8742], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.1588e-01, -2.6193e-01, -8.1242e-02, -8.0958e-02, -6.2150e-01,
         -5.1675e-01, -8.0945e-02, -5.8828e-01, -8.9724e-03, -2.3078e-01],
        [ 9.6069e+00, -8.8933e+00,  3.4680e+00,  3.4606e+00,  4.9050e+00,
         -8.0653e+00,  3.4602e+00, -4.8531e+00,  1.8559e+00, -7.2123e+00],
        [-1.1588e-01, -2.6193e-01, -8.1242e-02, -8.0958e-02, -6.2150e-01,
         -5.1675e-01, -8.0945e-02, -5.8828e-01, -8.9724e-03, -2.3078e-01],
        [-1.1588e-01, -2.6193e-01, -8.1242e-02, -8.0958e-02, -6.2150e-01,
         -5.1675e-01, -8.0945e-02, -5.8828e-01, -8.9724e-03, -2.3078e-01],
        [-1.1588e-01, -2.6193e-01, -8.1242e-02, -8.0958e-02, -6.2150e-01,
         -5.1675e-01, -8.0945e-02, -5.8828e-01, -8.9724e-03, -2.3078e-01],
        [-7.0096e+00,  7.7611e+00, -4.0171e+00, -4.0086e+00, -5.0346e+00,
          7.9603e+00, -4.0082e+00,  2.7645e+00, -2.5331e+00,  7.1570e+00],
        [ 9.1970e+00, -8.5538e+00,  3.3105e+00,  3.3032e+00,  4.6823e+00,
         -7.4684e+00,  3.3028e+00, -4.9637e+00,  1.7978e+00, -6.5533e+00],
        [-1.1588e-01, -2.6193e-01, -8.1242e-02, -8.0958e-02, -6.2150e-01,
         -5.1675e-01, -8.0945e-02, -5.8828e-01, -8.9724e-03, -2.3078e-01],
        [-1.1588e-01, -2.6193e-01, -8.1242e-02, -8.0958e-02, -6.2150e-01,
         -5.1675e-01, -8.0945e-02, -5.8828e-01, -8.9724e-03, -2.3078e-01],
        [-8.2784e+00,  8.3629e+00, -3.5172e+00, -3.5086e+00, -4.8886e+00,
          8.1218e+00, -3.5082e+00,  3.0934e+00, -2.7390e+00,  7.7462e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.6223,  8.8332, -0.6223, -0.6223, -0.6223, -8.0939,  8.1739, -0.6223,
        -0.6223, -8.2728], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-7.5362e-03, -1.5707e+01, -7.5362e-03, -7.5362e-03, -7.5362e-03,
          8.4895e+00, -1.2681e+01, -7.5362e-03, -7.5362e-03,  1.0327e+01]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-18.0454,   1.3901],
        [  2.2323,   6.7059],
        [-14.9205, -11.5518],
        [ 11.5886,  -0.6399],
        [ -2.4073,   2.5022],
        [ 10.9305,  -1.1011],
        [-10.8822,  -4.2389],
        [ -2.0658, -14.1360],
        [-12.7231, -15.6867],
        [ -6.9542,  13.1589]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  0.4989,  -2.6966,  -8.9065,  -8.8460,  -4.7509, -13.0815,  -1.7140,
        -12.1821, -11.9078,  10.0174], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.4316e+00, -8.0163e-02, -1.3372e+00,  1.3383e+00,  4.7472e-02,
          5.3591e+00, -5.0954e+00, -3.8184e+00, -5.6849e-01,  1.1931e+01],
        [-1.6041e+01,  2.3023e-01,  6.7482e-01,  1.5056e+01,  1.3720e-02,
          3.8913e-01, -8.6247e+00,  2.1303e+01,  1.2196e+01,  1.8949e+00],
        [ 4.6103e-02,  2.1854e-01, -7.0344e-01, -8.3927e-01,  1.1869e-01,
         -6.7452e-02, -2.1651e-01, -7.0616e-01,  1.4579e+00, -1.4942e+01],
        [ 4.6103e-02,  2.1854e-01, -7.0344e-01, -8.3927e-01,  1.1869e-01,
         -6.7456e-02, -2.1651e-01, -7.0616e-01,  1.4579e+00, -1.4942e+01],
        [-1.8034e+00,  1.0488e-02,  1.2783e+01, -1.0485e+01, -5.4377e-03,
         -2.9682e+00,  1.1642e+01,  2.2635e+00,  8.0089e+00, -1.1275e+01],
        [ 1.1667e+00,  5.4353e-02, -1.0531e+01,  1.3160e+01,  1.0860e-01,
          1.6945e+01, -7.7846e+00, -7.3821e+00, -1.2441e+01,  1.3120e+01],
        [ 4.6102e-02,  2.1854e-01, -7.0344e-01, -8.3927e-01,  1.1869e-01,
         -6.7454e-02, -2.1651e-01, -7.0616e-01,  1.4579e+00, -1.4942e+01],
        [-6.8960e+00, -1.9273e+00, -7.3177e-01,  2.7742e+00,  2.3447e+00,
         -5.8717e+00, -6.7361e+00,  1.0962e+01,  5.4203e+00,  3.5865e+00],
        [ 3.8843e+00, -1.8695e+00,  1.1888e+01,  2.2127e+01,  2.3513e+00,
          3.2487e-01, -9.3907e+00,  9.7626e+00,  1.5347e+01, -3.3885e+00],
        [-9.1097e-01, -1.2512e-02,  8.1464e+00,  9.9851e+00,  2.1785e-03,
         -1.4571e-01, -3.1547e+00, -5.2581e-01,  1.4280e+01,  6.9833e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  4.9417,   0.6945,  -4.2026,  -4.2026, -28.1597,  16.8323,  -4.2026,
          1.4130,   4.5673,   2.6353], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  2.4286,   0.9237,  -3.6495,  -3.6495,  17.6177,   2.9431,  -3.6495,
          -1.7485,  -1.3297,  -2.4388],
        [ -2.2031,  -0.9518,   3.6495,   3.6494, -17.6117,  -2.9042,   3.6494,
           1.7697,   1.5481,   2.5991]], device='cuda:0'))])
xi:  [188.63263]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 611.8858723596344
W_T_median: 406.18921303551235
W_T_pctile_5: 189.08929344913042
W_T_CVAR_5_pct: 23.30061021702093
Average q (qsum/M+1):  48.133808751260084
Optimal xi:  [188.63263]
Expected(across Rb) median(across samples) p_equity:  0.2473382977147897
obj fun:  tensor(-1608.6338, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
loaded xi:  188.63263
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2822.6571812897373
Current xi:  [195.3264]
objective value function right now is: -2822.6571812897373
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.37296]
objective value function right now is: -2724.229251481507
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [202.01157]
objective value function right now is: -2715.815644662386
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.52448]
objective value function right now is: -2789.665904154573
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.39418]
objective value function right now is: -2392.097468119
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -2864.9715526659124
Current xi:  [207.52687]
objective value function right now is: -2864.9715526659124
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [209.08144]
objective value function right now is: -2836.856091064415
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.07965]
objective value function right now is: -2260.068782784082
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.60345]
objective value function right now is: -2691.236552636805
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.0458]
objective value function right now is: -2721.333036635882
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.20352]
objective value function right now is: -2851.2742552751965
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.39722]
objective value function right now is: -2813.123433511631
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.42395]
objective value function right now is: -2818.5218521324305
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [211.0424]
objective value function right now is: -2707.685885123362
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.93153]
objective value function right now is: -2784.328993318607
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.53673]
objective value function right now is: -2833.1521225024853
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.64798]
objective value function right now is: -2805.8129236866766
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.484]
objective value function right now is: -2863.3485557736585
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.24432]
objective value function right now is: -2702.904587411639
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2893.7434644728864
Current xi:  [212.60416]
objective value function right now is: -2893.7434644728864
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.60365]
objective value function right now is: -2798.849067199431
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.38522]
objective value function right now is: -2845.1028907482996
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.71576]
objective value function right now is: -2887.3102155283036
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.98694]
objective value function right now is: -2874.6844792767224
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.54948]
objective value function right now is: -2693.0674483086773
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.65118]
objective value function right now is: -2364.559261959268
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.03085]
objective value function right now is: -2810.192388399553
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [212.11104]
objective value function right now is: -2758.4442075123966
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [212.57138]
objective value function right now is: -2665.1288685657746
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.7145]
objective value function right now is: -2730.001304752139
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.21875]
objective value function right now is: -2719.7055056686822
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.07526]
objective value function right now is: -2853.3425145593287
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.92413]
objective value function right now is: -2807.296101029656
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.87567]
objective value function right now is: -1601.715319265996
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.10591]
objective value function right now is: -2873.5391979764468
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2904.2553046611242
Current xi:  [211.35011]
objective value function right now is: -2904.2553046611242
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.75706]
objective value function right now is: -2895.5042138752815
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.28862]
objective value function right now is: -2898.550307465139
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.371]
objective value function right now is: -2897.601115954301
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.5111]
objective value function right now is: -2899.7582521036015
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.61353]
objective value function right now is: -2838.5194102994974
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.74486]
objective value function right now is: -2791.1094643426395
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.597]
objective value function right now is: -2860.6688843513484
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.8462]
objective value function right now is: -2898.357545515396
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.91609]
objective value function right now is: -2885.2093230912706
new min fval from sgd:  -2908.778653589718
new min fval from sgd:  -2911.8195486321656
new min fval from sgd:  -2912.041903247403
new min fval from sgd:  -2913.9441491438192
new min fval from sgd:  -2915.109164824665
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.07274]
objective value function right now is: -2897.833004762979
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.38547]
objective value function right now is: -2861.0299661787203
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.48604]
objective value function right now is: -2900.3431449415725
new min fval from sgd:  -2915.153454595099
new min fval from sgd:  -2915.167109115824
new min fval from sgd:  -2915.1763315313765
new min fval from sgd:  -2915.2142168730625
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.58101]
objective value function right now is: -2911.4563565879025
new min fval from sgd:  -2915.382076942339
new min fval from sgd:  -2915.661994659804
new min fval from sgd:  -2915.736398665165
new min fval from sgd:  -2915.768196439686
new min fval from sgd:  -2915.8083503327653
new min fval from sgd:  -2915.8116164608973
new min fval from sgd:  -2915.8210442147715
new min fval from sgd:  -2915.82524951161
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.55911]
objective value function right now is: -2915.386290574367
min fval:  -2915.82524951161
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  9.2423,   4.8632],
        [-10.5476,  10.3758],
        [ -7.0425,  -6.8430],
        [ -7.0423,  -6.8266],
        [  2.0162,  -0.5519],
        [-16.8810,   2.0103],
        [ -7.0422,  -6.8258],
        [ 27.5089,   6.4694],
        [  0.4764,   4.5518],
        [-11.2210,   9.1460]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-4.6000,  7.5847, -4.9445, -4.9566,  6.8211, 10.8399, -4.9571,  5.9884,
        -9.7423,  6.8800], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.1172, -0.3395, -0.1385, -0.1365, -0.8554, -0.7782, -0.1364, -0.8243,
         -0.0606, -0.3254],
        [ 8.9247, -9.7263,  3.2014,  3.1865,  5.8534, -9.0801,  3.1858, -4.1285,
          3.1824, -8.0611],
        [-0.1172, -0.3395, -0.1385, -0.1365, -0.8554, -0.7782, -0.1364, -0.8243,
         -0.0606, -0.3254],
        [-0.1172, -0.3395, -0.1385, -0.1365, -0.8554, -0.7782, -0.1364, -0.8243,
         -0.0606, -0.3254],
        [-0.1172, -0.3395, -0.1385, -0.1365, -0.8554, -0.7782, -0.1364, -0.8243,
         -0.0606, -0.3254],
        [-6.5644,  8.1878, -3.5762, -3.5571, -5.9343,  9.1638, -3.5562,  1.9960,
         -3.0628,  7.8157],
        [ 8.3825, -9.4461,  3.0857,  3.0714,  5.5049, -8.5431,  3.0707, -4.3626,
          3.1368, -7.3850],
        [-0.1172, -0.3395, -0.1385, -0.1365, -0.8554, -0.7782, -0.1364, -0.8243,
         -0.0606, -0.3254],
        [-0.1172, -0.3395, -0.1385, -0.1365, -0.8554, -0.7782, -0.1364, -0.8243,
         -0.0606, -0.3254],
        [-7.8068,  9.0677, -3.1735, -3.1562, -5.7990,  9.0338, -3.1554,  2.3618,
         -3.3158,  8.4341]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8558,  9.7633, -0.8558, -0.8558, -0.8558, -8.9607,  8.9638, -0.8558,
        -0.8558, -9.1614], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0419, -16.8034,  -0.0419,  -0.0419,  -0.0419,   8.0343, -12.8825,
          -0.0419,  -0.0419,  10.1412]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-22.4975,   1.9834],
        [  0.9446,   5.6770],
        [-15.2468, -11.9582],
        [ 11.9803,  -0.5467],
        [ -4.2227,   4.2083],
        [ 11.4335,  -1.3631],
        [-11.9876,  -4.6297],
        [ -1.9619, -14.5081],
        [-13.8516, -15.7442],
        [ -6.2315,  13.8134]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  1.0292,  -1.3571,  -9.4495,  -9.3602,  -6.2353, -13.8806,  -1.7228,
        -12.5593, -11.5105,  10.5228], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -3.7312,  -1.1694,  -0.8778,   1.4598,   0.1701,   4.7401,  -4.9204,
          -3.5035,  -0.7894,   9.1188],
        [-22.7119,  -6.4291,   2.3026,  13.0741,   2.8086,   0.2363, -10.0599,
          25.6528,  14.4456,   0.9463],
        [  1.9276,  -1.4112,   0.5814,  -1.4226,   0.0967,  -0.9701,   0.5987,
          -1.1969,   1.6903, -17.7342],
        [  1.9277,  -1.4112,   0.5814,  -1.4226,   0.0967,  -0.9702,   0.5987,
          -1.1969,   1.6903, -17.7343],
        [  2.9387,  -2.6746,  12.7678, -17.7321,  -0.1129,  -1.7939,  12.5417,
           4.2560,   7.7823, -20.4132],
        [  1.3320,   1.5447, -13.0325,  15.0116,   0.1629,  16.2614,  -8.9810,
          -8.5337, -10.9325,  15.4017],
        [  1.9276,  -1.4112,   0.5814,  -1.4226,   0.0967,  -0.9702,   0.5987,
          -1.1969,   1.6903, -17.7342],
        [ -6.8634,  -1.3752,  -0.8223,   2.1835,   2.3049,  -3.6229,  -5.5665,
          12.0434,   5.3168,   0.1343],
        [  7.4909,  -4.7923,   9.0421,  16.1816,   1.9539,  -0.0631,  -4.6264,
           1.5038,  12.6001,  -2.0699],
        [ -1.2139,  -0.2786,  15.7258,  10.0418,   0.1824,   0.8971,  -2.4326,
          -1.4747,  18.9847,   6.9745]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([  4.4435,   1.6857,  -4.1701,  -4.1701, -30.7996,  16.5058,  -4.1701,
          1.5784,   5.6621,   2.8311], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  2.1715,   0.8736,  -4.5391,  -4.5392,  24.5982,   3.0422,  -4.5392,
          -1.5213,  -1.3478,  -2.4572],
        [ -1.9459,  -0.9017,   4.5394,   4.5393, -24.5921,  -3.0033,   4.5393,
           1.5426,   1.5663,   2.6176]], device='cuda:0'))])
xi:  [213.5852]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 697.3108628204974
W_T_median: 504.67196417601735
W_T_pctile_5: 215.00304461043834
W_T_CVAR_5_pct: 29.973870096329055
Average q (qsum/M+1):  45.73348506804435
Optimal xi:  [213.5852]
Expected(across Rb) median(across samples) p_equity:  0.22275668544073898
obj fun:  tensor(-2915.8252, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
