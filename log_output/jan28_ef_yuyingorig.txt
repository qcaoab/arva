Starting at: 
28-01-23_09:51

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 40000
batchsize: 1000
remove neg:  False


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 4000, 'itbound_SGD_algorithms': 40000, 'nit_IterateAveragingStart': 36000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.02, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1693.5723070172053
Current xi:  [-15.0570345]
objective value function right now is: -1693.5723070172053
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1706.7402397409617
Current xi:  [-30.206987]
objective value function right now is: -1706.7402397409617
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1713.3318185008895
Current xi:  [-44.04033]
objective value function right now is: -1713.3318185008895
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1715.435122114291
Current xi:  [-58.915966]
objective value function right now is: -1715.435122114291
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1717.7450923918689
Current xi:  [-73.577225]
objective value function right now is: -1717.7450923918689
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-88.32748]
objective value function right now is: -1699.5553308415706
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1719.6556663460542
Current xi:  [-103.41538]
objective value function right now is: -1719.6556663460542
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1724.232171481252
Current xi:  [-117.6615]
objective value function right now is: -1724.232171481252
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.4336650232053
Current xi:  [-132.51826]
objective value function right now is: -1725.4336650232053
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1726.4356091090865
Current xi:  [-147.4341]
objective value function right now is: -1726.4356091090865
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-160.76971]
objective value function right now is: -1722.5373766238545
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-175.3555]
objective value function right now is: -1726.1265949611468
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-190.17892]
objective value function right now is: -1683.3783358679045
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-204.18263]
objective value function right now is: -1725.5056248539133
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1733.255282642856
Current xi:  [-217.79645]
objective value function right now is: -1733.255282642856
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-236.58784]
objective value function right now is: -1676.9179954542276
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-250.77484]
objective value function right now is: -1725.2735397821205
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-263.36166]
objective value function right now is: -1724.5683057686267
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-275.60864]
objective value function right now is: -1733.0755172973488
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-289.06335]
objective value function right now is: -1719.8881981013162
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-303.73587]
objective value function right now is: -1730.4450614698897
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-317.07925]
objective value function right now is: -1725.5659473888566
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-329.0831]
objective value function right now is: -1732.6167889702588
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-344.65744]
objective value function right now is: -1731.9013923272973
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-358.55545]
objective value function right now is: -1692.3706847745586
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1735.6018815164805
Current xi:  [-366.38806]
objective value function right now is: -1735.6018815164805
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1738.062949435447
Current xi:  [-378.29025]
objective value function right now is: -1738.062949435447
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-389.3587]
objective value function right now is: -1715.7934813375985
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-405.88547]
objective value function right now is: -1736.0103725979563
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-418.6761]
objective value function right now is: -1733.805143087075
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-432.26718]
objective value function right now is: -1722.047137448434
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-443.86935]
objective value function right now is: -1713.2792026838504
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-461.94272]
objective value function right now is: -1673.5188817672376
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-475.53757]
objective value function right now is: -1684.7252214288492
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-488.06918]
objective value function right now is: -1726.9263753924488
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-490.15628]
objective value function right now is: -1683.8973630811806
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.17233]
objective value function right now is: -1689.2122246378551
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-493.9661]
objective value function right now is: -1694.3631723000076
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-496.85916]
objective value function right now is: -1689.944845001894
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-499.04996]
objective value function right now is: -1691.7341245246464
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-501.1771]
objective value function right now is: -1721.3449152361138
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-502.47092]
objective value function right now is: -1710.2448547385052
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-503.13266]
objective value function right now is: -1706.1848844559142
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-505.9195]
objective value function right now is: -1694.1370102157018
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-508.86227]
objective value function right now is: -1695.279555829166
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-512.2194]
objective value function right now is: -1694.9032264161765
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-513.2346]
objective value function right now is: -1719.044018402248
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-515.3328]
objective value function right now is: -1716.7219983267562
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-516.7374]
objective value function right now is: -1717.6940756899962
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-517.16986]
objective value function right now is: -1717.2591149133664
min fval:  -1739.60185839046
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 1.4727,  3.9783],
        [ 3.5335,  4.3987],
        [-3.3107, -3.4161],
        [-2.8401, -1.8977],
        [-3.5760, -3.9241],
        [-2.6524, -1.6607],
        [ 3.9454,  4.0048],
        [11.5273, -1.8721],
        [-3.1559, -3.1518],
        [-3.4723, -3.6969]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-0.9321,  2.9823, -4.5442, -5.5684, -4.4670, -5.6759,  1.9458, -9.3657,
        -4.7035, -4.4437], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.6608, -5.1040,  1.8972,  0.6153,  1.9227,  0.5469, -3.2169,  5.3805,
          1.3017,  2.4622],
        [ 0.0705,  2.2835, -1.1379, -0.2145, -1.6535, -0.1844,  1.6147, -5.0127,
         -0.9726, -0.9421],
        [-0.5227, -4.9544,  1.9523,  0.4773,  2.4647,  0.5061, -3.5995,  5.7378,
          1.7698,  2.3236],
        [-0.6119, -5.1636,  1.4500,  0.4553,  1.7262,  0.1734, -3.6358,  5.1472,
          1.0035,  1.9788],
        [-0.5304, -4.9837,  1.8100,  0.6413,  2.6533,  0.5752, -3.7051,  5.9214,
          1.5858,  2.4658],
        [-0.1661,  2.9191, -0.7651,  0.3143, -1.5832,  0.2120,  1.9503, -5.8061,
         -0.7458, -0.9827],
        [-0.3146, -5.0304,  1.6746,  0.5505,  2.2728,  0.5531, -3.2930,  5.6557,
          1.5945,  2.4578],
        [-0.6729, -4.9524,  2.0375,  0.8404,  2.2329,  0.6764, -3.7097,  5.7535,
          1.5760,  2.3346],
        [-0.6085, -5.1706,  1.8645,  0.7110,  2.4219,  0.7746, -3.7661,  5.9667,
          1.5394,  2.4077],
        [-0.7370, -4.8999,  1.0353,  0.2830,  1.7175,  0.3252, -3.4785,  5.0520,
          1.1840,  1.7020]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-3.0728,  1.1553, -3.0596, -2.7765, -2.9655,  1.5819, -3.0366, -2.9584,
        -2.8228, -2.8054], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-3.5219,  3.8786, -3.6942, -2.9591, -3.9765,  6.3082, -3.5212, -3.8046,
         -3.9200, -2.6658]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  1.4958,  -4.5809],
        [-10.7886,  -2.9404],
        [  0.9845,  -6.0069],
        [  5.6886,   6.2721],
        [ -6.1954,   0.1917],
        [  4.9858,   5.3411],
        [  4.9373,  -3.6532],
        [ -2.1929,  -4.5211],
        [  5.1391,   5.5006],
        [ -5.9840,   0.3450]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-4.3790, -1.0782, -5.2450,  4.9251, -2.2369,  2.5517, -3.3129, -3.5417,
         4.0467,  3.2960], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.1979e+00,  7.4802e-01, -1.1600e+00, -1.1825e+01, -2.5207e+00,
         -8.4584e+00, -5.7258e-01, -1.0397e+00, -1.0240e+01,  1.3341e+01],
        [ 2.3117e+00, -9.6983e-01,  3.8595e+00, -8.2852e-01,  6.3016e-01,
         -4.2670e+00,  1.1036e+00,  2.4596e+00, -2.0097e+00, -4.6135e+00],
        [-7.1457e-01, -1.9796e+00, -3.9609e-01, -1.5147e+00, -3.4858e-01,
         -1.6107e+00, -1.4853e+00, -1.3167e+00, -1.6678e+00, -1.4934e+00],
        [-2.6087e+00,  1.4517e+00, -3.5047e+00, -6.9399e-01,  1.0266e+00,
          3.4786e-01, -3.4016e+00, -1.0676e+00, -2.2401e-02,  6.8810e+00],
        [-1.1861e+00, -2.8042e+00, -1.4324e+00, -2.1565e+00, -1.4112e+00,
         -2.1287e+00, -9.1455e-01, -1.3409e+00, -1.8140e+00,  9.1069e-01],
        [-3.4921e+00,  3.6449e+00, -3.5280e+00,  6.9223e-01,  4.8926e+00,
          3.9450e-01, -4.0983e+00, -9.3277e-01,  3.7405e-03, -6.5176e-01],
        [-6.7458e-01,  1.7135e-01, -1.6273e+00, -7.4709e-02, -1.3206e+00,
          2.5918e+00, -1.0783e+00, -5.8151e-01,  1.1782e+00,  8.5373e+00],
        [ 5.6451e-01,  7.5791e+00,  4.0214e-02, -1.2060e+01,  5.2777e+00,
         -9.5034e+00,  5.9034e-01, -6.1057e-01, -1.0993e+01,  5.9091e+00],
        [-7.4615e-01, -2.0229e+00, -1.0770e+00,  1.6116e-02, -1.3342e+00,
          3.0420e-01, -2.6038e-01, -1.9400e+00,  1.2364e-01, -3.7150e-02],
        [ 2.0349e+00,  1.1486e+00,  3.6870e+00, -6.9887e+00,  1.0167e+00,
         -7.6111e+00,  2.1880e+00,  1.5669e+00, -6.3720e+00,  1.0600e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.8285, -0.1103, -3.3332, -1.3706,  0.1502, -1.6606, -0.4123, -2.3756,
        -1.1634, -0.1540], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 5.6070, -0.7980, -0.0929,  0.3438, -0.0485, -0.2106,  0.4127,  3.8070,
         -0.5136, -3.6364],
        [-6.0064,  0.6323,  0.2837, -0.1288,  0.3313, -0.2119, -0.1462, -3.6061,
          0.4911,  3.2262]], device='cuda:0'))])
xi:  [-508.86227]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 540.1033352334325
W_T_median: 142.48425461327645
W_T_pctile_5: -461.81829758729623
W_T_CVAR_5_pct: -579.7518835889587
Average q (qsum/M+1):  57.067079605594756
Optimal xi:  [-508.86227]
Expected(across Rb) median(across samples) p_equity:  0.38276708448926605
obj fun:  tensor(-1739.6019, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1649.4975606166302
Current xi:  [-11.13978]
objective value function right now is: -1649.4975606166302
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1654.5663859426002
Current xi:  [-23.432669]
objective value function right now is: -1654.5663859426002
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1656.3189575831987
Current xi:  [-35.476643]
objective value function right now is: -1656.3189575831987
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1660.0785103376322
Current xi:  [-46.078583]
objective value function right now is: -1660.0785103376322
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1661.372014899361
Current xi:  [-55.038937]
objective value function right now is: -1661.372014899361
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1663.6486983696564
Current xi:  [-65.70652]
objective value function right now is: -1663.6486983696564
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1665.5221685367096
Current xi:  [-76.307625]
objective value function right now is: -1665.5221685367096
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1667.315009425804
Current xi:  [-87.46162]
objective value function right now is: -1667.315009425804
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-97.68522]
objective value function right now is: -1666.344822612065
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1669.4969981647703
Current xi:  [-108.39382]
objective value function right now is: -1669.4969981647703
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1670.1175487427763
Current xi:  [-119.35186]
objective value function right now is: -1670.1175487427763
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1670.67125046459
Current xi:  [-131.51477]
objective value function right now is: -1670.67125046459
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-143.44762]
objective value function right now is: -1668.618455317665
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-151.35558]
objective value function right now is: -1631.3924193861174
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.9884687439435
Current xi:  [-163.44382]
objective value function right now is: -1672.9884687439435
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-174.50128]
objective value function right now is: -1671.7481502880842
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1674.197417003253
Current xi:  [-185.13426]
objective value function right now is: -1674.197417003253
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-196.66667]
objective value function right now is: -1666.1555558532025
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-198.68839]
objective value function right now is: -1656.156134190415
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-210.98737]
objective value function right now is: -1656.4707711468363
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-220.63336]
objective value function right now is: -1655.364051058197
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-230.79395]
objective value function right now is: -1655.8695998675005
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-242.14633]
objective value function right now is: -1651.9453779975831
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-250.40378]
objective value function right now is: -1651.525325363437
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-260.39532]
objective value function right now is: -1651.266796265518
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-266.37338]
objective value function right now is: -1645.791000434418
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-274.68973]
objective value function right now is: -1642.9852802521893
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-283.29965]
objective value function right now is: -1646.281908537597
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-285.23267]
objective value function right now is: -1646.6545730731573
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-289.9128]
objective value function right now is: -1646.17018333226
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-297.23566]
objective value function right now is: -1646.9350610034617
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-302.16537]
objective value function right now is: -1658.2679846859594
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-311.1521]
objective value function right now is: -1651.8238763800773
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-315.6413]
objective value function right now is: -1654.0598235218256
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-317.5179]
objective value function right now is: -1656.0837188448063
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-318.20273]
objective value function right now is: -1656.7399796632471
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-318.53165]
objective value function right now is: -1649.789392206199
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-316.91977]
objective value function right now is: -1646.499984475546
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-314.96777]
objective value function right now is: -1647.2719186906745
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-315.96475]
objective value function right now is: -1650.7562431653216
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-317.4026]
objective value function right now is: -1648.6428356236358
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-318.82697]
objective value function right now is: -1661.5210712257954
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-321.55106]
objective value function right now is: -1660.0656261232286
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-322.01126]
objective value function right now is: -1661.1574919016637
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-320.51346]
objective value function right now is: -1661.444642210455
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-321.68777]
objective value function right now is: -1661.9274077786815
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-322.23453]
objective value function right now is: -1661.2280107481433
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-323.61188]
objective value function right now is: -1659.3116701247013
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-324.10107]
objective value function right now is: -1659.2902023923696
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-324.28992]
objective value function right now is: -1659.3473510457707
min fval:  -1659.601559050402
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 4.3840,  0.9974],
        [ 3.1602,  5.1405],
        [ 2.3837, -4.4017],
        [-0.8055,  2.3656],
        [ 0.9770, -4.7746],
        [-0.8974,  2.1900],
        [ 7.2839,  4.4960],
        [ 5.4261, -5.4271],
        [ 3.4918, -4.2249],
        [ 0.5413, -4.5420]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 0.5570,  3.0862, -5.2525, -3.6723, -5.2597, -3.7703,  2.0697, -8.0077,
        -5.5008, -5.1557], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 1.3615, -5.7661,  2.2964,  0.3763,  2.2416,  0.4225, -4.1777,  4.8790,
          2.3521,  2.4609],
        [ 0.9737,  4.2074, -0.8091, -0.0476, -1.1564,  0.1882,  3.8678, -5.0368,
         -1.2725, -0.3804],
        [ 1.1193, -5.7297,  2.2709, -0.1382,  2.5693,  0.0559, -4.5856,  5.4586,
          2.5891,  2.2698],
        [ 1.1717, -5.5671,  1.8708,  0.2960,  1.9235,  0.3148, -4.2481,  4.7132,
          2.0518,  1.8911],
        [ 1.1164, -5.8101,  2.2151, -0.0928,  2.7688,  0.0760, -4.7380,  5.5778,
          2.5287,  2.4295],
        [ 0.8457,  5.1846, -1.7024,  0.3246, -2.3311,  0.5280,  4.3812, -4.9758,
         -2.1619, -1.5176],
        [ 1.3361, -5.8013,  2.0822,  0.1609,  2.4500,  0.2576, -4.3177,  5.1512,
          2.4923,  2.4203],
        [ 1.2681, -5.7813,  2.3858,  0.2223,  2.4626,  0.2925, -4.7219,  5.2887,
          2.5125,  2.3769],
        [ 1.1769, -6.0198,  2.2758,  0.1493,  2.6190,  0.2521, -4.8077,  5.4754,
          2.5118,  2.4344],
        [ 0.9386, -5.2392,  1.2926,  0.1836,  1.5340,  0.1649, -4.0024,  4.4412,
          1.8414,  1.3103]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-3.4842,  2.4463, -3.6686, -3.3178, -3.5480,  3.0252, -3.4848, -3.4220,
        -3.3268, -3.5999], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-3.6185,  7.0368, -4.2563, -2.8315, -4.5635,  9.9760, -3.7479, -4.2082,
         -4.4609, -2.1966]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  5.1196,  -3.4137],
        [-13.9429,  -1.1456],
        [ -0.1598,  -7.6715],
        [  5.3520,   7.3456],
        [ -7.2603,  -0.5531],
        [  8.7875,   3.3793],
        [  4.2297,  -1.3128],
        [ -3.5116,  -6.1022],
        [  6.1663,   6.3653],
        [ -7.0494,   0.3435]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-4.4600, -0.4380, -7.4262,  6.6261,  3.8460,  2.6929, -5.6927, -5.3321,
         5.3606,  3.0124], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -0.5557,  -0.8795,   0.6205, -14.8713,  -1.4590,  -2.1455,  -2.9623,
           0.7732, -12.8277,  18.2180],
        [  0.0601,  -3.6026,   2.0616,   0.0566,  -0.7497,  -3.4070,  -2.7779,
           0.0369,  -0.3874,   2.5396],
        [ -1.2781,  -0.8873,  -0.8744,  -1.5879,  -1.1787,  -1.3896,  -1.6534,
          -1.2243,  -1.5370,  -0.3171],
        [ -0.7483,  -0.8477,  -3.7367,  -0.6985,   1.6096,  -1.3624,  -2.0968,
          -3.4183,  -0.7258,   3.2464],
        [ -3.3299,   0.2723,  -8.2033,   1.1067,   3.3772,  -0.1994,  -0.9676,
          -6.5784,   1.0030,   3.9390],
        [ -5.9737,   1.1367, -11.0533,   0.5708,   4.3495,  -0.9225,  -3.8253,
          -5.7813,  -0.2513,   3.2991],
        [ -1.6303,   3.2851,   1.3137,  -2.1942,   2.9190,   0.1702,  -1.4278,
           1.1469,  -0.6641,   2.4181],
        [  1.8195,   8.4566,   1.5654,  -9.9174,   4.7294,  -9.1698,   0.9690,
           1.2098, -10.5786,   7.9978],
        [ -1.4387,  -0.4104,  -1.2434,  -1.2941,  -1.2373,  -1.3083,  -0.9428,
          -1.0759,  -1.2734,  -0.6647],
        [  2.3115,   1.8697,   4.4921,  -9.1270,  10.3528,  -8.4735,  -4.6628,
           0.9959,  -7.7618,   1.3051]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.8645, -2.7240, -3.4192, -2.6833, -0.9068, -2.9504, -1.7484, -2.4670,
        -2.5735, -1.2190], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 10.8143,  -0.6808,  -0.1741,   0.5439,   0.8938,  -0.0117,  -0.2175,
           4.1082,   0.0173,  -3.8431],
        [-10.9895,   0.5292,   0.2579,  -0.3664,  -0.6137,  -0.4052,   0.4836,
          -3.9080,  -0.0287,   3.4342]], device='cuda:0'))])
xi:  [-320.51346]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 241.61344122084822
W_T_median: 40.49285887023207
W_T_pctile_5: -190.35644764495694
W_T_CVAR_5_pct: -280.4132406911865
Average q (qsum/M+1):  55.81925718245968
Optimal xi:  [-320.51346]
Expected(across Rb) median(across samples) p_equity:  0.3203963965177536
obj fun:  tensor(-1659.6016, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1600.1896948201736
Current xi:  [-6.457377]
objective value function right now is: -1600.1896948201736
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1601.3843466037947
Current xi:  [-14.273817]
objective value function right now is: -1601.3843466037947
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1603.6273208893142
Current xi:  [-21.863943]
objective value function right now is: -1603.6273208893142
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.4216619251501
Current xi:  [-28.601727]
objective value function right now is: -1604.4216619251501
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-33.97846]
objective value function right now is: -1603.4205787185012
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1607.5495053426876
Current xi:  [-38.363235]
objective value function right now is: -1607.5495053426876
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-42.33141]
objective value function right now is: -1605.0652190007715
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-46.152813]
objective value function right now is: -1606.0721802589046
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1608.362479997467
Current xi:  [-49.84335]
objective value function right now is: -1608.362479997467
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-52.46067]
objective value function right now is: -1605.5135192514294
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1608.7887241763215
Current xi:  [-54.327087]
objective value function right now is: -1608.7887241763215
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1608.9930357892233
Current xi:  [-55.40982]
objective value function right now is: -1608.9930357892233
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1609.242475492726
Current xi:  [-55.73366]
objective value function right now is: -1609.242475492726
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-56.052597]
objective value function right now is: -1608.903486459503
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.198902]
objective value function right now is: -1608.4490282548163
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.266937]
objective value function right now is: -1609.1838976432362
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1609.575313529633
Current xi:  [-56.490303]
objective value function right now is: -1609.575313529633
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.49513]
objective value function right now is: -1608.607248318868
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.3826]
objective value function right now is: -1609.3806004997925
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.486942]
objective value function right now is: -1607.5883072887805
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1609.6891080620449
Current xi:  [-56.42929]
objective value function right now is: -1609.6891080620449
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.71085]
objective value function right now is: -1609.5997684128392
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.688004]
objective value function right now is: -1609.2081081682913
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.59848]
objective value function right now is: -1609.3148811096414
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.664986]
objective value function right now is: -1608.8061002214554
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.70832]
objective value function right now is: -1608.9561990425707
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.802235]
objective value function right now is: -1609.2858742227243
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-56.909958]
objective value function right now is: -1608.5618416202258
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-56.824062]
objective value function right now is: -1609.2963415013073
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.8286]
objective value function right now is: -1609.4412958005805
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.970787]
objective value function right now is: -1609.1778202541173
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.934696]
objective value function right now is: -1609.419685879509
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.811646]
objective value function right now is: -1609.5854620529415
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-57.15712]
objective value function right now is: -1609.6664840993399
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.76216]
objective value function right now is: -1609.0875979507232
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1610.220002996293
Current xi:  [-57.251823]
objective value function right now is: -1610.220002996293
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1610.3479440371204
Current xi:  [-57.448612]
objective value function right now is: -1610.3479440371204
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1610.350015739982
Current xi:  [-57.602722]
objective value function right now is: -1610.350015739982
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-57.637905]
objective value function right now is: -1610.255261311118
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-57.68752]
objective value function right now is: -1610.1888024125262
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-57.686863]
objective value function right now is: -1610.2961401629225
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1610.5084798598994
Current xi:  [-57.692146]
objective value function right now is: -1610.5084798598994
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-57.726227]
objective value function right now is: -1610.419098574363
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-57.79095]
objective value function right now is: -1610.316843278254
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-57.84384]
objective value function right now is: -1610.484224293583
new min fval from sgd:  -1610.5084880370564
new min fval from sgd:  -1610.5203437403795
new min fval from sgd:  -1610.5249527897097
new min fval from sgd:  -1610.5254330911362
new min fval from sgd:  -1610.5260863464416
new min fval from sgd:  -1610.528643159093
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-57.86732]
objective value function right now is: -1610.4258433090818
new min fval from sgd:  -1610.531870741529
new min fval from sgd:  -1610.5323156009551
new min fval from sgd:  -1610.5380520102915
new min fval from sgd:  -1610.5399590757208
new min fval from sgd:  -1610.541290976532
new min fval from sgd:  -1610.543125308592
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-57.878773]
objective value function right now is: -1610.543125308592
new min fval from sgd:  -1610.544744700891
new min fval from sgd:  -1610.5461215639905
new min fval from sgd:  -1610.5464198473396
new min fval from sgd:  -1610.5495533387284
new min fval from sgd:  -1610.551703412651
new min fval from sgd:  -1610.5634975366509
new min fval from sgd:  -1610.5707424672314
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-57.974865]
objective value function right now is: -1610.5062315505995
new min fval from sgd:  -1610.5808101063808
new min fval from sgd:  -1610.5809781769972
new min fval from sgd:  -1610.5814007572228
new min fval from sgd:  -1610.5825581716313
new min fval from sgd:  -1610.5842476689695
new min fval from sgd:  -1610.5848146369462
new min fval from sgd:  -1610.5850927623032
new min fval from sgd:  -1610.586671394404
new min fval from sgd:  -1610.5873341659405
new min fval from sgd:  -1610.5886329756995
new min fval from sgd:  -1610.589873178263
new min fval from sgd:  -1610.5900339683988
new min fval from sgd:  -1610.590046538627
new min fval from sgd:  -1610.592607989124
new min fval from sgd:  -1610.5956573019716
new min fval from sgd:  -1610.5969959112444
new min fval from sgd:  -1610.5973151178337
new min fval from sgd:  -1610.597374777193
new min fval from sgd:  -1610.5980338843754
new min fval from sgd:  -1610.5993885629323
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-57.960815]
objective value function right now is: -1610.5828584928993
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-57.98734]
objective value function right now is: -1610.5851546650863
min fval:  -1610.5993885629323
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 4.8395,  5.0149],
        [ 4.8354,  5.5406],
        [ 4.1694, -4.8254],
        [ 4.5214,  3.5861],
        [ 3.8542, -5.5265],
        [ 4.5215,  3.5861],
        [13.3450,  4.7886],
        [ 5.7590, -7.6069],
        [ 4.9007, -4.1834],
        [ 0.5301, -4.9695]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-2.9493,  4.0821, -6.0910, -2.6115, -5.6087, -2.6116,  2.7491, -7.5216,
        -6.7716, -5.8483], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.5677, -6.5712,  2.3592,  0.8097,  2.4482,  0.8097, -2.9043,  6.1345,
          2.5633,  2.2008],
        [-0.8740,  3.6593, -2.6888, -0.9993, -2.6901, -0.9993, -1.3586, -8.7663,
         -3.2796, -1.1205],
        [ 4.9327, -5.8870,  2.1743,  4.0163,  2.6327,  4.0162, -3.3094,  6.4115,
          3.0907,  1.8463],
        [-0.0789, -4.6728,  1.0775,  0.0154,  1.3470,  0.0154, -1.1277,  4.1611,
          0.8152,  0.8469],
        [ 5.0174, -6.3461,  1.9476,  3.2252,  2.6669,  3.2252, -3.5078,  6.1874,
          2.6535,  2.3364],
        [-1.0994,  4.2067, -3.1461, -1.2657, -4.2692, -1.2655, -1.7691, -6.9652,
         -3.4389, -2.4970],
        [ 0.8355, -6.6547,  2.3335,  0.9224,  2.7239,  0.9226, -3.6234,  6.6048,
          2.9229,  2.2789],
        [ 1.3773, -6.5334,  2.5176,  0.9116,  2.4670,  0.9115, -3.5900,  6.5834,
          3.0473,  2.0927],
        [ 1.3978, -6.9691,  2.6018,  1.0982,  2.9910,  1.0985, -4.7655,  7.3058,
          3.2213,  2.4786],
        [-0.0430, -0.3021, -0.1446, -0.0598, -0.2919, -0.0598, -0.2595, -0.3129,
         -0.0680, -0.1187]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-3.2979,  2.4674, -3.1257, -3.3693, -3.1759,  2.6600, -3.2268, -3.3117,
        -2.9360, -0.7582], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-3.8278e+00,  7.5741e+00, -4.4041e+00, -1.8397e+00, -3.9782e+00,
          8.7337e+00, -4.2544e+00, -4.2814e+00, -5.3381e+00,  8.9061e-04]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  5.4417,  -6.9911],
        [ -1.5320,   4.2140],
        [  0.0302, -12.5782],
        [  3.0792,  10.1435],
        [ -7.4693,   0.1464],
        [  8.6225,   3.0009],
        [  7.6204,  -0.6290],
        [ -7.6588, -10.4333],
        [ 10.8863,   7.8170],
        [ -9.4421,  -0.0631]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -7.1990,  -1.3337, -12.5016,   9.0110,   5.9151,  -0.7861,  -7.7374,
         -9.6081,   6.0395,   2.5490], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -0.4850,  -0.1101,   1.5717, -20.9130,   5.6932,  -5.4646,  -7.0962,
           2.0122, -20.4703,  13.6308],
        [ -1.4578,  -0.1573,   0.8437,  -2.0672,   0.7223,  -0.8460,  -2.8216,
          -1.2222,  -1.6926,   2.2510],
        [  0.3100,  -0.6659,   2.6966,   0.7205,  -1.2555,   0.1254,   1.2510,
          -2.1422,   0.6007,   2.6566],
        [ -1.4752,  -0.1544,   0.8110,  -2.0622,   0.6976,  -0.8487,  -2.7873,
          -1.2309,  -1.6976,   2.1974],
        [ -1.4717,  -0.1550,   0.8177,  -2.0633,   0.7027,  -0.8481,  -2.7944,
          -1.2291,  -1.6966,   2.2084],
        [ -3.0502,  -0.1193,  -8.0618,   1.9123,   4.8510,  -2.0503,  -1.2136,
         -10.5568,   0.2935,   7.9350],
        [ -0.8903,  -0.0816,   3.9199, -11.7090,   3.5453,  -0.7255,  -7.1347,
           1.2700,  -3.9166,   2.4064],
        [ -1.4740,  -0.1546,   0.8133,  -2.0626,   0.6993,  -0.8485,  -2.7897,
          -1.2303,  -1.6973,   2.2012],
        [ -2.8874,   0.4794, -12.5645,   1.9055,   2.9385,  -2.1286,  -4.0765,
         -15.9787,   0.3783,   7.9400],
        [  1.8313,  -0.1389,   4.8903, -16.0790,  10.2787,  -7.0441,  -9.6386,
           0.7068, -13.5770,   2.3419]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-5.0007, -1.9307, -1.5742, -1.9480, -1.9444, -4.1677, -3.4019, -1.9467,
        -2.0629, -4.0969], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 13.3760,   1.3164,   0.2694,   1.2868,   1.3022,  -1.6989,  -3.4213,
           1.2882,   2.0640,  -4.9858],
        [-13.4040,  -1.3299,  -0.2253,  -1.2831,  -1.2834,   1.3002,   3.6593,
          -1.2871,  -2.0725,   4.6279]], device='cuda:0'))])
xi:  [-57.96695]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 311.67624749388824
W_T_median: 76.17660148869285
W_T_pctile_5: -57.96986116448923
W_T_CVAR_5_pct: -149.53812082967423
Average q (qsum/M+1):  54.366714969758064
Optimal xi:  [-57.96695]
Expected(across Rb) median(across samples) p_equity:  0.28234949012597405
obj fun:  tensor(-1610.5994, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1557.899080823246
Current xi:  [2.0836854]
objective value function right now is: -1557.899080823246
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1559.1890434134668
Current xi:  [6.587487]
objective value function right now is: -1559.1890434134668
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.1769180144342
Current xi:  [10.258238]
objective value function right now is: -1562.1769180144342
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.155512]
objective value function right now is: -1560.267236550425
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1563.4310914423402
Current xi:  [15.855779]
objective value function right now is: -1563.4310914423402
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [18.613115]
objective value function right now is: -1560.8658506030245
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [20.963202]
objective value function right now is: -1561.063596256115
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1563.6150947312308
Current xi:  [23.235428]
objective value function right now is: -1563.6150947312308
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1563.6993321090324
Current xi:  [25.650465]
objective value function right now is: -1563.6993321090324
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1564.4648419674938
Current xi:  [27.772608]
objective value function right now is: -1564.4648419674938
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [29.78049]
objective value function right now is: -1561.434759379684
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1565.2689268254978
Current xi:  [31.611881]
objective value function right now is: -1565.2689268254978
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [33.632275]
objective value function right now is: -1564.9494544226127
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [35.557472]
objective value function right now is: -1561.79807070817
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [37.23897]
objective value function right now is: -1564.6359444235952
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [38.56371]
objective value function right now is: -1564.779094878653
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [40.14484]
objective value function right now is: -1564.9742674579988
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [41.540905]
objective value function right now is: -1563.6975118891376
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1565.9502965936708
Current xi:  [42.971832]
objective value function right now is: -1565.9502965936708
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1566.4262548656575
Current xi:  [44.191315]
objective value function right now is: -1566.4262548656575
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.553093]
objective value function right now is: -1565.2373360202848
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [46.472446]
objective value function right now is: -1566.0409323549827
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [47.229183]
objective value function right now is: -1564.0259508730003
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [47.814655]
objective value function right now is: -1566.2146134080356
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [48.672256]
objective value function right now is: -1562.6921469530625
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [49.561546]
objective value function right now is: -1565.6193691254105
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [50.338802]
objective value function right now is: -1565.922771600716
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [50.94892]
objective value function right now is: -1565.081217796886
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [51.25308]
objective value function right now is: -1566.1799032706488
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [51.9061]
objective value function right now is: -1564.469268083835
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.588337]
objective value function right now is: -1565.6900827726747
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.125412]
objective value function right now is: -1564.260897761873
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.418922]
objective value function right now is: -1564.5146881500698
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1566.7578688550354
Current xi:  [53.91471]
objective value function right now is: -1566.7578688550354
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.18778]
objective value function right now is: -1566.4098732825728
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1567.2279702534104
Current xi:  [54.186596]
objective value function right now is: -1567.2279702534104
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1567.41537346445
Current xi:  [54.30568]
objective value function right now is: -1567.41537346445
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1567.4934578172224
Current xi:  [54.369564]
objective value function right now is: -1567.4934578172224
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1567.4964373583518
Current xi:  [54.398994]
objective value function right now is: -1567.4964373583518
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.5863]
objective value function right now is: -1567.4948713051729
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.66415]
objective value function right now is: -1567.129571133355
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.802635]
objective value function right now is: -1567.248646982995
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.782585]
objective value function right now is: -1567.338138181103
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.91112]
objective value function right now is: -1567.2551612158752
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.983555]
objective value function right now is: -1567.4705859071225
new min fval from sgd:  -1567.4968237072742
new min fval from sgd:  -1567.5121988647275
new min fval from sgd:  -1567.5166833864241
new min fval from sgd:  -1567.5196710153605
new min fval from sgd:  -1567.5395415115556
new min fval from sgd:  -1567.5425413983646
new min fval from sgd:  -1567.5448562640884
new min fval from sgd:  -1567.5517116693875
new min fval from sgd:  -1567.5620340287135
new min fval from sgd:  -1567.568088178846
new min fval from sgd:  -1567.587103828473
new min fval from sgd:  -1567.5919262460418
new min fval from sgd:  -1567.60495604719
new min fval from sgd:  -1567.6252286037668
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.187798]
objective value function right now is: -1567.4789824225015
new min fval from sgd:  -1567.642555539403
new min fval from sgd:  -1567.6508142508396
new min fval from sgd:  -1567.667527616674
new min fval from sgd:  -1567.6836030587583
new min fval from sgd:  -1567.721047787812
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.368763]
objective value function right now is: -1567.2963787815556
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.462887]
objective value function right now is: -1567.384522393147
new min fval from sgd:  -1567.722199199759
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.54033]
objective value function right now is: -1567.7096687068024
new min fval from sgd:  -1567.7282684953489
new min fval from sgd:  -1567.7284279644798
new min fval from sgd:  -1567.7311287177936
new min fval from sgd:  -1567.7372467985015
new min fval from sgd:  -1567.738500311703
new min fval from sgd:  -1567.740450807424
new min fval from sgd:  -1567.7458033867988
new min fval from sgd:  -1567.7532427299911
new min fval from sgd:  -1567.7542371687616
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.56854]
objective value function right now is: -1567.7031945610167
min fval:  -1567.7542371687616
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 5.2492,  4.6974],
        [ 9.0265,  6.4229],
        [ 7.6646, -4.9897],
        [ 5.0118,  4.0746],
        [ 5.7453, -5.9741],
        [ 5.0119,  4.0745],
        [43.7218,  4.1332],
        [ 7.5420, -8.3719],
        [ 8.1585, -1.2705],
        [-0.7479, -6.5752]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-4.8653,  4.3297, -5.8532, -4.7719, -5.6582, -4.7719,  4.2260, -7.3088,
        -7.3141, -5.5043], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.1590, -8.1396,  3.2684,  0.1724,  3.4332,  0.1724, -4.1497,  7.8502,
          0.3409,  3.4574],
        [-1.1626,  3.8408, -2.9474, -1.2230, -2.6049, -1.2230,  0.0715, -8.5783,
         -6.6435, -1.4258],
        [ 5.0774, -8.2721,  3.0941,  4.0446,  3.2918,  4.0445, -4.6679,  7.6628,
          4.5352,  3.9608],
        [-0.2450, -0.2739, -0.1061, -0.2444, -0.1179, -0.2444, -0.6849, -0.2223,
         -0.2351, -0.2536],
        [ 5.5489, -8.2408,  3.1679,  3.7744,  3.3219,  3.7745, -4.6382,  7.4647,
          4.7032,  4.0479],
        [-1.3138,  1.9881, -2.3591, -1.3688, -3.3308, -1.3687, -1.5123, -5.4992,
         -2.4748, -2.6790],
        [ 0.2460, -8.3467,  3.2289,  0.2387,  3.7699,  0.2388, -4.9370,  8.3452,
          0.4728,  3.8019],
        [ 5.4597, -8.4261,  3.4250,  3.9914,  3.1030,  3.9914, -4.7081,  7.7543,
          4.6015,  4.0665],
        [ 6.1976, -8.7946,  3.6167,  4.5053,  3.7420,  4.5056, -6.1686,  8.5961,
          4.5222,  4.4597],
        [-0.2450, -0.2739, -0.1061, -0.2444, -0.1179, -0.2444, -0.6849, -0.2223,
         -0.2351, -0.2536]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-3.4605,  2.2310, -3.0725, -1.2791, -3.0817,  1.0859, -3.1816, -3.0534,
        -2.5267, -1.2791], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-5.5952,  5.9560, -7.1735,  0.2670, -7.0145,  2.9567, -6.7863, -6.5962,
         -9.0045,  0.2670]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  6.2660,  -4.2916],
        [ -4.9073,   2.5867],
        [  3.6039, -15.3084],
        [  3.1057,  11.4242],
        [ -8.5929,   0.1283],
        [  9.5958,   2.3346],
        [  8.9250,  -1.0070],
        [-10.0918, -11.5193],
        [ 11.4774,   8.8802],
        [-11.3830,  -2.2996]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -7.0831,  -5.8354, -14.2011,   9.5179,   7.2746,  -1.6960,  -9.1928,
         -9.5215,   6.6185,   1.3875], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-6.6842e-01,  1.5284e-02, -4.0327e-01, -9.7421e+00,  3.1375e+00,
         -4.4799e+00, -2.7620e+00,  3.1927e+00, -1.9386e+01,  9.0022e+00],
        [-1.2734e+00, -1.2017e+00,  2.1297e+00, -6.4902e-01,  1.1767e+00,
         -2.1703e-01, -8.0701e+00, -6.0972e+00, -2.4724e+00,  7.0462e-01],
        [ 1.3107e+00,  1.3215e+00,  2.4349e+00,  2.4303e+00, -1.3338e+00,
         -9.9298e-02,  1.2945e+00, -2.5294e+00, -1.4463e-02,  5.5944e+00],
        [-1.2803e+00, -7.0777e-01,  4.0070e-01, -1.4382e+00,  1.0264e+00,
          1.5408e-02, -3.1952e+00, -1.1425e+00, -3.9398e+00,  1.5933e+00],
        [-1.3818e+00, -5.7045e-01,  3.6759e-01, -1.4786e+00,  8.1365e-01,
         -2.0855e-01, -2.6982e+00, -1.6527e+00, -3.7649e+00,  1.2280e+00],
        [-1.7075e+00, -1.0192e+00, -7.2895e+00,  2.9656e+00,  3.3293e+00,
         -4.3111e+00,  2.4709e+00, -1.3344e+01,  1.1251e+00,  7.2933e+00],
        [-2.8721e-01,  4.4205e-03,  5.5894e+00, -1.2642e+01,  3.3275e+00,
         -1.2192e+00, -6.3860e+00,  5.3060e-01, -3.5381e+00,  3.2871e+00],
        [-1.3184e+00, -6.6836e-01,  3.8264e-01, -1.4604e+00,  9.4309e-01,
         -5.2353e-02, -3.0619e+00, -1.3659e+00, -3.8777e+00,  1.4508e+00],
        [-8.4829e+00, -3.7281e+00, -1.4574e+01,  2.4579e+00,  2.4874e+00,
         -2.0412e+00, -1.0940e+01, -1.0606e+01,  5.4038e-01,  4.4991e+00],
        [ 9.1314e-01, -1.8669e-02,  6.4461e+00, -2.1388e+01,  9.5603e+00,
         -6.0788e+00, -1.1835e+01,  5.3953e+00, -1.6655e+01,  4.0254e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-5.8074, -2.6641, -1.4158, -2.2405, -2.4950, -4.8225, -3.1547, -2.3429,
        -2.0231, -5.1409], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 14.6143,   2.1957,   0.3715,   2.3268,   1.9269,  -0.5584,  -0.6773,
           2.1800,   0.7266,  -8.2378],
        [-14.6170,  -2.2043,  -0.3274,  -2.3249,  -1.9180,   0.1747,   0.9102,
          -2.1795,  -0.7348,   7.9923]], device='cuda:0'))])
xi:  [55.545288]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 470.9421720060922
W_T_median: 223.45355194882197
W_T_pctile_5: 55.54596144016817
W_T_CVAR_5_pct: -44.922195025997624
Average q (qsum/M+1):  52.02180727066532
Optimal xi:  [55.545288]
Expected(across Rb) median(across samples) p_equity:  0.27589199418822924
obj fun:  tensor(-1567.7542, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
loaded xi:  55.545288
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1547.5197763009483
Current xi:  [61.17164]
objective value function right now is: -1547.5197763009483
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1548.0477347698375
Current xi:  [66.89572]
objective value function right now is: -1548.0477347698375
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.734665]
objective value function right now is: -1547.903902625497
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1550.2956543581251
Current xi:  [76.1837]
objective value function right now is: -1550.2956543581251
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1551.1370433069678
Current xi:  [80.172935]
objective value function right now is: -1551.1370433069678
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1551.6831147434602
Current xi:  [84.05158]
objective value function right now is: -1551.6831147434602
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [87.81003]
objective value function right now is: -1551.5595508383499
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [91.45756]
objective value function right now is: -1551.398244691897
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1552.5344772093003
Current xi:  [94.87592]
objective value function right now is: -1552.5344772093003
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [97.64034]
objective value function right now is: -1550.8874553812186
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1552.9364610372875
Current xi:  [99.87127]
objective value function right now is: -1552.9364610372875
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [102.848755]
objective value function right now is: -1552.8488567985223
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.7512]
objective value function right now is: -1551.169291074206
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1553.893789579092
Current xi:  [106.55044]
objective value function right now is: -1553.893789579092
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.22797]
objective value function right now is: -1553.6206781584676
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.474266]
objective value function right now is: -1553.0803205954683
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [110.04546]
objective value function right now is: -1549.8258662385397
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1554.5023034170567
Current xi:  [111.04579]
objective value function right now is: -1554.5023034170567
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [111.287285]
objective value function right now is: -1552.173962066181
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.65398]
objective value function right now is: -1553.6946543225808
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [113.10479]
objective value function right now is: -1553.523799043554
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [113.31568]
objective value function right now is: -1549.9333112586448
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [114.00937]
objective value function right now is: -1552.760089565624
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.1295014853347
Current xi:  [114.413734]
objective value function right now is: -1555.1295014853347
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [114.43477]
objective value function right now is: -1552.1205348917651
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [114.30993]
objective value function right now is: -1550.50559694468
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.2284]
objective value function right now is: -1552.4418118197927
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [115.20324]
objective value function right now is: -1549.431516299561
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [115.13714]
objective value function right now is: -1551.2012532024696
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.49906]
objective value function right now is: -1550.5381019679967
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.08328]
objective value function right now is: -1554.1149569891465
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.45963]
objective value function right now is: -1552.1816496653357
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.59476]
objective value function right now is: -1552.2386880874312
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.02485]
objective value function right now is: -1552.3746868776186
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.26108]
objective value function right now is: -1554.3522715736572
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.5115848892633
Current xi:  [116.288414]
objective value function right now is: -1555.5115848892633
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.24942]
objective value function right now is: -1555.2390236093318
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.5697332043928
Current xi:  [116.36625]
objective value function right now is: -1555.5697332043928
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.7266158154812
Current xi:  [116.38968]
objective value function right now is: -1555.7266158154812
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.7591528510823
Current xi:  [116.59994]
objective value function right now is: -1555.7591528510823
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.66083]
objective value function right now is: -1555.62388896442
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.57503]
objective value function right now is: -1555.337136535543
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.53272]
objective value function right now is: -1555.743758782824
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.905840961443
Current xi:  [116.651276]
objective value function right now is: -1555.905840961443
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.701836]
objective value function right now is: -1555.867371465416
new min fval from sgd:  -1555.9334089528638
new min fval from sgd:  -1555.9540315712616
new min fval from sgd:  -1556.0552161048668
new min fval from sgd:  -1556.0995204641074
new min fval from sgd:  -1556.1052327794569
new min fval from sgd:  -1556.1165244948647
new min fval from sgd:  -1556.156117980336
new min fval from sgd:  -1556.191102752775
new min fval from sgd:  -1556.2180292901992
new min fval from sgd:  -1556.2305945865612
new min fval from sgd:  -1556.238239370762
new min fval from sgd:  -1556.2440793381877
new min fval from sgd:  -1556.2444760907285
new min fval from sgd:  -1556.250017355443
new min fval from sgd:  -1556.250487586973
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.654205]
objective value function right now is: -1555.1456500320598
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.77565]
objective value function right now is: -1555.9305547900283
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.73761]
objective value function right now is: -1555.9884763402554
new min fval from sgd:  -1556.2742142058646
new min fval from sgd:  -1556.2765790927704
new min fval from sgd:  -1556.2829542920679
new min fval from sgd:  -1556.2872349482793
new min fval from sgd:  -1556.2899289350137
new min fval from sgd:  -1556.2925460712268
new min fval from sgd:  -1556.2941930129841
new min fval from sgd:  -1556.2980687720967
new min fval from sgd:  -1556.2980967427418
new min fval from sgd:  -1556.3042540333565
new min fval from sgd:  -1556.306267651608
new min fval from sgd:  -1556.3118928834501
new min fval from sgd:  -1556.3152519305029
new min fval from sgd:  -1556.316552997517
new min fval from sgd:  -1556.3173272226336
new min fval from sgd:  -1556.3178509382371
new min fval from sgd:  -1556.3246117281699
new min fval from sgd:  -1556.3251575589475
new min fval from sgd:  -1556.32530198714
new min fval from sgd:  -1556.3292299197572
new min fval from sgd:  -1556.3293096154541
new min fval from sgd:  -1556.3337434933894
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.77509]
objective value function right now is: -1556.2604756867636
new min fval from sgd:  -1556.3377173006745
new min fval from sgd:  -1556.3442942771085
new min fval from sgd:  -1556.3476125361435
new min fval from sgd:  -1556.3486481419247
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.79245]
objective value function right now is: -1556.2952259444482
min fval:  -1556.3486481419247
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 6.1731,  7.8360],
        [ 7.7931,  7.1238],
        [ 9.1509, -3.7832],
        [ 5.8195,  1.9069],
        [ 6.3326, -6.1298],
        [ 5.8195,  1.9068],
        [43.1158,  4.6286],
        [ 8.6609, -8.6959],
        [ 9.8261, -0.6786],
        [ 0.5340, -7.4022]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-5.4511,  4.5658, -6.4090, -5.9652, -5.8630, -5.9653,  4.5025, -7.3267,
        -8.4470, -5.7463], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.0130, -8.2594,  3.1224,  0.0743,  3.9292,  0.0743, -4.5022,  8.6883,
          0.8002,  3.8468],
        [-0.0697,  3.8720, -4.5118, -0.1297, -2.7036, -0.1297,  0.3984, -8.1528,
         -7.4684, -1.7679],
        [ 5.0086, -9.0371,  4.0651,  3.6716,  3.4122,  3.6716, -5.0959,  8.0916,
          7.6217,  5.1106],
        [-0.2380, -0.5115, -0.2648, -0.1464, -0.2329, -0.1464, -0.8880, -0.3782,
         -0.1793, -0.3543],
        [ 5.4193, -8.9806,  4.2296,  3.3710,  3.4432,  3.3711, -5.0547,  7.8713,
          7.8264,  5.1967],
        [-0.2380, -0.5115, -0.2648, -0.1464, -0.2329, -0.1464, -0.8880, -0.3782,
         -0.1793, -0.3543],
        [ 0.0657, -8.6600,  3.4632,  0.2099,  4.4902,  0.2099, -5.3109,  9.5044,
          1.2136,  4.2948],
        [ 5.3331, -9.1677,  4.4142,  3.5705,  3.2277,  3.5706, -5.1173,  8.1287,
          7.7203,  5.2338],
        [ 6.2236, -9.6900,  4.9639,  4.1723,  3.9584,  4.1726, -6.6587,  9.0865,
          8.1858,  5.7176],
        [-0.2380, -0.5115, -0.2648, -0.1464, -0.2329, -0.1464, -0.8880, -0.3782,
         -0.1793, -0.3543]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-3.5686,  1.8960, -3.1197, -1.3055, -3.1193, -1.3055, -3.3356, -3.0833,
        -2.5367, -1.3055], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -5.6346,   3.7412,  -7.6871,   0.1595,  -7.5558,   0.1595,  -7.4080,
          -7.3765, -10.7870,   0.1595]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  6.7713,  -2.1405],
        [ -3.3391,   2.6009],
        [  8.1935, -15.6173],
        [  3.3630,  12.3390],
        [ -9.6926,   0.2393],
        [ 10.1288,   2.7984],
        [  9.9286,  -1.0365],
        [-11.3587, -12.0993],
        [ 12.4324,   9.4256],
        [-12.2511,  -3.2032]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -6.4491,  -5.0457, -14.2610,   9.8256,   8.1881,  -3.4200, -10.5182,
         -9.4098,   6.9232,   0.2273], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.9952e+00, -1.0539e-01,  2.9405e-01, -9.7296e+00,  3.7661e+00,
         -3.5620e+00, -4.2211e+00,  2.7613e+00, -1.8138e+01,  7.0347e+00],
        [-3.1800e+00, -7.6590e-01,  4.2219e+00,  3.2291e-01,  3.5668e-01,
         -1.9636e+00, -4.7457e+00, -9.7033e+00, -3.2057e+00,  1.1714e+00],
        [-1.5588e-01,  1.6221e+00,  2.6766e+00,  3.9308e+00, -8.4262e-01,
         -3.9886e-02,  4.2944e+00, -3.9432e+00, -4.6554e-01,  4.8274e+00],
        [-1.4879e+00, -1.4155e+00,  4.1634e-01, -9.9752e-01,  1.3347e+00,
          1.0644e+00, -3.1566e+00, -1.1474e+00, -4.5675e+00,  2.5811e+00],
        [-1.0092e+00, -2.6482e-01, -1.2711e+00, -9.9636e-01, -1.2094e+00,
         -1.1886e+00, -3.7764e-01, -4.1291e-01, -1.6278e+00, -2.1211e-01],
        [-1.0053e+00, -2.6491e-01, -1.2691e+00, -9.9581e-01, -1.2081e+00,
         -1.1883e+00, -3.7522e-01, -4.0971e-01, -1.6230e+00, -2.1226e-01],
        [-8.0663e-01, -5.1499e-02,  6.2155e+00, -1.1924e+01,  3.4074e+00,
         -2.3037e+00, -7.9578e+00,  1.0622e+00, -5.2757e+00,  4.0213e+00],
        [-6.2570e-01, -1.4273e-02, -6.8965e-01, -9.9841e-01, -9.4532e-01,
         -1.7771e+00,  8.8507e-01, -8.1096e-01, -3.4074e+00,  5.1986e-01],
        [-1.2835e+01, -4.2360e+00, -1.2425e+01,  2.0409e+00,  3.3787e+00,
         -8.8545e-01, -1.2465e+01, -9.4019e-01, -1.7185e-01,  4.1803e-01],
        [-9.7623e-02, -1.2428e-02,  5.1275e+00, -2.1481e+01,  9.3244e+00,
         -4.8752e+00, -1.2910e+01,  5.7930e+00, -1.6161e+01,  4.9098e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-6.8062, -3.8128, -2.0905, -2.1822, -2.2460, -2.2528, -3.6183, -2.6702,
        -2.6508, -5.9847], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 15.3942,   2.5364,   0.4132,   2.0516,  -0.0334,  -0.0902,  -0.8425,
           0.7180,   0.5379,  -7.5655],
        [-15.3823,  -2.5433,  -0.3691,  -2.0491,   0.0338,  -0.0245,   1.0735,
          -0.7176,  -0.5459,   7.3913]], device='cuda:0'))])
xi:  [116.77179]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 528.3212791995527
W_T_median: 293.9071418527218
W_T_pctile_5: 116.76209121933483
W_T_CVAR_5_pct: -9.548840982103819
Average q (qsum/M+1):  50.66683467741935
Optimal xi:  [116.77179]
Expected(across Rb) median(across samples) p_equity:  0.26220240717132887
obj fun:  tensor(-1556.3486, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
loaded xi:  116.77179
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1552.2148834136467
Current xi:  [126.27404]
objective value function right now is: -1552.2148834136467
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.9460887609143
Current xi:  [134.45117]
objective value function right now is: -1555.9460887609143
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1558.592902543361
Current xi:  [142.40231]
objective value function right now is: -1558.592902543361
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1559.5397405781182
Current xi:  [147.31]
objective value function right now is: -1559.5397405781182
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [152.25916]
objective value function right now is: -1557.342596442504
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [156.06274]
objective value function right now is: -1555.855986859281
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1563.1351876600868
Current xi:  [158.85379]
objective value function right now is: -1563.1351876600868
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.89783]
objective value function right now is: -1561.4828754057098
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.61198]
objective value function right now is: -1561.3477942900508
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1564.9467395165407
Current xi:  [164.53203]
objective value function right now is: -1564.9467395165407
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.55061]
objective value function right now is: -1563.3132177457335
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.042]
objective value function right now is: -1563.3735089363338
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.50449]
objective value function right now is: -1562.2572324440234
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [167.71504]
objective value function right now is: -1562.7159057158983
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1565.2745639664788
Current xi:  [167.8932]
objective value function right now is: -1565.2745639664788
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.11246]
objective value function right now is: -1553.9471068941411
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.67606]
objective value function right now is: -1562.7150283315511
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.91542]
objective value function right now is: -1563.8138803348065
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.99496]
objective value function right now is: -1562.0507686890614
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.46149]
objective value function right now is: -1556.3341527948655
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.84096]
objective value function right now is: -1559.1863164395065
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.42651]
objective value function right now is: -1562.671852473344
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.28104]
objective value function right now is: -1555.355038526399
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.61385]
objective value function right now is: -1560.1247045598059
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.59924]
objective value function right now is: -1561.3478847443225
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.25551]
objective value function right now is: -1561.1502268803727
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.2399]
objective value function right now is: -1559.4883836625695
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [168.0281]
objective value function right now is: -1561.6129840336894
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [168.90741]
objective value function right now is: -1564.2385734481206
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.64476]
objective value function right now is: -1557.8641772937742
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.46327]
objective value function right now is: -1557.1106324822667
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.89491]
objective value function right now is: -1562.8564206900223
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.29362]
objective value function right now is: -1564.6167083407122
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.9566]
objective value function right now is: -1559.6139790322902
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.33954]
objective value function right now is: -1563.3198620609812
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1566.3275547362505
Current xi:  [169.28018]
objective value function right now is: -1566.3275547362505
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1566.3617455037554
Current xi:  [169.53012]
objective value function right now is: -1566.3617455037554
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1566.7198996930206
Current xi:  [169.38602]
objective value function right now is: -1566.7198996930206
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.46747]
objective value function right now is: -1565.2611313645596
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.44817]
objective value function right now is: -1565.5801465983277
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.35965]
objective value function right now is: -1565.2716225043168
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.24413]
objective value function right now is: -1566.6754690913049
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.10918]
objective value function right now is: -1566.5186795659788
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.22675]
objective value function right now is: -1566.240656259437
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.32846]
objective value function right now is: -1566.1611752095832
new min fval from sgd:  -1566.7454799746295
new min fval from sgd:  -1566.7678640416148
new min fval from sgd:  -1566.7906505670383
new min fval from sgd:  -1566.7969007202773
new min fval from sgd:  -1566.8014487280286
new min fval from sgd:  -1566.865950496147
new min fval from sgd:  -1566.9147468071524
new min fval from sgd:  -1566.9428337293673
new min fval from sgd:  -1566.956701144063
new min fval from sgd:  -1566.9624182851705
new min fval from sgd:  -1566.9708968943007
new min fval from sgd:  -1566.9815552803009
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.56407]
objective value function right now is: -1566.1324064891935
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.52313]
objective value function right now is: -1565.3406797597777
new min fval from sgd:  -1566.9970540639897
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.5803]
objective value function right now is: -1566.1138587475214
new min fval from sgd:  -1566.9983656628867
new min fval from sgd:  -1567.0326943997238
new min fval from sgd:  -1567.0568705949852
new min fval from sgd:  -1567.0571833275014
new min fval from sgd:  -1567.0634216948902
new min fval from sgd:  -1567.0637037746612
new min fval from sgd:  -1567.0645595551025
new min fval from sgd:  -1567.0655504090635
new min fval from sgd:  -1567.069537302719
new min fval from sgd:  -1567.076031114826
new min fval from sgd:  -1567.0833148427423
new min fval from sgd:  -1567.0914728592104
new min fval from sgd:  -1567.1044357878352
new min fval from sgd:  -1567.1214726540377
new min fval from sgd:  -1567.1415199545654
new min fval from sgd:  -1567.1574433084263
new min fval from sgd:  -1567.1716378410536
new min fval from sgd:  -1567.1837674680273
new min fval from sgd:  -1567.1924055158095
new min fval from sgd:  -1567.199207575111
new min fval from sgd:  -1567.2059865319286
new min fval from sgd:  -1567.2126118017525
new min fval from sgd:  -1567.2180203018477
new min fval from sgd:  -1567.2276319938783
new min fval from sgd:  -1567.2304943711586
new min fval from sgd:  -1567.2324383607745
new min fval from sgd:  -1567.2325453789945
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.56044]
objective value function right now is: -1567.1997790334099
new min fval from sgd:  -1567.2333386115033
new min fval from sgd:  -1567.241392004145
new min fval from sgd:  -1567.2511562168684
new min fval from sgd:  -1567.258970575729
new min fval from sgd:  -1567.2701979960586
new min fval from sgd:  -1567.278776984951
new min fval from sgd:  -1567.2825064162341
new min fval from sgd:  -1567.2841140069045
new min fval from sgd:  -1567.2863937358713
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.5213]
objective value function right now is: -1567.0653928908794
min fval:  -1567.2863937358713
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 6.9401,  6.9366],
        [ 6.4341,  7.6438],
        [ 9.9518, -3.1617],
        [ 6.5026,  0.9471],
        [ 7.5686, -6.3982],
        [ 6.5027,  0.9470],
        [39.3796,  5.2464],
        [ 9.2027, -9.0944],
        [11.5454, -0.8540],
        [ 1.3361, -8.1033]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-5.0839,  5.0966, -7.2388, -7.2426, -5.8210, -7.2427,  4.8522, -7.4545,
        -9.0507, -6.1427], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.0135e-02, -1.7403e+00,  9.8689e-03, -1.2083e-03,  2.8678e-01,
         -1.2082e-03, -2.8693e+00,  1.5847e+00, -1.3104e-02,  4.3487e-01],
        [-7.9949e-01,  2.6659e-01, -6.2245e-01, -4.3453e-01, -3.6016e-01,
         -4.3455e-01, -7.8587e-01, -1.6182e+00, -1.4264e+00, -4.7565e-01],
        [ 4.4086e+00, -8.7729e+00,  4.5392e+00,  3.3080e+00,  3.5535e+00,
          3.3081e+00, -5.1602e+00,  8.6089e+00,  1.0512e+01,  5.8560e+00],
        [-3.9420e-01, -5.8794e-01, -3.1185e-01, -2.1863e-01, -1.3058e-01,
         -2.1863e-01, -7.6848e-01, -1.8062e-01, -3.2243e-01,  1.5108e-01],
        [ 4.5869e+00, -8.7677e+00,  4.4671e+00,  3.0777e+00,  3.6608e+00,
          3.0779e+00, -5.1619e+00,  8.5044e+00,  1.0349e+01,  5.9406e+00],
        [-3.9420e-01, -5.8794e-01, -3.1185e-01, -2.1863e-01, -1.3058e-01,
         -2.1863e-01, -7.6848e-01, -1.8062e-01, -3.2243e-01,  1.5108e-01],
        [-4.6627e-01, -7.7515e+00,  3.2415e+00,  4.9196e-01,  5.3784e+00,
          4.9198e-01, -5.0301e+00,  1.0946e+01,  1.4843e+00,  4.6030e+00],
        [ 4.5795e+00, -8.9157e+00,  4.7110e+00,  3.2387e+00,  3.4630e+00,
          3.2388e+00, -5.1968e+00,  8.7320e+00,  1.0384e+01,  5.9732e+00],
        [ 5.6928e+00, -9.6028e+00,  5.9831e+00,  3.8941e+00,  4.2636e+00,
          3.8945e+00, -6.6826e+00,  9.7881e+00,  1.1733e+01,  6.5600e+00],
        [-3.9420e-01, -5.8794e-01, -3.1185e-01, -2.1863e-01, -1.3058e-01,
         -2.1863e-01, -7.6848e-01, -1.8062e-01, -3.2243e-01,  1.5108e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.6815, -0.0177, -2.7898, -1.0294, -2.8321, -1.0294, -3.1445, -2.7833,
        -2.2220, -1.0294], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.7202,   0.9311,  -7.9879,   0.5854,  -7.8136,   0.5854,  -6.3665,
          -7.8399, -12.1045,   0.5854]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  7.2134,  -1.2327],
        [ -1.3671,   0.7687],
        [ 12.5862, -15.3392],
        [  2.8172,  12.5355],
        [-11.7055,  -0.1675],
        [  7.9508,   9.3957],
        [ 11.0070,  -0.8000],
        [-12.2902, -13.1033],
        [ 13.7221,   9.7380],
        [-12.4535,  -3.5335]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -7.6094,  -4.3475, -14.3872,   9.8497,   8.6338,  -6.0044, -11.1231,
         -9.5156,   7.0534,  -0.0926], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-8.7866e+00, -2.9547e-01, -1.2239e-01, -1.9958e+01,  3.5162e+00,
          4.6924e-04, -3.8044e+00,  3.4287e+00, -1.9268e+01,  9.0048e+00],
        [-6.7715e+00, -6.7458e-02,  3.5203e+00,  2.3643e+00,  5.2727e-01,
         -1.4456e+00, -1.2471e+01, -1.1628e+01, -4.1147e+00,  4.7070e+00],
        [-1.1732e-01, -4.4561e-01,  1.2698e+00,  2.8737e+00,  2.0191e-01,
          1.0692e+00,  4.1923e+00, -3.6673e+00, -1.6357e-01,  4.8698e+00],
        [-2.0749e+00,  1.3310e-01,  2.2220e-01, -8.9547e-01,  1.5080e+00,
          2.7977e-01, -8.8505e-01, -4.7081e+00, -3.6500e+00,  4.1542e+00],
        [-1.3374e-01, -2.9706e-01, -1.6151e+00, -1.0146e+00, -7.2218e-01,
         -2.2372e-01,  2.2852e-01,  1.0318e-01, -2.7672e+00,  4.4874e-01],
        [-1.3375e-01, -2.9706e-01, -1.6151e+00, -1.0146e+00, -7.2218e-01,
         -2.2372e-01,  2.2852e-01,  1.0318e-01, -2.7672e+00,  4.4874e-01],
        [-1.7344e+00,  1.2298e-01,  6.6160e+00, -9.4477e+00,  3.9199e+00,
         -2.9011e-02, -8.7849e+00,  1.3713e+00, -6.5189e+00,  5.1456e+00],
        [-1.3374e-01, -2.9706e-01, -1.6151e+00, -1.0146e+00, -7.2218e-01,
         -2.2371e-01,  2.2852e-01,  1.0318e-01, -2.7672e+00,  4.4874e-01],
        [-1.5041e+01, -2.5752e-01, -7.6639e+00, -7.4122e-01,  5.8231e+00,
          6.2116e-01, -4.6050e+00,  1.3390e+00, -1.6941e+00, -1.1203e+00],
        [-6.5585e-01, -9.2538e-02,  4.5613e+00, -2.0780e+01,  9.9109e+00,
          7.3611e-03, -1.4334e+01,  6.3877e+00, -1.4935e+01,  4.8331e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-8.0137, -4.7491, -1.8630, -3.1637, -2.7427, -2.7427, -3.8323, -2.7427,
        -3.8178, -6.1138], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 16.6509,   2.9196,   0.4815,   2.1080,   0.4380,   0.4332,  -0.5796,
           0.4380,   0.8605,  -7.2577],
        [-16.6395,  -2.9260,  -0.4374,  -2.1062,  -0.4379,  -0.4427,   0.8097,
          -0.4379,  -0.8683,   7.1029]], device='cuda:0'))])
xi:  [169.55748]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 603.6619305612696
W_T_median: 378.7328707024618
W_T_pctile_5: 169.82231206178867
W_T_CVAR_5_pct: 15.95558735366089
Average q (qsum/M+1):  49.01358918220766
Optimal xi:  [169.55748]
Expected(across Rb) median(across samples) p_equity:  0.2569806938370069
obj fun:  tensor(-1567.2864, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
loaded xi:  169.55748
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1597.1541330268337
Current xi:  [174.53238]
objective value function right now is: -1597.1541330268337
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [178.54987]
objective value function right now is: -1572.8773397495313
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.51564]
objective value function right now is: -1594.9606112874756
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.51639]
objective value function right now is: -1596.67638926835
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.914671124327
Current xi:  [184.66681]
objective value function right now is: -1602.914671124327
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.00153]
objective value function right now is: -1591.6899038994247
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1602.9446020967755
Current xi:  [187.27882]
objective value function right now is: -1602.9446020967755
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.32312]
objective value function right now is: -1595.5536013836993
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.75359]
objective value function right now is: -1601.1197867783123
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.256517968815
Current xi:  [188.68727]
objective value function right now is: -1604.256517968815
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.3227168183116
Current xi:  [188.81236]
objective value function right now is: -1604.3227168183116
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.83734]
objective value function right now is: -1601.3449090985555
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.62915]
objective value function right now is: -1599.136992608426
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [191.59715]
objective value function right now is: -1598.8614181342632
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [191.35782]
objective value function right now is: -1602.3044674703979
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [191.1792]
objective value function right now is: -1596.6636804149339
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.80597]
objective value function right now is: -1601.2848641716519
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.80588]
objective value function right now is: -1602.5621091294076
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.38779]
objective value function right now is: -1597.0979376211446
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.15561]
objective value function right now is: -1587.2826011170932
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.36655]
objective value function right now is: -1601.8589040844297
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.90265]
objective value function right now is: -1601.1420828615037
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.972]
objective value function right now is: -1604.2543982475318
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.97746]
objective value function right now is: -1604.1497733140359
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.55264]
objective value function right now is: -1602.5223845095736
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.4526328548563
Current xi:  [190.72804]
objective value function right now is: -1604.4526328548563
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.9669824939551
Current xi:  [190.02734]
objective value function right now is: -1604.9669824939551
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [189.56367]
objective value function right now is: -1601.8930019294462
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [190.27477]
objective value function right now is: -1601.3178521121606
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.26974]
objective value function right now is: -1597.3990843511726
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.56876]
objective value function right now is: -1601.318877461995
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.22049]
objective value function right now is: -1600.599654881647
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.34009]
objective value function right now is: -1593.5462878397284
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.07768]
objective value function right now is: -1602.2723552567252
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.05986]
objective value function right now is: -1604.9269266790454
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1607.3463211062556
Current xi:  [189.21825]
objective value function right now is: -1607.3463211062556
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1607.416218873776
Current xi:  [189.50682]
objective value function right now is: -1607.416218873776
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.74893]
objective value function right now is: -1607.300815873151
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.98904]
objective value function right now is: -1606.66468202441
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.06355]
objective value function right now is: -1606.6950250075622
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1608.4512449584918
Current xi:  [190.14973]
objective value function right now is: -1608.4512449584918
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.3903]
objective value function right now is: -1607.3502591510537
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.62326]
objective value function right now is: -1608.3019323381711
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.60245]
objective value function right now is: -1607.2016760687013
new min fval from sgd:  -1608.6382457084921
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.88895]
objective value function right now is: -1608.6382457084921
new min fval from sgd:  -1608.7472065106667
new min fval from sgd:  -1608.7482994329773
new min fval from sgd:  -1608.7590719764216
new min fval from sgd:  -1608.7599846716755
new min fval from sgd:  -1608.795830501286
new min fval from sgd:  -1608.8004105104042
new min fval from sgd:  -1608.806042889406
new min fval from sgd:  -1608.8087147196343
new min fval from sgd:  -1608.9517231203738
new min fval from sgd:  -1609.042666914772
new min fval from sgd:  -1609.071393986044
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [191.19418]
objective value function right now is: -1607.7663185073716
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [191.39534]
objective value function right now is: -1607.8361447541636
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [191.38695]
objective value function right now is: -1608.358897350833
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [191.33371]
objective value function right now is: -1608.361596062045
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [191.31152]
objective value function right now is: -1608.671221216838
min fval:  -1609.071393986044
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 7.3428, 13.8835],
        [ 6.6599,  8.3924],
        [11.0131, -3.0620],
        [ 7.3161,  1.0282],
        [ 8.6732, -6.3777],
        [ 7.3162,  1.0281],
        [35.2477,  5.8575],
        [ 9.6729, -9.3763],
        [12.7309, -0.9702],
        [ 2.0403, -8.5487]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-5.0894,  5.5191, -7.7189, -8.2039, -6.0001, -8.2038,  5.2260, -7.7251,
        -9.7858, -6.7139], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.2653, -0.5961, -0.3103, -0.1836, -0.2098, -0.1836, -0.8790, -0.2394,
         -0.2994,  0.1538],
        [-0.2653, -0.5961, -0.3103, -0.1836, -0.2098, -0.1836, -0.8790, -0.2394,
         -0.2994,  0.1538],
        [ 3.7810, -8.5348,  4.7555,  2.8279,  4.0993,  2.8280, -5.2398,  9.4058,
         12.0923,  6.1560],
        [-0.2653, -0.5961, -0.3103, -0.1836, -0.2098, -0.1836, -0.8790, -0.2394,
         -0.2994,  0.1538],
        [ 3.8974, -8.4389,  4.5587,  2.7249,  4.1686,  2.7251, -5.2582,  9.2730,
         11.6535,  6.1387],
        [-0.2653, -0.5961, -0.3103, -0.1836, -0.2098, -0.1836, -0.8790, -0.2394,
         -0.2994,  0.1538],
        [-1.3777, -6.9922,  4.6111,  0.1719,  6.0433,  0.1719, -4.2108, 12.1665,
          2.8744,  3.9437],
        [ 3.9291, -8.6110,  4.8594,  2.8425,  4.0582,  2.8426, -5.2778,  9.5706,
         11.8995,  6.2157],
        [ 5.0294, -9.7173,  6.3278,  3.6389,  5.1425,  3.6393, -6.7801, 11.0326,
         14.0066,  7.3168],
        [-0.2653, -0.5961, -0.3103, -0.1836, -0.2098, -0.1836, -0.8790, -0.2394,
         -0.2994,  0.1538]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.1443, -1.1443, -2.4130, -1.1443, -2.4465, -1.1443, -2.8712, -2.4196,
        -1.9318, -1.1443], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.5424,   0.5424,  -8.0735,   0.5424,  -7.6966,   0.5424,  -4.7147,
          -8.0072, -13.8610,   0.5424]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -0.9244,   1.5111],
        [ -2.9116,   3.8567],
        [ 13.8173, -15.3352],
        [  2.6558,  12.6134],
        [-13.4282,  -0.6732],
        [ 10.7240,  10.7440],
        [ 11.9492,  -0.7187],
        [-11.8898, -13.6927],
        [ 14.1968,  10.1523],
        [-13.3868,  -3.4836]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -4.5749,  -5.4073, -14.3103,   9.7693,   8.6645,  -3.1123, -11.5729,
        -10.1714,   7.3194,   0.9013], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.4986e-01,  9.4899e-02, -1.7383e+00, -2.3309e+01,  3.0139e+00,
         -3.1599e-04, -9.1293e+00,  3.4425e+00, -2.0480e+01,  1.1535e+01],
        [-3.9932e-01, -2.9166e+00,  3.6457e+00,  4.5940e+00,  3.1075e-01,
          9.3109e-01, -1.6516e+01, -7.5182e+00, -5.2683e+00,  4.1506e+00],
        [-5.9512e-01, -1.0557e+00,  5.5409e-01,  2.0389e+00,  1.1882e+00,
          4.1576e+00,  4.5629e+00, -3.7703e+00, -1.0397e-01,  4.0031e+00],
        [-4.2514e-01, -2.4620e+00,  2.7167e-01, -1.5396e+00,  1.2236e+00,
          2.0345e+00, -7.9058e-01, -5.8259e+00, -3.6543e+00,  4.5232e+00],
        [-3.2627e-01, -3.1733e+00, -5.3776e+00, -1.7872e-01, -5.3999e+00,
          1.0791e+00,  2.0338e+00,  2.4323e+00, -2.6439e+00,  2.1422e-01],
        [-3.2628e-01, -3.1733e+00, -5.3775e+00, -1.7875e-01, -5.3997e+00,
          1.0791e+00,  2.0337e+00,  2.4322e+00, -2.6439e+00,  2.1423e-01],
        [ 2.5475e-01,  2.6270e-02,  6.9516e+00, -1.1437e+01,  4.6472e+00,
         -2.5167e-01, -8.8639e+00,  1.3145e+00, -6.8072e+00,  5.8241e+00],
        [-3.2627e-01, -3.1733e+00, -5.3776e+00, -1.7871e-01, -5.3999e+00,
          1.0791e+00,  2.0338e+00,  2.4323e+00, -2.6439e+00,  2.1422e-01],
        [-1.5911e-01, -8.5075e-01, -6.3693e+00, -2.5854e+00,  6.9279e+00,
          7.9115e-02, -2.5811e+00,  1.1950e-01, -1.8986e+00, -1.0186e+00],
        [ 3.2193e-02, -4.8284e-02,  4.8403e+00, -1.9350e+01,  9.4903e+00,
          4.1018e-02, -1.3159e+01,  6.1116e+00, -1.4286e+01,  5.2451e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-9.4347, -5.7287, -1.5407, -4.0043, -3.5443, -3.5444, -3.6246, -3.5443,
        -3.9928, -6.3833], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 17.7508,   3.2599,   0.4742,   2.7135,   2.1391,   2.1347,  -0.3745,
           2.1391,   1.5120,  -7.0857],
        [-17.7439,  -3.2661,  -0.4301,  -2.7120,  -2.1390,  -2.1433,   0.6043,
          -2.1390,  -1.5195,   6.9408]], device='cuda:0'))])
xi:  [191.1766]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 651.7536445423377
W_T_median: 423.95838999651335
W_T_pctile_5: 192.3197078032197
W_T_CVAR_5_pct: 23.935685591597785
Average q (qsum/M+1):  48.046875
Optimal xi:  [191.1766]
Expected(across Rb) median(across samples) p_equity:  0.24975070878863334
obj fun:  tensor(-1609.0714, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
loaded xi:  191.1766
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2809.2351380864616
Current xi:  [198.3051]
objective value function right now is: -2809.2351380864616
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -2828.070000422469
Current xi:  [202.53708]
objective value function right now is: -2828.070000422469
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -2835.5323785849196
Current xi:  [205.76031]
objective value function right now is: -2835.5323785849196
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -2873.07839257883
Current xi:  [208.67993]
objective value function right now is: -2873.07839257883
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.45396]
objective value function right now is: -2804.6695894712693
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -2891.1520215720507
Current xi:  [210.9861]
objective value function right now is: -2891.1520215720507
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [212.89055]
objective value function right now is: -2747.9310759821606
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.28914]
objective value function right now is: -2871.206039785378
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.99004]
objective value function right now is: -2808.7810959862522
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.17336]
objective value function right now is: -2728.8750721123974
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.18393]
objective value function right now is: -2840.58974432098
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.1897]
objective value function right now is: -2816.896470463344
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.54318]
objective value function right now is: -2816.4329990410843
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -2902.1669649350215
Current xi:  [213.67032]
objective value function right now is: -2902.1669649350215
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.3258]
objective value function right now is: -2873.306476769912
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.18016]
objective value function right now is: -2879.81662139979
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.68481]
objective value function right now is: -2780.5933990094627
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.43434]
objective value function right now is: -2890.177249358695
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.38942]
objective value function right now is: -2849.469720567812
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.4301]
objective value function right now is: -2718.1666319937835
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.71883]
objective value function right now is: -2853.038072285004
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.40997]
objective value function right now is: -2819.680795601433
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.31834]
objective value function right now is: -2895.631717285693
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.1816]
objective value function right now is: -2783.9610311404012
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.77705]
objective value function right now is: -2893.3017510648706
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.7889]
objective value function right now is: -2888.1374172462492
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.06093]
objective value function right now is: -2840.183543621381
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [214.99467]
objective value function right now is: -2861.345120635152
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [214.73167]
objective value function right now is: -2871.970248652956
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.24649]
objective value function right now is: -2830.914871130166
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.44421]
objective value function right now is: -2846.045723830053
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.9154]
objective value function right now is: -2851.10660165657
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.4253]
objective value function right now is: -2894.570572905174
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.62718]
objective value function right now is: -2814.9118383197283
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.47679]
objective value function right now is: -2889.2167485113796
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2919.4668558744374
Current xi:  [215.28116]
objective value function right now is: -2919.4668558744374
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.25883]
objective value function right now is: -2903.518938216028
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.59988]
objective value function right now is: -2901.7235288230404
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.4065]
objective value function right now is: -2915.1844109752196
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.33063]
objective value function right now is: -2905.268709246819
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.31993]
objective value function right now is: -2909.9168288907535
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.34253]
objective value function right now is: -2910.297920707895
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -2922.850598857633
Current xi:  [215.01443]
objective value function right now is: -2922.850598857633
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.2079]
objective value function right now is: -2892.481087886041
new min fval from sgd:  -2924.8029252222627
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.32918]
objective value function right now is: -2924.8029252222627
new min fval from sgd:  -2925.0020473046134
new min fval from sgd:  -2925.3846445673403
new min fval from sgd:  -2925.970723477439
new min fval from sgd:  -2926.222680575542
new min fval from sgd:  -2926.4525800280117
new min fval from sgd:  -2926.6937089884923
new min fval from sgd:  -2926.9216116354214
new min fval from sgd:  -2927.018733080487
new min fval from sgd:  -2927.0974429851367
new min fval from sgd:  -2927.2141751900785
new min fval from sgd:  -2927.3397627481154
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.30453]
objective value function right now is: -2898.4595235303777
new min fval from sgd:  -2927.4447396010796
new min fval from sgd:  -2927.5966162813147
new min fval from sgd:  -2927.6853337663642
new min fval from sgd:  -2927.9724767348835
new min fval from sgd:  -2928.200209060085
new min fval from sgd:  -2928.436600764715
new min fval from sgd:  -2928.795136893043
new min fval from sgd:  -2928.9175963583275
new min fval from sgd:  -2929.0282444200266
new min fval from sgd:  -2929.1315984854114
new min fval from sgd:  -2929.35031958419
new min fval from sgd:  -2929.703863913917
new min fval from sgd:  -2929.7786170399504
new min fval from sgd:  -2929.8314583374727
new min fval from sgd:  -2930.041222856979
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.44568]
objective value function right now is: -2925.7325171626258
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.38841]
objective value function right now is: -2907.8115424031134
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.43307]
objective value function right now is: -2928.8332067426268
new min fval from sgd:  -2930.049474956571
new min fval from sgd:  -2930.116832501931
new min fval from sgd:  -2930.1658939585973
new min fval from sgd:  -2930.223456464343
new min fval from sgd:  -2930.258843642179
new min fval from sgd:  -2930.2907505801013
new min fval from sgd:  -2930.30938459602
new min fval from sgd:  -2930.3105760132307
new min fval from sgd:  -2930.3264487628776
new min fval from sgd:  -2930.3805693616923
new min fval from sgd:  -2930.4391942799957
new min fval from sgd:  -2930.4730080263253
new min fval from sgd:  -2930.5123662881088
new min fval from sgd:  -2930.550703707114
new min fval from sgd:  -2930.5883271842263
new min fval from sgd:  -2930.616232787015
new min fval from sgd:  -2930.635338214146
new min fval from sgd:  -2930.6594854645036
new min fval from sgd:  -2930.6910342096903
new min fval from sgd:  -2930.699945800958
new min fval from sgd:  -2930.717642455574
new min fval from sgd:  -2930.8147624253033
new min fval from sgd:  -2930.905613754453
new min fval from sgd:  -2930.954963679783
new min fval from sgd:  -2930.9594528963407
new min fval from sgd:  -2931.0001975904693
new min fval from sgd:  -2931.0832453546172
new min fval from sgd:  -2931.1745205075904
new min fval from sgd:  -2931.2452481477617
new min fval from sgd:  -2931.321296042752
new min fval from sgd:  -2931.369980526819
new min fval from sgd:  -2931.395980872435
new min fval from sgd:  -2931.4124342090477
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.39262]
objective value function right now is: -2929.765971581657
min fval:  -2931.4124342090477
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 8.2546, 12.3918],
        [ 5.7653,  9.0407],
        [12.1187, -3.4403],
        [ 7.8770,  1.1482],
        [ 9.6923, -6.5699],
        [ 7.8772,  1.1481],
        [22.5152,  5.9605],
        [11.7303, -9.3167],
        [14.4286, -1.4073],
        [ 2.7942, -9.7156]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-4.1993,  6.3688, -8.0696, -9.3021, -6.2113, -9.3021,  6.5473, -7.4411,
        -9.3573, -7.1345], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.6818e-01, -9.6333e-01,  4.9932e-03, -1.5906e-01, -2.4460e-01,
         -1.5907e-01, -1.1557e+00, -4.2161e-01, -1.8842e-01, -4.0386e-02],
        [-2.6818e-01, -9.6333e-01,  4.9932e-03, -1.5906e-01, -2.4460e-01,
         -1.5907e-01, -1.1557e+00, -4.2161e-01, -1.8842e-01, -4.0386e-02],
        [ 2.2141e+00, -8.6752e+00,  4.7103e+00,  2.6728e+00,  4.4011e+00,
          2.6729e+00, -5.0522e+00,  1.0515e+01,  1.2107e+01,  7.1677e+00],
        [-2.6818e-01, -9.6333e-01,  4.9931e-03, -1.5906e-01, -2.4460e-01,
         -1.5907e-01, -1.1557e+00, -4.2161e-01, -1.8842e-01, -4.0386e-02],
        [ 2.2587e+00, -8.4322e+00,  3.9517e+00,  1.6491e+00,  4.2792e+00,
          1.6492e+00, -5.0161e+00,  1.0080e+01,  1.0537e+01,  6.8369e+00],
        [-2.6818e-01, -9.6333e-01,  4.9930e-03, -1.5906e-01, -2.4460e-01,
         -1.5907e-01, -1.1557e+00, -4.2161e-01, -1.8842e-01, -4.0386e-02],
        [ 2.5539e-02, -1.3333e+00,  1.0131e-01, -1.2500e-01, -1.4289e-01,
         -1.2500e-01, -1.4236e+00,  1.8117e-01,  3.5553e-02, -1.9238e-01],
        [ 2.3618e+00, -8.7280e+00,  4.7773e+00,  2.7643e+00,  4.3696e+00,
          2.7644e+00, -5.0787e+00,  1.0704e+01,  1.2015e+01,  7.2512e+00],
        [ 3.6014e+00, -9.3769e+00,  6.8434e+00,  3.8086e+00,  6.1538e+00,
          3.8089e+00, -6.2475e+00,  1.2955e+01,  1.5373e+01,  8.3825e+00],
        [-2.6818e-01, -9.6333e-01,  4.9932e-03, -1.5906e-01, -2.4460e-01,
         -1.5907e-01, -1.1557e+00, -4.2161e-01, -1.8842e-01, -4.0386e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.2444, -1.2444, -1.9990, -1.2444, -2.0140, -1.2444, -1.2595, -1.9963,
        -1.4159, -1.2444], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.2372,   0.2372,  -8.0307,   0.2372,  -6.8753,   0.2372,   0.1279,
          -8.0935, -15.4169,   0.2372]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -4.0324,   3.9227],
        [ -4.7509,   5.9414],
        [ 12.8050, -15.8343],
        [  3.4282,  12.8677],
        [-14.5776,  -0.8820],
        [ 10.9069,  16.0781],
        [ 12.0002,  -1.1560],
        [-11.8719, -14.2188],
        [ 14.9896,  10.6010],
        [-14.3875,  -3.5557]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -6.6458,  -6.6455, -14.2884,   9.8756,   9.2651,   0.0655, -12.4624,
        -10.9370,   7.3764,   1.3203], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.3631e-02, -1.1263e-02, -2.7628e+00, -2.7205e+01,  2.4108e+00,
         -1.3597e-03, -1.1694e+01,  3.4418e+00, -2.5380e+01,  1.4093e+01],
        [-4.2953e+00, -6.1636e+00,  1.1195e+00,  6.3335e+00,  9.4102e-01,
          1.8330e-01, -1.9361e+01, -4.0726e+00, -5.2138e+00,  1.9460e+00],
        [-1.8027e+00, -2.3779e+00, -3.5704e-01,  1.6948e+00,  7.1199e-01,
          5.7683e+00,  4.6947e+00, -3.8637e+00,  8.7974e-02,  2.3967e+00],
        [-2.6928e+00, -6.2916e+00, -3.7103e+00, -5.6324e-01, -1.8051e+00,
          3.4843e+00, -5.5511e+00, -3.6465e+00, -2.7107e+00,  5.8733e+00],
        [-1.6414e+00, -4.0464e+00, -8.0686e+00, -5.6300e-01, -1.0838e+00,
          1.0911e+00, -1.4845e+00,  3.1759e+00, -3.3102e+00,  3.1918e+00],
        [-1.6414e+00, -4.0464e+00, -8.0685e+00, -5.6302e-01, -1.0838e+00,
          1.0910e+00, -1.4846e+00,  3.1758e+00, -3.3103e+00,  3.1917e+00],
        [ 7.6631e-02, -1.6179e-02,  7.4280e+00, -1.3701e+01,  4.9736e+00,
         -3.0983e-01, -9.6047e+00,  1.3363e+00, -6.9528e+00,  6.7956e+00],
        [-1.6414e+00, -4.0465e+00, -8.0686e+00, -5.6301e-01, -1.0839e+00,
          1.0911e+00, -1.4845e+00,  3.1760e+00, -3.3102e+00,  3.1918e+00],
        [-1.2612e+00, -1.6700e+00, -2.7026e+00, -9.3785e-01,  6.3766e+00,
         -2.6432e+00, -4.0900e+00, -4.8148e+00, -2.1279e+00, -6.6301e+00],
        [-3.3519e-02,  5.6869e-03,  4.9556e+00, -1.9432e+01,  9.7004e+00,
          2.3917e-02, -1.2071e+01,  5.6390e+00, -1.3955e+01,  5.6243e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-10.6094,  -5.6353,  -0.7540,  -3.1534,  -4.3852,  -4.3853,  -3.4601,
         -4.3852,  -4.4044,  -6.6011], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 23.7562,   3.5987,   0.5097,   4.0525,   2.9591,   2.9550,  -0.3393,
           2.9591,   1.9316,  -7.7295],
        [-23.7496,  -3.6049,  -0.4656,  -4.0510,  -2.9591,  -2.9631,   0.5691,
          -2.9591,  -1.9390,   7.5852]], device='cuda:0'))])
xi:  [215.42587]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 681.7173180789962
W_T_median: 480.28851582491643
W_T_pctile_5: 215.1306827539385
W_T_CVAR_5_pct: 30.290236056013484
Average q (qsum/M+1):  45.70735020791331
Optimal xi:  [215.42587]
Expected(across Rb) median(across samples) p_equity:  0.21033867647250493
obj fun:  tensor(-2931.4124, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
