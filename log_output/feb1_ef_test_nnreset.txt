Starting at: 
01-02-23_19:51

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 40000
batchsize: 1000
remove neg:  False


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 4000, 'itbound_SGD_algorithms': 40000, 'nit_IterateAveragingStart': 36000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.03, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1701.369675698902
Current xi:  [-21.332218]
objective value function right now is: -1701.369675698902
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1709.9955690319741
Current xi:  [-41.106247]
objective value function right now is: -1709.9955690319741
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1714.8010621722067
Current xi:  [-61.74403]
objective value function right now is: -1714.8010621722067
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1718.1123727004936
Current xi:  [-81.56003]
objective value function right now is: -1718.1123727004936
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1721.1034843539055
Current xi:  [-103.238205]
objective value function right now is: -1721.1034843539055
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1723.6261014503518
Current xi:  [-123.851776]
objective value function right now is: -1723.6261014503518
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1725.903372742084
Current xi:  [-145.2575]
objective value function right now is: -1725.903372742084
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1727.9162133196235
Current xi:  [-166.11171]
objective value function right now is: -1727.9162133196235
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1729.9437093353433
Current xi:  [-187.14404]
objective value function right now is: -1729.9437093353433
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1731.4895150020152
Current xi:  [-208.39725]
objective value function right now is: -1731.4895150020152
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1732.9580792648676
Current xi:  [-228.56706]
objective value function right now is: -1732.9580792648676
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1734.1817775743455
Current xi:  [-249.34831]
objective value function right now is: -1734.1817775743455
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1735.4503605794087
Current xi:  [-269.67383]
objective value function right now is: -1735.4503605794087
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1736.4984302726195
Current xi:  [-289.9893]
objective value function right now is: -1736.4984302726195
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1737.4986061677737
Current xi:  [-309.36554]
objective value function right now is: -1737.4986061677737
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1738.0199765552004
Current xi:  [-329.05893]
objective value function right now is: -1738.0199765552004
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1739.222808861745
Current xi:  [-347.83655]
objective value function right now is: -1739.222808861745
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1739.6746293792778
Current xi:  [-366.5798]
objective value function right now is: -1739.6746293792778
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1739.888097846012
Current xi:  [-384.3198]
objective value function right now is: -1739.888097846012
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1740.432392124776
Current xi:  [-401.33557]
objective value function right now is: -1740.432392124776
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1740.7447047812348
Current xi:  [-418.2127]
objective value function right now is: -1740.7447047812348
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-432.80582]
objective value function right now is: -1740.7130765469187
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.2555372818795
Current xi:  [-446.1599]
objective value function right now is: -1741.2555372818795
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-458.25516]
objective value function right now is: -1741.1959630802044
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.3985167575142
Current xi:  [-468.09482]
objective value function right now is: -1741.3985167575142
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-475.60916]
objective value function right now is: -1741.357958783595
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.4705789532122
Current xi:  [-478.93756]
objective value function right now is: -1741.4705789532122
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1741.4788775460795
Current xi:  [-484.7786]
objective value function right now is: -1741.4788775460795
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1741.4977009158158
Current xi:  [-485.46948]
objective value function right now is: -1741.4977009158158
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.4996486453304
Current xi:  [-488.06985]
objective value function right now is: -1741.4996486453304
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.63138]
objective value function right now is: -1741.487187515086
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.84088]
objective value function right now is: -1741.3708569160544
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.512118412951
Current xi:  [-489.36694]
objective value function right now is: -1741.512118412951
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.5212851423978
Current xi:  [-489.8772]
objective value function right now is: -1741.5212851423978
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.5887055297303
Current xi:  [-487.69992]
objective value function right now is: -1741.5887055297303
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.6758021741798
Current xi:  [-488.22168]
objective value function right now is: -1741.6758021741798
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-488.104]
objective value function right now is: -1741.6663206104581
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-488.35056]
objective value function right now is: -1741.6393175230526
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.7075291294
Current xi:  [-488.56235]
objective value function right now is: -1741.7075291294
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-488.56894]
objective value function right now is: -1741.6982173054564
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-488.9678]
objective value function right now is: -1741.6994007507483
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-488.54846]
objective value function right now is: -1741.6230375388573
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.7104699320594
Current xi:  [-488.7139]
objective value function right now is: -1741.7104699320594
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.15216]
objective value function right now is: -1741.6963675442582
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-488.74304]
objective value function right now is: -1741.6759211033448
new min fval from sgd:  -1741.7135166965309
new min fval from sgd:  -1741.7162726955828
new min fval from sgd:  -1741.7167064281653
new min fval from sgd:  -1741.7178419368097
new min fval from sgd:  -1741.719108572763
new min fval from sgd:  -1741.7197646250695
new min fval from sgd:  -1741.7200650947584
new min fval from sgd:  -1741.7238931708434
new min fval from sgd:  -1741.7242936824757
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.30573]
objective value function right now is: -1741.6815906047084
new min fval from sgd:  -1741.7257989073823
new min fval from sgd:  -1741.7258200899446
new min fval from sgd:  -1741.7268722234435
new min fval from sgd:  -1741.7274226944658
new min fval from sgd:  -1741.7275795552905
new min fval from sgd:  -1741.728457027504
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.60007]
objective value function right now is: -1741.654157918339
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.49496]
objective value function right now is: -1741.6898651177553
new min fval from sgd:  -1741.7322890455498
new min fval from sgd:  -1741.7328906922141
new min fval from sgd:  -1741.7342529252346
new min fval from sgd:  -1741.7347813677059
new min fval from sgd:  -1741.7348641037177
new min fval from sgd:  -1741.735462109276
new min fval from sgd:  -1741.7358487278584
new min fval from sgd:  -1741.7362412951734
new min fval from sgd:  -1741.7367080371666
new min fval from sgd:  -1741.737241150817
new min fval from sgd:  -1741.7377066770287
new min fval from sgd:  -1741.7378177537696
new min fval from sgd:  -1741.7378530603303
new min fval from sgd:  -1741.7384970680148
new min fval from sgd:  -1741.7387673496096
new min fval from sgd:  -1741.7387892458864
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.21982]
objective value function right now is: -1741.7323547774301
new min fval from sgd:  -1741.7393977655863
new min fval from sgd:  -1741.7398444662772
new min fval from sgd:  -1741.7398735995275
new min fval from sgd:  -1741.7399656446432
new min fval from sgd:  -1741.7401479379675
new min fval from sgd:  -1741.740230950419
new min fval from sgd:  -1741.7402787945118
new min fval from sgd:  -1741.740395664559
new min fval from sgd:  -1741.7407475525806
new min fval from sgd:  -1741.7410936532092
new min fval from sgd:  -1741.741512999542
new min fval from sgd:  -1741.741883908537
new min fval from sgd:  -1741.7420379618961
new min fval from sgd:  -1741.7421221264628
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.14594]
objective value function right now is: -1741.7356146964073
min fval:  -1741.7421221264628
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.4718,  1.2709],
        [-4.6294,  6.4362],
        [13.0462,  2.1534],
        [ 0.2581, -1.0481],
        [-0.4718,  1.2709],
        [-0.4718,  1.2709],
        [-0.4718,  1.2709],
        [-4.4897,  6.1221],
        [-0.4718,  1.2709],
        [-0.4718,  1.2709]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-0.7974, 11.5243, -6.7843,  1.0876, -0.7974, -0.7974, -0.7974, 10.8076,
        -0.7974, -0.7974], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.0749,  2.1028,  3.0167,  0.2464,  0.0749,  0.0749,  0.0749,  1.7323,
          0.0749,  0.0749],
        [-0.0219,  4.1560,  4.9117,  0.8208, -0.0219, -0.0219, -0.0219,  3.3948,
         -0.0219, -0.0219],
        [-0.0077, -0.0279, -0.1417, -0.4351, -0.0077, -0.0077, -0.0077, -0.0230,
         -0.0077, -0.0077],
        [ 0.0935, -5.4833, -6.4779, -1.1629,  0.0935,  0.0935,  0.0935, -4.5052,
          0.0935,  0.0935],
        [-0.0077, -0.0279, -0.1417, -0.4351, -0.0077, -0.0077, -0.0077, -0.0230,
         -0.0077, -0.0077],
        [ 0.0863, -5.3102, -6.2724, -1.0157,  0.0863,  0.0863,  0.0863, -4.3224,
          0.0863,  0.0863],
        [ 0.0721,  1.9230,  2.8492,  0.1926,  0.0721,  0.0721,  0.0721,  1.5870,
          0.0721,  0.0721],
        [ 0.0597, -4.7092, -5.6447, -1.0704,  0.0597,  0.0597,  0.0597, -3.8207,
          0.0597,  0.0597],
        [-0.0512,  4.7554,  5.5449,  0.8613, -0.0512, -0.0512, -0.0512,  3.8965,
         -0.0512, -0.0512],
        [-0.0077, -0.0279, -0.1417, -0.4351, -0.0077, -0.0077, -0.0077, -0.0230,
         -0.0077, -0.0077]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.2541, -3.7369, -0.4583,  5.0632, -0.4583,  4.7703, -2.2056,  4.3516,
        -4.2221, -0.4583], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 2.5085e+00,  5.5258e+00,  3.5083e-03, -7.3156e+00,  3.5082e-03,
         -6.9638e+00,  2.3349e+00, -5.8705e+00,  6.6281e+00,  3.5082e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -8.2141,   0.1877],
        [  6.5677,  -0.1128],
        [  1.1788,  -4.9546],
        [ -5.8002,   1.7047],
        [  7.6594,   8.2870],
        [-11.6127,  -6.2594],
        [ -4.4664,  -9.9022],
        [ -1.0815,   1.1127],
        [  0.7527,   6.2908],
        [-12.6303,  -1.1814]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  7.7597,  -4.3628,  -4.0279,   3.3738,   4.9879,  -4.8370, -10.8943,
         -2.0359,   4.5532,   2.6961], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[  7.2112,  -3.5337,  -0.9465,  -3.3719,  -7.3406,   2.8733,  -0.2522,
          -0.0702,  -4.2436,  -7.8882],
        [ -0.8516,  -1.1746,  -0.9227,  -0.1738,  -1.1416,  -0.2935,  -0.2696,
          -0.0444,  -0.7283,  -0.3108],
        [  3.3658,  -7.3839,  -4.3405,   3.7791,  -3.1675,   6.3931,   3.8042,
           0.0396,  -0.6845,  -1.0510],
        [ -0.8517,  -1.1749,  -0.9229,  -0.1738,  -1.1416,  -0.2935,  -0.2697,
          -0.0444,  -0.7284,  -0.3108],
        [  5.0549,  -2.1244,   1.2935,  -0.0224, -11.5917,   0.1304,  -6.3168,
           0.0561,  -5.5288,  -3.9485],
        [  0.5700,  -1.8749,   0.3560,  -0.1902,  -4.1606,   0.3189,  -3.6514,
          -0.0415,  -1.6141,  -0.9666],
        [  1.0555,  -2.1658,   3.1517,  -0.1723, -10.1977,   4.0866, -11.7696,
          -0.1431,  -9.7452,  -0.5951],
        [  2.0249,  -2.1829,  -3.7047,   4.6278,   0.2058,   0.6180,   0.8146,
          -0.0840,   1.1467,  -0.1300],
        [  3.3604,  -0.5829,   2.6049,  -2.7258,  -3.8574,  -0.6800,  -5.1891,
          -0.0256,  -3.0801,  -0.8688],
        [  2.3411, -10.5099,  -2.8983,  -0.2527,   1.8590,   6.8994,   5.4947,
          -0.3165,  -0.9977,  -0.9235]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.3252, -1.5711, -4.0655, -1.5707,  0.8331, -0.3138,  0.7279, -1.9825,
        -0.2832, -4.8127], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-5.7558,  0.0463,  5.3260,  0.0451, -5.5184, -0.9354, -9.1846,  0.5775,
         -1.0054,  9.8601],
        [ 5.7521, -0.0456, -5.2222, -0.0467,  5.5189,  0.9302,  9.1949, -0.6597,
          0.7868, -9.8554]], device='cuda:0'))])
xi:  [-489.18747]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 236.22842599250447
W_T_median: 111.27347393115159
W_T_pctile_5: -486.16309455435294
W_T_CVAR_5_pct: -595.9995995742147
Average q (qsum/M+1):  57.14657494329637
Optimal xi:  [-489.18747]
Expected(across Rb) median(across samples) p_equity:  0.29015198689885435
obj fun:  tensor(-1741.7421, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1634.7107909593517
Current xi:  [-21.1937]
objective value function right now is: -1634.7107909593517
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1651.5215533767648
Current xi:  [-37.763756]
objective value function right now is: -1651.5215533767648
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1657.2426058514975
Current xi:  [-50.719597]
objective value function right now is: -1657.2426058514975
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1661.5354965903866
Current xi:  [-71.38725]
objective value function right now is: -1661.5354965903866
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1663.0284007515597
Current xi:  [-83.91534]
objective value function right now is: -1663.0284007515597
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1664.2318560580534
Current xi:  [-101.39584]
objective value function right now is: -1664.2318560580534
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1666.6381319143566
Current xi:  [-118.67369]
objective value function right now is: -1666.6381319143566
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1667.3426898534854
Current xi:  [-132.70308]
objective value function right now is: -1667.3426898534854
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1670.0822392864154
Current xi:  [-152.02713]
objective value function right now is: -1670.0822392864154
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1670.7909407771435
Current xi:  [-164.16632]
objective value function right now is: -1670.7909407771435
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-175.62344]
objective value function right now is: -1670.4619222586725
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1671.9364044241088
Current xi:  [-191.93356]
objective value function right now is: -1671.9364044241088
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.4912956184676
Current xi:  [-203.05544]
objective value function right now is: -1672.4912956184676
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-210.18404]
objective value function right now is: -1672.4373388968386
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.7851222194045
Current xi:  [-217.11378]
objective value function right now is: -1672.7851222194045
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.8643829971547
Current xi:  [-224.50607]
objective value function right now is: -1672.8643829971547
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-233.31755]
objective value function right now is: -1672.7983128769574
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-239.8295]
objective value function right now is: -1672.5097535477537
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.103529441265
Current xi:  [-243.61751]
objective value function right now is: -1673.103529441265
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.1875959477156
Current xi:  [-244.26064]
objective value function right now is: -1673.1875959477156
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.37088]
objective value function right now is: -1672.7462277669927
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.1880762754668
Current xi:  [-244.67107]
objective value function right now is: -1673.1880762754668
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.2268997867243
Current xi:  [-245.11127]
objective value function right now is: -1673.2268997867243
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.3057431324805
Current xi:  [-245.49959]
objective value function right now is: -1673.3057431324805
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.16496]
objective value function right now is: -1673.0691073877567
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.40283]
objective value function right now is: -1673.0139353884913
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.3343788346206
Current xi:  [-244.65791]
objective value function right now is: -1673.3343788346206
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-244.96214]
objective value function right now is: -1673.0360212939934
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1673.3476517645302
Current xi:  [-245.08134]
objective value function right now is: -1673.3476517645302
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.50003]
objective value function right now is: -1673.3055874687136
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.91064]
objective value function right now is: -1672.9334384350734
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.352817338899
Current xi:  [-245.23203]
objective value function right now is: -1673.352817338899
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.5300471045398
Current xi:  [-245.01634]
objective value function right now is: -1673.5300471045398
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.11105]
objective value function right now is: -1673.3378007420456
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.48463]
objective value function right now is: -1673.1772385079944
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.6331524037046
Current xi:  [-244.68114]
objective value function right now is: -1673.6331524037046
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.59804]
objective value function right now is: -1673.4952076142627
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.66412]
objective value function right now is: -1673.5263881621893
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.6407398537933
Current xi:  [-244.47913]
objective value function right now is: -1673.6407398537933
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.50157]
objective value function right now is: -1673.6392268544068
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.67017]
objective value function right now is: -1673.5528569885967
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.6953531143038
Current xi:  [-244.88272]
objective value function right now is: -1673.6953531143038
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.90642]
objective value function right now is: -1673.6593606782815
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.79385]
objective value function right now is: -1673.5339407265606
new min fval from sgd:  -1673.7050618121848
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.69078]
objective value function right now is: -1673.7050618121848
new min fval from sgd:  -1673.7103692887592
new min fval from sgd:  -1673.7160283731293
new min fval from sgd:  -1673.719598259815
new min fval from sgd:  -1673.7222577746077
new min fval from sgd:  -1673.7229954391921
new min fval from sgd:  -1673.723205210874
new min fval from sgd:  -1673.726393591498
new min fval from sgd:  -1673.728962735987
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.86935]
objective value function right now is: -1673.6426904879138
new min fval from sgd:  -1673.7304981113894
new min fval from sgd:  -1673.7324257623623
new min fval from sgd:  -1673.7329797671614
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.7244]
objective value function right now is: -1673.6142823175703
new min fval from sgd:  -1673.73357057356
new min fval from sgd:  -1673.734479588112
new min fval from sgd:  -1673.7388527790042
new min fval from sgd:  -1673.74020705245
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.61203]
objective value function right now is: -1673.6783154129594
new min fval from sgd:  -1673.7408694409612
new min fval from sgd:  -1673.742304367661
new min fval from sgd:  -1673.743091212684
new min fval from sgd:  -1673.7432373801535
new min fval from sgd:  -1673.74356095318
new min fval from sgd:  -1673.7443315016235
new min fval from sgd:  -1673.7455833693775
new min fval from sgd:  -1673.7457179408493
new min fval from sgd:  -1673.7459221871168
new min fval from sgd:  -1673.74618590158
new min fval from sgd:  -1673.7466415258427
new min fval from sgd:  -1673.747261002393
new min fval from sgd:  -1673.7479473285794
new min fval from sgd:  -1673.7480532292834
new min fval from sgd:  -1673.7481966342182
new min fval from sgd:  -1673.7483049874093
new min fval from sgd:  -1673.748573310365
new min fval from sgd:  -1673.7488333485446
new min fval from sgd:  -1673.748987867289
new min fval from sgd:  -1673.7490804634176
new min fval from sgd:  -1673.7492140241957
new min fval from sgd:  -1673.7493315650777
new min fval from sgd:  -1673.7497518427328
new min fval from sgd:  -1673.7498291315599
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.48297]
objective value function right now is: -1673.7414010621553
new min fval from sgd:  -1673.750022716956
new min fval from sgd:  -1673.7502769717678
new min fval from sgd:  -1673.7505708403805
new min fval from sgd:  -1673.7507751556511
new min fval from sgd:  -1673.7509637856372
new min fval from sgd:  -1673.7511120147951
new min fval from sgd:  -1673.7514182174327
new min fval from sgd:  -1673.7516607605126
new min fval from sgd:  -1673.7518239612914
new min fval from sgd:  -1673.751912742791
new min fval from sgd:  -1673.7520261388486
new min fval from sgd:  -1673.7521025959823
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.48538]
objective value function right now is: -1673.7348721484234
min fval:  -1673.7521025959823
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-3.0275, -4.7378],
        [-3.1715, -5.1084],
        [-4.3121,  4.1404],
        [-3.1700, -5.1186],
        [-6.5402, -6.1907],
        [-2.8810, -4.2298],
        [-3.2222,  5.1964],
        [-3.4635,  4.7705],
        [-3.2585, -5.3586],
        [-3.2923,  4.3661]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-5.5654, -5.8626,  5.6806, -5.8780, -0.6899, -5.1921,  5.4400,  5.2532,
        -6.0886,  4.4373], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.8771, -2.4585,  3.6502, -2.2968, -2.7939, -1.3742,  3.0507,  3.0647,
         -2.7566,  2.1714],
        [ 2.2446,  2.7626, -3.0394,  2.8244,  1.8002,  1.5529, -2.6705, -2.5594,
          3.1698, -1.6585],
        [-0.1827, -0.2082, -0.0681, -0.2066, -1.7160, -0.1535, -0.0890, -0.0989,
         -0.2237, -0.0955],
        [-2.6045, -3.2177,  4.7070, -3.1811, -3.3955, -2.1169,  4.3559,  4.1607,
         -3.8093,  2.9876],
        [ 2.5902,  3.2464, -3.9422,  3.1620,  2.8679,  1.9615, -3.9120, -3.6819,
          3.7805, -2.3845],
        [-0.0349, -0.0376, -0.1023, -0.0374, -0.4380, -0.0286, -0.0310, -0.0406,
         -0.0383, -0.0259],
        [ 2.3181,  2.9401, -3.2046,  2.8190,  2.3854,  1.7426, -3.0473, -2.7911,
          3.2692, -1.8180],
        [-1.8177, -2.2359,  3.3666, -2.3013, -1.9353, -1.2226,  2.5961,  2.6351,
         -2.7092,  1.9270],
        [ 0.8885,  1.1270, -1.0289,  1.1234,  0.8140,  0.6540, -0.6556, -0.6452,
          1.2666, -0.3312],
        [-1.8401, -2.4186,  3.3782, -2.4085, -1.7749, -1.2378,  2.6069,  2.6383,
         -2.7827,  1.9204]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 1.7096, -1.9195, -0.2749,  2.1404, -2.3150, -0.5219, -2.3561,  1.1264,
        -1.6419,  1.0707], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 4.7946, -5.0769,  0.7049,  7.4657, -7.1120, -0.0097, -5.6346,  4.0245,
         -1.7335,  4.0654]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  1.2306,  -0.5145],
        [  7.6004,  -0.3478],
        [  6.9618,   8.9901],
        [  8.9534,  -3.0214],
        [ 10.4462,   2.4400],
        [  5.6109,  -1.0307],
        [ -6.4060, -10.5899],
        [-12.8558,  -6.1617],
        [  4.0502,  -1.4697],
        [ -0.9550,   8.9438]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -6.2819,  -7.2778,   8.5304,  -6.4745,  -1.2076,  -7.6755, -10.9446,
         -3.0484,  -7.8939,   7.4999], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[  1.0903,   0.5856,   0.8122,  -1.0276,  -5.3208,   0.8152,  -7.3969,
           4.0168,   0.8693,   2.6990],
        [ -1.0334,  -2.3979,   0.8882,  -1.0855,  -1.5001,  -0.9109,   0.2620,
          -3.6127,   0.1638,  -5.3037],
        [  0.0722,  -3.1333,   2.2885,  -3.5184,  -2.6788,  -2.5107,  -7.3328,
           3.2672,  -1.6017, -12.5463],
        [ -1.1045,  -5.0404,  -9.8483,  -7.2838,  -9.0612,  -4.1688,   6.0550,
          10.8908,  -3.6352, -10.1168],
        [ -0.1872,  -0.3286,  -0.9319,  -1.0534,  -1.1102,  -0.2280,  -0.7457,
          -0.0733,  -0.2531,   0.3416],
        [ -2.7658,  -8.4704,  -9.7587,   0.1777,  -6.3044,  -5.1247,   6.4749,
           6.0799,  -3.3105,   0.6606],
        [ -2.6339,   0.1330,  -1.8092,  -3.3799,   0.5985,   0.8723,   0.0701,
          -1.4910,   0.4608,  -2.0882],
        [ -0.0865,  -3.2308,   0.9028,  -2.3194,  -2.0083,  -1.2679,  -3.7686,
          -1.8515,  -0.5951,   4.1389],
        [ -0.1865,  -0.3270,  -0.9293,  -1.0517,  -1.1100,  -0.2272,  -0.7472,
          -0.0733,  -0.2527,   0.3445],
        [ -0.1555,  -0.3383,  -0.8992,  -1.0756,  -1.0961,  -0.2038,  -0.7495,
          -0.0744,  -0.2170,   0.3856]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.0945, -0.0280,  0.6726, -2.5394, -1.8406, -0.7455, -1.0001, -0.7770,
        -1.8396, -1.8240], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  5.7234,  -2.3000,  -8.9088,  13.7786,   0.3000,  -5.2175,  -1.8345,
           0.5015,   0.3012,   0.3455],
        [ -5.6660,   2.3631,   8.8852, -13.7802,  -0.2997,   5.2672,   1.8860,
          -0.9457,  -0.3022,  -0.3016]], device='cuda:0'))])
xi:  [-244.49423]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 284.73446868572177
W_T_median: 78.84192098756029
W_T_pctile_5: -244.38015152732711
W_T_CVAR_5_pct: -334.6116432885806
Average q (qsum/M+1):  56.150784400201616
Optimal xi:  [-244.49423]
Expected(across Rb) median(across samples) p_equity:  0.2962992333632428
obj fun:  tensor(-1673.7521, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1574.5323327034596
Current xi:  [-19.822624]
objective value function right now is: -1574.5323327034596
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1585.150975014973
Current xi:  [-34.50441]
objective value function right now is: -1585.150975014973
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1592.0276122236564
Current xi:  [-34.70487]
objective value function right now is: -1592.0276122236564
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1596.526827323282
Current xi:  [-34.849644]
objective value function right now is: -1596.526827323282
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1599.4887566337127
Current xi:  [-34.97885]
objective value function right now is: -1599.4887566337127
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.0027300021206
Current xi:  [-34.944145]
objective value function right now is: -1602.0027300021206
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-34.931946]
objective value function right now is: -1601.85638349612
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.7927667894628
Current xi:  [-34.974026]
objective value function right now is: -1602.7927667894628
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1603.1650513812547
Current xi:  [-34.97503]
objective value function right now is: -1603.1650513812547
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1603.3178449870327
Current xi:  [-34.932102]
objective value function right now is: -1603.3178449870327
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1603.8177648307235
Current xi:  [-34.954098]
objective value function right now is: -1603.8177648307235
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.971344]
objective value function right now is: -1603.016218185838
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.012367]
objective value function right now is: -1603.3470197163854
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-35.005676]
objective value function right now is: -1601.3208134398963
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.974667]
objective value function right now is: -1600.773220934588
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.2179880502397
Current xi:  [-34.99539]
objective value function right now is: -1604.2179880502397
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.8304678652323
Current xi:  [-35.032387]
objective value function right now is: -1604.8304678652323
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.97293]
objective value function right now is: -1604.6687037063605
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.976444]
objective value function right now is: -1604.4061637371415
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.952187]
objective value function right now is: -1604.667893480652
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.021984]
objective value function right now is: -1604.2587737631097
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.995434]
objective value function right now is: -1604.232301029497
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.009926]
objective value function right now is: -1604.4864680583487
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.956394]
objective value function right now is: -1604.6800397712905
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.936596]
objective value function right now is: -1604.0829347146548
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.9608232067555
Current xi:  [-34.94591]
objective value function right now is: -1604.9608232067555
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.0924]
objective value function right now is: -1604.480908559357
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1604.9933648548738
Current xi:  [-34.9718]
objective value function right now is: -1604.9933648548738
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-35.0935]
objective value function right now is: -1604.812441576091
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.058903]
objective value function right now is: -1598.1205091444222
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.94211]
objective value function right now is: -1604.5068794375584
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.0615690668076
Current xi:  [-35.000465]
objective value function right now is: -1605.0615690668076
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.02663]
objective value function right now is: -1604.4132004574155
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.967358]
objective value function right now is: -1604.1181076235105
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.969803]
objective value function right now is: -1604.6339395565678
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.2895846228162
Current xi:  [-34.963333]
objective value function right now is: -1605.2895846228162
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.5527473364261
Current xi:  [-34.97698]
objective value function right now is: -1605.5527473364261
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.991688]
objective value function right now is: -1605.5206835705187
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.985832]
objective value function right now is: -1605.501280121196
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.977318]
objective value function right now is: -1605.552680166785
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.5787653903571
Current xi:  [-34.967205]
objective value function right now is: -1605.5787653903571
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.98046]
objective value function right now is: -1605.4979131489706
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.97488]
objective value function right now is: -1605.506345332517
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.997726]
objective value function right now is: -1605.3384790833798
new min fval from sgd:  -1605.6833175654979
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.993073]
objective value function right now is: -1605.6833175654979
new min fval from sgd:  -1605.6866886272285
new min fval from sgd:  -1605.686762632416
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.991463]
objective value function right now is: -1605.4734623193476
new min fval from sgd:  -1605.6896587452786
new min fval from sgd:  -1605.717169270765
new min fval from sgd:  -1605.7251514000866
new min fval from sgd:  -1605.72873476402
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.991226]
objective value function right now is: -1605.5784890074392
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.98367]
objective value function right now is: -1605.5391471816822
new min fval from sgd:  -1605.7293985854833
new min fval from sgd:  -1605.7316365456638
new min fval from sgd:  -1605.7329684998329
new min fval from sgd:  -1605.7352942825385
new min fval from sgd:  -1605.7381744678376
new min fval from sgd:  -1605.7401486605224
new min fval from sgd:  -1605.741478191151
new min fval from sgd:  -1605.7417684515567
new min fval from sgd:  -1605.7429212710908
new min fval from sgd:  -1605.7442267220001
new min fval from sgd:  -1605.7446332847906
new min fval from sgd:  -1605.7459550439562
new min fval from sgd:  -1605.7472746861604
new min fval from sgd:  -1605.7481920484304
new min fval from sgd:  -1605.7483961431785
new min fval from sgd:  -1605.7501652477854
new min fval from sgd:  -1605.7525391896897
new min fval from sgd:  -1605.7539545618283
new min fval from sgd:  -1605.7542763974118
new min fval from sgd:  -1605.7547234351775
new min fval from sgd:  -1605.7548990065554
new min fval from sgd:  -1605.7577931332785
new min fval from sgd:  -1605.76298815723
new min fval from sgd:  -1605.7670378003404
new min fval from sgd:  -1605.7696338016935
new min fval from sgd:  -1605.7709845950794
new min fval from sgd:  -1605.7720393922168
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.97635]
objective value function right now is: -1605.7689756441537
new min fval from sgd:  -1605.7724556248852
new min fval from sgd:  -1605.7738524712363
new min fval from sgd:  -1605.7749537477725
new min fval from sgd:  -1605.775113429846
new min fval from sgd:  -1605.775521006343
new min fval from sgd:  -1605.776089354353
new min fval from sgd:  -1605.7763952509354
new min fval from sgd:  -1605.7781269435236
new min fval from sgd:  -1605.78017443144
new min fval from sgd:  -1605.783008406779
new min fval from sgd:  -1605.785498026767
new min fval from sgd:  -1605.7858094289536
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.968655]
objective value function right now is: -1605.7613884743614
min fval:  -1605.7858094289536
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -4.8663,   4.2625],
        [-14.5185,  -7.0941],
        [ -7.8677,  -7.4985],
        [  2.4108,  -0.7990],
        [  4.6688,  -4.4774],
        [  3.9785,  -3.3433],
        [  4.6820,  -4.4968],
        [ -5.3394,   4.9685],
        [ -6.4533,  -8.0997],
        [ -4.9473,   4.4318]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 3.7411, -4.7282, -3.9794, -5.6046, -4.9698, -4.7167, -4.9995,  4.5500,
         3.2812,  4.0061], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 3.8451e+00, -4.7302e+00, -4.5622e+00, -3.2524e-01, -3.0223e+00,
         -1.9635e+00, -3.0715e+00,  5.0869e+00,  3.1088e+00,  4.2054e+00],
        [-6.0916e+00,  8.2708e+00,  5.8323e+00,  6.2405e-01,  3.1463e+00,
          1.8268e+00,  3.1260e+00, -7.6831e+00, -3.7622e+00, -6.3446e+00],
        [-2.6220e-02, -1.9503e-02, -6.2532e-02, -6.3096e-03, -2.9950e-01,
         -1.9543e-01, -2.9869e-01, -2.8041e-02, -3.7317e-01, -2.9193e-02],
        [-4.3074e+00,  6.3548e+00,  5.0283e+00,  4.8101e-01,  2.8789e+00,
          1.8876e+00,  3.0578e+00, -5.2707e+00, -3.6895e+00, -4.3641e+00],
        [-2.6220e-02, -1.9503e-02, -6.2533e-02, -6.3096e-03, -2.9950e-01,
         -1.9543e-01, -2.9869e-01, -2.8041e-02, -3.7317e-01, -2.9193e-02],
        [-3.4760e+00,  5.3476e+00,  4.0770e+00,  1.2120e-01,  2.0658e+00,
          1.0937e+00,  2.1154e+00, -4.5944e+00, -3.1146e+00, -3.7688e+00],
        [ 1.7626e+00, -3.7663e+00, -2.1804e+00,  5.6485e-02, -9.3561e-01,
         -3.3475e-01, -8.6555e-01,  2.4203e+00,  2.6844e+00,  2.0195e+00],
        [-3.4166e+00,  5.3714e+00,  4.2546e+00,  2.5099e-01,  1.9855e+00,
          1.3975e+00,  2.4936e+00, -4.4516e+00, -3.1829e+00, -3.6467e+00],
        [ 4.8971e+00, -6.7219e+00, -4.9377e+00, -6.4720e-01, -3.2838e+00,
         -1.9768e+00, -3.1665e+00,  6.0671e+00,  3.8805e+00,  5.1111e+00],
        [ 3.5724e+00, -4.0586e+00, -4.1554e+00, -3.2228e-01, -2.8942e+00,
         -1.6605e+00, -2.8583e+00,  4.7443e+00,  2.9630e+00,  3.6675e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 1.2024, -1.6929, -0.4470, -2.2404, -0.4470, -1.4769,  1.6121, -1.8938,
         1.3428,  0.7400], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 6.8833e+00, -1.0843e+01,  1.6974e-03, -6.3371e+00,  1.6974e-03,
         -4.2978e+00,  4.5437e+00, -4.3875e+00,  9.6781e+00,  5.9483e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  4.9529,  -1.6975],
        [ 11.1372,   6.8433],
        [  6.4994,  -1.4483],
        [  1.4842,  -3.7240],
        [ -9.0085,   6.7723],
        [  0.5933,   6.2559],
        [  1.9609,   0.8447],
        [-11.2035,  -3.8836],
        [-13.9595,  -0.3578],
        [ -0.1820,  10.3074]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-9.2772,  4.8164, -9.2190, -9.1359,  7.7761, -0.1564,  6.0711, -1.3627,
         1.4948,  9.4078], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -1.4895,  -1.1726,  -1.9194,  -1.7340,   0.8793,  -0.7711,  -1.1738,
           0.4461,   1.4965,   0.0261],
        [ -0.8661,   0.2068,  -0.6132,  -4.7934,  -8.2197,   7.1983,  -0.6108,
           3.8385,   6.4663,  -2.3207],
        [ -1.4895,  -1.1726,  -1.9194,  -1.7340,   0.8793,  -0.7711,  -1.1738,
           0.4461,   1.4965,   0.0261],
        [  1.4565,   2.4603,   3.4415,   1.4830,   9.0953,  -7.3007,  -3.7652,
          -5.9321,   0.7794,   2.1243],
        [ -3.0144, -19.8899,  -6.7403,  -9.3280,  -4.5548,  -0.1294,   1.0783,
           8.6593,   9.1001, -21.0239],
        [ -5.3378,  -9.0538,  -9.3599,  -3.0495,  -6.1636,  -0.4644,   3.5754,
           5.8819,   6.5542, -23.3194],
        [ -1.4895,  -1.1726,  -1.9194,  -1.7340,   0.8793,  -0.7711,  -1.1738,
           0.4461,   1.4965,   0.0261],
        [ -1.5084,  -1.1541,  -1.9375,  -1.7529,   0.8836,  -0.7806,  -1.1433,
           0.4748,   1.6012,   0.0295],
        [ -0.6176,  -1.5397,  -0.6959,  -0.6288,  -0.1419,  -0.0395,  -1.7049,
          -0.6990,  -0.0404,  -0.6444],
        [ -1.4895,  -1.1726,  -1.9194,  -1.7340,   0.8793,  -0.7711,  -1.1737,
           0.4461,   1.4966,   0.0261]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.1629, -1.4150, -2.1629, -2.2124,  1.0933,  2.5761, -2.1629, -2.1353,
        -2.4566, -2.1629], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-7.2833e-01,  2.6804e+00, -7.3282e-01,  6.5583e-01,  1.2852e+01,
         -7.2265e+00, -7.2831e-01, -7.4426e-01,  8.2874e-03, -7.3207e-01],
        [ 7.2987e-01, -2.4677e+00,  7.2538e-01, -7.9434e-01, -1.2850e+01,
          6.9254e+00,  7.2988e-01,  7.7020e-01, -5.2569e-02,  7.2615e-01]],
       device='cuda:0'))])
xi:  [-34.9759]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 413.2838944203585
W_T_median: 145.04500716215156
W_T_pctile_5: -34.97213019552945
W_T_CVAR_5_pct: -137.2178794270946
Average q (qsum/M+1):  54.012722876764116
Optimal xi:  [-34.9759]
Expected(across Rb) median(across samples) p_equity:  0.34729577650626503
obj fun:  tensor(-1605.7858, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1523.1185465724263
Current xi:  [-7.567885]
objective value function right now is: -1523.1185465724263
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1535.3578304333837
Current xi:  [0.3613958]
objective value function right now is: -1535.3578304333837
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0768152]
objective value function right now is: -1532.2783133684857
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1543.2616770554241
Current xi:  [0.10544317]
objective value function right now is: -1543.2616770554241
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1546.0279931087043
Current xi:  [0.04844234]
objective value function right now is: -1546.0279931087043
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1554.3146527991958
Current xi:  [0.03181148]
objective value function right now is: -1554.3146527991958
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1556.4996278168314
Current xi:  [-0.10029006]
objective value function right now is: -1556.4996278168314
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00091015]
objective value function right now is: -1552.8198462892278
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1557.5054627090637
Current xi:  [0.00556387]
objective value function right now is: -1557.5054627090637
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.09691492]
objective value function right now is: -1556.1116931832519
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1557.5595325586344
Current xi:  [0.02292744]
objective value function right now is: -1557.5595325586344
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0004512]
objective value function right now is: -1556.576692310714
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1558.4503035074263
Current xi:  [-0.08676983]
objective value function right now is: -1558.4503035074263
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [0.01010406]
objective value function right now is: -1556.8164176100731
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06549826]
objective value function right now is: -1557.5799301383943
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01537586]
objective value function right now is: -1556.3387014754344
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.09437197]
objective value function right now is: -1556.8465819982193
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05452072]
objective value function right now is: -1558.016335775483
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0188734]
objective value function right now is: -1556.5892898984532
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1559.2069604928106
Current xi:  [0.00177627]
objective value function right now is: -1559.2069604928106
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0002122]
objective value function right now is: -1556.7205645500314
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00748822]
objective value function right now is: -1558.6309027871778
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00889578]
objective value function right now is: -1558.4909442941407
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00745366]
objective value function right now is: -1557.9179497419868
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01896412]
objective value function right now is: -1557.8738530173896
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.07831804]
objective value function right now is: -1555.6518232254618
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1559.2870523047295
Current xi:  [0.00574683]
objective value function right now is: -1559.2870523047295
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00904256]
objective value function right now is: -1558.310410456969
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [0.0021486]
objective value function right now is: -1559.0699993747137
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01019973]
objective value function right now is: -1558.6438327196624
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01240695]
objective value function right now is: -1557.618706195351
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00742524]
objective value function right now is: -1558.4778169793706
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00284406]
objective value function right now is: -1557.1370830629744
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02423822]
objective value function right now is: -1556.6896371995326
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00328263]
objective value function right now is: -1558.1922072317825
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1560.0672682833056
Current xi:  [0.00453042]
objective value function right now is: -1560.0672682833056
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0086253]
objective value function right now is: -1559.9350362825157
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00055713]
objective value function right now is: -1557.7344944787976
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1560.0969566618633
Current xi:  [0.00696799]
objective value function right now is: -1560.0969566618633
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00379071]
objective value function right now is: -1558.230662641483
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1560.2255976524984
Current xi:  [0.00382978]
objective value function right now is: -1560.2255976524984
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00179414]
objective value function right now is: -1560.1372007643608
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1560.3239318958745
Current xi:  [-0.00265209]
objective value function right now is: -1560.3239318958745
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00698317]
objective value function right now is: -1560.1028444301767
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00029303]
objective value function right now is: -1560.2786084086458
new min fval from sgd:  -1560.3327465277273
new min fval from sgd:  -1560.3489221831953
new min fval from sgd:  -1560.3775532717004
new min fval from sgd:  -1560.3895095090777
new min fval from sgd:  -1560.3916164405266
new min fval from sgd:  -1560.4038234458278
new min fval from sgd:  -1560.423287683035
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00011235]
objective value function right now is: -1560.1732210217199
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0013654]
objective value function right now is: -1559.9392699053756
new min fval from sgd:  -1560.428142065388
new min fval from sgd:  -1560.4289627945195
new min fval from sgd:  -1560.4301053528275
new min fval from sgd:  -1560.4379079265182
new min fval from sgd:  -1560.4384898432395
new min fval from sgd:  -1560.4452248263478
new min fval from sgd:  -1560.4496672256532
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00303291]
objective value function right now is: -1560.1728283324226
new min fval from sgd:  -1560.4681742291064
new min fval from sgd:  -1560.4759032073177
new min fval from sgd:  -1560.4801734811267
new min fval from sgd:  -1560.4836203165096
new min fval from sgd:  -1560.4872542670655
new min fval from sgd:  -1560.4914933912196
new min fval from sgd:  -1560.4949326702126
new min fval from sgd:  -1560.4990828275795
new min fval from sgd:  -1560.5018463356203
new min fval from sgd:  -1560.5033579065375
new min fval from sgd:  -1560.5045030038643
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00577779]
objective value function right now is: -1560.398328579852
new min fval from sgd:  -1560.5085427226602
new min fval from sgd:  -1560.509724502584
new min fval from sgd:  -1560.5102829483874
new min fval from sgd:  -1560.5123064296533
new min fval from sgd:  -1560.5127477841206
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00752309]
objective value function right now is: -1560.4887337851567
min fval:  -1560.5127477841206
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-4.0704, -4.7371],
        [ 7.4518, -0.1482],
        [ 7.4472,  1.1472],
        [-1.1361, -8.5983],
        [-8.7857, -4.9094],
        [ 7.1459, -3.2520],
        [-4.4308, -6.1670],
        [-6.5564,  4.6549],
        [-6.2374, -7.9221],
        [ 3.6723, -7.0056]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-4.5159, -6.6641, -6.6700, -7.2207,  8.7349, -6.5512, -5.0523,  5.8478,
        -6.2173, -6.0554], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.0854, -4.4176, -4.4846, -6.3818,  5.9449, -5.7466, -2.6226,  6.9399,
         -5.2193, -5.4864],
        [-0.0123, -0.2531, -0.2516, -0.0613, -0.5533, -0.3142, -0.0217, -0.2989,
         -0.0159, -0.2837],
        [ 1.7769,  4.6538,  4.6618,  6.8144, -6.0820,  5.7807,  3.5945, -8.2321,
          6.7624,  4.6625],
        [ 2.1165,  4.5491,  4.6083,  6.4905, -7.5044,  4.4015,  3.6312, -6.6703,
          7.0490,  4.5620],
        [-1.4274, -1.9594, -2.3750, -4.6800,  5.3957, -2.3651, -2.6237,  4.8760,
         -4.3756, -3.1566],
        [-0.0123, -0.2531, -0.2516, -0.0613, -0.5533, -0.3142, -0.0217, -0.2989,
         -0.0159, -0.2837],
        [-0.0123, -0.2531, -0.2516, -0.0613, -0.5533, -0.3142, -0.0217, -0.2989,
         -0.0159, -0.2837],
        [-0.0123, -0.2531, -0.2516, -0.0613, -0.5533, -0.3142, -0.0217, -0.2989,
         -0.0159, -0.2837],
        [ 2.5644,  4.4541,  4.8261,  6.5233, -6.9509,  4.5587,  4.2648, -7.8175,
          7.2871,  4.3087],
        [-0.0123, -0.2531, -0.2516, -0.0613, -0.5533, -0.3142, -0.0217, -0.2989,
         -0.0159, -0.2837]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 0.2528, -0.8097, -0.9097, -1.4118,  1.0535, -0.8097, -0.8097, -0.8097,
        -1.6494, -0.8097], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.0165e+01, -1.5590e-03, -6.8016e+00, -7.0455e+00,  5.4342e+00,
         -1.5590e-03, -1.5590e-03, -1.5590e-03, -7.4136e+00, -1.5590e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-10.1219,  -3.5481],
        [ -2.6458,   8.2564],
        [  0.2705,   5.6243],
        [  6.8806,   1.8697],
        [-11.2595,  -2.1285],
        [ -0.2199,   7.2030],
        [  4.0154,   5.6199],
        [-14.6394,  -3.4342],
        [ -3.2261,   8.1542],
        [-20.1216,  -7.6198]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-3.7291,  7.4446,  4.0129,  2.6957,  4.1313,  6.6540,  5.1856,  0.9583,
         7.5317, -5.4779], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -0.4243,  -2.3829,  -0.8356,  -2.3005,   1.4193,  -0.2549,  -0.0621,
          -1.0570,  -2.2980,  -0.6294],
        [ -0.5332, -16.2890,  -1.6971,  -1.1210,   5.3214,  -7.2366,  -0.9294,
           1.1310, -15.1809, -14.8992],
        [ -3.3393,  16.1450,   4.2591,   3.5198,  -7.4458,   9.7865,   6.0983,
         -13.1158,  16.0596, -12.7686],
        [  4.7239,  -9.8144,  -6.1776,  -3.3590,   0.0340, -15.2975, -14.3715,
           8.4296,  -7.3083,  24.5907],
        [ -0.4243,  -2.3829,  -0.8356,  -2.3005,   1.4193,  -0.2549,  -0.0621,
          -1.0570,  -2.2980,  -0.6294],
        [ -0.4243,  -2.3829,  -0.8356,  -2.3005,   1.4193,  -0.2549,  -0.0621,
          -1.0570,  -2.2980,  -0.6294],
        [ -0.5062,   2.9200,  -0.2526,   1.3064,   6.8405,  -1.5137,   1.5037,
           0.8224,   5.0345,  -0.7084],
        [ -0.4243,  -2.3829,  -0.8356,  -2.3005,   1.4193,  -0.2549,  -0.0621,
          -1.0570,  -2.2980,  -0.6294],
        [ -0.4243,  -2.3829,  -0.8356,  -2.3005,   1.4193,  -0.2549,  -0.0621,
          -1.0570,  -2.2980,  -0.6294],
        [ -0.4243,  -2.3829,  -0.8356,  -2.3005,   1.4193,  -0.2548,  -0.0621,
          -1.0570,  -2.2980,  -0.6294]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.3633, -2.3268,  1.6197, -0.7892, -2.3633, -2.3633,  1.2614, -2.3633,
        -2.3633, -2.3633], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 0.8172, -3.5952,  2.7418,  9.7539,  0.8204,  0.8208, -2.3468,  0.8034,
          0.8315,  0.8185],
        [-0.8172,  3.6083, -2.6373, -9.6670, -0.8140, -0.8135,  2.4458, -0.8310,
         -0.8029, -0.8158]], device='cuda:0'))])
xi:  [0.00592786]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 612.0627633085476
W_T_median: 227.19319268089026
W_T_pctile_5: 0.006637952956586801
W_T_CVAR_5_pct: -73.65317217439495
Average q (qsum/M+1):  52.71501701108871
Optimal xi:  [0.00592786]
Expected(across Rb) median(across samples) p_equity:  0.35710007200638455
obj fun:  tensor(-1560.5127, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1473.7326273224876
Current xi:  [5.828631]
objective value function right now is: -1473.7326273224876
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1491.7384479572527
Current xi:  [0.6434467]
objective value function right now is: -1491.7384479572527
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1498.6256751158364
Current xi:  [0.09630806]
objective value function right now is: -1498.6256751158364
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1515.4472124002982
Current xi:  [0.02403433]
objective value function right now is: -1515.4472124002982
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1520.806684274136
Current xi:  [-0.02432641]
objective value function right now is: -1520.806684274136
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1522.8595744565635
Current xi:  [-0.01983184]
objective value function right now is: -1522.8595744565635
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [0.01469546]
objective value function right now is: -1522.4145594437703
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1523.2171490126136
Current xi:  [0.00650225]
objective value function right now is: -1523.2171490126136
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1524.0825921797575
Current xi:  [-0.00860761]
objective value function right now is: -1524.0825921797575
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01284871]
objective value function right now is: -1523.8453857739685
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00513191]
objective value function right now is: -1522.1717405916052
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03050338]
objective value function right now is: -1519.700377130134
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1524.7670938943204
Current xi:  [-0.02209614]
objective value function right now is: -1524.7670938943204
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01366927]
objective value function right now is: -1524.3822137382617
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0131338]
objective value function right now is: -1523.274261849015
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1525.1439298763407
Current xi:  [-0.02737966]
objective value function right now is: -1525.1439298763407
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00330559]
objective value function right now is: -1520.435942408357
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00651378]
objective value function right now is: -1523.5401110444263
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.08232086]
objective value function right now is: -1523.9401236722651
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00355272]
objective value function right now is: -1524.505266582748
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.15217718]
objective value function right now is: -1521.01657150603
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00339042]
objective value function right now is: -1525.0928175127856
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01963773]
objective value function right now is: -1515.1276597939166
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01634262]
objective value function right now is: -1521.7156680770706
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04174632]
objective value function right now is: -1522.2351517281975
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00326908]
objective value function right now is: -1519.4296613752776
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00982953]
objective value function right now is: -1524.5384118697284
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [0.01027088]
objective value function right now is: -1524.677999849121
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.09657869]
objective value function right now is: -1523.101441747996
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02793629]
objective value function right now is: -1518.9186959948545
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00117346]
objective value function right now is: -1524.5678068526931
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1525.7932116850318
Current xi:  [0.00912948]
objective value function right now is: -1525.7932116850318
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01321445]
objective value function right now is: -1521.9013729967767
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01443698]
objective value function right now is: -1525.0866704031353
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00948714]
objective value function right now is: -1521.6059257503923
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1526.3589984204434
Current xi:  [0.00381951]
objective value function right now is: -1526.3589984204434
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00536517]
objective value function right now is: -1526.201721271045
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1526.3640863695118
Current xi:  [0.0022023]
objective value function right now is: -1526.3640863695118
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0027407]
objective value function right now is: -1525.9663920432931
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00618607]
objective value function right now is: -1526.2351101062818
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00460441]
objective value function right now is: -1526.2619588596617
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00315883]
objective value function right now is: -1526.1139903380872
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1526.3671096873052
Current xi:  [3.6886828e-05]
objective value function right now is: -1526.3671096873052
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00415098]
objective value function right now is: -1525.6131407576945
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00298399]
objective value function right now is: -1525.9414220532706
new min fval from sgd:  -1526.3797860846446
new min fval from sgd:  -1526.387671415953
new min fval from sgd:  -1526.3955642504006
new min fval from sgd:  -1526.4050302042756
new min fval from sgd:  -1526.4223456589148
new min fval from sgd:  -1526.4409760845756
new min fval from sgd:  -1526.4474228845743
new min fval from sgd:  -1526.447622244004
new min fval from sgd:  -1526.450699517466
new min fval from sgd:  -1526.4719434738122
new min fval from sgd:  -1526.5121790217406
new min fval from sgd:  -1526.545359176017
new min fval from sgd:  -1526.5624527548996
new min fval from sgd:  -1526.5814786392136
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0016551]
objective value function right now is: -1525.3581432571248
new min fval from sgd:  -1526.5912541756513
new min fval from sgd:  -1526.597025931201
new min fval from sgd:  -1526.599249551248
new min fval from sgd:  -1526.600470797998
new min fval from sgd:  -1526.6031454240303
new min fval from sgd:  -1526.6105685873315
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00423674]
objective value function right now is: -1525.6052836570982
new min fval from sgd:  -1526.6273091782136
new min fval from sgd:  -1526.6389762140948
new min fval from sgd:  -1526.6505569492892
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00623072]
objective value function right now is: -1526.6505569492892
new min fval from sgd:  -1526.657949726358
new min fval from sgd:  -1526.6596968412005
new min fval from sgd:  -1526.6629325178155
new min fval from sgd:  -1526.6690150642053
new min fval from sgd:  -1526.6690885412586
new min fval from sgd:  -1526.6692554971678
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00439399]
objective value function right now is: -1526.6492103562675
new min fval from sgd:  -1526.670263752679
new min fval from sgd:  -1526.6726847700315
new min fval from sgd:  -1526.6751013446114
new min fval from sgd:  -1526.676350701259
new min fval from sgd:  -1526.6784760317805
new min fval from sgd:  -1526.682149814182
new min fval from sgd:  -1526.6846876591458
new min fval from sgd:  -1526.6852314402033
new min fval from sgd:  -1526.686258311686
new min fval from sgd:  -1526.6869921770128
new min fval from sgd:  -1526.68738647332
new min fval from sgd:  -1526.6880614163774
new min fval from sgd:  -1526.6907506323369
new min fval from sgd:  -1526.692813801947
new min fval from sgd:  -1526.6928563436074
new min fval from sgd:  -1526.6932240624526
new min fval from sgd:  -1526.69526371986
new min fval from sgd:  -1526.6955820573285
new min fval from sgd:  -1526.696244072685
new min fval from sgd:  -1526.6998496817084
new min fval from sgd:  -1526.7037728703688
new min fval from sgd:  -1526.7064160681214
new min fval from sgd:  -1526.7096556650183
new min fval from sgd:  -1526.7131261103768
new min fval from sgd:  -1526.7156893886374
new min fval from sgd:  -1526.717102594574
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00544784]
objective value function right now is: -1526.6813317160622
min fval:  -1526.717102594574
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -5.4440,  -3.1573],
        [  4.9552,  -3.8880],
        [  5.6747,  -0.8228],
        [ -0.0756,  -6.7242],
        [ -5.3928,   1.8999],
        [  5.5054,   2.6822],
        [  5.7061,  -0.3463],
        [ -6.4203,   2.3605],
        [ -3.5217, -10.4643],
        [  5.6359,  -2.5933]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 6.0520, -4.9873, -5.5799, -5.0387,  3.8185, -6.1927, -5.8357,  4.9309,
        -8.8224, -5.0285], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 5.2111e+00, -4.9057e+00, -3.1467e+00, -5.9642e+00,  3.6156e+00,
         -3.4004e+00, -3.0731e+00,  5.2082e+00, -1.4404e+01, -4.0113e+00],
        [-5.1751e-01, -2.3703e-01, -1.8811e-01, -9.4585e-02, -2.4209e-01,
         -2.0269e-01, -1.6314e-01, -2.9264e-01, -3.2217e-02, -2.8310e-01],
        [-4.7798e+00,  2.4087e+00,  3.1725e+00,  4.5059e+00, -2.6692e+00,
          3.4068e+00,  3.2401e+00, -4.4474e+00,  1.2940e+01,  2.6957e+00],
        [ 1.4084e+00,  5.5897e-01,  5.1806e-01,  1.7218e-01,  6.8880e-01,
          4.4355e-01,  4.2019e-01,  8.4170e-01,  5.2663e-04,  7.6283e-01],
        [-5.1751e-01, -2.3703e-01, -1.8811e-01, -9.4585e-02, -2.4209e-01,
         -2.0269e-01, -1.6314e-01, -2.9264e-01, -3.2217e-02, -2.8310e-01],
        [-5.3986e+00,  3.0796e+00,  4.1421e+00,  4.7990e+00, -2.3719e+00,
          4.1218e+00,  3.9140e+00, -5.8049e+00,  1.5433e+01,  3.2452e+00],
        [-5.1751e-01, -2.3703e-01, -1.8811e-01, -9.4585e-02, -2.4209e-01,
         -2.0269e-01, -1.6314e-01, -2.9264e-01, -3.2217e-02, -2.8310e-01],
        [-5.1751e-01, -2.3703e-01, -1.8811e-01, -9.4585e-02, -2.4209e-01,
         -2.0269e-01, -1.6314e-01, -2.9264e-01, -3.2217e-02, -2.8310e-01],
        [-3.3920e+00,  3.6960e+00,  2.6418e+00,  5.2100e+00, -2.5268e+00,
          2.5517e+00,  2.3310e+00, -4.5946e+00,  1.0675e+01,  3.1712e+00],
        [-5.1751e-01, -2.3703e-01, -1.8811e-01, -9.4585e-02, -2.4209e-01,
         -2.0269e-01, -1.6314e-01, -2.9264e-01, -3.2217e-02, -2.8310e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 0.1462, -0.8150, -0.8078,  2.1744, -0.8150, -1.0090, -0.8150, -0.8150,
        -0.4594, -0.8150], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.2409e+01,  3.0332e-03, -7.0237e+00,  3.7567e+00,  3.0332e-03,
         -1.0823e+01,  3.0332e-03,  3.0332e-03, -5.1803e+00,  3.0332e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-2.9891e+00,  4.0948e+00],
        [-1.4093e-01,  1.0417e+01],
        [ 3.6647e-01,  9.1560e+00],
        [-1.0111e+01, -2.5192e+00],
        [-1.4489e+01, -3.9314e+00],
        [ 7.3363e+00,  7.0642e+00],
        [ 1.0532e+01,  2.6606e+00],
        [-8.5846e+00,  3.9075e-03],
        [ 8.8867e+00,  1.9947e+00],
        [ 1.4290e+00,  3.6216e+00]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 2.1622,  9.0036,  7.6402, -0.3470, -3.0398,  6.0192,  0.0796,  7.4881,
        -0.0755, -2.6701], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 8.0194e-01,  1.4380e+01,  1.1937e+01, -4.8852e+00,  6.6624e-01,
          1.2741e+01,  3.5362e+00, -9.7698e+00,  2.9388e+00,  7.9140e-02],
        [-1.7394e+00,  3.3583e+00,  2.0469e+00, -6.9952e+00, -1.6597e-02,
         -9.1973e-01, -1.9960e+00, -9.3418e-01, -1.9864e+00,  1.8025e-01],
        [-1.1086e+00, -2.0270e+01, -1.3355e+01,  6.5758e+00,  6.0223e+00,
         -2.5655e+00, -3.2507e+00,  4.8986e+00, -1.9071e+00, -3.8914e-01],
        [-2.4222e+00,  3.9574e-01,  6.2782e-01, -6.6781e-01, -9.9512e-01,
         -2.4416e+00, -3.3976e-01, -3.4710e+00, -7.4976e-01, -2.2166e+00],
        [-1.8984e-01, -1.1603e+00, -7.2252e-01, -2.8708e-01, -1.1678e-01,
         -1.5251e+00, -1.4827e+00, -1.1201e+00, -1.4646e+00, -1.3845e-01],
        [-1.1611e+00, -1.7719e+01, -1.6522e+01,  8.8736e+00,  1.0514e+01,
         -1.9652e+01, -6.5104e+00,  7.4121e+00, -5.8557e+00, -1.1713e-01],
        [-1.8984e-01, -1.1603e+00, -7.2252e-01, -2.8708e-01, -1.1678e-01,
         -1.5251e+00, -1.4827e+00, -1.1201e+00, -1.4646e+00, -1.3845e-01],
        [-1.8984e-01, -1.1603e+00, -7.2252e-01, -2.8708e-01, -1.1678e-01,
         -1.5251e+00, -1.4827e+00, -1.1201e+00, -1.4646e+00, -1.3845e-01],
        [-1.8984e-01, -1.1603e+00, -7.2252e-01, -2.8708e-01, -1.1678e-01,
         -1.5251e+00, -1.4827e+00, -1.1201e+00, -1.4646e+00, -1.3845e-01],
        [-1.8984e-01, -1.1603e+00, -7.2253e-01, -2.8708e-01, -1.1678e-01,
         -1.5251e+00, -1.4827e+00, -1.1201e+00, -1.4646e+00, -1.3845e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.9364, -3.5181, -0.0718, -0.7356, -1.9613,  0.5628, -1.9613, -1.9613,
        -1.9613, -1.9613], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.2126,  -2.2614,  -6.8044,   1.8249,   0.2839,  14.4118,   0.2842,
           0.2848,   0.2833,   0.2847],
        [ -0.5707,   2.2415,   6.7925,  -1.9794,  -0.2843, -14.4551,  -0.2840,
          -0.2833,  -0.2848,  -0.2834]], device='cuda:0'))])
xi:  [0.00497741]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 531.8135435610835
W_T_median: 198.3004720595457
W_T_pctile_5: 0.005068055004088734
W_T_CVAR_5_pct: -62.76204489254604
Average q (qsum/M+1):  52.28579810357863
Optimal xi:  [0.00497741]
Expected(across Rb) median(across samples) p_equity:  0.319991771876812
obj fun:  tensor(-1526.7171, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -910.2652971820509
Current xi:  [15.478115]
objective value function right now is: -910.2652971820509
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -942.9758223987066
Current xi:  [37.04294]
objective value function right now is: -942.9758223987066
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1026.145392160832
Current xi:  [59.625996]
objective value function right now is: -1026.145392160832
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1067.254111647509
Current xi:  [81.067894]
objective value function right now is: -1067.254111647509
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1189.7153049687201
Current xi:  [99.842834]
objective value function right now is: -1189.7153049687201
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1517.0774314503826
Current xi:  [114.5007]
objective value function right now is: -1517.0774314503826
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1532.5192672557353
Current xi:  [125.68804]
objective value function right now is: -1532.5192672557353
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1544.0254158838634
Current xi:  [135.17831]
objective value function right now is: -1544.0254158838634
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1553.6220953957443
Current xi:  [145.76971]
objective value function right now is: -1553.6220953957443
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.0825493776422
Current xi:  [151.9261]
objective value function right now is: -1555.0825493776422
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [158.82411]
objective value function right now is: -1555.0283713700853
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.7932737506176
Current xi:  [162.67595]
objective value function right now is: -1555.7932737506176
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1559.00969255723
Current xi:  [164.82849]
objective value function right now is: -1559.00969255723
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [166.50484]
objective value function right now is: -1553.1669506448927
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.3006]
objective value function right now is: -1555.1168160603163
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.49179]
objective value function right now is: -1553.8836282338966
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.48442]
objective value function right now is: -1546.124974198525
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1560.1906760256306
Current xi:  [168.00182]
objective value function right now is: -1560.1906760256306
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.51128]
objective value function right now is: -1553.7366433013835
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.73387]
objective value function right now is: -1545.1271583377954
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1561.4975050054277
Current xi:  [169.7577]
objective value function right now is: -1561.4975050054277
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.06493]
objective value function right now is: -1553.247877895338
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.87724]
objective value function right now is: -1554.3538446759778
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.54913]
objective value function right now is: -1553.157343140725
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.39241]
objective value function right now is: -1560.9701885464244
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.56882]
objective value function right now is: -1557.3537139038606
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.53156]
objective value function right now is: -1560.986821596939
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [167.9297]
objective value function right now is: -1558.8253204808716
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [168.75319]
objective value function right now is: -1558.5175890515932
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.03767]
objective value function right now is: -1555.9424290143527
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.65211]
objective value function right now is: -1550.8697470759485
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.15858]
objective value function right now is: -1560.688702187525
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1561.7587229456458
Current xi:  [168.76418]
objective value function right now is: -1561.7587229456458
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.23457]
objective value function right now is: -1557.665066716815
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.12173]
objective value function right now is: -1559.090314373127
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1564.6273988513542
Current xi:  [168.99814]
objective value function right now is: -1564.6273988513542
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.47096]
objective value function right now is: -1564.323080907818
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.52893]
objective value function right now is: -1564.233016435383
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.80927]
objective value function right now is: -1562.5534205331794
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1564.694224290018
Current xi:  [169.89905]
objective value function right now is: -1564.694224290018
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.89085]
objective value function right now is: -1564.0734692595374
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1564.9480249078913
Current xi:  [169.62596]
objective value function right now is: -1564.9480249078913
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.66606]
objective value function right now is: -1563.5823148260718
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1565.0119919873812
Current xi:  [169.79314]
objective value function right now is: -1565.0119919873812
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.02577]
objective value function right now is: -1564.1939213791973
new min fval from sgd:  -1565.0426812453024
new min fval from sgd:  -1565.073138171863
new min fval from sgd:  -1565.1423863402708
new min fval from sgd:  -1565.1713942026204
new min fval from sgd:  -1565.263644574617
new min fval from sgd:  -1565.3147517357245
new min fval from sgd:  -1565.315571776214
new min fval from sgd:  -1565.362594904594
new min fval from sgd:  -1565.4345915464326
new min fval from sgd:  -1565.473446349121
new min fval from sgd:  -1565.521387357156
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.37193]
objective value function right now is: -1564.7480042359948
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.19798]
objective value function right now is: -1564.1798969610516
new min fval from sgd:  -1565.526985188452
new min fval from sgd:  -1565.537702152608
new min fval from sgd:  -1565.5659747275902
new min fval from sgd:  -1565.565980624957
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.24936]
objective value function right now is: -1563.7782974463012
new min fval from sgd:  -1565.5775657383815
new min fval from sgd:  -1565.594369781937
new min fval from sgd:  -1565.6042101172413
new min fval from sgd:  -1565.6237048116434
new min fval from sgd:  -1565.6411465708297
new min fval from sgd:  -1565.6562702975102
new min fval from sgd:  -1565.6570891479603
new min fval from sgd:  -1565.6739006389678
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.35352]
objective value function right now is: -1565.660115516175
new min fval from sgd:  -1565.6760977363713
new min fval from sgd:  -1565.6882349059335
new min fval from sgd:  -1565.6923750654748
new min fval from sgd:  -1565.7043537066415
new min fval from sgd:  -1565.7114783952477
new min fval from sgd:  -1565.7173983101393
new min fval from sgd:  -1565.7222256969228
new min fval from sgd:  -1565.7268737180927
new min fval from sgd:  -1565.7336743924448
new min fval from sgd:  -1565.7432168171877
new min fval from sgd:  -1565.747935062803
new min fval from sgd:  -1565.748261842819
new min fval from sgd:  -1565.749204910674
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.36662]
objective value function right now is: -1565.6120123633846
min fval:  -1565.749204910674
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-34.9239, -10.1760],
        [  6.0463,   0.1908],
        [  6.3572,  -0.3794],
        [  5.8766,  -0.3233],
        [  4.9110,   0.9735],
        [ -1.9331,  -7.5223],
        [  5.7814,  -5.1791],
        [  3.6835,  -0.8832],
        [  1.7080,  -6.8265],
        [ -6.3055,   0.9230]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-10.1721,  -5.8840,  -5.4869,  -5.8470,  -6.5304,  -5.6194,  -4.3708,
         -6.6379,  -5.3209,   4.0856], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 7.7049e+00,  2.7522e+00,  2.6205e+00,  1.9912e+00,  2.0600e+00,
          5.9090e+00,  3.5892e+00,  3.6276e-01,  4.5711e+00, -5.3182e+00],
        [ 9.4709e+00,  3.5178e+00,  3.5438e+00,  2.3710e+00,  2.0707e+00,
          6.1099e+00,  4.1487e+00,  5.4762e-01,  4.7482e+00, -5.2672e+00],
        [-1.3554e-02, -2.3734e-01, -3.1103e-01, -1.7307e-01, -8.2867e-02,
         -1.4534e-01, -4.2549e-01, -9.5411e-03, -1.9383e-01, -6.7470e-01],
        [-9.3618e+00, -3.4806e+00, -3.6859e+00, -2.7929e+00, -2.1800e+00,
         -6.4490e+00, -5.4123e+00, -7.3876e-01, -5.5613e+00,  6.3768e+00],
        [-1.3554e-02, -2.3734e-01, -3.1103e-01, -1.7307e-01, -8.2867e-02,
         -1.4534e-01, -4.2549e-01, -9.5411e-03, -1.9383e-01, -6.7470e-01],
        [ 9.6416e+00,  3.6561e+00,  3.3098e+00,  2.3478e+00,  2.1761e+00,
          6.5443e+00,  4.2760e+00,  7.0342e-01,  5.0530e+00, -6.0347e+00],
        [ 8.3129e+00,  2.9362e+00,  3.1996e+00,  2.1174e+00,  2.0853e+00,
          6.1037e+00,  4.0442e+00,  5.8582e-01,  4.5460e+00, -5.3589e+00],
        [-9.6267e+00, -3.6932e+00, -4.3657e+00, -3.2336e+00, -2.5835e+00,
         -7.6428e+00, -5.5170e+00, -1.0038e+00, -6.1401e+00,  7.0227e+00],
        [ 1.0479e+01,  3.8786e+00,  3.2414e+00,  2.4512e+00,  2.0788e+00,
          6.4740e+00,  4.3414e+00,  9.8564e-01,  4.9543e+00, -5.9770e+00],
        [-8.6411e+00, -2.9189e+00, -3.4969e+00, -2.7136e+00, -1.9785e+00,
         -5.7755e+00, -5.1522e+00, -8.2814e-01, -5.0232e+00,  5.5310e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.9018, -3.3122, -1.3386,  2.6292, -1.3386, -2.8870, -2.9811,  3.2051,
        -2.9775,  2.3884], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.9010e+00, -6.4124e+00, -1.7859e-03,  1.1634e+01, -1.7859e-03,
         -7.1773e+00, -5.6049e+00,  9.1101e+00, -8.0718e+00,  6.3122e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 0.4355,  4.7873],
        [-8.8710, -2.5149],
        [-3.9204,  8.5888],
        [ 0.8216,  8.1288],
        [-8.6523, -2.1889],
        [ 5.6097,  6.6429],
        [ 7.3843,  2.5301],
        [ 8.9642, -0.0540],
        [-8.9793, -2.3859],
        [ 3.2686,  4.7671]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 0.0141, -2.7590,  6.8219,  6.4098, -0.3368,  5.0357, -2.3974, -7.3036,
        -0.7620, -5.1498], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 3.8621e+00,  6.0854e+00,  6.5005e+00, -1.6792e+00,  2.5331e-01,
         -9.3773e-02, -2.7845e+00, -1.8553e+01,  1.9468e+00,  3.2370e+00],
        [-2.8885e-01, -1.5748e+00,  2.5237e+01,  2.7417e+00, -6.2120e+00,
          1.0075e+00,  4.8953e-01,  2.7845e+00, -6.0072e+00, -7.2114e-01],
        [-2.1843e+00,  4.5812e+00, -1.4327e+01, -1.0438e+01,  4.2240e+00,
         -2.0886e+00, -3.1293e-01, -6.3384e+00,  5.3637e+00, -4.6309e-01],
        [-2.0502e+00,  3.6019e+00, -1.3051e+01, -1.2937e+01,  8.5149e+00,
         -1.2466e+01, -3.6305e+00, -8.0076e+00,  8.6140e+00, -2.1972e-02],
        [-1.8241e-01,  6.7884e+00, -2.1087e+00, -1.3797e+01,  6.5483e+00,
         -2.2175e+01, -8.9524e+00, -8.2507e+00,  8.0534e+00,  1.9559e-02],
        [-8.6052e-01,  5.5279e-01, -6.5937e-01, -9.2174e-02,  4.6133e-01,
         -2.1206e+00, -1.4039e+00, -3.3918e-01,  5.5186e-01, -2.8419e-01],
        [-1.1549e+00,  6.4394e-01, -3.8175e+00, -7.1369e-02, -6.7236e-01,
          2.2390e+00, -1.5631e+00, -1.1290e+01, -4.0634e-01, -2.2527e-01],
        [-8.6052e-01,  5.5280e-01, -6.5937e-01, -9.2172e-02,  4.6133e-01,
         -2.1206e+00, -1.4039e+00, -3.3917e-01,  5.5186e-01, -2.8419e-01],
        [-8.6052e-01,  5.5279e-01, -6.5937e-01, -9.2174e-02,  4.6132e-01,
         -2.1206e+00, -1.4039e+00, -3.3918e-01,  5.5185e-01, -2.8419e-01],
        [-8.6052e-01,  5.5280e-01, -6.5937e-01, -9.2172e-02,  4.6133e-01,
         -2.1206e+00, -1.4039e+00, -3.3917e-01,  5.5187e-01, -2.8419e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.6600, -2.1722,  0.2056,  1.8451, -2.3575, -4.2588, -3.7672, -4.2588,
        -4.2588, -4.2588], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.5047,   0.2139,  -3.1491,  -8.6795,  20.0647,   0.6368,   2.7693,
           0.6449,   0.6344,   0.6557],
        [ -0.6280,  -0.6574,   2.8277,   8.9832, -20.1401,  -0.6333,  -2.5622,
          -0.6253,  -0.6358,  -0.6145]], device='cuda:0'))])
xi:  [170.35132]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 581.4845535355769
W_T_median: 384.62678675556356
W_T_pctile_5: 170.45636070367217
W_T_CVAR_5_pct: 16.119491612872938
Average q (qsum/M+1):  48.94815555695565
Optimal xi:  [170.35132]
Expected(across Rb) median(across samples) p_equity:  0.25883832077185315
obj fun:  tensor(-1565.7492, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-34.9239, -10.1760],
        [  6.0463,   0.1908],
        [  6.3572,  -0.3794],
        [  5.8766,  -0.3233],
        [  4.9110,   0.9735],
        [ -1.9331,  -7.5223],
        [  5.7814,  -5.1791],
        [  3.6835,  -0.8832],
        [  1.7080,  -6.8265],
        [ -6.3055,   0.9230]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-10.1721,  -5.8840,  -5.4869,  -5.8470,  -6.5304,  -5.6194,  -4.3708,
         -6.6379,  -5.3209,   4.0856], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 7.7049e+00,  2.7522e+00,  2.6205e+00,  1.9912e+00,  2.0600e+00,
          5.9090e+00,  3.5892e+00,  3.6276e-01,  4.5711e+00, -5.3182e+00],
        [ 9.4709e+00,  3.5178e+00,  3.5438e+00,  2.3710e+00,  2.0707e+00,
          6.1099e+00,  4.1487e+00,  5.4762e-01,  4.7482e+00, -5.2672e+00],
        [-1.3554e-02, -2.3734e-01, -3.1103e-01, -1.7307e-01, -8.2867e-02,
         -1.4534e-01, -4.2549e-01, -9.5411e-03, -1.9383e-01, -6.7470e-01],
        [-9.3618e+00, -3.4806e+00, -3.6859e+00, -2.7929e+00, -2.1800e+00,
         -6.4490e+00, -5.4123e+00, -7.3876e-01, -5.5613e+00,  6.3768e+00],
        [-1.3554e-02, -2.3734e-01, -3.1103e-01, -1.7307e-01, -8.2867e-02,
         -1.4534e-01, -4.2549e-01, -9.5411e-03, -1.9383e-01, -6.7470e-01],
        [ 9.6416e+00,  3.6561e+00,  3.3098e+00,  2.3478e+00,  2.1761e+00,
          6.5443e+00,  4.2760e+00,  7.0342e-01,  5.0530e+00, -6.0347e+00],
        [ 8.3129e+00,  2.9362e+00,  3.1996e+00,  2.1174e+00,  2.0853e+00,
          6.1037e+00,  4.0442e+00,  5.8582e-01,  4.5460e+00, -5.3589e+00],
        [-9.6267e+00, -3.6932e+00, -4.3657e+00, -3.2336e+00, -2.5835e+00,
         -7.6428e+00, -5.5170e+00, -1.0038e+00, -6.1401e+00,  7.0227e+00],
        [ 1.0479e+01,  3.8786e+00,  3.2414e+00,  2.4512e+00,  2.0788e+00,
          6.4740e+00,  4.3414e+00,  9.8564e-01,  4.9543e+00, -5.9770e+00],
        [-8.6411e+00, -2.9189e+00, -3.4969e+00, -2.7136e+00, -1.9785e+00,
         -5.7755e+00, -5.1522e+00, -8.2814e-01, -5.0232e+00,  5.5310e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.9018, -3.3122, -1.3386,  2.6292, -1.3386, -2.8870, -2.9811,  3.2051,
        -2.9775,  2.3884], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.9010e+00, -6.4124e+00, -1.7859e-03,  1.1634e+01, -1.7859e-03,
         -7.1773e+00, -5.6049e+00,  9.1101e+00, -8.0718e+00,  6.3122e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 0.4355,  4.7873],
        [-8.8710, -2.5149],
        [-3.9204,  8.5888],
        [ 0.8216,  8.1288],
        [-8.6523, -2.1889],
        [ 5.6097,  6.6429],
        [ 7.3843,  2.5301],
        [ 8.9642, -0.0540],
        [-8.9793, -2.3859],
        [ 3.2686,  4.7671]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 0.0141, -2.7590,  6.8219,  6.4098, -0.3368,  5.0357, -2.3974, -7.3036,
        -0.7620, -5.1498], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 3.8621e+00,  6.0854e+00,  6.5005e+00, -1.6792e+00,  2.5331e-01,
         -9.3773e-02, -2.7845e+00, -1.8553e+01,  1.9468e+00,  3.2370e+00],
        [-2.8885e-01, -1.5748e+00,  2.5237e+01,  2.7417e+00, -6.2120e+00,
          1.0075e+00,  4.8953e-01,  2.7845e+00, -6.0072e+00, -7.2114e-01],
        [-2.1843e+00,  4.5812e+00, -1.4327e+01, -1.0438e+01,  4.2240e+00,
         -2.0886e+00, -3.1293e-01, -6.3384e+00,  5.3637e+00, -4.6309e-01],
        [-2.0502e+00,  3.6019e+00, -1.3051e+01, -1.2937e+01,  8.5149e+00,
         -1.2466e+01, -3.6305e+00, -8.0076e+00,  8.6140e+00, -2.1972e-02],
        [-1.8241e-01,  6.7884e+00, -2.1087e+00, -1.3797e+01,  6.5483e+00,
         -2.2175e+01, -8.9524e+00, -8.2507e+00,  8.0534e+00,  1.9559e-02],
        [-8.6052e-01,  5.5279e-01, -6.5937e-01, -9.2174e-02,  4.6133e-01,
         -2.1206e+00, -1.4039e+00, -3.3918e-01,  5.5186e-01, -2.8419e-01],
        [-1.1549e+00,  6.4394e-01, -3.8175e+00, -7.1369e-02, -6.7236e-01,
          2.2390e+00, -1.5631e+00, -1.1290e+01, -4.0634e-01, -2.2527e-01],
        [-8.6052e-01,  5.5280e-01, -6.5937e-01, -9.2172e-02,  4.6133e-01,
         -2.1206e+00, -1.4039e+00, -3.3917e-01,  5.5186e-01, -2.8419e-01],
        [-8.6052e-01,  5.5279e-01, -6.5937e-01, -9.2174e-02,  4.6132e-01,
         -2.1206e+00, -1.4039e+00, -3.3918e-01,  5.5185e-01, -2.8419e-01],
        [-8.6052e-01,  5.5280e-01, -6.5937e-01, -9.2172e-02,  4.6133e-01,
         -2.1206e+00, -1.4039e+00, -3.3917e-01,  5.5187e-01, -2.8419e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.6600, -2.1722,  0.2056,  1.8451, -2.3575, -4.2588, -3.7672, -4.2588,
        -4.2588, -4.2588], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.5047,   0.2139,  -3.1491,  -8.6795,  20.0647,   0.6368,   2.7693,
           0.6449,   0.6344,   0.6557],
        [ -0.6280,  -0.6574,   2.8277,   8.9832, -20.1401,  -0.6333,  -2.5622,
          -0.6253,  -0.6358,  -0.6145]], device='cuda:0'))])
loaded xi:  170.35132
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1586.322077050601
Current xi:  [176.89966]
objective value function right now is: -1586.322077050601
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1594.7829322603195
Current xi:  [181.85448]
objective value function right now is: -1594.7829322603195
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.30188]
objective value function right now is: -1593.917893732602
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1596.925187397542
Current xi:  [186.59538]
objective value function right now is: -1596.925187397542
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1598.8288748454233
Current xi:  [186.85667]
objective value function right now is: -1598.8288748454233
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.9583]
objective value function right now is: -1590.5224673837163
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [187.2392]
objective value function right now is: -1589.8114446312356
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1601.4895433278812
Current xi:  [187.98033]
objective value function right now is: -1601.4895433278812
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.47462]
objective value function right now is: -1595.588079049381
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.32434]
objective value function right now is: -1600.9011640299595
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.70834]
objective value function right now is: -1598.3284101059294
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.62006]
objective value function right now is: -1598.2785936376335
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.559]
objective value function right now is: -1595.8672739221345
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [191.3794]
objective value function right now is: -1595.7864195929399
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.6528581008145
Current xi:  [190.503]
objective value function right now is: -1602.6528581008145
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.5648]
objective value function right now is: -1598.6029954278706
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.74611]
objective value function right now is: -1596.0899974978875
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.93797]
objective value function right now is: -1594.2003176795934
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.24692]
objective value function right now is: -1576.4340268928443
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.6809]
objective value function right now is: -1595.5083563221717
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.3722]
objective value function right now is: -1601.990144833797
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.83133]
objective value function right now is: -1593.3282546095884
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.05258]
objective value function right now is: -1595.027341109888
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.31998]
objective value function right now is: -1600.1734048396884
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.19954]
objective value function right now is: -1596.7220015965925
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.62291]
objective value function right now is: -1601.7331365430293
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.32361]
objective value function right now is: -1598.2925303566144
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1603.7152287932577
Current xi:  [188.57536]
objective value function right now is: -1603.7152287932577
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [188.20462]
objective value function right now is: -1599.4119322102824
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.50514]
objective value function right now is: -1598.8931829386545
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.92491]
objective value function right now is: -1600.482846668529
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.70988]
objective value function right now is: -1595.6283282028307
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.95448]
objective value function right now is: -1583.7959548042177
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.62416]
objective value function right now is: -1597.4477979953135
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.74019]
objective value function right now is: -1599.3237798496332
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.83961]
objective value function right now is: -1603.3348257431637
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.718392967643
Current xi:  [189.10132]
objective value function right now is: -1605.718392967643
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.7331306661733
Current xi:  [189.16666]
objective value function right now is: -1605.7331306661733
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.32243]
objective value function right now is: -1595.989515079396
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.23886]
objective value function right now is: -1603.140099552472
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.27057]
objective value function right now is: -1605.5424481371324
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1606.1010145151877
Current xi:  [189.50838]
objective value function right now is: -1606.1010145151877
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1606.153422913877
Current xi:  [189.61403]
objective value function right now is: -1606.153422913877
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.65216]
objective value function right now is: -1605.1375858029203
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.92233]
objective value function right now is: -1605.7029646213116
new min fval from sgd:  -1606.3046300908834
new min fval from sgd:  -1606.4363600795223
new min fval from sgd:  -1606.5694674693173
new min fval from sgd:  -1606.5913930355289
new min fval from sgd:  -1606.6710049337205
new min fval from sgd:  -1606.7563682636521
new min fval from sgd:  -1606.8078301828107
new min fval from sgd:  -1606.8160480354354
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.22908]
objective value function right now is: -1604.6867042183908
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.21272]
objective value function right now is: -1605.7287445507661
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.9937]
objective value function right now is: -1605.5875188537193
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.78882]
objective value function right now is: -1606.5084299390621
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.73073]
objective value function right now is: -1606.4343771724823
min fval:  -1606.8160480354354
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-34.0283, -11.4026],
        [  8.9472,   0.0536],
        [  9.1814,  -0.2152],
        [  6.0955,  -1.0094],
        [ -1.2166,   0.4235],
        [ -2.5134,  -9.2916],
        [  8.3624,  -5.0469],
        [ -1.2181,   0.4246],
        [  5.3330,  -8.4723],
        [ -9.2555,   0.9514]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-10.4358,  -8.0840,  -7.8529,  -8.7867,  -3.5121,  -6.7477,  -5.3694,
         -3.5078,  -6.4388,   6.1950], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 1.4756e-02, -2.9608e-01, -3.4325e-01,  1.0682e-02, -2.2716e-02,
         -1.4371e-01, -6.6692e-01, -2.2808e-02, -3.9527e-01, -8.6477e-01],
        [ 7.1861e+00,  4.5245e+00,  4.5744e+00,  3.9672e-01,  1.8010e-01,
          7.7915e+00,  3.7890e+00,  1.8711e-01,  6.6083e+00, -6.0991e+00],
        [ 1.4756e-02, -2.9608e-01, -3.4325e-01,  1.0682e-02, -2.2716e-02,
         -1.4371e-01, -6.6692e-01, -2.2808e-02, -3.9527e-01, -8.6477e-01],
        [-7.7621e+00, -4.6472e+00, -4.9294e+00, -1.1919e+00,  2.0324e-02,
         -7.9259e+00, -6.4983e+00,  1.5559e-02, -8.0526e+00,  6.9370e+00],
        [ 1.4756e-02, -2.9608e-01, -3.4325e-01,  1.0682e-02, -2.2716e-02,
         -1.4371e-01, -6.6692e-01, -2.2808e-02, -3.9527e-01, -8.6477e-01],
        [ 7.8005e+00,  4.9970e+00,  4.7264e+00,  7.9684e-01,  2.0164e-02,
          8.2434e+00,  4.2085e+00,  3.3344e-02,  7.0872e+00, -6.9112e+00],
        [ 3.3287e+00,  2.3553e+00,  2.5085e+00,  1.8810e-02,  1.3578e-01,
          6.7905e+00,  2.4923e+00,  1.3730e-01,  5.5456e+00, -5.8014e+00],
        [-7.5080e+00, -4.9851e+00, -5.6698e+00, -1.4999e+00,  7.5643e-02,
         -8.7969e+00, -6.7997e+00,  1.0508e-01, -8.6079e+00,  7.6195e+00],
        [ 8.7351e+00,  5.2456e+00,  4.7247e+00,  9.4220e-01,  1.2129e-02,
          8.1600e+00,  4.3631e+00,  2.5398e-02,  7.0284e+00, -6.8890e+00],
        [-4.0065e+00, -3.7648e+00, -4.3653e+00,  3.1511e-03,  2.8596e-03,
         -6.6311e+00, -5.3897e+00,  4.5855e-03, -6.4973e+00,  4.7315e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.5759, -3.8257, -1.5759,  2.7892, -1.5759, -3.4493, -3.6645,  3.3873,
        -3.5512,  1.1220], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-0.0231, -6.5989, -0.0231, 11.2508, -0.0231, -8.2946, -3.8413, 10.6028,
         -9.1352,  3.9754]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-2.2994e+00,  1.0373e+01],
        [ 2.6136e-01,  4.5112e+00],
        [-5.8589e+00,  9.5914e+00],
        [ 2.6929e+00,  1.0032e+01],
        [-1.0031e+01, -3.3120e+00],
        [ 9.2992e+00,  7.6289e+00],
        [ 8.1520e+00,  6.3201e+00],
        [ 1.1018e+01, -8.7749e-03],
        [-1.0258e+01, -3.5024e+00],
        [ 1.6006e+00,  6.7351e+00]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 4.1495, -3.0514,  7.7318,  7.6063, -1.3787,  4.6484, -2.7163, -9.1000,
        -1.5895, -3.4984], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 4.1706e+00,  1.0959e+00,  4.9010e+00, -4.1410e-01,  7.2343e-01,
          1.8131e-01, -5.3557e+00, -1.4543e+01,  1.8638e+00,  3.8233e+00],
        [ 1.5668e+00, -6.9503e-01,  2.7313e+01,  1.6506e+00, -7.3504e+00,
          1.9440e+00, -7.9026e-01,  2.1952e+00, -8.6379e+00, -1.0386e+00],
        [-3.2598e+00, -2.9098e-01, -1.6273e+01, -1.0649e+01,  5.6174e+00,
         -7.5970e-01, -2.0964e+00, -6.7383e+00,  7.1543e+00, -3.2099e-02],
        [-4.7113e+00, -1.6423e-01, -1.2295e+01, -1.7241e+01,  7.9175e+00,
         -1.3149e+01,  6.7732e-02, -1.1436e+01,  7.7823e+00,  2.0361e-02],
        [-1.1431e-01, -1.2114e-01, -2.4467e+00, -2.3592e+01,  6.4379e+00,
         -2.9481e+01,  9.8919e-02, -1.2768e+01,  7.9606e+00,  2.2718e-02],
        [-5.3821e-01, -2.0905e-01, -7.2139e-01, -6.2884e-01, -1.9482e-01,
         -2.6370e+00, -7.4229e-02, -6.9181e-01, -1.9213e-01, -1.7959e-01],
        [-7.1709e+00,  4.3076e-01, -2.7111e+00,  4.1173e-01,  2.9280e+00,
          2.7537e+00,  4.8772e+00, -1.2184e+01,  3.0070e+00,  2.8671e-01],
        [-5.3821e-01, -2.0905e-01, -7.2140e-01, -6.2884e-01, -1.9483e-01,
         -2.6370e+00, -7.4227e-02, -6.9181e-01, -1.9214e-01, -1.7959e-01],
        [-5.3821e-01, -2.0905e-01, -7.2140e-01, -6.2884e-01, -1.9482e-01,
         -2.6370e+00, -7.4229e-02, -6.9181e-01, -1.9213e-01, -1.7959e-01],
        [-5.3821e-01, -2.0905e-01, -7.2140e-01, -6.2884e-01, -1.9482e-01,
         -2.6370e+00, -7.4228e-02, -6.9181e-01, -1.9214e-01, -1.7959e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-6.9349, -2.6281, -0.8829,  2.5503, -2.8998, -5.1550, -5.9262, -5.1550,
        -5.1550, -5.1550], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.5716,   0.2414,  -4.0376,  -9.4314,  23.5792,   0.3594,   3.6483,
           0.3605,   0.3590,   0.3621],
        [ -1.6813,  -0.6848,   3.7208,   9.7067, -23.6251,  -0.3589,  -3.4492,
          -0.3577,  -0.3592,  -0.3562]], device='cuda:0'))])
xi:  [189.94003]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 573.0158740592767
W_T_median: 416.36498605770095
W_T_pctile_5: 189.59643969627115
W_T_CVAR_5_pct: 22.94352925459411
Average q (qsum/M+1):  48.13235572076613
Optimal xi:  [189.94003]
Expected(across Rb) median(across samples) p_equity:  0.24998143116633098
obj fun:  tensor(-1606.8160, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-34.0283, -11.4026],
        [  8.9472,   0.0536],
        [  9.1814,  -0.2152],
        [  6.0955,  -1.0094],
        [ -1.2166,   0.4235],
        [ -2.5134,  -9.2916],
        [  8.3624,  -5.0469],
        [ -1.2181,   0.4246],
        [  5.3330,  -8.4723],
        [ -9.2555,   0.9514]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-10.4358,  -8.0840,  -7.8529,  -8.7867,  -3.5121,  -6.7477,  -5.3694,
         -3.5078,  -6.4388,   6.1950], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 1.4756e-02, -2.9608e-01, -3.4325e-01,  1.0682e-02, -2.2716e-02,
         -1.4371e-01, -6.6692e-01, -2.2808e-02, -3.9527e-01, -8.6477e-01],
        [ 7.1861e+00,  4.5245e+00,  4.5744e+00,  3.9672e-01,  1.8010e-01,
          7.7915e+00,  3.7890e+00,  1.8711e-01,  6.6083e+00, -6.0991e+00],
        [ 1.4756e-02, -2.9608e-01, -3.4325e-01,  1.0682e-02, -2.2716e-02,
         -1.4371e-01, -6.6692e-01, -2.2808e-02, -3.9527e-01, -8.6477e-01],
        [-7.7621e+00, -4.6472e+00, -4.9294e+00, -1.1919e+00,  2.0324e-02,
         -7.9259e+00, -6.4983e+00,  1.5559e-02, -8.0526e+00,  6.9370e+00],
        [ 1.4756e-02, -2.9608e-01, -3.4325e-01,  1.0682e-02, -2.2716e-02,
         -1.4371e-01, -6.6692e-01, -2.2808e-02, -3.9527e-01, -8.6477e-01],
        [ 7.8005e+00,  4.9970e+00,  4.7264e+00,  7.9684e-01,  2.0164e-02,
          8.2434e+00,  4.2085e+00,  3.3344e-02,  7.0872e+00, -6.9112e+00],
        [ 3.3287e+00,  2.3553e+00,  2.5085e+00,  1.8810e-02,  1.3578e-01,
          6.7905e+00,  2.4923e+00,  1.3730e-01,  5.5456e+00, -5.8014e+00],
        [-7.5080e+00, -4.9851e+00, -5.6698e+00, -1.4999e+00,  7.5643e-02,
         -8.7969e+00, -6.7997e+00,  1.0508e-01, -8.6079e+00,  7.6195e+00],
        [ 8.7351e+00,  5.2456e+00,  4.7247e+00,  9.4220e-01,  1.2129e-02,
          8.1600e+00,  4.3631e+00,  2.5398e-02,  7.0284e+00, -6.8890e+00],
        [-4.0065e+00, -3.7648e+00, -4.3653e+00,  3.1511e-03,  2.8596e-03,
         -6.6311e+00, -5.3897e+00,  4.5855e-03, -6.4973e+00,  4.7315e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.5759, -3.8257, -1.5759,  2.7892, -1.5759, -3.4493, -3.6645,  3.3873,
        -3.5512,  1.1220], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-0.0231, -6.5989, -0.0231, 11.2508, -0.0231, -8.2946, -3.8413, 10.6028,
         -9.1352,  3.9754]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-2.2994e+00,  1.0373e+01],
        [ 2.6136e-01,  4.5112e+00],
        [-5.8589e+00,  9.5914e+00],
        [ 2.6929e+00,  1.0032e+01],
        [-1.0031e+01, -3.3120e+00],
        [ 9.2992e+00,  7.6289e+00],
        [ 8.1520e+00,  6.3201e+00],
        [ 1.1018e+01, -8.7749e-03],
        [-1.0258e+01, -3.5024e+00],
        [ 1.6006e+00,  6.7351e+00]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 4.1495, -3.0514,  7.7318,  7.6063, -1.3787,  4.6484, -2.7163, -9.1000,
        -1.5895, -3.4984], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 4.1706e+00,  1.0959e+00,  4.9010e+00, -4.1410e-01,  7.2343e-01,
          1.8131e-01, -5.3557e+00, -1.4543e+01,  1.8638e+00,  3.8233e+00],
        [ 1.5668e+00, -6.9503e-01,  2.7313e+01,  1.6506e+00, -7.3504e+00,
          1.9440e+00, -7.9026e-01,  2.1952e+00, -8.6379e+00, -1.0386e+00],
        [-3.2598e+00, -2.9098e-01, -1.6273e+01, -1.0649e+01,  5.6174e+00,
         -7.5970e-01, -2.0964e+00, -6.7383e+00,  7.1543e+00, -3.2099e-02],
        [-4.7113e+00, -1.6423e-01, -1.2295e+01, -1.7241e+01,  7.9175e+00,
         -1.3149e+01,  6.7732e-02, -1.1436e+01,  7.7823e+00,  2.0361e-02],
        [-1.1431e-01, -1.2114e-01, -2.4467e+00, -2.3592e+01,  6.4379e+00,
         -2.9481e+01,  9.8919e-02, -1.2768e+01,  7.9606e+00,  2.2718e-02],
        [-5.3821e-01, -2.0905e-01, -7.2139e-01, -6.2884e-01, -1.9482e-01,
         -2.6370e+00, -7.4229e-02, -6.9181e-01, -1.9213e-01, -1.7959e-01],
        [-7.1709e+00,  4.3076e-01, -2.7111e+00,  4.1173e-01,  2.9280e+00,
          2.7537e+00,  4.8772e+00, -1.2184e+01,  3.0070e+00,  2.8671e-01],
        [-5.3821e-01, -2.0905e-01, -7.2140e-01, -6.2884e-01, -1.9483e-01,
         -2.6370e+00, -7.4227e-02, -6.9181e-01, -1.9214e-01, -1.7959e-01],
        [-5.3821e-01, -2.0905e-01, -7.2140e-01, -6.2884e-01, -1.9482e-01,
         -2.6370e+00, -7.4229e-02, -6.9181e-01, -1.9213e-01, -1.7959e-01],
        [-5.3821e-01, -2.0905e-01, -7.2140e-01, -6.2884e-01, -1.9482e-01,
         -2.6370e+00, -7.4228e-02, -6.9181e-01, -1.9214e-01, -1.7959e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-6.9349, -2.6281, -0.8829,  2.5503, -2.8998, -5.1550, -5.9262, -5.1550,
        -5.1550, -5.1550], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.5716,   0.2414,  -4.0376,  -9.4314,  23.5792,   0.3594,   3.6483,
           0.3605,   0.3590,   0.3621],
        [ -1.6813,  -0.6848,   3.7208,   9.7067, -23.6251,  -0.3589,  -3.4492,
          -0.3577,  -0.3592,  -0.3562]], device='cuda:0'))])
loaded xi:  189.94003
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2792.7942951970867
Current xi:  [199.39183]
objective value function right now is: -2792.7942951970867
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.78345]
objective value function right now is: -2672.9591651255264
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -2840.8429497035568
Current xi:  [206.77005]
objective value function right now is: -2840.8429497035568
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.82716]
objective value function right now is: -2740.63755330941
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -2843.1474370769615
Current xi:  [211.62726]
objective value function right now is: -2843.1474370769615
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -2850.3899496516983
Current xi:  [212.12537]
objective value function right now is: -2850.3899496516983
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [213.61281]
objective value function right now is: -2825.5044495681864
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.87463]
objective value function right now is: -2844.0266200259152
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.33136]
objective value function right now is: -2797.1195144044887
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.29156]
objective value function right now is: -2671.839937876032
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.39088]
objective value function right now is: -2838.6862807510333
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.33449]
objective value function right now is: -2810.7689050035283
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.72116]
objective value function right now is: -2822.1367336164817
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -2866.2487692325553
Current xi:  [214.1636]
objective value function right now is: -2866.2487692325553
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.45871]
objective value function right now is: -2728.0250443869245
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.90814]
objective value function right now is: -2840.287505476702
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.24579]
objective value function right now is: -2859.522327167307
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.99132]
objective value function right now is: -2861.070971056175
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.59007]
objective value function right now is: -2768.987068338002
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.13861]
objective value function right now is: -2821.896178017058
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.55182]
objective value function right now is: -2657.7237648387636
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.64569]
objective value function right now is: -2771.4945261399544
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.9325]
objective value function right now is: -2818.402833989697
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.80856]
objective value function right now is: -2855.92920305219
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.19121]
objective value function right now is: -2818.578132877044
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.10535]
objective value function right now is: -2834.3558825273403
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.0215]
objective value function right now is: -2802.224546222477
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [215.00006]
objective value function right now is: -2792.5865943537437
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [214.41533]
objective value function right now is: -2805.1199911852627
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.0872]
objective value function right now is: -2776.6595102466035
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.3138]
objective value function right now is: -2865.2199133086633
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.13683]
objective value function right now is: -2860.4970486932884
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.5505]
objective value function right now is: -2835.1430401961557
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.13292]
objective value function right now is: -2849.367085047929
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.0004]
objective value function right now is: -2813.4436835404085
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2888.4919137808197
Current xi:  [214.14317]
objective value function right now is: -2888.4919137808197
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.44833]
objective value function right now is: -2876.4891900219404
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.11073]
objective value function right now is: -2848.0165172009088
