Starting at: 
25-11-22_11:43

 Random seed:  2  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       8  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       8  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1  logistic_sigmoid   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 8)    False        None  
2       (8, 8)    False        None  
3       (8, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       8  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       8  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 8)    False        None  
2       (8, 8)    False        None  
3       (8, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer        8  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer        8  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0       (2, 8)     False        None  
0       (8, 8)     False        None  
0       (8, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer        8  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer        8  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0       (2, 8)     False        None  
0       (8, 8)     False        None  
0       (8, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1579.8088921591966
W_T_median: 1137.3149081751399
W_T_pctile_5: -129.57910035187072
W_T_CVAR_5_pct: -296.09225597438984
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.493665e-08]
objective value function right now is: -1710.7210374296685
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.8209842e-20]
objective value function right now is: -1715.7359644852677
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.2832635e-32]
objective value function right now is: -1715.9610508511234
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00085227]
objective value function right now is: -1716.2675165676787
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00395769]
objective value function right now is: -1714.2993070949972
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.094549e-05]
objective value function right now is: -1716.3516937846039
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00145179]
objective value function right now is: -1715.7647317917772
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00033057]
objective value function right now is: -1717.45558183202
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00020422]
objective value function right now is: -1717.467295450046
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00021914]
objective value function right now is: -1716.976058311461
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00741455]
objective value function right now is: -1716.1857902739935
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00298941]
objective value function right now is: -1716.6615860029553
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00238104]
objective value function right now is: -1718.5064893074496
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00021417]
objective value function right now is: -1718.9851711628744
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.848188e-05]
objective value function right now is: -1719.6966045121574
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0012319]
objective value function right now is: -1719.2886998442211
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00198095]
objective value function right now is: -1719.9362808585524
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0009364]
objective value function right now is: -1720.0693873908344
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.2838265e-06]
objective value function right now is: -1706.6692093523204
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00022131]
objective value function right now is: -1708.2416278768753
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00025852]
objective value function right now is: -1707.8770834749648
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00132769]
objective value function right now is: -1708.4532788398271
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00067269]
objective value function right now is: -1720.1915965049368
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00049342]
objective value function right now is: -1720.4321539876019
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00165959]
objective value function right now is: -1720.9470847816278
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.542846e-05]
objective value function right now is: -1721.1985738955257
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00025488]
objective value function right now is: -1719.0821311359186
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [0.00475594]
objective value function right now is: -1720.0623642502176
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00118402]
objective value function right now is: -1719.0197830629547
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00014926]
objective value function right now is: -1720.8266154654127
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00014126]
objective value function right now is: -1721.1217983572383
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.8876e-05]
objective value function right now is: -1720.0044619824832
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00672693]
objective value function right now is: -1720.428504388537
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00053307]
objective value function right now is: -1719.91001174874
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00050102]
objective value function right now is: -1721.0072619824596
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00014282]
objective value function right now is: -1719.4836614923854
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.416345e-05]
objective value function right now is: -1717.879790721244
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00278778]
objective value function right now is: -1720.0573616966085
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00062116]
objective value function right now is: -1720.7599335063346
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.5879664e-05]
objective value function right now is: -1720.1262930957168
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00477528]
objective value function right now is: -1712.6714296924354
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.1159612e-05]
objective value function right now is: -1721.188059429056
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00790206]
objective value function right now is: -1719.3086636737905
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0001453]
objective value function right now is: -1721.4328460571267
new min fval:  -1394.9232165571154
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00685771]
objective value function right now is: -1721.7918616714344
new min fval:  -1721.834907744445
new min fval:  -1721.8615396552207
new min fval:  -1721.8964294432667
new min fval:  -1721.925268243893
new min fval:  -1721.954458419523
new min fval:  -1721.9808919117202
new min fval:  -1722.0038645220027
new min fval:  -1722.0266322434493
new min fval:  -1722.040889018068
new min fval:  -1722.0446695425794
new min fval:  -1722.0478077919852
new min fval:  -1722.0526792422013
new min fval:  -1722.0564897052452
new min fval:  -1722.0604183927292
new min fval:  -1722.0626153108497
new min fval:  -1722.066006545227
new min fval:  -1722.0668059111008
new min fval:  -1722.0676815892589
new min fval:  -1722.0689569273047
new min fval:  -1722.0705129217476
new min fval:  -1722.072945217874
new min fval:  -1722.0748711711988
new min fval:  -1722.076235174011
new min fval:  -1722.0764306467076
new min fval:  -1722.0765010441528
new min fval:  -1722.076521424167
new min fval:  -1722.0768563142406
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00504522]
objective value function right now is: -1721.53770797131
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0033617]
objective value function right now is: -1720.958360355273
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00065908]
objective value function right now is: -1714.8077604798154
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.317195e-05]
objective value function right now is: -1721.4541561128028
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00069768]
objective value function right now is: -1718.9892226630539
Traceback (most recent call last):
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/decumulation_driver.py", line 856, in <module>
    fun_RUN__wrapper.RUN__wrapper_ONE_stage_optimization(
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_RUN__wrapper.py", line 115, in RUN__wrapper_ONE_stage_optimization
    RUN__wrapper_training_testing_NN(
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_RUN__wrapper.py", line 207, in RUN__wrapper_training_testing_NN
    res_adam = fun_train_NN.train_NN( theta0 = theta0,
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_train_NN.py", line 196, in train_NN
    result_pyt_adam = run_Gradient_Descent_pytorch(NN_list= NN_list,
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_train_NN_SGD_algorithms.py", line 236, in run_Gradient_Descent_pytorch
    min_fval, _ = objfun_pyt(NN_list_min, params, xi_min)
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_eval_objfun_NN_strategy.py", line 31, in eval_obj_NN_strategy_pyt
    params, g, qsum_T_vector = fun_invest_NN_strategy.withdraw_invest_NN_strategy(NN_list, params)
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_invest_NN_strategy.py", line 138, in withdraw_invest_NN_strategy
    q_n_proportion = torch.squeeze(NN_list[0].forward(phi_1))
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_NN_Pytorch.py", line 83, in forward
    return self.model(input_tensor)
  File "/home/marcchen/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/marcchen/.local/lib/python3.10/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/marcchen/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/marcchen/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 23.69 GiB total capacity; 19.62 GiB already allocated; 43.75 MiB free; 20.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
