Starting at: 
16-11-22_07:46

 Random seed:  2  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1  logistic_sigmoid   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer        4  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer        4  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0       (2, 4)     False        None  
0       (4, 4)     False        None  
0       (4, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer        4  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer        4  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0       (2, 4)     False        None  
0       (4, 4)     False        None  
0       (4, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.1365247]
objective value function right now is: -1694.6983705519276
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02967341]
objective value function right now is: -1706.5075006884317
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.9584754e-07]
objective value function right now is: -1706.9973692004808
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.3122674e-11]
objective value function right now is: -1710.7538368266137
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.035784e-15]
objective value function right now is: -1711.4006432312406
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.152654e-21]
objective value function right now is: -1711.139575411199
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-3.6779464e-24]
objective value function right now is: -1712.516031613662
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.5801783e-29]
objective value function right now is: -1712.5450545872477
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.1549885e-32]
objective value function right now is: -1712.1460194884755
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0002934]
objective value function right now is: -1713.0179288013624
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00228483]
objective value function right now is: -1713.4196910015103
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.2307873e-05]
objective value function right now is: -1713.23398524782
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00597798]
objective value function right now is: -1713.3849430449065
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00398933]
objective value function right now is: -1712.3764097720352
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00388585]
objective value function right now is: -1713.9368843239151
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00011596]
objective value function right now is: -1713.2390125200739
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00327553]
objective value function right now is: -1713.3606654665452
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00046494]
objective value function right now is: -1714.251499903306
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00931739]
objective value function right now is: -1714.4714009738796
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00418534]
objective value function right now is: -1713.8098056591978
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00044333]
objective value function right now is: -1713.2324677858521
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00844701]
objective value function right now is: -1714.8094886677643
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00010286]
objective value function right now is: -1714.6790276136876
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00141622]
objective value function right now is: -1714.951489726317
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00166974]
objective value function right now is: -1712.36594761812
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00058417]
objective value function right now is: -1714.8066527533601
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00156413]
objective value function right now is: -1709.3742065569388
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-6.578861e-05]
objective value function right now is: -1714.9470762171816
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [0.00038232]
objective value function right now is: -1712.7704695342702
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00037116]
objective value function right now is: -1715.4597706936863
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02279972]
objective value function right now is: -1715.48934828384
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.4599106e-05]
objective value function right now is: -1715.2164093270615
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.459825e-06]
objective value function right now is: -1715.4792385476762
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00096596]
objective value function right now is: -1715.451374666329
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.164366e-05]
objective value function right now is: -1715.7504969697063
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.655545e-06]
objective value function right now is: -1715.072050515838
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00011238]
objective value function right now is: -1715.252948033683
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00460097]
objective value function right now is: -1715.637506304193
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00308371]
objective value function right now is: -1713.3478148793076
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.1191327e-06]
objective value function right now is: -1716.0367704240512
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00448058]
objective value function right now is: -1712.4899278207954
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00306861]
objective value function right now is: -1715.9808008719501
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00030919]
objective value function right now is: -1715.854807577738
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00162802]
objective value function right now is: -1715.4449769280568
new min fval:  -1385.4083039314955
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00111328]
objective value function right now is: -1715.458632650023
new min fval:  -1714.9786872412044
new min fval:  -1715.0664742140664
new min fval:  -1715.360581384842
new min fval:  -1715.563213230558
new min fval:  -1715.7038656626519
new min fval:  -1715.7601947232636
new min fval:  -1715.7767566127945
new min fval:  -1715.815837308927
new min fval:  -1715.871420432979
new min fval:  -1715.908164427841
new min fval:  -1715.9333667429216
new min fval:  -1715.960109228693
new min fval:  -1715.9913666688674
new min fval:  -1716.0225115237747
new min fval:  -1716.0567865905327
new min fval:  -1716.0885662533296
new min fval:  -1716.1338590112962
new min fval:  -1716.1683635981658
new min fval:  -1716.1942148128649
new min fval:  -1716.2068563658377
new min fval:  -1716.2102966594957
new min fval:  -1716.2167074231918
new min fval:  -1716.2287714776212
new min fval:  -1716.2388724583848
new min fval:  -1716.2454949549633
new min fval:  -1716.2489882757898
new min fval:  -1716.2511922087965
new min fval:  -1716.255336080695
new min fval:  -1716.2618753763454
new min fval:  -1716.270123605415
new min fval:  -1716.2807246720836
new min fval:  -1716.292162771315
new min fval:  -1716.3033027712236
new min fval:  -1716.3109940448392
new min fval:  -1716.31674270866
new min fval:  -1716.322365355195
new min fval:  -1716.3279079577303
new min fval:  -1716.3324764799484
new min fval:  -1716.3363672083785
new min fval:  -1716.3385352502778
new min fval:  -1716.3397165827212
new min fval:  -1716.3403088758607
new min fval:  -1716.3409664442213
new min fval:  -1716.3432823824678
new min fval:  -1716.346816422767
new min fval:  -1716.3502659025612
new min fval:  -1716.3554446125613
new min fval:  -1716.359513912432
new min fval:  -1716.362010617827
new min fval:  -1716.3651703482806
new min fval:  -1716.3684338338865
new min fval:  -1716.3710096800014
new min fval:  -1716.3741192776317
new min fval:  -1716.3751617079267
new min fval:  -1716.3786579987411
new min fval:  -1716.3797398271213
new min fval:  -1716.3798059601131
new min fval:  -1716.381330079753
new min fval:  -1716.3821252430678
new min fval:  -1716.3828859651826
new min fval:  -1716.3833996269568
new min fval:  -1716.3837015353433
new min fval:  -1716.383714907012
new min fval:  -1716.3838540679853
new min fval:  -1716.3844920362026
new min fval:  -1716.3857091765503
new min fval:  -1716.387304702215
new min fval:  -1716.3888398612182
new min fval:  -1716.3909098467227
new min fval:  -1716.392997212248
new min fval:  -1716.394443908608
new min fval:  -1716.3949890490605
new min fval:  -1716.3953398438646
new min fval:  -1716.3958432342154
new min fval:  -1716.396567430261
new min fval:  -1716.3975771137714
new min fval:  -1716.3989515908474
new min fval:  -1716.4005531033883
new min fval:  -1716.402207841546
new min fval:  -1716.4038220133148
new min fval:  -1716.4051658575604
new min fval:  -1716.4059826306502
new min fval:  -1716.4065287572485
new min fval:  -1716.4069519005625
new min fval:  -1716.407001577998
new min fval:  -1716.4081648232248
new min fval:  -1716.4091102754644
new min fval:  -1716.4091954999042
new min fval:  -1716.409660433323
new min fval:  -1716.4096719152626
new min fval:  -1716.4099095371184
new min fval:  -1716.4101456606124
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00300401]
objective value function right now is: -1713.8315828471207
new min fval:  -1716.4107276613106
new min fval:  -1716.4114041230673
new min fval:  -1716.4116302990824
new min fval:  -1716.4116499125705
new min fval:  -1716.4123217798024
new min fval:  -1716.4128292957755
new min fval:  -1716.4129899501
new min fval:  -1716.4130202387469
new min fval:  -1716.4132168630842
new min fval:  -1716.4132521611896
new min fval:  -1716.4134080611964
new min fval:  -1716.4136256525549
new min fval:  -1716.4138596028192
new min fval:  -1716.414254176564
new min fval:  -1716.4147758579502
new min fval:  -1716.41513279322
new min fval:  -1716.4151563359853
new min fval:  -1716.4165735477177
new min fval:  -1716.4177437360645
new min fval:  -1716.4184924211338
new min fval:  -1716.4188969927034
new min fval:  -1716.4190005404912
new min fval:  -1716.4193601954819
new min fval:  -1716.419678346333
new min fval:  -1716.4199105344403
new min fval:  -1716.420326058153
new min fval:  -1716.4206846488355
new min fval:  -1716.421030755599
new min fval:  -1716.421079082781
new min fval:  -1716.4212705957434
new min fval:  -1716.4215233219218
new min fval:  -1716.421879515416
new min fval:  -1716.422037732518
new min fval:  -1716.422166915832
new min fval:  -1716.4222187333148
new min fval:  -1716.4223795447942
new min fval:  -1716.4228396697733
new min fval:  -1716.4231050772364
new min fval:  -1716.4232187014984
new min fval:  -1716.4232460076983
new min fval:  -1716.4233812421621
new min fval:  -1716.4238161177955
new min fval:  -1716.4243343680948
new min fval:  -1716.4246510537696
new min fval:  -1716.4248245915217
new min fval:  -1716.4251751053152
new min fval:  -1716.4255388574763
new min fval:  -1716.4256473528044
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01200038]
objective value function right now is: -1716.180905564707
new min fval:  -1716.4256675712852
new min fval:  -1716.4258380422834
new min fval:  -1716.4259938823643
new min fval:  -1716.426459604678
new min fval:  -1716.4268983094446
new min fval:  -1716.4273112727283
new min fval:  -1716.4274558271165
new min fval:  -1716.4275708459684
new min fval:  -1716.4276478750608
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00017302]
objective value function right now is: -1714.6221337699653
new min fval:  -1716.4277076459089
new min fval:  -1716.4278886763705
new min fval:  -1716.4280298171789
new min fval:  -1716.4280888209864
new min fval:  -1716.4282003379358
new min fval:  -1716.4283214219165
new min fval:  -1716.42849261365
new min fval:  -1716.4286913139113
new min fval:  -1716.428875957126
new min fval:  -1716.4291532987943
new min fval:  -1716.4293071395737
new min fval:  -1716.4294010096178
new min fval:  -1716.4295998496027
new min fval:  -1716.4301219705396
new min fval:  -1716.4304998492003
new min fval:  -1716.430705052157
new min fval:  -1716.430836584037
new min fval:  -1716.4308972711824
new min fval:  -1716.4310210297913
new min fval:  -1716.4312043127168
new min fval:  -1716.4312200501895
new min fval:  -1716.4313258266045
new min fval:  -1716.4315970237733
new min fval:  -1716.4317395886094
new min fval:  -1716.4318232122478
new min fval:  -1716.4319710408738
new min fval:  -1716.431980869381
new min fval:  -1716.4321833451643
new min fval:  -1716.432376299132
new min fval:  -1716.4325910673708
new min fval:  -1716.4326686463783
new min fval:  -1716.4327901669621
new min fval:  -1716.4332262135742
new min fval:  -1716.433742112001
new min fval:  -1716.4342119952844
new min fval:  -1716.4343902982023
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00505176]
objective value function right now is: -1714.4385543333758
new min fval:  -1716.4347078523544
new min fval:  -1716.4348211453525
new min fval:  -1716.4349041225394
new min fval:  -1716.4355927347715
new min fval:  -1716.4363017006633
new min fval:  -1716.4368324665672
new min fval:  -1716.4370096582263
new min fval:  -1716.4371041358868
new min fval:  -1716.4374097160487
new min fval:  -1716.437569254561
new min fval:  -1716.4376516291043
new min fval:  -1716.4376529751798
new min fval:  -1716.437842883478
new min fval:  -1716.4380552794398
new min fval:  -1716.4382094987996
new min fval:  -1716.43826746658
new min fval:  -1716.4385199523708
new min fval:  -1716.4388073894115
new min fval:  -1716.4388819450571
new min fval:  -1716.4389039014452
new min fval:  -1716.4391597484487
new min fval:  -1716.4392894371167
new min fval:  -1716.4394121312848
new min fval:  -1716.4396777090294
new min fval:  -1716.439914550031
new min fval:  -1716.4400048792102
new min fval:  -1716.440075215285
new min fval:  -1716.4401432417953
new min fval:  -1716.4403885016013
new min fval:  -1716.4406230780355
new min fval:  -1716.4408183592768
new min fval:  -1716.4409262326844
new min fval:  -1716.4411870121503
new min fval:  -1716.4412879403005
new min fval:  -1716.4413343992799
new min fval:  -1716.4414213926343
new min fval:  -1716.4416663651798
new min fval:  -1716.441841844366
new min fval:  -1716.4420349084246
new min fval:  -1716.4421860583732
new min fval:  -1716.4422991560218
new min fval:  -1716.4424827743396
new min fval:  -1716.4426666731968
new min fval:  -1716.4428441449081
new min fval:  -1716.4430796385366
new min fval:  -1716.4432664274002
new min fval:  -1716.4433812148156
new min fval:  -1716.4434183875885
new min fval:  -1716.443444418645
new min fval:  -1716.4435786958147
new min fval:  -1716.4437799220473
new min fval:  -1716.4439769749451
new min fval:  -1716.444003613061
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.1652444e-05]
objective value function right now is: -1715.0716017928935
min fval:  -1716.444003613061
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 497.2701756957194
W_T_median: 263.80204597990684
W_T_pctile_5: -227.86781229932313
W_T_CVAR_5_pct: -470.9232126877017
Average q (qsum/M+1):  56.43900422127016
Optimal xi:  [2.3590816e-05]
Expected(across Rb) median(across samples) p_equity:  0.39609493017196656
obj fun:  tensor(-1716.4440, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.415864]
objective value function right now is: -1655.6286389692161
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.18400197]
objective value function right now is: -1661.3928052702504
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.0735561e-05]
objective value function right now is: -1661.6740855141952
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.1428504e-10]
objective value function right now is: -1660.1094972353671
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.7987734e-15]
objective value function right now is: -1661.926437182434
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.7710453e-19]
objective value function right now is: -1662.5331748093922
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-1.013536e-23]
objective value function right now is: -1661.2914605123979
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.326281e-23]
objective value function right now is: -1661.9060539059099
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.7440155e-24]
objective value function right now is: -1662.0184904167365
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.051549]
objective value function right now is: -1663.0240075833942
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.9086134e-07]
objective value function right now is: -1660.2279545340195
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.692549e-07]
objective value function right now is: -1662.0052082729815
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.3867134e-07]
objective value function right now is: -1662.2165804067636
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01315]
objective value function right now is: -1660.6196819493512
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00024007]
objective value function right now is: -1662.0797694311868
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.0936343e-06]
objective value function right now is: -1662.654922169482
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00038834]
objective value function right now is: -1661.064757867895
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.03081385]
objective value function right now is: -1661.91251862514
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.03085991]
objective value function right now is: -1662.76025731779
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00368141]
objective value function right now is: -1662.4012783661979
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00653281]
objective value function right now is: -1662.923197172874
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.184871e-06]
objective value function right now is: -1662.3873935386441
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00527214]
objective value function right now is: -1660.8339269865123
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01761218]
objective value function right now is: -1662.6243882397462
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00104144]
objective value function right now is: -1662.5266524223302
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0172581]
objective value function right now is: -1662.514630548269
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0188818]
objective value function right now is: -1662.737800975112
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [0.02402573]
objective value function right now is: -1662.5620709657392
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00012948]
objective value function right now is: -1662.2078613289677
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01891697]
objective value function right now is: -1663.5521898376192
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0421528]
objective value function right now is: -1660.2808614782223
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0156777]
objective value function right now is: -1663.167335478167
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02201304]
objective value function right now is: -1662.8761951581516
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00016131]
objective value function right now is: -1662.8206458648058
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00016609]
objective value function right now is: -1660.4917904002202
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.0915083e-07]
objective value function right now is: -1662.2434258013573
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01663393]
objective value function right now is: -1663.0255725658524
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.0364163e-05]
objective value function right now is: -1662.5864485373822
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01392837]
objective value function right now is: -1663.3437519020226
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.3067696e-05]
objective value function right now is: -1653.959324006077
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.05727332]
objective value function right now is: -1662.2799027920523
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00016659]
objective value function right now is: -1661.2389023922983
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01183262]
objective value function right now is: -1663.4605768558592
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02819488]
objective value function right now is: -1660.6743412938793
new min fval:  -1603.453287105235
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00018935]
objective value function right now is: -1663.335290738573
new min fval:  -1663.278904954964
new min fval:  -1663.3531108363431
new min fval:  -1663.4305124139967
new min fval:  -1663.4389133520629
new min fval:  -1663.4489609413713
new min fval:  -1663.4651554569255
new min fval:  -1663.4847648051493
new min fval:  -1663.49159332093
new min fval:  -1663.4964277879367
new min fval:  -1663.518031829081
new min fval:  -1663.5467320296173
new min fval:  -1663.5743755822684
new min fval:  -1663.5893068649416
new min fval:  -1663.6024973088072
new min fval:  -1663.6269542020182
new min fval:  -1663.639447423784
new min fval:  -1663.6429757293542
new min fval:  -1663.6667154492109
new min fval:  -1663.68069326976
new min fval:  -1663.6835952297477
new min fval:  -1663.6891322317351
new min fval:  -1663.7127739887667
new min fval:  -1663.7259595282353
new min fval:  -1663.72957176319
new min fval:  -1663.7352221285578
new min fval:  -1663.7520484610336
new min fval:  -1663.765335937092
new min fval:  -1663.772196015095
new min fval:  -1663.7773793884405
new min fval:  -1663.7925253710825
new min fval:  -1663.7928563576704
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00682052]
objective value function right now is: -1663.1923006212971
new min fval:  -1663.7939585897543
new min fval:  -1663.794559385495
new min fval:  -1663.794617802603
new min fval:  -1663.7959215128142
new min fval:  -1663.796172050036
new min fval:  -1663.797055024281
new min fval:  -1663.7976894840046
new min fval:  -1663.7978490288742
new min fval:  -1663.7982291631558
new min fval:  -1663.8001620654832
new min fval:  -1663.8020851551921
new min fval:  -1663.804337368022
new min fval:  -1663.8077971983441
new min fval:  -1663.8115612187
new min fval:  -1663.8165006260888
new min fval:  -1663.8220270051686
new min fval:  -1663.8257735497964
new min fval:  -1663.829039114114
new min fval:  -1663.8314666414888
new min fval:  -1663.8345730215497
new min fval:  -1663.8358898188253
new min fval:  -1663.8362618522574
new min fval:  -1663.836879777396
new min fval:  -1663.8375751526949
new min fval:  -1663.8387816062584
new min fval:  -1663.8396199433132
new min fval:  -1663.8400749258453
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.04508401]
objective value function right now is: -1663.1113374924353
new min fval:  -1663.8406855081014
new min fval:  -1663.8418653589658
new min fval:  -1663.842499192624
new min fval:  -1663.8428475624078
new min fval:  -1663.8433720285348
new min fval:  -1663.8442305136557
new min fval:  -1663.8445781375403
new min fval:  -1663.845114644482
new min fval:  -1663.845703485138
new min fval:  -1663.8461038902892
new min fval:  -1663.8464483884338
new min fval:  -1663.8472260751055
new min fval:  -1663.8474670040687
new min fval:  -1663.847525149264
new min fval:  -1663.8481680557554
new min fval:  -1663.8483763236122
new min fval:  -1663.8488413876503
new min fval:  -1663.8495710642337
new min fval:  -1663.849915852136
new min fval:  -1663.850193528928
new min fval:  -1663.8510590482301
new min fval:  -1663.8512946013054
new min fval:  -1663.851417849585
new min fval:  -1663.8520167114898
new min fval:  -1663.8523266573911
new min fval:  -1663.852781948066
new min fval:  -1663.8529721329894
new min fval:  -1663.8532197546433
new min fval:  -1663.8534577866446
new min fval:  -1663.8536549272433
new min fval:  -1663.853665263712
new min fval:  -1663.8537708655667
new min fval:  -1663.8541383660809
new min fval:  -1663.854382114325
new min fval:  -1663.8544660683735
new min fval:  -1663.8548428567808
new min fval:  -1663.85492736361
new min fval:  -1663.855274618098
new min fval:  -1663.8559793582915
new min fval:  -1663.8561708441512
new min fval:  -1663.856482346408
new min fval:  -1663.8572184997388
new min fval:  -1663.8573897320969
new min fval:  -1663.8575478850535
new min fval:  -1663.8576369252248
new min fval:  -1663.8577850188237
new min fval:  -1663.8580342227713
new min fval:  -1663.8583431976695
new min fval:  -1663.8587680629985
new min fval:  -1663.8594604885789
new min fval:  -1663.8596870643087
new min fval:  -1663.859843332341
new min fval:  -1663.8606469546226
new min fval:  -1663.8609212832703
new min fval:  -1663.8609512617388
new min fval:  -1663.8615672813078
new min fval:  -1663.8620739811786
new min fval:  -1663.8620893845907
new min fval:  -1663.862287248291
new min fval:  -1663.8632513311632
new min fval:  -1663.8638224799672
new min fval:  -1663.8639653347645
new min fval:  -1663.8647657195243
new min fval:  -1663.8650622362766
new min fval:  -1663.8656485841013
new min fval:  -1663.865727716282
new min fval:  -1663.8657804208378
new min fval:  -1663.8661499451287
new min fval:  -1663.8662442371829
new min fval:  -1663.866355333338
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00529197]
objective value function right now is: -1662.5774938866305
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0128604]
objective value function right now is: -1663.635094695045
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00474736]
objective value function right now is: -1661.7502229103359
min fval:  -1663.866355333338
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 629.2658297807706
W_T_median: 218.98388537676374
W_T_pctile_5: -43.53835568666244
W_T_CVAR_5_pct: -221.9885826819994
Average q (qsum/M+1):  55.157490391885084
Optimal xi:  [-0.00035785]
Expected(across Rb) median(across samples) p_equity:  0.41215484142303466
obj fun:  tensor(-1663.8664, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.4068465]
objective value function right now is: -1594.7149647613953
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.7973633]
objective value function right now is: -1605.4973038699125
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02760214]
objective value function right now is: -1606.6932572188498
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.8091475e-07]
objective value function right now is: -1609.0459437119423
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.8111842e-11]
objective value function right now is: -1609.6968236265043
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.474523e-16]
objective value function right now is: -1604.964677165356
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [6.8586314e-20]
objective value function right now is: -1609.2382631405253
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.869629e-18]
objective value function right now is: -1609.2216460177265
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.108417e-18]
objective value function right now is: -1607.5558547240314
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.1157984e-10]
objective value function right now is: -1591.0626892034468
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00158953]
objective value function right now is: -1609.6816289018327
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.04707501]
objective value function right now is: -1606.2719666464639
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.942609e-06]
objective value function right now is: -1603.4224158083503
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [0.00100719]
objective value function right now is: -1609.452268103785
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.1509315e-05]
objective value function right now is: -1604.0247604595613
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00124578]
objective value function right now is: -1610.1167285934257
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.301325e-05]
objective value function right now is: -1594.4580486115703
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01585412]
objective value function right now is: -1600.5286699793237
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.14292955]
objective value function right now is: -1599.7983168058106
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.9068448e-05]
objective value function right now is: -1606.4489813338566
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00320513]
objective value function right now is: -1609.6433996617614
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.10545886]
objective value function right now is: -1609.8571295942236
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00911649]
objective value function right now is: -1608.3124015630788
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00071654]
objective value function right now is: -1610.468379260346
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00013586]
objective value function right now is: -1608.1946495588174
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0007596]
objective value function right now is: -1607.198655821785
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01385546]
objective value function right now is: -1608.1343367631584
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [0.00142116]
objective value function right now is: -1606.7790965956492
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06609678]
objective value function right now is: -1609.663058694202
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.3114262e-05]
objective value function right now is: -1596.4387721237922
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01466324]
objective value function right now is: -1607.409664539578
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00559565]
objective value function right now is: -1609.1830052679563
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00117681]
objective value function right now is: -1603.6583209506393
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01500328]
objective value function right now is: -1610.4984000409029
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00113857]
objective value function right now is: -1607.4394233294427
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00065654]
objective value function right now is: -1589.428999006729
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00013823]
objective value function right now is: -1604.8604929212393
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00143091]
objective value function right now is: -1608.1388889725226
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.10606546]
objective value function right now is: -1607.9121586783335
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.952427e-06]
objective value function right now is: -1607.6312562288624
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.07965828]
objective value function right now is: -1606.1731791879167
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00045995]
objective value function right now is: -1606.008189072433
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00680644]
objective value function right now is: -1607.3835889793872
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00130816]
objective value function right now is: -1605.8684437398567
new min fval:  -1591.3278231050122
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.6816495e-05]
objective value function right now is: -1608.3026223849827
new min fval:  -1609.259266384911
new min fval:  -1609.3621056573406
new min fval:  -1609.4049989230848
new min fval:  -1609.5626399445619
new min fval:  -1609.6299119859984
new min fval:  -1609.6350501905536
new min fval:  -1609.6723765417262
new min fval:  -1609.6830346958789
new min fval:  -1609.687798088552
new min fval:  -1609.694297899171
new min fval:  -1609.6996827249527
new min fval:  -1609.7050101424227
new min fval:  -1609.7115779159578
new min fval:  -1609.7158125573794
new min fval:  -1609.721375689538
new min fval:  -1609.724015927842
new min fval:  -1609.7278525529946
new min fval:  -1609.729533505805
new min fval:  -1609.7338783843397
new min fval:  -1609.7389453626395
new min fval:  -1609.7416395795647
new min fval:  -1609.7466724331186
new min fval:  -1609.7498714674468
new min fval:  -1609.7520629079108
new min fval:  -1609.7542813149903
new min fval:  -1609.7577021882637
new min fval:  -1609.7611101725843
new min fval:  -1609.76213084433
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01282218]
objective value function right now is: -1609.4808527466025
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0022851]
objective value function right now is: -1608.0132649054876
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01196515]
objective value function right now is: -1607.8886146075795
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00019674]
objective value function right now is: -1607.253596575669
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02528967]
objective value function right now is: -1608.4681496578676
min fval:  -1609.76213084433
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 344.60028123763016
W_T_median: 189.47486759149632
W_T_pctile_5: -5.516483143927202
W_T_CVAR_5_pct: -142.1728662712784
Average q (qsum/M+1):  54.22913385206653
Optimal xi:  [-0.0030271]
Expected(across Rb) median(across samples) p_equity:  0.3435551250376496
obj fun:  tensor(-1609.7621, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.209262]
objective value function right now is: -1530.146409383181
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.1043115]
objective value function right now is: -1544.5650361018422
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.3527222]
objective value function right now is: -1543.1244242133516
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.6459392]
objective value function right now is: -1546.7015235332894
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00903765]
objective value function right now is: -1544.161794869133
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.605662e-05]
objective value function right now is: -1546.79521805265
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-1.868165e-09]
objective value function right now is: -1544.9483530866862
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.1545307e-11]
objective value function right now is: -1545.3102335603012
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.8804096e-07]
objective value function right now is: -1545.0269808265496
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05674698]
objective value function right now is: -1546.1589211308883
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.3350676e-06]
objective value function right now is: -1541.9049000820826
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.10282275]
objective value function right now is: -1547.2654346992201
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00074348]
objective value function right now is: -1545.3256069331399
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [0.00099265]
objective value function right now is: -1538.9710587249588
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.3682442e-05]
objective value function right now is: -1547.5741615518243
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00051173]
objective value function right now is: -1546.3246096492373
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00177711]
objective value function right now is: -1546.8898343096273
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.000618]
objective value function right now is: -1545.9524019551054
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00879348]
objective value function right now is: -1548.1518253104189
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.0649839e-05]
objective value function right now is: -1541.5380502553571
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00223437]
objective value function right now is: -1547.3339767953023
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0394868]
objective value function right now is: -1549.2587453144872
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.07443066]
objective value function right now is: -1548.1006320450758
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.439729e-05]
objective value function right now is: -1549.650907032888
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.4308806e-06]
objective value function right now is: -1547.6601253004012
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.09607128]
objective value function right now is: -1549.2762956446993
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.03205815]
objective value function right now is: -1547.45151338739
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.2152686]
objective value function right now is: -1549.9636910240679
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [5.7774556e-05]
objective value function right now is: -1547.8160471521403
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.2217727]
objective value function right now is: -1545.3903927292731
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01218121]
objective value function right now is: -1548.8244146838117
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0003137]
objective value function right now is: -1550.217864757615
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02747881]
objective value function right now is: -1545.0820101359254
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03998866]
objective value function right now is: -1546.8055194063743
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00032466]
objective value function right now is: -1547.6486132042728
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00806766]
objective value function right now is: -1549.9391510645412
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.0393647e-06]
objective value function right now is: -1549.4509998652666
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00013736]
objective value function right now is: -1545.0178185260859
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.03355018]
objective value function right now is: -1544.8275365961786
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.3383481e-05]
objective value function right now is: -1545.7744183065415
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.43513358]
objective value function right now is: -1547.5172994311083
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00100548]
objective value function right now is: -1540.505578195446
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00025818]
objective value function right now is: -1549.9922860203274
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00160472]
objective value function right now is: -1550.0449932448853
new min fval:  -1535.8687219171347
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02928666]
objective value function right now is: -1549.1457811180999
new min fval:  -1550.4061833164103
new min fval:  -1550.4330260950094
new min fval:  -1550.4906831265607
new min fval:  -1550.54173513974
new min fval:  -1550.6045229891465
new min fval:  -1550.6556679511536
new min fval:  -1550.7073808011064
new min fval:  -1550.750518196385
new min fval:  -1550.7998990207475
new min fval:  -1550.8624859927752
new min fval:  -1550.9335589294913
new min fval:  -1551.000077227799
new min fval:  -1551.056456007097
new min fval:  -1551.1064803033348
new min fval:  -1551.1506913094272
new min fval:  -1551.191139944653
new min fval:  -1551.2233505074819
new min fval:  -1551.2591837860793
new min fval:  -1551.3045355011504
new min fval:  -1551.35212018289
new min fval:  -1551.394192694833
new min fval:  -1551.421899731402
new min fval:  -1551.438327113595
new min fval:  -1551.448429347645
new min fval:  -1551.4592165492725
new min fval:  -1551.469093786734
new min fval:  -1551.4760648376036
new min fval:  -1551.485081185383
new min fval:  -1551.4885691050695
new min fval:  -1551.4891937235648
new min fval:  -1551.4966493749941
new min fval:  -1551.5047553733239
new min fval:  -1551.5142628313336
new min fval:  -1551.5234319563563
new min fval:  -1551.5353509923052
new min fval:  -1551.543132048802
new min fval:  -1551.5477960103653
new min fval:  -1551.5502820349243
new min fval:  -1551.580927156921
new min fval:  -1551.613172219189
new min fval:  -1551.639265934457
new min fval:  -1551.674109556626
new min fval:  -1551.7124030017503
new min fval:  -1551.7444436262053
new min fval:  -1551.774409425275
new min fval:  -1551.7919270522327
new min fval:  -1551.817162133853
new min fval:  -1551.8399739181382
new min fval:  -1551.8625945959163
new min fval:  -1551.8881277018445
new min fval:  -1551.912832877941
new min fval:  -1551.937248532668
new min fval:  -1551.9557232377492
new min fval:  -1551.9653406240823
new min fval:  -1551.9659909866741
new min fval:  -1551.9730071456036
new min fval:  -1551.9787205898515
new min fval:  -1551.9842945062426
new min fval:  -1551.9895730620026
new min fval:  -1551.993174158798
new min fval:  -1551.9935239086735
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01905604]
objective value function right now is: -1550.0313101885522
new min fval:  -1551.9938053593746
new min fval:  -1551.994833881435
new min fval:  -1551.9952774970461
new min fval:  -1551.9961860194535
new min fval:  -1551.9985310404531
new min fval:  -1551.9996737873255
new min fval:  -1552.0013498082876
new min fval:  -1552.0028159866786
new min fval:  -1552.0032595378514
new min fval:  -1552.0036088310126
new min fval:  -1552.0038792373452
new min fval:  -1552.0039170281939
new min fval:  -1552.0060006865106
new min fval:  -1552.0076267658044
new min fval:  -1552.0089557121723
new min fval:  -1552.0093520993414
new min fval:  -1552.0098257359068
new min fval:  -1552.0115612481784
new min fval:  -1552.0139697368982
new min fval:  -1552.016163916439
new min fval:  -1552.0188669980205
new min fval:  -1552.0212144296747
new min fval:  -1552.0234208381885
new min fval:  -1552.0255026821717
new min fval:  -1552.0273639941436
new min fval:  -1552.0288224149167
new min fval:  -1552.0296020731391
new min fval:  -1552.029886019964
new min fval:  -1552.030298197622
new min fval:  -1552.0304764730008
new min fval:  -1552.0304960044593
new min fval:  -1552.0305064411573
new min fval:  -1552.0316989294206
new min fval:  -1552.0352568657377
new min fval:  -1552.0386580674376
new min fval:  -1552.0416822257287
new min fval:  -1552.044757226458
new min fval:  -1552.047237757593
new min fval:  -1552.0491128753877
new min fval:  -1552.0509189861355
new min fval:  -1552.0521796667563
new min fval:  -1552.0526612784056
new min fval:  -1552.053051208674
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.068189e-05]
objective value function right now is: -1550.2391150938722
new min fval:  -1552.0544663334097
new min fval:  -1552.0571214880683
new min fval:  -1552.0595910862153
new min fval:  -1552.0626939399117
new min fval:  -1552.0663681558428
new min fval:  -1552.0703159028096
new min fval:  -1552.0737875850177
new min fval:  -1552.0769826394483
new min fval:  -1552.0790740002808
new min fval:  -1552.0815649717683
new min fval:  -1552.0834213951991
new min fval:  -1552.0852313004386
new min fval:  -1552.0868215219093
new min fval:  -1552.0879128247518
new min fval:  -1552.0885818273432
new min fval:  -1552.0891975534134
new min fval:  -1552.089729442338
new min fval:  -1552.090142147516
new min fval:  -1552.0906078827002
new min fval:  -1552.0907941836335
new min fval:  -1552.0910608504073
new min fval:  -1552.0913690597172
new min fval:  -1552.091498114824
new min fval:  -1552.0917858226326
new min fval:  -1552.091838016363
new min fval:  -1552.0920265807385
new min fval:  -1552.0951157897775
new min fval:  -1552.0953878020935
new min fval:  -1552.0964122554628
new min fval:  -1552.0973987763286
new min fval:  -1552.0996285796036
new min fval:  -1552.1023725483624
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.09272905]
objective value function right now is: -1547.4076516678545
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.12545565]
objective value function right now is: -1550.1592066872124
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00038923]
objective value function right now is: -1550.5074439316322
min fval:  -1552.1023725483624
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 298.5013024819566
W_T_median: 157.4309666875576
W_T_pctile_5: -1.047350941020974
W_T_CVAR_5_pct: -103.67416965490227
Average q (qsum/M+1):  53.41276697958669
Optimal xi:  [0.00045923]
Expected(across Rb) median(across samples) p_equity:  0.281881110739414
obj fun:  tensor(-1552.1024, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.612547]
objective value function right now is: -1313.312329119158
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.571815]
objective value function right now is: -1449.458012419414
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.987431]
objective value function right now is: -1428.5203001606492
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.730727]
objective value function right now is: -1438.7981928811312
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.383733]
objective value function right now is: -1450.090718734419
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.942421]
objective value function right now is: -1466.2510310676048
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [13.59345]
objective value function right now is: -1552.74142366125
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.924723]
objective value function right now is: -1583.3836520916443
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.240423]
objective value function right now is: -1580.2475445885905
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.467025]
objective value function right now is: -1532.3876033271083
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.059099]
objective value function right now is: -1584.8729367754706
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.111765]
objective value function right now is: -1575.0644650789704
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.832237]
objective value function right now is: -1590.0060564732962
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [13.720841]
objective value function right now is: -1589.438516717378
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.646544]
objective value function right now is: -1551.2379604247349
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.374455]
objective value function right now is: -1581.8702643081906
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.982517]
objective value function right now is: -1593.9842387583444
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.584315]
objective value function right now is: -1589.5075940271436
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.6684065]
objective value function right now is: -1577.8289059382987
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.140736]
objective value function right now is: -1569.434459848037
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.282494]
objective value function right now is: -1558.5341053811349
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.384435]
objective value function right now is: -1571.7514257279884
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.022471]
objective value function right now is: -1587.6841455297488
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.234108]
objective value function right now is: -1593.2516727563436
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.932215]
objective value function right now is: -1576.2639404238037
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.189178]
objective value function right now is: -1587.5534518713775
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.025223]
objective value function right now is: -1578.1375281796245
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [14.524903]
objective value function right now is: -1551.0878625919977
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [13.72197]
objective value function right now is: -1571.2722941127006
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.7070875]
objective value function right now is: -1586.2166550326217
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.012119]
objective value function right now is: -1583.295959503253
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.847025]
objective value function right now is: -1587.695104522643
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.812751]
objective value function right now is: -1564.308939110389
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.149477]
objective value function right now is: -1589.37090087356
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.921956]
objective value function right now is: -1576.518714977883
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.368468]
objective value function right now is: -1554.9113541400106
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.54964]
objective value function right now is: -1578.515289435524
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.857404]
objective value function right now is: -1576.0177411664295
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.739128]
objective value function right now is: -1568.6255184265253
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.0389185]
objective value function right now is: -1586.0861673033633
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.252451]
objective value function right now is: -1585.9417159696661
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.089052]
objective value function right now is: -1590.0305296471963
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.8406105]
objective value function right now is: -1589.3577003406097
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.440034]
objective value function right now is: -1571.7888557404826
new min fval:  4758.493187193937
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.851288]
objective value function right now is: -1584.2093222129192
new min fval:  -1586.9598782741168
new min fval:  -1588.3676945618713
new min fval:  -1589.0893861420618
new min fval:  -1589.1086147786732
new min fval:  -1589.377207758233
new min fval:  -1590.0549655763862
new min fval:  -1590.7087685040287
new min fval:  -1591.289271038087
new min fval:  -1591.6884275359503
new min fval:  -1591.9056798631418
new min fval:  -1591.9488824920197
new min fval:  -1592.0375679572612
new min fval:  -1592.4097914884987
new min fval:  -1592.8072105169795
new min fval:  -1593.200996135145
new min fval:  -1593.5135936926204
new min fval:  -1593.7316584008643
new min fval:  -1593.9007556893746
new min fval:  -1594.008526420702
new min fval:  -1594.0429980390056
new min fval:  -1594.0500836064102
new min fval:  -1594.193148435583
new min fval:  -1594.6720790575407
new min fval:  -1595.145177169732
new min fval:  -1595.609744054045
new min fval:  -1596.0218991557701
new min fval:  -1596.3793216282431
new min fval:  -1596.6916043722351
new min fval:  -1596.9510281958771
new min fval:  -1597.142337889136
new min fval:  -1597.2910557463147
new min fval:  -1597.3862886233633
new min fval:  -1597.4274346311493
new min fval:  -1597.4512044522642
new min fval:  -1597.471396855576
new min fval:  -1597.4852810168154
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.749382]
objective value function right now is: -1587.717853479223
new min fval:  -1597.4867389044823
new min fval:  -1597.5045504917
new min fval:  -1597.517083306931
new min fval:  -1597.5230610951953
new min fval:  -1597.5251278673177
new min fval:  -1597.5293709672171
new min fval:  -1597.5330861837049
new min fval:  -1597.5354787190943
new min fval:  -1597.5418520424266
new min fval:  -1597.5478630270518
new min fval:  -1597.558463710115
new min fval:  -1597.5742177430059
new min fval:  -1597.5913603068825
new min fval:  -1597.605463462151
new min fval:  -1597.6185505090095
new min fval:  -1597.6294925846046
new min fval:  -1597.6369427212594
new min fval:  -1597.6377818425728
new min fval:  -1597.6650360542126
new min fval:  -1597.6994219298815
new min fval:  -1597.729244017707
new min fval:  -1597.7580038421454
new min fval:  -1597.7851757946746
new min fval:  -1597.8097151674883
new min fval:  -1597.8285137689295
new min fval:  -1597.840312850635
new min fval:  -1597.8457000641483
new min fval:  -1597.8464019439118
new min fval:  -1597.855840818388
new min fval:  -1597.8659494641097
new min fval:  -1597.877329697298
new min fval:  -1597.8902085278412
new min fval:  -1597.9000374712748
new min fval:  -1597.9109629526451
new min fval:  -1597.9215778222647
new min fval:  -1597.9297739895023
new min fval:  -1597.936486970882
new min fval:  -1597.9425981575039
new min fval:  -1597.9496005560416
new min fval:  -1597.957566021391
new min fval:  -1597.9664782353354
new min fval:  -1597.9763790287143
new min fval:  -1597.9858215277095
new min fval:  -1597.9968678754
new min fval:  -1598.0053286567386
new min fval:  -1598.0110819002978
new min fval:  -1598.014218563154
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.8604]
objective value function right now is: -1588.035307605208
new min fval:  -1598.0261284581081
new min fval:  -1598.0462622015784
new min fval:  -1598.0671491685785
new min fval:  -1598.0858084583567
new min fval:  -1598.1030556642165
new min fval:  -1598.1180490901668
new min fval:  -1598.125892541585
new min fval:  -1598.1295267785335
new min fval:  -1598.143241651965
new min fval:  -1598.161487331451
new min fval:  -1598.1842879584358
new min fval:  -1598.2091330728717
new min fval:  -1598.2298001154118
new min fval:  -1598.2450644471787
new min fval:  -1598.2541001475854
new min fval:  -1598.2571270502322
new min fval:  -1598.2679452430382
new min fval:  -1598.2837101716157
new min fval:  -1598.297797646122
new min fval:  -1598.3060063224593
new min fval:  -1598.3088336037742
new min fval:  -1598.3291173680636
new min fval:  -1598.3528901405855
new min fval:  -1598.376515325728
new min fval:  -1598.396321466895
new min fval:  -1598.4116119627506
new min fval:  -1598.423786724212
new min fval:  -1598.4322267414661
new min fval:  -1598.4360955786915
new min fval:  -1598.4365474410633
new min fval:  -1598.4391595372026
new min fval:  -1598.4476795838186
new min fval:  -1598.46030808509
new min fval:  -1598.4760585027946
new min fval:  -1598.491786556244
new min fval:  -1598.5074706510493
new min fval:  -1598.5226706046742
new min fval:  -1598.5340315394192
new min fval:  -1598.5409205609724
new min fval:  -1598.5455476490642
new min fval:  -1598.5473872438818
new min fval:  -1598.5487246009238
new min fval:  -1598.5573622073803
new min fval:  -1598.5676158533256
new min fval:  -1598.5773683018126
new min fval:  -1598.5840698122613
new min fval:  -1598.5886214614538
new min fval:  -1598.5890676976098
new min fval:  -1598.597616752893
new min fval:  -1598.607377973537
new min fval:  -1598.618288287467
new min fval:  -1598.6302187578183
new min fval:  -1598.6427256368606
new min fval:  -1598.6537398299477
new min fval:  -1598.6606243810588
new min fval:  -1598.6689268405037
new min fval:  -1598.679932649245
new min fval:  -1598.6902820376677
new min fval:  -1598.7008396580768
new min fval:  -1598.7115168990342
new min fval:  -1598.723176859052
new min fval:  -1598.733656107601
new min fval:  -1598.7446822782908
new min fval:  -1598.7540190089467
new min fval:  -1598.7636973971175
new min fval:  -1598.7675316570464
new min fval:  -1598.7759530283897
new min fval:  -1598.7868047180177
new min fval:  -1598.8032949490923
new min fval:  -1598.816096288322
new min fval:  -1598.8299198510056
new min fval:  -1598.8430115044064
new min fval:  -1598.8570497676083
new min fval:  -1598.8704419683477
new min fval:  -1598.8810952657996
new min fval:  -1598.888054418563
new min fval:  -1598.8900997466496
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.195377]
objective value function right now is: -1573.8325630087265
new min fval:  -1598.8943054316178
new min fval:  -1598.9015383391313
new min fval:  -1598.9081824959933
new min fval:  -1598.9155555585724
new min fval:  -1598.9210721759794
new min fval:  -1598.9240295248
new min fval:  -1598.9250877447319
new min fval:  -1598.9262460099633
new min fval:  -1598.926755163459
new min fval:  -1598.9268029562932
new min fval:  -1598.9269270996808
new min fval:  -1598.9291891302375
new min fval:  -1598.9362431974046
new min fval:  -1598.9409229944195
new min fval:  -1598.9470348023028
new min fval:  -1598.9528140814496
new min fval:  -1598.9589484317335
new min fval:  -1598.9627237901245
new min fval:  -1598.9657265872431
new min fval:  -1598.9690577230222
new min fval:  -1598.9714203541396
new min fval:  -1598.9734376574156
new min fval:  -1598.9754945758004
new min fval:  -1598.977819795344
new min fval:  -1598.9805036441442
new min fval:  -1598.983438143945
new min fval:  -1598.9848798229075
new min fval:  -1598.9857832268501
new min fval:  -1598.9863681849079
new min fval:  -1598.9893362596974
new min fval:  -1598.9928984838402
new min fval:  -1598.997090193829
new min fval:  -1599.0017111954164
new min fval:  -1599.0068257776736
new min fval:  -1599.010794026707
new min fval:  -1599.0134168920144
new min fval:  -1599.0152783111203
new min fval:  -1599.0196816136354
new min fval:  -1599.025177336961
new min fval:  -1599.0313174848472
new min fval:  -1599.0378530313246
new min fval:  -1599.0450404020223
new min fval:  -1599.0511231462974
new min fval:  -1599.055234480185
new min fval:  -1599.058628127692
new min fval:  -1599.0615036339132
new min fval:  -1599.0645784086664
new min fval:  -1599.0692388189216
new min fval:  -1599.0745814635643
new min fval:  -1599.0815140133166
new min fval:  -1599.090179708859
new min fval:  -1599.0979011118534
new min fval:  -1599.1046650274518
new min fval:  -1599.1124597006888
new min fval:  -1599.120370847026
new min fval:  -1599.1267311077256
new min fval:  -1599.1315134235426
new min fval:  -1599.1352464560075
new min fval:  -1599.1368967525423
new min fval:  -1599.13834856465
new min fval:  -1599.1394121973144
new min fval:  -1599.140052748703
new min fval:  -1599.1413593051057
new min fval:  -1599.142797474874
new min fval:  -1599.1452622612544
new min fval:  -1599.1488776123924
new min fval:  -1599.1530896676184
new min fval:  -1599.1578863661591
new min fval:  -1599.1620698992647
new min fval:  -1599.164872903565
new min fval:  -1599.1677151783742
new min fval:  -1599.1695115643863
new min fval:  -1599.171936315723
new min fval:  -1599.1791384090077
new min fval:  -1599.1845199467923
new min fval:  -1599.1883035549909
new min fval:  -1599.1899839875184
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.213522]
objective value function right now is: -1582.9356064693625
new min fval:  -1599.1914594816394
new min fval:  -1599.19492908757
new min fval:  -1599.1967206509048
new min fval:  -1599.1983754419512
new min fval:  -1599.1988284743247
new min fval:  -1599.1994594198677
new min fval:  -1599.202014581069
new min fval:  -1599.2051114390727
new min fval:  -1599.2102318133213
new min fval:  -1599.2169210439558
new min fval:  -1599.2238297734816
new min fval:  -1599.2294816740787
new min fval:  -1599.2334210341648
new min fval:  -1599.2361022954133
new min fval:  -1599.2379209508697
new min fval:  -1599.2382800547887
new min fval:  -1599.24006845728
new min fval:  -1599.2404319945194
new min fval:  -1599.2420198522886
new min fval:  -1599.2428756295426
new min fval:  -1599.2440517040197
new min fval:  -1599.2450883979277
new min fval:  -1599.245357430378
new min fval:  -1599.250694700474
new min fval:  -1599.2580628130577
new min fval:  -1599.26589717268
new min fval:  -1599.2724362632514
new min fval:  -1599.277750716718
new min fval:  -1599.2825470718806
new min fval:  -1599.2858510786696
new min fval:  -1599.2874283382246
new min fval:  -1599.2953939596077
new min fval:  -1599.3028686368589
new min fval:  -1599.3095541024393
new min fval:  -1599.3147958201916
new min fval:  -1599.3189958908633
new min fval:  -1599.3225410801113
new min fval:  -1599.3245356267544
new min fval:  -1599.3266411349994
new min fval:  -1599.328436115361
new min fval:  -1599.3317799304539
new min fval:  -1599.3346442940851
new min fval:  -1599.337454705043
new min fval:  -1599.3411602729827
new min fval:  -1599.344962072355
new min fval:  -1599.3463432500203
new min fval:  -1599.3477076504598
new min fval:  -1599.348297704967
new min fval:  -1599.34858752772
new min fval:  -1599.3491372529431
new min fval:  -1599.349610389548
new min fval:  -1599.3500204448337
new min fval:  -1599.350911339157
new min fval:  -1599.3527949382124
new min fval:  -1599.3549413697096
new min fval:  -1599.3582356129937
new min fval:  -1599.360657188335
new min fval:  -1599.3635742047638
new min fval:  -1599.3657117930434
new min fval:  -1599.368993679544
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.14761]
objective value function right now is: -1552.4311877540663
min fval:  -1599.368993679544
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 825.7998995258137
W_T_median: 644.7237482161152
W_T_pctile_5: 200.6669395585066
W_T_CVAR_5_pct: 31.798489845364166
Average q (qsum/M+1):  46.47388089087702
Optimal xi:  [14.046042]
Expected(across Rb) median(across samples) p_equity:  0.27734411135315895
obj fun:  tensor(-1599.3690, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.234306]
objective value function right now is: -2133.2528560504297
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.537198]
objective value function right now is: -3194.533412477961
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.857015]
objective value function right now is: -3218.499578239699
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.777358]
objective value function right now is: -3332.8640429525212
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.353845]
objective value function right now is: -3414.1223139993294
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.952605]
objective value function right now is: -3310.319064945036
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [14.327802]
objective value function right now is: -3307.4173236032684
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.498648]
objective value function right now is: -3322.0065404475367
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.938856]
objective value function right now is: -3475.3384054289168
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.50441]
objective value function right now is: -3262.4302246183565
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.03218]
objective value function right now is: -3453.9210109746687
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.241423]
objective value function right now is: -3230.949325995343
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.687924]
objective value function right now is: -3230.9186910945446
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [15.219299]
objective value function right now is: -3423.120280765315
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.7114105]
objective value function right now is: -3388.629824097804
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.969458]
objective value function right now is: -3364.7056720090027
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.280995]
objective value function right now is: -3305.655220904976
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.9382305]
objective value function right now is: -3214.3618003008028
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.773779]
objective value function right now is: -3328.6253083286324
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.857694]
objective value function right now is: -3389.694324179545
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.789007]
objective value function right now is: -3441.109136362111
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.75547]
objective value function right now is: -3296.6077993121025
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.463378]
objective value function right now is: -3106.5336163079955
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.202871]
objective value function right now is: -3232.6336940863107
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.369063]
objective value function right now is: -3381.5597157323946
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.064864]
objective value function right now is: -3341.2231630680326
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.778884]
objective value function right now is: -3319.7658832499824
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [15.279982]
objective value function right now is: -3291.0685932525976
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [15.0320835]
objective value function right now is: -3321.3847610822527
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.644986]
objective value function right now is: -3435.0777078789492
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.55599]
objective value function right now is: -3312.7274348318224
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.032049]
objective value function right now is: -3305.9040271482772
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.712616]
objective value function right now is: -3409.585936063815
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.872441]
objective value function right now is: -3074.8901792678134
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.188245]
objective value function right now is: -3262.2362196265785
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.748585]
objective value function right now is: -3411.705167258736
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.048601]
objective value function right now is: -3368.1509071011524
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.626258]
objective value function right now is: -3103.427941284051
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.873508]
objective value function right now is: -3475.7519281984205
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.621373]
objective value function right now is: -3443.4913631199515
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.157182]
objective value function right now is: -3330.8016882879206
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.768023]
objective value function right now is: -3502.9831840702723
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.115047]
objective value function right now is: -3360.6710073636614
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.82687]
objective value function right now is: -3412.7776734129516
new min fval:  -2245.4037419731208
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.531124]
objective value function right now is: -3440.137649146824
new min fval:  -3387.2275483235812
new min fval:  -3397.315841370406
new min fval:  -3418.6717454180484
new min fval:  -3441.8993477193976
new min fval:  -3461.6196638618244
new min fval:  -3477.1684231325053
new min fval:  -3488.8576167461565
new min fval:  -3497.886102509885
new min fval:  -3503.9687469256
new min fval:  -3508.6562824293514
new min fval:  -3512.0348719121116
new min fval:  -3514.358819940128
new min fval:  -3516.1594248016895
new min fval:  -3517.7825986890743
new min fval:  -3518.820819623829
new min fval:  -3519.4987938351323
new min fval:  -3519.8174728349195
new min fval:  -3520.513746209335
new min fval:  -3521.4760158060376
new min fval:  -3522.652270376899
new min fval:  -3524.042013338402
new min fval:  -3525.483728507054
new min fval:  -3526.6744172497215
new min fval:  -3527.474175252808
new min fval:  -3527.9100563887396
new min fval:  -3527.9169123950114
new min fval:  -3528.1614862329875
new min fval:  -3529.0489160303964
new min fval:  -3529.704945597674
new min fval:  -3530.167021911085
new min fval:  -3530.366864976034
new min fval:  -3530.7434106949363
new min fval:  -3531.334054216625
new min fval:  -3531.8759768343975
new min fval:  -3532.3429775293303
new min fval:  -3532.7252547767853
new min fval:  -3532.970849933003
new min fval:  -3533.155900415495
new min fval:  -3533.466116108528
new min fval:  -3534.033785330912
new min fval:  -3534.809818552977
new min fval:  -3535.6788761133867
new min fval:  -3536.5147217767176
new min fval:  -3537.2111865148936
new min fval:  -3537.718637126187
new min fval:  -3538.051440508996
new min fval:  -3538.2638939188987
new min fval:  -3538.40114634044
new min fval:  -3538.5190541676106
new min fval:  -3538.6665106957344
new min fval:  -3538.8175032950267
new min fval:  -3538.9734202603104
new min fval:  -3539.060250973149
new min fval:  -3539.0731419158
new min fval:  -3539.086698888925
new min fval:  -3539.3308722621996
new min fval:  -3539.6797918881894
new min fval:  -3540.031260281117
new min fval:  -3540.2895833902844
new min fval:  -3540.437985138439
new min fval:  -3540.521392421919
new min fval:  -3541.053343001123
new min fval:  -3541.685606378804
new min fval:  -3542.320129775721
new min fval:  -3542.953480897038
new min fval:  -3543.5175827091466
new min fval:  -3543.974426050987
new min fval:  -3544.3393947046543
new min fval:  -3544.5872719555177
new min fval:  -3544.717100583212
new min fval:  -3544.7788991064385
new min fval:  -3544.799776160555
new min fval:  -3545.061209342864
new min fval:  -3545.460005740936
new min fval:  -3545.7102469599145
new min fval:  -3545.817894753977
new min fval:  -3545.871057994764
new min fval:  -3546.1974420829024
new min fval:  -3546.4081660489796
new min fval:  -3546.492581294421
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.517292]
objective value function right now is: -3475.415109700899
new min fval:  -3546.558856542503
new min fval:  -3546.5818808739814
new min fval:  -3546.612557204945
new min fval:  -3546.646537258666
new min fval:  -3546.691531567909
new min fval:  -3546.73109050534
new min fval:  -3546.7400154444294
new min fval:  -3546.7878856010548
new min fval:  -3546.870062955271
new min fval:  -3546.9428811120547
new min fval:  -3547.038853267609
new min fval:  -3547.156730830531
new min fval:  -3547.2801044173043
new min fval:  -3547.4039400606434
new min fval:  -3547.50929017669
new min fval:  -3547.5906312136212
new min fval:  -3547.6380863498543
new min fval:  -3547.652114412771
new min fval:  -3547.7800006495518
new min fval:  -3547.9145178422928
new min fval:  -3548.0105534599506
new min fval:  -3548.046118840419
new min fval:  -3548.132389518726
new min fval:  -3548.2395730873973
new min fval:  -3548.325229706219
new min fval:  -3548.4064602937833
new min fval:  -3548.4819798981102
new min fval:  -3548.554584357135
new min fval:  -3548.632754410402
new min fval:  -3548.7058045871777
new min fval:  -3548.7905053745
new min fval:  -3548.8613902371803
new min fval:  -3548.9461883572176
new min fval:  -3549.046086664091
new min fval:  -3549.184680006369
new min fval:  -3549.3525408294677
new min fval:  -3549.51546549003
new min fval:  -3549.667399441618
new min fval:  -3549.7901387804895
new min fval:  -3549.887759268117
new min fval:  -3549.9633194383346
new min fval:  -3550.022051697977
new min fval:  -3550.053537516265
new min fval:  -3550.0613318537157
new min fval:  -3550.0854475598903
new min fval:  -3550.1247268596235
new min fval:  -3550.1536518673297
new min fval:  -3550.1649451888616
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.81904]
objective value function right now is: -3441.2934059418067
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.839987]
objective value function right now is: -3499.6238135304116
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.366935]
objective value function right now is: -2729.2078222761165
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.003432]
objective value function right now is: -3390.502357283581
min fval:  -3550.1649451888616
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 968.7775903527248
W_T_median: 811.9852247132906
W_T_pctile_5: 224.08207314998094
W_T_CVAR_5_pct: 44.432434814420084
Average q (qsum/M+1):  42.96850979712702
Optimal xi:  [14.829784]
Expected(across Rb) median(across samples) p_equity:  0.25142415712277094
obj fun:  tensor(-3550.1649, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.1232395]
objective value function right now is: -222693.09513535906
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.041613]
objective value function right now is: -208642.65571478527
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.229754]
objective value function right now is: -210555.48703483533
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.393082]
objective value function right now is: -188573.4498871273
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.550639]
objective value function right now is: -215707.98487386596
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.989861]
objective value function right now is: -196998.94731220658
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [14.766416]
objective value function right now is: -203365.9040136151
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.043571]
objective value function right now is: -210247.13795306504
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.000094]
objective value function right now is: -205532.06304965875
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.01477]
objective value function right now is: -216575.13696944457
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.375089]
objective value function right now is: -227409.27228902053
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.245705]
objective value function right now is: -222759.7291570751
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.20484]
objective value function right now is: -208963.9028577795
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [15.148801]
objective value function right now is: -204290.73453606904
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.179835]
objective value function right now is: -186028.29434900574
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.524254]
objective value function right now is: -189259.27163551282
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.597034]
objective value function right now is: -217188.23573238822
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.338668]
objective value function right now is: -219521.37020375897
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.748182]
objective value function right now is: -209927.3867503984
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.766289]
objective value function right now is: -220371.92513436725
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.04603]
objective value function right now is: -227919.2949859369
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.328993]
objective value function right now is: -227386.8295398037
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.696315]
objective value function right now is: -201421.46805820137
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.342753]
objective value function right now is: -185368.2801306733
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.96592]
objective value function right now is: -222632.14267111104
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.373643]
objective value function right now is: -222145.64729089633
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.309691]
objective value function right now is: -198850.66560153165
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [14.606905]
objective value function right now is: -224567.17909704644
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [15.3165245]
objective value function right now is: -221233.0569068065
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.840341]
objective value function right now is: -220611.17728396578
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.453183]
objective value function right now is: -218762.14183114696
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.113178]
objective value function right now is: -228007.50864113178
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.234774]
objective value function right now is: -217557.70433172944
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.874647]
objective value function right now is: -190362.5021592192
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.654521]
objective value function right now is: -210069.50928534896
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.728673]
objective value function right now is: -224933.1891775862
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.401109]
objective value function right now is: -222282.05350682823
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.212971]
objective value function right now is: -228486.29977434553
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.976177]
objective value function right now is: -221414.270441872
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.420233]
objective value function right now is: -216155.1335500572
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.900082]
objective value function right now is: -225235.45194781042
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.447759]
objective value function right now is: -216296.91557041122
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.803342]
objective value function right now is: -227317.86044842654
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.3206625]
objective value function right now is: -179254.42284762143
new min fval:  -207651.22355299085
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.055404]
objective value function right now is: -219613.5438629075
new min fval:  -219803.45790018974
new min fval:  -222306.79813086535
new min fval:  -223523.82371168156
new min fval:  -224110.3702936293
new min fval:  -224295.4612021867
new min fval:  -224402.64393327278
new min fval:  -224999.526128571
new min fval:  -225747.49647454466
new min fval:  -226679.18126757976
new min fval:  -227687.65848512226
new min fval:  -228320.55584395057
new min fval:  -228816.4996303967
new min fval:  -229112.26537290463
new min fval:  -229429.19967796985
new min fval:  -229724.48947181553
new min fval:  -229869.93800258415
new min fval:  -229889.33083696454
new min fval:  -229964.89320353695
new min fval:  -230204.08874375233
new min fval:  -230617.6974966533
new min fval:  -231094.42626528718
new min fval:  -231486.35069215126
new min fval:  -231731.40189345606
new min fval:  -231800.67618118747
new min fval:  -231822.46106268137
new min fval:  -231914.34275519263
new min fval:  -232101.51269372256
new min fval:  -232324.21571772813
new min fval:  -232560.65255976378
new min fval:  -232818.97919345004
new min fval:  -233030.21619279182
new min fval:  -233209.9746979733
new min fval:  -233405.1889873833
new min fval:  -233540.72769847183
new min fval:  -233608.3522645321
new min fval:  -233646.1931260061
new min fval:  -233670.4558022508
new min fval:  -233698.8822154379
new min fval:  -233738.35058429086
new min fval:  -233801.09113713962
new min fval:  -233873.88488982338
new min fval:  -233950.42945422782
new min fval:  -234005.5883622989
new min fval:  -234030.35832755067
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.779534]
objective value function right now is: -196822.78724603207
new min fval:  -234032.05098049698
new min fval:  -234038.90857944335
new min fval:  -234043.5375530693
new min fval:  -234046.4173389829
new min fval:  -234047.16099937714
new min fval:  -234056.57716355575
new min fval:  -234068.66708180335
new min fval:  -234081.62838106486
new min fval:  -234095.9222338583
new min fval:  -234111.39038059962
new min fval:  -234127.94301301497
new min fval:  -234146.04886245925
new min fval:  -234164.98557971575
new min fval:  -234184.47249353753
new min fval:  -234206.06428659742
new min fval:  -234227.82979195635
new min fval:  -234250.35346234354
new min fval:  -234273.85336750044
new min fval:  -234297.7347089232
new min fval:  -234322.0868653067
new min fval:  -234344.52462176522
new min fval:  -234366.23479511545
new min fval:  -234385.84276745605
new min fval:  -234404.31892656878
new min fval:  -234421.3119174403
new min fval:  -234438.31221941122
new min fval:  -234453.4182568974
new min fval:  -234468.11868568225
new min fval:  -234481.12137712387
new min fval:  -234492.3104258753
new min fval:  -234501.86859823955
new min fval:  -234510.33724034356
new min fval:  -234517.06640547092
new min fval:  -234520.56297493953
new min fval:  -234523.34740842975
new min fval:  -234525.87583294738
new min fval:  -234536.2744839381
new min fval:  -234547.23794578208
new min fval:  -234559.3013663954
new min fval:  -234570.788714041
new min fval:  -234581.6934388528
new min fval:  -234591.9340773685
new min fval:  -234601.6325975088
new min fval:  -234611.763298774
new min fval:  -234621.42239367784
new min fval:  -234631.8297252158
new min fval:  -234642.4486475644
new min fval:  -234653.3522818498
new min fval:  -234663.3510615146
new min fval:  -234673.06318538744
new min fval:  -234681.7743982564
new min fval:  -234690.35742172418
new min fval:  -234698.4970678467
new min fval:  -234706.52598435557
new min fval:  -234714.97931397424
new min fval:  -234723.191786645
new min fval:  -234730.4697891392
new min fval:  -234739.06155149703
new min fval:  -234746.70362576144
new min fval:  -234755.57932497078
new min fval:  -234763.7443255267
new min fval:  -234772.37833537572
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.719539]
objective value function right now is: -225253.62147459274
new min fval:  -234779.6742214603
new min fval:  -234786.39927340182
new min fval:  -234791.540441391
new min fval:  -234795.53264639757
new min fval:  -234797.84997184458
new min fval:  -234798.32023222794
new min fval:  -234799.49915712196
new min fval:  -234805.5859905388
new min fval:  -234810.00182532892
new min fval:  -234813.6077841962
new min fval:  -234814.98596642716
new min fval:  -234816.05390522326
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.711833]
objective value function right now is: -205561.5995409417
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.873335]
objective value function right now is: -216393.63408897954
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.911281]
objective value function right now is: -228044.19820926912
min fval:  -234816.05390522326
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1097.5359676267358
W_T_median: 922.0471703749164
W_T_pctile_5: 228.927672394811
W_T_CVAR_5_pct: 46.75174381610377
Average q (qsum/M+1):  40.22188445060484
Optimal xi:  [15.0251]
Expected(across Rb) median(across samples) p_equity:  0.24371910666426022
obj fun:  tensor(-234816.0539, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5000.0
-----------------------------------------------
