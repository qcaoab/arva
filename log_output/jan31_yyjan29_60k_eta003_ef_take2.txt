Starting at: 
31-01-23_08:11

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 60000
batchsize: 1000
remove neg:  False


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 6000, 'itbound_SGD_algorithms': 60000, 'nit_IterateAveragingStart': 54000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.03, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1706.035033252374
Current xi:  [-32.012154]
objective value function right now is: -1706.035033252374
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1714.151647809717
Current xi:  [-62.32414]
objective value function right now is: -1714.151647809717
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1719.0561104831156
Current xi:  [-92.728584]
objective value function right now is: -1719.0561104831156
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1719.110305378074
Current xi:  [-124.028564]
objective value function right now is: -1719.110305378074
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1725.0023030299103
Current xi:  [-155.50569]
objective value function right now is: -1725.0023030299103
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1729.236169617909
Current xi:  [-187.10338]
objective value function right now is: -1729.236169617909
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1730.6371984058824
Current xi:  [-218.56612]
objective value function right now is: -1730.6371984058824
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1733.814842397786
Current xi:  [-249.45358]
objective value function right now is: -1733.814842397786
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1735.1059863256523
Current xi:  [-279.9154]
objective value function right now is: -1735.1059863256523
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1737.4351752771076
Current xi:  [-309.43094]
objective value function right now is: -1737.4351752771076
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1738.661734273809
Current xi:  [-338.52933]
objective value function right now is: -1738.661734273809
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-366.73343]
objective value function right now is: -1738.561670491624
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1740.019188204553
Current xi:  [-393.19232]
objective value function right now is: -1740.019188204553
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1740.269584520206
Current xi:  [-418.15997]
objective value function right now is: -1740.269584520206
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1740.79061064003
Current xi:  [-439.49203]
objective value function right now is: -1740.79061064003
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.1660261731163
Current xi:  [-457.46243]
objective value function right now is: -1741.1660261731163
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.189873031124
Current xi:  [-472.10873]
objective value function right now is: -1741.189873031124
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-481.5012]
objective value function right now is: -1741.004740218384
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-486.51813]
objective value function right now is: -1740.8700850026235
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-490.24362]
objective value function right now is: -1741.0053642883447
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.3677221095243
Current xi:  [-491.93298]
objective value function right now is: -1741.3677221095243
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-490.97662]
objective value function right now is: -1740.8934320332478
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-490.8732]
objective value function right now is: -1740.7169599613
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.248]
objective value function right now is: -1740.2362418287803
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.68442]
objective value function right now is: -1740.9607534158108
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.59735]
objective value function right now is: -1740.9450540789621
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.68753]
objective value function right now is: -1740.8387889732676
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-492.0737]
objective value function right now is: -1740.7410133374592
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-492.752]
objective value function right now is: -1741.3283047331204
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-490.23615]
objective value function right now is: -1740.610506243927
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-493.44318]
objective value function right now is: -1741.1491217750474
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.06448]
objective value function right now is: -1741.328946501725
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.4153822900062
Current xi:  [-492.38153]
objective value function right now is: -1741.4153822900062
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.06778]
objective value function right now is: -1741.2655767190645
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-495.0598]
objective value function right now is: -1740.2732887027285
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.4776248522369
Current xi:  [-494.53928]
objective value function right now is: -1741.4776248522369
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-493.82407]
objective value function right now is: -1741.4512925857941
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.8658]
objective value function right now is: -1741.4766549468716
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.43445]
objective value function right now is: -1741.4158730521126
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.5117379360931
Current xi:  [-491.81033]
objective value function right now is: -1741.5117379360931
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.34402]
objective value function right now is: -1741.3087617026679
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.5868862950351
Current xi:  [-492.3424]
objective value function right now is: -1741.5868862950351
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.80466]
objective value function right now is: -1741.2286995656368
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.34042]
objective value function right now is: -1741.2846814105312
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.0944]
objective value function right now is: -1741.4420551172038
new min fval from sgd:  -1741.592292685974
new min fval from sgd:  -1741.6018257976486
new min fval from sgd:  -1741.6048402832337
new min fval from sgd:  -1741.6106016667145
new min fval from sgd:  -1741.6178294908448
new min fval from sgd:  -1741.6184869992167
new min fval from sgd:  -1741.6186554576368
new min fval from sgd:  -1741.6219134481253
new min fval from sgd:  -1741.633637038699
new min fval from sgd:  -1741.6382000117112
new min fval from sgd:  -1741.6409724991881
new min fval from sgd:  -1741.6420941516517
new min fval from sgd:  -1741.645222538088
new min fval from sgd:  -1741.6487182935919
new min fval from sgd:  -1741.651535397388
new min fval from sgd:  -1741.6523906547188
new min fval from sgd:  -1741.6527910707935
new min fval from sgd:  -1741.6540008441707
new min fval from sgd:  -1741.654540995602
new min fval from sgd:  -1741.655060784195
new min fval from sgd:  -1741.6561004088608
new min fval from sgd:  -1741.6592430036537
new min fval from sgd:  -1741.6645518316225
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.24057]
objective value function right now is: -1741.5678526619658
new min fval from sgd:  -1741.665132940749
new min fval from sgd:  -1741.6655436438436
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.2408]
objective value function right now is: -1741.4642567587305
new min fval from sgd:  -1741.6665531041228
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.10233]
objective value function right now is: -1741.5658469840173
new min fval from sgd:  -1741.6697549163587
new min fval from sgd:  -1741.6729817848554
new min fval from sgd:  -1741.6758204398488
new min fval from sgd:  -1741.6778391757389
new min fval from sgd:  -1741.6782656534729
new min fval from sgd:  -1741.6793027907654
new min fval from sgd:  -1741.6799052994195
new min fval from sgd:  -1741.680468413399
new min fval from sgd:  -1741.6811770336826
new min fval from sgd:  -1741.6814628827613
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-490.77838]
objective value function right now is: -1741.5871237925007
new min fval from sgd:  -1741.681642642462
new min fval from sgd:  -1741.6826509879254
new min fval from sgd:  -1741.6828800286166
new min fval from sgd:  -1741.6835123089852
new min fval from sgd:  -1741.6838406445338
new min fval from sgd:  -1741.684776288815
new min fval from sgd:  -1741.684833032305
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-490.79474]
objective value function right now is: -1741.6619312096705
min fval:  -1741.684833032305
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[11.3816,  2.0076],
        [11.4970,  1.2504],
        [-4.2693,  6.8515],
        [-0.3965,  1.1637],
        [-0.3965,  1.1637],
        [-0.3965,  1.1637],
        [-3.4148,  4.4355],
        [-0.3965,  1.1637],
        [-0.3965,  1.1637],
        [-0.3965,  1.1637]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-6.1892, -7.4966, 12.2938, -0.7302, -0.7302, -0.7302,  7.0437, -0.7302,
        -0.7302, -0.7302], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-6.3690e-02, -7.3496e-02, -7.7384e-02, -8.4506e-03, -8.4506e-03,
         -8.4506e-03, -1.6982e-02, -8.4506e-03, -8.4506e-03, -8.4506e-03],
        [-6.3690e-02, -7.3496e-02, -7.7384e-02, -8.4506e-03, -8.4506e-03,
         -8.4506e-03, -1.6982e-02, -8.4506e-03, -8.4506e-03, -8.4506e-03],
        [-6.3690e-02, -7.3496e-02, -7.7384e-02, -8.4506e-03, -8.4506e-03,
         -8.4506e-03, -1.6982e-02, -8.4506e-03, -8.4506e-03, -8.4506e-03],
        [ 1.8827e-01,  2.5424e-01,  1.1065e+00,  1.0356e-01,  1.0356e-01,
          1.0356e-01,  7.0342e-01,  1.0356e-01,  1.0356e-01,  1.0356e-01],
        [ 2.1136e+00,  3.6338e+00,  5.4938e+00,  5.5015e-02,  5.5015e-02,
          5.5015e-02,  1.4731e+00,  5.5014e-02,  5.5015e-02,  5.5015e-02],
        [-6.3690e-02, -7.3496e-02, -7.7384e-02, -8.4506e-03, -8.4506e-03,
         -8.4506e-03, -1.6982e-02, -8.4506e-03, -8.4506e-03, -8.4506e-03],
        [-2.4804e+00, -4.0634e+00, -6.0549e+00, -2.7245e-02, -2.7245e-02,
         -2.7245e-02, -1.4276e+00, -2.7245e-02, -2.7245e-02, -2.7245e-02],
        [-6.3690e-02, -7.3496e-02, -7.7384e-02, -8.4506e-03, -8.4506e-03,
         -8.4506e-03, -1.6982e-02, -8.4506e-03, -8.4506e-03, -8.4506e-03],
        [-3.7226e+00, -5.8678e+00, -8.7814e+00,  4.8369e-02,  4.8370e-02,
          4.8369e-02, -2.2431e+00,  4.8369e-02,  4.8369e-02,  4.8369e-02],
        [ 2.4175e+00,  4.1376e+00,  6.2021e+00,  3.2760e-02,  3.2760e-02,
          3.2760e-02,  1.6340e+00,  3.2760e-02,  3.2760e-02,  3.2760e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.4369, -0.4369, -0.4369, -1.3595, -3.0059, -0.4369,  3.3010, -0.4369,
         4.9754, -3.4291], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-9.8542e-05, -9.8536e-05, -9.8539e-05,  1.0482e+00,  6.7023e+00,
         -9.8515e-05, -6.5223e+00, -9.8536e-05, -1.0841e+01,  7.8122e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.8980,  -2.0595],
        [  6.7349,   0.8243],
        [-10.4608,  -8.5487],
        [-13.9360,  -1.4518],
        [ 16.6340,   2.9295],
        [  1.0632,   7.4826],
        [ -7.5957,   1.9773],
        [ 11.0292,   5.3369],
        [-10.5582,   5.0280],
        [ -4.4360,  -8.1489]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-5.2354, -4.4304, -4.9840,  4.4138, -2.6598,  5.1212,  4.2788,  3.5807,
         4.9442, -9.8900], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.7130e-01, -3.1299e+00,  1.1123e+01, -6.5456e+00,  2.3020e+00,
         -4.3364e+00,  6.0367e-01, -2.7629e+00, -1.2094e+00, -1.0874e+01],
        [ 5.7791e+00, -1.3605e+00, -7.9073e-01,  1.8630e-03, -1.6248e+00,
         -6.2704e-01, -2.0128e+00, -2.3624e+00, -2.3055e-01,  1.3862e+00],
        [-6.5366e+00,  1.5333e+00,  8.5119e-01,  2.1917e-01,  1.5355e+00,
          6.9808e-01,  2.2712e+00,  2.3883e+00,  2.5199e-01, -1.2605e+00],
        [-4.9772e+00,  1.0481e-01,  5.6513e+00,  2.0111e+00,  1.9166e+00,
         -3.1385e+00, -9.3285e+00,  1.7382e+00,  2.4227e+00, -8.9477e+00],
        [ 2.5487e-01,  1.3471e+00, -3.8704e+00, -6.0596e+00,  7.3145e+00,
          4.6617e+00,  4.0486e+00,  9.3009e+00,  7.8418e+00, -4.2913e+00],
        [-1.0480e+00, -7.6031e-01, -1.7086e+00,  5.8249e+00, -6.9704e+00,
          1.1258e+00, -1.5183e+00, -5.8397e+00,  2.2981e+00,  8.9711e+00],
        [ 3.5636e+00, -4.3948e+00,  1.0519e+01,  8.6176e-01,  3.1961e+00,
         -6.0888e+00, -3.9435e+00, -5.0830e+00, -9.7711e+00, -1.0037e+01],
        [-5.9865e-01, -5.7599e-01, -6.4821e-01, -7.1054e-01, -1.0410e+00,
         -4.6738e-01, -1.2936e-01, -1.0832e+00, -2.5015e-02, -6.1523e-01],
        [-6.6099e+00,  1.5322e+00,  8.7471e-01,  2.3211e-01,  1.5506e+00,
          6.9124e-01,  2.2788e+00,  2.4137e+00,  2.5032e-01, -1.2902e+00],
        [-5.9865e-01, -5.7599e-01, -6.4821e-01, -7.1054e-01, -1.0410e+00,
         -4.6738e-01, -1.2936e-01, -1.0832e+00, -2.5015e-02, -6.1523e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-0.9870, -1.3585,  1.2917,  0.5223,  1.1680, -1.3243,  1.2722, -1.7497,
         1.3145, -1.7497], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-5.7285, -1.3201,  2.1337, -0.6729, -3.7186,  4.7272, -3.5593, -0.0071,
          2.2340, -0.0071],
        [ 5.7343,  1.5017, -2.1795,  1.0062,  3.7196, -4.6782,  3.0778,  0.0070,
         -2.3132,  0.0070]], device='cuda:0'))])
xi:  [-490.78006]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 271.08012526393816
W_T_median: 110.35073628766843
W_T_pctile_5: -490.1251298916295
W_T_CVAR_5_pct: -598.9216277238565
Average q (qsum/M+1):  57.14938256048387
Optimal xi:  [-490.78006]
Expected(across Rb) median(across samples) p_equity:  0.2917440366814844
obj fun:  tensor(-1741.6848, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1653.6173377692578
Current xi:  [-28.6463]
objective value function right now is: -1653.6173377692578
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1658.4858890368937
Current xi:  [-52.005493]
objective value function right now is: -1658.4858890368937
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1662.973186021652
Current xi:  [-79.24227]
objective value function right now is: -1662.973186021652
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1663.8742124272992
Current xi:  [-107.19949]
objective value function right now is: -1663.8742124272992
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1667.957741219512
Current xi:  [-130.5723]
objective value function right now is: -1667.957741219512
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1669.0748389939106
Current xi:  [-157.46526]
objective value function right now is: -1669.0748389939106
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1670.714037415882
Current xi:  [-177.1039]
objective value function right now is: -1670.714037415882
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1671.8908279040163
Current xi:  [-200.95859]
objective value function right now is: -1671.8908279040163
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.7094632301585
Current xi:  [-213.44307]
objective value function right now is: -1672.7094632301585
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1672.7782995208358
Current xi:  [-228.60191]
objective value function right now is: -1672.7782995208358
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-242.01846]
objective value function right now is: -1672.2916850324361
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.2144178955434
Current xi:  [-245.64653]
objective value function right now is: -1673.2144178955434
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.3378251319234
Current xi:  [-246.25514]
objective value function right now is: -1673.3378251319234
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-246.40704]
objective value function right now is: -1672.8600725079064
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.09784]
objective value function right now is: -1672.2222371809892
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.52539]
objective value function right now is: -1672.3007064796134
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.15323]
objective value function right now is: -1673.2951234941288
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.4072017699016
Current xi:  [-246.41042]
objective value function right now is: -1673.4072017699016
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.15327]
objective value function right now is: -1672.8958122117972
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.12138]
objective value function right now is: -1673.2534301380065
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.42328]
objective value function right now is: -1672.5882374930266
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.95123]
objective value function right now is: -1672.9453816175103
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.5545302511837
Current xi:  [-246.51093]
objective value function right now is: -1673.5545302511837
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.3726]
objective value function right now is: -1671.6525636866488
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.79362]
objective value function right now is: -1672.2101507544978
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.42131]
objective value function right now is: -1673.0550659333728
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.29536]
objective value function right now is: -1672.4328083612272
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-245.95398]
objective value function right now is: -1673.3589858158223
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-246.1434]
objective value function right now is: -1673.4722728056179
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.51733]
objective value function right now is: -1673.2940256075403
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.06296]
objective value function right now is: -1673.3993864249608
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.94043]
objective value function right now is: -1673.0433097967023
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.38199]
objective value function right now is: -1672.9983779126128
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.1309]
objective value function right now is: -1671.7842748551047
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-246.31055]
objective value function right now is: -1673.3740533185892
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.6122215254927
Current xi:  [-245.9506]
objective value function right now is: -1673.6122215254927
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.71454]
objective value function right now is: -1673.4222791731383
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.6749708317236
Current xi:  [-245.50699]
objective value function right now is: -1673.6749708317236
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.7477291709292
Current xi:  [-245.22223]
objective value function right now is: -1673.7477291709292
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.7518454905623
Current xi:  [-245.00572]
objective value function right now is: -1673.7518454905623
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-245.0082]
objective value function right now is: -1673.7241270309669
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.798865914846
Current xi:  [-244.97287]
objective value function right now is: -1673.798865914846
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.7428]
objective value function right now is: -1673.7473305675787
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.82413]
objective value function right now is: -1673.6225183271138
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.66646]
objective value function right now is: -1673.7027221711623
new min fval from sgd:  -1673.8135411871467
new min fval from sgd:  -1673.816270684985
new min fval from sgd:  -1673.8167860949864
new min fval from sgd:  -1673.8174563893215
new min fval from sgd:  -1673.8206288341617
new min fval from sgd:  -1673.8222969048195
new min fval from sgd:  -1673.825258853205
new min fval from sgd:  -1673.82741652522
new min fval from sgd:  -1673.8276864712545
new min fval from sgd:  -1673.8352558714953
new min fval from sgd:  -1673.837676658738
new min fval from sgd:  -1673.8406194504337
new min fval from sgd:  -1673.8421006362846
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.70433]
objective value function right now is: -1673.666062694745
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.92929]
objective value function right now is: -1673.7261869686406
new min fval from sgd:  -1673.8443232231573
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.68335]
objective value function right now is: -1673.6963937584171
new min fval from sgd:  -1673.8447674631063
new min fval from sgd:  -1673.8456981478814
new min fval from sgd:  -1673.8464444388505
new min fval from sgd:  -1673.8477510198513
new min fval from sgd:  -1673.8488773835888
new min fval from sgd:  -1673.8492285109467
new min fval from sgd:  -1673.8493465085971
new min fval from sgd:  -1673.8500903521035
new min fval from sgd:  -1673.8504341142645
new min fval from sgd:  -1673.8514157557272
new min fval from sgd:  -1673.8534357569208
new min fval from sgd:  -1673.8553289598826
new min fval from sgd:  -1673.8558499158007
new min fval from sgd:  -1673.8558696040288
new min fval from sgd:  -1673.8582021581203
new min fval from sgd:  -1673.8620128236257
new min fval from sgd:  -1673.8631253418644
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.68355]
objective value function right now is: -1673.855437302532
new min fval from sgd:  -1673.8631436657115
new min fval from sgd:  -1673.8633092820137
new min fval from sgd:  -1673.863478168509
new min fval from sgd:  -1673.8640620811505
new min fval from sgd:  -1673.8644984976993
new min fval from sgd:  -1673.864687379412
new min fval from sgd:  -1673.8650564365014
new min fval from sgd:  -1673.865155009313
new min fval from sgd:  -1673.865549429835
new min fval from sgd:  -1673.8658962478391
new min fval from sgd:  -1673.8661459071309
new min fval from sgd:  -1673.8663199081393
new min fval from sgd:  -1673.867409145149
new min fval from sgd:  -1673.8682684176717
new min fval from sgd:  -1673.8682827873497
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-244.6878]
objective value function right now is: -1673.859579618651
min fval:  -1673.8682827873497
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 7.6048,  0.7785],
        [13.0475,  1.3342],
        [-0.7813, 11.3120],
        [-0.4666,  1.2282],
        [-0.4666,  1.2282],
        [-0.4666,  1.2282],
        [ 2.3060,  7.4889],
        [-0.4666,  1.2282],
        [-0.4666,  1.2282],
        [-0.4666,  1.2282]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.5361, -10.5185,  12.2109,  -1.0877,  -1.0877,  -1.0877,   6.9382,
         -1.0877,  -1.0877,  -1.0877], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.0998e-01, -4.6174e-02, -7.5416e-02, -1.2666e-02, -1.2666e-02,
         -1.2666e-02, -1.3701e-01, -1.2666e-02, -1.2666e-02, -1.2666e-02],
        [-2.0998e-01, -4.6174e-02, -7.5416e-02, -1.2666e-02, -1.2666e-02,
         -1.2666e-02, -1.3701e-01, -1.2666e-02, -1.2666e-02, -1.2666e-02],
        [-2.0998e-01, -4.6174e-02, -7.5416e-02, -1.2666e-02, -1.2666e-02,
         -1.2666e-02, -1.3701e-01, -1.2666e-02, -1.2666e-02, -1.2666e-02],
        [ 1.8229e+00,  1.0692e+00,  1.8620e+00, -2.9733e-03, -2.9733e-03,
         -2.9733e-03,  1.1102e+00, -2.9733e-03, -2.9733e-03, -2.9732e-03],
        [-1.7839e+00, -2.3659e+00,  8.4937e+00,  6.9012e-02,  6.9012e-02,
          6.9012e-02,  2.5756e+00,  6.9012e-02,  6.9012e-02,  6.9012e-02],
        [-2.0998e-01, -4.6174e-02, -7.5416e-02, -1.2666e-02, -1.2666e-02,
         -1.2666e-02, -1.3701e-01, -1.2666e-02, -1.2666e-02, -1.2666e-02],
        [ 1.8980e+00,  2.0778e+00, -8.2390e+00, -3.7629e-02, -3.7629e-02,
         -3.7629e-02, -2.2200e+00, -3.7629e-02, -3.7629e-02, -3.7629e-02],
        [-2.0998e-01, -4.6174e-02, -7.5416e-02, -1.2666e-02, -1.2666e-02,
         -1.2666e-02, -1.3701e-01, -1.2666e-02, -1.2666e-02, -1.2666e-02],
        [-2.5740e+00, -6.5405e+00, -1.0640e+01,  1.5429e-01,  1.5429e-01,
          1.5429e-01, -3.3617e+00,  1.5429e-01,  1.5429e-01,  1.5429e-01],
        [-2.3437e+00,  1.0994e+01,  8.0835e+00,  7.6927e-02,  7.6927e-02,
          7.6927e-02, -1.8681e+00,  7.6927e-02,  7.6927e-02,  7.6927e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.4798, -0.4798, -0.4798, -0.3156, -3.7169, -0.4798,  3.4551, -0.4798,
         4.7631, -2.8431], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-3.0993e-03, -3.0993e-03, -3.0993e-03,  2.3897e+00,  7.5137e+00,
         -3.0993e-03, -7.0850e+00, -3.0993e-03, -1.1771e+01,  1.1452e+01]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.6001,  -1.0559],
        [ 10.7215,  -0.6624],
        [-14.0006,  -9.4317],
        [-11.5220,  -3.0767],
        [ 13.0359,   0.6911],
        [  3.5974,   9.4634],
        [ -9.3081,   4.3105],
        [ 14.7576,   2.4187],
        [ -2.8318,  11.9886],
        [ -4.7345,  -7.3762]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -5.1726, -10.3676,  -7.2522,   1.1554,  -4.1888,   9.7968,   8.3194,
         -1.0052,  10.8491,  -9.1098], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.3044e-02, -4.3638e+00, -6.4408e-01, -1.1090e+00, -1.7806e+00,
          2.5120e+00,  2.0179e+00, -3.7792e+00,  4.3641e+00, -3.0796e-01],
        [ 3.9251e+00,  2.5582e+00,  5.2515e-01,  7.1379e-01, -5.2247e+00,
          4.2651e+00,  8.9295e+00, -5.8767e+00, -2.1471e+01, -2.3582e-02],
        [-1.4661e+01,  2.8755e+00,  9.6789e+00, -2.4001e+00,  1.2050e+00,
         -7.0932e-01, -1.5921e+00,  1.4744e+00,  8.4293e+00,  3.4662e-01],
        [-2.4167e+00, -1.2180e-01, -2.7655e+00,  2.1502e+00,  1.9358e+00,
         -6.9678e+00, -6.6262e+00,  4.3072e+00, -1.7104e+00, -3.6475e+00],
        [ 3.3172e-01,  2.2815e+00, -8.5017e+00, -4.1085e+00,  5.5628e+00,
          7.9689e+00, -1.1692e+00,  5.1778e+00,  6.3655e+00, -1.8522e+00],
        [-1.8058e-01, -5.3835e+00,  1.1071e+01,  4.9048e+00, -7.2561e+00,
         -1.0179e+01,  1.0899e+00, -7.1417e+00, -8.8392e+00,  3.1348e+00],
        [ 2.6327e+00, -1.3999e+01,  1.0802e+01,  6.5356e+00,  2.5608e+00,
         -9.9588e+00, -6.4094e+00, -8.4099e+00, -2.3249e+00, -1.0951e+00],
        [-4.4193e-01, -6.8482e-01, -4.6495e-01, -5.4855e-01, -1.0768e+00,
         -7.7689e-01, -1.3967e-01, -1.1404e+00, -2.2681e-01, -4.9242e-01],
        [-6.1985e+00,  1.8659e+00,  1.0499e+00,  8.2178e-01,  1.6102e+00,
         -1.0198e+00,  3.9055e+00,  2.6922e+00,  6.2463e-01, -4.1658e+00],
        [-4.4193e-01, -6.8482e-01, -4.6495e-01, -5.4855e-01, -1.0768e+00,
         -7.7689e-01, -1.3967e-01, -1.1404e+00, -2.2681e-01, -4.9242e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.4281, -2.1635, -0.2002,  2.1649,  3.5678, -3.7561,  1.2963, -1.6876,
         1.1867, -1.6876], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.1532e+00, -3.4669e+00,  2.1116e+00, -2.5247e-02, -4.4751e+00,
          7.1832e+00, -3.8290e+00,  1.3154e-03,  2.4411e+00,  1.3156e-03],
        [-1.1531e+00,  3.6406e+00, -2.1569e+00,  3.5218e-01,  4.4759e+00,
         -7.1367e+00,  3.3668e+00, -1.5854e-03, -2.5193e+00, -1.5851e-03]],
       device='cuda:0'))])
xi:  [-244.70447]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 266.2275695484865
W_T_median: 85.05129074238899
W_T_pctile_5: -245.15599537845358
W_T_CVAR_5_pct: -336.9811185059301
Average q (qsum/M+1):  56.16983524445565
Optimal xi:  [-244.70447]
Expected(across Rb) median(across samples) p_equity:  0.3040481177469095
obj fun:  tensor(-1673.8683, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1599.5771650994984
Current xi:  [-5.619925]
objective value function right now is: -1599.5771650994984
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.230168251407
Current xi:  [0.00456449]
objective value function right now is: -1602.230168251407
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.84338763836
Current xi:  [-0.00250356]
objective value function right now is: -1604.84338763836
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00659346]
objective value function right now is: -1603.3933436703678
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.30910406]
objective value function right now is: -1601.275513102247
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.08409648]
objective value function right now is: -1601.5205204416854
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06866334]
objective value function right now is: -1600.2043784198383
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.09143313]
objective value function right now is: -1601.6343359172893
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.09981208]
objective value function right now is: -1601.2496571883426
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06028497]
objective value function right now is: -1602.1311241376904
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01176665]
objective value function right now is: -1601.8722890092959
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00060511]
objective value function right now is: -1599.82414333791
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03167995]
objective value function right now is: -1604.4053047085652
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1605.05285878149
Current xi:  [0.02760338]
objective value function right now is: -1605.05285878149
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06377612]
objective value function right now is: -1604.6840239769092
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.3172925]
objective value function right now is: -1598.0248169509914
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00952319]
objective value function right now is: -1604.7284071913264
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.2134531055456
Current xi:  [-0.16189532]
objective value function right now is: -1605.2134531055456
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06445955]
objective value function right now is: -1601.5905796544403
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0628702]
objective value function right now is: -1601.2625797019834
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02653823]
objective value function right now is: -1603.2808334197966
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.2182175910166
Current xi:  [0.0296303]
objective value function right now is: -1605.2182175910166
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.07112771]
objective value function right now is: -1603.7038414000572
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0199143]
objective value function right now is: -1603.7457360694175
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00670629]
objective value function right now is: -1604.5854436761895
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.03118043]
objective value function right now is: -1605.0745058573716
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02719631]
objective value function right now is: -1604.5325453192327
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0114294]
objective value function right now is: -1605.0750630522277
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.11437167]
objective value function right now is: -1603.7396698042342
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03182398]
objective value function right now is: -1602.5661868118443
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.6301165]
objective value function right now is: -1580.501241696804
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.0572124]
objective value function right now is: -1581.2036059033273
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.6862187]
objective value function right now is: -1580.9156735301547
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.4212947]
objective value function right now is: -1581.362033716417
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-24.074919]
objective value function right now is: -1582.4959917633971
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-28.13998]
objective value function right now is: -1584.9997491792083
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-33.15949]
objective value function right now is: -1585.810047282853
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.033512]
objective value function right now is: -1585.884202261579
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.871994]
objective value function right now is: -1586.0477124621934
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.875134]
objective value function right now is: -1585.576676831463
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.879925]
objective value function right now is: -1585.9781246422835
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.899048]
objective value function right now is: -1586.2664722576799
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.9265]
objective value function right now is: -1586.4302122743118
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.0338]
objective value function right now is: -1585.5145925539646
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.91994]
objective value function right now is: -1586.349919153121
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.976643]
objective value function right now is: -1584.0432231572913
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.92812]
objective value function right now is: -1586.219515475759
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.891838]
objective value function right now is: -1586.4099684207263
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.88611]
objective value function right now is: -1586.5326836445943
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.87664]
objective value function right now is: -1586.590988704103
min fval:  -1598.8363257190697
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  8.3490,  -2.7661],
        [  3.9680,  -1.1692],
        [ -8.8801,   8.0243],
        [ -1.0541, -11.3486],
        [ -1.1261, -11.1708],
        [ -1.0911, -11.2525],
        [  4.1076,  10.1327],
        [ -1.0538, -11.3494],
        [ -1.0667, -11.3147],
        [ -1.0801, -11.2800]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-8.1930, -7.3728,  9.6898, -1.7078, -1.7769, -1.7442,  8.0592, -1.7075,
        -1.7204, -1.7335], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  2.9804,   0.1165,   0.8840,   0.8131,   0.7061,   0.7619,   0.4382,
           0.8135,   0.7962,   0.7777],
        [  2.9804,   0.1165,   0.8840,   0.8131,   0.7061,   0.7619,   0.4382,
           0.8135,   0.7962,   0.7777],
        [  2.9804,   0.1165,   0.8840,   0.8131,   0.7061,   0.7619,   0.4382,
           0.8135,   0.7962,   0.7777],
        [  0.4471,   0.2388,  -0.1677,  -1.1878,  -1.3160,  -1.2513,  -0.1987,
          -1.1873,  -1.2092,  -1.2321],
        [ -9.1547,  -1.5229,   6.9028,   1.0841,   1.2875,   1.1829,   2.7274,
           1.0833,   1.1169,   1.1526],
        [  2.9804,   0.1165,   0.8840,   0.8131,   0.7061,   0.7619,   0.4382,
           0.8135,   0.7962,   0.7777],
        [  8.9724,   1.5919,  -6.7379,  -1.0491,  -1.2484,  -1.1457,  -2.4488,
          -1.0483,  -1.0811,  -1.1161],
        [  2.9804,   0.1165,   0.8840,   0.8131,   0.7061,   0.7619,   0.4382,
           0.8135,   0.7962,   0.7777],
        [  4.9967,   0.8406,  -9.9982,   1.2401,   1.2124,   1.2252, -15.8512,
           1.2402,   1.2349,   1.2295],
        [ -7.3915,  -0.3099,   7.4484,   4.1987,   4.1710,   4.1824,  -0.3592,
           4.1988,   4.1927,   4.1869]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 1.0794,  1.0794,  1.0794, -0.4437, -3.8089,  1.0794,  3.3961,  1.0794,
         6.7777, -1.6996], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -1.2820,  -1.2820,  -1.2820,  -0.3394,  10.3172,  -1.2820, -10.8875,
          -1.2820, -32.8567,  11.1870]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.5698,   2.3719],
        [ 13.3622,  -0.2102],
        [-15.0515,  -9.6348],
        [-11.0227,  -2.2926],
        [ 11.6979,   1.0173],
        [  3.3772,  12.1790],
        [ -9.3378,   5.7365],
        [ 16.2664,   3.4362],
        [ -5.2060,  13.8625],
        [ -7.2404,  -9.1823]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -5.8445, -12.0743,  -7.3381,   0.3106,  -5.2771,  10.3422,   6.4842,
          1.9679,  13.4400,  -7.8498], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.4985e+00, -5.5535e+00, -7.5267e+00,  5.4507e+00, -3.7843e+00,
          4.6000e+00,  3.7660e+00, -6.0936e+00,  8.8959e+00, -1.3766e+01],
        [-2.7682e-01, -8.7471e-01, -3.0690e+00,  5.4379e-01, -6.1939e+00,
         -1.7278e-01, -1.0880e+00, -2.3395e+00, -6.6403e-01, -2.0060e+00],
        [ 2.9051e-02,  7.5248e-01,  7.6615e-01,  2.2175e+00,  6.5229e-01,
          4.7317e+00,  1.4992e+00,  2.8145e+00,  2.3318e+00,  4.8512e-01],
        [ 5.4779e-02,  6.2931e+00,  7.2054e+00,  4.3322e+00, -2.7586e-01,
         -9.0355e+00, -5.7134e+00,  2.9157e+00, -1.3473e+01,  7.6585e+00],
        [ 1.8853e-03,  2.2665e+00, -5.6459e+00, -1.7448e+00,  5.2667e+00,
          3.1303e+00,  3.6162e+00,  4.6528e+00,  3.6884e+00, -3.3879e+00],
        [-8.8263e-02, -4.1873e+00,  1.2933e+01,  5.8841e+00, -9.0153e+00,
         -1.4985e+01, -1.3676e+00, -1.1123e+01, -7.3188e+00,  6.9942e+00],
        [ 2.0185e-02, -1.8650e+01,  9.7313e+00,  5.9631e+00,  1.6887e-02,
         -1.4279e+01, -1.3923e+00, -5.9092e+00, -1.3256e+01,  4.5597e-01],
        [-2.0656e-01, -3.4542e-01, -3.5432e-01, -3.7672e-01, -1.5637e+00,
         -1.3209e+00, -3.1422e-01, -2.8571e+00, -6.5181e-01, -5.9838e-01],
        [-1.5252e-02,  4.2097e+00, -2.2922e-01,  1.2519e+00,  2.2751e+00,
          7.1399e+00,  1.3917e+00,  2.8914e+00,  3.2824e+00, -3.3247e+00],
        [-2.0656e-01, -3.4542e-01, -3.5432e-01, -3.7672e-01, -1.5637e+00,
         -1.3209e+00, -3.1422e-01, -2.8571e+00, -6.5180e-01, -5.9838e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.9320, -2.7938,  0.6645,  1.8345,  3.9255, -6.0517,  3.7794, -2.9948,
         0.7864, -2.9948], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.5475,  -1.7562,   2.1017,   0.2558,  -4.4172,  11.7851,  -6.0562,
          -0.1040,   2.4097,  -0.1040],
        [ -0.5474,   1.8361,  -2.1469,   0.0697,   4.4181, -11.7669,   5.6404,
           0.1040,  -2.4877,   0.1040]], device='cuda:0'))])
xi:  [-34.91994]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 422.6959516673791
W_T_median: 165.86921616639495
W_T_pctile_5: 0.05795548396605739
W_T_CVAR_5_pct: -107.3767985913073
Average q (qsum/M+1):  53.51318753150202
Optimal xi:  [-34.91994]
Expected(across Rb) median(across samples) p_equity:  0.2937604477008184
obj fun:  tensor(-1598.8363, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1560.8252181561668
Current xi:  [-0.03036384]
objective value function right now is: -1560.8252181561668
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1561.7769660114488
Current xi:  [-0.01293447]
objective value function right now is: -1561.7769660114488
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00091145]
objective value function right now is: -1556.9271173330808
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00478606]
objective value function right now is: -1561.0140655748785
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00307172]
objective value function right now is: -1560.1726579610142
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.12476937]
objective value function right now is: -1559.3390803615441
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [0.00276379]
objective value function right now is: -1559.5794678099776
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0002229]
objective value function right now is: -1559.2975871614008
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05596583]
objective value function right now is: -1560.501669541226
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00981337]
objective value function right now is: -1557.2184469621
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02115485]
objective value function right now is: -1559.0125383739764
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01835312]
objective value function right now is: -1514.0697689048336
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02978097]
objective value function right now is: -1559.1009961710547
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00749632]
objective value function right now is: -1556.7163241392439
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01255854]
objective value function right now is: -1560.2269939437538
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01027717]
objective value function right now is: -1560.7218816296722
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01327144]
objective value function right now is: -1561.634948085443
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00566766]
objective value function right now is: -1556.9177668917057
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06044066]
objective value function right now is: -1561.2524383128866
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00421245]
objective value function right now is: -1560.6467182694707
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05690986]
objective value function right now is: -1550.8658687102165
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0243207]
objective value function right now is: -1545.8128705955432
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00146176]
objective value function right now is: -1558.493973697428
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00863768]
objective value function right now is: -1557.3994963245775
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03497463]
objective value function right now is: -1558.9216344154565
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02731665]
objective value function right now is: -1558.3046812983189
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00617361]
objective value function right now is: -1561.4941844181815
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00876702]
objective value function right now is: -1561.74848979428
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1561.9514952353657
Current xi:  [-0.01052373]
objective value function right now is: -1561.9514952353657
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01293914]
objective value function right now is: -1557.8871594629643
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.1718624157481
Current xi:  [0.00736945]
objective value function right now is: -1562.1718624157481
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00380917]
objective value function right now is: -1558.1468571615399
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01073299]
objective value function right now is: -1558.2924640115693
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00408818]
objective value function right now is: -1560.8750276036408
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00365145]
objective value function right now is: -1560.9236348114234
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.3592967280115
Current xi:  [0.00513362]
objective value function right now is: -1562.3592967280115
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00614009]
objective value function right now is: -1562.2181718824745
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.5746819949936
Current xi:  [0.00139478]
objective value function right now is: -1562.5746819949936
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.8153965415108
Current xi:  [0.0020743]
objective value function right now is: -1562.8153965415108
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00471233]
objective value function right now is: -1562.5320032692653
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.892585414561
Current xi:  [0.00594549]
objective value function right now is: -1562.892585414561
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00572692]
objective value function right now is: -1562.6816997108667
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00246521]
objective value function right now is: -1562.5667110415748
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0042242]
objective value function right now is: -1562.054611614142
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00449341]
objective value function right now is: -1562.7763838511958
new min fval from sgd:  -1562.966452562057
new min fval from sgd:  -1563.0522806001893
new min fval from sgd:  -1563.0591665130319
new min fval from sgd:  -1563.0857125430703
new min fval from sgd:  -1563.0960153943045
new min fval from sgd:  -1563.1052062865508
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00229191]
objective value function right now is: -1562.6976299074256
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00074677]
objective value function right now is: -1562.3814673969177
new min fval from sgd:  -1563.1226911372114
new min fval from sgd:  -1563.1507806909933
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0038785]
objective value function right now is: -1562.1373123155195
new min fval from sgd:  -1563.155974641532
new min fval from sgd:  -1563.1598048174953
new min fval from sgd:  -1563.1606011312767
new min fval from sgd:  -1563.1638689346933
new min fval from sgd:  -1563.1653775828847
new min fval from sgd:  -1563.1699713585656
new min fval from sgd:  -1563.1726145632106
new min fval from sgd:  -1563.1745303422265
new min fval from sgd:  -1563.1750261646607
new min fval from sgd:  -1563.1770822093993
new min fval from sgd:  -1563.1786015848854
new min fval from sgd:  -1563.1812541426198
new min fval from sgd:  -1563.183329758349
new min fval from sgd:  -1563.1858722122472
new min fval from sgd:  -1563.1885100963937
new min fval from sgd:  -1563.1890958923707
new min fval from sgd:  -1563.1916643431587
new min fval from sgd:  -1563.1934248791863
new min fval from sgd:  -1563.1947909069715
new min fval from sgd:  -1563.196530532348
new min fval from sgd:  -1563.1990897798203
new min fval from sgd:  -1563.2006990050631
new min fval from sgd:  -1563.2033520519497
new min fval from sgd:  -1563.2063909016065
new min fval from sgd:  -1563.2070626740772
new min fval from sgd:  -1563.2128724258778
new min fval from sgd:  -1563.2141577613797
new min fval from sgd:  -1563.2160773804528
new min fval from sgd:  -1563.2209074553566
new min fval from sgd:  -1563.2231918734944
new min fval from sgd:  -1563.2244246197577
new min fval from sgd:  -1563.2253271144102
new min fval from sgd:  -1563.2255694257333
new min fval from sgd:  -1563.2258177616195
new min fval from sgd:  -1563.2259537484501
new min fval from sgd:  -1563.2277243792958
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00460013]
objective value function right now is: -1563.1824791474803
new min fval from sgd:  -1563.2307370762132
new min fval from sgd:  -1563.2313484369229
new min fval from sgd:  -1563.2314046226231
new min fval from sgd:  -1563.2327608017758
new min fval from sgd:  -1563.2334963784886
new min fval from sgd:  -1563.2337017667933
new min fval from sgd:  -1563.235719794584
new min fval from sgd:  -1563.2379211222908
new min fval from sgd:  -1563.2386156861955
new min fval from sgd:  -1563.2395533147878
new min fval from sgd:  -1563.2405215179851
new min fval from sgd:  -1563.243705820273
new min fval from sgd:  -1563.248540517699
new min fval from sgd:  -1563.2525852696301
new min fval from sgd:  -1563.254898402535
new min fval from sgd:  -1563.2569638240652
new min fval from sgd:  -1563.258698747119
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00535453]
objective value function right now is: -1562.8164499957916
min fval:  -1563.258698747119
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 11.2569,  -4.0648],
        [  2.8612,  10.1087],
        [-12.0968,   5.2868],
        [  1.2793,  -1.1218],
        [  1.0326,  -0.9177],
        [  1.0790,  -0.9551],
        [  0.8334,  12.1428],
        [  1.2797,  -1.1221],
        [  1.2573,  -1.1031],
        [  1.2006,  -1.0554]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-10.0985,  -3.4862,  10.6502,   3.5897,   3.0479,   3.1430,   9.8532,
          3.5906,   3.5380,   3.4077], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  0.1764,   0.0941,   0.2699,   0.4924,   0.4866,   0.4880,   0.2395,
           0.4924,   0.4920,   0.4909],
        [  0.1764,   0.0941,   0.2699,   0.4924,   0.4866,   0.4880,   0.2395,
           0.4924,   0.4920,   0.4909],
        [  0.1764,   0.0941,   0.2699,   0.4924,   0.4866,   0.4880,   0.2395,
           0.4924,   0.4920,   0.4909],
        [ -0.1025,  -0.0399,  -0.1358,  -0.2995,  -0.2963,  -0.2970,  -0.2253,
          -0.2995,  -0.2993,  -0.2987],
        [  0.2572,   0.0849,   0.4770,   0.7510,   0.7481,   0.7488,   0.7497,
           0.7510,   0.7509,   0.7503],
        [  0.1764,   0.0941,   0.2699,   0.4924,   0.4866,   0.4880,   0.2395,
           0.4924,   0.4920,   0.4909],
        [ -0.1025,  -0.0399,  -0.1358,  -0.2995,  -0.2963,  -0.2970,  -0.2253,
          -0.2995,  -0.2993,  -0.2987],
        [  0.1764,   0.0941,   0.2699,   0.4924,   0.4866,   0.4880,   0.2395,
           0.4924,   0.4920,   0.4909],
        [ 10.7759,   7.6127,  -9.4624,   1.7375,   0.8683,   0.9741, -21.0023,
           1.7395,   1.6209,   1.3641],
        [  0.2556,   0.0840,   0.4724,   0.7455,   0.7426,   0.7433,   0.7440,
           0.7455,   0.7454,   0.7448]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 0.4948,  0.4948,  0.4948, -0.3023,  0.7760,  0.4948, -0.3023,  0.4948,
         7.4525,  0.7696], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.0688e-01, -4.0688e-01, -4.0688e-01, -9.0384e-03,  7.6289e+00,
         -4.0688e-01, -9.0383e-03, -4.0689e-01, -2.7103e+01,  7.2263e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -5.1870,   2.6725],
        [ 15.5633,  -0.9727],
        [-16.5702, -11.4801],
        [-11.3797,  -2.9315],
        [ 12.5067,   3.4270],
        [  6.3885,  14.5582],
        [ -8.3630,   6.2553],
        [ 14.2168,   4.9613],
        [ -7.8521,  16.8144],
        [ -9.1699,  -6.3332]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -6.6533, -14.5615,  -8.9923,  -0.2402,  -4.8851,  12.5942,   7.9497,
          2.9705,  16.2749, -10.4220], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-5.8190e+00, -9.0413e+00, -1.7335e+01,  3.0291e+00, -4.4813e+00,
          5.9148e+00,  5.0723e+00, -7.8800e+00,  9.8703e+00, -1.3589e-01],
        [-5.1878e-01, -3.3070e+00, -4.5768e+00,  4.6685e+00, -1.8753e+00,
          3.7455e+00,  1.0625e+00, -2.2498e+00, -3.0160e+00, -2.1194e-02],
        [ 1.0118e+00,  1.4790e+00, -5.3654e-01, -6.1758e-01,  5.8345e-01,
         -2.9207e-01,  1.1386e-01,  2.8276e+00, -2.5788e+00, -1.2989e-01],
        [ 6.7356e-03, -9.2938e-01,  1.4381e+00,  2.9651e+00, -1.7759e+00,
         -2.0229e+00,  1.1685e+00, -2.7807e+00, -2.6186e+00,  1.6222e-01],
        [ 2.4460e-02,  8.7481e-02,  4.6704e+00, -6.3302e+00,  1.0212e+00,
         -1.2375e+00, -7.5987e-01,  2.1173e+00,  9.8238e+00, -2.9253e-02],
        [ 2.1842e-02, -2.2202e+00,  1.4391e+01,  1.1444e+01, -2.2044e+00,
         -1.8171e+01, -4.5287e+00, -1.1117e+01, -2.8431e+00,  3.4171e+00],
        [ 1.9299e-03, -1.6527e+01,  1.3895e+01,  5.9027e+00, -2.3025e+00,
         -1.7955e+01,  1.0191e-01, -6.9157e+00, -1.3987e+01,  1.2171e+00],
        [ 6.7383e-03, -9.2938e-01,  1.4381e+00,  2.9651e+00, -1.7759e+00,
         -2.0229e+00,  1.1685e+00, -2.7807e+00, -2.6186e+00,  1.6222e-01],
        [-2.0548e-04,  4.7264e+00, -3.4065e+00, -4.9522e+00,  2.1662e+00,
          5.0647e+00, -2.3838e+00,  1.5055e+00,  1.9256e+01,  2.8615e+00],
        [ 6.7366e-03, -9.2938e-01,  1.4381e+00,  2.9651e+00, -1.7759e+00,
         -2.0229e+00,  1.1685e+00, -2.7807e+00, -2.6186e+00,  1.6222e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-7.5323, -4.5597,  3.2486, -2.4818,  2.1421, -6.3404,  3.5403, -2.4818,
        -0.7000, -2.4818], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.5337,  -2.3993,   2.2352,   1.2051,  -4.1650,  13.3784,  -6.4335,
           1.1970,   2.1518,   1.1970],
        [ -0.5337,   2.4066,  -2.2801,  -1.1887,   4.1660, -13.3777,   6.2037,
          -1.1968,  -2.2294,  -1.1968]], device='cuda:0'))])
xi:  [0.00490124]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 423.0091808397773
W_T_median: 175.9423148628045
W_T_pctile_5: 0.004900080961826703
W_T_CVAR_5_pct: -73.57671890549409
Average q (qsum/M+1):  52.801131709929436
Optimal xi:  [0.00490124]
Expected(across Rb) median(across samples) p_equity:  0.30007456441720326
obj fun:  tensor(-1563.2587, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1525.311808748398
Current xi:  [0.0081985]
objective value function right now is: -1525.311808748398
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00332497]
objective value function right now is: -1521.811968577597
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1525.4702298945977
Current xi:  [-0.01584831]
objective value function right now is: -1525.4702298945977
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1525.6197669908295
Current xi:  [-0.02398135]
objective value function right now is: -1525.6197669908295
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.63634807]
objective value function right now is: -1444.9386821051198
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04691306]
objective value function right now is: -1521.0358717390104
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [0.02379112]
objective value function right now is: -1525.597741787142
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00195212]
objective value function right now is: -1524.34819657846
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1527.6396640898884
Current xi:  [-0.11138273]
objective value function right now is: -1527.6396640898884
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03571476]
objective value function right now is: -1524.1351091843756
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01282414]
objective value function right now is: -1525.5030482222126
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1527.658344055613
Current xi:  [0.0099925]
objective value function right now is: -1527.658344055613
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.07276174]
objective value function right now is: -1521.4166589467723
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03510744]
objective value function right now is: -1523.2132410094828
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.001979]
objective value function right now is: -1525.9682474153703
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1527.8317253057473
Current xi:  [-0.00793744]
objective value function right now is: -1527.8317253057473
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00152323]
objective value function right now is: -1525.1101334996474
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00286313]
objective value function right now is: -1522.998738851429
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00634508]
objective value function right now is: -1526.531259859276
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00057155]
objective value function right now is: -1525.1535704314465
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00774153]
objective value function right now is: -1522.6814610222368
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00849365]
objective value function right now is: -1517.8361417612953
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01483981]
objective value function right now is: -1496.8023019947698
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.2693922]
objective value function right now is: -1489.3826295241392
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02119663]
objective value function right now is: -1527.4356080829325
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01932119]
objective value function right now is: -1523.3761695511157
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02595178]
objective value function right now is: -1524.5401553625288
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00130212]
objective value function right now is: -1526.7284081299733
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01807884]
objective value function right now is: -1524.2394582619183
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00290963]
objective value function right now is: -1527.0606281263022
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.08740594]
objective value function right now is: -1527.4729485749986
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00079239]
objective value function right now is: -1526.9713724913647
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00293471]
objective value function right now is: -1526.816703407257
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1527.9036624319517
Current xi:  [0.00085074]
objective value function right now is: -1527.9036624319517
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00619163]
objective value function right now is: -1523.2720654581346
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1528.708519893143
Current xi:  [0.00858546]
objective value function right now is: -1528.708519893143
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0045063]
objective value function right now is: -1528.2561562806277
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00650168]
objective value function right now is: -1527.8947404985115
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00533999]
objective value function right now is: -1528.2275521381093
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1528.7499987067995
Current xi:  [0.00904989]
objective value function right now is: -1528.7499987067995
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00639669]
objective value function right now is: -1527.986013050662
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00368198]
objective value function right now is: -1528.6805391140063
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0051772]
objective value function right now is: -1528.744330885876
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1529.012702548608
Current xi:  [0.00614482]
objective value function right now is: -1529.012702548608
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00314533]
objective value function right now is: -1528.642400775581
new min fval from sgd:  -1529.0407832081432
new min fval from sgd:  -1529.0500875772568
new min fval from sgd:  -1529.0556327417855
new min fval from sgd:  -1529.0808647731053
new min fval from sgd:  -1529.10986851456
new min fval from sgd:  -1529.1111066809015
new min fval from sgd:  -1529.1353990668144
new min fval from sgd:  -1529.1626071889248
new min fval from sgd:  -1529.1716955349948
new min fval from sgd:  -1529.1873111539048
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00323202]
objective value function right now is: -1525.1862657219385
new min fval from sgd:  -1529.1960686200955
new min fval from sgd:  -1529.2047234414263
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00693128]
objective value function right now is: -1528.0601848392519
new min fval from sgd:  -1529.240867128074
new min fval from sgd:  -1529.2505867406885
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00376705]
objective value function right now is: -1528.4697635541968
new min fval from sgd:  -1529.2522583595062
new min fval from sgd:  -1529.2532492567423
new min fval from sgd:  -1529.2535299617862
new min fval from sgd:  -1529.2546164171188
new min fval from sgd:  -1529.259549705037
new min fval from sgd:  -1529.2626179228068
new min fval from sgd:  -1529.2689166362004
new min fval from sgd:  -1529.2776143474332
new min fval from sgd:  -1529.2861236158128
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00492771]
objective value function right now is: -1529.235317590961
new min fval from sgd:  -1529.2877238027143
new min fval from sgd:  -1529.2955846178154
new min fval from sgd:  -1529.2984653578
new min fval from sgd:  -1529.2985686116583
new min fval from sgd:  -1529.3038376252284
new min fval from sgd:  -1529.3041709637837
new min fval from sgd:  -1529.3052972215519
new min fval from sgd:  -1529.3105963477697
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00600205]
objective value function right now is: -1529.248857313002
min fval:  -1529.3105963477697
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 13.1835,  -3.7762],
        [  3.5889,   8.0332],
        [-14.3234,   5.4320],
        [  1.2154,  -0.9142],
        [  0.9648,  -0.7505],
        [  1.0179,  -0.7837],
        [  0.5997,  12.8070],
        [  1.2157,  -0.9144],
        [  1.1947,  -0.9000],
        [  1.1409,  -0.8636]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-11.2225,  -5.8190,  11.3237,   4.0804,   3.5591,   3.6580,  10.1240,
          4.0812,   4.0324,   3.9114], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.3823e-01, -2.1922e-03, -9.4615e-02, -3.1068e-01, -3.0829e-01,
         -3.0890e-01, -2.5420e-01, -3.1068e-01, -3.1052e-01, -3.1009e-01],
        [-1.3823e-01, -2.1921e-03, -9.4615e-02, -3.1068e-01, -3.0829e-01,
         -3.0890e-01, -2.5420e-01, -3.1068e-01, -3.1052e-01, -3.1009e-01],
        [-1.3823e-01, -2.1921e-03, -9.4615e-02, -3.1068e-01, -3.0829e-01,
         -3.0890e-01, -2.5420e-01, -3.1068e-01, -3.1052e-01, -3.1009e-01],
        [-1.3823e-01, -2.1921e-03, -9.4615e-02, -3.1068e-01, -3.0829e-01,
         -3.0890e-01, -2.5420e-01, -3.1068e-01, -3.1052e-01, -3.1009e-01],
        [ 3.4633e-01,  1.3606e-01,  4.0625e-01,  7.9216e-01,  7.8999e-01,
          7.9054e-01,  7.8527e-01,  7.9216e-01,  7.9203e-01,  7.9163e-01],
        [-1.3823e-01, -2.1921e-03, -9.4615e-02, -3.1068e-01, -3.0829e-01,
         -3.0890e-01, -2.5420e-01, -3.1068e-01, -3.1052e-01, -3.1009e-01],
        [-1.3823e-01, -2.1921e-03, -9.4615e-02, -3.1068e-01, -3.0829e-01,
         -3.0890e-01, -2.5420e-01, -3.1068e-01, -3.1052e-01, -3.1009e-01],
        [-1.3823e-01, -2.1921e-03, -9.4615e-02, -3.1068e-01, -3.0829e-01,
         -3.0890e-01, -2.5420e-01, -3.1068e-01, -3.1052e-01, -3.1009e-01],
        [ 1.1367e+01,  1.0836e+01, -1.1337e+01,  1.9408e+00,  9.9242e-01,
          1.1226e+00, -2.3898e+01,  1.9429e+00,  1.8214e+00,  1.5539e+00],
        [ 3.4419e-01,  1.3496e-01,  4.0213e-01,  7.8641e-01,  7.8423e-01,
          7.8479e-01,  7.7978e-01,  7.8641e-01,  7.8627e-01,  7.8588e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.3129, -0.3129, -0.3129, -0.3129,  0.8149, -0.3129, -0.3129, -0.3129,
         7.5758,  0.8084], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 2.1122e-03,  2.1123e-03,  2.1122e-03,  2.1122e-03,  6.0970e+00,
          2.1122e-03,  2.1123e-03,  2.1123e-03, -2.6432e+01,  5.7659e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -4.3832,   2.5806],
        [ 16.4368,  -0.7671],
        [-16.2676, -12.7464],
        [-12.3911,  -2.7493],
        [ 14.4834,   2.6240],
        [  6.6022,  15.9941],
        [ -9.2513,   8.8138],
        [ 12.2570,   4.4274],
        [ -9.5413,  18.1622],
        [ -2.2344,  -3.1724]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -5.5684, -14.8747, -10.1871,  -1.1601,  -6.1014,  13.9702,   7.7598,
          1.0374,  17.3809,  -7.4573], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.6200e+00, -1.0697e+01, -1.4870e+01,  3.6506e-01, -5.4891e+00,
          7.8081e+00,  2.9918e+00, -7.0394e+00,  1.0567e+01, -8.9543e-02],
        [ 5.1660e-02, -6.4239e+00, -3.5557e+00,  8.1715e+00, -1.6690e+00,
          8.9836e+00, -2.6786e+00, -4.5170e+00, -4.5016e+00,  4.4801e-01],
        [-1.0664e-01,  5.3375e+00,  1.4562e+00,  3.1503e-01, -1.6146e+00,
         -1.5548e+00,  8.0728e+00,  3.4728e+00, -2.3425e+00,  9.8239e-02],
        [ 4.3224e-03,  8.1031e-02,  1.3433e+00,  1.1360e+00, -1.0497e+00,
         -1.6833e+00, -1.3051e+00, -3.0972e+00, -1.2472e+00,  1.8077e+00],
        [ 2.1197e-02,  5.0315e-01,  2.8587e+00, -7.9976e+00,  9.6929e-01,
         -2.4412e+00, -5.1655e-01,  2.4521e+00,  9.2963e+00,  6.5130e-01],
        [ 2.4670e-02, -5.2261e+00,  1.4038e+01,  8.6764e+00, -2.9438e+00,
         -1.9569e+01, -5.5983e-01, -1.0783e+01,  1.2232e-01, -1.2690e+00],
        [ 4.4739e-03, -1.6330e+01,  1.4014e+01,  9.3943e+00, -1.0649e+01,
         -2.6969e+01,  1.9174e+00, -7.2325e+00, -1.1648e+01,  2.7786e+00],
        [ 4.3220e-03,  8.1029e-02,  1.3433e+00,  1.1360e+00, -1.0497e+00,
         -1.6833e+00, -1.3051e+00, -3.0972e+00, -1.2472e+00,  1.8077e+00],
        [-9.9509e-03,  6.0828e+00, -5.5863e+00, -7.7564e+00,  1.2345e+00,
          5.1880e+00,  4.6577e-01,  1.8194e+00,  2.0249e+01,  1.7197e+00],
        [ 4.3220e-03,  8.1030e-02,  1.3433e+00,  1.1360e+00, -1.0497e+00,
         -1.6833e+00, -1.3051e+00, -3.0972e+00, -1.2472e+00,  1.8077e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-7.6365, -6.9106,  4.1041, -2.7663,  2.5272, -8.7340,  2.9492, -2.7663,
        -1.0682, -2.7663], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.4597,  -3.6238,   2.3434,   0.7851,  -3.8560,  13.8779,  -5.9073,
           0.7850,   1.7902,   0.7851],
        [ -0.4597,   3.6277,  -2.3883,  -0.7850,   3.8569, -13.8750,   5.7935,
          -0.7850,  -1.8677,  -0.7850]], device='cuda:0'))])
xi:  [0.00641798]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 379.3081189813562
W_T_median: 158.99072862585842
W_T_pctile_5: 0.006518202423866981
W_T_CVAR_5_pct: -63.90169785171267
Average q (qsum/M+1):  52.42460779989919
Optimal xi:  [0.00641798]
Expected(across Rb) median(across samples) p_equity:  0.26953456724683444
obj fun:  tensor(-1529.3106, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1437.9909314642953
Current xi:  [0.0034547]
objective value function right now is: -1437.9909314642953
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00161945]
objective value function right now is: -1433.6119594497472
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0036056]
objective value function right now is: -1421.707675550403
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0085041]
objective value function right now is: -1433.4176358138095
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1441.326866455202
Current xi:  [0.00249014]
objective value function right now is: -1441.326866455202
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.3003672]
objective value function right now is: -1437.5842988766235
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02156793]
objective value function right now is: -1438.7036676002454
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0458427]
objective value function right now is: -1437.3139893471832
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00210461]
objective value function right now is: -1437.0896484546279
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00201652]
objective value function right now is: -1433.3274301230374
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01050432]
objective value function right now is: -1434.9414682933098
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00764568]
objective value function right now is: -1436.002380840168
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00058479]
objective value function right now is: -1427.8447301139672
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [0.01236535]
objective value function right now is: -1427.0783973194243
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01441702]
objective value function right now is: -1440.6944697473514
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02182584]
objective value function right now is: -1434.8145797311195
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02984958]
objective value function right now is: -1434.7051992441193
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03299985]
objective value function right now is: -1437.831292530062
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01809751]
objective value function right now is: -1433.7078735748992
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01076862]
objective value function right now is: -1440.439484946076
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02944172]
objective value function right now is: -1428.0012473336528
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1441.7202851688248
Current xi:  [-0.00655694]
objective value function right now is: -1441.7202851688248
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00795802]
objective value function right now is: -1441.5380390968276
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01544281]
objective value function right now is: -1441.3136317763096
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00148396]
objective value function right now is: -1441.248456504402
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0007129]
objective value function right now is: -1438.8832652004892
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0145459]
objective value function right now is: -1437.966262506067
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00384102]
objective value function right now is: -1431.762676983667
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [0.0073941]
objective value function right now is: -1424.0231953682985
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00289428]
objective value function right now is: -1438.6790637537877
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00608792]
objective value function right now is: -1438.0360375719208
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0092962]
objective value function right now is: -1439.518278113149
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00663263]
objective value function right now is: -1441.3499174566778
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01085256]
objective value function right now is: -1437.2351907123393
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04491383]
objective value function right now is: -1436.653899265291
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1442.265264132186
Current xi:  [0.00929171]
objective value function right now is: -1442.265264132186
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1442.8484805404871
Current xi:  [0.01046537]
objective value function right now is: -1442.8484805404871
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01674749]
objective value function right now is: -1442.4808273020956
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00755237]
objective value function right now is: -1442.7863627185957
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1443.178229876387
Current xi:  [0.0091069]
objective value function right now is: -1443.178229876387
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00546322]
objective value function right now is: -1441.6136840009854
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00706158]
objective value function right now is: -1438.7202467503796
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01034268]
objective value function right now is: -1438.531485032564
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00686874]
objective value function right now is: -1443.1338334280022
new min fval from sgd:  -1443.5934202654626
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00723505]
objective value function right now is: -1443.5934202654626
new min fval from sgd:  -1443.6196202673625
new min fval from sgd:  -1443.6451642805846
new min fval from sgd:  -1443.6533061856348
new min fval from sgd:  -1443.67641364166
new min fval from sgd:  -1443.6881687895414
new min fval from sgd:  -1443.692033727113
new min fval from sgd:  -1443.6954576079233
new min fval from sgd:  -1443.6970611136483
new min fval from sgd:  -1443.699091123004
new min fval from sgd:  -1443.7242678560126
new min fval from sgd:  -1443.7381239807894
new min fval from sgd:  -1443.7519919754227
new min fval from sgd:  -1443.7574591879788
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00617359]
objective value function right now is: -1442.6896827273881
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00683791]
objective value function right now is: -1443.0615102467473
new min fval from sgd:  -1443.7685197805386
new min fval from sgd:  -1443.7787385028969
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00912202]
objective value function right now is: -1443.165219183801
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00719798]
objective value function right now is: -1443.7481006903784
new min fval from sgd:  -1443.7835605901946
new min fval from sgd:  -1443.791949439368
new min fval from sgd:  -1443.7965204936431
new min fval from sgd:  -1443.7976206393694
new min fval from sgd:  -1443.8003043544293
new min fval from sgd:  -1443.8031624736407
new min fval from sgd:  -1443.8047110540726
new min fval from sgd:  -1443.806103647822
new min fval from sgd:  -1443.8065122601254
new min fval from sgd:  -1443.8086429722769
new min fval from sgd:  -1443.8154213102357
new min fval from sgd:  -1443.8184102225932
new min fval from sgd:  -1443.821810875461
new min fval from sgd:  -1443.823763744466
new min fval from sgd:  -1443.8290489348685
new min fval from sgd:  -1443.8335189896757
new min fval from sgd:  -1443.8335999708395
new min fval from sgd:  -1443.8358800996416
new min fval from sgd:  -1443.8384168847813
new min fval from sgd:  -1443.8384230719555
new min fval from sgd:  -1443.839620221028
new min fval from sgd:  -1443.8444628436544
new min fval from sgd:  -1443.8463324510014
new min fval from sgd:  -1443.8466262025656
new min fval from sgd:  -1443.8564156442415
new min fval from sgd:  -1443.8569630055115
new min fval from sgd:  -1443.8576549724132
new min fval from sgd:  -1443.8590311356998
new min fval from sgd:  -1443.859880780984
new min fval from sgd:  -1443.8607357322462
new min fval from sgd:  -1443.8832574312255
new min fval from sgd:  -1443.8981259148145
new min fval from sgd:  -1443.902648278923
new min fval from sgd:  -1443.9080257330859
new min fval from sgd:  -1443.9131218738635
new min fval from sgd:  -1443.9186717550335
new min fval from sgd:  -1443.9238000790806
new min fval from sgd:  -1443.9255478636535
new min fval from sgd:  -1443.9281803383146
new min fval from sgd:  -1443.9294927295132
new min fval from sgd:  -1443.9308398373364
new min fval from sgd:  -1443.933320021448
new min fval from sgd:  -1443.9344179746843
new min fval from sgd:  -1443.9351937078332
new min fval from sgd:  -1443.939875303904
new min fval from sgd:  -1443.9506506311088
new min fval from sgd:  -1443.963068027468
new min fval from sgd:  -1443.9668728068218
new min fval from sgd:  -1443.9683752822941
new min fval from sgd:  -1443.971522502507
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00655098]
objective value function right now is: -1443.4950562651045
min fval:  -1443.971522502507
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 15.5711,  -4.4216],
        [  5.1167,   5.1501],
        [-16.3447,   4.9775],
        [  0.9313,  -1.0668],
        [  0.6762,  -0.9045],
        [  0.7317,  -0.9394],
        [  0.1446,  12.5828],
        [  0.9316,  -1.0670],
        [  0.9104,  -1.0533],
        [  0.8562,  -1.0186]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-11.6074,  -7.4913,  12.4286,   4.7525,   4.1947,   4.3056,   9.5264,
          4.7534,   4.7026,   4.5763], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.3163e-01, -2.0999e-02, -1.5870e-01, -3.6076e-01, -3.5898e-01,
         -3.5945e-01, -2.1002e-01, -3.6076e-01, -3.6065e-01, -3.6034e-01],
        [-1.3163e-01, -2.0999e-02, -1.5870e-01, -3.6076e-01, -3.5898e-01,
         -3.5945e-01, -2.1002e-01, -3.6076e-01, -3.6065e-01, -3.6034e-01],
        [-1.3163e-01, -2.0999e-02, -1.5870e-01, -3.6076e-01, -3.5898e-01,
         -3.5945e-01, -2.1002e-01, -3.6076e-01, -3.6065e-01, -3.6034e-01],
        [-1.3163e-01, -2.0999e-02, -1.5870e-01, -3.6076e-01, -3.5898e-01,
         -3.5945e-01, -2.1002e-01, -3.6076e-01, -3.6065e-01, -3.6034e-01],
        [ 3.5271e-01,  1.1156e-01,  4.4732e-01,  8.1097e-01,  8.0998e-01,
          8.1026e-01,  6.5561e-01,  8.1098e-01,  8.1092e-01,  8.1076e-01],
        [-1.3163e-01, -2.0999e-02, -1.5870e-01, -3.6076e-01, -3.5898e-01,
         -3.5945e-01, -2.1002e-01, -3.6076e-01, -3.6065e-01, -3.6034e-01],
        [-1.3163e-01, -2.0999e-02, -1.5870e-01, -3.6076e-01, -3.5898e-01,
         -3.5945e-01, -2.1002e-01, -3.6076e-01, -3.6065e-01, -3.6034e-01],
        [-1.3163e-01, -2.0999e-02, -1.5870e-01, -3.6076e-01, -3.5898e-01,
         -3.5945e-01, -2.1002e-01, -3.6076e-01, -3.6065e-01, -3.6034e-01],
        [ 1.2423e+01,  1.6790e+01, -1.2855e+01,  1.9869e+00,  9.9389e-01,
          1.1382e+00, -2.7144e+01,  1.9890e+00,  1.8659e+00,  1.5925e+00],
        [ 3.5081e-01,  1.1119e-01,  4.4359e-01,  8.0564e-01,  8.0465e-01,
          8.0493e-01,  6.5170e-01,  8.0564e-01,  8.0559e-01,  8.0543e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.3624, -0.3624, -0.3624, -0.3624,  0.8264, -0.3624, -0.3624, -0.3624,
         7.5723,  0.8205], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-2.4725e-02, -2.4725e-02, -2.4725e-02, -2.4725e-02,  6.0121e+00,
         -2.4725e-02, -2.4725e-02, -2.4725e-02, -2.9197e+01,  5.7207e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.1476,   0.5078],
        [ 17.2163,  -0.8732],
        [-17.0648, -13.9349],
        [-14.7276,  -5.0864],
        [ 12.7208,   4.3026],
        [  7.8287,  16.9014],
        [ -8.7605,   9.3324],
        [ 13.7165,   4.5252],
        [-11.0293,  19.2734],
        [ -1.9837,   0.4809]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -4.1942, -15.6722, -11.0843,  -2.3402,  -7.7491,  14.2117,   7.2542,
         -2.0979,  19.0137,  -4.2783], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-8.6943e-03, -9.5596e+00, -7.7963e+00, -6.5605e-01, -4.8430e+00,
          6.4159e+00,  3.6542e+00, -7.4130e+00,  1.0737e+01, -1.0488e-03],
        [ 3.1832e-01, -7.3946e+00, -1.5892e+00,  7.8907e+00, -4.1511e+00,
          9.6680e+00,  1.1188e-01, -3.9085e+00, -4.1522e+00,  1.0867e-01],
        [ 3.5466e-01,  6.6985e+00,  4.2585e+00, -4.5041e+00, -1.4108e+00,
         -2.8042e+00,  7.8602e+00,  1.7268e+00, -1.9408e+00,  1.4700e-01],
        [ 9.8363e-03,  1.6847e+00, -1.9551e+00, -3.4674e-01, -5.5331e-01,
         -1.5713e+00, -3.1694e-02, -1.5014e-01, -2.2800e+00,  1.4027e-02],
        [ 2.3105e-01,  1.5544e+00,  4.2661e+00, -1.0505e+01, -1.5568e+00,
         -4.0686e+00,  1.1862e+00,  2.2387e+00,  8.8447e+00,  1.8637e-01],
        [ 2.7634e-01, -8.6315e+00,  1.1181e+01,  1.0370e+01, -6.2853e-02,
         -1.9284e+01, -2.9800e+00, -1.1297e+01,  9.5100e-01,  2.2214e-01],
        [-5.5510e-03, -1.9954e+01,  1.5345e+01,  9.2604e+00,  1.0936e-02,
         -2.9882e+01,  7.9882e-02, -1.1061e+01, -1.0291e+01, -3.5824e-02],
        [ 9.8369e-03,  1.6847e+00, -1.9551e+00, -3.4674e-01, -5.5332e-01,
         -1.5713e+00, -3.1694e-02, -1.5016e-01, -2.2800e+00,  1.4027e-02],
        [ 5.2293e-03,  5.1469e+00, -4.0038e+00, -8.1121e+00,  5.1906e-01,
          6.9912e+00, -4.0477e-01,  1.5191e+00,  1.9914e+01,  3.4139e-03],
        [ 9.8359e-03,  1.6847e+00, -1.9551e+00, -3.4674e-01, -5.5331e-01,
         -1.5713e+00, -3.1694e-02, -1.5013e-01, -2.2800e+00,  1.4026e-02]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-10.8186, -10.5780,   6.1320,  -5.1884,   5.1763, -13.2810,  -1.9065,
         -5.1885,  -0.7516,  -5.1884], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.4958,  -3.5400,   2.6876,  -0.9699,  -3.3199,  16.5762,  -6.8230,
          -0.9699,   1.0016,  -0.9699],
        [ -0.4957,   3.5418,  -2.7324,   0.9699,   3.3208, -16.5727,   6.7556,
           0.9699,  -1.0791,   0.9699]], device='cuda:0'))])
xi:  [0.00770985]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 400.73483009983903
W_T_median: 175.91185892163128
W_T_pctile_5: 0.007566132332694409
W_T_CVAR_5_pct: -52.81406493751192
Average q (qsum/M+1):  51.69075258316532
Optimal xi:  [0.00770985]
Expected(across Rb) median(across samples) p_equity:  0.26444618701934813
obj fun:  tensor(-1443.9715, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
loaded xi:  0.0077098473
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1322.3115609376516
Current xi:  [0.06749523]
objective value function right now is: -1322.3115609376516
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01199427]
objective value function right now is: -1319.269246341276
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1334.4388963749075
Current xi:  [0.07730052]
objective value function right now is: -1334.4388963749075
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0590559]
objective value function right now is: -1324.5454674607493
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01399424]
objective value function right now is: -1302.8467661143386
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01120692]
objective value function right now is: -1329.476412438452
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1337.829119438189
Current xi:  [-0.0428367]
objective value function right now is: -1337.829119438189
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.7097752]
objective value function right now is: -1333.2591174617603
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1375.0107213847289
Current xi:  [18.93944]
objective value function right now is: -1375.0107213847289
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1429.4101127244344
Current xi:  [44.25078]
objective value function right now is: -1429.4101127244344
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1486.924093533806
Current xi:  [70.78812]
objective value function right now is: -1486.924093533806
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1526.5196310851063
Current xi:  [97.83464]
objective value function right now is: -1526.5196310851063
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1559.1703642213195
Current xi:  [121.3462]
objective value function right now is: -1559.1703642213195
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1573.9876036761602
Current xi:  [142.35275]
objective value function right now is: -1573.9876036761602
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1592.6663149141655
Current xi:  [158.57559]
objective value function right now is: -1592.6663149141655
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1593.6880788831404
Current xi:  [171.13]
objective value function right now is: -1593.6880788831404
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [178.37227]
objective value function right now is: -1575.8257593043586
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.87302]
objective value function right now is: -1587.7348814760605
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.4085]
objective value function right now is: -1582.5438881604066
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1595.254276144832
Current xi:  [184.90025]
objective value function right now is: -1595.254276144832
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.63402]
objective value function right now is: -1593.964143790708
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.2698483320974
Current xi:  [187.1671]
objective value function right now is: -1602.2698483320974
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.21646]
objective value function right now is: -1594.8301087381933
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.2911]
objective value function right now is: -1587.000796439069
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.77928]
objective value function right now is: -1596.9315032451389
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.00421]
objective value function right now is: -1578.4623706029095
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.27815]
objective value function right now is: -1598.3484732582817
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [187.25851]
objective value function right now is: -1594.9459957140466
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [186.16803]
objective value function right now is: -1278.796271905667
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.87854]
objective value function right now is: -1597.9953734680003
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.84027]
objective value function right now is: -1589.6762045570729
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.17935]
objective value function right now is: -1561.9996395801008
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.29926]
objective value function right now is: -1585.3223947234983
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.66422]
objective value function right now is: -1585.5208195303771
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.2979]
objective value function right now is: -1598.7764987660078
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.2310335874872
Current xi:  [187.36307]
objective value function right now is: -1605.2310335874872
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.51334]
objective value function right now is: -1603.161262015034
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.3825725972927
Current xi:  [188.06271]
objective value function right now is: -1605.3825725972927
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.8343317470128
Current xi:  [188.16586]
objective value function right now is: -1605.8343317470128
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.03372]
objective value function right now is: -1580.1302676720338
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.95479]
objective value function right now is: -1605.4768314669418
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.06729]
objective value function right now is: -1600.4627331038846
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.1503]
objective value function right now is: -1602.7642838404831
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.5066]
objective value function right now is: -1591.983694575603
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.68842]
objective value function right now is: -1605.1920468597823
new min fval from sgd:  -1605.9707456954259
new min fval from sgd:  -1606.1250205558877
new min fval from sgd:  -1606.1580361249626
new min fval from sgd:  -1606.193523761972
new min fval from sgd:  -1606.2552020199564
new min fval from sgd:  -1606.3092197304427
new min fval from sgd:  -1606.5300592489189
new min fval from sgd:  -1606.6430245459205
new min fval from sgd:  -1606.7743450344592
new min fval from sgd:  -1606.8311373947693
new min fval from sgd:  -1606.8581668804432
new min fval from sgd:  -1606.8801102186735
new min fval from sgd:  -1606.9118086233868
new min fval from sgd:  -1606.9174752297135
new min fval from sgd:  -1606.9451772903287
new min fval from sgd:  -1606.9540619579993
new min fval from sgd:  -1606.9994518436763
new min fval from sgd:  -1607.091914269813
new min fval from sgd:  -1607.2318215645425
new min fval from sgd:  -1607.2434634595197
new min fval from sgd:  -1607.244130817968
new min fval from sgd:  -1607.2825473049072
new min fval from sgd:  -1607.3452041300195
new min fval from sgd:  -1607.44324091746
new min fval from sgd:  -1607.5444850185827
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.50464]
objective value function right now is: -1603.9374444805692
new min fval from sgd:  -1607.571103034847
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.32463]
objective value function right now is: -1605.7913387557153
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.44138]
objective value function right now is: -1606.1825427660053
new min fval from sgd:  -1607.5805366293373
new min fval from sgd:  -1607.8321086905805
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.82224]
objective value function right now is: -1607.4453484350393
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.8594]
objective value function right now is: -1607.322883623358
min fval:  -1607.8321086905805
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 18.3610,  -7.0521],
        [  9.1197,   1.4182],
        [-17.2027,   8.3011],
        [  7.1847,  -5.4726],
        [  0.6986,  -0.3390],
        [  0.7416,  -0.3632],
        [  1.2655,  13.3530],
        [  7.1113,  -5.5587],
        [  2.5959,  -2.1159],
        [  0.8526,  -0.4267]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-9.2070, -9.0915, 11.3112,  4.3264,  4.7666,  4.8742,  8.8160,  4.2577,
         5.0083,  5.1302], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -0.1374,  -0.0388,  -0.3559,  -0.4840,  -0.4743,  -0.4746,  -0.2263,
          -0.4842,  -0.4763,  -0.4753],
        [ -0.1374,  -0.0388,  -0.3559,  -0.4840,  -0.4743,  -0.4746,  -0.2263,
          -0.4842,  -0.4763,  -0.4753],
        [ -0.1374,  -0.0388,  -0.3559,  -0.4840,  -0.4743,  -0.4746,  -0.2263,
          -0.4842,  -0.4763,  -0.4753],
        [ -0.1374,  -0.0388,  -0.3559,  -0.4840,  -0.4743,  -0.4746,  -0.2263,
          -0.4842,  -0.4763,  -0.4753],
        [  0.3755,   0.0777,   0.6204,   0.8644,   0.9060,   0.9063,   0.7378,
           0.8617,   0.8721,   0.9068],
        [ -0.1374,  -0.0388,  -0.3559,  -0.4840,  -0.4743,  -0.4746,  -0.2263,
          -0.4842,  -0.4763,  -0.4753],
        [ -0.1374,  -0.0388,  -0.3559,  -0.4840,  -0.4743,  -0.4746,  -0.2263,
          -0.4842,  -0.4763,  -0.4753],
        [ -0.1374,  -0.0388,  -0.3559,  -0.4840,  -0.4743,  -0.4746,  -0.2263,
          -0.4842,  -0.4763,  -0.4753],
        [ 11.8379,  26.8783, -12.6226,   2.1955,   1.2028,   1.3560, -29.1487,
           2.2010,   2.0403,   1.8239],
        [  0.3739,   0.0774,   0.6160,   0.8596,   0.9004,   0.9006,   0.7339,
           0.8569,   0.8669,   0.9011]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.4771, -0.4771, -0.4771, -0.4771,  0.9094, -0.4771, -0.4771, -0.4771,
         7.7773,  0.9036], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 2.1115e-03,  2.1115e-03,  2.1115e-03,  2.1115e-03,  5.6909e+00,
          2.1115e-03,  2.1115e-03,  2.1115e-03, -2.8575e+01,  5.4222e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.9158,   1.9750],
        [ 17.7231,  -0.7855],
        [-19.7229, -13.7756],
        [-15.2101,  -5.6145],
        [ 12.3577,   8.6814],
        [  4.6122,  17.4248],
        [ -8.7571,  11.9824],
        [ 16.0174,   5.6643],
        [ -3.9372,  20.7267],
        [ -2.9523,   2.0282]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -4.9840, -17.2390, -10.3196,  -1.5222,  -7.1840,  13.8568,   8.8583,
         -5.7345,  17.2069,  -5.0364], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.4309e+00,  4.0353e+00,  3.5025e+00,  6.5987e+00, -2.5495e+00,
          1.3031e+00,  1.0326e+01, -3.9635e+00,  6.0598e+00, -2.6072e+00],
        [-1.3969e-01, -1.2882e+01,  7.8009e-01,  4.0267e+00,  1.1028e-01,
          3.5475e+00, -1.2160e+01, -5.0090e-01,  3.4999e+00, -1.4042e-01],
        [-5.4838e-02, -1.6279e-01,  2.7967e-01, -2.2501e+00, -2.6300e-02,
          2.2273e+00,  2.9985e-01,  3.3970e+00,  7.1863e-01, -5.3637e-02],
        [-4.0217e-02, -1.4587e+00,  1.7783e-01,  5.0555e+00, -2.3420e+00,
          2.0319e+00, -4.3270e+00, -5.2571e-01, -1.8701e+01, -3.3271e-02],
        [-2.7936e-02,  1.7928e+00,  5.7194e+00, -2.7957e+00,  6.0947e-02,
         -4.8985e+00,  4.9826e+00,  1.4797e+01,  6.1286e-01, -3.3641e-02],
        [ 9.1347e-01, -1.3353e+01,  1.0039e+01,  1.0308e+01, -1.2464e-03,
         -2.8657e+01,  3.0816e+00, -1.1847e+00,  4.5130e+00,  8.5659e-01],
        [-3.9243e-01, -2.1664e+01,  1.0509e+01,  9.5069e+00, -4.1105e-02,
         -2.1681e+01, -9.7429e-01, -1.1789e+01, -3.7461e+00, -3.7618e-01],
        [-4.0216e-02, -1.4586e+00,  1.7791e-01,  5.0561e+00, -2.3418e+00,
          2.0318e+00, -4.3277e+00, -5.2565e-01, -1.8702e+01, -3.3269e-02],
        [ 2.9997e-01,  1.4636e+00, -4.6652e+00, -1.2074e+01, -5.3781e+00,
          6.3220e+00,  1.1125e+01,  1.8772e+00,  1.3580e+01,  2.6928e-01],
        [-4.0224e-02, -1.4587e+00,  1.7775e-01,  5.0548e+00, -2.3424e+00,
          2.0319e+00, -4.3265e+00, -5.2580e-01, -1.8699e+01, -3.3278e-02]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-13.3867,  -9.7184,   7.6033,  -5.7078,   7.4662, -15.8707,  -2.5911,
         -5.7076,  -1.6287,  -5.7080], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.3293,   3.4175,   2.9686,  -4.3749,  -2.9440,  21.0113,  -7.5725,
          -4.3750,   0.3742,  -4.3747],
        [ -0.3292,  -3.4171,  -3.0134,   4.3751,   2.9449, -21.0047,   7.5128,
           4.3752,  -0.4516,   4.3750]], device='cuda:0'))])
xi:  [188.75986]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 631.9333439372857
W_T_median: 404.7693772943378
W_T_pctile_5: 188.93908181676866
W_T_CVAR_5_pct: 23.246152209022625
Average q (qsum/M+1):  48.11620306199597
Optimal xi:  [188.75986]
Expected(across Rb) median(across samples) p_equity:  0.2484052633245786
obj fun:  tensor(-1607.8321, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
loaded xi:  188.75986
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2754.5608261626885
Current xi:  [201.34401]
objective value function right now is: -2754.5608261626885
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -2815.856378217106
Current xi:  [208.02205]
objective value function right now is: -2815.856378217106
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.3649]
objective value function right now is: -2565.1167360127524
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -2867.407416124339
Current xi:  [209.97556]
objective value function right now is: -2867.407416124339
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.17162]
objective value function right now is: -2766.785323164455
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.2754]
objective value function right now is: -2808.8455077103163
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [211.44745]
objective value function right now is: -2772.2848442680934
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.11845]
objective value function right now is: -2683.690087588913
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.08673]
objective value function right now is: -2801.872472241795
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.9988]
objective value function right now is: -2753.589431979034
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.63228]
objective value function right now is: -2858.566931751084
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.73792]
objective value function right now is: -2676.496259283211
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.87183]
objective value function right now is: -2357.0578352813886
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [212.68292]
objective value function right now is: -2764.2463437949937
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.96744]
objective value function right now is: -2582.1090468443604
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.91255]
objective value function right now is: -2694.0204388452953
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.70164]
objective value function right now is: -2849.538592776516
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.10667]
objective value function right now is: -2789.265874675914
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.48099]
objective value function right now is: -2794.0802903905824
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.4288]
objective value function right now is: -2788.1745178938945
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.55507]
objective value function right now is: -2866.8188037293094
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.88925]
objective value function right now is: -2673.6525406857872
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.64107]
objective value function right now is: -2802.2895848037037
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.28795]
objective value function right now is: -2859.7581122130555
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.50735]
objective value function right now is: -2849.7335623407903
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.66685]
objective value function right now is: -2647.464328135943
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.74388]
objective value function right now is: -2850.7082550186365
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [213.11868]
objective value function right now is: -2819.9255963998157
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [212.57024]
objective value function right now is: -2866.7598246708512
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.25015]
objective value function right now is: -2645.842990058251
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.66872]
objective value function right now is: -2852.9695317603287
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.17876]
objective value function right now is: -2768.1263373111137
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.36673]
objective value function right now is: -2709.735664558199
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.9453]
objective value function right now is: -2515.172051408228
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.996]
objective value function right now is: -2843.6331840130615
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2900.5890599884847
Current xi:  [212.55154]
objective value function right now is: -2900.5890599884847
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.76755]
objective value function right now is: -2894.992307107712
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.18806]
objective value function right now is: -2853.5080534157373
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.2656]
objective value function right now is: -2894.005611298585
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.98224]
objective value function right now is: -2900.5656394262146
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.55965]
objective value function right now is: -2890.3406717048347
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -2904.447807134034
Current xi:  [214.74959]
objective value function right now is: -2904.447807134034
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.67332]
objective value function right now is: -2899.3232140332293
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.04031]
objective value function right now is: -2880.528207223139
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.0711]
objective value function right now is: -2891.79664181406
new min fval from sgd:  -2905.353823162706
new min fval from sgd:  -2906.4932978696256
new min fval from sgd:  -2907.649291282842
new min fval from sgd:  -2908.1289118508926
new min fval from sgd:  -2908.4704481599533
new min fval from sgd:  -2908.590914047989
new min fval from sgd:  -2908.8140012458125
new min fval from sgd:  -2909.0916905836248
new min fval from sgd:  -2909.4188584064386
new min fval from sgd:  -2909.432221358181
new min fval from sgd:  -2910.4162350297115
new min fval from sgd:  -2910.9188248177575
new min fval from sgd:  -2910.96812825282
new min fval from sgd:  -2911.9420016806976
new min fval from sgd:  -2912.221758501863
new min fval from sgd:  -2912.529782687822
new min fval from sgd:  -2913.1704739133947
new min fval from sgd:  -2913.5274801591486
new min fval from sgd:  -2913.6580353499476
new min fval from sgd:  -2913.836424026099
new min fval from sgd:  -2914.306228407009
new min fval from sgd:  -2914.4546298674295
new min fval from sgd:  -2915.222186298999
new min fval from sgd:  -2915.841967066145
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.09015]
objective value function right now is: -2898.1872687804885
new min fval from sgd:  -2916.041040436503
new min fval from sgd:  -2916.2935451970466
new min fval from sgd:  -2916.3923485865544
new min fval from sgd:  -2916.8691583711375
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.89305]
objective value function right now is: -2903.334216936934
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.92917]
objective value function right now is: -2906.6812863576884
new min fval from sgd:  -2916.893153905998
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.71976]
objective value function right now is: -2915.550605854057
new min fval from sgd:  -2916.994133852998
new min fval from sgd:  -2917.1109166537085
new min fval from sgd:  -2917.189813221559
new min fval from sgd:  -2917.225559051912
new min fval from sgd:  -2917.3268441515434
new min fval from sgd:  -2917.448467140566
new min fval from sgd:  -2917.6056805928333
new min fval from sgd:  -2917.680230059445
new min fval from sgd:  -2917.698450685045
new min fval from sgd:  -2917.9633680078236
new min fval from sgd:  -2918.5544302641797
new min fval from sgd:  -2919.0419051917484
new min fval from sgd:  -2919.1466532413897
new min fval from sgd:  -2919.183076264502
new min fval from sgd:  -2919.2100586681804
new min fval from sgd:  -2919.331715055355
new min fval from sgd:  -2919.4588151826574
new min fval from sgd:  -2919.513508019586
new min fval from sgd:  -2919.539749014997
new min fval from sgd:  -2919.5510608203776
new min fval from sgd:  -2919.5806175001526
new min fval from sgd:  -2919.7527175275673
new min fval from sgd:  -2919.774484981843
new min fval from sgd:  -2919.8452191165284
new min fval from sgd:  -2920.072639846486
new min fval from sgd:  -2920.08866009527
new min fval from sgd:  -2920.1603624438726
new min fval from sgd:  -2920.246813145738
new min fval from sgd:  -2920.3125765773375
new min fval from sgd:  -2920.3288116364442
new min fval from sgd:  -2920.360993138038
new min fval from sgd:  -2920.4059781670544
new min fval from sgd:  -2920.5089597486594
new min fval from sgd:  -2920.516822786768
new min fval from sgd:  -2920.5666375260375
new min fval from sgd:  -2920.586665250382
new min fval from sgd:  -2920.6132602208895
new min fval from sgd:  -2920.6162383797996
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.88136]
objective value function right now is: -2910.1143863431757
min fval:  -2920.6162383797996
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 19.0460,  -6.7529],
        [ 13.2553,  -0.1425],
        [-19.7938,  10.2323],
        [ -1.1811,   0.3670],
        [ -1.7676,   2.0890],
        [  4.9736,  -2.1863],
        [ -3.1837,  14.5892],
        [  7.5303,  -6.7345],
        [  4.2876,  -5.2467],
        [  4.8317,  -2.3175]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -9.9997, -10.2079,  11.2204,  -4.0054,  -5.6752,   3.3131,   9.7415,
          0.5547,   1.5170,   3.6069], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.3044e-01, -1.1328e-01, -5.3181e-01, -7.3931e-03, -6.9531e-03,
         -7.2381e-01, -4.9423e-01, -5.4581e-01, -5.4694e-01, -7.2142e-01],
        [-1.3044e-01, -1.1328e-01, -5.3181e-01, -7.3931e-03, -6.9531e-03,
         -7.2381e-01, -4.9423e-01, -5.4581e-01, -5.4694e-01, -7.2142e-01],
        [-1.3044e-01, -1.1328e-01, -5.3181e-01, -7.3931e-03, -6.9531e-03,
         -7.2381e-01, -4.9423e-01, -5.4581e-01, -5.4694e-01, -7.2142e-01],
        [-1.3044e-01, -1.1328e-01, -5.3181e-01, -7.3931e-03, -6.9531e-03,
         -7.2381e-01, -4.9423e-01, -5.4581e-01, -5.4694e-01, -7.2142e-01],
        [ 1.6553e-01,  2.2594e-01,  1.1432e+00,  1.6867e-02,  3.6576e-02,
          1.2675e+00,  1.0776e+00,  8.8916e-01,  9.0671e-01,  1.2913e+00],
        [-1.3044e-01, -1.1328e-01, -5.3181e-01, -7.3931e-03, -6.9531e-03,
         -7.2381e-01, -4.9423e-01, -5.4581e-01, -5.4694e-01, -7.2142e-01],
        [-1.3044e-01, -1.1328e-01, -5.3181e-01, -7.3931e-03, -6.9530e-03,
         -7.2381e-01, -4.9423e-01, -5.4581e-01, -5.4694e-01, -7.2142e-01],
        [-1.3044e-01, -1.1328e-01, -5.3181e-01, -7.3931e-03, -6.9531e-03,
         -7.2381e-01, -4.9423e-01, -5.4581e-01, -5.4694e-01, -7.2142e-01],
        [ 1.2773e+01,  3.2954e+01, -1.2890e+01,  6.4752e-02, -3.6335e+00,
          2.5075e+00, -2.8462e+01,  2.5635e+00,  2.4560e+00,  2.2523e+00],
        [ 1.6495e-01,  2.2359e-01,  1.1342e+00,  1.6752e-02,  3.5009e-02,
          1.2610e+00,  1.0692e+00,  8.8438e-01,  9.0194e-01,  1.2846e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.7482, -0.7482, -0.7482, -0.7482,  1.3188, -0.7482, -0.7482, -0.7482,
        10.0859,  1.3089], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.4791e-02, -1.4791e-02, -1.4791e-02, -1.4791e-02,  5.2791e+00,
         -1.4791e-02, -1.4791e-02, -1.4791e-02, -2.8139e+01,  5.0226e+00]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -4.6407,   6.3698],
        [ 18.6332,  -0.4610],
        [-21.0694, -14.0457],
        [-14.8562,  -5.3897],
        [ 10.5645,   4.8981],
        [  5.1931,  17.5436],
        [-13.7782,   9.4242],
        [ 17.4616,   4.8178],
        [ -4.9653,  22.2567],
        [ -4.6454,   6.3824]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -5.9604, -17.8631, -10.1538,  -0.9258,  -8.2397,  14.2129,   8.3136,
         -8.1501,  17.1710,  -5.9802], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-8.2844e-01,  1.0563e+01, -7.6085e+00,  1.1526e+01, -1.0710e+00,
          2.6825e+00,  7.6196e+00, -5.1727e+00,  3.9710e+00, -8.3093e-01],
        [-3.7330e+00, -1.5507e+01, -6.1782e+00,  6.1111e-01,  3.8520e+00,
          5.0424e+00, -4.7981e+00, -1.4440e+00,  9.7273e-01, -3.5972e+00],
        [-6.0181e-01,  3.9798e-01, -3.3012e-02, -2.2972e+00, -3.6707e-01,
          1.7111e+00, -1.0502e+00,  1.7494e+00,  6.9849e+00, -6.1358e-01],
        [-1.5143e-03, -1.6936e+00, -1.7831e-01,  2.3054e+00, -9.1137e+00,
          3.8751e-01, -2.0915e+01,  1.0172e+00, -6.6381e+00, -1.5269e-03],
        [ 4.3820e-02,  7.8928e-01,  1.1884e+01, -5.6809e+00, -1.7907e-02,
         -4.5873e+00,  3.2335e+00,  2.3749e+01,  3.4637e+00,  4.2581e-02],
        [-3.3697e-02, -2.7616e+01,  1.0617e+01,  1.1007e+01,  3.0498e-02,
         -4.4139e+01,  1.3945e+01, -5.8942e+00, -1.1106e+00, -3.2525e-02],
        [ 5.1210e-02, -2.2585e+01,  9.2522e+00,  9.7529e+00, -1.0471e+01,
         -1.9618e+01, -3.2774e-01, -2.2190e+01, -2.9226e+00,  5.0369e-02],
        [-1.8446e-03, -1.6062e+00, -2.2266e-01,  2.3438e+00, -9.1208e+00,
          3.2923e-01, -2.0936e+01,  9.4976e-01, -7.1172e+00, -1.8650e-03],
        [-7.9247e+00,  2.0716e+00, -1.7615e+01,  2.5418e+00, -9.0969e+00,
         -4.9365e+00,  2.0719e+01, -2.1873e-01,  8.6547e+00, -7.8778e+00],
        [-1.0379e-03, -1.8025e+00, -1.2533e-01,  2.2622e+00, -9.0974e+00,
          4.6145e-01, -2.0897e+01,  1.1027e+00, -6.1109e+00, -1.0457e-03]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-14.5988,  -9.2013,   1.5949,  -6.0422,   8.0435, -17.7052,  -2.9253,
         -6.0685,  -5.3161,  -6.0059], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.2074,   6.0435,   3.0862,  -6.4037,  -2.6092,  28.7176,  -9.8541,
          -6.3763,   0.1048,  -6.4412],
        [ -0.2073,  -6.0432,  -3.1309,   6.4039,   2.6101, -28.7097,   9.7917,
           6.3764,  -0.1821,   6.4414]], device='cuda:0'))])
xi:  [214.77596]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 683.4974596838158
W_T_median: 486.0861682824415
W_T_pctile_5: 215.38464687625995
W_T_CVAR_5_pct: 30.058301462873995
Average q (qsum/M+1):  45.73591466103831
Optimal xi:  [214.77596]
Expected(across Rb) median(across samples) p_equity:  0.21489546025792758
obj fun:  tensor(-2920.6162, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
