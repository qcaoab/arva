Starting at: 
05-12-22_18:05

 Random seed:  2  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1  logistic_sigmoid   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  75305.97818533768
Current xi:  [220.80037]
objective value function right now is: 75305.97818533768
4.0% of gradient descent iterations done. Method = Adam
new min fval:  35176.83659038479
Current xi:  [203.80518]
objective value function right now is: 35176.83659038479
6.0% of gradient descent iterations done. Method = Adam
new min fval:  2551.3702242200357
Current xi:  [194.88733]
objective value function right now is: 2551.3702242200357
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.00598]
objective value function right now is: 53791.82707020638
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -19192.26406617531
Current xi:  [194.41083]
objective value function right now is: -19192.26406617531
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -21071.068866601076
Current xi:  [195.9432]
objective value function right now is: -21071.068866601076
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [197.34221]
objective value function right now is: -12984.739159190209
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.05081]
objective value function right now is: -18988.893763718315
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.62204]
objective value function right now is: -18760.85391285788
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.62234]
objective value function right now is: 43306.474639528795
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -22305.835421576885
Current xi:  [195.8207]
objective value function right now is: -22305.835421576885
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [194.61009]
objective value function right now is: 6932.215277313909
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [194.35368]
objective value function right now is: 6086.579682781557
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [193.28154]
objective value function right now is: -7874.569695412579
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -26818.952773036093
Current xi:  [193.05545]
objective value function right now is: -26818.952773036093
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [194.03513]
objective value function right now is: -1633.2690443341155
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -27821.263379447693
Current xi:  [194.94035]
objective value function right now is: -27821.263379447693
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -31900.324071828873
Current xi:  [196.46504]
objective value function right now is: -31900.324071828873
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [193.01581]
objective value function right now is: 24710.453438121163
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.57495]
objective value function right now is: -26736.456101140357
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.24358]
objective value function right now is: 38685.869124442026
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.24889]
objective value function right now is: -24415.784864729852
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.72543]
objective value function right now is: -5509.993025796691
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.7544]
objective value function right now is: -20570.60354519918
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.91953]
objective value function right now is: -22785.68657293783
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -36438.87091989947
Current xi:  [198.3725]
objective value function right now is: -36438.87091989947
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.0866]
objective value function right now is: -22007.889954239563
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [197.06224]
objective value function right now is: -23536.22826686172
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [198.62056]
objective value function right now is: 1070.2875424237488
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.64507]
objective value function right now is: -36367.70990515651
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -46047.189741470385
Current xi:  [200.5075]
objective value function right now is: -46047.189741470385
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.75806]
objective value function right now is: -40996.41200337493
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.30739]
objective value function right now is: -30283.336935180676
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.66968]
objective value function right now is: 28870.311216485505
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.5048]
objective value function right now is: -1192.2333668614888
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.41391]
objective value function right now is: -34867.20513288352
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.32724]
objective value function right now is: -6978.440764112098
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [194.84874]
objective value function right now is: -10016.02294015231
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.31625]
objective value function right now is: -20372.06785616826
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.77925]
objective value function right now is: -33642.150288409335
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.46059]
objective value function right now is: 28006.51369964204
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.6203]
objective value function right now is: -27315.057603840964
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.16975]
objective value function right now is: 13720.493740832202
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.80125]
objective value function right now is: -40803.33326438789
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.28856]
objective value function right now is: -24645.00824109275
new min fval from sgd:  -46519.38099037553
new min fval from sgd:  -47422.941921531456
new min fval from sgd:  -48247.7215066399
new min fval from sgd:  -48437.76103807548
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.8094]
objective value function right now is: -32318.90912259704
new min fval from sgd:  -49254.77361510471
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.64854]
objective value function right now is: -42222.78880768357
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.68558]
objective value function right now is: -47422.52476021997
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.6864]
objective value function right now is: -27719.74238843716
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.8993]
objective value function right now is: -37408.21092381797
min fval:  -49254.77361510471
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.4969, -1.7426],
        [-1.1671,  1.4961],
        [ 1.5701, -1.0645],
        [-2.4641,  2.1059],
        [-0.3338,  0.1397],
        [ 2.1412, -1.1565],
        [ 0.5452, -0.5084],
        [ 1.2406, -1.3352],
        [ 1.9781, -1.9944],
        [-1.2160,  1.9982]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.8207,  0.1349,  1.0377, -0.4950,  0.7811,  1.4029,  1.1793,  0.8869,
          2.2442, -0.0637],
        [ 0.5521, -0.4584,  0.2778, -0.7050,  0.1248,  1.0339,  0.3652,  0.6702,
          1.1646, -0.3455],
        [ 1.3354,  0.4261,  1.2211,  0.3413,  0.7227,  1.7589,  0.8399,  1.0659,
          1.9655,  0.2778],
        [ 1.3017,  1.1791,  1.4180,  1.2863,  1.5231,  1.7031,  1.4877,  1.3330,
          1.3058,  1.1665],
        [ 1.5238,  1.4400,  1.3349,  0.9254,  1.5877,  1.5359,  1.6022,  1.4270,
          1.3786,  0.9440],
        [ 1.3091,  0.8196,  1.2640,  0.8598,  1.1985,  1.3510,  1.5658,  1.5829,
          1.4458,  1.0443],
        [ 0.9740,  0.3240,  0.8318, -0.3400,  0.9305,  1.9171,  1.1456,  1.2468,
          2.0805,  0.4806],
        [ 0.0411, -0.5129, -0.3181, -0.7503, -0.3565, -0.0149, -0.0248, -0.0668,
          0.4519, -0.6029],
        [ 0.8607, -0.2186,  0.2394, -0.3946,  0.2670,  1.0251,  0.5367,  0.3122,
          1.7624, -0.3938],
        [ 1.5447,  1.2769,  1.1942,  0.9727,  1.1259,  1.1315,  1.0576,  1.4270,
          1.4459,  1.1421]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.6426, -1.9284, -5.4070, -5.2672, -4.8652, -6.5580, -5.8118, -1.0908,
         -2.3352, -6.2141]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -5.4531,   0.5482],
        [ -2.1287,   4.1750],
        [ -9.1317,  -1.1395],
        [ -2.1029,   4.1718],
        [ -1.9910,   4.1393],
        [-10.0231,  -1.9413],
        [  7.6692,   2.9958],
        [ -2.1440,   4.1599],
        [ -2.0837,   4.1747],
        [ -2.1316,   4.1730]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-16.7987, -20.0469, -19.7387, -19.3745, -12.0673, -16.2227, -13.6946,
         -19.8848, -19.7717, -19.6451],
        [-31.8281, -73.4853, -35.2158, -72.3159, -49.1601, -24.0856,  -9.7368,
         -73.6831, -72.8443, -73.3466],
        [ -7.5505, -17.1481,  -5.8408, -16.1210,  -1.4956,  -8.9247, -13.3778,
         -16.5769, -15.9558, -16.7943],
        [-18.4275, -27.9366, -20.4450, -27.4317, -20.6209, -18.4640, -14.0613,
         -28.1142, -27.9772, -28.0288],
        [ -0.1237, -23.4889,  -3.3233, -21.9322,  -6.6331,  -1.6666,  -3.1579,
         -22.1198, -21.5506, -23.3114],
        [-22.7659, -44.0599, -26.9449, -43.8830, -32.2958, -24.9949, -15.4643,
         -44.0044, -43.5916, -44.3655],
        [-38.1371, -98.3908, -46.2938, -97.3193, -64.0420, -25.5591,  -3.9825,
         -97.7956, -96.9949, -98.2294],
        [  0.3359, -57.9149,   3.2871, -56.0216, -28.8956,   6.3776,  -4.4919,
         -54.1042, -56.2966, -57.3465],
        [ -2.8467,  74.0397,  -7.9308,  71.6465,  73.1253, -16.9797,  42.7868,
          69.1058,  72.5384,  73.5331],
        [-32.5704, -74.1883, -36.3270, -73.2658, -49.5586, -24.2890,  -8.9195,
         -73.9537, -73.6233, -74.3195]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -5.3482,   2.9168,  -6.2359,  -4.4733,  -6.8719,  -2.8548,  21.9702,
         -12.4241,   0.7293,   3.7308],
        [  5.1670,  -2.5962,   6.3253,   4.7663,   6.4400,   2.5643, -21.6527,
          12.4565,  -0.4598,  -4.0620]], device='cuda:0'))])
xi:  [199.09387]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1199.213937737389
W_T_median: 973.2007537524837
W_T_pctile_5: 202.27965763402418
W_T_CVAR_5_pct: 9.679784810152835
Average q (qsum/M+1):  35.0
Optimal xi:  [199.09387]
Expected(across Rb) median(across samples) p_equity:  0.23343009650707244
obj fun:  tensor(-49254.7736, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5000.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1468.5845085689273
Current xi:  [218.6956]
objective value function right now is: -1468.5845085689273
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1508.0468708566502
Current xi:  [208.0993]
objective value function right now is: -1508.0468708566502
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.07176]
objective value function right now is: -1272.0701981797781
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.00912]
objective value function right now is: -1377.9194620056585
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.16928]
objective value function right now is: -919.1293454806716
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1508.4962918468561
Current xi:  [195.32521]
objective value function right now is: -1508.4962918468561
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [198.13937]
objective value function right now is: -1236.3169200134994
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.79335]
objective value function right now is: -1490.9389076331145
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.55995]
objective value function right now is: -1175.657549878173
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.12822]
objective value function right now is: -1084.7230918521461
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [194.95589]
objective value function right now is: 544.3438743292938
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [194.21906]
objective value function right now is: -1397.7336788511775
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [194.23494]
objective value function right now is: -1380.331594847983
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [194.68684]
objective value function right now is: -598.4319710472038
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.51442]
objective value function right now is: -1400.1989126494075
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.54123]
objective value function right now is: -996.5690032100348
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.89131]
objective value function right now is: -1237.3834918847076
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.22237]
objective value function right now is: -1453.5717674768466
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.23405]
objective value function right now is: -1400.8036212985212
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.21112]
objective value function right now is: -535.3790482705637
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.48718]
objective value function right now is: -1323.1628237364152
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.00423]
objective value function right now is: -1122.3313624765644
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.77908]
objective value function right now is: -1265.6614590974189
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [194.65991]
objective value function right now is: -1386.4300547349537
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.31996]
objective value function right now is: -1486.61984362954
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.17256]
objective value function right now is: -1483.6749964911442
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.24379]
objective value function right now is: -1466.6725010813382
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [196.93806]
objective value function right now is: -1343.5047157175363
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [196.62599]
objective value function right now is: -1252.577835251352
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.27646]
objective value function right now is: -421.08049274675744
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.67659]
objective value function right now is: -905.4645249681317
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.25177]
objective value function right now is: -985.660534823397
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.25719]
objective value function right now is: -1381.196225184381
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [202.06573]
objective value function right now is: -1430.0547742218014
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.29697]
objective value function right now is: -1492.4517858339157
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.2547]
objective value function right now is: -1232.0010739282814
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.10754]
objective value function right now is: -968.3937107825099
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.7056]
objective value function right now is: -1425.635339314832
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.6801]
objective value function right now is: -1416.7875822724516
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.79308]
objective value function right now is: -1308.819695142818
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.14055]
objective value function right now is: -1427.167992714744
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.45348]
objective value function right now is: -701.392805211605
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.54514]
objective value function right now is: -1382.3147523100795
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1519.9235309628427
Current xi:  [196.85936]
objective value function right now is: -1519.9235309628427
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.4605]
objective value function right now is: -1491.0970933506428
new min fval from sgd:  -1528.551645998588
new min fval from sgd:  -1548.4775481132071
new min fval from sgd:  -1555.2579157956857
new min fval from sgd:  -1563.2087124417756
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [202.47069]
objective value function right now is: -1470.3046147981686
new min fval from sgd:  -1563.353018147031
new min fval from sgd:  -1563.6070785553463
new min fval from sgd:  -1572.3906868949102
new min fval from sgd:  -1576.3370360437277
new min fval from sgd:  -1579.0762859267713
new min fval from sgd:  -1579.270321498584
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.46475]
objective value function right now is: -1483.3432639158364
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.06274]
objective value function right now is: -1418.231895496266
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.96681]
objective value function right now is: -1481.7766109924157
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.8183]
objective value function right now is: -1388.8549297010566
min fval:  -1579.270321498584
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.4970, -1.7427],
        [-1.1673,  1.4963],
        [ 1.5701, -1.0644],
        [-2.4641,  2.1059],
        [-0.3338,  0.1397],
        [ 2.1413, -1.1565],
        [ 0.5454, -0.5084],
        [ 1.2406, -1.3352],
        [ 1.9783, -1.9946],
        [-1.2161,  1.9984]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.8208,  0.1348,  1.0378, -0.4949,  0.7812,  1.4029,  1.1794,  0.8870,
          2.2442, -0.0638],
        [ 0.5527, -0.4582,  0.2784, -0.7049,  0.1252,  1.0346,  0.3657,  0.6709,
          1.1653, -0.3453],
        [ 1.3354,  0.4262,  1.2211,  0.3413,  0.7227,  1.7589,  0.8399,  1.0659,
          1.9655,  0.2778],
        [ 1.3017,  1.1791,  1.4180,  1.2863,  1.5231,  1.7031,  1.4877,  1.3330,
          1.3058,  1.1665],
        [ 1.5238,  1.4400,  1.3349,  0.9254,  1.5877,  1.5359,  1.6022,  1.4270,
          1.3786,  0.9440],
        [ 1.3091,  0.8196,  1.2640,  0.8598,  1.1985,  1.3510,  1.5658,  1.5829,
          1.4458,  1.0443],
        [ 0.9740,  0.3241,  0.8319, -0.3400,  0.9305,  1.9171,  1.1456,  1.2468,
          2.0805,  0.4806],
        [ 0.0444, -0.5125, -0.3146, -0.7501, -0.3551, -0.0115, -0.0221, -0.0635,
          0.4556, -0.6026],
        [ 0.8611, -0.2184,  0.2399, -0.3945,  0.2673,  1.0255,  0.5371,  0.3126,
          1.7628, -0.3936],
        [ 1.5447,  1.2769,  1.1942,  0.9727,  1.1259,  1.1315,  1.0576,  1.4270,
          1.4459,  1.1421]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.6555, -1.9430, -5.4200, -5.2802, -4.8781, -6.5709, -5.8247, -1.0966,
         -2.3479, -6.2270]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.8656,   2.7725],
        [ -2.5115,   4.3931],
        [ -9.4520,  -0.9876],
        [ -2.4243,   4.3879],
        [ -1.6044,   4.2379],
        [-11.4715,  -2.2743],
        [  8.4936,   3.8320],
        [ -2.4705,   4.3692],
        [ -2.3874,   4.3923],
        [ -2.5089,   4.3898]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -17.1637,  -26.8502,  -46.2374,  -25.4344,  -12.9412,  -44.0235,
          -25.3530,  -26.1365,  -25.5944,  -26.3978],
        [ -45.9747, -106.5968,  -52.4831, -105.3521,  -82.1544,  -41.6921,
          -34.1863, -106.7292, -105.8595, -106.4503],
        [ -17.3287,  -44.9225,  -33.7311,  -43.1660,  -25.9329,  -38.2104,
          -26.8808,  -43.6787,  -42.8128,  -44.5014],
        [ -16.6189,  -27.5616,  -45.6742,  -26.4827,  -16.6242,  -44.8558,
          -27.4010,  -27.3010,  -26.8515,  -27.6128],
        [  -5.2476,  -37.7124,   -1.8942,  -34.2573,  -11.0797,    0.6695,
           -3.3699,  -35.0101,  -33.2527,  -37.4153],
        [ -19.1857,  -31.9480,  -49.7075,  -31.5604,  -21.9663,  -47.1472,
          -29.0944,  -31.7065,  -31.2167,  -32.2354],
        [ -94.7781, -206.4984,  -96.5106, -202.1210, -137.2459,  -51.1824,
           -4.9182, -204.5862, -200.3492, -206.2648],
        [  -5.4532,  -83.0940,    5.0765,  -80.9858,  -50.3359,    8.7706,
           -8.9359,  -78.7453,  -81.3075,  -82.4550],
        [  16.1051,  104.8691,   -3.8988,  102.3218,   98.7353,  -26.0683,
           85.6006,   99.7082,  103.1718,  104.3231],
        [ -56.4201, -143.9983,  -49.4332, -142.6569, -115.3950,  -33.0353,
          -26.6038, -143.5312, -142.8562, -144.1105]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.6516,   1.8632,  -0.0981,  -0.3615,  -7.6533,  -0.3117,  17.7568,
         -11.9676,   0.7002,   2.3627],
        [  0.4705,  -1.5425,   0.1876,   0.6545,   7.2215,   0.0213, -17.4394,
          12.0001,  -0.4307,  -2.6938]], device='cuda:0'))])
xi:  [201.92686]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1236.3047063367533
W_T_median: 1000.4734499382819
W_T_pctile_5: 202.5718819526829
W_T_CVAR_5_pct: 9.887692212267314
Average q (qsum/M+1):  35.0
Optimal xi:  [201.92686]
Expected(across Rb) median(across samples) p_equity:  0.2440051794052124
obj fun:  tensor(-1579.2703, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1079.1475575961167
Current xi:  [220.54932]
objective value function right now is: -1079.1475575961167
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.26674]
objective value function right now is: -1057.8107753499523
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1108.558900147005
Current xi:  [204.1662]
objective value function right now is: -1108.558900147005
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.3514]
objective value function right now is: -1103.137033253891
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1120.8829392343112
Current xi:  [196.0011]
objective value function right now is: -1120.8829392343112
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.22252]
objective value function right now is: -1105.7700185149652
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [195.69118]
objective value function right now is: -1064.0059458950202
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.71716]
objective value function right now is: -1112.0917515596736
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.13007]
objective value function right now is: -1115.859872603023
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.50563]
objective value function right now is: -1110.1949417028475
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.90074]
objective value function right now is: -1111.474119432283
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.40364]
objective value function right now is: -1094.4777244192458
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1129.476696742035
Current xi:  [200.70052]
objective value function right now is: -1129.476696742035
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [202.14375]
objective value function right now is: -1103.9915401922453
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.07631]
objective value function right now is: -1108.0480060710588
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.4933]
objective value function right now is: -1116.1398743823065
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.69681]
objective value function right now is: -1039.282933599625
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [193.40378]
objective value function right now is: -1121.2033933225694
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.14885]
objective value function right now is: -1110.210097037461
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [194.16548]
objective value function right now is: -1070.2856351613227
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.19624]
objective value function right now is: -1118.2877767268735
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.34041]
objective value function right now is: -1129.046115559045
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.68964]
objective value function right now is: -1108.5627957505192
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.22534]
objective value function right now is: -1123.9995500962757
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.36513]
objective value function right now is: -1124.1504130150854
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.51573]
objective value function right now is: -1115.3075354625578
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.40434]
objective value function right now is: -1097.6133893180224
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [197.16483]
objective value function right now is: -1076.628415328141
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [195.27544]
objective value function right now is: -1119.5797868397583
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.6937]
objective value function right now is: -1100.5094133652676
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.78616]
objective value function right now is: -1095.5504895400754
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.41618]
objective value function right now is: -1118.6102215010615
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.18614]
objective value function right now is: -1108.1630042863346
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1129.7959674605436
Current xi:  [198.35667]
objective value function right now is: -1129.7959674605436
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.70946]
objective value function right now is: -1119.272603531986
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.57436]
objective value function right now is: -1107.8885569551294
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.74103]
objective value function right now is: -1114.3387384040607
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.74202]
objective value function right now is: -1129.3989484743513
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.34839]
objective value function right now is: -1112.457322493427
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1130.8934201346194
Current xi:  [198.22614]
objective value function right now is: -1130.8934201346194
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.51566]
objective value function right now is: -1113.081058980743
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.48396]
objective value function right now is: -1109.1291240278604
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.53549]
objective value function right now is: -1107.2004223983288
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.71695]
objective value function right now is: -1112.0363838658288
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.2635]
objective value function right now is: -1089.4795876280218
new min fval from sgd:  -1131.4945985840777
new min fval from sgd:  -1132.520282840034
new min fval from sgd:  -1132.6682275968012
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.9661]
objective value function right now is: -1093.5139447576819
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.6361]
objective value function right now is: -1109.6282886778336
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.0504]
objective value function right now is: -1120.2631900787496
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.60928]
objective value function right now is: -1126.0528042319559
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.341]
objective value function right now is: -1095.958699939077
min fval:  -1132.6682275968012
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.4970, -1.7427],
        [-1.1673,  1.4963],
        [ 1.5701, -1.0644],
        [-2.4641,  2.1059],
        [-0.3338,  0.1393],
        [ 2.1413, -1.1565],
        [ 0.5454, -0.5084],
        [ 1.2406, -1.3352],
        [ 1.9783, -1.9946],
        [-1.2161,  1.9984]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.8208,  0.1345,  1.0378, -0.4950,  0.7812,  1.4029,  1.1794,  0.8870,
          2.2442, -0.0640],
        [ 0.5527, -0.4582,  0.2784, -0.7049,  0.1252,  1.0346,  0.3657,  0.6709,
          1.1653, -0.3453],
        [ 1.3354,  0.4262,  1.2211,  0.3413,  0.7227,  1.7589,  0.8399,  1.0659,
          1.9655,  0.2778],
        [ 1.3017,  1.1791,  1.4180,  1.2863,  1.5231,  1.7031,  1.4877,  1.3330,
          1.3058,  1.1665],
        [ 1.5238,  1.4400,  1.3349,  0.9254,  1.5877,  1.5359,  1.6022,  1.4270,
          1.3786,  0.9440],
        [ 1.3091,  0.8196,  1.2640,  0.8598,  1.1985,  1.3510,  1.5658,  1.5829,
          1.4458,  1.0443],
        [ 0.9740,  0.3241,  0.8319, -0.3400,  0.9305,  1.9171,  1.1456,  1.2468,
          2.0805,  0.4806],
        [ 0.0444, -0.5125, -0.3146, -0.7501, -0.3551, -0.0114, -0.0221, -0.0635,
          0.4557, -0.6026],
        [ 0.8611, -0.2184,  0.2399, -0.3945,  0.2673,  1.0255,  0.5371,  0.3126,
          1.7628, -0.3936],
        [ 1.5447,  1.2769,  1.1942,  0.9727,  1.1259,  1.1315,  1.0576,  1.4270,
          1.4459,  1.1421]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.6555, -1.9432, -5.4200, -5.2802, -4.8781, -6.5709, -5.8247, -1.0966,
         -2.3479, -6.2270]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.8706,   3.5327],
        [ -2.6185,   4.5545],
        [ -9.3099,   0.1106],
        [ -2.5005,   4.5692],
        [ -1.5115,   4.6564],
        [-12.2495,  -2.6296],
        [  9.1274,   4.3007],
        [ -2.5550,   4.5417],
        [ -2.4519,   4.5820],
        [ -2.6144,   4.5519]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -33.6895,  -39.4293,  -85.0659,  -38.2996,  -28.6495,  -80.5810,
          -53.6843,  -38.9385,  -38.5569,  -38.9985],
        [ -62.6264, -124.8657,  -84.3658, -123.6662,  -97.9276,  -71.1587,
          -60.3012, -125.1024, -124.1594, -124.7338],
        [ -34.9245,  -61.2230,  -72.2160,  -59.6385,  -44.0222,  -76.2927,
          -57.1226,  -60.1602,  -59.3297,  -60.8232],
        [ -33.2612,  -39.1822,  -83.2506,  -38.3632,  -31.3863,  -78.9728,
          -53.4462,  -39.1353,  -38.8169,  -39.2546],
        [  -8.8918,  -53.8094,   -3.2568,  -48.1867,  -12.0374,    2.9384,
           -3.4763,  -49.3953,  -46.4803,  -53.3541],
        [ -33.7854,  -42.1493,  -84.7704,  -42.0068,  -35.6899,  -78.9305,
          -53.3976,  -42.1337,  -41.7368,  -42.4605],
        [-154.1546, -318.8258, -143.0334, -310.7758, -199.0831,  -75.7022,
           -4.9581, -314.8039, -307.5197, -318.4362],
        [ -12.8461,  -95.9112,    4.7544,  -93.8977,  -62.3575,    9.9259,
          -13.1193,  -91.3645,  -94.3324,  -95.2391],
        [  30.5807,  125.8723,    5.0396,  122.7435,  111.3044,  -31.2740,
          125.3446,  120.2693,  123.3753,  125.2755],
        [ -80.5465, -214.2746,  -37.8830, -211.0370, -162.8239,  -13.3792,
          -19.8621, -212.3225, -210.6126, -214.2518]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.2670,  -0.3680,  -0.1042,  -0.0992,  -6.6398,  -0.4770,  10.8049,
         -10.6933,   0.6793,  12.4763],
        [  0.0857,   0.6884,   0.1935,   0.3920,   6.2080,   0.1863, -10.4874,
          10.7256,  -0.4098, -12.8075]], device='cuda:0'))])
xi:  [199.46034]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1262.4703674438147
W_T_median: 1017.6488041640322
W_T_pctile_5: 205.48197799446507
W_T_CVAR_5_pct: 9.707546019663534
Average q (qsum/M+1):  35.0
Optimal xi:  [199.46034]
Expected(across Rb) median(across samples) p_equity:  0.25178059935569763
obj fun:  tensor(-1132.6682, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.4970, -1.7427],
        [-1.1673,  1.4963],
        [ 1.5701, -1.0644],
        [-2.4641,  2.1059],
        [-0.3338,  0.1393],
        [ 2.1413, -1.1565],
        [ 0.5454, -0.5084],
        [ 1.2406, -1.3352],
        [ 1.9783, -1.9946],
        [-1.2161,  1.9984]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.8208,  0.1345,  1.0378, -0.4950,  0.7812,  1.4029,  1.1794,  0.8870,
          2.2442, -0.0640],
        [ 0.5527, -0.4582,  0.2784, -0.7049,  0.1252,  1.0346,  0.3657,  0.6709,
          1.1653, -0.3453],
        [ 1.3354,  0.4262,  1.2211,  0.3413,  0.7227,  1.7589,  0.8399,  1.0659,
          1.9655,  0.2778],
        [ 1.3017,  1.1791,  1.4180,  1.2863,  1.5231,  1.7031,  1.4877,  1.3330,
          1.3058,  1.1665],
        [ 1.5238,  1.4400,  1.3349,  0.9254,  1.5877,  1.5359,  1.6022,  1.4270,
          1.3786,  0.9440],
        [ 1.3091,  0.8196,  1.2640,  0.8598,  1.1985,  1.3510,  1.5658,  1.5829,
          1.4458,  1.0443],
        [ 0.9740,  0.3241,  0.8319, -0.3400,  0.9305,  1.9171,  1.1456,  1.2468,
          2.0805,  0.4806],
        [ 0.0444, -0.5125, -0.3146, -0.7501, -0.3551, -0.0114, -0.0221, -0.0635,
          0.4557, -0.6026],
        [ 0.8611, -0.2184,  0.2399, -0.3945,  0.2673,  1.0255,  0.5371,  0.3126,
          1.7628, -0.3936],
        [ 1.5447,  1.2769,  1.1942,  0.9727,  1.1259,  1.1315,  1.0576,  1.4270,
          1.4459,  1.1421]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.6555, -1.9432, -5.4200, -5.2802, -4.8781, -6.5709, -5.8247, -1.0966,
         -2.3479, -6.2270]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.8706,   3.5327],
        [ -2.6185,   4.5545],
        [ -9.3099,   0.1106],
        [ -2.5005,   4.5692],
        [ -1.5115,   4.6564],
        [-12.2495,  -2.6296],
        [  9.1274,   4.3007],
        [ -2.5550,   4.5417],
        [ -2.4519,   4.5820],
        [ -2.6144,   4.5519]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -33.6895,  -39.4293,  -85.0659,  -38.2996,  -28.6495,  -80.5810,
          -53.6843,  -38.9385,  -38.5569,  -38.9985],
        [ -62.6264, -124.8657,  -84.3658, -123.6662,  -97.9276,  -71.1587,
          -60.3012, -125.1024, -124.1594, -124.7338],
        [ -34.9245,  -61.2230,  -72.2160,  -59.6385,  -44.0222,  -76.2927,
          -57.1226,  -60.1602,  -59.3297,  -60.8232],
        [ -33.2612,  -39.1822,  -83.2506,  -38.3632,  -31.3863,  -78.9728,
          -53.4462,  -39.1353,  -38.8169,  -39.2546],
        [  -8.8918,  -53.8094,   -3.2568,  -48.1867,  -12.0374,    2.9384,
           -3.4763,  -49.3953,  -46.4803,  -53.3541],
        [ -33.7854,  -42.1493,  -84.7704,  -42.0068,  -35.6899,  -78.9305,
          -53.3976,  -42.1337,  -41.7368,  -42.4605],
        [-154.1546, -318.8258, -143.0334, -310.7758, -199.0831,  -75.7022,
           -4.9581, -314.8039, -307.5197, -318.4362],
        [ -12.8461,  -95.9112,    4.7544,  -93.8977,  -62.3575,    9.9259,
          -13.1193,  -91.3645,  -94.3324,  -95.2391],
        [  30.5807,  125.8723,    5.0396,  122.7435,  111.3044,  -31.2740,
          125.3446,  120.2693,  123.3753,  125.2755],
        [ -80.5465, -214.2746,  -37.8830, -211.0370, -162.8239,  -13.3792,
          -19.8621, -212.3225, -210.6126, -214.2518]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.2670,  -0.3680,  -0.1042,  -0.0992,  -6.6398,  -0.4770,  10.8049,
         -10.6933,   0.6793,  12.4763],
        [  0.0857,   0.6884,   0.1935,   0.3920,   6.2080,   0.1863, -10.4874,
          10.7256,  -0.4098, -12.8075]], device='cuda:0'))])
loaded xi:  199.46034
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1101.59932889741
Current xi:  [200.36554]
objective value function right now is: -1101.59932889741
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1111.337206537239
Current xi:  [201.44519]
objective value function right now is: -1111.337206537239
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.51161]
objective value function right now is: -1097.3924839861056
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.9421]
objective value function right now is: -1060.4518868892342
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.24644]
objective value function right now is: -1101.7924634805413
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.08897]
objective value function right now is: -1104.9289280061619
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [198.78226]
objective value function right now is: -1105.760218292449
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.34795]
objective value function right now is: -1093.9571846819672
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.60803]
objective value function right now is: -1098.543177378285
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.71089]
objective value function right now is: -1105.7009573550565
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.02867]
objective value function right now is: -1103.0375426101189
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.972]
objective value function right now is: -1103.2286788388412
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.39102]
objective value function right now is: -1107.5236775790538
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [194.76646]
objective value function right now is: -1110.247352057038
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.55412]
objective value function right now is: -1073.2445854709065
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.39305]
objective value function right now is: -1100.1483470760909
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.04382]
objective value function right now is: -1101.929885665181
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.10219]
objective value function right now is: -1105.6841518604824
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [202.27808]
objective value function right now is: -1094.0831701487239
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.52487]
objective value function right now is: -1095.1479428995258
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.32336]
objective value function right now is: -1097.3615211869865
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.61754]
objective value function right now is: -1082.5832100442515
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.62726]
objective value function right now is: -1091.6031576811001
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.147]
objective value function right now is: -1083.2282502054684
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.16837]
objective value function right now is: -1079.3747421609935
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.60304]
objective value function right now is: -1091.1917672215807
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.14989]
objective value function right now is: -1107.7888593464688
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [198.1931]
objective value function right now is: -1097.131494671358
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [196.48372]
objective value function right now is: -1082.4573957207065
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.38464]
objective value function right now is: -1090.0402350368938
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1112.567404856273
Current xi:  [198.10164]
objective value function right now is: -1112.567404856273
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.12434]
objective value function right now is: -1091.63773220683
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.18828]
objective value function right now is: -1111.7845934075037
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.73788]
objective value function right now is: -1099.3784558628488
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.50305]
objective value function right now is: -1086.6447208410352
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.56233]
objective value function right now is: -1107.6128834871179
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.87401]
objective value function right now is: -1098.5600971586196
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.54439]
objective value function right now is: -1089.2537099807234
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.57808]
objective value function right now is: -1085.6631065193258
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.4482]
objective value function right now is: -1096.9906025304506
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.9579]
objective value function right now is: -1104.9781214215257
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [194.6315]
objective value function right now is: -1106.5915222880458
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.97432]
objective value function right now is: -1103.1064067935201
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.02994]
objective value function right now is: -1110.032213310181
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.55626]
objective value function right now is: -1084.307955223267
new min fval from sgd:  -1112.7100622124308
new min fval from sgd:  -1113.5788681402105
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.38052]
objective value function right now is: -1105.620523293903
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.74388]
objective value function right now is: -1106.8558880122052
new min fval from sgd:  -1113.5838002088874
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.33205]
objective value function right now is: -1109.5420679703045
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.83792]
objective value function right now is: -1100.0357059442606
new min fval from sgd:  -1113.9507424000237
new min fval from sgd:  -1113.9972761509905
new min fval from sgd:  -1114.2697052137912
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.42111]
objective value function right now is: -1112.563610194636
min fval:  -1114.2697052137912
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.4970, -1.7427],
        [-1.1673,  1.4963],
        [ 1.5701, -1.0644],
        [-2.4641,  2.1059],
        [-0.3338,  0.1389],
        [ 2.1413, -1.1565],
        [ 0.5454, -0.5084],
        [ 1.2406, -1.3352],
        [ 1.9783, -1.9946],
        [-1.2161,  1.9984]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.8208,  0.1342,  1.0378, -0.4950,  0.7812,  1.4029,  1.1794,  0.8870,
          2.2442, -0.0642],
        [ 0.5527, -0.4582,  0.2784, -0.7049,  0.1252,  1.0346,  0.3657,  0.6709,
          1.1653, -0.3453],
        [ 1.3354,  0.4262,  1.2211,  0.3413,  0.7227,  1.7589,  0.8399,  1.0659,
          1.9655,  0.2778],
        [ 1.3017,  1.1791,  1.4180,  1.2863,  1.5231,  1.7031,  1.4877,  1.3330,
          1.3058,  1.1665],
        [ 1.5238,  1.4400,  1.3349,  0.9254,  1.5877,  1.5359,  1.6022,  1.4270,
          1.3786,  0.9440],
        [ 1.3091,  0.8196,  1.2640,  0.8598,  1.1985,  1.3510,  1.5658,  1.5829,
          1.4458,  1.0443],
        [ 0.9740,  0.3241,  0.8319, -0.3400,  0.9305,  1.9171,  1.1456,  1.2468,
          2.0805,  0.4806],
        [ 0.0444, -0.5125, -0.3146, -0.7501, -0.3551, -0.0114, -0.0221, -0.0635,
          0.4557, -0.6026],
        [ 0.8611, -0.2185,  0.2399, -0.3945,  0.2673,  1.0255,  0.5371,  0.3126,
          1.7628, -0.3936],
        [ 1.5447,  1.2769,  1.1942,  0.9727,  1.1259,  1.1315,  1.0576,  1.4270,
          1.4459,  1.1421]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.6555, -1.9432, -5.4200, -5.2802, -4.8781, -6.5709, -5.8247, -1.0966,
         -2.3479, -6.2270]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.2715,   3.9681],
        [ -2.8321,   4.8000],
        [ -9.7253,  -0.2187],
        [ -2.6673,   4.8091],
        [ -0.8023,   4.8330],
        [-12.6685,  -2.5854],
        [  9.8947,   4.6987],
        [ -2.7384,   4.7890],
        [ -2.5973,   4.8180],
        [ -2.8258,   4.7981]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -22.2975,  -21.7607,  -94.8227,  -20.9261,  -15.2333,  -94.4802,
          -54.1193,  -21.4788,  -21.2943,  -21.3471],
        [ -51.2049, -107.7903,  -92.9433, -106.8540,  -84.6985,  -84.5262,
          -59.8124, -108.1937, -107.4520, -107.6698],
        [ -23.6092,  -44.0143,  -81.5476,  -42.7110,  -30.8612,  -90.1342,
          -57.4144,  -43.1384,  -42.5122,  -43.6284],
        [ -21.8242,  -21.2839,  -93.1048,  -20.7727,  -17.8571,  -92.9146,
          -53.9733,  -21.4577,  -21.3412,  -21.3743],
        [  -9.3439,  -64.8848,   -4.0787,  -56.8871,   -9.1376,    2.2422,
           -2.9955,  -58.6361,  -54.3454,  -64.2653],
        [ -22.2883,  -24.1552,  -94.5451,  -24.3221,  -22.0795,  -92.7902,
          -53.8052,  -24.3623,  -24.1666,  -24.4846],
        [-208.9863, -434.8632, -185.6849, -422.3278, -245.5019, -100.3081,
           -4.1221, -427.9607, -417.2673, -434.2522],
        [ -21.6286, -110.9438,    5.9263, -109.0432,  -77.8299,    8.2326,
          -13.7553, -106.2407, -109.5896, -110.2435],
        [  41.1375,  144.2624,    8.5424,  140.4112,  120.5383,  -35.0540,
          173.5791,  138.1024,  140.7766,  143.6051],
        [-199.6647, -380.8747,  -68.8930, -375.1327, -296.6488,  -10.4758,
          -20.0401, -377.1490, -373.7579, -380.7036]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -4.1068,  -3.6469,  -3.6442,  -3.8046,  -5.0768,  -4.1835,   7.7247,
         -11.9832,   0.7058,  16.5058],
        [  3.9254,   3.9673,   3.7335,   4.0974,   4.6450,   3.8929,  -7.4072,
          12.0156,  -0.4363, -16.8370]], device='cuda:0'))])
xi:  [198.16766]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1225.5975226363144
W_T_median: 991.6115141367586
W_T_pctile_5: 199.37710794633236
W_T_CVAR_5_pct: 9.763601668206087
Average q (qsum/M+1):  35.0
Optimal xi:  [198.16766]
Expected(across Rb) median(across samples) p_equity:  0.24194756547609966
obj fun:  tensor(-1114.2697, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.4970, -1.7427],
        [-1.1673,  1.4963],
        [ 1.5701, -1.0644],
        [-2.4641,  2.1059],
        [-0.3338,  0.1389],
        [ 2.1413, -1.1565],
        [ 0.5454, -0.5084],
        [ 1.2406, -1.3352],
        [ 1.9783, -1.9946],
        [-1.2161,  1.9984]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.8208,  0.1342,  1.0378, -0.4950,  0.7812,  1.4029,  1.1794,  0.8870,
          2.2442, -0.0642],
        [ 0.5527, -0.4582,  0.2784, -0.7049,  0.1252,  1.0346,  0.3657,  0.6709,
          1.1653, -0.3453],
        [ 1.3354,  0.4262,  1.2211,  0.3413,  0.7227,  1.7589,  0.8399,  1.0659,
          1.9655,  0.2778],
        [ 1.3017,  1.1791,  1.4180,  1.2863,  1.5231,  1.7031,  1.4877,  1.3330,
          1.3058,  1.1665],
        [ 1.5238,  1.4400,  1.3349,  0.9254,  1.5877,  1.5359,  1.6022,  1.4270,
          1.3786,  0.9440],
        [ 1.3091,  0.8196,  1.2640,  0.8598,  1.1985,  1.3510,  1.5658,  1.5829,
          1.4458,  1.0443],
        [ 0.9740,  0.3241,  0.8319, -0.3400,  0.9305,  1.9171,  1.1456,  1.2468,
          2.0805,  0.4806],
        [ 0.0444, -0.5125, -0.3146, -0.7501, -0.3551, -0.0114, -0.0221, -0.0635,
          0.4557, -0.6026],
        [ 0.8611, -0.2185,  0.2399, -0.3945,  0.2673,  1.0255,  0.5371,  0.3126,
          1.7628, -0.3936],
        [ 1.5447,  1.2769,  1.1942,  0.9727,  1.1259,  1.1315,  1.0576,  1.4270,
          1.4459,  1.1421]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.6555, -1.9432, -5.4200, -5.2802, -4.8781, -6.5709, -5.8247, -1.0966,
         -2.3479, -6.2270]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.2715,   3.9681],
        [ -2.8321,   4.8000],
        [ -9.7253,  -0.2187],
        [ -2.6673,   4.8091],
        [ -0.8023,   4.8330],
        [-12.6685,  -2.5854],
        [  9.8947,   4.6987],
        [ -2.7384,   4.7890],
        [ -2.5973,   4.8180],
        [ -2.8258,   4.7981]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -22.2975,  -21.7607,  -94.8227,  -20.9261,  -15.2333,  -94.4802,
          -54.1193,  -21.4788,  -21.2943,  -21.3471],
        [ -51.2049, -107.7903,  -92.9433, -106.8540,  -84.6985,  -84.5262,
          -59.8124, -108.1937, -107.4520, -107.6698],
        [ -23.6092,  -44.0143,  -81.5476,  -42.7110,  -30.8612,  -90.1342,
          -57.4144,  -43.1384,  -42.5122,  -43.6284],
        [ -21.8242,  -21.2839,  -93.1048,  -20.7727,  -17.8571,  -92.9146,
          -53.9733,  -21.4577,  -21.3412,  -21.3743],
        [  -9.3439,  -64.8848,   -4.0787,  -56.8871,   -9.1376,    2.2422,
           -2.9955,  -58.6361,  -54.3454,  -64.2653],
        [ -22.2883,  -24.1552,  -94.5451,  -24.3221,  -22.0795,  -92.7902,
          -53.8052,  -24.3623,  -24.1666,  -24.4846],
        [-208.9863, -434.8632, -185.6849, -422.3278, -245.5019, -100.3081,
           -4.1221, -427.9607, -417.2673, -434.2522],
        [ -21.6286, -110.9438,    5.9263, -109.0432,  -77.8299,    8.2326,
          -13.7553, -106.2407, -109.5896, -110.2435],
        [  41.1375,  144.2624,    8.5424,  140.4112,  120.5383,  -35.0540,
          173.5791,  138.1024,  140.7766,  143.6051],
        [-199.6647, -380.8747,  -68.8930, -375.1327, -296.6488,  -10.4758,
          -20.0401, -377.1490, -373.7579, -380.7036]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -4.1068,  -3.6469,  -3.6442,  -3.8046,  -5.0768,  -4.1835,   7.7247,
         -11.9832,   0.7058,  16.5058],
        [  3.9254,   3.9673,   3.7335,   4.0974,   4.6450,   3.8929,  -7.4072,
          12.0156,  -0.4363, -16.8370]], device='cuda:0'))])
loaded xi:  198.16766
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1097.9256571904039
Current xi:  [198.74977]
objective value function right now is: -1097.9256571904039
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.77167]
objective value function right now is: -1096.8466490098854
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.80672]
objective value function right now is: -1090.859085014461
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.27585]
objective value function right now is: -1088.5841492287489
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.0996]
objective value function right now is: -1096.4723903934146
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.86597]
objective value function right now is: -1096.0085037573776
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [197.53822]
objective value function right now is: -1078.5629411720709
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.41023]
objective value function right now is: -1092.9934584001633
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.53993]
objective value function right now is: -1092.7167272797585
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.50974]
objective value function right now is: -1091.8068705538008
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.4551]
objective value function right now is: -1096.3252082478964
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.67902]
objective value function right now is: -1096.7772826511218
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1097.9666666341834
Current xi:  [199.23444]
objective value function right now is: -1097.9666666341834
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1099.503771316045
Current xi:  [198.24953]
objective value function right now is: -1099.503771316045
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.69286]
objective value function right now is: -1093.191000658881
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.25711]
objective value function right now is: -1091.0668066190285
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.6664]
objective value function right now is: -1093.2433770456553
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.2317]
objective value function right now is: -1091.508781931011
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.7013]
objective value function right now is: -1097.1701480287809
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.22478]
objective value function right now is: -1089.1370325525752
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.52708]
objective value function right now is: -1089.9038271728548
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.01273]
objective value function right now is: -1083.0750466347886
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.94328]
objective value function right now is: -1083.3246744897738
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.59798]
objective value function right now is: -1090.1229211709592
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.05316]
objective value function right now is: -1095.0805362416313
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.25793]
objective value function right now is: -1092.637875263575
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.65143]
objective value function right now is: -1095.3743839696158
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [199.44928]
objective value function right now is: -1092.5896843450116
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [198.15831]
objective value function right now is: -1094.2990279048292
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.687]
objective value function right now is: -1098.0694826012073
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.53781]
objective value function right now is: -1092.4917718169147
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.3065]
objective value function right now is: -1093.3065561981305
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.26161]
objective value function right now is: -1097.6691478592625
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.74593]
objective value function right now is: -1095.602940748223
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.93001]
objective value function right now is: -1095.5546058807877
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.92336]
objective value function right now is: -1099.1825382643115
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.45073]
objective value function right now is: -1092.5171746164108
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.23997]
objective value function right now is: -1092.0675236067332
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.9236]
objective value function right now is: -1090.8878518036827
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.3549]
objective value function right now is: -1090.8031550743056
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.25827]
objective value function right now is: -1094.3087251052737
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.53114]
objective value function right now is: -1090.8851812682785
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.05904]
objective value function right now is: -1083.3438895746435
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.54842]
objective value function right now is: -1084.568735464938
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.28842]
objective value function right now is: -1076.6943742634562
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.46626]
objective value function right now is: -1097.3679176047667
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.63176]
objective value function right now is: -1080.294912155529
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.82153]
objective value function right now is: -1089.5145134268566
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [202.42802]
objective value function right now is: -1089.3500085447863
new min fval from sgd:  -1099.521019288231
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.74763]
objective value function right now is: -1089.1072436127401
min fval:  -1099.521019288231
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.4970, -1.7427],
        [-1.1673,  1.4963],
        [ 1.5701, -1.0644],
        [-2.4641,  2.1059],
        [-0.3338,  0.1384],
        [ 2.1413, -1.1565],
        [ 0.5454, -0.5084],
        [ 1.2406, -1.3352],
        [ 1.9783, -1.9946],
        [-1.2161,  1.9984]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.8208,  0.1339,  1.0378, -0.4950,  0.7812,  1.4029,  1.1794,  0.8870,
          2.2442, -0.0644],
        [ 0.5527, -0.4582,  0.2784, -0.7049,  0.1252,  1.0346,  0.3657,  0.6709,
          1.1653, -0.3453],
        [ 1.3354,  0.4262,  1.2211,  0.3413,  0.7227,  1.7589,  0.8399,  1.0659,
          1.9655,  0.2778],
        [ 1.3017,  1.1791,  1.4180,  1.2863,  1.5231,  1.7031,  1.4877,  1.3330,
          1.3058,  1.1665],
        [ 1.5238,  1.4400,  1.3349,  0.9254,  1.5877,  1.5359,  1.6022,  1.4270,
          1.3786,  0.9440],
        [ 1.3091,  0.8196,  1.2640,  0.8598,  1.1985,  1.3510,  1.5658,  1.5829,
          1.4458,  1.0443],
        [ 0.9740,  0.3241,  0.8319, -0.3400,  0.9305,  1.9171,  1.1456,  1.2468,
          2.0805,  0.4806],
        [ 0.0444, -0.5125, -0.3146, -0.7501, -0.3551, -0.0114, -0.0221, -0.0635,
          0.4557, -0.6026],
        [ 0.8611, -0.2185,  0.2399, -0.3945,  0.2673,  1.0255,  0.5371,  0.3126,
          1.7628, -0.3936],
        [ 1.5447,  1.2769,  1.1942,  0.9727,  1.1259,  1.1315,  1.0576,  1.4270,
          1.4459,  1.1421]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.6555, -1.9432, -5.4200, -5.2802, -4.8781, -6.5709, -5.8247, -1.0966,
         -2.3479, -6.2270]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.8009,   4.2112],
        [ -2.6405,   5.0556],
        [ -9.9712,  -0.4700],
        [ -2.4732,   5.0630],
        [ -0.8928,   5.1925],
        [-12.3962,  -2.5489],
        [ 10.3256,   5.2135],
        [ -2.5387,   5.0448],
        [ -2.4018,   5.0704],
        [ -2.6332,   5.0536]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[   3.6457,    4.6120, -101.1429,    5.3481,   10.1756, -110.2730,
          -55.9881,    4.8530,    4.9379,    5.0248],
        [ -23.9347,  -72.7817,  -99.9254,  -72.3748,  -57.3679, -102.3711,
          -62.6953,  -73.5837,  -73.1858,  -72.6954],
        [   4.8592,   -8.0431,  -88.3353,   -7.2673,   -2.5430, -107.0487,
          -59.9678,   -7.5597,   -7.2816,   -7.6898],
        [   3.5605,    4.2921,  -99.5262,    4.6328,    7.4385, -108.9471,
          -56.0420,    4.0233,    4.0008,    4.1959],
        [ -11.9602,  -76.9917,   -3.5212,  -66.3841,   -8.4072,    2.7140,
           -2.9394,  -68.8439,  -62.8088,  -76.2121],
        [   4.5255,    5.7124, -100.8729,    5.1839,    3.4775, -108.7096,
          -56.3556,    5.2493,    5.2035,    5.3624],
        [-255.4746, -539.4113, -225.9256, -522.1897, -268.6475, -122.5683,
           -4.4747, -529.3607, -515.1644, -538.5514],
        [ -30.2172, -123.2381,    5.6375, -121.5437,  -93.9573,    6.9619,
          -17.0279, -118.4901, -122.2201, -122.5204],
        [  54.9815,  168.0837,   11.8904,  163.4649,  135.8923,  -34.4539,
          217.7148,  161.3039,  163.5413,  167.3638],
        [-314.9834, -529.1906,  -95.7328, -521.8366, -408.4284,  -10.4624,
          -21.5354, -524.2736, -519.7966, -528.9173]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -7.1575,  -5.7152,  -6.1756,  -6.7769,  -4.0134,  -7.1680,   6.3185,
         -13.3259,   0.6900,  17.5721],
        [  6.9761,   6.0355,   6.2648,   7.0698,   3.5817,   6.8773,  -6.0010,
          13.3584,  -0.4204, -17.9033]], device='cuda:0'))])
xi:  [201.60112]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1246.2278515585
W_T_median: 1005.7082452581666
W_T_pctile_5: 202.23016553389417
W_T_CVAR_5_pct: 9.68372440038715
Average q (qsum/M+1):  35.0
Optimal xi:  [201.60112]
Expected(across Rb) median(across samples) p_equity:  0.24779567768176397
obj fun:  tensor(-1099.5210, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.4970, -1.7427],
        [-1.1673,  1.4963],
        [ 1.5701, -1.0644],
        [-2.4641,  2.1059],
        [-0.3338,  0.1384],
        [ 2.1413, -1.1565],
        [ 0.5454, -0.5084],
        [ 1.2406, -1.3352],
        [ 1.9783, -1.9946],
        [-1.2161,  1.9984]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.8208,  0.1339,  1.0378, -0.4950,  0.7812,  1.4029,  1.1794,  0.8870,
          2.2442, -0.0644],
        [ 0.5527, -0.4582,  0.2784, -0.7049,  0.1252,  1.0346,  0.3657,  0.6709,
          1.1653, -0.3453],
        [ 1.3354,  0.4262,  1.2211,  0.3413,  0.7227,  1.7589,  0.8399,  1.0659,
          1.9655,  0.2778],
        [ 1.3017,  1.1791,  1.4180,  1.2863,  1.5231,  1.7031,  1.4877,  1.3330,
          1.3058,  1.1665],
        [ 1.5238,  1.4400,  1.3349,  0.9254,  1.5877,  1.5359,  1.6022,  1.4270,
          1.3786,  0.9440],
        [ 1.3091,  0.8196,  1.2640,  0.8598,  1.1985,  1.3510,  1.5658,  1.5829,
          1.4458,  1.0443],
        [ 0.9740,  0.3241,  0.8319, -0.3400,  0.9305,  1.9171,  1.1456,  1.2468,
          2.0805,  0.4806],
        [ 0.0444, -0.5125, -0.3146, -0.7501, -0.3551, -0.0114, -0.0221, -0.0635,
          0.4557, -0.6026],
        [ 0.8611, -0.2185,  0.2399, -0.3945,  0.2673,  1.0255,  0.5371,  0.3126,
          1.7628, -0.3936],
        [ 1.5447,  1.2769,  1.1942,  0.9727,  1.1259,  1.1315,  1.0576,  1.4270,
          1.4459,  1.1421]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.6555, -1.9432, -5.4200, -5.2802, -4.8781, -6.5709, -5.8247, -1.0966,
         -2.3479, -6.2270]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.8009,   4.2112],
        [ -2.6405,   5.0556],
        [ -9.9712,  -0.4700],
        [ -2.4732,   5.0630],
        [ -0.8928,   5.1925],
        [-12.3962,  -2.5489],
        [ 10.3256,   5.2135],
        [ -2.5387,   5.0448],
        [ -2.4018,   5.0704],
        [ -2.6332,   5.0536]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[   3.6457,    4.6120, -101.1429,    5.3481,   10.1756, -110.2730,
          -55.9881,    4.8530,    4.9379,    5.0248],
        [ -23.9347,  -72.7817,  -99.9254,  -72.3748,  -57.3679, -102.3711,
          -62.6953,  -73.5837,  -73.1858,  -72.6954],
        [   4.8592,   -8.0431,  -88.3353,   -7.2673,   -2.5430, -107.0487,
          -59.9678,   -7.5597,   -7.2816,   -7.6898],
        [   3.5605,    4.2921,  -99.5262,    4.6328,    7.4385, -108.9471,
          -56.0420,    4.0233,    4.0008,    4.1959],
        [ -11.9602,  -76.9917,   -3.5212,  -66.3841,   -8.4072,    2.7140,
           -2.9394,  -68.8439,  -62.8088,  -76.2121],
        [   4.5255,    5.7124, -100.8729,    5.1839,    3.4775, -108.7096,
          -56.3556,    5.2493,    5.2035,    5.3624],
        [-255.4746, -539.4113, -225.9256, -522.1897, -268.6475, -122.5683,
           -4.4747, -529.3607, -515.1644, -538.5514],
        [ -30.2172, -123.2381,    5.6375, -121.5437,  -93.9573,    6.9619,
          -17.0279, -118.4901, -122.2201, -122.5204],
        [  54.9815,  168.0837,   11.8904,  163.4649,  135.8923,  -34.4539,
          217.7148,  161.3039,  163.5413,  167.3638],
        [-314.9834, -529.1906,  -95.7328, -521.8366, -408.4284,  -10.4624,
          -21.5354, -524.2736, -519.7966, -528.9173]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -7.1575,  -5.7152,  -6.1756,  -6.7769,  -4.0134,  -7.1680,   6.3185,
         -13.3259,   0.6900,  17.5721],
        [  6.9761,   6.0355,   6.2648,   7.0698,   3.5817,   6.8773,  -6.0010,
          13.3584,  -0.4204, -17.9033]], device='cuda:0'))])
loaded xi:  201.60112
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1089.3671822639756
Current xi:  [198.60461]
objective value function right now is: -1089.3671822639756
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.73277]
objective value function right now is: -1088.888913374305
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.73967]
objective value function right now is: -1077.991066832113
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1091.1488868572421
Current xi:  [198.59448]
objective value function right now is: -1091.1488868572421
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1091.195225069167
Current xi:  [197.03748]
objective value function right now is: -1091.195225069167
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1092.9365832295287
Current xi:  [197.88857]
objective value function right now is: -1092.9365832295287
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [200.69534]
objective value function right now is: -1088.5230647831083
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.7975]
objective value function right now is: -1090.508275265005
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.56827]
objective value function right now is: -1084.0061787828386
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.7692]
objective value function right now is: -1090.7182591482165
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.86224]
objective value function right now is: -1090.5960301133318
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.5413]
objective value function right now is: -1091.8928079759721
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.29445]
objective value function right now is: -1092.3028471019995
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1092.9426733785729
Current xi:  [199.99997]
objective value function right now is: -1092.9426733785729
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.75203]
objective value function right now is: -1086.3868002742986
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.50954]
objective value function right now is: -1091.7962354808656
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.60722]
objective value function right now is: -1089.1481317743196
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.428]
objective value function right now is: -1092.3474571400268
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1094.5106323876032
Current xi:  [200.02441]
objective value function right now is: -1094.5106323876032
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.64026]
objective value function right now is: -1086.3129607223843
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.34752]
objective value function right now is: -1077.4721236180976
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.7479]
objective value function right now is: -1088.2735136265032
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.43784]
objective value function right now is: -1092.4769649813582
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.66235]
objective value function right now is: -1083.5784481555038
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.99423]
objective value function right now is: -1093.2925020698713
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.17776]
objective value function right now is: -1094.2363909124797
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.01956]
objective value function right now is: -1075.4202948537243
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [196.69196]
objective value function right now is: -1089.2088773728397
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [196.29788]
objective value function right now is: -1088.5533604547663
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.29387]
objective value function right now is: -1092.2310246670982
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.44632]
objective value function right now is: -1089.4441190519146
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.10144]
objective value function right now is: -1087.300430280996
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.20619]
objective value function right now is: -1093.3033337173601
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.11224]
objective value function right now is: -1093.1541472101997
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.44563]
objective value function right now is: -1092.6646366556674
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.96948]
objective value function right now is: -1092.2309528223736
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.51328]
objective value function right now is: -1091.4971367302057
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.72734]
objective value function right now is: -1090.5599578040392
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.02466]
objective value function right now is: -1088.4116084371585
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.1683]
objective value function right now is: -1089.301839193525
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.67165]
objective value function right now is: -1090.8388517146352
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.54584]
objective value function right now is: -1081.6597995338325
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.63663]
objective value function right now is: -1089.822529293141
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.11423]
objective value function right now is: -1081.8447607441396
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.0059]
objective value function right now is: -1090.6698954947983
new min fval from sgd:  -1094.6152604043277
new min fval from sgd:  -1094.7430227844584
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.12401]
objective value function right now is: -1092.412301208757
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.54475]
objective value function right now is: -1092.4097041497034
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.28712]
objective value function right now is: -1087.6158790760342
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.24782]
objective value function right now is: -1092.3963302678242
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.67528]
objective value function right now is: -1091.460924570674
min fval:  -1094.7430227844584
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.4970, -1.7427],
        [-1.1673,  1.4963],
        [ 1.5701, -1.0644],
        [-2.4641,  2.1059],
        [-0.3338,  0.1380],
        [ 2.1413, -1.1565],
        [ 0.5454, -0.5084],
        [ 1.2406, -1.3352],
        [ 1.9783, -1.9946],
        [-1.2161,  1.9984]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.8208,  0.1336,  1.0378, -0.4950,  0.7812,  1.4029,  1.1794,  0.8870,
          2.2442, -0.0646],
        [ 0.5527, -0.4582,  0.2784, -0.7049,  0.1252,  1.0346,  0.3657,  0.6709,
          1.1653, -0.3453],
        [ 1.3354,  0.4262,  1.2211,  0.3413,  0.7227,  1.7589,  0.8399,  1.0659,
          1.9655,  0.2778],
        [ 1.3017,  1.1791,  1.4180,  1.2863,  1.5231,  1.7031,  1.4877,  1.3330,
          1.3058,  1.1665],
        [ 1.5238,  1.4400,  1.3349,  0.9254,  1.5877,  1.5359,  1.6022,  1.4270,
          1.3786,  0.9440],
        [ 1.3091,  0.8196,  1.2640,  0.8598,  1.1985,  1.3510,  1.5658,  1.5829,
          1.4458,  1.0443],
        [ 0.9740,  0.3241,  0.8319, -0.3400,  0.9305,  1.9171,  1.1456,  1.2468,
          2.0805,  0.4806],
        [ 0.0444, -0.5125, -0.3146, -0.7501, -0.3551, -0.0114, -0.0221, -0.0635,
          0.4557, -0.6026],
        [ 0.8611, -0.2185,  0.2399, -0.3945,  0.2673,  1.0255,  0.5371,  0.3126,
          1.7628, -0.3936],
        [ 1.5447,  1.2769,  1.1942,  0.9727,  1.1259,  1.1315,  1.0576,  1.4270,
          1.4459,  1.1421]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.6555, -1.9432, -5.4200, -5.2802, -4.8781, -6.5709, -5.8247, -1.0966,
         -2.3479, -6.2270]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.4779,   4.4582],
        [ -2.9888,   5.1218],
        [-10.1860,  -0.4910],
        [ -2.8329,   5.1550],
        [ -0.7332,   5.2188],
        [-13.4368,  -2.6269],
        [  9.9500,   4.9186],
        [ -2.8977,   5.1327],
        [ -2.7608,   5.1695],
        [ -2.9829,   5.1219]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[  11.7059,    1.8240, -101.3053,    3.2968,   10.6241, -125.4015,
          -56.6139,    2.7285,    3.1323,    2.3012],
        [  -3.2439,  -45.3384, -103.1207,  -44.8810,  -37.5696, -121.2067,
          -64.9848,  -46.0821,  -45.6790,  -45.2452],
        [  18.2673,    2.2605,  -89.7200,    3.1457,    6.4371, -123.7380,
          -61.0242,    2.8310,    3.1676,    2.6227],
        [  12.1582,    2.9413,  -99.7761,    3.7678,    7.6133, -124.2912,
          -56.7718,    3.1142,    3.2861,    2.8894],
        [ -13.1116,  -89.3904,   -3.3058,  -76.2337,   -7.8924,    1.3510,
           -2.7172,  -79.2679,  -71.6002,  -88.4429],
        [  13.9301,    3.3283, -101.1006,    3.5577,    4.6686, -124.0594,
          -56.8780,    3.4905,    3.8749,    3.0313],
        [-293.7281, -650.9481, -259.2299, -629.1776, -294.3480, -141.7025,
           -4.3882, -637.7848, -620.1653, -649.8425],
        [ -33.6231, -130.6710,    5.3514, -129.1147, -105.1336,    6.8993,
          -16.6707, -125.8641, -129.8779, -129.9368],
        [  61.1800,  186.6925,   11.6953,  181.0107,  145.7384,  -36.3771,
          249.2027,  179.0892,  180.6693,  185.8967],
        [-417.7028, -687.3167, -126.6493, -677.1349, -523.9489,   -7.6882,
          -23.2825, -680.2225, -673.9100, -686.8533]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -5.8763,  -3.4458,  -4.3642,  -5.4448,  -4.2507,  -5.7936,   6.6472,
         -14.0272,   0.7117,  19.3615],
        [  5.6949,   3.7662,   4.4535,   5.7376,   3.8191,   5.5029,  -6.3297,
          14.0597,  -0.4421, -19.6925]], device='cuda:0'))])
xi:  [197.95192]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1220.798740978466
W_T_median: 989.0131119654731
W_T_pctile_5: 198.331234071273
W_T_CVAR_5_pct: 9.744888251604426
Average q (qsum/M+1):  35.0
Optimal xi:  [197.95192]
Expected(across Rb) median(across samples) p_equity:  0.23978811502456665
obj fun:  tensor(-1094.7430, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.4970, -1.7427],
        [-1.1673,  1.4963],
        [ 1.5701, -1.0644],
        [-2.4641,  2.1059],
        [-0.3338,  0.1380],
        [ 2.1413, -1.1565],
        [ 0.5454, -0.5084],
        [ 1.2406, -1.3352],
        [ 1.9783, -1.9946],
        [-1.2161,  1.9984]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.8208,  0.1336,  1.0378, -0.4950,  0.7812,  1.4029,  1.1794,  0.8870,
          2.2442, -0.0646],
        [ 0.5527, -0.4582,  0.2784, -0.7049,  0.1252,  1.0346,  0.3657,  0.6709,
          1.1653, -0.3453],
        [ 1.3354,  0.4262,  1.2211,  0.3413,  0.7227,  1.7589,  0.8399,  1.0659,
          1.9655,  0.2778],
        [ 1.3017,  1.1791,  1.4180,  1.2863,  1.5231,  1.7031,  1.4877,  1.3330,
          1.3058,  1.1665],
        [ 1.5238,  1.4400,  1.3349,  0.9254,  1.5877,  1.5359,  1.6022,  1.4270,
          1.3786,  0.9440],
        [ 1.3091,  0.8196,  1.2640,  0.8598,  1.1985,  1.3510,  1.5658,  1.5829,
          1.4458,  1.0443],
        [ 0.9740,  0.3241,  0.8319, -0.3400,  0.9305,  1.9171,  1.1456,  1.2468,
          2.0805,  0.4806],
        [ 0.0444, -0.5125, -0.3146, -0.7501, -0.3551, -0.0114, -0.0221, -0.0635,
          0.4557, -0.6026],
        [ 0.8611, -0.2185,  0.2399, -0.3945,  0.2673,  1.0255,  0.5371,  0.3126,
          1.7628, -0.3936],
        [ 1.5447,  1.2769,  1.1942,  0.9727,  1.1259,  1.1315,  1.0576,  1.4270,
          1.4459,  1.1421]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.6555, -1.9432, -5.4200, -5.2802, -4.8781, -6.5709, -5.8247, -1.0966,
         -2.3479, -6.2270]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.4779,   4.4582],
        [ -2.9888,   5.1218],
        [-10.1860,  -0.4910],
        [ -2.8329,   5.1550],
        [ -0.7332,   5.2188],
        [-13.4368,  -2.6269],
        [  9.9500,   4.9186],
        [ -2.8977,   5.1327],
        [ -2.7608,   5.1695],
        [ -2.9829,   5.1219]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[  11.7059,    1.8240, -101.3053,    3.2968,   10.6241, -125.4015,
          -56.6139,    2.7285,    3.1323,    2.3012],
        [  -3.2439,  -45.3384, -103.1207,  -44.8810,  -37.5696, -121.2067,
          -64.9848,  -46.0821,  -45.6790,  -45.2452],
        [  18.2673,    2.2605,  -89.7200,    3.1457,    6.4371, -123.7380,
          -61.0242,    2.8310,    3.1676,    2.6227],
        [  12.1582,    2.9413,  -99.7761,    3.7678,    7.6133, -124.2912,
          -56.7718,    3.1142,    3.2861,    2.8894],
        [ -13.1116,  -89.3904,   -3.3058,  -76.2337,   -7.8924,    1.3510,
           -2.7172,  -79.2679,  -71.6002,  -88.4429],
        [  13.9301,    3.3283, -101.1006,    3.5577,    4.6686, -124.0594,
          -56.8780,    3.4905,    3.8749,    3.0313],
        [-293.7281, -650.9481, -259.2299, -629.1776, -294.3480, -141.7025,
           -4.3882, -637.7848, -620.1653, -649.8425],
        [ -33.6231, -130.6710,    5.3514, -129.1147, -105.1336,    6.8993,
          -16.6707, -125.8641, -129.8779, -129.9368],
        [  61.1800,  186.6925,   11.6953,  181.0107,  145.7384,  -36.3771,
          249.2027,  179.0892,  180.6693,  185.8967],
        [-417.7028, -687.3167, -126.6493, -677.1349, -523.9489,   -7.6882,
          -23.2825, -680.2225, -673.9100, -686.8533]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -5.8763,  -3.4458,  -4.3642,  -5.4448,  -4.2507,  -5.7936,   6.6472,
         -14.0272,   0.7117,  19.3615],
        [  5.6949,   3.7662,   4.4535,   5.7376,   3.8191,   5.5029,  -6.3297,
          14.0597,  -0.4421, -19.6925]], device='cuda:0'))])
loaded xi:  197.95192
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1086.319244531376
Current xi:  [198.76825]
objective value function right now is: -1086.319244531376
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1087.9767965818517
Current xi:  [196.44017]
objective value function right now is: -1087.9767965818517
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1088.6049719842422
Current xi:  [198.31447]
objective value function right now is: -1088.6049719842422
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1088.7160025557935
Current xi:  [198.70393]
objective value function right now is: -1088.7160025557935
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.44817]
objective value function right now is: -1087.8173567578415
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.11913]
objective value function right now is: -1083.22192838511
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [199.43945]
objective value function right now is: -1082.1839535871459
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1089.504745467828
Current xi:  [199.09036]
objective value function right now is: -1089.504745467828
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.4896]
objective value function right now is: -1088.3499703521045
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.5876]
objective value function right now is: -1088.8594886820833
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.10707]
objective value function right now is: -1088.7104639642794
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.14864]
objective value function right now is: -1085.502630581574
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.37265]
objective value function right now is: -1087.0864491364148
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [201.21683]
objective value function right now is: -1081.237258026113
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.302]
objective value function right now is: -1087.0439101172026
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.62186]
objective value function right now is: -1084.0025236945291
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.20807]
objective value function right now is: -1088.276025770115
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.9405]
objective value function right now is: -1081.5041579364747
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.48505]
objective value function right now is: -1088.4916432482228
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.7368]
objective value function right now is: -1087.4523336095103
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1089.71190118714
Current xi:  [195.5414]
objective value function right now is: -1089.71190118714
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.60037]
objective value function right now is: -1089.073437333733
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [193.75656]
objective value function right now is: -1086.004998900892
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.11717]
objective value function right now is: -1088.5672886444797
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.97397]
objective value function right now is: -1088.8300940569175
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.48685]
objective value function right now is: -1089.5754144310322
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.84212]
objective value function right now is: -1086.9339141897483
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [197.94232]
objective value function right now is: -1087.6360924932708
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [199.2556]
objective value function right now is: -1089.6119830776565
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.51202]
objective value function right now is: -1088.8712432344116
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.17064]
objective value function right now is: -1087.1201539728074
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.49402]
objective value function right now is: -1089.041708151951
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.73882]
objective value function right now is: -1085.6413749845851
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.24025]
objective value function right now is: -1086.9591531129367
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.70361]
objective value function right now is: -1088.729272416262
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [193.91835]
objective value function right now is: -1088.2213421292595
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [193.56679]
objective value function right now is: -1087.9658935058046
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.79907]
objective value function right now is: -1087.6239112252147
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.46104]
objective value function right now is: -1089.5737326890182
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.86855]
objective value function right now is: -1089.0058371619862
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.17662]
objective value function right now is: -1089.627646144747
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.35611]
objective value function right now is: -1086.6659431064536
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.35469]
objective value function right now is: -1087.7735902651395
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.02019]
objective value function right now is: -1088.2301212407538
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.64687]
objective value function right now is: -1087.582252396747
new min fval from sgd:  -1089.794174444775
new min fval from sgd:  -1089.807358469441
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.71394]
objective value function right now is: -1086.9227767388948
new min fval from sgd:  -1089.8698240839062
new min fval from sgd:  -1089.9042966105524
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.98979]
objective value function right now is: -1084.4039783873604
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.70193]
objective value function right now is: -1087.7027124058852
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.1415]
objective value function right now is: -1088.1886187866344
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.6578]
objective value function right now is: -1087.490831733258
min fval:  -1089.9042966105524
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.4970, -1.7427],
        [-1.1673,  1.4963],
        [ 1.5701, -1.0644],
        [-2.4641,  2.1059],
        [-0.3338,  0.1376],
        [ 2.1413, -1.1565],
        [ 0.5454, -0.5084],
        [ 1.2406, -1.3352],
        [ 1.9783, -1.9946],
        [-1.2161,  1.9984]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.8208,  0.1333,  1.0378, -0.4951,  0.7812,  1.4029,  1.1794,  0.8870,
          2.2442, -0.0648],
        [ 0.5527, -0.4582,  0.2784, -0.7049,  0.1252,  1.0346,  0.3657,  0.6709,
          1.1653, -0.3453],
        [ 1.3354,  0.4262,  1.2211,  0.3413,  0.7227,  1.7589,  0.8399,  1.0659,
          1.9655,  0.2778],
        [ 1.3017,  1.1791,  1.4180,  1.2863,  1.5231,  1.7031,  1.4877,  1.3330,
          1.3058,  1.1665],
        [ 1.5238,  1.4400,  1.3349,  0.9254,  1.5877,  1.5359,  1.6022,  1.4270,
          1.3786,  0.9440],
        [ 1.3091,  0.8196,  1.2640,  0.8598,  1.1985,  1.3510,  1.5658,  1.5829,
          1.4458,  1.0443],
        [ 0.9740,  0.3241,  0.8319, -0.3400,  0.9305,  1.9171,  1.1456,  1.2468,
          2.0805,  0.4806],
        [ 0.0444, -0.5125, -0.3146, -0.7501, -0.3551, -0.0114, -0.0221, -0.0635,
          0.4557, -0.6026],
        [ 0.8611, -0.2185,  0.2399, -0.3945,  0.2673,  1.0255,  0.5371,  0.3126,
          1.7628, -0.3936],
        [ 1.5447,  1.2769,  1.1942,  0.9727,  1.1259,  1.1315,  1.0576,  1.4270,
          1.4459,  1.1421]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.6555, -1.9432, -5.4200, -5.2802, -4.8781, -6.5709, -5.8247, -1.0966,
         -2.3479, -6.2270]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -0.7874,   3.8829],
        [ -3.0650,   5.3508],
        [-10.3061,  -0.3706],
        [ -2.8792,   5.3636],
        [  0.5815,   5.2417],
        [-13.3099,  -2.5509],
        [ 10.3093,   5.2428],
        [ -2.9473,   5.3455],
        [ -2.7915,   5.3710],
        [ -3.0568,   5.3494]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.9898e+01, -5.8419e-01, -1.1371e+02,  1.7836e+00,  1.6339e+01,
         -1.3922e+02, -5.8687e+01,  9.7850e-01,  2.0338e+00, -5.4484e-02],
        [ 9.9068e+00, -1.9658e+01, -1.1914e+02, -1.9717e+01, -2.6052e+01,
         -1.4195e+02, -7.5355e+01, -2.0698e+01, -2.0850e+01, -1.9582e+01],
        [ 2.4432e+01,  1.8644e+00, -1.0377e+02,  2.9669e+00,  1.0548e+01,
         -1.4035e+02, -6.7420e+01,  2.6392e+00,  3.0237e+00,  2.2463e+00],
        [ 2.1378e+01,  4.8597e-01, -1.1229e+02,  2.1799e+00,  1.3687e+01,
         -1.3845e+02, -5.9392e+01,  1.2664e+00,  2.0899e+00,  4.8073e-01],
        [-9.6208e+00, -1.0414e+02, -2.3002e+00, -8.8655e+01, -4.4160e+00,
          2.9737e+00, -2.7350e+00, -9.2361e+01, -8.2871e+01, -1.0306e+02],
        [ 2.0934e+01,  1.9895e+00, -1.1339e+02,  2.7543e+00,  8.9709e+00,
         -1.3808e+02, -6.0293e+01,  2.6126e+00,  3.2489e+00,  1.7345e+00],
        [-3.2369e+02, -7.4765e+02, -2.9346e+02, -7.2298e+02, -2.9778e+02,
         -1.5673e+02, -3.3371e+00, -7.3254e+02, -7.1253e+02, -7.4640e+02],
        [-3.7299e+01, -1.3645e+02,  6.1117e+00, -1.3544e+02, -1.1880e+02,
          6.6172e+00, -1.6949e+01, -1.3186e+02, -1.3648e+02, -1.3572e+02],
        [ 6.9262e+01,  2.0862e+02,  1.5430e+01,  2.0212e+02,  1.5784e+02,
         -3.7906e+01,  2.7533e+02,  2.0036e+02,  2.0143e+02,  2.0776e+02],
        [-5.1598e+02, -8.3525e+02, -1.5883e+02, -8.2299e+02, -6.1825e+02,
         -3.4785e+00, -2.1478e+01, -8.2678e+02, -8.1871e+02, -8.3469e+02]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -4.8652,  -1.9424,  -3.0710,  -4.5372,  -3.6422,  -5.2519,   5.3414,
         -15.2402,   0.6937,  20.3831],
        [  4.6838,   2.2628,   3.1602,   4.8299,   3.2106,   4.9611,  -5.0239,
          15.2727,  -0.4241, -20.7142]], device='cuda:0'))])
xi:  [199.10828]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1242.1352112774703
W_T_median: 1003.0937379173379
W_T_pctile_5: 200.1344953381818
W_T_CVAR_5_pct: 9.816699673659098
Average q (qsum/M+1):  35.0
Optimal xi:  [199.10828]
Expected(across Rb) median(across samples) p_equity:  0.24641625136137008
obj fun:  tensor(-1089.9043, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.4970, -1.7427],
        [-1.1673,  1.4963],
        [ 1.5701, -1.0644],
        [-2.4641,  2.1059],
        [-0.3338,  0.1376],
        [ 2.1413, -1.1565],
        [ 0.5454, -0.5084],
        [ 1.2406, -1.3352],
        [ 1.9783, -1.9946],
        [-1.2161,  1.9984]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.8208,  0.1333,  1.0378, -0.4951,  0.7812,  1.4029,  1.1794,  0.8870,
          2.2442, -0.0648],
        [ 0.5527, -0.4582,  0.2784, -0.7049,  0.1252,  1.0346,  0.3657,  0.6709,
          1.1653, -0.3453],
        [ 1.3354,  0.4262,  1.2211,  0.3413,  0.7227,  1.7589,  0.8399,  1.0659,
          1.9655,  0.2778],
        [ 1.3017,  1.1791,  1.4180,  1.2863,  1.5231,  1.7031,  1.4877,  1.3330,
          1.3058,  1.1665],
        [ 1.5238,  1.4400,  1.3349,  0.9254,  1.5877,  1.5359,  1.6022,  1.4270,
          1.3786,  0.9440],
        [ 1.3091,  0.8196,  1.2640,  0.8598,  1.1985,  1.3510,  1.5658,  1.5829,
          1.4458,  1.0443],
        [ 0.9740,  0.3241,  0.8319, -0.3400,  0.9305,  1.9171,  1.1456,  1.2468,
          2.0805,  0.4806],
        [ 0.0444, -0.5125, -0.3146, -0.7501, -0.3551, -0.0114, -0.0221, -0.0635,
          0.4557, -0.6026],
        [ 0.8611, -0.2185,  0.2399, -0.3945,  0.2673,  1.0255,  0.5371,  0.3126,
          1.7628, -0.3936],
        [ 1.5447,  1.2769,  1.1942,  0.9727,  1.1259,  1.1315,  1.0576,  1.4270,
          1.4459,  1.1421]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.6555, -1.9432, -5.4200, -5.2802, -4.8781, -6.5709, -5.8247, -1.0966,
         -2.3479, -6.2270]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -0.7874,   3.8829],
        [ -3.0650,   5.3508],
        [-10.3061,  -0.3706],
        [ -2.8792,   5.3636],
        [  0.5815,   5.2417],
        [-13.3099,  -2.5509],
        [ 10.3093,   5.2428],
        [ -2.9473,   5.3455],
        [ -2.7915,   5.3710],
        [ -3.0568,   5.3494]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.9898e+01, -5.8419e-01, -1.1371e+02,  1.7836e+00,  1.6339e+01,
         -1.3922e+02, -5.8687e+01,  9.7850e-01,  2.0338e+00, -5.4484e-02],
        [ 9.9068e+00, -1.9658e+01, -1.1914e+02, -1.9717e+01, -2.6052e+01,
         -1.4195e+02, -7.5355e+01, -2.0698e+01, -2.0850e+01, -1.9582e+01],
        [ 2.4432e+01,  1.8644e+00, -1.0377e+02,  2.9669e+00,  1.0548e+01,
         -1.4035e+02, -6.7420e+01,  2.6392e+00,  3.0237e+00,  2.2463e+00],
        [ 2.1378e+01,  4.8597e-01, -1.1229e+02,  2.1799e+00,  1.3687e+01,
         -1.3845e+02, -5.9392e+01,  1.2664e+00,  2.0899e+00,  4.8073e-01],
        [-9.6208e+00, -1.0414e+02, -2.3002e+00, -8.8655e+01, -4.4160e+00,
          2.9737e+00, -2.7350e+00, -9.2361e+01, -8.2871e+01, -1.0306e+02],
        [ 2.0934e+01,  1.9895e+00, -1.1339e+02,  2.7543e+00,  8.9709e+00,
         -1.3808e+02, -6.0293e+01,  2.6126e+00,  3.2489e+00,  1.7345e+00],
        [-3.2369e+02, -7.4765e+02, -2.9346e+02, -7.2298e+02, -2.9778e+02,
         -1.5673e+02, -3.3371e+00, -7.3254e+02, -7.1253e+02, -7.4640e+02],
        [-3.7299e+01, -1.3645e+02,  6.1117e+00, -1.3544e+02, -1.1880e+02,
          6.6172e+00, -1.6949e+01, -1.3186e+02, -1.3648e+02, -1.3572e+02],
        [ 6.9262e+01,  2.0862e+02,  1.5430e+01,  2.0212e+02,  1.5784e+02,
         -3.7906e+01,  2.7533e+02,  2.0036e+02,  2.0143e+02,  2.0776e+02],
        [-5.1598e+02, -8.3525e+02, -1.5883e+02, -8.2299e+02, -6.1825e+02,
         -3.4785e+00, -2.1478e+01, -8.2678e+02, -8.1871e+02, -8.3469e+02]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -4.8652,  -1.9424,  -3.0710,  -4.5372,  -3.6422,  -5.2519,   5.3414,
         -15.2402,   0.6937,  20.3831],
        [  4.6838,   2.2628,   3.1602,   4.8299,   3.2106,   4.9611,  -5.0239,
          15.2727,  -0.4241, -20.7142]], device='cuda:0'))])
loaded xi:  199.10828
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1086.5071156074955
Current xi:  [200.35722]
objective value function right now is: -1086.5071156074955
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.2914]
objective value function right now is: -1085.5413376383958
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.47304]
objective value function right now is: -1085.8941536995326
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1086.923981687274
Current xi:  [200.19324]
objective value function right now is: -1086.923981687274
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.71167]
objective value function right now is: -1086.4686821094085
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.51898]
objective value function right now is: -1086.401933461068
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [195.60652]
objective value function right now is: -1086.0384181329941
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [193.23418]
objective value function right now is: -1086.266633894393
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.35199]
objective value function right now is: -1086.068302055188
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.14505]
objective value function right now is: -1086.0034410349651
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.96631]
objective value function right now is: -1086.099017821008
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.64035]
objective value function right now is: -1085.7545757156204
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.81013]
objective value function right now is: -1086.1169774587727
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [197.2744]
objective value function right now is: -1085.2943727227544
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.39742]
objective value function right now is: -1086.4556692185881
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.57257]
objective value function right now is: -1084.6149664151753
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.5203]
objective value function right now is: -1086.3978455668148
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.58717]
objective value function right now is: -1086.0657129357223
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.66147]
objective value function right now is: -1086.2681331366723
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.13512]
objective value function right now is: -1085.4822294487633
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.36765]
objective value function right now is: -1086.515202698301
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.38834]
objective value function right now is: -1086.191113852347
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.17769]
objective value function right now is: -1085.182432007193
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.54805]
objective value function right now is: -1086.4023968924978
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.02306]
objective value function right now is: -1086.7009765625867
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.05293]
objective value function right now is: -1085.9225166935214
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.97986]
objective value function right now is: -1086.4678816010933
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [198.29256]
objective value function right now is: -1086.5170642397497
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [199.51054]
objective value function right now is: -1086.492621861165
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.24442]
objective value function right now is: -1086.7952296079359
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.06644]
objective value function right now is: -1084.2659487864723
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.9235]
objective value function right now is: -1086.20624144038
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.88342]
objective value function right now is: -1086.786906279867
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.42845]
objective value function right now is: -1086.428708295271
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.76357]
objective value function right now is: -1084.3179206448765
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.21616]
objective value function right now is: -1085.14355155874
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.7585]
objective value function right now is: -1085.3797704339825
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.53778]
objective value function right now is: -1085.7887023957928
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.00996]
objective value function right now is: -1086.6749497992184
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.13747]
objective value function right now is: -1086.2478595805496
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.22057]
objective value function right now is: -1086.0984398509004
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.94885]
objective value function right now is: -1086.7671786105764
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.4971]
objective value function right now is: -1086.866974422966
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.1612]
objective value function right now is: -1085.7137339740225
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.53369]
objective value function right now is: -1086.2334304774813
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.24707]
objective value function right now is: -1085.9647463381448
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.44496]
objective value function right now is: -1086.7567485396053
new min fval from sgd:  -1086.9263873294751
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.93866]
objective value function right now is: -1086.1877223957567
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.72241]
objective value function right now is: -1084.9703165714263
new min fval from sgd:  -1086.9357547913476
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.14395]
objective value function right now is: -1085.873940769739
min fval:  -1086.9357547913476
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.4970, -1.7427],
        [-1.1673,  1.4963],
        [ 1.5701, -1.0644],
        [-2.4641,  2.1059],
        [-0.3338,  0.1372],
        [ 2.1413, -1.1565],
        [ 0.5454, -0.5084],
        [ 1.2406, -1.3352],
        [ 1.9783, -1.9946],
        [-1.2161,  1.9984]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.8208,  0.1330,  1.0378, -0.4951,  0.7812,  1.4029,  1.1794,  0.8870,
          2.2442, -0.0650],
        [ 0.5527, -0.4582,  0.2784, -0.7049,  0.1252,  1.0346,  0.3657,  0.6709,
          1.1653, -0.3453],
        [ 1.3354,  0.4262,  1.2211,  0.3413,  0.7227,  1.7589,  0.8399,  1.0659,
          1.9655,  0.2778],
        [ 1.3017,  1.1791,  1.4180,  1.2863,  1.5231,  1.7031,  1.4877,  1.3330,
          1.3058,  1.1665],
        [ 1.5238,  1.4400,  1.3349,  0.9254,  1.5877,  1.5359,  1.6022,  1.4270,
          1.3786,  0.9440],
        [ 1.3091,  0.8196,  1.2640,  0.8598,  1.1985,  1.3510,  1.5658,  1.5829,
          1.4458,  1.0443],
        [ 0.9740,  0.3241,  0.8319, -0.3400,  0.9305,  1.9171,  1.1456,  1.2468,
          2.0805,  0.4806],
        [ 0.0444, -0.5125, -0.3146, -0.7501, -0.3551, -0.0114, -0.0221, -0.0635,
          0.4557, -0.6026],
        [ 0.8611, -0.2185,  0.2399, -0.3945,  0.2673,  1.0255,  0.5371,  0.3126,
          1.7628, -0.3936],
        [ 1.5447,  1.2769,  1.1942,  0.9727,  1.1259,  1.1315,  1.0576,  1.4270,
          1.4459,  1.1421]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.6555, -1.9432, -5.4200, -5.2802, -4.8781, -6.5709, -5.8247, -1.0966,
         -2.3479, -6.2270]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.0912,   4.5716],
        [ -3.1627,   5.3690],
        [-10.1207,  -0.6991],
        [ -2.9725,   5.3832],
        [  0.3649,   5.6417],
        [-14.2577,  -3.0670],
        [ 10.3944,   5.1578],
        [ -3.0487,   5.3699],
        [ -2.8765,   5.3916],
        [ -3.1553,   5.3685]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.8789e+01, -2.2096e+00, -1.3078e+02,  4.4836e-01,  1.5908e+01,
         -1.5316e+02, -6.0030e+01, -4.3139e-01,  8.4735e-01, -1.6620e+00],
        [ 2.0729e+00, -1.7129e+01, -1.4307e+02, -1.7196e+01, -2.7599e+01,
         -1.6900e+02, -8.3007e+01, -1.8206e+01, -1.8330e+01, -1.7056e+01],
        [ 2.0661e+01,  2.9131e+00, -1.2361e+02,  4.4076e+00,  9.0719e+00,
         -1.5919e+02, -7.1837e+01,  3.9779e+00,  4.6416e+00,  3.3192e+00],
        [ 2.1680e+01, -2.1386e+00, -1.2953e+02,  1.6952e-01,  1.5095e+01,
         -1.5273e+02, -6.0921e+01, -8.9028e-01,  3.6935e-01, -2.1048e+00],
        [-9.0599e+00, -1.1737e+02, -5.2893e+00, -9.9799e+01, -3.9640e+00,
          2.4247e+00, -2.6000e+00, -1.0400e+02, -9.2974e+01, -1.1616e+02],
        [ 2.1756e+01, -8.4606e-01, -1.3003e+02,  4.8961e-01,  1.0826e+01,
         -1.5176e+02, -6.1166e+01,  2.0855e-01,  1.2629e+00, -1.0656e+00],
        [-3.4328e+02, -8.6441e+02, -3.2993e+02, -8.3672e+02, -3.0004e+02,
         -1.6711e+02, -4.0340e+00, -8.4724e+02, -8.2474e+02, -8.6300e+02],
        [-4.3427e+01, -1.4491e+02,  6.3001e+00, -1.4407e+02, -1.3379e+02,
          5.0980e+00, -1.8649e+01, -1.4030e+02, -1.4522e+02, -1.4417e+02],
        [ 7.4423e+01,  2.2771e+02,  1.5916e+01,  2.2043e+02,  1.7126e+02,
         -3.7896e+01,  3.0850e+02,  2.1881e+02,  2.1941e+02,  2.2679e+02],
        [-5.9659e+02, -9.8947e+02, -1.8337e+02, -9.7550e+02, -7.0593e+02,
         -6.5973e+00, -2.1365e+01, -9.7975e+02, -9.7038e+02, -9.8881e+02]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -8.6001,  -4.8140,  -6.2693,  -8.1464,  -3.8162,  -8.8317,   5.5765,
         -15.7415,   0.7140,  21.1527],
        [  8.4188,   5.1344,   6.3586,   8.4392,   3.3847,   8.5410,  -5.2590,
          15.7740,  -0.4444, -21.4838]], device='cuda:0'))])
xi:  [199.08244]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1216.7632992221309
W_T_median: 985.6818690009527
W_T_pctile_5: 198.58667607155053
W_T_CVAR_5_pct: 9.686226029129452
Average q (qsum/M+1):  35.0
Optimal xi:  [199.08244]
Expected(across Rb) median(across samples) p_equity:  0.23895135521888733
obj fun:  tensor(-1086.9358, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.4970, -1.7427],
        [-1.1673,  1.4963],
        [ 1.5701, -1.0644],
        [-2.4641,  2.1059],
        [-0.3338,  0.1372],
        [ 2.1413, -1.1565],
        [ 0.5454, -0.5084],
        [ 1.2406, -1.3352],
        [ 1.9783, -1.9946],
        [-1.2161,  1.9984]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.8208,  0.1330,  1.0378, -0.4951,  0.7812,  1.4029,  1.1794,  0.8870,
          2.2442, -0.0650],
        [ 0.5527, -0.4582,  0.2784, -0.7049,  0.1252,  1.0346,  0.3657,  0.6709,
          1.1653, -0.3453],
        [ 1.3354,  0.4262,  1.2211,  0.3413,  0.7227,  1.7589,  0.8399,  1.0659,
          1.9655,  0.2778],
        [ 1.3017,  1.1791,  1.4180,  1.2863,  1.5231,  1.7031,  1.4877,  1.3330,
          1.3058,  1.1665],
        [ 1.5238,  1.4400,  1.3349,  0.9254,  1.5877,  1.5359,  1.6022,  1.4270,
          1.3786,  0.9440],
        [ 1.3091,  0.8196,  1.2640,  0.8598,  1.1985,  1.3510,  1.5658,  1.5829,
          1.4458,  1.0443],
        [ 0.9740,  0.3241,  0.8319, -0.3400,  0.9305,  1.9171,  1.1456,  1.2468,
          2.0805,  0.4806],
        [ 0.0444, -0.5125, -0.3146, -0.7501, -0.3551, -0.0114, -0.0221, -0.0635,
          0.4557, -0.6026],
        [ 0.8611, -0.2185,  0.2399, -0.3945,  0.2673,  1.0255,  0.5371,  0.3126,
          1.7628, -0.3936],
        [ 1.5447,  1.2769,  1.1942,  0.9727,  1.1259,  1.1315,  1.0576,  1.4270,
          1.4459,  1.1421]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.6555, -1.9432, -5.4200, -5.2802, -4.8781, -6.5709, -5.8247, -1.0966,
         -2.3479, -6.2270]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.0912,   4.5716],
        [ -3.1627,   5.3690],
        [-10.1207,  -0.6991],
        [ -2.9725,   5.3832],
        [  0.3649,   5.6417],
        [-14.2577,  -3.0670],
        [ 10.3944,   5.1578],
        [ -3.0487,   5.3699],
        [ -2.8765,   5.3916],
        [ -3.1553,   5.3685]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.8789e+01, -2.2096e+00, -1.3078e+02,  4.4836e-01,  1.5908e+01,
         -1.5316e+02, -6.0030e+01, -4.3139e-01,  8.4735e-01, -1.6620e+00],
        [ 2.0729e+00, -1.7129e+01, -1.4307e+02, -1.7196e+01, -2.7599e+01,
         -1.6900e+02, -8.3007e+01, -1.8206e+01, -1.8330e+01, -1.7056e+01],
        [ 2.0661e+01,  2.9131e+00, -1.2361e+02,  4.4076e+00,  9.0719e+00,
         -1.5919e+02, -7.1837e+01,  3.9779e+00,  4.6416e+00,  3.3192e+00],
        [ 2.1680e+01, -2.1386e+00, -1.2953e+02,  1.6952e-01,  1.5095e+01,
         -1.5273e+02, -6.0921e+01, -8.9028e-01,  3.6935e-01, -2.1048e+00],
        [-9.0599e+00, -1.1737e+02, -5.2893e+00, -9.9799e+01, -3.9640e+00,
          2.4247e+00, -2.6000e+00, -1.0400e+02, -9.2974e+01, -1.1616e+02],
        [ 2.1756e+01, -8.4606e-01, -1.3003e+02,  4.8961e-01,  1.0826e+01,
         -1.5176e+02, -6.1166e+01,  2.0855e-01,  1.2629e+00, -1.0656e+00],
        [-3.4328e+02, -8.6441e+02, -3.2993e+02, -8.3672e+02, -3.0004e+02,
         -1.6711e+02, -4.0340e+00, -8.4724e+02, -8.2474e+02, -8.6300e+02],
        [-4.3427e+01, -1.4491e+02,  6.3001e+00, -1.4407e+02, -1.3379e+02,
          5.0980e+00, -1.8649e+01, -1.4030e+02, -1.4522e+02, -1.4417e+02],
        [ 7.4423e+01,  2.2771e+02,  1.5916e+01,  2.2043e+02,  1.7126e+02,
         -3.7896e+01,  3.0850e+02,  2.1881e+02,  2.1941e+02,  2.2679e+02],
        [-5.9659e+02, -9.8947e+02, -1.8337e+02, -9.7550e+02, -7.0593e+02,
         -6.5973e+00, -2.1365e+01, -9.7975e+02, -9.7038e+02, -9.8881e+02]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -8.6001,  -4.8140,  -6.2693,  -8.1464,  -3.8162,  -8.8317,   5.5765,
         -15.7415,   0.7140,  21.1527],
        [  8.4188,   5.1344,   6.3586,   8.4392,   3.3847,   8.5410,  -5.2590,
          15.7740,  -0.4444, -21.4838]], device='cuda:0'))])
loaded xi:  199.08244
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1085.130271495443
Current xi:  [198.70352]
objective value function right now is: -1085.130271495443
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.59895]
objective value function right now is: -1085.041645847005
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.61273]
objective value function right now is: -1084.8502603323675
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.15154]
objective value function right now is: -1084.944574381051
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [193.4371]
objective value function right now is: -1085.0029199550852
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1085.2278373609163
Current xi:  [194.5372]
objective value function right now is: -1085.2278373609163
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1085.3076137487012
Current xi:  [194.17084]
objective value function right now is: -1085.3076137487012
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1085.4407015678496
Current xi:  [194.18652]
objective value function right now is: -1085.4407015678496
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.8187]
objective value function right now is: -1085.3993536013033
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [194.48906]
objective value function right now is: -1085.1714020607744
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.94571]
objective value function right now is: -1085.4292937277492
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.16367]
objective value function right now is: -1085.2965650317442
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.24046]
objective value function right now is: -1085.2028376411058
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1085.451910591254
Current xi:  [199.51143]
objective value function right now is: -1085.451910591254
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.92181]
objective value function right now is: -1085.389114177549
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.09993]
objective value function right now is: -1085.30056706196
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.84703]
objective value function right now is: -1084.7299726368942
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.94714]
objective value function right now is: -1085.4070588053628
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.05005]
objective value function right now is: -1085.2000629043987
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.67497]
objective value function right now is: -1084.6441756893382
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.7527]
objective value function right now is: -1085.3399961294067
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.36942]
objective value function right now is: -1085.1087702461998
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.17264]
objective value function right now is: -1085.2123815779846
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.93613]
objective value function right now is: -1084.4955174701672
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.82903]
objective value function right now is: -1085.2361849087251
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.15526]
objective value function right now is: -1085.3575166728808
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.34035]
objective value function right now is: -1085.0382847217388
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [199.29092]
objective value function right now is: -1085.3486907396955
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [198.49254]
objective value function right now is: -1084.926658822767
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.58261]
objective value function right now is: -1085.3044051226586
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.82475]
objective value function right now is: -1085.3002035721538
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.17543]
objective value function right now is: -1085.4478403508008
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.76122]
objective value function right now is: -1084.7432412215514
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.10588]
objective value function right now is: -1085.3572758794312
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.22755]
objective value function right now is: -1084.929218669017
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.82416]
objective value function right now is: -1085.405265581655
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.67752]
objective value function right now is: -1085.4305130769746
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.57487]
objective value function right now is: -1085.2692577878865
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.84912]
objective value function right now is: -1085.1956382327016
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.32016]
objective value function right now is: -1085.1137860473168
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [194.61366]
objective value function right now is: -1085.391872901845
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.57648]
objective value function right now is: -1085.306261247535
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.27954]
objective value function right now is: -1085.4145945636083
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.0217]
objective value function right now is: -1085.4080296510908
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.57738]
objective value function right now is: -1085.1676299896599
new min fval from sgd:  -1085.4543428540958
new min fval from sgd:  -1085.4691418956754
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.34605]
objective value function right now is: -1085.3409790151072
new min fval from sgd:  -1085.471145279698
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.04959]
objective value function right now is: -1085.3146278493293
new min fval from sgd:  -1085.474439105075
new min fval from sgd:  -1085.4833272392545
new min fval from sgd:  -1085.4855243483746
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.19498]
objective value function right now is: -1085.3966767332524
new min fval from sgd:  -1085.487207563971
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.13077]
objective value function right now is: -1085.1576387907994
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.14647]
objective value function right now is: -1085.1370119328046
min fval:  -1085.487207563971
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.4970, -1.7427],
        [-1.1673,  1.4963],
        [ 1.5701, -1.0644],
        [-2.4641,  2.1059],
        [-0.3338,  0.1367],
        [ 2.1413, -1.1565],
        [ 0.5454, -0.5084],
        [ 1.2406, -1.3352],
        [ 1.9783, -1.9946],
        [-1.2161,  1.9984]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.8208,  0.1328,  1.0378, -0.4951,  0.7812,  1.4029,  1.1794,  0.8870,
          2.2442, -0.0653],
        [ 0.5527, -0.4582,  0.2784, -0.7049,  0.1252,  1.0346,  0.3657,  0.6709,
          1.1653, -0.3453],
        [ 1.3354,  0.4262,  1.2211,  0.3413,  0.7227,  1.7589,  0.8399,  1.0659,
          1.9655,  0.2778],
        [ 1.3017,  1.1791,  1.4180,  1.2863,  1.5231,  1.7031,  1.4877,  1.3330,
          1.3058,  1.1665],
        [ 1.5238,  1.4400,  1.3349,  0.9254,  1.5877,  1.5359,  1.6022,  1.4270,
          1.3786,  0.9440],
        [ 1.3091,  0.8196,  1.2640,  0.8598,  1.1985,  1.3510,  1.5658,  1.5829,
          1.4458,  1.0443],
        [ 0.9740,  0.3241,  0.8319, -0.3400,  0.9305,  1.9171,  1.1456,  1.2468,
          2.0805,  0.4806],
        [ 0.0444, -0.5125, -0.3146, -0.7501, -0.3551, -0.0114, -0.0221, -0.0635,
          0.4557, -0.6026],
        [ 0.8611, -0.2186,  0.2399, -0.3945,  0.2673,  1.0255,  0.5371,  0.3126,
          1.7628, -0.3936],
        [ 1.5447,  1.2769,  1.1942,  0.9727,  1.1259,  1.1315,  1.0576,  1.4270,
          1.4459,  1.1421]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.6555, -1.9432, -5.4200, -5.2802, -4.8781, -6.5709, -5.8247, -1.0966,
         -2.3479, -6.2270]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.7683,   4.8126],
        [ -3.1427,   5.3731],
        [-11.2255,  -0.4709],
        [ -2.9576,   5.3941],
        [  0.1700,   5.6711],
        [-13.4229,  -2.8611],
        [ 10.1314,   5.0596],
        [ -3.0309,   5.3715],
        [ -2.8646,   5.4065],
        [ -3.1356,   5.3717]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.3096e+01, -3.2645e+00, -1.4217e+02, -2.4712e-01,  1.6539e+01,
         -1.6975e+02, -5.8959e+01, -1.2230e+00,  2.9548e-01, -2.6948e+00],
        [ 1.0764e+00, -1.7426e+01, -1.5870e+02, -1.7531e+01, -3.1725e+01,
         -1.8753e+02, -8.7625e+01, -1.8534e+01, -1.8695e+01, -1.7356e+01],
        [ 2.1530e+01,  2.2899e+00, -1.3650e+02,  3.9373e+00,  6.1608e+00,
         -1.7583e+02, -7.3198e+01,  3.4689e+00,  4.2276e+00,  2.7056e+00],
        [ 2.5855e+01, -2.9865e+00, -1.4104e+02, -3.9572e-01,  1.5525e+01,
         -1.6933e+02, -6.0100e+01, -1.5304e+00, -9.2319e-02, -2.9348e+00],
        [-1.4279e+01, -1.2767e+02, -2.6949e+00, -1.0892e+02, -7.1091e+00,
          2.0403e+00, -2.9721e+00, -1.1331e+02, -1.0154e+02, -1.2638e+02],
        [ 2.5820e+01, -1.7403e+00, -1.4144e+02, -6.2965e-02,  1.1162e+01,
         -1.6829e+02, -6.0335e+01, -4.3585e-01,  8.4610e-01, -1.9390e+00],
        [-3.7437e+02, -9.5776e+02, -3.8552e+02, -9.2505e+02, -3.0815e+02,
         -1.7816e+02, -3.7619e+00, -9.3754e+02, -9.1050e+02, -9.5616e+02],
        [-4.6699e+01, -1.5161e+02,  7.0536e+00, -1.5064e+02, -1.4754e+02,
          5.7452e+00, -1.7483e+01, -1.4673e+02, -1.5178e+02, -1.5083e+02],
        [ 8.6422e+01,  2.5089e+02,  2.2277e+01,  2.4240e+02,  1.8942e+02,
         -4.2986e+01,  3.0663e+02,  2.4107e+02,  2.4086e+02,  2.4989e+02],
        [-7.2308e+02, -1.1575e+03, -2.4110e+02, -1.1419e+03, -8.3656e+02,
         -5.5322e-01, -2.2246e+01, -1.1466e+03, -1.1360e+03, -1.1567e+03]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -7.8768,  -3.7004,  -5.1056,  -7.3121,  -4.3360,  -7.9053,   5.5565,
         -17.4822,   0.6983,  24.8080],
        [  7.6956,   4.0211,   5.1951,   7.6050,   3.9044,   7.6148,  -5.2390,
          17.5147,  -0.4286, -25.1389]], device='cuda:0'))])
xi:  [199.03668]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1235.2654171504073
W_T_median: 998.0155104046612
W_T_pctile_5: 203.05930076133168
W_T_CVAR_5_pct: 9.844899523813247
Average q (qsum/M+1):  35.0
Optimal xi:  [199.03668]
Expected(across Rb) median(across samples) p_equity:  0.2447344958782196
obj fun:  tensor(-1085.4872, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
