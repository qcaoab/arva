Starting at: 
15-03-23_17:18

 numpy seed:  3  


 pytorch seed:  3  

Key parameters-------
paths: 256000
iterations: 100000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  hidden_layer    None       None         None   
4        obj.layers[4]        4  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  
4    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  hidden_layer      10  logistic_sigmoid   
4        obj.layers[4]        4  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3     (10, 10)     True          10  
4      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  hidden_layer    None       None         None   
4        obj.layers[4]        4  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  
4    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  hidden_layer      10  logistic_sigmoid   
4        obj.layers[4]        4  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3     (10, 10)     True          10  
4      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  hidden_layer       10  logistic_sigmoid   
0        obj.layers[4]         4  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  hidden_layer       10  logistic_sigmoid   
0        obj.layers[4]         4  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 10000, 'itbound_SGD_algorithms': 100000, 'nit_IterateAveragingStart': 90000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  hidden_layer       10  logistic_sigmoid   
0        obj.layers[4]         4  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  hidden_layer       10  logistic_sigmoid   
0        obj.layers[4]         4  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1583.6866209228313
W_T_median: 1138.6857387434734
W_T_pctile_5: -127.4429131675796
W_T_CVAR_5_pct: -297.80727420155023
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1084.4216458620554
Current xi:  [155.01375]
objective value function right now is: -1084.4216458620554
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1092.1577590201357
Current xi:  [186.58319]
objective value function right now is: -1092.1577590201357
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1094.558458969573
Current xi:  [195.88982]
objective value function right now is: -1094.558458969573
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.46754]
objective value function right now is: -1088.3896662413342
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.56488]
objective value function right now is: -1091.6425092223137
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.67047]
objective value function right now is: -1093.5873386600042
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1095.320234811603
Current xi:  [200.04219]
objective value function right now is: -1095.320234811603
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.00064]
objective value function right now is: -1092.7653187481387
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.49902]
objective value function right now is: -1094.8572525079267
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [202.31366]
objective value function right now is: -1094.3275242674263
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1095.769629336973
Current xi:  [201.89749]
objective value function right now is: -1095.769629336973
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.03871]
objective value function right now is: -1093.8526464732615
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.64862]
objective value function right now is: -1093.573593295034
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [204.18675]
objective value function right now is: -1092.0364671640461
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.5916]
objective value function right now is: -1094.191295997117
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.82063]
objective value function right now is: -1092.081618410241
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.11603]
objective value function right now is: -1089.897855598892
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.25093]
objective value function right now is: -1091.994807944248
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.71]
objective value function right now is: -1093.7704566938041
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.30794]
objective value function right now is: -1094.1187887168653
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.93344]
objective value function right now is: -1094.8217641010654
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.69511]
objective value function right now is: -1093.9688172102296
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.07526]
objective value function right now is: -1095.6156293676447
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.98828]
objective value function right now is: -1093.2355662694013
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.25082]
objective value function right now is: -1094.1790946983015
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.79501]
objective value function right now is: -1092.562389150959
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.17473]
objective value function right now is: -1094.4014883704722
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1095.9789794234075
Current xi:  [203.31905]
objective value function right now is: -1095.9789794234075
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [201.62169]
objective value function right now is: -1093.77702586261
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.93217]
objective value function right now is: -1094.251501932271
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.63197]
objective value function right now is: -1094.6474548080919
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.09528]
objective value function right now is: -1095.8561100159468
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.44875]
objective value function right now is: -1093.6325293863936
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.62659]
objective value function right now is: -1095.0545010020423
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.443]
objective value function right now is: -1093.7330987261153
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1096.5513041618522
Current xi:  [203.35036]
objective value function right now is: -1096.5513041618522
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1096.6141173846618
Current xi:  [203.94846]
objective value function right now is: -1096.6141173846618
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.70732]
objective value function right now is: -1096.6083674228034
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.06696]
objective value function right now is: -1096.3414129663556
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.58469]
objective value function right now is: -1096.2029253803282
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1096.6907517119773
Current xi:  [204.03322]
objective value function right now is: -1096.6907517119773
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.37335]
objective value function right now is: -1096.5697082345207
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.0603]
objective value function right now is: -1096.5224482662609
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.42162]
objective value function right now is: -1096.6419284473568
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.37596]
objective value function right now is: -1096.6646652181548
new min fval from sgd:  -1096.6920436913153
new min fval from sgd:  -1096.7198695096045
new min fval from sgd:  -1096.7320484491888
new min fval from sgd:  -1096.7349211658718
new min fval from sgd:  -1096.7353814084274
new min fval from sgd:  -1096.7472302004892
new min fval from sgd:  -1096.7507715198553
new min fval from sgd:  -1096.7526933939657
new min fval from sgd:  -1096.7650254221764
new min fval from sgd:  -1096.7713018573995
new min fval from sgd:  -1096.7714898206418
new min fval from sgd:  -1096.7914891879007
new min fval from sgd:  -1096.8019947073985
new min fval from sgd:  -1096.8133041687313
new min fval from sgd:  -1096.8168573090472
new min fval from sgd:  -1096.830968355095
new min fval from sgd:  -1096.844629116502
new min fval from sgd:  -1096.8539246314772
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.45158]
objective value function right now is: -1096.758389764245
new min fval from sgd:  -1096.854234856653
new min fval from sgd:  -1096.8661374019403
new min fval from sgd:  -1096.8787040330847
new min fval from sgd:  -1096.8822351709082
new min fval from sgd:  -1096.8824840256134
new min fval from sgd:  -1096.8825116463906
new min fval from sgd:  -1096.8832281559967
new min fval from sgd:  -1096.8892466367213
new min fval from sgd:  -1096.8957950400707
new min fval from sgd:  -1096.8980167349791
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.46448]
objective value function right now is: -1096.6119126142441
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.9802]
objective value function right now is: -1096.388760401242
new min fval from sgd:  -1096.8982910744644
new min fval from sgd:  -1096.9007726254774
new min fval from sgd:  -1096.9025363256912
new min fval from sgd:  -1096.904357663251
new min fval from sgd:  -1096.9062224977345
new min fval from sgd:  -1096.906712848215
new min fval from sgd:  -1096.9090444029805
new min fval from sgd:  -1096.9108968455978
new min fval from sgd:  -1096.9128562178244
new min fval from sgd:  -1096.9137140665268
new min fval from sgd:  -1096.9154935459715
new min fval from sgd:  -1096.9176428132114
new min fval from sgd:  -1096.9194558759891
new min fval from sgd:  -1096.921441333851
new min fval from sgd:  -1096.9226368886893
new min fval from sgd:  -1096.923833525217
new min fval from sgd:  -1096.9245530133107
new min fval from sgd:  -1096.925565003918
new min fval from sgd:  -1096.928006819636
new min fval from sgd:  -1096.9289523750417
new min fval from sgd:  -1096.9297223015467
new min fval from sgd:  -1096.931290454004
new min fval from sgd:  -1096.9320939900203
new min fval from sgd:  -1096.9322667650658
new min fval from sgd:  -1096.9358246210693
new min fval from sgd:  -1096.9387946957243
new min fval from sgd:  -1096.942035984492
new min fval from sgd:  -1096.94423903216
new min fval from sgd:  -1096.9466141428375
new min fval from sgd:  -1096.948660075434
new min fval from sgd:  -1096.9503044307953
new min fval from sgd:  -1096.9507061187553
new min fval from sgd:  -1096.9515283141977
new min fval from sgd:  -1096.9527546346117
new min fval from sgd:  -1096.9540731018856
new min fval from sgd:  -1096.954130462999
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.62363]
objective value function right now is: -1096.9321033277793
new min fval from sgd:  -1096.9545640096414
new min fval from sgd:  -1096.9548186300547
new min fval from sgd:  -1096.9548258408774
new min fval from sgd:  -1096.9551408055127
new min fval from sgd:  -1096.955448613251
new min fval from sgd:  -1096.95610736738
new min fval from sgd:  -1096.9566189676318
new min fval from sgd:  -1096.9575481024688
new min fval from sgd:  -1096.9584484613235
new min fval from sgd:  -1096.9593804623605
new min fval from sgd:  -1096.960661840285
new min fval from sgd:  -1096.9615067428194
new min fval from sgd:  -1096.962998465193
new min fval from sgd:  -1096.9641710292299
new min fval from sgd:  -1096.9648668807831
new min fval from sgd:  -1096.965202086327
new min fval from sgd:  -1096.9652874690285
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.62938]
objective value function right now is: -1096.9477664879848
min fval:  -1096.9652874690285
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.0298, -0.1158],
        [ 0.0298, -0.1158],
        [ 0.0298, -0.1158],
        [ 0.0298, -0.1158],
        [ 0.0298, -0.1158],
        [ 0.0298, -0.1158],
        [ 0.0298, -0.1158],
        [ 0.0298, -0.1158],
        [ 0.0298, -0.1158],
        [ 0.0298, -0.1158]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([0.0422, 0.0422, 0.0422, 0.0422, 0.0422, 0.0422, 0.0422, 0.0422, 0.0422,
        0.0422], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015,
         0.1015],
        [0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015,
         0.1015],
        [0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015,
         0.1015],
        [0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015,
         0.1015],
        [0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015,
         0.1015],
        [0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015,
         0.1015],
        [0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015,
         0.1015],
        [0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015,
         0.1015],
        [0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015,
         0.1015],
        [0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015, 0.1015,
         0.1015]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([0.1721, 0.1721, 0.1721, 0.1721, 0.1721, 0.1721, 0.1721, 0.1721, 0.1721,
        0.1721], device='cuda:0')), ('0.model.hidden_layer_3.weight', tensor([[0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341,
         0.2341],
        [0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341,
         0.2341],
        [0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341,
         0.2341],
        [0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341,
         0.2341],
        [0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341,
         0.2341],
        [0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341,
         0.2341],
        [0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341,
         0.2341],
        [0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341,
         0.2341],
        [0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341,
         0.2341],
        [0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341, 0.2341,
         0.2341]], device='cuda:0')), ('0.model.hidden_layer_3.bias', tensor([0.3417, 0.3417, 0.3417, 0.3417, 0.3417, 0.3417, 0.3417, 0.3417, 0.3417,
        0.3417], device='cuda:0')), ('0.model.output_layer_4.weight', tensor([[-1.6579, -1.6579, -1.6579, -1.6579, -1.6579, -1.6579, -1.6579, -1.6579,
         -1.6579, -1.6579]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-16.3909,  -6.2752],
        [ -3.8069,   1.9118],
        [ -1.5832,   0.4902],
        [ -4.4258,  16.6867],
        [ -3.0606,   8.2644],
        [ -0.1255,  15.8658],
        [ 11.1458,  14.0019],
        [-19.2724,  -5.1029],
        [ 14.8167,  -1.8829],
        [ 16.8083,   1.0028]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -1.7986,  -4.7711,  -3.4169,  12.2539,  -2.3700,  12.6762,   8.8416,
          4.5277, -17.3606, -12.7583], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 5.8830e+00,  2.3760e-02, -1.0643e-01,  7.7015e-01,  2.5239e-02,
         -7.7539e+00, -2.5974e+01,  6.5540e+00, -4.9076e+00, -1.1355e+01],
        [-4.7675e-01, -7.1962e-02,  2.1506e-02, -4.4793e-01, -1.0130e-01,
         -9.0477e-01, -1.6258e+00, -1.2194e+00, -2.4137e-01, -9.2079e-01],
        [-4.7675e-01, -7.1962e-02,  2.1506e-02, -4.4793e-01, -1.0130e-01,
         -9.0477e-01, -1.6258e+00, -1.2194e+00, -2.4137e-01, -9.2079e-01],
        [-4.7676e-01, -7.1962e-02,  2.1506e-02, -4.4793e-01, -1.0130e-01,
         -9.0477e-01, -1.6258e+00, -1.2194e+00, -2.4138e-01, -9.2079e-01],
        [-5.2795e-02, -1.1871e-01, -3.6526e-02, -6.7283e+00, -3.0832e-02,
         -5.8237e+00, -1.1365e+00,  1.8520e+00, -7.4196e-01, -1.5987e+00],
        [ 8.2573e+00,  1.2489e-01,  1.0417e-01, -1.0984e+01,  3.8380e-02,
         -1.5299e+01, -7.8463e+00,  2.6165e+00, -7.8748e+00, -6.9378e+00],
        [-4.7675e-01, -7.1962e-02,  2.1506e-02, -4.4793e-01, -1.0130e-01,
         -9.0477e-01, -1.6258e+00, -1.2194e+00, -2.4137e-01, -9.2079e-01],
        [-4.7675e-01, -7.1962e-02,  2.1506e-02, -4.4793e-01, -1.0130e-01,
         -9.0477e-01, -1.6258e+00, -1.2194e+00, -2.4137e-01, -9.2079e-01],
        [-4.7675e-01, -7.1962e-02,  2.1506e-02, -4.4793e-01, -1.0130e-01,
         -9.0477e-01, -1.6258e+00, -1.2194e+00, -2.4137e-01, -9.2079e-01],
        [ 2.4573e+00, -3.7444e+00,  6.8282e-02,  1.9628e+00,  2.4047e+00,
          5.1397e+00,  1.6047e+00,  3.3397e+00,  3.8110e+00, -1.9979e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-5.0967, -3.9911, -3.9912, -3.9911, -1.7978,  1.3216, -3.9912, -3.9911,
        -3.9911, -8.2917], device='cuda:0')), ('1.model.hidden_layer_3.weight', tensor([[ -0.0562,   0.0963,   0.0963,   0.0963,  -0.1957,  -0.1034,   0.0963,
           0.0963,   0.0963,  -0.2707],
        [  6.6917,  -0.7938,  -0.7938,  -0.7938,   1.0557,   4.8994,  -0.7938,
          -0.7938,  -0.7938,   1.9493],
        [ -0.0562,   0.0963,   0.0963,   0.0963,  -0.1957,  -0.1034,   0.0963,
           0.0963,   0.0963,  -0.2707],
        [ -0.0562,   0.0963,   0.0963,   0.0963,  -0.1957,  -0.1034,   0.0963,
           0.0963,   0.0963,  -0.2707],
        [ -0.0562,   0.0963,   0.0963,   0.0963,  -0.1957,  -0.1034,   0.0963,
           0.0963,   0.0963,  -0.2707],
        [ -0.0562,   0.0963,   0.0963,   0.0963,  -0.1957,  -0.1034,   0.0963,
           0.0963,   0.0963,  -0.2707],
        [ -0.0562,   0.0963,   0.0963,   0.0963,  -0.1957,  -0.1034,   0.0963,
           0.0963,   0.0963,  -0.2707],
        [  1.2646,  -0.1730,  -0.1730,  -0.1730, -13.8337,  -4.6406,  -0.1730,
          -0.1730,  -0.1730,   0.5355],
        [  2.7659,   0.0497,   0.0497,   0.0497,   6.2936,  -7.6349,   0.0497,
           0.0497,   0.0497,   0.8237],
        [ -0.0562,   0.0963,   0.0963,   0.0963,  -0.1957,  -0.1034,   0.0963,
           0.0963,   0.0963,  -0.2707]], device='cuda:0')), ('1.model.hidden_layer_3.bias', tensor([-6.3028, -7.6812, -6.3028, -6.3028, -6.3028, -6.3028, -6.3028, -2.8165,
        -4.7602, -6.3028], device='cuda:0')), ('1.model.output_layer_4.weight', tensor([[ 0.3320, -5.0273,  0.3320,  0.3320,  0.3320,  0.3320,  0.3320,  7.7738,
          6.9642,  0.3320],
        [-0.3320,  5.0829, -0.3320, -0.3320, -0.3320, -0.3320, -0.3320, -7.7751,
         -7.0807, -0.3320]], device='cuda:0'))])
xi:  [204.58473]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1092.0068445716179
W_T_median: 902.0275292805658
W_T_pctile_5: 204.68230584299357
W_T_CVAR_5_pct: 11.96631675562344
Average q (qsum/M+1):  35.00000787550403
Optimal xi:  [204.58473]
Expected(across Rb) median(across samples) p_equity:  0.20090447117884955
obj fun:  tensor(-1096.9653, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
