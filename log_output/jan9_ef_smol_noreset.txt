Starting at: 
09-01-23_12:10

 Random seed:  2  

Key parameters-------
paths: 25600
iterations: 3000
batchsize: 500


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1  logistic_sigmoid   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1584.180033097498
W_T_median: 1144.1177591225853
W_T_pctile_5: -126.33840259751949
W_T_CVAR_5_pct: -294.9519820713226
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1679.8299729969156
Current xi:  [-25.072227]
objective value function right now is: -1679.8299729969156
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1692.9248585221562
Current xi:  [-47.694324]
objective value function right now is: -1692.9248585221562
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1702.144867526843
Current xi:  [-72.36482]
objective value function right now is: -1702.144867526843
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1708.6400752231523
Current xi:  [-98.49725]
objective value function right now is: -1708.6400752231523
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1715.9281582351668
Current xi:  [-124.75978]
objective value function right now is: -1715.9281582351668
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1723.0198065394973
Current xi:  [-150.55382]
objective value function right now is: -1723.0198065394973
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1732.1463885235473
Current xi:  [-176.60359]
objective value function right now is: -1732.1463885235473
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1738.298526640421
Current xi:  [-202.86873]
objective value function right now is: -1738.298526640421
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1743.7322159792777
Current xi:  [-228.88135]
objective value function right now is: -1743.7322159792777
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1747.9666997286704
Current xi:  [-254.30788]
objective value function right now is: -1747.9666997286704
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1752.5219819250208
Current xi:  [-279.4204]
objective value function right now is: -1752.5219819250208
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1756.8532287241258
Current xi:  [-303.817]
objective value function right now is: -1756.8532287241258
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1760.2367109605148
Current xi:  [-327.50726]
objective value function right now is: -1760.2367109605148
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1762.931311632443
Current xi:  [-351.38806]
objective value function right now is: -1762.931311632443
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1767.3627003669562
Current xi:  [-374.54593]
objective value function right now is: -1767.3627003669562
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1770.606765653551
Current xi:  [-396.63028]
objective value function right now is: -1770.606765653551
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1773.2022266945685
Current xi:  [-417.90607]
objective value function right now is: -1773.2022266945685
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1775.8695346559812
Current xi:  [-438.6156]
objective value function right now is: -1775.8695346559812
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1778.277041790898
Current xi:  [-459.33835]
objective value function right now is: -1778.277041790898
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1780.4883624271918
Current xi:  [-479.4554]
objective value function right now is: -1780.4883624271918
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1782.756341295888
Current xi:  [-498.76065]
objective value function right now is: -1782.756341295888
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1784.3261294158688
Current xi:  [-517.40375]
objective value function right now is: -1784.3261294158688
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-535.6785]
objective value function right now is: -1780.9046563892352
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-556.0216]
objective value function right now is: -1783.2156754966124
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-575.5031]
objective value function right now is: -1783.9531562829097
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1785.445906785783
Current xi:  [-594.1007]
objective value function right now is: -1785.445906785783
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1787.6796021604393
Current xi:  [-612.04877]
objective value function right now is: -1787.6796021604393
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1788.2748007476473
Current xi:  [-628.9945]
objective value function right now is: -1788.2748007476473
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1790.0439775844752
Current xi:  [-645.0352]
objective value function right now is: -1790.0439775844752
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1794.2334749887016
Current xi:  [-661.12384]
objective value function right now is: -1794.2334749887016
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1795.9375381430675
Current xi:  [-674.69415]
objective value function right now is: -1795.9375381430675
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1796.2894122135115
Current xi:  [-687.2946]
objective value function right now is: -1796.2894122135115
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1796.7480960864752
Current xi:  [-699.3131]
objective value function right now is: -1796.7480960864752
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1797.7394041702782
Current xi:  [-711.45435]
objective value function right now is: -1797.7394041702782
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-723.289]
objective value function right now is: -1797.7294095369662
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1798.7385012391603
Current xi:  [-735.24603]
objective value function right now is: -1798.7385012391603
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1799.1049680613896
Current xi:  [-745.8994]
objective value function right now is: -1799.1049680613896
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1799.1393263478205
Current xi:  [-756.8286]
objective value function right now is: -1799.1393263478205
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1799.7943961726385
Current xi:  [-766.3704]
objective value function right now is: -1799.7943961726385
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1800.4716374448349
Current xi:  [-775.8816]
objective value function right now is: -1800.4716374448349
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1800.7714478679811
Current xi:  [-786.5482]
objective value function right now is: -1800.7714478679811
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1800.8536897329636
Current xi:  [-795.65137]
objective value function right now is: -1800.8536897329636
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1801.0653402745334
Current xi:  [-803.77106]
objective value function right now is: -1801.0653402745334
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-812.0378]
objective value function right now is: -1799.7691146362765
new min fval from sgd:  -1801.2621491975121
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-820.1782]
objective value function right now is: -1801.2621491975121
new min fval from sgd:  -1801.2965811384859
new min fval from sgd:  -1801.361001481808
new min fval from sgd:  -1801.3772536444062
new min fval from sgd:  -1801.5340168194791
new min fval from sgd:  -1801.6419057082433
new min fval from sgd:  -1801.7506029596495
new min fval from sgd:  -1801.752687668776
new min fval from sgd:  -1801.7708146763446
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-827.29065]
objective value function right now is: -1801.3902557173847
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-834.5937]
objective value function right now is: -1801.4433732613832
new min fval from sgd:  -1801.823501186445
new min fval from sgd:  -1801.887218546804
new min fval from sgd:  -1801.963289592576
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-842.23517]
objective value function right now is: -1801.9336209880678
new min fval from sgd:  -1802.0925743001071
new min fval from sgd:  -1802.0936232722709
new min fval from sgd:  -1802.139982110273
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-848.2044]
objective value function right now is: -1802.1314190601195
new min fval from sgd:  -1802.1466419060137
new min fval from sgd:  -1802.1622708848165
new min fval from sgd:  -1802.1762084103143
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-854.4068]
objective value function right now is: -1801.0534704819263
min fval:  -1802.1762084103143
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-4.9893e+00,  5.3833e-01],
        [-5.9357e+00, -2.5118e+00],
        [-2.1553e+00,  1.5484e+00],
        [ 5.7043e+00,  2.6008e+00],
        [-4.1230e+00,  8.2008e-01],
        [-6.4789e+00,  6.0180e-03],
        [ 1.6590e-01, -2.0052e+00],
        [-2.2198e+00,  1.5191e+00],
        [-1.4987e+00,  1.7982e+00],
        [ 7.0381e-02,  2.3760e+00]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  7.9312,  -3.5511,   8.9490,   4.0350,  10.1552,   8.8147,  -3.4489,
           9.8432,   8.1431,   7.1322],
        [  8.2681,  -3.7064,   8.5952,   4.5312,  10.0192,   8.6326,  -3.3440,
           9.2523,   7.8760,   6.8853],
        [ 10.3636,  -2.9536,  11.7759,   6.8144,  12.9382,  11.4101,  -3.5065,
          12.7762,  11.2588,   9.5895],
        [ -9.9602,   3.0619, -11.9550,  -6.5728, -13.8506, -12.5272,   3.0376,
         -12.9543, -11.9753, -10.5865],
        [ 11.9106,  -4.0163,  14.2144,   9.9832,  15.1458,  13.1897,  -3.4389,
          14.4036,  12.8665,  11.5880],
        [ 11.1899,  -4.2099,  14.0912,  10.2579,  14.4063,  13.0099,  -3.3402,
          13.5670,  12.0720,  10.7411],
        [ -3.2920,  -7.2300,  -1.5698,   0.4304,  -2.6055,  -3.3814,  -6.3700,
          -1.8732,  -1.7930,  -1.9921],
        [ 11.7767,  -3.7827,  13.3821,   8.9179,  14.1055,  12.3157,  -3.3082,
          13.6008,  12.0447,  10.6922],
        [  9.3884,  -3.2044,  10.1824,   4.5953,  11.1811,   9.3262,  -3.2842,
          10.6986,   9.4670,   8.0074],
        [ 11.2336,  -4.0026,  13.4719,   9.4968,  13.8923,  12.5472,  -3.2875,
          13.2495,  11.8708,  10.3836]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  3.5071,   3.8119,   6.2781, -18.3567,  11.6802,  10.7078,   0.4179,
           9.3877,   4.3583,   9.1608]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  1.3638,   3.3301],
        [  0.5501,   3.0732],
        [ -0.8565,   3.1477],
        [  9.9286,   3.7446],
        [ -4.2878,   0.6712],
        [ -1.3954,   3.2778],
        [ -4.3350,   0.3365],
        [ -3.8551,   0.3659],
        [-27.3095,  -2.1161],
        [-27.5949,  -6.6827]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-10.8323, -11.8680, -11.5470,  -4.2330,   1.8978, -11.9667,   2.5797,
           3.1471,  -5.2636,  -6.9930],
        [ -2.3542,  -9.2766,  -8.0997,  -8.8822,  -4.0184,  -7.0358,  -3.7628,
          -7.1092,  -5.9636,  -0.9273],
        [-10.9879, -12.4956, -10.8853,  -7.0270,  -3.2614, -11.2492,  -3.6472,
          -3.6383,  -3.9987, -10.8938],
        [ -0.5601,   0.9328,  -1.0115,  -5.8055,  15.7485,  -1.7891,  15.0803,
          15.0783,  -3.5415,  -1.0692],
        [-11.1479, -12.3709, -12.1411,  -3.6227,   4.1750, -12.0738,   4.4049,
           3.8954,  -5.5184,  -6.5541],
        [  0.1089,   0.2619,  -0.4471,  -0.9478,   7.3729,  -0.5018,   7.3235,
           7.1800,  -1.6984,   0.9225],
        [ -9.9765, -12.1425, -11.7574,  -4.2295,   1.5585, -12.4032,   2.2381,
           2.6316,  -5.2605,  -7.0525],
        [-10.1980, -12.0435, -11.9509,  -5.1526,  -0.1356, -12.4490,  -0.1732,
          -0.2546,  -4.8322,  -8.3979],
        [-11.7042, -13.3964, -11.9210,  -3.8046,   5.2437, -12.2389,   5.3189,
           5.0014,  -5.5529,  -6.7131],
        [-10.2130, -11.7837, -11.8519,  -4.5576,   1.1185, -12.4465,   1.0969,
           0.8163,  -5.0997,  -7.5631]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-3.6504,  3.8645, -0.5513,  3.3394, -5.1830,  1.5835, -3.7569, -2.1234,
         -6.2353, -2.9632],
        [ 4.1034, -3.8346,  0.4513, -3.2202,  4.8613, -1.6213,  4.1435,  2.4550,
          6.7184,  2.9244]], device='cuda:0'))])
xi:  [-851.14404]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: -352.22205352606665
W_T_median: -451.25358488966833
W_T_pctile_5: -882.5802021745166
W_T_CVAR_5_pct: -1002.6926483354903
Average q (qsum/M+1):  59.75777706023185
Optimal xi:  [-851.14404]
Expected(across Rb) median(across samples) p_equity:  0.09878834569371975
obj fun:  tensor(-1802.1762, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1584.180033097498
W_T_median: 1144.1177591225853
W_T_pctile_5: -126.33840259751949
W_T_CVAR_5_pct: -294.9519820713226
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1597.3620044293464
Current xi:  [-12.018675]
objective value function right now is: -1597.3620044293464
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1635.0073193541257
Current xi:  [-26.780241]
objective value function right now is: -1635.0073193541257
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1646.2437513531077
Current xi:  [-42.69757]
objective value function right now is: -1646.2437513531077
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1652.7627166581292
Current xi:  [-58.915977]
objective value function right now is: -1652.7627166581292
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1656.5710890329628
Current xi:  [-75.48581]
objective value function right now is: -1656.5710890329628
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1658.8754536448641
Current xi:  [-91.762146]
objective value function right now is: -1658.8754536448641
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1667.0576426144366
Current xi:  [-107.26734]
objective value function right now is: -1667.0576426144366
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-122.7816]
objective value function right now is: -1657.3582829626248
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-138.87218]
objective value function right now is: -1660.4973460556448
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-154.37851]
objective value function right now is: -1664.8283546515136
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-169.7805]
objective value function right now is: -1665.1164329774422
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1668.642408154257
Current xi:  [-183.75984]
objective value function right now is: -1668.642408154257
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1669.1727507158648
Current xi:  [-198.52432]
objective value function right now is: -1669.1727507158648
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1673.9006746986183
Current xi:  [-211.84944]
objective value function right now is: -1673.9006746986183
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1675.0681537533894
Current xi:  [-224.63039]
objective value function right now is: -1675.0681537533894
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1675.792857611872
Current xi:  [-237.61139]
objective value function right now is: -1675.792857611872
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.456633500104
Current xi:  [-250.61407]
objective value function right now is: -1678.456633500104
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1680.8391858079776
Current xi:  [-262.16025]
objective value function right now is: -1680.8391858079776
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1682.8215787319537
Current xi:  [-273.10507]
objective value function right now is: -1682.8215787319537
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1683.8518681336675
Current xi:  [-284.5997]
objective value function right now is: -1683.8518681336675
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-293.89728]
objective value function right now is: -1683.3234677607975
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1684.5876544488012
Current xi:  [-303.76367]
objective value function right now is: -1684.5876544488012
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.0590215430327
Current xi:  [-313.75244]
objective value function right now is: -1686.0590215430327
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1687.304981659139
Current xi:  [-323.0665]
objective value function right now is: -1687.304981659139
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-330.4965]
objective value function right now is: -1677.6139310279868
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-339.06647]
objective value function right now is: -1684.7687761616644
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-347.02957]
objective value function right now is: -1686.2701193588239
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1688.4553737401814
Current xi:  [-353.98035]
objective value function right now is: -1688.4553737401814
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1689.6099433365907
Current xi:  [-361.2384]
objective value function right now is: -1689.6099433365907
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-367.89163]
objective value function right now is: -1689.3266739294404
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-373.76913]
objective value function right now is: -1686.746870441074
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1690.0846399983038
Current xi:  [-380.9009]
objective value function right now is: -1690.0846399983038
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1690.7354870670956
Current xi:  [-387.44583]
objective value function right now is: -1690.7354870670956
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1690.7827805780357
Current xi:  [-392.70474]
objective value function right now is: -1690.7827805780357
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -1690.9603866996433
Current xi:  [-397.87543]
objective value function right now is: -1690.9603866996433
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-403.4825]
objective value function right now is: -1690.180438255607
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1691.6454226860405
Current xi:  [-409.06973]
objective value function right now is: -1691.6454226860405
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-412.98978]
objective value function right now is: -1691.285405329673
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-417.2045]
objective value function right now is: -1689.6338190944255
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1691.775496354719
Current xi:  [-422.1247]
objective value function right now is: -1691.775496354719
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-425.2489]
objective value function right now is: -1690.8518349740393
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1692.0503865390108
Current xi:  [-429.40918]
objective value function right now is: -1692.0503865390108
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1693.0338413432455
Current xi:  [-432.7666]
objective value function right now is: -1693.0338413432455
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-436.1171]
objective value function right now is: -1690.8825854534487
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-439.37518]
objective value function right now is: -1692.631083509464
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-440.79526]
objective value function right now is: -1692.2682836818392
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-441.95068]
objective value function right now is: -1692.5399539697205
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-444.6289]
objective value function right now is: -1689.5271096710453
new min fval from sgd:  -1693.1185738367735
new min fval from sgd:  -1693.1203097539396
new min fval from sgd:  -1693.1556834622409
new min fval from sgd:  -1693.1691829205404
new min fval from sgd:  -1693.2216712381025
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-446.43393]
objective value function right now is: -1692.6318009513122
new min fval from sgd:  -1693.2864040792
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-447.40182]
objective value function right now is: -1693.1077876271395
min fval:  -1693.2864040792
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.2845,  2.6883],
        [-2.4582, -2.9397],
        [ 0.1503,  2.3984],
        [ 5.6107,  3.9700],
        [ 0.2091,  2.5351],
        [ 0.3235,  2.9082],
        [-0.0914, -2.0686],
        [ 0.1543,  2.4002],
        [ 0.1510,  2.3905],
        [ 0.1596,  2.4119]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 8.2744e+00, -5.2323e+00,  9.4686e+00,  1.7381e+00,  1.0546e+01,
          9.4540e+00, -5.3230e+00,  1.0354e+01,  8.7791e+00,  8.1032e+00],
        [ 8.4751e+00, -5.4965e+00,  8.9250e+00,  2.7448e+00,  1.0274e+01,
          9.2564e+00, -5.3388e+00,  9.5763e+00,  8.2878e+00,  7.5480e+00],
        [ 1.1052e+01, -3.0549e+00,  1.4868e+01,  7.0148e+00,  1.4415e+01,
          1.0856e+01, -3.9823e+00,  1.5813e+01,  1.4889e+01,  1.4239e+01],
        [-1.0024e+01,  3.2743e+00, -1.5211e+01, -6.5643e+00, -1.4924e+01,
         -1.1111e+01,  3.6748e+00, -1.6139e+01, -1.5899e+01, -1.5650e+01],
        [ 1.2679e+01, -3.8391e+00,  1.7774e+01,  1.1484e+01,  1.6853e+01,
          1.2520e+01, -3.7836e+00,  1.7904e+01,  1.6955e+01,  1.6499e+01],
        [ 1.2366e+01, -3.8900e+00,  1.7792e+01,  1.1850e+01,  1.6470e+01,
          1.2767e+01, -3.5961e+00,  1.7217e+01,  1.6219e+01,  1.5543e+01],
        [-1.8393e+01, -6.5378e+00, -3.6080e+00,  3.8934e+00, -1.4154e+01,
         -1.5034e+01, -3.4981e+00, -4.1774e+00, -1.8359e+00,  1.0895e-02],
        [ 1.2620e+01, -3.6647e+00,  1.6841e+01,  1.0118e+01,  1.5833e+01,
          1.1781e+01, -3.6972e+00,  1.7004e+01,  1.6013e+01,  1.5485e+01],
        [ 1.0662e+01, -3.8285e+00,  1.2176e+01,  3.2337e+00,  1.2675e+01,
          1.0404e+01, -4.1394e+00,  1.2670e+01,  1.1712e+01,  1.0865e+01],
        [ 1.2446e+01, -3.7583e+00,  1.7039e+01,  1.0960e+01,  1.5930e+01,
          1.2430e+01, -3.5932e+00,  1.6769e+01,  1.5872e+01,  1.5052e+01]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  2.7074,   2.7590,   7.6886, -22.6800,  13.9511,  12.6310,  -7.5809,
          11.2786,   4.5351,  10.9264]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  1.9691,   3.6388],
        [ -2.0121,   2.8610],
        [ -3.4936,   2.5364],
        [ 12.1337,   3.7012],
        [ -3.5697,   0.7960],
        [ -3.4894,   2.5817],
        [ -3.4160,   0.7678],
        [ -3.2388,   0.7676],
        [-31.2904,  -3.3199],
        [-27.0345,  -7.7606]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-18.4070, -16.5906, -17.0761,  -4.1540,   7.9858, -18.8225,   8.1459,
           8.6828, -24.4973,  -4.7235],
        [-62.3957, -61.9091, -55.7580, -11.5639, -12.9600, -56.1975, -10.0647,
         -13.1822,   5.4804,  11.0579],
        [-21.2201, -31.6814, -37.4835,  -2.7013, -15.8415, -38.1030, -14.2241,
         -12.1668, -27.6657, -12.6153],
        [ -0.1906,   0.1379,  -4.1539,  -7.9172,  15.7931,  -6.2711,  15.2727,
          15.7743,  -4.4619,  -4.9447],
        [-17.9497, -18.8721, -20.6873,  -4.3991,   5.0731, -22.0590,   5.6484,
           5.4949, -18.0961,   0.1335],
        [  0.7675,  -0.3739,  -2.2781,  -2.9557,  11.0806,  -2.5538,  10.5634,
           9.7676,   3.8887,  -5.3092],
        [-17.4744, -16.7989, -17.0848,  -4.3362,   7.7984, -19.0376,   7.9574,
           8.3075, -23.9322,  -4.6853],
        [-16.0735, -15.0428, -15.2081,  -6.1993,   6.7088, -16.7802,   6.2122,
           5.9883, -20.5365,  -4.8813],
        [-18.1680, -19.7800, -20.6909,  -3.8071,   6.4282, -22.2591,   6.7118,
           6.6727, -18.1209,  -0.5581],
        [-16.8131, -15.5676, -16.1656,  -5.1694,   7.6243, -17.9403,   7.1023,
           6.7583, -22.2984,  -4.7395]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -6.1187,  30.5385,   9.1009,   7.0881,  -8.2511,   1.8466,  -6.0976,
          -4.3217,  -9.5433,  -5.2611],
        [  6.5717, -30.5085,  -9.2009,  -6.9687,   7.9294,  -1.8845,   6.4842,
           4.6532,  10.0264,   5.2224]], device='cuda:0'))])
xi:  [-446.5625]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: -115.57071401804504
W_T_median: -254.9440157111507
W_T_pctile_5: -447.2010706582424
W_T_CVAR_5_pct: -486.4541918976458
Average q (qsum/M+1):  57.760592552923384
Optimal xi:  [-446.5625]
Expected(across Rb) median(across samples) p_equity:  0.16744092977455097
obj fun:  tensor(-1693.2864, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1584.180033097498
W_T_median: 1144.1177591225853
W_T_pctile_5: -126.33840259751949
W_T_CVAR_5_pct: -294.9519820713226
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1494.732585872105
Current xi:  [-8.235314]
objective value function right now is: -1494.732585872105
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1544.6023403262452
Current xi:  [-17.600317]
objective value function right now is: -1544.6023403262452
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1558.3740557964315
Current xi:  [-27.584604]
objective value function right now is: -1558.3740557964315
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1565.5934475066813
Current xi:  [-38.147858]
objective value function right now is: -1565.5934475066813
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1570.40313497814
Current xi:  [-47.246216]
objective value function right now is: -1570.40313497814
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1572.676492238511
Current xi:  [-56.390953]
objective value function right now is: -1572.676492238511
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-64.901054]
objective value function right now is: -1568.7003403767874
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.12092]
objective value function right now is: -1567.9158824787144
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-81.88952]
objective value function right now is: -1566.073939272429
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1573.6712359960861
Current xi:  [-89.993706]
objective value function right now is: -1573.6712359960861
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1578.3010822255426
Current xi:  [-95.87907]
objective value function right now is: -1578.3010822255426
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1580.745800365237
Current xi:  [-100.359566]
objective value function right now is: -1580.745800365237
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1581.9681946886244
Current xi:  [-106.84908]
objective value function right now is: -1581.9681946886244
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-113.85274]
objective value function right now is: -1580.4650020799847
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-120.03712]
objective value function right now is: -1573.3010581377998
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-126.384926]
objective value function right now is: -1574.2102327363123
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-133.39427]
objective value function right now is: -1576.3829811966987
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-140.18088]
objective value function right now is: -1565.8050053588213
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-146.35063]
objective value function right now is: -1559.9595054094323
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-152.1616]
objective value function right now is: -1575.1188041002217
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1582.15042625897
Current xi:  [-154.54448]
objective value function right now is: -1582.15042625897
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1585.0535177856466
Current xi:  [-157.04369]
objective value function right now is: -1585.0535177856466
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-160.40562]
objective value function right now is: -1552.4322642188367
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-164.09576]
objective value function right now is: -1579.7074247495752
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-168.6673]
objective value function right now is: -1584.6715340467715
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-171.05156]
objective value function right now is: -1583.5523747575442
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-173.4382]
objective value function right now is: -1584.081718754991
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1586.394271187386
Current xi:  [-175.4009]
objective value function right now is: -1586.394271187386
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-177.19818]
objective value function right now is: -1585.091860290948
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-178.1034]
objective value function right now is: -1584.382383723486
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-179.82076]
objective value function right now is: -1583.7744276446513
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1587.3958683020567
Current xi:  [-181.32628]
objective value function right now is: -1587.3958683020567
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-183.16794]
objective value function right now is: -1587.2656551278092
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-185.24245]
objective value function right now is: -1584.1889964918857
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-187.06308]
objective value function right now is: -1587.2551570071205
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-186.42389]
objective value function right now is: -1586.1288798980454
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-188.61508]
objective value function right now is: -1585.058207475228
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-190.23654]
objective value function right now is: -1585.0118136443548
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-190.43959]
objective value function right now is: -1577.7746236891167
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-190.2277]
objective value function right now is: -1584.062885916683
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-191.72835]
objective value function right now is: -1573.501024127943
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-193.63733]
objective value function right now is: -1583.352317325068
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-192.74948]
objective value function right now is: -1570.1666191043598
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-193.6142]
objective value function right now is: -1582.2268589152102
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-194.87169]
objective value function right now is: -1586.6644251579012
new min fval from sgd:  -1587.5199959456002
new min fval from sgd:  -1587.7336308876338
new min fval from sgd:  -1588.0401390085754
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-195.17902]
objective value function right now is: -1588.0401390085754
new min fval from sgd:  -1588.2045248821214
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-195.9639]
objective value function right now is: -1580.1967787454726
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-193.68118]
objective value function right now is: -1576.8655058469574
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-194.25711]
objective value function right now is: -1583.0083044981602
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-195.71867]
objective value function right now is: -1587.2600756474576
min fval:  -1588.2045248821214
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.1486,  3.1529],
        [-0.1134, -2.7595],
        [ 0.1421,  3.0838],
        [ 0.2339,  4.0433],
        [ 0.1443,  3.1067],
        [ 0.1509,  3.1761],
        [-0.0927, -2.5491],
        [ 0.1421,  3.0840],
        [ 0.1423,  3.0867],
        [ 0.1428,  3.0923]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  7.4839,  -7.4903,   8.4402,   1.5001,   9.6174,   8.8206,  -7.5632,
           9.3269,   7.7490,   7.0904],
        [  7.5001,  -7.8963,   7.7281,   2.2537,   9.1695,   8.4264,  -7.7252,
           8.3802,   7.0894,   6.3655],
        [ 13.2189,  -3.0658,  16.6576,   8.1437,  16.3659,  13.2756,  -3.9812,
          17.6031,  16.6727,  16.0500],
        [-12.6973,   2.9441, -17.4727,  -7.7244, -17.3639, -14.0618,   3.3349,
         -18.4014, -18.1536, -17.9344],
        [ 15.2365,  -3.5592,  19.9699,  12.9083,  19.2027,  15.3185,  -3.5177,
          20.1000,  19.1436,  18.7134],
        [ 14.8561,  -3.6559,  19.9296,  13.2896,  18.7572,  15.4928,  -3.3771,
          19.3540,  18.3492,  17.6983],
        [-19.2222,  -5.8429,  -4.2702,   3.6191, -14.8749, -15.9343,  -2.8254,
          -4.8340,  -2.4901,  -0.6538],
        [ 15.0492,  -3.4752,  18.9126,  11.5000,  18.0571,  14.4501,  -3.5182,
          19.0755,  18.0775,  17.5755],
        [ 11.6632,  -4.7009,  12.8363,   3.9264,  13.4830,  11.6375,  -4.9842,
          13.3326,  12.3698,  11.5469],
        [ 14.8417,  -3.5902,  19.0869,  12.3740,  18.1254,  15.0588,  -3.4396,
          18.8156,  17.9121,  17.1173]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  1.6918,   1.5755,   8.4716, -24.0028,  15.1816,  13.6283,  -8.4356,
          12.3096,   4.4815,  11.8344]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  3.3207,   3.0732],
        [  1.3915,   2.7016],
        [ -0.3971,   2.5927],
        [ 15.1285,   2.8780],
        [ -5.8930,   1.4229],
        [ -0.3841,   2.5939],
        [ -5.9949,   1.4666],
        [ -5.9383,   1.4373],
        [-23.1288,  -9.2697],
        [-25.0664,  -8.9154]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-23.8874, -21.2018, -20.7055,  -9.5790,   8.4613, -22.6239,   8.5794,
           9.1611, -17.6872,  -6.3571],
        [-76.4565, -64.7491, -53.5886, -16.5674,  -3.8955, -54.2776,  -1.7215,
          -3.9272,   9.1519,  10.0448],
        [-10.7482, -17.6773, -22.6437,  -2.3780, -10.8100, -22.6622,  -9.1271,
          -7.9179, -47.1111, -28.8797],
        [ -1.1624,  -8.0230, -15.3232,  -5.0012,   2.2990, -17.4619,   1.9228,
           2.5981, -28.8707, -21.6781],
        [-14.9363, -27.6980, -31.9990,  -5.3620,  -2.7573, -33.4087,  -1.3898,
          -2.5756,   0.4118,   8.8803],
        [  1.1888,   0.7181,  -0.7365,  -2.7841,  22.2400,  -0.7254,  21.8292,
          20.8953,  -1.2860,  -9.9971],
        [-23.0081, -21.4278, -20.7426,  -9.7520,   8.2961, -22.8705,   8.4073,
           8.8042, -16.6252,  -6.0881],
        [-21.0872, -18.5435, -17.8899, -11.3404,   7.4744, -19.5979,   6.8990,
           6.7020, -11.8647,  -4.3750],
        [-23.7311, -25.9175, -27.3449,  -9.0310,   4.6329, -29.3146,   4.9151,
           4.9659, -11.1067,   1.1337],
        [-22.2231, -19.7888, -19.5047, -10.4780,   8.2478, -21.4437,   7.6671,
           7.3663, -14.2706,  -5.4727]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -1.1692,  30.8242,   9.7489,   9.8379,  -7.6338,   1.4532,  -1.1665,
           0.1320,  -5.0004,  -0.4961],
        [  1.6222, -30.7942,  -9.8489,  -9.7186,   7.3121,  -1.4910,   1.5531,
           0.1995,   5.4835,   0.4574]], device='cuda:0'))])
xi:  [-195.35837]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 152.12970749245912
W_T_median: 0.4317576527940403
W_T_pctile_5: -197.50154035661188
W_T_CVAR_5_pct: -257.36776218013637
Average q (qsum/M+1):  55.386206842237904
Optimal xi:  [-195.35837]
Expected(across Rb) median(across samples) p_equity:  0.3778722542027632
obj fun:  tensor(-1588.2045, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1584.180033097498
W_T_median: 1144.1177591225853
W_T_pctile_5: -126.33840259751949
W_T_CVAR_5_pct: -294.9519820713226
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1374.3920718912862
Current xi:  [-5.1837077]
objective value function right now is: -1374.3920718912862
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.403417]
objective value function right now is: -1291.1995718376822
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-28.914837]
objective value function right now is: -1338.544346025251
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1430.682776204678
Current xi:  [-32.929043]
objective value function right now is: -1430.682776204678
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1445.6269338234345
Current xi:  [-39.481323]
objective value function right now is: -1445.6269338234345
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1452.568350713034
Current xi:  [-50.57853]
objective value function right now is: -1452.568350713034
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1456.9523460497373
Current xi:  [-60.80586]
objective value function right now is: -1456.9523460497373
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1459.1599274337557
Current xi:  [-70.61769]
objective value function right now is: -1459.1599274337557
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1459.6697264146985
Current xi:  [-78.273865]
objective value function right now is: -1459.6697264146985
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-82.78402]
objective value function right now is: -1458.9939597514506
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-89.016235]
objective value function right now is: -1459.0956845180751
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-94.26212]
objective value function right now is: -1449.6062005754682
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1463.9596300029673
Current xi:  [-97.2045]
objective value function right now is: -1463.9596300029673
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-100.99116]
objective value function right now is: -1453.3399187311838
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-104.12424]
objective value function right now is: -1463.044581459199
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1466.2400921098388
Current xi:  [-103.61275]
objective value function right now is: -1466.2400921098388
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-104.01359]
objective value function right now is: -1464.8271391859512
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-102.98075]
objective value function right now is: -1465.6536795241927
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-104.88959]
objective value function right now is: -1464.0142668281485
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-107.95669]
objective value function right now is: -1459.9299684555613
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.86593]
objective value function right now is: -1459.6593705323048
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-107.70472]
objective value function right now is: -1440.864833836057
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-107.88341]
objective value function right now is: -1461.6904814712752
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-108.56816]
objective value function right now is: -1464.8846592801635
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-108.18184]
objective value function right now is: -1460.3696629969986
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-112.52396]
objective value function right now is: -1465.370611771536
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-109.94135]
objective value function right now is: -1456.9090266151316
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-109.6615]
objective value function right now is: -1462.217455315816
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-110.87268]
objective value function right now is: -1432.3433509091276
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-108.76745]
objective value function right now is: -1465.3680766839109
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-109.80572]
objective value function right now is: -1466.12976670684
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-111.807884]
objective value function right now is: -1463.7618884721055
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-112.74139]
objective value function right now is: -1463.1894736933214
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-111.011444]
objective value function right now is: -1465.822511803101
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-115.06303]
objective value function right now is: -1464.0416550079253
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-112.97549]
objective value function right now is: -1448.2504093039208
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-110.25758]
objective value function right now is: -1461.9683337570823
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-111.5068]
objective value function right now is: -1463.91099180963
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-113.84324]
objective value function right now is: -1462.4267659272791
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-113.14368]
objective value function right now is: -1465.6640013190731
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-114.507904]
objective value function right now is: -1463.6943315271883
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-113.25325]
objective value function right now is: -1458.556990199458
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-113.63737]
objective value function right now is: -1461.697446728212
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-116.643936]
objective value function right now is: -1461.1137794235256
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-116.806694]
objective value function right now is: -1463.1749985183171
new min fval from sgd:  -1466.2873858339112
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-114.6326]
objective value function right now is: -1464.4165645046162
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-115.6653]
objective value function right now is: -1460.6346238184906
new min fval from sgd:  -1466.355427130203
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-114.81146]
objective value function right now is: -1464.807047061967
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-115.42337]
objective value function right now is: -1465.669020723411
new min fval from sgd:  -1466.4763640383853
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-114.935234]
objective value function right now is: -1463.4770191585492
min fval:  -1466.4763640383853
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.5627,  3.6297],
        [ 0.4636, -2.8772],
        [-0.5584,  3.5984],
        [-0.5760,  3.7258],
        [-0.5591,  3.6035],
        [-0.5625,  3.6286],
        [ 0.4635, -2.8775],
        [-0.5583,  3.5977],
        [-0.5588,  3.6015],
        [-0.5592,  3.6043]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  6.1808, -11.0129,   7.0900,   0.6076,   8.2793,   7.5254, -11.0843,
           7.9766,   6.4024,   5.7478],
        [  6.2592, -11.2502,   6.4480,   1.3602,   7.8996,   7.1925, -11.0778,
           7.1000,   5.8121,   5.0916],
        [ 16.2541,  -3.0346,  19.5790,  12.0613,  19.3157,  16.3278,  -3.9441,
          20.5241,  19.6028,  18.9898],
        [-16.0558,   2.7032, -20.7176, -11.9506, -20.6368, -17.4365,   3.0890,
         -21.6459, -21.4073, -21.1976],
        [ 18.7134,  -3.1935,  23.3334,  17.2518,  22.5942,  18.8116,  -3.1471,
          23.4630,  22.5158,  22.0952],
        [ 18.3156,  -3.3015,  23.2759,  17.6144,  22.1315,  18.9685,  -3.0178,
          22.6999,  21.7042,  21.0628],
        [-24.0875,  -7.8171,  -9.0225,  -2.2050, -19.6564, -20.8191,  -4.8058,
          -9.5858,  -7.2505,  -5.4238],
        [ 18.4759,  -3.1412,  22.2260,  15.7935,  21.3985,  17.8930,  -3.1790,
          22.3885,  21.3996,  20.9071],
        [ 12.4015,  -6.4929,  13.4717,   5.5103,  14.1446,  12.3924,  -6.7701,
          13.9679,  13.0131,  12.1990],
        [ 18.2831,  -3.2430,  22.4151,  16.6816,  21.4815,  18.5165,  -3.0874,
          22.1434,  21.2490,  20.4637]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.6025,   0.6103,   9.4276, -25.7130,  17.3488,  15.6408,  -6.9878,
          14.0658,   3.7652,  13.6342]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  4.9136,   2.5516],
        [  4.0207,   2.4473],
        [  2.7373,   2.2977],
        [ 14.2359,   4.2304],
        [ -4.7211,  -2.9351],
        [  2.7123,   2.2982],
        [ -7.0821,  -2.6445],
        [  2.7588,  -1.5564],
        [-21.8347, -10.9365],
        [-22.8699, -11.6254]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-25.7593, -23.7710, -24.7234, -12.5207,   5.4221, -26.7908,   5.7364,
           5.7991, -21.3872,  -8.4003],
        [-78.7724, -67.7539, -56.9205, -15.2867,  -4.9644, -57.2598,  -6.7213,
          -5.0486,  10.1861,  10.6603],
        [-10.8528, -17.2921, -21.1006,  -1.9370,  -8.9946, -21.0613,  -7.5818,
          -6.0194, -46.6347, -28.7645],
        [  2.6059,  -3.2358,  -8.9523,  -4.9213,   2.3673, -11.1319,   7.2269,
           3.3054, -27.1585, -19.2392],
        [-15.9839, -27.8123, -32.2888,  -5.3316, -10.3408, -33.9178,  -8.2145,
         -10.6127,   0.3858,   8.1495],
        [  2.2732,   1.9140,   1.2066,  -1.9072,  29.1427,   1.2490,  25.1805,
          35.1991,   1.9823,  -4.0939],
        [-25.0648, -24.1681, -24.8971, -12.8441,   5.0153, -27.1715,   5.1352,
           5.2084, -20.2240,  -7.9342],
        [-39.9767, -34.1176, -30.8879, -19.1715,   9.1979, -32.3774,  12.5919,
           8.0127,  -8.1521,  -1.2849],
        [-25.4783, -28.3359, -31.3418, -11.4322,   4.0405, -33.3875,  -1.4377,
           4.1503, -15.4798,  -1.9187],
        [-24.0417, -22.2811, -22.4105, -11.4753,   5.4885, -24.5652,   3.2692,
           4.1095, -16.0655,  -5.5219]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.7151,  32.2334,  10.3476,  10.0841,  -5.9280,   0.2479,  -0.4858,
          11.1787,  -2.6953,  -0.5915],
        [  1.1681, -32.2034, -10.4476,  -9.9648,   5.6062,  -0.2857,   0.8725,
         -10.8460,   3.1784,   0.5528]], device='cuda:0'))])
xi:  [-114.841]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 401.4141464653541
W_T_median: 52.279144159966094
W_T_pctile_5: -112.3632412256007
W_T_CVAR_5_pct: -200.57643915181808
Average q (qsum/M+1):  53.77872983870968
Optimal xi:  [-114.841]
Expected(across Rb) median(across samples) p_equity:  0.3427344262910386
obj fun:  tensor(-1466.4764, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1584.180033097498
W_T_median: 1144.1177591225853
W_T_pctile_5: -126.33840259751949
W_T_CVAR_5_pct: -294.9519820713226
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1344.5583502924503
Current xi:  [-4.5988965]
objective value function right now is: -1344.5583502924503
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1365.7820047396276
Current xi:  [-10.575424]
objective value function right now is: -1365.7820047396276
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.217201]
objective value function right now is: -1352.094131284677
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-24.945032]
objective value function right now is: -1353.9434503664256
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1371.5064343011234
Current xi:  [-32.957123]
objective value function right now is: -1371.5064343011234
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1373.1451370015561
Current xi:  [-40.876945]
objective value function right now is: -1373.1451370015561
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-45.983967]
objective value function right now is: -1369.5490952316143
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-47.221405]
objective value function right now is: -1361.0601797362633
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-51.422527]
objective value function right now is: -1372.9914215933043
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-58.525116]
objective value function right now is: -1369.1715100381782
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1375.0866371225109
Current xi:  [-59.933155]
objective value function right now is: -1375.0866371225109
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-63.73713]
objective value function right now is: -1371.1361831265008
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-72.55576]
objective value function right now is: -1367.3480896148283
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-70.257454]
objective value function right now is: -1373.0510938040065
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-70.4595]
objective value function right now is: -1372.8043041205208
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-69.395515]
objective value function right now is: -1368.4931691547267
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-67.3435]
objective value function right now is: -1368.3169844518682
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-69.6595]
objective value function right now is: -1369.8507600848254
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-67.04448]
objective value function right now is: -1372.1822609412625
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-67.21988]
objective value function right now is: -1368.4496388759853
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-70.18261]
objective value function right now is: -1370.4061428643188
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-69.026955]
objective value function right now is: -1362.8114809760282
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-67.65712]
objective value function right now is: -1368.3443132819443
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-68.51475]
objective value function right now is: -1361.96180188307
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-66.8344]
objective value function right now is: -1372.1449950200963
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.52147]
objective value function right now is: -1365.7014385875368
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-69.57217]
objective value function right now is: -1366.9052560208809
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-63.793125]
objective value function right now is: -1369.596055683457
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-65.434494]
objective value function right now is: -1373.145961270947
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-66.48778]
objective value function right now is: -1370.6000816031317
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-69.05271]
objective value function right now is: -1373.1613672043777
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-69.46556]
objective value function right now is: -1370.55514094516
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-69.423965]
objective value function right now is: -1369.9017539620527
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-67.362045]
objective value function right now is: -1371.5671841350454
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-66.28714]
objective value function right now is: -1373.8441402186495
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-69.66031]
objective value function right now is: -1372.9446994954535
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-64.93677]
objective value function right now is: -1368.1968009445807
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.85285]
objective value function right now is: -1359.7448886897578
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-55.974545]
objective value function right now is: -1359.676465797984
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-63.29254]
objective value function right now is: -1374.2540552245785
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-62.905735]
objective value function right now is: -1371.3637206552544
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.57785]
objective value function right now is: -1364.4390639469348
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-55.822903]
objective value function right now is: -1371.1802504186253
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.773846]
objective value function right now is: -1373.7126796505547
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-63.839554]
objective value function right now is: -1370.8499153010466
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-61.20308]
objective value function right now is: -1361.435055994175
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-64.699455]
objective value function right now is: -1364.8008768869456
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-68.529144]
objective value function right now is: -1372.057795836723
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-64.360374]
objective value function right now is: -1370.4795141379893
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-71.4718]
objective value function right now is: -1369.663644325172
min fval:  -1374.622722977412
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.9392,  3.8385],
        [ 0.6949, -3.0560],
        [-0.9304,  3.8091],
        [-0.9585,  3.9055],
        [-0.9317,  3.8135],
        [-0.9385,  3.8365],
        [ 0.6975, -3.0628],
        [-0.9302,  3.8083],
        [-0.9313,  3.8120],
        [-0.9320,  3.8146]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  6.0452, -11.6854,   6.9513,   0.4813,   8.1411,   7.3898, -11.7569,
           7.8378,   6.2640,   5.6097],
        [  6.1531, -11.8795,   6.3394,   1.2617,   7.7914,   7.0863, -11.7071,
           6.9914,   5.7037,   4.9835],
        [ 16.7078,  -3.1693,  20.0184,  12.5566,  19.7574,  16.7809,  -4.0789,
          20.9632,  20.0436,  19.4319],
        [-16.6424,   2.7568, -21.2887, -12.5814, -21.2104, -18.0224,   3.1428,
         -22.2166, -21.9799, -21.7716],
        [ 19.3056,  -3.2401,  23.9102,  17.8882,  23.1735,  19.4031,  -3.1938,
          24.0395,  23.0942,  22.6749],
        [ 18.9015,  -3.3523,  23.8464,  18.2444,  22.7044,  19.5537,  -3.0687,
          23.2701,  22.2763,  21.6363],
        [-24.8631,  -8.0180,  -9.7851,  -3.0187, -20.4212, -21.5942,  -5.0067,
         -10.3482,  -8.0144,  -6.1889],
        [ 19.0507,  -3.1984,  22.7855,  16.4123,  21.9605,  18.4671,  -3.2365,
          22.9477,  21.9606,  21.4695],
        [ 12.2144,  -7.0729,  13.2750,   5.3508,  13.9495,  12.2049,  -7.3502,
          13.7711,  12.8174,  12.0041],
        [ 18.8593,  -3.2995,  22.9758,  17.3017,  22.0448,  19.0919,  -3.1440,
          22.7038,  21.8114,  21.0275]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.2115,   0.2180,   9.4467, -26.1356,  17.6354,  15.8976,  -6.9235,
          14.2698,   3.3839,  13.8470]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  4.4706,   3.1075],
        [  4.1621,   2.6827],
        [  2.0133,   2.8575],
        [ 15.0842,   3.6859],
        [ -5.8988,  -2.1349],
        [  1.9377,   2.8395],
        [ -7.8958,  -2.3291],
        [  6.7795,  -1.8488],
        [-20.4433, -13.8613],
        [-21.7439, -13.8061]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.5759e+01, -2.3771e+01, -2.4723e+01, -1.2518e+01,  5.4235e+00,
         -2.6791e+01,  5.7376e+00,  5.8008e+00, -2.1386e+01, -8.3987e+00],
        [-8.0255e+01, -7.0488e+01, -5.9562e+01, -1.6708e+01, -6.0917e+00,
         -5.9913e+01, -7.8284e+00, -6.3559e+00,  8.8973e+00,  9.3715e+00],
        [-1.0853e+01, -1.7292e+01, -2.1101e+01, -1.9370e+00, -8.9946e+00,
         -2.1061e+01, -7.5818e+00, -6.0194e+00, -4.6635e+01, -2.8765e+01],
        [ 3.3664e+00, -2.0117e+00, -7.1318e+00, -4.3549e+00,  1.7567e+00,
         -9.2895e+00,  7.3667e+00,  3.8639e+00, -2.7346e+01, -1.9834e+01],
        [-1.6265e+01, -2.8424e+01, -3.2506e+01, -6.3006e+00, -1.1184e+01,
         -3.4150e+01, -9.0043e+00, -1.1550e+01, -5.5095e-01,  7.2128e+00],
        [-2.3724e-02, -3.8322e-01, -1.0913e+00, -4.2046e+00,  3.6635e+01,
         -1.0489e+00,  3.1240e+01,  3.2006e+01,  1.9829e+00, -4.0935e+00],
        [-2.5065e+01, -2.4168e+01, -2.4897e+01, -1.2842e+01,  5.0159e+00,
         -2.7172e+01,  5.1357e+00,  5.2091e+00, -2.0223e+01, -7.9335e+00],
        [-4.1715e+01, -3.5624e+01, -3.2500e+01, -2.0929e+01,  8.3984e+00,
         -3.4010e+01,  1.1746e+01,  7.4020e+00, -9.0161e+00, -2.1489e+00],
        [-2.5837e+01, -2.9109e+01, -3.1655e+01, -1.1032e+01,  4.0093e+00,
         -3.3719e+01, -1.6769e+00,  4.2385e+00, -1.5395e+01, -1.8345e+00],
        [-2.4046e+01, -2.2296e+01, -2.2432e+01, -1.1123e+01,  5.5169e+00,
         -2.4587e+01,  3.2851e+00,  4.1767e+00, -1.6021e+01, -5.4776e+00]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.7160,  30.7139,  10.3476,  10.7367,  -5.2194,   0.2616,  -0.4863,
          11.4892,  -2.0826,  -0.7075],
        [  1.1689, -30.6840, -10.4476, -10.6174,   4.8977,  -0.2994,   0.8730,
         -11.1564,   2.5657,   0.6688]], device='cuda:0'))])
xi:  [-63.839554]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 469.74180323355023
W_T_median: 146.54369750044174
W_T_pctile_5: -59.75583329466091
W_T_CVAR_5_pct: -167.2191717437774
Average q (qsum/M+1):  52.44893523185484
Optimal xi:  [-63.839554]
Expected(across Rb) median(across samples) p_equity:  0.3476987421512623
obj fun:  tensor(-1374.6227, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1584.180033097498
W_T_median: 1144.1177591225853
W_T_pctile_5: -126.33840259751949
W_T_CVAR_5_pct: -294.9519820713226
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1132.590727539135
Current xi:  [-2.6337903]
objective value function right now is: -1132.590727539135
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1153.4944664665766
Current xi:  [-4.909965]
objective value function right now is: -1153.4944664665766
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.3515463]
objective value function right now is: -1123.8093877043889
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1159.844155422626
Current xi:  [-7.2364473]
objective value function right now is: -1159.844155422626
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.940491]
objective value function right now is: -1159.4263462516767
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.3714547]
objective value function right now is: -1151.1460521265046
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-10.617151]
objective value function right now is: -1153.257218956475
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.802028]
objective value function right now is: -1158.5166867163769
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.8185194]
objective value function right now is: -1146.1581034936492
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.0850585]
objective value function right now is: -1158.3199123437053
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1160.5636726798748
Current xi:  [-7.0533423]
objective value function right now is: -1160.5636726798748
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.9172287]
objective value function right now is: -1149.0775401705653
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-11.322784]
objective value function right now is: -1154.5560579759715
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-11.073355]
objective value function right now is: -1150.3130835353006
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-11.137411]
objective value function right now is: -1155.9658321946056
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.590644]
objective value function right now is: -1144.9470951091898
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1162.1030325188008
Current xi:  [-4.7732067]
objective value function right now is: -1162.1030325188008
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1193.3760462510365
Current xi:  [-1.6758552]
objective value function right now is: -1193.3760462510365
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.137665]
objective value function right now is: -1164.782338561138
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1200.856692957174
Current xi:  [2.0955315]
objective value function right now is: -1200.856692957174
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.041094]
objective value function right now is: -1190.1357057358807
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.238074]
objective value function right now is: -1198.4323804504054
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.1915393]
objective value function right now is: -1193.2802365963323
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.147658]
objective value function right now is: -1191.1555796609584
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1210.5472018154385
Current xi:  [14.861806]
objective value function right now is: -1210.5472018154385
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.961554]
objective value function right now is: -1181.5787522646124
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [27.867006]
objective value function right now is: -1181.0260701952463
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [25.411251]
objective value function right now is: -1176.8965305225518
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [29.13273]
objective value function right now is: -1208.3041672143788
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [29.727564]
objective value function right now is: -1205.257975236157
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [24.95892]
objective value function right now is: -1209.9594663842497
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1219.9778260756864
Current xi:  [28.799946]
objective value function right now is: -1219.9778260756864
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1305.4915560449738
Current xi:  [34.937252]
objective value function right now is: -1305.4915560449738
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [61.40472]
objective value function right now is: -1271.9450959998987
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [46.927845]
objective value function right now is: -1164.7854692538776
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1347.869790263478
Current xi:  [54.875473]
objective value function right now is: -1347.869790263478
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1356.3873299567058
Current xi:  [90.35886]
objective value function right now is: -1356.3873299567058
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.632866]
objective value function right now is: -1337.0527114994009
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1403.310486477196
Current xi:  [125.38293]
objective value function right now is: -1403.310486477196
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [136.04526]
objective value function right now is: -1385.2992422061286
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [136.59259]
objective value function right now is: -1197.5850085091527
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.6576]
objective value function right now is: -1207.581860513246
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [99.62556]
objective value function right now is: -1277.048295668587
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.00319]
objective value function right now is: -1316.6640952228022
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [98.31008]
objective value function right now is: -1238.2219556924028
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [87.63718]
objective value function right now is: -1272.4946479136079
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [79.38115]
objective value function right now is: -1277.742773482641
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [80.42634]
objective value function right now is: -1307.8107898441922
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [76.359634]
objective value function right now is: -1260.9398098245977
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [76.70578]
objective value function right now is: -1295.2579577219992
min fval:  -1396.2487527634084
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-4.5314,  4.2681],
        [ 3.7097, -3.5399],
        [-4.5125,  4.2510],
        [-4.5459,  4.2812],
        [-4.5146,  4.2529],
        [-4.5284,  4.2654],
        [ 3.7222, -3.5509],
        [-4.5115,  4.2501],
        [-4.5139,  4.2522],
        [-4.5154,  4.2536]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  6.0315, -11.9695,   6.9373,   0.4679,   8.1272,   7.3761, -12.0411,
           7.8239,   6.2501,   5.5958],
        [  6.1435, -12.1286,   6.3295,   1.2522,   7.7816,   7.0767, -11.9563,
           6.9815,   5.6939,   4.9737],
        [ 20.2220,  -4.5787,  23.4471,  16.1784,  23.1971,  20.2850,  -5.4907,
          24.3891,  23.4806,  22.8763],
        [-21.2411,   3.3462, -25.7980, -17.2915, -25.7313, -22.6105,   3.7346,
         -26.7231, -26.4979, -26.2974],
        [ 23.8876,  -3.8343,  28.4027,  22.5815,  27.6777,  23.9744,  -3.7903,
          28.5293,  27.5955,  27.1839],
        [ 23.4269,  -3.9948,  28.2826,  22.8809,  27.1522,  24.0685,  -3.7136,
          27.7035,  26.7212,  26.0889],
        [-27.9151,  -9.4567, -12.8180,  -6.1026, -23.4569, -24.6445,  -6.4431,
         -13.3806, -11.0494,  -9.2256],
        [ 23.4616,  -3.9346,  27.1076,  20.9341,  26.2940,  22.8675,  -3.9750,
          27.2669,  26.2913,  25.8078],
        [ 12.0250, -11.4129,  13.0395,   5.2260,  13.7203,  12.0107, -11.6922,
          13.5343,  12.5868,  11.7776],
        [ 23.2818,  -4.0265,  27.3094,  21.8351,  26.3899,  23.5039,  -3.8734,
          27.0346,  26.1536,  25.3774]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.3168,   0.2691,   9.1346, -28.5841,  19.0208,  17.0877,  -6.4727,
          15.1114,   1.2301,  14.7484]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  4.1315,   2.1637],
        [ -3.6695,   1.6025],
        [  5.2172,   2.0307],
        [ 15.7550,   4.7571],
        [ -6.9914,  -3.6966],
        [  0.9072,   2.6182],
        [-11.2058,  -3.7035],
        [  6.2324,  -0.0540],
        [-26.1717,  -9.0117],
        [-30.3820,  -6.0575]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.4832e+01, -2.3113e+01, -2.3671e+01, -1.2027e+01,  5.9765e+00,
         -2.5779e+01,  6.8058e+00,  6.4598e+00, -2.0400e+01, -8.3862e+00],
        [-8.2661e+01, -7.1183e+01, -5.9896e+01, -1.7270e+01, -5.8825e+00,
         -6.0282e+01, -8.0881e+00, -6.8652e+00,  8.4374e+00,  9.0283e+00],
        [-1.0876e+01, -1.7292e+01, -2.1139e+01, -1.9988e+00, -9.0010e+00,
         -2.1074e+01, -7.5814e+00, -6.0779e+00, -4.6634e+01, -2.8765e+01],
        [ 1.4801e+00, -1.2423e+00, -2.5897e+00, -6.0280e+00, -3.9485e+00,
         -2.9155e+00,  2.9932e+00,  2.1121e+00, -1.1189e+01, -1.6481e+01],
        [-1.7676e+01, -2.9916e+01, -3.4381e+01, -6.1985e+00, -1.0535e+01,
         -3.6049e+01, -8.8686e+00, -1.0792e+01,  2.0551e-01,  7.9689e+00],
        [-3.2676e+00, -1.7483e+00, -4.2754e+00, -7.4882e+00,  5.0164e+01,
         -4.9226e+00,  4.5489e+01,  2.8524e+01,  2.1068e-01, -8.4341e+00],
        [-2.4042e+01, -2.3187e+01, -2.3685e+01, -1.1876e+01,  6.0750e+00,
         -2.6040e+01,  6.4032e+00,  6.4225e+00, -1.8982e+01, -7.9569e+00],
        [-7.1276e+01, -5.1541e+01, -3.8303e+01, -1.6542e+01,  1.1662e+01,
         -4.7995e+01,  1.4141e+01,  1.1728e+01, -5.1432e+00, -1.8707e+00],
        [-2.5181e+01, -2.8342e+01, -3.1050e+01, -1.1214e+01,  3.9918e+00,
         -3.2815e+01, -1.0121e+00,  4.0930e+00, -1.5386e+01, -2.5773e+00],
        [-1.7499e+01, -2.1435e+01, -1.6753e+01,  5.9230e-02,  8.6765e+00,
         -2.1445e+01,  5.1934e+00,  8.9207e+00, -6.8843e+00, -2.3542e+00]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -1.5868,  30.4591,  10.3447,   8.0490,  -4.6163,   0.2750,  -1.7436,
          19.5636,  -2.3141,  -3.7389],
        [  2.0398, -30.4291, -10.4447,  -7.9297,   4.2945,  -0.3129,   2.1302,
         -19.2305,   2.7972,   3.7002]], device='cuda:0'))])
xi:  [98.31008]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 955.4786669812122
W_T_median: 654.8998296775751
W_T_pctile_5: 118.10282105538585
W_T_CVAR_5_pct: -28.39113913447735
Average q (qsum/M+1):  48.06604397681452
Optimal xi:  [98.31008]
Expected(across Rb) median(across samples) p_equity:  0.3543814371029536
obj fun:  tensor(-1396.2488, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1584.180033097498
W_T_median: 1144.1177591225853
W_T_pctile_5: -126.33840259751949
W_T_CVAR_5_pct: -294.9519820713226
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -996.062339960043
Current xi:  [17.980438]
objective value function right now is: -996.062339960043
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -997.3474348406095
Current xi:  [28.116058]
objective value function right now is: -997.3474348406095
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [31.853569]
objective value function right now is: -961.85478689309
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1011.8330251109555
Current xi:  [30.14361]
objective value function right now is: -1011.8330251109555
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [35.340725]
objective value function right now is: -986.8705543432811
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1153.7511362375742
Current xi:  [37.945065]
objective value function right now is: -1153.7511362375742
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1183.6142290663915
Current xi:  [71.946266]
objective value function right now is: -1183.6142290663915
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1234.1398351318016
Current xi:  [91.80912]
objective value function right now is: -1234.1398351318016
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1239.8481257417022
Current xi:  [94.44105]
objective value function right now is: -1239.8481257417022
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [101.3823]
objective value function right now is: -1197.576377714253
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1277.3200756438196
Current xi:  [110.37442]
objective value function right now is: -1277.3200756438196
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [121.32159]
objective value function right now is: -1257.739360810876
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1309.947645338919
Current xi:  [131.37706]
objective value function right now is: -1309.947645338919
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [131.28992]
objective value function right now is: -1282.837747471663
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [128.80817]
objective value function right now is: -1295.5247275720562
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [128.88844]
objective value function right now is: -1295.748119263433
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [126.029976]
objective value function right now is: -1235.7343653409218
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [120.82032]
objective value function right now is: -1299.7545095946543
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [128.51703]
objective value function right now is: -1271.2051687032567
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1317.41039061519
Current xi:  [127.04918]
objective value function right now is: -1317.41039061519
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [131.94774]
objective value function right now is: -1317.1172281619167
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [141.685]
objective value function right now is: -1285.8616353198586
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [136.10367]
objective value function right now is: -1295.2230222817777
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1324.449552567824
Current xi:  [138.16536]
objective value function right now is: -1324.449552567824
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [141.55363]
objective value function right now is: -1244.404137107313
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [146.28159]
objective value function right now is: -1323.0548091158796
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [144.61147]
objective value function right now is: -1324.410267171293
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1332.2215483850957
Current xi:  [143.95901]
objective value function right now is: -1332.2215483850957
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [139.4074]
objective value function right now is: -1264.4146528963815
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [141.24213]
objective value function right now is: -1318.407981677886
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [140.10664]
objective value function right now is: -987.2976549696515
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [135.20349]
objective value function right now is: -1218.7054219586432
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [138.02693]
objective value function right now is: -1325.921434559855
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [131.71661]
objective value function right now is: -1167.3188639525583
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [131.22102]
objective value function right now is: -1121.9143990135065
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [131.6588]
objective value function right now is: -1304.3525021799528
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [129.77661]
objective value function right now is: -1305.593199879977
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [128.52213]
objective value function right now is: -1264.6508046513502
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [128.8551]
objective value function right now is: -1128.931018144137
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [130.86324]
objective value function right now is: -1079.3755111253415
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [133.12744]
objective value function right now is: -1272.110981303883
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [138.42545]
objective value function right now is: -1199.2577844001942
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [141.30466]
objective value function right now is: -1272.4731915545876
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [138.52458]
objective value function right now is: -1313.5095832201414
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [135.68398]
objective value function right now is: -1305.9665340463737
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [133.70963]
objective value function right now is: -1304.4547717127657
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [129.14899]
objective value function right now is: -1301.4776092903517
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [134.556]
objective value function right now is: -1308.7354558469701
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [142.3739]
objective value function right now is: -1286.533671618902
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [138.54471]
objective value function right now is: -1200.4045819959072
min fval:  -1329.4541776205158
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-5.3440,  4.4881],
        [ 4.3875, -3.6970],
        [-5.3220,  4.4700],
        [-5.3633,  4.5036],
        [-5.3232,  4.4711],
        [-5.3385,  4.4838],
        [ 4.3978, -3.7061],
        [-5.3195,  4.4681],
        [-5.3211,  4.4696],
        [-5.3223,  4.4706]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  6.0364, -11.9083,   6.9422,   0.4728,   8.1321,   7.3810, -11.9799,
           7.8288,   6.2550,   5.6007],
        [  6.1471, -12.0835,   6.3331,   1.2558,   7.7851,   7.0802, -11.9112,
           6.9851,   5.6975,   4.9772],
        [ 20.6204,  -5.6976,  23.8194,  16.5986,  23.5710,  20.6770,  -6.6102,
          24.7587,  23.8522,  23.2493],
        [-23.3325,   2.9608, -27.8611, -19.4067, -27.7960, -24.6950,   3.3498,
         -28.7832, -28.5603, -28.3613],
        [ 25.7039,  -3.7102,  30.1907,  24.4216,  29.4674,  25.7837,  -3.6669,
          30.3144,  29.3828,  28.9727],
        [ 25.1061,  -3.9963,  29.9337,  24.5838,  28.8049,  25.7409,  -3.7157,
          29.3516,  28.3715,  27.7408],
        [-28.2661, -10.1543, -13.1702,  -6.4526, -23.8090, -24.9956,  -7.1402,
         -13.7328, -11.4015,  -9.5776],
        [ 24.8782,  -4.1708,  28.4964,  22.3741,  27.6845,  24.2773,  -4.2119,
          28.6527,  27.6794,  27.1973],
        [ 12.0506, -11.6794,  13.0648,   5.2519,  13.7456,  12.0362, -11.9590,
          13.5596,  12.6121,  11.8029],
        [ 24.7196,  -4.2439,  28.7193,  23.2963,  27.8014,  24.9349,  -4.0915,
          28.4416,  27.5628,  26.7881]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.5933,   0.5118,   8.4542, -27.8651,  19.3246,  17.2715,  -6.1195,
          15.0960,   1.5352,  14.7604]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  6.9524,   3.2290],
        [ -8.3879,   0.1975],
        [  8.3931,   2.4183],
        [ 14.9292,   3.6562],
        [-10.7573,  -2.0486],
        [ -5.9417,   0.7586],
        [-11.3106,  -1.9164],
        [ 15.3248,  -0.8138],
        [-25.1297,  -9.9604],
        [-35.5918,  -8.3154]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.4695e+01, -2.2252e+01, -2.3789e+01, -1.1516e+01,  6.0666e+00,
         -2.4918e+01,  7.9768e+00,  6.5977e+00, -2.0222e+01, -8.8337e+00],
        [-8.2661e+01, -7.1183e+01, -5.9896e+01, -1.7270e+01, -5.8830e+00,
         -6.0282e+01, -8.0886e+00, -6.8657e+00,  8.4368e+00,  9.0278e+00],
        [-1.0877e+01, -1.7292e+01, -2.1142e+01, -2.0019e+00, -9.0038e+00,
         -2.1074e+01, -7.5827e+00, -6.0825e+00, -4.6634e+01, -2.8765e+01],
        [ 2.0284e+00,  4.6952e+00, -1.5982e+00, -5.0371e+00,  2.8670e+00,
          4.0101e-01,  1.0440e+01,  2.7229e+00, -4.6180e+00, -1.2466e+01],
        [-1.7676e+01, -2.9916e+01, -3.4381e+01, -6.1988e+00, -1.0536e+01,
         -3.6049e+01, -8.8688e+00, -1.0793e+01,  2.0468e-01,  7.9684e+00],
        [-2.6212e+00, -1.0483e+00, -3.6458e+00, -6.8635e+00,  3.6913e+01,
         -3.9057e+00,  3.6717e+01,  2.8953e+01, -1.3339e+00, -8.2317e+00],
        [-2.4142e+01, -2.2506e+01, -2.4049e+01, -1.1465e+01,  5.9285e+00,
         -2.5313e+01,  7.4581e+00,  6.3343e+00, -1.8969e+01, -8.7839e+00],
        [-7.8435e+01, -5.1931e+01, -3.3019e+01, -1.2207e+01,  1.3107e+01,
         -5.7380e+01,  1.5780e+01,  1.4939e+01, -1.3241e+00,  2.9059e+00],
        [-2.4226e+01, -2.7859e+01, -3.0191e+01, -1.0303e+01,  4.8972e+00,
         -3.1907e+01, -1.6852e-02,  5.0230e+00, -1.4699e+01, -2.8368e+00],
        [-1.9028e+01, -2.3511e+01, -1.8916e+01, -2.5716e+00,  5.8645e+00,
         -2.2567e+01,  3.5546e+00,  6.3918e+00, -6.9989e+00, -6.7526e+00]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -2.0171,  30.4591,  10.3446,   7.4868,  -4.6162,  -0.4696,  -1.9579,
          23.7218,  -3.2303,  -1.5553],
        [  2.4701, -30.4291, -10.4446,  -7.3675,   4.2944,   0.4317,   2.3446,
         -23.3887,   3.7134,   1.5166]], device='cuda:0'))])
xi:  [135.68398]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 680.1550282204746
W_T_median: 579.9076979324782
W_T_pctile_5: 142.9880593915511
W_T_CVAR_5_pct: -15.560746006021839
Average q (qsum/M+1):  45.486591954385084
Optimal xi:  [135.68398]
Expected(across Rb) median(across samples) p_equity:  0.2616200688605507
obj fun:  tensor(-1329.4542, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1584.180033097498
W_T_median: 1144.1177591225853
W_T_pctile_5: -126.33840259751949
W_T_CVAR_5_pct: -294.9519820713226
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  2599.0665461414005
Current xi:  [29.21155]
objective value function right now is: 2599.0665461414005
4.0% of gradient descent iterations done. Method = Adam
new min fval:  675.7843441912494
Current xi:  [50.807667]
objective value function right now is: 675.7843441912494
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -13.04432335150037
Current xi:  [72.01017]
objective value function right now is: -13.04432335150037
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -379.1868941219611
Current xi:  [92.39037]
objective value function right now is: -379.1868941219611
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -420.17608757472664
Current xi:  [108.5633]
objective value function right now is: -420.17608757472664
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -455.15942422819876
Current xi:  [118.136665]
objective value function right now is: -455.15942422819876
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -497.12134584900565
Current xi:  [126.85266]
objective value function right now is: -497.12134584900565
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [138.70903]
objective value function right now is: -442.1408174082864
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [145.29074]
objective value function right now is: -397.8214528693507
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1017.489416995104
Current xi:  [150.82506]
objective value function right now is: -1017.489416995104
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [157.01456]
objective value function right now is: -703.3977301145517
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.89038]
objective value function right now is: -834.4729615614962
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1556.7959599489268
Current xi:  [174.57155]
objective value function right now is: -1556.7959599489268
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1870.5061357121076
Current xi:  [180.53874]
objective value function right now is: -1870.5061357121076
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.0853]
objective value function right now is: -1586.036637728149
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -2147.4235199085624
Current xi:  [191.69682]
objective value function right now is: -2147.4235199085624
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -2164.3824110063197
Current xi:  [197.76506]
objective value function right now is: -2164.3824110063197
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.91219]
objective value function right now is: -2105.145477329907
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.44313]
objective value function right now is: -1607.3998128305404
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2268.07165045402
Current xi:  [197.036]
objective value function right now is: -2268.07165045402
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.28618]
objective value function right now is: -2137.0473054494964
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.32979]
objective value function right now is: -327.07059478537104
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.45367]
objective value function right now is: -1371.2967293564764
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.55647]
objective value function right now is: -1748.8886542203231
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.66696]
objective value function right now is: -1935.6566009420774
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.37244]
objective value function right now is: -1499.6793218860305
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [193.07878]
objective value function right now is: -2144.394531377152
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [184.2298]
objective value function right now is: -1019.8600952823335
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [184.57182]
objective value function right now is: -1813.4174214014295
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [191.13843]
objective value function right now is: -1269.302946689278
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.20045]
objective value function right now is: -2081.402189634965
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -2370.33596719334
Current xi:  [202.67717]
objective value function right now is: -2370.33596719334
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.36322]
objective value function right now is: -1839.3448217510747
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.01744]
objective value function right now is: -1977.1841613151273
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [202.55765]
objective value function right now is: -2362.4065774293426
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.4326]
objective value function right now is: -1518.8799602474328
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [203.06995]
objective value function right now is: -2199.1610452858563
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.8358]
objective value function right now is: -2104.3350563610065
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.73837]
objective value function right now is: -2357.3753074464075
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.09106]
objective value function right now is: -2239.814359475071
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.81348]
objective value function right now is: -2140.143524441545
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -2403.866331738195
Current xi:  [210.26707]
objective value function right now is: -2403.866331738195
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.15541]
objective value function right now is: -2027.0516992659543
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.91328]
objective value function right now is: -1977.1432537553255
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.50023]
objective value function right now is: -2271.9694725398663
new min fval from sgd:  -2438.4068602827115
new min fval from sgd:  -2489.892099379378
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.97093]
objective value function right now is: -1553.2737548534694
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.53148]
objective value function right now is: -2411.674786730311
new min fval from sgd:  -2509.6667140338313
new min fval from sgd:  -2516.5181743706803
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.28915]
objective value function right now is: -2148.5300860686207
new min fval from sgd:  -2560.6313201217313
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.67993]
objective value function right now is: -2349.049907730744
new min fval from sgd:  -2565.6343358924273
new min fval from sgd:  -2582.3424680358607
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.08481]
objective value function right now is: -2312.3923339866133
min fval:  -2582.3424680358607
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-9.1112,  3.9532],
        [ 7.3320, -3.1621],
        [-9.0657,  3.9326],
        [-9.1515,  3.9740],
        [-9.0677,  3.9328],
        [-9.0982,  3.9463],
        [ 7.3562, -3.1712],
        [-9.0600,  3.9291],
        [-9.0629,  3.9298],
        [-9.0650,  3.9304]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  5.9720, -13.1838,   6.8769,   0.4092,   8.0668,   7.3163, -13.2560,
           7.7633,   6.1896,   5.5354],
        [  6.0879, -13.2428,   6.2731,   1.1974,   7.7252,   7.0208, -13.0712,
           6.9250,   5.6374,   4.9172],
        [ 16.3237, -12.4963,  19.4746,  12.3489,  19.2269,  16.3655, -13.4103,
          20.4062,  19.5018,  18.9004],
        [-24.1816,   5.6905, -28.6433, -20.3199, -28.5795, -25.5241,   6.0816,
         -29.5549, -29.3353, -29.1384],
        [ 25.4392,  -7.4213,  29.8607,  24.2208,  29.1383,  25.4989,  -7.3799,
          29.9737,  29.0451,  28.6371],
        [ 24.5822,  -7.9376,  29.3449,  24.1237,  28.2171,  25.1970,  -7.6590,
          28.7522,  27.7751,  27.1464],
        [-28.0183,  -8.9227, -12.9096,  -6.2169, -23.5487, -24.7438,  -5.9070,
         -13.4703, -11.1395,  -9.3160],
        [ 24.0126,  -8.3487,  27.5667,  21.5714,  26.7561,  23.3923,  -8.3917,
          27.7128,  26.7425,  26.2627],
        [ 11.8765, -14.1637,  12.8871,   5.0811,  13.5680,  11.8610, -14.4446,
          13.3813,  12.4340,  11.6250],
        [ 24.0039,  -8.2929,  27.9393,  22.6439,  27.0224,  24.1997,  -8.1425,
          27.6511,  26.7755,  26.0028]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.1395,   0.0930,   3.1520, -30.9523,  16.5306,  14.2001,  -8.0084,
          11.8515,   0.9558,  11.6686]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  6.6755,   4.7587],
        [ -5.2809,   1.2283],
        [  9.3623,   3.0092],
        [ 11.0362,   2.3344],
        [-23.5770,  -0.3182],
        [ -4.9722,   1.4645],
        [ -5.5990,   0.9904],
        [ 17.6727,  -2.1254],
        [-25.8573,  -8.1824],
        [-35.0125,  -3.4656]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.4398e+01, -2.4045e+01, -1.9742e+01, -6.8745e+00,  2.5810e+00,
         -2.6892e+01,  6.8699e+00,  1.1214e+01, -1.8678e+01, -1.2397e+01],
        [-8.2161e+01, -6.9381e+01, -5.8808e+01, -1.5705e+01, -3.8405e+00,
         -5.8599e+01, -6.3796e+00, -3.7497e+00,  1.1606e+01,  1.1920e+01],
        [-1.1526e+01, -1.7339e+01, -2.1884e+01, -2.9678e+00, -9.0046e+00,
         -2.1109e+01, -7.6482e+00, -7.1722e+00, -4.6727e+01, -2.8765e+01],
        [ 4.3211e+00,  1.2097e+01, -8.6466e-01, -4.5413e+00,  1.0261e+01,
          1.9351e+00,  1.7820e+01,  3.0703e+00, -3.2730e+00, -8.8726e+00],
        [-1.7702e+01, -3.0043e+01, -3.4529e+01, -6.5645e+00, -1.0641e+01,
         -3.6139e+01, -8.9950e+00, -1.1744e+01, -7.5157e-01,  7.5640e+00],
        [-1.2420e+01, -1.3211e+00, -1.5502e+01, -1.7761e+01,  3.7607e+01,
         -1.6730e+01,  3.5594e+01,  1.7577e+01,  1.3027e+01, -1.6351e+01],
        [-2.3802e+01, -2.4405e+01, -1.9703e+01, -6.8663e+00,  2.0837e+00,
         -2.7412e+01,  6.2825e+00,  1.1153e+01, -1.7222e+01, -1.2241e+01],
        [-1.0054e+02, -5.1668e+01, -3.3425e+01, -8.6997e+00,  1.0858e+01,
         -6.1624e+01,  1.1233e+01,  1.6352e+01,  1.3565e+00,  1.6512e+00],
        [-2.4094e+01, -2.7806e+01, -2.7135e+01, -6.8212e+00,  6.1235e+00,
         -3.1954e+01,  8.7244e-02,  8.2239e+00, -1.2398e+01, -2.7628e+00],
        [-1.4407e+01, -2.5477e+01, -1.2954e+01,  1.3772e-02,  3.9666e+00,
         -2.9630e+01,  1.4882e+00,  9.7684e+00, -7.7571e-01,  2.7984e+00]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -5.8585,  31.2412,  10.2969,   0.6437,  -4.4977,   0.4194,  -5.9376,
          28.8930,  -5.8373,  -6.5615],
        [  6.3014, -31.2113, -10.3968,  -0.5243,   4.1760,  -0.4573,   6.3125,
         -28.5599,   6.3122,   6.5230]], device='cuda:0'))])
xi:  [214.51633]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 969.5177303114162
W_T_median: 809.2348607470436
W_T_pctile_5: 222.92385183429346
W_T_CVAR_5_pct: 26.35705090721482
Average q (qsum/M+1):  41.26351042716734
Optimal xi:  [214.51633]
Expected(across Rb) median(across samples) p_equity:  0.2330093588680029
obj fun:  tensor(-2582.3425, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
