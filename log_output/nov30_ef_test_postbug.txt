Starting at: 
30-11-22_22:26

 Random seed:  2  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       8  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       8  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1  logistic_sigmoid   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 8)    False        None  
2       (8, 8)    False        None  
3       (8, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       8  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       8  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 8)    False        None  
2       (8, 8)    False        None  
3       (8, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer        8  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer        8  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0       (2, 8)     False        None  
0       (8, 8)     False        None  
0       (8, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer        8  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer        8  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0       (2, 8)     False        None  
0       (8, 8)     False        None  
0       (8, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.49851218]
objective value function right now is: -1697.1396736483994
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.856212e-07]
objective value function right now is: -1700.2248984023588
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.0011409e-13]
objective value function right now is: -1702.9763094779878
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.4360815e-20]
objective value function right now is: -1704.114820026581
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.296144e-28]
objective value function right now is: -1704.6523759117824
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.4235885e-33]
objective value function right now is: -1704.6984533531127
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-5.06347e-06]
objective value function right now is: -1705.411874312427
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00021328]
objective value function right now is: -1705.2039060357188
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00371107]
objective value function right now is: -1706.1185270899095
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00109531]
objective value function right now is: -1706.5376221801723
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.769514e-05]
objective value function right now is: -1706.397152012769
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00118723]
objective value function right now is: -1704.7689168568788
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00010671]
objective value function right now is: -1705.2516741309341
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00062219]
objective value function right now is: -1706.3613534869842
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.0505296e-05]
objective value function right now is: -1706.6346424941446
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.7939516e-05]
objective value function right now is: -1705.4119052544484
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00059116]
objective value function right now is: -1706.3597297305664
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00412473]
objective value function right now is: -1706.5107885288405
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00380331]
objective value function right now is: -1704.8751119770577
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00537836]
objective value function right now is: -1706.7257338119641
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00073141]
objective value function right now is: -1706.566668304101
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00145536]
objective value function right now is: -1706.7541332599574
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00133851]
objective value function right now is: -1693.6320909773945
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00031985]
objective value function right now is: -1693.2500469932554
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.0202074e-07]
objective value function right now is: -1692.0926816968888
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00105087]
objective value function right now is: -1692.3052193881158
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01182235]
objective value function right now is: -1693.5524819961527
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [0.00097708]
objective value function right now is: -1693.7365783513644
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [0.00036741]
objective value function right now is: -1691.4400092280143
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00025614]
objective value function right now is: -1692.8226229628908
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00029185]
objective value function right now is: -1692.9108808969315
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00528712]
objective value function right now is: -1693.0658126542405
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00030011]
objective value function right now is: -1693.6359000958682
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00669522]
objective value function right now is: -1692.19826038375
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00137327]
objective value function right now is: -1693.0264325948185
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00043686]
objective value function right now is: -1692.9855998393643
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00024347]
objective value function right now is: -1692.8030859893054
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00574931]
objective value function right now is: -1693.5911775887362
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00207595]
objective value function right now is: -1692.8349458346484
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.894828e-05]
objective value function right now is: -1693.3759226753718
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00693907]
objective value function right now is: -1693.0335521610855
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00441071]
objective value function right now is: -1693.3265866321733
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0032923]
objective value function right now is: -1693.4849700152527
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.652943e-05]
objective value function right now is: -1693.9815198538765
new min fval:  -1389.7430111357926
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00459575]
objective value function right now is: -1693.5337119152782
new min fval:  -1693.8256626767954
new min fval:  -1693.9693413053956
new min fval:  -1693.9997763050585
new min fval:  -1694.0146861894666
new min fval:  -1694.0312967396148
new min fval:  -1694.053286786139
new min fval:  -1694.0780754294121
new min fval:  -1694.0944104038697
new min fval:  -1694.099763542258
new min fval:  -1694.1088220555055
new min fval:  -1694.128408730429
new min fval:  -1694.1473867663328
new min fval:  -1694.1596085987821
new min fval:  -1694.1669276487762
new min fval:  -1694.1760041972161
new min fval:  -1694.1881018559488
new min fval:  -1694.1993761993133
new min fval:  -1694.2091461346467
new min fval:  -1694.2147363126646
new min fval:  -1694.2185850788833
new min fval:  -1694.219178952825
new min fval:  -1694.2200946771864
new min fval:  -1694.2213986227584
new min fval:  -1694.2283342188275
new min fval:  -1694.2376555458522
new min fval:  -1694.2470972404983
new min fval:  -1694.2553751127487
new min fval:  -1694.260349349833
new min fval:  -1694.264514262517
new min fval:  -1694.26657803043
new min fval:  -1694.2677502839226
new min fval:  -1694.269072648146
new min fval:  -1694.273869985973
new min fval:  -1694.2775813781955
new min fval:  -1694.281172945148
new min fval:  -1694.284714776063
new min fval:  -1694.2868217331006
new min fval:  -1694.2871990452406
new min fval:  -1694.2891323876586
new min fval:  -1694.2912571748302
new min fval:  -1694.2934312907903
new min fval:  -1694.2951196947197
new min fval:  -1694.2961618400416
new min fval:  -1694.2970823947105
new min fval:  -1694.297620808165
new min fval:  -1694.2982883631923
new min fval:  -1694.2990878180037
new min fval:  -1694.299711347415
new min fval:  -1694.299764068245
new min fval:  -1694.2998541890793
new min fval:  -1694.3008964043931
new min fval:  -1694.3021138423592
new min fval:  -1694.3030263498877
new min fval:  -1694.3038752121647
new min fval:  -1694.304603619201
new min fval:  -1694.3047140882327
new min fval:  -1694.305137045437
new min fval:  -1694.3056925108356
new min fval:  -1694.306189291466
new min fval:  -1694.3067181664703
new min fval:  -1694.3068241179105
new min fval:  -1694.3069334281283
new min fval:  -1694.3075110107354
new min fval:  -1694.3083709189682
new min fval:  -1694.3094199945401
new min fval:  -1694.3109017082961
new min fval:  -1694.3121116193038
new min fval:  -1694.3128387834838
new min fval:  -1694.3132654750134
new min fval:  -1694.3134033269207
new min fval:  -1694.3136349282745
new min fval:  -1694.314326213871
new min fval:  -1694.3152518888758
new min fval:  -1694.315928845197
new min fval:  -1694.3164017017307
new min fval:  -1694.3168509979002
new min fval:  -1694.317746592057
new min fval:  -1694.3190615515434
new min fval:  -1694.320640599036
new min fval:  -1694.3225611122039
new min fval:  -1694.323704863105
new min fval:  -1694.3243117404222
new min fval:  -1694.3244648728003
new min fval:  -1694.32571569294
new min fval:  -1694.326887733521
new min fval:  -1694.3275820744284
new min fval:  -1694.3286234532081
new min fval:  -1694.3295827446816
new min fval:  -1694.330300199602
new min fval:  -1694.3303310038298
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00085068]
objective value function right now is: -1691.5536145351966
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.3122825e-07]
objective value function right now is: -1693.3859246115185
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00018218]
objective value function right now is: -1693.692456017435
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00026193]
objective value function right now is: -1693.5910476304848
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00120674]
objective value function right now is: -1693.4535232412768
min fval:  -1694.3303310038298
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 408.37898711949987
W_T_median: 193.80683564301893
W_T_pctile_5: -257.89441707030574
W_T_CVAR_5_pct: -452.18939776546716
Average q (qsum/M+1):  55.932278540826616
Optimal xi:  [1.5826161e-05]
Expected(across Rb) median(across samples) p_equity:  0.3135235907509923
obj fun:  tensor(-1694.3303, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.2341056]
objective value function right now is: -1623.2292220311467
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.105575e-06]
objective value function right now is: -1548.6017456498437
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.6325667e-12]
objective value function right now is: -1626.6799434679508
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.9605535e-20]
objective value function right now is: -1627.8111711744282
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.4927914e-24]
objective value function right now is: -1622.788938020329
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.8373796e-27]
objective value function right now is: -1624.5778439736507
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [4.2574924e-05]
objective value function right now is: -1624.279921402207
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00047332]
objective value function right now is: -1624.1518774910876
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.2617505e-05]
objective value function right now is: -1624.6314748683353
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00234252]
objective value function right now is: -1627.6773853297204
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00059032]
objective value function right now is: -1628.5210719626575
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.7900982e-06]
objective value function right now is: -1628.0102893748083
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00078732]
objective value function right now is: -1628.745556858438
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [0.00014574]
objective value function right now is: -1628.4534084079783
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00087675]
objective value function right now is: -1628.8406372093004
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00603896]
objective value function right now is: -1630.4848316417508
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00089278]
objective value function right now is: -1629.5968717939465
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00531888]
objective value function right now is: -1624.2270285211312
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00402642]
objective value function right now is: -1622.2830989618387
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.5929192e-05]
objective value function right now is: -1624.0064623433668
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.4594323e-08]
objective value function right now is: -1623.1877000480533
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00038893]
objective value function right now is: -1625.6340915990659
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00274277]
objective value function right now is: -1624.743987733902
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.013815]
objective value function right now is: -1620.833113775843
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.000311]
objective value function right now is: -1624.2743936925508
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.2068774e-05]
objective value function right now is: -1625.59477453961
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00017134]
objective value function right now is: -1626.4039708459018
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00021257]
objective value function right now is: -1625.5273249118713
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02750694]
objective value function right now is: -1624.8504731413723
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00036132]
objective value function right now is: -1626.140364694154
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00682524]
objective value function right now is: -1625.3556060608698
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00214417]
objective value function right now is: -1625.8474269650706
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00025274]
objective value function right now is: -1626.131236347401
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.356099e-05]
objective value function right now is: -1625.95738924709
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00054108]
objective value function right now is: -1626.369506258078
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.7710568e-05]
objective value function right now is: -1625.197304789393
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00017469]
objective value function right now is: -1624.9565370884313
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04673272]
objective value function right now is: -1626.294227997459
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00146886]
objective value function right now is: -1626.1619070643624
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00317944]
objective value function right now is: -1622.6241149414714
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00180754]
objective value function right now is: -1625.099101930379
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00279298]
objective value function right now is: -1625.0964894920999
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02044173]
objective value function right now is: -1624.72236891029
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00328975]
objective value function right now is: -1626.9151226540068
new min fval:  -1587.831673629003
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01155377]
objective value function right now is: -1626.183943986484
new min fval:  -1625.7777961377842
new min fval:  -1626.0656999658352
new min fval:  -1626.2730222459809
new min fval:  -1626.2852222504694
new min fval:  -1626.3265917294573
new min fval:  -1626.368850529076
new min fval:  -1626.4112041811757
new min fval:  -1626.4363007333932
new min fval:  -1626.4786532756232
new min fval:  -1626.5047611686043
new min fval:  -1626.5201473317838
new min fval:  -1626.5400877564368
new min fval:  -1626.5545180461984
new min fval:  -1626.5676960202582
new min fval:  -1626.580053835045
new min fval:  -1626.601457604075
new min fval:  -1626.6316651539412
new min fval:  -1626.6601072618296
new min fval:  -1626.6812534393384
new min fval:  -1626.7029196714918
new min fval:  -1626.7195447115014
new min fval:  -1626.7297082939504
new min fval:  -1626.7302797888347
new min fval:  -1626.7350718527432
new min fval:  -1626.7396294626988
new min fval:  -1626.7514274355494
new min fval:  -1626.7579014982657
new min fval:  -1626.7639877819438
new min fval:  -1626.7676956345267
new min fval:  -1626.770311908335
new min fval:  -1626.7752569023353
new min fval:  -1626.7845316903451
new min fval:  -1626.7905570972205
new min fval:  -1626.7915180555979
new min fval:  -1626.7927922636113
new min fval:  -1626.8022829547554
new min fval:  -1626.808078906154
new min fval:  -1626.8096892341223
new min fval:  -1626.8130916732177
new min fval:  -1626.818070502909
new min fval:  -1626.8194339382646
new min fval:  -1626.8203052982817
new min fval:  -1626.8223099312652
new min fval:  -1626.825443483055
new min fval:  -1626.8275238199622
new min fval:  -1626.8291532223375
new min fval:  -1626.8303687099065
new min fval:  -1626.8332088363259
new min fval:  -1626.8345182598675
new min fval:  -1626.835938465638
new min fval:  -1626.836765866297
new min fval:  -1626.8373466277556
new min fval:  -1626.84054431831
new min fval:  -1626.8405592900538
new min fval:  -1626.8432186334692
new min fval:  -1626.8487503530835
new min fval:  -1626.8538067092165
new min fval:  -1626.8570965885065
new min fval:  -1626.8585901131266
new min fval:  -1626.8601700077309
new min fval:  -1626.8653368688297
new min fval:  -1626.8692763359304
new min fval:  -1626.8716783275722
new min fval:  -1626.8738984165443
new min fval:  -1626.8772672701996
new min fval:  -1626.879042646618
new min fval:  -1626.879190905436
new min fval:  -1626.8806921213172
new min fval:  -1626.883499683034
new min fval:  -1626.8868240176873
new min fval:  -1626.8882993195712
new min fval:  -1626.888746223913
new min fval:  -1626.8906361842974
new min fval:  -1626.893810705287
new min fval:  -1626.895491702319
new min fval:  -1626.8970898122952
new min fval:  -1626.8997773293647
new min fval:  -1626.9041128420379
new min fval:  -1626.9074511655838
new min fval:  -1626.9103125852216
new min fval:  -1626.914781450077
new min fval:  -1626.9181652067402
new min fval:  -1626.9205811029192
new min fval:  -1626.9240081662838
new min fval:  -1626.9276421591394
new min fval:  -1626.9324581404906
new min fval:  -1626.9360328860796
new min fval:  -1626.9390445061997
new min fval:  -1626.9421713891486
new min fval:  -1626.9460446323455
new min fval:  -1626.9480351915322
new min fval:  -1626.9488090701893
new min fval:  -1626.950017725268
new min fval:  -1626.9507467350038
new min fval:  -1626.9508145620664
new min fval:  -1626.9535023469268
new min fval:  -1626.9546233899434
new min fval:  -1626.955209826277
new min fval:  -1626.9557249745699
new min fval:  -1626.9571212243745
new min fval:  -1626.959821575271
new min fval:  -1626.9621865348645
new min fval:  -1626.9633423073697
new min fval:  -1626.964086607502
new min fval:  -1626.9651456755516
new min fval:  -1626.967120015251
new min fval:  -1626.968067002886
new min fval:  -1626.9687766908357
new min fval:  -1626.969970956291
new min fval:  -1626.9717085410614
new min fval:  -1626.9731739418978
new min fval:  -1626.9739015346663
new min fval:  -1626.9746884272379
new min fval:  -1626.9758966269253
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00857984]
objective value function right now is: -1626.415107146376
new min fval:  -1626.9766124578655
new min fval:  -1626.9769351833334
new min fval:  -1626.9775830695478
new min fval:  -1626.9784017854615
new min fval:  -1626.9793920540558
new min fval:  -1626.9794977064466
new min fval:  -1626.9803340803016
new min fval:  -1626.9821879590756
new min fval:  -1626.9836171503496
new min fval:  -1626.9843301750045
new min fval:  -1626.9854303805378
new min fval:  -1626.987159606317
new min fval:  -1626.9880775493584
new min fval:  -1626.9886146650715
new min fval:  -1626.989508152123
new min fval:  -1626.9910176146334
new min fval:  -1626.9917953352085
new min fval:  -1626.9924591496053
new min fval:  -1626.9937727116123
new min fval:  -1626.9962544607065
new min fval:  -1626.9976046628547
new min fval:  -1626.9982303101785
new min fval:  -1626.9987487119172
new min fval:  -1626.9999803307978
new min fval:  -1627.0028237997612
new min fval:  -1627.0044531820522
new min fval:  -1627.0055307987964
new min fval:  -1627.0064737221817
new min fval:  -1627.0073316309158
new min fval:  -1627.0087378343762
new min fval:  -1627.0101188839276
new min fval:  -1627.0110973891906
new min fval:  -1627.0117717837588
new min fval:  -1627.0126816664044
new min fval:  -1627.0145490085367
new min fval:  -1627.014883736446
new min fval:  -1627.014928017978
new min fval:  -1627.0154476742828
new min fval:  -1627.017034435711
new min fval:  -1627.019339336591
new min fval:  -1627.020490874265
new min fval:  -1627.020863417041
new min fval:  -1627.0213185563427
new min fval:  -1627.0225005446157
new min fval:  -1627.023303124995
new min fval:  -1627.0238308167152
new min fval:  -1627.0239294094245
new min fval:  -1627.0245951199186
new min fval:  -1627.0254584975307
new min fval:  -1627.0268555957684
new min fval:  -1627.0278467862602
new min fval:  -1627.02858542627
new min fval:  -1627.0294365318287
new min fval:  -1627.0301662244901
new min fval:  -1627.030444406013
new min fval:  -1627.0306811541184
new min fval:  -1627.0311191278063
new min fval:  -1627.0316130410895
new min fval:  -1627.0318191267872
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00050061]
objective value function right now is: -1623.7133270391512
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.4506613e-06]
objective value function right now is: -1624.8452401882023
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00067413]
objective value function right now is: -1625.8659962438894
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.4342785e-08]
objective value function right now is: -1626.876299661828
min fval:  -1627.0318191267872
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 436.7748394865785
W_T_median: 203.3575004389122
W_T_pctile_5: -39.03505853991691
W_T_CVAR_5_pct: -224.90305206908747
Average q (qsum/M+1):  54.305931829637096
Optimal xi:  [-9.056544e-05]
Expected(across Rb) median(across samples) p_equity:  0.345516720165809
obj fun:  tensor(-1627.0318, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.8289957]
objective value function right now is: -1553.3637734947088
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00748331]
objective value function right now is: -1557.3396442391313
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.3302165e-10]
objective value function right now is: -1559.31501747325
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.5386996e-18]
objective value function right now is: -1556.829094479208
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.363434e-21]
objective value function right now is: -1559.9425061194388
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.1734524e-21]
objective value function right now is: -1560.299949795395
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [0.0424212]
objective value function right now is: -1560.2155998316102
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00560814]
objective value function right now is: -1561.1432913097565
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.7629834e-05]
objective value function right now is: -1545.7346616132093
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00456872]
objective value function right now is: -1557.1624020004474
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00501719]
objective value function right now is: -1560.851657042291
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.036722e-05]
objective value function right now is: -1561.2659633595672
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.4348616e-07]
objective value function right now is: -1561.5040333961401
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [3.4381665e-05]
objective value function right now is: -1558.869059265572
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.8683744e-05]
objective value function right now is: -1561.490611517876
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04173766]
objective value function right now is: -1562.3728733388339
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00410133]
objective value function right now is: -1556.8166476633912
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.24986855e-05]
objective value function right now is: -1557.9777139490718
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00263775]
objective value function right now is: -1559.693518894194
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.3596053e-05]
objective value function right now is: -1561.4248627617544
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.7549288e-05]
objective value function right now is: -1561.705648187275
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02578166]
objective value function right now is: -1556.660019123051
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.706577e-05]
objective value function right now is: -1559.0323076330162
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00066961]
objective value function right now is: -1560.922667255629
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00012396]
objective value function right now is: -1561.237346667133
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00197907]
objective value function right now is: -1561.9430384047157
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.06851459]
objective value function right now is: -1560.7712749643358
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [0.00634132]
objective value function right now is: -1559.004183731968
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.21169797]
objective value function right now is: -1555.9328145569875
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.5095112e-05]
objective value function right now is: -1561.3181413120892
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00430556]
objective value function right now is: -1561.3005495142704
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04806932]
objective value function right now is: -1560.0063149952027
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00075252]
objective value function right now is: -1561.6357578308098
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0736874]
objective value function right now is: -1561.3305837893417
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00898836]
objective value function right now is: -1555.9965333026096
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0004854]
objective value function right now is: -1560.1594563325993
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00098243]
objective value function right now is: -1560.9169768013624
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00049636]
objective value function right now is: -1561.5258429398586
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00328535]
objective value function right now is: -1559.6462835900481
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00149596]
objective value function right now is: -1555.6131253849192
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00010995]
objective value function right now is: -1557.4782044076505
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00115447]
objective value function right now is: -1561.2095962135754
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00132574]
objective value function right now is: -1560.7588011621058
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00228211]
objective value function right now is: -1561.007222189901
new min fval:  -1540.7593780168747
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.03831546]
objective value function right now is: -1561.7792149627185
new min fval:  -1561.339986857277
new min fval:  -1561.3646378690673
new min fval:  -1561.4464406052134
new min fval:  -1561.509375983176
new min fval:  -1561.5581949781683
new min fval:  -1561.6333252190848
new min fval:  -1561.730684986433
new min fval:  -1561.9072739382316
new min fval:  -1562.0412044997938
new min fval:  -1562.1087889494204
new min fval:  -1562.140166456287
new min fval:  -1562.1814545007182
new min fval:  -1562.255198453008
new min fval:  -1562.3500329914496
new min fval:  -1562.3614544038867
new min fval:  -1562.3744907519101
new min fval:  -1562.3769984908388
new min fval:  -1562.3900273935558
new min fval:  -1562.4068855470587
new min fval:  -1562.4200660027088
new min fval:  -1562.4206650371198
new min fval:  -1562.4286331117205
new min fval:  -1562.4476770907897
new min fval:  -1562.4605061116345
new min fval:  -1562.4613719192414
new min fval:  -1562.4746481576706
new min fval:  -1562.487739920037
new min fval:  -1562.49611734227
new min fval:  -1562.500711786386
new min fval:  -1562.5107848235969
new min fval:  -1562.516861443666
new min fval:  -1562.521907444987
new min fval:  -1562.5285668031327
new min fval:  -1562.5355317125614
new min fval:  -1562.541719872235
new min fval:  -1562.5480914810626
new min fval:  -1562.5561851676782
new min fval:  -1562.567156973058
new min fval:  -1562.5752046035802
new min fval:  -1562.5833680083222
new min fval:  -1562.5906930584508
new min fval:  -1562.5967574663307
new min fval:  -1562.6043281885322
new min fval:  -1562.6111818332902
new min fval:  -1562.6172055675727
new min fval:  -1562.6214817903863
new min fval:  -1562.6264569819127
new min fval:  -1562.6371024426232
new min fval:  -1562.6391153138188
new min fval:  -1562.6404311728809
new min fval:  -1562.6408570536194
new min fval:  -1562.6446823778044
new min fval:  -1562.6569970853598
new min fval:  -1562.6611340215188
new min fval:  -1562.6615067691753
new min fval:  -1562.661886597925
new min fval:  -1562.6670622186778
new min fval:  -1562.6732042282813
new min fval:  -1562.6767388971537
new min fval:  -1562.6784888445825
new min fval:  -1562.6818437539068
new min fval:  -1562.6867116878298
new min fval:  -1562.6925925338069
new min fval:  -1562.6993053634958
new min fval:  -1562.7062175899307
new min fval:  -1562.7088685232734
new min fval:  -1562.7123338156912
new min fval:  -1562.716224879852
new min fval:  -1562.7237311276022
new min fval:  -1562.7364709104343
new min fval:  -1562.7492148814952
new min fval:  -1562.7539395213755
new min fval:  -1562.7590333053843
new min fval:  -1562.759829970002
new min fval:  -1562.7611112642815
new min fval:  -1562.7612421160168
new min fval:  -1562.7697664793736
new min fval:  -1562.7721096405303
new min fval:  -1562.7815367790408
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05500849]
objective value function right now is: -1557.9686846259942
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.732614e-07]
objective value function right now is: -1561.019960717616
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03079881]
objective value function right now is: -1561.06955192026
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04380462]
objective value function right now is: -1561.0549166090223
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.67633e-07]
objective value function right now is: -1561.7587003884978
min fval:  -1562.7815367790408
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 408.0318669916568
W_T_median: 210.36498296995393
W_T_pctile_5: 36.970292854435364
W_T_CVAR_5_pct: -132.73628272416065
Average q (qsum/M+1):  53.21305994833669
Optimal xi:  [0.0019223]
Expected(across Rb) median(across samples) p_equity:  0.33119685476024946
obj fun:  tensor(-1562.7815, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.076073]
objective value function right now is: -1482.1835499016238
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.71332943]
objective value function right now is: -1482.2211742421957
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.23566e-06]
objective value function right now is: -1487.2893187510754
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.4831206e-11]
objective value function right now is: -1484.6238562444435
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.119e-13]
objective value function right now is: -1479.6992328321144
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.10480258]
objective value function right now is: -1486.158504980472
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [0.07990175]
objective value function right now is: -1487.1899749922536
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.901215e-07]
objective value function right now is: -1486.0234549121833
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0014385]
objective value function right now is: -1482.3824024162946
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.3102222e-06]
objective value function right now is: -1484.346727516247
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00201592]
objective value function right now is: -1485.7930966482495
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.16240586]
objective value function right now is: -1480.6599399475085
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01887762]
objective value function right now is: -1488.4418626353058
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [3.5526897e-05]
objective value function right now is: -1481.2118842497407
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00027151]
objective value function right now is: -1484.5757046127983
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.08845279]
objective value function right now is: -1484.7816358389412
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00276417]
objective value function right now is: -1483.3535421400838
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00308738]
objective value function right now is: -1485.131982651932
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01941937]
objective value function right now is: -1483.8874133025442
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.122237e-07]
objective value function right now is: -1485.5863924841253
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.1556992]
objective value function right now is: -1483.1628721467039
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.088913e-06]
objective value function right now is: -1485.593854802095
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.06922118]
objective value function right now is: -1486.8526680819327
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00316134]
objective value function right now is: -1488.190610958278
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00699745]
objective value function right now is: -1486.645553914911
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.1919434]
objective value function right now is: -1483.015652099638
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06586868]
objective value function right now is: -1485.964131639516
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00241132]
objective value function right now is: -1487.773840707899
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00056528]
objective value function right now is: -1481.423531562121
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.03712003]
objective value function right now is: -1480.2933744183254
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.16509625]
objective value function right now is: -1483.4753894346397
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02486468]
objective value function right now is: -1483.2565583606136
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00109668]
objective value function right now is: -1485.622060699107
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00076094]
objective value function right now is: -1475.978181383138
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01723828]
objective value function right now is: -1485.4022636227405
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.6352816e-07]
objective value function right now is: -1485.515959255293
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01629113]
objective value function right now is: -1487.320538177868
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05377198]
objective value function right now is: -1486.0510786089037
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00016889]
objective value function right now is: -1488.6428296966728
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.03242747]
objective value function right now is: -1485.2411236741204
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.5657343e-05]
objective value function right now is: -1488.8553696423871
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04186292]
objective value function right now is: -1482.7368187412708
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.12301929]
objective value function right now is: -1489.3349464885873
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04274407]
objective value function right now is: -1488.1375075756616
new min fval:  -1475.9310670488048
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01960903]
objective value function right now is: -1487.2202317622878
new min fval:  -1487.674143164308
new min fval:  -1487.741822137457
new min fval:  -1487.7670689976449
new min fval:  -1487.9052504179792
new min fval:  -1488.213106761319
new min fval:  -1488.5379638032832
new min fval:  -1488.9142841435578
new min fval:  -1489.1992663139454
new min fval:  -1489.3770604849435
new min fval:  -1489.4747306393515
new min fval:  -1489.5898708531474
new min fval:  -1489.6993101675582
new min fval:  -1489.733141405662
new min fval:  -1489.743149147833
new min fval:  -1489.8234695041074
new min fval:  -1489.9151880287543
new min fval:  -1489.9845367451328
new min fval:  -1490.0201782481768
new min fval:  -1490.0333769015097
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00051973]
objective value function right now is: -1484.3555211754199
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00047007]
objective value function right now is: -1486.0066640548166
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.08643599]
objective value function right now is: -1489.8581007752596
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.3783035e-05]
objective value function right now is: -1485.7949409574755
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00720076]
objective value function right now is: -1487.8212917908365
min fval:  -1490.0333769015097
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 404.7911842985736
W_T_median: 225.30683578309407
W_T_pctile_5: 59.51985536947482
W_T_CVAR_5_pct: -81.41702591553067
Average q (qsum/M+1):  52.11592741935484
Optimal xi:  [-0.00653418]
Expected(across Rb) median(across samples) p_equity:  0.30554801126321157
obj fun:  tensor(-1490.0334, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.006268]
objective value function right now is: -1421.457074540179
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.6950207]
objective value function right now is: -1426.654705636019
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.1729527]
objective value function right now is: -1424.528562715357
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.292369]
objective value function right now is: -1428.8370006204018
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.945856]
objective value function right now is: -1427.8037301920926
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.1559215]
objective value function right now is: -1427.175617995198
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [6.050279]
objective value function right now is: -1427.6246524569663
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.659442]
objective value function right now is: -1427.4832361088027
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.771193]
objective value function right now is: -1428.672415239929
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.86755]
objective value function right now is: -1424.1145213586624
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.81653]
objective value function right now is: -1423.8525110383696
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.9431396]
objective value function right now is: -1423.864919191074
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.6072526]
objective value function right now is: -1426.0499729548367
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [4.6924105]
objective value function right now is: -1428.9390775829079
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.8557982]
objective value function right now is: -1417.2463856773893
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.6185946]
objective value function right now is: -1425.829829268442
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.3948307]
objective value function right now is: -1425.8018114553086
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.409456]
objective value function right now is: -1429.31675873488
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.3734136]
objective value function right now is: -1422.8143104716228
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.519892]
objective value function right now is: -1429.2854344914244
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.2333713]
objective value function right now is: -1421.4253374674186
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.9769545]
objective value function right now is: -1425.625068651874
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.14344]
objective value function right now is: -1426.4238285745948
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.4404545]
objective value function right now is: -1379.1887724318942
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.7493653]
objective value function right now is: -1423.2732040187377
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.7178497]
objective value function right now is: -1419.296493500397
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.8534656]
objective value function right now is: -1420.707070202305
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [4.200446]
objective value function right now is: -1421.4777753632975
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [5.3330035]
objective value function right now is: -1425.1262433377476
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.8443723]
objective value function right now is: -1416.8297435142774
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.0465245]
objective value function right now is: -1428.7101491834148
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.146599]
objective value function right now is: -1418.6617272867177
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.402993]
objective value function right now is: -1430.4475306250504
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.8989027]
objective value function right now is: -1425.3004178806777
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.708066]
objective value function right now is: -1424.1586102563822
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.710334]
objective value function right now is: -1423.0025833405364
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.9394517]
objective value function right now is: -1425.667897354144
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.2578764]
objective value function right now is: -1387.9483304805906
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.055875]
objective value function right now is: -1423.554266285171
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.1082273]
objective value function right now is: -1418.015005983781
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.4499903]
objective value function right now is: -1424.5380840988635
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.4297466]
objective value function right now is: -1425.0566562260422
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.1981096]
objective value function right now is: -1428.3097683083163
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.663217]
objective value function right now is: -1429.5096130266252
new min fval:  -1407.790086193215
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.6906977]
objective value function right now is: -1425.7438663042687
new min fval:  -1429.418089983766
new min fval:  -1429.6650289600555
new min fval:  -1429.7438527383085
new min fval:  -1429.7992054160502
new min fval:  -1429.8621508747958
new min fval:  -1430.0548570620501
new min fval:  -1430.183797442895
new min fval:  -1430.1944049968483
new min fval:  -1430.2052278301162
new min fval:  -1430.2436085190297
new min fval:  -1430.2596018546876
new min fval:  -1430.2708375614436
new min fval:  -1430.298407161153
new min fval:  -1430.3190541331462
new min fval:  -1430.3522878384351
new min fval:  -1430.4032378216134
new min fval:  -1430.4457673437407
new min fval:  -1430.4569212610638
new min fval:  -1430.4652252453682
new min fval:  -1430.4777956182197
new min fval:  -1430.4919940363266
new min fval:  -1430.5104909582617
new min fval:  -1430.5212711477604
new min fval:  -1430.5308816296442
new min fval:  -1430.5405219864813
new min fval:  -1430.5483508446566
new min fval:  -1430.554150965868
new min fval:  -1430.5553797630184
new min fval:  -1430.555527233361
new min fval:  -1430.5582709081382
new min fval:  -1430.5729021678665
new min fval:  -1430.5792270849688
new min fval:  -1430.5799725284467
new min fval:  -1430.5814736464242
new min fval:  -1430.5869630397087
new min fval:  -1430.5887663343028
new min fval:  -1430.590129058647
new min fval:  -1430.5956871776452
new min fval:  -1430.5968360540107
new min fval:  -1430.6024863856019
new min fval:  -1430.6096080490831
new min fval:  -1430.6171589948658
new min fval:  -1430.6193584190094
new min fval:  -1430.6202705575251
new min fval:  -1430.6268891934214
new min fval:  -1430.631741348647
new min fval:  -1430.6479033633202
new min fval:  -1430.66626970901
new min fval:  -1430.6875454237534
new min fval:  -1430.7077886336901
new min fval:  -1430.7249682773054
new min fval:  -1430.7372079475804
new min fval:  -1430.745920227516
new min fval:  -1430.751740757831
new min fval:  -1430.7565652178312
new min fval:  -1430.7648638065043
new min fval:  -1430.7758018595448
new min fval:  -1430.7872565198415
new min fval:  -1430.7994637547276
new min fval:  -1430.8107272835675
new min fval:  -1430.8183125173282
new min fval:  -1430.8251490310486
new min fval:  -1430.8337988160085
new min fval:  -1430.8436699019305
new min fval:  -1430.8507908248178
new min fval:  -1430.85977100067
new min fval:  -1430.866639117912
new min fval:  -1430.874738635798
new min fval:  -1430.8803886009634
new min fval:  -1430.884213997699
new min fval:  -1430.8882084152256
new min fval:  -1430.8916017717618
new min fval:  -1430.8974125890932
new min fval:  -1430.9006157534575
new min fval:  -1430.9071081010727
new min fval:  -1430.9129966623914
new min fval:  -1430.9224274477722
new min fval:  -1430.934153107567
new min fval:  -1430.9414211886779
new min fval:  -1430.9499067282934
new min fval:  -1430.9576537521104
new min fval:  -1430.9666972408177
new min fval:  -1430.9778856458493
new min fval:  -1430.9896228048722
new min fval:  -1430.9991287193054
new min fval:  -1431.0042898322577
new min fval:  -1431.0122067627424
new min fval:  -1431.02038260107
new min fval:  -1431.0321802035146
new min fval:  -1431.0426295385157
new min fval:  -1431.0534762240106
new min fval:  -1431.0614683718268
new min fval:  -1431.061649619502
new min fval:  -1431.0647853092185
new min fval:  -1431.069723754883
new min fval:  -1431.0744396187404
new min fval:  -1431.0808851481038
new min fval:  -1431.0880352695688
new min fval:  -1431.0983209526717
new min fval:  -1431.1080975358768
new min fval:  -1431.1170119163803
new min fval:  -1431.1253554311256
new min fval:  -1431.1322761627073
new min fval:  -1431.1392770744555
new min fval:  -1431.1471911430137
new min fval:  -1431.1553621436399
new min fval:  -1431.1621118608793
new min fval:  -1431.1685040439397
new min fval:  -1431.1740439235284
new min fval:  -1431.1800028299383
new min fval:  -1431.1858708021198
new min fval:  -1431.1860587559734
new min fval:  -1431.1894498938682
new min fval:  -1431.1949276431412
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.3210263]
objective value function right now is: -1431.2441472238738
new min fval:  -1431.2022164180498
new min fval:  -1431.2092408559117
new min fval:  -1431.213418589557
new min fval:  -1431.216661982714
new min fval:  -1431.2207767804439
new min fval:  -1431.2266765064328
new min fval:  -1431.2321626399244
new min fval:  -1431.2370577406334
new min fval:  -1431.2437823377206
new min fval:  -1431.248748474039
new min fval:  -1431.252780385183
new min fval:  -1431.2572446084218
new min fval:  -1431.261037333489
new min fval:  -1431.263244480254
new min fval:  -1431.2643619060057
new min fval:  -1431.2647177778852
new min fval:  -1431.2680862705474
new min fval:  -1431.2725763659116
new min fval:  -1431.2750566429559
new min fval:  -1431.2771104590922
new min fval:  -1431.281724664414
new min fval:  -1431.286770469434
new min fval:  -1431.290533322156
new min fval:  -1431.291623241565
new min fval:  -1431.2917309135385
new min fval:  -1431.301535254214
new min fval:  -1431.307713297874
new min fval:  -1431.3118448749394
new min fval:  -1431.3140053401446
new min fval:  -1431.3198716964346
new min fval:  -1431.326575738137
new min fval:  -1431.3340540605257
new min fval:  -1431.3418950380603
new min fval:  -1431.348946846779
new min fval:  -1431.3551357195354
new min fval:  -1431.360952463491
new min fval:  -1431.367532037382
new min fval:  -1431.3745839734288
new min fval:  -1431.3804569879576
new min fval:  -1431.3832831633774
new min fval:  -1431.3851380986002
new min fval:  -1431.3912554354545
new min fval:  -1431.3968869570872
new min fval:  -1431.4026678836499
new min fval:  -1431.4084001168894
new min fval:  -1431.4114719826155
new min fval:  -1431.413604354681
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.814833]
objective value function right now is: -1430.3988298677634
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.608714]
objective value function right now is: -1430.7979952039057
new min fval:  -1431.426398597471
new min fval:  -1431.4487344947383
new min fval:  -1431.4715195241
new min fval:  -1431.485559860501
new min fval:  -1431.4914460000912
new min fval:  -1431.495597453572
new min fval:  -1431.5004077926874
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.3071535]
objective value function right now is: -1362.5858956844957
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.7746656e-05]
objective value function right now is: -1381.4357037842494
min fval:  -1431.5004077926874
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 558.7214491761747
W_T_median: 327.93888138272644
W_T_pctile_5: 81.75319638341375
W_T_CVAR_5_pct: -56.73295799674095
Average q (qsum/M+1):  50.81579196068548
Optimal xi:  [5.046156]
Expected(across Rb) median(across samples) p_equity:  0.3186103492975235
obj fun:  tensor(-1431.5004, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.015782]
objective value function right now is: -1340.7864052313487
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.062994]
objective value function right now is: -432.03011543195555
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.246591]
objective value function right now is: -1302.1668024937446
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.5322843]
objective value function right now is: -1206.1527179590455
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.709861]
objective value function right now is: -1209.0737332221984
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.654252]
objective value function right now is: -1329.632411592776
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [9.593581]
objective value function right now is: -1326.6054612259306
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.195915]
objective value function right now is: -1334.1640028730167
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.883722]
objective value function right now is: -1335.3605771668865
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.797393]
objective value function right now is: -1332.6118768012964
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.908265]
objective value function right now is: -1341.0482307187485
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.397192]
objective value function right now is: -1340.052967381784
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.267304]
objective value function right now is: -1341.0825635723515
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [9.524759]
objective value function right now is: -1334.689395530277
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.499879]
objective value function right now is: -1331.8822603661497
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.235461]
objective value function right now is: -1350.4837936537954
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.969449]
objective value function right now is: -1351.0703424092635
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.283948]
objective value function right now is: -1342.14648410756
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.717077]
objective value function right now is: -1348.0865264822235
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.730115]
objective value function right now is: -1204.3100635002786
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.621106]
objective value function right now is: -1215.1884271124695
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.3310623]
objective value function right now is: -1208.8722846224632
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.4274216]
objective value function right now is: -1204.5130629169457
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.6256022]
objective value function right now is: -1212.0402466931514
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.0305743]
objective value function right now is: -1214.3068979707384
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.7080927]
objective value function right now is: -1199.3710865755331
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.441988]
objective value function right now is: -1207.4999736738973
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [5.030236]
objective value function right now is: -1209.1315295283493
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [7.9679604]
objective value function right now is: -1255.7221093780356
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.205042]
objective value function right now is: -1365.5068776300443
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.013533]
objective value function right now is: -1396.0226220785053
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.643111]
objective value function right now is: -1408.6966593832506
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.237211]
objective value function right now is: -1429.5428687959259
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.237071]
objective value function right now is: -1322.441545334662
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.061538]
objective value function right now is: -1426.1153602076406
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.981068]
objective value function right now is: -1420.8274622386778
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.63508]
objective value function right now is: -1430.5281820135215
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.499061]
objective value function right now is: -1374.145294969264
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.049563]
objective value function right now is: -1374.088538549544
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.151182]
objective value function right now is: -1425.863567153485
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.7218685]
objective value function right now is: -1433.4833802496044
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.793336]
objective value function right now is: -1409.6391186246924
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.896394]
objective value function right now is: -1438.3295826998105
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.42036]
objective value function right now is: -1417.832321443338
new min fval:  55.719425590197865
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.13979]
objective value function right now is: -1438.6013857142111
new min fval:  -1436.3054487306417
new min fval:  -1436.3969238698314
new min fval:  -1439.1335348430014
new min fval:  -1442.5919714995987
new min fval:  -1444.9922930085922
new min fval:  -1445.8468806519916
new min fval:  -1446.2018872374174
new min fval:  -1446.4078735268452
new min fval:  -1447.392957966296
new min fval:  -1448.4390703262902
new min fval:  -1448.8612281573821
new min fval:  -1449.1825632543753
new min fval:  -1449.2647894424993
new min fval:  -1449.3033337929148
new min fval:  -1449.4931871380024
new min fval:  -1449.547954962013
new min fval:  -1449.6800175121073
new min fval:  -1449.8791631531615
new min fval:  -1449.9357432084275
new min fval:  -1449.9948389653734
new min fval:  -1450.0406110031513
new min fval:  -1450.078735704963
new min fval:  -1450.0899835346793
new min fval:  -1450.111206462644
new min fval:  -1450.1219505051286
new min fval:  -1450.148500922275
new min fval:  -1450.2328886712887
new min fval:  -1450.2983578360388
new min fval:  -1450.4077465308808
new min fval:  -1450.4077785913983
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.250625]
objective value function right now is: -1441.9795514333416
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.294079]
objective value function right now is: -1440.5280608121468
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.326691]
objective value function right now is: -1408.582563870833
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.3502035]
objective value function right now is: -1429.846530268309
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.117901]
objective value function right now is: -1439.7899005291783
min fval:  -1450.4077785913983
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 898.28238750744
W_T_median: 631.8510040003475
W_T_pctile_5: 211.53009982128972
W_T_CVAR_5_pct: 32.45789707824291
Average q (qsum/M+1):  47.072856287802416
Optimal xi:  [12.891664]
Expected(across Rb) median(across samples) p_equity:  0.30156150783101715
obj fun:  tensor(-1450.4078, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.375032]
objective value function right now is: -1427.832188724183
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.427796]
objective value function right now is: -1434.1803407286154
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.835193]
objective value function right now is: -1426.1987240142637
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.145411]
objective value function right now is: -1298.8404424186065
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.905294]
objective value function right now is: -1450.2087902036171
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.58669]
objective value function right now is: -1410.3498279190476
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [13.101993]
objective value function right now is: -1452.8123173745332
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.618753]
objective value function right now is: -1153.497761770747
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.54912]
objective value function right now is: -1466.0399546945098
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.251978]
objective value function right now is: -1386.7772683430705
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.545626]
objective value function right now is: -1453.3900607958312
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.047853]
objective value function right now is: -1466.3998439993188
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.166747]
objective value function right now is: -1401.7182933567392
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [13.645967]
objective value function right now is: -1441.0552852207409
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.844701]
objective value function right now is: -1471.25756854718
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.928519]
objective value function right now is: -1465.9298973426944
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.378908]
objective value function right now is: -1443.728943349062
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.879826]
objective value function right now is: -1469.18041051064
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.84421]
objective value function right now is: -1481.178309019106
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.504651]
objective value function right now is: -1420.5701057043939
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.509991]
objective value function right now is: -1359.829844436231
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.709751]
objective value function right now is: -1129.702138709623
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.508111]
objective value function right now is: -1415.731762911466
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.245412]
objective value function right now is: -1449.722004475183
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.410731]
objective value function right now is: -1445.3879601626709
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.544183]
objective value function right now is: -1458.1546583861443
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.004002]
objective value function right now is: -1289.844762303825
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [13.558723]
objective value function right now is: -1475.340365571973
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [13.832739]
objective value function right now is: -1389.6903058600933
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.409692]
objective value function right now is: -1465.1458852767316
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.411555]
objective value function right now is: -1453.8591744811197
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.4424305]
objective value function right now is: -1468.0257823102243
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.931233]
objective value function right now is: -1463.3970867515495
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.009375]
objective value function right now is: -1451.9655166439532
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.592427]
objective value function right now is: -1476.266164248747
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.812424]
objective value function right now is: -1462.5757314810305
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.489118]
objective value function right now is: -1467.2585188277903
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.650737]
objective value function right now is: -1485.9651287869435
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.75992]
objective value function right now is: -1189.2856787811056
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.164129]
objective value function right now is: -1212.3498370144537
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.468452]
objective value function right now is: -1233.0352757807616
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.524077]
objective value function right now is: -1237.6755181003714
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.842218]
objective value function right now is: -1243.3962037633003
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.741956]
objective value function right now is: -1233.1479975004188
new min fval:  -1366.677639998173
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.458386]
objective value function right now is: -1245.4178681106637
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.676978]
objective value function right now is: -1181.3211432969563
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.734044]
objective value function right now is: -1240.2642477948195
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.587038]
objective value function right now is: -1322.6559027705125
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.6088285]
objective value function right now is: 17.967188812070827
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.626717]
objective value function right now is: -1295.6251784987674
min fval:  -1366.677639998173
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 808.4049363885931
W_T_median: 614.4676258906057
W_T_pctile_5: 214.05329450781304
W_T_CVAR_5_pct: 32.136099868604475
Average q (qsum/M+1):  46.741470829133064
Optimal xi:  [10.458386]
Expected(across Rb) median(across samples) p_equity:  0.2893002505103747
obj fun:  tensor(-1366.6776, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.074705]
objective value function right now is: -923.8067608386589
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.431851]
objective value function right now is: -2132.6128032567212
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.531043]
objective value function right now is: -2149.3508610214694
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.270202]
objective value function right now is: -2109.9035068120256
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.014494]
objective value function right now is: -2118.5346431185035
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.076764]
objective value function right now is: -2163.230946145189
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [14.558439]
objective value function right now is: -2490.502868695913
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.230571]
objective value function right now is: 5636.845660292156
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.174686]
objective value function right now is: -2304.0820795918935
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.843198]
objective value function right now is: -1917.3445243636754
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.975261]
objective value function right now is: -2068.490818620159
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.512372]
objective value function right now is: -1606.5818451372293
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.349825]
objective value function right now is: -2307.050116184175
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [13.308111]
objective value function right now is: -1373.9230225009862
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.5474]
objective value function right now is: -2427.183624246934
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.64542]
objective value function right now is: -2468.4613514299635
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.274163]
objective value function right now is: -2526.8967173572
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.265207]
objective value function right now is: -2577.050655284724
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.939997]
objective value function right now is: -2480.196293029965
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.734989]
objective value function right now is: -2502.013066241045
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.140543]
objective value function right now is: -2396.281899175179
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.746797]
objective value function right now is: -1803.0319330263344
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.48006]
objective value function right now is: -2621.9772190360286
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.737254]
objective value function right now is: -2642.0368954732244
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.424168]
objective value function right now is: -2468.6752322490806
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.837344]
objective value function right now is: -2482.965587691989
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.671098]
objective value function right now is: -2172.3907016412327
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [14.480561]
objective value function right now is: -2417.614424223816
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [14.3872]
objective value function right now is: -2188.493243918589
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.51078]
objective value function right now is: -2611.0475729434584
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.402175]
objective value function right now is: -2301.7022149824147
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.503886]
objective value function right now is: -2333.080926445319
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.935853]
objective value function right now is: -2456.41082997795
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.81412]
objective value function right now is: -2561.5058883643587
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.201473]
objective value function right now is: -2556.8485410786634
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.47426]
objective value function right now is: -2594.8989823029387
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.498081]
objective value function right now is: -2545.8429732961104
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.139693]
objective value function right now is: -1606.426335140195
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.069701]
objective value function right now is: -2614.623692574
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.964159]
objective value function right now is: -2455.596423774078
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.594626]
objective value function right now is: -2432.4206378025515
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.506749]
objective value function right now is: -2327.333449889578
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.263113]
objective value function right now is: -2475.6448288297547
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.850883]
objective value function right now is: -2326.045078787854
new min fval:  1312.754350317481
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.337964]
objective value function right now is: -619.3649959807509
new min fval:  -951.4495134125375
new min fval:  -1122.5604803758683
new min fval:  -1127.4334914408369
new min fval:  -1134.115675627173
new min fval:  -1140.810630306556
new min fval:  -1147.5241886372185
new min fval:  -1154.2210557228257
new min fval:  -1160.8402825428434
new min fval:  -1167.3704712198748
new min fval:  -1173.760216201563
new min fval:  -1180.0481618874417
new min fval:  -1186.20170600167
new min fval:  -1192.244508216194
new min fval:  -1198.111707829338
new min fval:  -1203.847773969111
new min fval:  -1209.471204169177
new min fval:  -1215.0851832912574
new min fval:  -1220.794360088829
new min fval:  -1226.5993548466242
new min fval:  -1232.4425420888392
new min fval:  -1238.0143287951462
new min fval:  -1244.0120929897284
new min fval:  -1250.024691774432
new min fval:  -1256.1656004777808
new min fval:  -1262.3245293307352
new min fval:  -1268.4527291709412
new min fval:  -1274.5849157721257
new min fval:  -1280.6847769200276
new min fval:  -1286.73179500492
new min fval:  -1292.746870533568
new min fval:  -1298.7185200551255
new min fval:  -1304.6288781705102
new min fval:  -1310.4486946614772
new min fval:  -1316.251155097935
new min fval:  -1322.0376146182632
new min fval:  -1327.8433983913349
new min fval:  -1333.689662876965
new min fval:  -1339.624742603086
new min fval:  -1345.7376357203057
new min fval:  -1352.0492288249766
new min fval:  -1358.5808241846669
new min fval:  -1365.2255348383162
new min fval:  -1371.7658773616156
new min fval:  -1378.1299590397064
new min fval:  -1384.2094279991732
new min fval:  -1390.0005426017187
new min fval:  -1395.5698112368784
new min fval:  -1400.941974386443
new min fval:  -1406.1268879199567
new min fval:  -1411.115959591605
new min fval:  -1416.0012222722735
new min fval:  -1420.8289233428045
new min fval:  -1425.4996792441452
new min fval:  -1430.0040759647866
new min fval:  -1434.4308508183724
new min fval:  -1438.8013347363078
new min fval:  -1443.1161463865467
new min fval:  -1447.4268243659224
new min fval:  -1451.7531886567622
new min fval:  -1456.0334265265449
new min fval:  -1460.2164313139347
new min fval:  -1464.4763702564642
new min fval:  -1468.7630839768167
new min fval:  -1473.0660434685135
new min fval:  -1477.436582418967
new min fval:  -1481.8908594445372
new min fval:  -1486.3946216851793
new min fval:  -1490.949763218629
new min fval:  -1495.485623223189
new min fval:  -1500.009450717339
new min fval:  -1504.5429186695383
new min fval:  -1509.0617755706255
new min fval:  -1513.527233659974
new min fval:  -1517.8930828030939
new min fval:  -1522.141153187193
new min fval:  -1526.2639688639995
new min fval:  -1530.3391148939272
new min fval:  -1534.4337422244726
new min fval:  -1538.465765817678
new min fval:  -1542.550964527859
new min fval:  -1546.7514146213398
new min fval:  -1550.9509212183175
new min fval:  -1555.2205847475996
new min fval:  -1559.5675586095065
new min fval:  -1563.9470540632788
new min fval:  -1568.352002221979
new min fval:  -1572.8160348827826
new min fval:  -1577.3501127625616
new min fval:  -1581.9169918669718
new min fval:  -1586.4906933526934
new min fval:  -1591.0389875976264
new min fval:  -1595.517421099671
new min fval:  -1599.897633068907
new min fval:  -1604.2016329039782
new min fval:  -1608.38106515585
new min fval:  -1612.4292894998594
new min fval:  -1616.3867707256315
new min fval:  -1620.2431850542353
new min fval:  -1624.088860192936
new min fval:  -1627.9704043070678
new min fval:  -1631.8490791870197
new min fval:  -1635.720053565635
new min fval:  -1639.5936923666914
new min fval:  -1643.437143717508
new min fval:  -1647.2593218136374
new min fval:  -1651.0803215903939
new min fval:  -1654.9156496039986
new min fval:  -1658.5317593317423
new min fval:  -1662.0940038413507
new min fval:  -1665.5704621734517
new min fval:  -1668.998128069266
new min fval:  -1672.4189181577888
new min fval:  -1675.8467295712971
new min fval:  -1679.2091744147165
new min fval:  -1682.5570136778942
new min fval:  -1686.0074856355934
new min fval:  -1689.5058574216423
new min fval:  -1693.0079662072978
new min fval:  -1696.513604809424
new min fval:  -1700.0300158929024
new min fval:  -1703.5279423618629
new min fval:  -1706.9525127322977
new min fval:  -1710.24392304121
new min fval:  -1713.4112316927517
new min fval:  -1716.4925869596693
new min fval:  -1719.511501948281
new min fval:  -1722.5277538619723
new min fval:  -1725.4881457554536
new min fval:  -1728.4872117585776
new min fval:  -1731.5715245137126
new min fval:  -1734.7384590978231
new min fval:  -1738.0672472429324
new min fval:  -1741.4377762338493
new min fval:  -1744.8174717801646
new min fval:  -1748.2298702022742
new min fval:  -1751.679869808521
new min fval:  -1755.1148325607062
new min fval:  -1758.5360527035784
new min fval:  -1761.965098280027
new min fval:  -1765.409783321537
new min fval:  -1768.8170617017113
new min fval:  -1772.2272281342493
new min fval:  -1775.633551006295
new min fval:  -1779.0457855174723
new min fval:  -1782.4067321606112
new min fval:  -1785.719703125957
new min fval:  -1788.9583259881297
new min fval:  -1792.158559883019
new min fval:  -1795.27725712283
new min fval:  -1798.3535557504902
new min fval:  -1801.4252113739753
new min fval:  -1804.4989206010118
new min fval:  -1807.55552369847
new min fval:  -1810.581647778324
new min fval:  -1813.66255553272
new min fval:  -1816.8329799906774
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.568478]
objective value function right now is: -2450.4117596482447
new min fval:  -1820.0402893281316
new min fval:  -1823.267665508001
new min fval:  -1826.515650565237
new min fval:  -1829.786674121038
new min fval:  -1833.0550851262146
new min fval:  -1836.292312742817
new min fval:  -1839.468721972339
new min fval:  -1842.6114800345506
new min fval:  -1845.714184360407
new min fval:  -1848.780175830585
new min fval:  -1851.8174168418595
new min fval:  -1854.8432215161201
new min fval:  -1857.8798359950865
new min fval:  -1860.9103588031637
new min fval:  -1863.9503296754565
new min fval:  -1867.0009580755645
new min fval:  -1870.0215786831016
new min fval:  -1873.090813633061
new min fval:  -1876.1803992936602
new min fval:  -1879.2727450221587
new min fval:  -1882.2991400752967
new min fval:  -1885.2795385562285
new min fval:  -1888.1909210350384
new min fval:  -1890.9426146371259
new min fval:  -1893.7463860451328
new min fval:  -1896.6086619155767
new min fval:  -1899.4204583189123
new min fval:  -1902.119335031184
new min fval:  -1904.8598290973462
new min fval:  -1907.5845540301582
new min fval:  -1910.2931536979954
new min fval:  -1913.0614767205514
new min fval:  -1915.8323612713564
new min fval:  -1918.5998240694541
new min fval:  -1921.3628693930098
new min fval:  -1924.071146942906
new min fval:  -1926.7379545436386
new min fval:  -1929.373521113776
new min fval:  -1931.959952039649
new min fval:  -1934.5137509689764
new min fval:  -1936.9954632738024
new min fval:  -1939.3925642165439
new min fval:  -1941.7428488936694
new min fval:  -1944.0749348336426
new min fval:  -1946.4070616385222
new min fval:  -1948.7811277696185
new min fval:  -1951.206528969715
new min fval:  -1953.6743996993275
new min fval:  -1956.1343065810663
new min fval:  -1958.6018571451116
new min fval:  -1961.0368036290768
new min fval:  -1963.4562386269763
new min fval:  -1965.906639930808
new min fval:  -1968.353728500632
new min fval:  -1970.780475346768
new min fval:  -1973.1214856310132
new min fval:  -1975.4013570456555
new min fval:  -1977.6979210445334
new min fval:  -1979.986949355425
new min fval:  -1982.2063410955109
new min fval:  -1984.3471360774054
new min fval:  -1986.4098251157366
new min fval:  -1988.4162498451367
new min fval:  -1990.4091210377912
new min fval:  -1992.402005232772
new min fval:  -1994.3675241057924
new min fval:  -1996.302638211748
new min fval:  -1998.2069788808892
new min fval:  -2000.1598341585486
new min fval:  -2002.0889046470882
new min fval:  -2004.0270122483664
new min fval:  -2005.9669670284754
new min fval:  -2007.9254550820217
new min fval:  -2009.8796260444296
new min fval:  -2011.799046691473
new min fval:  -2013.715722703353
new min fval:  -2015.581377547651
new min fval:  -2017.4185210875905
new min fval:  -2019.2027736610307
new min fval:  -2020.9647114082059
new min fval:  -2022.7308626795666
new min fval:  -2024.5138054827773
new min fval:  -2026.3304832123151
new min fval:  -2028.1711454498884
new min fval:  -2030.0061635836644
new min fval:  -2031.8026856075953
new min fval:  -2033.552144357562
new min fval:  -2035.2592545139844
new min fval:  -2036.92982245391
new min fval:  -2038.5354886243633
new min fval:  -2040.1025147643932
new min fval:  -2041.6585293159733
new min fval:  -2043.2243419667886
new min fval:  -2044.8160152528583
new min fval:  -2046.4688920336255
new min fval:  -2048.140076917601
new min fval:  -2049.855376501742
new min fval:  -2051.5539032626843
new min fval:  -2053.2770896223096
new min fval:  -2055.0131053641867
new min fval:  -2056.7559218217607
new min fval:  -2058.4893010267074
new min fval:  -2060.1825676006442
new min fval:  -2061.8601667108233
new min fval:  -2063.506273384224
new min fval:  -2065.1520510536643
new min fval:  -2066.774194795467
new min fval:  -2068.3533036080394
new min fval:  -2069.9188315965002
new min fval:  -2071.5251893571804
new min fval:  -2073.1641753781932
new min fval:  -2074.836887072424
new min fval:  -2076.5284877896956
new min fval:  -2078.2345456439225
new min fval:  -2079.9163963765022
new min fval:  -2081.518500748972
new min fval:  -2083.106569708997
new min fval:  -2084.634711017204
new min fval:  -2086.072629429461
new min fval:  -2087.529313099878
new min fval:  -2088.946606439577
new min fval:  -2090.4128240801256
new min fval:  -2091.880294372537
new min fval:  -2093.3706381011484
new min fval:  -2094.8697265656447
new min fval:  -2096.3791148592823
new min fval:  -2097.9490453764133
new min fval:  -2099.5654504986724
new min fval:  -2101.1949178197665
new min fval:  -2102.8403908934174
new min fval:  -2104.485387182406
new min fval:  -2106.102872389754
new min fval:  -2107.6652440128482
new min fval:  -2109.1913227384307
new min fval:  -2110.6604960093123
new min fval:  -2112.0669781968713
new min fval:  -2113.446952583668
new min fval:  -2114.830791732019
new min fval:  -2116.225473868467
new min fval:  -2117.625633278014
new min fval:  -2119.045063343458
new min fval:  -2120.4879793724986
new min fval:  -2121.9583615751817
new min fval:  -2123.4414986468
new min fval:  -2124.9501888524314
new min fval:  -2126.4812245629087
new min fval:  -2128.0442737903595
new min fval:  -2129.608136454712
new min fval:  -2131.1596949483996
new min fval:  -2132.68837013599
new min fval:  -2134.1907793121127
new min fval:  -2135.6584991824043
new min fval:  -2137.1387939612277
new min fval:  -2138.618440919065
new min fval:  -2140.084151697717
new min fval:  -2141.5438581897374
new min fval:  -2142.9503582886373
new min fval:  -2144.3178617103695
new min fval:  -2145.7447661076135
new min fval:  -2147.1956150258034
new min fval:  -2148.660657489066
new min fval:  -2150.1292390563376
new min fval:  -2151.6117963601896
new min fval:  -2153.1309928900496
new min fval:  -2154.661024954295
new min fval:  -2156.189186834989
new min fval:  -2157.7348924474336
new min fval:  -2159.2685346319527
new min fval:  -2160.7845350107696
new min fval:  -2162.3122792266913
new min fval:  -2163.7922850858527
new min fval:  -2165.235687581681
new min fval:  -2166.665200343969
new min fval:  -2168.051473933957
new min fval:  -2169.371383489278
new min fval:  -2170.6596033316378
new min fval:  -2171.893684581753
new min fval:  -2173.07734279128
new min fval:  -2174.217381947729
new min fval:  -2175.3370152526936
new min fval:  -2176.447303000032
new min fval:  -2177.54700383968
new min fval:  -2178.734736843533
new min fval:  -2179.8822475245333
new min fval:  -2181.0030208080284
new min fval:  -2182.134863245
new min fval:  -2183.270706598673
new min fval:  -2184.400404798642
new min fval:  -2185.522219699484
new min fval:  -2186.6358293208573
new min fval:  -2187.7559805377296
new min fval:  -2188.870935201983
new min fval:  -2189.9587053943274
new min fval:  -2191.056810839721
new min fval:  -2192.134078349512
new min fval:  -2193.2249371042453
new min fval:  -2194.3370678023166
new min fval:  -2195.474551182738
new min fval:  -2196.5853281919167
new min fval:  -2197.700709829481
new min fval:  -2198.783114358765
new min fval:  -2199.848954688628
new min fval:  -2200.9120832164017
new min fval:  -2201.9626962270218
new min fval:  -2203.0185187167235
new min fval:  -2204.100342396297
new min fval:  -2205.2007210215706
new min fval:  -2206.309236925381
new min fval:  -2207.4015381747217
new min fval:  -2208.502944748945
new min fval:  -2209.582236619631
new min fval:  -2210.623500300939
new min fval:  -2211.6031756622624
new min fval:  -2212.503621608563
new min fval:  -2213.3869446720755
new min fval:  -2214.2669147791285
new min fval:  -2215.146503163081
new min fval:  -2216.105312456729
new min fval:  -2217.1380696004685
new min fval:  -2218.246215883836
new min fval:  -2219.4209643870527
new min fval:  -2220.6365457612696
new min fval:  -2221.881063535596
new min fval:  -2223.1302805535634
new min fval:  -2224.310073813027
new min fval:  -2225.382426998998
new min fval:  -2226.30944495063
new min fval:  -2227.2028188553745
new min fval:  -2228.0931002871484
new min fval:  -2228.946214541295
new min fval:  -2229.7996950940574
new min fval:  -2230.6622513571037
new min fval:  -2231.55717137082
new min fval:  -2232.475150750229
new min fval:  -2233.417481455521
new min fval:  -2234.349796176917
new min fval:  -2235.2747308835424
new min fval:  -2236.1352885828114
new min fval:  -2236.9428089013823
new min fval:  -2237.680364475537
new min fval:  -2238.3781933482555
new min fval:  -2239.0595235940646
new min fval:  -2239.737129914975
new min fval:  -2240.4521941372595
new min fval:  -2241.192224681737
new min fval:  -2241.939100545155
new min fval:  -2242.6861553915674
new min fval:  -2243.445798760575
new min fval:  -2244.191870226166
new min fval:  -2244.902107859257
new min fval:  -2245.5611853358
new min fval:  -2246.187599845714
new min fval:  -2246.8008374994765
new min fval:  -2247.4538538294137
new min fval:  -2248.1361184277516
new min fval:  -2248.8398279970547
new min fval:  -2249.602169970968
new min fval:  -2250.409261901656
new min fval:  -2251.224663776437
new min fval:  -2252.0714237162815
new min fval:  -2252.946780106123
new min fval:  -2253.8344341474085
new min fval:  -2254.687246654967
new min fval:  -2255.515817025126
new min fval:  -2256.3332694001
new min fval:  -2257.133142885293
new min fval:  -2257.925211350373
new min fval:  -2258.7088056189027
new min fval:  -2259.544027139004
new min fval:  -2260.440985312977
new min fval:  -2261.3749757249243
new min fval:  -2262.337236693891
new min fval:  -2263.3493710023927
new min fval:  -2264.3875064863955
new min fval:  -2265.4393268704416
new min fval:  -2266.4838597582266
new min fval:  -2267.5230238094996
new min fval:  -2268.6314517005517
new min fval:  -2269.760524646248
new min fval:  -2270.958544903752
new min fval:  -2272.1914236802386
new min fval:  -2273.427000878961
new min fval:  -2274.591402683025
new min fval:  -2275.71345732863
new min fval:  -2276.760076493685
new min fval:  -2277.778896525989
new min fval:  -2278.89482494835
new min fval:  -2279.850101676409
new min fval:  -2280.7209150210156
new min fval:  -2281.5767264430574
new min fval:  -2282.3976627750585
new min fval:  -2283.158935083563
new min fval:  -2283.877676125749
new min fval:  -2284.552641134119
new min fval:  -2285.203320398861
new min fval:  -2285.8472202857893
new min fval:  -2286.463574443469
new min fval:  -2287.0561553550465
new min fval:  -2287.615002538201
new min fval:  -2288.148278550304
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.115566]
objective value function right now is: -2581.3453619986417
new min fval:  -2288.6600163536386
new min fval:  -2289.167342558491
new min fval:  -2289.666693989927
new min fval:  -2290.148259156043
new min fval:  -2290.6395181742755
new min fval:  -2291.1710574371555
new min fval:  -2291.701939081407
new min fval:  -2292.2506394485486
new min fval:  -2292.8175377354078
new min fval:  -2293.3826587417675
new min fval:  -2293.9658817095637
new min fval:  -2294.53963754711
new min fval:  -2295.145192887276
new min fval:  -2295.7474229782447
new min fval:  -2296.383837835867
new min fval:  -2297.05462433516
new min fval:  -2297.7522948375563
new min fval:  -2298.474404237504
new min fval:  -2299.1848270805126
new min fval:  -2299.8995858859676
new min fval:  -2300.6622337006575
new min fval:  -2301.434508153719
new min fval:  -2302.224821936605
new min fval:  -2303.0557441388423
new min fval:  -2303.8867072685443
new min fval:  -2304.698037464383
new min fval:  -2305.520742560033
new min fval:  -2306.3607515297035
new min fval:  -2307.210606535796
new min fval:  -2308.084813538437
new min fval:  -2308.9985154905276
new min fval:  -2309.932323466452
new min fval:  -2310.8626657359787
new min fval:  -2311.7969745489213
new min fval:  -2312.7522410333954
new min fval:  -2313.730358344736
new min fval:  -2314.692850871247
new min fval:  -2315.6314264859516
new min fval:  -2316.540959083679
new min fval:  -2317.37341827216
new min fval:  -2318.1752547357037
new min fval:  -2318.9650357440382
new min fval:  -2319.7753750507477
new min fval:  -2320.6044836030474
new min fval:  -2321.479879378696
new min fval:  -2322.3860570367706
new min fval:  -2323.346029714479
new min fval:  -2324.3224237367103
new min fval:  -2325.294497908672
new min fval:  -2326.275768280319
new min fval:  -2327.214570270219
new min fval:  -2328.099317742988
new min fval:  -2328.9406299761868
new min fval:  -2329.7444137181637
new min fval:  -2330.5280124887695
new min fval:  -2331.329984937733
new min fval:  -2332.1312351398924
new min fval:  -2332.933135236552
new min fval:  -2333.7350219337104
new min fval:  -2334.5092889894154
new min fval:  -2335.2683300343856
new min fval:  -2336.0112095109944
new min fval:  -2336.751097986751
new min fval:  -2337.4633373756355
new min fval:  -2338.160747100653
new min fval:  -2338.851774110794
new min fval:  -2339.5192170614555
new min fval:  -2340.193098175607
new min fval:  -2340.8981076809478
new min fval:  -2341.6223026536773
new min fval:  -2342.3591729391133
new min fval:  -2343.072882797969
new min fval:  -2343.754257506184
new min fval:  -2344.4203911037434
new min fval:  -2345.049106608633
new min fval:  -2345.6461837574557
new min fval:  -2346.2250473807203
new min fval:  -2346.79866347123
new min fval:  -2347.371294987543
new min fval:  -2347.961359321175
new min fval:  -2348.5948620278537
new min fval:  -2349.2020073762806
new min fval:  -2349.7666548176007
new min fval:  -2350.2680181481514
new min fval:  -2350.704074297161
new min fval:  -2351.088157590271
new min fval:  -2351.46315533141
new min fval:  -2351.8557898914937
new min fval:  -2352.278017547438
new min fval:  -2352.7510062393144
new min fval:  -2353.2768367505455
new min fval:  -2353.8139941531886
new min fval:  -2354.4015952949626
new min fval:  -2354.9779891930148
new min fval:  -2355.5077973755942
new min fval:  -2356.003421403474
new min fval:  -2356.4522609063183
new min fval:  -2356.869823813011
new min fval:  -2357.2727526756266
new min fval:  -2357.648440233189
new min fval:  -2357.997245780841
new min fval:  -2358.3428456116767
new min fval:  -2358.721648333964
new min fval:  -2359.15250109409
new min fval:  -2359.6617680961785
new min fval:  -2360.186535303888
new min fval:  -2360.731707808624
new min fval:  -2361.2560882976545
new min fval:  -2361.752421920279
new min fval:  -2362.2381193930646
new min fval:  -2362.700164256272
new min fval:  -2363.180465278823
new min fval:  -2363.680538037527
new min fval:  -2364.225734079834
new min fval:  -2364.822917590473
new min fval:  -2365.473028037577
new min fval:  -2366.190921653255
new min fval:  -2366.923053815669
new min fval:  -2367.6508493117317
new min fval:  -2368.319768481195
new min fval:  -2368.916655673364
new min fval:  -2369.439987758455
new min fval:  -2369.9294594607013
new min fval:  -2370.401142558953
new min fval:  -2370.888676646434
new min fval:  -2371.437314737697
new min fval:  -2372.0252006432584
new min fval:  -2372.6776390341465
new min fval:  -2373.351973390421
new min fval:  -2374.063271275586
new min fval:  -2374.7621261753866
new min fval:  -2375.4259972892023
new min fval:  -2375.977693270012
new min fval:  -2376.4440343979795
new min fval:  -2376.846069415191
new min fval:  -2377.238454704927
new min fval:  -2377.6248834220005
new min fval:  -2378.031417902465
new min fval:  -2378.4961193499125
new min fval:  -2379.0318236243415
new min fval:  -2379.6381828482477
new min fval:  -2380.3378308179226
new min fval:  -2381.109169450866
new min fval:  -2381.9195937167588
new min fval:  -2382.758521442189
new min fval:  -2383.5246440932983
new min fval:  -2384.182541805889
new min fval:  -2384.7905613200655
new min fval:  -2385.333559877255
new min fval:  -2385.848968531199
new min fval:  -2386.370952632052
new min fval:  -2386.9073461200674
new min fval:  -2387.4878768954745
new min fval:  -2388.119481565286
new min fval:  -2388.7607564371237
new min fval:  -2389.421247499932
new min fval:  -2390.0838203293115
new min fval:  -2390.7552982670754
new min fval:  -2391.4048993897563
new min fval:  -2392.0432790061964
new min fval:  -2392.6099480236007
new min fval:  -2393.133975402254
new min fval:  -2393.6327000434885
new min fval:  -2394.1097011746842
new min fval:  -2394.5740971124246
new min fval:  -2395.0204223891533
new min fval:  -2395.467497911548
new min fval:  -2395.9217216093107
new min fval:  -2396.357843029077
new min fval:  -2396.775163595457
new min fval:  -2397.1811855720653
new min fval:  -2397.6053930257813
new min fval:  -2398.0402810880983
new min fval:  -2398.5186714827732
new min fval:  -2399.0131594056284
new min fval:  -2399.4962094444636
new min fval:  -2399.903698475283
new min fval:  -2400.27253877566
new min fval:  -2400.6497878820273
new min fval:  -2401.059432689728
new min fval:  -2401.4935935527897
new min fval:  -2401.97024467746
new min fval:  -2402.461722969492
new min fval:  -2402.9855704097813
new min fval:  -2403.505984190681
new min fval:  -2404.011426928099
new min fval:  -2404.5099587648065
new min fval:  -2405.0299586463098
new min fval:  -2405.571758238154
new min fval:  -2406.148202954962
new min fval:  -2406.7400625230466
new min fval:  -2407.3659211957392
new min fval:  -2408.0163051717172
new min fval:  -2408.7009197729594
new min fval:  -2409.359083322499
new min fval:  -2409.9663599085443
new min fval:  -2410.4977298159915
new min fval:  -2410.97148069435
new min fval:  -2411.380189695144
new min fval:  -2411.7429605689745
new min fval:  -2412.0956745791877
new min fval:  -2412.4485887385395
new min fval:  -2412.8562973342046
new min fval:  -2413.326094490735
new min fval:  -2413.8261737423177
new min fval:  -2414.357532772967
new min fval:  -2414.855634817768
new min fval:  -2415.3327257001642
new min fval:  -2415.803334181671
new min fval:  -2416.223766530537
new min fval:  -2416.564633526115
new min fval:  -2416.880954503717
new min fval:  -2417.2020544526167
new min fval:  -2417.536678513185
new min fval:  -2417.935921827317
new min fval:  -2418.3696395243874
new min fval:  -2418.8441088387876
new min fval:  -2419.3604310741566
new min fval:  -2419.8961466448673
new min fval:  -2420.394538016327
new min fval:  -2420.8559725399055
new min fval:  -2421.2928909319016
new min fval:  -2421.7378361896463
new min fval:  -2422.199963656607
new min fval:  -2422.642351409056
new min fval:  -2423.0909509721437
new min fval:  -2423.571828151439
new min fval:  -2424.076866967814
new min fval:  -2424.597491327633
new min fval:  -2425.1391032637384
new min fval:  -2425.66830232362
new min fval:  -2426.183335110762
new min fval:  -2426.6450273505834
new min fval:  -2427.0704730387815
new min fval:  -2427.500547565593
new min fval:  -2427.9515416992
new min fval:  -2428.372290311612
new min fval:  -2428.782465823664
new min fval:  -2429.177790120616
new min fval:  -2429.5821556684623
new min fval:  -2429.9838953649496
new min fval:  -2430.4175082340375
new min fval:  -2430.8477963362257
new min fval:  -2431.2785389512264
new min fval:  -2431.7022077624333
new min fval:  -2432.1114372704797
new min fval:  -2432.4997570297683
new min fval:  -2432.886922483951
new min fval:  -2433.2519786505004
new min fval:  -2433.619073291032
new min fval:  -2434.0033219420857
new min fval:  -2434.425808720414
new min fval:  -2434.9112817092437
new min fval:  -2435.38474761869
new min fval:  -2435.8784710332043
new min fval:  -2436.3339159133343
new min fval:  -2436.7214803530956
new min fval:  -2437.0631847082145
new min fval:  -2437.349040315449
new min fval:  -2437.6380073482437
new min fval:  -2437.941547413936
new min fval:  -2438.27699121674
new min fval:  -2438.638594327299
new min fval:  -2439.03423237672
new min fval:  -2439.462304461163
new min fval:  -2439.896174483121
new min fval:  -2440.34410720751
new min fval:  -2440.767918831882
new min fval:  -2441.1565405948704
new min fval:  -2441.46459478464
new min fval:  -2441.737264625512
new min fval:  -2441.9448257772915
new min fval:  -2442.141293863183
new min fval:  -2442.3628697943623
new min fval:  -2442.6325754433974
new min fval:  -2442.982522611972
new min fval:  -2443.424148401635
new min fval:  -2443.910188263623
new min fval:  -2444.425569702075
new min fval:  -2444.906790584222
new min fval:  -2445.3532416132725
new min fval:  -2445.7349815374732
new min fval:  -2446.053017460367
new min fval:  -2446.3648640633055
new min fval:  -2446.6496929291784
new min fval:  -2446.969026878047
new min fval:  -2447.294183563765
new min fval:  -2447.657528940499
new min fval:  -2448.077309430773
new min fval:  -2448.50593605715
new min fval:  -2448.91687393181
new min fval:  -2449.2995014989183
new min fval:  -2449.64657084502
new min fval:  -2449.9505783817367
new min fval:  -2450.2558065237235
new min fval:  -2450.5397367487085
new min fval:  -2450.816789204306
new min fval:  -2451.1115920320403
new min fval:  -2451.3959006161876
new min fval:  -2451.693733830714
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.21361]
objective value function right now is: -2609.0198084221465
new min fval:  -2452.0260807390196
new min fval:  -2452.360339212377
new min fval:  -2452.720967900226
new min fval:  -2453.0935014743914
new min fval:  -2453.4665899061188
new min fval:  -2453.7933940856738
new min fval:  -2454.06416523455
new min fval:  -2454.2755113157937
new min fval:  -2454.46968114343
new min fval:  -2454.6302438363255
new min fval:  -2454.7491341065984
new min fval:  -2454.901806868628
new min fval:  -2455.069160686216
new min fval:  -2455.291477442791
new min fval:  -2455.5495996832133
new min fval:  -2455.8478300068364
new min fval:  -2456.152496553896
new min fval:  -2456.4540147977086
new min fval:  -2456.752204339929
new min fval:  -2457.022538493017
new min fval:  -2457.302850189673
new min fval:  -2457.5719785504616
new min fval:  -2457.835448074469
new min fval:  -2458.126014342987
new min fval:  -2458.4666743733715
new min fval:  -2458.8347241109805
new min fval:  -2459.2367074820845
new min fval:  -2459.6691877965654
new min fval:  -2460.0986828211862
new min fval:  -2460.5054669004985
new min fval:  -2460.876563734766
new min fval:  -2461.220458881007
new min fval:  -2461.5749590111686
new min fval:  -2461.9354029123356
new min fval:  -2462.301575727605
new min fval:  -2462.71290769559
new min fval:  -2463.1209257400455
new min fval:  -2463.5440504321355
new min fval:  -2463.956548108818
new min fval:  -2464.3309501660465
new min fval:  -2464.6754688422625
new min fval:  -2464.9823123044557
new min fval:  -2465.270353601352
new min fval:  -2465.5382790895133
new min fval:  -2465.8218303513577
new min fval:  -2466.1347462752606
new min fval:  -2466.477436856349
new min fval:  -2466.8442294865895
new min fval:  -2467.2654482621506
new min fval:  -2467.686398390525
new min fval:  -2468.0927500932557
new min fval:  -2468.4462286168255
new min fval:  -2468.754849838347
new min fval:  -2469.0226463115164
new min fval:  -2469.254273246085
new min fval:  -2469.43212178962
new min fval:  -2469.5720146769418
new min fval:  -2469.6804969052796
new min fval:  -2469.757522047546
new min fval:  -2469.848766741461
new min fval:  -2469.9467893632836
new min fval:  -2470.102492915871
new min fval:  -2470.2956872390087
new min fval:  -2470.509460601884
new min fval:  -2470.722170148828
new min fval:  -2470.929693732934
new min fval:  -2471.1387333712664
new min fval:  -2471.3256790011574
new min fval:  -2471.5080449683373
new min fval:  -2471.689197553118
new min fval:  -2471.862890139514
new min fval:  -2472.0934204466744
new min fval:  -2472.396603305472
new min fval:  -2472.768277066228
new min fval:  -2473.1989208244263
new min fval:  -2473.6726224786453
new min fval:  -2474.1368481863533
new min fval:  -2474.549239637306
new min fval:  -2474.881454482442
new min fval:  -2475.1594525777527
new min fval:  -2475.3865486613668
new min fval:  -2475.5917551219686
new min fval:  -2475.81781279823
new min fval:  -2476.066600789962
new min fval:  -2476.3423711726978
new min fval:  -2476.6649314804295
new min fval:  -2476.9988035434717
new min fval:  -2477.3330717292943
new min fval:  -2477.6499801086693
new min fval:  -2477.9258485085834
new min fval:  -2478.1503324417463
new min fval:  -2478.3182149043523
new min fval:  -2478.467275951638
new min fval:  -2478.5686436919823
new min fval:  -2478.6647520776014
new min fval:  -2478.773396503854
new min fval:  -2478.9210310084554
new min fval:  -2479.100455155546
new min fval:  -2479.32474763675
new min fval:  -2479.564747653692
new min fval:  -2479.796305321245
new min fval:  -2479.985232876542
new min fval:  -2480.1162789119944
new min fval:  -2480.172695152876
new min fval:  -2480.1893719498385
new min fval:  -2480.259810341514
new min fval:  -2480.4245133866925
new min fval:  -2480.6834952290187
new min fval:  -2480.9949434265104
new min fval:  -2481.323810771745
new min fval:  -2481.627665297726
new min fval:  -2481.855940560141
new min fval:  -2482.0338781900377
new min fval:  -2482.115892144448
new min fval:  -2482.1606768342594
new min fval:  -2482.1813529478122
new min fval:  -2482.1972216411614
new min fval:  -2482.211113615932
new min fval:  -2482.2614500275586
new min fval:  -2482.361197323516
new min fval:  -2482.5401092700818
new min fval:  -2482.8163233673276
new min fval:  -2483.1164999213265
new min fval:  -2483.415093806603
new min fval:  -2483.6751795467176
new min fval:  -2483.88547809309
new min fval:  -2484.073354585427
new min fval:  -2484.2113585941074
new min fval:  -2484.3273523362645
new min fval:  -2484.4483501869386
new min fval:  -2484.5748974160483
new min fval:  -2484.718946255609
new min fval:  -2484.9337843023386
new min fval:  -2485.19541140195
new min fval:  -2485.4649444412025
new min fval:  -2485.7291260096795
new min fval:  -2485.9762701428444
new min fval:  -2486.1762472199175
new min fval:  -2486.349426084745
new min fval:  -2486.4830876361757
new min fval:  -2486.62677254725
new min fval:  -2486.7698567859056
new min fval:  -2486.938382611602
new min fval:  -2487.1565348341023
new min fval:  -2487.3812906993885
new min fval:  -2487.620541573614
new min fval:  -2487.847559044949
new min fval:  -2488.0703581320886
new min fval:  -2488.250624501332
new min fval:  -2488.4089782711317
new min fval:  -2488.5418663122487
new min fval:  -2488.680963664679
new min fval:  -2488.821712778239
new min fval:  -2489.0005108908636
new min fval:  -2489.2190746904075
new min fval:  -2489.4857180145523
new min fval:  -2489.755583221052
new min fval:  -2490.0360000815763
new min fval:  -2490.2873511857133
new min fval:  -2490.515817855243
new min fval:  -2490.719487944985
new min fval:  -2490.892008326666
new min fval:  -2491.0883466656906
new min fval:  -2491.291579946432
new min fval:  -2491.4934792909994
new min fval:  -2491.696199747805
new min fval:  -2491.916237826872
new min fval:  -2492.148738411234
new min fval:  -2492.3622059497825
new min fval:  -2492.5545636711367
new min fval:  -2492.735737921318
new min fval:  -2492.9087716615286
new min fval:  -2493.07218334037
new min fval:  -2493.22784122344
new min fval:  -2493.3855087841757
new min fval:  -2493.573801987762
new min fval:  -2493.760764961168
new min fval:  -2493.9695697995885
new min fval:  -2494.187908304597
new min fval:  -2494.4129194390457
new min fval:  -2494.6283006656554
new min fval:  -2494.833868937484
new min fval:  -2495.036141491618
new min fval:  -2495.207176915348
new min fval:  -2495.3616809214445
new min fval:  -2495.52647463494
new min fval:  -2495.673712863903
new min fval:  -2495.857574666142
new min fval:  -2496.0458499006154
new min fval:  -2496.2672855855917
new min fval:  -2496.496286823783
new min fval:  -2496.7478305470654
new min fval:  -2496.9963387868575
new min fval:  -2497.2455334174547
new min fval:  -2497.4943927655495
new min fval:  -2497.7441248965015
new min fval:  -2498.025446739609
new min fval:  -2498.2900388795324
new min fval:  -2498.5867101930257
new min fval:  -2498.8722018266117
new min fval:  -2499.137006383844
new min fval:  -2499.4632079029584
new min fval:  -2499.834875579176
new min fval:  -2500.2246293959884
new min fval:  -2500.628711507666
new min fval:  -2501.0072854267255
new min fval:  -2501.348875728235
new min fval:  -2501.643284189002
new min fval:  -2501.905011020855
new min fval:  -2502.143573476039
new min fval:  -2502.3526205270764
new min fval:  -2502.5574030806247
new min fval:  -2502.7530551770656
new min fval:  -2502.9231205155393
new min fval:  -2503.1103907634038
new min fval:  -2503.28016879043
new min fval:  -2503.4477735640035
new min fval:  -2503.585551137847
new min fval:  -2503.705043492933
new min fval:  -2503.7983562328873
new min fval:  -2503.882525771358
new min fval:  -2503.930594859971
new min fval:  -2503.968423449156
new min fval:  -2504.0148970972145
new min fval:  -2504.088518474165
new min fval:  -2504.21914343349
new min fval:  -2504.405687271622
new min fval:  -2504.6222060819623
new min fval:  -2504.845174216697
new min fval:  -2505.0852715816645
new min fval:  -2505.3242652313606
new min fval:  -2505.588660022512
new min fval:  -2505.8405623369017
new min fval:  -2506.0967881402216
new min fval:  -2506.373579399814
new min fval:  -2506.6220681195464
new min fval:  -2506.8682760304355
new min fval:  -2507.119973140764
new min fval:  -2507.389469781779
new min fval:  -2507.6873873851187
new min fval:  -2507.9748306601996
new min fval:  -2508.272771421427
new min fval:  -2508.5732471017827
new min fval:  -2508.85628401164
new min fval:  -2509.1299100619344
new min fval:  -2509.360543340584
new min fval:  -2509.5837504606907
new min fval:  -2509.771720244863
new min fval:  -2509.9487718198106
new min fval:  -2510.1246767840053
new min fval:  -2510.340467120029
new min fval:  -2510.588050268575
new min fval:  -2510.870310158315
new min fval:  -2511.1574493204603
new min fval:  -2511.4490632515485
new min fval:  -2511.7288119457926
new min fval:  -2511.9982407288194
new min fval:  -2512.2985504832996
new min fval:  -2512.577463027718
new min fval:  -2512.858581336979
new min fval:  -2513.117096919366
new min fval:  -2513.3673799645744
new min fval:  -2513.6055175250444
new min fval:  -2513.8292444794947
new min fval:  -2514.046451232981
new min fval:  -2514.249384286096
new min fval:  -2514.4622659155925
new min fval:  -2514.6512216740593
new min fval:  -2514.8680901214666
new min fval:  -2515.0752045573417
new min fval:  -2515.3224663475216
new min fval:  -2515.5758537140973
new min fval:  -2515.797251856722
new min fval:  -2516.023708640093
new min fval:  -2516.2279770542636
new min fval:  -2516.429406453011
new min fval:  -2516.6235420511493
new min fval:  -2516.7772932252096
new min fval:  -2516.927618405764
new min fval:  -2517.0920423985426
new min fval:  -2517.2794972673805
new min fval:  -2517.482715605839
new min fval:  -2517.716458400691
new min fval:  -2517.9465429496977
new min fval:  -2518.164094793084
new min fval:  -2518.3606337109936
new min fval:  -2518.5445946730347
new min fval:  -2518.7697708135183
new min fval:  -2519.015594205526
new min fval:  -2519.2962375754505
new min fval:  -2519.611107340229
new min fval:  -2519.9055158988076
new min fval:  -2520.20050361407
new min fval:  -2520.5284417384432
new min fval:  -2520.860518420831
new min fval:  -2521.2265403801166
new min fval:  -2521.604404888738
new min fval:  -2521.9577879633352
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.75795]
objective value function right now is: -2369.9710738275953
new min fval:  -2522.303855542665
new min fval:  -2522.6159530477653
new min fval:  -2522.9049769214225
new min fval:  -2523.135717479662
new min fval:  -2523.3522949939056
new min fval:  -2523.543007274976
new min fval:  -2523.691419610536
new min fval:  -2523.834826455222
new min fval:  -2523.9548774137265
new min fval:  -2524.0872733569613
new min fval:  -2524.2509009914024
new min fval:  -2524.427463544026
new min fval:  -2524.637178005594
new min fval:  -2524.854681015834
new min fval:  -2525.0658753894736
new min fval:  -2525.257518642511
new min fval:  -2525.42165362674
new min fval:  -2525.564786276764
new min fval:  -2525.695570510798
new min fval:  -2525.830206335238
new min fval:  -2525.982251286385
new min fval:  -2526.167876079582
new min fval:  -2526.376348838216
new min fval:  -2526.621720450921
new min fval:  -2526.895491752682
new min fval:  -2527.174329218835
new min fval:  -2527.473064976929
new min fval:  -2527.7605544906696
new min fval:  -2528.009898931677
new min fval:  -2528.244655145796
new min fval:  -2528.4696994279493
new min fval:  -2528.688006477585
new min fval:  -2528.890915665647
new min fval:  -2529.0801032519653
new min fval:  -2529.270466649499
new min fval:  -2529.468103637158
new min fval:  -2529.6744127242296
new min fval:  -2529.871420952844
new min fval:  -2530.0451086382554
new min fval:  -2530.165912266917
new min fval:  -2530.25247482973
new min fval:  -2530.320208063015
new min fval:  -2530.3843618700844
new min fval:  -2530.444853900342
new min fval:  -2530.496522827359
new min fval:  -2530.5596450858275
new min fval:  -2530.605853315103
new min fval:  -2530.6668134330575
new min fval:  -2530.7083697483554
new min fval:  -2530.740761425549
new min fval:  -2530.7662871903826
new min fval:  -2530.8008720316097
new min fval:  -2530.8631506609513
new min fval:  -2530.9749674508826
new min fval:  -2531.102888850526
new min fval:  -2531.251405419955
new min fval:  -2531.4324166461715
new min fval:  -2531.6244670138817
new min fval:  -2531.821126377077
new min fval:  -2532.0527088528734
new min fval:  -2532.2696595590746
new min fval:  -2532.4779426506816
new min fval:  -2532.659985896523
new min fval:  -2532.8279310568955
new min fval:  -2532.9718406579227
new min fval:  -2533.1361239262374
new min fval:  -2533.3172877878023
new min fval:  -2533.528274440352
new min fval:  -2533.763916330778
new min fval:  -2534.0001238223854
new min fval:  -2534.2415976289158
new min fval:  -2534.45008868647
new min fval:  -2534.633251756808
new min fval:  -2534.760335472404
new min fval:  -2534.8431601193483
new min fval:  -2534.9295174430376
new min fval:  -2535.0043521006655
new min fval:  -2535.11752179436
new min fval:  -2535.2280176194517
new min fval:  -2535.3455668927813
new min fval:  -2535.4449456781285
new min fval:  -2535.5555205933397
new min fval:  -2535.654775973204
new min fval:  -2535.7337524512586
new min fval:  -2535.8054038188316
new min fval:  -2535.8584150262254
new min fval:  -2535.8867145124286
new min fval:  -2535.9264896141526
new min fval:  -2535.9757301710656
new min fval:  -2536.025364294193
new min fval:  -2536.093563416282
new min fval:  -2536.1479946041422
new min fval:  -2536.2080753166015
new min fval:  -2536.246594824678
new min fval:  -2536.3186804444863
new min fval:  -2536.4191301311594
new min fval:  -2536.56076145057
new min fval:  -2536.7273230015576
new min fval:  -2536.9112470210157
new min fval:  -2537.1243819477886
new min fval:  -2537.3623986342195
new min fval:  -2537.6174171566026
new min fval:  -2537.8820741892046
new min fval:  -2538.167828595
new min fval:  -2538.463266230684
new min fval:  -2538.7615902771076
new min fval:  -2539.0361431179376
new min fval:  -2539.302192869084
new min fval:  -2539.5476289945477
new min fval:  -2539.7728930536246
new min fval:  -2539.985893362785
new min fval:  -2540.1870155746665
new min fval:  -2540.3850622880173
new min fval:  -2540.5852250650023
new min fval:  -2540.7877189118553
new min fval:  -2541.0071713438965
new min fval:  -2541.2513307533127
new min fval:  -2541.4801910077426
new min fval:  -2541.696230631289
new min fval:  -2541.8884109371893
new min fval:  -2542.061994650484
new min fval:  -2542.2204105178994
new min fval:  -2542.3524804745634
new min fval:  -2542.4928340731008
new min fval:  -2542.6369133141106
new min fval:  -2542.788376621699
new min fval:  -2542.963258142179
new min fval:  -2543.1171377177657
new min fval:  -2543.2353110547856
new min fval:  -2543.33581448563
new min fval:  -2543.437958425751
new min fval:  -2543.6335808142835
new min fval:  -2543.884863994661
new min fval:  -2544.1965258850223
new min fval:  -2544.5370071120333
new min fval:  -2544.8709063758597
new min fval:  -2545.1968343970316
new min fval:  -2545.506301595241
new min fval:  -2545.7960120347307
new min fval:  -2546.0585590909463
new min fval:  -2546.2914448107267
new min fval:  -2546.499718822824
new min fval:  -2546.6674668726605
new min fval:  -2546.8262638283786
new min fval:  -2546.9857177943622
new min fval:  -2547.141255809035
new min fval:  -2547.3200908967356
new min fval:  -2547.505501915286
new min fval:  -2547.6760156380146
new min fval:  -2547.857687544076
new min fval:  -2547.9792911668114
new min fval:  -2548.0398803624967
new min fval:  -2548.0572990789406
new min fval:  -2548.109899060683
new min fval:  -2548.253811458086
new min fval:  -2548.417069337654
new min fval:  -2548.5637764301428
new min fval:  -2548.7093689943777
new min fval:  -2548.8488002329636
new min fval:  -2548.992727790673
new min fval:  -2549.127320057125
new min fval:  -2549.2721331580883
new min fval:  -2549.433718136871
new min fval:  -2549.5932339616265
new min fval:  -2549.7724381399817
new min fval:  -2549.9385783510847
new min fval:  -2550.1044370192135
new min fval:  -2550.268024493592
new min fval:  -2550.433317737382
new min fval:  -2550.587339338836
new min fval:  -2550.7461643005086
new min fval:  -2550.877977943987
new min fval:  -2551.0196734717147
new min fval:  -2551.134733350399
new min fval:  -2551.2472384738317
new min fval:  -2551.358290290819
new min fval:  -2551.4625373262606
new min fval:  -2551.5968238951773
new min fval:  -2551.7461653530777
new min fval:  -2551.8852153943444
new min fval:  -2552.036388206195
new min fval:  -2552.177528802965
new min fval:  -2552.3215832947753
new min fval:  -2552.461686628853
new min fval:  -2552.6041867419376
new min fval:  -2552.7505173420295
new min fval:  -2552.9053888154876
new min fval:  -2553.090995153669
new min fval:  -2553.2843368102594
new min fval:  -2553.476744504172
new min fval:  -2553.6700615677655
new min fval:  -2553.872919276573
new min fval:  -2554.0477413697895
new min fval:  -2554.1974401927937
new min fval:  -2554.3398103122586
new min fval:  -2554.476739428126
new min fval:  -2554.611313610663
new min fval:  -2554.731869001327
new min fval:  -2554.871189517091
new min fval:  -2555.0219984103046
new min fval:  -2555.183446044666
new min fval:  -2555.3232558993122
new min fval:  -2555.459094037585
new min fval:  -2555.5894883888122
new min fval:  -2555.6889426067005
new min fval:  -2555.7707584372283
new min fval:  -2555.8405873009247
new min fval:  -2555.8985177443624
new min fval:  -2555.9674727961824
new min fval:  -2556.075417926588
new min fval:  -2556.188300567777
new min fval:  -2556.3277861548177
new min fval:  -2556.483609551392
new min fval:  -2556.674641770824
new min fval:  -2556.875274874993
new min fval:  -2557.068101004101
new min fval:  -2557.2518686646326
new min fval:  -2557.449347975783
new min fval:  -2557.6416966982842
new min fval:  -2557.832547276815
new min fval:  -2558.00451871798
new min fval:  -2558.1565531194624
new min fval:  -2558.2842829533333
new min fval:  -2558.3866748993137
new min fval:  -2558.4571807094626
new min fval:  -2558.5158662004396
new min fval:  -2558.5742339111034
new min fval:  -2558.6142976829833
new min fval:  -2558.657343349483
new min fval:  -2558.693190191768
new min fval:  -2558.7124820694553
new min fval:  -2558.7429919224605
new min fval:  -2558.768788888895
new min fval:  -2558.7952540727165
new min fval:  -2558.8335382116124
new min fval:  -2558.879478350668
new min fval:  -2558.9474426178717
new min fval:  -2559.008389335681
new min fval:  -2559.086136289296
new min fval:  -2559.1731831325646
new min fval:  -2559.287876826185
new min fval:  -2559.4115636409624
new min fval:  -2559.535848380061
new min fval:  -2559.6660186069585
new min fval:  -2559.8157967953534
new min fval:  -2559.964699569478
new min fval:  -2560.1099614169925
new min fval:  -2560.2448344723734
new min fval:  -2560.3642736484467
new min fval:  -2560.4460890748646
new min fval:  -2560.500645722109
new min fval:  -2560.54629221587
new min fval:  -2560.5891305307923
new min fval:  -2560.6395073643057
new min fval:  -2560.7075478806983
new min fval:  -2560.8106055268036
new min fval:  -2560.931850463025
new min fval:  -2561.0729943349347
new min fval:  -2561.2109010678555
new min fval:  -2561.3481288676953
new min fval:  -2561.47818085105
new min fval:  -2561.62168536028
new min fval:  -2561.718567858849
new min fval:  -2561.827983063428
new min fval:  -2561.932134628747
new min fval:  -2562.064928195959
new min fval:  -2562.208256043881
new min fval:  -2562.349938495592
new min fval:  -2562.4830084963814
new min fval:  -2562.627343678453
new min fval:  -2562.7676769715213
new min fval:  -2562.8946825880153
new min fval:  -2563.029175795647
new min fval:  -2563.1585599019904
new min fval:  -2563.2756851636545
new min fval:  -2563.3918331871237
new min fval:  -2563.4908577236674
new min fval:  -2563.577965586312
new min fval:  -2563.6590618504365
new min fval:  -2563.76495397559
new min fval:  -2563.886306378278
new min fval:  -2564.013314990734
new min fval:  -2564.1428604510184
new min fval:  -2564.2926217849504
new min fval:  -2564.4265035188337
new min fval:  -2564.553248778359
new min fval:  -2564.688716578892
new min fval:  -2564.824084510292
new min fval:  -2564.987303259873
new min fval:  -2565.1517493980855
new min fval:  -2565.311150872806
new min fval:  -2565.4682429488907
new min fval:  -2565.632276893136
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.8492565]
objective value function right now is: -2523.833505768844
min fval:  -2565.632276893136
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1001.2116024588695
W_T_median: 858.2090444853816
W_T_pctile_5: 245.44625306581554
W_T_CVAR_5_pct: 61.09607623788077
Average q (qsum/M+1):  41.37006599672379
Optimal xi:  [14.164052]
Expected(across Rb) median(across samples) p_equity:  0.23588811084628106
obj fun:  tensor(-2565.6323, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.82211]
objective value function right now is: -128487.34696178936
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.678146]
objective value function right now is: -116008.19660888516
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.484811]
objective value function right now is: -127461.44961285683
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.508223]
objective value function right now is: -132958.11410796398
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.029178]
objective value function right now is: -101628.5161071425
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.581568]
objective value function right now is: -123860.89111257275
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [14.368782]
objective value function right now is: -122568.43682673865
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.540153]
objective value function right now is: -126333.72733755595
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.414695]
objective value function right now is: -123612.20349050533
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.812272]
objective value function right now is: -136453.97357941268
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.538179]
objective value function right now is: -129451.85535651399
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.214558]
objective value function right now is: -114447.01584942121
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.823873]
objective value function right now is: -115290.67016658749
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [14.577821]
objective value function right now is: -122972.18270911327
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.747273]
objective value function right now is: -105629.82896204387
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.754241]
objective value function right now is: -122183.40813012392
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.478229]
objective value function right now is: -139681.7795885886
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.633812]
objective value function right now is: -118715.51322982035
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.69453]
objective value function right now is: -98054.97270127332
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.798056]
objective value function right now is: -120886.79195724377
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.477537]
objective value function right now is: -116600.91293937746
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.698824]
objective value function right now is: -130865.13819204099
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.980473]
objective value function right now is: -124226.89333267004
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.646711]
objective value function right now is: -13352.930884956617
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.395998]
objective value function right now is: -125028.47082822281
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.117962]
objective value function right now is: -132188.75795231684
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.269502]
objective value function right now is: -97758.26904111117
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [14.231116]
objective value function right now is: -112792.81902590203
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [14.582141]
objective value function right now is: -138508.1326487312
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.281947]
objective value function right now is: 25828.078313149475
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.756805]
objective value function right now is: -119162.26630462024
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.645029]
objective value function right now is: -127307.5227305491
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.915469]
objective value function right now is: -130783.23081716991
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.756034]
objective value function right now is: -134346.4716347783
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.638482]
objective value function right now is: -138202.274117453
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.692065]
objective value function right now is: -136235.365521805
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.305087]
objective value function right now is: -91622.5497275773
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.674181]
objective value function right now is: -132933.29554470693
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.625521]
objective value function right now is: -131999.10605596364
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.349085]
objective value function right now is: -137901.0643806144
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.40932]
objective value function right now is: -92377.62448787825
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.340612]
objective value function right now is: -130215.86240279267
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.449162]
objective value function right now is: -127708.05359802915
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.527931]
objective value function right now is: -137285.47581545514
new min fval:  -117211.94396659605
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.494374]
objective value function right now is: -132585.11132244574
new min fval:  -126991.15974825395
new min fval:  -127056.32079834977
new min fval:  -127674.7947865159
new min fval:  -128538.73664032978
new min fval:  -129127.00377601346
new min fval:  -130044.20616787425
new min fval:  -131462.56490350573
new min fval:  -132946.54803640884
new min fval:  -134482.55182943965
new min fval:  -135695.5205380286
new min fval:  -136076.2474028016
new min fval:  -136325.93028699828
new min fval:  -136867.18390374078
new min fval:  -137288.97101585328
new min fval:  -137549.8502085018
new min fval:  -137718.48248471736
new min fval:  -137756.66498437338
new min fval:  -137953.99573370029
new min fval:  -138356.15722159913
new min fval:  -138757.7102271768
new min fval:  -139066.0055681917
new min fval:  -139334.49166295875
new min fval:  -139475.97889972347
new min fval:  -139528.5068736002
new min fval:  -139533.60281664194
new min fval:  -139559.3379180537
new min fval:  -139590.72205477196
new min fval:  -139631.09406769573
new min fval:  -139685.45674868132
new min fval:  -139716.82371671242
new min fval:  -139739.8413979836
new min fval:  -139754.1663675769
new min fval:  -139763.36277834437
new min fval:  -139769.58957825016
new min fval:  -139777.52206682484
new min fval:  -139782.25962966992
new min fval:  -139793.11126925028
new min fval:  -139818.52222069047
new min fval:  -139845.39953478216
new min fval:  -139856.58973174
new min fval:  -139865.21695072608
new min fval:  -139872.66606902052
new min fval:  -139880.33027894644
new min fval:  -139890.7955818762
new min fval:  -139905.78610535205
new min fval:  -139924.60644085077
new min fval:  -139942.3343282711
new min fval:  -139962.01309054112
new min fval:  -139979.68582028095
new min fval:  -139997.54866772538
new min fval:  -140019.61264520558
new min fval:  -140041.16173226523
new min fval:  -140068.51586463384
new min fval:  -140106.22495375227
new min fval:  -140146.96217518777
new min fval:  -140189.23841794464
new min fval:  -140232.89912430794
new min fval:  -140274.33490001445
new min fval:  -140309.58781116238
new min fval:  -140338.21293726267
new min fval:  -140361.73325826993
new min fval:  -140377.98389664185
new min fval:  -140386.65679061913
new min fval:  -140389.25331719074
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.658132]
objective value function right now is: -133629.6915084554
new min fval:  -140397.81421548614
new min fval:  -140409.44168358666
new min fval:  -140417.3824974178
new min fval:  -140417.40596435082
new min fval:  -140425.8771079381
new min fval:  -140450.10972019672
new min fval:  -140468.91237729907
new min fval:  -140480.64650186713
new min fval:  -140480.73968898642
new min fval:  -140501.37080930258
new min fval:  -140527.22012286517
new min fval:  -140556.70117862296
new min fval:  -140589.30287521493
new min fval:  -140622.49396565405
new min fval:  -140652.34700034803
new min fval:  -140676.50687071588
new min fval:  -140696.88103798367
new min fval:  -140714.81249748528
new min fval:  -140728.12081273537
new min fval:  -140740.4398219959
new min fval:  -140749.26750847464
new min fval:  -140754.7013809309
new min fval:  -140758.67309567984
new min fval:  -140763.08062553662
new min fval:  -140768.25976184875
new min fval:  -140775.22857093264
new min fval:  -140784.46969070082
new min fval:  -140793.63216839478
new min fval:  -140805.07959194152
new min fval:  -140817.91540737308
new min fval:  -140831.42673689075
new min fval:  -140847.11997603113
new min fval:  -140861.29402611344
new min fval:  -140875.11391505157
new min fval:  -140889.25024022063
new min fval:  -140904.9133582651
new min fval:  -140926.68495371757
new min fval:  -140949.92254444936
new min fval:  -140970.1689450342
new min fval:  -140986.17395793338
new min fval:  -140998.50773024803
new min fval:  -141011.99440532777
new min fval:  -141019.42059790433
new min fval:  -141023.572372752
new min fval:  -141026.26008164705
new min fval:  -141031.29483718704
new min fval:  -141038.42293269146
new min fval:  -141045.56877406305
new min fval:  -141053.21210647243
new min fval:  -141062.86421538773
new min fval:  -141069.0077168332
new min fval:  -141080.20058217543
new min fval:  -141089.7898921359
new min fval:  -141098.34011872418
new min fval:  -141112.7151184861
new min fval:  -141134.655550282
new min fval:  -141194.88257348957
new min fval:  -141237.6410235614
new min fval:  -141261.1632696377
new min fval:  -141278.3255390088
new min fval:  -141293.00914660227
new min fval:  -141305.7612396155
new min fval:  -141318.139332293
new min fval:  -141325.41376977362
new min fval:  -141333.00152912585
new min fval:  -141348.29717024
new min fval:  -141371.6079261989
new min fval:  -141392.647300323
new min fval:  -141412.95274942846
new min fval:  -141432.9413301937
new min fval:  -141449.6039872204
new min fval:  -141463.3234081406
new min fval:  -141475.35415649193
new min fval:  -141485.4734553035
new min fval:  -141489.9043081227
new min fval:  -141492.14103748705
new min fval:  -141494.58593820257
new min fval:  -141504.73044301287
new min fval:  -141514.98459677183
new min fval:  -141524.83755958296
new min fval:  -141532.38325888274
new min fval:  -141540.9084874027
new min fval:  -141551.1182319787
new min fval:  -141561.09080482708
new min fval:  -141572.74249362692
new min fval:  -141588.82942350078
new min fval:  -141608.0907605453
new min fval:  -141629.83208244873
new min fval:  -141650.06512224377
new min fval:  -141662.93173120683
new min fval:  -141673.60901665044
new min fval:  -141680.16930309625
new min fval:  -141683.6648140974
new min fval:  -141715.5610140396
new min fval:  -141728.29357774384
new min fval:  -141738.9110510046
new min fval:  -141749.58026738322
new min fval:  -141760.43651431135
new min fval:  -141767.85241517032
new min fval:  -141775.43608814233
new min fval:  -141786.6382271967
new min fval:  -141793.22636131156
new min fval:  -141796.48369357627
new min fval:  -141800.3775868064
new min fval:  -141806.0781926161
new min fval:  -141811.9490055998
new min fval:  -141820.2832335161
new min fval:  -141827.3042687465
new min fval:  -141834.03057811363
new min fval:  -141840.87814098079
new min fval:  -141845.62288098605
new min fval:  -141850.53299394835
new min fval:  -141851.8807659466
new min fval:  -141852.9382529313
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.021613]
objective value function right now is: 218720.5525196004
new min fval:  -141862.59318760573
new min fval:  -141863.53510386863
new min fval:  -141885.51404324025
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.227083]
objective value function right now is: -102887.57475641312
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.76822]
objective value function right now is: 3172127.02451693
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00844883]
objective value function right now is: 1822893.5713841924
min fval:  -141885.51404324025
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1118.0995176119743
W_T_median: 950.1331165906631
W_T_pctile_5: 248.52638746459567
W_T_CVAR_5_pct: 63.21125163821102
Average q (qsum/M+1):  39.696017357610884
Optimal xi:  [14.472341]
Expected(across Rb) median(across samples) p_equity:  0.24473966012398402
obj fun:  tensor(-141885.5140, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5000.0
-----------------------------------------------
