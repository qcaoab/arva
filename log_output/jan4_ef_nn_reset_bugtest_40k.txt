Starting at: 
04-01-23_17:19

 Random seed:  2  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1  logistic_sigmoid   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -552.9634342543877
Current xi:  [3.55113]
objective value function right now is: -552.9634342543877
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.7644267]
objective value function right now is: -423.17120308864924
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1246.8660403306797
Current xi:  [3.877329]
objective value function right now is: -1246.8660403306797
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.1281157]
objective value function right now is: -1239.5395153367467
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.7435536]
objective value function right now is: -738.9474001650376
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.4455268]
objective value function right now is: -1068.6659597925338
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [3.555067]
objective value function right now is: -973.1082968769292
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.092913]
objective value function right now is: -930.1774604147769
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1301.6704468854587
Current xi:  [3.5822856]
objective value function right now is: -1301.6704468854587
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.2418323]
objective value function right now is: -1056.1954429826885
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.238731]
objective value function right now is: -1174.0480024291605
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.2305336]
objective value function right now is: -1160.1929173778622
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1358.9873206381785
Current xi:  [3.8086748]
objective value function right now is: -1358.9873206381785
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [3.8414032]
objective value function right now is: -1147.7386534445072
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1414.8026105427296
Current xi:  [3.9752083]
objective value function right now is: -1414.8026105427296
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.800499]
objective value function right now is: -1367.1460511329562
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.8749387]
objective value function right now is: -910.4959020094963
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1530.5766046145786
Current xi:  [4.151956]
objective value function right now is: -1530.5766046145786
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.9655995]
objective value function right now is: -1464.5042137809853
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.2683105]
objective value function right now is: -1355.123505076416
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.7454972]
objective value function right now is: -1382.1959139272867
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.9852326]
objective value function right now is: -1517.3586658771085
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.9153984]
objective value function right now is: -1409.989786764868
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.9422946]
objective value function right now is: -1268.0154788907407
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.320045]
objective value function right now is: -1451.5433407374794
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.228174]
objective value function right now is: -1425.9231626243238
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.154069]
objective value function right now is: -1382.121227599375
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [4.0356584]
objective value function right now is: -1439.786341469184
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [4.0708046]
objective value function right now is: -1415.4388157641567
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.6862738]
objective value function right now is: -1447.9519601017246
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.6585355]
objective value function right now is: -1391.955356128976
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.4444351]
objective value function right now is: -1199.2883566717076
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.1525474]
objective value function right now is: -1314.9167252876894
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.561035]
objective value function right now is: -1017.3344450676077
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.9250793]
objective value function right now is: -1360.3028350179547
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.1346226]
objective value function right now is: -690.9582981542977
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.8846061]
objective value function right now is: -1191.5030192185473
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.9044993]
objective value function right now is: -1484.7788014160537
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.044665]
objective value function right now is: -1334.4326306840983
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.9094381]
objective value function right now is: -336.07068839625236
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.6677337]
objective value function right now is: -1201.8770319202345
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.905364]
objective value function right now is: -1486.2360074644005
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.9760406]
objective value function right now is: -1123.8079853455465
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.593556]
objective value function right now is: -989.6632253136949
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.1856813]
objective value function right now is: -1266.0193271264563
new min fval from sgd:  -1537.7719063580596
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.7360034]
objective value function right now is: -885.1708609854344
new min fval from sgd:  -1549.714401767488
new min fval from sgd:  -1550.7777576998035
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.8612425]
objective value function right now is: -1534.0441760462088
new min fval from sgd:  -1551.823862369474
new min fval from sgd:  -1557.3584555450682
new min fval from sgd:  -1558.9816442438726
new min fval from sgd:  -1559.9844868701452
new min fval from sgd:  -1560.2833500377858
new min fval from sgd:  -1561.0460106694486
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.1939983]
objective value function right now is: -1475.1545224925612
new min fval from sgd:  -1564.8520104293
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.140834]
objective value function right now is: -1450.3555191553478
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.4343054]
objective value function right now is: -1198.6220636233636
min fval:  -1564.8520104293
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.2514, -0.8944],
        [ 1.2942, -1.1841],
        [ 0.1737,  0.8964],
        [ 0.8566, -1.2708],
        [ 1.8042, -0.7913],
        [ 0.1443,  0.2269],
        [-1.2466,  0.8158],
        [ 0.9539, -0.2052],
        [ 1.2432, -1.4496],
        [ 1.2378, -0.9554]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.9920,  1.3962,  1.0920,  1.3394,  1.4942,  1.1633,  0.8812,  1.1598,
          1.1103,  1.6016],
        [ 0.9879,  1.5178,  0.8752,  1.3152,  1.4255,  0.9488,  0.8872,  1.2577,
          1.2917,  1.0837],
        [ 0.3425,  0.8645,  0.1933,  0.5032,  0.7395,  0.1323, -0.1217,  0.5318,
          0.8165,  0.8493],
        [ 1.1074,  1.3130,  0.8133,  1.1991,  1.3833,  1.3365,  1.1662,  1.0087,
          1.1905,  1.1477],
        [ 0.6760,  1.3085,  0.3541,  1.3950,  1.0302,  0.2981, -0.1435,  0.6821,
          0.8475,  1.5231],
        [ 1.2480,  1.4503,  0.9156,  1.4279,  1.1324,  0.7920,  0.8522,  1.1323,
          1.3567,  1.4139],
        [ 1.1725,  1.2902,  1.4252,  1.1111,  1.0686,  0.9024,  1.0322,  1.3080,
          1.0722,  1.1866],
        [-0.6000, -0.2990, -0.7174, -0.3215, -0.6827, -0.6405, -0.8481, -0.4542,
         -0.6931, -0.2469],
        [ 1.3737,  1.2815,  1.4752,  1.0402,  1.2297,  1.2199,  1.3502,  1.1538,
          1.4633,  1.0704],
        [ 0.9464,  1.0591,  0.9385,  1.3445,  1.6123,  1.2432,  0.8818,  1.1103,
          1.0677,  1.2413]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-5.2592, -5.1957, -1.9610, -4.3762, -3.3682, -4.3640, -5.1335, -0.6984,
         -4.2347, -5.8800]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.6925,   4.5678],
        [ -2.6031,   4.3777],
        [ -1.7886,   4.4604],
        [ -7.4268,   0.1412],
        [ -2.6483,   4.3692],
        [ -2.3622,   4.4332],
        [-12.0180,  -2.1527],
        [ -2.3354,   4.1277],
        [  8.6741,   3.6678],
        [ -5.2275,   0.8163]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.7384e+02, -2.6188e+02, -2.1539e+02, -7.6779e+01, -2.6483e+02,
         -2.5068e+02, -4.2014e+01, -2.0205e+02, -5.7353e+00, -9.6197e+01],
        [-2.0592e+02, -2.7072e+02, -2.4173e+02, -6.5970e+01, -2.7255e+02,
         -2.6513e+02, -4.7619e+01, -2.1456e+02, -2.8955e+01, -8.1456e+01],
        [-7.6937e+01, -1.7660e+02, -1.2418e+02, -1.0080e+02, -1.7913e+02,
         -1.6219e+02, -5.8696e+01, -1.2801e+02, -4.4053e+00, -1.1617e+02],
        [-2.1918e+02, -2.9135e+02, -2.5493e+02, -5.7446e+01, -2.9362e+02,
         -2.8369e+02, -3.4201e+01, -2.2697e+02, -1.1024e+01, -8.0230e+01],
        [-3.3212e+01, -6.4958e+01, -5.8549e+01, -2.0713e+01, -6.5141e+01,
         -6.3419e+01, -2.1799e+01, -3.6210e+01, -1.9298e+01, -1.2490e+01],
        [-5.5931e+01, -8.9674e+01, -7.0578e+01,  3.2594e+00, -9.0592e+01,
         -8.9176e+01,  9.0941e+00, -3.5768e+01, -8.2721e+00,  1.5286e+00],
        [-2.0506e+02, -2.7140e+02, -2.4196e+02, -5.4518e+01, -2.7284e+02,
         -2.6471e+02, -3.7231e+01, -2.1458e+02, -1.8195e+01, -7.7840e+01],
        [-1.4265e+01, -4.5536e+01, -1.9590e+01,  7.5972e-01, -4.7484e+01,
         -3.7669e+01, -2.1446e-01, -1.8143e+01, -3.2741e+00, -5.6517e+00],
        [-3.3580e+01, -6.6435e+01, -5.9945e+01, -2.0030e+01, -6.7494e+01,
         -6.5033e+01, -2.1097e+01, -3.5999e+01, -1.9347e+01, -1.2153e+01],
        [ 8.2032e+01,  1.0836e+02,  8.9035e+01, -2.6536e+00,  1.0814e+02,
          1.0922e+02, -2.7405e+01,  5.6097e+01,  7.5670e+01,  8.7442e+00]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 10.0039,  -0.4585,  12.3821,   3.9512, -13.9034, -14.4773,   1.3220,
          -8.7016, -12.9681,   0.5161],
        [-10.4444,   0.3278, -12.5294,  -3.8330,  13.7381,  14.2273,  -1.4120,
           8.6017,  12.7433,  -0.6100]], device='cuda:0'))])
xi:  [4.0585446]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1243.7511007148748
W_T_median: 1007.5874680794618
W_T_pctile_5: 204.1099219645743
W_T_CVAR_5_pct: 9.60377202177862
Average q (qsum/M+1):  35.0
Optimal xi:  [4.0585446]
Expected(across Rb) median(across samples) p_equity:  0.24488391627868017
obj fun:  tensor(-1564.8520, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1248.9406048520184
Current xi:  [2.6279783]
objective value function right now is: -1248.9406048520184
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1359.074206995998
Current xi:  [3.3575704]
objective value function right now is: -1359.074206995998
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.5229542]
objective value function right now is: -1347.0478632091128
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.522962]
objective value function right now is: -1344.5884019274158
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.2360833]
objective value function right now is: -1337.1886857371926
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1372.1300901478467
Current xi:  [3.4895647]
objective value function right now is: -1372.1300901478467
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [3.8437705]
objective value function right now is: -1352.2879910486254
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.1130302]
objective value function right now is: -1332.4747149037778
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.3908947]
objective value function right now is: -1366.4693807466879
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.4064572]
objective value function right now is: -1367.4132836407252
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.5033383]
objective value function right now is: -1274.9829979534093
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1387.42515057243
Current xi:  [3.803971]
objective value function right now is: -1387.42515057243
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.459895]
objective value function right now is: -1367.119150877497
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [3.4385655]
objective value function right now is: -1191.4978608501194
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.0292058]
objective value function right now is: -1356.3089220712914
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.749839]
objective value function right now is: -1384.10557324009
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.5329523]
objective value function right now is: -1358.855836760362
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.4206345]
objective value function right now is: -1349.2128863338548
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.6295505]
objective value function right now is: -1386.0531848652479
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1398.4865126693564
Current xi:  [3.569127]
objective value function right now is: -1398.4865126693564
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.5403686]
objective value function right now is: -1369.92848884003
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.9040194]
objective value function right now is: -1372.1866619320322
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.6016471]
objective value function right now is: -1395.3619503110094
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1458.5971282120493
Current xi:  [3.2086322]
objective value function right now is: -1458.5971282120493
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.5320966]
objective value function right now is: -1399.822280712253
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1481.3237847194616
Current xi:  [3.6288443]
objective value function right now is: -1481.3237847194616
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.5269961]
objective value function right now is: -1452.423934608684
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [3.598876]
objective value function right now is: -1478.0427782806237
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1485.188730012042
Current xi:  [3.1344454]
objective value function right now is: -1485.188730012042
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1487.4320041926467
Current xi:  [3.3825953]
objective value function right now is: -1487.4320041926467
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1488.6815757121187
Current xi:  [3.7003386]
objective value function right now is: -1488.6815757121187
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.031123]
objective value function right now is: -1477.269648388952
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.6760798]
objective value function right now is: -1485.6389954291974
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1490.032950018102
Current xi:  [3.3443182]
objective value function right now is: -1490.032950018102
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.7601378]
objective value function right now is: -1481.0323593140283
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.8560498]
objective value function right now is: -1444.0381485469882
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.9840333]
objective value function right now is: -1436.6140379799376
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.3931334]
objective value function right now is: -1465.359400218856
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.44959]
objective value function right now is: -1447.0809735096136
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.3345609]
objective value function right now is: -1459.5515188215948
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.5618656]
objective value function right now is: -1448.0391298664144
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1500.622260116146
Current xi:  [3.2554402]
objective value function right now is: -1500.622260116146
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.2800536]
objective value function right now is: -1490.7852230518515
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.2063098]
objective value function right now is: -1443.3770640051016
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.7772923]
objective value function right now is: -1489.7485700188147
new min fval from sgd:  -1501.9818544430514
new min fval from sgd:  -1502.8341571101555
new min fval from sgd:  -1503.277366285306
new min fval from sgd:  -1503.4354838496024
new min fval from sgd:  -1504.5397475882307
new min fval from sgd:  -1505.1404789381165
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.3244405]
objective value function right now is: -1489.339961957658
new min fval from sgd:  -1505.277936445057
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.555515]
objective value function right now is: -1489.3153310779187
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.38345]
objective value function right now is: -1500.2982721498067
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.4237127]
objective value function right now is: -1488.9121374381139
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.835351]
objective value function right now is: -1485.7709823804503
min fval:  -1505.277936445057
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -8.1485,   3.6190],
        [ -8.1920,   3.6496],
        [ -8.4656,   3.8332],
        [ -8.9705,   4.1613],
        [ -5.6917,   5.9173],
        [  1.2399,   5.9173],
        [ -8.8283,   4.0720],
        [ -8.2943,   3.7163],
        [-37.5143,  -0.9890],
        [ -8.6665,   3.9639]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-8.4933e+01, -8.5216e+01, -8.7567e+01, -9.3536e+01,  7.9777e+00,
          2.4039e+00, -9.2587e+01, -8.6228e+01,  2.6749e+01, -8.9484e+01],
        [-5.1248e+01, -5.0234e+01, -5.1468e+01, -5.4730e+01,  1.7440e+01,
          3.8700e-01, -5.4643e+01, -5.1638e+01,  1.4977e+01, -5.2563e+01],
        [-1.8653e+01, -1.8384e+01, -2.0645e+01, -2.4331e+01, -4.7290e+01,
         -4.6182e+01, -2.3908e+01, -1.9898e+01,  1.1333e+01, -2.2389e+01],
        [-1.8860e+00, -1.4527e+00, -1.0975e+00, -8.6545e-02,  7.2000e+01,
          5.3950e+01, -5.3263e-01, -1.7044e+00,  6.5084e-01, -1.1368e+00],
        [ 2.0097e+00,  1.9352e+00,  9.9530e-01, -2.1493e+00, -7.3726e+01,
         -5.6131e+01, -1.1088e+00,  1.5793e+00, -1.9470e-01,  1.5487e-01],
        [-2.0301e+01, -2.0459e+01, -2.2484e+01, -2.7774e+01, -5.2612e+01,
         -1.1129e+01, -2.7023e+01, -2.1190e+01,  7.7818e+00, -2.4251e+01],
        [ 3.3177e+00,  3.1529e+00,  2.7179e+00,  6.0095e-01, -7.6818e+01,
         -5.7040e+01,  1.0480e+00,  3.0876e+00, -1.2332e+00,  1.9041e+00],
        [-2.3016e+01, -2.3071e+01, -2.4802e+01, -3.0448e+01, -6.8872e+01,
         -1.6313e+00, -2.9301e+01, -2.3861e+01,  8.6793e+00, -2.6884e+01],
        [-5.0555e+01, -4.9555e+01, -5.0898e+01, -5.3812e+01,  1.7485e+01,
          1.2974e-03, -5.4576e+01, -5.0865e+01,  1.4825e+01, -5.2163e+01],
        [-4.2026e+01, -4.1839e+01, -4.2341e+01, -4.5375e+01,  1.7630e+01,
         -4.4764e+00, -4.5588e+01, -4.2913e+01,  1.2413e+01, -4.3495e+01]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-35.4638, -16.6011, -28.2889,  35.8850, -51.1038, -27.0057, -50.4416,
         -27.3631, -12.8902, -12.0942]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-1.4033,  4.2815],
        [-1.4727,  4.9690],
        [-1.4519,  4.8901],
        [ 7.9690,  3.2326],
        [-1.4753,  4.9741],
        [-9.7684, -1.7053],
        [-1.4670,  4.9651],
        [-1.4674,  4.9632],
        [-1.4619,  4.9553],
        [-1.4063,  4.1378]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -16.3885,  -79.8333,  -27.4717,   -3.9386,  -80.6942,    6.6764,
          -78.1703,  -77.9277,  -76.8224,  -10.2316],
        [ -39.2493,  -77.0288,    3.1748,  -19.5841,  -77.0195,  -42.1738,
          -75.7052,  -76.2068,  -76.4676,  -10.2680],
        [ -37.7821,  -75.3570,    3.7773,  -19.5149,  -75.0677,  -42.6837,
          -74.4290,  -73.7161,  -74.7409,   -9.6430],
        [ -20.6297,  -28.2256,  -33.5463,  -20.1163,  -27.4478,  -29.4991,
          -27.4266,  -27.2916,  -26.9678,  -18.5940],
        [-151.6489, -231.8929, -131.2233,   -8.8814, -232.0980,  -71.5129,
         -230.3429, -230.0918, -230.4803, -121.2032],
        [-103.3566, -186.8875,  -87.9787,   -3.8722, -186.8776,  -95.2780,
         -185.9223, -185.7209, -186.0959,  -75.9332],
        [  -5.6602,    0.8340,   -3.0177,   -8.3151,    0.9784,   -1.7811,
            1.8005,    1.5508,    0.9242,   -4.1083],
        [ -99.7929, -119.0719,  -29.1570,  -29.0839, -118.6089,  -59.8933,
         -117.6062, -117.6926, -118.8609,  -70.1638],
        [ -93.6555, -122.3433,  -31.3690,  -21.2084, -121.8303,  -50.2377,
         -120.0813, -120.4017, -121.0142,  -67.6979],
        [  62.3593,  117.8036,   95.1608,   49.3726,  118.5768,  -25.9360,
          123.9270,  123.5340,  114.3991,   24.3821]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-12.7751,  -6.8756,  -7.3658,  -9.1014,   7.1703,  12.7393,   2.9779,
          -1.9273,  -2.0395,   0.4468],
        [ 12.4728,   6.9701,   7.3270,   8.7757,  -6.9484, -13.0841,  -2.6526,
           2.0847,   2.3324,  -0.5032]], device='cuda:0'))])
xi:  [3.5660832]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 687.6296584038827
W_T_median: 424.9169325243097
W_T_pctile_5: 174.88926957204248
W_T_CVAR_5_pct: 1.9721507596291508
Average q (qsum/M+1):  48.25232721144153
Optimal xi:  [3.5660832]
Expected(across Rb) median(across samples) p_equity:  0.2735772281885147
obj fun:  tensor(-1505.2779, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1080.584932747217
Current xi:  [3.769427]
objective value function right now is: -1080.584932747217
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1453.3667874623573
Current xi:  [2.6317973]
objective value function right now is: -1453.3667874623573
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1481.22537141039
Current xi:  [2.85359]
objective value function right now is: -1481.22537141039
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.8521888]
objective value function right now is: -1424.8278198179034
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.7414594]
objective value function right now is: -1449.272545980684
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.5961854]
objective value function right now is: -1476.9278509799171
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [2.8548617]
objective value function right now is: -1439.5406741685044
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1490.7430451553855
Current xi:  [2.79937]
objective value function right now is: -1490.7430451553855
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.1070163]
objective value function right now is: -1453.0656002064068
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.104332]
objective value function right now is: -1477.1161770298133
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.0765183]
objective value function right now is: -1490.113837691281
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.9883976]
objective value function right now is: -1473.7395905761439
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.7536333]
objective value function right now is: -1402.5407612977467
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1493.11149851798
Current xi:  [2.9283004]
objective value function right now is: -1493.11149851798
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1497.9485638928124
Current xi:  [2.8309007]
objective value function right now is: -1497.9485638928124
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.0010014]
objective value function right now is: -1475.018376669631
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.659718]
objective value function right now is: -1436.6634910996752
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.1095757]
objective value function right now is: -1461.0072119266674
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.8348022]
objective value function right now is: -1468.131060104298
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.9251604]
objective value function right now is: -1468.5118957757234
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1502.3466821248853
Current xi:  [2.903629]
objective value function right now is: -1502.3466821248853
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.8674996]
objective value function right now is: -1488.6119681369657
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.365586]
objective value function right now is: -1500.9608923183387
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.946769]
objective value function right now is: -1470.4160916943597
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.2103038]
objective value function right now is: -1490.0444526751444
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.9295838]
objective value function right now is: -1488.8206802384639
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.899869]
objective value function right now is: -1475.5658934482997
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [2.7119133]
objective value function right now is: -1495.7651638738446
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [3.0235145]
objective value function right now is: -1497.48905143934
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.836737]
objective value function right now is: -1455.9739906826512
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.920452]
objective value function right now is: -1469.0600202036985
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.0189607]
objective value function right now is: -1477.6042640339
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.1989534]
objective value function right now is: -1494.6977410353754
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1503.3697711011255
Current xi:  [2.9026277]
objective value function right now is: -1503.3697711011255
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.2024388]
objective value function right now is: -1498.3074320692128
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1503.8395499384883
Current xi:  [3.429199]
objective value function right now is: -1503.8395499384883
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.8832784]
objective value function right now is: -1487.5202418509925
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.6391962]
objective value function right now is: -1470.515670605443
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.8048491]
objective value function right now is: -1485.8035374924598
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.0893095]
objective value function right now is: -1482.0328090682258
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.042574]
objective value function right now is: -1489.0925796127838
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.244436]
objective value function right now is: -1493.9016758221985
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.6890028]
objective value function right now is: -1498.5300606221665
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1506.5665142152598
Current xi:  [2.8881948]
objective value function right now is: -1506.5665142152598
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.853709]
objective value function right now is: -1505.5224922402592
new min fval from sgd:  -1509.0999546303506
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.9354284]
objective value function right now is: -1483.2086079759258
new min fval from sgd:  -1509.7359768515753
new min fval from sgd:  -1510.1919293255496
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.91756]
objective value function right now is: -1484.3599766932457
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.1088662]
objective value function right now is: -1475.9956739085433
new min fval from sgd:  -1510.4228632698027
new min fval from sgd:  -1510.7504903241013
new min fval from sgd:  -1510.7951370586165
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.8711803]
objective value function right now is: -1490.1655671939916
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.0335083]
objective value function right now is: -1496.306526905027
min fval:  -1510.7951370586165
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[   6.1448,   -2.9790],
        [  -7.8102,    3.7930],
        [  -4.7414,    5.4131],
        [  -6.0836,    2.6057],
        [ -19.6214,   -4.3023],
        [-127.4525,   -2.5330],
        [  -6.9391,    3.2298],
        [  -6.2440,    2.6537],
        [ -23.7829,    5.8339],
        [   9.5831,    4.7911]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[   4.5669,  -27.8103,  -58.9929,  -20.7852,   -0.2814,    1.2219,
          -21.7115,  -15.4412,  -23.8253,  -95.3819],
        [   6.0183,  -27.7111,  -58.7952,  -20.4007,    1.3770,    1.1394,
          -21.2146,  -15.5613,  -24.1431,  -93.1217],
        [   6.2844,  -27.6026,  -58.6870,  -20.5985,    1.3903,    1.1320,
          -21.6326,  -15.0105,  -24.2960,  -93.5505],
        [   5.0817,  -27.4036,  -58.1247,  -20.4185,   -0.1944,    1.1653,
          -21.5140,  -14.8210,  -24.0611,  -96.5066],
        [   6.1160,  -27.6662,  -58.6676,  -20.8536,   -0.3639,    1.2572,
          -21.6798,  -15.2905,  -23.9073,  -95.0351],
        [   7.0148,  -28.7732,   12.5608,  -98.7485,    7.4891,   12.9877,
          -43.7681, -104.3649,  -83.3603,   -5.6218],
        [   7.2994,  -27.8967,  -59.6386,  -20.9528,    0.5325,    1.2688,
          -22.2868,  -15.5334,  -24.2380,  -92.5650],
        [   5.2209,  -27.6658,  -58.6444,  -20.2745,    0.1126,    1.1607,
          -21.4199,  -14.9984,  -23.8295,  -95.6803],
        [ -10.9569,   28.8752,   63.7406,   22.2636,   -0.1623,   -2.1341,
           24.5029,   15.9640,   22.6689,   70.1818],
        [   5.0444,  -27.4541,  -58.2500,  -20.4282,    0.2732,    1.1297,
          -21.1210,  -14.8010,  -23.8845,  -96.0109]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-14.1547, -16.0508, -17.3913, -15.8697, -16.9582, -66.5150, -20.0033,
         -15.6731,  26.7215, -15.6846]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.7465,   4.0879],
        [ -1.7195,   4.0934],
        [ -1.7827,   4.0820],
        [ -1.7168,   4.0954],
        [-19.9683,  -6.0063],
        [ -1.7069,   4.0979],
        [-14.9991,  -2.3858],
        [-14.9265,  -2.3147],
        [-28.1205,  -7.4456],
        [  8.5159,   4.6798]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-127.5539, -118.4186, -138.7870, -124.7885,  -41.5366, -122.1761,
         -203.1409, -122.6254,  -55.9376,  -44.6447],
        [ -46.6299,  -42.8595,  -51.9248,  -45.2503,    3.8067,  -44.2234,
          -14.6917,   -7.4115,    6.6079,   -3.9157],
        [   0.6104,   -0.6884,    0.8412,   -0.8344,  -28.7397,   -0.7161,
           16.6365,  -23.7643,  -50.7455,  -41.5338],
        [ -47.5957,  -46.2617,  -48.4989,  -47.0080,  -56.1644,  -46.2201,
          -54.1310,  -76.6825,  -76.4422,  -80.6870],
        [ -46.9459,  -48.0032,  -44.7859,  -49.7453,  -70.5548,  -49.5671,
          -56.1094,  -81.5266,  -85.4393,  -85.1031],
        [-214.2633, -208.4860, -222.3135, -212.9985,    4.2203, -211.3814,
         -126.6291,  -80.2056,   -2.1558,  -31.4920],
        [ 173.6727,  165.2846,  184.6967,  177.6906,  -16.8924,  176.1725,
           -3.8527,   -2.9703,   -7.0531,   64.5661],
        [ -16.4470,  -17.1508,  -15.9675,  -18.1805,  -15.1941,  -17.9659,
           41.1087,   -7.0653,  -31.5257,  -19.0348],
        [-137.1596, -127.5387, -150.1692, -133.9494,   -3.3279, -131.3526,
         -213.0031, -122.1781,  -79.9249,   -4.5339],
        [ -40.6534,  -39.6022,  -39.6709,  -43.2609,  -41.2236,  -42.7928,
          -41.4288,  -64.2760,  -60.9738,  -75.1114]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 2.6622, -7.7706,  0.4961,  0.1156,  1.1559,  4.9145,  0.5564, -4.4500,
          8.6181, -1.0858],
        [-2.4973,  7.7028, -0.2831, -0.2418, -1.5585, -5.2736, -0.3991,  3.9974,
         -8.6137,  0.9241]], device='cuda:0'))])
xi:  [2.9923956]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 629.3584912556844
W_T_median: 371.70858868515234
W_T_pctile_5: 149.6223925725588
W_T_CVAR_5_pct: -4.356110699035772
Average q (qsum/M+1):  49.15691154233871
Optimal xi:  [2.9923956]
Expected(across Rb) median(across samples) p_equity:  0.27778334518273673
obj fun:  tensor(-1510.7951, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1068.5922719191108
Current xi:  [3.8559542]
objective value function right now is: -1068.5922719191108
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1440.8763745231088
Current xi:  [2.0804658]
objective value function right now is: -1440.8763745231088
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1497.481497298968
Current xi:  [2.0764425]
objective value function right now is: -1497.481497298968
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1503.6656881741546
Current xi:  [2.288195]
objective value function right now is: -1503.6656881741546
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1510.7672358467105
Current xi:  [2.1786501]
objective value function right now is: -1510.7672358467105
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.3085582]
objective value function right now is: -1460.4958823752634
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1513.6328387940032
Current xi:  [2.1773002]
objective value function right now is: -1513.6328387940032
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.143812]
objective value function right now is: -1500.04033217574
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.2015932]
objective value function right now is: -1468.8864471339734
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.3562567]
objective value function right now is: -1495.631216129412
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1515.3976466828599
Current xi:  [2.0924683]
objective value function right now is: -1515.3976466828599
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.455443]
objective value function right now is: -1503.5212635972146
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.8519636]
objective value function right now is: -1450.3424183347372
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [2.2847424]
objective value function right now is: -1510.231806644078
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.2291489]
objective value function right now is: -1515.2623227987187
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.632807]
objective value function right now is: -1408.822818625452
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.0774643]
objective value function right now is: -1511.6979922653159
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.8110011]
objective value function right now is: -1500.5258942756732
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.313634]
objective value function right now is: -1498.620726136464
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.1499636]
objective value function right now is: -1507.012606598419
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.3269215]
objective value function right now is: -1506.5521873609061
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.1254113]
objective value function right now is: -1495.161339536306
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.1814842]
objective value function right now is: -1501.2793309863532
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.9055248]
objective value function right now is: -1479.2725597724807
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.082975]
objective value function right now is: -1497.9123417920061
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.1031454]
objective value function right now is: -1476.9411533741322
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.0499532]
objective value function right now is: -1512.8595907368247
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1517.7771379817357
Current xi:  [2.1773138]
objective value function right now is: -1517.7771379817357
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1518.0506024433828
Current xi:  [2.2539928]
objective value function right now is: -1518.0506024433828
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.8825808]
objective value function right now is: -1494.9399508175172
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.340097]
objective value function right now is: -1511.921190970602
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.1376996]
objective value function right now is: -1509.0948826424778
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.0735674]
objective value function right now is: -1484.8660958315572
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.1384168]
objective value function right now is: -1493.9363715273541
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.3014483]
objective value function right now is: -1506.194928990027
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.0307755]
objective value function right now is: -1493.488495055288
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.8819482]
objective value function right now is: -1511.9170369006038
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1519.459570082696
Current xi:  [2.1216702]
objective value function right now is: -1519.459570082696
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.1424413]
objective value function right now is: -1498.5222745134993
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.1257408]
objective value function right now is: -1511.7398037204794
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.0996158]
objective value function right now is: -1509.8171573163338
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.4948626]
objective value function right now is: -1514.7660280828027
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.1871514]
objective value function right now is: -1506.956051108846
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.114805]
objective value function right now is: -1504.9706316313352
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.1751003]
objective value function right now is: -1501.6736212849146
new min fval from sgd:  -1520.8379055950656
new min fval from sgd:  -1521.2223680028762
new min fval from sgd:  -1522.4121804150034
new min fval from sgd:  -1522.536591115867
new min fval from sgd:  -1522.5722020183186
new min fval from sgd:  -1522.5901041865209
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.1479368]
objective value function right now is: -1498.2830574657778
new min fval from sgd:  -1523.64822665391
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.195533]
objective value function right now is: -1513.35574963562
new min fval from sgd:  -1523.7513306279745
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.321941]
objective value function right now is: -1503.0540923012904
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.9353427]
objective value function right now is: -1510.906410909783
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.1170363]
objective value function right now is: -1509.4248995195755
min fval:  -1523.7513306279745
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  15.2308,  -30.0806],
        [   6.9996,  -21.6303],
        [  -0.3932,  -12.0191],
        [ -11.2979,   -3.4562],
        [-248.2136,   -2.4856],
        [  -4.1760,    2.8368],
        [ -35.1013,   -3.0962],
        [   3.9286,   -2.6670],
        [  -4.1764,    2.8378],
        [   3.8920,   -2.6455]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  -6.1873,   -5.7051,   -3.8449,    6.8519,    2.0509,  -72.8209,
            3.7105,    7.5232,  -69.0572,    6.7531],
        [   4.4877,    4.6716,    4.5814,   -7.1765,   -2.1704,   67.2137,
           -4.5111,  -10.1847,   63.1304,   -8.9128],
        [  -3.7150,   -4.8316,   -4.5218,    6.8422,    1.9330,  -75.9500,
            3.9777,    8.0354,  -71.8295,    7.1785],
        [ -20.5325,  -17.3932,  -10.6856,    7.4052,    0.3842,  -60.1909,
            1.9039,   -0.2113,  -56.2246,   -0.6113],
        [  -3.6360,   -4.3651,   -4.2738,    6.7213,    1.9547,  -75.8304,
            4.2188,    7.0463,  -72.3465,    6.2880],
        [  -5.5829,   -5.6519,   -3.9559,    6.9422,    2.0287,  -73.0269,
            3.7543,    7.7395,  -69.9782,    6.4880],
        [   4.7348,    4.7158,    3.7897,   -7.3762,   -2.1054,   70.7196,
           -4.5764,   -8.8558,   67.1777,   -8.2320],
        [   4.4404,    4.6311,    4.0327,   -7.4868,   -2.1169,   71.2266,
           -4.6260,   -9.1030,   67.6773,   -7.5231],
        [  -6.4459,   -5.8342,   -3.5655,    6.6821,    2.0992,  -72.9079,
            3.6248,    6.3089,  -69.4117,    5.6079],
        [ -56.8588, -120.0394, -126.4563, -172.7599,   -0.6496,  -38.5641,
            1.4931,    7.0909,  -34.7143,    6.4267]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-14.6421,  13.1717, -24.1967,  -5.5757, -14.7361, -14.4927,  17.2093,
          18.9660, -12.8885, -36.9668]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.0578,   4.2636],
        [  8.4616,   2.7608],
        [ -1.0109,   4.2738],
        [ -0.8516,   4.3365],
        [-10.5770,  -1.5507],
        [ -8.8409,  -1.3061],
        [ -0.9934,   4.2781],
        [ -1.0126,   4.2711],
        [ -8.4570,  -2.5121],
        [ -0.9937,   4.2760]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.4223e+01, -5.3476e+00, -1.2358e+01, -2.2387e+00, -8.4848e+01,
         -4.5081e+01, -1.1538e+01, -1.2914e+01, -2.3291e+01, -1.1975e+01],
        [-1.3850e+02, -1.6560e+01, -1.3754e+02, -1.2947e+02, -7.9971e+01,
         -3.3062e+01, -1.3649e+02, -1.3757e+02, -1.2459e+01, -1.3701e+02],
        [-1.1937e+02, -2.8922e+01, -1.1885e+02, -1.1160e+02, -6.9119e+01,
         -3.1413e+01, -1.1836e+02, -1.1932e+02, -1.2825e+01, -1.1869e+02],
        [-1.0136e+02, -5.0872e+00, -1.0316e+02, -9.9601e+01,  4.3257e+00,
          4.8954e+00, -1.0343e+02, -1.0373e+02,  7.2196e+00, -1.0420e+02],
        [-1.4313e+02, -1.1845e+01, -1.4229e+02, -1.3430e+02, -9.2645e+01,
         -3.6203e+01, -1.4145e+02, -1.4247e+02, -8.0227e+00, -1.4293e+02],
        [-1.2582e+02, -3.5614e+00, -1.2559e+02, -1.1751e+02, -1.0976e+02,
         -3.6666e+01, -1.2500e+02, -1.2576e+02,  2.6384e+00, -1.2542e+02],
        [ 8.1373e+01, -9.6651e-02,  7.8694e+01,  6.1134e+01,  1.6961e+01,
          2.6249e+00,  7.7355e+01,  7.9599e+01, -2.0753e+01,  7.8226e+01],
        [-1.4371e+02, -6.4502e+00, -1.4295e+02, -1.3474e+02, -1.0554e+02,
         -3.7667e+01, -1.4262e+02, -1.4353e+02,  3.7499e+00, -1.4346e+02],
        [-7.9091e+01, -2.4228e+01, -7.8485e+01, -7.1840e+01, -6.0727e+01,
         -3.3239e+01, -7.8003e+01, -7.8587e+01, -1.9531e+01, -7.8382e+01],
        [-1.4846e+02, -1.0876e+01, -1.4790e+02, -1.3931e+02, -9.4778e+01,
         -3.5723e+01, -1.4737e+02, -1.4875e+02, -6.7538e+00, -1.4915e+02]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  8.6040,   2.4623,  -2.2907, -15.9075,   4.0100,  13.2309,   0.4716,
           9.2121,  -2.1560,   4.6303],
        [ -8.8528,  -2.2602,   2.0521,  15.8363,  -4.1629, -13.7695,  -0.3561,
          -8.9003,   2.0467,  -4.5024]], device='cuda:0'))])
xi:  [2.1164236]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 617.0467989802572
W_T_median: 312.3407326185161
W_T_pctile_5: 108.3007805847419
W_T_CVAR_5_pct: -27.151231365202484
Average q (qsum/M+1):  50.47156549269153
Optimal xi:  [2.1164236]
Expected(across Rb) median(across samples) p_equity:  0.3041424036026001
obj fun:  tensor(-1523.7513, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1498.6694752192261
Current xi:  [0.9450704]
objective value function right now is: -1498.6694752192261
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1504.2491655779654
Current xi:  [0.6895543]
objective value function right now is: -1504.2491655779654
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1535.2345408238
Current xi:  [1.027998]
objective value function right now is: -1535.2345408238
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1541.18482743087
Current xi:  [0.99135524]
objective value function right now is: -1541.18482743087
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.8810369]
objective value function right now is: -1538.0845351193311
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.86854136]
objective value function right now is: -1536.964780727161
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [1.0859209]
objective value function right now is: -1538.6686603069413
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.3197523]
objective value function right now is: -1512.3877498242834
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1553.1033507403736
Current xi:  [1.4475197]
objective value function right now is: -1553.1033507403736
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.1248226]
objective value function right now is: -1533.3714331863955
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.339611]
objective value function right now is: -1546.9864494538124
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.441187]
objective value function right now is: -1528.800538867795
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1557.8158190005902
Current xi:  [1.2414501]
objective value function right now is: -1557.8158190005902
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [1.2442635]
objective value function right now is: -1545.0425250999099
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.3841752]
objective value function right now is: -1536.684548926063
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.2663015]
objective value function right now is: -1548.5685120245987
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.3989073]
objective value function right now is: -1555.9292015854398
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.1944885]
objective value function right now is: -1543.2916781936199
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.324002]
objective value function right now is: -1555.2959057454561
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1559.2895676515068
Current xi:  [1.3995249]
objective value function right now is: -1559.2895676515068
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.2909815]
objective value function right now is: -1538.7750377545715
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.5138342]
objective value function right now is: -1550.8430876384662
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.1765175]
objective value function right now is: -1550.7160666314871
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.1671119]
objective value function right now is: -1333.378144169015
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0772211]
objective value function right now is: -1521.1369481744193
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.12869814]
objective value function right now is: -1524.4235178389856
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.2569695]
objective value function right now is: -1527.0808735892538
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [0.29673287]
objective value function right now is: -1530.4853047043039
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [0.07812551]
objective value function right now is: -1510.2648374069706
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.10322002]
objective value function right now is: -1521.1606697429734
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.13886079]
objective value function right now is: -1528.9741681706612
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.20289089]
objective value function right now is: -1523.5622562157548
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.18653117]
objective value function right now is: -1525.311802052936
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.13429882]
objective value function right now is: -1528.2923841703077
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.30195594]
objective value function right now is: -1529.8122441700018
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.05126673]
objective value function right now is: -1530.2195588375105
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.11753761]
objective value function right now is: -1524.8127167461864
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.26484233]
objective value function right now is: -1524.9953083187356
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.23020935]
objective value function right now is: -1528.4147518147724
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.12443481]
objective value function right now is: -1520.2577099899304
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.23741639]
objective value function right now is: -1533.1916235599067
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.07823473]
objective value function right now is: -1526.2698508895578
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02529899]
objective value function right now is: -1515.048963634356
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.21434401]
objective value function right now is: -1535.0966491870442
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.1730057]
objective value function right now is: -1525.242303195389
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.14192916]
objective value function right now is: -1533.0409726103253
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.16230829]
objective value function right now is: -1531.0615357134309
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.23368993]
objective value function right now is: -1534.066772287275
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.1978513]
objective value function right now is: -1536.0954618822502
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.8267986]
objective value function right now is: -1542.1290210757138
min fval:  -1531.2580913591032
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -10.4889,   -2.5420],
        [  -4.4342,    3.6520],
        [ -30.0705,   -2.0289],
        [  -4.0381,    3.7683],
        [  -4.7870,    3.7325],
        [  -3.6898,    0.9729],
        [  -0.8773,    4.1586],
        [ -16.8634,    5.1935],
        [-142.2243,   -0.3197],
        [  -4.3549,    3.6784]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 2.8572e+00, -3.5698e+01,  1.9351e+00, -3.0849e+01, -1.7999e+01,
          3.0964e-01, -2.2342e+01, -6.5481e+00,  1.3148e+00, -3.4239e+01],
        [-1.1199e+02, -1.5091e+01, -8.4369e-02, -6.4340e+00, -1.8196e+00,
         -2.9561e+01,  2.4679e+00, -4.6989e+01, -8.3792e-01, -1.2574e+01],
        [ 2.6850e+00, -3.5152e+01,  1.7898e+00, -2.9108e+01, -1.8629e+01,
         -7.1234e-01, -2.2571e+01, -7.2154e+00,  1.1591e+00, -3.2655e+01],
        [-4.7652e+01, -2.2029e+01,  3.0613e+01, -1.2696e+01, -9.7393e+00,
         -2.7757e+01,  6.0152e-01, -2.5548e+01,  3.8690e-01, -1.8919e+01],
        [ 2.8327e+00, -3.5324e+01,  1.7413e+00, -2.9646e+01, -1.8101e+01,
         -1.6748e-01, -2.3104e+01, -6.7106e+00,  1.2204e+00, -3.3465e+01],
        [ 2.9365e+00, -3.5821e+01,  2.0633e+00, -3.0690e+01, -1.7892e+01,
          3.7030e-01, -2.2310e+01, -6.7430e+00,  1.4698e+00, -3.4172e+01],
        [ 2.9750e+00, -3.5165e+01,  1.9032e+00, -3.0345e+01, -1.7811e+01,
          4.0555e-02, -2.3050e+01, -6.7222e+00,  1.2442e+00, -3.3808e+01],
        [ 2.8169e+00, -3.5269e+01,  1.7311e+00, -2.9947e+01, -1.8112e+01,
         -2.2616e-01, -2.2619e+01, -7.1184e+00,  1.2366e+00, -3.3329e+01],
        [ 1.9903e+02,  6.6741e+00,  2.5670e+00, -8.8703e-01,  1.4910e+01,
          5.7674e+01, -6.8948e+00,  3.6199e+01, -3.8019e-01,  4.0420e+00],
        [ 2.6915e+00, -3.5217e+01,  1.7655e+00, -2.9478e+01, -1.8315e+01,
         -6.4232e-01, -2.2709e+01, -6.7766e+00,  1.1732e+00, -3.2880e+01]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-18.8957, -46.8222, -17.2471, -39.3327, -17.7200, -18.3457, -18.5930,
         -17.7936,  22.2805, -16.8748]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -4.4721,   2.4115],
        [ -4.5651,   2.3557],
        [ -9.4207,  -2.4985],
        [-27.6734,  -2.6890],
        [ -4.5229,   2.4316],
        [ -3.9003,   2.6708],
        [ -2.1800,   3.2054],
        [ -9.6742,  -2.4657],
        [  9.8827,   2.6490],
        [ -3.6436,   2.7766]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -25.4811,  -25.7831,   -6.2146,  -22.9503,  -14.3656,  -25.9122,
          -24.7402,  -16.1552,   -3.7186,  -24.5906],
        [  -6.2038,   -7.1434,   -5.8631,  -89.8118,   -9.6872,   -4.9141,
            4.8975,   -6.6533,   -7.9959,   -2.1654],
        [   0.8293,    0.1562,   -4.0239, -112.6725,   -3.3632,   -0.6904,
            0.6614,   -5.2608,   -5.5197,   -0.3437],
        [  24.7171,   30.4060,    2.9660,   -8.7722,   23.3235,    7.5757,
           -8.9188,    2.4386,   -2.3045,    3.9571],
        [ -22.1193,  -21.6284,   -9.8987,  -57.0614,  -16.1628,  -21.6824,
          -19.1063,  -16.8696,  -10.6310,  -21.4463],
        [ -53.6436,  -52.0471,   10.6640,   13.1549,  -48.6996,  -58.4767,
          -67.9008,    7.8671,   -3.9740,  -60.8165],
        [ -22.3710,  -21.7064,   -9.0925,  -56.2296,  -11.6112,  -21.2021,
          -19.2086,  -16.8231,  -10.1107,  -20.7285],
        [ -20.7959,  -22.0406,  -13.2842,  -27.4025,  -25.3498,  -19.3455,
          -15.8483,  -13.0473,  -12.2920,  -18.9269],
        [  -0.4182,    0.2022,   -2.9956, -113.8741,   -0.9754,   -1.1709,
            1.1193,   -4.2568,   -3.9000,   -1.3996],
        [ -52.2674,  -52.1023,    5.3135,   14.2058,  -49.5988,  -55.4298,
          -63.7352,    8.6052,  -11.9366,  -57.0391]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  9.8692,   8.5816,  10.5724,   0.6008,   6.4023,  -8.8920,   5.9461,
           4.3097,  10.7870,  28.5338],
        [ -9.9393,  -8.1650, -10.5036,  -0.1712,  -6.4997,   9.3110,  -5.9256,
          -4.2582, -10.4841, -28.5191]], device='cuda:0'))])
xi:  [0.1730057]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 539.9662692029086
W_T_median: 268.3278310119805
W_T_pctile_5: 76.71516601654056
W_T_CVAR_5_pct: -32.7050557011557
Average q (qsum/M+1):  51.39842962449597
Optimal xi:  [0.1730057]
Expected(across Rb) median(across samples) p_equity:  0.3149863650401433
obj fun:  tensor(-1531.2581, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1346.9923396297638
Current xi:  [-0.7785677]
objective value function right now is: -1346.9923396297638
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1594.2172563445283
Current xi:  [-3.5430639]
objective value function right now is: -1594.2172563445283
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.153079]
objective value function right now is: -1554.6310476363963
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.067909]
objective value function right now is: -1583.444853621826
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.044692]
objective value function right now is: -1585.3261335037123
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.9502897]
objective value function right now is: -1577.148436257652
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-3.8621204]
objective value function right now is: -1587.1310544202588
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.9497995]
objective value function right now is: -1582.8712135696685
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.0375605]
objective value function right now is: -1535.5801998075815
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.875728]
objective value function right now is: -1586.6818242470895
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.0522265]
objective value function right now is: -1578.4552035718552
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.07233]
objective value function right now is: -1580.3227308826085
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.027077]
objective value function right now is: -1584.3465057983053
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-3.9729242]
objective value function right now is: -1563.291975334532
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.8268335]
objective value function right now is: -1585.4524734189986
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.4891918]
objective value function right now is: -1579.8007755462315
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.9782703]
objective value function right now is: -1584.099485561912
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.0864925]
objective value function right now is: -1585.2574731109032
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.0405526]
objective value function right now is: -1585.8550574241654
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.0339804]
objective value function right now is: -1582.8649886927533
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.6386623]
objective value function right now is: -1524.4822653390113
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.3027544]
objective value function right now is: -1577.740140298951
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.456502]
objective value function right now is: -1584.5434539124644
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.855674]
objective value function right now is: -1580.693732417823
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.8111243]
objective value function right now is: -1583.3586346241214
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1595.7770575758866
Current xi:  [-3.375091]
objective value function right now is: -1595.7770575758866
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1597.0446572695855
Current xi:  [-3.5588498]
objective value function right now is: -1597.0446572695855
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-3.4606147]
objective value function right now is: -1588.774346356885
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1598.9995787725202
Current xi:  [-3.5531187]
objective value function right now is: -1598.9995787725202
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.6212924]
objective value function right now is: -1597.5076571754837
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.5549636]
objective value function right now is: -1595.9946573031134
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.6470208]
objective value function right now is: -1592.8545666819907
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1599.9487196591729
Current xi:  [-3.5841172]
objective value function right now is: -1599.9487196591729
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.6754863]
objective value function right now is: -1595.6923143781885
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.576368]
objective value function right now is: -1597.1634372451585
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.708071]
objective value function right now is: -1592.1411965324785
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.712064]
objective value function right now is: -1590.1732341295478
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1600.0665342658922
Current xi:  [-3.7102869]
objective value function right now is: -1600.0665342658922
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.6654773]
objective value function right now is: -1591.5657115696106
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.7130427]
objective value function right now is: -1589.8748039436268
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.607477]
objective value function right now is: -1599.6926500390205
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.8163233]
objective value function right now is: -1593.1894478241802
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.7786744]
objective value function right now is: -1594.3859149615546
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.6987805]
objective value function right now is: -1595.3993160063
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.809327]
objective value function right now is: -1599.6574108174198
new min fval from sgd:  -1600.9052843017128
new min fval from sgd:  -1601.2157794309467
new min fval from sgd:  -1601.3634766295113
new min fval from sgd:  -1601.533182365866
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.741985]
objective value function right now is: -1599.1007070468552
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.7813783]
objective value function right now is: -1599.8751959994872
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.7450588]
objective value function right now is: -1590.954674617235
new min fval from sgd:  -1601.6399257444987
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.6305487]
objective value function right now is: -1600.1084729077293
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.8264697]
objective value function right now is: -1597.6053144664397
min fval:  -1601.6399257444987
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-9.3032e-02,  3.4293e+00],
        [ 7.0723e-02, -2.2499e+00],
        [ 3.9564e-02, -1.3314e+00],
        [-9.4580e-02,  4.0778e+00],
        [ 6.9093e-02, -2.1723e+00],
        [-9.1389e-02,  3.3383e+00],
        [-5.2046e+01, -1.4871e-01],
        [-9.1310e-02,  3.3343e+00],
        [-9.1317e-02,  3.3346e+00],
        [ 7.6682e-02, -2.5568e+00]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -3.7223,  -4.9014,  -5.6297,  -5.2527,  -5.8041,  -3.6141,  -4.6247,
          -8.1484,  -6.8945,  -5.2030],
        [ -2.6005,  -6.7614,  -6.7360,  -5.9222,  -6.8805,  19.7254,  -2.8644,
          17.6747,  18.6335,  -7.2962],
        [ -3.6420,  -5.3441,  -5.6626,  -5.3374,  -5.5119,  -3.4544,  -4.7677,
          -5.4026,  -4.4843,  -5.2343],
        [ -3.7061,  -5.2274,  -5.7292,  -5.1069,  -5.3148,  -3.5227,  -4.8242,
          -6.1841,  -4.9370,  -5.4340],
        [ -3.6641,  -5.0457,  -5.8857,  -5.6437,  -5.6777,  -2.5811,  -4.9116,
          -5.0042,  -4.0915,  -5.2040],
        [-17.6738,   2.0516,   1.1138,  -6.9282,   1.9228, -59.4372,   1.7130,
         -66.9138, -66.1826,   2.8140],
        [ -3.2847,  -5.0108,  -5.6892,  -5.1706,  -5.5522,  -4.0252,  -4.6790,
          -6.3332,  -5.1471,  -5.4338],
        [ -3.8990,  -4.9746,  -5.9742,  -5.6221,  -5.5911,  -3.8486,  -4.5604,
          -6.8356,  -5.7738,  -5.0852],
        [ -5.0949,  -4.6279,  -5.0418,  -5.1259,  -4.5336, -28.0553,  -2.0285,
         -33.4551, -32.5841,  -3.9428],
        [  3.9808,   4.7400,   4.7229,   4.0733,   4.6069,   5.5524,   4.2206,
           4.1827,   4.7250,   4.6137]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -1.4585,   5.4257,  -1.3697,  -1.4097,  -1.3480, -47.9281,  -1.3992,
          -1.4086,  -2.9600,  20.5809]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  14.3956,    9.2764],
        [  -6.3809,    0.7460],
        [ -63.8463,  -11.8183],
        [-118.0165,  -27.4579],
        [  -9.1212,    2.5632],
        [  18.2704,    2.0171],
        [  76.4816,   14.1609],
        [ -26.2389,   -9.0790],
        [  -2.6852,   -4.1749],
        [  -6.3925,    0.7570]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.3076e+01, -6.6993e+01,  1.5641e+00,  3.5209e+00, -4.1439e+01,
         -6.7173e+00, -5.0623e+00,  8.8445e+00, -4.7921e+01, -6.7056e+01],
        [-2.5664e+01, -4.2320e+01, -2.6626e+01,  8.0237e+00, -2.3141e+01,
         -4.6744e+00, -7.1038e+00,  4.7603e-02, -7.0649e+01, -4.2685e+01],
        [-6.6690e+01, -8.0523e+01, -1.6040e+00, -1.0783e+01, -6.6724e+01,
         -4.1812e+01, -3.3086e+01, -5.8546e+00, -2.2218e+01, -8.0227e+01],
        [-1.4846e+00, -5.4119e+01, -2.0373e+02, -3.4964e+01, -5.2223e+01,
         -3.3198e+00, -4.1446e+00, -2.4359e+01,  5.1750e+00, -5.6185e+01],
        [-8.4802e+01, -1.1666e+02, -4.6775e+01,  1.9394e+00, -8.3858e+01,
         -2.3294e+01, -4.4527e+00, -1.1087e+00, -9.5338e+01, -1.1681e+02],
        [-8.6156e+01, -1.2510e+02, -4.9610e+01,  4.5628e+00, -8.4953e+01,
         -2.5912e+01, -2.5129e+00,  2.7125e+00, -1.0050e+02, -1.2517e+02],
        [-8.5855e+01, -1.2264e+02, -4.7374e+01,  3.9526e+00, -8.4486e+01,
         -2.5253e+01, -3.3086e+00,  7.6752e-01, -9.9591e+01, -1.2321e+02],
        [-3.9221e+01, -4.2265e+01,  5.7136e+00,  2.6496e+01, -4.4323e+01,
         -9.1286e+01, -6.7470e+01,  1.6412e+01, -6.0909e+01, -4.3917e+01],
        [ 5.2716e+00, -2.2283e+01, -2.5121e+00, -3.5847e+00, -1.0543e-01,
         -1.9067e+01, -1.8265e+01,  3.4978e-01, -1.3166e+01, -2.3273e+01],
        [ 3.0670e+00,  5.9043e+01,  2.1584e+01, -1.4610e+01, -7.0610e-01,
          1.4775e-01, -5.7937e+00, -8.5603e+00, -1.2842e+00,  5.5905e+01]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -1.1473, -13.7029,  -0.7577,   5.1177,  10.7195,  11.7002,  12.0221,
          32.0062,   3.9168,   3.5709],
        [  1.2807,  13.2805,   0.4761,  -5.3992, -10.9473, -11.6961, -12.4649,
         -31.7701,  -4.0776,  -3.6809]], device='cuda:0'))])
xi:  [-3.702694]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 91.81705210844592
W_T_median: -23.932064110224125
W_T_pctile_5: -184.75706073011187
W_T_CVAR_5_pct: -242.26939019410466
Average q (qsum/M+1):  55.574931483114916
Optimal xi:  [-3.702694]
Expected(across Rb) median(across samples) p_equity:  0.29562655781337527
obj fun:  tensor(-1601.6399, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1694.424471652194
Current xi:  [-8.869566]
objective value function right now is: -1694.424471652194
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1696.429985646583
Current xi:  [-8.684628]
objective value function right now is: -1696.429985646583
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1697.7616623887325
Current xi:  [-8.802579]
objective value function right now is: -1697.7616623887325
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1698.4160964342416
Current xi:  [-8.700548]
objective value function right now is: -1698.4160964342416
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.682017]
objective value function right now is: -1697.7833661462096
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1699.0083716489291
Current xi:  [-8.714337]
objective value function right now is: -1699.0083716489291
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-8.705969]
objective value function right now is: -1697.284588682698
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.367242]
objective value function right now is: -1698.5824033236647
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.647166]
objective value function right now is: -1697.8694827721226
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.51278]
objective value function right now is: -1698.973954666413
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1699.115900855918
Current xi:  [-8.70171]
objective value function right now is: -1699.115900855918
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.531272]
objective value function right now is: -1698.6030218123697
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1699.279658467437
Current xi:  [-8.461876]
objective value function right now is: -1699.279658467437
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-8.569698]
objective value function right now is: -1699.0442075580017
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.6702585]
objective value function right now is: -1695.7281125205434
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1699.5546786734328
Current xi:  [-8.759204]
objective value function right now is: -1699.5546786734328
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.4369545]
objective value function right now is: -1699.1680425406503
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.61554]
objective value function right now is: -1696.7888360462714
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.356378]
objective value function right now is: -1692.4997172880278
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.744536]
objective value function right now is: -1697.2678123505182
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.4320135]
objective value function right now is: -1691.0328891026888
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.784134]
objective value function right now is: -1696.2521002974067
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.16757]
objective value function right now is: -1698.7346707953545
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.452095]
objective value function right now is: -1698.72637148016
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.625037]
objective value function right now is: -1699.2018666793958
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.383919]
objective value function right now is: -1698.7133985913692
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.939585]
objective value function right now is: -1697.9898229741984
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-8.794065]
objective value function right now is: -1699.5219998929826
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-8.840437]
objective value function right now is: -1699.281182994401
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.762189]
objective value function right now is: -1698.2961517448696
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.547126]
objective value function right now is: -1699.2639964999023
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.653878]
objective value function right now is: -1699.2610571815894
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.70875]
objective value function right now is: -1698.4753223389455
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1699.686014168405
Current xi:  [-8.852646]
objective value function right now is: -1699.686014168405
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.669572]
objective value function right now is: -1698.436214985568
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1699.8009120305655
Current xi:  [-8.597555]
objective value function right now is: -1699.8009120305655
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.82013]
objective value function right now is: -1699.536139859053
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.116731]
objective value function right now is: -1697.530764504657
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.295368]
objective value function right now is: -1698.1233659953682
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.856231]
objective value function right now is: -1695.6955300601787
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.9912386]
objective value function right now is: -1699.639773405901
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1700.0051220960859
Current xi:  [-8.647172]
objective value function right now is: -1700.0051220960859
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.434522]
objective value function right now is: -1693.4894362870157
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.7366295]
objective value function right now is: -1697.8538777434912
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.41685]
objective value function right now is: -1699.292179489005
new min fval from sgd:  -1700.009383550784
new min fval from sgd:  -1700.1172336933516
new min fval from sgd:  -1700.3571762767783
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.222166]
objective value function right now is: -1697.1936417398747
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.9902935]
objective value function right now is: -1698.508011720009
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.368134]
objective value function right now is: -1700.0620632352816
new min fval from sgd:  -1700.4276303433594
new min fval from sgd:  -1700.4442898868858
new min fval from sgd:  -1700.488623299255
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.643623]
objective value function right now is: -1699.407136833944
new min fval from sgd:  -1700.546194645749
new min fval from sgd:  -1700.554587049627
new min fval from sgd:  -1700.7605770002122
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.320439]
objective value function right now is: -1693.6902017614996
min fval:  -1700.7605770002122
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.7609,  3.2724],
        [-0.5392, -2.6126],
        [ 0.7610,  3.2724],
        [19.9366, 11.9868],
        [ 0.7611,  3.2723],
        [ 0.7611,  3.2720],
        [ 0.7609,  3.2720],
        [-0.5169, -2.5381],
        [ 0.7607,  3.2719],
        [-0.3849, -2.1416]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-32.7480,   0.3103, -33.4412, -12.0259, -33.1025, -34.0585, -33.4557,
           0.3185, -33.5236,  -0.2997],
        [  2.2675,   7.3665,   2.5927,   4.1083,   2.8847,   2.8842,   2.7455,
           7.1501,   2.3676,   7.9562],
        [  6.2636,   5.7848,   9.6235,  18.6587,   9.5381,  10.8146,   7.0933,
           5.3938,   7.1705,   6.2198],
        [-37.0219,   2.5730, -36.9216, -12.6352, -36.9232, -37.0321, -37.2416,
           2.1161, -37.4337,   1.4166],
        [-33.5224,  -1.9998, -33.5259, -11.1244, -33.3263, -33.5923, -33.4960,
          -1.8961, -34.1128,  -2.5524],
        [-32.7209,   0.8304, -33.5407, -12.2606, -33.6732, -33.9854, -33.4799,
           0.8573, -33.6562,  -0.0879],
        [-37.4913,   2.4258, -37.2717, -13.4896, -37.0530, -37.2086, -37.5954,
           2.2769, -37.7614,   1.4297],
        [-33.7017,   1.3974, -33.8387, -13.0534, -34.1224, -34.5089, -34.2837,
           1.5192, -33.9568,   0.9538],
        [-27.0227,  -9.4047, -28.2817,  22.8408, -27.8556, -27.5804, -27.4554,
         -10.3972, -27.2999,  -7.4039],
        [-32.7044,   0.2235, -33.7213, -12.0331, -33.3777, -33.9213, -33.5310,
           0.1640, -33.3865,  -0.1866]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-10.5929,  19.4792,  17.9947, -33.6073,  -6.1540, -11.7444, -34.8067,
         -16.0356,  27.3396, -10.0753]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -5.7024,   0.5348],
        [-26.9960,  -4.9975],
        [-58.2550, -15.5654],
        [ -5.8745,   0.4228],
        [-37.0239, -11.1101],
        [ -5.7007,   0.5439],
        [-86.2842, -16.6311],
        [ -6.3180,  -2.9037],
        [  0.6867,   0.4703],
        [-21.1606,  -7.4073]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[  53.4105,   31.0601,  -14.3157,   68.3802,   -4.1786,   48.2165,
           -8.7271,   -1.8423,   -6.7162,   -4.3905],
        [ -82.0297,  -23.8696,   -3.1211,  -79.8804,  -24.8504,  -83.7486,
           -4.0729,  -52.4197,  -65.9010,  -18.7843],
        [ -96.4502,   -8.8884,  -34.0954, -100.2342,  -78.8967,  -96.2754,
          -45.6482,  -68.2817,  -59.7914,  -77.9269],
        [ -83.3365,  -23.9252,   -8.5918,  -77.4583,  -32.9434,  -81.9583,
          -11.8817,  -55.6648,  -53.5183,  -22.3453],
        [ -51.8814,   -1.4074,  -21.2501,  -53.2819,  -65.8083,  -47.9717,
          -30.8511,  -12.4386,  -23.4550,  -17.7646],
        [  42.7364,    3.5698,  -11.9024,   53.2348,   -0.3647,   38.9300,
          -37.0994,   -2.1357,   -4.0624,    2.6601],
        [ -91.8323,  -23.6257,   -5.0099,  -87.9523,  -28.7120,  -93.5797,
           -6.0830,  -55.1026,  -70.8440,  -20.1285],
        [  71.4658,   -9.1136,   22.7382,   78.6765,  -10.3623,   66.0836,
            4.1175,   -8.3182,   -2.5305,   -5.7601],
        [ -87.5119,  -24.7516,   -1.9300,  -82.5474,  -12.4853,  -89.0425,
           -0.6342,  -58.4062,  -66.2990,  -15.3381],
        [ -92.9564,  -20.6349,  -12.9217,  -86.6769,  -41.7485,  -91.1114,
          -18.7652,  -53.1733,  -59.8088,  -28.2344]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 2.3394,  3.2918,  5.4240,  3.7508, -3.4981, -3.5487,  4.2307,  2.0141,
          4.8132,  4.7955],
        [-2.0803, -3.4540, -5.1122, -4.1184,  3.7267,  3.9562, -3.9422, -2.2020,
         -5.1923, -4.6084]], device='cuda:0'))])
xi:  [-8.849111]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 55.96115308402766
W_T_median: -170.91672583535924
W_T_pctile_5: -439.036147100536
W_T_CVAR_5_pct: -481.76086753166436
Average q (qsum/M+1):  57.972593245967744
Optimal xi:  [-8.849111]
Expected(across Rb) median(across samples) p_equity:  0.23363208702794508
obj fun:  tensor(-1700.7606, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1801.8232226781981
Current xi:  [-18.911867]
objective value function right now is: -1801.8232226781981
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.2460895840177
Current xi:  [-18.984642]
objective value function right now is: -1802.2460895840177
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-19.066364]
objective value function right now is: -1801.9681441233772
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-19.067144]
objective value function right now is: -1802.0213166548526
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.5445428437936
Current xi:  [-18.654282]
objective value function right now is: -1802.5445428437936
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.6550686859077
Current xi:  [-18.339165]
objective value function right now is: -1802.6550686859077
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1802.6762045268915
Current xi:  [-18.273172]
objective value function right now is: -1802.6762045268915
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.613625]
objective value function right now is: -1801.8750051056015
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.416168]
objective value function right now is: -1802.428854435464
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.864857]
objective value function right now is: -1802.4535187300942
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.51636]
objective value function right now is: -1802.4193478608909
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.058031]
objective value function right now is: -1802.486478464164
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.735537]
objective value function right now is: -1801.892319678235
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-18.613745]
objective value function right now is: -1802.6413955379958
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.65206]
objective value function right now is: -1802.2756746536215
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.7125020888775
Current xi:  [-18.314043]
objective value function right now is: -1802.7125020888775
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.482826]
objective value function right now is: -1802.5159318719
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.69944]
objective value function right now is: -1802.3055743654934
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.29722]
objective value function right now is: -1801.9937453896516
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.811407437303
Current xi:  [-18.4531]
objective value function right now is: -1802.811407437303
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.640028]
objective value function right now is: -1802.6065926467725
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.333315]
objective value function right now is: -1802.3201548760367
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.327124]
objective value function right now is: -1802.6490760641386
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.608723]
objective value function right now is: -1802.5510488806901
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.496101]
objective value function right now is: -1802.6847960403675
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.376629]
objective value function right now is: -1802.7412545583052
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-19.093237]
objective value function right now is: -1802.009651034954
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-18.382586]
objective value function right now is: -1802.4103213786868
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-18.30553]
objective value function right now is: -1802.7875411316297
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.491898]
objective value function right now is: -1802.5653625907694
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.324934]
objective value function right now is: -1802.4457827111376
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.34965]
objective value function right now is: -1802.587373389842
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.17787]
objective value function right now is: -1800.5354389800336
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.74469]
objective value function right now is: -1802.4257343506792
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.698841]
objective value function right now is: -1802.5526282899002
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.961678]
objective value function right now is: -1802.5217699536295
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.478992]
objective value function right now is: -1802.7006072866018
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.541534]
objective value function right now is: -1802.7654850977494
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.622076]
objective value function right now is: -1802.525626757252
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.366192]
objective value function right now is: -1802.1266295013568
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.577234]
objective value function right now is: -1802.5279589455445
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.338087]
objective value function right now is: -1798.1575424236303
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-19.21285]
objective value function right now is: -1801.2254848165517
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.817198]
objective value function right now is: -1800.966727273827
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.587038]
objective value function right now is: -1802.4911639256086
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.454178]
objective value function right now is: -1800.750125815787
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.963245]
objective value function right now is: -1801.279880216566
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.356892]
objective value function right now is: -1802.278028151073
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-19.259184]
objective value function right now is: -1800.671577852772
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-18.522036]
objective value function right now is: -1802.0619036356113
min fval:  -1802.8039039248622
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -4.5227,   0.9597],
        [  0.4016,  -2.2196],
        [ -3.6267,   1.2697],
        [ 10.8848,   5.8749],
        [ -3.4473,   1.3268],
        [  7.5761,   4.9242],
        [-15.7985,  -5.4305],
        [ -0.2869,   2.3997],
        [ -2.6906,   1.5810],
        [ -3.9511,   1.1564]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-99.0846,   0.7906, -85.0253,   5.7153, -82.0096,  -7.1249,  -2.2108,
         -49.1047, -71.0664, -88.6732],
        [ 30.4193,  -4.2862,  32.9644,  30.9204,  32.4197,  34.4272,  -1.9289,
          35.7223,  31.8607,  31.6743],
        [ 89.5801,  -1.9987,  75.9068, -11.5303,  72.6661,  -2.9047,   4.9785,
          38.0410,  60.3434,  79.8036],
        [ 28.6374,  -4.3673,  30.6776,  27.3294,  30.2068,  31.7948,  -1.6612,
          33.3373,  29.3644,  29.0724],
        [-98.6873,   0.9129, -84.4067,   5.9008, -82.0259,  -6.4341,  -2.5539,
         -48.7306, -70.5335, -88.4531],
        [ 18.3584,  -9.9231,  18.6618,  15.2005,  18.7168,  18.6291, -10.5896,
          20.0756,  17.3166,  17.3262],
        [-21.2774, -11.4454, -20.7051, -12.7324, -21.0014, -13.0696, -10.1665,
         -16.1840, -19.6612, -20.4082],
        [ 24.8647,  -6.4701,  26.7838,  21.3098,  26.4166,  25.6523,  -2.1904,
          29.0036,  26.0711,  25.4759],
        [ 15.8566, -10.0024,  16.5138,  14.4230,  15.7842,  17.1433, -12.4355,
          17.3197,  14.6704,  15.1249],
        [-99.5609,   0.1930, -86.4727,   1.6674, -83.7456, -12.1323,  -1.3885,
         -52.2811, -73.1462, -90.0270]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 15.7759,  30.9532, -18.8859,  20.7910,  17.0170,   5.4523,   1.5697,
           9.5145,   5.4900,  10.8678]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -4.6471,   1.1741],
        [ -4.8980,   1.0378],
        [ 15.5930,   6.2900],
        [ -4.1307,   1.4762],
        [ -5.0708,   0.9518],
        [-10.9396,  -1.4021],
        [-12.2241,  -6.3663],
        [ -3.8373,   1.6822],
        [ -4.6844,   1.1429],
        [-28.8827,  -6.5570]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[  -9.8774,   -8.7067,  -18.1533,  -12.1909,   -9.1312,  -13.3450,
          -21.2332,  -14.2803,   -6.7123,   -3.2531],
        [   5.4073,    6.8050,  -21.2035,    3.3794,    7.6712,   -6.6369,
            0.7228,   -0.7817,    6.2984,  -10.5303],
        [  -7.6723,   -7.2042,  -19.0405,   -8.2863,   -7.0971,  -10.0241,
          -16.4933,  -11.2521,   -6.6992,   -5.7988],
        [   3.0435,    3.8595,  -21.7138,    1.4862,    4.0748,  -13.7282,
           -6.0599,   -0.6510,    4.2215,  -17.2607],
        [  54.0460,   58.0659,   -0.3093,   44.3430,   62.5647,   52.2311,
           -3.5668,   31.1289,   44.7468,   28.9440],
        [ -28.2831,  -28.2788,   -6.5115,  -30.9118,  -29.1202,    5.8535,
            2.3383,  -27.8617,  -23.4178,   -1.4865],
        [   3.2782,    3.9549,  -22.1067,    1.8216,    4.4445,  -13.4516,
           -6.3568,   -1.2066,    3.6421,  -18.0427],
        [ -39.1079,  -34.7118, -107.5950,  -49.9177,  -31.2806,    4.6698,
            6.1685,  -49.6091,  -35.0871,   36.1687],
        [  -7.6644,   -7.1185,  -19.1128,   -7.8459,   -6.5932,  -10.0125,
          -16.0844,   -9.8013,   -6.6938,   -6.4111],
        [   1.3870,    1.6063,  -23.5528,   -0.1590,    2.5595,  -15.4812,
           -8.7700,   -1.2676,    3.4606,  -18.8035]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 4.1795, -5.3775,  2.0968, -4.1541,  0.6659, -6.5205, -3.8450,  6.3050,
          1.1161, -2.1606],
        [-3.8435,  5.8949, -1.6892,  4.3794, -0.7362,  6.4957,  4.0578, -6.2713,
         -1.4840,  2.1601]], device='cuda:0'))])
xi:  [-18.587038]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: -258.06017819588595
W_T_median: -403.4972484773019
W_T_pctile_5: -922.7876935739906
W_T_CVAR_5_pct: -1051.9009428223064
Average q (qsum/M+1):  59.85181845388105
Optimal xi:  [-18.587038]
Expected(across Rb) median(across samples) p_equity:  0.1552696073572226
obj fun:  tensor(-1802.8039, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
