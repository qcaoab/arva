Starting at: 
08-02-23_11:19

 Random seed:  2  

Key parameters-------
paths: 25600
iterations: 5000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 500, 'itbound_SGD_algorithms': 5000, 'nit_IterateAveragingStart': 4500, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1584.180033097498
W_T_median: 1144.1177591225853
W_T_pctile_5: -126.33840259751949
W_T_CVAR_5_pct: -294.9519820713226
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1047.935084756211
Current xi:  [99.71512]
objective value function right now is: -1047.935084756211
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1054.733475116208
Current xi:  [101.3056]
objective value function right now is: -1054.733475116208
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.67275]
objective value function right now is: -1041.6010836385199
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1059.5141830287278
Current xi:  [106.433655]
objective value function right now is: -1059.5141830287278
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1163.6213633712603
Current xi:  [108.67049]
objective value function right now is: -1163.6213633712603
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1262.2979569908086
Current xi:  [108.617035]
objective value function right now is: -1262.2979569908086
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1298.4161664478295
Current xi:  [107.226524]
objective value function right now is: -1298.4161664478295
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1308.887714104844
Current xi:  [106.93974]
objective value function right now is: -1308.887714104844
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1315.7690056261097
Current xi:  [106.49065]
objective value function right now is: -1315.7690056261097
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1319.4101936151362
Current xi:  [106.85184]
objective value function right now is: -1319.4101936151362
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.549805]
objective value function right now is: -1303.751951078169
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1322.0927056465762
Current xi:  [107.892876]
objective value function right now is: -1322.0927056465762
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1327.379307127433
Current xi:  [108.332306]
objective value function right now is: -1327.379307127433
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [109.13156]
objective value function right now is: -1321.4164632384447
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1328.109233411201
Current xi:  [109.82664]
objective value function right now is: -1328.109233411201
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [110.38628]
objective value function right now is: -1326.7776421501255
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1530.149797977443
Current xi:  [108.593575]
objective value function right now is: -1530.149797977443
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1539.8273803306975
Current xi:  [105.08785]
objective value function right now is: -1539.8273803306975
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [101.70656]
objective value function right now is: -1531.2415243430023
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1548.7926511233331
Current xi:  [99.42984]
objective value function right now is: -1548.7926511233331
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1550.9341771712539
Current xi:  [97.65839]
objective value function right now is: -1550.9341771712539
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1556.2456765073878
Current xi:  [95.92447]
objective value function right now is: -1556.2456765073878
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [94.348206]
objective value function right now is: -1554.1988439506729
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [92.72238]
objective value function right now is: -1555.4408604950866
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [91.61245]
objective value function right now is: -1501.5146921280975
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [90.309]
objective value function right now is: -1555.6435732989669
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [88.74033]
objective value function right now is: -1552.3492442923625
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [87.38982]
objective value function right now is: -1551.6321889214134
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1557.0156142030567
Current xi:  [86.562035]
objective value function right now is: -1557.0156142030567
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [86.101494]
objective value function right now is: -1553.849470460704
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1558.960671356513
Current xi:  [82.648544]
objective value function right now is: -1558.960671356513
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [81.88801]
objective value function right now is: -1539.0775261220708
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [81.45941]
objective value function right now is: -1558.353005822045
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [80.835495]
objective value function right now is: -1553.5238861483372
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -1560.5570640705146
Current xi:  [80.16158]
objective value function right now is: -1560.5570640705146
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.5654009968514
Current xi:  [80.073166]
objective value function right now is: -1562.5654009968514
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.8777207629446
Current xi:  [79.98106]
objective value function right now is: -1562.8777207629446
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [79.88527]
objective value function right now is: -1562.2642292882038
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [79.76595]
objective value function right now is: -1562.1241935638543
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [79.65955]
objective value function right now is: -1562.052285668968
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [79.60785]
objective value function right now is: -1562.6257952004366
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.9995542371057
Current xi:  [79.448906]
objective value function right now is: -1562.9995542371057
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [79.36488]
objective value function right now is: -1562.8335108978138
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [79.2446]
objective value function right now is: -1557.5533189065154
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [79.12818]
objective value function right now is: -1562.7519090625794
new min fval from sgd:  -1563.0446753094675
new min fval from sgd:  -1563.0913646033493
new min fval from sgd:  -1563.1264177206995
new min fval from sgd:  -1563.205450432093
new min fval from sgd:  -1563.3176548491576
new min fval from sgd:  -1563.4411581804554
new min fval from sgd:  -1563.4943836389418
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [78.92361]
objective value function right now is: -1561.247318494161
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [78.85537]
objective value function right now is: -1563.0483439072007
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [78.73885]
objective value function right now is: -1563.4079001051614
new min fval from sgd:  -1563.5428298437098
new min fval from sgd:  -1563.554689995435
new min fval from sgd:  -1563.5749523779436
new min fval from sgd:  -1563.5815224915098
new min fval from sgd:  -1563.6001812061586
new min fval from sgd:  -1563.6017582287798
new min fval from sgd:  -1563.6069058349663
new min fval from sgd:  -1563.6127831595218
new min fval from sgd:  -1563.6199038957961
new min fval from sgd:  -1563.6234419045045
new min fval from sgd:  -1563.6317423639807
new min fval from sgd:  -1563.6497805786132
new min fval from sgd:  -1563.6610260578775
new min fval from sgd:  -1563.6613428885112
new min fval from sgd:  -1563.6682862895614
new min fval from sgd:  -1563.6790876295622
new min fval from sgd:  -1563.685054225928
new min fval from sgd:  -1563.686890114638
new min fval from sgd:  -1563.69211129368
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [78.63515]
objective value function right now is: -1563.6780063099857
new min fval from sgd:  -1563.6962375622873
new min fval from sgd:  -1563.7012443738513
new min fval from sgd:  -1563.7047418108607
new min fval from sgd:  -1563.7138320821414
new min fval from sgd:  -1563.7238765562047
new min fval from sgd:  -1563.7365346640338
new min fval from sgd:  -1563.749512777123
new min fval from sgd:  -1563.7647089468758
new min fval from sgd:  -1563.7776314508303
new min fval from sgd:  -1563.786140746649
new min fval from sgd:  -1563.79179051619
new min fval from sgd:  -1563.7970072519634
new min fval from sgd:  -1563.7995872836307
new min fval from sgd:  -1563.8002996703135
new min fval from sgd:  -1563.8037824662408
new min fval from sgd:  -1563.8099608310833
new min fval from sgd:  -1563.8153681003657
new min fval from sgd:  -1563.8236946844524
new min fval from sgd:  -1563.8264390098857
new min fval from sgd:  -1563.8379888271866
new min fval from sgd:  -1563.846768441297
new min fval from sgd:  -1563.8478665338055
new min fval from sgd:  -1563.8504261994258
new min fval from sgd:  -1563.8507104409869
new min fval from sgd:  -1563.8522112993587
new min fval from sgd:  -1563.8545583808323
new min fval from sgd:  -1563.8617788762315
new min fval from sgd:  -1563.8633847855112
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [78.62521]
objective value function right now is: -1563.7442645306658
min fval:  -1563.8633847855112
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -5.3265,  -4.9356],
        [  6.6195,  -1.8237],
        [-24.7638,  -8.2766],
        [ -2.1097, -11.2859],
        [  5.5180,  -3.9317],
        [ -5.0348,   1.1757],
        [ -1.5344,  -2.8366],
        [ -1.6394,  -8.2636],
        [ -4.2871,  -0.5252],
        [  1.0142,  -8.2127]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 6.6284, -4.8246, -8.0652, -8.7318, -4.6418,  3.4572, -4.5399, -6.1297,
         3.3075, -6.3578], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.0753,  0.3301, -0.1188,  0.5257,  0.2766, -7.1152, -0.5656, -0.0128,
         -1.6641,  0.3490],
        [-4.6060,  8.2421,  9.1768, 10.0201,  5.3742, -9.2041,  0.9049,  6.3141,
         -5.3989,  7.2123],
        [-1.0830,  2.5392,  1.1956,  3.7359,  1.9117, -7.0228, -0.1024,  2.6710,
         -2.4976,  3.9104],
        [-1.3566,  1.0134,  0.4515,  1.0782,  0.9156, -6.9314, -0.3225,  0.3558,
         -2.1410,  1.2657],
        [-2.1709,  0.3172,  0.0228,  0.8946,  0.2093, -7.2278, -0.1872, -0.1588,
         -1.6493,  0.2022],
        [-1.0530,  1.7577,  0.9369,  1.7858,  1.6850, -6.8842, -0.5215,  1.1879,
         -2.2147,  1.9935],
        [-5.2256,  9.3096, 10.1993, 11.8408,  5.9722, -9.9360,  1.0406,  7.5013,
         -7.0234,  7.4922],
        [-5.1153,  9.6572, 10.2884, 11.5899,  5.8204, -9.8695,  1.3019,  7.5738,
         -6.9403,  7.7091],
        [-4.7147,  8.5839,  9.7987, 10.3222,  6.1520, -9.4130,  1.2810,  6.9398,
         -5.6610,  7.0288],
        [ 5.8218,  1.0625, -0.0261, -0.1323,  1.4005,  1.7485,  0.6523,  0.3175,
          2.0869,  0.8185]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.3873, -0.4955, -0.2811, -0.7008, -0.3987, -0.6713, -0.1536, -0.4595,
        -0.7857,  2.5469], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-0.6353, -4.2925, -1.2187, -0.8150, -0.7681, -0.9439, -5.9920, -5.7799,
         -5.6025,  9.4234]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-6.2409, -2.2111],
        [-5.3205,  6.1180],
        [-4.3517,  5.7959],
        [ 0.3294,  1.1183],
        [-8.8570, -3.2902],
        [-8.5243, -3.1661],
        [-1.1714,  0.6762],
        [-7.4264, -2.6803],
        [ 6.1157,  3.5068],
        [-8.1752, -3.0592]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-2.2703,  6.4889,  5.8538, -1.7909, -1.6920, -1.6902, -3.5296, -1.3849,
         3.8422, -1.6867], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[  0.1194,   0.0341,  -0.6744,  -0.4486,   0.2531,   0.3994,  -0.4804,
           0.2873,  -5.9923,   0.2662],
        [ -0.0808,  -2.2003,  -2.5690,  -0.9065,  -0.8115,  -0.6691,  -0.6523,
          -0.5593,  -5.2665,  -0.5561],
        [  3.8369, -14.7699, -10.1302,   1.2803,   8.5419,   8.1416,   0.7957,
           5.6736,  -2.6913,   6.9357],
        [ -0.4830,  -2.9677,  -3.2629,  -0.9429,  -1.6467,  -1.2597,  -0.2855,
          -1.0214,  -3.6222,  -1.3361],
        [ -1.4433,  -7.0695,  -9.0309,  -2.3772,  -4.0738,  -3.5365,  -1.3137,
          -2.8453,  -2.2638,  -2.9359],
        [  2.1370,  -3.2427,  -3.9265,   0.2464,   4.1142,   4.0016,  -0.1041,
           3.1850,  -6.1563,   3.7016],
        [  2.9303,  -5.9459,  -6.4803,  -0.7348,   3.9249,   3.9133,  -0.5389,
           3.9435,  -5.9063,   4.1245],
        [ -3.1386,  12.9488,   7.2675,  -0.8213,  -5.5074,  -5.3319,  -0.4932,
          -4.5431,   1.0340,  -4.8777],
        [ -0.6437,  -3.2034,  -3.4604,  -0.9462,  -1.5439,  -1.6193,  -0.2641,
          -0.9941,  -4.0038,  -1.2346],
        [ -0.3300,  -2.8375,  -3.3668,  -1.0755,  -1.5552,  -1.0213,  -0.5848,
          -1.1269,  -4.5077,  -1.0734]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.4359, -1.0797, -2.1967, -1.7389, -0.2573, -2.2171, -2.4450,  0.7485,
        -1.5324, -1.2483], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 0.0088,  0.6038, -4.9937,  0.3050,  1.4299,  5.7231,  4.7445,  0.1620,
          0.3074,  0.6490],
        [ 0.3028, -0.2585,  4.8087, -0.4367, -1.0978, -5.2571, -4.3677, -0.4815,
         -0.4483, -0.2984]], device='cuda:0'))])
xi:  [78.627]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 681.3762618039765
W_T_median: 320.4246206078686
W_T_pctile_5: 78.57731050276313
W_T_CVAR_5_pct: -33.781084486313354
Average q (qsum/M+1):  51.537003055695564
Optimal xi:  [78.627]
Observed VAR:  320.4246206078686
Expected(across Rb) median(across samples) p_equity:  0.34367820223172507
obj fun:  tensor(-1563.8634, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
25600
25594
25583
25567
25542
25510
25464
25416
25358
25286
25212
25118
25040
24948
24855
24745
24645
24575
24490
24378
24239
24158
24033
23909
23804
23710
23572
23460
23355
23227
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
