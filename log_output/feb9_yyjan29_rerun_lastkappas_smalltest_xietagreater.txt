Starting at: 
09-02-23_12:08

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 10000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 1000, 'itbound_SGD_algorithms': 10000, 'nit_IterateAveragingStart': 9000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  9.8056,   0.3702],
        [ -4.3465,   8.7513],
        [ 11.6827,  -4.5826],
        [  0.3149,  11.8025],
        [ -0.7516,   1.0731],
        [ -0.7516,   1.0731],
        [-33.7974,  -9.1431],
        [  5.3946,  -4.5009],
        [ -0.7516,   1.0731],
        [ 10.9840,   1.5848]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -8.5826,   5.8637,  -9.1605,   9.5059,  -2.6000,  -2.6001,  -8.5071,
          1.6323,  -2.6001, -10.2237], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [ 5.2973e+00, -1.0926e+01,  1.1810e+01, -1.5601e+01, -4.0905e-02,
         -4.0919e-02,  1.3139e+01,  2.8679e+00, -4.0979e-02,  1.7610e+01],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-1.6569e-01,  3.5122e+00,  1.2751e+00,  1.6088e+01, -9.9319e-02,
         -9.9316e-02,  1.2755e+01, -4.1160e+00, -9.9300e-02, -6.4273e-02],
        [ 4.1958e+00, -1.0778e+01,  1.2722e+01, -1.4714e+01,  8.3874e-02,
          8.3891e-02,  1.2004e+01,  3.9389e+00,  8.3961e-02,  1.5171e+01],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.7286, -0.7286, -0.7286,  1.5126, -0.7286, -0.7286, -0.7286, -2.8475,
         2.4285, -0.7286], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 8.8609e-03,  8.8609e-03,  8.8609e-03, -1.4825e+01,  8.8609e-03,
          8.8609e-03,  8.8609e-03,  1.2082e+01, -1.0963e+01,  8.8609e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 17.5169,   2.4047],
        [-11.9075,  -5.4174],
        [ -2.9792,  -0.9803],
        [ -9.7518,  -4.0833],
        [ 11.3921,   5.1641],
        [ 13.7315,  -0.0707],
        [ -8.8519,  10.6386],
        [  7.6684,  14.1374],
        [-16.4605,  -4.5943],
        [ 14.0679,   5.0107]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-10.0398,  -4.7415,  -4.9321,  -4.8020,   4.9201, -12.9012,  10.5438,
         11.8370,   0.8090,  -3.9403], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.8323e+00,  4.5980e+00,  3.9185e-01,  3.6615e+00, -2.6056e+00,
         -5.2731e-02, -2.9941e+00,  7.1657e-01,  1.9169e+00, -3.9848e+00],
        [ 9.5454e+00,  1.7276e+00, -6.8431e-02,  9.5238e-03, -2.5478e+00,
          5.6171e+00,  1.4304e+01,  1.7821e+01, -1.7394e+00,  4.0096e+00],
        [-6.9749e-01, -1.6425e-01,  1.4400e-02, -9.4509e-02, -2.5979e+00,
         -2.4345e-01, -1.8580e-01, -1.0674e+00, -4.0907e-01, -8.7679e-01],
        [-1.5408e+00,  1.3537e+00,  1.1014e-01,  6.5129e-01, -2.6476e+00,
         -2.6944e-01, -5.7968e+00,  1.5919e+00,  1.8653e+00, -2.8750e+00],
        [ 1.5086e+00,  9.7123e+00,  5.2281e-01,  4.8078e+00, -1.4468e+01,
          2.2508e+00,  6.0173e-01, -3.3019e+01,  1.0199e+01, -2.1604e+00],
        [-3.3438e+00,  1.9099e+00, -1.5542e-01,  1.1551e+00, -1.7673e+00,
         -7.1039e+00,  1.1353e+01,  2.7938e+00, -3.5859e+00, -3.1004e+00],
        [-5.8361e+00,  4.0316e+00,  4.1967e-01,  1.6997e+00, -2.4723e+00,
         -6.8854e+00, -5.0376e+00, -3.2686e+01,  1.5702e+01, -6.0404e+00],
        [-7.3342e-01, -1.6558e-01,  1.4866e-02, -9.5380e-02, -2.5807e+00,
         -2.4601e-01, -1.7490e-01, -1.0871e+00, -4.0997e-01, -8.5503e-01],
        [-7.4823e+00,  3.6824e+00,  2.7743e-01,  3.3016e+00,  7.7796e-01,
          6.7700e-01, -9.1973e+00, -5.8992e+00,  3.2522e+00, -2.3277e+00],
        [-2.7056e-02,  1.0243e+00,  2.5504e-01,  9.7638e-01, -2.4098e+00,
          2.7460e-01, -9.3762e+00,  2.3750e+00,  2.9668e+00, -3.1138e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.3811, -8.4615, -2.6506, -2.6500,  3.0872, -5.6053,  1.2951, -2.6386,
        -1.2181, -2.5183], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.7230,   0.1490,  -0.2400,   1.4434,  14.4126,   0.3764,  -7.1038,
          -0.2402,  -2.2275,   1.6248],
        [ -1.7230,  -0.3536,   0.2400,  -1.4433, -14.4142,  -0.5248,   7.1825,
           0.2402,   2.2745,  -1.6251]], device='cuda:0'))])
loaded xi:  58.6305
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1534.069336500596
Current xi:  [63.950294]
objective value function right now is: -1534.069336500596
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1542.0360189910377
Current xi:  [69.03294]
objective value function right now is: -1542.0360189910377
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1542.1968819656604
Current xi:  [73.16452]
objective value function right now is: -1542.1968819656604
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1543.5716684177132
Current xi:  [77.15827]
objective value function right now is: -1543.5716684177132
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1548.3382347048623
Current xi:  [81.23376]
objective value function right now is: -1548.3382347048623
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [83.871216]
objective value function right now is: -1545.1236913376465
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1549.5767934018097
Current xi:  [87.26767]
objective value function right now is: -1549.5767934018097
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [90.81636]
objective value function right now is: -1548.9834481952234
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [93.647125]
objective value function right now is: -1546.2466092280176
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1550.863811557881
Current xi:  [95.98972]
objective value function right now is: -1550.863811557881
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1552.587562871525
Current xi:  [97.80206]
objective value function right now is: -1552.587562871525
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [99.87859]
objective value function right now is: -1548.2181944454378
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [102.19465]
objective value function right now is: -1546.0969655277781
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [104.457436]
objective value function right now is: -1541.6504945681418
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.840485]
objective value function right now is: -1548.1137450971796
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.50711]
objective value function right now is: -1550.7849817555255
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.20209]
objective value function right now is: -1550.0710915399281
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.884895]
objective value function right now is: -1549.0823793551062
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [110.68524]
objective value function right now is: -1550.0200003899947
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.90992]
objective value function right now is: -1548.9240279742016
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.156334]
objective value function right now is: -1551.1274873150303
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [110.45024]
objective value function right now is: -1551.1352117468944
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [111.233955]
objective value function right now is: -1549.0149747270934
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [111.88433]
objective value function right now is: -1550.706626846678
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.227325]
objective value function right now is: -1551.7305518178116
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.44642]
objective value function right now is: -1551.3413218564897
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.618706]
objective value function right now is: -1546.1301727742782
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [112.93479]
objective value function right now is: -1551.6525200989067
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [112.72749]
objective value function right now is: -1547.6847401761338
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.40694]
objective value function right now is: -1549.7508914004045
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.34054]
objective value function right now is: -1551.0008014041382
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.4262]
objective value function right now is: -1549.23334866177
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [113.140366]
objective value function right now is: -1551.0531174066896
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.98241]
objective value function right now is: -1545.5831316293425
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [113.1143]
objective value function right now is: -1548.082486265549
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [113.1871]
objective value function right now is: -1552.585067928681
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1553.6963307429787
Current xi:  [112.94023]
objective value function right now is: -1553.6963307429787
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1553.799297936844
Current xi:  [112.75677]
objective value function right now is: -1553.799297936844
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1554.0953927057155
Current xi:  [112.71204]
objective value function right now is: -1554.0953927057155
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.86524]
objective value function right now is: -1553.5753459528607
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1554.4164764619202
Current xi:  [112.786606]
objective value function right now is: -1554.4164764619202
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.77595]
objective value function right now is: -1553.9202220256534
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1554.4753256819142
Current xi:  [112.97317]
objective value function right now is: -1554.4753256819142
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.89756]
objective value function right now is: -1554.0850779182758
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.73394]
objective value function right now is: -1553.563287841787
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.851105]
objective value function right now is: -1553.3601222669447
new min fval from sgd:  -1554.5517870795788
new min fval from sgd:  -1554.5577070341606
new min fval from sgd:  -1554.5759207009235
new min fval from sgd:  -1554.5950757937474
new min fval from sgd:  -1554.602793945325
new min fval from sgd:  -1554.611096399845
new min fval from sgd:  -1554.6426250941527
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.9416]
objective value function right now is: -1554.498526278006
new min fval from sgd:  -1554.65607306907
new min fval from sgd:  -1554.6628108313794
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [113.09321]
objective value function right now is: -1554.051652584594
new min fval from sgd:  -1554.6682168499153
new min fval from sgd:  -1554.6901654821888
new min fval from sgd:  -1554.710384072432
new min fval from sgd:  -1554.719400922343
new min fval from sgd:  -1554.7238540893482
new min fval from sgd:  -1554.7251295541353
new min fval from sgd:  -1554.7261756693815
new min fval from sgd:  -1554.7285704277494
new min fval from sgd:  -1554.728732628643
new min fval from sgd:  -1554.7295191635735
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [113.25396]
objective value function right now is: -1554.6782226063517
new min fval from sgd:  -1554.7448020872498
new min fval from sgd:  -1554.7646834325496
new min fval from sgd:  -1554.778907675448
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [113.23766]
objective value function right now is: -1554.7610542548946
min fval:  -1554.778907675448
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 10.6272,   0.5063],
        [ -5.6495,   9.1966],
        [ 12.9617,  -4.1726],
        [ -1.2320,  12.5245],
        [ -0.9488,   1.0436],
        [ -0.9488,   1.0436],
        [-36.0897,  -9.5201],
        [  6.8692,  -4.0935],
        [ -0.9487,   1.0436],
        [ 12.0218,   0.5832]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -9.5631,   5.7757,  -9.3870,   9.6359,  -3.0942,  -3.0942,  -8.7490,
          2.4138,  -3.0942, -10.8922], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.5692e-02, -2.1797e-01, -1.2483e-01, -5.3867e-01, -6.2126e-03,
         -6.2126e-03, -1.6289e-01, -1.0983e+00, -6.2124e-03, -4.2981e-02],
        [-4.5692e-02, -2.1797e-01, -1.2483e-01, -5.3867e-01, -6.2126e-03,
         -6.2126e-03, -1.6289e-01, -1.0983e+00, -6.2124e-03, -4.2981e-02],
        [-4.5692e-02, -2.1797e-01, -1.2483e-01, -5.3867e-01, -6.2126e-03,
         -6.2126e-03, -1.6289e-01, -1.0983e+00, -6.2124e-03, -4.2981e-02],
        [ 5.8272e+00, -1.0279e+01,  1.2349e+01, -1.6324e+01, -4.5490e-02,
         -4.5493e-02,  1.2874e+01,  2.9060e+00, -4.5502e-02,  1.8120e+01],
        [-4.5692e-02, -2.1797e-01, -1.2483e-01, -5.3867e-01, -6.2126e-03,
         -6.2126e-03, -1.6289e-01, -1.0983e+00, -6.2124e-03, -4.2981e-02],
        [-4.5692e-02, -2.1797e-01, -1.2483e-01, -5.3867e-01, -6.2126e-03,
         -6.2126e-03, -1.6289e-01, -1.0983e+00, -6.2124e-03, -4.2981e-02],
        [-4.5692e-02, -2.1797e-01, -1.2483e-01, -5.3867e-01, -6.2126e-03,
         -6.2126e-03, -1.6289e-01, -1.0983e+00, -6.2124e-03, -4.2981e-02],
        [ 1.0000e-01,  2.8162e+00,  5.0315e+00,  1.8117e+01, -3.2368e-01,
         -3.2367e-01,  1.1851e+01, -3.7477e+00, -3.2367e-01,  1.4752e-02],
        [ 5.3446e+00, -9.7858e+00,  1.4118e+01, -1.5454e+01,  5.7579e-02,
          5.7583e-02,  1.1662e+01,  3.8807e+00,  5.7595e-02,  1.6053e+01],
        [-4.5692e-02, -2.1797e-01, -1.2483e-01, -5.3867e-01, -6.2126e-03,
         -6.2126e-03, -1.6289e-01, -1.0983e+00, -6.2124e-03, -4.2981e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.1015, -1.1015, -1.1015,  1.6525, -1.1015, -1.1015, -1.1015, -2.5325,
         2.4642, -1.1015], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.9858e-03,  1.9858e-03,  1.9858e-03, -1.5634e+01,  1.9858e-03,
          1.9858e-03,  1.9858e-03,  1.2094e+01, -1.1659e+01,  1.9858e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 18.6276,   2.2094],
        [-14.3284,  -4.4002],
        [ -1.4073,   4.5240],
        [ -3.7272,  -1.6585],
        [ 14.4490,   4.3128],
        [ 15.1038,   0.4498],
        [ -7.3956,  11.7599],
        [  7.9410,  14.3119],
        [-16.8138,  -4.9003],
        [ 15.6825,   5.5659]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-11.0580,  -4.0697,  -1.4994,  -5.7208,   3.7908, -13.0826,  10.1292,
         11.7852,   0.9497,  -3.9511], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.3570e+00,  1.2491e+00,  2.0230e-02,  2.5289e-01, -3.8834e+00,
         -1.0281e-01, -3.1864e+00,  2.4471e+00,  1.7722e+00, -4.6525e+00],
        [ 9.6224e+00, -4.8903e+00,  1.0822e-01, -1.1770e-01, -2.6731e+00,
          5.0148e+00,  1.1889e+01,  1.4186e+01,  1.1265e+00,  7.9015e-01],
        [-9.0614e-01, -1.4573e-01, -1.2739e-01, -4.2021e-02, -2.9481e+00,
         -3.4428e-01, -4.8080e-01, -1.6419e+00, -7.1922e-01, -1.0573e+00],
        [-7.7006e+00,  2.6741e+00, -3.7658e-02,  3.5677e-01, -3.1243e+00,
         -2.0557e+00, -8.6053e+00,  4.0543e+00,  1.3496e-02, -5.4281e+00],
        [-5.0751e+00,  8.6350e+00, -8.9599e-01,  1.3650e+00, -1.3199e+01,
         -6.6987e+00,  9.7336e-01, -3.7670e+01,  1.0575e+01, -8.8981e-01],
        [-2.7424e+00,  7.7010e+00,  5.4019e+00, -3.2645e-01, -2.4678e+00,
         -7.4259e+00,  1.0820e+01,  2.2199e+00, -3.6746e-01, -3.7578e+00],
        [-5.5074e+00,  3.3320e+00, -7.6939e-02,  7.3339e-02, -2.6236e+00,
         -6.0196e+00, -3.4807e+00, -3.1694e+01,  1.6257e+01, -6.9061e+00],
        [-9.0618e-01, -1.4575e-01, -1.2739e-01, -4.2025e-02, -2.9481e+00,
         -3.4429e-01, -4.8085e-01, -1.6420e+00, -7.1925e-01, -1.0573e+00],
        [-7.0333e+00,  5.0319e+00, -5.0312e-02,  9.2979e-01,  1.3475e+00,
          3.8925e-01, -1.2047e+01, -9.0398e+00,  3.9612e+00, -4.8572e+00],
        [ 3.5305e-01,  1.3200e+00,  7.9291e-01,  4.5523e-01, -4.0931e+00,
          1.8359e+00, -4.8265e+00,  3.8624e+00,  3.7333e+00, -1.4984e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.8218, -8.6904, -3.0301, -3.2234,  3.4458, -5.5791,  1.2132, -3.0301,
        -0.5183, -4.2588], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  2.4298,   0.2206,  -0.1196,   3.2158,  16.6130,   0.3897,  -5.8326,
          -0.1196,  -2.1622,   2.2775],
        [ -2.4297,  -0.4250,   0.1196,  -3.2157, -16.6121,  -0.5352,   5.9054,
           0.1196,   2.2080,  -2.2777]], device='cuda:0'))])
xi:  [113.26295]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 518.2047442521953
W_T_median: 296.87659789991335
W_T_pctile_5: 113.24731025347396
W_T_CVAR_5_pct: -12.132196042763189
Average q (qsum/M+1):  50.74121881300403
Optimal xi:  [113.26295]
Observed VAR:  296.87659789991335
Expected(across Rb) median(across samples) p_equity:  0.2720428747435411
obj fun:  tensor(-1554.7789, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
256000
255950
255871
255744
255568
255353
255108
254885
254639
254351
254104
253782
253496
253172
252874
252494
252110
251691
251228
250812
250396
249970
249410
248859
248282
247730
247170
246510
245720
245012
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  9.8056,   0.3702],
        [ -4.3465,   8.7513],
        [ 11.6827,  -4.5826],
        [  0.3149,  11.8025],
        [ -0.7516,   1.0731],
        [ -0.7516,   1.0731],
        [-33.7974,  -9.1431],
        [  5.3946,  -4.5009],
        [ -0.7516,   1.0731],
        [ 10.9840,   1.5848]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -8.5826,   5.8637,  -9.1605,   9.5059,  -2.6000,  -2.6001,  -8.5071,
          1.6323,  -2.6001, -10.2237], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [ 5.2973e+00, -1.0926e+01,  1.1810e+01, -1.5601e+01, -4.0905e-02,
         -4.0919e-02,  1.3139e+01,  2.8679e+00, -4.0979e-02,  1.7610e+01],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-1.6569e-01,  3.5122e+00,  1.2751e+00,  1.6088e+01, -9.9319e-02,
         -9.9316e-02,  1.2755e+01, -4.1160e+00, -9.9300e-02, -6.4273e-02],
        [ 4.1958e+00, -1.0778e+01,  1.2722e+01, -1.4714e+01,  8.3874e-02,
          8.3891e-02,  1.2004e+01,  3.9389e+00,  8.3961e-02,  1.5171e+01],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.7286, -0.7286, -0.7286,  1.5126, -0.7286, -0.7286, -0.7286, -2.8475,
         2.4285, -0.7286], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 8.8609e-03,  8.8609e-03,  8.8609e-03, -1.4825e+01,  8.8609e-03,
          8.8609e-03,  8.8609e-03,  1.2082e+01, -1.0963e+01,  8.8609e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 17.5169,   2.4047],
        [-11.9075,  -5.4174],
        [ -2.9792,  -0.9803],
        [ -9.7518,  -4.0833],
        [ 11.3921,   5.1641],
        [ 13.7315,  -0.0707],
        [ -8.8519,  10.6386],
        [  7.6684,  14.1374],
        [-16.4605,  -4.5943],
        [ 14.0679,   5.0107]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-10.0398,  -4.7415,  -4.9321,  -4.8020,   4.9201, -12.9012,  10.5438,
         11.8370,   0.8090,  -3.9403], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.8323e+00,  4.5980e+00,  3.9185e-01,  3.6615e+00, -2.6056e+00,
         -5.2731e-02, -2.9941e+00,  7.1657e-01,  1.9169e+00, -3.9848e+00],
        [ 9.5454e+00,  1.7276e+00, -6.8431e-02,  9.5238e-03, -2.5478e+00,
          5.6171e+00,  1.4304e+01,  1.7821e+01, -1.7394e+00,  4.0096e+00],
        [-6.9749e-01, -1.6425e-01,  1.4400e-02, -9.4509e-02, -2.5979e+00,
         -2.4345e-01, -1.8580e-01, -1.0674e+00, -4.0907e-01, -8.7679e-01],
        [-1.5408e+00,  1.3537e+00,  1.1014e-01,  6.5129e-01, -2.6476e+00,
         -2.6944e-01, -5.7968e+00,  1.5919e+00,  1.8653e+00, -2.8750e+00],
        [ 1.5086e+00,  9.7123e+00,  5.2281e-01,  4.8078e+00, -1.4468e+01,
          2.2508e+00,  6.0173e-01, -3.3019e+01,  1.0199e+01, -2.1604e+00],
        [-3.3438e+00,  1.9099e+00, -1.5542e-01,  1.1551e+00, -1.7673e+00,
         -7.1039e+00,  1.1353e+01,  2.7938e+00, -3.5859e+00, -3.1004e+00],
        [-5.8361e+00,  4.0316e+00,  4.1967e-01,  1.6997e+00, -2.4723e+00,
         -6.8854e+00, -5.0376e+00, -3.2686e+01,  1.5702e+01, -6.0404e+00],
        [-7.3342e-01, -1.6558e-01,  1.4866e-02, -9.5380e-02, -2.5807e+00,
         -2.4601e-01, -1.7490e-01, -1.0871e+00, -4.0997e-01, -8.5503e-01],
        [-7.4823e+00,  3.6824e+00,  2.7743e-01,  3.3016e+00,  7.7796e-01,
          6.7700e-01, -9.1973e+00, -5.8992e+00,  3.2522e+00, -2.3277e+00],
        [-2.7056e-02,  1.0243e+00,  2.5504e-01,  9.7638e-01, -2.4098e+00,
          2.7460e-01, -9.3762e+00,  2.3750e+00,  2.9668e+00, -3.1138e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.3811, -8.4615, -2.6506, -2.6500,  3.0872, -5.6053,  1.2951, -2.6386,
        -1.2181, -2.5183], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.7230,   0.1490,  -0.2400,   1.4434,  14.4126,   0.3764,  -7.1038,
          -0.2402,  -2.2275,   1.6248],
        [ -1.7230,  -0.3536,   0.2400,  -1.4433, -14.4142,  -0.5248,   7.1825,
           0.2402,   2.2745,  -1.6251]], device='cuda:0'))])
loaded xi:  58.6305
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1505.7516142917768
Current xi:  [73.197495]
objective value function right now is: -1505.7516142917768
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1520.19195874772
Current xi:  [85.42822]
objective value function right now is: -1520.19195874772
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1531.4883694063622
Current xi:  [96.989265]
objective value function right now is: -1531.4883694063622
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1537.1771887053912
Current xi:  [106.59217]
objective value function right now is: -1537.1771887053912
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [114.79005]
objective value function right now is: -1531.4031756122124
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1539.650717110169
Current xi:  [123.44259]
objective value function right now is: -1539.650717110169
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1549.5011607345507
Current xi:  [131.53673]
objective value function right now is: -1549.5011607345507
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [139.52676]
objective value function right now is: -1544.5206107221995
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1549.943489008877
Current xi:  [144.50934]
objective value function right now is: -1549.943489008877
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [149.02254]
objective value function right now is: -1541.089762263779
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [150.80704]
objective value function right now is: -1538.547876565073
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1551.7114371111202
Current xi:  [155.0067]
objective value function right now is: -1551.7114371111202
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [157.99959]
objective value function right now is: -1546.5561185352915
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1557.8277754000026
Current xi:  [159.18318]
objective value function right now is: -1557.8277754000026
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [159.37065]
objective value function right now is: -1556.7635015510916
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.859]
objective value function right now is: -1551.6466474188398
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.06735]
objective value function right now is: -1541.7489217068855
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.51918]
objective value function right now is: -1541.1602708968542
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.34196]
objective value function right now is: -1551.2963861487722
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.51945]
objective value function right now is: -1550.4354969985588
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.19681]
objective value function right now is: -1548.7917233519954
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.68431]
objective value function right now is: -1556.6458786904248
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.8776]
objective value function right now is: -1551.8059225088216
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.63733]
objective value function right now is: -1556.5931011522814
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.45923]
objective value function right now is: -1548.3357783973372
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.26567]
objective value function right now is: -1555.0571958493158
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [153.39424]
objective value function right now is: -1552.4831400522285
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [156.25012]
objective value function right now is: -1551.8526542840234
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [159.63556]
objective value function right now is: -1557.0387992787403
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1559.2551541806793
Current xi:  [164.05501]
objective value function right now is: -1559.2551541806793
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1559.7509141699438
Current xi:  [165.22942]
objective value function right now is: -1559.7509141699438
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.70596]
objective value function right now is: -1555.4222416603247
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.36813]
objective value function right now is: -1557.10518784907
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.62108]
objective value function right now is: -1547.5994949029157
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.10406]
objective value function right now is: -1557.043284969502
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.1441825234301
Current xi:  [165.24362]
objective value function right now is: -1562.1441825234301
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.52386]
objective value function right now is: -1561.0076806948289
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.97061]
objective value function right now is: -1555.3341886643739
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.97379]
objective value function right now is: -1561.1900640507702
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.20428]
objective value function right now is: -1559.9352965471708
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.3216681438253
Current xi:  [166.21945]
objective value function right now is: -1562.3216681438253
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.03654]
objective value function right now is: -1561.81515947539
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.5394]
objective value function right now is: -1562.0877971869265
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.729]
objective value function right now is: -1558.559299998997
new min fval from sgd:  -1562.4058120411419
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.0624]
objective value function right now is: -1562.4058120411419
new min fval from sgd:  -1562.4236635404727
new min fval from sgd:  -1562.4245074590035
new min fval from sgd:  -1562.447407903565
new min fval from sgd:  -1562.4778463747696
new min fval from sgd:  -1562.5230396868833
new min fval from sgd:  -1562.53542369694
new min fval from sgd:  -1562.5784129996691
new min fval from sgd:  -1562.5830676201113
new min fval from sgd:  -1562.5907007611725
new min fval from sgd:  -1562.670593261027
new min fval from sgd:  -1562.7135397123182
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.3009]
objective value function right now is: -1559.987167167264
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.46649]
objective value function right now is: -1562.434930493348
new min fval from sgd:  -1562.7645403717286
new min fval from sgd:  -1562.8300572928674
new min fval from sgd:  -1562.8728185312746
new min fval from sgd:  -1562.8876017327393
new min fval from sgd:  -1562.9171301882425
new min fval from sgd:  -1563.0251879794976
new min fval from sgd:  -1563.090145475242
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.44756]
objective value function right now is: -1562.2422751859467
new min fval from sgd:  -1563.1045250010345
new min fval from sgd:  -1563.1208151955466
new min fval from sgd:  -1563.1437568985866
new min fval from sgd:  -1563.1606252773208
new min fval from sgd:  -1563.1768906483505
new min fval from sgd:  -1563.1864024678346
new min fval from sgd:  -1563.2000139305633
new min fval from sgd:  -1563.2044146572646
new min fval from sgd:  -1563.2053834120563
new min fval from sgd:  -1563.2160274077705
new min fval from sgd:  -1563.2263019127254
new min fval from sgd:  -1563.2317077050382
new min fval from sgd:  -1563.2418000976825
new min fval from sgd:  -1563.256862187526
new min fval from sgd:  -1563.2686230544537
new min fval from sgd:  -1563.2847010121538
new min fval from sgd:  -1563.2955995990062
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.6536]
objective value function right now is: -1562.5583797091404
new min fval from sgd:  -1563.3086642518897
new min fval from sgd:  -1563.3142162521035
new min fval from sgd:  -1563.3202734088165
new min fval from sgd:  -1563.333097204084
new min fval from sgd:  -1563.3493971828116
new min fval from sgd:  -1563.3711035709018
new min fval from sgd:  -1563.3958222051983
new min fval from sgd:  -1563.4128083255164
new min fval from sgd:  -1563.4269099345688
new min fval from sgd:  -1563.4413948647464
new min fval from sgd:  -1563.4437419565986
new min fval from sgd:  -1563.4559991085791
new min fval from sgd:  -1563.461235009627
new min fval from sgd:  -1563.4750235057115
new min fval from sgd:  -1563.510150134092
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.68816]
objective value function right now is: -1563.3731960760122
min fval:  -1563.510150134092
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 1.1583e+01, -7.2068e-01],
        [-6.0927e+00,  9.1522e+00],
        [ 1.3251e+01, -4.7111e+00],
        [-2.4971e+00,  1.2750e+01],
        [-8.2906e-01,  3.6261e-01],
        [-8.2905e-01,  3.6261e-01],
        [-3.8983e+01, -9.5069e+00],
        [ 8.1308e+00, -4.5544e+00],
        [-8.2901e-01,  3.6260e-01],
        [ 1.2560e+01, -3.8531e-02]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -9.0992,   5.7942,  -9.2977,   9.4867,  -3.3264,  -3.3265,  -8.7733,
          3.5416,  -3.3265, -10.6688], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.9434e-02, -2.1574e-01, -1.0431e-01, -5.2750e-01, -1.0707e-02,
         -1.0707e-02, -5.1820e-02, -1.0925e+00, -1.0706e-02,  1.6129e-02],
        [-1.9434e-02, -2.1574e-01, -1.0431e-01, -5.2750e-01, -1.0707e-02,
         -1.0707e-02, -5.1820e-02, -1.0925e+00, -1.0706e-02,  1.6129e-02],
        [-1.9434e-02, -2.1574e-01, -1.0431e-01, -5.2750e-01, -1.0707e-02,
         -1.0707e-02, -5.1820e-02, -1.0925e+00, -1.0706e-02,  1.6129e-02],
        [ 6.5635e+00, -9.5867e+00,  1.1820e+01, -1.5894e+01, -5.6179e-02,
         -5.6188e-02,  1.3016e+01,  3.6331e+00, -5.6227e-02,  1.8969e+01],
        [-1.9434e-02, -2.1574e-01, -1.0431e-01, -5.2750e-01, -1.0707e-02,
         -1.0707e-02, -5.1820e-02, -1.0925e+00, -1.0706e-02,  1.6129e-02],
        [-1.9434e-02, -2.1574e-01, -1.0431e-01, -5.2750e-01, -1.0707e-02,
         -1.0707e-02, -5.1820e-02, -1.0925e+00, -1.0706e-02,  1.6129e-02],
        [-1.9434e-02, -2.1574e-01, -1.0431e-01, -5.2750e-01, -1.0707e-02,
         -1.0707e-02, -5.1820e-02, -1.0925e+00, -1.0706e-02,  1.6129e-02],
        [ 2.7075e-01,  3.3879e+00,  1.8091e+00,  1.5713e+01, -4.5982e-01,
         -4.5982e-01,  3.6711e+00, -3.0930e+00, -4.5981e-01,  2.5939e-02],
        [ 6.0394e+00, -9.5223e+00,  1.4358e+01, -1.4751e+01,  2.9552e-02,
          2.9564e-02,  1.1151e+01,  4.3549e+00,  2.9614e-02,  1.6879e+01],
        [-1.9434e-02, -2.1574e-01, -1.0431e-01, -5.2750e-01, -1.0707e-02,
         -1.0707e-02, -5.1820e-02, -1.0925e+00, -1.0706e-02,  1.6129e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.0969, -1.0969, -1.0969,  2.1569, -1.0969, -1.0969, -1.0969, -1.9337,
         2.7916, -1.0969], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.2422e-02,  1.2422e-02,  1.2422e-02, -1.6289e+01,  1.2422e-02,
          1.2422e-02,  1.2422e-02,  1.2024e+01, -1.1742e+01,  1.2422e-02]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 18.2238,   2.2137],
        [-12.7810,  -5.5935],
        [  1.5486,   4.9927],
        [ -9.1468,  -4.0386],
        [ 12.3162,   5.9307],
        [ 13.6473,  -0.4148],
        [ -8.8297,  12.6586],
        [  6.1712,  14.1276],
        [-17.0981,  -4.8518],
        [ 15.8720,   5.0171]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-12.2023,  -5.5624,  -2.3650,  -6.7630,   4.8326, -14.2905,   9.0882,
         11.5038,   0.5557,  -4.7189], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-6.6384e-01,  1.8333e+00,  1.7885e+00, -4.4698e-02, -3.3624e+00,
          3.2435e+00,  3.5200e-01,  4.1195e+00, -3.5742e-01, -2.3795e+00],
        [ 1.0175e+01,  7.0724e-01,  1.4873e+00,  1.4517e+00, -7.9725e-01,
          4.1884e+00,  1.1967e+01,  1.0993e+01,  7.7935e-01, -4.6951e-01],
        [-1.1109e+00, -2.3004e-01, -1.4138e-01, -9.4828e-03, -3.4298e+00,
         -6.5142e-01, -6.0073e-01, -8.2468e-01, -8.4043e-01, -7.6900e-01],
        [-6.2334e+00,  2.8098e+00,  3.5017e+00,  7.8614e-01, -4.1096e+00,
          4.7810e-01,  4.1639e+00,  3.6566e+00,  4.6165e-01, -1.7508e+00],
        [ 2.5161e+00,  1.0430e+01, -8.9216e-01,  4.6396e+00, -1.4902e+01,
          8.7241e-01,  1.0236e+01, -4.5815e+01,  1.0566e+01, -4.0218e+00],
        [-6.2624e+00,  1.2782e+00,  3.8304e-01, -1.7868e-02, -3.2119e+00,
         -3.7359e+00,  8.4443e+00,  1.3474e+00, -3.8596e-01, -4.9693e+00],
        [-7.6839e+00,  4.4963e+00, -3.1007e-01,  2.4311e+00, -2.6984e+00,
         -7.0072e+00,  8.8493e-01, -3.2043e+01,  1.5456e+01, -6.5418e+00],
        [-1.0965e+00, -2.2088e-01, -1.5368e-01, -8.0970e-03, -3.4898e+00,
         -6.0323e-01, -5.7515e-01, -7.9161e-01, -8.3770e-01, -7.7835e-01],
        [-7.4062e+00,  5.9418e+00, -2.7395e-01,  4.6744e+00,  4.9260e-01,
          7.7654e-01, -1.4933e+01, -7.8774e+00,  3.6584e+00, -6.9727e+00],
        [-1.4266e+00,  2.6078e+00,  4.0541e+00,  2.4405e-01, -4.0600e+00,
          2.0492e+00, -5.8167e+00,  5.0143e+00,  2.3972e+00, -1.1918e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.0126, -6.6504, -3.1633, -4.2255,  2.2568, -6.9262,  1.4702, -3.1770,
        -0.9550, -4.6187], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.8391,   0.2997,   0.1540,   2.5890,  16.4364,  -0.5655,  -6.1548,
           0.1480,  -3.2607,   2.1724],
        [ -1.8389,  -0.5042,  -0.1539,  -2.5889, -16.4367,   0.4187,   6.2301,
          -0.1479,   3.3067,  -2.1726]], device='cuda:0'))])
xi:  [167.68103]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 540.0266859038295
W_T_median: 365.6598727298582
W_T_pctile_5: 168.19249975226938
W_T_CVAR_5_pct: 14.228793381370851
Average q (qsum/M+1):  49.05919228830645
Optimal xi:  [167.68103]
Observed VAR:  365.6598727298582
Expected(across Rb) median(across samples) p_equity:  0.25733122527599334
obj fun:  tensor(-1563.5102, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:198: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
256000
255966
255905
255811
255690
255500
255237
254994
254649
254173
253735
253166
252580
252016
251496
250983
250708
250485
250292
250064
249842
249557
249255
248877
248524
248185
247851
247383
246864
246356
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  9.8056,   0.3702],
        [ -4.3465,   8.7513],
        [ 11.6827,  -4.5826],
        [  0.3149,  11.8025],
        [ -0.7516,   1.0731],
        [ -0.7516,   1.0731],
        [-33.7974,  -9.1431],
        [  5.3946,  -4.5009],
        [ -0.7516,   1.0731],
        [ 10.9840,   1.5848]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -8.5826,   5.8637,  -9.1605,   9.5059,  -2.6000,  -2.6001,  -8.5071,
          1.6323,  -2.6001, -10.2237], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [ 5.2973e+00, -1.0926e+01,  1.1810e+01, -1.5601e+01, -4.0905e-02,
         -4.0919e-02,  1.3139e+01,  2.8679e+00, -4.0979e-02,  1.7610e+01],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-1.6569e-01,  3.5122e+00,  1.2751e+00,  1.6088e+01, -9.9319e-02,
         -9.9316e-02,  1.2755e+01, -4.1160e+00, -9.9300e-02, -6.4273e-02],
        [ 4.1958e+00, -1.0778e+01,  1.2722e+01, -1.4714e+01,  8.3874e-02,
          8.3891e-02,  1.2004e+01,  3.9389e+00,  8.3961e-02,  1.5171e+01],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.7286, -0.7286, -0.7286,  1.5126, -0.7286, -0.7286, -0.7286, -2.8475,
         2.4285, -0.7286], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 8.8609e-03,  8.8609e-03,  8.8609e-03, -1.4825e+01,  8.8609e-03,
          8.8609e-03,  8.8609e-03,  1.2082e+01, -1.0963e+01,  8.8609e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 17.5169,   2.4047],
        [-11.9075,  -5.4174],
        [ -2.9792,  -0.9803],
        [ -9.7518,  -4.0833],
        [ 11.3921,   5.1641],
        [ 13.7315,  -0.0707],
        [ -8.8519,  10.6386],
        [  7.6684,  14.1374],
        [-16.4605,  -4.5943],
        [ 14.0679,   5.0107]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-10.0398,  -4.7415,  -4.9321,  -4.8020,   4.9201, -12.9012,  10.5438,
         11.8370,   0.8090,  -3.9403], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.8323e+00,  4.5980e+00,  3.9185e-01,  3.6615e+00, -2.6056e+00,
         -5.2731e-02, -2.9941e+00,  7.1657e-01,  1.9169e+00, -3.9848e+00],
        [ 9.5454e+00,  1.7276e+00, -6.8431e-02,  9.5238e-03, -2.5478e+00,
          5.6171e+00,  1.4304e+01,  1.7821e+01, -1.7394e+00,  4.0096e+00],
        [-6.9749e-01, -1.6425e-01,  1.4400e-02, -9.4509e-02, -2.5979e+00,
         -2.4345e-01, -1.8580e-01, -1.0674e+00, -4.0907e-01, -8.7679e-01],
        [-1.5408e+00,  1.3537e+00,  1.1014e-01,  6.5129e-01, -2.6476e+00,
         -2.6944e-01, -5.7968e+00,  1.5919e+00,  1.8653e+00, -2.8750e+00],
        [ 1.5086e+00,  9.7123e+00,  5.2281e-01,  4.8078e+00, -1.4468e+01,
          2.2508e+00,  6.0173e-01, -3.3019e+01,  1.0199e+01, -2.1604e+00],
        [-3.3438e+00,  1.9099e+00, -1.5542e-01,  1.1551e+00, -1.7673e+00,
         -7.1039e+00,  1.1353e+01,  2.7938e+00, -3.5859e+00, -3.1004e+00],
        [-5.8361e+00,  4.0316e+00,  4.1967e-01,  1.6997e+00, -2.4723e+00,
         -6.8854e+00, -5.0376e+00, -3.2686e+01,  1.5702e+01, -6.0404e+00],
        [-7.3342e-01, -1.6558e-01,  1.4866e-02, -9.5380e-02, -2.5807e+00,
         -2.4601e-01, -1.7490e-01, -1.0871e+00, -4.0997e-01, -8.5503e-01],
        [-7.4823e+00,  3.6824e+00,  2.7743e-01,  3.3016e+00,  7.7796e-01,
          6.7700e-01, -9.1973e+00, -5.8992e+00,  3.2522e+00, -2.3277e+00],
        [-2.7056e-02,  1.0243e+00,  2.5504e-01,  9.7638e-01, -2.4098e+00,
          2.7460e-01, -9.3762e+00,  2.3750e+00,  2.9668e+00, -3.1138e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.3811, -8.4615, -2.6506, -2.6500,  3.0872, -5.6053,  1.2951, -2.6386,
        -1.2181, -2.5183], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.7230,   0.1490,  -0.2400,   1.4434,  14.4126,   0.3764,  -7.1038,
          -0.2402,  -2.2275,   1.6248],
        [ -1.7230,  -0.3536,   0.2400,  -1.4433, -14.4142,  -0.5248,   7.1825,
           0.2402,   2.2745,  -1.6251]], device='cuda:0'))])
loaded xi:  58.6305
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1481.2359708049823
Current xi:  [72.59037]
objective value function right now is: -1481.2359708049823
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1507.0579560935082
Current xi:  [88.545135]
objective value function right now is: -1507.0579560935082
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1524.6944715033537
Current xi:  [102.05555]
objective value function right now is: -1524.6944715033537
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [114.38199]
objective value function right now is: -1518.3736917652682
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1554.7521069014847
Current xi:  [126.22456]
objective value function right now is: -1554.7521069014847
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1559.7752423012307
Current xi:  [136.7142]
objective value function right now is: -1559.7752423012307
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1572.2084611523878
Current xi:  [145.24933]
objective value function right now is: -1572.2084611523878
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1580.9265020006249
Current xi:  [153.89565]
objective value function right now is: -1580.9265020006249
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [154.44759]
objective value function right now is: -1557.6228209384283
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [159.11942]
objective value function right now is: -1576.1646572482523
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.261]
objective value function right now is: -1574.920536005741
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.48282]
objective value function right now is: -1530.375712937495
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1581.1324979551828
Current xi:  [172.7234]
objective value function right now is: -1581.1324979551828
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [176.48111]
objective value function right now is: -1566.3907716459055
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1585.9573609476186
Current xi:  [177.77025]
objective value function right now is: -1585.9573609476186
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.77354]
objective value function right now is: -1569.1868029959971
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.43362]
objective value function right now is: -1565.892249678749
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.25427]
objective value function right now is: -1583.1899811091841
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.46933]
objective value function right now is: -1579.9514511489717
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.31976]
objective value function right now is: -1582.0816905123527
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1586.009022398477
Current xi:  [184.2145]
objective value function right now is: -1586.009022398477
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.26308]
objective value function right now is: -1572.5241636148687
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1586.4556207412734
Current xi:  [182.98465]
objective value function right now is: -1586.4556207412734
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1590.2809447723118
Current xi:  [181.53578]
objective value function right now is: -1590.2809447723118
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.85388]
objective value function right now is: -1577.93756832973
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.50488]
objective value function right now is: -1582.1568038305363
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [179.41306]
objective value function right now is: -1580.5637671931142
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [177.57564]
objective value function right now is: -1576.1160122518804
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [180.0101]
objective value function right now is: -1582.0651608334854
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.7745]
objective value function right now is: -1578.4429273797846
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.80606]
objective value function right now is: -1586.6163196741202
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.50809]
objective value function right now is: -1584.2357706014866
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.57639]
objective value function right now is: -1574.0240773449111
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.02957]
objective value function right now is: -1586.9397435834228
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [177.6172]
objective value function right now is: -1567.1885363514252
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1590.4187089641225
Current xi:  [178.11932]
objective value function right now is: -1590.4187089641225
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1592.7408166178268
Current xi:  [179.00276]
objective value function right now is: -1592.7408166178268
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [179.29636]
objective value function right now is: -1587.5155428760193
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [179.66423]
objective value function right now is: -1592.5800383579724
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1595.5391676172264
Current xi:  [180.13927]
objective value function right now is: -1595.5391676172264
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.54037]
objective value function right now is: -1593.9819930700628
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.50246]
objective value function right now is: -1594.6293071781245
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.55042]
objective value function right now is: -1592.0123734282502
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.7794]
objective value function right now is: -1595.1790399100694
new min fval from sgd:  -1596.6764634375106
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.25786]
objective value function right now is: -1596.6764634375106
new min fval from sgd:  -1597.0054420186802
new min fval from sgd:  -1597.4843341318592
new min fval from sgd:  -1597.9510999193103
new min fval from sgd:  -1598.1461228587339
new min fval from sgd:  -1598.1910787193608
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.13152]
objective value function right now is: -1597.3675677854694
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.46274]
objective value function right now is: -1592.811615690967
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.75746]
objective value function right now is: -1595.3240497023687
new min fval from sgd:  -1598.2015288287687
new min fval from sgd:  -1598.2759983484561
new min fval from sgd:  -1598.3220811672636
new min fval from sgd:  -1598.350515726696
new min fval from sgd:  -1598.3719019596922
new min fval from sgd:  -1598.3823112909179
new min fval from sgd:  -1598.3927318644292
new min fval from sgd:  -1598.407009827917
new min fval from sgd:  -1598.433773767696
new min fval from sgd:  -1598.4622716470967
new min fval from sgd:  -1598.4781075859848
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.73268]
objective value function right now is: -1598.4781075859848
new min fval from sgd:  -1598.484628331379
new min fval from sgd:  -1598.4871850552393
new min fval from sgd:  -1598.4977540622942
new min fval from sgd:  -1598.4983647403658
new min fval from sgd:  -1598.5092136177955
new min fval from sgd:  -1598.5168290690594
new min fval from sgd:  -1598.5190576325544
new min fval from sgd:  -1598.5311533258193
new min fval from sgd:  -1598.5451803084268
new min fval from sgd:  -1598.5597835133515
new min fval from sgd:  -1598.5793675407374
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.85103]
objective value function right now is: -1597.9148648738512
min fval:  -1598.5793675407374
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 11.4358,  -0.1433],
        [ -7.4529,   9.0897],
        [ 13.6224,  -3.9045],
        [ -4.0808,  12.7678],
        [ -1.3529,   0.8915],
        [ -1.3529,   0.8914],
        [-30.9369,  -9.6183],
        [  7.7662,  -3.8658],
        [ -1.3530,   0.8915],
        [ 12.4371,   0.3869]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -9.3810,   5.2695,  -8.8553,   9.3736,  -4.1638,  -4.1637,  -8.7366,
          3.2087,  -4.1638, -10.8745], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-8.5286e-02, -2.4011e-01,  6.2389e-02, -5.6715e-01,  2.5050e-03,
          2.5048e-03, -5.3167e-02, -1.3029e+00,  2.5052e-03, -1.1930e-01],
        [-8.5286e-02, -2.4011e-01,  6.2389e-02, -5.6715e-01,  2.5050e-03,
          2.5048e-03, -5.3168e-02, -1.3029e+00,  2.5052e-03, -1.1930e-01],
        [-8.5286e-02, -2.4011e-01,  6.2389e-02, -5.6715e-01,  2.5050e-03,
          2.5048e-03, -5.3168e-02, -1.3029e+00,  2.5052e-03, -1.1930e-01],
        [ 6.4698e+00, -1.0045e+01,  1.2555e+01, -1.6005e+01, -3.5421e-01,
         -3.5420e-01,  1.2847e+01,  3.3797e+00, -3.5425e-01,  1.9040e+01],
        [-8.5286e-02, -2.4011e-01,  6.2389e-02, -5.6715e-01,  2.5050e-03,
          2.5048e-03, -5.3167e-02, -1.3029e+00,  2.5052e-03, -1.1930e-01],
        [-8.5286e-02, -2.4011e-01,  6.2389e-02, -5.6715e-01,  2.5050e-03,
          2.5048e-03, -5.3167e-02, -1.3029e+00,  2.5052e-03, -1.1930e-01],
        [-8.5286e-02, -2.4011e-01,  6.2389e-02, -5.6715e-01,  2.5050e-03,
          2.5048e-03, -5.3167e-02, -1.3029e+00,  2.5052e-03, -1.1930e-01],
        [ 1.0645e-01,  1.8524e+00,  2.0517e+00,  1.5109e+01, -2.7418e-01,
         -2.7421e-01, -1.8472e-01, -2.3250e+00, -2.7416e-01,  1.8241e-02],
        [ 5.4864e+00, -1.0222e+01,  1.4011e+01, -1.5086e+01,  2.4829e-01,
          2.4829e-01,  1.1008e+01,  3.9129e+00,  2.4833e-01,  1.6738e+01],
        [-8.5286e-02, -2.4011e-01,  6.2389e-02, -5.6715e-01,  2.5050e-03,
          2.5048e-03, -5.3167e-02, -1.3029e+00,  2.5052e-03, -1.1930e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.3179, -1.3179, -1.3179,  2.0188, -1.3179, -1.3179, -1.3179, -1.2050,
         2.4655, -1.3179], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0174,   0.0174,   0.0174, -16.3984,   0.0174,   0.0174,   0.0174,
          11.9651, -11.6972,   0.0174]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 18.4783,   2.1593],
        [-14.8867,  -4.9657],
        [ -1.6890,  -0.0977],
        [-11.2912,  -3.6966],
        [ 13.9007,   4.7299],
        [ 14.4449,  -0.2313],
        [ -9.5150,  12.2283],
        [  5.8527,  14.1370],
        [-16.5503,  -5.3138],
        [ 15.0020,   4.9640]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-12.1081,  -4.3608,  -6.4109,  -5.8593,   4.5039, -13.9909,   9.0302,
         11.6293,   0.4756,  -4.9212], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.0768e+00, -4.7064e-02, -8.9947e-04, -1.9930e-03, -4.1981e+00,
          7.4535e-02,  7.2481e-02, -1.6666e-01, -9.8350e-01, -1.3330e+00],
        [ 8.3274e+00, -5.7195e+00,  2.8119e-01, -1.7114e+00, -6.9500e-01,
          4.8572e+00,  1.9757e+01,  1.0308e+01, -5.7596e-01, -9.8158e-02],
        [-1.5167e+00,  1.9795e-01, -6.3664e-02,  2.8618e-01, -3.9327e+00,
          8.7915e-01,  1.6892e+00,  8.6263e-01,  1.2573e+00, -6.5410e-01],
        [-1.1502e+01, -2.1542e+00,  7.5989e-02,  2.2464e-01, -3.2988e+00,
         -2.6592e+00, -6.2243e+00,  3.7162e+00,  9.5176e-01, -3.5092e+00],
        [-4.5721e+00,  1.0662e+01, -1.5870e+00,  4.7454e+00, -1.4371e+01,
         -2.9761e+00,  6.9606e+00, -4.6811e+01,  1.1768e+01, -1.5539e+00],
        [-2.6989e+00,  2.5310e+00, -1.2610e-01,  1.8605e+00, -1.3947e+00,
          1.7184e+00,  5.1584e+00,  4.4435e+00,  2.1275e+00, -2.4236e+00],
        [-7.4126e+00,  7.8422e+00,  4.3169e-01,  3.9773e+00, -3.0115e+00,
         -6.6707e+00, -4.7249e-01, -3.2290e+01,  1.6433e+01, -9.7770e+00],
        [-1.1099e+00,  2.1085e-01, -5.9799e-02,  4.4110e-01, -3.7606e+00,
          1.3052e+00,  2.2781e+00,  6.4346e-01,  1.1400e+00, -8.5167e-01],
        [-1.2433e+01,  5.6905e+00, -1.2058e-01,  4.0820e+00,  3.4436e-01,
          1.3875e+00, -1.5200e+01, -7.9468e+00,  4.6794e+00, -7.4864e+00],
        [ 1.0731e-01, -4.3045e-01, -4.6013e-03, -2.4415e-02, -4.3864e+00,
          7.8918e-02,  2.9474e-01,  8.9432e-02,  7.6908e-01, -4.2185e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.6663, -6.9001, -4.2945, -3.7632,  2.6846, -5.5261,  1.3429, -4.1609,
        -1.4394, -4.6779], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.5730,   0.3447,   1.7224,   3.5907,  18.6162,   0.3107,  -6.5814,
           1.8809,  -2.4021,   0.2161],
        [ -0.5730,  -0.5492,  -1.7224,  -3.5906, -18.6158,  -0.4581,   6.6586,
          -1.8809,   2.4486,  -0.2163]], device='cuda:0'))])
xi:  [181.77287]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 603.5282105773097
W_T_median: 388.42656564032785
W_T_pctile_5: 181.97061028546298
W_T_CVAR_5_pct: 20.268741690388445
Average q (qsum/M+1):  48.29797757056452
Optimal xi:  [181.77287]
Observed VAR:  388.42656564032785
Expected(across Rb) median(across samples) p_equity:  0.24775355930129686
obj fun:  tensor(-1598.5794, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:198: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
256000
255970
255918
255837
255715
255549
255308
255076
254736
254304
253892
253322
252743
252201
251613
250905
250380
249809
249199
248644
248033
247401
246765
246151
245527
244961
244347
243641
242909
242101
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  9.8056,   0.3702],
        [ -4.3465,   8.7513],
        [ 11.6827,  -4.5826],
        [  0.3149,  11.8025],
        [ -0.7516,   1.0731],
        [ -0.7516,   1.0731],
        [-33.7974,  -9.1431],
        [  5.3946,  -4.5009],
        [ -0.7516,   1.0731],
        [ 10.9840,   1.5848]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -8.5826,   5.8637,  -9.1605,   9.5059,  -2.6000,  -2.6001,  -8.5071,
          1.6323,  -2.6001, -10.2237], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [ 5.2973e+00, -1.0926e+01,  1.1810e+01, -1.5601e+01, -4.0905e-02,
         -4.0919e-02,  1.3139e+01,  2.8679e+00, -4.0979e-02,  1.7610e+01],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02],
        [-1.6569e-01,  3.5122e+00,  1.2751e+00,  1.6088e+01, -9.9319e-02,
         -9.9316e-02,  1.2755e+01, -4.1160e+00, -9.9300e-02, -6.4273e-02],
        [ 4.1958e+00, -1.0778e+01,  1.2722e+01, -1.4714e+01,  8.3874e-02,
          8.3891e-02,  1.2004e+01,  3.9389e+00,  8.3961e-02,  1.5171e+01],
        [-4.3069e-02, -1.1202e-01, -5.1012e-02, -2.7575e-01, -1.3719e-02,
         -1.3719e-02, -9.8452e-02, -7.2462e-01, -1.3718e-02, -3.4693e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.7286, -0.7286, -0.7286,  1.5126, -0.7286, -0.7286, -0.7286, -2.8475,
         2.4285, -0.7286], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 8.8609e-03,  8.8609e-03,  8.8609e-03, -1.4825e+01,  8.8609e-03,
          8.8609e-03,  8.8609e-03,  1.2082e+01, -1.0963e+01,  8.8609e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 17.5169,   2.4047],
        [-11.9075,  -5.4174],
        [ -2.9792,  -0.9803],
        [ -9.7518,  -4.0833],
        [ 11.3921,   5.1641],
        [ 13.7315,  -0.0707],
        [ -8.8519,  10.6386],
        [  7.6684,  14.1374],
        [-16.4605,  -4.5943],
        [ 14.0679,   5.0107]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-10.0398,  -4.7415,  -4.9321,  -4.8020,   4.9201, -12.9012,  10.5438,
         11.8370,   0.8090,  -3.9403], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.8323e+00,  4.5980e+00,  3.9185e-01,  3.6615e+00, -2.6056e+00,
         -5.2731e-02, -2.9941e+00,  7.1657e-01,  1.9169e+00, -3.9848e+00],
        [ 9.5454e+00,  1.7276e+00, -6.8431e-02,  9.5238e-03, -2.5478e+00,
          5.6171e+00,  1.4304e+01,  1.7821e+01, -1.7394e+00,  4.0096e+00],
        [-6.9749e-01, -1.6425e-01,  1.4400e-02, -9.4509e-02, -2.5979e+00,
         -2.4345e-01, -1.8580e-01, -1.0674e+00, -4.0907e-01, -8.7679e-01],
        [-1.5408e+00,  1.3537e+00,  1.1014e-01,  6.5129e-01, -2.6476e+00,
         -2.6944e-01, -5.7968e+00,  1.5919e+00,  1.8653e+00, -2.8750e+00],
        [ 1.5086e+00,  9.7123e+00,  5.2281e-01,  4.8078e+00, -1.4468e+01,
          2.2508e+00,  6.0173e-01, -3.3019e+01,  1.0199e+01, -2.1604e+00],
        [-3.3438e+00,  1.9099e+00, -1.5542e-01,  1.1551e+00, -1.7673e+00,
         -7.1039e+00,  1.1353e+01,  2.7938e+00, -3.5859e+00, -3.1004e+00],
        [-5.8361e+00,  4.0316e+00,  4.1967e-01,  1.6997e+00, -2.4723e+00,
         -6.8854e+00, -5.0376e+00, -3.2686e+01,  1.5702e+01, -6.0404e+00],
        [-7.3342e-01, -1.6558e-01,  1.4866e-02, -9.5380e-02, -2.5807e+00,
         -2.4601e-01, -1.7490e-01, -1.0871e+00, -4.0997e-01, -8.5503e-01],
        [-7.4823e+00,  3.6824e+00,  2.7743e-01,  3.3016e+00,  7.7796e-01,
          6.7700e-01, -9.1973e+00, -5.8992e+00,  3.2522e+00, -2.3277e+00],
        [-2.7056e-02,  1.0243e+00,  2.5504e-01,  9.7638e-01, -2.4098e+00,
          2.7460e-01, -9.3762e+00,  2.3750e+00,  2.9668e+00, -3.1138e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.3811, -8.4615, -2.6506, -2.6500,  3.0872, -5.6053,  1.2951, -2.6386,
        -1.2181, -2.5183], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  1.7230,   0.1490,  -0.2400,   1.4434,  14.4126,   0.3764,  -7.1038,
          -0.2402,  -2.2275,   1.6248],
        [ -1.7230,  -0.3536,   0.2400,  -1.4433, -14.4142,  -0.5248,   7.1825,
           0.2402,   2.2745,  -1.6251]], device='cuda:0'))])
loaded xi:  58.6305
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -641.2655400836715
Current xi:  [77.27515]
objective value function right now is: -641.2655400836715
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1419.267073005173
Current xi:  [95.81145]
objective value function right now is: -1419.267073005173
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1736.2756221317213
Current xi:  [112.17964]
objective value function right now is: -1736.2756221317213
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1897.7106290190086
Current xi:  [126.2355]
objective value function right now is: -1897.7106290190086
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -2052.601256104735
Current xi:  [139.03395]
objective value function right now is: -2052.601256104735
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -2347.1259266496754
Current xi:  [151.97122]
objective value function right now is: -2347.1259266496754
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [163.3587]
objective value function right now is: -2210.3594564667296
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -2379.837631996058
Current xi:  [171.1934]
objective value function right now is: -2379.837631996058
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -2572.3358495432294
Current xi:  [178.71729]
objective value function right now is: -2572.3358495432294
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.48265]
objective value function right now is: -2485.9618024213564
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -2583.0458998362938
Current xi:  [191.02083]
objective value function right now is: -2583.0458998362938
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.10237]
objective value function right now is: -2566.1321664038874
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -2597.592108460589
Current xi:  [201.43407]
objective value function right now is: -2597.592108460589
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -2599.907230366546
Current xi:  [205.30507]
objective value function right now is: -2599.907230366546
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -2675.5056212021577
Current xi:  [208.35304]
objective value function right now is: -2675.5056212021577
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -2702.2194048465053
Current xi:  [208.98712]
objective value function right now is: -2702.2194048465053
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -2798.170869965232
Current xi:  [209.52704]
objective value function right now is: -2798.170869965232
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.68504]
objective value function right now is: -2510.8200481108215
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.65262]
objective value function right now is: -2753.882385690178
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.09409]
objective value function right now is: -2736.2601692989283
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.63365]
objective value function right now is: -2575.3036462023465
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -2803.2733065000466
Current xi:  [210.1314]
objective value function right now is: -2803.2733065000466
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2809.580225806873
Current xi:  [209.79945]
objective value function right now is: -2809.580225806873
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.98198]
objective value function right now is: -2747.1030589480947
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.2704]
objective value function right now is: -2439.838261882065
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.3431]
objective value function right now is: -1786.2705354556897
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.85106]
objective value function right now is: -2757.3034121711753
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [210.43155]
objective value function right now is: -2759.40740637531
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [211.07974]
objective value function right now is: -2739.179358767349
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.24055]
objective value function right now is: -2703.0476502506854
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.10312]
objective value function right now is: -2609.1022369994253
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.4238]
objective value function right now is: -2709.4313371629164
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.2345]
objective value function right now is: -2596.8697818632254
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.39838]
objective value function right now is: -2643.9455700883896
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.54414]
objective value function right now is: -2722.707074073399
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2818.2812414651016
Current xi:  [208.88275]
objective value function right now is: -2818.2812414651016
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.67552]
objective value function right now is: -2814.795332000932
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.36324]
objective value function right now is: -2606.460832125008
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2832.3512020760913
Current xi:  [210.11143]
objective value function right now is: -2832.3512020760913
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.34674]
objective value function right now is: -2810.538905481648
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.7233]
objective value function right now is: -2830.298384012863
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -2852.2914979602647
Current xi:  [211.7895]
objective value function right now is: -2852.2914979602647
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -2854.0507112491887
Current xi:  [212.21175]
objective value function right now is: -2854.0507112491887
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.85022]
objective value function right now is: -2811.8355466447892
new min fval from sgd:  -2856.8649738590316
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.95146]
objective value function right now is: -2856.8649738590316
new min fval from sgd:  -2857.6825678685036
new min fval from sgd:  -2859.665890354002
new min fval from sgd:  -2860.964832397035
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.7849]
objective value function right now is: -2849.6964497181325
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.86208]
objective value function right now is: -2846.593178375681
new min fval from sgd:  -2860.9770665665096
new min fval from sgd:  -2862.538270373772
new min fval from sgd:  -2864.9894710148183
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.78465]
objective value function right now is: -2845.9693799606584
new min fval from sgd:  -2866.9048382588694
new min fval from sgd:  -2867.158771037047
new min fval from sgd:  -2867.4372365118625
new min fval from sgd:  -2867.4952797216965
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.91818]
objective value function right now is: -2861.965283952971
new min fval from sgd:  -2868.7248508263883
new min fval from sgd:  -2868.7947447215374
new min fval from sgd:  -2869.362745838376
new min fval from sgd:  -2869.425028066078
new min fval from sgd:  -2869.596561581126
new min fval from sgd:  -2869.8078556037103
new min fval from sgd:  -2869.858974061582
new min fval from sgd:  -2870.800893405436
new min fval from sgd:  -2870.910053429305
new min fval from sgd:  -2870.9933085474977
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.78145]
objective value function right now is: -2761.814777388763
min fval:  -2870.9933085474977
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 13.1726,  -1.2082],
        [ -8.4058,   8.6598],
        [ 14.2320,  -5.5290],
        [ -4.5315,  13.3278],
        [ -1.2463,   0.1726],
        [ -1.2463,   0.1726],
        [-32.7721,  -9.4606],
        [  4.5911,  -8.6044],
        [ -1.2463,   0.1726],
        [ 13.3447,  -0.8485]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-7.7718,  5.0457, -8.2605,  9.5039, -5.0709, -5.0709, -9.2912, -1.1768,
        -5.0709, -9.7166], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-4.7682e-01, -8.3677e-01, -5.2921e-02, -1.2984e+00, -1.0301e-03,
         -1.0301e-03, -1.0481e-02, -1.4016e+00, -1.0301e-03, -4.2937e-01],
        [-4.7682e-01, -8.3677e-01, -5.2921e-02, -1.2984e+00, -1.0301e-03,
         -1.0301e-03, -1.0481e-02, -1.4016e+00, -1.0301e-03, -4.2937e-01],
        [-4.7682e-01, -8.3677e-01, -5.2921e-02, -1.2984e+00, -1.0301e-03,
         -1.0301e-03, -1.0481e-02, -1.4016e+00, -1.0301e-03, -4.2937e-01],
        [ 8.4404e+00, -9.0209e+00,  1.4275e+01, -1.4794e+01,  9.9187e-02,
          9.9182e-02,  1.2439e+01,  3.0625e+00,  9.9159e-02,  2.0244e+01],
        [-4.7682e-01, -8.3677e-01, -5.2921e-02, -1.2984e+00, -1.0301e-03,
         -1.0301e-03, -1.0481e-02, -1.4016e+00, -1.0301e-03, -4.2937e-01],
        [-4.7682e-01, -8.3677e-01, -5.2921e-02, -1.2984e+00, -1.0301e-03,
         -1.0301e-03, -1.0481e-02, -1.4016e+00, -1.0301e-03, -4.2937e-01],
        [-4.7682e-01, -8.3677e-01, -5.2921e-02, -1.2984e+00, -1.0301e-03,
         -1.0301e-03, -1.0481e-02, -1.4016e+00, -1.0301e-03, -4.2937e-01],
        [ 2.8176e-01,  3.7909e+00,  3.9409e-02,  1.5569e+01, -3.3889e-01,
         -3.3889e-01,  2.3436e-01, -2.6727e+00, -3.3889e-01,  9.6183e-02],
        [ 8.1248e+00, -8.5565e+00,  1.4806e+01, -1.4411e+01, -1.2388e-02,
         -1.2381e-02,  9.9148e+00,  3.0415e+00, -1.2343e-02,  1.8795e+01],
        [-4.7682e-01, -8.3677e-01, -5.2921e-02, -1.2984e+00, -1.0301e-03,
         -1.0301e-03, -1.0481e-02, -1.4016e+00, -1.0301e-03, -4.2937e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.1361, -2.1361, -2.1361,  3.2211, -2.1361, -2.1361, -2.1361, -1.4746,
         3.1838, -2.1361], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0340,  -0.0340,  -0.0340, -18.2213,  -0.0340,  -0.0340,  -0.0340,
          11.2102, -12.8569,  -0.0340]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 1.8213e+01,  2.4907e+00],
        [-1.4078e+01, -5.5792e+00],
        [-1.8928e+00,  1.2198e+00],
        [-1.0166e+01, -4.4054e+00],
        [ 1.5131e+01,  4.9327e+00],
        [ 1.4475e+01,  3.6286e-03],
        [-3.8762e-01,  1.4179e+01],
        [ 7.0799e+00,  1.4286e+01],
        [-1.6827e+01, -5.5760e+00],
        [ 1.5604e+01,  4.6949e+00]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-11.9199,  -4.8875,  -7.4631,  -7.0576,   4.4502, -13.8861,  10.5362,
         11.4276,   0.3205,  -5.4940], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.5712e+00,  6.2229e+00, -2.9128e-03,  3.6406e+00, -5.0906e+00,
         -3.2376e-01, -1.6138e+00,  1.7494e+00, -8.9364e-01, -7.9859e+00],
        [ 1.0684e+01, -2.8450e-01,  3.3180e-01, -8.3934e-01, -1.0638e+00,
          7.6918e+00,  1.1411e+01,  1.0501e+01, -2.5067e+00, -1.9024e-01],
        [ 3.4482e-01,  5.3388e-01,  4.5481e-01,  1.5145e-02, -2.7046e+00,
         -1.2711e+00, -9.8598e+00,  1.4715e+00,  1.4146e+00,  7.3158e-01],
        [-7.7819e+00,  6.8347e+00,  5.7905e-01,  3.6136e+00, -4.0367e+00,
         -1.1929e+00, -3.3897e-01,  3.5280e+00,  1.2001e+00, -9.1726e+00],
        [-4.0129e+00,  1.1617e+01, -5.8978e-01,  6.5962e+00, -1.4471e+01,
         -1.6783e+00,  4.3713e+00, -5.6404e+01,  1.0974e+01, -7.5331e+00],
        [-3.3479e+00,  7.4532e+00, -1.3062e+00,  4.6786e+00, -1.5836e+00,
         -3.7113e-01,  9.8824e+00,  3.6860e+00, -6.2466e-02, -2.9099e+00],
        [-5.4529e+00,  4.9466e+00,  1.2185e-01,  1.7450e+00, -3.1717e+00,
         -6.2374e+00, -4.1820e+00, -3.1677e+01,  1.8400e+01, -9.3018e+00],
        [-1.3241e-01,  3.7101e-01, -7.2993e-03, -3.7866e-02, -3.6646e+00,
         -6.8023e-01, -8.7326e+00,  2.6210e-01,  1.4978e+00, -1.8048e-01],
        [-1.3095e+01,  9.4772e+00,  6.7469e-02,  6.7740e+00,  7.1896e-01,
         -1.2311e-01, -9.3124e+00, -6.6163e+00,  3.8949e+00, -6.6995e+00],
        [-5.3236e-01, -4.5899e-01,  1.0273e-01, -1.8602e-01, -3.9233e+00,
         -6.4546e-01, -5.6379e+00,  1.6511e+00,  3.4284e-01, -2.0285e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.9975, -7.0426, -2.7625, -3.8323,  2.9917, -5.3881,  0.8002, -3.7249,
        -1.1144, -4.0935], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.6091,   0.3026,  -1.7890,   2.9325,  20.2708,   0.1389,  -6.5261,
          -0.7817,  -2.7727,  -1.3315],
        [ -0.6090,  -0.5072,   1.7890,  -2.9324, -20.2700,  -0.2873,   6.6022,
           0.7817,   2.8193,   1.3313]], device='cuda:0'))])
xi:  [212.97643]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 737.3404337773342
W_T_median: 522.9097043137244
W_T_pctile_5: 214.34395367800946
W_T_CVAR_5_pct: 29.03925144470237
Average q (qsum/M+1):  45.794000441028224
Optimal xi:  [212.97643]
Observed VAR:  522.9097043137244
Expected(across Rb) median(across samples) p_equity:  0.22980172261595727
obj fun:  tensor(-2870.9933, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:198: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
256000
255978
255957
255917
255855
255747
255598
255468
255258
255003
254697
254345
253899
253510
253089
252591
252156
251696
251163
250580
250061
249441
248749
248035
247362
246680
245943
245138
244318
243406
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
