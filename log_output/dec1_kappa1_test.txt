Starting at: 
01-12-22_17:19

 Random seed:  2  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       8  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       8  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1  logistic_sigmoid   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 8)    False        None  
2       (8, 8)    False        None  
3       (8, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       8  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       8  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 8)    False        None  
2       (8, 8)    False        None  
3       (8, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer        8  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer        8  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0       (2, 8)     False        None  
0       (8, 8)     False        None  
0       (8, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer        8  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer        8  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0       (2, 8)     False        None  
0       (8, 8)     False        None  
0       (8, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.762166]
objective value function right now is: -1061.5752739901177
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.684017]
objective value function right now is: -1173.9103918335159
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.747783]
objective value function right now is: -1516.9065376973895
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.244705]
objective value function right now is: -1527.3742088064155
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.4006114]
objective value function right now is: -1536.3543502838006
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.210013]
objective value function right now is: -1540.3266570820088
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [6.287442]
objective value function right now is: -1542.218511156558
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.712013]
objective value function right now is: -1546.8529165014359
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.614531]
objective value function right now is: -1489.4124542078018
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.7238245]
objective value function right now is: -1532.3291873342098
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.274385]
objective value function right now is: -1518.619073405242
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.3877854]
objective value function right now is: -1553.5650157697683
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.469725]
objective value function right now is: -1540.4196245758142
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [7.546516]
objective value function right now is: -1519.136389596182
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.8096013]
objective value function right now is: -1541.9708296528222
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.1583114]
objective value function right now is: -1523.5085509259889
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.6762102]
objective value function right now is: -1478.3040347997623
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.3060904]
objective value function right now is: -1521.5746733505734
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.4366817]
objective value function right now is: -1526.3630452695847
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.229754]
objective value function right now is: -1520.9037727157065
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.3510175]
objective value function right now is: -1521.0947026500787
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.261269]
objective value function right now is: -1531.7191151905172
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.1739783]
objective value function right now is: -1488.1376195099097
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.052536]
objective value function right now is: -1533.23906823831
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.1577425]
objective value function right now is: -1526.1624007075327
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.9726522]
objective value function right now is: -1532.5480308002714
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.120992]
objective value function right now is: -1520.22937756353
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [3.884241]
objective value function right now is: -1523.9095710676745
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [3.9444315]
objective value function right now is: -1518.949038517904
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.8505445]
objective value function right now is: -1505.71215426405
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.1427445]
objective value function right now is: -1537.7051645364497
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.9665427]
objective value function right now is: -1504.7711284277511
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.067092]
objective value function right now is: -1527.952667111862
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.129033]
objective value function right now is: -1522.8124997371608
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.923663]
objective value function right now is: -1519.9816332720416
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.9623396]
objective value function right now is: -1521.054087049388
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.075773]
objective value function right now is: -1518.5134340027626
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.239222]
objective value function right now is: -1537.937941740702
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.2321677]
objective value function right now is: -1531.4060134544477
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.079796]
objective value function right now is: -1517.4969455289058
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.1497574]
objective value function right now is: -1504.2257631491082
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.675529e-06]
objective value function right now is: -1427.8318076516712
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.1984619e-07]
objective value function right now is: -1471.0760655418412
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.0054156e-12]
objective value function right now is: -1526.8870767225899
new min fval:  553.6486017548226
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00782913]
objective value function right now is: -1522.9895325227417
new min fval:  -1524.0910890017412
new min fval:  -1524.7164662628456
new min fval:  -1525.3337761995667
new min fval:  -1526.0977059439115
new min fval:  -1527.0039775000796
new min fval:  -1527.965097423545
new min fval:  -1529.0167180569317
new min fval:  -1530.0400942325416
new min fval:  -1531.054049422495
new min fval:  -1531.9825003242586
new min fval:  -1532.712176275531
new min fval:  -1533.2730723101386
new min fval:  -1533.7192641993431
new min fval:  -1534.080828061743
new min fval:  -1534.4152834310298
new min fval:  -1534.7784761022901
new min fval:  -1535.1759812112143
new min fval:  -1535.517354749212
new min fval:  -1535.894803823622
new min fval:  -1536.0792582618944
new min fval:  -1536.129803767166
new min fval:  -1536.181394975259
new min fval:  -1536.282659803238
new min fval:  -1536.3836977553105
new min fval:  -1536.5018926529654
new min fval:  -1536.6287592308227
new min fval:  -1536.8142503111023
new min fval:  -1536.9615125976914
new min fval:  -1537.051871525296
new min fval:  -1537.0635298649697
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.6553452]
objective value function right now is: -1520.0381338774735
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.660808]
objective value function right now is: -1522.4591242290546
new min fval:  -1537.0636864434555
new min fval:  -1537.089829374228
new min fval:  -1537.1154637969555
new min fval:  -1537.1386096203444
new min fval:  -1537.1626999254952
new min fval:  -1537.1829496139246
new min fval:  -1537.201245939231
new min fval:  -1537.2204932222523
new min fval:  -1537.2405410236424
new min fval:  -1537.2595744561654
new min fval:  -1537.2811818865887
new min fval:  -1537.2984148999365
new min fval:  -1537.3130245430918
new min fval:  -1537.3258656807902
new min fval:  -1537.33794867579
new min fval:  -1537.3494951829825
new min fval:  -1537.3616784134017
new min fval:  -1537.3750501249158
new min fval:  -1537.3913482889225
new min fval:  -1537.4102531377005
new min fval:  -1537.4321718688793
new min fval:  -1537.4597998444988
new min fval:  -1537.4796075715612
new min fval:  -1537.4902475862498
new min fval:  -1537.4943610945913
new min fval:  -1537.4976540088385
new min fval:  -1537.4996508635331
new min fval:  -1537.509925198345
new min fval:  -1537.528359743568
new min fval:  -1537.5473650539661
new min fval:  -1537.5650506738546
new min fval:  -1537.5839043640024
new min fval:  -1537.6017062805358
new min fval:  -1537.6186697595338
new min fval:  -1537.6237874055016
new min fval:  -1537.6307179024905
new min fval:  -1537.6535101456614
new min fval:  -1537.665638371129
new min fval:  -1537.6859937309055
new min fval:  -1537.6992184479884
new min fval:  -1537.7100116788404
new min fval:  -1537.718500603296
new min fval:  -1537.7252038202914
new min fval:  -1537.7307690710886
new min fval:  -1537.736392998983
new min fval:  -1537.7426768300077
new min fval:  -1537.7479097190173
new min fval:  -1537.7539141859813
new min fval:  -1537.7551526959503
new min fval:  -1537.764809168921
new min fval:  -1537.7823782192977
new min fval:  -1537.8016651148037
new min fval:  -1537.8269225267586
new min fval:  -1537.8520603350523
new min fval:  -1537.875130658125
new min fval:  -1537.930030298701
new min fval:  -1537.9556579213422
new min fval:  -1537.9697594580728
new min fval:  -1537.983533057385
new min fval:  -1537.997827928827
new min fval:  -1538.0129858926298
new min fval:  -1538.0299603582505
new min fval:  -1538.0501926445042
new min fval:  -1538.0665549409744
new min fval:  -1538.0795840299734
new min fval:  -1538.0903000679064
new min fval:  -1538.0995781157003
new min fval:  -1538.1080976491905
new min fval:  -1538.1162439009747
new min fval:  -1538.124954069952
new min fval:  -1538.1342135820837
new min fval:  -1538.1445194899254
new min fval:  -1538.1567149640487
new min fval:  -1538.1709648667459
new min fval:  -1538.1878670932595
new min fval:  -1538.207871907563
new min fval:  -1538.2310326631405
new min fval:  -1538.2472428367603
new min fval:  -1538.2578659637343
new min fval:  -1538.2636345181006
new min fval:  -1538.2652810719264
new min fval:  -1538.2793443541598
new min fval:  -1538.290758281107
new min fval:  -1538.3083042292492
new min fval:  -1538.3289165389706
new min fval:  -1538.3467918613867
new min fval:  -1538.3657750847365
new min fval:  -1538.3856545166443
new min fval:  -1538.406044093011
new min fval:  -1538.4255948932664
new min fval:  -1538.4464066655846
new min fval:  -1538.4676831039228
new min fval:  -1538.481619515084
new min fval:  -1538.4904877173378
new min fval:  -1538.4957158055072
new min fval:  -1538.4985635010264
new min fval:  -1538.500531422514
new min fval:  -1538.5028084801484
new min fval:  -1538.5063517632782
new min fval:  -1538.5114701621237
new min fval:  -1538.5180763818723
new min fval:  -1538.5259832817353
new min fval:  -1538.536298016443
new min fval:  -1538.549056271032
new min fval:  -1538.563632003125
new min fval:  -1538.5798056726908
new min fval:  -1538.5909477880398
new min fval:  -1538.5982115655058
new min fval:  -1538.6017421205165
new min fval:  -1538.6063223261272
new min fval:  -1538.6092396940621
new min fval:  -1538.6104269772502
new min fval:  -1538.6124357631472
new min fval:  -1538.61721968834
new min fval:  -1538.62411295112
new min fval:  -1538.6329876136397
new min fval:  -1538.6443331895607
new min fval:  -1538.6579470098416
new min fval:  -1538.6738972637615
new min fval:  -1538.6923692138719
new min fval:  -1538.7119173419392
new min fval:  -1538.7310317112406
new min fval:  -1538.7495728908327
new min fval:  -1538.7672282298934
new min fval:  -1538.7842849679887
new min fval:  -1538.8006076023564
new min fval:  -1538.8172894214872
new min fval:  -1538.8347966445021
new min fval:  -1538.846892609112
new min fval:  -1538.8558516950602
new min fval:  -1538.8618483520636
new min fval:  -1538.8650230160358
new min fval:  -1538.8750891147859
new min fval:  -1538.894780240953
new min fval:  -1538.910473255061
new min fval:  -1538.9223990346143
new min fval:  -1538.9312811606446
new min fval:  -1538.937541592939
new min fval:  -1538.9421916374517
new min fval:  -1538.9488862256967
new min fval:  -1538.9557321603359
new min fval:  -1538.9614244687505
new min fval:  -1538.967180338694
new min fval:  -1538.9730961539433
new min fval:  -1538.9810708439318
new min fval:  -1538.9908203921038
new min fval:  -1539.0040460598132
new min fval:  -1539.0205332814432
new min fval:  -1539.039524080102
new min fval:  -1539.059268007494
new min fval:  -1539.066244511453
new min fval:  -1539.0677056061934
new min fval:  -1539.0760686138772
new min fval:  -1539.0895673398893
new min fval:  -1539.0925687392207
new min fval:  -1539.0991589265252
new min fval:  -1539.1029788827373
new min fval:  -1539.1052609971337
new min fval:  -1539.106970343043
new min fval:  -1539.1169735083333
new min fval:  -1539.1176803820422
new min fval:  -1539.125342733848
new min fval:  -1539.138152838586
new min fval:  -1539.153770439507
new min fval:  -1539.1719649169802
new min fval:  -1539.1876320251945
new min fval:  -1539.200720714352
new min fval:  -1539.2117582463632
new min fval:  -1539.2219124039063
new min fval:  -1539.2315975040808
new min fval:  -1539.2414624534308
new min fval:  -1539.2520000508357
new min fval:  -1539.2637450295506
new min fval:  -1539.2758570265096
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.637026]
objective value function right now is: -1555.0169325412928
new min fval:  -1539.2837469054941
new min fval:  -1539.3007296068458
new min fval:  -1539.313006068456
new min fval:  -1539.3216884447895
new min fval:  -1539.3271922811427
new min fval:  -1539.3302943948042
new min fval:  -1539.3316432213273
new min fval:  -1539.331920312682
new min fval:  -1539.3330856299242
new min fval:  -1539.3359816127831
new min fval:  -1539.3409989400366
new min fval:  -1539.3488486283381
new min fval:  -1539.3608282633245
new min fval:  -1539.377041646108
new min fval:  -1539.3942482912125
new min fval:  -1539.413440114235
new min fval:  -1539.4341264968655
new min fval:  -1539.4545755343238
new min fval:  -1539.4735327276662
new min fval:  -1539.4908028900572
new min fval:  -1539.5066110797352
new min fval:  -1539.5216125196741
new min fval:  -1539.5362826254825
new min fval:  -1539.551058325048
new min fval:  -1539.5670769199248
new min fval:  -1539.5843517120509
new min fval:  -1539.5968408415142
new min fval:  -1539.6057630230216
new min fval:  -1539.6118334291837
new min fval:  -1539.615829207478
new min fval:  -1539.6196010082097
new min fval:  -1539.6252264853006
new min fval:  -1539.632280121913
new min fval:  -1539.639598131334
new min fval:  -1539.6480779578196
new min fval:  -1539.6585164050593
new min fval:  -1539.66456046868
new min fval:  -1539.681101304831
new min fval:  -1539.6926543579627
new min fval:  -1539.7500987194228
new min fval:  -1539.7701308859778
new min fval:  -1539.7811326393917
new min fval:  -1539.7865129036538
new min fval:  -1539.7867467358053
new min fval:  -1539.795807651642
new min fval:  -1539.8103443654013
new min fval:  -1539.8213047558359
new min fval:  -1539.8281143109411
new min fval:  -1539.8310195065524
new min fval:  -1539.8312088248156
new min fval:  -1539.84001161712
new min fval:  -1539.8548026910569
new min fval:  -1539.8694728536886
new min fval:  -1539.8831191147422
new min fval:  -1539.8959613699922
new min fval:  -1539.908059183664
new min fval:  -1539.9201248848133
new min fval:  -1539.9325784483433
new min fval:  -1539.9460153844734
new min fval:  -1539.9608702814257
new min fval:  -1539.9754092505468
new min fval:  -1539.989500585867
new min fval:  -1540.0033514230768
new min fval:  -1540.0176473322726
new min fval:  -1540.0313954369235
new min fval:  -1540.044896450387
new min fval:  -1540.0576873344871
new min fval:  -1540.070443800723
new min fval:  -1540.0849955679994
new min fval:  -1540.0953650693036
new min fval:  -1540.1016202221053
new min fval:  -1540.104435407248
new min fval:  -1540.1050145561514
new min fval:  -1540.1067786060892
new min fval:  -1540.1149685582172
new min fval:  -1540.125182544095
new min fval:  -1540.137623592788
new min fval:  -1540.1522916844003
new min fval:  -1540.168060550627
new min fval:  -1540.1835847691473
new min fval:  -1540.1978222727548
new min fval:  -1540.2111477920157
new min fval:  -1540.224328749172
new min fval:  -1540.2376573475158
new min fval:  -1540.2514572584835
new min fval:  -1540.2657466260614
new min fval:  -1540.2789047069546
new min fval:  -1540.2913584843889
new min fval:  -1540.3044302416415
new min fval:  -1540.319025383431
new min fval:  -1540.3348630443654
new min fval:  -1540.3501738974194
new min fval:  -1540.3603616686764
new min fval:  -1540.3675114394734
new min fval:  -1540.3719003223714
new min fval:  -1540.374429924949
new min fval:  -1540.3758872277895
new min fval:  -1540.3775036305656
new min fval:  -1540.379973196363
new min fval:  -1540.3837408910306
new min fval:  -1540.3892083535065
new min fval:  -1540.3967786476776
new min fval:  -1540.4068844298192
new min fval:  -1540.4193637054955
new min fval:  -1540.43396900394
new min fval:  -1540.450666042217
new min fval:  -1540.467776896656
new min fval:  -1540.4830876391575
new min fval:  -1540.4969527405146
new min fval:  -1540.5097226283517
new min fval:  -1540.5217323217219
new min fval:  -1540.533744725069
new min fval:  -1540.5464608297846
new min fval:  -1540.5601790550343
new min fval:  -1540.5755720760726
new min fval:  -1540.5922341360692
new min fval:  -1540.6039077155228
new min fval:  -1540.611035083388
new min fval:  -1540.6146554185332
new min fval:  -1540.6155387079216
new min fval:  -1540.62204703757
new min fval:  -1540.6312339693839
new min fval:  -1540.6426824155128
new min fval:  -1540.6561929990426
new min fval:  -1540.671289602222
new min fval:  -1540.682548148884
new min fval:  -1540.6909007855454
new min fval:  -1540.696829339644
new min fval:  -1540.7009646067852
new min fval:  -1540.703792458556
new min fval:  -1540.7062231129862
new min fval:  -1540.7087562065326
new min fval:  -1540.712247250001
new min fval:  -1540.7171048749847
new min fval:  -1540.723405552687
new min fval:  -1540.7311826415237
new min fval:  -1540.7410426127465
new min fval:  -1540.7535000279358
new min fval:  -1540.7681315032821
new min fval:  -1540.7784257920016
new min fval:  -1540.785284166282
new min fval:  -1540.789673485827
new min fval:  -1540.7915810489424
new min fval:  -1540.792135124196
new min fval:  -1540.7932713432363
new min fval:  -1540.7957472167775
new min fval:  -1540.7998074103914
new min fval:  -1540.8058407790793
new min fval:  -1540.814329419572
new min fval:  -1540.8255250636264
new min fval:  -1540.8395702456514
new min fval:  -1540.8549949115634
new min fval:  -1540.8701326341622
new min fval:  -1540.88393639734
new min fval:  -1540.8965006453632
new min fval:  -1540.9081504799574
new min fval:  -1540.9195072688592
new min fval:  -1540.9313188886058
new min fval:  -1540.9436114633336
new min fval:  -1540.9557235509703
new min fval:  -1540.9687430527001
new min fval:  -1540.977953918628
new min fval:  -1540.9865543730652
new min fval:  -1540.9943996105064
new min fval:  -1541.0015835319184
new min fval:  -1541.008615129163
new min fval:  -1541.0160452143282
new min fval:  -1541.0241954464186
new min fval:  -1541.0337582017585
new min fval:  -1541.0451084862823
new min fval:  -1541.058212765571
new min fval:  -1541.0732234116103
new min fval:  -1541.085167848413
new min fval:  -1541.094578502295
new min fval:  -1541.1021304756432
new min fval:  -1541.1087031329437
new min fval:  -1541.1151555225003
new min fval:  -1541.1221732452348
new min fval:  -1541.1301651246933
new min fval:  -1541.135315025968
new min fval:  -1541.1429458165053
new min fval:  -1541.1536200771466
new min fval:  -1541.1651497064447
new min fval:  -1541.1794740842479
new min fval:  -1541.1921212255393
new min fval:  -1541.1997488151771
new min fval:  -1541.2005338306549
new min fval:  -1541.2042934773576
new min fval:  -1541.2194542415868
new min fval:  -1541.2243069454948
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.699465]
objective value function right now is: -1517.976821218812
new min fval:  -1541.2245673855414
new min fval:  -1541.238058045295
new min fval:  -1541.2529664735657
new min fval:  -1541.2671698647123
new min fval:  -1541.2774825213978
new min fval:  -1541.2834535064555
new min fval:  -1541.2907013014483
new min fval:  -1541.297685721607
new min fval:  -1541.3041981307408
new min fval:  -1541.3096757927726
new min fval:  -1541.3148959261594
new min fval:  -1541.321304088583
new min fval:  -1541.3289310139762
new min fval:  -1541.3431210305682
new min fval:  -1541.3547151310563
new min fval:  -1541.3676634257831
new min fval:  -1541.3819988336097
new min fval:  -1541.3920924109407
new min fval:  -1541.3985566579256
new min fval:  -1541.4019428239085
new min fval:  -1541.4049616112566
new min fval:  -1541.407358206502
new min fval:  -1541.4078137807362
new min fval:  -1541.408864605618
new min fval:  -1541.4142256134414
new min fval:  -1541.421749464893
new min fval:  -1541.4301916688173
new min fval:  -1541.4386233861499
new min fval:  -1541.4476353617733
new min fval:  -1541.4567174919594
new min fval:  -1541.466454968147
new min fval:  -1541.4809907565966
new min fval:  -1541.4968586988225
new min fval:  -1541.5113058427885
new min fval:  -1541.5247592630624
new min fval:  -1541.5385023966428
new min fval:  -1541.555998610795
new min fval:  -1541.5812230051267
new min fval:  -1541.6077375836142
new min fval:  -1541.6350257398894
new min fval:  -1541.6688040882343
new min fval:  -1541.697285653771
new min fval:  -1541.7240862307967
new min fval:  -1541.750942918913
new min fval:  -1541.7708195859461
new min fval:  -1541.7878173462605
new min fval:  -1541.80345177679
new min fval:  -1541.8147579376878
new min fval:  -1541.8244765472325
new min fval:  -1541.8359277650393
new min fval:  -1541.8498280959388
new min fval:  -1541.8671869828117
new min fval:  -1541.8865646103282
new min fval:  -1541.9070939727576
new min fval:  -1541.9240162973542
new min fval:  -1541.9380822775443
new min fval:  -1541.9448764019016
new min fval:  -1541.9519691499454
new min fval:  -1541.9580766724528
new min fval:  -1541.9792314114516
new min fval:  -1541.9803288424666
new min fval:  -1541.9909352408936
new min fval:  -1542.006450128502
new min fval:  -1542.0225816708983
new min fval:  -1542.0380412262768
new min fval:  -1542.0520250191864
new min fval:  -1542.0651231295592
new min fval:  -1542.0786523422562
new min fval:  -1542.1219566378259
new min fval:  -1542.1407694208706
new min fval:  -1542.1626322903662
new min fval:  -1542.1682508270833
new min fval:  -1542.1720012423993
new min fval:  -1542.174083689587
new min fval:  -1542.1787694714703
new min fval:  -1542.1858576744362
new min fval:  -1542.193617769795
new min fval:  -1542.2010999307383
new min fval:  -1542.2083628077824
new min fval:  -1542.215183834489
new min fval:  -1542.2188200063763
new min fval:  -1542.2192443748881
new min fval:  -1542.220893245875
new min fval:  -1542.2414801943246
new min fval:  -1542.2462155056148
new min fval:  -1542.24889752001
new min fval:  -1542.2872188318438
new min fval:  -1542.3055669610912
new min fval:  -1542.3058493633587
new min fval:  -1542.3300104176508
new min fval:  -1542.3472728627903
new min fval:  -1542.3582870498049
new min fval:  -1542.3636229769947
new min fval:  -1542.3671123354254
new min fval:  -1542.3694587983093
new min fval:  -1542.3710076106886
new min fval:  -1542.3721066265086
new min fval:  -1542.3725705439442
new min fval:  -1542.3726339337989
new min fval:  -1542.3825774159382
new min fval:  -1542.3943992457785
new min fval:  -1542.4017039121977
new min fval:  -1542.4045018273778
new min fval:  -1542.4107652516245
new min fval:  -1542.4203881874275
new min fval:  -1542.4300836358086
new min fval:  -1542.4401767481054
new min fval:  -1542.4514632885334
new min fval:  -1542.461592011936
new min fval:  -1542.47133774035
new min fval:  -1542.481432612338
new min fval:  -1542.4915544865974
new min fval:  -1542.5025304721914
new min fval:  -1542.5144208062184
new min fval:  -1542.5228480846142
new min fval:  -1542.528803595734
new min fval:  -1542.5326510458933
new min fval:  -1542.5353493886716
new min fval:  -1542.537416049753
new min fval:  -1542.5393372974438
new min fval:  -1542.5414450549945
new min fval:  -1542.544166704343
new min fval:  -1542.547750368198
new min fval:  -1542.5522654053586
new min fval:  -1542.5582998044124
new min fval:  -1542.5656538896128
new min fval:  -1542.5746278900408
new min fval:  -1542.585269213702
new min fval:  -1542.5976627081
new min fval:  -1542.6110731718443
new min fval:  -1542.6210260869711
new min fval:  -1542.6284068163002
new min fval:  -1542.6332248712326
new min fval:  -1542.6361920514466
new min fval:  -1542.637707210827
new min fval:  -1542.637944269234
new min fval:  -1542.638555272599
new min fval:  -1542.642443535645
new min fval:  -1542.6479822456176
new min fval:  -1542.6552886578718
new min fval:  -1542.6643399125583
new min fval:  -1542.6753131208047
new min fval:  -1542.6874971215188
new min fval:  -1542.6957212959705
new min fval:  -1542.7001927835388
new min fval:  -1542.7017219239651
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.0412416]
objective value function right now is: -1537.2181590841358
min fval:  -1542.7017219239651
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 668.9250943278287
W_T_median: 357.75831748720486
W_T_pctile_5: 100.47764941369456
W_T_CVAR_5_pct: -23.17187932521914
Average q (qsum/M+1):  51.831034998739916
Optimal xi:  [-5.795943]
Expected(across Rb) median(across samples) p_equity:  0.3371462032198906
obj fun:  tensor(-1542.7017, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
