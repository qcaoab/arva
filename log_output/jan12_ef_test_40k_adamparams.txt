Starting at: 
12-01-23_23:57

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 40000
batchsize: 1000


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1  logistic_sigmoid   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 4000, 'itbound_SGD_algorithms': 40000, 'nit_IterateAveragingStart': 36000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1703.5870731186094
Current xi:  [-34.227814]
objective value function right now is: -1703.5870731186094
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1713.6333101955977
Current xi:  [-71.93028]
objective value function right now is: -1713.6333101955977
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1724.2110871575014
Current xi:  [-109.39814]
objective value function right now is: -1724.2110871575014
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1732.0443259070073
Current xi:  [-146.9624]
objective value function right now is: -1732.0443259070073
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1738.8482068235935
Current xi:  [-184.55623]
objective value function right now is: -1738.8482068235935
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1746.1731098890496
Current xi:  [-222.09602]
objective value function right now is: -1746.1731098890496
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1751.4723750096307
Current xi:  [-259.0872]
objective value function right now is: -1751.4723750096307
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1757.8963613293745
Current xi:  [-295.80722]
objective value function right now is: -1757.8963613293745
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1763.577960946055
Current xi:  [-332.34476]
objective value function right now is: -1763.577960946055
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1768.9977168825728
Current xi:  [-368.6742]
objective value function right now is: -1768.9977168825728
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1773.3460478696402
Current xi:  [-404.2797]
objective value function right now is: -1773.3460478696402
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1777.8872989083372
Current xi:  [-439.65796]
objective value function right now is: -1777.8872989083372
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1780.7060418229537
Current xi:  [-474.8452]
objective value function right now is: -1780.7060418229537
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1784.7299915648052
Current xi:  [-509.6635]
objective value function right now is: -1784.7299915648052
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1787.739885565822
Current xi:  [-543.9999]
objective value function right now is: -1787.739885565822
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1790.7296384984272
Current xi:  [-578.2386]
objective value function right now is: -1790.7296384984272
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1792.8781185669875
Current xi:  [-611.6714]
objective value function right now is: -1792.8781185669875
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1794.8437791725312
Current xi:  [-644.8306]
objective value function right now is: -1794.8437791725312
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1796.5090186667437
Current xi:  [-677.762]
objective value function right now is: -1796.5090186667437
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1798.1060986048153
Current xi:  [-709.41724]
objective value function right now is: -1798.1060986048153
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1799.4582739007305
Current xi:  [-740.2015]
objective value function right now is: -1799.4582739007305
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1800.0057453175154
Current xi:  [-769.65106]
objective value function right now is: -1800.0057453175154
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1801.1250837705522
Current xi:  [-797.96967]
objective value function right now is: -1801.1250837705522
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1801.6426473899842
Current xi:  [-824.7925]
objective value function right now is: -1801.6426473899842
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.1873429881218
Current xi:  [-848.9756]
objective value function right now is: -1802.1873429881218
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.4909717503517
Current xi:  [-871.07324]
objective value function right now is: -1802.4909717503517
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.575380462166
Current xi:  [-889.3411]
objective value function right now is: -1802.575380462166
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1802.6019881081781
Current xi:  [-903.5789]
objective value function right now is: -1802.6019881081781
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-912.7216]
objective value function right now is: -1802.318112698649
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-918.99475]
objective value function right now is: -1802.3988492022593
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-921.90027]
objective value function right now is: -1802.490789129423
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-923.49976]
objective value function right now is: -1800.8653399833677
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-926.03326]
objective value function right now is: -1802.4703815196194
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-927.12585]
objective value function right now is: -1802.4793090293242
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-926.6664]
objective value function right now is: -1802.4621000042805
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.7485500195276
Current xi:  [-925.85425]
objective value function right now is: -1802.7485500195276
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.7547399220343
Current xi:  [-925.0946]
objective value function right now is: -1802.7547399220343
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-924.22235]
objective value function right now is: -1802.741427360564
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-923.7522]
objective value function right now is: -1802.6828699943921
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-923.1209]
objective value function right now is: -1802.74643972648
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-922.00336]
objective value function right now is: -1802.741270369409
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-921.4442]
objective value function right now is: -1802.4460253773477
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-921.6108]
objective value function right now is: -1802.7164078610977
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.7642058669367
Current xi:  [-921.40515]
objective value function right now is: -1802.7642058669367
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-920.44617]
objective value function right now is: -1802.7115763260956
new min fval from sgd:  -1802.7873396813968
new min fval from sgd:  -1802.7989861128872
new min fval from sgd:  -1802.8015639287203
new min fval from sgd:  -1802.8031354105826
new min fval from sgd:  -1802.8059232717774
new min fval from sgd:  -1802.810151040065
new min fval from sgd:  -1802.8135883901957
new min fval from sgd:  -1802.8177767421782
new min fval from sgd:  -1802.8206161173648
new min fval from sgd:  -1802.8211978100856
new min fval from sgd:  -1802.822489729392
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-920.8037]
objective value function right now is: -1802.7545981843668
new min fval from sgd:  -1802.8238856980108
new min fval from sgd:  -1802.828057007873
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-920.49365]
objective value function right now is: -1802.7875777975748
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-919.92413]
objective value function right now is: -1802.7872327854657
new min fval from sgd:  -1802.828246700066
new min fval from sgd:  -1802.828320172743
new min fval from sgd:  -1802.828401659343
new min fval from sgd:  -1802.8285292033295
new min fval from sgd:  -1802.8288661008576
new min fval from sgd:  -1802.8293574969177
new min fval from sgd:  -1802.8297579557209
new min fval from sgd:  -1802.8297967911453
new min fval from sgd:  -1802.8301114979015
new min fval from sgd:  -1802.830225109727
new min fval from sgd:  -1802.8313181097349
new min fval from sgd:  -1802.831682501346
new min fval from sgd:  -1802.8321007364523
new min fval from sgd:  -1802.832307809599
new min fval from sgd:  -1802.8324323612667
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-919.53955]
objective value function right now is: -1802.8197724913066
new min fval from sgd:  -1802.8326289220422
new min fval from sgd:  -1802.8329209489552
new min fval from sgd:  -1802.8331061873785
new min fval from sgd:  -1802.8332836758218
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-919.49133]
objective value function right now is: -1802.8093658425748
min fval:  -1802.8332836758218
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 8.1718,  3.3722],
        [ 8.0037,  3.3163],
        [-3.7619,  0.1500],
        [-3.5474,  0.2173],
        [-3.7004,  0.1691],
        [-3.2910,  0.2983],
        [ 2.7678, -0.4160],
        [-3.5478,  0.2173],
        [ 2.6216, -0.4236],
        [ 2.9461, -0.3764]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 6.9522,  6.6752,  4.4590,  4.3431,  4.4248,  4.2050, -2.0967,  4.3480,
         -1.7135, -2.3711],
        [-0.0426, -0.0424, -0.0149, -0.0149, -0.0149, -0.0149, -0.1835, -0.0149,
         -0.1816, -0.1843],
        [-2.3494, -2.4368, -8.7711, -8.7086, -8.7721, -8.5588,  2.1852, -8.7006,
          2.4364,  2.2921],
        [ 6.2709,  6.0967,  3.7708,  3.6818,  3.7467,  3.5741, -1.8205,  3.6848,
         -1.3614, -2.3291],
        [ 4.4082,  4.2181,  1.4708,  1.4335,  1.4602,  1.3888, -1.3666,  1.4338,
         -0.8153, -1.4945],
        [-0.0426, -0.0424, -0.0149, -0.0149, -0.0149, -0.0149, -0.1835, -0.0149,
         -0.1816, -0.1843],
        [-0.2597, -0.2506, -0.0582, -0.0592, -0.0584, -0.0603,  0.9275, -0.0592,
          0.9147,  0.9328],
        [-0.0426, -0.0424, -0.0149, -0.0149, -0.0149, -0.0149, -0.1835, -0.0149,
         -0.1816, -0.1843],
        [-0.0426, -0.0424, -0.0149, -0.0149, -0.0149, -0.0149, -0.1835, -0.0149,
         -0.1816, -0.1843],
        [-0.0426, -0.0424, -0.0149, -0.0149, -0.0149, -0.0149, -0.1835, -0.0149,
         -0.1816, -0.1843]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.2408e+01, -5.0728e-03, -1.7667e+01,  1.0700e+01,  5.7178e+00,
         -5.0728e-03, -1.8561e+00, -5.0728e-03, -5.0728e-03, -5.0728e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-2.8026,  0.0293],
        [-1.6517,  0.6289],
        [14.9186,  4.9147],
        [-0.9404,  1.0899],
        [ 6.3019,  3.6701],
        [-2.6609,  0.0443],
        [-6.6456, -0.6529],
        [-1.5322,  1.0043],
        [ 1.2236,  0.2732],
        [-7.6681, -3.1543]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -1.5110,  -1.2990,  -3.2247,  -0.9317,  -3.1045,  -1.7007,  -5.9886,
          -0.8471,  -0.2416,   1.1658],
        [ -0.1234,  -0.7395,  -5.2848,  -0.9769,  -3.2188,  -0.1190,  -7.8977,
          -0.8500,   0.5027,   3.7583],
        [ -1.9028,  -2.8711,  -1.2891,  -4.9293,  -6.1615,   2.9033,  -7.9377,
          -5.1059,   0.0713,   2.5323],
        [ -0.9497,  -0.8346,  -2.3559,  -0.6386,  -1.3917,  -0.8778,   2.3658,
          -0.5617,  -2.2225,   1.2098],
        [ -0.9455,  -0.8314,  -2.3547,  -0.6368,  -1.3895,  -0.8717,   2.3592,
          -0.5599,  -2.2174,   1.2020],
        [ -1.9151,  -1.5236,  -3.5691,  -1.0202,  -3.4052,  -2.3838,  -6.4919,
          -0.9519,   0.2832,   1.3830],
        [  2.8037,   1.7526,  -2.7001,   0.8577,   0.8176,  11.6438,   2.8577,
           0.7092,  -1.9460,   0.6351],
        [ -9.7652,  -8.7233,   1.9310,  -7.1149, -10.6788, -13.9119,  -6.0401,
          -5.6173,   0.4814,   1.2512],
        [  0.9302,  -1.8288, -18.0217,  -3.0389,  -5.0160,   3.0538, -10.0715,
          -2.6665,   2.2219,   5.1464],
        [ -1.0803,  -0.4028,  -1.8757,   0.3270,   0.2733,  -3.7745,  -0.7382,
           0.0537,   0.4372,  -4.5408]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-3.1878, -2.6667, -6.9322, -0.8174, -0.8115, -4.4613,  2.0861,  6.6060,
          9.8895,  1.4874],
        [ 3.1949,  2.6532,  6.9178,  0.8176,  0.8116,  4.4619, -2.1775, -6.7596,
         -9.8880, -1.4991]], device='cuda:0'))])
xi:  [-919.47906]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: -355.8658356442842
W_T_median: -437.54469727215724
W_T_pctile_5: -917.930204091027
W_T_CVAR_5_pct: -1043.7476196254427
Average q (qsum/M+1):  59.839383033014116
Optimal xi:  [-919.47906]
Expected(across Rb) median(across samples) p_equity:  0.11237742967251733
obj fun:  tensor(-1802.8333, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1647.679275010148
Current xi:  [-24.159966]
objective value function right now is: -1647.679275010148
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1656.8846767533812
Current xi:  [-54.912296]
objective value function right now is: -1656.8846767533812
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1664.0120520623782
Current xi:  [-82.571785]
objective value function right now is: -1664.0120520623782
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1667.5666591205538
Current xi:  [-108.533554]
objective value function right now is: -1667.5666591205538
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-134.97325]
objective value function right now is: -1662.1488271195103
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-159.74594]
objective value function right now is: -1665.2578016289158
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1671.7777058783256
Current xi:  [-182.84618]
objective value function right now is: -1671.7777058783256
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-217.76219]
objective value function right now is: -1288.3072239826229
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-262.7208]
objective value function right now is: -1451.7209106871142
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-270.05484]
objective value function right now is: -1139.188668691096
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-267.38013]
objective value function right now is: -1474.558283782131
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-266.73105]
objective value function right now is: -1518.5919130030486
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-272.7723]
objective value function right now is: -1670.943681535574
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1681.4856437695832
Current xi:  [-297.55084]
objective value function right now is: -1681.4856437695832
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.4392421629352
Current xi:  [-324.37656]
objective value function right now is: -1686.4392421629352
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1687.5803821308125
Current xi:  [-347.9452]
objective value function right now is: -1687.5803821308125
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1690.4765555214021
Current xi:  [-367.82022]
objective value function right now is: -1690.4765555214021
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1690.7436211120664
Current xi:  [-383.69583]
objective value function right now is: -1690.7436211120664
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1691.3914029995442
Current xi:  [-396.32635]
objective value function right now is: -1691.3914029995442
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1692.1303019498703
Current xi:  [-406.0107]
objective value function right now is: -1692.1303019498703
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-414.4027]
objective value function right now is: -1690.4747890416959
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1693.5377177701132
Current xi:  [-420.56677]
objective value function right now is: -1693.5377177701132
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1697.9938132332525
Current xi:  [-424.18884]
objective value function right now is: -1697.9938132332525
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-425.85715]
objective value function right now is: -1693.0927079412854
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1700.3489708504442
Current xi:  [-427.23743]
objective value function right now is: -1700.3489708504442
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-427.58157]
objective value function right now is: -1699.751472077974
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-427.58673]
objective value function right now is: -1699.9838232324564
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-428.58807]
objective value function right now is: -1694.2642842776859
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-435.04858]
objective value function right now is: -1693.1043084586477
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-438.58536]
objective value function right now is: -1693.5893693973385
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-442.005]
objective value function right now is: -1692.0524156824054
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-444.90372]
objective value function right now is: -1694.723001126991
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-447.40533]
objective value function right now is: -1693.9608285438
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-449.41208]
objective value function right now is: -1694.6042655723081
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-451.96817]
objective value function right now is: -1693.3983031510027
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-452.04074]
objective value function right now is: -1693.3632718802878
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-452.11017]
objective value function right now is: -1695.360126721075
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-452.0685]
objective value function right now is: -1695.3234068096112
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-452.08145]
objective value function right now is: -1694.1379601774245
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-452.21378]
objective value function right now is: -1695.469679981876
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-452.1651]
objective value function right now is: -1695.4336792257866
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-452.2894]
objective value function right now is: -1695.471740316152
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-452.48938]
objective value function right now is: -1695.623938964082
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1702.2505568020492
Current xi:  [-452.02612]
objective value function right now is: -1702.2505568020492
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-450.37885]
objective value function right now is: -1701.7894840124795
new min fval from sgd:  -1702.2795445509407
new min fval from sgd:  -1702.3003718513428
new min fval from sgd:  -1702.3708360588082
new min fval from sgd:  -1702.3874045780715
new min fval from sgd:  -1702.4105690071037
new min fval from sgd:  -1702.4405902200187
new min fval from sgd:  -1702.4419932869966
new min fval from sgd:  -1702.4764482798123
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-448.86996]
objective value function right now is: -1702.2217758174386
new min fval from sgd:  -1702.4803873227756
new min fval from sgd:  -1702.491424692999
new min fval from sgd:  -1702.5023401481105
new min fval from sgd:  -1702.524076383664
new min fval from sgd:  -1702.542791606586
new min fval from sgd:  -1702.5449417706873
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-447.2827]
objective value function right now is: -1702.068402315435
new min fval from sgd:  -1702.551613871615
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-446.20947]
objective value function right now is: -1702.3362075299858
new min fval from sgd:  -1702.573635710119
new min fval from sgd:  -1702.5777953568982
new min fval from sgd:  -1702.5871306897106
new min fval from sgd:  -1702.590285957288
new min fval from sgd:  -1702.5912252799549
new min fval from sgd:  -1702.5922663844765
new min fval from sgd:  -1702.5957454726306
new min fval from sgd:  -1702.59759596931
new min fval from sgd:  -1702.5982857351048
new min fval from sgd:  -1702.59986360206
new min fval from sgd:  -1702.6016399502903
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-445.2257]
objective value function right now is: -1702.402316732637
new min fval from sgd:  -1702.60258436237
new min fval from sgd:  -1702.606207624966
new min fval from sgd:  -1702.6084623883903
new min fval from sgd:  -1702.6157965982813
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-444.8838]
objective value function right now is: -1702.5624083491252
min fval:  -1702.6157965982813
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.1930,  2.2363],
        [28.1364, -0.3109],
        [ 3.6174,  2.1926],
        [-1.3412,  1.9484],
        [-0.9644,  1.9937],
        [-1.2049,  1.9629],
        [ 0.9161, -1.8132],
        [-1.3431,  1.9482],
        [ 0.8365, -1.8267],
        [ 0.9617, -1.8142]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  7.1010,   5.2217,  11.6386,  13.1017,  12.3351,  14.2043,  -3.8411,
          13.1109,  -3.5665,  -4.0236],
        [ -0.0231,  -0.1240,  -0.0264,  -0.0262,  -0.0258,  -0.0261,  -0.1792,
          -0.0262,  -0.1796,  -0.1793],
        [ -4.4648,  -2.0027, -18.8821, -19.4872, -19.6799, -18.6570,   3.9805,
         -19.4773,   4.2111,   4.1069],
        [  4.5734,   1.1245,  11.5532,  12.6217,  12.5767,  12.2484,  -2.6476,
          12.6225,  -2.1842,  -3.1600],
        [ -0.0231,  -0.1240,  -0.0264,  -0.0262,  -0.0258,  -0.0261,  -0.1792,
          -0.0262,  -0.1796,  -0.1793],
        [ -0.0231,  -0.1240,  -0.0264,  -0.0262,  -0.0258,  -0.0261,  -0.1792,
          -0.0262,  -0.1796,  -0.1793],
        [ -0.0231,  -0.1240,  -0.0264,  -0.0262,  -0.0258,  -0.0261,  -0.1792,
          -0.0262,  -0.1796,  -0.1793],
        [ -0.0231,  -0.1240,  -0.0264,  -0.0262,  -0.0258,  -0.0261,  -0.1792,
          -0.0262,  -0.1796,  -0.1793],
        [ -0.0231,  -0.1240,  -0.0264,  -0.0262,  -0.0258,  -0.0261,  -0.1792,
          -0.0262,  -0.1796,  -0.1793],
        [ -0.0231,  -0.1240,  -0.0264,  -0.0262,  -0.0258,  -0.0261,  -0.1792,
          -0.0262,  -0.1796,  -0.1793]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 2.1507e+01, -5.0416e-03, -3.9771e+01,  1.8942e+01, -5.0416e-03,
         -5.0417e-03, -5.0417e-03, -5.0416e-03, -5.0416e-03, -5.0416e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-4.3767e+00,  6.0837e-01],
        [-4.0827e+00,  7.8020e-01],
        [ 1.7973e+01,  5.2695e+00],
        [ 5.5916e+00, -1.2840e-02],
        [ 5.4625e+00,  3.4707e+00],
        [-4.1136e+00,  7.8005e-01],
        [-4.4846e+00,  6.4567e-01],
        [-2.1047e+00,  1.5270e+00],
        [-6.9305e+00, -6.8159e-01],
        [-1.0694e+01, -3.6110e+00]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.2105e+01, -1.8991e+01, -2.2948e+00,  5.6447e+00, -2.6406e+00,
         -1.9311e+01, -2.7910e+01, -4.1868e+00, -4.0686e+00,  1.0107e+01],
        [-1.2974e-01, -1.1003e-01, -2.5155e+00, -2.4409e+00, -7.7140e-01,
         -1.0875e-01, -1.1982e-01, -2.9384e-02,  3.6409e-01, -1.4889e-01],
        [ 3.0210e+00,  1.2477e+00, -1.2266e+01, -1.3776e+00, -3.2386e+00,
          2.9337e-01,  1.1883e+00, -2.0746e+00,  3.6830e+00,  3.2110e+00],
        [-1.3123e-02, -1.0560e-02, -1.7029e+00, -2.8672e+00, -4.5420e-01,
         -1.0101e-02, -1.0848e-02, -1.6357e-02,  3.5705e-02, -4.5713e-01],
        [ 9.2323e+00,  8.5818e+00,  6.9230e-01, -6.7900e+00, -3.0495e-01,
          8.2613e+00,  9.3912e+00,  8.8968e-02,  2.0582e+00,  7.0413e-01],
        [ 1.5187e+01,  1.6997e+01, -1.2425e+00,  6.1802e-01,  9.8712e-01,
          1.6482e+01,  1.6773e+01,  9.8688e+00,  3.4859e+00, -2.1088e+00],
        [-2.0929e+00,  2.5343e+00,  3.8409e+00, -3.4821e-01,  5.9689e+00,
          4.5109e+00,  3.2777e+00,  4.6192e+00, -2.7860e+00, -4.5238e+00],
        [-8.9637e+00, -1.2058e+01, -1.9715e+01,  5.4457e+00, -1.1962e+01,
         -1.4334e+01, -1.3144e+01, -1.4611e+01, -3.6952e+00,  8.6288e+00],
        [ 4.8608e+00,  4.6363e+00, -2.7517e-01, -1.3550e+01,  2.4066e+00,
          4.6425e+00,  4.6420e+00,  3.6053e+00,  1.1246e+00,  6.9201e+00],
        [-1.2044e-02, -9.5292e-03, -1.6710e+00, -2.8754e+00, -4.5418e-01,
         -9.0734e-03, -9.7901e-03, -1.7632e-02,  1.9882e-02, -4.6739e-01]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -2.1271,  -0.1089,  -7.9056,  -0.0162,   7.0738,  -2.5702,   3.1531,
          12.3324,   7.5877,  -0.0141],
        [  2.1330,   0.1083,   7.9020,   0.0158,  -7.0751,   2.5739,  -3.2426,
         -12.3844,  -7.5958,   0.0136]], device='cuda:0'))])
xi:  [-444.9391]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: -78.15283963551575
W_T_median: -231.11332591090797
W_T_pctile_5: -444.67177240043367
W_T_CVAR_5_pct: -483.67191181291213
Average q (qsum/M+1):  58.043720860635084
Optimal xi:  [-444.9391]
Expected(across Rb) median(across samples) p_equity:  0.16946706167630812
obj fun:  tensor(-1702.6158, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1588.2326577537044
Current xi:  [-12.277212]
objective value function right now is: -1588.2326577537044
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1592.5596638512122
Current xi:  [-26.005365]
objective value function right now is: -1592.5596638512122
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1596.9137325234526
Current xi:  [-39.490723]
objective value function right now is: -1596.9137325234526
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1600.0340612253972
Current xi:  [-51.709843]
objective value function right now is: -1600.0340612253972
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-62.60288]
objective value function right now is: -1595.4377857819936
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.0522824637312
Current xi:  [-72.02508]
objective value function right now is: -1604.0522824637312
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-81.271034]
objective value function right now is: -1602.4495777123334
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-88.91346]
objective value function right now is: -1599.238119080751
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.6005541547006
Current xi:  [-95.387505]
objective value function right now is: -1605.6005541547006
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-101.28292]
objective value function right now is: -1595.3066733423886
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-106.1606]
objective value function right now is: -1602.4088375277477
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-114.11812]
objective value function right now is: -1587.7379996901454
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-120.8707]
objective value function right now is: -1587.3719872867725
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-127.82668]
objective value function right now is: -1581.3445963339127
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-133.64444]
objective value function right now is: -1586.798797602716
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-137.21576]
objective value function right now is: -1585.174139346499
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-143.00056]
objective value function right now is: -1580.8610773572257
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-142.49799]
objective value function right now is: -1579.7746161124542
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-142.96696]
objective value function right now is: -1599.5354760472915
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-145.06686]
objective value function right now is: -1600.4911040876434
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-147.70245]
objective value function right now is: -1600.3609370509735
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-149.1744]
objective value function right now is: -1599.8012367609022
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-150.28406]
objective value function right now is: -1594.857801950115
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-152.57608]
objective value function right now is: -1597.628027640304
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-154.31555]
objective value function right now is: -1600.9676076312235
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-155.45905]
objective value function right now is: -1601.2113953810344
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-157.08156]
objective value function right now is: -1594.1270335504055
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-158.56146]
objective value function right now is: -1585.9667333121104
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-161.12718]
objective value function right now is: -1589.2958591945169
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.97432]
objective value function right now is: -1587.3875513663734
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-165.50732]
objective value function right now is: -1600.2642688037863
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-164.13518]
objective value function right now is: -1595.35392222453
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.2147]
objective value function right now is: -1600.994606905463
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-162.88217]
objective value function right now is: -1596.4613692617716
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-164.3393]
objective value function right now is: -1595.0813775465194
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-164.2611]
objective value function right now is: -1602.5088480339182
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-164.08385]
objective value function right now is: -1604.5304876709288
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.92804]
objective value function right now is: -1604.544587831394
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.68886]
objective value function right now is: -1604.2957802690073
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.43216]
objective value function right now is: -1595.0363529701606
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.34462]
objective value function right now is: -1604.7568194309238
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.47604]
objective value function right now is: -1590.661874942549
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.50565]
objective value function right now is: -1604.230659820311
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1606.905698930147
Current xi:  [-163.17119]
objective value function right now is: -1606.905698930147
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.4301]
objective value function right now is: -1591.4063782824921
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.77406]
objective value function right now is: -1591.5598718450005
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.81566]
objective value function right now is: -1606.5686029433543
new min fval from sgd:  -1606.9240323826004
new min fval from sgd:  -1606.9618610234302
new min fval from sgd:  -1607.000598950891
new min fval from sgd:  -1607.0349710416704
new min fval from sgd:  -1607.0983192737792
new min fval from sgd:  -1607.1726967994707
new min fval from sgd:  -1607.2572589386893
new min fval from sgd:  -1607.2925730990373
new min fval from sgd:  -1607.3063556986413
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.26236]
objective value function right now is: -1606.5946155219553
new min fval from sgd:  -1607.3212000281853
new min fval from sgd:  -1607.3555757872787
new min fval from sgd:  -1607.3587078349772
new min fval from sgd:  -1607.3737502527958
new min fval from sgd:  -1607.378338809583
new min fval from sgd:  -1607.4144859673297
new min fval from sgd:  -1607.4154290614931
new min fval from sgd:  -1607.4353708098424
new min fval from sgd:  -1607.4385149313525
new min fval from sgd:  -1607.4432456303698
new min fval from sgd:  -1607.4702080047994
new min fval from sgd:  -1607.4869291740854
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-163.08449]
objective value function right now is: -1607.4226016345422
new min fval from sgd:  -1607.489473043059
new min fval from sgd:  -1607.5230606419238
new min fval from sgd:  -1607.5338327238053
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-162.93904]
objective value function right now is: -1607.1674900211337
min fval:  -1607.5338327238053
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.7604,  2.9390],
        [ 9.8212,  0.8474],
        [-0.8203,  2.7255],
        [-0.7867,  2.7350],
        [-0.7978,  2.7245],
        [-0.7676,  2.7572],
        [ 0.4455, -2.2579],
        [-0.7865,  2.7352],
        [ 0.4950, -2.2484],
        [ 0.4472, -2.2824]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 7.0236e+00,  1.8697e+01,  9.0402e+00,  1.1588e+01,  1.0943e+01,
          1.2535e+01, -3.4784e+00,  1.1596e+01, -3.1999e+00, -3.6680e+00],
        [-7.7725e-03, -1.2588e-01, -9.7396e-03, -9.6670e-03, -9.7647e-03,
         -9.4596e-03, -6.2413e-01, -9.6650e-03, -6.2344e-01, -6.2620e-01],
        [-1.0379e+01, -3.5630e+00, -2.1989e+01, -2.3529e+01, -2.3741e+01,
         -2.2776e+01,  3.3627e+00, -2.3519e+01,  3.5940e+00,  3.4945e+00],
        [-7.7725e-03, -1.2588e-01, -9.7397e-03, -9.6670e-03, -9.7647e-03,
         -9.4596e-03, -6.2413e-01, -9.6650e-03, -6.2344e-01, -6.2620e-01],
        [-7.7725e-03, -1.2588e-01, -9.7397e-03, -9.6670e-03, -9.7647e-03,
         -9.4596e-03, -6.2413e-01, -9.6650e-03, -6.2344e-01, -6.2620e-01],
        [-7.7725e-03, -1.2588e-01, -9.7397e-03, -9.6670e-03, -9.7647e-03,
         -9.4596e-03, -6.2413e-01, -9.6650e-03, -6.2344e-01, -6.2620e-01],
        [-7.7725e-03, -1.2588e-01, -9.7397e-03, -9.6670e-03, -9.7647e-03,
         -9.4596e-03, -6.2413e-01, -9.6650e-03, -6.2344e-01, -6.2620e-01],
        [-7.7725e-03, -1.2588e-01, -9.7397e-03, -9.6670e-03, -9.7646e-03,
         -9.4596e-03, -6.2413e-01, -9.6650e-03, -6.2344e-01, -6.2620e-01],
        [-7.7725e-03, -1.2588e-01, -9.7397e-03, -9.6670e-03, -9.7647e-03,
         -9.4596e-03, -6.2413e-01, -9.6650e-03, -6.2344e-01, -6.2620e-01],
        [-7.7725e-03, -1.2588e-01, -9.7397e-03, -9.6670e-03, -9.7647e-03,
         -9.4596e-03, -6.2413e-01, -9.6650e-03, -6.2344e-01, -6.2620e-01]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 2.4872e+01,  8.9892e-04, -4.0320e+01,  8.9892e-04,  8.9893e-04,
          8.9893e-04,  8.9892e-04,  8.9893e-04,  8.9892e-04,  8.9892e-04]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -7.9667,  -0.0977],
        [ -6.9801,  -0.0394],
        [ 12.7825,   4.9270],
        [ 25.4716,   1.4975],
        [ -4.8479,  -1.9195],
        [ -5.4540,  -1.9520],
        [ -2.8782,   0.5094],
        [  4.9434,  -0.6297],
        [ -3.8102,  -2.0344],
        [-10.5704,  -3.5926]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.0689e+01, -2.4792e+00, -2.0842e+01, -2.9009e+00, -9.7867e-02,
          1.1598e+00,  1.1696e+00, -1.4400e+00,  1.8789e+00,  1.8526e+00],
        [-4.0601e-01, -2.7946e-01, -1.1912e+00, -5.2463e+00, -1.3665e+00,
          1.5591e-01, -6.2277e-02, -2.1345e+00, -1.1535e+00, -1.7421e+00],
        [-8.6400e-03, -1.4514e-03, -1.3651e+00, -2.6739e+00, -2.1663e+00,
         -1.5218e+00, -2.7807e-02, -2.1782e+00, -1.1581e+00, -7.9250e-01],
        [-2.6225e-01, -4.5382e-02, -3.3307e+00, -2.5523e+00, -2.5798e+00,
          4.7548e-01, -1.5117e-01, -1.1293e+00, -5.2728e+00,  1.6560e-02],
        [ 9.5326e+00,  5.2813e+00, -2.5231e-01, -1.6894e+00, -3.1205e+00,
         -3.2203e+00,  2.6425e+01, -2.4250e+00,  1.1171e+00, -3.0520e+00],
        [ 6.7205e+00,  2.2676e+00, -1.7869e+00,  1.1759e+00,  4.0896e+00,
          3.4762e+00,  2.9229e+00,  6.3867e+00, -9.7335e+00,  6.3362e+00],
        [-4.0712e-01,  1.5969e+01,  4.3760e+00,  1.4180e+00, -5.3013e-03,
         -1.8523e+00,  9.8712e+00, -1.2534e+00,  3.1118e+00, -6.0943e+00],
        [ 2.7706e+00,  2.7098e+00, -3.4525e+00,  7.9892e-01, -1.0302e+01,
         -5.9422e+00,  6.5501e+00, -6.9435e-02, -1.5929e-01, -1.0151e+00],
        [-4.0058e+00, -1.1448e+01, -4.8592e+01, -1.7175e+01,  5.4075e+00,
          2.9820e+00, -7.1020e-01,  1.9096e+01, -9.2696e+00,  7.4149e+00],
        [-2.4634e-01, -4.5675e-02, -1.6838e+00, -2.8815e+00, -2.6054e+00,
         -1.7929e-01, -2.5855e-02, -2.2775e+00, -2.8968e+00,  2.2681e-01]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-3.6784, -2.3014,  0.0116, -0.4201,  1.6765, -4.6463,  4.6089,  1.9249,
          8.3990, -0.0740],
        [ 3.6785,  2.3011, -0.0123,  0.4201, -1.6767,  4.6500, -4.6980, -1.9408,
         -8.4013,  0.0740]], device='cuda:0'))])
xi:  [-163.06645]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 170.89043323421734
W_T_median: 16.614192127853926
W_T_pctile_5: -163.0971544197018
W_T_CVAR_5_pct: -223.2515289082044
Average q (qsum/M+1):  55.45679892263105
Optimal xi:  [-163.06645]
Expected(across Rb) median(across samples) p_equity:  0.3098993278426595
obj fun:  tensor(-1607.5338, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1490.009363924135
Current xi:  [-9.718272]
objective value function right now is: -1490.009363924135
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1520.4104931651802
Current xi:  [-13.345853]
objective value function right now is: -1520.4104931651802
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1527.0478514911847
Current xi:  [-15.581702]
objective value function right now is: -1527.0478514911847
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1527.119932805427
Current xi:  [-16.08405]
objective value function right now is: -1527.119932805427
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-16.123177]
objective value function right now is: -1510.6035669956393
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-16.076797]
objective value function right now is: -1515.8350575633262
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1532.9529635125416
Current xi:  [-16.142542]
objective value function right now is: -1532.9529635125416
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.553488]
objective value function right now is: -1531.6277184368116
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.007997]
objective value function right now is: -1528.4111231148975
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.251999]
objective value function right now is: -1525.0931693026237
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.143159]
objective value function right now is: -1530.0479505872268
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.824013]
objective value function right now is: -1510.0783940414508
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.070257]
objective value function right now is: -1519.810968546254
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-14.051148]
objective value function right now is: -1529.2827933635504
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.763225]
objective value function right now is: -1532.028523830692
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.680071]
objective value function right now is: -1528.0502029070403
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1533.33816426114
Current xi:  [-11.562215]
objective value function right now is: -1533.33816426114
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-11.102929]
objective value function right now is: -1246.3431568896915
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.951949]
objective value function right now is: -1526.3860040954191
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.577178]
objective value function right now is: -1507.5910273382613
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.210441]
objective value function right now is: -1511.8545232006425
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.811388]
objective value function right now is: -1520.3604016085844
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1534.2012753222587
Current xi:  [-8.558243]
objective value function right now is: -1534.2012753222587
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.0921335]
objective value function right now is: -1517.3167896771329
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.797964]
objective value function right now is: -1493.7514290939896
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.510859]
objective value function right now is: -1531.5613355647633
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1535.829407600416
Current xi:  [3.0125854]
objective value function right now is: -1535.829407600416
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [8.301561]
objective value function right now is: -1527.5495216928443
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1536.5621483092393
Current xi:  [13.748402]
objective value function right now is: -1536.5621483092393
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1540.917037421105
Current xi:  [18.587736]
objective value function right now is: -1540.917037421105
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [22.958788]
objective value function right now is: -1536.0965471890647
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1545.6631679520383
Current xi:  [26.125101]
objective value function right now is: -1545.6631679520383
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [29.154203]
objective value function right now is: -1541.2488602877754
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [37.554207]
objective value function right now is: -1469.3661743709654
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.214638]
objective value function right now is: -1538.5744773213958
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1547.1362517097318
Current xi:  [53.62807]
objective value function right now is: -1547.1362517097318
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1552.1243796481062
Current xi:  [54.94477]
objective value function right now is: -1552.1243796481062
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.7637414692147
Current xi:  [55.33931]
objective value function right now is: -1555.7637414692147
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.92185]
objective value function right now is: -1554.8090363239776
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.78153]
objective value function right now is: -1553.7904537732004
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.482834]
objective value function right now is: -1551.9307490373958
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.218655]
objective value function right now is: -1539.680222330932
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1556.4788862175415
Current xi:  [53.98991]
objective value function right now is: -1556.4788862175415
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1558.0119566227265
Current xi:  [53.770164]
objective value function right now is: -1558.0119566227265
new min fval from sgd:  -1559.2113685594722
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.76636]
objective value function right now is: -1559.2113685594722
new min fval from sgd:  -1559.4249265240167
new min fval from sgd:  -1559.436910969968
new min fval from sgd:  -1559.5133860003966
new min fval from sgd:  -1559.5667470471737
new min fval from sgd:  -1559.8731386586046
new min fval from sgd:  -1559.962526401495
new min fval from sgd:  -1559.9711695422552
new min fval from sgd:  -1559.9921039956384
new min fval from sgd:  -1560.1659172660736
new min fval from sgd:  -1560.2663598737065
new min fval from sgd:  -1560.3122038406973
new min fval from sgd:  -1560.3485390939293
new min fval from sgd:  -1560.405211220614
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.867462]
objective value function right now is: -1558.6102644021119
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.867817]
objective value function right now is: -1559.3003924799261
new min fval from sgd:  -1560.451021458202
new min fval from sgd:  -1560.5047862007996
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.960907]
objective value function right now is: -1558.5760960122002
new min fval from sgd:  -1560.5351843899196
new min fval from sgd:  -1560.5445091305446
new min fval from sgd:  -1560.5450570274154
new min fval from sgd:  -1560.5469841136476
new min fval from sgd:  -1560.5504693735043
new min fval from sgd:  -1560.5558762290823
new min fval from sgd:  -1560.5587386439176
new min fval from sgd:  -1560.5665062293015
new min fval from sgd:  -1560.5780956849121
new min fval from sgd:  -1560.5854862974468
new min fval from sgd:  -1560.5911482388085
new min fval from sgd:  -1560.6230656010578
new min fval from sgd:  -1560.6393421717007
new min fval from sgd:  -1560.6495391173703
new min fval from sgd:  -1560.6616665343688
new min fval from sgd:  -1560.6687419632344
new min fval from sgd:  -1560.707501382336
new min fval from sgd:  -1560.729910276011
new min fval from sgd:  -1560.7487394686166
new min fval from sgd:  -1560.7571912633855
new min fval from sgd:  -1560.7674675752842
new min fval from sgd:  -1560.7839801485186
new min fval from sgd:  -1560.7947751394088
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.99849]
objective value function right now is: -1560.591976463372
new min fval from sgd:  -1560.8200040299043
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.00593]
objective value function right now is: -1560.5799944349906
min fval:  -1560.8200040299043
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-4.4878,  3.3231],
        [ 8.7169,  1.7740],
        [-3.7438,  2.6901],
        [-4.0829,  2.9726],
        [-3.9883,  2.8909],
        [-4.2031,  3.0834],
        [ 2.5643, -1.9518],
        [-4.0843,  2.9738],
        [-0.9772,  7.0067],
        [ 2.6529, -2.0230]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  5.3026,  36.4087,   4.3923,   8.3409,   7.2834,   9.8485,  -5.6231,
           8.3549,  -2.8268,  -5.8967],
        [ -2.7414, -10.3732,   0.1584,  -1.5468,  -1.0747,  -2.1846,   1.2959,
          -1.5537,  -1.6784,   1.4707],
        [-11.9932,  -8.0476, -22.5791, -23.6598, -23.9397, -22.9077,   5.2993,
         -23.6501,   7.2845,   5.3490],
        [ -2.7414, -10.3731,   0.1584,  -1.5468,  -1.0747,  -2.1846,   1.2959,
          -1.5537,  -1.6784,   1.4707],
        [ -2.7414, -10.3732,   0.1584,  -1.5468,  -1.0747,  -2.1846,   1.2959,
          -1.5537,  -1.6784,   1.4707],
        [ -2.7414, -10.3732,   0.1584,  -1.5468,  -1.0747,  -2.1846,   1.2959,
          -1.5537,  -1.6784,   1.4707],
        [ -2.7414, -10.3732,   0.1584,  -1.5468,  -1.0747,  -2.1846,   1.2959,
          -1.5537,  -1.6784,   1.4707],
        [ -2.7414, -10.3732,   0.1584,  -1.5468,  -1.0747,  -2.1846,   1.2959,
          -1.5537,  -1.6784,   1.4707],
        [ -2.7414, -10.3732,   0.1584,  -1.5468,  -1.0747,  -2.1846,   1.2959,
          -1.5537,  -1.6784,   1.4707],
        [ -2.7414, -10.3732,   0.1584,  -1.5468,  -1.0747,  -2.1846,   1.2959,
          -1.5537,  -1.6785,   1.4707]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 34.6338,  -5.3152, -56.8193,  -5.3152,  -5.3152,  -5.3152,  -5.3152,
          -5.3152,  -5.3152,  -5.3153]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.4333,   0.6565],
        [  4.9432,   1.9656],
        [  9.1713,   3.2829],
        [ 30.1074,   1.9635],
        [ -8.8418,  -2.3225],
        [ -1.6705,   0.7872],
        [ -3.0103,   0.6560],
        [  4.3666,  -0.6111],
        [-15.7926, -11.2812],
        [-16.8864,  -5.6522]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.0754e+01, -7.2332e+00, -1.3598e+01, -6.0915e-01,  2.0259e+00,
         -2.8663e+00, -8.6827e+00,  4.3458e+00,  5.5654e+00, -1.4890e+00],
        [-4.1599e+00, -7.1922e+00, -1.3774e+01, -6.5654e-01,  1.0660e+00,
         -5.0859e+00, -4.1510e+00, -1.4535e+00, -3.8595e-01,  3.5115e+00],
        [ 1.0848e-01, -2.4653e+00, -2.9488e+00, -1.8395e+00,  2.2984e+00,
          1.0915e+00,  2.2711e-02, -1.9528e+00,  8.1915e-01,  2.1354e+00],
        [ 2.4270e+01,  1.2256e+00, -3.9432e-01, -3.6439e+00,  1.6933e+00,
          1.4906e+01,  2.5355e+01, -5.6185e+00,  3.9200e+00, -1.0351e+01],
        [-1.2999e+01,  2.3084e+00,  2.5452e+00,  1.4254e+00, -4.5836e+00,
         -1.4926e+01, -1.7075e+01,  4.8648e-01, -2.0324e+00, -1.1992e+01],
        [ 8.2877e-01,  2.1320e+00,  1.1621e+00,  2.2677e+00,  6.0194e-01,
          1.0881e+00,  8.5497e-01,  3.3155e+00, -4.0823e-02,  1.4493e+00],
        [-7.2368e-01,  1.3097e+00,  2.0160e+00,  2.1531e+00,  1.0854e+00,
         -5.7920e-01, -8.7381e-01,  3.4933e+00,  4.1310e+00,  6.8191e-01],
        [-3.1195e-01, -1.5344e+00, -2.1551e+00, -1.5632e+00, -2.8297e-01,
         -6.0284e-01, -3.8134e-01, -2.1108e+00, -9.5459e-01, -3.8037e-01],
        [-3.3780e+01, -2.9445e+01, -3.7789e+01, -5.0357e+00,  9.2881e+00,
         -5.9369e+00, -2.7990e+01,  2.3197e+01, -3.7162e+00,  9.0076e+00],
        [-3.1428e-01, -1.5691e+00, -1.5683e+00, -2.2885e+00, -1.7872e-01,
         -6.0580e-01, -3.8391e-01, -2.1417e+00, -7.0854e-01, -3.0203e-01]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-8.3846, -1.6860,  1.6758,  1.5267,  1.4733, -5.6308,  4.4499,  0.0197,
          9.8787,  0.0651],
        [ 8.3847,  1.6859, -1.6758, -1.5267, -1.4734,  5.6345, -4.5387, -0.0198,
         -9.8809, -0.0651]], device='cuda:0'))])
xi:  [53.998848]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 549.415544234462
W_T_median: 226.41049952926346
W_T_pctile_5: 54.09046271589762
W_T_CVAR_5_pct: -49.536692830062485
Average q (qsum/M+1):  51.947245936239916
Optimal xi:  [53.998848]
Expected(across Rb) median(across samples) p_equity:  0.3088356576859951
obj fun:  tensor(-1560.8200, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
loaded xi:  53.998848
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1358.3935891484105
Current xi:  [55.27839]
objective value function right now is: -1358.3935891484105
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1518.8334077574143
Current xi:  [60.960464]
objective value function right now is: -1518.8334077574143
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1519.8325973617239
Current xi:  [67.40097]
objective value function right now is: -1519.8325973617239
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1521.6030220378043
Current xi:  [71.85898]
objective value function right now is: -1521.6030220378043
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1524.1577146106865
Current xi:  [75.37098]
objective value function right now is: -1524.1577146106865
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1533.5595046877145
Current xi:  [80.48331]
objective value function right now is: -1533.5595046877145
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [71.780266]
objective value function right now is: -1506.5431551726701
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [73.38877]
objective value function right now is: -1500.9258715504288
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.92386]
objective value function right now is: -1518.8967493140328
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [77.5631]
objective value function right now is: -1524.8148344173958
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [80.00015]
objective value function right now is: -1530.958574868672
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1534.7225768023704
Current xi:  [85.958046]
objective value function right now is: -1534.7225768023704
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [90.41006]
objective value function right now is: -1524.485576118662
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1538.8132391051422
Current xi:  [92.01472]
objective value function right now is: -1538.8132391051422
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [96.48062]
objective value function right now is: -1538.2934808591685
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [98.699104]
objective value function right now is: -1524.8416058177772
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [100.9737]
objective value function right now is: -1536.9558811008405
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [102.77796]
objective value function right now is: -1529.3050202913653
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.63768]
objective value function right now is: -1533.3642392719723
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1541.757506901346
Current xi:  [104.95841]
objective value function right now is: -1541.757506901346
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.174484]
objective value function right now is: -1535.3220060087144
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.84714]
objective value function right now is: -1540.8435554464656
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.41838]
objective value function right now is: -1540.2281597078993
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.37332]
objective value function right now is: -1538.366570441069
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.870865]
objective value function right now is: -1536.6046216365285
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.3503]
objective value function right now is: -1541.2868720150263
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.719406]
objective value function right now is: -1531.6322029835117
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [107.14958]
objective value function right now is: -1540.101778197374
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [107.66783]
objective value function right now is: -1531.7524677516642
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.87615]
objective value function right now is: -1526.9863637444748
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1541.8466431284155
Current xi:  [108.51953]
objective value function right now is: -1541.8466431284155
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.13778]
objective value function right now is: -1503.2824016711725
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [101.92083]
objective value function right now is: -1508.2799556296432
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [99.92609]
objective value function right now is: -1532.2652842647854
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [100.76667]
objective value function right now is: -1535.658218857763
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [101.14438]
objective value function right now is: -1538.3186116952486
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1543.7230278455172
Current xi:  [101.585754]
objective value function right now is: -1543.7230278455172
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [102.449745]
objective value function right now is: -1543.0467062171585
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.41359]
objective value function right now is: -1542.8498798558671
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1545.0166100725842
Current xi:  [104.24301]
objective value function right now is: -1545.0166100725842
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.918526]
objective value function right now is: -1543.7044804638076
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.35427]
objective value function right now is: -1539.283801459419
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1546.097435553837
Current xi:  [106.28283]
objective value function right now is: -1546.097435553837
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1546.933795057654
Current xi:  [106.97446]
objective value function right now is: -1546.933795057654
new min fval from sgd:  -1546.9442020828906
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.73677]
objective value function right now is: -1546.9442020828906
new min fval from sgd:  -1547.1223487336524
new min fval from sgd:  -1547.1733456049817
new min fval from sgd:  -1547.1896976263083
new min fval from sgd:  -1547.2046455851244
new min fval from sgd:  -1547.2287940145582
new min fval from sgd:  -1547.2829973679725
new min fval from sgd:  -1547.4362257068149
new min fval from sgd:  -1547.5649630975738
new min fval from sgd:  -1547.6482801760155
new min fval from sgd:  -1547.6728149264236
new min fval from sgd:  -1547.6957122478252
new min fval from sgd:  -1547.8175547070969
new min fval from sgd:  -1547.8850474133892
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.27649]
objective value function right now is: -1540.3532554947758
new min fval from sgd:  -1547.9317143470437
new min fval from sgd:  -1547.990471965374
new min fval from sgd:  -1548.0103511425673
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.18747]
objective value function right now is: -1544.690918678485
new min fval from sgd:  -1548.120385335308
new min fval from sgd:  -1548.1669652130504
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.732864]
objective value function right now is: -1546.8145457099788
new min fval from sgd:  -1548.1770027766029
new min fval from sgd:  -1548.2250233539526
new min fval from sgd:  -1548.2659273643492
new min fval from sgd:  -1548.2724617594872
new min fval from sgd:  -1548.2739526940559
new min fval from sgd:  -1548.289841332522
new min fval from sgd:  -1548.2902338315414
new min fval from sgd:  -1548.2967230905933
new min fval from sgd:  -1548.3049039879484
new min fval from sgd:  -1548.3073339258065
new min fval from sgd:  -1548.3131129813003
new min fval from sgd:  -1548.3182119564758
new min fval from sgd:  -1548.3215306569944
new min fval from sgd:  -1548.325261537903
new min fval from sgd:  -1548.3261274664806
new min fval from sgd:  -1548.335137688614
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [110.07381]
objective value function right now is: -1547.8498244725004
new min fval from sgd:  -1548.3354849356563
new min fval from sgd:  -1548.3427766777609
new min fval from sgd:  -1548.3448063661922
new min fval from sgd:  -1548.3452161987648
new min fval from sgd:  -1548.369489733742
new min fval from sgd:  -1548.383522763763
new min fval from sgd:  -1548.3868589557346
new min fval from sgd:  -1548.3964723771903
new min fval from sgd:  -1548.421272594259
new min fval from sgd:  -1548.431341535411
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [110.25347]
objective value function right now is: -1548.2765581859737
min fval:  -1548.431341535411
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-4.4123,  3.4562],
        [ 9.7357,  1.5935],
        [-4.1941,  3.2984],
        [-4.2713,  3.3611],
        [-4.2429,  3.3389],
        [-4.3201,  3.3983],
        [ 3.7013, -3.0480],
        [-4.2718,  3.3615],
        [-2.3205,  5.7076],
        [ 3.7421, -3.0787]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  3.2291,  42.6367,   2.0707,   5.5651,   4.6182,   6.9160,  -6.4514,
           5.5777,  10.8963,  -6.7462],
        [ -0.0494,  -0.7052,  -0.0586,  -0.0547,  -0.0561,  -0.0526,  -1.3070,
          -0.0547,   0.0485,  -1.3097],
        [-17.2282,  -8.3287, -25.8331, -27.0873, -27.2544, -26.5285,   5.2514,
         -27.0801,  19.8481,   5.3116],
        [ -0.0494,  -0.7052,  -0.0586,  -0.0547,  -0.0561,  -0.0526,  -1.3070,
          -0.0547,   0.0485,  -1.3097],
        [ -0.0494,  -0.7052,  -0.0586,  -0.0547,  -0.0561,  -0.0526,  -1.3070,
          -0.0547,   0.0485,  -1.3097],
        [ -0.0494,  -0.7052,  -0.0586,  -0.0547,  -0.0561,  -0.0526,  -1.3070,
          -0.0547,   0.0485,  -1.3097],
        [ -0.0494,  -0.7052,  -0.0586,  -0.0547,  -0.0561,  -0.0526,  -1.3070,
          -0.0547,   0.0485,  -1.3097],
        [ -0.0494,  -0.7052,  -0.0586,  -0.0547,  -0.0561,  -0.0526,  -1.3070,
          -0.0547,   0.0485,  -1.3097],
        [ -0.0494,  -0.7052,  -0.0585,  -0.0547,  -0.0561,  -0.0526,  -1.3070,
          -0.0547,   0.0485,  -1.3097],
        [ -0.0494,  -0.7052,  -0.0585,  -0.0547,  -0.0561,  -0.0526,  -1.3070,
          -0.0547,   0.0485,  -1.3097]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 32.9311,  -0.0682, -57.2395,  -0.0682,  -0.0682,  -0.0682,  -0.0682,
          -0.0682,  -0.0682,  -0.0682]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.1060,   2.3393],
        [ -1.6997,   0.3179],
        [  6.5959,   2.1169],
        [ -8.2670,  -5.9793],
        [ -9.2340,  -0.9889],
        [ -2.0510,   2.3448],
        [ -2.2027,   2.3136],
        [ -2.6614,   2.0401],
        [-11.9072,  -4.1611],
        [-20.4378,  -2.1575]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.2271e-01, -7.0972e-01, -1.5247e+01, -2.7417e+00, -5.5394e+00,
         -4.2683e-01, -4.2513e-01, -5.2820e-01,  4.3946e+00, -2.9222e+00],
        [-3.8773e+01, -2.8408e+00, -1.5755e+01,  3.5158e+00,  4.1314e+00,
         -3.9302e+01, -3.8925e+01, -3.3780e+01,  7.1605e+00,  6.4466e+00],
        [ 2.2908e-01, -8.2263e-01, -3.2645e+00, -3.5595e+00, -7.0766e+00,
         -1.5006e-02, -2.5330e-01, -1.2468e+00, -4.0174e+00, -3.7120e-01],
        [ 8.2464e+00, -2.8768e+00,  3.1749e+00, -8.8858e-01,  4.5110e+01,
          5.0957e+00,  1.0252e+01,  2.2548e+00, -7.2297e+00,  2.7505e+01],
        [ 1.8304e+00, -1.3234e-01,  8.4344e+00, -1.4267e-01,  3.2973e+00,
          2.2094e+00,  1.5840e+00,  2.7047e+00, -2.5058e+00, -4.7180e+00],
        [ 9.6901e-01,  1.9209e+00,  7.8765e+00,  2.0661e+00,  1.0724e+00,
          1.0573e+00,  8.6291e-01,  3.6347e-01,  8.5565e-01,  2.0777e-01],
        [ 1.7380e+00,  3.0389e+01,  9.0224e+00, -1.5519e+00,  5.5287e+00,
          1.8984e+00,  1.7203e+00,  2.4965e+00, -7.8177e+00,  3.4540e+00],
        [-3.0413e+01,  1.4332e+00, -4.7781e+00, -6.4982e-01, -6.1992e+00,
         -3.1949e+01, -3.3277e+01, -9.0170e+00,  1.0065e+01,  8.2194e+00],
        [-1.9744e-01, -5.5719e+00, -6.5067e+00, -9.5934e-01, -1.3062e-01,
         -2.0010e-01, -1.8401e-01, -1.2830e-01, -1.0945e+00,  1.0826e+00],
        [-6.2195e+00, -1.8820e+00, -1.7992e+00, -3.0712e+00,  2.7667e-01,
         -6.1143e+00, -6.4579e+00, -6.8051e+00,  1.4490e+00,  2.2840e+00]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -3.8020,  12.9804,  -4.1326,   1.8537,  -1.4303,  -5.2790,   5.2726,
          -8.4570,   0.7251,   2.2054],
        [  3.8020, -12.9804,   4.1326,  -1.8537,   1.4302,   5.2827,  -5.3613,
           8.4570,  -0.7253,  -2.2054]], device='cuda:0'))])
xi:  [110.17684]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 627.336889534679
W_T_median: 304.22507720089664
W_T_pctile_5: 110.26200361129122
W_T_CVAR_5_pct: -14.291516442459297
Average q (qsum/M+1):  50.6412353515625
Optimal xi:  [110.17684]
Expected(across Rb) median(across samples) p_equity:  0.2959145528574785
obj fun:  tensor(-1548.4313, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
loaded xi:  110.17684
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1513.458505901024
Current xi:  [126.17707]
objective value function right now is: -1513.458505901024
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1520.7583062705708
Current xi:  [138.5238]
objective value function right now is: -1520.7583062705708
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1539.9377050400024
Current xi:  [147.67863]
objective value function right now is: -1539.9377050400024
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1543.915316012255
Current xi:  [153.44626]
objective value function right now is: -1543.915316012255
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [157.98177]
objective value function right now is: -1542.8397262672927
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [160.31343]
objective value function right now is: -1510.1911237112054
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1547.9210229747425
Current xi:  [160.95074]
objective value function right now is: -1547.9210229747425
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.28276]
objective value function right now is: -1546.3849563133829
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.1141]
objective value function right now is: -1540.625521890294
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1552.2891266821441
Current xi:  [163.99954]
objective value function right now is: -1552.2891266821441
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.80206]
objective value function right now is: -1539.2581489488125
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.01619]
objective value function right now is: -1538.0883542816812
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1553.3501973030102
Current xi:  [166.33751]
objective value function right now is: -1553.3501973030102
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [151.9074]
objective value function right now is: -1440.6370922651315
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [155.8975]
objective value function right now is: -1523.0969446137751
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [158.1213]
objective value function right now is: -1541.6830578326271
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [158.76341]
objective value function right now is: -1540.3464036201303
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [156.62276]
objective value function right now is: -1548.9442988712383
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [158.50183]
objective value function right now is: -1529.4942925482808
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [160.81837]
objective value function right now is: -1526.954681807107
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.9044]
objective value function right now is: -1475.4814760263623
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.88597]
objective value function right now is: -1548.4169746555376
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.82233]
objective value function right now is: -1548.2904507958315
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.33502]
objective value function right now is: -1545.0926750269423
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.96407]
objective value function right now is: -1516.2010936779232
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.94229]
objective value function right now is: -1535.3281786902562
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.0278]
objective value function right now is: -1540.6808899216082
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [162.77077]
objective value function right now is: -1544.555506165515
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [163.12619]
objective value function right now is: -1526.2410931302504
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.16217]
objective value function right now is: -1519.5467129899357
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.10974]
objective value function right now is: -1542.801997749452
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.3562]
objective value function right now is: -1544.615252238057
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1553.5251791828787
Current xi:  [163.94643]
objective value function right now is: -1553.5251791828787
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.52916]
objective value function right now is: -1546.5976459522665
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [146.12888]
objective value function right now is: -1509.7436078821036
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [146.63489]
objective value function right now is: -1532.9498804965076
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [147.60036]
objective value function right now is: -1540.1030360798102
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.6009]
objective value function right now is: -1544.2153647849061
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [150.12538]
objective value function right now is: -1547.2485636297854
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [151.78174]
objective value function right now is: -1546.7243724366963
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [153.65816]
objective value function right now is: -1550.8617862514734
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [155.03148]
objective value function right now is: -1551.0697525426444
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [156.65479]
objective value function right now is: -1532.3117263766655
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1554.0870852511
Current xi:  [158.08846]
objective value function right now is: -1554.0870852511
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [159.76259]
objective value function right now is: -1554.0021091322917
new min fval from sgd:  -1554.211242410077
new min fval from sgd:  -1554.2266693972208
new min fval from sgd:  -1554.2286030431405
new min fval from sgd:  -1554.406648281434
new min fval from sgd:  -1554.452842811009
new min fval from sgd:  -1554.8171150375915
new min fval from sgd:  -1554.855147817497
new min fval from sgd:  -1555.003216974702
new min fval from sgd:  -1555.014360174007
new min fval from sgd:  -1555.1665201552203
new min fval from sgd:  -1555.3471476486377
new min fval from sgd:  -1555.5337230962866
new min fval from sgd:  -1555.808866417144
new min fval from sgd:  -1555.8727557271277
new min fval from sgd:  -1555.8784231908567
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.10327]
objective value function right now is: -1554.524149011422
new min fval from sgd:  -1555.9778793196992
new min fval from sgd:  -1556.0340024878826
new min fval from sgd:  -1556.0912794874257
new min fval from sgd:  -1556.2359992593565
new min fval from sgd:  -1556.26068928823
new min fval from sgd:  -1556.2806563230963
new min fval from sgd:  -1556.3178809960277
new min fval from sgd:  -1556.3657618391535
new min fval from sgd:  -1556.4707609709785
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [161.85753]
objective value function right now is: -1555.0622663128813
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.62108]
objective value function right now is: -1555.1115453384004
new min fval from sgd:  -1556.4816219886145
new min fval from sgd:  -1556.5499098766315
new min fval from sgd:  -1556.6061009510304
new min fval from sgd:  -1556.6369193851492
new min fval from sgd:  -1556.6578509719368
new min fval from sgd:  -1556.664726083048
new min fval from sgd:  -1556.7710549262579
new min fval from sgd:  -1556.8248017013445
new min fval from sgd:  -1557.0276357847256
new min fval from sgd:  -1557.0497953930362
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.23518]
objective value function right now is: -1556.2871150497695
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.43297]
objective value function right now is: -1555.323011117353
min fval:  -1557.0497953930362
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-5.4770,  3.2109],
        [ 9.6191,  1.4411],
        [-5.2962,  3.1155],
        [-5.3350,  3.1339],
        [-5.3164,  3.1245],
        [-5.3699,  3.1514],
        [ 4.8013, -2.8443],
        [-5.3354,  3.1340],
        [-4.2183,  5.2450],
        [ 4.8460, -2.8677]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 2.5047e+00,  4.4145e+01,  1.6871e+00,  4.2694e+00,  3.5647e+00,
          5.2752e+00, -7.8022e+00,  4.2788e+00,  1.4213e+01, -8.0979e+00],
        [-4.5124e-02, -7.4269e-01, -5.0383e-02, -4.9291e-02, -4.9830e-02,
         -4.8289e-02, -1.3642e+00, -4.9281e-02, -3.5704e-02, -1.3663e+00],
        [-1.7342e+01, -1.1848e+01, -2.5005e+01, -2.6106e+01, -2.6262e+01,
         -2.5592e+01,  7.1120e+00, -2.6099e+01,  2.2785e+01,  7.1707e+00],
        [-4.5124e-02, -7.4269e-01, -5.0383e-02, -4.9291e-02, -4.9830e-02,
         -4.8289e-02, -1.3642e+00, -4.9281e-02, -3.5704e-02, -1.3663e+00],
        [-4.5124e-02, -7.4269e-01, -5.0383e-02, -4.9291e-02, -4.9830e-02,
         -4.8289e-02, -1.3642e+00, -4.9281e-02, -3.5704e-02, -1.3663e+00],
        [-4.5124e-02, -7.4269e-01, -5.0383e-02, -4.9291e-02, -4.9830e-02,
         -4.8289e-02, -1.3642e+00, -4.9281e-02, -3.5704e-02, -1.3663e+00],
        [-4.5124e-02, -7.4269e-01, -5.0383e-02, -4.9291e-02, -4.9830e-02,
         -4.8289e-02, -1.3642e+00, -4.9281e-02, -3.5704e-02, -1.3663e+00],
        [-4.5124e-02, -7.4269e-01, -5.0383e-02, -4.9291e-02, -4.9830e-02,
         -4.8289e-02, -1.3642e+00, -4.9281e-02, -3.5704e-02, -1.3663e+00],
        [-4.5124e-02, -7.4269e-01, -5.0383e-02, -4.9291e-02, -4.9830e-02,
         -4.8289e-02, -1.3642e+00, -4.9281e-02, -3.5704e-02, -1.3663e+00],
        [-4.5124e-02, -7.4269e-01, -5.0383e-02, -4.9291e-02, -4.9830e-02,
         -4.8289e-02, -1.3642e+00, -4.9281e-02, -3.5704e-02, -1.3663e+00]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 3.8119e+01,  2.4307e-02, -5.8850e+01,  2.4307e-02,  2.4307e-02,
          2.4307e-02,  2.4307e-02,  2.4307e-02,  2.4307e-02,  2.4307e-02]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.2296,   2.6666],
        [  5.3848,  -0.1342],
        [  5.0384,   0.8641],
        [-10.3616,  -1.7960],
        [ -2.2986,   2.5076],
        [ -1.1143,   2.7216],
        [ -1.2449,   2.6434],
        [ -1.2517,   2.8327],
        [ -6.3581,  -1.6625],
        [-17.2017,  -6.1046]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.9253e-01, -4.1371e+00, -3.9188e+00, -6.2628e-01, -3.1266e-01,
         -3.9664e-01, -3.9525e-01, -3.5794e-01, -1.0142e+00, -1.1180e+00],
        [-5.2397e+01, -4.6384e+00, -1.5370e+01,  4.8160e+00, -1.2493e+01,
         -5.3273e+01, -5.2293e+01, -4.7552e+01,  8.6534e+00,  1.3145e+01],
        [-1.4691e+01,  1.6337e+00, -5.8319e+00, -3.3083e+00, -1.8650e+01,
         -1.4558e+01, -1.4762e+01, -1.4585e+01,  1.6756e+01, -4.2342e+00],
        [-2.7881e-01, -8.4560e+00, -1.3255e+00,  4.3151e-01, -2.1391e-01,
         -2.9642e-01, -2.6784e-01, -2.2995e-01, -1.9932e+00, -1.7036e+00],
        [-1.3812e+00, -2.2261e+00, -3.2786e+00, -4.4761e+00, -1.7995e+00,
         -1.7347e+00, -1.1103e+00, -2.5169e+00,  7.1576e+00, -2.5834e+00],
        [-9.6325e-03,  2.8229e+00,  7.7218e+00,  1.5105e+00, -4.3703e-02,
          5.3026e-03, -2.0098e-02,  7.4714e-02,  9.6105e-01,  2.4990e+00],
        [ 2.7268e+00,  1.5507e+00,  3.2370e+00,  3.4789e+00,  4.1320e+00,
          2.6491e+00,  2.7874e+00,  2.6026e+00, -1.4182e+00,  5.9967e+00],
        [-4.3773e+01, -3.8615e-01, -7.1749e+00,  9.8417e-01, -9.6160e+00,
         -4.5678e+01, -4.5934e+01, -2.5333e+01,  1.2361e+01,  6.5987e+00],
        [ 2.2653e+00, -8.0430e+00, -1.0572e+01, -2.2838e-01,  5.8095e+00,
          1.5893e+00,  2.7426e+00,  6.4671e-01, -2.2626e-01, -2.5192e-01],
        [-2.3067e+00, -1.5573e+00, -2.7839e+00, -2.3526e+00, -2.6599e+00,
         -2.7532e+00, -1.9773e+00, -3.6980e+00,  7.8887e+00, -9.4990e-01]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-3.4960e-03,  2.3679e+01, -9.4620e+00,  1.2187e+00,  1.5458e+00,
         -5.0991e+00,  5.5066e+00, -1.0387e+01,  6.9770e-02,  1.4917e+00],
        [ 3.5117e-03, -2.3678e+01,  9.4620e+00, -1.2187e+00, -1.5458e+00,
          5.1029e+00, -5.5952e+00,  1.0387e+01, -6.9766e-02, -1.4917e+00]],
       device='cuda:0'))])
xi:  [163.07114]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 658.8192545297445
W_T_median: 371.36243242227414
W_T_pctile_5: 163.29358310459884
W_T_CVAR_5_pct: 12.302762100034624
Average q (qsum/M+1):  49.03690461189516
Optimal xi:  [163.07114]
Expected(across Rb) median(across samples) p_equity:  0.26979493697484336
obj fun:  tensor(-1557.0498, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
loaded xi:  163.07114
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1537.5543775874694
Current xi:  [172.6101]
objective value function right now is: -1537.5543775874694
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1563.5407457676242
Current xi:  [174.47185]
objective value function right now is: -1563.5407457676242
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [179.23846]
objective value function right now is: -1554.638036565918
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.6602]
objective value function right now is: -1553.5882243729839
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1572.4564265247852
Current xi:  [182.1895]
objective value function right now is: -1572.4564265247852
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1580.795493948923
Current xi:  [183.34993]
objective value function right now is: -1580.795493948923
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [183.03748]
objective value function right now is: -1496.9089602772524
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.55458]
objective value function right now is: -1546.699131174146
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.44716]
objective value function right now is: -1564.8244118277566
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.07373]
objective value function right now is: -1574.8601802024748
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1588.438032663219
Current xi:  [183.73763]
objective value function right now is: -1588.438032663219
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.5238]
objective value function right now is: -1580.8513296772894
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.58272]
objective value function right now is: -1565.4253849913973
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [188.16396]
objective value function right now is: -1555.5521832601419
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.87457]
objective value function right now is: -1582.51116898091
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1589.0518909597668
Current xi:  [184.22186]
objective value function right now is: -1589.0518909597668
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.04245]
objective value function right now is: -1556.7176435816893
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.0301]
objective value function right now is: -1565.2373203865668
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.97885]
objective value function right now is: -1562.1770212919657
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.6943]
objective value function right now is: -1535.8416826588325
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.86194]
objective value function right now is: -1579.5510479204356
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.42393]
objective value function right now is: -1543.1682884682996
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.58565]
objective value function right now is: -1573.8143594101364
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1592.6323991089403
Current xi:  [185.4587]
objective value function right now is: -1592.6323991089403
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.88698]
objective value function right now is: -1576.9661752681834
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.8106]
objective value function right now is: -1585.5718429150097
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1594.155890918367
Current xi:  [186.34666]
objective value function right now is: -1594.155890918367
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [186.42442]
objective value function right now is: -1574.7454641749907
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [185.38329]
objective value function right now is: -1574.5886772155036
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.65968]
objective value function right now is: -1589.5970330447308
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.43475]
objective value function right now is: -1558.82700761837
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.80032]
objective value function right now is: -1546.0788031844602
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.7682]
objective value function right now is: -1553.6899129135893
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [183.27989]
objective value function right now is: -1521.806143213971
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.46187]
objective value function right now is: -1572.3034150530839
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1594.6157238058927
Current xi:  [185.69928]
objective value function right now is: -1594.6157238058927
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.63794]
objective value function right now is: -1593.061671298558
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.00829]
objective value function right now is: -1591.2295773008552
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.29378]
objective value function right now is: -1591.1147155012002
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1595.9103252900002
Current xi:  [186.15228]
objective value function right now is: -1595.9103252900002
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.7821]
objective value function right now is: -1590.9387004688115
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.90869]
objective value function right now is: -1593.9297240602762
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.97804]
objective value function right now is: -1595.8923331421197
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.13004]
objective value function right now is: -1589.7238509111014
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.67183]
objective value function right now is: -1594.1345098533398
new min fval from sgd:  -1596.115507447094
new min fval from sgd:  -1596.228644605509
new min fval from sgd:  -1596.5933373471933
new min fval from sgd:  -1596.780762301975
new min fval from sgd:  -1596.8060319885583
new min fval from sgd:  -1597.275830209672
new min fval from sgd:  -1597.5807512423019
new min fval from sgd:  -1597.6041343857107
new min fval from sgd:  -1597.7948536077365
new min fval from sgd:  -1598.0182532212496
new min fval from sgd:  -1598.0787201490357
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.8223]
objective value function right now is: -1598.0291100262614
new min fval from sgd:  -1598.203906285288
new min fval from sgd:  -1598.2552646370848
new min fval from sgd:  -1598.3086139992918
new min fval from sgd:  -1598.5146037678644
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.0594]
objective value function right now is: -1597.413668072721
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.77693]
objective value function right now is: -1589.1473340182454
new min fval from sgd:  -1598.5648465361808
new min fval from sgd:  -1598.5730551366576
new min fval from sgd:  -1598.582627038473
new min fval from sgd:  -1598.596182932759
new min fval from sgd:  -1598.6157724931259
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.69316]
objective value function right now is: -1598.181025285522
new min fval from sgd:  -1598.6451026324978
new min fval from sgd:  -1598.678599911338
new min fval from sgd:  -1598.6915959244511
new min fval from sgd:  -1598.7063587690077
new min fval from sgd:  -1598.7220432928198
new min fval from sgd:  -1598.7373242695069
new min fval from sgd:  -1598.740887811131
new min fval from sgd:  -1598.7463659442951
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.60815]
objective value function right now is: -1598.474047818832
min fval:  -1598.7463659442951
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-5.9675,  2.9909],
        [ 9.3930,  1.3607],
        [-5.7942,  2.9112],
        [-5.8220,  2.9239],
        [-5.8070,  2.9170],
        [-5.8502,  2.9369],
        [ 5.3411, -2.7159],
        [-5.8223,  2.9240],
        [-5.1265,  5.0241],
        [ 5.3797, -2.7338]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 2.1933e+00,  4.5867e+01,  1.7139e+00,  3.5152e+00,  3.0221e+00,
          4.2176e+00, -8.7499e+00,  3.5217e+00,  1.5551e+01, -9.0429e+00],
        [-7.7010e-02, -9.4066e-01, -8.4302e-02, -8.2751e-02, -8.3484e-02,
         -8.1431e-02, -1.4879e+00, -8.2739e-02, -1.9605e-02, -1.4896e+00],
        [-1.6960e+01, -1.4517e+01, -2.3848e+01, -2.4821e+01, -2.4967e+01,
         -2.4350e+01,  8.5972e+00, -2.4815e+01,  2.2331e+01,  8.6551e+00],
        [-7.7010e-02, -9.4066e-01, -8.4302e-02, -8.2751e-02, -8.3484e-02,
         -8.1431e-02, -1.4879e+00, -8.2738e-02, -1.9605e-02, -1.4896e+00],
        [-7.7010e-02, -9.4066e-01, -8.4302e-02, -8.2751e-02, -8.3484e-02,
         -8.1431e-02, -1.4879e+00, -8.2738e-02, -1.9605e-02, -1.4896e+00],
        [-7.7010e-02, -9.4066e-01, -8.4302e-02, -8.2751e-02, -8.3484e-02,
         -8.1431e-02, -1.4879e+00, -8.2739e-02, -1.9605e-02, -1.4896e+00],
        [-7.7010e-02, -9.4066e-01, -8.4302e-02, -8.2751e-02, -8.3484e-02,
         -8.1431e-02, -1.4879e+00, -8.2738e-02, -1.9605e-02, -1.4896e+00],
        [-7.7010e-02, -9.4066e-01, -8.4302e-02, -8.2751e-02, -8.3484e-02,
         -8.1431e-02, -1.4879e+00, -8.2739e-02, -1.9605e-02, -1.4896e+00],
        [-7.7010e-02, -9.4066e-01, -8.4302e-02, -8.2751e-02, -8.3484e-02,
         -8.1431e-02, -1.4879e+00, -8.2738e-02, -1.9605e-02, -1.4896e+00],
        [-7.7010e-02, -9.4066e-01, -8.4302e-02, -8.2751e-02, -8.3484e-02,
         -8.1431e-02, -1.4879e+00, -8.2739e-02, -1.9605e-02, -1.4896e+00]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 4.1740e+01,  3.2982e-04, -6.0591e+01,  3.2988e-04,  3.2982e-04,
          3.2980e-04,  3.2984e-04,  3.2982e-04,  3.2983e-04,  3.2981e-04]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.1357,   3.3302],
        [ -0.6805,   4.1491],
        [  4.4706,   0.9325],
        [-25.9160,  -2.4602],
        [ -3.0693,   3.0598],
        [ -2.1062,   3.3386],
        [ -2.1137,   3.3423],
        [ -2.4112,   3.2070],
        [ -5.0540,  -1.1641],
        [-18.6251,  -5.8714]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.0611e+00, -6.4873e+00, -5.0372e+00,  5.5693e+00, -7.0089e-02,
          1.0927e+00,  1.0714e+00,  8.3790e-01,  6.6745e-02, -6.0801e-01],
        [-5.5777e+01, -1.7368e+01, -1.0836e+01,  6.2325e+00, -3.7619e+01,
         -5.6092e+01, -5.5616e+01, -5.4426e+01,  6.6993e+00,  5.1051e+00],
        [-2.0922e+01, -2.9899e-01, -5.2094e+00,  4.4333e-01, -2.8993e+01,
         -2.0657e+01, -2.1006e+01, -2.1205e+01,  1.0098e+01, -8.8783e+00],
        [-2.8219e-02, -2.2071e-01, -8.1526e+00, -2.7200e-01,  7.0200e-02,
         -3.2123e-02, -3.0382e-02,  1.3065e-04, -2.1851e+00, -1.7566e+00],
        [-2.6892e+00, -2.6380e+00, -7.9303e+00, -1.4558e+00, -9.1890e+00,
         -2.6571e+00, -2.4618e+00, -4.7510e+00,  1.5356e+01, -2.0281e+01],
        [-9.0722e-01,  1.3803e+00,  9.4413e+00,  7.1623e+00,  8.8410e-01,
         -9.5781e-01, -9.3305e-01, -4.7151e-01, -6.2927e+00,  3.3847e+00],
        [ 1.4439e+01,  4.3968e+00,  4.7603e+00,  4.3188e+00,  2.0343e+01,
          1.4244e+01,  1.4576e+01,  1.4174e+01, -5.8281e+00,  3.5753e+00],
        [-5.5814e+01, -1.2773e+01, -6.0674e+00,  5.0057e+00, -2.2679e+01,
         -5.7675e+01, -5.7807e+01, -3.8651e+01,  1.3640e+01,  1.8393e+00],
        [-2.8218e-02, -2.2071e-01, -8.1526e+00, -2.7200e-01,  7.0202e-02,
         -3.2121e-02, -3.0381e-02,  1.3238e-04, -2.1851e+00, -1.7566e+00],
        [-2.8233e-02, -2.2071e-01, -8.1526e+00, -2.7200e-01,  7.0178e-02,
         -3.2136e-02, -3.0395e-02,  1.1451e-04, -2.1850e+00, -1.7566e+00]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.7161,  29.8807,  -9.5046,  -0.0310,   7.0167,  -5.0414,   5.4803,
         -13.7124,  -0.0310,  -0.0310],
        [ -0.7160, -29.8800,   9.5046,   0.0310,  -7.0168,   5.0451,  -5.5689,
          13.7129,   0.0310,   0.0310]], device='cuda:0'))])
xi:  [187.69496]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 677.8215895640901
W_T_median: 420.2891628993185
W_T_pctile_5: 187.8967821487733
W_T_CVAR_5_pct: 21.587509368284344
Average q (qsum/M+1):  48.0906982421875
Optimal xi:  [187.69496]
Expected(across Rb) median(across samples) p_equity:  0.25771340131759646
obj fun:  tensor(-1598.7464, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
loaded xi:  187.69496
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2681.0896349886107
Current xi:  [198.60916]
objective value function right now is: -2681.0896349886107
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [202.5952]
objective value function right now is: -2593.1660909861894
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -2689.332556055786
Current xi:  [204.93344]
objective value function right now is: -2689.332556055786
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -2723.6004323288917
Current xi:  [205.86424]
objective value function right now is: -2723.6004323288917
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [204.06902]
objective value function right now is: -2700.2999386023985
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.60158]
objective value function right now is: -2636.1832989343247
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [209.03642]
objective value function right now is: -2589.2919896588364
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.2287]
objective value function right now is: -1904.0509973794772
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.58156]
objective value function right now is: -2650.6937867236306
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -2731.2429258572474
Current xi:  [209.13475]
objective value function right now is: -2731.2429258572474
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.08376]
objective value function right now is: -2617.8344553574548
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -2780.1934359198976
Current xi:  [208.3144]
objective value function right now is: -2780.1934359198976
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.67073]
objective value function right now is: -2695.330814818306
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [206.95743]
objective value function right now is: -2487.484368290841
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.98645]
objective value function right now is: -2432.790225643909
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.69907]
objective value function right now is: -2687.92095313206
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.99872]
objective value function right now is: -2432.3343425694834
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.28947]
objective value function right now is: -2519.880539698002
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.60687]
objective value function right now is: -2625.7389740046724
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.11475]
objective value function right now is: -2611.517809037717
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.3426]
objective value function right now is: -2778.441573919505
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.87323]
objective value function right now is: -2320.0114056020966
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.90916]
objective value function right now is: -2768.1897519445756
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.89838]
objective value function right now is: -2714.009220199398
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [206.32909]
objective value function right now is: -2397.4304797543455
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.05467]
objective value function right now is: -2559.874315819692
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.465]
objective value function right now is: -2522.84070808809
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [210.9881]
objective value function right now is: -2763.209749382935
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [210.7719]
objective value function right now is: -2689.508423196068
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.11143]
objective value function right now is: -2537.8695174533837
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [205.62769]
objective value function right now is: -2745.333586875498
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.88171]
objective value function right now is: -2640.488358745903
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [207.77113]
objective value function right now is: -2767.8864956624643
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [208.56436]
objective value function right now is: -2642.0615462293745
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [209.53154]
objective value function right now is: -2620.6752926166937
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [210.06543]
objective value function right now is: -2740.5891788394947
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -2782.229341643517
Current xi:  [210.38556]
objective value function right now is: -2782.229341643517
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.13246]
objective value function right now is: -2685.9263946410038
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2817.6062016432197
Current xi:  [211.4438]
objective value function right now is: -2817.6062016432197
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -2831.97885282549
Current xi:  [211.54102]
objective value function right now is: -2831.97885282549
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.47792]
objective value function right now is: -2773.5102814117336
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.498]
objective value function right now is: -2729.1560422155617
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.40337]
objective value function right now is: -2831.857764571675
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.8315]
objective value function right now is: -2789.961351303216
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.79117]
objective value function right now is: -2814.3497893188746
new min fval from sgd:  -2832.001272968482
new min fval from sgd:  -2834.713690449279
new min fval from sgd:  -2837.347986426486
new min fval from sgd:  -2838.4782041483313
new min fval from sgd:  -2839.5519362917253
new min fval from sgd:  -2840.5767023097787
new min fval from sgd:  -2840.9996972165854
new min fval from sgd:  -2841.5752704577862
new min fval from sgd:  -2841.7873856585475
new min fval from sgd:  -2842.9912784136804
new min fval from sgd:  -2844.6875190828555
new min fval from sgd:  -2845.2874354639603
new min fval from sgd:  -2845.4771150496294
new min fval from sgd:  -2845.6109055328934
new min fval from sgd:  -2846.332697784864
new min fval from sgd:  -2847.6433028774804
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.9098]
objective value function right now is: -2820.8520174509176
new min fval from sgd:  -2847.800033736269
new min fval from sgd:  -2849.7396623407285
new min fval from sgd:  -2850.1697880590527
new min fval from sgd:  -2851.213822358277
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.11494]
objective value function right now is: -2813.8797236109835
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.53355]
objective value function right now is: -2816.169767781183
new min fval from sgd:  -2852.026690415102
new min fval from sgd:  -2852.051362262721
new min fval from sgd:  -2852.0984060463716
new min fval from sgd:  -2852.296296985946
new min fval from sgd:  -2852.435288791703
new min fval from sgd:  -2852.4823522824013
new min fval from sgd:  -2852.520350025848
new min fval from sgd:  -2852.5357632329055
new min fval from sgd:  -2852.5536072402515
new min fval from sgd:  -2852.636597129638
new min fval from sgd:  -2853.0062848349094
new min fval from sgd:  -2853.2744141548937
new min fval from sgd:  -2853.3835115566026
new min fval from sgd:  -2853.437055808022
new min fval from sgd:  -2853.4406324936544
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.62733]
objective value function right now is: -2849.9916464430617
new min fval from sgd:  -2853.5198545548046
new min fval from sgd:  -2853.6745085994044
new min fval from sgd:  -2853.6827733739083
new min fval from sgd:  -2853.778923371347
new min fval from sgd:  -2854.122912686552
new min fval from sgd:  -2854.1405599637947
new min fval from sgd:  -2854.326683954529
new min fval from sgd:  -2854.5766854987705
new min fval from sgd:  -2854.6788383168478
new min fval from sgd:  -2854.7448325016553
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.63376]
objective value function right now is: -2854.7448325016553
min fval:  -2854.7448325016553
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-7.1287,  2.4713],
        [10.0333,  1.6454],
        [-6.8421,  2.3546],
        [-6.8734,  2.3699],
        [-6.8545,  2.3615],
        [-6.9089,  2.3854],
        [ 6.3510, -2.2013],
        [-6.8737,  2.3701],
        [-8.3449,  4.9821],
        [ 6.3800, -2.2137]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  2.7444,  51.4272,   2.2472,   3.5997,   3.2212,   4.1436, -10.2792,
           3.6048,  24.1833, -10.5728],
        [  0.8407,  -1.9154,   0.8299,   0.8317,   0.8308,   0.8334,  -1.3381,
           0.8317,   0.2214,  -1.3395],
        [-12.6028, -18.0015, -19.5244, -20.3269, -20.5105, -19.8019,  10.7329,
         -20.3205,  19.7042,  10.7898],
        [  0.8407,  -1.9154,   0.8299,   0.8317,   0.8308,   0.8334,  -1.3381,
           0.8317,   0.2214,  -1.3395],
        [  0.8407,  -1.9154,   0.8299,   0.8317,   0.8308,   0.8334,  -1.3381,
           0.8317,   0.2214,  -1.3395],
        [  0.8407,  -1.9154,   0.8299,   0.8317,   0.8308,   0.8334,  -1.3381,
           0.8317,   0.2214,  -1.3395],
        [  0.8407,  -1.9154,   0.8299,   0.8317,   0.8308,   0.8334,  -1.3381,
           0.8317,   0.2214,  -1.3395],
        [  0.8407,  -1.9154,   0.8299,   0.8317,   0.8308,   0.8334,  -1.3381,
           0.8317,   0.2214,  -1.3395],
        [  0.8407,  -1.9154,   0.8299,   0.8317,   0.8308,   0.8334,  -1.3381,
           0.8317,   0.2214,  -1.3395],
        [  0.8407,  -1.9154,   0.8299,   0.8317,   0.8308,   0.8334,  -1.3381,
           0.8317,   0.2214,  -1.3395]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 48.8139,  -0.9331, -65.3513,  -0.9331,  -0.9331,  -0.9331,  -0.9331,
          -0.9331,  -0.9331,  -0.9331]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.0713,   3.5667],
        [  1.3368,   2.9464],
        [  4.5011,   1.6082],
        [-28.9446,  -2.4371],
        [ -4.6721,   3.2849],
        [ -1.9994,   3.5475],
        [ -2.0536,   3.5579],
        [ -2.3300,   3.7248],
        [ -5.9014,  -1.2151],
        [-16.5193,  -3.9430]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.2341e-02, -1.5257e+00, -1.2701e+01, -5.2747e-01,  3.7302e-02,
         -1.5591e-02, -1.2684e-02, -5.5689e-04, -2.9871e+00, -2.3650e+00],
        [-8.3509e+01, -5.3260e+01, -5.1059e+00,  8.7184e+00, -5.5122e+01,
         -8.4301e+01, -8.3644e+01, -7.8963e+01,  2.1153e+00,  9.1428e+00],
        [-2.4635e+01,  1.7084e+00, -5.7931e+00, -2.1946e+00, -4.5817e+01,
         -2.3904e+01, -2.4554e+01, -2.7340e+01,  8.8296e+00, -3.0397e+01],
        [-1.2300e-02, -1.5252e+00, -1.2702e+01, -5.2756e-01,  3.7350e-02,
         -1.5549e-02, -1.2643e-02, -5.2526e-04, -2.9868e+00, -2.3655e+00],
        [ 7.7671e-01,  3.0771e+00, -8.5197e+00, -5.7673e+00, -5.2539e+00,
          7.8910e-01,  1.0034e+00, -1.1530e+00,  1.1302e+01, -4.7597e+01],
        [-1.6686e+00, -1.2024e+00,  8.3273e+00, -1.7134e+00,  6.2543e+00,
         -1.7795e+00, -1.7412e+00, -7.3642e-01, -5.7702e+00,  5.5451e+00],
        [ 1.7889e+01, -4.9834e-01,  4.0542e+00,  2.7543e+00,  3.1277e+01,
          1.7299e+01,  1.7916e+01,  1.9396e+01, -5.7139e+00,  6.8075e-01],
        [-6.9196e+01, -2.7039e+01, -5.7322e+00,  4.6674e+00, -3.2679e+01,
         -7.1064e+01, -7.1164e+01, -5.2376e+01,  1.5717e+01,  3.3699e+00],
        [-1.2299e-02, -1.5252e+00, -1.2702e+01, -5.2756e-01,  3.7350e-02,
         -1.5548e-02, -1.2641e-02, -5.2471e-04, -2.9868e+00, -2.3655e+00],
        [-1.2300e-02, -1.5252e+00, -1.2702e+01, -5.2756e-01,  3.7349e-02,
         -1.5550e-02, -1.2644e-02, -5.2632e-04, -2.9868e+00, -2.3655e+00]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.5468,  38.1398, -10.3841,   0.5467,   8.5490,  -4.9135,   5.3966,
         -15.4199,   0.5467,   0.5467],
        [ -0.5468, -38.1394,  10.3842,  -0.5467,  -8.5490,   4.9173,  -5.4852,
          15.4199,  -0.5467,  -0.5467]], device='cuda:0'))])
xi:  [212.63376]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 713.4345473292063
W_T_median: 503.8220008276995
W_T_pctile_5: 212.57262149313357
W_T_CVAR_5_pct: 28.776535251903823
Average q (qsum/M+1):  45.67484800277218
Optimal xi:  [212.63376]
Expected(across Rb) median(across samples) p_equity:  0.2217334399620692
obj fun:  tensor(-2854.7448, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
