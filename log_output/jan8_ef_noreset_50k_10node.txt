Starting at: 
08-01-23_14:42

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 50000
batchsize: 1000


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      12  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      12  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1  logistic_sigmoid   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 12)    False        None  
2     (12, 12)    False        None  
3      (12, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      12  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      12  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 12)    False        None  
2     (12, 12)    False        None  
3      (12, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       12  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       12  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 12)     False        None  
0     (12, 12)     False        None  
0      (12, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       12  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       12  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 12)     False        None  
0     (12, 12)     False        None  
0      (12, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1771.7502114875135
Current xi:  [-404.62994]
objective value function right now is: -1771.7502114875135
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1796.948258708631
Current xi:  [-688.31085]
objective value function right now is: -1796.948258708631
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.0917580544804
Current xi:  [-847.9343]
objective value function right now is: -1802.0917580544804
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.253381295804
Current xi:  [-914.06366]
objective value function right now is: -1802.253381295804
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.49526904994
Current xi:  [-928.2248]
objective value function right now is: -1802.49526904994
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-926.40247]
objective value function right now is: -1802.1768478262331
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-931.57654]
objective value function right now is: -1802.4645062854338
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-926.1146]
objective value function right now is: -1802.2010775680092
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.540764492863
Current xi:  [-927.0951]
objective value function right now is: -1802.540764492863
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-924.174]
objective value function right now is: -1802.5313945479522
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-931.42096]
objective value function right now is: -1802.2988803542005
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-924.2106]
objective value function right now is: -1801.7598790955558
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-932.67236]
objective value function right now is: -1801.2413597576015
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1802.6052791913564
Current xi:  [-921.13586]
objective value function right now is: -1802.6052791913564
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-926.56805]
objective value function right now is: -1802.2444017478704
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-931.4764]
objective value function right now is: -1802.0182614961736
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-932.7627]
objective value function right now is: -1802.2740138303473
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-930.48505]
objective value function right now is: -1802.392081731679
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-932.68195]
objective value function right now is: -1802.319999288302
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.6723749377463
Current xi:  [-933.0241]
objective value function right now is: -1802.6723749377463
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-930.88086]
objective value function right now is: -1802.490248164207
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-923.8736]
objective value function right now is: -1802.0177278518422
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-925.9665]
objective value function right now is: -1802.5673926777895
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-922.00806]
objective value function right now is: -1802.165518725215
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-934.3705]
objective value function right now is: -1802.3110446728665
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-922.9383]
objective value function right now is: -1802.4852376725503
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-929.481]
objective value function right now is: -1802.502618746473
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-926.8644]
objective value function right now is: -1802.479765022318
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-918.8616]
objective value function right now is: -1802.1052405099372
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.6954676757653
Current xi:  [-926.29956]
objective value function right now is: -1802.6954676757653
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-931.298]
objective value function right now is: -1802.5161445417618
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-933.1755]
objective value function right now is: -1802.680597422728
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-920.28125]
objective value function right now is: -1802.4991801657595
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-934.08185]
objective value function right now is: -1802.4370895421935
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-922.3726]
objective value function right now is: -1802.5227567504212
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-920.8206]
objective value function right now is: -1802.1927594655726
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-940.271]
objective value function right now is: -1802.3361421028524
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-936.02747]
objective value function right now is: -1802.5499984363007
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-932.9007]
objective value function right now is: -1800.5158020679462
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-934.1361]
objective value function right now is: -1802.032161766565
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-937.7773]
objective value function right now is: -1802.36209548841
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-990.8199]
objective value function right now is: -1800.5238843564014
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-954.03577]
objective value function right now is: -1801.047691984435
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-968.50385]
objective value function right now is: -1802.3813224684638
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-932.5954]
objective value function right now is: -1802.2135551948013
new min fval from sgd:  -1802.6969680599464
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-923.2778]
objective value function right now is: -1802.3380679376012
new min fval from sgd:  -1802.7020750342456
new min fval from sgd:  -1802.7039496782088
new min fval from sgd:  -1802.7126205811053
new min fval from sgd:  -1802.7252710374119
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-934.1815]
objective value function right now is: -1802.4936159276879
new min fval from sgd:  -1802.7558897399383
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-931.0263]
objective value function right now is: -1802.3161747641427
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-927.9892]
objective value function right now is: -1802.3936917926567
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-955.83826]
objective value function right now is: -1802.3951772459466
min fval:  -1802.7558897399383
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -0.5367,   2.9890],
        [ -0.5745,   2.9847],
        [ 26.1101,  11.2923],
        [ -0.4400,   3.0054],
        [ -0.4781,   2.9986],
        [ -0.4868,   2.9970],
        [ -0.5007,   2.9948],
        [-24.7141, -10.1578],
        [-13.9717, -16.5607],
        [ -0.4748,   2.9990],
        [ -0.4748,   2.9993],
        [ -0.4610,   3.0015]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  -8.1236,   -5.3617,    7.2371,   -8.7646,   -9.9628,   -8.9671,
           -8.5201,   -7.1880,  -26.6523,   -8.7466,   -9.7981,   -9.3021],
        [   6.4546,    3.5254,  -10.0778,    5.1625,    6.5451,    6.5816,
            5.5945,    6.6829,   24.5561,    6.0420,    5.9449,    6.3613],
        [   5.4153,    5.9229,   13.1438,    7.0949,    5.2225,    5.8187,
            6.0207,   -8.4977,  -24.4128,    6.4049,    5.2629,    5.6947],
        [  88.6189,   85.8999,  -15.1566,   71.6942,   79.0147,   80.8495,
           82.2877,    1.0607,   16.3681,   78.9646,   77.4577,   76.4324],
        [  -2.5445,   -3.9742,  -13.9530,   -4.3649,   -2.6212,   -2.9186,
           -4.0710,    7.8638,   21.9836,   -3.7844,   -3.6030,   -3.1775],
        [  71.0531,   62.8887,   18.9285,   78.9051,   76.2095,   76.2229,
           74.2049,   -5.8037,    0.7154,   77.2481,   76.0420,   78.1745],
        [  69.6176,   62.1061,   18.2406,   77.2230,   75.3855,   75.4346,
           73.4742,   -5.9044,    0.5851,   76.2591,   74.8404,   76.6739],
        [-133.7149, -134.8517,   21.0812, -106.5638, -116.6204, -119.8715,
         -122.7277,    1.5279,  -22.0891, -116.5332, -115.0737, -112.3902],
        [ -40.0595,  -33.0813,   -2.1917,  -40.2491,  -42.0328,  -42.7078,
          -42.3306,   -2.4192,  -27.1198,  -42.7459,  -41.7942,  -41.8802],
        [-129.7836, -130.2035,   20.1655, -103.5488, -113.4501, -116.6635,
         -119.3451,    1.5568,  -22.3901, -113.5639, -111.5274, -109.4343],
        [  72.2844,   64.1238,   19.9823,   79.6856,   77.8508,   77.4649,
           75.6761,   -5.7628,    0.8019,   78.4985,   77.5164,   79.1581],
        [  13.6193,   13.4873,   12.5389,   14.6149,   13.1931,   14.2540,
           14.1882,   -8.7761,  -24.5190,   14.4696,   13.1980,   14.1752]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 20.4621, -12.3664,  21.8355,  -5.4392, -13.9320,  26.3635,  22.1652,
          53.4375,  25.4925,  44.6208,  32.9366,  21.6672]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -7.9298,   0.6150],
        [-50.0566, -11.8676],
        [ -4.5356,   2.3413],
        [ -4.2362,   2.6256],
        [ -4.5539,   2.2034],
        [ -4.6070,   2.0188],
        [ -4.2296,   2.6350],
        [ 17.5084,   9.6259],
        [ -4.6433,   1.8766],
        [ 25.1134,   9.6152],
        [ -4.0412,   2.7285],
        [ -4.5749,   2.1393]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -63.9055, -160.7035, -113.8017, -133.9484, -117.1257, -105.6858,
         -138.6437, -155.7171,  -91.8805,   -8.2844, -143.8545, -112.2463],
        [ -68.7334, -155.4455, -118.0178, -137.3829, -118.8737, -108.2388,
         -139.3913, -153.9639,  -94.0863,   -8.4290, -145.1992, -113.7768],
        [ -59.6826, -164.1792, -109.2993, -129.8125, -116.1084, -103.5866,
         -136.5121, -157.0477,  -90.9926,   -8.1465, -142.0511, -111.5363],
        [ -72.8211, -152.1099, -117.6915, -137.1320, -116.4368, -107.7118,
         -138.5697, -154.0504,  -93.9233,   -8.4644, -144.2557, -111.9595],
        [  84.1518,    1.4358,   99.1826,   79.2681,  101.5853,  103.5983,
           76.2219,    2.4128,  101.4601,   -2.2966,   67.0344,  101.6943],
        [ -67.9709, -158.6814, -118.3697, -137.8053, -121.1241, -110.0262,
         -143.5992, -149.0277,  -96.0761,   -8.4282, -148.6339, -116.3180],
        [ -73.7251, -150.0378, -124.1499, -142.9676, -122.4791, -110.7968,
         -143.7036, -150.1725,  -97.8778,   -8.6189, -149.7277, -117.7705],
        [ -68.7460, -156.7431, -120.7382, -139.4547, -122.3322, -110.2906,
         -143.3084, -149.4093,  -96.7666,   -8.4818, -148.8080, -117.7749],
        [ -13.3208, -127.5685,  -61.5159,  -79.5988,  -48.5177,  -36.7429,
          -75.8910,  -15.6716,  -28.1879,   -2.4619,  -83.6316,  -43.9067],
        [ 125.6368,   -5.0773,   14.5336,   -4.7609,   47.0798,   61.5201,
           -5.6474,  -65.8892,   69.2901, -121.7935,   -8.8685,   51.6933],
        [  46.6198,  -33.5334,   15.4197,    9.3419,   21.9128,   27.2270,
            9.4585, -114.7731,   30.3393,  -44.0604,    7.9410,   23.4997],
        [ -65.5311, -149.5879, -120.6070, -148.8443, -115.8946, -101.5385,
         -143.4737, -164.6972,  -91.6956,   -8.5102, -150.4746, -110.5491]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-20.4637, -18.1771, -22.9136, -17.5634,   0.6779, -18.9245, -16.0207,
         -17.8609,  -8.2703,   8.9343, -12.9522, -17.8391],
        [ 20.4104,  18.2319,  22.9331,  17.7443,  -0.5708,  18.9741,  16.2689,
          18.0210,   8.2020,  -8.9401,  13.2514,  17.5536]], device='cuda:0'))])
xi:  [-922.63055]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: -236.91794630456948
W_T_median: -392.91562056518137
W_T_pctile_5: -916.912139377659
W_T_CVAR_5_pct: -1038.3677378056175
Average q (qsum/M+1):  59.82837701612903
Optimal xi:  [-922.63055]
Expected(across Rb) median(across samples) p_equity:  0.1591949994750365
obj fun:  tensor(-1802.7559, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1685.844094648404
Current xi:  [-231.05974]
objective value function right now is: -1685.844094648404
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1696.66311397673
Current xi:  [-360.57843]
objective value function right now is: -1696.66311397673
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1697.7281575203408
Current xi:  [-416.33847]
objective value function right now is: -1697.7281575203408
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1698.701364345206
Current xi:  [-442.1356]
objective value function right now is: -1698.701364345206
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1699.2269253223692
Current xi:  [-436.71817]
objective value function right now is: -1699.2269253223692
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-433.62665]
objective value function right now is: -1698.3762982321664
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-425.27203]
objective value function right now is: -1697.4457020503971
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-418.37653]
objective value function right now is: -1698.1550395677064
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1699.2737949213292
Current xi:  [-434.4937]
objective value function right now is: -1699.2737949213292
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1699.3686022327925
Current xi:  [-444.53925]
objective value function right now is: -1699.3686022327925
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1699.6767254805345
Current xi:  [-444.60318]
objective value function right now is: -1699.6767254805345
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-435.35114]
objective value function right now is: -1698.4187295043537
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-447.29053]
objective value function right now is: -1631.3292732047992
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-433.67346]
objective value function right now is: -1698.7100496057071
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-419.9671]
objective value function right now is: -1699.3502796838175
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-417.9719]
objective value function right now is: -1699.3047147714133
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1699.7030679351128
Current xi:  [-428.89902]
objective value function right now is: -1699.7030679351128
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1700.4813389159515
Current xi:  [-426.7406]
objective value function right now is: -1700.4813389159515
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-447.17786]
objective value function right now is: -1700.2051466654727
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-430.79584]
objective value function right now is: -1699.0558419179597
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1700.760867121725
Current xi:  [-428.96057]
objective value function right now is: -1700.760867121725
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-419.94302]
objective value function right now is: -1700.616567168151
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-431.399]
objective value function right now is: -1700.4403026533942
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-417.26514]
objective value function right now is: -1699.3702004731908
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1700.8766933240988
Current xi:  [-432.9754]
objective value function right now is: -1700.8766933240988
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-430.59418]
objective value function right now is: -1698.5735646233045
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-457.91086]
objective value function right now is: -1683.3671886613429
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-458.561]
objective value function right now is: -1692.2069525939633
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-462.7279]
objective value function right now is: -1693.2712656772612
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-488.42514]
objective value function right now is: -1690.6917786667395
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.76617]
objective value function right now is: -1693.0909310802929
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-459.12878]
objective value function right now is: -1691.7456616255608
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-468.26276]
objective value function right now is: -1690.6098771748245
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-457.74448]
objective value function right now is: -1692.846311839473
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-460.87314]
objective value function right now is: -1692.989124316096
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-453.64084]
objective value function right now is: -1693.141490720969
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-455.3397]
objective value function right now is: -1692.8316008148656
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-457.0731]
objective value function right now is: -1692.2370079499487
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-450.11038]
objective value function right now is: -1693.6068473004386
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-458.2095]
objective value function right now is: -1693.3805821112112
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-464.2505]
objective value function right now is: -1693.4162569438008
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-473.51617]
objective value function right now is: -1693.5369309074172
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-472.1058]
objective value function right now is: -1693.2977506907364
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-464.79996]
objective value function right now is: -1693.7404982553503
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-464.5327]
objective value function right now is: -1691.2142057563792
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-458.46875]
objective value function right now is: -1693.3090086702384
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-452.7038]
objective value function right now is: -1692.5266624953692
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-458.54086]
objective value function right now is: -1693.5160763448766
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-478.0133]
objective value function right now is: -1691.7615500903303
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-467.56522]
objective value function right now is: -1676.2070411516981
min fval:  -1698.3726319954237
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  0.9864,   4.4839],
        [  0.9871,   4.4868],
        [ 26.9534,  16.1697],
        [  0.9857,   4.4818],
        [  0.9859,   4.4825],
        [  0.9858,   4.4824],
        [  0.9860,   4.4830],
        [-25.6630, -14.7142],
        [-19.9573, -12.1187],
        [  0.9858,   4.4821],
        [  0.9859,   4.4825],
        [  0.9858,   4.4819]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -10.5119,   -7.7543,    5.2025,  -11.1391,  -12.3418,  -11.3472,
          -10.9021,  -10.1193,  -28.4545,  -11.1251,  -12.1765,  -11.6789],
        [   9.2983,    6.3732,   -8.4063,    7.9926,    9.3797,    9.4173,
            8.4320,    8.7342,   25.5574,    8.8762,    8.7789,    9.1937],
        [   2.2861,    2.7880,   11.0583,    3.9834,    2.1052,    2.6998,
            2.8994,  -11.2124,  -25.5702,    3.2880,    2.1463,    2.5801],
        [  94.6154,   91.8659,  -16.9352,   77.7582,   85.0509,   86.8814,
           88.3084,    3.9605,   16.1568,   85.0046,   83.4953,   82.4812],
        [   2.4274,    1.0110,  -13.6445,    0.5739,    2.3285,    2.0345,
            0.8867,    9.1713,   21.3553,    1.1646,    1.3453,    1.7669],
        [  87.3390,   79.1840,   16.3155,   95.2587,   92.5369,   92.5431,
           90.5182,   -5.0248,   -0.9465,   93.5752,   92.3735,   94.5116],
        [  85.4143,   77.9077,   11.2277,   93.0970,   91.2300,   91.2711,
           89.3028,   -5.6163,   -1.7180,   92.1031,   90.6894,   92.5289],
        [-135.1793, -136.2693,   21.5870, -108.1411, -118.1545, -121.3967,
         -124.2358,   -0.7237,  -21.9374, -118.0719, -116.6108, -113.9440],
        [ -43.4550,  -36.4752,   -5.0883,  -43.6456,  -45.4289,  -46.1038,
          -45.7264,   -6.1607,  -29.9744,  -46.1421,  -45.1904,  -45.2765],
        [-129.5556, -129.9414,   21.9568, -103.4057, -113.2739, -116.4807,
         -119.1495,   -0.3387,  -22.4846, -113.3914, -111.3536, -109.2735],
        [  88.8262,   80.6784,   18.4540,   96.2853,   94.4282,   94.0349,
           92.2417,   -4.6266,   -0.3610,   95.0743,   94.0973,   95.7427],
        [  10.6546,   10.5170,   10.3469,   11.6677,   10.2402,   11.2995,
           11.2313,  -11.6742,  -25.9748,   11.5170,   10.2458,   11.2249]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 17.6769, -13.1987,  18.5821,  -6.2452, -14.7642,  24.7344,  19.9111,
          53.6606,  22.1144,  45.9688,  32.4831,  18.3992]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -4.1749,   0.2528],
        [-48.3069,  -3.6499],
        [ -7.0459,   2.3852],
        [ -6.7093,   2.4993],
        [ -7.2086,   2.3534],
        [ -7.3306,   2.3139],
        [ -6.6800,   2.5040],
        [  9.8963,   9.5950],
        [ -7.3977,   2.2944],
        [ 24.1044,   7.5820],
        [ -6.5017,   2.5594],
        [ -7.2444,   2.3418]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[  -7.8420, -216.0810, -118.3516, -139.6959, -127.4767, -117.4418,
         -144.8294, -282.2742, -102.9620,  -41.3291, -150.5325, -123.0856],
        [ -10.5784, -214.5214, -113.3822, -133.7571, -119.2975, -109.9761,
         -136.1761, -277.1125,  -95.3257,  -41.8339, -142.3637, -114.6538],
        [  -5.2856, -217.2216, -122.2524, -143.9998, -135.2400, -124.1795,
         -151.1463, -283.1681, -110.7673,  -40.3093, -157.1767, -131.1785],
        [ -14.8204, -211.4424, -104.0827, -124.3091, -106.3325,  -98.6785,
         -125.9439, -267.7992,  -84.5161,  -42.2781, -131.8327, -102.2248],
        [  50.6578,   -1.1275,   83.4655,   61.9267,   93.8150,   99.6817,
           58.6609,    2.3715,   99.6691,   -3.6991,   48.4948,   94.9057],
        [ -10.3972, -219.0209, -113.3650, -133.7847, -121.0979, -111.3419,
         -139.9947, -273.3390,  -96.9997,  -42.7072, -145.3939, -116.7507],
        [ -17.3628, -206.5886,  -98.6698, -118.2018, -100.3335,  -89.9433,
         -119.1947, -251.5792,  -77.0280,  -43.1997, -125.4149,  -96.0484],
        [ -10.7877, -217.0983, -113.6502, -133.2660, -120.0591, -109.3663,
         -137.5298, -271.9691,  -95.5151,  -42.5717, -143.3516, -115.9610],
        [  57.9172,  -54.6292, -177.6835, -197.3831, -177.8130, -169.9141,
         -193.7837,  -29.7746, -160.8487,   -4.9525, -202.7680, -174.4845],
        [  82.1768,   -7.5478,  -75.0409,  -94.4496,  -37.9215,  -21.2220,
          -95.2757, -584.7355,  -12.9108, -294.7909,  -98.8872,  -32.7217],
        [  47.4018,  -12.5036, -149.4573, -157.8995, -148.0914, -142.6494,
         -157.8152, -585.3307, -138.9663,  -20.4247, -161.9530, -146.6527],
        [  17.9520, -173.0797,  -38.6770,  -64.5634,  -38.4184,  -27.3044,
          -60.1778, -362.3139,  -19.2636,  -13.7999,  -66.6199,  -34.3008]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-15.4999, -12.8834, -17.9992, -11.1422,   0.7017, -12.9945,  -5.6346,
         -11.4741, -10.6232,  27.4263,  45.7589, -13.7708],
        [ 15.4465,  12.9381,  18.0186,  11.3228,  -0.5945,  13.0439,   5.8827,
          11.6339,  10.5554, -27.4310, -45.4595,  13.4855]], device='cuda:0'))])
xi:  [-464.5327]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 39.24926524933984
W_T_median: -178.61057510131337
W_T_pctile_5: -436.71706548293827
W_T_CVAR_5_pct: -474.838074914275
Average q (qsum/M+1):  57.93214071950605
Optimal xi:  [-464.5327]
Expected(across Rb) median(across samples) p_equity:  0.23561679448327064
obj fun:  tensor(-1698.3726, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1567.2567187525422
Current xi:  [-164.88542]
objective value function right now is: -1567.2567187525422
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1581.6832931588842
Current xi:  [-161.90634]
objective value function right now is: -1581.6832931588842
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1585.9818476978605
Current xi:  [-157.7746]
objective value function right now is: -1585.9818476978605
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1587.5381153908145
Current xi:  [-155.43945]
objective value function right now is: -1587.5381153908145
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-156.14687]
objective value function right now is: -1587.2239797414843
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-153.8918]
objective value function right now is: -1587.4698071727278
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-154.6339]
objective value function right now is: -1587.4877294450346
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-151.9776]
objective value function right now is: -1586.8938865560513
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-151.73042]
objective value function right now is: -1583.8255529804617
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-152.25697]
objective value function right now is: -1586.2915961497336
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-151.82697]
objective value function right now is: -1586.3394674955716
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1588.570384554157
Current xi:  [-150.75775]
objective value function right now is: -1588.570384554157
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-148.78905]
objective value function right now is: -1587.582779173351
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-151.35289]
objective value function right now is: -1586.9901250534062
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1588.6848573431446
Current xi:  [-149.99976]
objective value function right now is: -1588.6848573431446
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-147.02339]
objective value function right now is: -1588.2963164850455
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-146.31255]
objective value function right now is: -1588.2992295823135
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-148.609]
objective value function right now is: -1587.2507095808312
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-146.68584]
objective value function right now is: -1588.350412717962
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-146.3179]
objective value function right now is: -1588.6674195603075
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1588.8958910191488
Current xi:  [-145.99042]
objective value function right now is: -1588.8958910191488
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-145.74902]
objective value function right now is: -1588.7108137885843
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-148.4686]
objective value function right now is: -1586.705313121874
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-146.24391]
objective value function right now is: -1588.1104094179375
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-146.55656]
objective value function right now is: -1585.8789051307967
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1588.9871706631675
Current xi:  [-148.3729]
objective value function right now is: -1588.9871706631675
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-148.44368]
objective value function right now is: -1588.4712496872094
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-149.86572]
objective value function right now is: -1588.7181035788385
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-150.37117]
objective value function right now is: -1588.7097276147056
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-144.45203]
objective value function right now is: -1586.2888880474004
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1589.116816776148
Current xi:  [-147.00691]
objective value function right now is: -1589.116816776148
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-146.04362]
objective value function right now is: -1586.7689810088998
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-145.20467]
objective value function right now is: -1588.3807630302053
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-145.81798]
objective value function right now is: -1588.1425217390345
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-145.34267]
objective value function right now is: -1588.5829824585644
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-146.96246]
objective value function right now is: -1588.702002488646
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-146.03093]
objective value function right now is: -1587.8359724423037
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-145.85718]
objective value function right now is: -1587.899447825424
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-146.63028]
objective value function right now is: -1588.412597038169
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1589.20607065044
Current xi:  [-145.01872]
objective value function right now is: -1589.20607065044
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-144.59984]
objective value function right now is: -1589.0841157834461
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-145.57129]
objective value function right now is: -1588.900916489222
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-148.76573]
objective value function right now is: -1587.0439564937742
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-146.53793]
objective value function right now is: -1589.1638165673155
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-144.95547]
objective value function right now is: -1587.8863955124637
new min fval from sgd:  -1589.2531292222193
new min fval from sgd:  -1589.2554272203831
new min fval from sgd:  -1589.2771280936506
new min fval from sgd:  -1589.2962035500589
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-145.93623]
objective value function right now is: -1589.1738259643475
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-145.77692]
objective value function right now is: -1588.663245417412
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-146.46086]
objective value function right now is: -1586.0887823358141
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-145.20395]
objective value function right now is: -1587.0459036081731
new min fval from sgd:  -1589.2970659282796
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-144.47853]
objective value function right now is: -1589.2317672504964
min fval:  -1589.2970659282796
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  0.4491,   5.6120],
        [  0.4474,   5.6159],
        [  7.1852,   9.0171],
        [  0.4498,   5.6083],
        [  0.4476,   5.6097],
        [  0.4477,   5.6095],
        [  0.4474,   5.6105],
        [ -4.0310, -38.1855],
        [ -0.6027, -32.0956],
        [  0.4481,   5.6091],
        [  0.4485,   5.6098],
        [  0.4493,   5.6087]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -10.7129,   -7.9552,    4.6913,  -11.3403,  -12.5430,  -11.5485,
          -11.1032,  -10.3982,  -28.6638,  -11.3264,  -12.3777,  -11.8801],
        [   9.6716,    6.7462,   -7.9272,    8.3661,    9.7531,    9.7908,
            8.8054,    9.1816,   25.8951,    9.2497,    9.1523,    9.5672],
        [   1.9262,    2.4289,   10.4154,    3.6237,    1.7451,    2.3402,
            2.5398,  -11.6891,  -25.9377,    2.9283,    1.7863,    2.2205],
        [  96.1036,   93.3527,  -15.1140,   79.2478,   86.5402,   88.3708,
           89.7974,    6.5279,   18.1855,   86.4940,   84.9845,   83.9706],
        [   3.0899,    1.6733,  -13.2592,    1.2365,    2.9911,    2.6972,
            1.5493,    9.7933,   21.8376,    1.8273,    2.0079,    2.4295],
        [ 111.3195,  103.2138, -231.6965,  119.1847,  116.4687,  116.4687,
          114.4597,   -3.2979,   -1.1435,  117.4992,  116.3134,  118.4406],
        [ 103.4751,   96.0382,  -50.7766,  111.1383,  109.2919,  109.3498,
          107.3821,   -6.2112,   -3.9411,  110.1677,  108.7368,  110.5741],
        [-136.0718, -137.1612,   19.7498, -109.0346, -119.0480, -122.2901,
         -125.1291,   -3.8609,  -24.5467, -118.9654, -117.5042, -114.8375],
        [ -43.7185,  -36.7385,   -5.6077,  -43.9093,  -45.6926,  -46.3675,
          -45.9901,   -6.4568,  -30.2000,  -46.4058,  -45.4541,  -45.5402],
        [-130.4241, -130.8094,   20.1768, -104.2740, -114.1422, -117.3490,
         -120.0178,   -3.4903,  -25.1341, -114.2597, -112.2219, -110.1418],
        [ 119.3107,  111.2350,  -50.6327,  126.7191,  124.8795,  124.4911,
          122.7095,   -3.7848,   -1.1759,  125.5206,  124.5477,  126.1810],
        [  10.3261,   10.1888,    9.5345,   11.3390,    9.9115,   10.9708,
           10.9027,  -12.3366,  -26.5343,   11.1884,    9.9171,   10.8962]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 17.2346, -13.3800,  18.2381,  -6.4265, -14.9455,  23.3948,  26.3957,
          52.5677,  21.6581,  44.9399,  34.2968,  18.0237]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.8952,  -1.0819],
        [-24.4776,   7.0816],
        [ -4.4647,   4.0236],
        [ -4.3966,   4.1018],
        [ -4.8313,   2.0393],
        [ -4.9848,  -1.3767],
        [ -4.2930,   4.1396],
        [ 13.9276,  10.9689],
        [ -3.2456,  -2.8446],
        [ 28.5618,   8.7799],
        [ -4.1617,   4.0113],
        [ -4.7949,   1.5554]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -17.0447, -223.2363, -161.1736, -183.1857, -170.0246, -159.3719,
         -188.0185, -291.9415, -144.2231,  -21.7894, -193.5953, -165.3637],
        [ -19.8654, -219.7758, -151.0318, -172.0353, -156.5930, -146.6436,
         -174.1807, -286.4320, -131.3817,  -22.3063, -180.2711, -151.6712],
        [ -14.1887, -226.6779, -172.1883, -194.5570, -185.0023, -173.3087,
         -201.3748, -292.9716, -159.1880,  -20.7245, -207.2219, -180.6787],
        [ -24.0825, -214.9676, -136.6862, -157.5131, -138.5268, -130.1826,
         -158.9088, -276.7522, -115.4388,  -22.6778, -164.7506, -134.1225],
        [  39.6398, -129.1591,  125.8234,  104.4463,  139.6574,  147.1107,
          100.8063,    3.2601,  147.7079,   -2.7438,   91.2758,  141.0600],
        [ -20.0882, -223.8512, -149.7657, -170.8635, -157.1057, -146.6910,
         -176.8106, -282.5223, -131.7309,  -23.3194, -182.1478, -152.4696],
        [ -28.8389, -207.7686, -125.4441, -145.2693, -126.8107, -115.5519,
         -146.0960, -259.7542, -101.9573,  -26.7232, -152.2472, -122.2442],
        [ -20.4788, -221.3629, -149.0091, -169.2230, -155.0291, -143.6754,
         -173.2323, -280.9750, -129.2272,  -23.1773, -178.9713, -150.6399],
        [  49.7451, -210.9428, -237.1403, -258.9533, -235.7839, -227.1385,
         -255.6207,  -18.5972, -217.6209,   -3.2999, -265.7391, -232.1873],
        [  87.3687,  -11.1428, -118.2633, -138.2791,  -79.8024,  -63.5800,
         -139.1356, -769.7421,  -55.4574, -302.6262, -142.8912,  -74.3832],
        [  31.5207,  126.6159, -215.5392, -223.8701, -213.1106, -207.4444,
         -224.2332, -958.8752, -203.9612,  -18.4596, -228.5331, -211.6682],
        [  25.6785, -172.7357,  -65.9896,  -91.1454,  -60.3887,  -47.8306,
          -87.4095, -557.3551,  -40.0865,  -14.1636,  -93.1939,  -56.0546]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-12.2114,  -9.9347, -14.4276,  -8.6779,   0.2800, -10.0607,  -4.0086,
          -8.5871,  -9.8898,  45.4524,  61.5125,  -9.7923],
        [ 12.1501,   9.9808,  14.4406,   8.8492,  -0.1721,  10.1005,   4.2442,
           8.7374,   9.8228, -45.4628, -61.2133,   9.5075]], device='cuda:0'))])
xi:  [-145.36275]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 503.237688920569
W_T_median: 74.3540571219438
W_T_pctile_5: -145.50709188498033
W_T_CVAR_5_pct: -218.16207206296608
Average q (qsum/M+1):  54.78642026839718
Optimal xi:  [-145.36275]
Expected(across Rb) median(across samples) p_equity:  0.38884586095809937
obj fun:  tensor(-1589.2971, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1494.8834496840686
Current xi:  [-66.480835]
objective value function right now is: -1494.8834496840686
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [-62.683697]
objective value function right now is: -1485.6872777873864
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-67.873146]
objective value function right now is: -1492.9780927404652
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-71.080635]
objective value function right now is: -1483.428133328495
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1495.2426357918312
Current xi:  [-66.3002]
objective value function right now is: -1495.2426357918312
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-66.0365]
objective value function right now is: -1483.0295448429686
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1497.7902855971
Current xi:  [-60.599613]
objective value function right now is: -1497.7902855971
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-64.14045]
objective value function right now is: -1492.9539854714196
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-61.96095]
objective value function right now is: -1497.678163791591
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-65.821724]
objective value function right now is: -1442.2999321062587
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-64.94645]
objective value function right now is: -1479.7729051939157
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-66.169586]
objective value function right now is: -1494.983034949414
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-64.94628]
objective value function right now is: -1496.3515717453727
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-62.710087]
objective value function right now is: -1493.0255151606207
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-63.96735]
objective value function right now is: -1494.2964995722348
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-60.803715]
objective value function right now is: -1492.7484584841663
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-67.78063]
objective value function right now is: -1494.1763147194422
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-62.267258]
objective value function right now is: -1472.0456148618036
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-61.089428]
objective value function right now is: -1484.9441445112611
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-59.833168]
objective value function right now is: -1493.2134272331273
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1498.1308751092442
Current xi:  [-62.262184]
objective value function right now is: -1498.1308751092442
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-63.414932]
objective value function right now is: -1496.1428175589383
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-65.054245]
objective value function right now is: -1486.8916610000497
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-61.086502]
objective value function right now is: -1489.1896906937072
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-62.877403]
objective value function right now is: -1489.5074949096747
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-62.213554]
objective value function right now is: -1488.498891110409
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-65.048355]
objective value function right now is: -1496.1989822168382
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-60.285263]
objective value function right now is: -1496.9826946530004
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-64.52247]
objective value function right now is: -1490.0066803420186
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-61.307728]
objective value function right now is: -1484.5011853727183
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-60.738823]
objective value function right now is: -1492.6445012900629
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.33289]
objective value function right now is: -1498.1074602545873
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-59.237278]
objective value function right now is: -1497.97776059028
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1499.572910818157
Current xi:  [-63.84388]
objective value function right now is: -1499.572910818157
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-60.642727]
objective value function right now is: -1492.4904455955539
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-58.57694]
objective value function right now is: -1496.346731636103
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-56.999928]
objective value function right now is: -1492.614912019805
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-55.637596]
objective value function right now is: -1469.002997686168
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-65.88605]
objective value function right now is: -1485.202662342264
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-61.77152]
objective value function right now is: -1496.5342590581565
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.38969]
objective value function right now is: -1484.4097452476597
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-76.109985]
objective value function right now is: -1484.700266508245
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.00497]
objective value function right now is: -1482.0335066535258
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.44411]
objective value function right now is: -1481.9537719425707
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.371796]
objective value function right now is: -1481.0733738061151
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-80.194786]
objective value function right now is: -1481.49406553418
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-81.5458]
objective value function right now is: -1480.3206689586677
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-77.06656]
objective value function right now is: -1478.8929614377887
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.013275]
objective value function right now is: -1455.7737594175117
new min fval from sgd:  -1499.7659127681616
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-64.23528]
objective value function right now is: -1482.7654704272559
min fval:  -1499.7659127681616
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  0.2456,   6.0518],
        [  0.2937,   6.0145],
        [  7.2581,  10.7597],
        [  0.2076,   6.0770],
        [  0.2033,   6.0824],
        [  0.2064,   6.0797],
        [  0.2146,   6.0748],
        [  4.6773, -48.9132],
        [ 12.7140, -43.4027],
        [  0.2035,   6.0814],
        [  0.2105,   6.0770],
        [  0.2074,   6.0777]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -10.7129,   -7.9552,    4.6913,  -11.3403,  -12.5430,  -11.5485,
          -11.1032,  -10.3982,  -28.6638,  -11.3264,  -12.3777,  -11.8801],
        [   9.6716,    6.7462,   -7.9272,    8.3661,    9.7531,    9.7908,
            8.8054,    9.1816,   25.8951,    9.2497,    9.1523,    9.5672],
        [   1.8901,    2.3918,   10.1050,    3.5885,    1.7099,    2.3050,
            2.5044,  -12.0183,  -26.2671,    2.8932,    1.7509,    2.1852],
        [  96.1036,   93.3527,  -15.1140,   79.2478,   86.5402,   88.3708,
           89.7974,    6.5279,   18.1855,   86.4940,   84.9845,   83.9706],
        [   3.0899,    1.6733,  -13.2592,    1.2365,    2.9911,    2.6972,
            1.5493,    9.7933,   21.8376,    1.8273,    2.0079,    2.4295],
        [ 149.2704,  141.5106, -458.0671,  156.7912,  154.1142,  154.1029,
          152.1817,   -3.9320,   -1.7776,  155.1086,  154.0046,  156.0627],
        [  57.7375,   50.3502, -125.1616,   65.3402,   63.6117,   63.6217,
           61.6811,   26.1930,   28.4631,   64.4375,   63.0290,   64.8030],
        [-136.0718, -137.1612,   19.7498, -109.0346, -119.0480, -122.2901,
         -125.1291,   -3.8706,  -24.5565, -118.9654, -117.5042, -114.8375],
        [ -43.7185,  -36.7385,   -5.6077,  -43.9093,  -45.6926,  -46.3675,
          -45.9901,   -6.4568,  -30.2000,  -46.4058,  -45.4541,  -45.5402],
        [-130.4241, -130.8094,   20.1768, -104.2740, -114.1422, -117.3490,
         -120.0178,   -3.4983,  -25.1418, -114.2597, -112.2219, -110.1418],
        [ 116.2207,  105.4306,  -81.1668,  126.3475,  124.7037,  124.3060,
          121.8649,   -6.1644,   -3.5555,  125.4644,  123.8062,  125.7885],
        [  28.3699,   27.3580,   -1.1331,   30.1828,   28.7512,   29.7829,
           29.5257,  -12.5477,  -26.7449,   30.0639,   28.6290,   29.7221]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 17.2346, -16.9676,  18.2255, -10.0139, -18.5333,  47.0274,  24.6024,
          52.5677,  21.6581,  44.9399,  32.8571,  19.7216]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -6.4826,   0.5066],
        [-37.6115,  14.1718],
        [  6.3467,  -0.4821],
        [  4.4613,   0.9693],
        [  4.3540,   0.7979],
        [  2.7094,   0.1339],
        [  4.9712,   0.5883],
        [  5.5740,   4.1049],
        [ 25.5794,   6.9639],
        [ 30.2987,   8.8833],
        [  4.4646,   0.9525],
        [  4.6841,   0.5578]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -17.0447, -223.2363, -161.1736, -183.1857, -170.0246, -159.3719,
         -188.0185, -291.9415, -144.2231,  -21.7894, -193.5953, -165.3637],
        [ -19.8654, -219.7758, -151.0318, -172.0353, -156.5930, -146.6436,
         -174.1807, -286.4320, -131.3817,  -22.3063, -180.2711, -151.6712],
        [ -14.1887, -226.6779, -172.1883, -194.5570, -185.0023, -173.3087,
         -201.3748, -292.9716, -159.1880,  -20.7245, -207.2219, -180.6787],
        [ -24.0825, -214.9676, -136.6862, -157.5131, -138.5268, -130.1826,
         -158.9088, -276.7522, -115.4388,  -22.6778, -164.7506, -134.1225],
        [  39.6398, -129.1591,  125.8234,  104.4463,  139.6574,  147.1107,
          100.8063,    3.2601,  147.7079,   -2.7438,   91.2758,  141.0600],
        [ -20.0882, -223.8512, -149.7657, -170.8635, -157.1057, -146.6910,
         -176.8106, -282.5223, -131.7309,  -23.3194, -182.1478, -152.4696],
        [ -28.8389, -207.7686, -125.4441, -145.2693, -126.8107, -115.5519,
         -146.0960, -259.7542, -101.9573,  -26.7232, -152.2472, -122.2442],
        [ -20.4788, -221.3629, -149.0091, -169.2230, -155.0291, -143.6754,
         -173.2323, -280.9750, -129.2272,  -23.1773, -178.9713, -150.6399],
        [  49.8749, -210.9428, -237.1266, -258.9489, -235.7800, -227.1376,
         -255.6147,  -18.5961, -217.6189,   -3.2754, -265.7347, -232.1825],
        [  88.9922,    1.0302, -140.0858, -160.5608,  -99.8482,  -65.9594,
         -161.4077, -803.1593,  -50.1925, -294.0198, -164.9350,  -94.6735],
        [  32.8073,  126.6159, -213.7051, -222.4867, -211.7916, -206.3608,
         -222.7355, -957.5829, -202.7527,  -17.1321, -227.1473, -210.2905],
        [  35.9539, -173.1362,  -53.6929,  -74.6749,  -43.5703,  -30.5943,
          -72.0268, -535.7640,  -30.3784,   -7.6395,  -76.7651,  -39.9250]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-12.2114,  -9.9347, -14.4276,  -8.6779,   0.3774, -10.0607,  -4.0086,
          -8.5871,  -8.5948,  45.0835,  62.7785, -18.2120],
        [ 12.1501,   9.9808,  14.4406,   8.8492,  -0.2695,  10.1005,   4.2442,
           8.7374,   8.5276, -45.0936, -62.4793,  17.9272]], device='cuda:0'))])
xi:  [-65.573135]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 452.9242820070187
W_T_median: 99.1161743203241
W_T_pctile_5: -60.946090302496614
W_T_CVAR_5_pct: -158.94556474923797
Average q (qsum/M+1):  53.511132024949596
Optimal xi:  [-65.573135]
Expected(across Rb) median(across samples) p_equity:  0.343680739402771
obj fun:  tensor(-1499.7659, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1412.2909432247905
Current xi:  [-28.941298]
objective value function right now is: -1412.2909432247905
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1423.3126983598802
Current xi:  [-20.78732]
objective value function right now is: -1423.3126983598802
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1429.5983798928235
Current xi:  [-20.554266]
objective value function right now is: -1429.5983798928235
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-28.839598]
objective value function right now is: -1425.7972905213346
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-24.296782]
objective value function right now is: -1425.0712217752884
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1430.6555424384378
Current xi:  [-24.35787]
objective value function right now is: -1430.6555424384378
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1454.4284434115816
Current xi:  [-18.867899]
objective value function right now is: -1454.4284434115816
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-24.65594]
objective value function right now is: -1430.1187600516616
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-16.047308]
objective value function right now is: -1428.0479150876631
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-24.257004]
objective value function right now is: -1429.3676958151268
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1457.8830187611325
Current xi:  [14.02358]
objective value function right now is: -1457.8830187611325
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1459.0138300697915
Current xi:  [12.821712]
objective value function right now is: -1459.0138300697915
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.148082]
objective value function right now is: -1430.2882404989703
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1460.9477872712994
Current xi:  [16.727467]
objective value function right now is: -1460.9477872712994
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.758312]
objective value function right now is: -1459.5241218463739
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-24.874283]
objective value function right now is: -1444.6664976284937
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-26.168695]
objective value function right now is: -1428.928729806864
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-19.626667]
objective value function right now is: -1419.573050505763
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1464.4589469591965
Current xi:  [11.132583]
objective value function right now is: -1464.4589469591965
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [18.674955]
objective value function right now is: -1462.1007819264396
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.142418]
objective value function right now is: -1463.5933323549975
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.216895]
objective value function right now is: -1460.9579839974745
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.960735]
objective value function right now is: -1462.6129218368444
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [23.789396]
objective value function right now is: -1450.5514502102249
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [18.323536]
objective value function right now is: -1461.2817091757968
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.262406]
objective value function right now is: -1463.4802208179356
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.8632]
objective value function right now is: -1462.8871119092335
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [17.03068]
objective value function right now is: -1461.1910599279868
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [18.958952]
objective value function right now is: -1464.1593163145046
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1466.1240449166796
Current xi:  [15.79081]
objective value function right now is: -1466.1240449166796
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [23.363571]
objective value function right now is: -1462.2365731624932
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.370625]
objective value function right now is: -1458.6942300256626
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.653738]
objective value function right now is: -1442.8758403991346
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.85917]
objective value function right now is: -1432.3458019207756
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.010546]
objective value function right now is: -1463.89770149495
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.62154]
objective value function right now is: -1429.5635003406562
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.376017]
objective value function right now is: -1463.9118533672342
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [21.790844]
objective value function right now is: -1461.1940778621051
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [20.843222]
objective value function right now is: -1464.0438986391982
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [21.269176]
objective value function right now is: -1465.7754399827818
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.643732]
objective value function right now is: -1446.3130124680988
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [18.026379]
objective value function right now is: -1461.5091714162315
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.520175]
objective value function right now is: -1411.6365707273876
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.818197]
objective value function right now is: -1465.7118121562987
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-21.876972]
objective value function right now is: -1433.0390412267284
new min fval from sgd:  -1466.2816460953065
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.05928]
objective value function right now is: -1458.666359966739
new min fval from sgd:  -1467.9480698819395
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.350964]
objective value function right now is: -1450.8059399516271
new min fval from sgd:  -1468.3590335528138
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.936491]
objective value function right now is: -1464.496328363598
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.291312]
objective value function right now is: -1464.1494475726722
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [21.982967]
objective value function right now is: -1464.7321008939464
min fval:  -1468.3590335528138
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -0.3422,   4.3449],
        [ -0.3445,   4.4722],
        [  2.6070,   8.4305],
        [ -5.2090,   7.3038],
        [ -5.1576,   7.2724],
        [ -5.1575,   7.2668],
        [ -5.0804,   6.9737],
        [  7.2252, -22.0604],
        [ 42.4685, -39.2190],
        [ -5.1878,   7.2996],
        [ -5.1316,   7.1941],
        [ -5.1921,   7.2916]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.0713e+01, -7.9552e+00,  4.6913e+00, -1.1340e+01, -1.2543e+01,
         -1.1548e+01, -1.1103e+01, -1.0398e+01, -2.8664e+01, -1.1326e+01,
         -1.2378e+01, -1.1880e+01],
        [ 9.6716e+00,  6.7462e+00, -7.9272e+00,  8.3661e+00,  9.7531e+00,
          9.7908e+00,  8.8054e+00,  9.1816e+00,  2.5895e+01,  9.2497e+00,
          9.1523e+00,  9.5672e+00],
        [-1.9060e-01, -3.3215e-01,  5.8078e+00,  2.2999e+00,  3.6913e-01,
          9.6223e-01,  9.8806e-01, -1.3311e+01, -3.0595e+01,  1.5974e+00,
          3.2638e-01,  8.7430e-01],
        [ 9.6104e+01,  9.3353e+01, -1.5114e+01,  7.9248e+01,  8.6540e+01,
          8.8371e+01,  8.9797e+01,  6.5279e+00,  1.8185e+01,  8.6494e+01,
          8.4985e+01,  8.3971e+01],
        [ 4.5961e+00,  3.3702e+00, -1.1290e+01,  2.5557e+00,  4.3276e+00,
          4.0336e+00,  2.9300e+00,  1.1222e+01,  2.3810e+01,  3.1483e+00,
          3.3684e+00,  3.7565e+00],
        [ 1.8360e+02,  1.7854e+02, -4.4715e+02, -7.4513e+01, -7.3625e+01,
         -7.3793e+01, -7.5044e+01, -4.9801e+00, -2.8258e+00, -7.3891e+01,
         -7.4304e+01, -7.4293e+01],
        [ 1.4544e+01,  1.1187e+01, -1.2767e+02,  6.9631e+01,  6.5269e+01,
          6.5464e+01,  6.1337e+01,  4.7681e+01,  4.8415e+01,  6.7676e+01,
          6.3341e+01,  6.8216e+01],
        [-1.3607e+02, -1.3716e+02,  1.9750e+01, -1.0903e+02, -1.1905e+02,
         -1.2229e+02, -1.2513e+02, -3.8741e+00, -2.4559e+01, -1.1897e+02,
         -1.1750e+02, -1.1484e+02],
        [-4.3718e+01, -3.6738e+01, -5.6077e+00, -4.3909e+01, -4.5693e+01,
         -4.6367e+01, -4.5990e+01, -6.4568e+00, -3.0200e+01, -4.6406e+01,
         -4.5454e+01, -4.5540e+01],
        [-1.3042e+02, -1.3081e+02,  2.0177e+01, -1.0427e+02, -1.1414e+02,
         -1.1735e+02, -1.2002e+02, -3.5011e+00, -2.5144e+01, -1.1426e+02,
         -1.1222e+02, -1.1014e+02],
        [ 1.2351e+02,  8.8149e+01, -7.5072e+01,  2.5736e+02,  2.4954e+02,
          2.4904e+02,  2.0437e+02, -7.2962e+00, -4.6935e+00,  2.5542e+02,
          2.3514e+02,  2.5458e+02],
        [ 8.9517e+00,  6.9644e+00, -2.1483e+01,  4.2118e+01,  4.0638e+01,
          4.1704e+01,  4.1459e+01,  1.8560e+01, -4.1092e+01,  4.1770e+01,
          4.1161e+01,  4.1678e+01]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 17.2346, -18.7092,  17.6644, -11.7556, -20.2749,  61.4431,  31.6357,
          52.5677,  21.6581,  44.9399,  39.7433,  11.3794]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -6.4826,   0.5066],
        [-37.6115,  14.1718],
        [  6.3467,  -0.4821],
        [  4.4613,   0.9693],
        [  4.3540,   0.7979],
        [  2.7094,   0.1339],
        [  4.9712,   0.5883],
        [  5.5740,   4.1049],
        [ 25.5794,   6.9639],
        [ 30.2987,   8.8833],
        [  4.4646,   0.9525],
        [  4.6841,   0.5578]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -17.0447, -223.2363, -161.1736, -183.1857, -170.0246, -159.3719,
         -188.0185, -291.9415, -144.2231,  -21.7894, -193.5953, -165.3637],
        [ -19.8654, -219.7758, -151.0318, -172.0353, -156.5930, -146.6436,
         -174.1807, -286.4320, -131.3817,  -22.3063, -180.2711, -151.6712],
        [ -14.1887, -226.6779, -172.1883, -194.5570, -185.0023, -173.3087,
         -201.3748, -292.9716, -159.1880,  -20.7245, -207.2219, -180.6787],
        [ -24.0825, -214.9676, -136.6862, -157.5131, -138.5268, -130.1826,
         -158.9088, -276.7522, -115.4388,  -22.6778, -164.7506, -134.1225],
        [  39.6398, -129.1591,  125.8234,  104.4463,  139.6574,  147.1107,
          100.8063,    3.2601,  147.7079,   -2.7438,   91.2758,  141.0600],
        [ -20.0882, -223.8512, -149.7657, -170.8635, -157.1057, -146.6910,
         -176.8106, -282.5223, -131.7309,  -23.3194, -182.1478, -152.4696],
        [ -28.8389, -207.7686, -125.4441, -145.2693, -126.8107, -115.5519,
         -146.0960, -259.7542, -101.9573,  -26.7232, -152.2472, -122.2442],
        [ -20.4788, -221.3629, -149.0091, -169.2230, -155.0291, -143.6754,
         -173.2323, -280.9750, -129.2272,  -23.1773, -178.9713, -150.6399],
        [  49.8749, -210.9428, -237.1266, -258.9489, -235.7800, -227.1376,
         -255.6147,  -18.5961, -217.6189,   -3.2754, -265.7347, -232.1825],
        [  88.9922,    1.0302, -140.0858, -160.5608,  -99.8482,  -65.9594,
         -161.4077, -803.1593,  -50.1925, -294.0198, -164.9350,  -94.6735],
        [  32.8073,  126.6159, -213.7051, -222.4867, -211.7916, -206.3608,
         -222.7355, -957.5829, -202.7527,  -17.1321, -227.1473, -210.2905],
        [  35.9539, -173.1362,  -53.6929,  -74.6749,  -43.5703,  -30.5943,
          -72.0268, -535.7640,  -30.3784,   -7.6395,  -76.7651,  -39.9250]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-12.2114,  -9.9347, -14.4276,  -8.6779,   0.4121, -10.0607,  -4.0086,
          -8.5871,  -8.5948,  45.0835,  62.7785, -18.2120],
        [ 12.1501,   9.9808,  14.4406,   8.8492,  -0.3041,  10.1005,   4.2442,
           8.7374,   8.5276, -45.0936, -62.4793,  17.9272]], device='cuda:0'))])
xi:  [20.148298]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 603.6553835006512
W_T_median: 262.731099153689
W_T_pctile_5: 18.527565433050736
W_T_CVAR_5_pct: -88.14928649880011
Average q (qsum/M+1):  51.63412377142137
Optimal xi:  [20.148298]
Expected(across Rb) median(across samples) p_equity:  0.3282337784767151
obj fun:  tensor(-1468.3590, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1347.5391329948106
Current xi:  [63.505524]
objective value function right now is: -1347.5391329948106
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [50.13502]
objective value function right now is: -1341.8298189661423
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [57.313656]
objective value function right now is: -1338.2188999105654
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1354.237185046216
Current xi:  [59.293186]
objective value function right now is: -1354.237185046216
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.96663]
objective value function right now is: -1340.2345845223138
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.18027]
objective value function right now is: -1348.95277297987
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1355.0320957242836
Current xi:  [65.78496]
objective value function right now is: -1355.0320957242836
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.480022]
objective value function right now is: -1353.8861916365208
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [59.26203]
objective value function right now is: -1323.4226207826675
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [46.07687]
objective value function right now is: -1329.6191257850123
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [58.135853]
objective value function right now is: -1344.8269003157789
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [40.708904]
objective value function right now is: -1219.5412241253434
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [59.608166]
objective value function right now is: -1343.4227621846856
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [54.249317]
objective value function right now is: -1350.6804895068515
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [61.644817]
objective value function right now is: -1354.7205429158162
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.39751]
objective value function right now is: -1350.1020393580716
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.292774]
objective value function right now is: -1331.8304791501248
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [56.71618]
objective value function right now is: -1339.081118591698
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [58.442104]
objective value function right now is: -1327.5639024231625
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.93646]
objective value function right now is: -1347.9678780879387
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [56.98154]
objective value function right now is: -1329.7703832304537
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [56.950577]
objective value function right now is: -1352.5239239254622
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [59.020187]
objective value function right now is: -1344.4989379989547
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [58.918354]
objective value function right now is: -1349.8714349044678
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [64.546844]
objective value function right now is: -1340.6117141772345
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.990486]
objective value function right now is: -1348.7361935742567
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [61.05523]
objective value function right now is: -1320.735393746194
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [60.433815]
objective value function right now is: -1307.3200839199046
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [59.29022]
objective value function right now is: -1160.8544957750175
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.042755]
objective value function right now is: -1353.549029943261
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [58.568256]
objective value function right now is: -1351.7131062406684
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [56.367413]
objective value function right now is: -1341.2081890283364
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.087147]
objective value function right now is: -1352.0835347658615
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [41.089954]
objective value function right now is: -1289.4810806235962
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [49.78061]
objective value function right now is: -1324.8688710816873
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [47.43334]
objective value function right now is: -1346.7935482519279
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [57.37032]
objective value function right now is: -1352.7518883977361
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [58.465317]
objective value function right now is: -1347.0213405851378
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.468075]
objective value function right now is: -1318.1768499092752
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [48.62679]
objective value function right now is: -1299.3141712854306
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.091854]
objective value function right now is: -1351.3209115566583
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [56.49082]
objective value function right now is: -1350.1186775575864
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [57.901806]
objective value function right now is: -1341.9836694687694
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [57.491886]
objective value function right now is: -1347.3039291392654
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.6073]
objective value function right now is: -1348.7934008320572
new min fval from sgd:  -1355.3958132691043
new min fval from sgd:  -1356.327424213006
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [57.651627]
objective value function right now is: -1344.986440449848
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.868362]
objective value function right now is: -1346.4367559616146
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [64.7276]
objective value function right now is: -1353.5342701656166
new min fval from sgd:  -1356.5646163686845
new min fval from sgd:  -1356.652661756096
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.57698]
objective value function right now is: -1332.4853138091396
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.355595]
objective value function right now is: -1337.8133107497592
min fval:  -1356.652661756096
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -2.3099,   4.8177],
        [ -2.3923,   4.9495],
        [  0.4745,  11.0375],
        [ -8.9387,   7.8343],
        [ -8.9538,   7.8509],
        [ -8.9472,   7.8418],
        [-24.1759,   4.5826],
        [ 33.2607, -17.4640],
        [ 51.3008, -30.8132],
        [ -8.9557,   7.8563],
        [ -8.8679,   7.7350],
        [ -8.9419,   7.8372]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.0713e+01, -7.9552e+00,  4.6913e+00, -1.1340e+01, -1.2543e+01,
         -1.1548e+01, -1.1103e+01, -1.0398e+01, -2.8664e+01, -1.1326e+01,
         -1.2378e+01, -1.1880e+01],
        [ 9.6716e+00,  6.7462e+00, -7.9272e+00,  8.3661e+00,  9.7531e+00,
          9.7908e+00,  8.8054e+00,  9.1816e+00,  2.5895e+01,  9.2497e+00,
          9.1523e+00,  9.5672e+00],
        [-2.3405e-01, -3.7521e-01,  5.7598e+00,  2.2946e+00,  3.6377e-01,
          9.5687e-01,  9.8310e-01, -1.3312e+01, -3.0644e+01,  1.5921e+00,
          3.2101e-01,  8.6897e-01],
        [ 9.6104e+01,  9.3353e+01, -1.5114e+01,  7.9248e+01,  8.6540e+01,
          8.8371e+01,  8.9797e+01,  6.5279e+00,  1.8185e+01,  8.6494e+01,
          8.4985e+01,  8.3971e+01],
        [ 4.5961e+00,  3.3702e+00, -1.1290e+01,  2.5557e+00,  4.3276e+00,
          4.0336e+00,  2.9300e+00,  1.1222e+01,  2.3810e+01,  3.1483e+00,
          3.3684e+00,  3.7565e+00],
        [ 2.0380e+02,  2.0148e+02, -5.0739e+02, -5.4205e+01, -5.2464e+01,
         -5.2642e+01, -1.6482e+02, -3.8295e+00, -1.6767e+00, -5.3251e+01,
         -5.2705e+01, -5.3695e+01],
        [-3.8599e+00, -5.5320e+00, -1.2061e+02,  1.2558e+02,  1.2092e+02,
          1.2153e+02,  5.4729e+02,  5.5956e+01,  5.8050e+01,  1.2278e+02,
          1.2440e+02,  1.2419e+02],
        [-1.3607e+02, -1.3716e+02,  1.9750e+01, -1.0903e+02, -1.1905e+02,
         -1.2229e+02, -1.2513e+02, -3.8611e+00, -2.4546e+01, -1.1897e+02,
         -1.1750e+02, -1.1484e+02],
        [-4.3718e+01, -3.6738e+01, -5.6077e+00, -4.3909e+01, -4.5693e+01,
         -4.6367e+01, -4.5990e+01, -6.4568e+00, -3.0200e+01, -4.6406e+01,
         -4.5454e+01, -4.5540e+01],
        [-1.3042e+02, -1.3081e+02,  2.0177e+01, -1.0427e+02, -1.1414e+02,
         -1.1735e+02, -1.2002e+02, -3.4913e+00, -2.5136e+01, -1.1426e+02,
         -1.1222e+02, -1.1014e+02],
        [ 1.6503e+02,  1.3315e+02, -6.1692e+01,  2.5594e+02,  2.4912e+02,
          2.4832e+02,  6.8737e+01, -6.1750e+00, -3.5752e+00,  2.5490e+02,
          2.3128e+02,  2.5341e+02],
        [ 8.2856e+00,  6.5914e+00, -1.8756e+01,  4.4188e+01,  4.2695e+01,
          4.3759e+01,  4.1584e+01,  1.7670e+01, -4.0834e+01,  4.3837e+01,
          4.3192e+01,  4.3743e+01]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 17.2346, -19.9203,  17.6616, -12.9667, -21.4860,  80.4622,  33.8101,
          52.5677,  21.6581,  44.9399,  41.1845,   8.7168]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -6.4826,   0.5066],
        [-37.6115,  14.1718],
        [  6.3467,  -0.4821],
        [  4.4613,   0.9693],
        [  4.3540,   0.7979],
        [  2.7094,   0.1339],
        [  4.9712,   0.5883],
        [  5.5740,   4.1049],
        [ 25.5794,   6.9639],
        [ 30.2987,   8.8833],
        [  4.4646,   0.9525],
        [  4.6841,   0.5578]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -17.0447, -223.2363, -161.1736, -183.1857, -170.0246, -159.3719,
         -188.0185, -291.9415, -144.2231,  -21.7894, -193.5953, -165.3637],
        [ -19.8654, -219.7758, -151.0318, -172.0353, -156.5930, -146.6436,
         -174.1807, -286.4320, -131.3817,  -22.3063, -180.2711, -151.6712],
        [ -14.1887, -226.6779, -172.1883, -194.5570, -185.0023, -173.3087,
         -201.3748, -292.9716, -159.1880,  -20.7245, -207.2219, -180.6787],
        [ -24.0825, -214.9676, -136.6862, -157.5131, -138.5268, -130.1826,
         -158.9088, -276.7522, -115.4388,  -22.6778, -164.7506, -134.1225],
        [  39.6398, -129.1591,  125.8234,  104.4463,  139.6574,  147.1107,
          100.8063,    3.2601,  147.7079,   -2.7438,   91.2758,  141.0600],
        [ -20.0882, -223.8512, -149.7657, -170.8635, -157.1057, -146.6910,
         -176.8106, -282.5223, -131.7309,  -23.3194, -182.1478, -152.4696],
        [ -28.8389, -207.7686, -125.4441, -145.2693, -126.8107, -115.5519,
         -146.0960, -259.7542, -101.9573,  -26.7232, -152.2472, -122.2442],
        [ -20.4788, -221.3629, -149.0091, -169.2230, -155.0291, -143.6754,
         -173.2323, -280.9750, -129.2272,  -23.1773, -178.9713, -150.6399],
        [  49.8749, -210.9428, -237.1266, -258.9489, -235.7800, -227.1376,
         -255.6147,  -18.5961, -217.6189,   -3.2754, -265.7347, -232.1825],
        [  88.9922,    1.0302, -140.0858, -160.5608,  -99.8482,  -65.9594,
         -161.4077, -803.1593,  -50.1925, -294.0198, -164.9350,  -94.6735],
        [  32.8073,  126.6159, -213.7051, -222.4867, -211.7916, -206.3608,
         -222.7355, -957.5829, -202.7527,  -17.1321, -227.1473, -210.2905],
        [  35.9539, -173.1362,  -53.6929,  -74.6749,  -43.5703,  -30.5943,
          -72.0268, -535.7640,  -30.3784,   -7.6395,  -76.7651,  -39.9250]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-12.2114,  -9.9347, -14.4276,  -8.6779,   0.4614, -10.0607,  -4.0086,
          -8.5871,  -8.5948,  45.0835,  62.7785, -18.2120],
        [ 12.1501,   9.9808,  14.4406,   8.8492,  -0.3536,  10.1005,   4.2442,
           8.7374,   8.5276, -45.0936, -62.4793,  17.9272]], device='cuda:0'))])
xi:  [60.647118]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 625.8338533181166
W_T_median: 328.64798407622123
W_T_pctile_5: 59.22297711792259
W_T_CVAR_5_pct: -69.69693362846321
Average q (qsum/M+1):  50.509615990423384
Optimal xi:  [60.647118]
Expected(across Rb) median(across samples) p_equity:  0.30681490898132324
obj fun:  tensor(-1356.6527, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1197.2545607658453
Current xi:  [74.397865]
objective value function right now is: -1197.2545607658453
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.38085]
objective value function right now is: -1177.2210919630415
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1204.1115137504662
Current xi:  [73.12878]
objective value function right now is: -1204.1115137504662
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1210.3826555265416
Current xi:  [71.02037]
objective value function right now is: -1210.3826555265416
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1221.6531672438095
Current xi:  [69.59087]
objective value function right now is: -1221.6531672438095
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [72.34518]
objective value function right now is: -1193.1489005817384
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [74.93969]
objective value function right now is: -1210.8403126755077
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [72.30888]
objective value function right now is: -1206.9290446615958
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [73.16838]
objective value function right now is: -1208.8694510562138
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [78.04973]
objective value function right now is: -1206.5566598544517
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [75.9111]
objective value function right now is: -1177.4136045727157
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [77.7458]
objective value function right now is: -1216.0744459450527
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.255615]
objective value function right now is: -1208.92878235359
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [75.52592]
objective value function right now is: -1212.4848538949534
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [81.11757]
objective value function right now is: -1209.6400230520055
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.92407]
objective value function right now is: -1177.2341685149784
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1222.4237764459742
Current xi:  [74.20002]
objective value function right now is: -1222.4237764459742
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [76.08058]
objective value function right now is: -1220.762130516517
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [67.78815]
objective value function right now is: -1172.8723994687286
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.9753]
objective value function right now is: -1222.3488273192174
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [73.7117]
objective value function right now is: -1202.3758386236068
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.65497]
objective value function right now is: -1199.5900900726929
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [77.8025]
objective value function right now is: -1205.6134185740168
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.521515]
objective value function right now is: -1210.8509740937836
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [82.66861]
objective value function right now is: -1214.3602624787666
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [79.89372]
objective value function right now is: -1214.5177119825055
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [79.03768]
objective value function right now is: -1204.2560235074131
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [77.80252]
objective value function right now is: -1220.665030167974
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [76.89048]
objective value function right now is: -1222.3798421615786
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [76.63578]
objective value function right now is: -1216.2452561254586
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [72.671196]
objective value function right now is: -1220.0829235435842
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [78.368225]
objective value function right now is: -1200.521425021843
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [77.670616]
objective value function right now is: -1221.4808820724027
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.86299]
objective value function right now is: -1222.0621172754466
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -1225.5341365961367
Current xi:  [73.69638]
objective value function right now is: -1225.5341365961367
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [73.54643]
objective value function right now is: -1190.5341477095494
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [80.896164]
objective value function right now is: -1217.8734380576466
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.79234]
objective value function right now is: -1223.3834334149888
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.47138]
objective value function right now is: -1205.6197033020906
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [74.53417]
objective value function right now is: -1197.7840881889933
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.698494]
objective value function right now is: -1197.173684456548
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [75.96777]
objective value function right now is: -1222.487561490727
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [72.81508]
objective value function right now is: -1217.543698811266
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [73.247154]
objective value function right now is: -1213.4160920431889
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [73.114006]
objective value function right now is: -1212.9469732537457
new min fval from sgd:  -1226.2497040982655
new min fval from sgd:  -1226.7803636671529
new min fval from sgd:  -1226.9000864799784
new min fval from sgd:  -1227.1800199102636
new min fval from sgd:  -1228.177489734566
new min fval from sgd:  -1228.185369590546
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [77.03641]
objective value function right now is: -1215.0948364997596
new min fval from sgd:  -1229.3126874451414
new min fval from sgd:  -1229.886995036104
new min fval from sgd:  -1230.1809019379746
new min fval from sgd:  -1231.0983157407607
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [73.739685]
objective value function right now is: -1199.4839733549925
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [75.55008]
objective value function right now is: -1220.0751639137693
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [71.34407]
objective value function right now is: -1171.7987182921436
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [72.181694]
objective value function right now is: -1217.542552487515
min fval:  -1231.0983157407607
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -4.0279,   4.5580],
        [ -4.0733,   4.5576],
        [ -3.0932,  10.2119],
        [-11.9161,   8.2473],
        [-11.8248,   8.1995],
        [-11.8213,   8.1943],
        [-12.9500,   2.6448],
        [ 18.3003, -18.4396],
        [ 28.7137, -11.6309],
        [-11.8860,   8.2369],
        [-11.6770,   8.0793],
        [-11.8856,   8.2304]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.0712e+01, -7.9547e+00,  4.6919e+00, -1.1340e+01, -1.2543e+01,
         -1.1548e+01, -1.1103e+01, -1.0398e+01, -2.8664e+01, -1.1326e+01,
         -1.2378e+01, -1.1880e+01],
        [ 9.6716e+00,  6.7462e+00, -7.9272e+00,  8.3661e+00,  9.7531e+00,
          9.7908e+00,  8.8054e+00,  9.1816e+00,  2.5895e+01,  9.2497e+00,
          9.1523e+00,  9.5672e+00],
        [ 5.4192e-01,  4.3387e-01,  5.8005e+00,  6.9767e+00,  5.0324e+00,
          5.6245e+00,  7.7582e+00, -1.1542e+01, -3.0821e+01,  6.2709e+00,
          4.9610e+00,  5.5449e+00],
        [ 9.6104e+01,  9.3353e+01, -1.5114e+01,  7.9248e+01,  8.6540e+01,
          8.8371e+01,  8.9797e+01,  6.5279e+00,  1.8185e+01,  8.6494e+01,
          8.4985e+01,  8.3971e+01],
        [ 4.9649e+00,  3.7155e+00, -1.0836e+01,  7.8450e-02,  1.8682e+00,
          1.5746e+00, -4.8850e+00,  1.1343e+01,  2.4633e+01,  6.8025e-01,
          9.1857e-01,  1.2850e+00],
        [ 2.4066e+02,  2.3932e+02, -5.9214e+02,  1.1586e+02,  1.1703e+02,
          1.1667e+02, -1.2231e+02, -2.3885e+00, -2.5556e-01,  1.1684e+02,
          1.1393e+02,  1.1615e+02],
        [ 2.3326e-01,  1.1672e+00, -1.1966e+02,  1.8001e+02,  1.7498e+02,
          1.7585e+02,  1.6870e+03,  5.1154e+01,  5.6640e+01,  1.7661e+02,
          1.8161e+02,  1.7857e+02],
        [-1.3607e+02, -1.3716e+02,  1.9750e+01, -1.0903e+02, -1.1905e+02,
         -1.2229e+02, -1.2513e+02, -3.7779e+00, -2.4463e+01, -1.1897e+02,
         -1.1750e+02, -1.1484e+02],
        [-4.3718e+01, -3.6738e+01, -5.6077e+00, -4.3909e+01, -4.5693e+01,
         -4.6367e+01, -4.5990e+01, -6.4568e+00, -3.0200e+01, -4.6406e+01,
         -4.5454e+01, -4.5540e+01],
        [-1.3042e+02, -1.3081e+02,  2.0177e+01, -1.0427e+02, -1.1414e+02,
         -1.1735e+02, -1.2002e+02, -3.4298e+00, -2.5074e+01, -1.1426e+02,
         -1.1222e+02, -1.1014e+02],
        [ 1.8397e+02,  1.5607e+02, -5.9839e+01,  2.3606e+02,  2.2913e+02,
          2.2815e+02, -2.7241e+01, -4.9648e+00, -2.3942e+00,  2.3525e+02,
          2.0893e+02,  2.3344e+02],
        [ 3.3006e+01,  3.3311e+01, -6.1157e+00,  2.2377e+02,  2.2138e+02,
          2.2257e+02,  3.0257e+02,  2.1520e+01, -3.7124e+01,  2.2283e+02,
          2.2263e+02,  2.2304e+02]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 17.2346, -31.8683,  19.6157, -24.9149, -33.4134,  64.2356,  37.0587,
          52.5677,  21.6581,  44.9399,  63.5795,  22.1026]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -6.4826,   0.5066],
        [-37.6115,  14.1718],
        [  6.3467,  -0.4821],
        [  4.4613,   0.9693],
        [  4.3540,   0.7979],
        [  2.7094,   0.1339],
        [  4.9712,   0.5883],
        [  5.5740,   4.1049],
        [ 25.5794,   6.9639],
        [ 30.2987,   8.8833],
        [  4.4646,   0.9525],
        [  4.6841,   0.5578]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -17.0447, -223.2363, -161.1736, -183.1857, -170.0246, -159.3719,
         -188.0185, -291.9415, -144.2231,  -21.7894, -193.5953, -165.3637],
        [ -19.8654, -219.7758, -151.0318, -172.0353, -156.5930, -146.6436,
         -174.1807, -286.4320, -131.3817,  -22.3063, -180.2711, -151.6712],
        [ -14.1887, -226.6779, -172.1883, -194.5570, -185.0023, -173.3087,
         -201.3748, -292.9716, -159.1880,  -20.7245, -207.2219, -180.6787],
        [ -24.0825, -214.9676, -136.6862, -157.5131, -138.5268, -130.1826,
         -158.9088, -276.7522, -115.4388,  -22.6778, -164.7506, -134.1225],
        [  39.6398, -129.1591,  125.8234,  104.4463,  139.6574,  147.1107,
          100.8063,    3.2601,  147.7079,   -2.7438,   91.2758,  141.0600],
        [ -20.0882, -223.8512, -149.7657, -170.8635, -157.1057, -146.6910,
         -176.8106, -282.5223, -131.7309,  -23.3194, -182.1478, -152.4696],
        [ -28.8389, -207.7686, -125.4441, -145.2693, -126.8107, -115.5519,
         -146.0960, -259.7542, -101.9573,  -26.7232, -152.2472, -122.2442],
        [ -20.4788, -221.3629, -149.0091, -169.2230, -155.0291, -143.6754,
         -173.2323, -280.9750, -129.2272,  -23.1773, -178.9713, -150.6399],
        [  49.8749, -210.9428, -237.1266, -258.9489, -235.7800, -227.1376,
         -255.6147,  -18.5961, -217.6189,   -3.2754, -265.7347, -232.1825],
        [  88.9922,    1.0302, -140.0858, -160.5608,  -99.8482,  -65.9594,
         -161.4077, -803.1593,  -50.1925, -294.0198, -164.9350,  -94.6735],
        [  32.8073,  126.6159, -213.7051, -222.4867, -211.7916, -206.3608,
         -222.7355, -957.5829, -202.7527,  -17.1321, -227.1473, -210.2905],
        [  35.9539, -173.1362,  -53.6929,  -74.6749,  -43.5703,  -30.5943,
          -72.0268, -535.7640,  -30.3784,   -7.6395,  -76.7651,  -39.9250]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-12.2114,  -9.9347, -14.4276,  -8.6779,   0.4666, -10.0607,  -4.0086,
          -8.5871,  -8.5948,  45.0835,  62.7785, -18.2120],
        [ 12.1501,   9.9808,  14.4406,   8.8492,  -0.3592,  10.1005,   4.2442,
           8.7374,   8.5276, -45.0936, -62.4793,  17.9272]], device='cuda:0'))])
xi:  [79.93893]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 668.5565449113378
W_T_median: 381.7633575509386
W_T_pctile_5: 77.52680529312613
W_T_CVAR_5_pct: -63.2900949552374
Average q (qsum/M+1):  49.926175025201616
Optimal xi:  [79.93893]
Expected(across Rb) median(across samples) p_equity:  0.3045326769351959
obj fun:  tensor(-1231.0983, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  1522.2994388418351
Current xi:  [99.419876]
objective value function right now is: 1522.2994388418351
4.0% of gradient descent iterations done. Method = Adam
new min fval:  1469.7250128304665
Current xi:  [92.060715]
objective value function right now is: 1469.7250128304665
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [98.74744]
objective value function right now is: 1474.1353420002229
8.0% of gradient descent iterations done. Method = Adam
new min fval:  1463.3383212276383
Current xi:  [98.967896]
objective value function right now is: 1463.3383212276383
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [89.43236]
objective value function right now is: 1512.8025447221655
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [100.17332]
objective value function right now is: 1487.7228586226727
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [97.61772]
objective value function right now is: 1696.2578838859488
16.0% of gradient descent iterations done. Method = Adam
new min fval:  1462.0780240468118
Current xi:  [94.14284]
objective value function right now is: 1462.0780240468118
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [89.22523]
objective value function right now is: 1504.1669017764425
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [94.33101]
objective value function right now is: 1468.3931957221425
