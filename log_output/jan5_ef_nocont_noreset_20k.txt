Starting at: 
05-01-23_15:02

 Random seed:  2  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1  logistic_sigmoid   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -232.03602154289734
Current xi:  [15.904764]
objective value function right now is: -232.03602154289734
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -757.2282417547918
Current xi:  [18.224245]
objective value function right now is: -757.2282417547918
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -884.6491664177698
Current xi:  [18.345346]
objective value function right now is: -884.6491664177698
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.606514]
objective value function right now is: -489.85036897032035
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1253.8111600957257
Current xi:  [19.40763]
objective value function right now is: -1253.8111600957257
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1283.410595971507
Current xi:  [19.191212]
objective value function right now is: -1283.410595971507
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1399.2921233774903
Current xi:  [18.735971]
objective value function right now is: -1399.2921233774903
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1446.1270018725138
Current xi:  [19.462332]
objective value function right now is: -1446.1270018725138
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.718037]
objective value function right now is: -1406.6198086830475
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [18.36421]
objective value function right now is: -1059.3567737864767
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1534.8708762630101
Current xi:  [19.695047]
objective value function right now is: -1534.8708762630101
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.209242]
objective value function right now is: -1209.4026982867927
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [18.825298]
objective value function right now is: -1238.9452120042558
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [18.868969]
objective value function right now is: -1365.711988854994
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1554.5184332247463
Current xi:  [19.050081]
objective value function right now is: -1554.5184332247463
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.392723]
objective value function right now is: -1361.228617330208
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.153236]
objective value function right now is: -1510.0363355337904
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1718.3237142899125
Current xi:  [19.583317]
objective value function right now is: -1718.3237142899125
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [18.450161]
objective value function right now is: -983.7151553283716
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.926657]
objective value function right now is: -1573.7198571053316
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.934591]
objective value function right now is: -1018.798232383358
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.800097]
objective value function right now is: -1534.513866270056
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.74377]
objective value function right now is: -1269.9418814610815
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.74325]
objective value function right now is: -1582.0847234511114
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.313637]
objective value function right now is: -1337.0889855338994
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.211937]
objective value function right now is: -1647.8312102615982
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1721.623703315212
Current xi:  [18.625828]
objective value function right now is: -1721.623703315212
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [19.19662]
objective value function right now is: -1590.1588299679865
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [20.085173]
objective value function right now is: -1323.4821752980674
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.91276]
objective value function right now is: -1589.801013468279
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [20.181223]
objective value function right now is: -1649.2741435051314
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.496767]
objective value function right now is: -1715.7726868923971
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.823967]
objective value function right now is: -1709.4695789534815
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.25825]
objective value function right now is: -1646.3783491732622
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [20.036238]
objective value function right now is: -1354.255076807196
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [20.034206]
objective value function right now is: -1701.5128184048774
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.13438]
objective value function right now is: -1296.5430193513105
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [18.894697]
objective value function right now is: -1587.8292756318526
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [18.910387]
objective value function right now is: -1368.8534863268562
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [20.169529]
objective value function right now is: -1453.5083166660495
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.715042]
objective value function right now is: -1378.6438385158508
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.11797]
objective value function right now is: -1604.0116666711701
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [20.212175]
objective value function right now is: -1688.0512755987857
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.35338]
objective value function right now is: -1638.5021518634185
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.136587]
objective value function right now is: -1482.0067085697399
new min fval from sgd:  -1731.4718458450031
new min fval from sgd:  -1743.0735316168355
new min fval from sgd:  -1747.640632781385
new min fval from sgd:  -1771.9277279730945
new min fval from sgd:  -1793.021810295965
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.579233]
objective value function right now is: -1744.0546432799363
new min fval from sgd:  -1811.4012360501638
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [18.928493]
objective value function right now is: -1775.629466384747
new min fval from sgd:  -1817.8911632034303
new min fval from sgd:  -1818.5849729810013
new min fval from sgd:  -1819.157405377323
new min fval from sgd:  -1820.4187639507109
new min fval from sgd:  -1822.6747411075546
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.786318]
objective value function right now is: -1765.8378894642524
new min fval from sgd:  -1841.4632174053077
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.220564]
objective value function right now is: -1612.330182812586
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [20.641418]
objective value function right now is: -1756.848715733619
min fval:  -1841.4632174053077
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 5.3407, -1.4005],
        [ 4.9799, -1.3552],
        [-6.2534,  1.6216],
        [ 5.2635, -1.3793],
        [10.8619, -6.9089],
        [ 5.2895, -1.3926],
        [ 5.2485, -1.3787],
        [ 5.1273, -1.3664],
        [11.5184,  4.5019],
        [17.5097, -9.8650]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[  5.6313,   3.6934, -53.2102,   5.8087,  -4.0760,   5.2688,   5.3793,
           4.5977, -46.1673,  -4.8023],
        [  2.3525,   4.0816,   2.8768,   2.8404,  17.2886,   2.3773,   3.2524,
           3.6658,   5.4994,  16.1966],
        [  6.8434,   5.3948, -75.7623,   6.0030,  -9.3626,   6.2581,   6.0870,
           5.7150, -14.1284,  -9.1689],
        [  6.1847,   4.4409, -65.7111,   5.7326,  -7.9236,   6.2349,   5.8405,
           5.3421, -19.2534,  -8.6211],
        [  3.2228,   1.5547, -28.5710,   3.2612,  -1.6310,   2.3370,   3.0768,
           2.1954, -74.6013,  -1.9317],
        [  3.0033,   2.1021, -27.6204,   3.0301,  -1.8215,   2.9141,   2.8228,
           2.4486, -74.3168,  -2.7416],
        [  3.6012,   2.2151, -29.4574,   3.2132,  -1.4405,   3.2140,   2.9369,
           2.2653, -71.9701,  -2.1519],
        [  6.6312,   5.0246, -86.9782,   6.0300, -10.1044,   6.7340,   5.9899,
           5.5607, -10.8511,  -9.7053],
        [  2.2815,   0.4020, -40.7696,   1.9304,  -5.6672,   2.0023,   1.2422,
           0.8458, -39.2145,  -6.4220],
        [  3.4565,   2.2741, -29.5149,   3.4723,  -1.1863,   3.6120,   3.3171,
           2.7372, -71.4173,  -1.8861]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -6.8568,  25.1748, -15.5241, -10.2848, -15.0080, -11.6993, -16.0824,
         -22.8125,  -5.9130, -16.7898]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.2581,   3.8458],
        [  7.1906,   2.3774],
        [ -6.7434,  -0.9445],
        [ -3.3078,   3.8533],
        [-24.3811,  -2.2233],
        [ -3.2541,   3.8502],
        [ -8.4353,  -1.7739],
        [ -3.3200,   3.8555],
        [ -6.3705,  -0.7688],
        [ -3.3143,   3.8546]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-17.3778, -17.8336,  -9.2045, -17.8315, -36.2324, -20.4473, -15.8381,
         -18.3732,  -8.0215, -18.1791],
        [-23.5658, -17.7950, -10.4507, -24.0126, -33.9471, -27.1380, -15.9173,
         -24.5703,  -9.8284, -24.2880],
        [-60.3922,  -3.6955,   2.1778, -63.1662,   5.8045, -61.6835,   4.0852,
         -63.8132,   1.9640, -63.0042],
        [-66.7790,  -4.1963, -13.8894, -67.3622, -94.3754, -69.6384, -16.0176,
         -67.5966, -14.1057, -66.8848],
        [-64.1992,  -8.6892, -13.3717, -64.6013, -63.0467, -66.9594, -13.6188,
         -65.0829, -14.6133, -64.3795],
        [-57.7178, -11.1708, -13.5836, -58.1516, -46.0842, -61.2088, -13.2481,
         -59.2475, -14.5777, -58.4685],
        [ -9.8263,  -8.4955,  -2.4773, -10.4347, -55.5051, -13.8308, -13.5285,
         -11.9352,  -0.3317, -10.6023],
        [ -9.8567, -12.4905,  -3.9694, -10.1682, -47.6299, -13.1114, -15.5063,
         -11.1093,  -2.1936, -10.8617],
        [ 84.6735,  25.6633,  -9.9070,  82.1787,   9.1278,  73.1095, -14.3550,
          80.1501,  -9.2846,  81.2000],
        [ -9.4881, -13.6278,  -4.8100, -10.3878, -44.9458, -13.1652, -15.9823,
         -10.8996,  -2.4952, -10.3523]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  4.4177,   5.6054, -14.3935,  16.8966,  13.8339,  12.2760,  -4.5339,
          -1.4348,   0.6249,  -0.6765],
        [ -4.5813,  -5.7340,  14.3669, -16.4546, -13.6000, -12.1488,   4.3529,
           1.2990,  -0.4761,   0.4112]], device='cuda:0'))])
xi:  [20.089155]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 784.4401454753146
W_T_median: 533.4981888952871
W_T_pctile_5: 199.81998088765584
W_T_CVAR_5_pct: 8.566197379290156
Average q (qsum/M+1):  45.59342316658266
Optimal xi:  [20.089155]
Expected(across Rb) median(across samples) p_equity:  0.24956209386388462
obj fun:  tensor(-1841.4632, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1476.349943383428
Current xi:  [16.254278]
objective value function right now is: -1476.349943383428
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.432705]
objective value function right now is: -1426.6225444543522
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.14606]
objective value function right now is: -1433.0822387503154
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.223207]
objective value function right now is: -1458.0571496269426
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.721058]
objective value function right now is: -1446.0141395620728
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.345182]
objective value function right now is: -1472.2590142454087
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [17.374517]
objective value function right now is: -1465.856140114483
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1477.307698049435
Current xi:  [17.350433]
objective value function right now is: -1477.307698049435
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.185165]
objective value function right now is: -1463.9567921162081
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.986856]
objective value function right now is: -1408.209604506293
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1493.6780366422595
Current xi:  [17.165684]
objective value function right now is: -1493.6780366422595
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1497.615795005311
Current xi:  [16.725061]
objective value function right now is: -1497.615795005311
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.990911]
objective value function right now is: -1450.3198260073896
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [16.740606]
objective value function right now is: -1460.0609594139507
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [18.202133]
objective value function right now is: -1483.6516200567771
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.186039]
objective value function right now is: -1406.8139603002776
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.533371]
objective value function right now is: -1462.7475953313362
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [18.391418]
objective value function right now is: -1457.9026066234742
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.911057]
objective value function right now is: -1488.7258885570793
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.947573]
objective value function right now is: -1431.515344078155
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.891555]
objective value function right now is: -1417.913224579027
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.160658]
objective value function right now is: -1473.8628566334314
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.221193]
objective value function right now is: -1464.7130548889227
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.115623]
objective value function right now is: -1392.6904307251161
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.820005]
objective value function right now is: -1475.6221613764978
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.35567]
objective value function right now is: -1486.2602427180004
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.963919]
objective value function right now is: -1479.1420009405078
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [17.235678]
objective value function right now is: -1470.8779513165334
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [17.15367]
objective value function right now is: -1470.6752714006743
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.698097]
objective value function right now is: -1378.7891888690938
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.302065]
objective value function right now is: -1489.7653540396834
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.210176]
objective value function right now is: -1449.3859076606498
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.518736]
objective value function right now is: -1487.7267449997955
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.227966]
objective value function right now is: -1490.886145076558
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.37962]
objective value function right now is: -1493.8508366381443
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.355392]
objective value function right now is: -1450.2878682391126
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.60155]
objective value function right now is: -1435.886320689147
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.98922]
objective value function right now is: -1482.3532559784874
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.840275]
objective value function right now is: -1436.4745737320072
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.725752]
objective value function right now is: -1468.237333919537
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.61874]
objective value function right now is: -1475.3347183538044
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.630205]
objective value function right now is: -1468.7928806159814
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.425863]
objective value function right now is: -1481.7579404041014
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.403044]
objective value function right now is: -1474.8087488969443
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.074476]
objective value function right now is: -1477.7674901665239
new min fval from sgd:  -1498.3458648130136
new min fval from sgd:  -1499.4788211937926
new min fval from sgd:  -1499.5556680011355
new min fval from sgd:  -1500.826105246714
new min fval from sgd:  -1501.2392523235806
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [18.479607]
objective value function right now is: -1484.5348757121653
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.654764]
objective value function right now is: -1474.6430823580088
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.10655]
objective value function right now is: -1455.4892684786535
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [17.251295]
objective value function right now is: -1490.6360085635145
new min fval from sgd:  -1501.5920840179906
new min fval from sgd:  -1501.726430592874
new min fval from sgd:  -1501.9318215269725
new min fval from sgd:  -1502.0155345703868
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.163097]
objective value function right now is: -1467.387434921713
min fval:  -1502.0155345703868
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  3.9204,  -2.1747],
        [  3.8091,  -2.0865],
        [ -4.3417,   2.5001],
        [  3.8987,  -2.1746],
        [  3.8443,  -7.5750],
        [  3.9153,  -2.1841],
        [  3.8912,  -2.1619],
        [  3.8538,  -2.1295],
        [ 11.2683,   2.5530],
        [ 17.1010, -19.2349]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 6.7306e+00,  4.7544e+00, -7.4896e+01,  6.8189e+00, -5.0391e+00,
          6.4272e+00,  6.4175e+00,  5.6640e+00, -6.6507e+01, -6.1644e+00],
        [ 2.3525e+00,  4.0816e+00,  2.8768e+00,  2.8404e+00,  1.7289e+01,
          2.3773e+00,  3.2524e+00,  3.6658e+00,  5.4994e+00,  1.6197e+01],
        [ 8.3688e+00,  6.8491e+00, -1.0399e+02,  7.5272e+00, -1.1233e+01,
          7.8045e+00,  7.6031e+00,  7.2070e+00, -1.8770e+01, -1.1185e+01],
        [ 3.8011e+00,  1.9454e+00, -7.5749e+01,  3.3234e+00, -1.0511e+01,
          3.8528e+00,  3.4263e+00,  2.8884e+00, -3.5880e+01, -1.1378e+01],
        [ 1.4697e+00, -1.1049e-01, -4.1051e+01,  1.4292e+00, -4.4656e+00,
          6.4483e-01,  1.2843e+00,  4.8898e-01, -9.4977e+01, -4.9999e+00],
        [ 1.3071e+00,  4.9330e-01, -3.9855e+01,  1.2555e+00, -4.5855e+00,
          1.2796e+00,  1.0876e+00,  7.9897e-01, -9.4490e+01, -5.7394e+00],
        [ 1.6316e+00,  3.2004e-01, -4.2176e+01,  1.1663e+00, -4.5037e+00,
          1.3049e+00,  9.2721e-01,  3.3509e-01, -9.2749e+01, -5.4541e+00],
        [ 8.9765e+00,  7.2559e+00, -1.4368e+02,  8.4173e+00, -1.3121e+01,
          9.0157e+00,  8.3527e+00,  7.8463e+00, -1.4137e+01, -1.3441e+01],
        [ 2.0144e+00,  1.2448e-01, -4.5986e+01,  1.6070e+00, -7.0139e+00,
          1.7763e+00,  9.3718e-01,  5.6856e-01, -5.0713e+01, -8.0186e+00],
        [ 4.5139e+00,  3.3066e+00, -3.7878e+01,  4.4575e+00, -4.4771e-01,
          4.7187e+00,  4.3215e+00,  3.7801e+00, -1.0394e+02, -1.4356e+00]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -8.8337,  27.3717, -23.2310,  -6.5357, -13.5304, -10.8526, -13.7451,
         -25.8912,  -7.3446, -14.8167]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.3893,   4.5730],
        [  8.6113,   3.4052],
        [ -6.4185,   0.1984],
        [ -2.4100,   4.5760],
        [-40.5167,  -2.6505],
        [ -2.4325,   4.5698],
        [-11.6873,  -2.3487],
        [ -2.4243,   4.5761],
        [ -5.3748,   0.6667],
        [ -2.4130,   4.5763]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.7506e+01, -1.4701e+01, -1.2549e+01, -4.8330e+01, -2.2700e+01,
         -5.1523e+01, -1.9969e+01, -4.9198e+01, -6.3414e+00, -4.8724e+01],
        [-4.1516e+01, -1.6983e+01, -1.4809e+01, -4.2283e+01, -3.3724e+01,
         -4.5856e+01, -2.2662e+01, -4.3105e+01, -9.7824e+00, -4.2598e+01],
        [-9.1872e+01, -3.8624e+00,  6.8474e-01, -9.5233e+01,  3.0630e+00,
         -9.3523e+01,  6.8889e+00, -9.6105e+01, -3.5735e-01, -9.5134e+01],
        [-1.2835e+02, -4.1826e+00, -6.4127e+01, -1.3056e+02, -3.1112e+02,
         -1.3372e+02, -5.4448e+01, -1.3185e+02, -6.2943e+01, -1.3027e+02],
        [-1.6165e+02, -5.7357e+00, -6.2126e+01, -1.6373e+02, -2.3561e+02,
         -1.6698e+02, -4.1980e+01, -1.6528e+02, -7.1709e+01, -1.6370e+02],
        [-1.8365e+02, -1.3070e+01, -5.4560e+01, -1.8536e+02, -1.5077e+02,
         -1.8875e+02, -3.7583e+01, -1.8715e+02, -6.9679e+01, -1.8581e+02],
        [ 4.4235e-01, -6.3148e+00, -4.2162e+00, -9.9036e-01, -1.1931e+02,
         -4.5873e+00, -1.8279e+01, -2.9256e+00, -3.4152e+00, -1.2501e+00],
        [-2.1249e+01, -1.3808e+01, -1.4282e+01, -2.2199e+01, -4.2889e+01,
         -2.5707e+01, -2.3615e+01, -2.3596e+01, -7.8323e+00, -2.2972e+01],
        [ 1.3211e+02,  5.1700e+01, -5.2819e+00,  1.2974e+02,  6.6028e+00,
          1.2047e+02, -2.4509e+01,  1.2759e+02, -1.3337e-01,  1.2877e+02],
        [-3.6565e+01, -1.4253e+01, -1.2664e+01, -3.8001e+01, -3.3182e+01,
         -4.1231e+01, -2.1758e+01, -3.8890e+01, -6.4024e+00, -3.8028e+01]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-12.3540, -12.1499, -14.5866,  19.6553,  14.5999,   2.0817,  -9.2270,
         -14.2041,   0.5913, -15.1779],
        [ 12.1902,  12.0210,  14.5602, -19.2133, -14.3660,  -1.9545,   9.0460,
          14.0682,  -0.4425,  14.9125]], device='cuda:0'))])
xi:  [17.940994]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 639.741926310327
W_T_median: 405.22649175094875
W_T_pctile_5: 178.41590310864012
W_T_CVAR_5_pct: 1.3981545094144077
Average q (qsum/M+1):  48.245542464717744
Optimal xi:  [17.940994]
Expected(across Rb) median(across samples) p_equity:  0.2648974806070328
obj fun:  tensor(-1502.0155, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1485.5161385873832
Current xi:  [14.6664715]
objective value function right now is: -1485.5161385873832
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.1615095]
objective value function right now is: -1444.2118231692907
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.655366]
objective value function right now is: -1479.4177318039979
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.3545265]
objective value function right now is: -1476.3601877872
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.830986]
objective value function right now is: -1461.7923497607462
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1497.3821598909763
Current xi:  [16.215427]
objective value function right now is: -1497.3821598909763
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [14.891345]
objective value function right now is: -1490.5305875016973
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.95602]
objective value function right now is: -1436.674400714959
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.718379]
objective value function right now is: -1491.4474568094968
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.293419]
objective value function right now is: -1434.803498595596
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.07384]
objective value function right now is: -1497.0472271947324
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.765148]
objective value function right now is: -1495.0466858567975
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.846577]
objective value function right now is: -1487.1665907956167
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [15.966052]
objective value function right now is: -1475.3629946751753
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.21259]
objective value function right now is: -1484.6896536627992
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.015123]
objective value function right now is: -1453.5461324089295
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.53098]
objective value function right now is: -1463.6253277993064
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.954644]
objective value function right now is: -1492.4940057275123
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.14085]
objective value function right now is: -1469.5562557595758
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.621705]
objective value function right now is: -1403.4369546522087
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.624745]
objective value function right now is: -1481.8335024923786
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.166041]
objective value function right now is: -1426.1760647207484
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.537437]
objective value function right now is: -1473.5714196591657
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.491804]
objective value function right now is: -1496.2264865226675
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.456688]
objective value function right now is: -1485.5921561270166
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.946611]
objective value function right now is: -1491.253614427827
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.392333]
objective value function right now is: -1445.3482756300414
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [14.590948]
objective value function right now is: -1445.1791593376406
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [14.983595]
objective value function right now is: -1468.320062126531
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.142636]
objective value function right now is: -1475.212775850489
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.749344]
objective value function right now is: -1485.071118587086
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.477791]
objective value function right now is: -1428.5277738735592
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.566392]
objective value function right now is: -1476.4051997554711
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.293493]
objective value function right now is: -1467.6365331297704
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.574844]
objective value function right now is: -1482.1853909294894
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.029789]
objective value function right now is: -1464.2226094394618
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.347822]
objective value function right now is: -1484.8458412796585
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.756634]
objective value function right now is: -1491.016562547608
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.242264]
objective value function right now is: -1483.8373335935917
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.660176]
objective value function right now is: -1458.2168429102114
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.312858]
objective value function right now is: -1495.6132530293828
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.916604]
objective value function right now is: -1476.797341450227
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.451596]
objective value function right now is: -1494.928610217873
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.383184]
objective value function right now is: -1488.362323086402
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.010005]
objective value function right now is: -1477.305901106623
new min fval from sgd:  -1497.607606574141
new min fval from sgd:  -1499.7787307576523
new min fval from sgd:  -1500.9048312971443
new min fval from sgd:  -1502.6476426105476
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.8821]
objective value function right now is: -1486.7050249354065
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.663143]
objective value function right now is: -1483.484834336749
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.415186]
objective value function right now is: -1480.6535426963712
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.592336]
objective value function right now is: -1492.9677935065715
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.371195]
objective value function right now is: -1482.3657555368598
min fval:  -1502.6476426105476
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  3.9782,  -2.4521],
        [  3.8991,  -2.3780],
        [ -4.3452,   2.7084],
        [  3.9633,  -2.4394],
        [  3.1362,  -7.9992],
        [  3.9785,  -2.4413],
        [  3.9579,  -2.4332],
        [  3.9310,  -2.4080],
        [ 12.0623,   2.2999],
        [ 14.9673, -18.8654]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[   7.5420,    5.5445,  -94.9443,    7.6159,   -5.9680,    7.2711,
            7.2178,    6.4634,  -71.5869,   -7.3976],
        [   2.3525,    4.0816,    2.8768,    2.8404,   17.2886,    2.3773,
            3.2524,    3.6658,    5.4994,   16.1966],
        [   9.1279,    7.6011, -125.4396,    8.2964,  -12.7761,    8.5923,
            8.3676,    7.9706,  -19.4202,  -12.8650],
        [   4.2585,    2.3681,  -85.8070,    3.7725,  -10.7260,    4.3194,
            3.8743,    3.3258,  -41.7707,  -11.7515],
        [   2.1184,    0.5230,  -43.5966,    2.0831,   -4.3788,    1.3209,
            1.9341,    1.1349,  -96.9002,   -5.0391],
        [   1.9508,    1.1218,  -42.3715,    1.9045,   -4.4999,    1.9504,
            1.7324,    1.4399,  -96.3938,   -5.7794],
        [   2.2663,    0.9399,  -44.7241,    1.8062,   -4.4472,    1.9669,
            1.5630,    0.9673,  -94.6625,   -5.5249],
        [  10.3235,    8.6066, -187.6573,    9.7452,  -17.1963,   10.3188,
            9.6910,    9.1839,  -15.6051,  -16.5971],
        [   2.4490,    0.5479,  -48.2143,    2.0436,   -7.2669,    2.2337,
            1.3713,    1.0012,  -52.3323,   -8.4050],
        [   4.9494,    3.6996,  -46.5358,    4.8917,   -0.4517,    5.1732,
            4.7479,    4.1991, -120.4388,   -1.6488]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -6.6614,  28.3233, -24.8630,  -5.7902, -13.4884, -10.8843, -13.7153,
         -23.1278,  -7.4116, -19.2462]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.6900,   4.8385],
        [  9.1198,   3.8060],
        [ -6.2427,   0.3476],
        [ -2.2527,   4.7685],
        [-41.0987,  -1.9785],
        [ -2.1666,   4.7872],
        [-11.3648,  -2.1608],
        [ -2.1656,   4.7878],
        [ -3.0829,   3.4726],
        [ -2.2385,   4.7718]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-7.2418e+01, -2.7882e+01, -3.0181e+01, -7.6802e+01, -2.9307e+00,
         -7.9626e+01, -4.4133e+01, -7.7386e+01, -1.1091e+01, -7.7170e+01],
        [-6.4523e+01, -2.9146e+01, -3.0827e+01, -6.8919e+01, -2.1758e+01,
         -7.2284e+01, -4.6203e+01, -6.9563e+01, -1.3145e+01, -6.9220e+01],
        [-1.2270e+02, -3.9796e+00,  1.0416e+00, -1.2728e+02,  2.1645e+00,
         -1.2669e+02,  7.9492e+00, -1.2879e+02, -8.8044e+00, -1.2728e+02],
        [-1.4979e+02, -4.5333e+00, -1.0541e+02, -1.6441e+02, -3.6045e+02,
         -1.6131e+02, -7.8527e+01, -1.6144e+02, -1.0287e+02, -1.6352e+02],
        [-2.4101e+02, -7.4222e+00, -1.0259e+02, -2.5555e+02, -2.6342e+02,
         -2.5324e+02, -5.0792e+01, -2.5341e+02, -1.2970e+02, -2.5502e+02],
        [-2.7525e+02, -1.9570e+01, -6.1301e+01, -2.8008e+02, -1.5253e+02,
         -2.8289e+02, -3.2145e+01, -2.8149e+02, -9.7115e+01, -2.8049e+02],
        [ 4.2403e+00, -7.3952e+00, -6.2824e+00, -3.1075e-01, -1.8090e+02,
         -4.1960e-01, -2.2805e+01, -4.9976e-03, -7.3516e+00, -2.5785e-01],
        [-4.6684e+01, -2.3074e+01, -2.9218e+01, -5.1036e+01, -2.7001e+01,
         -5.4084e+01, -4.3099e+01, -5.2052e+01, -1.6669e+01, -5.1762e+01],
        [ 1.7444e+02,  7.8703e+01,  1.5389e+00,  1.6894e+02,  7.4945e+00,
          1.6296e+02, -3.2205e+01,  1.6896e+02,  1.3486e+01,  1.6830e+02],
        [-6.3120e+01, -2.2776e+01, -2.6517e+01, -6.8236e+01, -1.1399e+01,
         -7.1035e+01, -4.0325e+01, -6.8776e+01, -1.4293e+01, -6.8227e+01]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -1.3471,  -1.3472, -14.8018,  20.7605,  15.0021,   3.6888,  -3.2342,
          -3.1164,   0.5554,  -4.0849],
        [  1.1832,   1.2183,  14.7756, -20.3185, -14.7683,  -3.5617,   3.0532,
           2.9805,  -0.4066,   3.8194]], device='cuda:0'))])
xi:  [15.394799]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 637.7157124576581
W_T_median: 381.5120507956293
W_T_pctile_5: 154.87475933368583
W_T_CVAR_5_pct: -4.90172616469853
Average q (qsum/M+1):  48.947655462449596
Optimal xi:  [15.394799]
Expected(across Rb) median(across samples) p_equity:  0.27701667745908104
obj fun:  tensor(-1502.6476, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1498.182471496591
Current xi:  [9.765864]
objective value function right now is: -1498.182471496591
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1507.4512410388686
Current xi:  [11.096726]
objective value function right now is: -1507.4512410388686
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1508.6552416968732
Current xi:  [10.645194]
objective value function right now is: -1508.6552416968732
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.171847]
objective value function right now is: -1479.8942247940518
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1511.952490486457
Current xi:  [10.928051]
objective value function right now is: -1511.952490486457
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1519.8714888399243
Current xi:  [10.850655]
objective value function right now is: -1519.8714888399243
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [10.401769]
objective value function right now is: -1514.2528085352906
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.449697]
objective value function right now is: -1513.674370258835
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.037195]
objective value function right now is: -1508.4972252929022
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.408411]
objective value function right now is: -1499.858841777006
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.6706085]
objective value function right now is: -1490.7919498096771
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.512069]
objective value function right now is: -1510.7338335943941
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.517334]
objective value function right now is: -1507.385344330975
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [10.211526]
objective value function right now is: -1504.9524412818566
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.266953]
objective value function right now is: -1491.317814454746
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.181642]
objective value function right now is: -1507.522158450203
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.623869]
objective value function right now is: -1505.3630453852304
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.550703]
objective value function right now is: -1497.974212969063
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.107096]
objective value function right now is: -1510.4476649219434
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.045905]
objective value function right now is: -1508.6418022804535
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.731917]
objective value function right now is: -1494.2615636113583
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.08652]
objective value function right now is: -1511.2182512688548
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.505887]
objective value function right now is: -1515.1414607120296
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.983844]
objective value function right now is: -1508.4541616395015
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.492973]
objective value function right now is: -1511.8907419774462
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.131376]
objective value function right now is: -1514.771554496193
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.970031]
objective value function right now is: -1514.571187611944
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [10.347331]
objective value function right now is: -1491.0576995757158
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [10.434019]
objective value function right now is: -1490.6864100757182
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.595425]
objective value function right now is: -1506.8677304517423
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.495572]
objective value function right now is: -1497.790537537992
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.705504]
objective value function right now is: -1498.3076063809633
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.045924]
objective value function right now is: -1486.7014710542987
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.771014]
objective value function right now is: -1511.9835782613259
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.571291]
objective value function right now is: -1508.7817846690316
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.537177]
objective value function right now is: -1513.347135380712
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.477204]
objective value function right now is: -1511.044082103969
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.051978]
objective value function right now is: -1485.3025627795691
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.682392]
objective value function right now is: -1506.4125668390072
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.773233]
objective value function right now is: -1513.5458040854521
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.546124]
objective value function right now is: -1511.5179632998952
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.936356]
objective value function right now is: -1505.6057748120625
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.326063]
objective value function right now is: -1498.947377738676
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.650671]
objective value function right now is: -1516.513917127254
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.693125]
objective value function right now is: -1508.1848519583575
new min fval from sgd:  -1521.331501663526
new min fval from sgd:  -1521.5429716135486
new min fval from sgd:  -1521.6096187143103
new min fval from sgd:  -1521.6555561395824
new min fval from sgd:  -1522.0493235444221
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.578024]
objective value function right now is: -1496.6397903054078
new min fval from sgd:  -1522.524150212423
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.9598255]
objective value function right now is: -1506.0635233559958
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.436971]
objective value function right now is: -1509.743485929535
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.263794]
objective value function right now is: -1491.4986438645217
new min fval from sgd:  -1522.5689336255537
new min fval from sgd:  -1522.9528775298834
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.029496]
objective value function right now is: -1517.0676298694982
min fval:  -1522.9528775298834
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  3.3006,  -2.5125],
        [  3.2776,  -2.4707],
        [ -3.4250,   2.6694],
        [  3.2906,  -2.5019],
        [  0.7047,  -8.5675],
        [  3.2953,  -2.5100],
        [  3.2912,  -2.5005],
        [  3.2846,  -2.4858],
        [ 11.7698,   2.2084],
        [  9.5851, -19.9583]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 4.8875e+00,  2.8853e+00, -1.1114e+02,  4.9556e+00, -9.7342e+00,
          4.6310e+00,  4.5594e+00,  3.8060e+00, -7.6679e+01, -1.1345e+01],
        [ 2.3525e+00,  4.0816e+00,  2.8768e+00,  2.8404e+00,  1.7289e+01,
          2.3773e+00,  3.2524e+00,  3.6658e+00,  5.4994e+00,  1.6197e+01],
        [ 9.5662e+00,  8.0282e+00, -1.5023e+02,  8.7393e+00, -1.4265e+01,
          9.0440e+00,  8.8075e+00,  8.4065e+00, -1.8337e+01, -1.4493e+01],
        [ 4.1624e+00,  2.2584e+00, -9.2016e+01,  3.6740e+00, -1.1560e+01,
          4.2289e+00,  3.7749e+00,  3.2222e+00, -4.3996e+01, -1.2690e+01],
        [ 1.9421e+00,  3.4047e-01, -4.4968e+01,  1.9099e+00, -4.9526e+00,
          1.1553e+00,  1.7589e+00,  9.5751e-01, -9.7963e+01, -5.6824e+00],
        [ 1.7742e+00,  9.3907e-01, -4.3726e+01,  1.7309e+00, -5.0746e+00,
          1.7846e+00,  1.5569e+00,  1.2622e+00, -9.7442e+01, -6.4234e+00],
        [ 2.0881e+00,  7.5587e-01, -4.6091e+01,  1.6311e+00, -5.0322e+00,
          1.7996e+00,  1.3859e+00,  7.8816e-01, -9.5711e+01, -6.1799e+00],
        [ 1.1785e+01,  1.0096e+01, -2.5519e+02,  1.1203e+01, -2.4056e+01,
          1.1757e+01,  1.1156e+01,  1.0657e+01, -2.2017e-01, -2.0566e+01],
        [ 2.2954e+00,  3.9180e-01, -4.9488e+01,  1.8913e+00, -7.9315e+00,
          2.0896e+00,  1.2182e+00,  8.4775e-01, -5.3065e+01, -9.1474e+00],
        [ 6.1288e+00,  4.8526e+00, -5.2938e+01,  6.0624e+00,  3.9123e-01,
          6.3544e+00,  5.9171e+00,  5.3638e+00, -1.3481e+02, -9.7305e-01]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -4.6559,  27.7438, -34.3991,  -4.8201, -13.3377, -10.7327, -13.5767,
         -17.8347,  -7.3848, -21.0430]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.0671,   4.9627],
        [  9.9758,   4.0150],
        [ -6.7578,   0.0834],
        [ -2.0155,   4.9863],
        [-38.6374,  -1.0966],
        [ -1.9497,   5.0126],
        [ -9.8234,  -1.2923],
        [ -1.9905,   4.9981],
        [ -2.4141,   4.7218],
        [ -2.0095,   4.9890]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-8.6949e+01, -3.9198e+01, -3.6528e+01, -9.3022e+01, -1.4576e+00,
         -9.6058e+01, -5.0784e+01, -9.3496e+01, -2.4448e+01, -9.3369e+01],
        [-7.7503e+01, -4.0541e+01, -3.6647e+01, -8.3526e+01, -2.0289e+01,
         -8.7101e+01, -5.3295e+01, -8.4064e+01, -2.5520e+01, -8.3806e+01],
        [-1.6582e+02, -4.1602e+00,  2.9854e-01, -1.7247e+02,  8.9682e-01,
         -1.7233e+02,  1.1622e+01, -1.7391e+02, -5.6721e+01, -1.7246e+02],
        [-1.3159e+02, -4.4429e+00, -1.4103e+02, -1.3193e+02, -4.4095e+02,
         -1.2544e+02, -9.0587e+01, -1.2972e+02, -9.5623e+01, -1.3119e+02],
        [-3.8723e+02, -6.3672e+00, -1.4700e+02, -3.9556e+02, -3.0134e+02,
         -3.9130e+02, -3.2162e+01, -3.9377e+02, -2.8011e+02, -3.9511e+02],
        [-3.4456e+02, -2.1452e+01, -6.8640e+01, -3.5092e+02, -1.5938e+02,
         -3.5374e+02, -2.0768e+01, -3.5220e+02, -1.6994e+02, -3.5132e+02],
        [-7.3351e-01, -2.2696e+01, -7.1361e+00, -1.2793e+00, -1.9565e+02,
         -7.1861e-01, -3.5306e+01, -1.3273e+00, -1.4198e+01, -1.3017e+00],
        [-5.9224e+01, -3.9563e+01, -3.9183e+01, -6.4890e+01, -2.5274e+01,
         -6.7962e+01, -5.2217e+01, -6.5786e+01, -2.8321e+01, -6.5594e+01],
        [ 2.1433e+02,  9.5644e+01,  2.8412e+00,  2.1357e+02,  1.2151e+01,
          2.0937e+02, -3.8637e+01,  2.1362e+02,  4.8441e+01,  2.1295e+02],
        [-7.1502e+01, -3.8232e+01, -3.5566e+01, -7.7535e+01, -9.3848e+00,
         -8.0301e+01, -5.0529e+01, -7.7981e+01, -2.1236e+01, -7.7508e+01]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -8.7792,  -9.2922, -15.5928,  16.3378,  17.1865,   4.2387,  -1.5404,
          -9.8034,   0.4983, -10.3481],
        [  8.6156,   9.1636,  15.5667, -15.8958, -16.9526,  -4.1116,   1.3594,
           9.6680,  -0.3495,  10.0831]], device='cuda:0'))])
xi:  [10.80019]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 604.567268715786
W_T_median: 308.5523731836581
W_T_pctile_5: 110.95742353899874
W_T_CVAR_5_pct: -26.114644096951434
Average q (qsum/M+1):  50.396334740423384
Optimal xi:  [10.80019]
Expected(across Rb) median(across samples) p_equity:  0.29991061687469484
obj fun:  tensor(-1522.9529, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1535.925971568523
Current xi:  [4.685661]
objective value function right now is: -1535.925971568523
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.493991]
objective value function right now is: -1526.9799645187459
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.266724]
objective value function right now is: -1512.5305982856478
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1537.2154184327649
Current xi:  [5.5727267]
objective value function right now is: -1537.2154184327649
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.821417]
objective value function right now is: -1526.5481615690294
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.023137]
objective value function right now is: -1536.7680528310946
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [5.5531]
objective value function right now is: -1496.5032022747298
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.6139774]
objective value function right now is: -1537.2080515130226
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.0909286]
objective value function right now is: -1532.341522285341
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.0539484]
objective value function right now is: -1527.5404531582794
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.100421]
objective value function right now is: -1532.4535070408228
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.7099323]
objective value function right now is: -1513.0309329810493
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1539.5415998215815
Current xi:  [5.7434473]
objective value function right now is: -1539.5415998215815
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [5.8692346]
objective value function right now is: -1533.7836468892947
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.68608]
objective value function right now is: -1492.2969612050492
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.4053316]
objective value function right now is: -1516.8427697840752
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.621899]
objective value function right now is: -1518.5344439366436
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.47994]
objective value function right now is: -1513.816112553816
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.0403256]
objective value function right now is: -1535.740599952677
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.6639028]
objective value function right now is: -1523.9442671537395
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.98567]
objective value function right now is: -1499.3543015116907
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.1274924]
objective value function right now is: -1536.3392639403633
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.4468083]
objective value function right now is: -1491.5416896114
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.357955]
objective value function right now is: -1500.0613889171186
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1541.661991768721
Current xi:  [5.7428055]
objective value function right now is: -1541.661991768721
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.6168203]
objective value function right now is: -1534.7690308121494
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.6318517]
objective value function right now is: -1525.2444821713482
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [5.6465583]
objective value function right now is: -1527.6990297223083
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [5.614907]
objective value function right now is: -1532.2455531546502
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.4759083]
objective value function right now is: -1539.054898195939
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.4287095]
objective value function right now is: -1513.8635314382777
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.888469]
objective value function right now is: -1529.382593356227
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.485962]
objective value function right now is: -1539.2828413887746
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.614655]
objective value function right now is: -1503.2483083241345
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.279368]
objective value function right now is: -1528.6632989047007
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.578793]
objective value function right now is: -1537.5769325920428
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.600947]
objective value function right now is: -1538.1648982629217
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.5304456]
objective value function right now is: -1539.9780401568607
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.5282755]
objective value function right now is: -1493.418792925776
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.747141]
objective value function right now is: -1540.0177422310044
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.464835]
objective value function right now is: -1530.1985985915092
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.8638315]
objective value function right now is: -1534.6158702850028
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.4813967]
objective value function right now is: -1524.6066329311836
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.273604]
objective value function right now is: -1536.9131865141758
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.417028]
objective value function right now is: -1534.1397734546717
new min fval from sgd:  -1541.8345384263368
new min fval from sgd:  -1542.3719116130085
new min fval from sgd:  -1542.9359298327076
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.8312187]
objective value function right now is: -1517.3496165974818
new min fval from sgd:  -1543.2293675979554
new min fval from sgd:  -1544.1204506955362
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.33543]
objective value function right now is: -1520.4008724163853
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.8006916]
objective value function right now is: -1537.811393362703
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.9725447]
objective value function right now is: -1519.2270774566339
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.409812]
objective value function right now is: -1526.4708976426944
min fval:  -1544.1204506955362
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  2.6382,  -2.6743],
        [  2.6340,  -2.6195],
        [ -2.7035,   2.8879],
        [  2.6314,  -2.6583],
        [ -1.5763,  -8.6783],
        [  2.6325,  -2.6690],
        [  2.6331,  -2.6571],
        [  2.6336,  -2.6388],
        [ 13.7898,   2.8374],
        [  3.8705, -18.8613]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 3.9864e+00,  1.9729e+00, -1.1219e+02,  4.0516e+00, -1.0994e+01,
          3.7294e+00,  3.6550e+00,  2.8978e+00, -7.7746e+01, -1.2634e+01],
        [ 2.3525e+00,  4.0816e+00,  2.8768e+00,  2.8404e+00,  1.7289e+01,
          2.3773e+00,  3.2524e+00,  3.6658e+00,  5.4994e+00,  1.6197e+01],
        [ 9.1880e+00,  7.6254e+00, -1.7547e+02,  8.3552e+00, -1.5014e+01,
          8.6645e+00,  8.4226e+00,  8.0131e+00, -1.3696e+01, -1.5783e+01],
        [ 3.6184e+00,  1.7069e+00, -9.3049e+01,  3.1276e+00, -1.2640e+01,
          3.6842e+00,  3.2284e+00,  2.6735e+00, -4.4337e+01, -1.3804e+01],
        [ 1.0974e+00, -5.1211e-01, -4.5552e+01,  1.0637e+00, -6.1553e+00,
          3.1087e-01,  9.1232e-01,  1.0809e-01, -9.8717e+01, -6.9118e+00],
        [ 9.3449e-01,  9.1475e-02, -4.4305e+01,  8.8967e-01, -6.2718e+00,
          9.4506e-01,  7.1520e-01,  4.1777e-01, -9.8190e+01, -7.6471e+00],
        [ 1.2564e+00, -8.3773e-02, -4.6670e+01,  7.9781e-01, -6.2209e+00,
          9.6802e-01,  5.5226e-01, -4.8345e-02, -9.6457e+01, -7.3953e+00],
        [ 1.2406e+01,  1.0715e+01, -3.1803e+02,  1.1829e+01, -2.7918e+01,
          1.2379e+01,  1.1780e+01,  1.1278e+01,  4.2104e+00, -2.3681e+01],
        [ 1.7761e+00, -1.3363e-01, -4.9916e+01,  1.3706e+00, -8.7827e+00,
          1.5702e+00,  6.9722e-01,  3.2469e-01, -5.3532e+01, -1.0023e+01],
        [ 5.0713e+00,  3.7645e+00, -6.1285e+01,  4.9875e+00, -1.0920e+00,
          5.2857e+00,  4.8444e+00,  4.2839e+00, -1.5167e+02, -2.5277e+00]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -4.6376,  29.9275, -37.3301,  -4.8786, -13.3021, -10.6930, -13.5429,
         -20.4004,  -7.3811, -23.4191]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.7591,   4.7546],
        [ 10.1081,   3.7636],
        [ -4.0988,   4.1780],
        [ -1.7395,   4.7649],
        [-42.8912,  -4.9512],
        [ -1.5994,   4.7825],
        [ -9.0518,  -1.6686],
        [ -1.6905,   4.7728],
        [ -0.3459,   4.5878],
        [ -1.7246,   4.7669]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -74.9090,  -35.1476,  -30.5710,  -81.0025,    1.4092,  -84.0311,
          -47.9605,  -81.4833,  -11.9628,  -81.3484],
        [ -66.2184,  -36.2737,  -30.9916,  -72.2591,  -18.0283,  -75.8216,
          -49.9799,  -72.8004,  -13.9035,  -72.5375],
        [-200.9407,   -4.6086,  -30.7852, -207.8381,    2.4734, -207.8902,
           13.7030, -209.4027,  -84.1536, -207.8446],
        [ -40.1874,   -4.0765,  -82.0056,  -40.1179, -476.7457,  -32.4449,
          -91.7060,  -37.4838,   -1.8216,  -39.2518],
        [-543.8661,   -5.8823, -295.1164, -551.9153, -378.8038, -546.5561,
          -33.6217, -549.7589, -427.4268, -551.3497],
        [-379.1505,  -27.8407,  -79.7824, -385.4655, -162.6375, -388.3737,
          -27.0543, -386.7554, -208.9497, -385.8741],
        [   3.1523,  -29.1835,  -17.1820,    2.5831, -183.8937,    3.1518,
          -43.9768,    2.5316,   -9.9309,    2.5617],
        [ -47.3242,  -35.3698,  -33.2112,  -53.0135,  -21.7693,  -56.0849,
          -49.1790,  -53.9168,  -16.0320,  -53.7159],
        [ 236.8685,  104.1981,   18.1256,  236.8514,   10.9436,  234.2157,
          -41.7126,  237.5212,   70.7637,  236.3931],
        [ -58.6076,  -34.1965,  -29.1252,  -64.6667,   -5.2250,  -67.4386,
          -47.9340,  -65.1231,   -7.8823,  -64.6384]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-12.5979, -13.0848, -17.1318,  11.4463,  21.6025,   0.3974,  -4.2054,
         -13.6243,   0.4006, -14.1943],
        [ 12.4332,  12.9551,  17.1058, -11.0042, -21.3685,  -0.2704,   4.0234,
          13.4877,  -0.2517,  13.9281]], device='cuda:0'))])
xi:  [5.441573]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 628.4684861598579
W_T_median: 270.8715562984397
W_T_pctile_5: 52.14890146479261
W_T_CVAR_5_pct: -63.04790294778877
Average q (qsum/M+1):  51.84997164818548
Optimal xi:  [5.441573]
Expected(across Rb) median(across samples) p_equity:  0.3424709141254425
obj fun:  tensor(-1544.1205, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1580.803745465905
Current xi:  [-10.748268]
objective value function right now is: -1580.803745465905
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1595.8864398905773
Current xi:  [-12.279327]
objective value function right now is: -1595.8864398905773
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1598.3527976453631
Current xi:  [-13.348279]
objective value function right now is: -1598.3527976453631
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.406877]
objective value function right now is: -1590.620723100418
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1601.0040908114747
Current xi:  [-15.017887]
objective value function right now is: -1601.0040908114747
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1601.15575684612
Current xi:  [-15.291732]
objective value function right now is: -1601.15575684612
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-15.603257]
objective value function right now is: -1596.4906430275655
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.868681]
objective value function right now is: -1600.1209012065121
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.929202]
objective value function right now is: -1599.226638316754
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-16.075941]
objective value function right now is: -1589.0683503546404
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-16.009743]
objective value function right now is: -1591.7437773934016
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.960106]
objective value function right now is: -1588.0227365836079
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.86624]
objective value function right now is: -1590.6744178065744
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-15.928355]
objective value function right now is: -1598.1714884156224
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.871309]
objective value function right now is: -1600.7173977710006
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-16.064478]
objective value function right now is: -1599.3305816901066
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.900487]
objective value function right now is: -1600.9270265375962
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.90048]
objective value function right now is: -1598.298246118927
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-16.070068]
objective value function right now is: -1599.6005370714297
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.831547]
objective value function right now is: -1584.6937901896551
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.915453]
objective value function right now is: -1596.9386429185386
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.766589]
objective value function right now is: -1596.3713425664873
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-16.017153]
objective value function right now is: -1598.9143661545327
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-15.957166]
objective value function right now is: -1596.3441620256426
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-16.196136]
objective value function right now is: -1583.4763116058841
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.232209521158
Current xi:  [-16.437656]
objective value function right now is: -1602.232209521158
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-16.256477]
objective value function right now is: -1583.4213631018233
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-15.000018]
objective value function right now is: -1596.3631063942712
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-14.50712]
objective value function right now is: -1585.6917712165962
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.05512]
objective value function right now is: -1591.3091498285205
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.949869]
objective value function right now is: -1581.1475072001488
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.9376545]
objective value function right now is: -1581.3677894463137
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.760875]
objective value function right now is: -1593.7709021709234
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.7622385]
objective value function right now is: -1566.0739146435262
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.825224]
objective value function right now is: -1592.1240113081847
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.073054]
objective value function right now is: -1589.3104500924276
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.354047]
objective value function right now is: -1597.2535555845616
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.04814]
objective value function right now is: -1591.9542212917477
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.949572]
objective value function right now is: -1591.5254029480332
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.582926]
objective value function right now is: -1573.2158575073067
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.681523]
objective value function right now is: -1591.3364001358657
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.062236]
objective value function right now is: -1595.7159102390608
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.7414255]
objective value function right now is: -1597.497161612106
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.913433]
objective value function right now is: -1585.909200868704
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.581101]
objective value function right now is: -1582.7492908770485
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.513305]
objective value function right now is: -1591.6402473788212
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.794161]
objective value function right now is: -1596.5678782704897
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.7093]
objective value function right now is: -1589.0566466446571
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.517294]
objective value function right now is: -1595.830335606122
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.460319]
objective value function right now is: -1596.7851064420292
min fval:  -1552.341180048321
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  0.2600,  -2.5938],
        [  0.2622,  -2.5552],
        [ -0.1530,   2.7878],
        [  0.2560,  -2.5750],
        [ -7.8630,  -8.4567],
        [  0.2526,  -2.5824],
        [  0.2578,  -2.5766],
        [  0.2592,  -2.5662],
        [ 15.8599,   0.9353],
        [  5.7409, -24.5255]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 3.5934e+00,  1.5797e+00, -1.1229e+02,  3.6585e+00, -1.1429e+01,
          3.3364e+00,  3.2619e+00,  2.5047e+00, -7.7845e+01, -1.3071e+01],
        [ 2.3525e+00,  4.0816e+00,  2.8768e+00,  2.8404e+00,  1.7289e+01,
          2.3773e+00,  3.2524e+00,  3.6658e+00,  5.4994e+00,  1.6197e+01],
        [ 9.3517e+00,  7.7853e+00, -1.8537e+02,  8.5179e+00, -1.4632e+01,
          8.8280e+00,  8.5852e+00,  8.1745e+00, -8.9969e+00, -1.6235e+01],
        [ 3.2856e+00,  1.3741e+00, -9.3154e+01,  2.7948e+00, -1.3020e+01,
          3.3514e+00,  2.8956e+00,  2.3406e+00, -4.4411e+01, -1.4186e+01],
        [ 8.9697e-01, -7.1289e-01, -4.5594e+01,  8.6320e-01, -6.3882e+00,
          1.1049e-01,  7.1182e-01, -9.2542e-02, -9.8774e+01, -7.1463e+00],
        [ 7.3486e-01, -1.0848e-01, -4.4347e+01,  6.9003e-01, -6.5039e+00,
          7.4551e-01,  5.1554e-01,  2.1797e-01, -9.8246e+01, -7.8807e+00],
        [ 1.0552e+00, -2.8524e-01, -4.6712e+01,  5.9666e-01, -6.4546e+00,
          7.6695e-01,  3.5107e-01, -2.4966e-01, -9.6513e+01, -7.6305e+00],
        [ 1.2249e+01,  1.0558e+01, -3.3311e+02,  1.1673e+01, -2.8523e+01,
          1.2222e+01,  1.1624e+01,  1.1121e+01,  1.6846e+01, -2.4613e+01],
        [ 1.6174e+00, -2.9243e-01, -4.9950e+01,  1.2119e+00, -8.9719e+00,
          1.4116e+00,  5.3858e-01,  1.6600e-01, -5.3572e+01, -1.0213e+01],
        [ 3.6157e+00,  2.3043e+00, -6.3169e+01,  3.5303e+00, -2.5610e+00,
          3.8295e+00,  3.3872e+00,  2.8252e+00, -1.5449e+02, -4.0013e+00]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -4.5302,  32.2109, -40.1540,  -4.7761, -13.2926, -10.6805, -13.5335,
         -25.8094,  -7.3667, -23.4013]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-77.4592,  10.7366],
        [ 12.8248,   3.8956],
        [ -4.7008,   1.6012],
        [-77.7173,  10.7604],
        [-77.3887, -13.7387],
        [-92.4871,  -3.7098],
        [-11.2084,  -1.8171],
        [-77.7785,  10.7617],
        [-71.2367,   9.9978],
        [-77.7497,  10.7607]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -77.8111,  -27.1284,  -33.4547,  -83.9064,  -30.1500,  -86.9258,
          -48.4376,  -84.3857,  -12.9644,  -84.2518],
        [ -62.5290,  -28.1382,  -24.0615,  -68.5343,  -55.4976,  -72.0243,
          -47.1816,  -69.0411,  -11.7169,  -68.8027],
        [-159.5032,   -1.9467,   -9.6908, -168.6569,   -7.3471, -278.9030,
           -1.1556, -170.9087,  -13.7633, -168.9462],
        [ -52.9687,   -5.8625, -131.6620,  -52.8687, -518.6254,  -44.8481,
         -128.9908,  -50.0979,  -10.2617,  -51.9696],
        [-551.3336,   -4.8942, -334.8233, -559.3860, -466.7885, -553.9276,
          -80.2459, -557.1981, -432.6682, -558.8085],
        [-383.0539,  -29.8571,  -84.2695, -389.3459, -185.2767, -392.1365,
          -27.1922, -390.5979, -211.6851, -389.7421],
        [   4.6728,  -21.6152,   -8.3146,    4.0884, -212.3413,    4.3953,
          -42.5144,    4.0287,   -7.4589,    4.0652],
        [ -43.5829,  -27.1140,  -24.9603,  -49.2368,  -64.7047,  -52.2318,
          -46.3315,  -50.1049,  -13.7020,  -49.9287],
        [ 249.9068,  160.3748,   86.4054,  249.9046,   27.3640,  248.2731,
          -29.2273,  250.4316,   84.3464,  249.4284],
        [ -52.4093,  -25.8072,  -24.1407,  -58.4349,  -53.1100,  -61.1212,
          -45.6060,  -58.8557,   -4.9574,  -58.3961]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-19.0203, -20.4088, -12.9140,  14.0957,  28.6404,  -5.8261, -11.9862,
         -21.0248,   1.7591, -21.7005],
        [ 18.8556,  20.2792,  12.8880, -13.6536, -28.4065,   5.9530,  11.8042,
          20.8883,  -1.6103,  21.4344]], device='cuda:0'))])
xi:  [-13.581101]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 175.1378141298992
W_T_median: 20.388640138915562
W_T_pctile_5: -160.17118069255642
W_T_CVAR_5_pct: -226.80179869821492
Average q (qsum/M+1):  55.36164708291331
Optimal xi:  [-13.581101]
Expected(across Rb) median(across samples) p_equity:  0.3108333417524894
obj fun:  tensor(-1552.3412, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.2530106097558
Current xi:  [-17.847113]
objective value function right now is: -1673.2530106097558
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.3094441325661
Current xi:  [-30.632107]
objective value function right now is: -1686.3094441325661
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1690.9148072048677
Current xi:  [-38.80817]
objective value function right now is: -1690.9148072048677
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1690.9488155817094
Current xi:  [-42.250854]
objective value function right now is: -1690.9488155817094
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-44.05429]
objective value function right now is: -1689.5209568597106
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1691.0633992797038
Current xi:  [-44.38651]
objective value function right now is: -1691.0633992797038
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1692.3725520248895
Current xi:  [-45.987843]
objective value function right now is: -1692.3725520248895
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-46.226006]
objective value function right now is: -1689.4031677097307
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-47.793827]
objective value function right now is: -1685.2041609182309
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-47.54484]
objective value function right now is: -1675.84648048834
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-48.327427]
objective value function right now is: -1690.6427167427134
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-47.57151]
objective value function right now is: -1691.137171617788
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-46.544567]
objective value function right now is: -1686.3406058158291
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-46.87948]
objective value function right now is: -1688.7593990172647
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-46.4535]
objective value function right now is: -1689.8764321916601
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-45.830185]
objective value function right now is: -1689.963313111844
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-45.056118]
objective value function right now is: -1689.1606864312357
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-46.069912]
objective value function right now is: -1691.1120631923673
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-46.257874]
objective value function right now is: -1689.052674796454
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-45.64685]
objective value function right now is: -1691.6520808443963
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-46.514595]
objective value function right now is: -1689.1778057305587
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-46.528385]
objective value function right now is: -1691.9902033374474
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-45.784176]
objective value function right now is: -1690.3791287179195
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-45.602272]
objective value function right now is: -1692.0930808592834
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-45.430264]
objective value function right now is: -1691.5223225236823
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-45.594353]
objective value function right now is: -1691.0954884143957
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-44.56196]
objective value function right now is: -1678.8448854304036
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-47.95132]
objective value function right now is: -1689.2008186746998
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-46.97679]
objective value function right now is: -1690.8974493047745
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-46.67972]
objective value function right now is: -1691.6735869023084
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-46.558777]
objective value function right now is: -1690.4842826815175
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1698.2579112608037
Current xi:  [-45.117405]
objective value function right now is: -1698.2579112608037
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-43.80789]
objective value function right now is: -1695.7215903374586
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1698.479190719789
Current xi:  [-44.574986]
objective value function right now is: -1698.479190719789
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -1698.4974469193269
Current xi:  [-43.860634]
objective value function right now is: -1698.4974469193269
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-45.264046]
objective value function right now is: -1690.3300837934958
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-46.68673]
objective value function right now is: -1690.9920116691035
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-43.42523]
objective value function right now is: -1695.7678003974047
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-44.378174]
objective value function right now is: -1696.8037552911546
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-45.520256]
objective value function right now is: -1690.9654528238807
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-47.09553]
objective value function right now is: -1691.4112201124965
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-47.127655]
objective value function right now is: -1691.989637175414
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-44.254337]
objective value function right now is: -1674.4696484031176
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-43.641945]
objective value function right now is: -1698.105050637237
new min fval from sgd:  -1698.7897539584824
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-44.160896]
objective value function right now is: -1698.7897539584824
new min fval from sgd:  -1698.826647925125
new min fval from sgd:  -1698.964814215507
new min fval from sgd:  -1699.2323261070708
new min fval from sgd:  -1699.403883791688
new min fval from sgd:  -1699.418278030957
new min fval from sgd:  -1699.4335379649183
new min fval from sgd:  -1699.5563731360292
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-43.53876]
objective value function right now is: -1695.7355014118912
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-43.2259]
objective value function right now is: -1688.718717603737
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-44.3772]
objective value function right now is: -1697.6892073901022
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-44.026966]
objective value function right now is: -1694.3889680287393
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-44.58323]
objective value function right now is: -1698.897206952698
min fval:  -1699.5563731360292
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  1.0824,  -2.6096],
        [  1.0957,  -2.5861],
        [  1.3703,   3.3161],
        [  1.0865,  -2.6023],
        [  0.1243, -21.3221],
        [  1.0826,  -2.6091],
        [  1.0868,  -2.6017],
        [  1.0908,  -2.5947],
        [ -1.7943,  23.5893],
        [  6.4480, -25.4318]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 2.4372e+00,  4.2037e-01, -1.1696e+02,  2.5021e+00, -1.2640e+01,
          2.1813e+00,  2.1051e+00,  1.3469e+00, -8.0717e+01, -1.4282e+01],
        [ 2.3525e+00,  4.0816e+00,  2.8768e+00,  2.8404e+00,  1.7289e+01,
          2.3773e+00,  3.2524e+00,  3.6658e+00,  5.4994e+00,  1.6197e+01],
        [ 5.2572e+00,  3.6852e+00, -2.0905e+02,  4.4228e+00, -1.9535e+01,
          4.7351e+00,  4.4896e+00,  4.0770e+00, -1.7254e+01, -2.1326e+01],
        [ 2.5877e+00,  6.7258e-01, -9.4837e+01,  2.0961e+00, -1.3745e+01,
          2.6542e+00,  2.1967e+00,  1.6406e+00, -4.6479e+01, -1.4911e+01],
        [-2.3953e-01, -1.8528e+00, -4.8555e+01, -2.7384e-01, -7.4873e+00,
         -1.0253e+00, -4.2550e-01, -1.2309e+00, -1.0039e+02, -8.2453e+00],
        [-3.8643e-01, -1.2325e+00, -4.7681e+01, -4.3145e-01, -7.6100e+00,
         -3.7481e-01, -6.0625e-01, -9.0471e-01, -1.0013e+02, -8.9869e+00],
        [-6.0609e-02, -1.4040e+00, -4.9869e+01, -5.1942e-01, -7.5483e+00,
         -3.4789e-01, -7.6533e-01, -1.3670e+00, -9.7652e+01, -8.7243e+00],
        [ 1.2037e+01,  1.0337e+01, -3.5272e+02,  1.1455e+01, -2.9453e+01,
          1.2005e+01,  1.1407e+01,  1.0902e+01,  8.9572e+00, -2.5850e+01],
        [ 9.7458e-01, -9.3819e-01, -5.2651e+01,  5.6864e-01, -9.6009e+00,
          7.6952e-01, -1.0494e-01, -4.7845e-01, -5.6903e+01, -1.0842e+01],
        [-5.6114e-01, -1.8765e+00, -6.8153e+01, -6.4748e-01, -6.7335e+00,
         -3.4686e-01, -7.9082e-01, -1.3540e+00, -1.5730e+02, -8.1741e+00]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -3.2145,  28.9989, -38.9095,  -3.6416, -10.8579,  -8.5708, -11.4026,
         -45.1260,  -5.8176, -19.4004]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-102.5430,   12.2178],
        [   9.9926,    5.4380],
        [  -5.1008,    2.1382],
        [-134.2195,  -25.1728],
        [-107.5218,    9.8163],
        [-136.0272,  -35.2896],
        [  -5.2966,    1.9371],
        [ -78.8923,   15.3342],
        [ -11.2930,    0.3380],
        [-108.4814,   -4.2684]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -78.0862,  -46.2499,  -26.6380,  -83.7838,  -77.7228,   -8.5493,
           13.1623,  -84.6517,   -6.2612,  -86.5715],
        [ -64.5777,  -32.8903,  -44.6046,    4.5046,  -28.5235,   -9.9436,
            2.9924,  -70.9625,  -21.9661,  -83.5639],
        [-159.9393,   -2.2485,  -68.4259, -133.3470,  -39.9254,  -11.0733,
          -87.7694, -171.3221,  -64.3906, -173.9992],
        [ -25.5741,   -5.7953, -200.7536,   72.4401, -435.4525,    7.4430,
         -201.8449,  -28.8105,  -33.4782,  -12.4864],
        [-551.3336,  -29.6298, -253.6115, -588.9741, -580.2673, -681.1413,
            8.7782, -557.1981, -414.3567, -558.9351],
        [-383.7602,  -11.7657, -108.1769, -448.9980, -284.0014, -272.3360,
           10.0886, -391.2668, -221.1803, -399.5457],
        [  -1.7110,  -20.6704, -108.9847,   34.3873, -312.5015,  -29.0822,
          -87.3381,   -1.4441,  -93.8424,  -23.7527],
        [ -49.2341,  -31.2593,  -42.9642,   14.3716,  -44.8968,   -4.9088,
            0.6840,  -53.5817,  -22.3473, -188.4718],
        [ 249.9068,  268.1675,  158.4011,  249.9736,   88.6341,  262.5276,
           31.1522,  250.4316,   73.5342,  249.4284],
        [-101.9343,  -67.2336,   20.6957,  -30.1608,   21.0615,    6.6837,
           18.5000, -117.8572,  -56.5099,  -86.9955]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.8213,  -9.6974,  -6.7600,  12.8167,  29.5554,  -9.9893,  -3.9986,
          -9.2378,   0.7588,  -4.0744],
        [  0.6566,   9.5678,   6.7340, -12.3746, -29.3216,  10.1166,   3.8165,
           9.1013,  -0.6100,   3.8085]], device='cuda:0'))])
xi:  [-43.5009]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 45.949086181307216
W_T_median: -132.62962033314028
W_T_pctile_5: -436.23379050894033
W_T_CVAR_5_pct: -475.42900591744075
Average q (qsum/M+1):  57.891861454133064
Optimal xi:  [-43.5009]
Expected(across Rb) median(across samples) p_equity:  0.24096808202401127
obj fun:  tensor(-1699.5564, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1758.4210041525457
Current xi:  [-34.984634]
objective value function right now is: -1758.4210041525457
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1792.1908991550638
Current xi:  [-67.24792]
objective value function right now is: -1792.1908991550638
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1798.4407900029128
Current xi:  [-90.08781]
objective value function right now is: -1798.4407900029128
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1800.151790558812
Current xi:  [-98.00574]
objective value function right now is: -1800.151790558812
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1800.976114987199
Current xi:  [-95.42552]
objective value function right now is: -1800.976114987199
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-100.13786]
objective value function right now is: -1799.8725536288819
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-100.09388]
objective value function right now is: -1799.2404178457266
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-100.76436]
objective value function right now is: -1799.9112113445872
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-101.94404]
objective value function right now is: -1799.6607684856258
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-101.74207]
objective value function right now is: -1794.521238766997
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-101.74194]
objective value function right now is: -1799.684808144885
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-100.81102]
objective value function right now is: -1798.4921168052158
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-100.87595]
objective value function right now is: -1799.7682215451296
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-101.41465]
objective value function right now is: -1799.76238521087
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-102.5429]
objective value function right now is: -1799.6977144612904
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-101.788284]
objective value function right now is: -1799.5511423596874
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-101.51609]
objective value function right now is: -1799.7177310118757
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-101.01637]
objective value function right now is: -1799.8314755656254
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-101.7047]
objective value function right now is: -1799.8219005460194
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-102.09084]
objective value function right now is: -1799.902689208796
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-102.027435]
objective value function right now is: -1799.6588595198264
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-100.96991]
objective value function right now is: -1799.408638359732
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-102.16408]
objective value function right now is: -1799.0336960988964
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-100.67958]
objective value function right now is: -1799.8491962659034
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-101.050934]
objective value function right now is: -1799.8964425965753
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-100.37537]
objective value function right now is: -1799.757237773079
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-101.02804]
objective value function right now is: -1799.818005215853
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-101.850105]
objective value function right now is: -1799.7878507738262
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-100.60427]
objective value function right now is: -1799.877626290032
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-102.158035]
objective value function right now is: -1799.7455610110771
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-102.38561]
objective value function right now is: -1799.2286919371802
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-100.41199]
objective value function right now is: -1800.5973102459095
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1801.0013101267095
Current xi:  [-98.68036]
objective value function right now is: -1801.0013101267095
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1801.1398444645638
Current xi:  [-99.13715]
objective value function right now is: -1801.1398444645638
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-98.68347]
objective value function right now is: -1800.972597926433
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-98.769264]
objective value function right now is: -1801.1153228225237
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-98.38492]
objective value function right now is: -1801.0567086114336
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1801.2829867779283
Current xi:  [-98.57333]
objective value function right now is: -1801.2829867779283
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-98.81395]
objective value function right now is: -1801.143411210131
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-98.23779]
objective value function right now is: -1801.1510111147115
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-99.13093]
objective value function right now is: -1801.1297284402488
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-99.856384]
objective value function right now is: -1800.6065437374525
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-98.28359]
objective value function right now is: -1800.9668493126717
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-98.475174]
objective value function right now is: -1801.1960637638858
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-100.64171]
objective value function right now is: -1800.488672093654
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-98.77894]
objective value function right now is: -1801.051319178512
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-98.317]
objective value function right now is: -1800.9892908405639
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-99.51047]
objective value function right now is: -1801.1189196060704
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-99.68017]
objective value function right now is: -1800.98510176104
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-98.77687]
objective value function right now is: -1801.1372179862565
min fval:  -1801.2320876638237
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  8.4552,   2.6176],
        [  8.4782,   2.6389],
        [  4.6278,   2.9342],
        [  8.4492,   2.6207],
        [  0.1243, -21.3221],
        [  8.4348,   2.6124],
        [  8.4548,   2.6226],
        [  8.4637,   2.6294],
        [ -1.9408,  25.2482],
        [  6.4480, -25.4318]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 2.3897e+00,  3.7237e-01, -1.1699e+02,  2.4544e+00, -1.2700e+01,
          2.1337e+00,  2.0575e+00,  1.2990e+00, -8.0717e+01, -1.4342e+01],
        [ 2.3525e+00,  4.0816e+00,  2.8768e+00,  2.8404e+00,  1.7289e+01,
          2.3773e+00,  3.2524e+00,  3.6658e+00,  5.4994e+00,  1.6197e+01],
        [ 9.3377e+00,  7.7788e+00, -2.0653e+02,  8.5064e+00, -1.6421e+01,
          8.8143e+00,  8.5740e+00,  8.1653e+00, -1.7254e+01, -1.8213e+01],
        [ 2.4809e+00,  5.6558e-01, -9.4864e+01,  1.9892e+00, -1.3871e+01,
          2.5473e+00,  2.0898e+00,  1.5337e+00, -4.6479e+01, -1.5037e+01],
        [-2.7196e-01, -1.8856e+00, -4.8565e+01, -3.0637e-01, -7.5154e+00,
         -1.0577e+00, -4.5804e-01, -1.2636e+00, -1.0039e+02, -8.2734e+00],
        [-4.2240e-01, -1.2687e+00, -4.7691e+01, -4.6749e-01, -7.6451e+00,
         -4.1075e-01, -6.4231e-01, -9.4087e-01, -1.0013e+02, -9.0219e+00],
        [-9.3025e-02, -1.4368e+00, -4.9879e+01, -5.5193e-01, -7.5775e+00,
         -3.8027e-01, -7.9785e-01, -1.3996e+00, -9.7652e+01, -8.7534e+00],
        [ 1.1476e+01,  9.7791e+00, -3.5496e+02,  1.0896e+01, -3.0212e+01,
          1.1445e+01,  1.0848e+01,  1.0344e+01,  8.5882e+00, -2.6609e+01],
        [ 9.3976e-01, -9.7317e-01, -5.2661e+01,  5.3377e-01, -9.6415e+00,
          7.3470e-01, -1.3981e-01, -5.1337e-01, -5.6903e+01, -1.0883e+01],
        [-5.4396e-01, -1.8604e+00, -6.8167e+01, -6.3056e-01, -6.6928e+00,
         -3.2961e-01, -7.7395e-01, -1.3374e+00, -1.5730e+02, -8.1334e+00]],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -3.1926,  30.1618, -41.9101,  -3.5931, -10.8549,  -8.5663, -11.3997,
         -44.6977,  -5.8100, -19.4022]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-102.7791,   14.6872],
        [  14.4022,    6.2997],
        [  -0.8676,    3.2621],
        [-127.6032,  -30.3135],
        [-108.5510,   21.1351],
        [-133.4160,  -38.1606],
        [ -10.1470,   -1.3412],
        [ -79.1036,   17.5172],
        [   1.2734,    3.4716],
        [ -82.7361,  -18.8159]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-7.8086e+01, -5.9085e+01, -6.0019e+01, -1.2255e+02, -7.7723e+01,
          2.8209e-01, -1.7829e+01, -8.4652e+01, -2.1345e+01, -8.6928e+01],
        [-6.4578e+01, -2.6913e+01, -2.9631e+01,  7.0422e+00, -2.8523e+01,
          3.8814e-02, -1.4206e+00, -7.0963e+01, -1.0922e+01, -5.4445e+01],
        [-1.5994e+02, -2.9594e+00, -4.1498e+01, -5.4863e+01, -3.9925e+01,
          1.5952e+00, -3.6463e+01, -1.7132e+02, -4.6839e+01, -9.7090e+01],
        [-2.5283e+01, -5.7131e+00, -2.4415e+02,  1.0876e+02, -4.3544e+02,
          6.3601e+01, -2.5727e+02, -2.8558e+01, -1.3761e+02,  7.3170e+01],
        [-5.5133e+02, -2.8510e+01, -2.7781e+02, -6.2018e+02, -5.8027e+02,
         -7.1179e+02, -3.3854e+01, -5.5720e+02, -5.1639e+02, -6.3595e+02],
        [-3.8376e+02, -1.4405e+01, -1.1116e+02, -4.1578e+02, -2.8400e+02,
         -1.6554e+02,  2.1418e+01, -3.9127e+02, -2.0764e+02, -3.8142e+02],
        [-1.7110e+00, -4.1391e+01, -1.2841e+02,  2.7119e+01, -3.1250e+02,
         -1.8138e+01, -9.9703e+01, -1.4441e+00, -7.7264e+01, -6.8773e+01],
        [-4.9236e+01, -2.8669e+01, -4.2781e+01,  2.9432e+00, -4.4899e+01,
         -1.4291e+00, -2.0578e+01, -5.3586e+01, -9.8397e+00, -3.1348e+02],
        [ 2.4991e+02,  2.8434e+02,  1.4397e+02,  2.4669e+02,  8.8634e+01,
          2.6330e+02,  2.4802e+01,  2.5043e+02,  4.1378e+01,  2.4633e+02],
        [-1.0193e+02, -5.5511e+01,  7.3736e+00, -3.7488e+01,  2.1061e+01,
         -3.6987e+00,  4.5374e+00, -1.1786e+02, -5.4055e+01, -7.2141e+01]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -4.6324, -10.2182,  -5.1063,  12.8440,  35.0585,  -5.4331,  -0.9924,
          -9.9086,   0.6664,  -7.3411],
        [  4.4674,  10.0886,   5.0803, -12.4020, -34.8247,   5.5604,   0.8104,
           9.7717,  -0.5177,   7.0753]], device='cuda:0'))])
xi:  [-100.64171]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: -264.6389293132221
W_T_median: -407.39508331637967
W_T_pctile_5: -987.0462254798847
W_T_CVAR_5_pct: -1174.3414107185986
Average q (qsum/M+1):  60.00000393775202
Optimal xi:  [-100.64171]
Expected(across Rb) median(across samples) p_equity:  0.16483870670304632
obj fun:  tensor(-1801.2321, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
