Starting at: 
04-12-22_10:31

 Random seed:  2  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1  logistic_sigmoid   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  77586.33092281029
Current xi:  [13.163228]
objective value function right now is: 77586.33092281029
4.0% of gradient descent iterations done. Method = Adam
new min fval:  34902.19334208724
Current xi:  [13.612661]
objective value function right now is: 34902.19334208724
6.0% of gradient descent iterations done. Method = Adam
new min fval:  1390.6745163472583
Current xi:  [13.643949]
objective value function right now is: 1390.6745163472583
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.951134]
objective value function right now is: 42648.83494014112
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -20612.190310423455
Current xi:  [13.894841]
objective value function right now is: -20612.190310423455
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.772364]
objective value function right now is: -18707.643558320546
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [13.43093]
objective value function right now is: 6860.062196073122
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -23138.00671310812
Current xi:  [13.997656]
objective value function right now is: -23138.00671310812
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.444582]
objective value function right now is: -22738.297123863194
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.349441]
objective value function right now is: -7909.776676300803
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -29937.196124075912
Current xi:  [14.1726465]
objective value function right now is: -29937.196124075912
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.6758375]
objective value function right now is: -8697.160885373927
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.800818]
objective value function right now is: -14511.187531546571
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [13.85007]
objective value function right now is: -18862.745125505782
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -36969.277864804404
Current xi:  [14.192842]
objective value function right now is: -36969.277864804404
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.30431]
objective value function right now is: 34727.21133855149
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.922956]
objective value function right now is: -19085.12163824838
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -39718.733147271945
Current xi:  [13.772092]
objective value function right now is: -39718.733147271945
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.481547]
objective value function right now is: -13219.921281138288
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.593503]
objective value function right now is: 76247.77200421054
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.217088]
objective value function right now is: -19094.901025496343
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.590275]
objective value function right now is: -931.143662037231
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.3848715]
objective value function right now is: -7519.337916005891
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.080612]
objective value function right now is: 6164.968592682847
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.347183]
objective value function right now is: 6376.487937561628
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.968657]
objective value function right now is: -28605.148066062553
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.522528]
objective value function right now is: -6052.865431689242
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [13.6599455]
objective value function right now is: 1910.1013614152223
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [14.042043]
objective value function right now is: -3468.8812577241274
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.031465]
objective value function right now is: 22309.718678462392
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.622685]
objective value function right now is: -36807.99570189738
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.026347]
objective value function right now is: -29909.913714817383
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.265616]
objective value function right now is: -21771.963888841976
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.773695]
objective value function right now is: 19211.26823492346
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.336586]
objective value function right now is: -13159.452453790727
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.457067]
objective value function right now is: -37120.90754184633
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.561938]
objective value function right now is: -17107.256503441015
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.026092]
objective value function right now is: -36775.85116262393
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.175806]
objective value function right now is: -18363.874026883856
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -40703.9120461901
Current xi:  [14.636794]
objective value function right now is: -40703.9120461901
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.764334]
objective value function right now is: 66397.47181438263
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.93596]
objective value function right now is: -33431.932424012164
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.2324095]
objective value function right now is: 87330.42830595952
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.003837]
objective value function right now is: -34026.8612370589
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.40907]
objective value function right now is: -33233.5528315635
new min fval from sgd:  -41257.06898572138
new min fval from sgd:  -43891.53890446101
new min fval from sgd:  -45286.9859698605
new min fval from sgd:  -45665.326692547285
new min fval from sgd:  -46512.81894810734
new min fval from sgd:  -47674.540507072954
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.085144]
objective value function right now is: -32891.45692120413
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.643794]
objective value function right now is: -41708.48353331143
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.875868]
objective value function right now is: -41642.7765369173
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.575606]
objective value function right now is: -18053.745081047924
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.81922]
objective value function right now is: -39847.72645376583
min fval:  -47674.540507072954
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.9682,  1.3081],
        [ 1.6718, -1.4647],
        [ 1.7643, -0.6943],
        [ 1.6903, -0.9931],
        [ 1.6771, -1.2105],
        [ 0.6061, -1.1958],
        [ 1.4004, -0.6313],
        [ 0.7991, -1.7876],
        [ 1.5716, -0.9714],
        [ 1.3263, -1.8036]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[0.6905, 1.0937, 1.1063, 1.0350, 1.0743, 0.8729, 1.0388, 1.0903, 1.4194,
         1.1123],
        [0.8053, 1.2367, 1.4744, 1.3764, 0.9419, 0.8681, 1.1776, 0.9896, 1.3853,
         0.8897],
        [0.8780, 0.8347, 1.0176, 1.2761, 1.1399, 1.2498, 0.7890, 1.1890, 1.1154,
         1.2454],
        [0.8755, 1.2700, 1.1237, 0.9648, 1.2718, 1.0774, 1.2349, 1.2981, 1.1792,
         1.0142],
        [0.0254, 1.6004, 1.0943, 1.2109, 0.3339, 0.7902, 1.1091, 0.5149, 0.8857,
         0.5122],
        [0.9900, 1.0081, 1.3964, 1.3591, 1.1591, 1.0511, 1.0248, 1.0449, 1.0548,
         1.0320],
        [0.0162, 1.3911, 1.2638, 1.4028, 0.4860, 1.0017, 0.8874, 1.0351, 0.9763,
         0.7321],
        [0.7344, 1.1343, 1.3993, 1.2204, 1.0186, 1.1419, 1.2626, 1.1932, 1.0829,
         1.3297],
        [1.2059, 1.1240, 0.8035, 0.8916, 1.3028, 1.2072, 1.0610, 1.3774, 1.2499,
         1.3554],
        [0.2265, 1.5555, 1.3250, 1.0195, 0.5897, 1.0682, 0.9886, 0.8967, 1.1120,
         0.8583]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.7158, -5.0270, -4.3581, -4.0299, -3.4761, -4.5207, -4.4005, -4.2790,
         -3.9730, -4.6477]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-0.9929,  4.5867],
        [-7.5868, -1.0867],
        [ 8.7913,  3.5716],
        [-1.3522,  4.4125],
        [-1.4382,  4.3850],
        [-1.1774,  4.4889],
        [-3.0582,  3.8025],
        [-7.6157, -1.0868],
        [-1.3532,  4.4190],
        [-1.1123,  4.5221]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-70.4915, -15.3813, -15.7869, -76.9580, -77.5042, -74.8216, -82.7869,
         -15.5046, -75.5780, -72.9184],
        [-81.1687, -13.7141, -14.5581, -87.6891, -88.7604, -85.0691, -94.6937,
         -13.5853, -86.8115, -83.6016],
        [ 19.1763,  -3.4598,  -1.4424,  32.4872,  32.6982,  26.5089,  50.1776,
          -2.6110,  30.3008,  23.8540],
        [-72.1380, -15.3517, -15.5318, -78.0975, -78.2880, -75.7253, -84.1532,
         -15.3204, -76.9798, -74.0517],
        [-52.2575, -20.3008, -22.7095, -54.6300, -53.9744, -55.1950, -58.2621,
         -20.5528, -53.1191, -52.9589],
        [-49.0486, -21.1917, -22.9331, -52.5636, -51.0128, -52.5729, -54.8328,
         -20.9418, -50.1507, -49.9807],
        [-63.4761,   5.5420,  -4.6009, -57.6175, -51.8930, -61.3290, -19.8202,
           5.6018, -53.9003, -62.0493],
        [-81.2589, -13.7579, -14.5111, -87.4053, -88.7237, -85.0752, -94.4546,
         -13.6626, -86.9956, -83.2619],
        [-15.1737, -30.2767,  -3.2215, -22.9240, -22.4697, -19.7349, -27.6806,
         -31.0912, -21.4044, -18.1748],
        [-79.1125, -14.6337, -14.3695, -85.5944, -86.1618, -82.2703, -92.1782,
         -14.2250, -84.6879, -80.8874]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  3.7595,   5.7849,   0.5518,   4.2317,   1.0766,   0.8227, -13.4256,
           5.8316,  13.5162,   5.3113],
        [ -4.1394,  -6.1735,  -0.6046,  -3.9874,  -0.9768,  -0.8921,  13.0950,
          -6.0987, -13.6892,  -5.4081]], device='cuda:0'))])
xi:  [14.029557]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1215.7593180660099
W_T_median: 984.247924870807
W_T_pctile_5: 200.57942275075035
W_T_CVAR_5_pct: 9.385193697828958
Average q (qsum/M+1):  35.0
Optimal xi:  [14.029557]
Expected(across Rb) median(across samples) p_equity:  0.23933562636375427
obj fun:  tensor(-47674.5405, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5000.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1399.2590386161871
Current xi:  [14.25269]
objective value function right now is: -1399.2590386161871
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1484.3001320172646
Current xi:  [14.220507]
objective value function right now is: -1484.3001320172646
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.762091]
objective value function right now is: -1186.8778953079272
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1504.1603252701452
Current xi:  [14.1618805]
objective value function right now is: -1504.1603252701452
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.794666]
objective value function right now is: -1320.1179741422752
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.016647]
objective value function right now is: -1466.0115826641224
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [14.176518]
objective value function right now is: -1230.4015540502207
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1520.6785035628727
Current xi:  [14.11539]
objective value function right now is: -1520.6785035628727
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.477929]
objective value function right now is: -971.4863143770609
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.962371]
objective value function right now is: -1464.9412982853553
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.265809]
objective value function right now is: -1252.4467682633424
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.835822]
objective value function right now is: -1367.2782084819175
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.679426]
objective value function right now is: -1455.4146926726253
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [13.397248]
objective value function right now is: -880.0874011610412
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.477346]
objective value function right now is: -1335.2813482920244
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.167712]
objective value function right now is: -1337.0955902788612
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.274699]
objective value function right now is: -1358.0403487731055
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.717963]
objective value function right now is: -1066.8742337602714
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.770659]
objective value function right now is: -1462.27828658586
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.948229]
objective value function right now is: -1013.8351096947339
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.879639]
objective value function right now is: -1269.2445535246095
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.273027]
objective value function right now is: -1387.0293048007957
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.843974]
objective value function right now is: -1366.660523140791
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.042636]
objective value function right now is: -1478.5609027762787
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.903031]
objective value function right now is: -1371.528162329041
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.61695]
objective value function right now is: -1448.5198450257112
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.815066]
objective value function right now is: -1371.3959338569264
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [13.913997]
objective value function right now is: -1499.05704994743
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [14.09181]
objective value function right now is: -1304.0284371352816
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.107659]
objective value function right now is: -871.3198969973936
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.597796]
objective value function right now is: -1229.6147888526787
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.400879]
objective value function right now is: -1177.2555398110176
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.023077]
objective value function right now is: -1439.2904423248283
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.659572]
objective value function right now is: -1336.2486158002982
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.118807]
objective value function right now is: -1410.3157463773694
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.466033]
objective value function right now is: -1267.3131037960823
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.60184]
objective value function right now is: -1377.3457690108653
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.979553]
objective value function right now is: -1506.7536424775712
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.346045]
objective value function right now is: -1474.3031590849594
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.488371]
objective value function right now is: -1487.8576056430882
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.950444]
objective value function right now is: -1357.0715834181588
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.587314]
objective value function right now is: -1466.9578080332903
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.049815]
objective value function right now is: -1383.0684583969703
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.949294]
objective value function right now is: -1497.0694670166554
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.1572895]
objective value function right now is: -1451.652772346005
new min fval from sgd:  -1523.0164656611182
new min fval from sgd:  -1540.5734151174129
new min fval from sgd:  -1563.0828633109074
new min fval from sgd:  -1563.9314917197871
new min fval from sgd:  -1574.5177904738723
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.672834]
objective value function right now is: -1467.2004275667225
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.596151]
objective value function right now is: -1497.4381056160007
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.079274]
objective value function right now is: -1441.5645803124853
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.301487]
objective value function right now is: -1453.700297013103
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.317364]
objective value function right now is: -1474.989035487735
min fval:  -1574.5177904738723
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.9682,  1.3079],
        [ 1.6718, -1.4657],
        [ 1.7623, -0.7246],
        [ 1.6905, -1.0005],
        [ 1.6772, -1.2128],
        [ 0.6061, -1.1981],
        [ 1.3984, -0.6626],
        [ 0.7991, -1.7876],
        [ 1.5718, -0.9791],
        [ 1.3263, -1.8036]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.6732,  1.0937,  1.1063,  1.0351,  1.0743,  0.8729,  1.0388,  1.0903,
          1.4194,  1.1123],
        [ 0.7878,  1.2367,  1.4744,  1.3764,  0.9419,  0.8681,  1.1776,  0.9896,
          1.3853,  0.8897],
        [ 0.8631,  0.8347,  1.0176,  1.2761,  1.1399,  1.2498,  0.7884,  1.1890,
          1.1155,  1.2454],
        [ 0.8618,  1.2700,  1.1237,  0.9648,  1.2718,  1.0774,  1.2349,  1.2981,
          1.1793,  1.0142],
        [ 0.0106,  1.6004,  1.0943,  1.2109,  0.3339,  0.7902,  1.1091,  0.5149,
          0.8857,  0.5122],
        [ 0.9755,  1.0081,  1.3965,  1.3592,  1.1591,  1.0511,  1.0248,  1.0449,
          1.0548,  1.0320],
        [-0.0026,  1.3911,  1.2638,  1.4028,  0.4860,  1.0017,  0.8865,  1.0351,
          0.9763,  0.7321],
        [ 0.7191,  1.1343,  1.3993,  1.2204,  1.0186,  1.1419,  1.2626,  1.1932,
          1.0829,  1.3297],
        [ 1.1945,  1.1240,  0.8034,  0.8916,  1.3028,  1.2072,  1.0611,  1.3774,
          1.2500,  1.3554],
        [ 0.2071,  1.5555,  1.3250,  1.0195,  0.5897,  1.0682,  0.9876,  0.8967,
          1.1120,  0.8583]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.7195, -5.0303, -4.3613, -4.0330, -3.4828, -4.5234, -4.4068, -4.2825,
         -3.9753, -4.6530]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-0.1156,  5.3791],
        [-7.7474, -1.0843],
        [11.5163,  4.7412],
        [-1.2209,  5.0378],
        [-1.3546,  4.9928],
        [-0.8157,  5.1691],
        [-2.9469,  4.0247],
        [-9.3212, -0.9066],
        [-1.2021,  5.0472],
        [-0.6052,  5.2338]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -87.8959,  -40.0894,  -28.9812,  -91.6121,  -91.6547,  -90.7464,
          -93.3667,  -37.7537,  -90.2649,  -89.3944],
        [-125.3243,  -36.8981,  -30.8127, -129.7693, -130.5482, -128.1021,
         -132.4566,  -33.4251, -128.9541, -127.0905],
        [   7.5903,  -12.3191,   -0.5880,   38.4078,   42.4786,   23.8147,
          114.8843,    8.5250,   36.0422,   17.8601],
        [ -90.0871,  -39.0200,  -28.6444,  -93.7101,  -93.5831,  -92.3520,
          -96.2264,  -35.8855,  -92.6244,  -91.1807],
        [ -56.2181,  -31.6874,  -26.4117,  -52.2496,  -50.7936,  -55.1736,
          -47.7781,  -27.7942,  -50.8183,  -54.1720],
        [ -53.4257,  -31.8169,  -26.4260,  -50.7570,  -48.4360,  -53.0566,
          -45.2792,  -27.4820,  -48.4233,  -51.6698],
        [-102.9080,    5.8754,   -3.8124,  -92.3820,  -85.7545,  -98.2737,
          -23.9509,    6.5385,  -88.8234,  -99.8577],
        [-126.4918,  -37.0124,  -30.9730, -130.7592, -131.8209, -129.2973,
         -133.8436,  -33.6784, -130.4115, -127.9029],
        [ -18.6275,  -57.3768,   -3.6168,  -35.6506,  -36.6581,  -28.4270,
          -64.1768,  -84.6006,  -33.8745,  -25.0041],
        [-121.9948,  -38.5953,  -30.8150, -126.1390, -126.3657, -123.8633,
         -127.6733,  -35.0403, -125.2892, -122.9805]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -6.4855,  -2.6851,   0.5404,  -4.5720,  -8.2956,  -8.6264,  -9.6926,
          -2.5750,  15.1871,  -2.4287],
        [  6.1057,   2.2967,  -0.5931,   4.8164,   8.3959,   8.5574,   9.3624,
           2.3082, -15.3601,   2.3321]], device='cuda:0'))])
xi:  [14.284318]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1231.086361136112
W_T_median: 995.5505363836688
W_T_pctile_5: 203.76512055866402
W_T_CVAR_5_pct: 9.79065288179374
Average q (qsum/M+1):  35.0
Optimal xi:  [14.284318]
Expected(across Rb) median(across samples) p_equity:  0.24351787567138672
obj fun:  tensor(-1574.5178, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.9682,  1.3079],
        [ 1.6718, -1.4657],
        [ 1.7623, -0.7246],
        [ 1.6905, -1.0005],
        [ 1.6772, -1.2128],
        [ 0.6061, -1.1981],
        [ 1.3984, -0.6626],
        [ 0.7991, -1.7876],
        [ 1.5718, -0.9791],
        [ 1.3263, -1.8036]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.6732,  1.0937,  1.1063,  1.0351,  1.0743,  0.8729,  1.0388,  1.0903,
          1.4194,  1.1123],
        [ 0.7878,  1.2367,  1.4744,  1.3764,  0.9419,  0.8681,  1.1776,  0.9896,
          1.3853,  0.8897],
        [ 0.8631,  0.8347,  1.0176,  1.2761,  1.1399,  1.2498,  0.7884,  1.1890,
          1.1155,  1.2454],
        [ 0.8618,  1.2700,  1.1237,  0.9648,  1.2718,  1.0774,  1.2349,  1.2981,
          1.1793,  1.0142],
        [ 0.0106,  1.6004,  1.0943,  1.2109,  0.3339,  0.7902,  1.1091,  0.5149,
          0.8857,  0.5122],
        [ 0.9755,  1.0081,  1.3965,  1.3592,  1.1591,  1.0511,  1.0248,  1.0449,
          1.0548,  1.0320],
        [-0.0026,  1.3911,  1.2638,  1.4028,  0.4860,  1.0017,  0.8865,  1.0351,
          0.9763,  0.7321],
        [ 0.7191,  1.1343,  1.3993,  1.2204,  1.0186,  1.1419,  1.2626,  1.1932,
          1.0829,  1.3297],
        [ 1.1945,  1.1240,  0.8034,  0.8916,  1.3028,  1.2072,  1.0611,  1.3774,
          1.2500,  1.3554],
        [ 0.2071,  1.5555,  1.3250,  1.0195,  0.5897,  1.0682,  0.9876,  0.8967,
          1.1120,  0.8583]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.7195, -5.0303, -4.3613, -4.0330, -3.4828, -4.5234, -4.4068, -4.2825,
         -3.9753, -4.6530]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-0.1156,  5.3791],
        [-7.7474, -1.0843],
        [11.5163,  4.7412],
        [-1.2209,  5.0378],
        [-1.3546,  4.9928],
        [-0.8157,  5.1691],
        [-2.9469,  4.0247],
        [-9.3212, -0.9066],
        [-1.2021,  5.0472],
        [-0.6052,  5.2338]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -87.8959,  -40.0894,  -28.9812,  -91.6121,  -91.6547,  -90.7464,
          -93.3667,  -37.7537,  -90.2649,  -89.3944],
        [-125.3243,  -36.8981,  -30.8127, -129.7693, -130.5482, -128.1021,
         -132.4566,  -33.4251, -128.9541, -127.0905],
        [   7.5903,  -12.3191,   -0.5880,   38.4078,   42.4786,   23.8147,
          114.8843,    8.5250,   36.0422,   17.8601],
        [ -90.0871,  -39.0200,  -28.6444,  -93.7101,  -93.5831,  -92.3520,
          -96.2264,  -35.8855,  -92.6244,  -91.1807],
        [ -56.2181,  -31.6874,  -26.4117,  -52.2496,  -50.7936,  -55.1736,
          -47.7781,  -27.7942,  -50.8183,  -54.1720],
        [ -53.4257,  -31.8169,  -26.4260,  -50.7570,  -48.4360,  -53.0566,
          -45.2792,  -27.4820,  -48.4233,  -51.6698],
        [-102.9080,    5.8754,   -3.8124,  -92.3820,  -85.7545,  -98.2737,
          -23.9509,    6.5385,  -88.8234,  -99.8577],
        [-126.4918,  -37.0124,  -30.9730, -130.7592, -131.8209, -129.2973,
         -133.8436,  -33.6784, -130.4115, -127.9029],
        [ -18.6275,  -57.3768,   -3.6168,  -35.6506,  -36.6581,  -28.4270,
          -64.1768,  -84.6006,  -33.8745,  -25.0041],
        [-121.9948,  -38.5953,  -30.8150, -126.1390, -126.3657, -123.8633,
         -127.6733,  -35.0403, -125.2892, -122.9805]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -6.4855,  -2.6851,   0.5404,  -4.5720,  -8.2956,  -8.6264,  -9.6926,
          -2.5750,  15.1871,  -2.4287],
        [  6.1057,   2.2967,  -0.5931,   4.8164,   8.3959,   8.5574,   9.3624,
           2.3082, -15.3601,   2.3321]], device='cuda:0'))])
loaded xi:  14.284318
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1106.5313539791382
Current xi:  [14.3155985]
objective value function right now is: -1106.5313539791382
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.351544]
objective value function right now is: -1094.6804214780686
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1126.62500474431
Current xi:  [14.41736]
objective value function right now is: -1126.62500474431
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.193545]
objective value function right now is: -1067.2084173265089
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.956415]
objective value function right now is: -1107.1795676146805
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1127.5807220068941
Current xi:  [14.558846]
objective value function right now is: -1127.5807220068941
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [13.7986145]
objective value function right now is: -1102.163263446461
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.377779]
objective value function right now is: -1125.6046473340332
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.90673]
objective value function right now is: -1118.0360862240123
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.0212345]
objective value function right now is: -1082.7740013527703
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.472254]
objective value function right now is: -1120.715889969343
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1128.0636572956346
Current xi:  [14.128217]
objective value function right now is: -1128.0636572956346
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.920767]
objective value function right now is: -1118.0093384968657
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [14.886356]
objective value function right now is: -1119.512455930601
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.446544]
objective value function right now is: -1118.3838890016339
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.999397]
objective value function right now is: -1118.4109650986816
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.827596]
objective value function right now is: -1114.4572032425438
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.93224]
objective value function right now is: -1116.5341797125363
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.906783]
objective value function right now is: -1106.2429992123102
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.69792]
objective value function right now is: -1041.0566595204336
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.057501]
objective value function right now is: -1122.687902143669
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.196339]
objective value function right now is: -1111.4954616808564
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.0315695]
objective value function right now is: -1123.2643488829165
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.412197]
objective value function right now is: -1125.7024952410814
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.128316]
objective value function right now is: -1104.1194428098709
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.858186]
objective value function right now is: -1078.266693293594
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.259067]
objective value function right now is: -1124.5661750984964
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [13.857981]
objective value function right now is: -1078.8672101026525
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [13.769983]
objective value function right now is: -1113.5122795367095
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.794689]
objective value function right now is: -1123.3454603427606
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.385004]
objective value function right now is: -1110.021774198988
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.494622]
objective value function right now is: -1102.1646885521823
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.308643]
objective value function right now is: -1111.7508189090202
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.281929]
objective value function right now is: -1127.1610429683542
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.305668]
objective value function right now is: -1106.2882678244273
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.185878]
objective value function right now is: -1117.9047485950628
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.366688]
objective value function right now is: -1124.0327804799897
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.306804]
objective value function right now is: -1078.7259013122255
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.441582]
objective value function right now is: -1095.3472833338362
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.199185]
objective value function right now is: -1119.7604484310011
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.089142]
objective value function right now is: -1127.9073582718008
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.8463125]
objective value function right now is: -1090.8402484946487
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1128.4704537200873
Current xi:  [14.167192]
objective value function right now is: -1128.4704537200873
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.556053]
objective value function right now is: -1119.4841581458822
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.031204]
objective value function right now is: -1097.090432054929
new min fval from sgd:  -1131.167707375167
new min fval from sgd:  -1132.2029445929509
new min fval from sgd:  -1132.4782827225406
new min fval from sgd:  -1132.5377355491626
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.179214]
objective value function right now is: -1123.0296216260106
new min fval from sgd:  -1133.08472971287
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.110107]
objective value function right now is: -1126.0566038417987
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.776185]
objective value function right now is: -1122.9408856240632
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.300096]
objective value function right now is: -1083.6803678323683
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.98481]
objective value function right now is: -1099.6509757455062
min fval:  -1133.08472971287
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.9682,  1.3064],
        [ 1.6718, -1.4679],
        [ 1.7588, -0.7669],
        [ 1.6898, -1.0130],
        [ 1.6772, -1.2165],
        [ 0.6061, -1.2014],
        [ 1.3951, -0.7049],
        [ 0.7991, -1.7876],
        [ 1.5710, -0.9917],
        [ 1.3263, -1.8036]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.6484,  1.0937,  1.1060,  1.0351,  1.0743,  0.8729,  1.0381,  1.0903,
          1.4194,  1.1123],
        [ 0.7629,  1.2367,  1.4741,  1.3764,  0.9419,  0.8681,  1.1769,  0.9896,
          1.3853,  0.8897],
        [ 0.8418,  0.8347,  1.0175,  1.2761,  1.1399,  1.2498,  0.7874,  1.1890,
          1.1155,  1.2454],
        [ 0.8423,  1.2700,  1.1237,  0.9648,  1.2718,  1.0774,  1.2347,  1.2981,
          1.1793,  1.0142],
        [-0.0105,  1.6004,  1.0941,  1.2109,  0.3339,  0.7902,  1.1088,  0.5149,
          0.8857,  0.5122],
        [ 0.9548,  1.0081,  1.3964,  1.3592,  1.1591,  1.0511,  1.0245,  1.0449,
          1.0548,  1.0320],
        [-0.0293,  1.3911,  1.2632,  1.4028,  0.4860,  1.0017,  0.8853,  1.0351,
          0.9763,  0.7321],
        [ 0.6972,  1.1343,  1.3991,  1.2204,  1.0186,  1.1419,  1.2623,  1.1932,
          1.0829,  1.3297],
        [ 1.1781,  1.1240,  0.8029,  0.8916,  1.3028,  1.2072,  1.0610,  1.3774,
          1.2500,  1.3554],
        [ 0.1796,  1.5555,  1.3244,  1.0195,  0.5897,  1.0682,  0.9865,  0.8967,
          1.1120,  0.8583]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.7028, -5.0130, -4.3437, -4.0154, -3.4699, -4.5052, -4.3941, -4.2655,
         -3.9562, -4.6390]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -0.0729,   5.5746],
        [ -8.5190,  -1.4473],
        [ 14.0878,   6.0075],
        [ -1.3847,   5.0609],
        [ -1.5442,   5.0093],
        [ -0.9385,   5.2167],
        [ -3.4611,   4.4528],
        [-11.1331,  -1.2309],
        [ -1.3719,   5.0677],
        [ -0.7148,   5.3027]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -84.5457,  -49.7253,  -33.7997,  -85.6649,  -85.5814,  -85.5992,
          -87.8414,  -51.0196,  -84.3102,  -84.8994],
        [-124.1659,  -48.2707,  -37.5384, -126.9173, -127.6438, -125.8066,
         -130.5367,  -48.7872, -126.0935, -125.2629],
        [   4.8252,  -18.7205,   -0.8906,   56.0650,   62.9261,   33.3337,
          150.7206,   16.5538,   53.4930,   23.3847],
        [ -87.2299,  -49.6255,  -34.4945,  -88.4385,  -88.1908,  -87.8436,
          -91.3611,  -49.9833,  -87.3458,  -87.2856],
        [ -46.0892,  -43.1811,  -33.3887,  -37.1688,  -35.4229,  -41.4785,
          -32.3808,  -40.9014,  -35.7393,  -41.5341],
        [ -43.2793,  -43.2381,  -33.3379,  -35.5775,  -32.9615,  -39.2837,
          -29.7546,  -40.5209,  -33.2467,  -38.9682],
        [-133.2940,    6.1801,   -4.1564, -114.1189, -106.4678, -122.9875,
          -32.9685,    7.8670, -110.7085, -126.1039],
        [-125.4008,  -48.2569,  -37.5752, -127.9859, -128.9984, -127.0746,
         -132.0278,  -48.9740, -127.6279, -126.1466],
        [ -20.2165,  -68.4173,   -3.6315,  -52.6881,  -55.3241,  -40.4001,
         -105.2749, -166.9168,  -50.6470,  -34.1396],
        [-121.8667,  -49.8356,  -37.3890, -124.4758, -124.6603, -122.7183,
         -126.9081,  -50.5431, -123.6175, -122.2786]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -7.2910,  -4.3853,   0.5649,  -5.3078,  -7.4077,  -7.7362,  -8.6137,
          -4.3469,  13.2670,  -4.2784],
        [  6.9114,   3.9972,  -0.6177,   5.5525,   7.5082,   7.6675,   8.2859,
           4.0804, -13.4401,   4.1821]], device='cuda:0'))])
xi:  [14.288631]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1201.9022178973858
W_T_median: 974.6583272490764
W_T_pctile_5: 203.45228664345447
W_T_CVAR_5_pct: 9.619960046727734
Average q (qsum/M+1):  35.0
Optimal xi:  [14.288631]
Expected(across Rb) median(across samples) p_equity:  0.23459340631961823
obj fun:  tensor(-1133.0847, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.9682,  1.3064],
        [ 1.6718, -1.4679],
        [ 1.7588, -0.7669],
        [ 1.6898, -1.0130],
        [ 1.6772, -1.2165],
        [ 0.6061, -1.2014],
        [ 1.3951, -0.7049],
        [ 0.7991, -1.7876],
        [ 1.5710, -0.9917],
        [ 1.3263, -1.8036]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.6484,  1.0937,  1.1060,  1.0351,  1.0743,  0.8729,  1.0381,  1.0903,
          1.4194,  1.1123],
        [ 0.7629,  1.2367,  1.4741,  1.3764,  0.9419,  0.8681,  1.1769,  0.9896,
          1.3853,  0.8897],
        [ 0.8418,  0.8347,  1.0175,  1.2761,  1.1399,  1.2498,  0.7874,  1.1890,
          1.1155,  1.2454],
        [ 0.8423,  1.2700,  1.1237,  0.9648,  1.2718,  1.0774,  1.2347,  1.2981,
          1.1793,  1.0142],
        [-0.0105,  1.6004,  1.0941,  1.2109,  0.3339,  0.7902,  1.1088,  0.5149,
          0.8857,  0.5122],
        [ 0.9548,  1.0081,  1.3964,  1.3592,  1.1591,  1.0511,  1.0245,  1.0449,
          1.0548,  1.0320],
        [-0.0293,  1.3911,  1.2632,  1.4028,  0.4860,  1.0017,  0.8853,  1.0351,
          0.9763,  0.7321],
        [ 0.6972,  1.1343,  1.3991,  1.2204,  1.0186,  1.1419,  1.2623,  1.1932,
          1.0829,  1.3297],
        [ 1.1781,  1.1240,  0.8029,  0.8916,  1.3028,  1.2072,  1.0610,  1.3774,
          1.2500,  1.3554],
        [ 0.1796,  1.5555,  1.3244,  1.0195,  0.5897,  1.0682,  0.9865,  0.8967,
          1.1120,  0.8583]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.7028, -5.0130, -4.3437, -4.0154, -3.4699, -4.5052, -4.3941, -4.2655,
         -3.9562, -4.6390]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -0.0729,   5.5746],
        [ -8.5190,  -1.4473],
        [ 14.0878,   6.0075],
        [ -1.3847,   5.0609],
        [ -1.5442,   5.0093],
        [ -0.9385,   5.2167],
        [ -3.4611,   4.4528],
        [-11.1331,  -1.2309],
        [ -1.3719,   5.0677],
        [ -0.7148,   5.3027]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -84.5457,  -49.7253,  -33.7997,  -85.6649,  -85.5814,  -85.5992,
          -87.8414,  -51.0196,  -84.3102,  -84.8994],
        [-124.1659,  -48.2707,  -37.5384, -126.9173, -127.6438, -125.8066,
         -130.5367,  -48.7872, -126.0935, -125.2629],
        [   4.8252,  -18.7205,   -0.8906,   56.0650,   62.9261,   33.3337,
          150.7206,   16.5538,   53.4930,   23.3847],
        [ -87.2299,  -49.6255,  -34.4945,  -88.4385,  -88.1908,  -87.8436,
          -91.3611,  -49.9833,  -87.3458,  -87.2856],
        [ -46.0892,  -43.1811,  -33.3887,  -37.1688,  -35.4229,  -41.4785,
          -32.3808,  -40.9014,  -35.7393,  -41.5341],
        [ -43.2793,  -43.2381,  -33.3379,  -35.5775,  -32.9615,  -39.2837,
          -29.7546,  -40.5209,  -33.2467,  -38.9682],
        [-133.2940,    6.1801,   -4.1564, -114.1189, -106.4678, -122.9875,
          -32.9685,    7.8670, -110.7085, -126.1039],
        [-125.4008,  -48.2569,  -37.5752, -127.9859, -128.9984, -127.0746,
         -132.0278,  -48.9740, -127.6279, -126.1466],
        [ -20.2165,  -68.4173,   -3.6315,  -52.6881,  -55.3241,  -40.4001,
         -105.2749, -166.9168,  -50.6470,  -34.1396],
        [-121.8667,  -49.8356,  -37.3890, -124.4758, -124.6603, -122.7183,
         -126.9081,  -50.5431, -123.6175, -122.2786]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -7.2910,  -4.3853,   0.5649,  -5.3078,  -7.4077,  -7.7362,  -8.6137,
          -4.3469,  13.2670,  -4.2784],
        [  6.9114,   3.9972,  -0.6177,   5.5525,   7.5082,   7.6675,   8.2859,
           4.0804, -13.4401,   4.1821]], device='cuda:0'))])
loaded xi:  14.288631
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1104.4294403560493
Current xi:  [14.744923]
objective value function right now is: -1104.4294403560493
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.4661665]
objective value function right now is: -1095.811164088603
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.909753]
objective value function right now is: -1099.6609553903845
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.886282]
objective value function right now is: -1074.9909832704116
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1105.543258186263
Current xi:  [14.184948]
objective value function right now is: -1105.543258186263
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.291888]
objective value function right now is: -1104.693527741028
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1110.1739325546469
Current xi:  [14.2313175]
objective value function right now is: -1110.1739325546469
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.644852]
objective value function right now is: -1079.37564646403
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.499945]
objective value function right now is: -1098.7727614022097
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.143573]
objective value function right now is: -843.3148745466941
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.557363]
objective value function right now is: -1092.8551181943
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.098589]
objective value function right now is: -1103.4759596941537
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.282882]
objective value function right now is: -1107.155815816543
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [13.614388]
objective value function right now is: -1107.6747422035835
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.686705]
objective value function right now is: -1070.094362629347
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.977484]
objective value function right now is: -1106.2309578400086
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1110.3787077881818
Current xi:  [14.390318]
objective value function right now is: -1110.3787077881818
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.693621]
objective value function right now is: -1107.1970223983626
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.453027]
objective value function right now is: -1108.7535762808993
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.548496]
objective value function right now is: -1090.2342045598448
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.733874]
objective value function right now is: -1069.1884414663157
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.540786]
objective value function right now is: -1101.3884279293018
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.287114]
objective value function right now is: -1096.756969633246
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.000112]
objective value function right now is: -1101.4962201727656
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.998372]
objective value function right now is: -1086.7440124609323
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.157938]
objective value function right now is: -1103.6210868407338
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.129676]
objective value function right now is: -1108.4440548042994
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [14.161232]
objective value function right now is: -1099.9988654358333
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [14.477647]
objective value function right now is: -1105.539956380487
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.98705]
objective value function right now is: -1108.8750344830225
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.074856]
objective value function right now is: -1096.920371480056
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.254627]
objective value function right now is: -1079.8765751801538
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.038598]
objective value function right now is: -1109.9596406020173
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.479851]
objective value function right now is: -1098.8188380434938
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.287381]
objective value function right now is: -1103.2751310452022
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.224762]
objective value function right now is: -1109.440520655395
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1111.2920284028212
Current xi:  [14.138426]
objective value function right now is: -1111.2920284028212
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.9473095]
objective value function right now is: -1080.7996976396466
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.027643]
objective value function right now is: -1101.610345429046
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.683198]
objective value function right now is: -1089.6200104273742
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.178364]
objective value function right now is: -1103.5540854665905
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.735665]
objective value function right now is: -1105.984128546831
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.848017]
objective value function right now is: -1090.141605858248
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.904358]
objective value function right now is: -1097.1647618895004
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.785834]
objective value function right now is: -1090.450622947149
new min fval from sgd:  -1111.6213674213018
new min fval from sgd:  -1113.3416260332
new min fval from sgd:  -1113.7419453843186
new min fval from sgd:  -1114.096710836002
new min fval from sgd:  -1114.7067852193643
new min fval from sgd:  -1114.7701808785553
new min fval from sgd:  -1114.913868398853
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.047283]
objective value function right now is: -1103.8074982371074
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.274776]
objective value function right now is: -1106.4370601219384
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.81537]
objective value function right now is: -1110.7797832153774
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.648013]
objective value function right now is: -1098.3477268829427
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.3352995]
objective value function right now is: -1100.6683640645003
min fval:  -1114.913868398853
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.9682,  1.3037],
        [ 1.6718, -1.4720],
        [ 1.7528, -0.8348],
        [ 1.6875, -1.0373],
        [ 1.6770, -1.2246],
        [ 0.6057, -1.2083],
        [ 1.3894, -0.7720],
        [ 0.7991, -1.7880],
        [ 1.5688, -1.0163],
        [ 1.3263, -1.8040]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.6025,  1.0937,  1.1044,  1.0350,  1.0743,  0.8729,  1.0362,  1.0903,
          1.4193,  1.1123],
        [ 0.7164,  1.2367,  1.4725,  1.3764,  0.9419,  0.8681,  1.1751,  0.9896,
          1.3853,  0.8897],
        [ 0.8024,  0.8347,  1.0163,  1.2761,  1.1399,  1.2498,  0.7860,  1.1890,
          1.1154,  1.2454],
        [ 0.8062,  1.2700,  1.1226,  0.9645,  1.2718,  1.0774,  1.2334,  1.2981,
          1.1792,  1.0142],
        [-0.0492,  1.6004,  1.0928,  1.2109,  0.3338,  0.7902,  1.1071,  0.5149,
          0.8853,  0.5122],
        [ 0.9164,  1.0081,  1.3952,  1.3591,  1.1591,  1.0511,  1.0231,  1.0449,
          1.0547,  1.0320],
        [-0.0783,  1.3911,  1.2614,  1.4027,  0.4857,  1.0017,  0.8835,  1.0351,
          0.9756,  0.7321],
        [ 0.6567,  1.1343,  1.3978,  1.2203,  1.0186,  1.1419,  1.2607,  1.1932,
          1.0829,  1.3297],
        [ 1.1478,  1.1240,  0.8019,  0.8915,  1.3028,  1.2072,  1.0601,  1.3774,
          1.2499,  1.3554],
        [ 0.1288,  1.5555,  1.3225,  1.0194,  0.5897,  1.0682,  0.9846,  0.8967,
          1.1119,  0.8583]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.6723, -4.9812, -4.3115, -3.9830, -3.4464, -4.4718, -4.3709, -4.2344,
         -3.9210, -4.6135]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  0.3122,   6.1004],
        [ -8.6927,  -1.2305],
        [ 15.5796,   6.7267],
        [ -1.5869,   5.2544],
        [ -1.7472,   5.1893],
        [ -1.0748,   5.4676],
        [ -3.2199,   4.6901],
        [-10.8150,  -1.4003],
        [ -1.5772,   5.2602],
        [ -0.7563,   5.6014]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -88.3954,  -74.5005,  -36.3000,  -84.4482,  -83.8881,  -86.1950,
          -84.4945,  -67.7575,  -83.1043,  -86.7187],
        [-125.3424,  -83.0691,  -63.3151, -133.8238, -134.7185, -131.6976,
         -136.6430,  -73.6968, -132.9711, -130.2621],
        [  -1.1331,  -22.0049,   -0.8431,   73.7719,   83.3088,   42.2645,
          180.9569,   14.9513,   71.0488,   27.4452],
        [ -95.5900,  -82.5573,  -42.9825,  -93.7402,  -93.0899,  -94.5927,
          -94.4881,  -74.2590,  -92.6506,  -94.9468],
        [ -46.6782,  -68.4109,  -37.5931,  -30.9325,  -28.6222,  -37.4288,
          -23.6978,  -57.3909,  -29.5137,  -39.0041],
        [ -43.5556,  -68.1922,  -37.2315,  -28.8912,  -25.7063,  -34.8102,
          -20.6635,  -56.6383,  -26.5713,  -36.0372],
        [-163.2097,    4.2885,   -3.6368, -132.1614, -123.4414, -144.3665,
          -42.5629,    9.5604, -128.8587, -149.4072],
        [-126.8030,  -83.6066,  -63.5595, -135.0231, -136.1853, -133.1527,
         -138.0631,  -74.3176, -134.6363, -131.3593],
        [  -9.4695,  -77.0802,   -3.0983,  -64.1893,  -68.7326,  -45.5251,
         -133.8408, -233.8823,  -61.9978,  -35.2992],
        [-126.2825,  -90.1292,  -64.7716, -132.5733, -132.6205, -130.8428,
         -131.9387,  -80.8946, -131.7002, -130.0921]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -3.2524,   2.0422,   0.5472,  -0.6844,  -2.2201,  -2.5886,  -6.7506,
           2.0779,  10.2652,   1.7867],
        [  2.8741,  -2.4276,  -0.6000,   0.9308,   2.3221,   2.5214,   6.4233,
          -2.3417, -10.4382,  -1.8800]], device='cuda:0'))])
xi:  [14.492676]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1222.628335124254
W_T_median: 989.4598850542986
W_T_pctile_5: 205.4019403850563
W_T_CVAR_5_pct: 10.070795060335564
Average q (qsum/M+1):  35.0
Optimal xi:  [14.492676]
Expected(across Rb) median(across samples) p_equity:  0.24098771810531616
obj fun:  tensor(-1114.9139, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.9682,  1.3037],
        [ 1.6718, -1.4720],
        [ 1.7528, -0.8348],
        [ 1.6875, -1.0373],
        [ 1.6770, -1.2246],
        [ 0.6057, -1.2083],
        [ 1.3894, -0.7720],
        [ 0.7991, -1.7880],
        [ 1.5688, -1.0163],
        [ 1.3263, -1.8040]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 0.6025,  1.0937,  1.1044,  1.0350,  1.0743,  0.8729,  1.0362,  1.0903,
          1.4193,  1.1123],
        [ 0.7164,  1.2367,  1.4725,  1.3764,  0.9419,  0.8681,  1.1751,  0.9896,
          1.3853,  0.8897],
        [ 0.8024,  0.8347,  1.0163,  1.2761,  1.1399,  1.2498,  0.7860,  1.1890,
          1.1154,  1.2454],
        [ 0.8062,  1.2700,  1.1226,  0.9645,  1.2718,  1.0774,  1.2334,  1.2981,
          1.1792,  1.0142],
        [-0.0492,  1.6004,  1.0928,  1.2109,  0.3338,  0.7902,  1.1071,  0.5149,
          0.8853,  0.5122],
        [ 0.9164,  1.0081,  1.3952,  1.3591,  1.1591,  1.0511,  1.0231,  1.0449,
          1.0547,  1.0320],
        [-0.0783,  1.3911,  1.2614,  1.4027,  0.4857,  1.0017,  0.8835,  1.0351,
          0.9756,  0.7321],
        [ 0.6567,  1.1343,  1.3978,  1.2203,  1.0186,  1.1419,  1.2607,  1.1932,
          1.0829,  1.3297],
        [ 1.1478,  1.1240,  0.8019,  0.8915,  1.3028,  1.2072,  1.0601,  1.3774,
          1.2499,  1.3554],
        [ 0.1288,  1.5555,  1.3225,  1.0194,  0.5897,  1.0682,  0.9846,  0.8967,
          1.1119,  0.8583]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.6723, -4.9812, -4.3115, -3.9830, -3.4464, -4.4718, -4.3709, -4.2344,
         -3.9210, -4.6135]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  0.3122,   6.1004],
        [ -8.6927,  -1.2305],
        [ 15.5796,   6.7267],
        [ -1.5869,   5.2544],
        [ -1.7472,   5.1893],
        [ -1.0748,   5.4676],
        [ -3.2199,   4.6901],
        [-10.8150,  -1.4003],
        [ -1.5772,   5.2602],
        [ -0.7563,   5.6014]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -88.3954,  -74.5005,  -36.3000,  -84.4482,  -83.8881,  -86.1950,
          -84.4945,  -67.7575,  -83.1043,  -86.7187],
        [-125.3424,  -83.0691,  -63.3151, -133.8238, -134.7185, -131.6976,
         -136.6430,  -73.6968, -132.9711, -130.2621],
        [  -1.1331,  -22.0049,   -0.8431,   73.7719,   83.3088,   42.2645,
          180.9569,   14.9513,   71.0488,   27.4452],
        [ -95.5900,  -82.5573,  -42.9825,  -93.7402,  -93.0899,  -94.5927,
          -94.4881,  -74.2590,  -92.6506,  -94.9468],
        [ -46.6782,  -68.4109,  -37.5931,  -30.9325,  -28.6222,  -37.4288,
          -23.6978,  -57.3909,  -29.5137,  -39.0041],
        [ -43.5556,  -68.1922,  -37.2315,  -28.8912,  -25.7063,  -34.8102,
          -20.6635,  -56.6383,  -26.5713,  -36.0372],
        [-163.2097,    4.2885,   -3.6368, -132.1614, -123.4414, -144.3665,
          -42.5629,    9.5604, -128.8587, -149.4072],
        [-126.8030,  -83.6066,  -63.5595, -135.0231, -136.1853, -133.1527,
         -138.0631,  -74.3176, -134.6363, -131.3593],
        [  -9.4695,  -77.0802,   -3.0983,  -64.1893,  -68.7326,  -45.5251,
         -133.8408, -233.8823,  -61.9978,  -35.2992],
        [-126.2825,  -90.1292,  -64.7716, -132.5733, -132.6205, -130.8428,
         -131.9387,  -80.8946, -131.7002, -130.0921]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -3.2524,   2.0422,   0.5472,  -0.6844,  -2.2201,  -2.5886,  -6.7506,
           2.0779,  10.2652,   1.7867],
        [  2.8741,  -2.4276,  -0.6000,   0.9308,   2.3221,   2.5214,   6.4233,
          -2.3417, -10.4382,  -1.8800]], device='cuda:0'))])
loaded xi:  14.492676
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1096.008150842258
Current xi:  [13.747504]
objective value function right now is: -1096.008150842258
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1096.254167687582
Current xi:  [14.473496]
objective value function right now is: -1096.254167687582
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.1041975]
objective value function right now is: -1095.395509085082
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.061978]
objective value function right now is: -1091.8056038182121
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1096.5582629557089
Current xi:  [14.184444]
objective value function right now is: -1096.5582629557089
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.314314]
objective value function right now is: -1093.0883836906553
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [13.654062]
objective value function right now is: -1085.1761156806003
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.11194]
objective value function right now is: -1093.9602406187373
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1098.4765671787663
Current xi:  [14.105643]
objective value function right now is: -1098.4765671787663
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.810944]
objective value function right now is: -1094.5894205661639
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.3077345]
objective value function right now is: -1095.1602311681575
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.742038]
objective value function right now is: -1088.3628344346823
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1099.3760021967043
Current xi:  [13.957651]
objective value function right now is: -1099.3760021967043
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [14.406937]
objective value function right now is: -1097.7453713168036
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.111137]
objective value function right now is: -1094.784737635119
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.091882]
objective value function right now is: -1094.1185948059863
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.246192]
objective value function right now is: -1097.8225940085924
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.202077]
objective value function right now is: -1090.7044227868546
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.045822]
objective value function right now is: -1096.895375662377
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.413122]
objective value function right now is: -1093.7604441224853
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.686404]
objective value function right now is: -1086.0862652318751
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.301438]
objective value function right now is: -1079.792673790576
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.6270485]
objective value function right now is: -1088.0634705175037
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.006814]
objective value function right now is: -1091.9645221284645
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.137407]
objective value function right now is: -1094.068580853015
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.864712]
objective value function right now is: -1085.5779305506833
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.137211]
objective value function right now is: -1098.621153037229
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [14.22791]
objective value function right now is: -1096.0496340231534
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [13.939948]
objective value function right now is: -1094.3512199836935
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.7137]
objective value function right now is: -1093.5245913258266
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.933523]
objective value function right now is: -1092.6796413316433
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.203032]
objective value function right now is: -1097.8986390130776
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.47793]
objective value function right now is: -1091.1588071992346
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.781504]
objective value function right now is: -1096.1601249694756
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.34917]
objective value function right now is: -1096.4552317210487
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.952048]
objective value function right now is: -1094.9084258227754
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.135433]
objective value function right now is: -1091.420924993936
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.080406]
objective value function right now is: -1098.496687153284
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.11636]
objective value function right now is: -1095.3456082717548
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1440.8296865454274
Current xi:  [11.567726]
objective value function right now is: -1440.8296865454274
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1445.510612514955
Current xi:  [11.575005]
objective value function right now is: -1445.510612514955
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1445.897441453551
Current xi:  [11.209974]
objective value function right now is: -1445.897441453551
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.710009]
objective value function right now is: -1435.4488442896763
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.097791]
objective value function right now is: -1443.9730717398957
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.224077]
objective value function right now is: -1443.2724468957135
new min fval from sgd:  -1446.3073671495886
new min fval from sgd:  -1448.232575486575
new min fval from sgd:  -1450.0415880105534
new min fval from sgd:  -1450.1587986557292
new min fval from sgd:  -1450.1695339701682
new min fval from sgd:  -1450.3519365237419
new min fval from sgd:  -1450.8715643813032
new min fval from sgd:  -1451.2025896790046
new min fval from sgd:  -1451.3486848005764
new min fval from sgd:  -1451.4904269590486
new min fval from sgd:  -1451.689042277653
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.9445653]
objective value function right now is: -1018.8211721164358
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0669437]
objective value function right now is: -1090.2013173697205
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.3096355e-10]
objective value function right now is: -1039.3025709671601
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.8409284e-17]
objective value function right now is: -1099.605796641695
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.5506611e-15]
objective value function right now is: -1082.0142180745438
min fval:  -1451.689042277653
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-2.0919,  2.4414],
        [ 1.4805, -1.7071],
        [ 1.3922, -1.5998],
        [ 1.3306, -1.5254],
        [-2.1066,  2.4582],
        [-2.1440,  2.5016],
        [ 1.4285, -1.6444],
        [ 1.3508, -1.5486],
        [ 0.8437, -0.9340],
        [ 1.0542, -1.1888]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-20.2133,   0.8767,   0.7151,   0.6340, -13.9393,  -7.9183,   0.9022,
           0.8986,   0.7223,   0.7197],
        [-19.5861,   0.9114,   0.9724,   0.8538, -13.4110,  -8.0725,   0.9413,
           0.6606,   0.5441,   0.3536],
        [-16.8625,   0.5371,   0.5481,   0.8072, -13.2589,  -7.5770,   0.3845,
           0.9891,   0.4236,   0.8536],
        [-15.7830,   0.8108,   0.4957,   0.3228, -11.4026,  -8.1129,   0.5953,
           0.8975,   0.2839,   0.4118],
        [  3.1410,   2.6348,   2.0381,   2.1302,   6.8934,   4.0617,   2.3993,
           1.9671,   1.9578,   1.7695],
        [-16.7585,   0.6627,   0.8739,   0.8205, -12.8944,  -7.9236,   0.5372,
           0.7731,   0.2680,   0.5443],
        [-22.9170,   1.2249,   0.9439,   1.0973, -14.5374,  -7.5602,   1.0848,
           0.8509,   0.3392,   0.4016],
        [-19.3130,   0.7868,   0.8770,   0.6767, -13.5156,  -7.9010,   0.9459,
           0.8508,   0.2398,   0.7849],
        [  0.9431,   2.7637,   2.3735,   2.5669,   4.4775,   3.8029,   3.5395,
           3.2698,   3.0157,   3.3162],
        [-22.2622,   1.3725,   0.9821,   0.6839, -14.3349,  -7.5507,   1.1123,
           0.7000,   0.4475,   0.5010]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-11.0038, -10.8061,  -6.1158,  -4.1294,  12.5920,  -6.2344, -14.4796,
          -9.5092,  12.0738, -13.8448]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  1.3358,   6.3214],
        [ -9.0722,  -1.1177],
        [ 16.2492,   7.3858],
        [ -1.8200,   4.9421],
        [ -1.8564,   4.8822],
        [ -1.6485,   5.1199],
        [ -2.6968,   4.2452],
        [-11.2243,  -1.3651],
        [ -1.8097,   4.9455],
        [ -1.4476,   5.2352]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-9.4913e+01, -1.0693e+02, -5.2592e+01, -8.1147e+01, -8.0270e+01,
         -8.4961e+01, -8.0489e+01, -9.4292e+01, -7.9807e+01, -8.7513e+01],
        [-1.5156e+02, -8.4350e+01, -7.9637e+01, -1.6337e+02, -1.6419e+02,
         -1.6101e+02, -1.6322e+02, -7.7482e+01, -1.6253e+02, -1.5912e+02],
        [ 2.3686e+00, -2.5395e+01, -1.2910e-02,  1.0237e+02,  1.1395e+02,
          6.2789e+01,  2.1621e+02,  1.6236e+01,  9.9539e+01,  4.2301e+01],
        [-1.1151e+02, -1.1647e+02, -5.7928e+01, -1.0373e+02, -1.0283e+02,
         -1.0600e+02, -1.0184e+02, -1.0296e+02, -1.0265e+02, -1.0765e+02],
        [-6.7058e+01, -1.0360e+02, -5.4961e+01, -4.1526e+01, -3.8504e+01,
         -5.0836e+01, -3.0501e+01, -8.2435e+01, -4.0137e+01, -5.4402e+01],
        [-5.8103e+01, -1.0304e+02, -5.4579e+01, -3.4302e+01, -3.0593e+01,
         -4.2605e+01, -2.3206e+01, -8.1144e+01, -3.1998e+01, -4.5729e+01],
        [-1.9771e+02,  6.4131e+00, -4.2469e+00, -1.4986e+02, -1.3994e+02,
         -1.6610e+02, -5.2258e+01,  1.0983e+01, -1.4665e+02, -1.7389e+02],
        [-1.5291e+02, -8.5251e+01, -8.0288e+01, -1.6439e+02, -1.6547e+02,
         -1.6229e+02, -1.6442e+02, -7.8440e+01, -1.6401e+02, -1.6006e+02],
        [ 9.1338e+00, -8.2625e+01, -3.2483e+00, -6.6174e+01, -7.2159e+01,
         -4.2128e+01, -1.4550e+02, -2.9582e+02, -6.3911e+01, -2.7549e+01],
        [-1.5154e+02, -9.7305e+01, -8.6983e+01, -1.6007e+02, -1.5997e+02,
         -1.5831e+02, -1.5602e+02, -9.1100e+01, -1.5920e+02, -1.5727e+02]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 0.8795,  5.1840,  0.3366,  1.3826,  1.8905,  1.7136, -6.7758,  5.2216,
          8.6529,  4.8541],
        [-1.2579, -5.5697, -0.3895, -1.1364, -1.7886, -1.7809,  6.4489, -5.4858,
         -8.8260, -4.9478]], device='cuda:0'))])
xi:  [10.915632]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 760.6641103361694
W_T_median: 507.0725035014612
W_T_pctile_5: 120.7019153136781
W_T_CVAR_5_pct: -41.96571685271066
Average q (qsum/M+1):  48.86002472908266
Optimal xi:  [10.915632]
Expected(across Rb) median(across samples) p_equity:  0.32605743408203125
obj fun:  tensor(-1451.6890, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-2.0919,  2.4414],
        [ 1.4805, -1.7071],
        [ 1.3922, -1.5998],
        [ 1.3306, -1.5254],
        [-2.1066,  2.4582],
        [-2.1440,  2.5016],
        [ 1.4285, -1.6444],
        [ 1.3508, -1.5486],
        [ 0.8437, -0.9340],
        [ 1.0542, -1.1888]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-20.2133,   0.8767,   0.7151,   0.6340, -13.9393,  -7.9183,   0.9022,
           0.8986,   0.7223,   0.7197],
        [-19.5861,   0.9114,   0.9724,   0.8538, -13.4110,  -8.0725,   0.9413,
           0.6606,   0.5441,   0.3536],
        [-16.8625,   0.5371,   0.5481,   0.8072, -13.2589,  -7.5770,   0.3845,
           0.9891,   0.4236,   0.8536],
        [-15.7830,   0.8108,   0.4957,   0.3228, -11.4026,  -8.1129,   0.5953,
           0.8975,   0.2839,   0.4118],
        [  3.1410,   2.6348,   2.0381,   2.1302,   6.8934,   4.0617,   2.3993,
           1.9671,   1.9578,   1.7695],
        [-16.7585,   0.6627,   0.8739,   0.8205, -12.8944,  -7.9236,   0.5372,
           0.7731,   0.2680,   0.5443],
        [-22.9170,   1.2249,   0.9439,   1.0973, -14.5374,  -7.5602,   1.0848,
           0.8509,   0.3392,   0.4016],
        [-19.3130,   0.7868,   0.8770,   0.6767, -13.5156,  -7.9010,   0.9459,
           0.8508,   0.2398,   0.7849],
        [  0.9431,   2.7637,   2.3735,   2.5669,   4.4775,   3.8029,   3.5395,
           3.2698,   3.0157,   3.3162],
        [-22.2622,   1.3725,   0.9821,   0.6839, -14.3349,  -7.5507,   1.1123,
           0.7000,   0.4475,   0.5010]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-11.0038, -10.8061,  -6.1158,  -4.1294,  12.5920,  -6.2344, -14.4796,
          -9.5092,  12.0738, -13.8448]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  1.3358,   6.3214],
        [ -9.0722,  -1.1177],
        [ 16.2492,   7.3858],
        [ -1.8200,   4.9421],
        [ -1.8564,   4.8822],
        [ -1.6485,   5.1199],
        [ -2.6968,   4.2452],
        [-11.2243,  -1.3651],
        [ -1.8097,   4.9455],
        [ -1.4476,   5.2352]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-9.4913e+01, -1.0693e+02, -5.2592e+01, -8.1147e+01, -8.0270e+01,
         -8.4961e+01, -8.0489e+01, -9.4292e+01, -7.9807e+01, -8.7513e+01],
        [-1.5156e+02, -8.4350e+01, -7.9637e+01, -1.6337e+02, -1.6419e+02,
         -1.6101e+02, -1.6322e+02, -7.7482e+01, -1.6253e+02, -1.5912e+02],
        [ 2.3686e+00, -2.5395e+01, -1.2910e-02,  1.0237e+02,  1.1395e+02,
          6.2789e+01,  2.1621e+02,  1.6236e+01,  9.9539e+01,  4.2301e+01],
        [-1.1151e+02, -1.1647e+02, -5.7928e+01, -1.0373e+02, -1.0283e+02,
         -1.0600e+02, -1.0184e+02, -1.0296e+02, -1.0265e+02, -1.0765e+02],
        [-6.7058e+01, -1.0360e+02, -5.4961e+01, -4.1526e+01, -3.8504e+01,
         -5.0836e+01, -3.0501e+01, -8.2435e+01, -4.0137e+01, -5.4402e+01],
        [-5.8103e+01, -1.0304e+02, -5.4579e+01, -3.4302e+01, -3.0593e+01,
         -4.2605e+01, -2.3206e+01, -8.1144e+01, -3.1998e+01, -4.5729e+01],
        [-1.9771e+02,  6.4131e+00, -4.2469e+00, -1.4986e+02, -1.3994e+02,
         -1.6610e+02, -5.2258e+01,  1.0983e+01, -1.4665e+02, -1.7389e+02],
        [-1.5291e+02, -8.5251e+01, -8.0288e+01, -1.6439e+02, -1.6547e+02,
         -1.6229e+02, -1.6442e+02, -7.8440e+01, -1.6401e+02, -1.6006e+02],
        [ 9.1338e+00, -8.2625e+01, -3.2483e+00, -6.6174e+01, -7.2159e+01,
         -4.2128e+01, -1.4550e+02, -2.9582e+02, -6.3911e+01, -2.7549e+01],
        [-1.5154e+02, -9.7305e+01, -8.6983e+01, -1.6007e+02, -1.5997e+02,
         -1.5831e+02, -1.5602e+02, -9.1100e+01, -1.5920e+02, -1.5727e+02]],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 0.8795,  5.1840,  0.3366,  1.3826,  1.8905,  1.7136, -6.7758,  5.2216,
          8.6529,  4.8541],
        [-1.2579, -5.5697, -0.3895, -1.1364, -1.7886, -1.7809,  6.4489, -5.4858,
         -8.8260, -4.9478]], device='cuda:0'))])
loaded xi:  10.915632
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1258.6129975795416
Current xi:  [0.08975285]
objective value function right now is: -1258.6129975795416
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1272.5887863893518
Current xi:  [-5.6095354e-12]
objective value function right now is: -1272.5887863893518
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1487.3163922573845
Current xi:  [-1.4013549e-16]
objective value function right now is: -1487.3163922573845
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1492.56244684043
Current xi:  [-0.17897055]
objective value function right now is: -1492.56244684043
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00129342]
objective value function right now is: -1489.8189254939414
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00995819]
objective value function right now is: -1492.4625651966862
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-9.18973e-09]
objective value function right now is: -1492.311469603999
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.9306408]
objective value function right now is: -1473.834503795956
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.9283357e-08]
objective value function right now is: -1482.6005474575245
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.6980829e-12]
objective value function right now is: -1487.1852375075182
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1494.0444966573098
Current xi:  [0.00984509]
objective value function right now is: -1494.0444966573098
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.68422484]
objective value function right now is: -1487.0485322665018
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00690622]
objective value function right now is: -1489.024325799903
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1495.078705710784
Current xi:  [-3.5746954e-08]
objective value function right now is: -1495.078705710784
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.06837659]
objective value function right now is: -1494.775118432009
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1495.7834904998037
Current xi:  [-8.5613865e-06]
objective value function right now is: -1495.7834904998037
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.570306e-07]
objective value function right now is: -1494.3949646499716
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1506.549739159529
Current xi:  [1.6286143]
objective value function right now is: -1506.549739159529
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1520.6946237072457
Current xi:  [0.36557016]
objective value function right now is: -1520.6946237072457
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1523.0449601233588
Current xi:  [-3.4351285e-06]
objective value function right now is: -1523.0449601233588
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1534.419080163645
Current xi:  [-4.055035]
objective value function right now is: -1534.419080163645
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.0838387]
objective value function right now is: -1518.807378913071
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.76122]
objective value function right now is: -1530.887249992432
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1539.0968609225424
Current xi:  [-7.1771135]
objective value function right now is: -1539.0968609225424
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1540.2549162902887
Current xi:  [-6.410008]
objective value function right now is: -1540.2549162902887
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1540.7948372678036
Current xi:  [-6.1103125]
objective value function right now is: -1540.7948372678036
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.278156]
objective value function right now is: -1535.1807183438455
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-6.2202487]
objective value function right now is: -1540.7555705749253
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1545.023979826842
Current xi:  [-6.505734]
objective value function right now is: -1545.023979826842
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.6746097]
objective value function right now is: -1534.6545237185383
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.750891]
objective value function right now is: -1544.936270272403
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.756292]
objective value function right now is: -1534.422518827808
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.384612]
objective value function right now is: -1526.4469890351863
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.8784595]
objective value function right now is: -1542.7819363127398
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -1545.3002577485718
Current xi:  [-6.30008]
objective value function right now is: -1545.3002577485718
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.2517757]
objective value function right now is: -1537.755235294878
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.5158544]
objective value function right now is: -1531.8718960526771
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.7429276]
objective value function right now is: -1541.6899902092696
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.5410957]
objective value function right now is: -1540.5874551977706
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1547.7589142028867
Current xi:  [-6.7961073]
objective value function right now is: -1547.7589142028867
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.877687]
objective value function right now is: -1524.3651450094726
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.635319]
objective value function right now is: -1546.8332917594958
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.526254]
objective value function right now is: -1540.2222252645674
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.297055]
objective value function right now is: -1540.477089519971
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.460237]
objective value function right now is: -1537.7285659817276
new min fval from sgd:  -1548.1462070108516
new min fval from sgd:  -1548.5279773878547
new min fval from sgd:  -1548.8424645745706
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.539619]
objective value function right now is: -1535.8238806972859
new min fval from sgd:  -1549.0005648813246
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.2838297]
objective value function right now is: -1537.1971172681922
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.7795067]
objective value function right now is: -1539.3976509022955
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.501577]
objective value function right now is: -1544.224637333833
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.5909157]
objective value function right now is: -1545.9023129120226
min fval:  -1549.0005648813246
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -2.0545,   4.4754],
        [ -0.1670,  14.0578],
        [  3.4873,   8.0368],
        [ -2.7198,   4.8550],
        [ -2.1629,   4.5166],
        [ -2.1929,   4.5478],
        [ -2.8629,   5.0060],
        [-12.6032,   0.4938],
        [ -2.3483,   4.6494],
        [ -2.3737,   4.6631]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-36.8411,  15.8795,   4.8261, -11.1996, -30.7597, -25.1086, -10.7518,
           5.2626, -16.6511, -16.2185],
        [-36.0804,  16.3983,   4.8668, -11.1344, -30.1170, -25.1568, -10.7778,
           4.7065, -16.7333, -16.5008],
        [-33.9335,  13.2060,   4.5316, -11.1506, -30.4990, -25.1795, -10.0015,
           4.6481, -17.3018, -16.4241],
        [-32.2660,  14.0770,   8.6374, -12.8107, -28.2371, -25.3508, -10.2116,
           4.2943, -17.4099, -16.8641],
        [  9.2324, -14.1527,  -4.3571,  13.0617,  13.3965,  10.8981,  11.8433,
           0.9510,   9.5144,   9.5184],
        [-33.7225,  14.6981,   4.6486, -11.3499, -30.0483, -25.4511, -10.3921,
           4.4004, -17.4164, -16.6953],
        [-38.2580,  17.3276,   8.1479, -10.4868, -30.2223, -23.6260, -10.4274,
           5.1308, -16.0756, -15.6386],
        [-36.1164,  15.9217,   4.8262, -11.3201, -30.5108, -25.2670, -10.7011,
           5.1390, -17.2790, -16.3086],
        [  3.2475, -11.0172,  -3.1159,  12.4136,   7.1339,   6.7682,  12.7204,
           2.5472,   7.3315,   7.9105],
        [-38.1038,  18.5120,   5.2249, -10.9852, -30.4141, -24.0193, -10.6189,
           4.4824, -16.2850, -15.8289]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-17.4143, -18.2964, -10.6384, -12.1254,  18.6347, -11.9665, -23.3108,
         -15.6234,  17.1908, -24.2901]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.1661,   4.3001],
        [ -7.9982,  -1.7663],
        [ 49.2631,  11.9944],
        [ -3.0802,   4.3114],
        [ -2.9583,   4.3527],
        [ -3.3768,   4.2099],
        [  7.7272,   7.8419],
        [ -4.8250,   3.3181],
        [ -3.0848,   4.3098],
        [-52.4107,  -8.1164]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -95.6560, -107.8737,  -52.5836,  -81.1962,  -80.2847,  -85.1325,
          -80.4263,  -93.9792,  -79.8593,  -87.7819],
        [-153.0393,  -85.3491,  -83.1173, -165.5876, -166.4148, -163.1699,
         -165.8420,  -79.9137, -164.7370, -161.2326],
        [  33.6111,  -24.0827,   22.5292,   81.7918,  103.0704,   29.1768,
          307.7524,   -4.8446,   78.6644,   70.1384],
        [-112.3163, -117.1967,  -57.3673, -103.8477, -102.9176, -106.2402,
         -101.8845, -102.5135, -102.7731, -107.9978],
        [ -68.0929, -104.5979,  -55.5992,  -42.5229,  -39.4866,  -51.8777,
          -31.2675,  -82.2890,  -41.1343,  -55.4745],
        [ -59.1167, -104.0078,  -55.0010,  -35.1757,  -31.4409,  -43.5523,
          -23.6616,  -80.9007,  -32.8736,  -46.7228],
        [-261.5686,   18.2252,   -3.9795, -211.0096, -200.3536, -221.0118,
          -77.1437,  -24.7678, -207.7925, -128.3207],
        [-154.3957,  -86.2671,  -83.7663, -166.6048, -167.6944, -164.4606,
         -167.0401,  -80.8847, -166.2198, -162.1795],
        [ -81.0927,  -33.7253,   -3.0084, -157.6513, -161.3775, -136.5242,
         -133.1475, -324.0453, -155.4997, -208.1654],
        [-153.0606,  -98.5529,  -90.4601, -162.2925, -162.2113, -160.4894,
         -158.6311,  -93.7150, -161.4204, -159.4171]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -3.3994,   1.4022,   0.3226,  -3.0660,  -2.3588,  -2.5209, -14.7132,
           1.4229,  10.7523,   0.8340],
        [  3.0211,  -1.7867,  -0.3755,   3.3123,   2.4609,   2.4538,  14.3867,
          -1.6860, -10.9253,  -0.9276]], device='cuda:0'))])
xi:  [-6.5089374]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 593.5064620517866
W_T_median: 258.90453146027323
W_T_pctile_5: 42.31557229912742
W_T_CVAR_5_pct: -59.496791336454535
Average q (qsum/M+1):  51.88703377016129
Optimal xi:  [-6.5089374]
Expected(across Rb) median(across samples) p_equity:  0.3322326848904292
obj fun:  tensor(-1549.0006, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -2.0545,   4.4754],
        [ -0.1670,  14.0578],
        [  3.4873,   8.0368],
        [ -2.7198,   4.8550],
        [ -2.1629,   4.5166],
        [ -2.1929,   4.5478],
        [ -2.8629,   5.0060],
        [-12.6032,   0.4938],
        [ -2.3483,   4.6494],
        [ -2.3737,   4.6631]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-36.8411,  15.8795,   4.8261, -11.1996, -30.7597, -25.1086, -10.7518,
           5.2626, -16.6511, -16.2185],
        [-36.0804,  16.3983,   4.8668, -11.1344, -30.1170, -25.1568, -10.7778,
           4.7065, -16.7333, -16.5008],
        [-33.9335,  13.2060,   4.5316, -11.1506, -30.4990, -25.1795, -10.0015,
           4.6481, -17.3018, -16.4241],
        [-32.2660,  14.0770,   8.6374, -12.8107, -28.2371, -25.3508, -10.2116,
           4.2943, -17.4099, -16.8641],
        [  9.2324, -14.1527,  -4.3571,  13.0617,  13.3965,  10.8981,  11.8433,
           0.9510,   9.5144,   9.5184],
        [-33.7225,  14.6981,   4.6486, -11.3499, -30.0483, -25.4511, -10.3921,
           4.4004, -17.4164, -16.6953],
        [-38.2580,  17.3276,   8.1479, -10.4868, -30.2223, -23.6260, -10.4274,
           5.1308, -16.0756, -15.6386],
        [-36.1164,  15.9217,   4.8262, -11.3201, -30.5108, -25.2670, -10.7011,
           5.1390, -17.2790, -16.3086],
        [  3.2475, -11.0172,  -3.1159,  12.4136,   7.1339,   6.7682,  12.7204,
           2.5472,   7.3315,   7.9105],
        [-38.1038,  18.5120,   5.2249, -10.9852, -30.4141, -24.0193, -10.6189,
           4.4824, -16.2850, -15.8289]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-17.4143, -18.2964, -10.6384, -12.1254,  18.6347, -11.9665, -23.3108,
         -15.6234,  17.1908, -24.2901]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -3.1661,   4.3001],
        [ -7.9982,  -1.7663],
        [ 49.2631,  11.9944],
        [ -3.0802,   4.3114],
        [ -2.9583,   4.3527],
        [ -3.3768,   4.2099],
        [  7.7272,   7.8419],
        [ -4.8250,   3.3181],
        [ -3.0848,   4.3098],
        [-52.4107,  -8.1164]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -95.6560, -107.8737,  -52.5836,  -81.1962,  -80.2847,  -85.1325,
          -80.4263,  -93.9792,  -79.8593,  -87.7819],
        [-153.0393,  -85.3491,  -83.1173, -165.5876, -166.4148, -163.1699,
         -165.8420,  -79.9137, -164.7370, -161.2326],
        [  33.6111,  -24.0827,   22.5292,   81.7918,  103.0704,   29.1768,
          307.7524,   -4.8446,   78.6644,   70.1384],
        [-112.3163, -117.1967,  -57.3673, -103.8477, -102.9176, -106.2402,
         -101.8845, -102.5135, -102.7731, -107.9978],
        [ -68.0929, -104.5979,  -55.5992,  -42.5229,  -39.4866,  -51.8777,
          -31.2675,  -82.2890,  -41.1343,  -55.4745],
        [ -59.1167, -104.0078,  -55.0010,  -35.1757,  -31.4409,  -43.5523,
          -23.6616,  -80.9007,  -32.8736,  -46.7228],
        [-261.5686,   18.2252,   -3.9795, -211.0096, -200.3536, -221.0118,
          -77.1437,  -24.7678, -207.7925, -128.3207],
        [-154.3957,  -86.2671,  -83.7663, -166.6048, -167.6944, -164.4606,
         -167.0401,  -80.8847, -166.2198, -162.1795],
        [ -81.0927,  -33.7253,   -3.0084, -157.6513, -161.3775, -136.5242,
         -133.1475, -324.0453, -155.4997, -208.1654],
        [-153.0606,  -98.5529,  -90.4601, -162.2925, -162.2113, -160.4894,
         -158.6311,  -93.7150, -161.4204, -159.4171]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -3.3994,   1.4022,   0.3226,  -3.0660,  -2.3588,  -2.5209, -14.7132,
           1.4229,  10.7523,   0.8340],
        [  3.0211,  -1.7867,  -0.3755,   3.3123,   2.4609,   2.4538,  14.3867,
          -1.6860, -10.9253,  -0.9276]], device='cuda:0'))])
loaded xi:  -6.5089374
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1587.9013226028171
Current xi:  [-0.1575315]
objective value function right now is: -1587.9013226028171
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.2393053e-10]
objective value function right now is: -1585.498341831132
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.271266e-08]
objective value function right now is: -1585.035096954733
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02082205]
objective value function right now is: -1583.435208493194
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.044451e-05]
objective value function right now is: -1579.973663105466
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04694287]
objective value function right now is: -1579.6155203574676
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [0.02286716]
objective value function right now is: -1536.1223809450494
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00902342]
objective value function right now is: -1582.6087591493176
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00087999]
objective value function right now is: -1584.7683394276376
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00847992]
objective value function right now is: -1585.1068289455568
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1587.9833085460577
Current xi:  [0.02211125]
objective value function right now is: -1587.9833085460577
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01009066]
objective value function right now is: -1587.0502857676468
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.7341741e-05]
objective value function right now is: -1583.2269479516708
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [6.082817e-06]
objective value function right now is: -1587.9824475824075
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00085596]
objective value function right now is: -1582.2937237879157
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00679168]
objective value function right now is: -1580.1295354357153
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0008828]
objective value function right now is: -1569.840169458956
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1588.4714358588253
Current xi:  [-5.2712846e-07]
objective value function right now is: -1588.4714358588253
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00361999]
objective value function right now is: -1588.2143267014442
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1589.081458416505
Current xi:  [0.00096079]
objective value function right now is: -1589.081458416505
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00015894]
objective value function right now is: -1586.6768725887584
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00316201]
objective value function right now is: -1586.5071260052323
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00030156]
objective value function right now is: -1587.2082833649927
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01869661]
objective value function right now is: -1588.1562153285363
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00356179]
objective value function right now is: -1581.340370412219
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00204894]
objective value function right now is: -1581.5946236097361
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.444373e-06]
objective value function right now is: -1573.6745002683042
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04555374]
objective value function right now is: -1577.0949044799975
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01967422]
objective value function right now is: -1571.8845984551344
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02716396]
objective value function right now is: -1581.609435569033
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00144829]
objective value function right now is: -1588.034260582775
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.4574975e-05]
objective value function right now is: -1581.1901943283747
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0030846]
objective value function right now is: -1582.5944194417352
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01032262]
objective value function right now is: -1583.227615578099
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00196245]
objective value function right now is: -1584.6221742870562
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00056168]
objective value function right now is: -1570.618987016422
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00425543]
objective value function right now is: -1584.919406331515
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.601875e-05]
objective value function right now is: -1582.8416550351926
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00044315]
objective value function right now is: -1578.5569772363626
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.260802e-05]
objective value function right now is: -1580.6275048606615
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00539075]
objective value function right now is: -1585.1981068073997
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06879961]
objective value function right now is: -1575.0078094051164
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00048926]
objective value function right now is: -1586.1767973178999
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00072475]
objective value function right now is: -1588.8155290595355
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00052701]
objective value function right now is: -1575.2010211943061
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.7978841e-06]
objective value function right now is: -1583.3147308825833
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.04738341]
objective value function right now is: -1580.6740667288632
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00383138]
objective value function right now is: -1582.8596836292134
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00820298]
objective value function right now is: -1586.3330155360804
new min fval from sgd:  -1589.7328658885347
new min fval from sgd:  -1589.8943085150988
new min fval from sgd:  -1590.173264300129
new min fval from sgd:  -1590.5220977073848
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00633457]
objective value function right now is: -1589.9446502110204
min fval:  -1590.5220977073848
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-3.3999,  4.7924],
        [ 2.3597, 53.5087],
        [-2.8466, 48.2234],
        [-3.3263,  4.8218],
        [-3.3748,  4.7862],
        [-3.3774,  4.7964],
        [-3.3310,  4.8329],
        [-8.9665,  0.6624],
        [-3.3752,  4.8228],
        [-3.3727,  4.8243]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-42.0869,   8.2086, -12.8684, -16.9553, -36.0374, -30.4815, -16.6793,
           8.5253, -22.2399, -21.8218],
        [-41.7494,   8.6324, -15.0267, -17.3813, -35.8288, -30.9695, -17.2080,
           8.2282, -22.7785, -22.5629],
        [-39.5113,   7.5747,  -3.5696, -17.1487, -36.1019, -30.8692, -16.1421,
           7.3714, -23.1818, -22.3155],
        [-42.4629,   8.6030,  -5.2242, -23.8122, -38.5210, -35.7567, -21.4162,
           3.3284, -28.1134, -27.5923],
        [ 23.5224,  -3.8847,  13.5139,  28.8759,  27.8540,  25.5991,  28.0239,
          -0.5601,  24.7892,  24.8391],
        [-41.1016,   8.3745,  -5.2898, -19.2437, -37.4682, -32.9714, -18.4389,
           5.9171, -25.1557, -24.4484],
        [-43.7404,   9.2650,  -7.3616, -16.6611, -35.7610, -29.2760, -16.8237,
           8.6181, -21.9913, -21.5748],
        [-41.4663,   8.3373, -13.2887, -17.1906, -35.8949, -30.7466, -16.7442,
           8.4506, -22.9763, -22.0210],
        [ 16.1974,  -0.8528,  13.9193,  27.0414,  20.2741,  20.1750,  27.7241,
           0.6785,  21.3682,  21.9975],
        [-44.9410,   9.5516, -15.5702, -18.5539, -37.3148, -31.0380, -18.4066,
           7.7200, -23.5806, -23.1457]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-14.7865, -14.1675,  -6.8661,  -6.1107,  25.5194,  -6.8669, -20.8540,
         -12.6114,  23.5309, -18.6976]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.9439,   4.1478],
        [ -7.5569,  -1.4458],
        [ 70.6095,  16.6632],
        [ -2.5405,   4.2290],
        [ -2.4131,   4.2585],
        [ -2.8214,   4.1637],
        [ 12.9778,  10.8439],
        [ -3.7653,   3.9021],
        [ -2.5466,   4.2274],
        [-75.0268, -13.4281]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -95.6560, -108.8572,  -53.6364,  -81.1962,  -80.2847,  -85.1325,
          -80.4263,  -93.9792,  -79.8593,  -87.7821],
        [-153.0393,  -84.5016,  -82.1653, -165.5876, -166.4148, -163.1699,
         -165.8420,  -79.9137, -164.7370, -161.2326],
        [  20.3556,  -31.4956,   26.2374,   72.1003,   95.2169,   15.3422,
          449.0506,  -41.2219,   68.8659,   36.0396],
        [-112.3163, -118.1601,  -58.3952, -103.8477, -102.9176, -106.2402,
         -101.8845, -102.5135, -102.7731, -107.9980],
        [ -68.0929, -105.5509,  -56.6253,  -42.5229,  -39.4866,  -51.8777,
          -31.2675,  -82.2890,  -41.1343,  -55.4746],
        [ -59.1167, -104.9661,  -56.0316,  -35.1757,  -31.4409,  -43.5523,
          -23.6616,  -80.9007,  -32.8736,  -46.7230],
        [-319.3789,   27.3061,   -4.9537, -267.6667, -257.1376, -277.2149,
          -88.6897,  -53.5746, -264.4046,  -84.6459],
        [-154.3957,  -85.4284,  -82.8193, -166.6048, -167.6944, -164.4606,
         -167.0401,  -80.8847, -166.2198, -162.1795],
        [-210.9916,  -24.0190,   -3.4963, -279.7684, -280.9037, -264.3252,
         -104.8485, -406.1547, -277.7054, -242.6923],
        [-153.0606,  -97.9018,  -89.6288, -162.2925, -162.2113, -160.4894,
         -158.6311,  -93.7150, -161.4204, -159.4171]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -2.3438,   2.4509,   0.1958,  -2.0157,  -1.3056,  -1.4671, -20.5709,
           2.4713,  13.7926,   1.8789],
        [  1.9669,  -2.8331,  -0.2488,   2.2632,   1.4091,   1.4014,  20.2444,
          -2.7322, -13.9656,  -1.9704]], device='cuda:0'))])
xi:  [-0.0002359]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 675.914326700424
W_T_median: 247.6943404196042
W_T_pctile_5: -4.2189635472732565
W_T_CVAR_5_pct: -118.91676777712408
Average q (qsum/M+1):  53.230984595514116
Optimal xi:  [-0.0002359]
Expected(across Rb) median(across samples) p_equity:  0.3906603803237279
obj fun:  tensor(-1590.5221, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-3.3999,  4.7924],
        [ 2.3597, 53.5087],
        [-2.8466, 48.2234],
        [-3.3263,  4.8218],
        [-3.3748,  4.7862],
        [-3.3774,  4.7964],
        [-3.3310,  4.8329],
        [-8.9665,  0.6624],
        [-3.3752,  4.8228],
        [-3.3727,  4.8243]], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-42.0869,   8.2086, -12.8684, -16.9553, -36.0374, -30.4815, -16.6793,
           8.5253, -22.2399, -21.8218],
        [-41.7494,   8.6324, -15.0267, -17.3813, -35.8288, -30.9695, -17.2080,
           8.2282, -22.7785, -22.5629],
        [-39.5113,   7.5747,  -3.5696, -17.1487, -36.1019, -30.8692, -16.1421,
           7.3714, -23.1818, -22.3155],
        [-42.4629,   8.6030,  -5.2242, -23.8122, -38.5210, -35.7567, -21.4162,
           3.3284, -28.1134, -27.5923],
        [ 23.5224,  -3.8847,  13.5139,  28.8759,  27.8540,  25.5991,  28.0239,
          -0.5601,  24.7892,  24.8391],
        [-41.1016,   8.3745,  -5.2898, -19.2437, -37.4682, -32.9714, -18.4389,
           5.9171, -25.1557, -24.4484],
        [-43.7404,   9.2650,  -7.3616, -16.6611, -35.7610, -29.2760, -16.8237,
           8.6181, -21.9913, -21.5748],
        [-41.4663,   8.3373, -13.2887, -17.1906, -35.8949, -30.7466, -16.7442,
           8.4506, -22.9763, -22.0210],
        [ 16.1974,  -0.8528,  13.9193,  27.0414,  20.2741,  20.1750,  27.7241,
           0.6785,  21.3682,  21.9975],
        [-44.9410,   9.5516, -15.5702, -18.5539, -37.3148, -31.0380, -18.4066,
           7.7200, -23.5806, -23.1457]], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-14.7865, -14.1675,  -6.8661,  -6.1107,  25.5194,  -6.8669, -20.8540,
         -12.6114,  23.5309, -18.6976]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.9439,   4.1478],
        [ -7.5569,  -1.4458],
        [ 70.6095,  16.6632],
        [ -2.5405,   4.2290],
        [ -2.4131,   4.2585],
        [ -2.8214,   4.1637],
        [ 12.9778,  10.8439],
        [ -3.7653,   3.9021],
        [ -2.5466,   4.2274],
        [-75.0268, -13.4281]], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -95.6560, -108.8572,  -53.6364,  -81.1962,  -80.2847,  -85.1325,
          -80.4263,  -93.9792,  -79.8593,  -87.7821],
        [-153.0393,  -84.5016,  -82.1653, -165.5876, -166.4148, -163.1699,
         -165.8420,  -79.9137, -164.7370, -161.2326],
        [  20.3556,  -31.4956,   26.2374,   72.1003,   95.2169,   15.3422,
          449.0506,  -41.2219,   68.8659,   36.0396],
        [-112.3163, -118.1601,  -58.3952, -103.8477, -102.9176, -106.2402,
         -101.8845, -102.5135, -102.7731, -107.9980],
        [ -68.0929, -105.5509,  -56.6253,  -42.5229,  -39.4866,  -51.8777,
          -31.2675,  -82.2890,  -41.1343,  -55.4746],
        [ -59.1167, -104.9661,  -56.0316,  -35.1757,  -31.4409,  -43.5523,
          -23.6616,  -80.9007,  -32.8736,  -46.7230],
        [-319.3789,   27.3061,   -4.9537, -267.6667, -257.1376, -277.2149,
          -88.6897,  -53.5746, -264.4046,  -84.6459],
        [-154.3957,  -85.4284,  -82.8193, -166.6048, -167.6944, -164.4606,
         -167.0401,  -80.8847, -166.2198, -162.1795],
        [-210.9916,  -24.0190,   -3.4963, -279.7684, -280.9037, -264.3252,
         -104.8485, -406.1547, -277.7054, -242.6923],
        [-153.0606,  -97.9018,  -89.6288, -162.2925, -162.2113, -160.4894,
         -158.6311,  -93.7150, -161.4204, -159.4171]], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -2.3438,   2.4509,   0.1958,  -2.0157,  -1.3056,  -1.4671, -20.5709,
           2.4713,  13.7926,   1.8789],
        [  1.9669,  -2.8331,  -0.2488,   2.2632,   1.4091,   1.4014,  20.2444,
          -2.7322, -13.9656,  -1.9704]], device='cuda:0'))])
loaded xi:  -0.00023589958
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1635.2108380638172
Current xi:  [-1.3825276e-06]
objective value function right now is: -1635.2108380638172
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1636.0417211994882
Current xi:  [0.00012809]
objective value function right now is: -1636.0417211994882
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1638.9407954076735
Current xi:  [-0.00266223]
objective value function right now is: -1638.9407954076735
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0754462]
objective value function right now is: -1638.1268825151358
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1639.9264654321357
Current xi:  [0.03198223]
objective value function right now is: -1639.9264654321357
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1640.2553706958279
Current xi:  [9.213866e-06]
objective value function right now is: -1640.2553706958279
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00124069]
objective value function right now is: -1635.08341412778
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00274422]
objective value function right now is: -1635.263363591019
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01761027]
objective value function right now is: -1639.4132726984894
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00346702]
objective value function right now is: -1637.095825902461
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.2303073e-05]
objective value function right now is: -1639.7675397209264
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.10730653]
objective value function right now is: -1639.275711614897
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1641.2104875373352
Current xi:  [0.00073629]
objective value function right now is: -1641.2104875373352
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [0.05104226]
objective value function right now is: -1639.807415286829
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00782053]
objective value function right now is: -1634.7731902262449
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00024921]
objective value function right now is: -1640.8971850451117
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.09154048]
objective value function right now is: -1640.7298195820144
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1641.4029125644402
Current xi:  [0.00259909]
objective value function right now is: -1641.4029125644402
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0451244]
objective value function right now is: -1640.686902760322
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0343753]
objective value function right now is: -1631.8145306909266
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.395783e-07]
objective value function right now is: -1631.1361948610545
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01400725]
objective value function right now is: -1637.703601675206
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00726418]
objective value function right now is: -1637.539927568236
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00035547]
objective value function right now is: -1637.4517334235334
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.05496246]
objective value function right now is: -1640.2804309502835
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00681811]
objective value function right now is: -1639.5974743059946
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02921409]
objective value function right now is: -1635.407298420442
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02293582]
objective value function right now is: -1639.1579295884133
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [0.00771977]
objective value function right now is: -1639.497411915582
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.7935053e-05]
objective value function right now is: -1639.0707421329125
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02251191]
objective value function right now is: -1636.3218566900698
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-4.031788e-05]
objective value function right now is: -1639.4299740330907
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00494207]
objective value function right now is: -1639.8479373018451
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.8771935e-06]
objective value function right now is: -1638.7890416210364
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00041849]
objective value function right now is: -1638.929146258389
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.03793281]
objective value function right now is: -1614.1469388893674
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.3976544e-07]
objective value function right now is: -1636.4248562286043
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.2349824e-05]
objective value function right now is: -1640.7544685619744
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1641.7518901682095
Current xi:  [1.6830858e-05]
objective value function right now is: -1641.7518901682095
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00259199]
objective value function right now is: -1640.4093305093
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00160655]
objective value function right now is: -1636.9809921157994
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.9573504e-05]
objective value function right now is: -1639.44539456135
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00913677]
objective value function right now is: -1638.8848855811839
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00226628]
objective value function right now is: -1637.3520671392614
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.6235411e-05]
objective value function right now is: -1634.2960117699186
new min fval from sgd:  -1641.769486422083
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00313815]
objective value function right now is: -1638.3372453892312
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0001276]
objective value function right now is: -1637.7565754957916
new min fval from sgd:  -1641.9211832968153
new min fval from sgd:  -1641.9658660186735
new min fval from sgd:  -1642.0042857134513
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00015445]
objective value function right now is: -1639.8994316465494
new min fval from sgd:  -1642.0715233356852
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0007432]
objective value function right now is: -1639.8292732659108
new min fval from sgd:  -1642.0788796727163
new min fval from sgd:  -1642.172182578411
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00159234]
objective value function right now is: -1640.571686785485
Traceback (most recent call last):
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/decumulation_driver.py", line 863, in <module>
    fun_RUN__wrapper.RUN__wrapper_ONE_stage_optimization(
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_RUN__wrapper.py", line 116, in RUN__wrapper_ONE_stage_optimization
    RUN__wrapper_training_testing_NN(
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_RUN__wrapper.py", line 216, in RUN__wrapper_training_testing_NN
    res_adam = fun_train_NN.train_NN( theta0 = theta0,
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_train_NN.py", line 196, in train_NN
    result_pyt_adam = run_Gradient_Descent_pytorch(NN_list= NN_list,
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_train_NN_SGD_algorithms.py", line 256, in run_Gradient_Descent_pytorch
    min_fval, _ = objfun_pyt(NN_list_min, params, xi_min)
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_eval_objfun_NN_strategy.py", line 31, in eval_obj_NN_strategy_pyt
    params, g, qsum_T_vector = fun_invest_NN_strategy.withdraw_invest_NN_strategy(NN_list, params)
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/fun_invest_NN_strategy.py", line 138, in withdraw_invest_NN_strategy
    q_n_proportion = torch.squeeze(NN_list[0].forward(phi_1))
  File "/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_NN_Pytorch.py", line 83, in forward
    return self.model(input_tensor)
  File "/home/marcchen/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/marcchen/.local/lib/python3.10/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/marcchen/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/marcchen/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py", line 290, in forward
    return torch.sigmoid(input)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.69 GiB total capacity; 1.71 GiB already allocated; 20.19 MiB free; 1.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
