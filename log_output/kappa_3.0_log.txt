tracing parameter entered from terminal:  3.0


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = basic_T30_VWD
timeseries_basket['basket_desc'] = CRSP data: T30 and VWD
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
5.0% of MC simulations done.
10.0% of MC simulations done.
15.0% of MC simulations done.
20.0% of MC simulations done.
25.0% of MC simulations done.
30.0% of MC simulations done.
35.0% of MC simulations done.
40.0% of MC simulations done.
45.0% of MC simulations done.
50.0% of MC simulations done.
55.00000000000001% of MC simulations done.
60.0% of MC simulations done.
65.0% of MC simulations done.
70.0% of MC simulations done.
75.0% of MC simulations done.
80.0% of MC simulations done.
85.0% of MC simulations done.
90.0% of MC simulations done.
95.0% of MC simulations done.
100.0% of MC simulations done.
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  hidden_layer    None       None         None   
4        obj.layers[4]        4  hidden_layer    None       None         None   
5        obj.layers[5]        5  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  
4    False        None  
5    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  hidden_layer       4  logistic_sigmoid   
4        obj.layers[4]        4  hidden_layer       4  logistic_sigmoid   
5        obj.layers[5]        5  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 4)    False        None  
4       (4, 4)    False        None  
5       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
[-0.62277812  0.75294104  0.37108154 -0.23457045  0.87734683 -0.86163273
  0.02785573 -0.48145794 -0.54600176  0.39576202 -0.65000431 -0.36885567
  0.66919174 -0.62014212 -0.6576227  -0.09547811  0.44289714 -0.62745997
 -0.69254554 -0.71497204 -0.09160633 -0.70209214 -0.08649162 -0.03995664
  0.43014226  0.54196156 -0.18869499  0.3803182  -0.4213674   0.51582093
  0.719146   -0.41932646  0.60494673  0.54081414  0.80135845 -0.24869297
 -0.78078131  0.14937982  0.57187137  0.48684314 -0.81297698 -0.8032804
 -0.30872557  0.20747554 -0.27489946  0.56429979 -0.5811595  -0.16646221
 -0.44248831  0.58917257  0.54882857 -0.57567892 -0.30430046  0.43734894
 -0.41036775  0.8237008   0.69471977 -0.63290457 -0.75257106 -0.88717124
  0.98028969  0.25945333  0.60467509  0.43128316] (64,)
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.232828508735
gradient value of function right now is: [ 1.32596881e-05  7.95180538e-07  1.32497644e-05  1.32537849e-05
 -1.32596860e-05 -7.95180411e-07 -1.32497623e-05 -1.32537828e-05
  2.08753821e-07  2.55108258e-08  2.04380222e-07  2.18664824e-07
  7.70998562e-07  9.42802082e-08  7.54914719e-07  8.07640975e-07
  1.95380207e-07  2.38773859e-08  1.91287483e-07  2.04656786e-07
  1.91014108e-07  2.33432097e-08  1.87012584e-07  2.00083068e-07
 -1.05626783e-08 -3.31239377e-09 -1.20453814e-08 -2.11649131e-09
  7.40458339e-09  2.52843197e-09  8.69973652e-09  1.52447428e-09
  8.65159564e-08  2.95917087e-08  1.01758129e-07  1.78100718e-08
  9.83581326e-09  3.60946916e-09  1.18795333e-08  2.07224811e-09
  1.22524065e-08  1.92046389e-08  4.83730464e-09  1.02024355e-08
  5.57173649e-09  8.63829445e-09  2.18493750e-09  4.54908654e-09
  8.79887435e-09  1.39915941e-08  2.58369105e-09  5.90807660e-09
  8.61201883e-09  1.33606063e-08  2.39705625e-09  5.39502332e-09
  3.19875542e-09  5.63018676e-09  6.97195430e-10  3.78650334e-11
 -3.30278742e-10  7.22708374e-11 -1.32283716e-09 -6.86200725e-10
 -3.85281496e+00]
supnorm grad right now is: 3.8528149589402565
Weights right now are: 
[-3.61068563 -0.61234728 -2.43733239 -3.22139599  3.86525434  0.50365559
  2.83626966  2.50536761 -2.23916935 -0.90684688 -2.21525226 -1.96931837
  1.7296651   0.48501173  0.16654262  0.98278553 -1.39857378 -0.50897647
 -2.6045598  -2.16043768 -2.10862102 -0.94603861 -2.35546651 -1.77358245
 -0.97674012 -0.83432961 -1.72739262 -0.91359168  0.46514642  1.45530421
  1.54307512  0.50860629 -1.07001816 -0.62767045 -1.36303242 -1.20110651
 -2.51854006 -1.43565928 -1.29671552 -1.0720376  -0.59641602 -1.09205804
  0.24335924  0.3339776   0.43658917  1.1224408  -0.17829875  0.38903083
 -1.46060067 -0.57379521 -0.43875286 -1.59344045  0.61751589  1.16497158
  0.5270573   1.68244985  1.28482626 -0.47723388 -0.12908146 -2.19425405
  2.09366026  1.58999429 -0.07278646  1.08622139 24.69461787]
(65,)
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.793393038714
gradient value of function right now is: [ 1.02647934e-07  8.45494034e-09  1.02588067e-07  1.02614771e-07
 -1.02648397e-07 -8.45497904e-09 -1.02588530e-07 -1.02615233e-07
  1.79415355e-09  2.19521555e-10  1.76342955e-09  1.88044694e-09
  8.91082345e-09  1.09093263e-09  8.75904701e-09  9.33981666e-09
  1.73669892e-09  2.12497810e-10  1.70696313e-09  1.82023231e-09
  1.69308108e-09  2.07156044e-10  1.66409008e-09  1.77451398e-09
 -2.30239224e-10 -7.37258029e-11 -2.64153414e-10 -4.65617210e-11
 -6.49986788e-12 -9.31408527e-13 -6.04239064e-12 -1.08441084e-12
  8.10084948e-10  2.72311113e-10  9.45585790e-10  1.66346571e-10
  6.34697774e-11  2.31709587e-11  7.63876804e-11  1.33982699e-11
  9.64992654e-11  1.52627906e-10  3.54046078e-11  7.78963770e-11
  4.82797623e-11  7.55079078e-11  1.68718000e-11  3.69471127e-11
  7.13364817e-11  1.13885840e-10  1.96164143e-11  4.64421508e-11
  7.18358440e-11  1.11886733e-10  1.86006004e-11  4.34646516e-11
  2.50141697e-11  4.49573498e-11  4.70452643e-12  1.38361526e-13
 -2.58863151e-12  4.08693525e-13 -1.11627254e-11 -6.69285980e-12
  7.61012466e-01]
supnorm grad right now is: 0.7610124656136714
Weights right now are: 
[-4.39236503 -0.67577059 -3.2255892  -4.05495887  4.64693382  0.5670789
  3.62452655  3.33893057 -2.26555781 -0.91004299 -2.24713102 -1.99337272
  1.60895268  0.47119727  0.02999895  0.87331445 -1.43845236 -0.51395771
 -2.64610675 -2.2006174  -2.1474457  -0.95101743 -2.3964055  -1.81300104
 -0.97370057 -0.83330154 -1.72366559 -0.91289504  0.46451799  1.45505146
  1.54219725  0.50844503 -1.08932106 -0.63425757 -1.3856702  -1.20516539
 -2.52033539 -1.43632483 -1.29889187 -1.07243544 -0.59901423 -1.09614504
  0.24239457  0.33187073  0.43536631  1.12053227 -0.17874151  0.38807197
 -1.46247567 -0.57675697 -0.4392767  -1.59467228  0.61564761  1.16208767
  0.52656577  1.68130941  1.28414699 -0.47844357 -0.12922287 -2.19426252
  2.09373005  1.58998325 -0.07249672  1.08638879 25.03637148]
(65,)
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.699962447643
gradient value of function right now is: [ 4.53908009e-08  3.85667158e-09  4.53645118e-08  4.53762078e-08
 -4.53904516e-08 -3.85664141e-09 -4.53641628e-08 -4.53758586e-08
  8.10929705e-10  9.92495544e-11  7.97247406e-10  8.49992432e-10
  4.12639105e-09  5.05325830e-10  4.05714524e-09  4.32534549e-09
  7.92262985e-10  9.69676307e-11  7.78897575e-10  8.30428039e-10
  7.70414020e-10  9.42913279e-11  7.57416305e-10  8.07525444e-10
 -1.08790230e-10 -3.47467485e-11 -1.24687653e-10 -2.19915016e-11
 -5.43487666e-12 -1.21312926e-12 -5.58678185e-12 -9.93623283e-13
  3.71244847e-10  1.24308681e-10  4.32668843e-10  7.61717482e-11
  2.86010675e-11  1.03997741e-11  3.43630663e-11  6.03300621e-12
  4.38560470e-11  6.95128564e-11  1.57848245e-11  3.50966740e-11
  2.21577503e-11  3.47238609e-11  7.56525187e-12  1.67571333e-11
  3.25541220e-11  5.20224706e-11  8.80408625e-12  2.10238841e-11
  3.27771488e-11  5.11018848e-11  8.33772910e-12  1.96630539e-11
  1.13659893e-11  2.05823875e-11  2.10655017e-12  6.56388451e-14
 -1.16707849e-12  1.66496609e-13 -5.07353635e-12 -3.11576891e-12
  1.79096803e+00]
supnorm grad right now is: 1.790968030126643
Weights right now are: 
[-4.52677313e+00 -6.87013975e-01 -3.35991913e+00 -4.18932362e+00
  4.78134192e+00  5.78322291e-01  3.75885648e+00  3.47329532e+00
 -2.26793416e+00 -9.10333599e-01 -2.24946684e+00 -1.99586334e+00
  1.59701459e+00  4.69736427e-01  1.82633749e-02  8.60801757e-01
 -1.44076331e+00 -5.14240327e-01 -2.64837829e+00 -2.20303948e+00
 -2.14969576e+00 -9.51292598e-01 -2.39861719e+00 -1.81535929e+00
 -9.73389887e-01 -8.33201591e-01 -1.72330852e+00 -9.12832139e-01
  4.64529949e-01  1.45505375e+00  1.54220907e+00  5.08447142e-01
 -1.09039975e+00 -6.34622048e-01 -1.38693182e+00 -1.20538715e+00
 -2.52041946e+00 -1.43635573e+00 -1.29899334e+00 -1.07245322e+00
 -5.99142388e-01 -1.09634731e+00  2.42346385e-01  3.31765927e-01
  4.35301915e-01  1.12043177e+00 -1.78764570e-01  3.88022059e-01
 -1.46257031e+00 -5.76907885e-01 -4.39303289e-01 -1.59473460e+00
  6.15552355e-01  1.16193947e+00  5.26540525e-01  1.68125105e+00
  1.28411374e+00 -4.78502694e-01 -1.29229103e-01 -2.19426266e+00
  2.09373352e+00  1.58998263e+00 -7.24818007e-02  1.08639746e+00
  2.51289946e+01]
(65,)
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.814965285532
gradient value of function right now is: [ 2.95363744e-08  2.55436356e-09  2.95193249e-08  2.95268992e-08
 -2.95362538e-08 -2.55435282e-09 -2.95192044e-08 -2.95267786e-08
  5.34120760e-10  6.53278653e-11  5.25147207e-10  5.59839875e-10
  2.75533087e-09  3.37213303e-10  2.70929796e-09  2.88813709e-09
  5.24307522e-10  6.41294694e-11  5.15500163e-10  5.49555149e-10
  5.09199328e-10  6.22800872e-11  5.00645136e-10  5.33718665e-10
 -7.33446925e-11 -2.35636046e-11 -8.42438291e-11 -1.48471478e-11
 -4.55295576e-12 -1.10667244e-12 -4.79058877e-12 -8.50742053e-13
  2.45934316e-10  8.28829049e-11  2.87336203e-10  5.05373176e-11
  1.87853335e-11  6.88950013e-12  2.26487577e-11  3.97091552e-12
  2.89494487e-11  4.57302701e-11  1.07650198e-11  2.35289660e-11
  1.47038419e-11  2.29654656e-11  5.18617790e-12  1.12883443e-11
  2.14536729e-11  3.42290892e-11  5.95980042e-12  1.40373601e-11
  2.15969112e-11  3.36180504e-11  5.65791518e-12  1.31463291e-11
  7.50816352e-12  1.34087567e-11  1.38293318e-12  2.68898126e-14
 -7.77018504e-13  1.32827349e-13 -3.37164184e-12 -1.98773752e-12
  9.09809195e-02]
supnorm grad right now is: 0.0909809195408686
Weights right now are: 
[-4.59987128e+00 -6.93279822e-01 -3.43297501e+00 -4.26239829e+00
  4.85444006e+00  5.84588138e-01  3.83191235e+00  3.54636998e+00
 -2.26924835e+00 -9.10494344e-01 -2.25075887e+00 -1.99724080e+00
  1.59028364e+00  4.68912614e-01  1.16453128e-02  8.53746454e-01
 -1.44205029e+00 -5.14397748e-01 -2.64964357e+00 -2.20438842e+00
 -2.15094645e+00 -9.51445577e-01 -2.39984679e+00 -1.81667019e+00
 -9.73211810e-01 -8.33144380e-01 -1.72310398e+00 -9.12796096e-01
  4.64539872e-01  1.45505607e+00  1.54221940e+00  5.08448978e-01
 -1.09100269e+00 -6.34825276e-01 -1.38763632e+00 -1.20551104e+00
 -2.52046578e+00 -1.43637271e+00 -1.29904918e+00 -1.07246301e+00
 -5.99213561e-01 -1.09645976e+00  2.42319867e-01  3.31708066e-01
  4.35265874e-01  1.12037547e+00 -1.78777322e-01  3.87994358e-01
 -1.46262299e+00 -5.76991991e-01 -4.39317934e-01 -1.59476909e+00
  6.15499332e-01  1.16185689e+00  5.26526616e-01  1.68121876e+00
  1.28409529e+00 -4.78535643e-01 -1.29232519e-01 -2.19426273e+00
  2.09373543e+00  1.58998232e+00 -7.24735271e-02  1.08640235e+00
  2.50164377e+01]
(65,)
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.779262206714
gradient value of function right now is: [ 2.16366524e-08  1.89438245e-09  2.16242000e-08  2.16297266e-08
 -2.16369599e-08 -1.89440954e-09 -2.16245073e-08 -2.16300340e-08
  3.94276886e-10  4.82372021e-11  3.87697993e-10  4.13278763e-10
  2.05515317e-09  2.51588063e-10  2.02105193e-09  2.15429497e-09
  3.88271643e-10  4.75038447e-11  3.81793920e-10  4.06984858e-10
  3.76760619e-10  4.60944459e-11  3.70474492e-10  3.94918490e-10
 -5.52577448e-11 -1.77082378e-11 -6.34080695e-11 -1.11801743e-11
 -3.93327482e-12 -9.98279207e-13 -4.19061658e-12 -7.43387705e-13
  1.82588789e-10  6.13272514e-11  2.13044324e-10  3.74925443e-11
  1.38096924e-11  5.04591293e-12  1.66237820e-11  2.91684298e-12
  2.13897812e-11  3.38478505e-11  7.82403571e-12  1.72513342e-11
  1.09116703e-11  1.70712567e-11  3.77730525e-12  8.29872913e-12
  1.58910097e-11  2.53739862e-11  4.35292339e-12  1.03257734e-11
  1.60028538e-11  2.49300070e-11  4.12933304e-12  9.66814080e-12
  5.54415712e-12  9.96976910e-12  1.01335001e-12  2.24025714e-14
 -5.70976231e-13  8.98870218e-14 -2.48970763e-12 -1.49980679e-12
  9.92891621e-01]
supnorm grad right now is: 0.9928916210980924
Weights right now are: 
[-4.65054370e+00 -6.97688983e-01 -3.48361822e+00 -4.31305447e+00
  4.90511247e+00  5.88997297e-01  3.88255554e+00  3.59702615e+00
 -2.27016828e+00 -9.10606854e-01 -2.25166338e+00 -1.99820503e+00
  1.58551413e+00  4.68328929e-01  6.95528102e-03  8.48747027e-01
 -1.44295476e+00 -5.14508369e-01 -2.65053288e+00 -2.20533644e+00
 -2.15182448e+00 -9.51552962e-01 -2.40071010e+00 -1.81759051e+00
 -9.73084299e-01 -8.33103408e-01 -1.72295751e+00 -9.12770281e-01
  4.64548353e-01  1.45505818e+00  1.54222838e+00  5.08450572e-01
 -1.09142730e+00 -6.34968384e-01 -1.38813242e+00 -1.20559830e+00
 -2.52049808e+00 -1.43638456e+00 -1.29908812e+00 -1.07246983e+00
 -5.99263444e-01 -1.09653853e+00  2.42301342e-01  3.31667509e-01
  4.35240482e-01  1.12033583e+00 -1.78786258e-01  3.87974872e-01
 -1.46265998e+00 -5.77050977e-01 -4.39328202e-01 -1.59479330e+00
  6.15462085e-01  1.16179894e+00  5.26516868e-01  1.68119608e+00
  1.28408235e+00 -4.78558706e-01 -1.29234899e-01 -2.19426277e+00
  2.09373677e+00  1.58998208e+00 -7.24677036e-02  1.08640577e+00
  2.50737102e+01]
(65,)
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.251991269451
gradient value of function right now is: [ 1.76791771e-08  1.56351452e-09  1.76690100e-08  1.76735186e-08
 -1.76786402e-08 -1.56346648e-09 -1.76684734e-08 -1.76729818e-08
  3.24518918e-10  3.96313483e-11  3.19077202e-10  3.40121595e-10
  1.70324300e-09  2.08153744e-10  1.67485146e-09  1.78522523e-09
  3.20324863e-10  3.91203875e-11  3.14954344e-10  3.35726590e-10
  3.10639916e-10  3.79366387e-11  3.05431352e-10  3.25575468e-10
 -4.58225808e-11 -1.49196425e-11 -5.28953507e-11 -9.30464503e-12
 -3.53193036e-12 -9.18858364e-13 -3.79016410e-12 -6.72047109e-13
  1.50256745e-10  5.14107645e-11  1.76589370e-10  3.09845602e-11
  1.13606047e-11  4.24880055e-12  1.38083349e-11  2.41278835e-12
  1.76226941e-11  2.76112657e-11  7.05064392e-12  1.48399436e-11
  9.00901738e-12  1.39583129e-11  3.42367664e-12  7.16894861e-12
  1.29908831e-11  2.06457940e-11  3.83650359e-12  8.76242812e-12
  1.30748849e-11  2.02732958e-11  3.66123229e-12  8.23084162e-12
  4.57633599e-12  7.90584820e-12  8.37987719e-13 -4.54288858e-15
 -4.82827612e-13  1.12550830e-13 -2.08080409e-12 -1.10856497e-12
 -3.79070817e+00]
supnorm grad right now is: 3.7907081683907133
Weights right now are: 
[-4.68917201e+00 -7.01087592e-01 -3.52222431e+00 -4.35167042e+00
  4.94374079e+00  5.92395907e-01  3.92116164e+00  3.63564211e+00
 -2.27087456e+00 -9.10693214e-01 -2.25235786e+00 -1.99894532e+00
  1.58181938e+00  4.67876859e-01  3.32187317e-03  8.44874143e-01
 -1.44365109e+00 -5.14593517e-01 -2.65121758e+00 -2.20606631e+00
 -2.15249996e+00 -9.51635558e-01 -2.40137430e+00 -1.81829852e+00
 -9.72984805e-01 -8.33071369e-01 -1.72284314e+00 -9.12750131e-01
  4.64555743e-01  1.45506008e+00  1.54223628e+00  5.08451973e-01
 -1.09175462e+00 -6.35078919e-01 -1.38851515e+00 -1.20566559e+00
 -2.52052280e+00 -1.43639366e+00 -1.29911797e+00 -1.07247506e+00
 -5.99301780e-01 -1.09659904e+00  2.42286947e-01  3.31636185e-01
  4.35220906e-01  1.12030527e+00 -1.78793221e-01  3.87959780e-01
 -1.46268839e+00 -5.77096282e-01 -4.39336170e-01 -1.59481198e+00
  6.15433481e-01  1.16175443e+00  5.26509298e-01  1.68117857e+00
  1.28407242e+00 -4.78576380e-01 -1.29236715e-01 -2.19426280e+00
  2.09373780e+00  1.58998190e+00 -7.24632213e-02  1.08640839e+00
  2.47189850e+01]
(65,)
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.750334752388
gradient value of function right now is: [ 1.43945776e-08  1.28282863e-09  1.43863199e-08  1.43899796e-08
 -1.43942597e-08 -1.28279987e-09 -1.43860022e-08 -1.43896619e-08
  2.65298664e-10  3.24287677e-11  2.60885801e-10  2.78075604e-10
  1.40253992e-09  1.71552524e-10  1.37934572e-09  1.47015704e-09
  2.62371292e-10  3.20718965e-11  2.58007803e-10  2.75007787e-10
  2.54308459e-10  3.10855613e-11  2.50078740e-10  2.66556233e-10
 -3.80876357e-11 -1.23022841e-11 -4.38331749e-11 -7.72041235e-12
 -3.17317277e-12 -8.40858223e-13 -3.42426863e-12 -6.06821379e-13
  1.23490592e-10  4.18413205e-11  1.44575779e-10  2.54088146e-11
  9.24552698e-12  3.41728289e-12  1.11821350e-11  1.95830401e-12
  1.44087543e-11  2.26950554e-11  5.50320081e-12  1.18644616e-11
  7.39050804e-12  1.15097401e-11  2.67199827e-12  5.73711920e-12
  1.06754531e-11  1.70085420e-11  3.03107325e-12  7.06076370e-12
  1.07510926e-11  1.67119424e-11  2.88525823e-12  6.62435121e-12
  3.73718065e-12  6.59327804e-12  6.78838729e-13  4.57933150e-15
 -3.89309226e-13  7.56026868e-14 -1.69240509e-12 -9.63718947e-13
 -1.32135430e+00]
supnorm grad right now is: 1.3213542964297014
Weights right now are: 
[-4.72058974e+00 -7.03875925e-01 -3.55362401e+00 -4.38307811e+00
  4.97515850e+00  5.95184239e-01  3.95256133e+00  3.66704979e+00
 -2.27145196e+00 -9.10763818e-01 -2.25292567e+00 -1.99955054e+00
  1.57877602e+00  4.67504488e-01  3.28803795e-04  8.41684006e-01
 -1.44422158e+00 -5.14663277e-01 -2.65177859e+00 -2.20666429e+00
 -2.15305306e+00 -9.51703190e-01 -2.40191821e+00 -1.81887827e+00
 -9.72902257e-01 -8.33044832e-01 -1.72274829e+00 -9.12733419e-01
  4.64562413e-01  1.45506183e+00  1.54224346e+00  5.08453245e-01
 -1.09202323e+00 -6.35169442e-01 -1.38882901e+00 -1.20572078e+00
 -2.52054293e+00 -1.43640105e+00 -1.29914225e+00 -1.07247932e+00
 -5.99333159e-01 -1.09664852e+00  2.42275279e-01  3.31610702e-01
  4.35204823e-01  1.12028020e+00 -1.78798872e-01  3.87947478e-01
 -1.46271168e+00 -5.77133377e-01 -4.39342640e-01 -1.59482720e+00
  6.15410008e-01  1.16171796e+00  5.26503148e-01  1.68116430e+00
  1.28406431e+00 -4.78590842e-01 -1.29238190e-01 -2.19426282e+00
  2.09373865e+00  1.58998175e+00 -7.24595582e-02  1.08641053e+00
  2.48855662e+01]
(65,)
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.811798328015
gradient value of function right now is: [ 1.22007213e-08  1.09451173e-09  1.21937345e-08  1.21968294e-08
 -1.22003741e-08 -1.09448014e-09 -1.21933875e-08 -1.21964823e-08
  2.25692587e-10  2.75969773e-11  2.21956067e-10  2.36570381e-10
  1.20022914e-09  1.46854313e-10  1.18047266e-09  1.25813531e-09
  2.23553137e-10  2.73361690e-11  2.19852608e-10  2.34328264e-10
  2.16593311e-10  2.64844952e-11  2.13007722e-10  2.27032650e-10
 -3.28037390e-11 -1.05634810e-11 -3.77085659e-11 -6.64511218e-12
 -2.89783931e-12 -7.77666864e-13 -3.13916663e-12 -5.56153251e-13
  1.05422860e-10  3.55839690e-11  1.23239019e-10  2.16729096e-11
  7.83852921e-12  2.88458233e-12  9.46302515e-12  1.65866296e-12
  1.22581271e-11  1.93469440e-11  4.59744431e-12  1.00061966e-11
  6.30393524e-12  9.83681208e-12  2.23424830e-12  4.84517361e-12
  9.10319743e-12  1.45174032e-11  2.54481983e-12  5.97422605e-12
  9.17112333e-12  1.42695669e-11  2.42014588e-12  5.60301038e-12
  3.17790474e-12  5.65165047e-12  5.74002134e-13  6.24483917e-15
 -3.29322570e-13  5.90118195e-14 -1.43768675e-12 -8.39443253e-13
 -3.12803248e-01]
supnorm grad right now is: 0.31280324845688806
Weights right now are: 
[-4.74691819e+00 -7.06229684e-01 -3.57993737e+00 -4.40939816e+00
  5.00148695e+00  5.97537997e-01  3.97887469e+00  3.69336983e+00
 -2.27193792e+00 -9.10823255e-01 -2.25340358e+00 -2.00005993e+00
  1.57619866e+00  4.67189060e-01 -2.20612976e-03  8.38982261e-01
 -1.44470256e+00 -5.14722106e-01 -2.65225161e+00 -2.20716846e+00
 -2.15351916e+00 -9.51760198e-01 -2.40237659e+00 -1.81936684e+00
 -9.72831930e-01 -8.33022239e-01 -1.72266752e+00 -9.12719181e-01
  4.64568465e-01  1.45506344e+00  1.54225001e+00  5.08454404e-01
 -1.09225006e+00 -6.35245790e-01 -1.38909388e+00 -1.20576738e+00
 -2.52055983e+00 -1.43640724e+00 -1.29916261e+00 -1.07248289e+00
 -5.99359559e-01 -1.09669024e+00  2.42265532e-01  3.31589304e-01
  4.35191262e-01  1.12025901e+00 -1.78803600e-01  3.87937131e-01
 -1.46273130e+00 -5.77164679e-01 -4.39348060e-01 -1.59484000e+00
  6.15390242e-01  1.16168719e+00  5.26497997e-01  1.68115230e+00
  1.28405747e+00 -4.78603068e-01 -1.29239427e-01 -2.19426284e+00
  2.09373935e+00  1.58998163e+00 -7.24564679e-02  1.08641236e+00
  2.49766921e+01]
(65,)
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.784200298248
gradient value of function right now is: [ 1.06752884e-08  9.63328871e-10  1.06691820e-08  1.06718857e-08
 -1.06757860e-08 -9.63374552e-10 -1.06696793e-08 -1.06723832e-08
  1.98189740e-10  2.42281950e-11  1.94912921e-10  2.07740604e-10
  1.05909449e-09  1.29556325e-10  1.04168549e-09  1.11018519e-09
  1.96572945e-10  2.40312560e-11  1.93323363e-10  2.06046294e-10
  1.90385868e-10  2.32743240e-11  1.87238329e-10  1.99560754e-10
 -2.90529837e-11 -9.37590555e-12 -3.34236568e-11 -5.88838973e-12
 -2.68456214e-12 -7.28079050e-13 -2.91754053e-12 -5.16726978e-13
  9.27445029e-11  3.13780011e-11  1.08515854e-10  1.90771993e-11
  6.86799891e-12  2.53538482e-12  8.30199096e-12  1.45446314e-12
  1.07657491e-11  1.69709994e-11  4.08366866e-12  8.83747183e-12
  5.54705233e-12  8.64551705e-12  1.98809438e-12  4.28652906e-12
  7.98982464e-12  1.27345859e-11  2.25483374e-12  5.26900489e-12
  8.05016806e-12  1.25183568e-11  2.14649686e-12  4.94466464e-12
  2.79163494e-12  4.93848754e-12  5.03082925e-13  3.20493410e-15
 -2.90245683e-13  5.48491357e-14 -1.26643064e-12 -7.28206541e-13
 -9.23110471e-01]
supnorm grad right now is: 0.9231104712040933
Weights right now are: 
[-4.76971194e+00 -7.08280339e-01 -3.60271808e+00 -4.43218464e+00
  5.02428069e+00  5.99588652e-01  4.00165538e+00  3.71615630e+00
 -2.27236025e+00 -9.10874895e-01 -2.25381893e+00 -2.00050262e+00
  1.57394701e+00  4.66913568e-01 -4.42076418e-03  8.36621981e-01
 -1.44512116e+00 -5.14773292e-01 -2.65266330e+00 -2.20760724e+00
 -2.15392466e+00 -9.51809780e-01 -2.40277539e+00 -1.81979189e+00
 -9.72770239e-01 -8.33002386e-01 -1.72259662e+00 -9.12706689e-01
  4.64574048e-01  1.45506495e+00  1.54225606e+00  5.08455477e-01
 -1.09244758e+00 -6.35312412e-01 -1.38932474e+00 -1.20580797e+00
 -2.52057447e+00 -1.43641263e+00 -1.29918029e+00 -1.07248599e+00
 -5.99382507e-01 -1.09672644e+00  2.42256933e-01  3.31570609e-01
  4.35179446e-01  1.12024059e+00 -1.78807783e-01  3.87928070e-01
 -1.46274835e+00 -5.77191852e-01 -4.39352819e-01 -1.59485116e+00
  6.15373060e-01  1.16166047e+00  5.26493470e-01  1.68114183e+00
  1.28405154e+00 -4.78613650e-01 -1.29240494e-01 -2.19426285e+00
  2.09373997e+00  1.58998152e+00 -7.24537846e-02  1.08641394e+00
  2.49117421e+01]
(65,)
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.797350339843
gradient value of function right now is: [ 9.36346199e-09  8.49201215e-10  9.35811455e-09  9.36048132e-09
 -9.36309859e-09 -8.49167931e-10 -9.35775136e-09 -9.36011804e-09
  1.74262722e-10  2.13152174e-11  1.71396543e-10  1.82669352e-10
  9.35738361e-10  1.14527506e-10  9.20435932e-10  9.80923562e-10
  1.73043502e-10  2.11666896e-11  1.70197810e-10  1.81391654e-10
  1.67546153e-10  2.04937805e-11  1.64790663e-10  1.75628847e-10
 -2.58278870e-11 -8.29263345e-12 -2.96563301e-11 -5.22879743e-12
 -2.49107290e-12 -6.80231705e-13 -2.71292498e-12 -4.80464095e-13
  8.18254409e-11  2.75121534e-11  9.55087368e-11  1.68071924e-11
  6.01803038e-12  2.20509849e-12  7.25205853e-12  1.27225167e-12
  9.46537438e-12  1.49684607e-11  3.48383992e-12  7.65774177e-12
  4.88828888e-12  7.64213859e-12  1.69624230e-12  3.71724283e-12
  7.04765178e-12  1.12492482e-11  1.93891740e-12  4.58842102e-12
  7.10421430e-12  1.10634607e-11  1.84267928e-12  4.30286589e-12
  2.45164852e-12  4.39543931e-12  4.39181776e-13  6.13263497e-15
 -2.52786587e-13  4.13702197e-14 -1.10927442e-12 -6.64133659e-13
  6.95520038e-01]
supnorm grad right now is: 0.6955200376035479
Weights right now are: 
[-4.78972345e+00 -7.10090645e-01 -3.62271815e+00 -4.45218978e+00
  5.04429220e+00  6.01398958e-01  4.02165546e+00  3.73616144e+00
 -2.27273222e+00 -9.10920379e-01 -2.25418477e+00 -2.00089253e+00
  1.57195451e+00  4.66669769e-01 -6.38059719e-03  8.34533318e-01
 -1.44549031e+00 -5.14818433e-01 -2.65302636e+00 -2.20799419e+00
 -2.15428214e+00 -9.51853493e-01 -2.40312698e+00 -1.82016660e+00
 -9.72715413e-01 -8.32984739e-01 -1.72253360e+00 -9.12695583e-01
  4.64579225e-01  1.45506635e+00  1.54226169e+00  5.08456475e-01
 -1.09262193e+00 -6.35371229e-01 -1.38952850e+00 -1.20584381e+00
 -2.52058734e+00 -1.43641736e+00 -1.29919582e+00 -1.07248871e+00
 -5.99402705e-01 -1.09675832e+00  2.42249364e-01  3.31554136e-01
  4.35169025e-01  1.12022433e+00 -1.78811471e-01  3.87920074e-01
 -1.46276337e+00 -5.77215806e-01 -4.39357009e-01 -1.59486100e+00
  6.15357920e-01  1.16163692e+00  5.26489485e-01  1.68113260e+00
  1.28404630e+00 -4.78622973e-01 -1.29241433e-01 -2.19426286e+00
  2.09374051e+00  1.58998142e+00 -7.24514139e-02  1.08641533e+00
  2.50809614e+01]
(65,)
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.8057396481845
gradient value of function right now is: [ 8.47393608e-09  7.72180704e-10  8.46909997e-09  8.47123963e-09
 -8.47430513e-09 -7.72214876e-10 -8.46946879e-09 -8.47160855e-09
  1.58184174e-10  1.93398349e-11  1.55581620e-10  1.65811267e-10
  8.52568045e-10  1.04303780e-10  8.38622748e-10  8.93717720e-10
  1.57237189e-10  1.92246179e-11  1.54650617e-10  1.64818938e-10
  1.52201263e-10  1.86084601e-11  1.49697346e-10  1.59539955e-10
 -2.35795810e-11 -7.60171918e-12 -2.71158144e-11 -4.77817781e-12
 -2.34626760e-12 -6.45663953e-13 -2.56134733e-12 -4.53496010e-13
  7.43374748e-11  2.51115507e-11  8.69252700e-11  1.52858226e-11
  5.45446130e-12  2.01070486e-12  6.58924340e-12  1.15480363e-12
  8.59196973e-12  1.35556089e-11  3.23594458e-12  7.02940050e-12
  4.44326294e-12  6.93062513e-12  1.57873837e-12  3.41789146e-12
  6.38624528e-12  1.01826376e-11  1.79103122e-12  4.19818892e-12
  6.43727609e-12  1.00141274e-11  1.70491923e-12  3.94054554e-12
  2.22718246e-12  3.95203145e-12  3.98602384e-13  2.67501308e-15
 -2.31085784e-13  4.23570663e-14 -1.01156563e-12 -5.87767656e-13
 -5.20776701e-01]
supnorm grad right now is: 0.520776700696919
Weights right now are: 
[-4.80755259e+00 -7.11711434e-01 -3.64053711e+00 -4.47001324e+00
  5.06212135e+00  6.03019748e-01  4.03947443e+00  3.75398492e+00
 -2.27306455e+00 -9.10961016e-01 -2.25451162e+00 -2.00124087e+00
  1.57016680e+00  4.66451025e-01 -8.13906697e-03  8.32659306e-01
 -1.44582048e+00 -5.14858807e-01 -2.65335110e+00 -2.20834028e+00
 -2.15460177e+00 -9.51892578e-01 -2.40344135e+00 -1.82050165e+00
 -9.72666028e-01 -8.32968845e-01 -1.72247685e+00 -9.12685580e-01
  4.64584062e-01  1.45506768e+00  1.54226697e+00  5.08457409e-01
 -1.09277802e+00 -6.35423857e-01 -1.38971089e+00 -1.20587589e+00
 -2.52059881e+00 -1.43642158e+00 -1.29920966e+00 -1.07249114e+00
 -5.99420759e-01 -1.09678682e+00  2.42242623e-01  3.31539437e-01
  4.35159695e-01  1.12020976e+00 -1.78814756e-01  3.87912932e-01
 -1.46277680e+00 -5.77237220e-01 -4.39360747e-01 -1.59486979e+00
  6.15344386e-01  1.16161586e+00  5.26485929e-01  1.68112435e+00
  1.28404163e+00 -4.78631314e-01 -1.29242270e-01 -2.19426287e+00
  2.09374099e+00  1.58998134e+00 -7.24492945e-02  1.08641657e+00
  2.49304323e+01]
(65,)
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.707487857534
gradient value of function right now is: [ 7.74419294e-09  7.08745469e-10  7.73977602e-09  7.74172955e-09
 -7.74420340e-09 -7.08746601e-10 -7.73978647e-09 -7.74174001e-09
  1.44949022e-10  1.77138219e-11  1.42563298e-10  1.51934418e-10
  7.83902801e-10  9.58630245e-11  7.71076892e-10  8.21720514e-10
  1.44212662e-10  1.76243629e-11  1.41839434e-10  1.51162869e-10
  1.39561648e-10  1.70555462e-11  1.37264782e-10  1.46287486e-10
 -2.17194064e-11 -7.02931497e-12 -2.50131233e-11 -4.40511762e-12
 -2.22147818e-12 -6.15616837e-13 -2.43044165e-12 -4.30187252e-13
  6.81661254e-11  2.31290142e-11  7.98465695e-11  1.40309594e-11
  4.99076479e-12  1.85022905e-12  6.04325661e-12  1.05805716e-12
  7.87417839e-12  1.23926747e-11  3.03034926e-12  6.50883874e-12
  4.07708451e-12  6.34432573e-12  1.48107452e-12  3.16952282e-12
  5.84238394e-12  9.30452151e-12  1.66837498e-12  3.87494274e-12
  5.88901712e-12  9.15049011e-12  1.59079187e-12  3.64059900e-12
  2.04146433e-12  3.58787826e-12  3.65018007e-13 -1.97943018e-16
 -2.13084065e-13  4.29282543e-14 -9.30562906e-13 -5.25189452e-13
 -1.68417384e+00]
supnorm grad right now is: 1.6841738436200768
Weights right now are: 
[-4.82372302e+00 -7.13187916e-01 -3.65669832e+00 -4.48617853e+00
  5.07829178e+00  6.04496229e-01  4.05563564e+00  3.77015020e+00
 -2.27336667e+00 -9.10997962e-01 -2.25480879e+00 -2.00155757e+00
  1.56853512e+00  4.66251366e-01 -9.74412519e-03  8.30948836e-01
 -1.44612093e+00 -5.14895550e-01 -2.65364662e+00 -2.20865523e+00
 -2.15489257e+00 -9.51928139e-01 -2.40372737e+00 -1.82080647e+00
 -9.72620780e-01 -8.32954293e-01 -1.72242486e+00 -9.12676413e-01
  4.64588641e-01  1.45506894e+00  1.54227197e+00  5.08458294e-01
 -1.09292021e+00 -6.35471742e-01 -1.38987695e+00 -1.20590512e+00
 -2.52060921e+00 -1.43642540e+00 -1.29922221e+00 -1.07249334e+00
 -5.99437181e-01 -1.09681274e+00  2.42236541e-01  3.31526116e-01
  4.35151192e-01  1.12019650e+00 -1.78817721e-01  3.87906457e-01
 -1.46278902e+00 -5.77256704e-01 -4.39364127e-01 -1.59487777e+00
  6.15332058e-01  1.16159669e+00  5.26482712e-01  1.68111687e+00
  1.28403737e+00 -4.78638904e-01 -1.29243030e-01 -2.19426287e+00
  2.09374143e+00  1.58998126e+00 -7.24473647e-02  1.08641771e+00
  2.48461988e+01]
(65,)
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.620772565123
gradient value of function right now is: [ 7.11638235e-09  6.53829438e-10  7.11232632e-09  7.11411970e-09
 -7.11615136e-09 -6.53807725e-10 -7.11209547e-09 -7.11388879e-09
  1.33501593e-10  1.63111482e-11  1.31305486e-10  1.39934047e-10
  7.24324796e-10  8.85582105e-11  7.12480813e-10  7.59262082e-10
  1.32932467e-10  1.62421066e-11  1.30746073e-10  1.39337778e-10
  1.28618388e-10  1.57146163e-11  1.26502782e-10  1.34815624e-10
 -2.01144760e-11 -6.52285479e-12 -2.31820140e-11 -4.08151770e-12
 -2.10997116e-12 -5.88033046e-13 -2.31254690e-12 -4.09236170e-13
  6.28480893e-11  2.13715718e-11  7.36803735e-11  1.29429394e-11
  4.58886440e-12  1.70619023e-12  5.56329601e-12  9.73543040e-13
  7.25240871e-12  1.14000546e-11  2.82100517e-12  6.02570774e-12
  3.75989553e-12  5.84376340e-12  1.38057284e-12  2.93782364e-12
  5.37715147e-12  8.55845425e-12  1.54924619e-12  3.58200439e-12
  5.42051411e-12  8.41747895e-12  1.47850431e-12  3.36725103e-12
  1.88031162e-12  3.28887630e-12  3.35649245e-13 -1.54389649e-15
 -1.96849252e-13  4.14228017e-14 -8.58977274e-13 -4.77792021e-13
 -2.25658445e+00]
supnorm grad right now is: 2.2565844516026794
Weights right now are: 
[-4.83834567e+00 -7.14528511e-01 -3.67131264e+00 -4.50079653e+00
  5.09291442e+00  6.05836824e-01  4.07024995e+00  3.78476820e+00
 -2.27364052e+00 -9.11031448e-01 -2.25507815e+00 -2.00184463e+00
  1.56705106e+00  4.66069780e-01 -1.12039924e-02  8.29393120e-01
 -1.44639350e+00 -5.14928881e-01 -2.65391472e+00 -2.20894095e+00
 -2.15515632e+00 -9.51960391e-01 -2.40398680e+00 -1.82108295e+00
 -9.72579507e-01 -8.32941008e-01 -1.72237743e+00 -9.12668054e-01
  4.64592932e-01  1.45507013e+00  1.54227667e+00  5.08459126e-01
 -1.09304927e+00 -6.35515246e-01 -1.39002775e+00 -1.20593164e+00
 -2.52061862e+00 -1.43642886e+00 -1.29923356e+00 -1.07249533e+00
 -5.99452055e-01 -1.09683623e+00  2.42231001e-01  3.31514006e-01
  4.35143482e-01  1.12018446e+00 -1.78820426e-01  3.87900561e-01
 -1.46280009e+00 -5.77274360e-01 -4.39367204e-01 -1.59488501e+00
  6.15320895e-01  1.16157932e+00  5.26479784e-01  1.68111006e+00
  1.28403352e+00 -4.78645762e-01 -1.29243717e-01 -2.19426288e+00
  2.09374183e+00  1.58998119e+00 -7.24456133e-02  1.08641874e+00
  2.47690702e+01]
(65,)
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.807274838279
gradient value of function right now is: [ 6.44552517e-09  5.94161349e-10  6.44185724e-09  6.44347865e-09
 -6.44500627e-09 -5.94112973e-10 -6.44133865e-09 -6.44295993e-09
  1.21055685e-10  1.48051231e-11  1.19077574e-10  1.26897965e-10
  6.59270740e-10  8.06796271e-11  6.48560434e-10  6.91119303e-10
  1.20632964e-10  1.47538464e-11  1.18662063e-10  1.26455080e-10
  1.16694268e-10  1.42717994e-11  1.14787576e-10  1.22326117e-10
 -1.84262339e-11 -5.92317527e-12 -2.11662005e-11 -3.73165098e-12
 -1.98988463e-12 -5.55142539e-13 -2.18153445e-12 -3.86162576e-13
  5.71999985e-11  1.92483593e-11  6.67854559e-11  1.17517152e-11
  4.14596234e-12  1.52194013e-12  4.99966580e-12  8.76971122e-13
  6.57373023e-12  1.03912436e-11  2.43063554e-12  5.33081258e-12
  3.41517090e-12  5.33670508e-12  1.18840655e-12  2.59917000e-12
  4.89917067e-12  7.81826233e-12  1.35247581e-12  3.19504442e-12
  4.94160579e-12  7.69405266e-12  1.28677413e-12  2.99905224e-12
  1.70224230e-12  3.04488753e-12  3.01841829e-13  2.85530938e-15
 -1.75753214e-13  2.95182314e-14 -7.73532527e-13 -4.60869332e-13
  4.69095493e-01]
supnorm grad right now is: 0.4690954931753015
Weights right now are: 
[-4.85183625e+00 -7.15769910e-01 -3.68479553e+00 -4.51428283e+00
  5.10640499e+00  6.07078221e-01  4.08373283e+00  3.79825448e+00
 -2.27389368e+00 -9.11062408e-01 -2.25532717e+00 -2.00211001e+00
  1.56567466e+00  4.65901351e-01 -1.25580071e-02  8.27950234e-01
 -1.44664568e+00 -5.14959721e-01 -2.65416278e+00 -2.20920530e+00
 -2.15540029e+00 -9.51990226e-01 -2.40422678e+00 -1.82133869e+00
 -9.72541111e-01 -8.32928650e-01 -1.72233330e+00 -9.12660273e-01
  4.64597026e-01  1.45507127e+00  1.54228115e+00  5.08459920e-01
 -1.09316877e+00 -6.35555518e-01 -1.39016734e+00 -1.20595621e+00
 -2.52062730e+00 -1.43643205e+00 -1.29924404e+00 -1.07249717e+00
 -5.99465796e-01 -1.09685795e+00  2.42225905e-01  3.31502820e-01
  4.35136349e-01  1.12017331e+00 -1.78822916e-01  3.87895110e-01
 -1.46281033e+00 -5.77290696e-01 -4.39370041e-01 -1.59489172e+00
  6.15310571e-01  1.16156324e+00  5.26477085e-01  1.68110377e+00
  1.28402996e+00 -4.78652119e-01 -1.29244352e-01 -2.19426288e+00
  2.09374220e+00  1.58998113e+00 -7.24439891e-02  1.08641969e+00
  2.50198075e+01]
(65,)
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.696767782062
gradient value of function right now is: [ 6.06641595e-09  5.61172800e-10  6.06296412e-09  6.06448958e-09
 -6.06612810e-09 -5.61145565e-10 -6.06267645e-09 -6.06420183e-09
  1.14214745e-10  1.39568837e-11  1.12343131e-10  1.19720664e-10
  6.23495587e-10  7.62418124e-11  6.13339482e-10  6.53584095e-10
  1.13892942e-10  1.39179785e-11  1.12026901e-10  1.19383585e-10
  1.10155447e-10  1.34609236e-11  1.08350499e-10  1.15465738e-10
 -1.74206447e-11 -5.64135427e-12 -2.00664749e-11 -3.53388697e-12
 -1.91389726e-12 -5.37471440e-13 -2.10265869e-12 -3.72035779e-13
  5.39373820e-11  1.83066609e-11  6.31864203e-11  1.11031509e-11
  3.90991285e-12  1.45086478e-12  4.73614965e-12  8.29155859e-13
  6.20313838e-12  9.76061870e-12  2.39161706e-12  5.13216355e-12
  3.22460040e-12  5.01664285e-12  1.17204377e-12  2.50638944e-12
  4.60579540e-12  7.33422649e-12  1.31689377e-12  3.05640902e-12
  4.64472409e-12  7.21620710e-12  1.25654057e-12  2.87338481e-12
  1.60764857e-12  2.82281218e-12  2.85438454e-13 -9.55038850e-16
 -1.67904353e-13  3.41299906e-14 -7.34846709e-13 -4.14033639e-13
 -1.76724174e+00]
supnorm grad right now is: 1.7672417449924365
Weights right now are: 
[-4.86426219e+00 -7.16917267e-01 -3.69721441e+00 -4.52670483e+00
  5.11883093e+00  6.08225579e-01  4.09615171e+00  3.81067648e+00
 -2.27412730e+00 -9.11090978e-01 -2.25555697e+00 -2.00235490e+00
  1.56440067e+00  4.65745452e-01 -1.38113036e-02  8.26614699e-01
 -1.44687857e+00 -5.14988202e-01 -2.65439186e+00 -2.20944942e+00
 -2.15562555e+00 -9.52017774e-01 -2.40444836e+00 -1.82157482e+00
 -9.72505472e-01 -8.32917181e-01 -1.72229235e+00 -9.12653053e-01
  4.64600913e-01  1.45507236e+00  1.54228542e+00  5.08460675e-01
 -1.09327921e+00 -6.35592725e-01 -1.39029634e+00 -1.20597891e+00
 -2.52063530e+00 -1.43643499e+00 -1.29925369e+00 -1.07249886e+00
 -5.99478478e-01 -1.09687800e+00  2.42221192e-01  3.31492501e-01
  4.35129758e-01  1.12016301e+00 -1.78825222e-01  3.87890076e-01
 -1.46281978e+00 -5.77305778e-01 -4.39372660e-01 -1.59489790e+00
  6.15301044e-01  1.16154840e+00  5.26474592e-01  1.68109797e+00
  1.28402667e+00 -4.78657990e-01 -1.29244936e-01 -2.19426289e+00
  2.09374254e+00  1.58998107e+00 -7.24424928e-02  1.08642058e+00
  2.47932565e+01]
(65,)
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.798208747015
gradient value of function right now is: [ 5.56856811e-09  5.16598286e-10  5.56540393e-09  5.56680199e-09
 -5.56875614e-09 -5.16616170e-10 -5.56559184e-09 -5.56698995e-09
  1.04947555e-10  1.28357285e-11  1.03237983e-10  1.10014066e-10
  5.74789396e-10  7.03442757e-11  5.65480650e-10  6.02565541e-10
  1.04720778e-10  1.28083571e-11  1.03015162e-10  1.09776546e-10
  1.01265926e-10  1.23855093e-11  9.96164605e-11  1.06154749e-10
 -1.61510251e-11 -5.18939502e-12 -1.85492114e-11 -3.27064655e-12
 -1.81763077e-12 -5.10503421e-13 -1.99687246e-12 -3.53429242e-13
  4.97226306e-11  1.67192638e-11  5.80372115e-11  1.02138725e-11
  3.58092818e-12  1.31368924e-12  4.31706248e-12  7.57383664e-13
  5.69774279e-12  9.01015560e-12  2.09899000e-12  4.61275395e-12
  2.96737612e-12  4.63866837e-12  1.02774732e-12  2.25280302e-12
  4.24999707e-12  6.78350186e-12  1.16948967e-12  2.76734542e-12
  4.28821438e-12  6.67792934e-12  1.11281486e-12  2.59817657e-12
  1.47499431e-12  2.64211099e-12  2.60392637e-13  2.36195408e-15
 -1.52148932e-13  2.50973438e-14 -6.71059396e-13 -4.01846490e-13
  6.77883027e-01]
supnorm grad right now is: 0.677883026801153
Weights right now are: 
[-4.87584214e+00 -7.17989934e-01 -3.70878777e+00 -4.53828110e+00
  5.13041089e+00  6.09298247e-01  4.10772509e+00  3.82225277e+00
 -2.27434539e+00 -9.11117647e-01 -2.25577151e+00 -2.00258352e+00
  1.56320800e+00  4.65599515e-01 -1.49846158e-02  8.25364421e-01
 -1.44709612e+00 -5.15014806e-01 -2.65460586e+00 -2.20967747e+00
 -2.15583594e+00 -9.52043502e-01 -2.40465532e+00 -1.82179536e+00
 -9.72472028e-01 -8.32906414e-01 -1.72225391e+00 -9.12646277e-01
  4.64604635e-01  1.45507341e+00  1.54228950e+00  5.08461398e-01
 -1.09338242e+00 -6.35627515e-01 -1.39041692e+00 -1.20600012e+00
 -2.52064275e+00 -1.43643773e+00 -1.29926268e+00 -1.07250044e+00
 -5.99490316e-01 -1.09689671e+00  2.42216783e-01  3.31482859e-01
  4.35123598e-01  1.12015339e+00 -1.78827381e-01  3.87885368e-01
 -1.46282860e+00 -5.77319854e-01 -4.39375111e-01 -1.59490367e+00
  6.15292146e-01  1.16153455e+00  5.26472259e-01  1.68109255e+00
  1.28402360e+00 -4.78663455e-01 -1.29245479e-01 -2.19426289e+00
  2.09374286e+00  1.58998101e+00 -7.24410948e-02  1.08642140e+00
  2.50650285e+01]
(65,)
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.811531041949
gradient value of function right now is: [ 5.24930671e-09  4.88437687e-10  5.24632502e-09  5.24764214e-09
 -5.24958862e-09 -4.88464376e-10 -5.24660676e-09 -5.24792396e-09
  9.91137345e-11  1.21177356e-11  9.74981234e-11  1.03896454e-10
  5.44103137e-10  6.65656173e-11  5.35286248e-10  5.70385415e-10
  9.89578633e-11  1.20990312e-11  9.73450451e-11  1.03733260e-10
  9.56787344e-11  1.16978349e-11  9.41192431e-11  1.00295745e-10
 -1.53041444e-11 -4.93374791e-12 -1.75984573e-11 -3.10154009e-12
 -1.75039824e-12 -4.93754045e-13 -1.92566379e-12 -3.40751150e-13
  4.69735174e-11  1.58557888e-11  5.49100653e-11  9.65774989e-12
  3.37830177e-12  1.24552099e-12  4.08111450e-12  7.15377493e-13
  5.38053666e-12  8.49196741e-12  2.02031099e-12  4.39617449e-12
  2.80447065e-12  4.37573334e-12  9.90668492e-13  2.14948554e-12
  4.00737194e-12  6.39053969e-12  1.12037928e-12  2.63009354e-12
  4.04327521e-12  6.29087029e-12  1.06752326e-12  2.47117331e-12
  1.39372230e-12  2.47553948e-12  2.45926737e-13  7.94346250e-16
 -1.44502527e-13  2.61776668e-14 -6.35958873e-13 -3.71589552e-13
 -3.24493201e-01]
supnorm grad right now is: 0.3244932006025504
Weights right now are: 
[-4.88667872e+00 -7.18996750e-01 -3.71961820e+00 -4.54911424e+00
  5.14124748e+00  6.10305063e-01  4.11855552e+00  3.83308592e+00
 -2.27454981e+00 -9.11142648e-01 -2.25597260e+00 -2.00279781e+00
  1.56208716e+00  4.65462350e-01 -1.60873064e-02  8.24189411e-01
 -1.44730016e+00 -5.15039760e-01 -2.65480657e+00 -2.20989136e+00
 -2.15603323e+00 -9.52067630e-01 -2.40484939e+00 -1.82200218e+00
 -9.72440517e-01 -8.32896270e-01 -1.72221770e+00 -9.12639892e-01
  4.64608208e-01  1.45507441e+00  1.54229343e+00  5.08462094e-01
 -1.09347930e+00 -6.35660151e-01 -1.39053007e+00 -1.20602004e+00
 -2.52064972e+00 -1.43644029e+00 -1.29927109e+00 -1.07250191e+00
 -5.99501409e-01 -1.09691425e+00  2.42212665e-01  3.31473834e-01
  4.35117820e-01  1.12014435e+00 -1.78829398e-01  3.87880958e-01
 -1.46283687e+00 -5.77333061e-01 -4.39377402e-01 -1.59490908e+00
  6.15283808e-01  1.16152155e+00  5.26470077e-01  1.68108747e+00
  1.28402072e+00 -4.78668592e-01 -1.29245987e-01 -2.19426289e+00
  2.09374316e+00  1.58998096e+00 -7.24397829e-02  1.08642218e+00
  2.49343508e+01]
(65,)
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.804710722734
gradient value of function right now is: [ 4.90973559e-09  4.58056129e-10  4.90694916e-09  4.90817980e-09
 -4.90976077e-09 -4.58058357e-10 -4.90697433e-09 -4.90820497e-09
  9.28116170e-11  1.13506064e-11  9.13029724e-11  9.72927302e-11
  5.10846651e-10  6.25145133e-11  5.02591434e-10  5.35535635e-10
  9.27177198e-11  1.13394469e-11  9.12108342e-11  9.71944814e-11
  8.96325673e-11  1.09618773e-11  8.81757152e-11  9.39602332e-11
 -1.44165113e-11 -4.63501539e-12 -1.65608627e-11 -2.91990520e-12
 -1.67911105e-12 -4.74368207e-13 -1.84808971e-12 -3.27049625e-13
  4.40658715e-11  1.48252828e-11  5.14450615e-11  9.05314145e-12
  3.15658301e-12  1.15915924e-12  3.80696935e-12  6.67819651e-13
  5.03767089e-12  7.96400753e-12  1.86116415e-12  4.08417142e-12
  2.62916105e-12  4.10873011e-12  9.12701814e-13  1.99785696e-12
  3.75848254e-12  5.99811069e-12  1.03657474e-12  2.44998912e-12
  3.79322924e-12  5.90625646e-12  9.86847367e-13  2.30117903e-12
  1.30401842e-12  2.33267690e-12  2.29341780e-13  1.61016680e-15
 -1.34642459e-13  2.25774682e-14 -5.94328943e-13 -3.54689570e-13
  5.27840894e-01]
supnorm grad right now is: 0.5278408937580188
Weights right now are: 
[-4.89685514e+00 -7.19944910e-01 -3.72978883e+00 -4.55928743e+00
  5.15142389e+00  6.11253223e-01  4.12872614e+00  3.84325910e+00
 -2.27474208e+00 -9.11166156e-01 -2.25616173e+00 -2.00299936e+00
  1.56103036e+00  4.65333051e-01 -1.71269933e-02  8.23081560e-01
 -1.44749217e+00 -5.15063239e-01 -2.65499546e+00 -2.21009264e+00
 -2.15621887e+00 -9.52090329e-01 -2.40503201e+00 -1.82219678e+00
 -9.72410752e-01 -8.32886681e-01 -1.72218348e+00 -9.12633861e-01
  4.64611641e-01  1.45507538e+00  1.54229721e+00  5.08462762e-01
 -1.09357049e+00 -6.35690906e-01 -1.39063663e+00 -1.20603878e+00
 -2.52065627e+00 -1.43644271e+00 -1.29927900e+00 -1.07250330e+00
 -5.99511845e-01 -1.09693073e+00  2.42208769e-01  3.31465320e-01
  4.35112377e-01  1.12013586e+00 -1.78831309e-01  3.87876795e-01
 -1.46284464e+00 -5.77345465e-01 -4.39379566e-01 -1.59491418e+00
  6.15275962e-01  1.16150934e+00  5.26468016e-01  1.68108268e+00
  1.28401801e+00 -4.78673391e-01 -1.29246465e-01 -2.19426290e+00
  2.09374344e+00  1.58998091e+00 -7.24385474e-02  1.08642291e+00
  2.49919725e+01]
(65,)
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.29680455778
gradient value of function right now is: [ 4.53674542e-09  4.24226208e-10  4.53417453e-09  4.53530981e-09
 -4.53675425e-09 -4.24227036e-10 -4.53418336e-09 -4.53531864e-09
  8.57971499e-11  1.05052326e-11  8.44127231e-11  8.99473542e-11
  4.73680920e-10  5.80313738e-11  4.66080625e-10  4.96614285e-10
  8.57569048e-11  1.05005814e-11  8.43733291e-11  8.99053165e-11
  8.28911424e-11  1.01494617e-11  8.15537128e-11  8.69008122e-11
 -1.34528597e-11 -4.27933857e-12 -1.53925831e-11 -2.71816233e-12
 -1.60015217e-12 -4.50923086e-13 -1.75963217e-12 -3.11565960e-13
  4.08883486e-11  1.35836021e-11  4.75034878e-11  8.37583420e-12
  2.90737194e-12  1.05115980e-12  3.48405482e-12  6.12827191e-13
  4.65394351e-12  7.40751265e-12  1.61323994e-12  3.66282066e-12
  2.43372573e-12  3.82805635e-12  7.90032322e-13  1.79131397e-12
  3.49355991e-12  5.59304991e-12  9.13742562e-13  2.21945867e-12
  3.52819593e-12  5.51102592e-12  8.66134598e-13  2.08040273e-12
  1.20291575e-12  2.20901178e-12  2.10085820e-13  5.32671011e-15
 -1.22023276e-13  1.37880019e-14 -5.44469520e-13 -3.51611812e-13
  3.87620221e+00]
supnorm grad right now is: 3.876202210922051
Weights right now are: 
[-4.90641357e+00 -7.20837898e-01 -3.73934184e+00 -4.56884283e+00
  5.16098231e+00  6.12146211e-01  4.13827915e+00  3.85281449e+00
 -2.27492295e+00 -9.11188268e-01 -2.25633966e+00 -2.00318895e+00
  1.56003397e+00  4.65211159e-01 -1.81072526e-02  8.22037045e-01
 -1.44767291e+00 -5.15085335e-01 -2.65517325e+00 -2.21028210e+00
 -2.15639358e+00 -9.52111688e-01 -2.40520388e+00 -1.82237992e+00
 -9.72382645e-01 -8.32877612e-01 -1.72215115e+00 -9.12628163e-01
  4.64614934e-01  1.45507631e+00  1.54230084e+00  5.08463404e-01
 -1.09365633e+00 -6.35719912e-01 -1.39073702e+00 -1.20605643e+00
 -2.52066242e+00 -1.43644498e+00 -1.29928644e+00 -1.07250460e+00
 -5.99521654e-01 -1.09694622e+00  2.42205055e-01  3.31457275e-01
  4.35107256e-01  1.12012786e+00 -1.78833134e-01  3.87872855e-01
 -1.46285195e+00 -5.77357124e-01 -4.39381621e-01 -1.59491899e+00
  6.15268587e-01  1.16149786e+00  5.26466059e-01  1.68107816e+00
  1.28401547e+00 -4.78677902e-01 -1.29246912e-01 -2.19426290e+00
  2.09374370e+00  1.58998087e+00 -7.24373865e-02  1.08642358e+00
  2.52511631e+01]
(65,)
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.812566080516
gradient value of function right now is: [ 4.39455988e-09  4.12068578e-10  4.39206845e-09  4.39316839e-09
 -4.39505051e-09 -4.12115428e-10 -4.39255878e-09 -4.39365885e-09
  8.33040174e-11  1.01867228e-11  8.19521433e-11  8.73261939e-11
  4.60539277e-10  5.63522788e-11  4.53109553e-10  4.82797730e-10
  8.33039005e-11  1.01870014e-11  8.19522380e-11  8.73262359e-11
  8.05106021e-11  9.84518716e-12  7.92041676e-11  8.43979422e-11
 -1.30447639e-11 -4.19826834e-12 -1.49906150e-11 -2.64275606e-12
 -1.56447062e-12 -4.44232357e-13 -1.72468059e-12 -3.05165226e-13
  3.96219273e-11  1.33439324e-11  4.62750212e-11  8.14220279e-12
  2.82507269e-12  1.03904447e-12  3.40929204e-12  5.97931558e-13
  4.52038035e-12  7.14290337e-12  1.67875997e-12  3.67440466e-12
  2.36356220e-12  3.69195880e-12  8.24477802e-13  1.80008397e-12
  3.37238738e-12  5.38088837e-12  9.34035973e-13  2.20314076e-12
  3.40423446e-12  5.29952980e-12  8.89766974e-13  2.07022898e-12
  1.17028309e-12  2.08827769e-12  2.05216366e-13  9.29952014e-16
 -1.20998224e-13  2.08139318e-14 -5.34411129e-13 -3.16932628e-13
  2.67231813e-01]
supnorm grad right now is: 0.2672318134309648
Weights right now are: 
[-4.91548482e+00 -7.21687494e-01 -3.74840795e+00 -4.57791121e+00
  5.17005356e+00  6.12995806e-01  4.14734525e+00  3.86188287e+00
 -2.27509482e+00 -9.11209279e-01 -2.25650873e+00 -2.00336912e+00
  1.55908500e+00  4.65095078e-01 -1.90408833e-02  8.21042233e-01
 -1.44784473e+00 -5.15106340e-01 -2.65534229e+00 -2.21046222e+00
 -2.15655966e+00 -9.52131990e-01 -2.40536726e+00 -1.82255401e+00
 -9.72355820e-01 -8.32868957e-01 -1.72212029e+00 -9.12622726e-01
  4.64618125e-01  1.45507722e+00  1.54230435e+00  5.08464026e-01
 -1.09373798e+00 -6.35747495e-01 -1.39083250e+00 -1.20607322e+00
 -2.52066826e+00 -1.43644713e+00 -1.29929349e+00 -1.07250584e+00
 -5.99530983e-01 -1.09696093e+00  2.42201533e-01  3.31449639e-01
  4.35102382e-01  1.12012027e+00 -1.78834864e-01  3.87869113e-01
 -1.46285890e+00 -5.77368199e-01 -4.39383571e-01 -1.59492356e+00
  6.15261572e-01  1.16148696e+00  5.26464199e-01  1.68107387e+00
  1.28401305e+00 -4.78682180e-01 -1.29247335e-01 -2.19426290e+00
  2.09374395e+00  1.58998082e+00 -7.24362831e-02  1.08642423e+00
  2.50351866e+01]
(65,)
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5141.462370662214
gradient value of function right now is: [ 4.34051067e-09  4.08173274e-10  4.33804684e-09  4.33913430e-09
 -4.34069132e-09 -4.08190682e-10 -4.33822738e-09 -4.33931489e-09
  8.25225809e-11  1.00681639e-11  8.11688260e-11  8.64935610e-11
  4.56509008e-10  5.57390515e-11  4.49067537e-10  4.78502485e-10
  8.25571028e-11  1.00727059e-11  8.12030132e-11  8.65299319e-11
  7.97818472e-11  9.73384695e-12  7.84731651e-11  8.36209894e-11
 -1.28498005e-11 -4.21993000e-12 -1.48796968e-11 -2.61512881e-12
 -1.54584389e-12 -4.44008592e-13 -1.71066446e-12 -3.02319461e-13
  3.90822516e-11  1.34780653e-11  4.60708283e-11  8.07489357e-12
  2.80537624e-12  1.06278193e-12  3.42770327e-12  5.97894595e-13
  4.47956358e-12  6.98832536e-12  1.86194358e-12  3.84621962e-12
  2.34030501e-12  3.61072128e-12  9.18728891e-13  1.88936893e-12
  3.30640573e-12  5.24369102e-12  1.00716263e-12  2.26539134e-12
  3.33476951e-12  5.15999969e-12  9.66239855e-13  2.13685991e-12
  1.16267578e-12  1.96979367e-12  2.05399591e-13 -5.97894055e-15
 -1.23979096e-13  3.31440734e-14 -5.38502368e-13 -2.71691047e-13
 -5.78251423e+00]
supnorm grad right now is: 5.782514229425045
Weights right now are: 
[-4.92407254e+00 -7.22493711e-01 -3.75699079e+00 -4.58649621e+00
  5.17864129e+00  6.13802024e-01  4.15592811e+00  3.87046788e+00
 -2.27525772e+00 -9.11229194e-01 -2.25666899e+00 -2.00353989e+00
  1.55818358e+00  4.64984808e-01 -1.99277540e-02  8.20097261e-01
 -1.44800767e+00 -5.15126260e-01 -2.65550258e+00 -2.21063302e+00
 -2.15671713e+00 -9.52151241e-01 -2.40552217e+00 -1.82271908e+00
 -9.72330283e-01 -8.32860723e-01 -1.72209093e+00 -9.12617550e-01
  4.64621206e-01  1.45507810e+00  1.54230775e+00  5.08464627e-01
 -1.09381545e+00 -6.35773647e-01 -1.39092306e+00 -1.20608915e+00
 -2.52067378e+00 -1.43644917e+00 -1.29930016e+00 -1.07250701e+00
 -5.99539820e-01 -1.09697487e+00  2.42198221e-01  3.31442415e-01
  4.35097759e-01  1.12011306e+00 -1.78836492e-01  3.87865572e-01
 -1.46286549e+00 -5.77378700e-01 -4.39385412e-01 -1.59492788e+00
  6.15254919e-01  1.16147661e+00  5.26462445e-01  1.68106980e+00
  1.28401077e+00 -4.78686244e-01 -1.29247737e-01 -2.19426290e+00
  2.09374419e+00  1.58998077e+00 -7.24352349e-02  1.08642484e+00
  2.44909605e+01]
(65,)
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.784236200727
gradient value of function right now is: [ 4.00281930e-09  3.77084887e-10  4.00055150e-09  4.00155235e-09
 -4.00325178e-09 -3.77126102e-10 -4.00098373e-09 -4.00198469e-09
  7.60831784e-11  9.29955255e-12  7.48480972e-11  7.97548742e-11
  4.22213586e-10  5.16407881e-11  4.15400702e-10  4.42610287e-10
  7.61509329e-11  9.30810884e-12  7.49149479e-11  7.98260532e-11
  7.35809708e-11  8.99376235e-12  7.23866075e-11  7.71319492e-11
 -1.19844330e-11 -3.87252319e-12 -1.37927432e-11 -2.43020106e-12
 -1.47233806e-12 -4.20540502e-13 -1.62620088e-12 -2.87646317e-13
  3.62168543e-11  1.22528400e-11  4.23728767e-11  7.45034980e-12
  2.57477326e-12  9.52599197e-13  3.11480699e-12  5.45740082e-13
  4.12789205e-12  6.50710457e-12  1.56753376e-12  3.39174389e-12
  2.16139404e-12  3.36836665e-12  7.71342090e-13  1.66433593e-12
  3.07425533e-12  4.89965580e-12  8.67491435e-13  2.02717043e-12
  3.10343327e-12  4.82580786e-12  8.27794796e-13  1.90678838e-12
  1.06916292e-12  1.88882422e-12  1.87242734e-13 -5.58868369e-16
 -1.11251807e-13  2.12602411e-14 -4.90191101e-13 -2.82260669e-13
 -9.23111912e-01]
supnorm grad right now is: 0.923111911796495
Weights right now are: 
[-4.93221542e+00 -7.23259928e-01 -3.76512907e+00 -4.59463652e+00
  5.18678418e+00  6.14568241e-01  4.16406638e+00  3.87860819e+00
 -2.27541239e+00 -9.11248100e-01 -2.25682114e+00 -2.00370201e+00
  1.55732609e+00  4.64879915e-01 -2.07714080e-02  8.19198344e-01
 -1.44816244e+00 -5.15145180e-01 -2.65565484e+00 -2.21079526e+00
 -2.15686668e+00 -9.52169523e-01 -2.40566929e+00 -1.82287585e+00
 -9.72305954e-01 -8.32852873e-01 -1.72206294e+00 -9.12612619e-01
  4.64624178e-01  1.45507894e+00  1.54231103e+00  5.08465208e-01
 -1.09388906e+00 -6.35798518e-01 -1.39100914e+00 -1.20610429e+00
 -2.52067901e+00 -1.43645111e+00 -1.29930650e+00 -1.07250812e+00
 -5.99548205e-01 -1.09698810e+00  2.42195049e-01  3.31435538e-01
  4.35093369e-01  1.12010621e+00 -1.78838054e-01  3.87862197e-01
 -1.46287174e+00 -5.77388668e-01 -4.39387170e-01 -1.59493199e+00
  6.15248608e-01  1.16146679e+00  5.26460770e-01  1.68106593e+00
  1.28400860e+00 -4.78690095e-01 -1.29248117e-01 -2.19426290e+00
  2.09374442e+00  1.58998073e+00 -7.24342409e-02  1.08642542e+00
  2.49118256e+01]
(65,)
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.808610717331
gradient value of function right now is: [ 3.78562402e-09  3.57340344e-10  3.78348112e-09  3.78442672e-09
 -3.78579518e-09 -3.57356321e-10 -3.78365218e-09 -3.78459782e-09
  7.20032372e-11  8.80512323e-12  7.08384891e-11  7.54808680e-11
  4.00464399e-10  4.90029931e-11  3.94024579e-10  4.19825357e-10
  7.20980115e-11  8.81696518e-12  7.09319116e-11  7.55803614e-11
  6.96573500e-11  8.51829682e-12  6.85306410e-11  7.30217122e-11
 -1.14070199e-11 -3.66980216e-12 -1.31065801e-11 -2.31082880e-12
 -1.42145161e-12 -4.05857531e-13 -1.56974688e-12 -2.77724855e-13
  3.43426796e-11  1.15580715e-11  4.00984755e-11  7.05626404e-12
  2.43118509e-12  8.93751312e-13  2.93332297e-12  5.14530206e-13
  3.90567279e-12  6.17336085e-12  1.44601692e-12  3.16996226e-12
  2.04759136e-12  3.19922948e-12  7.11322684e-13  1.55580148e-12
  2.91630956e-12  4.65368920e-12  8.05486320e-13  1.90236122e-12
  2.94500800e-12  4.58513961e-12  7.67492313e-13  1.78817795e-12
  1.01069943e-12  1.80585860e-12  1.76296646e-13  6.77439207e-16
 -1.04433938e-13  1.77265126e-14 -4.62182648e-13 -2.75334587e-13
  4.31516727e-01]
supnorm grad right now is: 0.431516726756198
Weights right now are: 
[-4.94001269e+00 -7.23995160e-01 -3.77292192e+00 -4.60243133e+00
  5.19458145e+00  6.15303474e-01  4.17185925e+00  3.88640300e+00
 -2.27556061e+00 -9.11266223e-01 -2.25696697e+00 -2.00385739e+00
  1.55650251e+00  4.64779155e-01 -2.15817341e-02  8.18334957e-01
 -1.44831083e+00 -5.15163324e-01 -2.65580082e+00 -2.21095081e+00
 -2.15701005e+00 -9.52187053e-01 -2.40581034e+00 -1.82302615e+00
 -9.72282522e-01 -8.32845330e-01 -1.72203601e+00 -9.12607872e-01
  4.64627081e-01  1.45507977e+00  1.54231424e+00  5.08465775e-01
 -1.09395971e+00 -6.35822318e-01 -1.39109167e+00 -1.20611880e+00
 -2.52068402e+00 -1.43645295e+00 -1.29931254e+00 -1.07250918e+00
 -5.99556251e-01 -1.09700080e+00  2.42192052e-01  3.31428999e-01
  4.35089152e-01  1.12009963e+00 -1.78839529e-01  3.87858989e-01
 -1.46287775e+00 -5.77398236e-01 -4.39388835e-01 -1.59493591e+00
  6.15242543e-01  1.16145736e+00  5.26459182e-01  1.68106225e+00
  1.28400652e+00 -4.78693803e-01 -1.29248479e-01 -2.19426290e+00
  2.09374463e+00  1.58998069e+00 -7.24332906e-02  1.08642598e+00
  2.50319595e+01]
(65,)
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.730341342882
gradient value of function right now is: [ 3.66467189e-09  3.46685599e-10  3.66259727e-09  3.66351258e-09
 -3.66422720e-09 -3.46643168e-10 -3.66215284e-09 -3.66306803e-09
  6.98076834e-11  8.53058123e-12  6.86752889e-11  7.31759429e-11
  3.88791098e-10  4.75427602e-11  3.82522372e-10  4.07570267e-10
  6.99261437e-11  8.54531258e-12  6.87920099e-11  7.33002629e-11
  6.75535156e-11  8.25516797e-12  6.64577771e-11  7.08130447e-11
 -1.10650269e-11 -3.58238855e-12 -1.27438665e-11 -2.24476156e-12
 -1.39014411e-12 -3.98844294e-13 -1.53763477e-12 -2.71923019e-13
  3.32690572e-11  1.12794423e-11  3.89562201e-11  6.84721351e-12
  2.35680312e-12  8.74454417e-13  2.85449697e-12  4.99883892e-13
  3.78689102e-12  5.96215228e-12  1.45323614e-12  3.12690155e-12
  1.98574031e-12  3.09092454e-12  7.16106178e-13  1.53643851e-12
  2.81838750e-12  4.48912376e-12  8.02274438e-13  1.86621334e-12
  2.84553997e-12  4.42211806e-12  7.66321620e-13  1.75652831e-12
  9.80700539e-13  1.72469812e-12  1.71316515e-13 -1.21845319e-15
 -1.02364400e-13  2.04896621e-14 -4.50689317e-13 -2.55953801e-13
 -1.49393654e+00]
supnorm grad right now is: 1.493936540939409
Weights right now are: 
[-4.94744266e+00 -7.24697235e-01 -3.78034769e+00 -4.60985895e+00
  5.20201143e+00  6.16005549e-01  4.17928501e+00  3.89383062e+00
 -2.27570202e+00 -9.11283513e-01 -2.25710609e+00 -2.00400563e+00
  1.55571541e+00  4.64682854e-01 -2.23561720e-02  8.17509811e-01
 -1.44845245e+00 -5.15180640e-01 -2.65594015e+00 -2.21109927e+00
 -2.15714687e+00 -9.52203782e-01 -2.40594495e+00 -1.82316957e+00
 -9.72260096e-01 -8.32838105e-01 -1.72201023e+00 -9.12603328e-01
  4.64629889e-01  1.45508057e+00  1.54231734e+00  5.08466323e-01
 -1.09402716e+00 -6.35845052e-01 -1.39117047e+00 -1.20613267e+00
 -2.52068879e+00 -1.43645471e+00 -1.29931830e+00 -1.07251019e+00
 -5.99563920e-01 -1.09701291e+00  2.42189194e-01  3.31422751e-01
  4.35085130e-01  1.12009335e+00 -1.78840935e-01  3.87855921e-01
 -1.46288347e+00 -5.77407366e-01 -4.39390425e-01 -1.59493966e+00
  6.15236764e-01  1.16144837e+00  5.26457666e-01  1.68105873e+00
  1.28400453e+00 -4.78697337e-01 -1.29248826e-01 -2.19426290e+00
  2.09374484e+00  1.58998066e+00 -7.24323807e-02  1.08642651e+00
  2.48978179e+01]
(65,)
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.619515018073
gradient value of function right now is: [ 3.42537295e-09  3.24571757e-10  3.42343672e-09  3.42429090e-09
 -3.42574530e-09 -3.24607458e-10 -3.42380885e-09 -3.42466313e-09
  6.52514957e-11  7.98480628e-12  6.42015253e-11  6.84066760e-11
  3.64370632e-10  4.46144814e-11  3.58541431e-10  3.82005959e-10
  6.53887941e-11  8.00182631e-12  6.43367728e-11  6.85507358e-11
  6.31620286e-11  7.72915794e-12  6.21457626e-11  6.62162030e-11
 -1.04376478e-11 -3.33789282e-12 -1.19658759e-11 -2.11164119e-12
 -1.33333743e-12 -3.80722693e-13 -1.47237610e-12 -2.60581186e-13
  3.12200379e-11  1.04320601e-11  3.63513154e-11  6.40418007e-12
  2.19484471e-12  7.99886338e-13  2.63865751e-12  4.63575736e-13
  3.53741890e-12  5.61312786e-12  1.26353950e-12  2.82360276e-12
  1.85851869e-12  2.91459893e-12  6.21502851e-13  1.38663382e-12
  2.65116014e-12  4.23831434e-12  7.10618299e-13  1.70447329e-12
  2.67863043e-12  4.17803315e-12  6.75586891e-13  1.60059336e-12
  9.14656997e-13  1.65864945e-12  1.58648670e-13  2.12857799e-15
 -9.35879587e-14  1.30122685e-14 -4.16976707e-13 -2.59927712e-13
  2.34083762e+00]
supnorm grad right now is: 2.340837623082756
Weights right now are: 
[-4.95455425e+00 -7.25370585e-01 -3.78745525e+00 -4.61696828e+00
  5.20912301e+00  6.16678898e-01  4.18639257e+00  3.90093995e+00
 -2.27583751e+00 -9.11300080e-01 -2.25723939e+00 -2.00414767e+00
  1.55495992e+00  4.64590420e-01 -2.30995216e-02  8.16717792e-01
 -1.44858820e+00 -5.15197239e-01 -2.65607371e+00 -2.21124158e+00
 -2.15727801e+00 -9.52219816e-01 -2.40607397e+00 -1.82330704e+00
 -9.72238541e-01 -8.32831156e-01 -1.72198544e+00 -9.12598959e-01
  4.64632616e-01  1.45508136e+00  1.54232036e+00  5.08466857e-01
 -1.09409183e+00 -6.35866869e-01 -1.39124605e+00 -1.20614596e+00
 -2.52069336e+00 -1.43645639e+00 -1.29932382e+00 -1.07251116e+00
 -5.99571264e-01 -1.09702452e+00  2.42186443e-01  3.31416749e-01
  4.35081277e-01  1.12008733e+00 -1.78842290e-01  3.87852972e-01
 -1.46288895e+00 -5.77416114e-01 -4.39391952e-01 -1.59494326e+00
  6.15231232e-01  1.16143975e+00  5.26456210e-01  1.68105535e+00
  1.28400263e+00 -4.78700713e-01 -1.29249159e-01 -2.19426290e+00
  2.09374503e+00  1.58998063e+00 -7.24315076e-02  1.08642703e+00
  2.51890631e+01]
(65,)
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5141.998274815607
gradient value of function right now is: [ 3.23482812e-09  3.07024517e-10  3.23300165e-09  3.23380732e-09
 -3.23510223e-09 -3.07050992e-10 -3.23327560e-09 -3.23408135e-09
  6.16357268e-11  7.54911449e-12  6.06494617e-11  6.46202615e-11
  3.44963094e-10  4.22739014e-11  3.39474196e-10  3.61681319e-10
  6.17887450e-11  7.56805009e-12  6.08001737e-11  6.47807972e-11
  5.96785524e-11  7.30943384e-12  5.87236816e-11  6.25683413e-11
 -9.92879722e-12 -3.14938586e-12 -1.13481382e-11 -2.00496502e-12
 -1.28621292e-12 -3.66205956e-13 -1.41891431e-12 -2.51246089e-13
  2.95730328e-11  9.78695068e-12  3.43066056e-11  6.05271020e-12
  2.06720730e-12  7.44460500e-13  2.47316061e-12  4.35371352e-13
  3.33970747e-12  5.32635839e-12  1.13546416e-12  2.60548750e-12
  1.75737304e-12  2.76928344e-12  5.57965108e-13  1.27936775e-12
  2.51473082e-12  4.02960286e-12  6.47022875e-13  1.58494430e-12
  2.54209526e-12  3.97436969e-12  6.13005133e-13  1.48601674e-12
  8.62342466e-13  1.59549542e-12  1.48742698e-13  4.10722752e-15
 -8.70271499e-14  8.38779727e-15 -3.91000665e-13 -2.58534085e-13
  4.88148412e+00]
supnorm grad right now is: 4.881484115288719
Weights right now are: 
[-4.96137288e+00 -7.26017434e-01 -3.79427003e+00 -4.62378476e+00
  5.21594164e+00  6.17325747e-01  4.19320734e+00  3.90775643e+00
 -2.27596756e+00 -9.11315979e-01 -2.25736733e+00 -2.00428399e+00
  1.55423358e+00  4.64501561e-01 -2.38141842e-02  8.15956349e-01
 -1.44871854e+00 -5.15213174e-01 -2.65620194e+00 -2.21137821e+00
 -2.15740391e+00 -9.52235208e-01 -2.40619783e+00 -1.82343902e+00
 -9.72217788e-01 -8.32824463e-01 -1.72196158e+00 -9.12594753e-01
  4.64635270e-01  1.45508212e+00  1.54232329e+00  5.08467376e-01
 -1.09415394e+00 -6.35887828e-01 -1.39131864e+00 -1.20615873e+00
 -2.52069774e+00 -1.43645801e+00 -1.29932911e+00 -1.07251208e+00
 -5.99578314e-01 -1.09703566e+00  2.42183798e-01  3.31410988e-01
  4.35077575e-01  1.12008155e+00 -1.78843594e-01  3.87850140e-01
 -1.46289421e+00 -5.77424507e-01 -4.39393421e-01 -1.59494671e+00
  6.15225920e-01  1.16143148e+00  5.26454810e-01  1.68105210e+00
  1.28400080e+00 -4.78703959e-01 -1.29249477e-01 -2.19426290e+00
  2.09374522e+00  1.58998059e+00 -7.24306702e-02  1.08642752e+00
  2.53284195e+01]
(65,)
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.732041892152
gradient value of function right now is: [ 3.23817072e-09  3.08066163e-10  3.23633987e-09  3.23714729e-09
 -3.23793148e-09 -3.08043241e-10 -3.23610076e-09 -3.23690812e-09
  6.18611428e-11  7.55937324e-12  6.08600955e-11  6.48465647e-11
  3.46283643e-10  4.23440911e-11  3.40714002e-10  3.63012946e-10
  6.20332464e-11  7.58063057e-12  6.10295756e-11  6.50271016e-11
  5.99117550e-11  7.32120299e-12  5.89423328e-11  6.28031298e-11
 -9.90041531e-12 -3.20576916e-12 -1.14030004e-11 -2.00861602e-12
 -1.28228626e-12 -3.69556712e-13 -1.42036921e-12 -2.51157355e-13
  2.95480894e-11  1.00174510e-11  3.45983186e-11  6.08140966e-12
  2.08071644e-12  7.72230638e-13  2.52035027e-12  4.41376567e-13
  3.35446847e-12  5.28149189e-12  1.28721896e-12  2.76997062e-12
  1.76290092e-12  2.74410747e-12  6.35225700e-13  1.36322500e-12
  2.49779514e-12  3.97852347e-12  7.10884414e-13  1.65380335e-12
  2.52265161e-12  3.92037782e-12  6.79273071e-13  1.55715611e-12
  8.68564044e-13  1.52729361e-12  1.51102692e-13 -1.27535472e-15
 -9.06729124e-14  1.81583169e-14 -3.99786202e-13 -2.27160237e-13
 -1.48390004e+00]
supnorm grad right now is: 1.48390004268758
Weights right now are: 
[-4.96791620e+00 -7.26639293e-01 -3.80080965e+00 -4.63032601e+00
  5.22248497e+00  6.17947607e-01  4.19974697e+00  3.91429769e+00
 -2.27609245e+00 -9.11331250e-01 -2.25749021e+00 -2.00441492e+00
  1.55353478e+00  4.64416063e-01 -2.45017758e-02  8.15223758e-01
 -1.44884376e+00 -5.15228486e-01 -2.65632514e+00 -2.21150948e+00
 -2.15752485e+00 -9.52249996e-01 -2.40631682e+00 -1.82356581e+00
 -9.72197785e-01 -8.32818017e-01 -1.72193858e+00 -9.12590698e-01
  4.64637853e-01  1.45508286e+00  1.54232615e+00  5.08467882e-01
 -1.09421365e+00 -6.35907957e-01 -1.39138840e+00 -1.20617101e+00
 -2.52070194e+00 -1.43645956e+00 -1.29933419e+00 -1.07251297e+00
 -5.99585082e-01 -1.09704635e+00  2.42181282e-01  3.31405468e-01
  4.35074018e-01  1.12007599e+00 -1.78844834e-01  3.87847425e-01
 -1.46289926e+00 -5.77432570e-01 -4.39394824e-01 -1.59495002e+00
  6.15220815e-01  1.16142354e+00  5.26453473e-01  1.68104898e+00
  1.28399905e+00 -4.78707074e-01 -1.29249783e-01 -2.19426290e+00
  2.09374541e+00  1.58998056e+00 -7.24298635e-02  1.08642799e+00
  2.48407815e+01]
(65,)
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.737341139927
gradient value of function right now is: [ 3.05883021e-09  2.91437651e-10  3.05710284e-09  3.05786457e-09
 -3.05864173e-09 -2.91419675e-10 -3.05691447e-09 -3.05767615e-09
  5.84383389e-11  7.14860964e-12  5.74986139e-11  6.12632205e-11
  3.27860220e-10  4.01309533e-11  3.22618978e-10  3.43724127e-10
  5.86214970e-11  7.17121533e-12  5.76789714e-11  6.14553447e-11
  5.66111842e-11  6.92513495e-12  5.57009148e-11  5.93477683e-11
 -9.42236899e-12 -3.02241436e-12 -1.08141133e-11 -1.90761913e-12
 -1.23716661e-12 -3.55230170e-13 -1.36862134e-12 -2.42157952e-13
  2.79999264e-11  9.38789154e-12  3.26444854e-11  5.74822546e-12
  1.95955958e-12  7.17399343e-13  2.36017473e-12  4.14349042e-13
  3.16723337e-12  5.01637622e-12  1.15107270e-12  2.54864127e-12
  1.66717549e-12  2.60984536e-12  5.67310920e-13  1.25388562e-12
  2.37107936e-12  3.78717008e-12  6.44668992e-13  1.53486341e-12
  2.39605512e-12  3.73396981e-12  6.13845853e-13  1.44269202e-12
  8.19038200e-13  1.47471868e-12  1.41655986e-13  9.88905237e-16
 -8.42325721e-14  1.29871559e-14 -3.74714309e-13 -2.28718760e-13
  1.45373108e+00]
supnorm grad right now is: 1.4537310848029301
Weights right now are: 
[-4.97422016 -0.72723946 -3.80711004 -4.63662797  5.22878892  0.61854777
  4.20604737  3.92059966 -2.27621288 -0.91134597 -2.2576087  -2.00454117
  1.55285986  0.46433349 -0.02516587  0.81451621 -1.44896455 -0.51524325
 -2.65644398 -2.2116361  -2.1576415  -0.95226426 -2.40643159 -1.82368809
 -0.97217844 -0.83281178 -1.72191634 -0.91258678  0.46464038  1.45508358
  1.54232894  0.50846838 -1.09427127 -0.63592737 -1.39145571 -1.20618285
 -2.52070599 -1.43646105 -1.29933907 -1.07251383 -0.59959161 -1.09705666
  0.24217885  0.33140015  0.43507058  1.12007063 -0.17884603  0.38784481
 -1.46290414 -0.57744035 -0.43939618 -1.59495321  0.61521589  1.16141587
  0.52645218  1.68104598  1.28399736 -0.47871008 -0.12925008 -2.1942629
  2.09374558  1.58998053 -0.07242909  1.08642845 25.07264225]
(65,)
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.81356159239
gradient value of function right now is: [ 2.97304834e-09  2.83767802e-10  2.97136946e-09  2.97210970e-09
 -2.97258208e-09 -2.83722734e-10 -2.97090348e-09 -2.97164359e-09
  5.68637032e-11  6.95287937e-12  5.59478421e-11  5.96108092e-11
  3.19419717e-10  3.90813388e-11  3.14305728e-10  3.34866486e-10
  5.70593724e-11  6.97700494e-12  5.61405041e-11  5.98160439e-11
  5.50988833e-11  6.73712769e-12  5.42115184e-11  5.77607571e-11
 -9.17792060e-12 -2.95576615e-12 -1.05492769e-11 -1.85978781e-12
 -1.21335976e-12 -3.49561646e-13 -1.34377308e-12 -2.37690386e-13
  2.72363022e-11  9.17409616e-12  3.18110444e-11  5.59729022e-12
  1.90604477e-12  7.01890025e-13  2.30126086e-12  4.03583871e-13
  3.08209461e-12  4.86933797e-12  1.14639912e-12  2.50707571e-12
  1.62281483e-12  2.53434479e-12  5.65675948e-13  1.23446415e-12
  2.30262146e-12  3.67355963e-12  6.38266288e-13  1.50452459e-12
  2.32662810e-12  3.62157678e-12  6.08781163e-13  1.41545256e-12
  7.97312932e-13  1.42165620e-12  1.37938309e-13 -1.09070285e-17
 -8.25109670e-14  1.43491627e-14 -3.65902869e-13 -2.16949904e-13
  2.05467789e-01]
supnorm grad right now is: 0.20546778902304733
Weights right now are: 
[-4.98024841 -0.72781434 -3.81313489 -4.64265432  5.23481718  0.61912266
  4.21207222  3.92662601 -2.27632814 -0.91136007 -2.2577221  -2.00466199
  1.55221292  0.46425434 -0.02580246  0.81383798 -1.44908018 -0.51525739
 -2.65655775 -2.21175733 -2.15775317 -0.95227791 -2.40654146 -1.82380515
 -0.97215986 -0.8328058  -1.72189498 -0.91258301  0.46464282  1.45508429
  1.54233165  0.50846885 -1.09432646 -0.63594596 -1.39152017 -1.20619419
 -2.52070985 -1.43646247 -1.29934374 -1.07251465 -0.59959786 -1.09706654
  0.24217653  0.33139507  0.43506729  1.12006549 -0.17884718  0.38784231
 -1.46290881 -0.57744779 -0.43939747 -1.59495626  0.61521117  1.16140853
  0.52645095  1.68104311  1.28399574 -0.47871296 -0.12925036 -2.1942629
  2.09374575  1.5899805  -0.07242835  1.08642889 25.01194988]
(65,)
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.708835557262
gradient value of function right now is: [ 2.90467638e-09  2.77738451e-10  2.90303584e-09  2.90375907e-09
 -2.90476590e-09 -2.77747066e-10 -2.90312531e-09 -2.90384856e-09
  5.56364343e-11  6.79809587e-12  5.47377178e-11  5.83216104e-11
  3.12844299e-10  3.82517650e-11  3.07821634e-10  3.27959128e-10
  5.58443461e-11  6.82370483e-12  5.49424170e-11  5.85396721e-11
  5.39211682e-11  6.58855031e-12  5.30502314e-11  5.65235882e-11
 -8.97861481e-12 -2.90958934e-12 -1.03442766e-11 -1.82196746e-12
 -1.19352818e-12 -3.45388319e-13 -1.32379487e-12 -2.34049717e-13
  2.66235710e-11  9.03284020e-12  3.11830280e-11  5.48050216e-12
  1.86516295e-12  6.93111801e-13  2.26041549e-12  3.95785293e-13
  3.01575052e-12  4.74631244e-12  1.16177588e-12  2.49507814e-12
  1.58798936e-12  2.47086971e-12  5.74174258e-13  1.22980339e-12
  2.24579519e-12  3.57646530e-12  6.41150233e-13  1.48923541e-12
  2.26873765e-12  3.52511332e-12  6.12981925e-13  1.40282527e-12
  7.80824173e-13  1.37044833e-12  1.35365878e-13 -1.45006693e-15
 -8.16010588e-14  1.66109776e-14 -3.60065446e-13 -2.03617587e-13
 -1.67218695e+00]
supnorm grad right now is: 1.6721869483192509
Weights right now are: 
[-4.98610593 -0.72837388 -3.8189891  -4.64851     5.24067471  0.61968219
  4.21792644  3.93248169 -2.27644023 -0.91137377 -2.25783239 -2.0047795
  1.55158283  0.46417726 -0.02642245  0.81317743 -1.44919268 -0.51527115
 -2.65666843 -2.21187525 -2.15786179 -0.95229119 -2.40664833 -1.82391902
 -0.97214175 -0.83279997 -1.72187416 -0.91257934  0.46464523  1.45508498
  1.54233432  0.50846933 -1.09438016 -0.63596406 -1.3915829  -1.20620522
 -2.52071361 -1.43646385 -1.29934827 -1.07251544 -0.59960394 -1.09707613
  0.24217425  0.33139012  0.43506409  1.1200605  -0.1788483   0.38783987
 -1.46291335 -0.57745503 -0.43939873 -1.59495922  0.61520658  1.1614014
  0.52644974  1.68104032  1.28399417 -0.47871576 -0.12925063 -2.1942629
  2.09374591  1.58998047 -0.07242763  1.08642932 24.82133138]
(65,)
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.656957319821
gradient value of function right now is: [ 2.81423870e-09  2.69521538e-10  2.81264963e-09  2.81335008e-09
 -2.81436862e-09 -2.69533812e-10 -2.81277948e-09 -2.81347996e-09
  5.39520943e-11  6.59136256e-12  5.30805733e-11  5.65555973e-11
  3.03778980e-10  3.71383971e-11  2.98901934e-10  3.18453837e-10
  5.41694591e-11  6.61811811e-12  5.32945693e-11  5.67835640e-11
  5.23001256e-11  6.38957925e-12  5.14553598e-11  5.48239387e-11
 -8.72541712e-12 -2.83102316e-12 -1.00572067e-11 -1.77109935e-12
 -1.16857160e-12 -3.38777603e-13 -1.29688381e-12 -2.29266279e-13
  2.58258386e-11  8.77423075e-12  3.02648696e-11  5.31799674e-12
  1.80714588e-12  6.72745944e-13  2.19171819e-12  3.83641196e-13
  2.92431729e-12  4.59880628e-12  1.13396649e-12  2.42709970e-12
  1.54063468e-12  2.39538793e-12  5.60764319e-13  1.19694174e-12
  2.17653839e-12  3.46485178e-12  6.24811137e-13  1.44727036e-12
  2.19886992e-12  3.41524728e-12  5.97693857e-13  1.36375982e-12
  7.57155619e-13  1.32486861e-12  1.31190709e-13 -1.73011777e-15
 -7.92808620e-14  1.65755906e-14 -3.49604512e-13 -1.95891337e-13
 -2.04433181e+00]
supnorm grad right now is: 2.044331805502661
Weights right now are: 
[-4.99174281 -0.72891322 -3.8246228  -4.6541451   5.24631159  0.62022153
  4.22356014  3.93811679 -2.2765482  -0.91138697 -2.25793861 -2.00489268
  1.55097511  0.46410291 -0.02702045  0.81254032 -1.44930106 -0.5152844
 -2.65677507 -2.21198888 -2.15796644 -0.95230399 -2.40675129 -1.82402872
 -0.97212426 -0.83279433 -1.72185405 -0.9125758   0.46464757  1.45508566
  1.54233691  0.50846978 -1.0944319  -0.63598151 -1.39164337 -1.20621586
 -2.52071722 -1.43646519 -1.29935264 -1.07251621 -0.59960979 -1.09708537
  0.24217206  0.33138534  0.43506101  1.12005569 -0.17884939  0.38783752
 -1.46291771 -0.57746199 -0.43939995 -1.59496209  0.61520217  1.16139453
  0.52644858  1.68103763  1.28399266 -0.47871845 -0.12925089 -2.1942629
  2.09374607  1.58998044 -0.07242693  1.08642973 24.84141058]
(65,)
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.814820584883
gradient value of function right now is: [ 2.68528797e-09  2.57514898e-10  2.68377315e-09  2.68444082e-09
 -2.68544557e-09 -2.57530449e-10 -2.68393066e-09 -2.68459837e-09
  5.14876486e-11  6.29521679e-12  5.06598922e-11  5.39753127e-11
  2.90448205e-10  3.55349407e-11  2.85806694e-10  3.04495563e-10
  5.17104960e-11  6.32264586e-12  5.08792875e-11  5.42090296e-11
  4.99219273e-11  6.10381486e-12  4.91194095e-11  5.23339664e-11
 -8.37616203e-12 -2.69888277e-12 -9.62935193e-12 -1.69757706e-12
 -1.13407930e-12 -3.27863071e-13 -1.25737135e-12 -2.22388160e-13
  2.47053622e-11  8.32504455e-12  2.88592861e-11  5.07780594e-12
  1.72036486e-12  6.34020023e-13  2.07771577e-12  3.64362834e-13
  2.78950583e-12  4.40656940e-12  1.03958897e-12  2.27178145e-12
  1.47148130e-12  2.29771882e-12  5.13646977e-13  1.12015477e-12
  2.08451761e-12  3.32548737e-12  5.78717068e-13  1.36336710e-12
  2.10676127e-12  3.27922692e-12  5.52215193e-13  1.28310196e-12
  7.21728363e-13  1.28527551e-12  1.24482467e-13 -2.21646975e-16
 -7.47449417e-14  1.31325377e-14 -3.31780936e-13 -1.96212219e-13
  1.09339563e-01]
supnorm grad right now is: 0.10933956346555217
Weights right now are: 
[-4.99720014 -0.72943617 -3.83007706 -4.65960071  5.25176892  0.62074449
  4.22901439  3.9435724  -2.2766528  -0.91139976 -2.25804153 -2.00500233
  1.55038547  0.46403078 -0.02760065  0.81192218 -1.4494061  -0.51529724
 -2.65687842 -2.21209899 -2.15806785 -0.95231638 -2.40685107 -1.82413503
 -0.97210727 -0.83278885 -1.72183452 -0.91257236  0.46464986  1.45508632
  1.54233945  0.50847023 -1.09448206 -0.63599843 -1.391702   -1.20622617
 -2.52072072 -1.43646648 -1.29935687 -1.07251695 -0.59961546 -1.09709431
  0.24216994  0.33138072  0.43505802  1.12005103 -0.17885043  0.38783524
 -1.46292195 -0.57746875 -0.43940113 -1.59496486  0.61519789  1.16138787
  0.52644745  1.68103502  1.2839912  -0.47872106 -0.12925114 -2.1942629
  2.09374622  1.58998042 -0.07242626  1.08643012 24.98752577]
(65,)
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5141.8422823975525
gradient value of function right now is: [ 2.51573845e-09  2.41504644e-10  2.51432180e-09  2.51494619e-09
 -2.51608521e-09 -2.41538278e-10 -2.51466836e-09 -2.51529284e-09
  4.82031558e-11  5.90459092e-12  4.74363060e-11  5.05387101e-11
  2.72613345e-10  3.34113747e-11  2.68300863e-10  2.85833304e-10
  4.84268637e-11  5.93214383e-12  4.76565653e-11  5.07733406e-11
  4.67473421e-11  5.72628831e-12  4.60037121e-11  4.90123780e-11
 -7.92369861e-12 -2.51043799e-12 -9.05231744e-12 -1.59970697e-12
 -1.08900120e-12 -3.12172428e-13 -1.20390129e-12 -2.13174532e-13
  2.32417779e-11  7.67826677e-12  2.69437222e-11  4.75500014e-12
  1.60341730e-12  5.76569606e-13  1.91706042e-12  3.37595713e-13
  2.60924451e-12  4.16526860e-12  8.79486111e-13  2.02775971e-12
  1.37956010e-12  2.17571394e-12  4.33441022e-13  9.98941133e-13
  1.96827872e-12  3.15527595e-12  5.02687308e-13  1.23606455e-12
  1.99115533e-12  3.11433031e-12  4.76291719e-13  1.15941499e-12
  6.73296267e-13  1.24971838e-12  1.15030485e-13  3.17575695e-15
 -6.77992743e-14  6.03503761e-15 -3.05954107e-13 -2.04532416e-13
  5.34496148e+00]
supnorm grad right now is: 5.34496147522515
Weights right now are: 
[-5.00250441 -0.72994522 -3.83537833 -4.6649033   5.25707318  0.62125354
  4.23431566  3.94887499 -2.27675454 -0.9114122  -2.25814164 -2.00510899
  1.54981117  0.46396053 -0.02816578  0.8113201  -1.4495083  -0.51530973
 -2.65697898 -2.21220612 -2.15816651 -0.95232844 -2.40694815 -1.82423846
 -0.97209071 -0.83278351 -1.72181546 -0.912569    0.46465211  1.45508697
  1.54234194  0.50847067 -1.09453089 -0.6360149  -1.39175905 -1.20623621
 -2.52072412 -1.43646773 -1.29936097 -1.07251767 -0.59962097 -1.09710301
  0.24216787  0.33137622  0.43505511  1.12004649 -0.17885146  0.38783302
 -1.46292607 -0.57747531 -0.43940228 -1.59496756  0.61519373  1.16138139
  0.52644636  1.68103248  1.28398977 -0.47872359 -0.12925139 -2.1942629
  2.09374637  1.58998039 -0.0724256   1.08643051 25.30287324]
(65,)
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.540639217142
gradient value of function right now is: [ 2.47845985e-09  2.38320151e-10  2.47706366e-09  2.47767894e-09
 -2.47882580e-09 -2.38356084e-10 -2.47742938e-09 -2.47804477e-09
  4.75567764e-11  5.82009914e-12  4.67970230e-11  4.98580378e-11
  2.69150337e-10  3.29585695e-11  2.64875477e-10  2.82186508e-10
  4.77892944e-11  5.84871341e-12  4.70259409e-11  5.01018957e-11
  4.61292745e-11  5.64542595e-12  4.53923873e-11  4.83614795e-11
 -7.80675511e-12 -2.49410252e-12 -8.94623394e-12 -1.57916145e-12
 -1.07655872e-12 -3.10340266e-13 -1.19238370e-12 -2.11011275e-13
  2.28955944e-11  7.63777865e-12  2.66410739e-11  4.69499290e-12
  1.58298639e-12  5.76201763e-13  1.90202651e-12  3.34297711e-13
  2.57487898e-12  4.08963680e-12  9.12189356e-13  2.04811907e-12
  1.36102626e-12  2.13620170e-12  4.50343863e-13  1.00999399e-12
  1.93374487e-12  3.09271403e-12  5.14655300e-13  1.23914668e-12
  1.95556636e-12  3.05153951e-12  4.89476883e-13  1.16441972e-12
  6.65384302e-13  1.21006383e-12  1.14078095e-13  1.39117353e-15
 -6.79780568e-14  9.00958689e-15 -3.04381403e-13 -1.91772009e-13
  2.79229802e+00]
supnorm grad right now is: 2.792298020898714
Weights right now are: 
[-5.00763958 -0.73043877 -3.84051061 -4.67003686  5.26220837  0.62174708
  4.23944795  3.95400855 -2.27685311 -0.91142425 -2.25823862 -2.00521232
  1.54925404  0.46389237 -0.02871401  0.81073602 -1.44960733 -0.51532184
 -2.65707642 -2.21230994 -2.15826211 -0.95234013 -2.40704221 -1.82433868
 -0.97207461 -0.83277832 -1.72179696 -0.91256574  0.46465431  1.45508761
  1.54234439  0.50847111 -1.09457822 -0.63603086 -1.39181435 -1.20624594
 -2.5207274  -1.43646894 -1.29936494 -1.07251837 -0.59962631 -1.09711144
  0.24216587  0.33137187  0.43505229  1.12004209 -0.17885244  0.38783087
 -1.46293006 -0.57748168 -0.43940339 -1.59497017  0.6151897   1.16137512
  0.5264453   1.68103002  1.28398839 -0.47872605 -0.12925162 -2.1942629
  2.09374651  1.58998036 -0.07242497  1.08643088 25.18152391]
(65,)
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.421793724215
gradient value of function right now is: [ 2.49905389e-09  2.40760058e-10  2.49764416e-09  2.49826529e-09
 -2.49905549e-09 -2.40760461e-10 -2.49764575e-09 -2.49826688e-09
  4.80624530e-11  5.86917935e-12  4.72862544e-11  5.03806915e-11
  2.71980780e-10  3.32368208e-11  2.67615760e-10  2.85114086e-10
  4.83077320e-11  5.89931465e-12  4.75277017e-11  5.06379048e-11
  4.66283457e-11  5.69408767e-12  4.58753715e-11  4.88774365e-11
 -7.83723149e-12 -2.55302431e-12 -9.04700189e-12 -1.59227318e-12
 -1.07887437e-12 -3.14705732e-13 -1.19976452e-12 -2.12017366e-13
  2.30372618e-11  7.86210392e-12  2.70444123e-11  4.74864750e-12
  1.60444383e-12  6.00877886e-13  1.95073791e-12  3.41098841e-13
  2.60390225e-12  4.08523161e-12  1.03192191e-12  2.18424660e-12
  1.37457209e-12  2.13231788e-12  5.11499211e-13  1.07941548e-12
  1.93506742e-12  3.07706312e-12  5.65667477e-13  1.29845195e-12
  1.95520813e-12  3.03347117e-12  5.42042037e-13  1.22485398e-12
  6.74457200e-13  1.16836404e-12  1.16531246e-13 -2.41564978e-15
 -7.10373262e-14  1.61482771e-14 -3.12734388e-13 -1.70076946e-13
 -3.17348245e+00]
supnorm grad right now is: 3.1734824467808
Weights right now are: 
[-5.01261426 -0.73091754 -3.84548248 -4.67500997  5.26718306  0.62222586
  4.24441983  3.95898167 -2.27694865 -0.91143593 -2.25833263 -2.00531248
  1.54871326  0.4638262  -0.02924617  0.81016908 -1.44970336 -0.51533358
 -2.6571709  -2.21241061 -2.1583548  -0.95235147 -2.40713341 -1.82443584
 -0.97205897 -0.83277328 -1.72177898 -0.91256257  0.46465647  1.45508823
  1.54234678  0.50847153 -1.09462414 -0.63604632 -1.39186797 -1.20625538
 -2.52073059 -1.43647012 -1.29936879 -1.07251904 -0.59963149 -1.09711962
  0.24216395  0.33136766  0.43504956  1.12003782 -0.17885339  0.38782879
 -1.46293393 -0.57748785 -0.43940446 -1.59497269  0.61518578  1.16136903
  0.52644427  1.68102764  1.28398705 -0.47872844 -0.12925185 -2.1942629
  2.09374665  1.58998034 -0.07242435  1.08643125 24.71113699]
(65,)
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.708031084721
gradient value of function right now is: [ 2.35168605e-09  2.26773039e-10  2.35036170e-09  2.35094520e-09
 -2.35167299e-09 -2.26771729e-10 -2.35034865e-09 -2.35093214e-09
  4.51943946e-11  5.52883262e-12  4.44718243e-11  4.73803161e-11
  2.56363562e-10  3.13812548e-11  2.52288981e-10  2.68775002e-10
  4.54378289e-11  5.55876718e-12  4.47114781e-11  4.76356109e-11
  4.38545994e-11  5.36495756e-12  4.31535068e-11  4.59757391e-11
 -7.44273248e-12 -2.38598421e-12 -8.53996288e-12 -1.50671462e-12
 -1.03874015e-12 -3.00537036e-13 -1.15188372e-12 -2.03791049e-13
  2.17635969e-11  7.28871569e-12  2.53622333e-11  4.46689742e-12
  1.50231870e-12  5.49615911e-13  1.80885637e-12  3.17646963e-13
  2.44689656e-12  3.87778164e-12  8.84536936e-13  1.96431659e-12
  1.29440851e-12  2.02736448e-12  4.37282410e-13  9.69727160e-13
  1.83465285e-12  2.93115148e-12  4.96469714e-13  1.18489163e-12
  1.85538507e-12  2.89217621e-12  4.72962034e-13  1.11445368e-12
  6.32411278e-13  1.14077959e-12  1.08318660e-13  5.93411915e-16
 -6.49716878e-14  9.74536110e-15 -2.90177316e-13 -1.78434779e-13
  1.73061307e+00]
supnorm grad right now is: 1.7306130729385658
Weights right now are: 
[-5.01744497 -0.73138313 -3.85031046 -4.67983915  5.27201376  0.62269145
  4.24924782  3.96381086 -2.27704152 -0.91144728 -2.258424   -2.00540983
  1.5481871   0.46376184 -0.02976393  0.80961748 -1.4497967  -0.515345
 -2.65726275 -2.21250846 -2.15844489 -0.95236248 -2.40722206 -1.82453029
 -0.97204374 -0.83276837 -1.72176146 -0.91255948  0.46465858  1.45508885
  1.54234912  0.50847194 -1.09466877 -0.63606138 -1.39192014 -1.20626455
 -2.52073368 -1.43647126 -1.29937253 -1.0725197  -0.59963651 -1.09712756
  0.24216206  0.33136355  0.4350469   1.12003367 -0.17885433  0.38782676
 -1.46293768 -0.57749384 -0.43940551 -1.59497516  0.61518199  1.16136312
  0.52644327  1.68102532  1.28398575 -0.47873075 -0.12925208 -2.1942629
  2.09374678  1.58998031 -0.07242375  1.0864316  25.09584422]
(65,)
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.6251203204465
gradient value of function right now is: [ 2.34491353e-09  2.26496103e-10  2.34359191e-09  2.34417411e-09
 -2.34509371e-09 -2.26513973e-10 -2.34377198e-09 -2.34435423e-09
  4.51460099e-11  5.51493005e-12  4.44191127e-11  4.73249589e-11
  2.56158735e-10  3.13134419e-11  2.52059813e-10  2.68535492e-10
  4.53989010e-11  5.54599106e-12  4.46680515e-11  4.75901508e-11
  4.38148570e-11  5.35235141e-12  4.31094526e-11  4.59295811e-11
 -7.40669016e-12 -2.40538407e-12 -8.54002908e-12 -1.50380406e-12
 -1.03443148e-12 -3.01753538e-13 -1.15031329e-12 -2.03315693e-13
  2.16800709e-11  7.37153221e-12  2.54140331e-11  4.46521463e-12
  1.50322549e-12  5.60460074e-13  1.82422700e-12  3.19262285e-13
  2.44514255e-12  3.84380584e-12  9.51977116e-13  2.03366908e-12
  1.29253296e-12  2.00888933e-12  4.71903094e-13  1.00546487e-12
  1.82059576e-12  2.89771861e-12  5.24299719e-13  1.21258760e-12
  1.84013616e-12  2.85758962e-12  5.01954507e-13  1.14341668e-12
  6.33004213e-13  1.10540648e-12  1.09003642e-13 -1.79748313e-15
 -6.63734732e-14  1.41080995e-14 -2.93158333e-13 -1.63494094e-13
 -2.23271952e+00]
supnorm grad right now is: 2.232719523854356
Weights right now are: 
[-5.02213936 -0.73183618 -3.85500221 -4.68453206  5.27670814  0.62314449
  4.25393956  3.96850376 -2.27713181 -0.91145832 -2.25851285 -2.00550448
  1.54767484  0.46369917 -0.03026803  0.80908044 -1.44988749 -0.51535609
 -2.65735208 -2.21260364 -2.15853251 -0.95237319 -2.40730828 -1.82462215
 -0.97202889 -0.83276358 -1.72174439 -0.91255647  0.46466065  1.45508945
  1.54235143  0.50847235 -1.09471219 -0.63607603 -1.39197088 -1.20627348
 -2.52073668 -1.43647237 -1.29937616 -1.07252033 -0.5996414  -1.09713527
  0.24216023  0.33135956  0.43504432  1.12002964 -0.17885524  0.38782479
 -1.46294134 -0.57749967 -0.43940653 -1.59497755  0.61517829  1.16135737
  0.5264423   1.68102307  1.28398448 -0.478733   -0.12925229 -2.1942629
  2.09374692  1.58998029 -0.07242317  1.08643194 24.81437192]
(65,)
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.571435739755
gradient value of function right now is: [ 2.21106285e-09  2.13753448e-10  2.20981873e-09  2.21036678e-09
 -2.21131449e-09 -2.13777876e-10 -2.21007022e-09 -2.21061834e-09
  4.25378603e-11  5.20544137e-12  4.18596620e-11  4.45964457e-11
  2.41917311e-10  2.96214425e-11  2.38082880e-10  2.53635608e-10
  4.27875755e-11  5.23614153e-12  4.21054990e-11  4.48583252e-11
  4.12912847e-11  5.05292113e-12  4.06330146e-11  4.32895617e-11
 -7.04601174e-12 -2.25255002e-12 -8.07638654e-12 -1.42551195e-12
 -9.96970165e-13 -2.88437231e-13 -1.10550886e-12 -1.95618281e-13
  2.05203746e-11  6.84945964e-12  2.38826740e-11  4.20848609e-12
  1.41051265e-12  5.13970829e-13  1.69551525e-12  2.97958545e-13
  2.30212133e-12  3.65507082e-12  8.18268116e-13  1.83388594e-12
  1.21947107e-12  1.91330842e-12  4.04615500e-13  9.05835767e-13
  1.72921473e-12  2.76504964e-12  4.61416876e-13  1.10934207e-12
  1.74927967e-12  2.72910494e-12  4.39109973e-13  1.04295728e-12
  5.94737743e-13  1.08009852e-12  1.01541417e-13  1.00494928e-15
 -6.08282772e-14  8.23826604e-15 -2.72595028e-13 -1.71168428e-13
  2.62151190e+00]
supnorm grad right now is: 2.6215119035459797
Weights right now are: 
[-5.02671172 -0.732278   -3.859572   -4.68910298  5.28128051  0.62358631
  4.25850935  3.97307469 -2.27721979 -0.91146908 -2.25859942 -2.00559672
  1.54717499  0.46363801 -0.03075992  0.80855641 -1.44997598 -0.51536692
 -2.65743916 -2.2126964  -2.15861791 -0.95238364 -2.40739231 -1.82471168
 -0.97201438 -0.83275891 -1.72172772 -0.91255353  0.46466269  1.45509004
  1.54235369  0.50847275 -1.09475455 -0.63609028 -1.39202033 -1.20628218
 -2.52073961 -1.43647344 -1.29937968 -1.07252095 -0.59964617 -1.0971428
  0.24215847  0.3313557   0.43504179  1.1200257  -0.17885611  0.38782288
 -1.4629449  -0.57750536 -0.43940751 -1.59497988  0.61517469  1.16135175
  0.52644136  1.68102088  1.28398325 -0.4787352  -0.12925251 -2.1942629
  2.09374704  1.58998027 -0.0724226   1.08643228 25.18830645]
(65,)
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.602643353504
gradient value of function right now is: [ 2.15503325e-09  2.08601062e-10  2.15382093e-09  2.15435492e-09
 -2.15495059e-09 -2.08592423e-10 -2.15373832e-09 -2.15427229e-09
  4.14837013e-11  5.07606466e-12  4.08224472e-11  4.34911467e-11
  2.36185371e-10  2.89175791e-11  2.32442635e-10  2.47625383e-10
  4.17365288e-11  5.10714036e-12  4.10713459e-11  4.37562866e-11
  4.02751697e-11  4.92821121e-12  3.96332306e-11  4.22241515e-11
 -6.88460639e-12 -2.20225754e-12 -7.89313596e-12 -1.39303426e-12
 -9.79747181e-13 -2.83756984e-13 -1.08678799e-12 -1.92295278e-13
  2.00189208e-11  6.68649320e-12  2.33050828e-11  4.10619836e-12
  1.37438026e-12  5.01270726e-13  1.65272072e-12  2.90384631e-13
  2.24493391e-12  3.56267149e-12  8.00922298e-13  1.79104207e-12
  1.18974395e-12  1.86585889e-12  3.96234146e-13  8.85072519e-13
  1.68592964e-12  2.69520639e-12  4.51192986e-13  1.08285351e-12
  1.70560670e-12  2.66035087e-12  4.29533188e-13  1.01828998e-12
  5.79854310e-13  1.05179078e-12  9.88800557e-14  8.41961514e-16
 -5.93624817e-14  8.22781748e-15 -2.65944402e-13 -1.66344218e-13
  2.43781183e+00]
supnorm grad right now is: 2.437811831267842
Weights right now are: 
[-5.03114838 -0.73270728 -3.86400616 -4.69353825  5.28571717  0.6240156
  4.26294351  3.97750995 -2.27730523 -0.91147952 -2.2586835  -2.00568629
  1.54668911  0.46357857 -0.03123806  0.80804702 -1.45006193 -0.51537742
 -2.65752373 -2.21278651 -2.15870086 -0.95239378 -2.40747392 -1.82479863
 -0.97200027 -0.83275436 -1.72171149 -0.91255067  0.46466469  1.45509062
  1.54235591  0.50847314 -1.09479569 -0.63610415 -1.3920684  -1.20629064
 -2.52074244 -1.43647449 -1.29938311 -1.07252155 -0.59965079 -1.09715011
  0.24215673  0.33135192  0.43503935  1.12002187 -0.17885697  0.38782101
 -1.46294836 -0.57751088 -0.43940848 -1.59498214  0.61517119  1.16134631
  0.52644044  1.68101875  1.28398205 -0.47873733 -0.12925271 -2.19426289
  2.09374717  1.58998025 -0.07242205  1.0864326  25.14811758]
(65,)
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.1619895291615
gradient value of function right now is: [ 2.18959892e-09  2.12325946e-10  2.18836520e-09  2.18890852e-09
 -2.18996211e-09 -2.12361795e-10 -2.18872818e-09 -2.18927159e-09
  4.22575716e-11  5.15839480e-12  4.15757620e-11  4.42951921e-11
  2.40485246e-10  2.93776768e-11  2.36629699e-10  2.52094512e-10
  4.25226392e-11  5.19091582e-12  4.18366685e-11  4.45731341e-11
  4.10320279e-11  5.00882408e-12  4.03700494e-11  4.30105761e-11
 -6.95739233e-12 -2.27386607e-12 -8.04121936e-12 -1.41462570e-12
 -9.86641324e-13 -2.89530419e-13 -1.09936458e-12 -1.94209277e-13
  2.02905501e-11  6.94987957e-12  2.38536077e-11  4.18606469e-12
  1.40493926e-12  5.28769148e-13  1.71167793e-12  2.99055993e-13
  2.28811379e-12  3.58284183e-12  9.22418805e-13  1.93583236e-12
  1.21069722e-12  1.87460195e-12  4.58274227e-13  9.58725968e-13
  1.69856022e-12  2.69849538e-12  5.03702749e-13  1.14812168e-12
  1.71666712e-12  2.66092760e-12  4.83383687e-13  1.08412662e-12
  5.92827037e-13  1.01847256e-12  1.02072268e-13 -2.81983366e-15
 -6.27553607e-14  1.51958543e-14 -2.76003270e-13 -1.46404258e-13
 -4.07668183e+00]
supnorm grad right now is: 4.076681825047594
Weights right now are: 
[-5.03546575 -0.73312553 -3.8683211  -4.69785425  5.29003455  0.62443385
  4.26725846  3.98182597 -2.27738843 -0.9114897  -2.25876536 -2.00577351
  1.54621547  0.46352063 -0.03170416  0.80755047 -1.45014564 -0.51538766
 -2.6576061  -2.21287426 -2.15878163 -0.95240365 -2.40755341 -1.82488331
 -0.9719865  -0.83274992 -1.72169566 -0.91254788  0.46466665  1.45509119
  1.54235808  0.50847353 -1.09483576 -0.63611767 -1.39211523 -1.20629887
 -2.5207452  -1.43647551 -1.29938645 -1.07252214 -0.59965529 -1.09715722
  0.24215504  0.33134824  0.43503696  1.12001815 -0.17885781  0.38781919
 -1.46295173 -0.57751625 -0.43940942 -1.59498435  0.61516779  1.16134101
  0.52643954  1.68101667  1.28398089 -0.47873939 -0.12925291 -2.19426289
  2.09374729  1.58998022 -0.07242151  1.08643292 24.64707705]
(65,)
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.7712437700975
gradient value of function right now is: [ 2.06413774e-09  2.00308700e-10  2.06297675e-09  2.06348804e-09
 -2.06475268e-09 -2.00368997e-10 -2.06359132e-09 -2.06410277e-09
  3.98023771e-11  4.86795056e-12  3.91669495e-11  4.17272443e-11
  2.27038302e-10  2.77848243e-11  2.23435379e-10  2.38028767e-10
  4.00621214e-11  4.89985607e-12  3.94226463e-11  4.19996274e-11
  3.86545075e-11  4.72758779e-12  3.80374570e-11  4.05238811e-11
 -6.61933764e-12 -2.12691200e-12 -7.60160098e-12 -1.34077871e-12
 -9.50880755e-13 -2.76494859e-13 -1.05616536e-12 -1.86815171e-13
  1.92046019e-11  6.44778466e-12  2.24014613e-11  3.94405398e-12
  1.31757434e-12  4.83731722e-13  1.58868196e-12  2.78840609e-13
  2.15373210e-12  3.40892866e-12  7.88387696e-13  1.73962930e-12
  1.14202584e-12  1.78653186e-12  3.90577435e-13  8.60557511e-13
  1.61389939e-12  2.57698057e-12  4.41268919e-13  1.04777699e-12
  1.63259218e-12  2.54342116e-12  4.20905368e-13  9.86305568e-13
  5.56789400e-13  9.98691727e-13  9.50242391e-14  2.06049319e-17
 -5.74170976e-14  9.24193783e-15 -2.56369971e-13 -1.55215500e-13
  1.09932876e+00]
supnorm grad right now is: 1.0993287572229644
Weights right now are: 
[-5.03968847 -0.73353509 -3.87254144 -4.70207564  5.29425727  0.62484341
  4.2714788   3.98604736 -2.27746983 -0.91149965 -2.25884546 -2.00585885
  1.54575144  0.46346386 -0.03216082  0.80706399 -1.45022757 -0.51539767
 -2.65768672 -2.21296015 -2.15886069 -0.95241332 -2.4076312  -1.82496619
 -0.971973   -0.83274557 -1.72168013 -0.91254514  0.46466858  1.45509175
  1.54236023  0.50847391 -1.094875   -0.63613088 -1.39216106 -1.20630694
 -2.5207479  -1.4364765  -1.29938971 -1.07252271 -0.5996597  -1.09716418
  0.2421534   0.33134466  0.43503463  1.12001451 -0.17885862  0.38781742
 -1.46295502 -0.5775215  -0.43941033 -1.5949865   0.61516445  1.16133582
  0.52643867  1.68101464  1.28397975 -0.47874142 -0.1292531  -2.19426289
  2.09374741  1.5899802  -0.07242099  1.08643323 25.0521607 ]
(65,)
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.734914487716
gradient value of function right now is: [ 2.04691582e-09  1.98912428e-10  2.04576400e-09  2.04627119e-09
 -2.04677179e-09 -1.98898110e-10 -2.04562005e-09 -2.04612720e-09
  3.95130201e-11  4.82808727e-12  3.88794705e-11  4.14213283e-11
  2.25500746e-10  2.75725707e-11  2.21907315e-10  2.36402994e-10
  3.97782788e-11  4.86064406e-12  3.91405796e-11  4.16994794e-11
  3.83800119e-11  4.68967324e-12  3.77646797e-11  4.02336203e-11
 -6.55961757e-12 -2.12514600e-12 -7.55637578e-12 -1.33112169e-12
 -9.44212289e-13 -2.76087116e-13 -1.05075391e-12 -1.85735275e-13
  1.90340805e-11  6.45207658e-12  2.22855722e-11  3.91750723e-12
  1.30896284e-12  4.86371391e-13  1.58619374e-12  2.77794041e-13
  2.13865120e-12  3.36733910e-12  8.20742779e-13  1.76637721e-12
  1.13364515e-12  1.76459212e-12  4.07331601e-13  8.74699862e-13
  1.59550470e-12  2.54129569e-12  4.53860232e-13  1.05605171e-12
  1.61346131e-12  2.50740856e-12  4.34324479e-13  9.95795053e-13
  5.53283206e-13  9.72427918e-13  9.46604254e-14 -1.31090801e-15
 -5.78078446e-14  1.16137543e-14 -2.56266316e-13 -1.45875715e-13
 -1.45991763e+00]
supnorm grad right now is: 1.4599176263701503
Weights right now are: 
[-5.04380013 -0.73393435 -3.87665079 -4.70618601  5.29836894  0.62524267
  4.27558816  3.99015773 -2.27754914 -0.91150935 -2.25892351 -2.00594199
  1.54529887  0.46340849 -0.03260619  0.80658952 -1.4503074  -0.51540744
 -2.65776528 -2.21304385 -2.15893772 -0.95242273 -2.407707   -1.82504694
 -0.97195981 -0.83274132 -1.72166497 -0.91254247  0.46467048  1.45509231
  1.54236234  0.50847428 -1.09491326 -0.63614376 -1.39220573 -1.2063148
 -2.52075053 -1.43647747 -1.29939288 -1.07252327 -0.59966399 -1.09717096
  0.24215181  0.33134117  0.43503235  1.12001095 -0.17885941  0.3878157
 -1.46295824 -0.57752663 -0.43941122 -1.5949886   0.6151612   1.16133077
  0.52643782  1.68101267  1.28397864 -0.4787434  -0.12925329 -2.19426289
  2.09374752  1.58998018 -0.07242048  1.08643353 24.8754994 ]
(65,)
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.669995072664
gradient value of function right now is: [ 2.00443572e-09  1.95013699e-10  2.00330792e-09  2.00380448e-09
 -2.00400633e-09 -1.94971621e-10 -2.00287879e-09 -2.00337524e-09
  3.87165607e-11  4.72988133e-12  3.80954912e-11  4.05859598e-11
  2.21161428e-10  2.70372116e-11  2.17635641e-10  2.31851521e-10
  3.89841385e-11  4.76271409e-12  3.83588791e-11  4.08665384e-11
  3.76123190e-11  4.59500717e-12  3.70090132e-11  3.94284201e-11
 -6.43526363e-12 -2.08830102e-12 -7.41772319e-12 -1.30637690e-12
 -9.30594483e-13 -2.72550741e-13 -1.03616668e-12 -1.83132370e-13
  1.86511289e-11  6.33425037e-12  2.18533508e-11  3.84034064e-12
  1.28187315e-12  4.77467701e-13  1.55494653e-12  2.72200065e-13
  2.09545830e-12  3.29583842e-12  8.11670394e-13  1.73829898e-12
  1.11109717e-12  1.72775010e-12  4.03074971e-13  8.61212354e-13
  1.56204840e-12  2.48676758e-12  4.47789384e-13  1.03778406e-12
  1.57962185e-12  2.45360143e-12  4.28808093e-13  9.78960571e-13
  5.42146137e-13  9.49056202e-13  9.27225114e-14 -1.56695827e-15
 -5.67837407e-14  1.18443577e-14 -2.51448178e-13 -1.41379925e-13
 -1.95289081e+00]
supnorm grad right now is: 1.9528908143595431
Weights right now are: 
[-5.04779594 -0.73432282 -3.88064436 -4.71018056  5.30236475  0.62563114
  4.27958172  3.99415228 -2.27762627 -0.91151877 -2.2589994  -2.00602285
  1.54485834  0.4633546  -0.03303973  0.80612767 -1.45038506 -0.51541693
 -2.65784169 -2.21312525 -2.15901264 -0.95243189 -2.40778072 -1.82512548
 -0.97194696 -0.83273718 -1.7216502  -0.91253986  0.46467233  1.45509285
  1.54236441  0.50847465 -1.09495046 -0.63615631 -1.3922492  -1.20632245
 -2.52075308 -1.43647841 -1.29939596 -1.07252381 -0.59966817 -1.09717755
  0.24215025  0.33133777  0.43503013  1.1200075  -0.17886018  0.38781401
 -1.46296136 -0.57753161 -0.43941209 -1.59499064  0.61515804  1.16132585
  0.52643699  1.68101074  1.28397756 -0.47874532 -0.12925348 -2.19426289
  2.09374763  1.58998016 -0.07241998  1.08643383 24.86493842]
(65,)
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.758056520094
gradient value of function right now is: [ 1.91764228e-09  1.86725909e-10  1.91656455e-09  1.91703905e-09
 -1.91718373e-09 -1.86680535e-10 -1.91610628e-09 -1.91658066e-09
  3.70256518e-11  4.52851843e-12  3.64356202e-11  3.88165825e-11
  2.11876920e-10  2.59302989e-11  2.08520664e-10  2.22135411e-10
  3.72896916e-11  4.56094034e-12  3.66955449e-11  3.90934655e-11
  3.59753148e-11  4.40007826e-12  3.54020667e-11  3.77154579e-11
 -6.19602499e-12 -1.98997901e-12 -7.11424153e-12 -1.25490177e-12
 -9.04536989e-13 -2.63459486e-13 -1.00522115e-12 -1.77804317e-13
  1.78917416e-11  6.00301083e-12  2.08647846e-11  3.67379418e-12
  1.22222295e-12  4.48459535e-13  1.47335179e-12  2.58622858e-13
  2.00286442e-12  3.17073470e-12  7.31092777e-13  1.61513345e-12
  1.06361058e-12  1.66414856e-12  3.62510823e-13  7.99785304e-13
  1.50175861e-12  2.39801220e-12  4.09570457e-13  9.73397660e-13
  1.51955519e-12  2.36743857e-12  3.90704044e-13  9.16477627e-13
  5.17478991e-13  9.29751025e-13  8.79651875e-14  7.13798300e-18
 -5.33369350e-14  8.47039047e-15 -2.38394775e-13 -1.44984837e-13
  1.25341154e+00]
supnorm grad right now is: 1.2534115431848376
Weights right now are: 
[-5.05170437 -0.73470322 -3.88455059 -4.71408777  5.30627318  0.62601154
  4.28348796  3.99805949 -2.27770175 -0.911528   -2.25907367 -2.00610197
  1.54442677  0.4633018  -0.03346444  0.80567522 -1.45046107 -0.51542622
 -2.65791648 -2.21320494 -2.15908597 -0.95244086 -2.40785288 -1.82520236
 -0.97193437 -0.83273312 -1.72163572 -0.91253731  0.46467417  1.45509338
  1.54236644  0.50847501 -1.09498689 -0.63616859 -1.39229175 -1.20632994
 -2.52075557 -1.43647933 -1.29939897 -1.07252433 -0.59967225 -1.097184
  0.24214873  0.33133444  0.43502797  1.12000411 -0.17886093  0.38781236
 -1.46296441 -0.57753648 -0.43941294 -1.59499264  0.61515495  1.16132104
  0.52643618  1.68100886  1.2839765  -0.4787472  -0.12925366 -2.19426289
  2.09374774  1.58998014 -0.07241949  1.08643412 25.07926431]
(65,)
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.50349316208
gradient value of function right now is: [ 1.85337988e-09  1.80637990e-10  1.85233903e-09  1.85279727e-09
 -1.85284073e-09 -1.80584959e-10 -1.85180018e-09 -1.85225829e-09
  3.57849199e-11  4.37949667e-12  3.52168174e-11  3.75175254e-11
  2.05064649e-10  2.51113244e-11  2.01828204e-10  2.15002457e-10
  3.60474442e-11  4.41174400e-12  3.54752605e-11  3.77928266e-11
  3.47750339e-11  4.25592415e-12  3.42230077e-11  3.64587591e-11
 -6.01538161e-12 -1.92117051e-12 -6.89240759e-12 -1.21676665e-12
 -8.84579492e-13 -2.56940207e-13 -9.82104738e-13 -1.73785553e-13
  1.73244840e-11  5.77438488e-12  2.01518114e-11  3.55188229e-12
  1.17891357e-12  4.29030896e-13  1.41634350e-12  2.48973545e-13
  1.93501963e-12  3.07431163e-12  6.83084389e-13  1.53636880e-12
  1.02862691e-12  1.61487149e-12  3.38469780e-13  7.60689410e-13
  1.45550399e-12  2.32805703e-12  3.86088064e-13  9.30850623e-13
  1.47324412e-12  2.29914718e-12  3.67474052e-13  8.75487566e-13
  4.99566118e-13  9.09979772e-13  8.46202535e-14  8.20327973e-16
 -5.10204795e-14  6.63863663e-15 -2.29328351e-13 -1.45339747e-13
  2.99096759e+00]
supnorm grad right now is: 2.9909675907787014
Weights right now are: 
[-5.05552137 -0.73507515 -3.88836544 -4.71790356  5.31009018  0.62638346
  4.28730281  4.00187528 -2.27777551 -0.91153702 -2.25914625 -2.0061793
  1.54400464  0.46325017 -0.03387987  0.80523266 -1.45053536 -0.5154353
 -2.65798959 -2.21328282 -2.15915764 -0.95244962 -2.4079234  -1.82527749
 -0.97192204 -0.83272914 -1.72162154 -0.91253481  0.46467596  1.45509391
  1.54236844  0.50847536 -1.0950225  -0.6361806  -1.39233337 -1.20633726
 -2.52075801 -1.43648023 -1.29940192 -1.07252485 -0.59967624 -1.0971903
  0.24214724  0.33133117  0.43502585  1.12000081 -0.17886168  0.38781075
 -1.4629674  -0.57754124 -0.43941377 -1.5949946   0.61515194  1.16131635
  0.52643539  1.68100701  1.28397547 -0.47874903 -0.12925383 -2.19426289
  2.09374785  1.58998012 -0.07241901  1.08643439 25.17897009]
(65,)
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.815158766932
gradient value of function right now is: [ 1.86697803e-09  1.82105830e-10  1.86592882e-09  1.86639071e-09
 -1.86739758e-09 -1.82146932e-10 -1.86634813e-09 -1.86681012e-09
  3.60959424e-11  4.41293211e-12  3.55198255e-11  3.78409051e-11
  2.06802139e-10  2.52990892e-11  2.03521441e-10  2.16809564e-10
  3.63636770e-11  4.44579225e-12  3.57833787e-11  3.81216546e-11
  3.50785026e-11  4.28856852e-12  3.45186715e-11  3.67742971e-11
 -6.04625267e-12 -1.94944045e-12 -6.95234062e-12 -1.22565945e-12
 -8.87703104e-13 -2.59384298e-13 -9.87576119e-13 -1.74629000e-13
  1.74367342e-11  5.87673379e-12  2.03695226e-11  3.58410867e-12
  1.19109718e-12  4.39537496e-13  1.43920681e-12  2.52382624e-13
  1.95247537e-12  3.08394723e-12  7.28710490e-13  1.59145438e-12
  1.03712508e-12  1.61920091e-12  3.61734411e-13  7.88669348e-13
  1.46108559e-12  2.33070964e-12  4.05940551e-13  9.55931061e-13
  1.47823939e-12  2.30072479e-12  3.87855723e-13  9.00767175e-13
  5.04838532e-13  8.98065434e-13  8.59121205e-14 -5.66825732e-16
 -5.23377724e-14  9.28371632e-15 -2.33307198e-13 -1.37907571e-13
  5.73939294e-02]
supnorm grad right now is: 0.05739392944376842
Weights right now are: 
[-5.05925941 -0.73543976 -3.89210138 -4.72164042  5.31382821  0.62674808
  4.29103874  4.00561214 -2.27784778 -0.91154585 -2.25921737 -2.00625506
  1.54359062  0.46319953 -0.03428731  0.80479862 -1.45060816 -0.5154442
 -2.65806123 -2.21335914 -2.15922787 -0.95245821 -2.40799251 -1.82535112
 -0.97190994 -0.83272523 -1.72160762 -0.91253236  0.46467774  1.45509443
  1.54237042  0.50847571 -1.09505739 -0.63619238 -1.39237416 -1.20634444
 -2.52076039 -1.43648111 -1.2994048  -1.07252536 -0.59968015 -1.09719647
  0.24214577  0.33132798  0.43502377  1.11999757 -0.1788624   0.38780916
 -1.46297032 -0.5775459  -0.43941459 -1.59499652  0.61514898  1.16131175
  0.5264346   1.6810052   1.28397446 -0.47875082 -0.12925401 -2.19426289
  2.09374795  1.5899801  -0.07241854  1.08643467 24.98430173]
(65,)
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.815270462516
gradient value of function right now is: [ 1.84718939e-09  1.80271638e-10  1.84615139e-09  1.84660832e-09
 -1.84703003e-09 -1.80256128e-10 -1.84599213e-09 -1.84644902e-09
  3.57172934e-11  4.36651170e-12  3.51472673e-11  3.74439103e-11
  2.04731644e-10  2.50451062e-11  2.01484086e-10  2.14638669e-10
  3.59853950e-11  4.39941479e-12  3.54111813e-11  3.77250438e-11
  3.47135087e-11  4.24382051e-12  3.41595484e-11  3.63916184e-11
 -5.98784250e-12 -1.93110372e-12 -6.88583179e-12 -1.21388676e-12
 -8.81191204e-13 -2.57597390e-13 -9.80474871e-13 -1.73369547e-13
  1.72564588e-11  5.81762416e-12  2.01611417e-11  3.54726027e-12
  1.17813245e-12  4.34926834e-13  1.42377724e-12  2.49657133e-13
  1.93190204e-12  3.05091712e-12  7.22162948e-13  1.57569690e-12
  1.02639827e-12  1.60219287e-12  3.58557489e-13  7.81013093e-13
  1.44557714e-12  2.30578865e-12  4.02141639e-13  9.46261310e-13
  1.46256727e-12  2.27617122e-12  3.84283552e-13  8.91751392e-13
  4.99496972e-13  8.88177230e-13  8.49414062e-14 -6.11044058e-16
 -5.18049012e-14  9.26137124e-15 -2.30892475e-13 -1.36260254e-13
 -1.83467351e-02]
supnorm grad right now is: 0.018346735058031506
Weights right now are: 
[-5.06289529 -0.73579479 -3.89573522 -4.72527516  5.3174641   0.62710311
  4.29467258  4.00924688 -2.27791811 -0.91155445 -2.25928657 -2.00632879
  1.54318733  0.46315019 -0.0346842   0.80437581 -1.45067902 -0.51545287
 -2.65813096 -2.21343343 -2.15929623 -0.95246656 -2.40805978 -1.82542278
 -0.97189815 -0.83272142 -1.72159405 -0.91252996  0.46467948  1.45509494
  1.54237236  0.50847605 -1.09509138 -0.63620385 -1.39241387 -1.20635143
 -2.52076271 -1.43648197 -1.29940761 -1.07252585 -0.59968395 -1.09720248
  0.24214435  0.33132487  0.43502175  1.11999441 -0.17886311  0.38780762
 -1.46297317 -0.57755044 -0.43941538 -1.59499839  0.6151461   1.16130727
  0.52643385  1.68100344  1.28397348 -0.47875257 -0.12925417 -2.19426288
  2.09374806  1.58998009 -0.07241809  1.08643494 24.99248425]
(65,)
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.815231544306
gradient value of function right now is: [ 1.82726028e-09  1.78419550e-10  1.82623360e-09  1.82668553e-09
 -1.82707028e-09 -1.78401015e-10 -1.82604370e-09 -1.82649559e-09
  3.53404854e-11  4.32039860e-12  3.47765756e-11  3.70488940e-11
  2.02667754e-10  2.47923712e-11  1.99453533e-10  2.12474957e-10
  3.56089882e-11  4.35334911e-12  3.50408843e-11  3.73304474e-11
  3.43496682e-11  4.19929385e-12  3.38016138e-11  3.60101960e-11
 -5.92982250e-12 -1.91255157e-12 -6.81932114e-12 -1.20214796e-12
 -8.74682453e-13 -2.55778407e-13 -9.73336041e-13 -1.72105396e-13
  1.70776267e-11  5.75781269e-12  1.99528552e-11  3.51055955e-12
  1.16524586e-12  4.30232080e-13  1.40828799e-12  2.46934804e-13
  1.91142528e-12  3.01840514e-12  7.14855759e-13  1.55930764e-12
  1.01573499e-12  1.58545779e-12  3.54988821e-13  7.73019487e-13
  1.43027172e-12  2.28130381e-12  3.98030378e-13  9.36374642e-13
  1.44712850e-12  2.25207592e-12  3.80380641e-13  8.82482629e-13
  4.94180799e-13  8.78595961e-13  8.39996920e-14 -6.27323269e-16
 -5.12596601e-14  9.18467212e-15 -2.28478501e-13 -1.34772465e-13
 -3.86429672e-02]
supnorm grad right now is: 0.03864296721299375
Weights right now are: 
[-5.06647581 -0.73614475 -3.89931372 -4.72885455  5.32104461  0.62745307
  4.29825108  4.01282627 -2.27798739 -0.91156292 -2.25935475 -2.00640142
  1.54278962  0.46310153 -0.03507561  0.80395885 -1.45074885 -0.5154614
 -2.65819967 -2.21350663 -2.15936358 -0.9524748  -2.40812605 -1.82549338
 -0.9718865  -0.83271767 -1.72158066 -0.9125276   0.46468121  1.45509544
  1.54237428  0.50847639 -1.09512488 -0.63621513 -1.39245299 -1.20635831
 -2.520765   -1.43648281 -1.29941036 -1.07252633 -0.5996877  -1.0972084
  0.24214296  0.33132182  0.43501976  1.1199913  -0.1788638   0.38780611
 -1.46297597 -0.57755492 -0.43941616 -1.59500022  0.61514326  1.16130285
  0.5264331   1.68100172  1.28397251 -0.4787543  -0.12925434 -2.19426288
  2.09374816  1.58998007 -0.07241764  1.0864352  25.13023101]
(65,)
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.815009644635
gradient value of function right now is: [ 1.80815882e-09  1.76645668e-10  1.80714297e-09  1.80759012e-09
 -1.80844124e-09 -1.76673848e-10 -1.80742522e-09 -1.80787244e-09
  3.49848548e-11  4.27683751e-12  3.44266771e-11  3.66760572e-11
  2.00716884e-10  2.45532548e-11  1.97533955e-10  2.10429612e-10
  3.52539184e-11  4.30985489e-12  3.46915371e-11  3.69581978e-11
  3.40058020e-11  4.15717270e-12  3.34632911e-11  3.56496923e-11
 -5.87460120e-12 -1.89516496e-12 -6.75635470e-12 -1.19103124e-12
 -8.68414432e-13 -2.54046785e-13 -9.66487332e-13 -1.70891207e-13
  1.69083275e-11  5.70213348e-12  1.97568297e-11  3.47602092e-12
  1.15316789e-12  4.25912452e-13  1.39386810e-12  2.44401947e-13
  1.89203053e-12  2.98764607e-12  7.08291722e-13  1.54452300e-12
  1.00561347e-12  1.56958835e-12  3.51785166e-13  7.65804559e-13
  1.41567382e-12  2.25800999e-12  3.94311262e-13  9.27412077e-13
  1.43237855e-12  2.22910386e-12  3.76856921e-13  8.74074117e-13
  4.89271224e-13  8.69203330e-13  8.31789593e-14 -6.57373466e-16
 -5.07672977e-14  9.13589811e-15 -2.26302385e-13 -1.33265757e-13
 -8.89837913e-02]
supnorm grad right now is: 0.08898379125847827
Weights right now are: 
[-5.06996458 -0.7364861  -3.90280053 -4.73234223  5.32453339  0.62779442
  4.3017379   4.01631395 -2.27805493 -0.91157118 -2.25942122 -2.00647223
  1.54240155  0.46305406 -0.03545754  0.803552   -1.45081693 -0.51546973
 -2.65826667 -2.213578   -2.15942924 -0.95248282 -2.40819067 -1.82556223
 -0.97187512 -0.832714   -1.72156758 -0.9125253   0.4646829   1.45509594
  1.54237616  0.50847673 -1.09515755 -0.63622614 -1.39249116 -1.20636503
 -2.52076722 -1.43648363 -1.29941305 -1.0725268  -0.59969135 -1.09721417
  0.24214159  0.33131885  0.43501782  1.11998827 -0.17886448  0.38780464
 -1.4629787  -0.57755927 -0.43941691 -1.59500201  0.61514049  1.16129854
  0.52643238  1.68100003  1.28397156 -0.47875597 -0.1292545  -2.19426288
  2.09374826  1.58998005 -0.0724172   1.08643546 24.81482114]
(65,)
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5142.815248635043
gradient value of function right now is: [ 1.78826370e-09  1.74789336e-10  1.78725916e-09  1.78770132e-09
 -1.78834952e-09 -1.74798004e-10 -1.78734492e-09 -1.78778710e-09
  3.46054991e-11  4.23054993e-12  3.40535694e-11  3.62784505e-11
  1.98637465e-10  2.42993610e-11  1.95488614e-10  2.08250055e-10
  3.48746954e-11  4.26358265e-12  3.43185605e-11  3.65607300e-11
  3.36395031e-11  4.11247875e-12  3.31030252e-11  3.52657710e-11
 -5.81669863e-12 -1.87608259e-12 -6.68922623e-12 -1.17923018e-12
 -8.61911005e-13 -2.52183046e-13 -9.59294119e-13 -1.69620923e-13
  1.67293497e-11  5.64023572e-12  1.95456631e-11  3.43898380e-12
  1.14013654e-12  4.20970281e-13  1.37794375e-12  2.41620609e-13
  1.87144554e-12  2.95543022e-12  6.99737375e-13  1.52668942e-12
  9.94908640e-13  1.55302288e-12  3.47571842e-13  7.57068140e-13
  1.40049730e-12  2.23388867e-12  3.89669521e-13  9.16895579e-13
  1.41709252e-12  2.20540186e-12  3.72413040e-13  8.64173904e-13
  4.83873027e-13  8.60174946e-13  8.21991295e-14 -6.31781877e-16
 -5.01902632e-14  8.98414172e-15 -2.23794170e-13 -1.32023488e-13
 -2.81045555e-02]
supnorm grad right now is: 0.028104555539179107
Weights right now are: 
[-5.07338763 -0.73682136 -3.90622167 -4.7357642   5.32795644  0.62812968
  4.30515903  4.01973593 -2.27812123 -0.91157928 -2.25948646 -2.00654174
  1.54202027  0.46300742 -0.03583278  0.80315226 -1.45088377 -0.5154779
 -2.65833244 -2.21364808 -2.15949371 -0.95249071 -2.40825412 -1.82562981
 -0.97186394 -0.83271039 -1.72155471 -0.91252303  0.46468457  1.45509643
  1.54237802  0.50847706 -1.09518963 -0.63623695 -1.39252864 -1.20637162
 -2.5207694  -1.43648444 -1.29941569 -1.07252727 -0.59969494 -1.09721983
  0.24214026  0.33131592  0.43501591  1.11998529 -0.17886514  0.38780318
 -1.46298139 -0.57756356 -0.43941766 -1.59500377  0.61513778  1.16129432
  0.52643167  1.68099837  1.28397064 -0.47875762 -0.12925466 -2.19426288
  2.09374835  1.58998003 -0.07241677  1.08643571 25.23545528]
(65,)
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1551.6347824812183
W_T_median: 1406.0687944591186
W_T_pctile_5: 624.1985510158019
W_T_CVAR_5_pct: 487.91093300726106
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
F value: -5142.815248635043
-----------------------------------------------
