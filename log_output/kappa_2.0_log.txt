tracing parameter entered from terminal:  2.0


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = basic_T30_VWD
timeseries_basket['basket_desc'] = CRSP data: T30 and VWD
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
5.0% of MC simulations done.
10.0% of MC simulations done.
15.0% of MC simulations done.
20.0% of MC simulations done.
25.0% of MC simulations done.
30.0% of MC simulations done.
35.0% of MC simulations done.
40.0% of MC simulations done.
45.0% of MC simulations done.
50.0% of MC simulations done.
55.00000000000001% of MC simulations done.
60.0% of MC simulations done.
65.0% of MC simulations done.
70.0% of MC simulations done.
75.0% of MC simulations done.
80.0% of MC simulations done.
85.0% of MC simulations done.
90.0% of MC simulations done.
95.0% of MC simulations done.
100.0% of MC simulations done.
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  hidden_layer    None       None         None   
4        obj.layers[4]        4  hidden_layer    None       None         None   
5        obj.layers[5]        5  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  
4    False        None  
5    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  hidden_layer       4  logistic_sigmoid   
4        obj.layers[4]        4  hidden_layer       4  logistic_sigmoid   
5        obj.layers[5]        5  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 4)    False        None  
4       (4, 4)    False        None  
5       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
[-0.62277812  0.75294104  0.37108154 -0.23457045  0.87734683 -0.86163273
  0.02785573 -0.48145794 -0.54600176  0.39576202 -0.65000431 -0.36885567
  0.66919174 -0.62014212 -0.6576227  -0.09547811  0.44289714 -0.62745997
 -0.69254554 -0.71497204 -0.09160633 -0.70209214 -0.08649162 -0.03995664
  0.43014226  0.54196156 -0.18869499  0.3803182  -0.4213674   0.51582093
  0.719146   -0.41932646  0.60494673  0.54081414  0.80135845 -0.24869297
 -0.78078131  0.14937982  0.57187137  0.48684314 -0.81297698 -0.8032804
 -0.30872557  0.20747554 -0.27489946  0.56429979 -0.5811595  -0.16646221
 -0.44248831  0.58917257  0.54882857 -0.57567892 -0.30430046  0.43734894
 -0.41036775  0.8237008   0.69471977 -0.63290457 -0.75257106 -0.88717124
  0.98028969  0.25945333  0.60467509  0.43128316] (64,)
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.577071555226
gradient value of function right now is: [ 8.06409242e-06  6.37552637e-07  8.06197977e-06  8.06550068e-06
 -8.06409208e-06 -6.37552609e-07 -8.06197942e-06 -8.06550034e-06
  1.48551368e-07  1.78614120e-08  1.47893528e-07  1.56979977e-07
  5.45230182e-07  6.54923311e-08  5.42807961e-07  5.76104951e-07
  1.12926194e-07  1.35783356e-08  1.12426205e-07  1.19333827e-07
  1.04896378e-07  1.26126831e-08  1.04431708e-07  1.10848225e-07
 -9.61306885e-09 -2.73822361e-09 -1.11552728e-08 -1.75553020e-09
  1.43269037e-09  4.26083405e-10  1.67332751e-09  2.76507407e-10
  5.25497349e-08  1.49252405e-08  6.07084229e-08  9.78681654e-09
  5.76433281e-09  1.58163018e-09  6.56165081e-09  1.08637041e-09
  9.10133995e-09  1.27924840e-08  6.46036673e-10  4.44552389e-09
  2.80587975e-09  3.97603999e-09 -9.34744188e-11  1.12709416e-09
  6.35046958e-09  8.80086798e-09  4.53417312e-10  2.83685373e-09
  4.93732509e-09  6.65206706e-09  3.42455301e-10  1.96075593e-09
  1.92453694e-09  2.93551241e-09  1.03337941e-09  3.12832895e-10
 -4.43793206e-10 -8.10229388e-11 -6.93671486e-10 -7.49409871e-10
 -3.92486315e+00]
supnorm grad right now is: 3.9248631471774815
Weights right now are: 
[-3.62567704 -0.56040308 -2.40011684 -3.19185121  3.88024575  0.45171139
  2.79905411  2.47582283 -2.19074109 -0.84156376 -2.15432972 -1.91080973
  1.62082485  0.4368784   0.06729449  0.89007779 -1.48961638 -0.41727675
 -2.53621248 -2.16625322 -2.2313289  -0.8014385  -2.29815864 -1.81099831
 -0.91876519 -0.74471358 -1.66388543 -0.83498549  0.4710722   1.45824095
  1.56350379  0.50670209 -1.08757716 -0.42557769 -1.50820263 -1.00710825
 -2.51863056 -1.34203362 -1.31368973 -0.98122506 -0.48663447 -1.12749021
  0.57882104  0.52928716  0.4359723   1.12828768 -0.38031802  0.39847843
 -1.49740338 -0.6267043  -0.42806559 -1.59872262  0.63303635  1.18686467
  0.55530538  1.71051879  1.20652695 -0.85895427 -0.10806403 -1.87775424
  2.04300651  1.72872421 -0.13328458  1.04711393 24.6885468 ]
(65,)
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1555320233656
gradient value of function right now is: [ 9.57209625e-08  1.00401765e-08  9.56986440e-08  9.57357713e-08
 -9.57206039e-08 -1.00401382e-08 -9.56982855e-08 -9.57354126e-08
  1.93510680e-09  2.33137358e-10  1.93241826e-09  2.04669399e-09
  9.54060085e-09  1.14768497e-09  9.52654855e-09  1.00890467e-08
  1.56783426e-09  1.88896272e-10  1.56565934e-09  1.65824939e-09
  1.45338672e-09  1.75104874e-10  1.45136676e-09  1.53719900e-09
 -2.80018041e-10 -7.64050826e-11 -3.19874730e-10 -5.09327009e-11
 -4.73616013e-11 -1.30035306e-11 -5.43620460e-11 -8.49989107e-12
  7.79903234e-10  2.05173393e-10  8.78358999e-10  1.42605436e-10
  5.83198540e-11  1.34229342e-11  6.27614534e-11  1.06512556e-11
  1.12926193e-10  1.60653318e-10 -2.31851002e-12  4.54670196e-11
  3.69744932e-11  5.28739714e-11 -5.21736391e-12  1.06041997e-11
  8.14991550e-11  1.13728646e-10  4.27087022e-13  3.10092341e-11
  6.38394652e-11  8.64806000e-11  7.93618107e-13  2.16605901e-11
  2.39369906e-11  3.98479193e-11  1.25571733e-11  4.56037165e-12
 -5.22983315e-12 -1.94116442e-12 -8.37432533e-12 -1.08146837e-11
  8.11221016e-01]
supnorm grad right now is: 0.8112210161509816
Weights right now are: 
[-4.34248934 -0.63272229 -3.11046959 -3.93875226  4.59705815  0.52403061
  3.50940695  3.22272397 -2.21597618 -0.84450607 -2.18298034 -1.93456739
  1.51063578  0.42441633 -0.05316768  0.78618278 -1.51568686 -0.42041894
 -2.5626797  -2.19331923 -2.25563085 -0.80439305 -2.32288081 -1.83634807
 -0.9157424  -0.74384308 -1.66025033 -0.8343934   0.47123444  1.45828463
  1.56368389  0.50672766 -1.10040816 -0.42902332 -1.52276181 -1.00947866
 -2.5197959  -1.34231883 -1.31497095 -0.98144273 -0.48869505 -1.13041049
  0.57881846  0.52841638  0.43532736  1.12736435 -0.38023848  0.39827736
 -1.49886143 -0.62872918 -0.42809496 -1.59930021  0.63189516  1.18532481
  0.55527648  1.71011615  1.20609043 -0.85966247 -0.10829669 -1.87783743
  2.04310266  1.72875732 -0.13313082  1.04730431 25.04000931]
(65,)
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.0666502024264
gradient value of function right now is: [ 4.35856760e-08  4.72710458e-09  4.35754559e-08  4.35921771e-08
 -4.35856800e-08 -4.72710526e-09 -4.35754600e-08 -4.35921812e-08
  8.98390780e-10  1.08288907e-10  8.97448875e-10  9.50328842e-10
  4.58031432e-09  5.51196805e-10  4.57505958e-09  4.84422891e-09
  7.35926137e-10  8.87094170e-11  7.35156311e-10  7.78474940e-10
  6.80652848e-10  8.20454924e-11  6.79939007e-10  7.20004575e-10
 -1.38974906e-10 -3.75520206e-11 -1.58243523e-10 -2.52242228e-11
 -2.59355516e-11 -7.08414046e-12 -2.97064051e-11 -4.65894911e-12
  3.69307057e-10  9.55903741e-11  4.13788509e-10  6.72678401e-11
  2.65775720e-11  5.86632034e-12  2.82543819e-11  4.81741649e-12
  5.25633261e-11  7.49566387e-11 -2.06505647e-12  2.02100582e-11
  1.73978801e-11  2.49277822e-11 -2.81790271e-12  4.60564435e-12
  3.81159124e-11  5.32591752e-11 -3.06278602e-13  1.39806856e-11
  2.98709287e-11  4.05071598e-11  2.78286747e-14  9.77711891e-12
  1.11212095e-11  1.88550346e-11  5.80820393e-12  2.19657616e-12
 -2.39894913e-12 -9.99245457e-13 -3.87143047e-12 -5.17759306e-12
  1.77678288e+00]
supnorm grad right now is: 1.7767828821164384
Weights right now are: 
[-4.47083444 -0.64641494 -3.2387846  -4.06711689  4.72540324  0.53772325
  3.63772196  3.3510886  -2.218601   -0.84482206 -2.18560143 -1.9373434
  1.49750136  0.42283741 -0.06628254  0.77229401 -1.51782533 -0.42067638
 -2.56481513 -2.19558088 -2.25761093 -0.80463143 -2.32485807 -1.8384422
 -0.91535316 -0.74373574 -1.65980417 -0.83432239  0.47130339  1.45830361
  1.56376309  0.50674006 -1.10147195 -0.42930867 -1.52396732 -1.00967413
 -2.51987468 -1.34233773 -1.31505676 -0.98145725 -0.48884799 -1.13062772
  0.57881847  0.52835133  0.43527697  1.12729248 -0.38023271  0.39826151
 -1.49897167 -0.62888288 -0.42809723 -1.59934413  0.63180894  1.18520806
  0.55527428  1.71008555  1.20605781 -0.85971551 -0.10831384 -1.87784331
  2.04310992  1.72875959 -0.13311932  1.04731854 25.13070758]
(65,)
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1802754606792
gradient value of function right now is: [ 2.89089998e-08  3.19870477e-09  2.89021797e-08  2.89132337e-08
 -2.89088633e-08 -3.19868960e-09 -2.89020433e-08 -2.89130973e-08
  6.03596059e-10  7.26693939e-11  6.02915504e-10  6.38426019e-10
  3.13166856e-09  3.76478729e-10  3.12790514e-09  3.31183645e-09
  4.97343534e-10  5.98793443e-11  4.96783734e-10  5.26044250e-10
  4.59451465e-10  5.53164184e-11  4.58933131e-10  4.85964627e-10
 -9.60140086e-11 -2.63497712e-11 -1.09866238e-10 -1.75014834e-11
 -1.87625127e-11 -5.15330036e-12 -2.15230282e-11 -3.37960289e-12
  2.49515054e-10  6.64523979e-11  2.82093113e-10  4.57641603e-11
  1.77335447e-11  4.17310246e-12  1.92038209e-11  3.25728885e-12
  3.52041925e-11  5.00399944e-11 -2.83740791e-13  1.46617984e-11
  1.17476048e-11  1.67484095e-11 -1.42844454e-12  3.56982120e-12
  2.54819732e-11  3.55407932e-11  3.71783371e-13  9.96466598e-12
  1.99325443e-11  2.69954787e-11  4.07344534e-13  6.95054908e-12
  7.50916149e-12  1.23032931e-11  3.92560354e-12  1.36746484e-12
 -1.65031289e-12 -5.54961342e-13 -2.63616706e-12 -3.31229335e-12
  7.53585487e-02]
supnorm grad right now is: 0.07535854871252738
Weights right now are: 
[-4.54205507 -0.65421804 -3.30998846 -4.13834806  4.79662385  0.54552636
  3.7089258   3.42231975 -2.2200799  -0.84500012 -2.18707857 -1.93890762
  1.48990272  0.42192387 -0.07387153  0.76425836 -1.51904035 -0.42082268
 -2.5660287  -2.19686599 -2.25873403 -0.80476665 -2.32597983 -1.83963009
 -0.91512223 -0.74367233 -1.65953988 -0.8342803   0.47134741  1.45831571
  1.5638136   0.50674799 -1.10208022 -0.42947072 -1.5246551  -1.00978567
 -2.51991839 -1.34234803 -1.31510413 -0.98146528 -0.4889343  -1.13075038
  0.57881906  0.52831539  0.43524826  1.12725156 -0.38022925  0.39825278
 -1.49903406 -0.62896993 -0.42809816 -1.59936851  0.63176014  1.18514195
  0.55527326  1.71006853  1.20603942 -0.85974566 -0.10832347 -1.87784669
  2.04311393  1.72876098 -0.13311288  1.04732666 25.01486144]
(65,)
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1437306480943
gradient value of function right now is: [ 2.11634081e-08  2.37382905e-09  2.11584105e-08  2.11664612e-08
 -2.11633266e-08 -2.37381922e-09 -2.11583291e-08 -2.11663797e-08
  4.44739308e-10  5.35716718e-11  4.44328148e-10  4.70449520e-10
  2.34180418e-09  2.81643057e-10  2.33943299e-09  2.47674940e-09
  3.67896081e-10  4.43170510e-11  3.67556753e-10  3.89165531e-10
  3.39603917e-10  4.09083572e-11  3.39289781e-10  3.59237021e-10
 -7.29522161e-11 -1.98432243e-11 -8.32350858e-11 -1.32689074e-11
 -1.47680231e-11 -4.03877013e-12 -1.69152382e-11 -2.65887295e-12
  1.85592072e-10  4.86835791e-11  2.08812430e-10  3.39150625e-11
  1.29149073e-11  2.92850348e-12  1.38335002e-11  2.35539033e-12
  2.59640827e-11  3.69776104e-11 -6.58652145e-13  1.03663721e-11
  8.70175736e-12  1.24315229e-11 -1.23533507e-12  2.46237044e-12
  1.88524069e-11  2.63218389e-11  4.13057636e-14  7.12556599e-12
  1.47584794e-11  2.00037199e-11  1.43375587e-13  4.97672869e-12
  5.52104693e-12  9.21085082e-12  2.87818926e-12  1.04423555e-12
 -1.20059041e-12 -4.53278432e-13 -1.92983216e-12 -2.50767042e-12
  1.00153506e+00]
supnorm grad right now is: 1.0015350639156146
Weights right now are: 
[-4.59180521 -0.65976083 -3.35972685 -4.18810544  4.84637398  0.55106915
  3.75866418  3.47207712 -2.22112259 -0.84512564 -2.18812012 -1.94001048
  1.48445608  0.42126914 -0.07931184  0.75849831 -1.51990118 -0.42092631
 -2.5668886  -2.19777651 -2.25952897 -0.80486235 -2.3267739  -1.84047091
 -0.91495426 -0.74362622 -1.65934766 -0.83424967  0.47138078  1.45832488
  1.56385188  0.506754   -1.10251269 -0.42958602 -1.52514418 -1.00986503
 -2.51994889 -1.34235522 -1.31513717 -0.98147088 -0.48899506 -1.13083676
  0.57881954  0.52828998  0.43522794  1.12722259 -0.38022679  0.39824657
 -1.49907808 -0.62903131 -0.42809882 -1.59938579  0.6317257   1.18509533
  0.55527255  1.71005648  1.20602644 -0.85976684 -0.10833027 -1.87784903
  2.04311681  1.72876192 -0.13310831  1.04733236 25.0741506 ]
(65,)
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.616002342619
gradient value of function right now is: [ 1.77936881e-08  2.02058823e-09  1.77894493e-08  1.77962400e-08
 -1.77933180e-08 -2.02054560e-09 -1.77890793e-08 -1.77958698e-08
  3.78056334e-10  4.53974860e-11  3.77515117e-10  3.99764780e-10
  2.00294014e-09  2.40245613e-10  2.00003386e-09  2.11769550e-09
  3.13687036e-10  3.76690234e-11  3.13238270e-10  3.31700289e-10
  2.89394836e-10  3.47514687e-11  2.88980159e-10  3.06012638e-10
 -6.18177435e-11 -1.75865448e-11 -7.15682264e-11 -1.13793486e-11
 -1.27051120e-11 -3.54958717e-12 -1.46504427e-11 -2.30239738e-12
  1.56706803e-10  4.44158240e-11  1.80795616e-10  2.91930359e-11
  1.10402614e-11  2.96924934e-12  1.24610401e-11  2.08822276e-12
  2.18783207e-11  3.08616103e-11  1.39406200e-12  1.07115115e-11
  7.38605045e-12  1.04163225e-11 -2.31678106e-13  2.89691053e-12
  1.57478311e-11  2.18678155e-11  1.05545825e-12  7.03943299e-12
  1.22699513e-11  1.65648016e-11  8.08114279e-13  4.88597548e-12
  4.74620976e-12  7.17409712e-12  2.49232675e-12  7.06553000e-13
 -1.08632260e-12 -1.86498124e-13 -1.69749086e-12 -1.83613877e-12
 -3.79528916e+00]
supnorm grad right now is: 3.795289158151711
Weights right now are: 
[-4.62983577 -0.66405109 -3.3977484  -4.22614146  4.88440454  0.55535941
  3.79668573  3.51011314 -2.2219251  -0.84522224 -2.1889218  -1.94085931
  1.48021269  0.42075914 -0.08355055  0.75401071 -1.52056602 -0.42100634
 -2.56755275 -2.19847973 -2.2601425  -0.8049362  -2.3273868  -1.84111986
 -0.91482207 -0.74358985 -1.65919627 -0.83422556  0.4714078   1.45833231
  1.56388289  0.50675887 -1.10284751 -0.42967557 -1.52552324 -1.0099265
 -2.5199722  -1.34236073 -1.31516245 -0.98147517 -0.48904184 -1.13090323
  0.57881973  0.52827026  0.43521222  1.1272002  -0.38022496  0.39824171
 -1.49911195 -0.62907854 -0.42809943 -1.59939918  0.63169922  1.18505946
  0.55527193  1.71004715  1.20601645 -0.85978313 -0.10833549 -1.87785082
  2.04311902  1.72876264 -0.13310479  1.04733675 24.71927506]
(65,)
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1305621575466
gradient value of function right now is: [ 1.42952007e-08  1.63658851e-09  1.42918040e-08  1.42972270e-08
 -1.42955654e-08 -1.63663067e-09 -1.42921686e-08 -1.42975918e-08
  3.04219454e-10  3.65913191e-11  3.03898528e-10  3.21762511e-10
  1.63059474e-09  1.95857776e-10  1.62878146e-09  1.72436351e-09
  2.53011205e-10  3.04330296e-11  2.52744705e-10  2.67602217e-10
  2.33313056e-10  2.80632848e-11  2.33066723e-10  2.46767654e-10
 -5.12558964e-11 -1.42249420e-11 -5.88584595e-11 -9.37410743e-12
 -1.08007467e-11 -2.98190923e-12 -1.24061112e-11 -1.95158749e-12
  1.27541317e-10  3.46632998e-11  1.45131085e-10  2.35109665e-11
  8.76299527e-12  2.15058525e-12  9.60850295e-12  1.62516861e-12
  1.76757245e-11  2.50701066e-11  2.52011052e-13  7.77542318e-12
  5.97545797e-12  8.48472355e-12 -5.48598111e-13  1.98341913e-12
  1.28004113e-11  1.78306441e-11  3.95907230e-13  5.23360498e-12
  9.99804629e-12  1.35292571e-11  3.45630696e-13  3.64458609e-12
  3.79653907e-12  6.06004638e-12  1.98333244e-12  6.45547171e-13
 -8.45119707e-13 -2.36888686e-13 -1.34141847e-12 -1.60790518e-12
 -1.16520796e+00]
supnorm grad right now is: 1.1652079627904834
Weights right now are: 
[-4.66085842 -0.66758536 -3.4287637  -4.25716852  4.91542719  0.55889368
  3.82770103  3.5411402  -2.2225829  -0.84530141 -2.18957898 -1.94155511
  1.47669882  0.42033685 -0.08706084  0.75029453 -1.52111243 -0.4210721
 -2.56809865 -2.1990577  -2.26064649 -0.80499686 -2.32789031 -1.84165296
 -0.91471156 -0.74355951 -1.65906981 -0.83420541  0.47143092  1.45833866
  1.5639094   0.50676304 -1.10312344 -0.42974915 -1.52583533 -1.00997714
 -2.51999114 -1.34236519 -1.31518295 -0.98147866 -0.48908017 -1.13095766
  0.57882     0.52825425  0.43519929  1.12718181 -0.38022342  0.39823777
 -1.49913976 -0.62911728 -0.42809985 -1.59941007  0.63167747  1.18503003
  0.55527147  1.71003955  1.20600829 -0.85979645 -0.10833976 -1.87785227
  2.04312082  1.72876321 -0.13310191  1.04734033 24.8971604 ]
(65,)
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1784408045346
gradient value of function right now is: [ 1.20790500e-08  1.39303094e-09  1.20761807e-08  1.20807477e-08
 -1.20787933e-08 -1.39300085e-09 -1.20759240e-08 -1.20804909e-08
  2.57756561e-10  3.10195584e-11  2.57526040e-10  2.72644188e-10
  1.39350236e-09  1.67455228e-10  1.39215794e-09  1.47375022e-09
  2.14787787e-10  2.58493970e-11  2.14596086e-10  2.27194422e-10
  1.97993231e-10  2.38278590e-11  1.97816010e-10  2.09429395e-10
 -4.42555379e-11 -1.21739380e-11 -5.06729974e-11 -8.07561379e-12
 -9.49464608e-12 -2.60951427e-12 -1.08894123e-11 -1.71402604e-12
  1.08737899e-10  2.91165046e-11  1.23142295e-10  1.99710806e-11
  7.36207661e-12  1.74532948e-12  7.98816831e-12  1.35579543e-12
  1.49893228e-11  2.13002007e-11 -4.29586432e-14  6.33479777e-12
  5.07856920e-12  7.22694865e-12 -5.69573848e-13  1.58049352e-12
  1.08838572e-11  1.51765981e-11  2.02307781e-13  4.30645016e-12
  8.50842419e-12  1.15223521e-11  2.02996485e-13  3.00250556e-12
  3.20874546e-12  5.21858686e-12  1.67180464e-12  5.69151224e-13
 -7.06763746e-13 -2.26357631e-13 -1.12863679e-12 -1.40068074e-12
 -2.43428645e-01]
supnorm grad right now is: 0.24342864473951453
Weights right now are: 
[-4.68687649 -0.67057396 -3.4547756  -4.28319026  4.94144525  0.56188227
  3.85371291  3.56716193 -2.22313675 -0.84536808 -2.19013235 -1.94214096
  1.47371474  0.41997815 -0.09004212  0.74713851 -1.52157349 -0.42112761
 -2.56855931 -2.1995454  -2.26107158 -0.80504804 -2.32831503 -1.84210261
 -0.91461693 -0.74353359 -1.65896161 -0.83418816  0.47145107  1.45834419
  1.5639325   0.50676668 -1.10335695 -0.42981114 -1.52609904 -1.01001994
 -2.52000699 -1.34236886 -1.31520004 -0.98148156 -0.48911244 -1.13100354
  0.57882042  0.52824092  0.43518838  1.12716626 -0.38022205  0.39823451
 -1.49916319 -0.62914996 -0.42810012 -1.59941917  0.63165914  1.18500521
  0.55527114  1.71003319  1.2060014  -0.85980775 -0.10834335 -1.87785352
  2.04312234  1.72876373 -0.13309949  1.04734338 24.9831404 ]
(65,)
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.14956683492
gradient value of function right now is: [ 1.06205543e-08  1.23326581e-09  1.06180266e-08  1.06220383e-08
 -1.06207899e-08 -1.23329362e-09 -1.06182622e-08 -1.06222739e-08
  2.27546745e-10  2.73714537e-11  2.27334725e-10  2.40679692e-10
  1.23769597e-09  1.48673124e-10  1.23646741e-09  1.30892766e-09
  1.89936469e-10  2.28480977e-11  1.89759808e-10  2.00899421e-10
  1.75029009e-10  2.10545359e-11  1.74865774e-10  1.85131189e-10
 -3.94329621e-11 -1.09166431e-11 -4.52434787e-11 -7.20829512e-12
 -8.56642893e-12 -2.36226397e-12 -9.83473304e-12 -1.54833229e-12
  9.61574735e-11  2.60325031e-11  1.09279744e-10  1.77090458e-11
  6.47808549e-12  1.57352455e-12  7.08023083e-12  1.19930975e-12
  1.32103291e-11  1.87488596e-11  1.25038610e-13  5.75192238e-12
  4.48878644e-12  6.37578038e-12 -4.33992441e-13  1.46522137e-12
  9.58468284e-12  1.33558317e-11  2.64003336e-13  3.88591499e-12
  7.48745717e-12  1.01348764e-11  2.36760523e-13  2.70693391e-12
  2.83734667e-12  4.54972967e-12  1.47949509e-12  4.86048753e-13
 -6.29650398e-13 -1.82646224e-13 -1.00149324e-12 -1.21123553e-12
 -9.21169775e-01]
supnorm grad right now is: 0.9211697753022543
Weights right now are: 
[-4.70944791 -0.67318551 -3.47734166 -4.30576484  4.96401667  0.56449382
  3.87627897  3.58973651 -2.22361912 -0.84542613 -2.1906143  -1.94265119
  1.47109786  0.4196637  -0.09265656  0.74437093 -1.52197578 -0.42117602
 -2.56896125 -2.19997093 -2.26144235 -0.80509265 -2.32868549 -1.84249481
 -0.91453354 -0.74351069 -1.65886617 -0.83417295  0.47146909  1.45834913
  1.56395316  0.50676993 -1.10356081 -0.42986565 -1.52632982 -1.01005735
 -2.52002072 -1.34237211 -1.31521493 -0.98148409 -0.48914049 -1.13104338
  0.57882051  0.52822913  0.43517886  1.12715273 -0.38022099  0.39823157
 -1.49918357 -0.62917836 -0.42810049 -1.59942719  0.63164321  1.18498364
  0.55527077  1.7100276   1.20599542 -0.85981753 -0.10834645 -1.87785459
  2.04312366  1.72876415 -0.13309739  1.04734601 24.91190365]
(65,)
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1638719729103
gradient value of function right now is: [ 9.24335870e-09  1.07917349e-09  9.24116227e-09  9.24464063e-09
 -9.24306910e-09 -1.07913908e-09 -9.24087274e-09 -9.24435099e-09
  1.98239010e-10  2.38692401e-11  1.98100202e-10  2.09709594e-10
  1.08652025e-09  1.30620601e-10  1.08566775e-09  1.14918964e-09
  1.65711317e-10  1.99534131e-11  1.65595626e-10  1.75300433e-10
  1.52664980e-10  1.83822253e-11  1.52557992e-10  1.61498849e-10
 -3.50108447e-11 -9.54445401e-12 -3.99702527e-11 -6.37439133e-12
 -7.71950089e-12 -2.11145951e-12 -8.83904536e-12 -1.39243398e-12
  8.43702651e-11  2.22518536e-11  9.50858458e-11  1.54383806e-11
  5.59027136e-12  1.27626350e-12  5.99833094e-12  1.02205267e-12
  1.15340394e-11  1.64207213e-11 -2.33182698e-13  4.67367295e-12
  3.92416880e-12  5.59515223e-12 -5.19616494e-13  1.13906506e-12
  8.40059731e-12  1.17255506e-11  5.13635953e-14  3.21239995e-12
  6.57255167e-12  8.90735141e-12  8.60046903e-14  2.24288779e-12
  2.46058674e-12  4.07643508e-12  1.27831261e-12  4.53656835e-13
 -5.36286389e-13 -1.93877913e-13 -8.61753454e-13 -1.10665934e-12
  6.70824412e-01]
supnorm grad right now is: 0.6708244116474037
Weights right now are: 
[-4.72927254 -0.67549373 -3.49716157 -4.32559223  4.9838413   0.56680205
  3.89609889  3.60956391 -2.22404408 -0.84547727 -2.19103892 -1.94310071
  1.46877767  0.41938489 -0.09497468  0.74191707 -1.52233076 -0.42121874
 -2.56931594 -2.20034642 -2.26176943 -0.80513202 -2.32901229 -1.84284078
 -0.91445921 -0.74349027 -1.65878109 -0.83415939  0.47148536  1.4583536
  1.56397181  0.50677287 -1.10374099 -0.42991385 -1.5265338  -1.01009043
 -2.52003276 -1.34237496 -1.31522799 -0.98148631 -0.48916516 -1.13107845
  0.57882061  0.52821874  0.43517047  1.12714079 -0.38022005  0.39822896
 -1.49920152 -0.62920339 -0.42810081 -1.59943428  0.63162917  1.18496464
  0.55527044  1.71002266  1.20599013 -0.85982614 -0.1083492  -1.87785553
  2.04312482  1.72876453 -0.13309553  1.04734832 25.08022894]
(65,)
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.173845132544
gradient value of function right now is: [ 8.41419442e-09  9.87918846e-10  8.41219038e-09  8.41535669e-09
 -8.41436228e-09 -9.87938635e-10 -8.41235820e-09 -8.41552458e-09
  1.81139302e-10  2.17944372e-11  1.80994397e-10  1.91605282e-10
  9.97072697e-10  1.19792499e-10  9.96207306e-10  1.05451287e-09
  1.51616803e-10  1.82429481e-11  1.51495783e-10  1.60377581e-10
  1.39645819e-10  1.68023288e-11  1.39533998e-10  1.47714619e-10
 -3.21379086e-11 -8.85441089e-12 -3.68149716e-11 -5.86795204e-12
 -7.14618003e-12 -1.96554717e-12 -8.19669485e-12 -1.29129013e-12
  7.70839064e-11  2.07065789e-11  8.73828129e-11  1.41687293e-11
  5.10243281e-12  1.21585991e-12  5.54424047e-12  9.41138528e-13
  1.05129198e-11  1.49364667e-11  3.64676780e-15  4.48180177e-12
  3.58632677e-12  5.09878707e-12 -3.83292940e-13  1.13175389e-12
  7.64387799e-12  1.06574124e-11  1.60778499e-13  3.04591750e-12
  5.97381764e-12  8.08956940e-12  1.55040130e-13  2.12311637e-12
  2.25482408e-12  3.65132998e-12  1.17301628e-12  3.94424817e-13
 -4.97379238e-13 -1.54718665e-13 -7.93991724e-13 -9.78183787e-13
 -4.38633610e-01]
supnorm grad right now is: 0.43863360958775494
Weights right now are: 
[-4.74693757 -0.67756197 -3.51482239 -4.3432597   5.00150633  0.56887029
  3.91375971  3.62723138 -2.2244237  -0.84552296 -2.19141825 -1.94350228
  1.46669287  0.41913436 -0.09705773  0.73971211 -1.5226483  -0.42125696
 -2.56963324 -2.20068232 -2.26206193 -0.80516722 -2.32930457 -1.84315019
 -0.91439205 -0.74347184 -1.65870427 -0.83414714  0.47150023  1.45835768
  1.56398885  0.50677555 -1.10390249 -0.42995692 -1.52671646 -1.01012007
 -2.52004346 -1.34237747 -1.31523956 -0.98148827 -0.48918722 -1.13110981
  0.57882078  0.52820952  0.43516296  1.12713009 -0.38021917  0.39822667
 -1.49921756 -0.62922577 -0.42810106 -1.59944057  0.63161663  1.18494764
  0.55527018  1.71001827  1.20598541 -0.85983385 -0.10835165 -1.87785638
  2.04312585  1.72876487 -0.13309388  1.04735039 24.93721001]
(65,)
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.072701389342
gradient value of function right now is: [ 7.74795370e-09  9.14454845e-10  7.74610388e-09  7.74902029e-09
 -7.74794377e-09 -9.14453398e-10 -7.74609396e-09 -7.74901036e-09
  1.67403692e-10  2.01253450e-11  1.67250170e-10  1.77060122e-10
  9.24907603e-10  1.11044329e-10  9.24012942e-10  9.78116080e-10
  1.40286583e-10  1.68658517e-11  1.40158136e-10  1.48379293e-10
  1.29182365e-10  1.55306440e-11  1.29063766e-10  1.36634268e-10
 -2.97938840e-11 -8.30637879e-12 -3.42611329e-11 -5.45723926e-12
 -6.67270824e-12 -1.84732893e-12 -7.66935713e-12 -1.20812593e-12
  7.11916558e-11  1.95103370e-11  8.12269625e-11  1.31508528e-11
  4.71363757e-12  1.17496384e-12  5.19233933e-12  8.77804324e-13
  9.69216525e-12  1.37356191e-11  2.26495994e-13  4.35834757e-12
  3.31487136e-12  4.69686552e-12 -2.60853766e-13  1.13804573e-12
  7.03207318e-12  9.79037144e-12  2.65028410e-13  2.92705276e-12
  5.48889212e-12  7.42512346e-12  2.21850194e-13  2.03722190e-12
  2.08948246e-12  3.29807038e-12  1.08894766e-12  3.43187469e-13
 -4.67194803e-13 -1.20112437e-13 -7.40259348e-13 -8.69886590e-13
 -1.68417368e+00]
supnorm grad right now is: 1.6841736832675025
Weights right now are: 
[-4.7629909  -0.67945105 -3.5308719  -4.35931524  5.01755966  0.57075936
  3.92980922  3.64328692 -2.22476948 -0.84556457 -2.19176378 -1.94386805
  1.46478389  0.41890496 -0.09896518  0.73769308 -1.52293788 -0.42129181
 -2.56992262 -2.20098865 -2.26232863 -0.80519932 -2.32957107 -1.84343231
 -0.91433028 -0.7434549  -1.65863361 -0.83413587  0.47151404  1.45836147
  1.56400468  0.50677805 -1.10405001 -0.42999622 -1.52688324 -1.01014714
 -2.52005317 -1.34237974 -1.31525004 -0.98149006 -0.48920731 -1.13113836
  0.57882096  0.52820115  0.43515609  1.12712034 -0.38021835  0.39822459
 -1.49923219 -0.62924616 -0.42810126 -1.59944628  0.63160519  1.18493216
  0.55526995  1.71001429  1.20598111 -0.85984085 -0.10835389 -1.87785713
  2.0431268   1.72876518 -0.13309236  1.04735228 24.84619647]
(65,)
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.9859846702634
gradient value of function right now is: [ 7.14449029e-09  8.47096339e-10  7.14278222e-09  7.14547020e-09
 -7.14492101e-09 -8.47147891e-10 -7.14321284e-09 -7.14590098e-09
  1.54782097e-10  1.86007470e-11  1.54634034e-10  1.63704374e-10
  8.58552736e-10  1.03043066e-10  8.57695277e-10  9.07915823e-10
  1.29846910e-10  1.56046743e-11  1.29722874e-10  1.37332258e-10
  1.19545526e-10  1.43664876e-11  1.19431042e-10  1.26436809e-10
 -2.77002641e-11 -7.76477921e-12 -3.19099370e-11 -5.08133930e-12
 -6.24985968e-12 -1.73564943e-12 -7.19029278e-12 -1.13273333e-12
  6.58778337e-11  1.82193758e-11  7.53879156e-11  1.21971763e-11
  4.34932986e-12  1.10590158e-12  4.82064000e-12  8.13524289e-13
  8.94920340e-12  1.26681856e-11  3.03925957e-13  4.12097254e-12
  3.06692162e-12  4.33856443e-12 -2.01221587e-13  1.09209751e-12
  6.48810086e-12  9.02709805e-12  2.94449787e-13  2.75402588e-12
  5.06140842e-12  6.84355912e-12  2.38397054e-13  1.91550510e-12
  1.93407119e-12  3.01661825e-12  1.00867009e-12  3.07896771e-13
 -4.35056221e-13 -1.01414860e-13 -6.87109664e-13 -7.89802128e-13
 -2.25658423e+00]
supnorm grad right now is: 2.2565842334739408
Weights right now are: 
[-4.77748654 -0.68116473 -3.5453641  -4.37381288  5.0320553   0.57247305
  3.94430141  3.65778455 -2.22508238 -0.84560223 -2.19207646 -1.94419905
  1.46304823  0.41869637 -0.10069951  0.73585734 -1.52320022 -0.42132339
 -2.57018478 -2.20126617 -2.26257018 -0.80522839 -2.32981246 -1.84368784
 -0.91427388 -0.74343944 -1.65856911 -0.83412559  0.47152676  1.45836496
  1.56401926  0.50678035 -1.10418383 -0.43003186 -1.52703453 -1.01017169
 -2.52006192 -1.34238178 -1.31525948 -0.98149166 -0.48922548 -1.1311642
  0.57882114  0.52819358  0.43514988  1.12711149 -0.38021761  0.3982227
 -1.49924542 -0.62926462 -0.42810144 -1.59945145  0.63159484  1.18491814
  0.55526975  1.71001067  1.20597722 -0.85984721 -0.10835591 -1.87785783
  2.04312765  1.72876546 -0.13309099  1.04735399 24.7690678 ]
(65,)
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1724934971176
gradient value of function right now is: [ 6.37842592e-09  7.58753507e-10  6.37690634e-09  6.37929483e-09
 -6.37844915e-09 -7.58756700e-10 -6.37692956e-09 -6.37931806e-09
  1.38018435e-10  1.66146108e-11  1.37935351e-10  1.46007516e-10
  7.70890125e-10  9.26562423e-11  7.70362971e-10  8.15372226e-10
  1.15890081e-10  1.39512993e-11  1.15820548e-10  1.22598736e-10
  1.06678652e-10  1.28422025e-11  1.06614366e-10  1.12853864e-10
 -2.52222828e-11 -6.88703911e-12 -2.88084374e-11 -4.59509576e-12
 -5.76187834e-12 -1.57695018e-12 -6.59776366e-12 -1.04025439e-12
  5.92793607e-11  1.56878751e-11  6.68790789e-11  1.08564914e-11
  3.83286855e-12  8.79648776e-13  4.11840241e-12  7.01903411e-13
  8.01158693e-12  1.14043924e-11 -1.35617399e-13  3.27755810e-12
  2.74570459e-12  3.91122084e-12 -3.48255027e-13  8.09860275e-13
  5.84371009e-12  8.15596117e-12  5.04110164e-14  2.25200745e-12
  4.57062009e-12  6.19424783e-12  6.96082096e-14  1.57188679e-12
  1.71296995e-12  2.82465869e-12  8.88543119e-13  3.11315119e-13
 -3.73938161e-13 -1.31474530e-13 -6.00655686e-13 -7.65401995e-13
  4.68704535e-01]
supnorm grad right now is: 0.46870453468096057
Weights right now are: 
[-4.79087813 -0.68275467 -3.55875249 -4.3872063   5.04544689  0.57406298
  3.95768981  3.67117797 -2.22537202 -0.84563709 -2.19236591 -1.94450544
  1.46143462  0.41850245 -0.10231193  0.73415067 -1.52344331 -0.42135265
 -2.5704277  -2.20152332 -2.26279396 -0.80525533 -2.3300361  -1.84392457
 -0.91422127 -0.74342501 -1.65850894 -0.83411599  0.47153873  1.45836824
  1.56403297  0.50678251 -1.10430798 -0.43006494 -1.52717488 -1.01019447
 -2.52006999 -1.34238366 -1.3152682  -0.98149315 -0.48924227 -1.13118811
  0.57882132  0.52818656  0.43514413  1.1271033  -0.38021692  0.39822095
 -1.49925766 -0.62928171 -0.42810161 -1.59945626  0.63158527  1.18490517
  0.55526957  1.71000733  1.20597361 -0.8598531  -0.1083578  -1.87785847
  2.04312845  1.72876572 -0.13308972  1.04735558 25.01980509]
(65,)
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.061980933732
gradient value of function right now is: [ 6.07775799e-09  7.26212015e-10  6.07630466e-09  6.07858486e-09
 -6.07804819e-09 -7.26247424e-10 -6.07659478e-09 -6.07887510e-09
  1.32070523e-10  1.58756800e-11  1.31958752e-10  1.39691262e-10
  7.38890127e-10  8.87007579e-11  7.38228442e-10  7.81411337e-10
  1.10997646e-10  1.33430141e-11  1.10903870e-10  1.17402822e-10
  1.02157958e-10  1.22802280e-11  1.02071401e-10  1.08052847e-10
 -2.40462294e-11 -6.70871948e-12 -2.76571834e-11 -4.40586138e-12
 -5.51043686e-12 -1.52647240e-12 -6.33412435e-12 -9.98307456e-13
  5.65120930e-11  1.55086966e-11  6.45062649e-11  1.04429089e-11
  3.68038527e-12  9.19028378e-13  4.05606022e-12  6.85900425e-13
  7.63540174e-12  1.08202133e-11  1.88854201e-13  3.44623016e-12
  2.62388388e-12  3.71585745e-12 -2.00082435e-13  9.05502699e-13
  5.54562512e-12  7.72050247e-12  2.14776328e-13  2.31520966e-12
  4.32798140e-12  5.85466568e-12  1.78839660e-13  1.61130032e-12
  1.64787064e-12  2.59550976e-12  8.57889776e-13  2.68380119e-13
 -3.68608888e-13 -9.33030048e-14 -5.84134716e-13 -6.84065646e-13
 -1.76724157e+00]
supnorm grad right now is: 1.767241569304093
Weights right now are: 
[-4.80320149 -0.6842235  -3.57107291 -4.39953133  5.05777025  0.57553181
  3.97001023  3.683503   -2.22563899 -0.84566923 -2.19263272 -1.94478787
  1.45994105  0.41832295 -0.10380445  0.73257093 -1.52366758 -0.42137964
 -2.57065183 -2.20176057 -2.26300039 -0.80528017 -2.3302424  -1.84414294
 -0.91417238 -0.74341162 -1.65845304 -0.83410708  0.47154993  1.45837131
  1.5640458   0.50678453 -1.10442267 -0.43009545 -1.52730448 -1.01021551
 -2.5200774  -1.34238538 -1.31527618 -0.98149451 -0.48925775 -1.13121015
  0.57882151  0.52818014  0.43513882  1.12709574 -0.38021628  0.39821935
 -1.49926895 -0.62929746 -0.42810174 -1.59946066  0.63157645  1.18489321
  0.5552694   1.71000425  1.20597029 -0.85985854 -0.10835952 -1.87785906
  2.04312918  1.72876597 -0.13308856  1.04735705 24.79325405]
(65,)
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.16342796254
gradient value of function right now is: [ 5.50720879e-09  6.59900763e-10  5.50589608e-09  5.50795357e-09
 -5.50703478e-09 -6.59879741e-10 -5.50572211e-09 -5.50777954e-09
  1.19529300e-10  1.43901552e-11  1.19466144e-10  1.26452085e-10
  6.72828202e-10  8.08749968e-11  6.72415384e-10  7.11672014e-10
  1.00535416e-10  1.21039078e-11  1.00482502e-10  1.06358530e-10
  9.25165976e-11  1.11383214e-11  9.24676578e-11  9.78750686e-11
 -2.21693196e-11 -6.04034298e-12 -2.53033963e-11 -4.03688450e-12
 -5.13464487e-12 -1.40346006e-12 -5.87679741e-12 -9.26920685e-13
  5.15619961e-11  1.35985581e-11  5.81085673e-11  9.43514009e-12
  3.29545721e-12  7.49023201e-13  3.53085365e-12  6.02461016e-13
  6.93528422e-12  9.87648304e-12 -1.45252635e-13  2.80946136e-12
  2.38344388e-12  3.39609257e-12 -3.12443847e-13  6.91698426e-13
  5.06452927e-12  7.06992100e-12  2.92132659e-14  1.93620156e-12
  3.96186280e-12  5.37009482e-12  5.05532778e-14  1.35194948e-12
  1.48185059e-12  2.45388605e-12  7.67705491e-13  2.71271676e-13
 -3.22642293e-13 -1.16452204e-13 -5.19121054e-13 -6.66703193e-13
  6.77882959e-01]
supnorm grad right now is: 0.6778829589942368
Weights right now are: 
[-4.81469504 -0.68559851 -3.58256372 -4.41102644  5.06926379  0.57690682
  3.98150103  3.6949981  -2.22588844 -0.84569925 -2.19288201 -1.94505175
  1.45854045  0.41815465 -0.10520408  0.73108953 -1.52387731 -0.42140488
 -2.57086143 -2.20198243 -2.2631934  -0.8053034  -2.33043529 -1.84434712
 -0.91412641 -0.74339902 -1.65840047 -0.83409869  0.47156052  1.45837422
  1.56405795  0.50678645 -1.10453001 -0.43012405 -1.52742584 -1.0102352
 -2.5200843  -1.34238699 -1.31528364 -0.98149578 -0.48927221 -1.13123072
  0.57882165  0.5281741   0.43513386  1.12708867 -0.3802157   0.39821783
 -1.49927949 -0.62931218 -0.42810189 -1.5994648   0.63156821  1.18488204
  0.55526924  1.71000137  1.20596718 -0.8598636  -0.10836113 -1.87785962
  2.04312986  1.7287662  -0.13308747  1.04735842 25.06502596]
(65,)
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.176747735744
gradient value of function right now is: [ 5.22029174e-09  6.27801386e-10  5.21904512e-09  5.22099622e-09
 -5.21995595e-09 -6.27760412e-10 -5.21870941e-09 -5.22066038e-09
  1.13596282e-10  1.36669578e-11  1.13524947e-10  1.20166519e-10
  6.40989650e-10  7.70050993e-11  6.40542129e-10  6.77952986e-10
  9.56195299e-11  1.15045339e-11  9.55596554e-11  1.01150377e-10
  8.79808420e-11  1.05853283e-11  8.79255226e-11  9.30696806e-11
 -2.10989546e-11 -5.80469940e-12 -2.41567286e-11 -3.85189844e-12
 -4.90779099e-12 -1.34878406e-12 -5.62676091e-12 -8.87407988e-13
  4.89616594e-11  1.31285333e-11  5.54698060e-11  8.99574631e-12
  3.13207095e-12  7.40651418e-13  3.39497955e-12  5.77230042e-13
  6.57691282e-12  9.34847790e-12 -1.48258431e-14  2.78987593e-12
  2.26446786e-12  3.21831727e-12 -2.45024158e-13  7.08598339e-13
  4.79478749e-12  6.68647117e-12  9.24710990e-14  1.90264484e-12
  3.74709970e-12  5.07528213e-12  9.15388637e-14  1.32655684e-12
  1.41193971e-12  2.29053351e-12  7.32415815e-13  2.46495974e-13
 -3.10779150e-13 -9.79554232e-14 -4.97036711e-13 -6.14867799e-13
 -3.24493168e-01]
supnorm grad right now is: 0.3244931675831614
Weights right now are: 
[-4.82546013 -0.6868908  -3.59332624 -4.42179298  5.08002886  0.57819911
  3.99226353  3.70576463 -2.22612242 -0.84572741 -2.19311585 -1.94529927
  1.45722194  0.41799619 -0.10652171  0.72969494 -1.52407418 -0.42142858
 -2.57105819 -2.2021907  -2.26337456 -0.80532521 -2.33061634 -1.84453877
 -0.914083   -0.74338712 -1.65835084 -0.83409077  0.4715706   1.45837698
  1.56406949  0.50678827 -1.10463089 -0.43015091 -1.52753986 -1.0102537
 -2.52009076 -1.34238848 -1.31529059 -0.98149696 -0.48928577 -1.13125003
  0.5788218   0.52816846  0.4351292   1.12708203 -0.38021514  0.39821642
 -1.49928938 -0.62932599 -0.42810202 -1.59946866  0.63156048  1.18487156
  0.55526909  1.70999867  1.20596427 -0.85986837 -0.10836264 -1.87786014
  2.0431305   1.72876641 -0.13308644  1.0473597  24.93434827]
(65,)
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.169929602477
gradient value of function right now is: [ 4.86086354e-09  5.86272404e-10  4.85970382e-09  4.86151695e-09
 -4.86096917e-09 -5.86285101e-10 -4.85980943e-09 -4.86162259e-09
  1.05824070e-10  1.27385550e-11  1.05770774e-10  1.11953213e-10
  5.99595235e-10  7.20639187e-11  5.99243436e-10  6.34212769e-10
  8.91395505e-11  1.07305364e-11  8.90948361e-11  9.43027053e-11
  8.20078780e-11  9.87188687e-12  8.19665242e-11  8.67577882e-11
 -1.98535178e-11 -5.41655978e-12 -2.26694274e-11 -3.61672155e-12
 -4.65054366e-12 -1.27199008e-12 -5.32359529e-12 -8.39878676e-13
  4.57872758e-11  1.21056675e-11  5.16408787e-11  8.38366726e-12
  2.90162944e-12  6.62809407e-13  3.11324214e-12  5.31114394e-13
  6.13359747e-12  8.73309874e-12 -1.12566825e-13  2.50219510e-12
  2.11354755e-12  3.00994184e-12 -2.69258723e-13  6.20545011e-13
  4.48087043e-12  6.25442831e-12  3.44610746e-14  1.72277751e-12
  3.50463004e-12  4.75004191e-12  5.05067759e-14  1.20266954e-12
  1.31211299e-12  2.16568065e-12  6.79461216e-13  2.38065885e-13
 -2.86132266e-13 -1.01157842e-13 -4.60047385e-13 -5.87449127e-13
  5.27840839e-01]
supnorm grad right now is: 0.5278408393400613
Weights right now are: 
[-4.83557104 -0.68810859 -3.60343474 -4.43190526  5.09013981  0.5794169
  4.00237206  3.71587694 -2.22634255 -0.8457539  -2.19333585 -1.94553214
  1.45597758  0.41784668 -0.10776524  0.7283788  -1.52425954 -0.42145089
 -2.57124344 -2.20238678 -2.2635451  -0.80534573 -2.33078678 -1.84471918
 -0.91404196 -0.74337585 -1.65830388 -0.83408328  0.47158017  1.45837961
  1.56408046  0.50679    -1.10472591 -0.43017629 -1.52764738 -1.01027115
 -2.52009682 -1.3423899  -1.31529714 -0.98149808 -0.48929851 -1.13126817
  0.5788219   0.5281631   0.43512481  1.12707578 -0.38021464  0.39821507
 -1.49929867 -0.62933896 -0.42810217 -1.59947233  0.63155322  1.18486171
  0.55526894  1.70999612  1.20596153 -0.85987282 -0.10836407 -1.87786062
  2.0431311   1.72876661 -0.13308548  1.0473609  24.99196991]
(65,)
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.662032191453
gradient value of function right now is: [ 4.41039690e-09  5.32991474e-10  4.40934955e-09  4.41098598e-09
 -4.41003668e-09 -5.32946983e-10 -4.40898943e-09 -4.41062570e-09
  9.57701275e-11  1.15531126e-11  9.57630733e-11  1.01345121e-10
  5.46170011e-10  6.57620898e-11  5.46056181e-10  5.77840201e-10
  8.07198467e-11  9.73793614e-12  8.07141303e-11  8.54191029e-11
  7.42543082e-11  8.95779682e-12  7.42488317e-11  7.85769984e-11
 -1.83641179e-11 -4.84567289e-12 -2.07471665e-11 -3.31667060e-12
 -4.34816246e-12 -1.16674778e-12 -4.94728212e-12 -7.81269663e-13
  4.18625756e-11  1.04526305e-11  4.63810642e-11  7.56045078e-12
  2.59022487e-12  5.09534207e-13  2.66692364e-12  4.60958477e-13
  5.57718168e-12  7.99740875e-12 -4.55460324e-13  1.91851226e-12
  1.92073422e-12  2.75938718e-12 -3.89386646e-13  4.19446492e-13
  4.10513501e-12  5.75253203e-12 -1.52953412e-13  1.38175957e-12
  3.22127480e-12  4.37862774e-12 -7.88973100e-14  9.69381593e-13
  1.17674553e-12  2.07764200e-12  6.05262642e-13  2.48718032e-13
 -2.46255102e-13 -1.28498973e-13 -4.05455900e-13 -5.85566721e-13
  3.87620181e+00]
supnorm grad right now is: 3.876201810420263
Weights right now are: 
[-4.84505684 -0.68925468 -3.61291827 -4.44139233  5.0996256   0.580563
  4.01185559  3.72536401 -2.22654942 -0.84577878 -2.19354259 -1.94575097
  1.45480484  0.4177058  -0.10893718  0.72713844 -1.52443385 -0.42147186
 -2.57141764 -2.20257118 -2.26370546 -0.80536502 -2.33094704 -1.84488881
 -0.91400322 -0.74336519 -1.65825952 -0.83407621  0.47158926  1.45838211
  1.56409088  0.50679164 -1.10481525 -0.43020031 -1.52774868 -1.01028757
 -2.5201025  -1.34239126 -1.31530331 -0.98149912 -0.48931046 -1.13128516
  0.57882189  0.52815799  0.43512068  1.12706992 -0.38021422  0.39821376
 -1.4993074  -0.62935113 -0.42810236 -1.59947581  0.6315464   1.18485248
  0.55526876  1.70999369  1.20595896 -0.85987699 -0.1083654  -1.87786107
  2.04313166  1.72876679 -0.13308457  1.04736202 25.25116045]
(65,)
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.177784304983
gradient value of function right now is: [ 4.35799612e-09  5.28720607e-10  4.35695534e-09  4.35857887e-09
 -4.35776616e-09 -5.28692329e-10 -4.35672544e-09 -4.35834888e-09
  9.51399681e-11  1.14501983e-11  9.50926779e-11  1.00649356e-10
  5.42164062e-10  6.51500495e-11  5.41851575e-10  5.73462032e-10
  8.02440073e-11  9.65778829e-12  8.02042769e-11  8.48911062e-11
  7.38075480e-11  8.88299655e-12  7.37708096e-11  7.80817520e-11
 -1.80216134e-11 -4.92880014e-12 -2.05935635e-11 -3.28527658e-12
 -4.26245275e-12 -1.16741742e-12 -4.88123411e-12 -7.70224493e-13
  4.12620348e-11  1.09566027e-11  4.66010098e-11  7.56302371e-12
  2.59663487e-12  5.98886519e-13  2.79374752e-12  4.76282755e-13
  5.50794386e-12  7.83883260e-12 -7.47849964e-14  2.27423607e-12
  1.90260979e-12  2.70738226e-12 -2.30462573e-13  5.69845444e-13
  4.02410548e-12  5.61550930e-12  4.49393435e-14  1.56225814e-12
  3.14645591e-12  4.26392210e-12  5.47657654e-14  1.09020465e-12
  1.18002115e-12  1.93736740e-12  6.10860326e-13  2.11236309e-13
 -2.57957080e-13 -8.81431036e-14 -4.14252323e-13 -5.23973366e-13
  2.67231785e-01]
supnorm grad right now is: 0.2672317851769065
Weights right now are: 
[-4.85407409 -0.69034732 -3.62193336 -4.45041079  5.10864284  0.58165564
  4.02087067  3.73438245 -2.22674631 -0.84580247 -2.19373937 -1.94595925
  1.45368527  0.41757133 -0.11005601  0.72595431 -1.52459987 -0.42149183
 -2.57158356 -2.2027468  -2.26385816 -0.80538339 -2.33109965 -1.84505034
 -0.91396615 -0.74335498 -1.65821706 -0.83406944  0.471598    1.45838451
  1.5641009   0.50679322 -1.10490041 -0.43022319 -1.52784523 -1.01030322
 -2.52010789 -1.34239254 -1.31530917 -0.98150012 -0.48932185 -1.13130133
  0.57882188  0.52815314  0.43511675  1.12706434 -0.38021381  0.39821251
 -1.4993157  -0.62936271 -0.42810254 -1.59947912  0.63153991  1.18484369
  0.55526859  1.70999139  1.20595652 -0.85988095 -0.10836666 -1.8778615
  2.0431322   1.72876696 -0.13308372  1.04736308 25.03518397]
(65,)
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3589.8275727001214
gradient value of function right now is: [ 4.44592820e-09  5.41794112e-10  4.44485671e-09  4.44652496e-09
 -4.44610724e-09 -5.41816320e-10 -4.44503570e-09 -4.44670402e-09
  9.78354016e-11  1.17290995e-11  9.77174522e-11  1.03450817e-10
  5.55692123e-10  6.65556133e-11  5.55026500e-10  5.87527618e-10
  8.25746421e-11  9.89979576e-12  8.24751328e-11  8.73143593e-11
  7.59414157e-11  9.10443760e-12  7.58497389e-11  8.03002707e-11
 -1.80893444e-11 -5.24394515e-12 -2.10702137e-11 -3.34916426e-12
 -4.25568141e-12 -1.20618895e-12 -4.92759957e-12 -7.76375476e-13
  4.18332307e-11  1.22247508e-11  4.87587081e-11  7.85511230e-12
  2.71344190e-12  7.75205841e-13  3.12346896e-12  5.21407689e-13
  5.60388547e-12  7.87789848e-12  5.61534270e-13  2.95855325e-12
  1.94407052e-12  2.72340386e-12  3.09224218e-14  8.46906994e-13
  4.04559214e-12  5.60616001e-12  3.80768025e-13  1.92713002e-12
  3.14489329e-12  4.23984321e-12  2.81357877e-13  1.33504913e-12
  1.23057696e-12  1.77893959e-12  6.43451401e-13  1.59253944e-13
 -2.86539264e-13 -2.61802499e-14 -4.44355068e-13 -4.42552978e-13
 -5.78251360e+00]
supnorm grad right now is: 5.782513598316149
Weights right now are: 
[-4.86260034 -0.69138326 -3.63045758 -4.45893818  5.1171691   0.58269158
  4.0293949   3.74290985 -2.22693266 -0.84582489 -2.19392561 -1.94615638
  1.4526224   0.41744365 -0.11111824  0.72483011 -1.52475709 -0.42151074
 -2.57174069 -2.20291311 -2.26400276 -0.80540079 -2.33124417 -1.84520331
 -0.91393084 -0.74334528 -1.65817666 -0.834063    0.47160636  1.45838681
  1.56411048  0.50679473 -1.10498118 -0.43024481 -1.52793668 -1.01031806
 -2.52011297 -1.34239374 -1.31531467 -0.98150106 -0.48933262 -1.13131664
  0.57882194  0.52814859  0.43511303  1.12705905 -0.38021339  0.39821136
 -1.49932357 -0.62937367 -0.42810267 -1.59948223  0.63153376  1.18483536
  0.55526845  1.70998921  1.20595421 -0.8598847  -0.10836786 -1.8778619
  2.04313271  1.72876711 -0.1330829   1.04736409 24.49095778]
(65,)
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1494512154677
gradient value of function right now is: [ 3.99615722e-09  4.87542479e-10  3.99520067e-09  3.99668960e-09
 -3.99643581e-09 -4.87576839e-10 -3.99547919e-09 -3.99696823e-09
  8.75599126e-11  1.05297067e-11  8.75068190e-11  9.26225039e-11
  5.01080390e-10  6.01728968e-11  5.00745585e-10  5.29969012e-10
  7.39375352e-11  8.89180942e-12  7.38928234e-11  7.82127667e-11
  6.79923900e-11  8.17672420e-12  6.79511023e-11  7.19237308e-11
 -1.66530556e-11 -4.60755059e-12 -1.91007624e-11 -3.04525052e-12
 -3.96637863e-12 -1.09372058e-12 -4.55187189e-12 -7.18187439e-13
  3.79629240e-11  1.02789994e-11  4.31432980e-11  6.99202567e-12
  2.38701526e-12  5.76359964e-13  2.60335719e-12  4.42045808e-13
  5.05577461e-12  7.17896684e-12  4.31599257e-14  2.20241353e-12
  1.75112167e-12  2.48412396e-12 -1.65024425e-13  5.71267855e-13
  3.68664680e-12  5.13815488e-12  1.00113124e-13  1.49494159e-12
  2.87915032e-12  3.89819037e-12  8.99863082e-14  1.04154906e-12
  1.08918221e-12  1.74436517e-12  5.64899991e-13  1.83703370e-13
 -2.41287546e-13 -6.94791406e-14 -3.84677744e-13 -4.64992938e-13
 -9.23111811e-01]
supnorm grad right now is: 0.9231118113354801
Weights right now are: 
[-4.87068686 -0.6923684  -3.63854216 -4.46702578  5.12525562  0.58367671
  4.03747948  3.75099744 -2.22710962 -0.84584618 -2.19410248 -1.94634358
  1.45161043  0.41732209 -0.11212957  0.72375978 -1.52490648 -0.42152871
 -2.57189    -2.20307115 -2.26414015 -0.80541731 -2.33138148 -1.84534865
 -0.91389717 -0.74333602 -1.65813811 -0.83405685  0.47161437  1.45838901
  1.56411966  0.50679618 -1.10505795 -0.43026542 -1.52802369 -1.01033216
 -2.52011779 -1.34239488 -1.3153199  -0.98150195 -0.48934283 -1.13133117
  0.57882196  0.52814424  0.43510949  1.12705402 -0.38021302  0.39821024
 -1.49933103 -0.62938408 -0.42810283 -1.59948521  0.63152792  1.18482746
  0.5552683   1.70998714  1.20595202 -0.85988827 -0.10836899 -1.87786228
  2.0431332   1.72876727 -0.13308213  1.04736505 24.91182291]
(65,)
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1738294216616
gradient value of function right now is: [ 3.75196268e-09  4.58696145e-10  3.75106615e-09  3.75246065e-09
 -3.75185725e-09 -4.58682629e-10 -3.75096076e-09 -3.75235521e-09
  8.21651442e-11  9.88927975e-12  8.21301420e-11  8.69257064e-11
  4.72063410e-10  5.67287073e-11  4.71823589e-10  4.99327929e-10
  6.94175781e-11  8.35529216e-12  6.93881438e-11  7.34398295e-11
  6.38306459e-11  7.68272084e-12  6.38034104e-11  6.75290477e-11
 -1.58024388e-11 -4.31423239e-12 -1.80470421e-11 -2.87962177e-12
 -3.78782824e-12 -1.03629357e-12 -4.33597161e-12 -6.84426830e-13
  3.57947199e-11  9.47876215e-12  4.03906794e-11  6.55654352e-12
  2.22485705e-12  5.08997999e-13  2.38796237e-12  4.07539198e-13
  4.75346344e-12  6.76778642e-12 -8.02811375e-14  1.94754950e-12
  1.64699804e-12  2.34409459e-12 -2.04957425e-13  4.87089589e-13
  3.47695977e-12  4.85296353e-12  3.07239956e-14  1.34139712e-12
  2.71896519e-12  3.68521491e-12  4.18300714e-14  9.36335740e-13
  1.01812074e-12  1.67691177e-12  5.26419769e-13  1.83246766e-13
 -2.22086742e-13 -7.74970822e-14 -3.57161657e-13 -4.54529959e-13
  4.31516679e-01]
supnorm grad right now is: 0.43151667933224014
Weights right now are: 
[-4.87844097 -0.69331531 -3.64629442 -4.47478092  5.13300973  0.58462362
  4.04523174  3.75875259 -2.22727942 -0.8458666  -2.1942722  -1.94652321
  1.45063654  0.4172051  -0.11310292  0.72272968 -1.5250499  -0.42154597
 -2.57203335 -2.20322287 -2.26427203 -0.80543318 -2.3315133  -1.84548816
 -0.91386466 -0.74332711 -1.65810093 -0.83405092  0.47162215  1.45839114
  1.56412857  0.50679759 -1.10513178 -0.43028512 -1.5281072  -1.01034571
 -2.5201224  -1.34239596 -1.31532487 -0.98150279 -0.48935265 -1.13134512
  0.57882203  0.52814014  0.43510609  1.12704919 -0.38021264  0.3982092
 -1.49933821 -0.62939408 -0.42810294 -1.59948802  0.63152231  1.18481987
  0.55526818  1.70998518  1.20594992 -0.8598917  -0.10837008 -1.87786265
  2.04313366  1.72876742 -0.13308139  1.04736597 25.03195679]
(65,)
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.0955546970977
gradient value of function right now is: [ 3.67025314e-09  4.50015091e-10  3.66937347e-09  3.67074017e-09
 -3.67017418e-09 -4.50005114e-10 -3.66929452e-09 -3.67066120e-09
  8.06298174e-11  9.69254290e-12  8.05780432e-11  8.52886969e-11
  4.63472602e-10  5.56379519e-11  4.63150073e-10  4.90178721e-10
  6.81572093e-11  8.19346624e-12  6.81135476e-11  7.20956459e-11
  6.26658204e-11  7.53321903e-12  6.26255197e-11  6.62868220e-11
 -1.54319259e-11 -4.29293785e-12 -1.77313029e-11 -2.82610095e-12
 -3.70198233e-12 -1.02417100e-12 -4.25280806e-12 -6.71012746e-13
  3.49963818e-11  9.56182666e-12  3.98885465e-11  6.46010901e-12
  2.19185046e-12  5.40137309e-13  2.40533592e-12  4.07700295e-13
  4.64891546e-12  6.59349841e-12  8.83676212e-14  2.07447455e-12
  1.61377609e-12  2.28552920e-12 -1.31410019e-13  5.46460925e-13
  3.38760088e-12  4.71814269e-12  1.17626069e-13  1.40086055e-12
  2.64408611e-12  3.57814668e-12  1.00057105e-13  9.75409145e-13
  1.00388250e-12  1.58912623e-12  5.20849509e-13  1.64311088e-13
 -2.23752356e-13 -5.89059997e-14 -3.55536609e-13 -4.20621242e-13
 -1.49393637e+00]
supnorm grad right now is: 1.4939363735396505
Weights right now are: 
[-4.88582574 -0.6942193  -3.65367743 -4.48216667  5.1403945   0.58552761
  4.05261474  3.76613834 -2.22744131 -0.84588608 -2.19443401 -1.94669447
  1.44970577  0.41709327 -0.11403318  0.72174517 -1.5251867  -0.42156243
 -2.57217009 -2.2033676  -2.26439782 -0.80544832 -2.33163903 -1.84562123
 -0.91383352 -0.74331857 -1.65806532 -0.83404524  0.47162962  1.45839319
  1.56413713  0.50679894 -1.10520227 -0.43030392 -1.52818692 -1.01035864
 -2.52012679 -1.34239698 -1.3153296  -0.9815036  -0.48936201 -1.13135844
  0.57882212  0.52813623  0.43510284  1.12704458 -0.38021227  0.39820821
 -1.49934505 -0.62940362 -0.42810304 -1.5994907   0.63151697  1.18481262
  0.55526808  1.70998331  1.20594791 -0.85989498 -0.10837112 -1.877863
  2.0431341   1.72876756 -0.13308068  1.04736686 24.89781513]
(65,)
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.9847391464264
gradient value of function right now is: [ 3.36014882e-09  4.12463948e-10  3.35934781e-09  3.36059196e-09
 -3.35981735e-09 -4.12422560e-10 -3.35901643e-09 -3.36026044e-09
  7.35661438e-11  8.86493881e-12  7.35540750e-11  7.78411995e-11
  4.25516058e-10  5.11866542e-11  4.25398973e-10  4.50155341e-10
  6.22124382e-11  7.49706770e-12  6.22023840e-11  6.58279764e-11
  5.71961507e-11  6.89245964e-12  5.71867457e-11  6.05200415e-11
 -1.44023472e-11 -3.85808850e-12 -1.63483695e-11 -2.61170913e-12
 -3.48934039e-12 -9.43948655e-13 -3.97990196e-12 -6.28659600e-13
  3.22846006e-11  8.28049310e-12  3.60656560e-11  5.86809456e-12
  1.97063938e-12  4.15149338e-13  2.06628835e-12  3.55321417e-13
  4.26567045e-12  6.09800418e-12 -2.25647113e-13  1.59323250e-12
  1.47942932e-12  2.11583561e-12 -2.46792275e-13  3.74781976e-13
  3.13449249e-12  4.38482733e-12 -5.26087161e-14  1.12406650e-12
  2.45564247e-12  3.33387760e-12 -1.67531940e-14  7.86719832e-13
  9.06856424e-13  1.55192745e-12  4.67022396e-13  1.78100335e-13
 -1.93373072e-13 -8.52766782e-14 -3.15095948e-13 -4.30190731e-13
  2.34044434e+00]
supnorm grad right now is: 2.340444338533038
Weights right now are: 
[-4.89289339 -0.69508653 -3.66074338 -4.48923525  5.14746215  0.58639485
  4.0596807   3.77320692 -2.22759643 -0.84590475 -2.19458906 -1.94685858
  1.44881194  0.41698588 -0.11492651  0.72079975 -1.52531786 -0.42157821
 -2.57230118 -2.20350635 -2.2645184  -0.80546283 -2.33175956 -1.8457488
 -0.91380359 -0.74331035 -1.65803107 -0.83403978  0.47163683  1.45839517
  1.56414539  0.50680024 -1.10526987 -0.43032201 -1.52826345 -1.01037106
 -2.52013098 -1.34239796 -1.31533414 -0.98150437 -0.48937096 -1.13137119
  0.57882217  0.52813244  0.43509974  1.12704016 -0.38021192  0.39820724
 -1.49935159 -0.62941276 -0.42810315 -1.59949329  0.63151186  1.18480569
  0.55526796  1.70998151  1.20594597 -0.85989811 -0.10837213 -1.87786335
  2.04313452  1.7287677  -0.13308     1.0473677  25.18906032]
(65,)
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.363506304124
gradient value of function right now is: [ 3.12890563e-09  3.84621510e-10  3.12816246e-09  3.12931630e-09
 -3.12893443e-09 -3.84625577e-10 -3.12819125e-09 -3.12934510e-09
  6.83674960e-11  8.25203264e-12  6.83788794e-11  7.23558223e-11
  3.97393645e-10  4.78698259e-11  3.97399558e-10  4.20480571e-10
  5.78405774e-11  6.98171596e-12  5.78503857e-11  6.12150772e-11
  5.31724674e-11  6.41813738e-12  5.31813269e-11  5.62745030e-11
 -1.36066433e-11 -3.55111051e-12 -1.53189026e-11 -2.45095415e-12
 -3.32181779e-12 -8.84902426e-13 -3.77046992e-12 -5.96028150e-13
  3.02379019e-11  7.41542209e-12  3.33190348e-11  5.43795898e-12
  1.81140220e-12  3.36560502e-13  1.83782488e-12  3.19363616e-13
  3.97830212e-12  5.71798690e-12 -4.03943976e-13  1.28982971e-12
  1.37938001e-12  1.98577885e-12 -3.09124017e-13  2.70206026e-13
  2.94046641e-12  4.12544697e-12 -1.50069148e-13  9.46697480e-13
  2.30947044e-12  3.14213444e-12 -8.41439770e-14  6.65300755e-13
  8.36428899e-13  1.50703326e-12  4.28504754e-13  1.83993909e-13
 -1.72545615e-13 -9.98418374e-14 -2.86651683e-13 -4.29579788e-13
  4.88108785e+00]
supnorm grad right now is: 4.8810878532196735
Weights right now are: 
[-4.89967128 -0.69592005 -3.66751965 -4.49601404  5.15424003  0.58722836
  4.06645696  3.7799857  -2.22774533 -0.84592266 -2.19473789 -1.9470161
  1.44795199  0.41688257 -0.115786    0.71989016 -1.52544381 -0.42159337
 -2.57242708 -2.20363959 -2.2646342  -0.80547676 -2.33187529 -1.8458713
 -0.91377474 -0.74330242 -1.65799805 -0.83403451  0.4716438   1.45839708
  1.56415338  0.5068015  -1.10533482 -0.4303394  -1.528337   -1.01038299
 -2.520135   -1.34239891 -1.31533849 -0.98150511 -0.48937956 -1.13138342
  0.57882221  0.52812881  0.43509675  1.12703591 -0.3802116   0.39820631
 -1.49935787 -0.62942153 -0.42810326 -1.59949578  0.63150695  1.18479903
  0.55526785  1.70997977  1.20594412 -0.85990111 -0.10837308 -1.87786367
  2.04313492  1.72876784 -0.13307935  1.04736851 25.32841667]
(65,)
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.0972551043546
gradient value of function right now is: [ 3.24352820e-09  4.00292557e-10  3.24275033e-09  3.24395597e-09
 -3.24316290e-09 -4.00246952e-10 -3.24238512e-09 -3.24359062e-09
  7.14450601e-11  8.58818199e-12  7.14023589e-11  7.55743111e-11
  4.13476664e-10  4.96344767e-11  4.13207174e-10  4.37307968e-10
  6.04772427e-11  7.27000674e-12  6.04411890e-11  6.39728070e-11
  5.55916619e-11  6.68261516e-12  5.55583805e-11  5.88047364e-11
 -1.38422171e-11 -3.85001718e-12 -1.59035471e-11 -2.53500550e-12
 -3.35651458e-12 -9.28570500e-13 -3.85574472e-12 -6.08508907e-13
  3.11175797e-11  8.50045417e-12  3.54651797e-11  5.74383393e-12
  1.92979306e-12  4.74951963e-13  2.11683415e-12  3.58936059e-13
  4.11593848e-12  5.83804846e-12  7.68380811e-14  1.83574367e-12
  1.43253517e-12  2.02854016e-12 -1.16579409e-13  4.84600183e-13
  3.00139832e-12  4.18038104e-12  1.03579730e-13  1.24059146e-12
  2.34257647e-12  3.17025409e-12  8.82132342e-14  8.63861593e-13
  8.89072734e-13  1.40761736e-12  4.60887571e-13  1.45293388e-13
 -1.98084786e-13 -5.21925287e-14 -3.14878365e-13 -3.72700492e-13
 -1.48389987e+00]
supnorm grad right now is: 1.4838998695861054
Weights right now are: 
[-4.90617234 -0.69672119 -3.67401916 -4.50251596  5.1607411   0.5880295
  4.07295648  3.78648763 -2.22788825 -0.84593986 -2.19488074 -1.94716729
  1.44712461  0.41678317 -0.11661296  0.71901501 -1.52556476 -0.42160792
 -2.57254797 -2.20376755 -2.26474538 -0.80549014 -2.33198642 -1.84598892
 -0.91374691 -0.74329479 -1.65796623 -0.83402943  0.47165055  1.45839893
  1.56416112  0.50680273 -1.10539727 -0.43035606 -1.52840763 -1.01039445
 -2.52013885 -1.3423998  -1.31534264 -0.98150582 -0.4893878  -1.13139516
  0.5788223   0.52812535  0.43509389  1.12703183 -0.38021126  0.39820543
 -1.4993639  -0.62942995 -0.42810334 -1.59949816  0.63150223  1.18479264
  0.55526776  1.70997812  1.20594235 -0.859904   -0.10837401 -1.87786398
  2.04313532  1.72876796 -0.13307872  1.04736929 24.84077862]
(65,)
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.102562947373
gradient value of function right now is: [ 3.01562318e-09  3.72603834e-10  3.01490292e-09  3.01601892e-09
 -3.01538086e-09 -3.72573702e-10 -3.01466066e-09 -3.01577657e-09
  6.62638613e-11  7.98007289e-12  6.62484450e-11  7.01103536e-11
  3.85436242e-10  4.63407551e-11  3.85309012e-10  4.07734485e-10
  5.61127152e-11  6.75782912e-12  5.60997848e-11  5.93701815e-11
  5.15762501e-11  6.21139322e-12  5.15642240e-11  5.45702581e-11
 -1.30681229e-11 -3.53179566e-12 -1.48754883e-11 -2.37538815e-12
 -3.19391066e-12 -8.68444539e-13 -3.64867977e-12 -5.76348766e-13
  2.91087658e-11  7.58096174e-12  3.26733573e-11  5.31049289e-12
  1.76926706e-12  3.87316038e-13  1.87496852e-12  3.21435301e-13
  3.83367228e-12  5.47049439e-12 -1.38383545e-13  1.49782873e-12
  1.33365803e-12  1.90254578e-12 -1.95066139e-13  3.64774624e-13
  2.81359813e-12  3.93188896e-12 -1.34584797e-14  1.04526170e-12
  2.20224661e-12  2.98764099e-12  7.83300696e-15  7.30613337e-13
  8.18434930e-13  1.37515232e-12  4.21945641e-13  1.53849932e-13
 -1.76433983e-13 -6.98319661e-14 -2.85745922e-13 -3.77323799e-13
  1.45373091e+00]
supnorm grad right now is: 1.453730913849969
Weights right now are: 
[-4.91243881 -0.69749498 -3.68028412 -4.50878325  5.16700757  0.5888033
  4.07922144  3.79275492 -2.22802611 -0.84595645 -2.19501855 -1.94731314
  1.44632471  0.41668707 -0.11741247  0.71816891 -1.52568148 -0.42162196
 -2.57266465 -2.20389103 -2.26485266 -0.80550304 -2.33209367 -1.84610242
 -0.91371996 -0.7432874  -1.65793541 -0.83402451  0.47165711  1.45840073
  1.56416863  0.50680391 -1.10545757 -0.43037214 -1.52847582 -1.01040552
 -2.52014255 -1.34240066 -1.31534663 -0.9815065  -0.48939576 -1.13140648
  0.57882238  0.52812203  0.43509112  1.1270279  -0.38021095  0.39820459
 -1.49936973 -0.62943807 -0.42810343 -1.59950044  0.63149768  1.18478647
  0.55526767  1.70997652  1.20594064 -0.85990679 -0.10837489 -1.87786428
  2.04313569  1.72876808 -0.13307812  1.04737004 25.0726393 ]
(65,)
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1787797255947
gradient value of function right now is: [ 2.95132693e-09  3.65495391e-10  2.95062062e-09  2.95171404e-09
 -2.95158277e-09 -3.65527897e-10 -2.95087639e-09 -2.95196991e-09
  6.49987510e-11  7.82148303e-12  6.49747321e-11  6.87652625e-11
  3.78358854e-10  4.54590737e-11  3.78189204e-10  4.00215408e-10
  5.50649681e-11  6.62635094e-12  5.50447251e-11  5.82560528e-11
  5.06089146e-11  6.09003242e-12  5.05901789e-11  5.35416649e-11
 -1.27884193e-11 -3.49860170e-12 -1.46141120e-11 -2.33196017e-12
 -3.12908431e-12 -8.57058722e-13 -3.58291064e-12 -5.65802577e-13
  2.84916554e-11  7.57395118e-12  3.21888253e-11  5.22383351e-12
  1.73931904e-12  4.00787731e-13  1.87056808e-12  3.19203800e-13
  3.75246206e-12  5.34125338e-12 -4.83706062e-14  1.55414337e-12
  1.30715516e-12  1.85880387e-12 -1.54971564e-13  3.93418736e-13
  2.74726889e-12  3.83397743e-12  3.24277198e-14  1.06917262e-12
  2.14765812e-12  2.91072987e-12  3.85171372e-14  7.46076306e-13
  8.05345752e-13  1.31967322e-12  4.16001860e-13  1.42826683e-13
 -1.75998764e-13 -5.95223238e-14 -2.82879477e-13 -3.56870663e-13
  2.05467764e-01]
supnorm grad right now is: 0.2054677644140202
Weights right now are: 
[-4.91842046 -0.69823501 -3.68626435 -4.51476568  5.17298923  0.58954332
  4.08520167  3.79873737 -2.22815777 -0.84597229 -2.19515016 -1.94745243
  1.44555899  0.41659507 -0.11817785  0.71735895 -1.52579299 -0.42163538
 -2.57277612 -2.20400901 -2.26495516 -0.80551538 -2.33219613 -1.84621085
 -0.91369409 -0.74328033 -1.65790585 -0.8340198   0.47166343  1.45840246
  1.56417587  0.50680505 -1.10551528 -0.43038744 -1.52854097 -1.01041609
 -2.52014607 -1.34240147 -1.31535041 -0.98150715 -0.48940338 -1.13141732
  0.57882249  0.5281189   0.43508847  1.12702413 -0.38021062  0.3982038
 -1.4993753  -0.62944584 -0.42810348 -1.59950259  0.63149332  1.18478057
  0.55526759  1.70997502  1.20593901 -0.85990946 -0.10837573 -1.87786457
  2.04313605  1.72876821 -0.13307755  1.04737077 25.01194688]
(65,)
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.0740480006916
gradient value of function right now is: [ 2.91289927e-09  3.61612613e-10  2.91220013e-09  2.91328143e-09
 -2.91263983e-09 -3.61579454e-10 -2.91194075e-09 -2.91302194e-09
  6.43309294e-11  7.73176866e-12  6.42934623e-11  6.80487229e-11
  3.74502091e-10  4.49494251e-11  3.74264779e-10  3.96086210e-10
  5.45221851e-11  6.55308642e-12  5.44905115e-11  5.76733023e-11
  5.01070851e-11  6.02234824e-12  5.00778498e-11  5.30029376e-11
 -1.25883935e-11 -3.50754673e-12 -1.44712496e-11 -2.30657789e-12
 -3.08039761e-12 -8.53140308e-13 -3.53971015e-12 -5.58716947e-13
  2.80915157e-11  7.69767282e-12  3.20485636e-11  5.18918831e-12
  1.72863837e-12  4.28292487e-13  1.90000902e-12  3.22023471e-13
  3.70192080e-12  5.24889423e-12  8.23600311e-14  1.66459261e-12
  1.29166001e-12  1.82780760e-12 -9.89676806e-14  4.42493725e-13
  2.70023766e-12  3.76005174e-12  1.00291872e-13  1.12353002e-12
  2.10709599e-12  2.85110894e-12  8.41203134e-14  7.82156178e-13
  8.00461095e-13  1.26233330e-12  4.14711337e-13  1.29340810e-13
 -1.78628267e-13 -4.55886592e-14 -2.83740192e-13 -3.33441254e-13
 -1.67218674e+00]
supnorm grad right now is: 1.6721867441903353
Weights right now are: 
[-4.92424459 -0.69895699 -3.69208708 -4.52059058  5.17881336  0.5902653
  4.09102441  3.80456226 -2.22828611 -0.84598773 -2.19527845 -1.9475882
  1.44481134  0.41650526 -0.11892516  0.71656813 -1.52590174 -0.42164847
 -2.57288483 -2.20412405 -2.2650551  -0.8055274  -2.33229603 -1.84631659
 -0.91366882 -0.74327341 -1.65787696 -0.83401519  0.47166962  1.45840416
  1.56418296  0.50680617 -1.10557152 -0.43040245 -1.5286046  -1.01042641
 -2.52014951 -1.34240227 -1.31535411 -0.98150778 -0.48941078 -1.13142784
  0.57882254  0.5281158   0.43508588  1.12702046 -0.38021034  0.39820301
 -1.49938072 -0.6294534  -0.42810357 -1.59950471  0.63148909  1.18477483
  0.5552675   1.70997354  1.20593743 -0.85991205 -0.10837655 -1.87786485
  2.04313639  1.72876832 -0.13307699  1.04737147 24.82132835]
(65,)
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.0221685556344
gradient value of function right now is: [ 2.82799861e-09  3.51748007e-10  2.82731937e-09  2.82836913e-09
 -2.82840289e-09 -3.51799077e-10 -2.82772355e-09 -2.82877347e-09
  6.25353992e-11  7.51419004e-12  6.24969884e-11  6.61477838e-11
  3.64599382e-10  4.37519326e-11  3.64358644e-10  3.85604733e-10
  5.30209366e-11  6.37113845e-12  5.29884430e-11  5.60838928e-11
  4.87233982e-11  5.85465395e-12  4.86934210e-11  5.15380009e-11
 -1.22564336e-11 -3.42732800e-12 -1.41060723e-11 -2.24797317e-12
 -3.00601575e-12 -8.34416539e-13 -3.45670096e-12 -5.45599145e-13
  2.73082005e-11  7.52680716e-12  3.12140512e-11  5.05202402e-12
  1.67948361e-12  4.21613879e-13  1.85346364e-12  3.13773723e-13
  3.59574147e-12  5.09484138e-12  1.04278859e-13  1.64207333e-12
  1.25573680e-12  1.77531950e-12 -8.60671049e-14  4.40338566e-13
  2.62120088e-12  3.64865987e-12  1.10130995e-13  1.10466317e-12
  2.04464550e-12  2.76588478e-12  9.03171746e-14  7.68694453e-13
  7.78887820e-13  1.21852851e-12  4.03885684e-13  1.23321056e-13
 -1.74495093e-13 -4.18682345e-14 -2.76603391e-13 -3.20372272e-13
 -2.04433155e+00]
supnorm grad right now is: 2.0443315525120944
Weights right now are: 
[-4.92984569 -0.69965262 -3.69768684 -4.52619241  5.18441446  0.59096094
  4.09662417  3.81016409 -2.22840964 -0.84600259 -2.19540193 -1.94771889
  1.44409038  0.41641865 -0.11964579  0.71580553 -1.52600645 -0.42166107
 -2.5729895  -2.20423483 -2.26515133 -0.80553898 -2.33239222 -1.84641839
 -0.91364442 -0.74326671 -1.65784905 -0.83401074  0.47167562  1.4584058
  1.56418983  0.50680726 -1.10562571 -0.43041692 -1.52866591 -1.01043635
 -2.52015281 -1.34240304 -1.31535767 -0.98150838 -0.4894179  -1.13143798
  0.5788226   0.52811281  0.4350834   1.12701693 -0.38021006  0.39820225
 -1.49938593 -0.62946068 -0.42810365 -1.59950676  0.63148501  1.18476931
  0.55526742  1.70997211  1.2059359  -0.85991455 -0.10837734 -1.87786512
  2.04313673  1.72876843 -0.13307646  1.04737214 24.84140751]
(65,)
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.180038443245
gradient value of function right now is: [ 2.66741417e-09  3.32156221e-10  2.66677541e-09  2.66776230e-09
 -2.66753471e-09 -3.32171464e-10 -2.66689591e-09 -2.66788285e-09
  5.88819103e-11  7.08477558e-12  5.88616033e-11  6.22941908e-11
  3.44680836e-10  4.14092107e-11  3.44535225e-10  3.64593551e-10
  4.99397396e-11  6.00904313e-12  4.99226108e-11  5.28339997e-11
  4.58897134e-11  5.52163893e-12  4.58738541e-11  4.85491633e-11
 -1.16978254e-11 -3.20293715e-12 -1.33712542e-11 -2.13368056e-12
 -2.88668104e-12 -7.91046154e-13 -3.30574488e-12 -5.22125123e-13
  2.58790591e-11  6.89021859e-12  2.92516224e-11  4.74667393e-12
  1.56742596e-12  3.62229428e-13  1.68706795e-12  2.87883745e-13
  3.39620608e-12  4.83351822e-12 -3.82341095e-14  1.41269750e-12
  1.18580452e-12  1.68558022e-12 -1.37717148e-13  3.59403720e-13
  2.48749848e-12  3.47114175e-12  3.24063581e-14  9.71454630e-13
  1.94434422e-12  2.63504539e-12  3.69135014e-14  6.77806453e-13
  7.29438093e-13  1.19290661e-12  3.76567976e-13  1.28559316e-13
 -1.59573031e-13 -5.31986380e-14 -2.56355857e-13 -3.22255441e-13
  1.09339550e-01]
supnorm grad right now is: 0.10933954984583322
Weights right now are: 
[-4.9352674  -0.70032718 -3.70310725 -4.53161483  5.18983617  0.5916355
  4.10204458  3.81558651 -2.2285293  -0.84601699 -2.19552154 -1.94784548
  1.44339069  0.41633461 -0.12034516  0.71506544 -1.52610792 -0.42167327
 -2.57309093 -2.20434218 -2.26524457 -0.8055502  -2.33248543 -1.84651703
 -0.91362071 -0.74326021 -1.65782193 -0.83400641  0.47168146  1.45840741
  1.56419652  0.50680832 -1.10567825 -0.43043095 -1.52872535 -1.010446
 -2.52015599 -1.34240378 -1.31536112 -0.98150897 -0.4894248  -1.13144779
  0.57882265  0.52810992  0.43508099  1.12701351 -0.38020979  0.39820151
 -1.49939099 -0.62946772 -0.42810373 -1.59950875  0.63148106  1.18476396
  0.55526733  1.70997072  1.20593442 -0.85991697 -0.1083781  -1.87786538
  2.04313706  1.72876854 -0.13307593  1.0473728  24.98752265]
(65,)
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.2075167653843
gradient value of function right now is: [ 2.42786422e-09  3.02383836e-10  2.42728727e-09  2.42817880e-09
 -2.42793197e-09 -3.02392885e-10 -2.42735501e-09 -2.42824657e-09
  5.33091746e-11  6.43592190e-12  5.33257733e-11  5.64227093e-11
  3.14326931e-10  3.78700683e-11  3.14374607e-10  3.32607398e-10
  4.52254652e-11  5.46022321e-12  4.52396905e-11  4.78670898e-11
  4.15558832e-11  5.01709559e-12  4.15688303e-11  4.39830718e-11
 -1.08971887e-11 -2.82895950e-12 -1.22478810e-11 -1.96048337e-12
 -2.71844052e-12 -7.21385476e-13 -3.08161904e-12 -4.87455182e-13
  2.37727871e-11  5.78173895e-12  2.61297170e-11  4.26692072e-12
  1.39087452e-12  2.50789273e-13  1.40060023e-12  2.44101005e-13
  3.09837869e-12  4.45851138e-12 -3.43418172e-13  9.76152800e-13
  1.08017993e-12  1.55627747e-12 -2.51934191e-13  2.00765344e-13
  2.29590312e-12  3.22308398e-12 -1.31725086e-13  7.23762755e-13
  1.80389414e-12  2.45546864e-12 -7.56847761e-14  5.09018186e-13
  6.50577554e-13  1.18319050e-12  3.32386999e-13  1.45645853e-13
 -1.33231360e-13 -8.07246017e-14 -2.22392127e-13 -3.39056705e-13
  5.34496081e+00]
supnorm grad right now is: 5.344960810696293
Weights right now are: 
[-4.94054479 -0.70098496 -3.70838338 -4.53689291  5.19511356  0.59229327
  4.1073207   3.82086459 -2.22864587 -0.84603101 -2.19563807 -1.9479688
  1.4427079   0.41625259 -0.12102764  0.71434322 -1.5262068  -0.42168517
 -2.57318978 -2.20444679 -2.26533544 -0.80556113 -2.33257626 -1.84661316
 -0.91359754 -0.74325385 -1.65779543 -0.83400218  0.47168718  1.45840898
  1.56420308  0.50680935 -1.10572947 -0.43044465 -1.52878333 -1.0104554
 -2.5201591  -1.34240451 -1.31536447 -0.98150954 -0.48943152 -1.13145735
  0.57882269  0.52810709  0.43507864  1.12701018 -0.38020953  0.39820078
 -1.49939591 -0.62947458 -0.42810381 -1.59951069  0.63147722  1.18475876
  0.55526725  1.70996937  1.20593297 -0.85991932 -0.10837885 -1.87786563
  2.04313737  1.72876864 -0.13307543  1.04737343 25.30287009]
(65,)
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.9058656770194
gradient value of function right now is: [ 2.42625203e-09  3.02918347e-10  2.42567314e-09  2.42656679e-09
 -2.42612774e-09 -3.02902947e-10 -2.42554888e-09 -2.42644249e-09
  5.34665872e-11  6.44414073e-12  5.34667446e-11  5.65776885e-11
  3.14955158e-10  3.78922726e-11  3.14918709e-10  3.33214240e-10
  4.53763882e-11  5.46926776e-12  4.53766370e-11  4.80169362e-11
  4.16921324e-11  5.02511940e-12  4.16922430e-11  4.41181958e-11
 -1.08268800e-11 -2.88615035e-12 -1.22701801e-11 -1.96122930e-12
 -2.69697608e-12 -7.27137188e-13 -3.07251617e-12 -4.85715249e-13
  2.37046455e-11  6.03400485e-12  2.64181958e-11  4.30096460e-12
  1.40547078e-12  2.88752300e-13  1.46346200e-12  2.52419920e-13
  3.09493657e-12  4.42972364e-12 -1.91717450e-13  1.12957971e-12
  1.08091801e-12  1.54699390e-12 -1.89941245e-13  2.63350495e-13
  2.28072270e-12  3.19254679e-12 -5.25100885e-14  8.03501972e-13
  1.78734238e-12  2.42784642e-12 -2.18981077e-14  5.62731024e-13
  6.57595633e-13  1.13481888e-12  3.37787515e-13  1.31106052e-13
 -1.39414008e-13 -6.44943128e-14 -2.28055895e-13 -3.16344442e-13
  2.79229767e+00]
supnorm grad right now is: 2.7922976658402776
Weights right now are: 
[-4.9456503  -0.70162237 -3.71348767 -4.54199909  5.20021907  0.59293069
  4.11242499  3.82597077 -2.2287587  -0.84604459 -2.19575087 -1.94808818
  1.44204574  0.41617305 -0.12168952  0.71364281 -1.52630256 -0.42169669
 -2.5732855  -2.20454809 -2.26542342 -0.80557171 -2.33266421 -1.84670624
 -0.91357504 -0.74324768 -1.65776969 -0.83399808  0.47169276  1.45841051
  1.56420946  0.50681036 -1.1057791  -0.43045791 -1.5288395  -1.01046451
 -2.5201621  -1.3424052  -1.3153677  -0.98151009 -0.48943803 -1.1314666
  0.57882274  0.52810436  0.43507637  1.12700695 -0.38020928  0.39820008
 -1.49940067 -0.62948123 -0.42810389 -1.59951256  0.63147349  1.18475371
  0.55526717  1.70996806  1.20593157 -0.85992159 -0.10837957 -1.87786588
  2.04313768  1.72876874 -0.13307494  1.04737404 25.18152071]
(65,)
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.787000948458
gradient value of function right now is: [ 2.52662214e-09  3.16500702e-10  2.52601392e-09  2.52695149e-09
 -2.52624144e-09 -3.16452306e-10 -2.52563333e-09 -2.52657075e-09
  5.60864686e-11  6.73410620e-12  5.60468678e-11  5.93216521e-11
  3.28947849e-10  3.94476085e-11  3.28705875e-10  3.47876391e-10
  4.76200917e-11  5.71774333e-12  4.75865225e-11  5.03670661e-11
  4.37507876e-11  5.25308788e-12  4.37198410e-11  4.62744828e-11
 -1.10715480e-11 -3.13017981e-12 -1.27882468e-11 -2.03664336e-12
 -2.73961512e-12 -7.65706173e-13 -3.15723532e-12 -4.98270572e-13
  2.45081445e-11  6.87784500e-12  2.81798958e-11  4.55441133e-12
  1.50140594e-12  3.92521670e-13  1.67821548e-12  2.83012763e-13
  3.21563441e-12  4.54584256e-12  1.62219758e-13  1.53804355e-12
  1.12673031e-12  1.58797848e-12 -4.78362139e-14  4.23698170e-13
  2.34053161e-12  3.25365427e-12  1.34970350e-13  1.02501134e-12
  1.82366206e-12  2.46457028e-12  1.05323081e-13  7.12304704e-13
  6.99897915e-13  1.06937590e-12  3.63166651e-13  1.04230019e-13
 -1.58567784e-13 -3.05829125e-14 -2.49844550e-13 -2.76884048e-13
 -3.17348203e+00]
supnorm grad right now is: 3.173482030731111
Weights right now are: 
[-4.95059705 -0.70224093 -3.71843323 -4.54694647  5.20516582  0.59354924
  4.11737055  3.83091816 -2.22886807 -0.84605774 -2.1958602  -1.94820389
  1.44140265  0.4160958  -0.12233236  0.71296257 -1.5263954  -0.42170786
 -2.57337831 -2.20464631 -2.26550871 -0.80558198 -2.33274948 -1.84679648
 -0.91355313 -0.74324169 -1.65774466 -0.83399408  0.4716982   1.458412
  1.56421569  0.50681134 -1.10582728 -0.43047073 -1.52889394 -1.01047334
 -2.52016499 -1.34240587 -1.31537082 -0.98151062 -0.48944434 -1.13147557
  0.57882281  0.52810175  0.43507416  1.12700381 -0.38020902  0.39819942
 -1.49940529 -0.62948768 -0.42810395 -1.59951436  0.63146988  1.18474882
  0.5552671   1.70996681  1.20593022 -0.85992381 -0.10838026 -1.87786612
  2.04313797  1.72876884 -0.13307446  1.04737464 24.71113375]
(65,)
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.0732543055055
gradient value of function right now is: [ 2.31573267e-09  2.90135005e-10  2.31517913e-09  2.31603254e-09
 -2.31555165e-09 -2.90112225e-10 -2.31499815e-09 -2.31585150e-09
  5.11571216e-11  6.16144727e-12  5.11517372e-11  5.41295491e-11
  3.02031222e-10  3.63156157e-11  3.01968508e-10  3.19519779e-10
  4.34453694e-11  5.23282336e-12  4.34408959e-11  4.59698934e-11
  3.99135005e-11  4.80734895e-12  3.99092804e-11  4.22327111e-11
 -1.03677777e-11 -2.79316090e-12 -1.17893430e-11 -1.88326881e-12
 -2.59108748e-12 -7.03035131e-13 -2.95775955e-12 -4.67491373e-13
  2.26572863e-11  5.87230149e-12  2.53929385e-11  4.12873886e-12
  1.34517610e-12  2.89803725e-13  1.41900585e-12  2.43771031e-13
  2.95536312e-12  4.22042204e-12 -1.23945617e-13  1.13822198e-12
  1.03406300e-12  1.47564513e-12 -1.57000121e-13  2.76225096e-13
  2.17367602e-12  3.03886010e-12 -1.92278308e-14  7.98432050e-13
  1.70165367e-12  2.30931493e-12  5.06361733e-17  5.58354109e-13
  6.30787215e-13  1.06563008e-12  3.24462740e-13  1.19645629e-13
 -1.35418213e-13 -5.54485347e-14 -2.19938082e-13 -2.93492923e-13
  1.73061285e+00]
supnorm grad right now is: 1.7306128458246557
Weights right now are: 
[-4.95539952 -0.70284248 -3.72323455 -4.55174957  5.20996829  0.5941508
  4.12217187  3.83572125 -2.22897437 -0.84607053 -2.19596647 -1.94831634
  1.44077686  0.41602063 -0.12295789  0.71230062 -1.52648566 -0.42171872
 -2.57346854 -2.2047418  -2.26559164 -0.80559195 -2.33283238 -1.84688421
 -0.91353182 -0.74323583 -1.65772028 -0.83399019  0.4717035   1.45841345
  1.56422177  0.5068123  -1.1058741  -0.43048327 -1.52894696 -1.01048194
 -2.52016781 -1.34240653 -1.31537386 -0.98151114 -0.48945045 -1.13148428
  0.57882284  0.52809916  0.43507202  1.12700077 -0.3802088   0.39819875
 -1.49940977 -0.62949393 -0.42810403 -1.59951614  0.63146638  1.18474407
  0.55526702  1.70996557  1.2059289  -0.85992595 -0.10838094 -1.87786635
  2.04313826  1.72876893 -0.133074    1.04737522 25.09584092]
(65,)
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.99033038674
gradient value of function right now is: [ 2.35911717e-09  2.96358609e-10  2.35854992e-09  2.35942351e-09
 -2.35897434e-09 -2.96340833e-10 -2.35840711e-09 -2.35928065e-09
  5.23802410e-11  6.29279591e-12  5.23502711e-11  5.54062583e-11
  3.08503731e-10  3.70141865e-11  3.08313834e-10  3.26279376e-10
  4.45007648e-11  5.34634347e-12  4.44753630e-11  4.70717310e-11
  4.08804418e-11  4.91132873e-12  4.08570072e-11  4.32421737e-11
 -1.04465274e-11 -2.92646549e-12 -1.20298471e-11 -1.91707835e-12
 -2.60108792e-12 -7.22931029e-13 -2.99211828e-12 -4.72393940e-13
  2.29826826e-11  6.35408513e-12  2.62961584e-11  4.25501370e-12
  1.39354966e-12  3.51944461e-13  1.54069906e-12  2.60768499e-13
  3.00677740e-12  4.25891263e-12  9.77199400e-14  1.38417203e-12
  1.05445137e-12  1.48958282e-12 -6.71661972e-14  3.74161409e-13
  2.19340174e-12  3.05249544e-12  9.78448096e-14  9.30433494e-13
  1.71052956e-12  2.31358605e-12  7.93953665e-14  6.47332172e-13
  6.52092048e-13  1.01614182e-12  3.37784758e-13  1.01872326e-13
 -1.46337948e-13 -3.38612126e-14 -2.31808159e-13 -2.66554048e-13
 -2.23271922e+00]
supnorm grad right now is: 2.2327192235852764
Weights right now are: 
[-4.96006566 -0.70342784 -3.72789957 -4.55641631  5.21463442  0.59473616
  4.12683689  3.84038799 -2.2290777  -0.84608296 -2.19606976 -1.94842566
  1.44016746  0.41594743 -0.12356704  0.71165603 -1.52657343 -0.42172928
 -2.57355628 -2.20483466 -2.26567227 -0.80560165 -2.33291298 -1.84696951
 -0.91351103 -0.74323013 -1.65769651 -0.8339864   0.47170869  1.45841487
  1.56422771  0.50681324 -1.10591967 -0.43049544 -1.52899852 -1.01049031
 -2.52017054 -1.34240717 -1.31537681 -0.98151165 -0.4894564  -1.13149274
  0.57882288  0.52809666  0.43506994  1.12699781 -0.38020857  0.3981981
 -1.49941413 -0.62950001 -0.4281041  -1.59951785  0.63146297  1.18473945
  0.55526694  1.70996437  1.20592762 -0.85992804 -0.1083816  -1.87786657
  2.04313854  1.72876902 -0.13307355  1.04737578 24.81436858]
(65,)
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.936662131787
gradient value of function right now is: [ 2.16668741e-09  2.72217067e-10  2.16617005e-09  2.16696693e-09
 -2.16651581e-09 -2.72194834e-10 -2.16599850e-09 -2.16679531e-09
  4.78735478e-11  5.76917743e-12  4.78746770e-11  5.06591593e-11
  2.83804719e-10  3.41399915e-11  2.83778404e-10  3.00258157e-10
  4.06811145e-11  4.90261350e-12  4.06821758e-11  4.30483958e-11
  3.73702450e-11  4.50353896e-12  3.73711129e-11  3.95447832e-11
 -9.79851565e-12 -2.61599572e-12 -1.11099384e-11 -1.77573290e-12
 -2.46344002e-12 -6.64676738e-13 -2.80703213e-12 -4.43823348e-13
  2.12864427e-11  5.43403517e-12  2.37442358e-11  3.86479972e-12
  1.25092729e-12  2.58567934e-13  1.30462631e-12  2.24962864e-13
  2.76809971e-12  3.96070111e-12 -1.63169604e-13  1.01889764e-12
  9.69428656e-13  1.38652095e-12 -1.66236998e-13  2.39846061e-13
  2.04056127e-12  2.85580278e-12 -4.25318686e-14  7.23613104e-13
  1.59881505e-12  2.17147331e-12 -1.65973526e-14  5.06662764e-13
  5.88691336e-13  1.01258339e-12  3.02222812e-13  1.16323533e-13
 -1.25020598e-13 -5.67969816e-14 -2.04347885e-13 -2.81814696e-13
  2.62151155e+00]
supnorm grad right now is: 2.6215115510218054
Weights right now are: 
[-4.96461491 -0.70399935 -3.73244773 -4.56096615  5.21918367  0.59530767
  4.13138505  3.84493783 -2.22917846 -0.84609509 -2.19617049 -1.94853226
  1.43957206  0.4158759  -0.12416224  0.71102621 -1.52665904 -0.42173958
 -2.57364187 -2.20492523 -2.26575091 -0.80561111 -2.3329916  -1.84705272
 -0.91349067 -0.74322457 -1.65767324 -0.83398269  0.47171379  1.45841627
  1.56423354  0.50681416 -1.10596418 -0.43050726 -1.52904879 -1.01049847
 -2.5201732  -1.34240778 -1.31537966 -0.98151213 -0.4894622  -1.13150101
  0.57882297  0.52809427  0.4350679   1.12699492 -0.38020832  0.39819749
 -1.49941839 -0.62950596 -0.42810415 -1.59951951  0.63145964  1.18473494
  0.55526689  1.70996322  1.20592637 -0.85993008 -0.10838225 -1.87786679
  2.04313881  1.72876911 -0.13307311  1.04737633 25.18830307]
(65,)
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.96746255025
gradient value of function right now is: [ 2.11389333e-09  2.65985564e-10  2.11338837e-09  2.11416573e-09
 -2.11412807e-09 -2.66016248e-10 -2.11362305e-09 -2.11440050e-09
  4.67488436e-11  5.63291829e-12  4.67492854e-11  4.94683960e-11
  2.77501279e-10  3.33780573e-11  2.77472486e-10  2.93586297e-10
  3.97372417e-11  4.78824899e-12  3.97377138e-11  4.20490726e-11
  3.65008550e-11  4.39820047e-12  3.65011891e-11  3.86243216e-11
 -9.58491555e-12 -2.56381952e-12 -1.08742096e-11 -1.73790416e-12
 -2.41429714e-12 -6.52134280e-13 -2.75196241e-12 -4.35118336e-13
  2.07920985e-11  5.32552271e-12  2.32166379e-11  3.77807366e-12
  1.22053159e-12  2.54510992e-13  1.27594362e-12  2.19866484e-13
  2.70157982e-12  3.86429293e-12 -1.49528708e-13  1.00457177e-12
  9.46802576e-13  1.35351503e-12 -1.58155595e-13  2.38424788e-13
  1.99107486e-12  2.78611599e-12 -3.63907964e-14  7.11693397e-13
  1.55971242e-12  2.11815519e-12 -1.27481008e-14  4.98144290e-13
  5.75201491e-13  9.85398924e-13  2.95399612e-13  1.12624646e-13
 -1.22396147e-13 -5.44644650e-14 -1.99858924e-13 -2.73679779e-13
  2.43940020e+00]
supnorm grad right now is: 2.439400198597644
Weights right now are: 
[-4.9690254  -0.7045543  -3.73685716 -4.56537721  5.22359416  0.59586262
  4.13579448  3.84934888 -2.22927624 -0.84610685 -2.19626825 -1.94863571
  1.43899356  0.4158064  -0.12474052  0.71041429 -1.52674215 -0.42174958
 -2.57372495 -2.20501316 -2.26582726 -0.8056203  -2.33306792 -1.84713348
 -0.91347088 -0.74321914 -1.65765062 -0.83397907  0.47171875  1.45841763
  1.56423922  0.50681506 -1.10600737 -0.4305188  -1.52909766 -1.0105064
 -2.52017577 -1.34240838 -1.31538244 -0.98151261 -0.48946783 -1.13150902
  0.57882301  0.5280919   0.43506593  1.12699212 -0.38020811  0.39819688
 -1.49942252 -0.62951172 -0.42810421 -1.59952113  0.63145642  1.18473057
  0.55526682  1.70996209  1.20592516 -0.85993206 -0.10838287 -1.877867
  2.04313908  1.7287692  -0.13307268  1.04737686 25.14826582]
(65,)
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.530565487956
gradient value of function right now is: [ 2.22433618e-09  2.80798303e-10  2.22379970e-09  2.22462441e-09
 -2.22425051e-09 -2.80787283e-10 -2.22371405e-09 -2.22453872e-09
  4.95757223e-11  5.94869439e-12  4.95378074e-11  5.24323297e-11
  2.92776911e-10  3.50911157e-11  2.92548145e-10  3.09609373e-10
  4.21564107e-11  5.05857469e-12  4.21242085e-11  4.45856333e-11
  3.87208487e-11  4.64626455e-12  3.86911820e-11  4.09520340e-11
 -9.87984384e-12 -2.81778845e-12 -1.14445766e-11 -1.82181279e-12
 -2.46946565e-12 -6.94118217e-13 -2.85101590e-12 -4.49929441e-13
  2.16988415e-11  6.17610408e-12  2.50668765e-11  4.04695362e-12
  1.32091533e-12  3.56165544e-13  1.49118732e-12  2.50773000e-13
  2.83493400e-12  4.00045144e-12  1.91124580e-13  1.40511899e-12
  9.96846631e-13  1.40139997e-12 -2.18194774e-14  3.94871074e-13
  2.06145887e-12  2.86269783e-12  1.44492434e-13  9.30149529e-13
  1.60476822e-12  2.16708668e-12  1.10037672e-13  6.45736634e-13
  6.19600302e-13  9.28423711e-13  3.21712055e-13  8.73746522e-14
 -1.41675380e-13 -2.20313077e-14 -2.22136330e-13 -2.37308523e-13
 -4.06872307e+00]
supnorm grad right now is: 4.068723068186894
Weights right now are: 
[-4.97331723 -0.7050951  -3.74114796 -4.56966959  5.22788599  0.59640341
  4.14008528  3.85364127 -2.22937144 -0.8461183  -2.19636342 -1.94873643
  1.43842946  0.41573865 -0.12530442  0.70981759 -1.52682308 -0.42175931
 -2.57380586 -2.20509879 -2.2659016  -0.80562924 -2.33314224 -1.84721214
 -0.91345157 -0.74321385 -1.65762853 -0.83397555  0.4717236   1.45841896
  1.56424478  0.50681594 -1.10604945 -0.43053004 -1.52914528 -1.01051412
 -2.52017828 -1.34240896 -1.31538514 -0.98151307 -0.4894733  -1.13151681
  0.57882305  0.52808961  0.43506401  1.12698939 -0.3802079   0.39819629
 -1.49942653 -0.62951732 -0.42810427 -1.59952271  0.63145328  1.18472631
  0.55526675  1.70996099  1.20592398 -0.85993398 -0.10838348 -1.87786721
  2.04313934  1.72876928 -0.13307227  1.04737738 24.64802412]
(65,)
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.136480997312
gradient value of function right now is: [ 2.03973171e-09  2.57476557e-10  2.03924339e-09  2.03999425e-09
 -2.03969446e-09 -2.57471750e-10 -2.03920615e-09 -2.03995699e-09
  4.52243666e-11  5.44439765e-12  4.52182328e-11  4.78502936e-11
  2.68874305e-10  3.23160365e-11  2.68812933e-10  2.84434490e-10
  3.84637540e-11  4.63067672e-12  3.84586188e-11  4.06972830e-11
  3.53280084e-11  4.25309764e-12  3.53231960e-11  3.73793773e-11
 -9.26088986e-12 -2.51068721e-12 -1.05516728e-11 -1.68506254e-12
 -2.33777240e-12 -6.36670907e-13 -2.67164446e-12 -4.22297180e-13
  2.00744475e-11  5.25965459e-12  2.25749720e-11  3.66769212e-12
  1.18268012e-12  2.61708700e-13  1.25695949e-12  2.15488459e-13
  2.60721370e-12  3.71854494e-12 -7.80444520e-14  1.03622483e-12
  9.15358388e-13  1.30380823e-12 -1.25337332e-13  2.57724394e-13
  1.91662367e-12  2.67755799e-12 -4.09894406e-16  7.21734910e-13
  1.49942166e-12  2.03382198e-12  1.12228913e-14  5.04248139e-13
  5.58216547e-13  9.30797147e-13  2.87183072e-13  1.02509685e-13
 -1.20731776e-13 -4.55831451e-14 -1.95293160e-13 -2.54481510e-13
  1.09893633e+00]
supnorm grad right now is: 1.0989363327506632
Weights right now are: 
[-4.97751743 -0.70562507 -3.74534716 -4.57387034  5.23208619  0.59693338
  4.14428448  3.85784201 -2.22946465 -0.84612951 -2.1964566  -1.94883504
  1.43787629  0.4156722  -0.1258574   0.70923246 -1.52690235 -0.42176885
 -2.5738851  -2.20518264 -2.2659744  -0.805638   -2.33321503 -1.84728916
 -0.9134326  -0.74320865 -1.65760685 -0.83397209  0.47172838  1.45842027
  1.56425025  0.5068168  -1.1060907  -0.43054103 -1.52919191 -1.01052168
 -2.52018072 -1.34240953 -1.31538778 -0.98151352 -0.48947867 -1.13152444
  0.5788231   0.52808737  0.43506212  1.12698671 -0.38020768  0.39819572
 -1.49943047 -0.62952281 -0.42810433 -1.59952425  0.6314502   1.18472215
  0.55526669  1.70995992  1.20592283 -0.85993586 -0.10838407 -1.87786741
  2.04313959  1.72876937 -0.13307186  1.04737789 25.05214277]
(65,)
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1102977080436
gradient value of function right now is: [ 2.04883384e-09  2.59157329e-10  2.04834152e-09  2.04909790e-09
 -2.04882141e-09 -2.59155628e-10 -2.04832909e-09 -2.04908547e-09
  4.55762246e-11  5.47816942e-12  4.55569776e-11  4.82132379e-11
  2.70672632e-10  3.24889797e-11  2.70543143e-10  2.86289668e-10
  3.87749919e-11  4.66082291e-12  3.87586769e-11  4.10186251e-11
  3.56120021e-11  4.28056577e-12  3.55969282e-11  3.76725479e-11
 -9.24744910e-12 -2.56877463e-12 -1.06194724e-11 -1.69337787e-12
 -2.33040541e-12 -6.44369319e-13 -2.67617953e-12 -4.22720557e-13
  2.01180446e-11  5.48656289e-12  2.29159099e-11  3.71202706e-12
  1.20022402e-12  2.93259584e-13  1.31340088e-12  2.23096241e-13
  2.61711250e-12  3.71400532e-12  4.22581081e-14  1.16232141e-12
  9.20222588e-13  1.30261075e-12 -7.59629867e-14  3.08978039e-13
  1.91410619e-12  2.66663935e-12  6.28603932e-14  7.88339770e-13
  1.49385467e-12  2.02216882e-12  5.40559814e-14  5.49051897e-13
  5.66015350e-13  8.97473712e-13  2.92533880e-13  9.22145583e-14
 -1.25830003e-13 -3.36368292e-14 -2.00432666e-13 -2.38148192e-13
 -1.37596845e+00]
supnorm grad right now is: 1.3759684476214593
Weights right now are: 
[-4.98160676 -0.70614175 -3.7494355  -4.57796019  5.23617552  0.59745006
  4.14837282  3.86193187 -2.22955544 -0.84614044 -2.19654736 -1.94893109
  1.43733667  0.41560738 -0.12639685  0.70866164 -1.52697957 -0.42177814
 -2.57396231 -2.20526435 -2.26604533 -0.80564653 -2.33328594 -1.8473642
 -0.91341407 -0.74320358 -1.65758567 -0.83396871  0.47173305  1.45842155
  1.5642556   0.50681765 -1.10613092 -0.43055173 -1.52923736 -1.01052906
 -2.5201831  -1.34241007 -1.31539033 -0.98151395 -0.4894839  -1.13153188
  0.57882317  0.5280852   0.43506029  1.1269841  -0.38020747  0.39819517
 -1.49943431 -0.62952816 -0.42810438 -1.59952574  0.6314472   1.18471808
  0.55526664  1.70995887  1.20592171 -0.85993769 -0.10838465 -1.87786761
  2.04313983  1.72876944 -0.13307147  1.04737838 24.88231703]
(65,)
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.029456504874
gradient value of function right now is: [ 2.01460371e-09  2.55207763e-10  2.01411914e-09  2.01486321e-09
 -2.01506565e-09 -2.55266380e-10 -2.01458097e-09 -2.01532522e-09
  4.48737081e-11  5.39156573e-12  4.48518709e-11  4.74678575e-11
  2.66701621e-10  3.20013732e-11  2.66559264e-10  2.82078375e-10
  3.81877736e-11  4.58839321e-12  3.81692434e-11  4.03955373e-11
  3.50705349e-11  4.21378876e-12  3.50534315e-11  3.70980160e-11
 -9.10043527e-12 -2.54302238e-12 -1.04709533e-11 -1.66908836e-12
 -2.29575005e-12 -6.37160137e-13 -2.63953695e-12 -4.16873619e-13
  1.97907162e-11  5.45007012e-12  2.26146248e-11  3.66047288e-12
  1.18251918e-12  2.95625579e-13  1.30317165e-12  2.20869633e-13
  2.57388650e-12  3.64788682e-12  7.12118941e-14  1.17282044e-12
  9.05772751e-13  1.28003901e-12 -6.23506859e-14  3.16303020e-13
  1.88038489e-12  2.61769102e-12  7.73635426e-14  7.90866481e-13
  1.46669333e-12  1.98427269e-12  6.36599234e-14  5.50414689e-13
  5.57973798e-13  8.73621735e-13  2.88614568e-13  8.80322061e-14
 -1.24825630e-13 -3.01416485e-14 -1.98136057e-13 -2.29972282e-13
 -1.98952597e+00]
supnorm grad right now is: 1.9895259671714256
Weights right now are: 
[-4.98557788 -0.70664418 -3.75340567 -4.58193182  5.24014663  0.59795249
  4.15234298  3.86590349 -2.22964365 -0.84615105 -2.19663556 -1.94902442
  1.43681163  0.41554431 -0.12692172  0.70810624 -1.52705463 -0.42178717
 -2.57403735 -2.20534376 -2.26611427 -0.80565483 -2.33335486 -1.84743713
 -0.91339602 -0.74319864 -1.65756504 -0.83396542  0.47173761  1.4584228
  1.56426083  0.50681848 -1.10617002 -0.43056215 -1.52928156 -1.01053624
 -2.52018541 -1.34241061 -1.31539282 -0.98151438 -0.48948898 -1.13153911
  0.57882323  0.52808309  0.4350585   1.12698156 -0.38020726  0.39819463
 -1.49943803 -0.62953336 -0.42810442 -1.5995272   0.63144429  1.18471414
  0.55526658  1.70995786  1.20592062 -0.85993948 -0.10838521 -1.8778678
  2.04314007  1.72876952 -0.13307108  1.04737886 24.86205364]
(65,)
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.121007810864
gradient value of function right now is: [ 1.89293809e-09  2.39886360e-10  1.89248488e-09  1.89318080e-09
 -1.89267386e-09 -2.39852287e-10 -1.89222073e-09 -1.89291654e-09
  4.20235866e-11  5.05951364e-12  4.20198918e-11  4.44646629e-11
  2.50934667e-10  3.01619468e-11  2.50888646e-10  2.65462008e-10
  3.57694073e-11  4.30668344e-12  3.57663400e-11  3.78473329e-11
  3.28493467e-11  3.95504394e-12  3.28464388e-11  3.47575713e-11
 -8.67593298e-12 -2.34776239e-12 -9.87924745e-12 -1.57793437e-12
 -2.20359233e-12 -5.99414572e-13 -2.51728996e-12 -3.97974502e-13
  1.87020292e-11  4.88563950e-12  2.10119951e-11  3.41451812e-12
  1.09362673e-12  2.39928589e-13  1.15944523e-12  1.98963256e-13
  2.42194918e-12  3.45566805e-12 -8.08829355e-14  9.54354791e-13
  8.51723310e-13  1.21354264e-12 -1.19754823e-13  2.36474910e-13
  1.78193692e-12  2.48988547e-12 -4.72084804e-15  6.66445232e-13
  1.39425819e-12  1.89146748e-12  7.48532175e-15  4.65743267e-13
  5.18268589e-13  8.67262147e-13  2.66412336e-13  9.58339105e-14
 -1.11861307e-13 -4.31626287e-14 -1.81173490e-13 -2.37630396e-13
  1.27622796e+00]
supnorm grad right now is: 1.2762279586834602
Weights right now are: 
[-4.98946252 -0.70713631 -3.75728938 -4.58581696  5.24403127  0.59844462
  4.15622669  3.86978863 -2.22972999 -0.84616144 -2.19672188 -1.94911577
  1.43629705  0.41548249 -0.12743613  0.70756192 -1.52712811 -0.42179601
 -2.57411082 -2.2054215  -2.26618175 -0.80566295 -2.33342232 -1.84750853
 -0.91337832 -0.74319379 -1.65754481 -0.83396219  0.4717421   1.45842403
  1.56426596  0.50681929 -1.10620832 -0.43057234 -1.52932484 -1.01054326
 -2.52018767 -1.34241112 -1.31539524 -0.98151479 -0.48949395 -1.13154619
  0.57882329  0.52808102  0.43505675  1.12697908 -0.38020706  0.3981941
 -1.49944168 -0.62953845 -0.42810447 -1.59952862  0.63144144  1.18471028
  0.55526653  1.70995686  1.20591954 -0.85994122 -0.10838576 -1.87786798
  2.04314031  1.7287696  -0.13307071  1.04737934 25.0816441 ]
(65,)
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3590.8570165286114
gradient value of function right now is: [ 1.81141764e-09  2.29729087e-10  1.81098506e-09  1.81164919e-09
 -1.81127144e-09 -2.29709924e-10 -1.81083890e-09 -1.81150297e-09
  4.01527763e-11  4.83983456e-12  4.01583722e-11  4.24914865e-11
  2.40526619e-10  2.89389197e-11  2.40530433e-10  2.54483568e-10
  3.41845656e-11  4.12061270e-12  3.41894189e-11  3.61758054e-11
  3.13926348e-11  3.78401249e-12  3.13970002e-11  3.32211774e-11
 -8.38036358e-12 -2.22705397e-12 -9.48784439e-12 -1.51707789e-12
 -2.13813319e-12 -5.75128178e-13 -2.43384097e-12 -3.84998586e-13
  1.79649591e-11  4.55222083e-12  1.99931282e-11  3.25606233e-12
  1.03694502e-12  2.09352948e-13  1.07456153e-12  1.85762137e-13
  2.32002775e-12  3.32288901e-12 -1.56525704e-13  8.34564209e-13
  8.15755991e-13  1.16763976e-12 -1.47211766e-13  1.94047186e-13
  1.71374600e-12  2.39965844e-12 -4.59051480e-14  5.97031809e-13
  1.34322169e-12  1.82506658e-12 -2.08583135e-14  4.18326856e-13
  4.92813738e-13  8.54821046e-13  2.52484534e-13  9.89457759e-14
 -1.04091171e-13 -4.94750191e-14 -1.70712684e-13 -2.39087561e-13
  3.05324753e+00]
supnorm grad right now is: 3.053247531652529
Weights right now are: 
[-4.99325649 -0.70761761 -3.76108244 -4.58961142  5.24782525  0.59892592
  4.16001976  3.87358309 -2.22981438 -0.84617159 -2.19680625 -1.94920505
  1.43579355  0.41542202 -0.12793947  0.70702932 -1.52719996 -0.42180466
 -2.57418264 -2.20549751 -2.26624772 -0.80567088 -2.33348828 -1.84757833
 -0.913361   -0.74318903 -1.65752499 -0.83395902  0.47174649  1.45842524
  1.564271    0.50682008 -1.10624575 -0.43058235 -1.52936722 -1.01055014
 -2.52018988 -1.34241164 -1.31539762 -0.9815152  -0.4894988  -1.13155309
  0.57882332  0.52807897  0.43505505  1.12697666 -0.38020687  0.39819357
 -1.49944524 -0.62954342 -0.42810453 -1.59953003  0.63143866  1.18470651
  0.55526647  1.70995588  1.2059185  -0.85994292 -0.1083863  -1.87786816
  2.04314054  1.72876967 -0.13307034  1.04737979 25.18259835]
(65,)
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.180389780109
gradient value of function right now is: [ 1.85551261e-09  2.35668645e-10  1.85506752e-09  1.85575042e-09
 -1.85612720e-09 -2.35747240e-10 -1.85568196e-09 -1.85636509e-09
  4.12891151e-11  4.96721698e-12  4.12799628e-11  4.36834603e-11
  2.46725507e-10  2.96364375e-11  2.46651789e-10  2.60988631e-10
  3.51581906e-11  4.22978973e-12  3.51504627e-11  3.71971372e-11
  3.22848740e-11  3.88405162e-12  3.22776949e-11  3.41571236e-11
 -8.50437793e-12 -2.32914358e-12 -9.72128437e-12 -1.55159256e-12
 -2.16204262e-12 -5.92524378e-13 -2.47569144e-12 -3.91280674e-13
  1.83365442e-11  4.88689417e-12  2.07323422e-11  3.36409467e-12
  1.07694164e-12  2.48603932e-13  1.15859534e-12  1.97898855e-13
  2.37458188e-12  3.37990168e-12 -2.52510249e-14  9.90395853e-13
  8.36115973e-13  1.18766761e-12 -9.50145255e-14  2.54518905e-13
  1.74294426e-12  2.43221027e-12  2.37387454e-14  6.82111168e-13
  1.36210963e-12  1.84610082e-12  2.65177668e-14  4.75895898e-13
  5.10801169e-13  8.33949469e-13  2.63092618e-13  8.92078763e-14
 -1.11703212e-13 -3.68842050e-14 -1.79608115e-13 -2.25264730e-13
  5.58316600e-02]
supnorm grad right now is: 0.05583166002464014
Weights right now are: 
[-4.99697254 -0.70808961 -3.7647976  -4.59332795  5.2515413   0.59939793
  4.16373492  3.87729962 -2.22989708 -0.84618154 -2.19688893 -1.94929254
  1.4352995   0.41536268 -0.12843336  0.70650672 -1.52727037 -0.42181313
 -2.57425304 -2.20557201 -2.26631239 -0.80567866 -2.33355293 -1.84764675
 -0.91334399 -0.74318436 -1.65750553 -0.83395592  0.47175081  1.45842642
  1.56427595  0.50682087 -1.10628245 -0.43059218 -1.52940877 -1.01055688
 -2.52019204 -1.34241214 -1.31539995 -0.9815156  -0.48950356 -1.13155985
  0.57882335  0.52807697  0.43505337  1.12697428 -0.38020669  0.39819305
 -1.49944873 -0.62954828 -0.42810459 -1.5995314   0.63143594  1.18470282
  0.55526641  1.70995492  1.20591748 -0.85994458 -0.10838683 -1.87786834
  2.04314076  1.72876974 -0.13306998  1.04738024 24.98157645]
(65,)
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1804898342125
gradient value of function right now is: [ 1.83655707e-09  2.33408966e-10  1.83611646e-09  1.83679233e-09
 -1.83692727e-09 -2.33457021e-10 -1.83648656e-09 -1.83716258e-09
  4.08777730e-11  4.91749158e-12  4.08685042e-11  4.32480659e-11
  2.44407164e-10  2.93567323e-11  2.44333177e-10  2.58535299e-10
  3.48120922e-11  4.18794724e-12  3.48042628e-11  3.68307978e-11
  3.19666866e-11  3.84558315e-12  3.19594161e-11  3.38203277e-11
 -8.42622716e-12 -2.30944515e-12 -9.63421576e-12 -1.53764207e-12
 -2.14386017e-12 -5.87810583e-13 -2.45521900e-12 -3.88044887e-13
  1.81563895e-11  4.84489556e-12  2.05367367e-11  3.33206908e-12
  1.06577938e-12  2.46768081e-13  1.14758906e-12  1.95971632e-13
  2.35037884e-12  3.34505609e-12 -2.17010431e-14  9.83734275e-13
  8.27832636e-13  1.17569340e-12 -9.26654586e-14  2.53396908e-13
  1.72504223e-12  2.40709748e-12  2.52431082e-14  6.77021418e-13
  1.34799466e-12  1.82691787e-12  2.74265672e-14  4.72297068e-13
  5.05837333e-13  8.24545138e-13  2.60539434e-13  8.79929277e-14
 -1.10698956e-13 -3.61602768e-14 -1.77925746e-13 -2.22515021e-13
 -1.63950231e-02]
supnorm grad right now is: 0.016395023089598136
Weights right now are: 
[-5.00058263 -0.70854869 -3.76840682 -4.5969385   5.25515139  0.59985701
  4.16734414  3.88091017 -2.22997744 -0.84619121 -2.19696927 -1.94937756
  1.43481871  0.41530493 -0.12891401  0.70599812 -1.52733881 -0.42182136
 -2.57432147 -2.20564442 -2.26637524 -0.80568622 -2.33361576 -1.84771324
 -0.9133274  -0.74317982 -1.65748656 -0.83395289  0.47175504  1.45842758
  1.56428079  0.50682163 -1.10631817 -0.43060169 -1.52944914 -1.01056343
 -2.52019413 -1.34241262 -1.3154022  -0.98151598 -0.48950818 -1.13156644
  0.57882341  0.52807504  0.43505174  1.12697197 -0.38020649  0.39819256
 -1.49945212 -0.62955302 -0.42810463 -1.59953273  0.63143329  1.18469922
  0.55526636  1.70995399  1.20591648 -0.8599462  -0.10838735 -1.87786851
  2.04314098  1.72876981 -0.13306963  1.04738068 24.99249897]
(65,)
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.180458209942
gradient value of function right now is: [ 1.81688061e-09  2.31049014e-10  1.81644468e-09  1.81711322e-09
 -1.81704714e-09 -2.31071099e-10 -1.81661116e-09 -1.81727978e-09
  4.04476903e-11  4.86567719e-12  4.04385975e-11  4.27930363e-11
  2.41986517e-10  2.90655640e-11  2.41913789e-10  2.55974717e-10
  3.44498535e-11  4.14430414e-12  3.44421723e-11  3.64475460e-11
  3.16336783e-11  3.80546194e-12  3.16265445e-11  3.34680021e-11
 -8.34626862e-12 -2.28788845e-12 -9.54327015e-12 -1.52311572e-12
 -2.12534690e-12 -5.82789509e-13 -2.43408352e-12 -3.84708730e-13
  1.79704320e-11  4.79660418e-12  2.03282127e-11  3.29814932e-12
  1.05392010e-12  2.44165335e-13  1.13501173e-12  1.93816736e-13
  2.32538929e-12  3.30934822e-12 -2.07172032e-14  9.73989412e-13
  8.19251606e-13  1.16341962e-12 -9.13487263e-14  2.51068689e-13
  1.70676155e-12  2.38151780e-12  2.53767484e-14  6.70228523e-13
  1.33368115e-12  1.80748277e-12  2.74073021e-14  4.67553678e-13
  5.00486470e-13  8.15600941e-13  2.57753416e-13  8.69739287e-14
 -1.09545113e-13 -3.56929318e-14 -1.76056927e-13 -2.20054905e-13
 -3.23979726e-02]
supnorm grad right now is: 0.032397972602820894
Weights right now are: 
[-5.00414333 -0.70900203 -3.77196667 -4.60049965  5.25871209  0.60031034
  4.17090399  3.88447133 -2.23005673 -0.84620074 -2.19704854 -1.94946145
  1.43434368  0.41524786 -0.12938891  0.70549562 -1.52740636 -0.42182949
 -2.574389   -2.20571589 -2.26643726 -0.80569369 -2.33367778 -1.84777886
 -0.91331099 -0.74317532 -1.65746781 -0.8339499   0.47175922  1.45842873
  1.56428558  0.50682239 -1.10635344 -0.43061108 -1.529489   -1.0105699
 -2.5201962  -1.3424131  -1.31540442 -0.98151636 -0.48951274 -1.13157293
  0.57882346  0.52807314  0.43505013  1.12696968 -0.38020631  0.39819208
 -1.49945547 -0.62955769 -0.42810467 -1.59953404  0.63143067  1.18469568
  0.55526631  1.70995308  1.2059155  -0.8599478  -0.10838785 -1.87786868
  2.0431412   1.72876988 -0.13306928  1.04738111 25.12710507]
(65,)
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.1802416338774
gradient value of function right now is: [ 1.79845661e-09  2.28846289e-10  1.79802505e-09  1.79868675e-09
 -1.79892428e-09 -2.28906535e-10 -1.79849260e-09 -1.79915448e-09
  4.00526490e-11  4.81796095e-12  4.00435340e-11  4.23749464e-11
  2.39757596e-10  2.87968345e-11  2.39685074e-10  2.53616268e-10
  3.41174631e-11  4.10415137e-12  3.41097609e-11  3.60957589e-11
  3.13275052e-11  3.76847844e-12  3.13203532e-11  3.31439642e-11
 -8.27146700e-12 -2.26860800e-12 -9.45937676e-12 -1.50969137e-12
 -2.10794256e-12 -5.78210062e-13 -2.41440146e-12 -3.81599852e-13
  1.77979982e-11  4.75491682e-12  2.01390214e-11  3.26724784e-12
  1.04317865e-12  2.42209472e-13  1.12416627e-12  1.91931297e-13
  2.30226947e-12  3.27606857e-12 -1.81458804e-14  9.66758116e-13
  8.11338197e-13  1.15199202e-12 -8.94359884e-14  2.49650631e-13
  1.68972545e-12  2.35757280e-12  2.63843666e-14  6.64897231e-13
  1.32030931e-12  1.78925453e-12  2.79799944e-14  4.63793446e-13
  4.95631593e-13  8.06726847e-13  2.55276662e-13  8.58668161e-14
 -1.08556447e-13 -3.50910038e-14 -1.74404997e-13 -2.17520312e-13
 -8.58621248e-02]
supnorm grad right now is: 0.08586212479367712
Weights right now are: 
[-5.00760932 -0.70944383 -3.77543183 -4.60396609  5.26217808  0.60075214
  4.17436915  3.88793776 -2.23013394 -0.84621003 -2.19712574 -1.94954314
  1.4338805   0.41519223 -0.12985197  0.70500566 -1.52747216 -0.4218374
 -2.57445479 -2.2057855  -2.26649767 -0.80570095 -2.33373818 -1.84784278
 -0.91329498 -0.74317094 -1.65744951 -0.83394698  0.47176331  1.45842985
  1.56429027  0.50682313 -1.1063878  -0.43062023 -1.52952785 -1.01057621
 -2.5201982  -1.34241356 -1.31540657 -0.98151673 -0.48951718 -1.13157925
  0.57882352  0.52807129  0.43504857  1.12696746 -0.38020613  0.3981916
 -1.49945873 -0.62956224 -0.42810471 -1.59953531  0.63142812  1.18469222
  0.55526626  1.70995219  1.20591454 -0.85994936 -0.10838834 -1.87786885
  2.04314141  1.72876995 -0.13306895  1.04738153 24.80729686]
(65,)
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.180468259471
gradient value of function right now is: [ 1.77814822e-09  2.26391518e-10  1.77772156e-09  1.77837563e-09
 -1.77777588e-09 -2.26343274e-10 -1.77734931e-09 -1.77800324e-09
  3.95965210e-11  4.76325120e-12  3.95879988e-11  4.18926351e-11
  2.37197631e-10  2.84901372e-11  2.37128543e-10  2.50909707e-10
  3.37325365e-11  4.05798167e-12  3.37253393e-11  3.56887375e-11
  3.09745134e-11  3.72614088e-12  3.09678214e-11  3.27707109e-11
 -8.18932221e-12 -2.24449927e-12 -9.36333342e-12 -1.49440169e-12
 -2.08906110e-12 -5.72778468e-13 -2.39243554e-12 -3.78137211e-13
  1.76041244e-11  4.69768340e-12  1.99124092e-11  3.23063934e-12
  1.03028049e-12  2.38491548e-13  1.10928840e-12  1.89437720e-13
  2.27620343e-12  3.23913573e-12 -2.07475726e-14  9.52511384e-13
  8.02342163e-13  1.13928495e-12 -8.95669177e-14  2.45567869e-13
  1.67093723e-12  2.33135864e-12  2.45934094e-14  6.55602435e-13
  1.30571797e-12  1.76947255e-12  2.66655629e-14  4.57386024e-13
  4.89722879e-13  7.98559950e-13  2.52065850e-13  8.51421080e-14
 -1.07174168e-13 -3.49689513e-14 -1.72247284e-13 -2.15481032e-13
 -2.73239564e-02]
supnorm grad right now is: 0.027323956448332014
Weights right now are: 
[-5.01101356 -0.70987827 -3.77883525 -4.60737076  5.26558232  0.60118658
  4.17777257  3.89134243 -2.23020982 -0.84621916 -2.19720161 -1.94962343
  1.43342482  0.41513749 -0.13030753  0.70452363 -1.52753683 -0.42184518
 -2.57451945 -2.20585393 -2.26655705 -0.8057081  -2.33379755 -1.8479056
 -0.91327921 -0.74316662 -1.65743148 -0.8339441   0.47176735  1.45843096
  1.56429489  0.50682386 -1.10642159 -0.43062923 -1.52956605 -1.01058241
 -2.52020017 -1.34241401 -1.31540869 -0.98151709 -0.48952155 -1.13158547
  0.57882357  0.52806947  0.43504703  1.12696527 -0.38020595  0.39819113
 -1.49946193 -0.62956671 -0.42810475 -1.59953657  0.63142562  1.18468883
  0.55526621  1.70995131  1.2059136  -0.8599509  -0.10838883 -1.87786901
  2.04314161  1.72877002 -0.13306862  1.04738195 25.22998891]
(65,)
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1551.6347824805373
W_T_median: 1406.0687944588276
W_T_pctile_5: 624.1985510164443
W_T_CVAR_5_pct: 487.9109330078141
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.0
F value: -3591.180468259471
-----------------------------------------------
