Starting at: 
05-12-22_18:11

 Random seed:  1  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = basic_T30_VWD
timeseries_basket['basket_desc'] = CRSP data: T30 and VWD
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
5.0% of MC simulations done.
10.0% of MC simulations done.
15.0% of MC simulations done.
20.0% of MC simulations done.
25.0% of MC simulations done.
30.0% of MC simulations done.
35.0% of MC simulations done.
40.0% of MC simulations done.
45.0% of MC simulations done.
50.0% of MC simulations done.
55.00000000000001% of MC simulations done.
60.0% of MC simulations done.
65.0% of MC simulations done.
70.0% of MC simulations done.
75.0% of MC simulations done.
80.0% of MC simulations done.
85.0% of MC simulations done.
90.0% of MC simulations done.
95.0% of MC simulations done.
100.0% of MC simulations done.
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_1/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_1/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)    False        None  
2     (10, 10)    False        None  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)     False        None  
0     (10, 10)     False        None  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -1037.3244459781001
objective value function right now is: -1037.3244459781001
4.0% of gradient descent iterations done. Method = Adam
updated min: -1039.8198232377792
objective value function right now is: -1039.8198232377792
6.0% of gradient descent iterations done. Method = Adam
updated min: -1043.249876656805
objective value function right now is: -1043.249876656805
8.0% of gradient descent iterations done. Method = Adam
updated min: -1043.642716115055
objective value function right now is: -1043.642716115055
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1041.1972707526515
12.0% of gradient descent iterations done. Method = Adam
updated min: -1043.7057917185307
objective value function right now is: -1043.7057917185307
14.000000000000002% of gradient descent iterations done. Method = Adam
updated min: -1045.3487628771677
objective value function right now is: -1045.3487628771677
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1043.1231075605556
18.0% of gradient descent iterations done. Method = Adam
updated min: -1045.9834519230956
objective value function right now is: -1045.9834519230956
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1045.6478916232522
22.0% of gradient descent iterations done. Method = Adam
updated min: -1046.3947699863788
objective value function right now is: -1046.3947699863788
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1045.9509600984798
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1043.2191386890597
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -1044.8605095014204
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1046.3013055220317
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1044.8203010214522
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1046.1750055322373
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1046.2561012900637
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1045.2961944623194
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1045.8000009628165
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1046.0892068298829
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1045.0991825230683
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1046.1653343644987
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1045.069412966699
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1043.5113752663644
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1044.9004521780564
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1040.8273307720694
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -1046.2185699933686
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -1046.0963577985306
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1046.3494806668266
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1044.4836186218643
64.0% of gradient descent iterations done. Method = Adam
updated min: -1046.8080030403921
objective value function right now is: -1046.8080030403921
66.0% of gradient descent iterations done. Method = Adam
updated min: -1046.820501795275
objective value function right now is: -1046.820501795275
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1045.4356622524338
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1037.134042458105
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1045.6865601331276
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1045.4992727938518
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1046.551692987188
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1046.4750868701462
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1046.141908510471
82.0% of gradient descent iterations done. Method = Adam
updated min: -1047.0012094194303
objective value function right now is: -1047.0012094194303
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1044.9795164360803
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1045.6571664300538
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1044.7019843180094
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1046.5365983858362
updated min: 
obj fun: -1047.0505819532148
updated min: 
obj fun: -1047.1314098995601
updated min: 
obj fun: -1047.1963102498178
updated min: 
obj fun: -1047.2365233572943
updated min: 
obj fun: -1047.2501299271053
updated min: 
obj fun: -1047.2558401417073
updated min: 
obj fun: -1047.261563687032
updated min: 
obj fun: -1047.2667499552408
updated min: 
obj fun: -1047.2702548590228
updated min: 
obj fun: -1047.2712266046249
updated min: 
obj fun: -1047.2715365730205
updated min: 
obj fun: -1047.2772578293832
updated min: 
obj fun: -1047.2833691393496
updated min: 
obj fun: -1047.289344029286
updated min: 
obj fun: -1047.2958180356477
updated min: 
obj fun: -1047.3028794686397
updated min: 
obj fun: -1047.3096687079556
updated min: 
obj fun: -1047.315810331705
updated min: 
obj fun: -1047.3194772000902
updated min: 
obj fun: -1047.3212992899205
updated min: 
obj fun: -1047.3219867038838
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1046.2696813720208
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1043.1073224657637
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1046.9113199862884
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1046.4376736451766
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1044.365332569272
NN weights: [ -3.000031     1.4737152   -2.7220075   -2.5432918   -3.3733625
  -8.398888    -0.9032622   -2.7399218   -2.612378    -2.6279597
   2.7210808   -1.7370601    2.9351475    3.101249     3.2204242
   8.783122     0.90635127   2.8663595    2.8765833    2.417404
 -12.626322   -11.321881   -11.212114   -11.552491   -11.2005825
 -10.999081    -9.35348    -10.95575    -11.333473    -6.204695
  -3.498804    29.138687     4.756052    29.274195    27.61953
  26.57243     16.60739     28.53996     25.175434    -6.9850574
 -12.480937   -11.118477   -11.2930765  -10.803042   -11.062396
 -11.27752     -9.515957   -10.790725   -11.186742    -6.3098373
 -12.538786   -10.619271   -11.43109    -11.193183   -10.947292
 -11.299104    -9.004427   -10.97349    -11.441264    -6.402382
 -10.11641    -12.725292   -10.974178   -13.152701   -11.922975
 -11.544196   -10.178759   -12.659624   -11.167668    -5.1920896
  -0.81809235 -15.45404    -25.19613    -15.745      -16.80146
 -17.199984   -23.348633   -15.823887   -18.773544     2.7711387
  -4.142433     7.936937     3.2507408    7.275493     5.8650627
   4.951032    -0.4734991    7.587731     2.7467165   -2.2457962
 -12.473704   -11.10813    -11.386956   -11.1529665  -10.588057
 -10.830391    -9.584858   -10.974353   -10.802252    -6.4073105
 -13.087635   -10.9132595  -11.586733   -10.923919   -10.665622
 -10.410549   -10.234186   -11.095591   -10.608059    -6.6400747
 -13.925522    -9.500549   -12.069472   -10.101805    -9.432701
  -9.6284685  -10.06031     -9.825715   -10.048966    -7.5448647
   5.523916    -2.2938259   -4.334583     2.9998817    4.773359
   3.5208297   -4.2282195    2.9971144   -4.029804     2.983202
  -3.893466     2.9763527   -2.7574499    2.8693817   -4.293595
   2.9955387   -3.4761171    2.9508865  -12.428624    -1.662506  ]
Minimum obj value:-1047.3220155653544
Optimal xi: 31.134325
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1063.4350957698825
W_T_median: 1048.502737901366
W_T_pctile_5: 969.928824496766
W_T_CVAR_5_pct: 940.9878159045709
F value: -1047.3220155653544
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.1
F value: -1047.3220155653544
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[  5.5239,  -2.2938],
        [ -4.3346,   2.9999],
        [  4.7734,   3.5208],
        [ -4.2282,   2.9971],
        [ -4.0298,   2.9832],
        [ -3.8935,   2.9764],
        [ -2.7574,   2.8694],
        [ -4.2936,   2.9955],
        [ -3.4761,   2.9509],
        [-12.4286,  -1.6625]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[-12.6263, -11.3219, -11.2121, -11.5525, -11.2006, -10.9991,  -9.3535,
         -10.9558, -11.3335,  -6.2047],
        [ -3.4988,  29.1387,   4.7561,  29.2742,  27.6195,  26.5724,  16.6074,
          28.5400,  25.1754,  -6.9851],
        [-12.4809, -11.1185, -11.2931, -10.8030, -11.0624, -11.2775,  -9.5160,
         -10.7907, -11.1867,  -6.3098],
        [-12.5388, -10.6193, -11.4311, -11.1932, -10.9473, -11.2991,  -9.0044,
         -10.9735, -11.4413,  -6.4024],
        [-10.1164, -12.7253, -10.9742, -13.1527, -11.9230, -11.5442, -10.1788,
         -12.6596, -11.1677,  -5.1921],
        [ -0.8181, -15.4540, -25.1961, -15.7450, -16.8015, -17.2000, -23.3486,
         -15.8239, -18.7735,   2.7711],
        [ -4.1424,   7.9369,   3.2507,   7.2755,   5.8651,   4.9510,  -0.4735,
           7.5877,   2.7467,  -2.2458],
        [-12.4737, -11.1081, -11.3870, -11.1530, -10.5881, -10.8304,  -9.5849,
         -10.9744, -10.8023,  -6.4073],
        [-13.0876, -10.9133, -11.5867, -10.9239, -10.6656, -10.4105, -10.2342,
         -11.0956, -10.6081,  -6.6401],
        [-13.9255,  -9.5005, -12.0695, -10.1018,  -9.4327,  -9.6285, -10.0603,
          -9.8257, -10.0490,  -7.5449]], device='cuda:0')), ('model.output_layer_3.weight', tensor([[-3.0000,  1.4737, -2.7220, -2.5433, -3.3734, -8.3989, -0.9033, -2.7399,
         -2.6124, -2.6280],
        [ 2.7211, -1.7371,  2.9351,  3.1012,  3.2204,  8.7831,  0.9064,  2.8664,
          2.8766,  2.4174]], device='cuda:0'))])
loaded xi:  31.134325
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -1207.2474709625674
objective value function right now is: -1207.2474709625674
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1206.23165125694
6.0% of gradient descent iterations done. Method = Adam
updated min: -1207.4210807093193
objective value function right now is: -1207.4210807093193
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.0527399561747
10.0% of gradient descent iterations done. Method = Adam
updated min: -1207.819287238779
objective value function right now is: -1207.819287238779
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1205.1286584286775
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -1206.4017052259303
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.438732479965
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1205.595175443633
20.0% of gradient descent iterations done. Method = Adam
updated min: -1208.0156260696183
objective value function right now is: -1208.0156260696183
22.0% of gradient descent iterations done. Method = Adam
updated min: -1208.2201382163628
objective value function right now is: -1208.2201382163628
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.3833752817325
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.7586125941725
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -1208.0227510483874
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.89252102407
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1206.7033277814028
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.3062753680722
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1204.4246768492385
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1206.9431697356245
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.512721104442
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.1905133526689
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1203.068948159405
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1204.5867949557844
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1204.0630068178386
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.10902825716
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1205.47396046499
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1203.9814560180585
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.3083765554225
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.9743447242556
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1206.9099380950481
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1204.6730435181717
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1206.4704719550361
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.272493570176
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1206.4688842836972
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.3103701034522
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.4052309380418
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.3821305814065
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1206.3254249934355
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1206.4756416635353
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.2740915814356
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1206.6243583380692
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.3851810337344
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1203.5598943646369
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1204.851885468538
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1207.4143948297008
updated min: 
obj fun: -1208.2235949843428
updated min: 
obj fun: -1208.2256481025852
updated min: 
obj fun: -1208.225730965483
updated min: 
obj fun: -1208.231471815182
updated min: 
obj fun: -1208.2367607694987
updated min: 
obj fun: -1208.241193035173
updated min: 
obj fun: -1208.2437969651785
updated min: 
obj fun: -1208.2445550708537
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1205.4951519851352
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1206.7488130874765
updated min: 
obj fun: -1208.2453654827154
updated min: 
obj fun: -1208.246683017303
updated min: 
obj fun: -1208.2482550055715
updated min: 
obj fun: -1208.2496964113732
updated min: 
obj fun: -1208.250648318803
updated min: 
obj fun: -1208.251366601985
updated min: 
obj fun: -1208.2516500392576
updated min: 
obj fun: -1208.251715666533
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1205.4746827198203
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1202.5790913591147
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1205.606050862732
NN weights: [ -8.480517    1.4901172  -8.278789   -8.16731    -8.374556  -11.6808405
  -1.0779213  -8.368658   -8.305012   -8.68414     8.201597   -1.7535049
   8.491936    8.725245    8.221607   12.064844    1.0809898   8.495098
   8.569202    8.473547  -10.773176  -13.178281  -18.855543  -13.280134
 -12.617395  -12.260087  -10.311672  -12.727902  -12.522926   -5.4725914
  -2.9562685  39.94456     4.7666254  40.33304    39.07019    38.283985
  29.384237   39.42239    37.30621    -8.053302  -10.599746  -12.94986
 -18.89079   -12.500379  -12.438343  -12.491277  -10.408877  -12.535504
 -12.315554   -5.5014977 -10.57248   -12.386368  -18.92848   -12.824877
 -12.25551   -12.443467   -9.825111  -12.653785  -12.495944   -5.5274243
  -8.799645  -14.327452  -18.515127  -14.617357  -13.041941  -12.491763
 -10.650298  -14.1647625 -12.0154     -4.959442   -1.2972497 -31.638453
 -35.251373  -32.490772  -34.184383  -35.17388   -41.665623  -32.031796
 -38.632175    4.086186   -3.2593331   8.720993    2.7582283   7.6361012
   5.5565915   4.254382    1.2325916   8.25556     2.014357   -2.4610403
 -10.530822  -12.902973  -18.929035  -12.810737  -11.919461  -11.996183
 -10.422845  -12.681684  -11.875184   -5.5306745 -10.944542  -12.632708
 -19.019714  -12.518816  -11.963478  -11.558478  -11.121798  -12.735652
 -11.69117    -5.62009   -11.17871   -11.119694  -19.369848  -11.602504
 -10.6629505 -10.718249  -10.955756  -11.375784  -11.081555   -5.963157
   5.0735016  -1.7575496  -4.657437    3.1411355   4.818377    3.6133437
  -4.528163    3.1814482  -4.285864    3.2533667  -4.1057105   3.304528
  -3.6262627   3.4477541  -4.617391    3.1545758  -3.7090204   3.409286
 -11.392159   -1.7067783]
Minimum obj value:-1208.2517875258447
Optimal xi: 31.09596
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1075.8037813164058
W_T_median: 1051.2042147792577
W_T_pctile_5: 967.7069091238352
W_T_CVAR_5_pct: 939.3164055866822
F value: -1208.2517875258447
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.25
F value: -1208.2517875258447
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[  5.0735,  -1.7575],
        [ -4.6574,   3.1411],
        [  4.8184,   3.6133],
        [ -4.5282,   3.1814],
        [ -4.2859,   3.2534],
        [ -4.1057,   3.3045],
        [ -3.6263,   3.4478],
        [ -4.6174,   3.1546],
        [ -3.7090,   3.4093],
        [-11.3922,  -1.7068]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[-10.7732, -13.1783, -18.8555, -13.2801, -12.6174, -12.2601, -10.3117,
         -12.7279, -12.5229,  -5.4726],
        [ -2.9563,  39.9446,   4.7666,  40.3330,  39.0702,  38.2840,  29.3842,
          39.4224,  37.3062,  -8.0533],
        [-10.5997, -12.9499, -18.8908, -12.5004, -12.4383, -12.4913, -10.4089,
         -12.5355, -12.3156,  -5.5015],
        [-10.5725, -12.3864, -18.9285, -12.8249, -12.2555, -12.4435,  -9.8251,
         -12.6538, -12.4959,  -5.5274],
        [ -8.7996, -14.3275, -18.5151, -14.6174, -13.0419, -12.4918, -10.6503,
         -14.1648, -12.0154,  -4.9594],
        [ -1.2972, -31.6385, -35.2514, -32.4908, -34.1844, -35.1739, -41.6656,
         -32.0318, -38.6322,   4.0862],
        [ -3.2593,   8.7210,   2.7582,   7.6361,   5.5566,   4.2544,   1.2326,
           8.2556,   2.0144,  -2.4610],
        [-10.5308, -12.9030, -18.9290, -12.8107, -11.9195, -11.9962, -10.4228,
         -12.6817, -11.8752,  -5.5307],
        [-10.9445, -12.6327, -19.0197, -12.5188, -11.9635, -11.5585, -11.1218,
         -12.7357, -11.6912,  -5.6201],
        [-11.1787, -11.1197, -19.3698, -11.6025, -10.6630, -10.7182, -10.9558,
         -11.3758, -11.0816,  -5.9632]], device='cuda:0')), ('model.output_layer_3.weight', tensor([[ -8.4805,   1.4901,  -8.2788,  -8.1673,  -8.3746, -11.6808,  -1.0779,
          -8.3687,  -8.3050,  -8.6841],
        [  8.2016,  -1.7535,   8.4919,   8.7252,   8.2216,  12.0648,   1.0810,
           8.4951,   8.5692,   8.4735]], device='cuda:0'))])
loaded xi:  31.09596
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -1365.8416653382708
objective value function right now is: -1365.8416653382708
4.0% of gradient descent iterations done. Method = Adam
updated min: -1369.449091121799
objective value function right now is: -1369.449091121799
6.0% of gradient descent iterations done. Method = Adam
updated min: -1370.9019569278717
objective value function right now is: -1370.9019569278717
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1370.0906114804363
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1369.1425570103083
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1370.4540053869184
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -1367.5250170063819
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1367.7424424091378
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1370.8156779741882
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1357.086094107661
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1370.4404273895368
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1368.417067364058
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1370.0526722015363
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -1369.5519692199412
30.0% of gradient descent iterations done. Method = Adam
updated min: -1371.0091996916856
objective value function right now is: -1371.0091996916856
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1366.6583395554583
34.0% of gradient descent iterations done. Method = Adam
updated min: -1371.0301694242658
objective value function right now is: -1371.0301694242658
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1364.832044517763
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1370.2557049010998
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1370.9369539566605
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1364.7608434286644
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1370.3766031176224
46.0% of gradient descent iterations done. Method = Adam
updated min: -1371.4235106716342
objective value function right now is: -1371.4235106716342
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1371.246575874731
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1369.474088915772
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1371.0334269064722
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1370.8052522355574
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -1371.135112583919
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -1370.5301134593649
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1370.3825610405383
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1370.910636133803
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1369.1371387136292
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1370.711028251633
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1369.9452193252337
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1369.5959780415167
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1366.9845029861392
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1365.7836400218782
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1369.0372839873974
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1371.3952260265573
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1365.1321900887388
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1370.3912153717629
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1369.7718107761089
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1370.126994985585
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1367.869801396262
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1369.372753215958
updated min: 
obj fun: -1371.5545859556323
updated min: 
obj fun: -1371.5996838402025
updated min: 
obj fun: -1371.6004292918067
updated min: 
obj fun: -1371.6253205520222
updated min: 
obj fun: -1371.6466892017297
updated min: 
obj fun: -1371.67146741503
updated min: 
obj fun: -1371.69030080767
updated min: 
obj fun: -1371.7079218003666
updated min: 
obj fun: -1371.7281027228662
updated min: 
obj fun: -1371.742775524316
updated min: 
obj fun: -1371.75513935477
updated min: 
obj fun: -1371.7635156344747
updated min: 
obj fun: -1371.7655805467662
updated min: 
obj fun: -1371.765837098527
updated min: 
obj fun: -1371.7671954151774
updated min: 
obj fun: -1371.7725359956055
updated min: 
obj fun: -1371.7825398533375
updated min: 
obj fun: -1371.796400692558
updated min: 
obj fun: -1371.8165500728337
updated min: 
obj fun: -1371.8422569926493
updated min: 
obj fun: -1371.8676126494063
updated min: 
obj fun: -1371.884076023492
updated min: 
obj fun: -1371.8954702613905
updated min: 
obj fun: -1371.9046675260934
updated min: 
obj fun: -1371.9126141168858
updated min: 
obj fun: -1371.9167528953114
updated min: 
obj fun: -1371.9168399340879
updated min: 
obj fun: -1371.9245847606878
updated min: 
obj fun: -1371.9367358888703
updated min: 
obj fun: -1371.938956393418
updated min: 
obj fun: -1371.940627787292
updated min: 
obj fun: -1371.9492480930085
updated min: 
obj fun: -1371.9567685263137
updated min: 
obj fun: -1371.963727098845
updated min: 
obj fun: -1371.970621283032
updated min: 
obj fun: -1371.9764004963242
updated min: 
obj fun: -1371.9790541742957
updated min: 
obj fun: -1371.9794868662148
updated min: 
obj fun: -1371.9796558348023
updated min: 
obj fun: -1371.9800438742684
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1368.57523439513
updated min: 
obj fun: -1371.9811095728735
updated min: 
obj fun: -1371.9856593435666
updated min: 
obj fun: -1371.988887574406
updated min: 
obj fun: -1371.9904484275146
updated min: 
obj fun: -1371.990649648617
updated min: 
obj fun: -1371.9922240057767
updated min: 
obj fun: -1371.998940180585
updated min: 
obj fun: -1372.003827231689
updated min: 
obj fun: -1372.0054589078982
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1370.5795432202146
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1371.0871415591873
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1370.5622463506763
updated min: 
obj fun: -1372.0063163784143
updated min: 
obj fun: -1372.0074293369753
updated min: 
obj fun: -1372.0091013445551
updated min: 
obj fun: -1372.011179449528
updated min: 
obj fun: -1372.0138749039108
updated min: 
obj fun: -1372.0166829906866
updated min: 
obj fun: -1372.0190541030413
updated min: 
obj fun: -1372.0206654181359
updated min: 
obj fun: -1372.0213908353812
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1366.9749556501638
NN weights: [-12.415098     1.4456006  -12.208912   -12.105118   -11.5742
 -12.712185    -1.2865772  -12.305193   -12.390974   -13.089974
  12.136155    -1.7090199   12.421997    12.663004    11.4212475
  13.095534     1.2896048   12.431603    12.65511     12.879338
  -8.429429   -18.97575    -21.25466    -19.653059   -20.873272
 -22.346111   -25.658943   -18.69775    -24.840456    -4.027345
  -3.0133874   44.46763      6.086211    46.37927     49.26421
  52.010384    52.05346     44.44648     55.758713    -8.1476965
  -8.278432   -18.798058   -21.310303   -18.920696   -20.750298
 -22.65178    -25.897749   -18.554968   -24.736382    -4.0275817
  -8.237396   -18.26842    -21.329037   -19.275785   -20.595984
 -22.634426   -25.356659   -18.705948   -24.950418    -4.026143
  -7.391948   -18.847004   -21.513346   -19.825369   -20.392138
 -21.836443   -25.54877    -18.893091   -23.808378    -3.7655475
  -1.735108   -39.607166   -48.0438     -41.994766   -47.270344
 -50.77576    -59.923473   -40.39201    -57.172596     3.7942283
  -2.8930767    8.412123     2.476575     5.792414     0.8107178
  -0.50092596  -3.0114417    7.456318    -1.4580766   -2.1120095
  -8.204471   -18.773552   -21.342287   -19.243896   -20.233397
 -22.158241   -25.931307   -18.720026   -24.30409     -4.0129776
  -8.367712   -18.76202    -21.24306    -19.210758   -20.516989
 -21.934072   -26.744389   -19.033298   -24.278923    -4.0357237
  -8.103412   -17.953886   -21.239668   -18.975132   -19.877653
 -21.763788   -27.292881   -18.370644   -24.33324     -4.05513
   2.741286    -1.2199455   -3.8609643    2.7218397    4.1402555
   3.4740787   -3.7855299    2.9748197   -3.6807356    3.844965
  -3.8022037    4.256051    -4.3676577    4.9720855   -3.8428786
   2.7985249   -3.9631507    4.5014052  -10.611792    -1.7766227 ]
Minimum obj value:-1372.021350898871
Optimal xi: 31.011982
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1096.399224258871
W_T_median: 1056.6643060654392
W_T_pctile_5: 962.1911947697332
W_T_CVAR_5_pct: 933.4664886104434
F value: -1372.021350898871
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.4
F value: -1372.021350898871
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[  2.7413,  -1.2199],
        [ -3.8610,   2.7218],
        [  4.1403,   3.4741],
        [ -3.7855,   2.9748],
        [ -3.6807,   3.8450],
        [ -3.8022,   4.2561],
        [ -4.3677,   4.9721],
        [ -3.8429,   2.7985],
        [ -3.9632,   4.5014],
        [-10.6118,  -1.7766]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[ -8.4294, -18.9757, -21.2547, -19.6531, -20.8733, -22.3461, -25.6589,
         -18.6978, -24.8405,  -4.0273],
        [ -3.0134,  44.4676,   6.0862,  46.3793,  49.2642,  52.0104,  52.0535,
          44.4465,  55.7587,  -8.1477],
        [ -8.2784, -18.7981, -21.3103, -18.9207, -20.7503, -22.6518, -25.8977,
         -18.5550, -24.7364,  -4.0276],
        [ -8.2374, -18.2684, -21.3290, -19.2758, -20.5960, -22.6344, -25.3567,
         -18.7059, -24.9504,  -4.0261],
        [ -7.3919, -18.8470, -21.5133, -19.8254, -20.3921, -21.8364, -25.5488,
         -18.8931, -23.8084,  -3.7655],
        [ -1.7351, -39.6072, -48.0438, -41.9948, -47.2703, -50.7758, -59.9235,
         -40.3920, -57.1726,   3.7942],
        [ -2.8931,   8.4121,   2.4766,   5.7924,   0.8107,  -0.5009,  -3.0114,
           7.4563,  -1.4581,  -2.1120],
        [ -8.2045, -18.7736, -21.3423, -19.2439, -20.2334, -22.1582, -25.9313,
         -18.7200, -24.3041,  -4.0130],
        [ -8.3677, -18.7620, -21.2431, -19.2108, -20.5170, -21.9341, -26.7444,
         -19.0333, -24.2789,  -4.0357],
        [ -8.1034, -17.9539, -21.2397, -18.9751, -19.8777, -21.7638, -27.2929,
         -18.3706, -24.3332,  -4.0551]], device='cuda:0')), ('model.output_layer_3.weight', tensor([[-12.4151,   1.4456, -12.2089, -12.1051, -11.5742, -12.7122,  -1.2866,
         -12.3052, -12.3910, -13.0900],
        [ 12.1362,  -1.7090,  12.4220,  12.6630,  11.4212,  13.0955,   1.2896,
          12.4316,  12.6551,  12.8793]], device='cuda:0'))])
loaded xi:  31.011982
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -1594.9368223996726
objective value function right now is: -1594.9368223996726
4.0% of gradient descent iterations done. Method = Adam
updated min: -1595.9541123530778
objective value function right now is: -1595.9541123530778
6.0% of gradient descent iterations done. Method = Adam
updated min: -1596.2361252767826
objective value function right now is: -1596.2361252767826
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1595.9228126722078
10.0% of gradient descent iterations done. Method = Adam
updated min: -1596.952920331842
objective value function right now is: -1596.952920331842
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1593.0096484248286
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -1584.3732892659223
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1593.2808538960785
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1596.0270027016995
20.0% of gradient descent iterations done. Method = Adam
updated min: -1597.3315508891576
objective value function right now is: -1597.3315508891576
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1594.1308253572245
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1591.4344265588259
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1597.128048894214
28.000000000000004% of gradient descent iterations done. Method = Adam
updated min: -1597.552063541193
objective value function right now is: -1597.552063541193
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1591.6744084729376
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1596.9000508160034
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1593.75521604531
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1595.5219556454201
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1589.5494740193835
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1595.1259542178152
42.0% of gradient descent iterations done. Method = Adam
updated min: -1598.4833812979357
objective value function right now is: -1598.4833812979357
44.0% of gradient descent iterations done. Method = Adam
updated min: -1598.679586530092
objective value function right now is: -1598.679586530092
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1596.385358536989
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1598.3406252749567
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1598.0967320078923
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1593.7414066591646
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1596.452165340063
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -1595.3013633882995
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -1593.0021585329741
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1596.1187510870404
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1592.6821988999216
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1594.6978838251416
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1595.4556544732716
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1597.0478795237827
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1592.2264818008541
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1596.0964353515558
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1596.2300004490114
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1593.148361416972
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1596.9973863049029
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1596.7696637527129
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1596.580038083687
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1597.3900385532468
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1596.4904452148849
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1596.7628340871343
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1594.6284413331673
updated min: 
obj fun: -1598.6820525810572
updated min: 
obj fun: -1598.6975323617617
updated min: 
obj fun: -1598.7103320773954
updated min: 
obj fun: -1598.7204122138082
updated min: 
obj fun: -1598.7247961084786
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1586.858804677952
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1592.1304288847534
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1598.3887946473371
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1589.8274588189713
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1597.2462802372524
NN weights: [ -2.8387978    1.3806596   -2.5376143   -2.3946996   -1.733616
 -10.803786    -1.8455987   -2.5849      -2.7383876   -3.201583
   2.5597754   -1.6441017    2.7506557    2.952506     1.5805889
  11.185736     1.8486207    2.711248     3.0024667    2.9908874
 -19.965288   -26.713655   -30.329645   -32.27226    -39.611298
 -44.579323   -48.723362   -27.884556   -48.398956    -1.5569636
  -1.9037987   42.127422     7.3553634   59.159637    73.87581
 102.88157     87.09016     44.78953     85.47372     -8.013073
 -19.904913   -26.53185    -30.43428    -31.76351    -39.815678
 -45.31525    -49.339798   -27.767286   -48.655346    -1.5263587
 -19.897106   -26.001451   -30.477886   -32.190628   -39.771996
 -45.45992    -48.931515   -27.926416   -48.996723    -1.535022
 -19.349018   -26.63785    -30.811537   -33.99944    -41.263924
 -47.062084   -51.0483     -28.306526   -49.53947     -1.2467887
  -3.3887663  -38.083286   -50.12912    -43.553467   -53.407547
 -68.1744     -66.37772    -40.207054   -64.23163      7.0913596
  -2.4108868    5.457225     2.2542765   -0.76576954  -0.4926548
  -1.3214122   -1.3996176    2.8647237   -0.34934646  -1.048882
 -19.87215    -26.54682    -30.484991   -32.264053   -39.521225
 -45.084904   -49.61821    -27.990763   -48.454533    -1.4986118
 -19.952976   -26.542896   -30.36669    -32.064262   -39.56314
 -44.536556   -50.181843   -28.291998   -48.17847     -1.5262455
 -19.873764   -25.779247   -30.542377   -32.41155    -39.766773
 -45.52134    -51.787983   -27.745111   -49.185387    -1.5075451
   1.8644068   -1.4901291   -3.1535447    2.1353083    2.8995965
   2.8567863   -6.4818425    4.333691    -5.1640186    4.034481
  -2.4543078    5.0229197   -5.75974      4.1955953   -3.2004235
   2.521508    -4.07906      3.9974768   -8.319294    -1.8415018 ]
Minimum obj value:-1598.7247530168827
Optimal xi: 30.6675
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1159.0213646039874
W_T_median: 1068.878185345868
W_T_pctile_5: 941.6063380293306
W_T_CVAR_5_pct: 903.3339696133149
F value: -1598.7247530168827
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.6
F value: -1598.7247530168827
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ 1.8644, -1.4901],
        [-3.1535,  2.1353],
        [ 2.8996,  2.8568],
        [-6.4818,  4.3337],
        [-5.1640,  4.0345],
        [-2.4543,  5.0229],
        [-5.7597,  4.1956],
        [-3.2004,  2.5215],
        [-4.0791,  3.9975],
        [-8.3193, -1.8415]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[-19.9653, -26.7137, -30.3296, -32.2723, -39.6113, -44.5793, -48.7234,
         -27.8846, -48.3990,  -1.5570],
        [ -1.9038,  42.1274,   7.3554,  59.1596,  73.8758, 102.8816,  87.0902,
          44.7895,  85.4737,  -8.0131],
        [-19.9049, -26.5319, -30.4343, -31.7635, -39.8157, -45.3153, -49.3398,
         -27.7673, -48.6553,  -1.5264],
        [-19.8971, -26.0015, -30.4779, -32.1906, -39.7720, -45.4599, -48.9315,
         -27.9264, -48.9967,  -1.5350],
        [-19.3490, -26.6378, -30.8115, -33.9994, -41.2639, -47.0621, -51.0483,
         -28.3065, -49.5395,  -1.2468],
        [ -3.3888, -38.0833, -50.1291, -43.5535, -53.4075, -68.1744, -66.3777,
         -40.2071, -64.2316,   7.0914],
        [ -2.4109,   5.4572,   2.2543,  -0.7658,  -0.4927,  -1.3214,  -1.3996,
           2.8647,  -0.3493,  -1.0489],
        [-19.8722, -26.5468, -30.4850, -32.2641, -39.5212, -45.0849, -49.6182,
         -27.9908, -48.4545,  -1.4986],
        [-19.9530, -26.5429, -30.3667, -32.0643, -39.5631, -44.5366, -50.1818,
         -28.2920, -48.1785,  -1.5262],
        [-19.8738, -25.7792, -30.5424, -32.4115, -39.7668, -45.5213, -51.7880,
         -27.7451, -49.1854,  -1.5075]], device='cuda:0')), ('model.output_layer_3.weight', tensor([[ -2.8388,   1.3807,  -2.5376,  -2.3947,  -1.7336, -10.8038,  -1.8456,
          -2.5849,  -2.7384,  -3.2016],
        [  2.5598,  -1.6441,   2.7507,   2.9525,   1.5806,  11.1857,   1.8486,
           2.7112,   3.0025,   2.9909]], device='cuda:0'))])
loaded xi:  30.6675
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -1717.8434662585448
objective value function right now is: -1717.8434662585448
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1714.4291866698227
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1717.5641789136455
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1711.9480080447076
10.0% of gradient descent iterations done. Method = Adam
updated min: -1718.6307973621952
objective value function right now is: -1718.6307973621952
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1716.6533068490478
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -1696.2184217056529
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1717.7677791627725
18.0% of gradient descent iterations done. Method = Adam
updated min: -1718.8304944357328
objective value function right now is: -1718.8304944357328
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1718.2908201885527
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1716.6610567770126
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1711.544408112288
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1717.7168828978772
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -1712.9747571157925
30.0% of gradient descent iterations done. Method = Adam
updated min: -1720.136305966091
objective value function right now is: -1720.136305966091
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1717.0214245510522
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1713.987725580158
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1716.1458277218417
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1716.5702380937555
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1719.9924628648544
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1712.3458557997187
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1714.808239634948
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1713.888970160931
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1700.2950981964764
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1716.3850049640619
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1719.095946215129
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1706.379720407981
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -1716.7219269748716
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -1717.2452925544849
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1715.4330171225956
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1719.5951952485127
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1715.154237147885
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1710.1880213132017
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1718.1789409464905
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1718.2744234805377
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1718.541667636003
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1718.6519844215215
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1713.6649610996926
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1711.7578552340162
80.0% of gradient descent iterations done. Method = Adam
updated min: -1720.163274072723
objective value function right now is: -1720.163274072723
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1713.4539973548328
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1713.818418957563
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1712.4263967111078
88.0% of gradient descent iterations done. Method = Adam
updated min: -1721.7465684829283
objective value function right now is: -1721.7465684829283
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1715.4739216230787
updated min: 
obj fun: -1721.7748404395797
updated min: 
obj fun: -1721.8692366671978
updated min: 
obj fun: -1721.946168926392
updated min: 
obj fun: -1722.0083907486367
updated min: 
obj fun: -1722.0602663400362
updated min: 
obj fun: -1722.0736483376959
updated min: 
obj fun: -1722.0844898275232
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1714.540143242844
updated min: 
obj fun: -1722.0895859597276
updated min: 
obj fun: -1722.0990831364031
updated min: 
obj fun: -1722.1064849117092
updated min: 
obj fun: -1722.111043006174
updated min: 
obj fun: -1722.113922384834
updated min: 
obj fun: -1722.1151759814306
updated min: 
obj fun: -1722.1155759996616
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1713.9976397317698
updated min: 
obj fun: -1722.1179852401763
updated min: 
obj fun: -1722.1241847981898
updated min: 
obj fun: -1722.1293921685806
updated min: 
obj fun: -1722.133654633258
updated min: 
obj fun: -1722.1370106933327
updated min: 
obj fun: -1722.1395969357422
updated min: 
obj fun: -1722.1416666345247
updated min: 
obj fun: -1722.1428374826457
updated min: 
obj fun: -1722.1434664747007
updated min: 
obj fun: -1722.1439199468643
updated min: 
obj fun: -1722.144648844522
updated min: 
obj fun: -1722.1460821258127
updated min: 
obj fun: -1722.1475511013514
updated min: 
obj fun: -1722.1489811283943
updated min: 
obj fun: -1722.1503640517765
updated min: 
obj fun: -1722.1517132704012
updated min: 
obj fun: -1722.152653492872
updated min: 
obj fun: -1722.1534556887216
updated min: 
obj fun: -1722.153800362473
updated min: 
obj fun: -1722.1540148384906
updated min: 
obj fun: -1722.1547523336078
updated min: 
obj fun: -1722.1570679668696
updated min: 
obj fun: -1722.1598692180567
updated min: 
obj fun: -1722.1624849387404
updated min: 
obj fun: -1722.164730819744
updated min: 
obj fun: -1722.1665438861817
updated min: 
obj fun: -1722.1680396773895
updated min: 
obj fun: -1722.1687623089892
updated min: 
obj fun: -1722.169009794402
updated min: 
obj fun: -1722.169130107698
updated min: 
obj fun: -1722.169200493592
updated min: 
obj fun: -1722.1692727020536
updated min: 
obj fun: -1722.1699905422524
updated min: 
obj fun: -1722.17212545332
updated min: 
obj fun: -1722.1746205737948
updated min: 
obj fun: -1722.1778356662273
updated min: 
obj fun: -1722.1814425155671
updated min: 
obj fun: -1722.1856751883133
updated min: 
obj fun: -1722.1897251364526
updated min: 
obj fun: -1722.1938446582626
updated min: 
obj fun: -1722.1976178734333
updated min: 
obj fun: -1722.201342509796
updated min: 
obj fun: -1722.2053897004912
updated min: 
obj fun: -1722.2110650267186
updated min: 
obj fun: -1722.217940440441
updated min: 
obj fun: -1722.2246517899
updated min: 
obj fun: -1722.2305467137517
updated min: 
obj fun: -1722.235148685113
updated min: 
obj fun: -1722.238250776209
updated min: 
obj fun: -1722.2397086605433
updated min: 
obj fun: -1722.240324270139
updated min: 
obj fun: -1722.2405921427865
updated min: 
obj fun: -1722.2410940333311
updated min: 
obj fun: -1722.2429897685963
updated min: 
obj fun: -1722.245211222633
updated min: 
obj fun: -1722.2477749852453
updated min: 
obj fun: -1722.2508828618131
updated min: 
obj fun: -1722.25431629582
updated min: 
obj fun: -1722.2580645109995
updated min: 
obj fun: -1722.2620073246346
updated min: 
obj fun: -1722.265940310265
updated min: 
obj fun: -1722.2698726401613
updated min: 
obj fun: -1722.2741053015598
updated min: 
obj fun: -1722.2781691201155
updated min: 
obj fun: -1722.2824568270007
updated min: 
obj fun: -1722.286393631836
updated min: 
obj fun: -1722.2901663411958
updated min: 
obj fun: -1722.2938634876748
updated min: 
obj fun: -1722.2972642883858
updated min: 
obj fun: -1722.3005425567633
updated min: 
obj fun: -1722.3037345008174
updated min: 
obj fun: -1722.3070453655712
updated min: 
obj fun: -1722.3101440591756
updated min: 
obj fun: -1722.3133046916294
updated min: 
obj fun: -1722.316296506236
updated min: 
obj fun: -1722.3187262484644
updated min: 
obj fun: -1722.3203489425105
updated min: 
obj fun: -1722.3212869641752
updated min: 
obj fun: -1722.321714988413
updated min: 
obj fun: -1722.3221049351919
updated min: 
obj fun: -1722.3227236884316
updated min: 
obj fun: -1722.3230843268173
updated min: 
obj fun: -1722.3231048116425
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1720.180736489479
updated min: 
obj fun: -1722.3243117711945
updated min: 
obj fun: -1722.3259313666522
updated min: 
obj fun: -1722.328186270426
updated min: 
obj fun: -1722.3308340375143
updated min: 
obj fun: -1722.3349276561137
updated min: 
obj fun: -1722.3400208795022
updated min: 
obj fun: -1722.3457327694216
updated min: 
obj fun: -1722.3514193944777
updated min: 
obj fun: -1722.3573232257907
updated min: 
obj fun: -1722.3632617089042
updated min: 
obj fun: -1722.3686134231925
updated min: 
obj fun: -1722.3733487673746
updated min: 
obj fun: -1722.3774389975895
updated min: 
obj fun: -1722.3810862434202
updated min: 
obj fun: -1722.3839497280821
updated min: 
obj fun: -1722.3862806376583
updated min: 
obj fun: -1722.3886384685864
updated min: 
obj fun: -1722.390449640278
updated min: 
obj fun: -1722.3924132744642
updated min: 
obj fun: -1722.3940680557557
updated min: 
obj fun: -1722.395583701552
updated min: 
obj fun: -1722.3965478128646
updated min: 
obj fun: -1722.3976277313575
updated min: 
obj fun: -1722.3988906065883
updated min: 
obj fun: -1722.4001821571283
updated min: 
obj fun: -1722.4015748093177
updated min: 
obj fun: -1722.4031451356007
updated min: 
obj fun: -1722.404955858313
updated min: 
obj fun: -1722.4070325324824
updated min: 
obj fun: -1722.4091344394935
updated min: 
obj fun: -1722.411325364787
updated min: 
obj fun: -1722.4136633055261
updated min: 
obj fun: -1722.4161199465739
updated min: 
obj fun: -1722.418911310384
updated min: 
obj fun: -1722.423124583837
updated min: 
obj fun: -1722.428451558256
updated min: 
obj fun: -1722.43424029078
updated min: 
obj fun: -1722.4396471889393
updated min: 
obj fun: -1722.4444953869797
updated min: 
obj fun: -1722.448629420845
updated min: 
obj fun: -1722.4521284404336
updated min: 
obj fun: -1722.454698405657
updated min: 
obj fun: -1722.4561934479864
updated min: 
obj fun: -1722.456966957361
updated min: 
obj fun: -1722.457184223397
updated min: 
obj fun: -1722.4599163375412
updated min: 
obj fun: -1722.464897667397
updated min: 
obj fun: -1722.4712783775558
updated min: 
obj fun: -1722.4785102655476
updated min: 
obj fun: -1722.4861762126782
updated min: 
obj fun: -1722.4939365100554
updated min: 
obj fun: -1722.50151655276
updated min: 
obj fun: -1722.5085575448522
updated min: 
obj fun: -1722.5150946250703
updated min: 
obj fun: -1722.5210328159512
updated min: 
obj fun: -1722.5261529627182
updated min: 
obj fun: -1722.5304348132825
updated min: 
obj fun: -1722.5337978791556
updated min: 
obj fun: -1722.5366421768624
updated min: 
obj fun: -1722.5385785252479
updated min: 
obj fun: -1722.5399992473647
updated min: 
obj fun: -1722.5409015630628
updated min: 
obj fun: -1722.5415735919962
updated min: 
obj fun: -1722.5418532543038
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1707.3488155435286
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1714.9283380945296
NN weights: [ 1.02586145e+01  1.24188793e+00  1.26032085e+01  1.21465712e+01
  1.84062958e+01 -7.82190800e+00 -2.60335612e+00  1.27277918e+01
  4.56396341e+00  2.19023085e+00 -1.05376568e+01 -1.50533676e+00
 -1.23901787e+01 -1.15887871e+01 -1.85594330e+01  8.20064545e+00
  2.60637045e+00 -1.26014671e+01 -4.29988623e+00 -2.40091729e+00
 -1.33037577e+01 -2.75653095e+01 -2.68766022e+01 -3.26628876e+01
 -3.64070282e+01 -4.65637856e+01 -4.58030243e+01 -2.46390533e+01
 -4.61507950e+01  9.56928551e-01 -1.43433654e+00  3.90466957e+01
  9.53800964e+00  2.81944656e+01  8.18451843e+01  1.56645569e+02
  1.21194878e+02  4.77455063e+01  1.06667450e+02 -9.46973896e+00
 -1.12162666e+01 -2.59694424e+01 -2.47223072e+01 -3.41761284e+01
 -3.48748817e+01 -4.52774429e+01 -4.26562996e+01 -2.17071266e+01
 -4.30168190e+01  4.44844335e-01 -1.18564987e+01 -2.60001965e+01
 -2.54624081e+01 -3.37797966e+01 -3.55440445e+01 -4.62262955e+01
 -4.35625191e+01 -2.29568672e+01 -4.45054817e+01  5.89083850e-01
 -7.30818033e+00 -7.51755142e+00 -5.75265551e+00 -3.79088898e+01
 -1.95718403e+01 -9.93316460e+00 -1.82897911e+01 -3.74007344e+00
 -1.96471367e+01  4.92525291e+00 -5.30085897e+00 -3.36121407e+01
 -5.36532173e+01 -7.97818232e+00 -3.71422119e+01 -1.08096596e+02
 -6.43183289e+01 -3.38070145e+01 -6.57982178e+01  1.08033266e+01
 -2.68272758e+00  5.90220785e+00  6.55458391e-01  7.27996707e-01
 -1.29454985e-01 -7.33973086e-01 -1.01149952e+00 -3.15897226e-01
 -6.33014023e-01 -8.36940289e-01 -1.10440741e+01 -2.57769756e+01
 -2.45592957e+01 -3.49752464e+01 -3.43396721e+01 -4.46845055e+01
 -4.25262985e+01 -2.15975761e+01 -4.24592094e+01  4.12737906e-01
 -2.04498539e+01 -2.82524891e+01 -3.03737202e+01 -3.20639763e+01
 -3.95004463e+01 -4.70290871e+01 -5.08627167e+01 -2.83272800e+01
 -4.87580948e+01 -9.08887029e-01 -2.44539318e+01 -2.69269886e+01
 -3.07968063e+01 -3.24126854e+01 -3.97700043e+01 -4.72330055e+01
 -5.20736465e+01 -2.78070641e+01 -4.94504662e+01 -1.96907377e+00
 -1.84681559e+00 -1.31823075e+00 -3.48944783e+00  1.62859333e+00
  1.20618522e+00  2.56846857e+00 -3.92983208e+01  4.60642004e+00
 -1.40301800e+01  4.33764315e+00 -3.06339771e-01  3.76185870e+00
 -6.44053173e+00  3.99517965e+00 -1.01309824e+01  3.93990827e+00
 -6.66406631e+00  3.97694802e+00 -7.39535666e+00 -1.58882570e+00]
Minimum obj value:-1722.5418540845249
Optimal xi: 30.167307
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1241.4421606879805
W_T_median: 1086.6392717936212
W_T_pctile_5: 911.0200214238359
W_T_CVAR_5_pct: 853.5483379768427
F value: -1722.5418540845249
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.7
F value: -1722.5418540845249
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ -1.8468,  -1.3182],
        [ -3.4894,   1.6286],
        [  1.2062,   2.5685],
        [-39.2983,   4.6064],
        [-14.0302,   4.3376],
        [ -0.3063,   3.7619],
        [ -6.4405,   3.9952],
        [-10.1310,   3.9399],
        [ -6.6641,   3.9769],
        [ -7.3954,  -1.5888]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[-1.3304e+01, -2.7565e+01, -2.6877e+01, -3.2663e+01, -3.6407e+01,
         -4.6564e+01, -4.5803e+01, -2.4639e+01, -4.6151e+01,  9.5693e-01],
        [-1.4343e+00,  3.9047e+01,  9.5380e+00,  2.8194e+01,  8.1845e+01,
          1.5665e+02,  1.2119e+02,  4.7746e+01,  1.0667e+02, -9.4697e+00],
        [-1.1216e+01, -2.5969e+01, -2.4722e+01, -3.4176e+01, -3.4875e+01,
         -4.5277e+01, -4.2656e+01, -2.1707e+01, -4.3017e+01,  4.4484e-01],
        [-1.1856e+01, -2.6000e+01, -2.5462e+01, -3.3780e+01, -3.5544e+01,
         -4.6226e+01, -4.3563e+01, -2.2957e+01, -4.4505e+01,  5.8908e-01],
        [-7.3082e+00, -7.5176e+00, -5.7527e+00, -3.7909e+01, -1.9572e+01,
         -9.9332e+00, -1.8290e+01, -3.7401e+00, -1.9647e+01,  4.9253e+00],
        [-5.3009e+00, -3.3612e+01, -5.3653e+01, -7.9782e+00, -3.7142e+01,
         -1.0810e+02, -6.4318e+01, -3.3807e+01, -6.5798e+01,  1.0803e+01],
        [-2.6827e+00,  5.9022e+00,  6.5546e-01,  7.2800e-01, -1.2945e-01,
         -7.3397e-01, -1.0115e+00, -3.1590e-01, -6.3301e-01, -8.3694e-01],
        [-1.1044e+01, -2.5777e+01, -2.4559e+01, -3.4975e+01, -3.4340e+01,
         -4.4685e+01, -4.2526e+01, -2.1598e+01, -4.2459e+01,  4.1274e-01],
        [-2.0450e+01, -2.8252e+01, -3.0374e+01, -3.2064e+01, -3.9500e+01,
         -4.7029e+01, -5.0863e+01, -2.8327e+01, -4.8758e+01, -9.0889e-01],
        [-2.4454e+01, -2.6927e+01, -3.0797e+01, -3.2413e+01, -3.9770e+01,
         -4.7233e+01, -5.2074e+01, -2.7807e+01, -4.9450e+01, -1.9691e+00]],
       device='cuda:0')), ('model.output_layer_3.weight', tensor([[ 10.2586,   1.2419,  12.6032,  12.1466,  18.4063,  -7.8219,  -2.6034,
          12.7278,   4.5640,   2.1902],
        [-10.5377,  -1.5053, -12.3902, -11.5888, -18.5594,   8.2006,   2.6064,
         -12.6015,  -4.2999,  -2.4009]], device='cuda:0'))])
loaded xi:  30.167307
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -1849.7659764890298
objective value function right now is: -1849.7659764890298
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1848.5976465417127
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1849.2749598364258
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1840.336100512302
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1847.790551228268
12.0% of gradient descent iterations done. Method = Adam
updated min: -1850.52730544888
objective value function right now is: -1850.52730544888
14.000000000000002% of gradient descent iterations done. Method = Adam
updated min: -1852.0971369781078
objective value function right now is: -1852.0971369781078
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1835.8571363351596
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1846.9848438585038
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1852.0490218196144
22.0% of gradient descent iterations done. Method = Adam
updated min: -1852.67047120173
objective value function right now is: -1852.67047120173
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1850.6922273316115
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1849.4823602668764
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -1847.2224720038112
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1851.387571227021
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1849.9591682409762
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1848.1313415016755
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1847.6405648120424
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1845.4945192470043
40.0% of gradient descent iterations done. Method = Adam
updated min: -1852.9671933118634
objective value function right now is: -1852.9671933118634
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1849.7062333133938
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1850.239681442285
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1849.1920677867768
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1851.3101965394383
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1848.222302614975
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1849.7991290735995
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1848.5466888672245
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -1846.6854685390704
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -1849.005720153092
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1850.145621458442
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1852.7738273811528
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1852.362032831265
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1848.6915254470746
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1850.46126032141
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1849.5658174744844
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1847.8660155607938
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1846.555221418514
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1848.246168385855
78.0% of gradient descent iterations done. Method = Adam
updated min: -1853.3087324011747
objective value function right now is: -1853.3087324011747
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1852.8464610196897
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1849.9580213384518
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1836.5954954775582
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1849.5506505703518
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1848.8064513146971
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1851.1479332877173
updated min: 
obj fun: -1853.3168280451605
updated min: 
obj fun: -1853.3225374760814
updated min: 
obj fun: -1853.3267478851556
updated min: 
obj fun: -1853.3315451108965
updated min: 
obj fun: -1853.340772164931
updated min: 
obj fun: -1853.347046162629
updated min: 
obj fun: -1853.3501916817968
updated min: 
obj fun: -1853.354044338622
updated min: 
obj fun: -1853.3573174641142
updated min: 
obj fun: -1853.3589959707679
updated min: 
obj fun: -1853.360961226592
updated min: 
obj fun: -1853.3640479531857
updated min: 
obj fun: -1853.3694960886562
updated min: 
obj fun: -1853.3780372512376
updated min: 
obj fun: -1853.3885917670095
updated min: 
obj fun: -1853.4009467062633
updated min: 
obj fun: -1853.412606182592
updated min: 
obj fun: -1853.4230359379374
updated min: 
obj fun: -1853.43155008183
updated min: 
obj fun: -1853.4369213559912
updated min: 
obj fun: -1853.4395391785
updated min: 
obj fun: -1853.4417479879933
updated min: 
obj fun: -1853.4449552413826
updated min: 
obj fun: -1853.4503774819495
updated min: 
obj fun: -1853.4583170816961
updated min: 
obj fun: -1853.4669234204791
updated min: 
obj fun: -1853.475480283046
updated min: 
obj fun: -1853.4836606962265
updated min: 
obj fun: -1853.4913822385563
updated min: 
obj fun: -1853.4994034985104
updated min: 
obj fun: -1853.50612020642
updated min: 
obj fun: -1853.5112615042597
updated min: 
obj fun: -1853.5151153103454
updated min: 
obj fun: -1853.5197166026953
updated min: 
obj fun: -1853.5250583309135
updated min: 
obj fun: -1853.5313209108785
updated min: 
obj fun: -1853.5371648105668
updated min: 
obj fun: -1853.5407277884415
updated min: 
obj fun: -1853.542593188727
updated min: 
obj fun: -1853.5436057985428
updated min: 
obj fun: -1853.545380778847
updated min: 
obj fun: -1853.549972741259
updated min: 
obj fun: -1853.5572087258486
updated min: 
obj fun: -1853.5685994406128
updated min: 
obj fun: -1853.585043364149
updated min: 
obj fun: -1853.6069673561208
updated min: 
obj fun: -1853.6321734872224
updated min: 
obj fun: -1853.6588524701535
updated min: 
obj fun: -1853.6844576415622
updated min: 
obj fun: -1853.7050749966602
updated min: 
obj fun: -1853.718835503616
updated min: 
obj fun: -1853.7266495627698
updated min: 
obj fun: -1853.7292097201966
updated min: 
obj fun: -1853.7487597800791
updated min: 
obj fun: -1853.7774201466157
updated min: 
obj fun: -1853.8077663062718
updated min: 
obj fun: -1853.8363673237302
updated min: 
obj fun: -1853.8602353830977
updated min: 
obj fun: -1853.8778165301235
updated min: 
obj fun: -1853.8908130117036
updated min: 
obj fun: -1853.8995036272775
updated min: 
obj fun: -1853.9052043967881
updated min: 
obj fun: -1853.9084084889305
updated min: 
obj fun: -1853.9095443726076
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1849.1218455259423
updated min: 
obj fun: -1853.9111035723613
updated min: 
obj fun: -1853.9174346314057
updated min: 
obj fun: -1853.9239515764486
updated min: 
obj fun: -1853.930768229648
updated min: 
obj fun: -1853.9374120967543
updated min: 
obj fun: -1853.9438585867679
updated min: 
obj fun: -1853.9493439092655
updated min: 
obj fun: -1853.953759699809
updated min: 
obj fun: -1853.957426625247
updated min: 
obj fun: -1853.9604987286841
updated min: 
obj fun: -1853.9628947461117
updated min: 
obj fun: -1853.9645027850247
updated min: 
obj fun: -1853.964948986212
updated min: 
obj fun: -1853.9669543494374
updated min: 
obj fun: -1853.9698298045998
updated min: 
obj fun: -1853.9731577386062
updated min: 
obj fun: -1853.976992959847
updated min: 
obj fun: -1853.9816527378605
updated min: 
obj fun: -1853.986732507337
updated min: 
obj fun: -1853.9924532004084
updated min: 
obj fun: -1853.998745065872
updated min: 
obj fun: -1854.005292290404
updated min: 
obj fun: -1854.0118332896388
updated min: 
obj fun: -1854.0181626182518
updated min: 
obj fun: -1854.0237821362493
updated min: 
obj fun: -1854.0287785291905
updated min: 
obj fun: -1854.0332966167018
updated min: 
obj fun: -1854.0373171722529
updated min: 
obj fun: -1854.0411757723796
updated min: 
obj fun: -1854.0449577348593
updated min: 
obj fun: -1854.0485508793302
updated min: 
obj fun: -1854.0518770157305
updated min: 
obj fun: -1854.054763424482
updated min: 
obj fun: -1854.0576632057898
updated min: 
obj fun: -1854.060526406156
updated min: 
obj fun: -1854.063142684138
updated min: 
obj fun: -1854.065369727371
updated min: 
obj fun: -1854.0670317226034
updated min: 
obj fun: -1854.068085198651
updated min: 
obj fun: -1854.068883710236
updated min: 
obj fun: -1854.0693442907127
updated min: 
obj fun: -1854.0693544953824
updated min: 
obj fun: -1854.0695431628696
updated min: 
obj fun: -1854.0704302128033
updated min: 
obj fun: -1854.0718206666722
updated min: 
obj fun: -1854.0732296441693
updated min: 
obj fun: -1854.0744479310229
updated min: 
obj fun: -1854.0756347672468
updated min: 
obj fun: -1854.076544986043
updated min: 
obj fun: -1854.0765942500789
updated min: 
obj fun: -1854.0767663495885
updated min: 
obj fun: -1854.0781706655655
updated min: 
obj fun: -1854.079584502902
updated min: 
obj fun: -1854.0804653862638
updated min: 
obj fun: -1854.0805056122392
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1846.9907239990384
updated min: 
obj fun: -1854.0809204093835
updated min: 
obj fun: -1854.082497303506
updated min: 
obj fun: -1854.082892286372
updated min: 
obj fun: -1854.0839696895573
updated min: 
obj fun: -1854.0853164836394
updated min: 
obj fun: -1854.0868674402857
updated min: 
obj fun: -1854.087953119823
updated min: 
obj fun: -1854.0888554113396
updated min: 
obj fun: -1854.0897837870905
updated min: 
obj fun: -1854.090693695606
updated min: 
obj fun: -1854.0918235704999
updated min: 
obj fun: -1854.0927715771047
updated min: 
obj fun: -1854.0937666247844
updated min: 
obj fun: -1854.0946658360904
updated min: 
obj fun: -1854.0952601158135
updated min: 
obj fun: -1854.095616572616
updated min: 
obj fun: -1854.0959197329983
updated min: 
obj fun: -1854.0962724472115
updated min: 
obj fun: -1854.096820812746
updated min: 
obj fun: -1854.0974967651368
updated min: 
obj fun: -1854.0982481420501
updated min: 
obj fun: -1854.0987374539907
updated min: 
obj fun: -1854.0990164385853
updated min: 
obj fun: -1854.0994374306945
updated min: 
obj fun: -1854.099665204059
updated min: 
obj fun: -1854.1000739102465
updated min: 
obj fun: -1854.1002817851663
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1853.6602960404853
updated min: 
obj fun: -1854.1006388612516
updated min: 
obj fun: -1854.101543122854
updated min: 
obj fun: -1854.10260781653
updated min: 
obj fun: -1854.103570261978
updated min: 
obj fun: -1854.10450656373
updated min: 
obj fun: -1854.1051400322874
updated min: 
obj fun: -1854.105521995021
updated min: 
obj fun: -1854.1055506579196
updated min: 
obj fun: -1854.1057993616485
updated min: 
obj fun: -1854.1070168354422
updated min: 
obj fun: -1854.1087563614024
updated min: 
obj fun: -1854.1108338714043
updated min: 
obj fun: -1854.1130681471936
updated min: 
obj fun: -1854.1152975772372
updated min: 
obj fun: -1854.117188448691
updated min: 
obj fun: -1854.1184777919013
updated min: 
obj fun: -1854.11953631058
updated min: 
obj fun: -1854.1202832752008
updated min: 
obj fun: -1854.1209957370902
updated min: 
obj fun: -1854.1216767042936
updated min: 
obj fun: -1854.1227424760793
updated min: 
obj fun: -1854.123678526829
updated min: 
obj fun: -1854.1246467966598
updated min: 
obj fun: -1854.1253304735772
updated min: 
obj fun: -1854.1262158797313
updated min: 
obj fun: -1854.126843339217
updated min: 
obj fun: -1854.127434691362
updated min: 
obj fun: -1854.1280626394027
updated min: 
obj fun: -1854.128663981185
updated min: 
obj fun: -1854.12929451387
updated min: 
obj fun: -1854.1297116551627
updated min: 
obj fun: -1854.130456169729
updated min: 
obj fun: -1854.131085707798
updated min: 
obj fun: -1854.1319480086615
updated min: 
obj fun: -1854.1324001780251
updated min: 
obj fun: -1854.1327391650236
updated min: 
obj fun: -1854.133222345006
updated min: 
obj fun: -1854.13352914178
updated min: 
obj fun: -1854.1341810382162
updated min: 
obj fun: -1854.1349980302068
updated min: 
obj fun: -1854.1361863012464
updated min: 
obj fun: -1854.1372525517145
updated min: 
obj fun: -1854.138208379127
updated min: 
obj fun: -1854.1392396582403
updated min: 
obj fun: -1854.1402086392675
updated min: 
obj fun: -1854.1413906053333
updated min: 
obj fun: -1854.1423017543088
updated min: 
obj fun: -1854.1432728104037
updated min: 
obj fun: -1854.1443141408365
updated min: 
obj fun: -1854.1451357127037
updated min: 
obj fun: -1854.1459253001892
updated min: 
obj fun: -1854.1467564166119
updated min: 
obj fun: -1854.1472884414989
updated min: 
obj fun: -1854.1477366244294
updated min: 
obj fun: -1854.1482294362663
updated min: 
obj fun: -1854.148797397355
updated min: 
obj fun: -1854.1492220926596
updated min: 
obj fun: -1854.1496824546007
updated min: 
obj fun: -1854.149949045186
updated min: 
obj fun: -1854.1500339498205
updated min: 
obj fun: -1854.15006974807
updated min: 
obj fun: -1854.1512598221068
updated min: 
obj fun: -1854.1525443516252
updated min: 
obj fun: -1854.1535543405953
updated min: 
obj fun: -1854.154229822665
updated min: 
obj fun: -1854.1545101671559
updated min: 
obj fun: -1854.1546211713087
updated min: 
obj fun: -1854.1547509444513
updated min: 
obj fun: -1854.1551561049143
updated min: 
obj fun: -1854.1557690208817
updated min: 
obj fun: -1854.1570490392182
updated min: 
obj fun: -1854.1585714704242
updated min: 
obj fun: -1854.1601413986762
updated min: 
obj fun: -1854.1617832696686
updated min: 
obj fun: -1854.1630790911638
updated min: 
obj fun: -1854.164217083374
updated min: 
obj fun: -1854.1651381384838
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1852.0971653008312
updated min: 
obj fun: -1854.16594427132
updated min: 
obj fun: -1854.1668435569788
updated min: 
obj fun: -1854.1682279421893
updated min: 
obj fun: -1854.1695019149522
updated min: 
obj fun: -1854.1711818024837
updated min: 
obj fun: -1854.1730866960147
updated min: 
obj fun: -1854.1746162815025
updated min: 
obj fun: -1854.175846332872
updated min: 
obj fun: -1854.1769512425437
updated min: 
obj fun: -1854.1777550407323
updated min: 
obj fun: -1854.1783910157662
updated min: 
obj fun: -1854.1785668353505
updated min: 
obj fun: -1854.1789502108827
updated min: 
obj fun: -1854.1793301587836
updated min: 
obj fun: -1854.179446629156
updated min: 
obj fun: -1854.1800798983472
updated min: 
obj fun: -1854.1807914724482
updated min: 
obj fun: -1854.1819018601902
updated min: 
obj fun: -1854.1832206137346
updated min: 
obj fun: -1854.1850106050997
updated min: 
obj fun: -1854.1865697405385
updated min: 
obj fun: -1854.1879919768528
updated min: 
obj fun: -1854.1893795295753
updated min: 
obj fun: -1854.1908259635295
updated min: 
obj fun: -1854.1917713162616
updated min: 
obj fun: -1854.1923773510568
updated min: 
obj fun: -1854.1930097863262
updated min: 
obj fun: -1854.1933648018398
updated min: 
obj fun: -1854.1935855665677
updated min: 
obj fun: -1854.194016815199
updated min: 
obj fun: -1854.1946848374305
updated min: 
obj fun: -1854.1950945782608
updated min: 
obj fun: -1854.195728926774
updated min: 
obj fun: -1854.1963123157122
updated min: 
obj fun: -1854.1970172887916
updated min: 
obj fun: -1854.197534579501
updated min: 
obj fun: -1854.1982561879206
updated min: 
obj fun: -1854.1987923241677
updated min: 
obj fun: -1854.1995720604777
updated min: 
obj fun: -1854.200061117757
updated min: 
obj fun: -1854.2006926278204
updated min: 
obj fun: -1854.200978949197
updated min: 
obj fun: -1854.2015198104805
updated min: 
obj fun: -1854.201828799319
updated min: 
obj fun: -1854.2020200302316
updated min: 
obj fun: -1854.2020807692372
updated min: 
obj fun: -1854.202273844984
updated min: 
obj fun: -1854.2027527185483
updated min: 
obj fun: -1854.2032286941321
updated min: 
obj fun: -1854.2041268243386
updated min: 
obj fun: -1854.2049206099573
updated min: 
obj fun: -1854.2060412622072
updated min: 
obj fun: -1854.2071727264472
updated min: 
obj fun: -1854.2081002164045
updated min: 
obj fun: -1854.2088294235252
updated min: 
obj fun: -1854.209520779278
updated min: 
obj fun: -1854.2097437384177
updated min: 
obj fun: -1854.2097704713442
updated min: 
obj fun: -1854.2104561086512
updated min: 
obj fun: -1854.2114248396877
updated min: 
obj fun: -1854.2123547810927
updated min: 
obj fun: -1854.213248162983
updated min: 
obj fun: -1854.2137234525137
updated min: 
obj fun: -1854.2143735197453
updated min: 
obj fun: -1854.2145815498873
updated min: 
obj fun: -1854.2151305150821
updated min: 
obj fun: -1854.2158353820669
updated min: 
obj fun: -1854.2166893820586
updated min: 
obj fun: -1854.217474028728
updated min: 
obj fun: -1854.2184122759695
updated min: 
obj fun: -1854.2190592679033
updated min: 
obj fun: -1854.2197468197294
updated min: 
obj fun: -1854.2202208834904
updated min: 
obj fun: -1854.2208211052648
updated min: 
obj fun: -1854.221578512143
updated min: 
obj fun: -1854.222060235845
updated min: 
obj fun: -1854.2225719557534
updated min: 
obj fun: -1854.2232440645412
updated min: 
obj fun: -1854.2238391699477
updated min: 
obj fun: -1854.2245029218443
updated min: 
obj fun: -1854.2254799969273
updated min: 
obj fun: -1854.2265071967151
updated min: 
obj fun: -1854.2275304662178
updated min: 
obj fun: -1854.2283204898297
updated min: 
obj fun: -1854.228810422401
updated min: 
obj fun: -1854.229001835663
updated min: 
obj fun: -1854.2293087516994
updated min: 
obj fun: -1854.2302891713532
updated min: 
obj fun: -1854.2312377773558
updated min: 
obj fun: -1854.2323081599345
updated min: 
obj fun: -1854.2329924211288
updated min: 
obj fun: -1854.2333762739943
updated min: 
obj fun: -1854.2335961011606
updated min: 
obj fun: -1854.2337561325537
updated min: 
obj fun: -1854.234284071222
updated min: 
obj fun: -1854.2348156262888
updated min: 
obj fun: -1854.2354322222207
updated min: 
obj fun: -1854.235884998585
updated min: 
obj fun: -1854.2362415970174
updated min: 
obj fun: -1854.2365222683043
updated min: 
obj fun: -1854.236695924158
updated min: 
obj fun: -1854.2367550045894
updated min: 
obj fun: -1854.2371339862598
updated min: 
obj fun: -1854.237415554048
updated min: 
obj fun: -1854.237767393861
updated min: 
obj fun: -1854.238276565623
updated min: 
obj fun: -1854.2386094735516
updated min: 
obj fun: -1854.238846892792
updated min: 
obj fun: -1854.238868570639
updated min: 
obj fun: -1854.2395327922325
updated min: 
obj fun: -1854.2402873571968
updated min: 
obj fun: -1854.2412273084885
updated min: 
obj fun: -1854.242193720294
updated min: 
obj fun: -1854.2427685979656
updated min: 
obj fun: -1854.2428646813828
updated min: 
obj fun: -1854.2433918071736
updated min: 
obj fun: -1854.2448069544491
updated min: 
obj fun: -1854.2458025679043
updated min: 
obj fun: -1854.2467641862072
updated min: 
obj fun: -1854.2473203209986
updated min: 
obj fun: -1854.247512680503
updated min: 
obj fun: -1854.2492333570492
updated min: 
obj fun: -1854.2511402729303
updated min: 
obj fun: -1854.2527834472642
updated min: 
obj fun: -1854.2540735339392
updated min: 
obj fun: -1854.2550383997782
updated min: 
obj fun: -1854.2554972128332
updated min: 
obj fun: -1854.2557378120168
updated min: 
obj fun: -1854.256182957132
updated min: 
obj fun: -1854.256564561449
updated min: 
obj fun: -1854.2572168867332
updated min: 
obj fun: -1854.2579552830891
updated min: 
obj fun: -1854.259260145274
updated min: 
obj fun: -1854.2604015456648
updated min: 
obj fun: -1854.2616966584508
updated min: 
obj fun: -1854.263147803462
updated min: 
obj fun: -1854.2646733594775
updated min: 
obj fun: -1854.2658693808098
updated min: 
obj fun: -1854.267150756645
updated min: 
obj fun: -1854.2684536088184
updated min: 
obj fun: -1854.2702003210106
updated min: 
obj fun: -1854.2715783982155
updated min: 
obj fun: -1854.273089120336
updated min: 
obj fun: -1854.2747267698599
updated min: 
obj fun: -1854.2761589531372
updated min: 
obj fun: -1854.277626823952
updated min: 
obj fun: -1854.2788133695315
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1852.7278903801257
NN weights: [   0.50333637   -0.5380065     9.600653      5.1502676    16.236212
   -6.287487     -4.2408767    10.757258    -24.183918    -29.351303
   -0.78239053    0.27462542   -9.387642     -4.592478    -16.389297
    6.6669116     4.2473054   -10.630932     24.448053     29.140669
  -21.56133     -35.151432    -37.9366      -32.675076    -34.528015
  -57.094       -45.680027    -22.018047    -45.481796     -6.6094437
   -2.4794807    75.84119      34.458233     43.077106     92.83266
  174.81146     194.9041      126.20618     194.14606     -18.88306
  -12.337405    -32.80433     -58.357643    -33.97452     -30.161312
  -63.35013     -37.15827     -15.680731    -38.42317     -13.268067
  -16.801033    -33.687046    -44.40949     -33.73904     -31.84506
  -58.697994    -41.078285    -18.229671    -41.47205     -12.292737
   -2.8639607    -3.0395525    -1.6659484     2.7216496    -3.5121968
   -4.387326     -2.3259606     2.5575304     0.40967456    1.1352966
   -2.5977554   -35.449467    -71.99991      16.720528      3.1264946
 -126.23481     -72.18846      -7.5284815   -74.58135       5.6238236
   -4.1822376     3.5133088    -2.018626      1.1920035     0.2963687
   -0.94210917    1.0741963     1.3933332     0.4835191   -15.225213
  -11.125485    -32.205605    -60.700798    -34.689713    -29.330568
  -63.66117     -34.790913    -15.119935    -37.258385    -13.305067
   -5.932588    -69.97047       1.2120404   -29.297974    -33.55875
  -38.663525   -117.4405      -57.72775    -105.09299     -23.664946
   -3.927792    -70.584335      1.1316067   -27.334536    -31.127472
  -35.736893   -122.1073      -74.32126    -118.79917     -24.465704
    2.255595     -4.7237678    -6.3269134     2.6534674     1.1315647
    3.4618979   -62.43203       0.7555697   -64.081856      3.7836938
   -0.7047722     3.318347     -6.3017855     3.1672041   -28.194538
    3.5514348   -21.45322       4.2680817    -5.151408     -1.0422914 ]
Minimum obj value:-1854.2788944628487
Optimal xi: 29.184973
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1345.7444906770513
W_T_median: 1153.7075828870402
W_T_pctile_5: 853.6614479474661
W_T_CVAR_5_pct: 777.7397642291627
F value: -1854.2788944628487
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.8
F value: -1854.2788944628487
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[  2.2556,  -4.7238],
        [ -6.3269,   2.6535],
        [  1.1316,   3.4619],
        [-62.4320,   0.7556],
        [-64.0819,   3.7837],
        [ -0.7048,   3.3183],
        [ -6.3018,   3.1672],
        [-28.1945,   3.5514],
        [-21.4532,   4.2681],
        [ -5.1514,  -1.0423]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[ -21.5613,  -35.1514,  -37.9366,  -32.6751,  -34.5280,  -57.0940,
          -45.6800,  -22.0180,  -45.4818,   -6.6094],
        [  -2.4795,   75.8412,   34.4582,   43.0771,   92.8327,  174.8115,
          194.9041,  126.2062,  194.1461,  -18.8831],
        [ -12.3374,  -32.8043,  -58.3576,  -33.9745,  -30.1613,  -63.3501,
          -37.1583,  -15.6807,  -38.4232,  -13.2681],
        [ -16.8010,  -33.6870,  -44.4095,  -33.7390,  -31.8451,  -58.6980,
          -41.0783,  -18.2297,  -41.4720,  -12.2927],
        [  -2.8640,   -3.0396,   -1.6659,    2.7216,   -3.5122,   -4.3873,
           -2.3260,    2.5575,    0.4097,    1.1353],
        [  -2.5978,  -35.4495,  -71.9999,   16.7205,    3.1265, -126.2348,
          -72.1885,   -7.5285,  -74.5814,    5.6238],
        [  -4.1822,    3.5133,   -2.0186,    1.1920,    0.2964,   -0.9421,
            1.0742,    1.3933,    0.4835,  -15.2252],
        [ -11.1255,  -32.2056,  -60.7008,  -34.6897,  -29.3306,  -63.6612,
          -34.7909,  -15.1199,  -37.2584,  -13.3051],
        [  -5.9326,  -69.9705,    1.2120,  -29.2980,  -33.5588,  -38.6635,
         -117.4405,  -57.7277, -105.0930,  -23.6649],
        [  -3.9278,  -70.5843,    1.1316,  -27.3345,  -31.1275,  -35.7369,
         -122.1073,  -74.3213, -118.7992,  -24.4657]], device='cuda:0')), ('model.output_layer_3.weight', tensor([[  0.5033,  -0.5380,   9.6007,   5.1503,  16.2362,  -6.2875,  -4.2409,
          10.7573, -24.1839, -29.3513],
        [ -0.7824,   0.2746,  -9.3876,  -4.5925, -16.3893,   6.6669,   4.2473,
         -10.6309,  24.4481,  29.1407]], device='cuda:0'))])
loaded xi:  29.184973
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -1991.0463710228566
objective value function right now is: -1991.0463710228566
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1989.5647030215562
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1983.5511705902293
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1989.4598460115426
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1990.642834938744
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1986.9868876132634
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -1987.249988779407
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1987.045679842271
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1990.2885086320175
20.0% of gradient descent iterations done. Method = Adam
updated min: -1991.2301268926972
objective value function right now is: -1991.2301268926972
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1985.611034549307
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1984.2020811177188
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1989.7096213009595
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -1988.7349736089284
30.0% of gradient descent iterations done. Method = Adam
updated min: -1991.393389101877
objective value function right now is: -1991.393389101877
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1989.6325113266525
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1989.845597666108
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1988.308029293359
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1990.084198361499
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1984.034735246788
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1990.9200904983002
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1988.7291898058063
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1985.9705444826802
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1985.1045003889203
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1980.3283251985265
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1990.1584154556097
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1988.1421497682575
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -1990.0562934747297
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -1986.0159102535913
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1991.1516167123013
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1985.0845949017469
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1982.009241074241
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1988.2140296333482
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1982.5251380721131
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1982.6673191139014
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1981.6325522174222
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1990.1768307002467
76.0% of gradient descent iterations done. Method = Adam
updated min: -1991.6309662945484
objective value function right now is: -1991.6309662945484
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1975.4963293368942
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1991.0627698127034
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1987.6431719279042
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1989.4990366620925
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1985.03730342136
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1990.113791429554
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1988.5732326484745
updated min: 
obj fun: -1991.6338227156123
updated min: 
obj fun: -1991.6488292520244
updated min: 
obj fun: -1991.662318125662
updated min: 
obj fun: -1991.6739544486413
updated min: 
obj fun: -1991.6813235896873
updated min: 
obj fun: -1991.6851254277058
updated min: 
obj fun: -1991.6874556680489
updated min: 
obj fun: -1991.689967136105
updated min: 
obj fun: -1991.6947002108714
updated min: 
obj fun: -1991.7032660950563
updated min: 
obj fun: -1991.7116589992559
updated min: 
obj fun: -1991.7174702729567
updated min: 
obj fun: -1991.7216835228637
updated min: 
obj fun: -1991.723938357719
updated min: 
obj fun: -1991.7270511390466
updated min: 
obj fun: -1991.7298238098747
updated min: 
obj fun: -1991.7335509199334
updated min: 
obj fun: -1991.7376933877165
updated min: 
obj fun: -1991.7394082260655
updated min: 
obj fun: -1991.7410627373768
updated min: 
obj fun: -1991.7435619450696
updated min: 
obj fun: -1991.7466541951035
updated min: 
obj fun: -1991.7491032525982
updated min: 
obj fun: -1991.7511929691339
updated min: 
obj fun: -1991.756888241734
updated min: 
obj fun: -1991.76635732702
updated min: 
obj fun: -1991.774494635417
updated min: 
obj fun: -1991.7810008019944
updated min: 
obj fun: -1991.786422737108
updated min: 
obj fun: -1991.7900938387031
updated min: 
obj fun: -1991.7938490156434
updated min: 
obj fun: -1991.7969748767862
updated min: 
obj fun: -1991.800189517721
updated min: 
obj fun: -1991.8036435036481
updated min: 
obj fun: -1991.8070122537856
updated min: 
obj fun: -1991.8095484740852
updated min: 
obj fun: -1991.8115970682609
updated min: 
obj fun: -1991.8128848146296
updated min: 
obj fun: -1991.8139424321093
updated min: 
obj fun: -1991.8148390006018
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1986.5922884473537
updated min: 
obj fun: -1991.8201423582066
updated min: 
obj fun: -1991.827236102593
updated min: 
obj fun: -1991.8347355792755
updated min: 
obj fun: -1991.8416385282908
updated min: 
obj fun: -1991.8478981789974
updated min: 
obj fun: -1991.853587980789
updated min: 
obj fun: -1991.8588351580054
updated min: 
obj fun: -1991.8634464722245
updated min: 
obj fun: -1991.8675878576573
updated min: 
obj fun: -1991.8714603701549
updated min: 
obj fun: -1991.8752526245378
updated min: 
obj fun: -1991.8794730860493
updated min: 
obj fun: -1991.8838250064657
updated min: 
obj fun: -1991.888401164019
updated min: 
obj fun: -1991.893079907296
updated min: 
obj fun: -1991.8972183764554
updated min: 
obj fun: -1991.9006669512687
updated min: 
obj fun: -1991.9043807831206
updated min: 
obj fun: -1991.9075202170104
updated min: 
obj fun: -1991.9100800092092
updated min: 
obj fun: -1991.911852027784
updated min: 
obj fun: -1991.9132124744947
updated min: 
obj fun: -1991.914187030636
updated min: 
obj fun: -1991.9150605614716
updated min: 
obj fun: -1991.9161858939626
updated min: 
obj fun: -1991.917292393513
updated min: 
obj fun: -1991.9187622959637
updated min: 
obj fun: -1991.919952914882
updated min: 
obj fun: -1991.920827536552
updated min: 
obj fun: -1991.9211920814466
updated min: 
obj fun: -1991.9229992167668
updated min: 
obj fun: -1991.9255972293001
updated min: 
obj fun: -1991.9274735762808
updated min: 
obj fun: -1991.9280606217958
updated min: 
obj fun: -1991.931491555826
updated min: 
obj fun: -1991.936629746244
updated min: 
obj fun: -1991.941349603868
updated min: 
obj fun: -1991.9457211736142
updated min: 
obj fun: -1991.9492530900814
updated min: 
obj fun: -1991.9518647991013
updated min: 
obj fun: -1991.9532012854909
updated min: 
obj fun: -1991.9534740172426
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1979.9219283974462
updated min: 
obj fun: -1991.9545048024395
updated min: 
obj fun: -1991.955709518468
updated min: 
obj fun: -1991.9567877218462
updated min: 
obj fun: -1991.9575689935436
updated min: 
obj fun: -1991.958544318973
updated min: 
obj fun: -1991.9595708519219
updated min: 
obj fun: -1991.9602926129146
updated min: 
obj fun: -1991.9611935601495
updated min: 
obj fun: -1991.9621405271535
updated min: 
obj fun: -1991.9632994607812
updated min: 
obj fun: -1991.9647691134833
updated min: 
obj fun: -1991.9660475500664
updated min: 
obj fun: -1991.9673426739034
updated min: 
obj fun: -1991.9682297619393
updated min: 
obj fun: -1991.968785841832
updated min: 
obj fun: -1991.9691171805748
updated min: 
obj fun: -1991.9693575674457
updated min: 
obj fun: -1991.9696745050685
updated min: 
obj fun: -1991.9706848574328
updated min: 
obj fun: -1991.9719999326896
updated min: 
obj fun: -1991.9734963809858
updated min: 
obj fun: -1991.9748385357889
updated min: 
obj fun: -1991.975382811299
updated min: 
obj fun: -1991.9757187633707
updated min: 
obj fun: -1991.9759772782847
updated min: 
obj fun: -1991.9762093467475
updated min: 
obj fun: -1991.9765305982796
updated min: 
obj fun: -1991.977121470272
updated min: 
obj fun: -1991.977405967102
updated min: 
obj fun: -1991.97850008611
updated min: 
obj fun: -1991.9797540391028
updated min: 
obj fun: -1991.9812556040374
updated min: 
obj fun: -1991.982924850074
updated min: 
obj fun: -1991.9842249285095
updated min: 
obj fun: -1991.9849396423226
updated min: 
obj fun: -1991.985502726297
updated min: 
obj fun: -1991.9856434506642
updated min: 
obj fun: -1991.9861723716817
updated min: 
obj fun: -1991.9872964639017
updated min: 
obj fun: -1991.9883700723187
updated min: 
obj fun: -1991.9889276121503
updated min: 
obj fun: -1991.9893920217835
updated min: 
obj fun: -1991.9899289076516
updated min: 
obj fun: -1991.9901929287919
updated min: 
obj fun: -1991.9905147864945
updated min: 
obj fun: -1991.9913292341957
updated min: 
obj fun: -1991.9924791271085
updated min: 
obj fun: -1991.9941709067748
updated min: 
obj fun: -1991.9960665203373
updated min: 
obj fun: -1991.9983291921142
updated min: 
obj fun: -1992.0009652038802
updated min: 
obj fun: -1992.004165355487
updated min: 
obj fun: -1992.0070310995309
updated min: 
obj fun: -1992.0098622012636
updated min: 
obj fun: -1992.0125945835098
updated min: 
obj fun: -1992.0152003425055
updated min: 
obj fun: -1992.0168387585777
updated min: 
obj fun: -1992.0180133123174
updated min: 
obj fun: -1992.0186382608595
updated min: 
obj fun: -1992.0192481489992
updated min: 
obj fun: -1992.0195314531727
updated min: 
obj fun: -1992.0198724805464
updated min: 
obj fun: -1992.0203983050114
updated min: 
obj fun: -1992.0210198275875
updated min: 
obj fun: -1992.0222486652203
updated min: 
obj fun: -1992.0232681874843
updated min: 
obj fun: -1992.0248387036927
updated min: 
obj fun: -1992.0261658403501
updated min: 
obj fun: -1992.0277621692276
updated min: 
obj fun: -1992.0291141513824
updated min: 
obj fun: -1992.0300095443238
updated min: 
obj fun: -1992.0303476672493
updated min: 
obj fun: -1992.0306062366847
updated min: 
obj fun: -1992.031499708198
updated min: 
obj fun: -1992.0323712864722
updated min: 
obj fun: -1992.0341722794312
updated min: 
obj fun: -1992.0367185201958
updated min: 
obj fun: -1992.0397295864307
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1984.6809867363015
updated min: 
obj fun: -1992.0427641949675
updated min: 
obj fun: -1992.0458718167658
updated min: 
obj fun: -1992.0484856688547
updated min: 
obj fun: -1992.0509681248575
updated min: 
obj fun: -1992.0530740729084
updated min: 
obj fun: -1992.0544861017722
updated min: 
obj fun: -1992.0557405230018
updated min: 
obj fun: -1992.0567489573966
updated min: 
obj fun: -1992.0572010375615
updated min: 
obj fun: -1992.0577169219616
updated min: 
obj fun: -1992.0581146485079
updated min: 
obj fun: -1992.0585232497704
updated min: 
obj fun: -1992.0587582600747
updated min: 
obj fun: -1992.0589825434363
updated min: 
obj fun: -1992.0597967719032
updated min: 
obj fun: -1992.0604247262293
updated min: 
obj fun: -1992.0607677994906
updated min: 
obj fun: -1992.0610649010316
updated min: 
obj fun: -1992.0612677653733
updated min: 
obj fun: -1992.0618293099476
updated min: 
obj fun: -1992.0630580478792
updated min: 
obj fun: -1992.0638231236435
updated min: 
obj fun: -1992.0639225801285
updated min: 
obj fun: -1992.0647235389285
updated min: 
obj fun: -1992.0658320190473
updated min: 
obj fun: -1992.0668105942443
updated min: 
obj fun: -1992.067911165651
updated min: 
obj fun: -1992.0689126373172
updated min: 
obj fun: -1992.0700289814101
updated min: 
obj fun: -1992.0711428161874
updated min: 
obj fun: -1992.0719316171812
updated min: 
obj fun: -1992.0724857538312
updated min: 
obj fun: -1992.072644044259
updated min: 
obj fun: -1992.0730833755788
updated min: 
obj fun: -1992.0737042572239
updated min: 
obj fun: -1992.0746419370887
updated min: 
obj fun: -1992.0756792236148
updated min: 
obj fun: -1992.076398514968
updated min: 
obj fun: -1992.0771709992114
updated min: 
obj fun: -1992.077802489256
updated min: 
obj fun: -1992.0783763384454
updated min: 
obj fun: -1992.0789992067967
updated min: 
obj fun: -1992.079685457982
updated min: 
obj fun: -1992.080103010329
updated min: 
obj fun: -1992.0804026829155
updated min: 
obj fun: -1992.0805767754873
updated min: 
obj fun: -1992.0810750431183
updated min: 
obj fun: -1992.0815851161453
updated min: 
obj fun: -1992.082087985043
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1990.8912810572967
updated min: 
obj fun: -1992.082652364721
updated min: 
obj fun: -1992.0834360562706
updated min: 
obj fun: -1992.0848184172455
updated min: 
obj fun: -1992.0864687975993
updated min: 
obj fun: -1992.0885772536963
updated min: 
obj fun: -1992.0903171574325
updated min: 
obj fun: -1992.0916257447825
updated min: 
obj fun: -1992.0927528290135
updated min: 
obj fun: -1992.092809846356
updated min: 
obj fun: -1992.0929002528942
updated min: 
obj fun: -1992.0933712442989
updated min: 
obj fun: -1992.0941212358234
updated min: 
obj fun: -1992.0953771847762
updated min: 
obj fun: -1992.096620907028
updated min: 
obj fun: -1992.0982625404804
updated min: 
obj fun: -1992.0996629253882
updated min: 
obj fun: -1992.1010062258601
updated min: 
obj fun: -1992.1022423218578
updated min: 
obj fun: -1992.1031920919413
updated min: 
obj fun: -1992.1040824639451
updated min: 
obj fun: -1992.1051023019543
updated min: 
obj fun: -1992.1060683345763
updated min: 
obj fun: -1992.1067549425143
updated min: 
obj fun: -1992.1077163295556
updated min: 
obj fun: -1992.1087010452234
updated min: 
obj fun: -1992.1095025071086
updated min: 
obj fun: -1992.1102201921187
updated min: 
obj fun: -1992.1110174911096
updated min: 
obj fun: -1992.1115305454146
updated min: 
obj fun: -1992.112078293643
updated min: 
obj fun: -1992.1123738278177
updated min: 
obj fun: -1992.1128869144452
updated min: 
obj fun: -1992.1134907845608
updated min: 
obj fun: -1992.1138138217514
updated min: 
obj fun: -1992.114093535278
updated min: 
obj fun: -1992.1145043927552
updated min: 
obj fun: -1992.1150599952716
updated min: 
obj fun: -1992.1160133736207
updated min: 
obj fun: -1992.1171287829554
updated min: 
obj fun: -1992.1181594488942
updated min: 
obj fun: -1992.1190792959792
updated min: 
obj fun: -1992.1198366402703
updated min: 
obj fun: -1992.1202608396563
updated min: 
obj fun: -1992.12029595291
updated min: 
obj fun: -1992.1203086402827
updated min: 
obj fun: -1992.120375910719
updated min: 
obj fun: -1992.1205720387802
updated min: 
obj fun: -1992.120892805033
updated min: 
obj fun: -1992.121544592085
updated min: 
obj fun: -1992.1226549111711
updated min: 
obj fun: -1992.12410550347
updated min: 
obj fun: -1992.125632656313
updated min: 
obj fun: -1992.1270389878302
updated min: 
obj fun: -1992.1282207605745
updated min: 
obj fun: -1992.1294666599313
updated min: 
obj fun: -1992.130605449767
updated min: 
obj fun: -1992.1314941578587
updated min: 
obj fun: -1992.1324301148904
updated min: 
obj fun: -1992.1332345224155
updated min: 
obj fun: -1992.1338037433538
updated min: 
obj fun: -1992.1347689222089
updated min: 
obj fun: -1992.1357022608731
updated min: 
obj fun: -1992.1369710328775
updated min: 
obj fun: -1992.1381951936917
updated min: 
obj fun: -1992.1391854989736
updated min: 
obj fun: -1992.140685192739
updated min: 
obj fun: -1992.1418227079935
updated min: 
obj fun: -1992.1431447349912
updated min: 
obj fun: -1992.1443621963717
updated min: 
obj fun: -1992.1457990492938
updated min: 
obj fun: -1992.1469839516678
updated min: 
obj fun: -1992.1481819939825
updated min: 
obj fun: -1992.1489074640651
updated min: 
obj fun: -1992.149532444031
updated min: 
obj fun: -1992.1498118935467
updated min: 
obj fun: -1992.1504865193574
updated min: 
obj fun: -1992.150877604729
updated min: 
obj fun: -1992.1516305134505
updated min: 
obj fun: -1992.1524394622268
updated min: 
obj fun: -1992.1536990572554
updated min: 
obj fun: -1992.1550429808763
updated min: 
obj fun: -1992.1559728612765
updated min: 
obj fun: -1992.1572602019696
updated min: 
obj fun: -1992.1582788420935
updated min: 
obj fun: -1992.159177363393
updated min: 
obj fun: -1992.160135333498
updated min: 
obj fun: -1992.1609852904858
updated min: 
obj fun: -1992.161545775116
updated min: 
obj fun: -1992.162275547178
updated min: 
obj fun: -1992.1629005551495
updated min: 
obj fun: -1992.1635632516895
updated min: 
obj fun: -1992.1646027556294
updated min: 
obj fun: -1992.165625579907
updated min: 
obj fun: -1992.1669109232478
updated min: 
obj fun: -1992.1681483128502
updated min: 
obj fun: -1992.1698959387857
updated min: 
obj fun: -1992.1713456734064
updated min: 
obj fun: -1992.1727006233857
updated min: 
obj fun: -1992.1735355599578
updated min: 
obj fun: -1992.1740061518292
updated min: 
obj fun: -1992.174273871728
updated min: 
obj fun: -1992.1755736478913
updated min: 
obj fun: -1992.176736346286
updated min: 
obj fun: -1992.1777362511743
updated min: 
obj fun: -1992.1791544518605
updated min: 
obj fun: -1992.1800399735841
updated min: 
obj fun: -1992.1811098942558
updated min: 
obj fun: -1992.1819123906614
updated min: 
obj fun: -1992.1824732707535
updated min: 
obj fun: -1992.1833494462633
updated min: 
obj fun: -1992.1838464275368
updated min: 
obj fun: -1992.184419355636
updated min: 
obj fun: -1992.185381952982
updated min: 
obj fun: -1992.1861873419746
updated min: 
obj fun: -1992.1872560307681
updated min: 
obj fun: -1992.188340849951
updated min: 
obj fun: -1992.189422210042
updated min: 
obj fun: -1992.190433407953
updated min: 
obj fun: -1992.1912581660165
updated min: 
obj fun: -1992.1921768166899
updated min: 
obj fun: -1992.1925955020174
updated min: 
obj fun: -1992.1929445167077
updated min: 
obj fun: -1992.1934425739123
updated min: 
obj fun: -1992.1937785319315
updated min: 
obj fun: -1992.1939910910214
updated min: 
obj fun: -1992.1948158080336
updated min: 
obj fun: -1992.195612181162
updated min: 
obj fun: -1992.196153843257
updated min: 
obj fun: -1992.1964154023137
updated min: 
obj fun: -1992.1965225154847
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -1988.3331678272161
NN weights: [   0.70837057   -0.7191865     4.6650944     2.9677505    13.71489
   -6.910674     -4.2905836     5.1685796   -24.577505    -30.480915
   -0.9874284     0.45660275   -4.452101     -2.4099677   -13.867856
    7.291136      4.320821     -5.0422544    24.841623     30.270273
  -21.919043    -35.150753    -37.96749     -32.675076    -34.528015
  -57.120655    -45.678272    -22.018047    -45.481796     -6.821292
   -2.830579    125.20886      46.006535     49.457874    108.330025
  171.14583     233.85698     168.35422     239.81581     -12.786942
  -19.940767    -33.535984    -78.28892     -33.974205    -30.161312
  -82.870964    -38.492847    -15.680337    -38.422882    -15.347675
  -21.730839    -33.68879     -45.945576    -33.73904     -31.84506
  -59.688457    -41.081818    -18.229671    -41.47205     -13.274279
   -2.6443827    -1.5449871    -2.4604914     2.1384928   -30.995056
   -4.0072956    -2.9741817     4.1199903     3.5834377     0.93979025
   -2.6433105   -45.23678     -95.19355      20.061356    -20.934288
 -153.53946     -82.760574    -19.162334    -86.27134       5.200822
   -2.960323      2.5555146    -1.1437954    -1.0052439     1.8209229
   -1.3070735     2.6144984     1.9401186     1.8348099   -26.509792
  -18.957928    -33.6602      -85.97566     -34.687737    -29.330595
  -89.190254    -37.74349     -15.118007    -37.256447    -15.3453455
   -6.7660394   -71.03434       2.8764448   -29.299536    -33.55875
  -22.411936   -112.78978     -57.86425    -105.80341     -38.962624
   -3.3624086   -71.45558       3.1706574   -27.34358     -31.127514
  -19.19269    -117.1339      -75.60767    -123.42015     -38.13847
    1.6876419    -4.5927653    -6.5483813     3.0546608    -1.160848
    3.020467    -76.76613       0.7802901   -60.691975     15.258185
    0.5529567     3.1673048    -4.962837      2.8731983   -32.221294
    5.0692058   -31.83311       5.342556     -6.5286674    -1.2963266 ]
Minimum obj value:-1992.1965975835974
Optimal xi: 28.403734
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1412.4544059630266
W_T_median: 1206.8648867563443
W_T_pctile_5: 808.3120882118814
W_T_CVAR_5_pct: 721.0201801858415
F value: -1992.1965975835974
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.9
F value: -1992.1965975835974
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[  1.6876,  -4.5928],
        [ -6.5484,   3.0547],
        [ -1.1608,   3.0205],
        [-76.7661,   0.7803],
        [-60.6920,  15.2582],
        [  0.5530,   3.1673],
        [ -4.9628,   2.8732],
        [-32.2213,   5.0692],
        [-31.8331,   5.3426],
        [ -6.5287,  -1.2963]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[ -21.9190,  -35.1508,  -37.9675,  -32.6751,  -34.5280,  -57.1207,
          -45.6783,  -22.0180,  -45.4818,   -6.8213],
        [  -2.8306,  125.2089,   46.0065,   49.4579,  108.3300,  171.1458,
          233.8570,  168.3542,  239.8158,  -12.7869],
        [ -19.9408,  -33.5360,  -78.2889,  -33.9742,  -30.1613,  -82.8710,
          -38.4928,  -15.6803,  -38.4229,  -15.3477],
        [ -21.7308,  -33.6888,  -45.9456,  -33.7390,  -31.8451,  -59.6885,
          -41.0818,  -18.2297,  -41.4720,  -13.2743],
        [  -2.6444,   -1.5450,   -2.4605,    2.1385,  -30.9951,   -4.0073,
           -2.9742,    4.1200,    3.5834,    0.9398],
        [  -2.6433,  -45.2368,  -95.1936,   20.0614,  -20.9343, -153.5395,
          -82.7606,  -19.1623,  -86.2713,    5.2008],
        [  -2.9603,    2.5555,   -1.1438,   -1.0052,    1.8209,   -1.3071,
            2.6145,    1.9401,    1.8348,  -26.5098],
        [ -18.9579,  -33.6602,  -85.9757,  -34.6877,  -29.3306,  -89.1903,
          -37.7435,  -15.1180,  -37.2564,  -15.3453],
        [  -6.7660,  -71.0343,    2.8764,  -29.2995,  -33.5588,  -22.4119,
         -112.7898,  -57.8643, -105.8034,  -38.9626],
        [  -3.3624,  -71.4556,    3.1707,  -27.3436,  -31.1275,  -19.1927,
         -117.1339,  -75.6077, -123.4202,  -38.1385]], device='cuda:0')), ('model.output_layer_3.weight', tensor([[  0.7084,  -0.7192,   4.6651,   2.9678,  13.7149,  -6.9107,  -4.2906,
           5.1686, -24.5775, -30.4809],
        [ -0.9874,   0.4566,  -4.4521,  -2.4100, -13.8679,   7.2911,   4.3208,
          -5.0423,  24.8416,  30.2703]], device='cuda:0'))])
loaded xi:  28.403734
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -2132.194383446005
objective value function right now is: -2132.194383446005
4.0% of gradient descent iterations done. Method = Adam
updated min: -2132.6353061208333
objective value function right now is: -2132.6353061208333
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2129.901110738735
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2130.1239565772116
10.0% of gradient descent iterations done. Method = Adam
updated min: -2133.5144706522647
objective value function right now is: -2133.5144706522647
12.0% of gradient descent iterations done. Method = Adam
updated min: -2133.8214330919877
objective value function right now is: -2133.8214330919877
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -2126.3089552388883
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2132.956437964066
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2113.2348759615265
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2132.5436515359515
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2133.5865788122596
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2130.7905579235653
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2133.144682110019
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -2128.5511936638245
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2129.728741053031
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2128.808077027234
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2133.1139057222845
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2132.9264664260018
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2122.263281495837
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2131.309640908808
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2129.81990555891
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2132.083743753203
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2130.562258593943
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2127.5110942278475
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2131.8185222057105
52.0% of gradient descent iterations done. Method = Adam
updated min: -2133.864610124476
objective value function right now is: -2133.864610124476
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2131.8915906072657
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -2132.962330686383
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -2128.32826186621
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2131.612707604063
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2126.235873620945
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2131.9532979300348
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2132.2616427204853
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2130.112978776411
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2130.381390313301
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2130.9852678776633
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2129.213229318666
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2126.0448622934878
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2130.4116450897836
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2127.747766458022
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2131.22302815919
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2131.8016527009922
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2131.446281757191
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2130.59901695761
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2128.6438889141923
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2130.5631557862757
updated min: 
obj fun: -2133.866955671599
updated min: 
obj fun: -2133.870386741627
updated min: 
obj fun: -2133.8732131628535
updated min: 
obj fun: -2133.8754930096297
updated min: 
obj fun: -2133.8766611786414
updated min: 
obj fun: -2133.877808957957
updated min: 
obj fun: -2133.8789918273087
updated min: 
obj fun: -2133.880504500106
updated min: 
obj fun: -2133.8825257009103
updated min: 
obj fun: -2133.885033787541
updated min: 
obj fun: -2133.887550474922
updated min: 
obj fun: -2133.8903510186224
updated min: 
obj fun: -2133.8935461141136
updated min: 
obj fun: -2133.896498323188
updated min: 
obj fun: -2133.89893360658
updated min: 
obj fun: -2133.9013578585923
updated min: 
obj fun: -2133.9028821016796
updated min: 
obj fun: -2133.9036913455648
updated min: 
obj fun: -2133.904607069867
updated min: 
obj fun: -2133.9051172843774
updated min: 
obj fun: -2133.905913730376
updated min: 
obj fun: -2133.907299062724
updated min: 
obj fun: -2133.9088164915993
updated min: 
obj fun: -2133.9118105339894
updated min: 
obj fun: -2133.9156509096483
updated min: 
obj fun: -2133.9198059792097
updated min: 
obj fun: -2133.9238471967283
updated min: 
obj fun: -2133.927874817821
updated min: 
obj fun: -2133.9316340409046
updated min: 
obj fun: -2133.93463472406
updated min: 
obj fun: -2133.938576623965
updated min: 
obj fun: -2133.942066321701
updated min: 
obj fun: -2133.945516166114
updated min: 
obj fun: -2133.949347273611
updated min: 
obj fun: -2133.9527520808347
updated min: 
obj fun: -2133.9561848578933
updated min: 
obj fun: -2133.9587945194153
updated min: 
obj fun: -2133.961998538187
updated min: 
obj fun: -2133.9647121296175
updated min: 
obj fun: -2133.967380149739
updated min: 
obj fun: -2133.969581590496
updated min: 
obj fun: -2133.971724032441
updated min: 
obj fun: -2133.973782104623
updated min: 
obj fun: -2133.9749472653198
updated min: 
obj fun: -2133.9764385177577
updated min: 
obj fun: -2133.977880197073
updated min: 
obj fun: -2133.978850075869
updated min: 
obj fun: -2133.9793802585455
updated min: 
obj fun: -2133.9799564437185
updated min: 
obj fun: -2133.980160448245
updated min: 
obj fun: -2133.9817185050156
updated min: 
obj fun: -2133.9855066867026
updated min: 
obj fun: -2133.9899850840643
updated min: 
obj fun: -2133.994621313342
updated min: 
obj fun: -2133.9989335967516
updated min: 
obj fun: -2134.003176423013
updated min: 
obj fun: -2134.006392432659
updated min: 
obj fun: -2134.010393852641
updated min: 
obj fun: -2134.014171125627
updated min: 
obj fun: -2134.018142671125
updated min: 
obj fun: -2134.0221067714715
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2132.0769393518362
updated min: 
obj fun: -2134.0253041901487
updated min: 
obj fun: -2134.0292174731717
updated min: 
obj fun: -2134.0328920188444
updated min: 
obj fun: -2134.0362879117047
updated min: 
obj fun: -2134.039251670732
updated min: 
obj fun: -2134.041793483063
updated min: 
obj fun: -2134.0431665877813
updated min: 
obj fun: -2134.0449678763093
updated min: 
obj fun: -2134.0467609620714
updated min: 
obj fun: -2134.048733166289
updated min: 
obj fun: -2134.0506018594874
updated min: 
obj fun: -2134.052823544016
updated min: 
obj fun: -2134.0553252106106
updated min: 
obj fun: -2134.058006419406
updated min: 
obj fun: -2134.060664823658
updated min: 
obj fun: -2134.063347502821
updated min: 
obj fun: -2134.0656802858534
updated min: 
obj fun: -2134.067774342582
updated min: 
obj fun: -2134.0687611877006
updated min: 
obj fun: -2134.069176492666
updated min: 
obj fun: -2134.0708646773323
updated min: 
obj fun: -2134.072986785084
updated min: 
obj fun: -2134.0752473879174
updated min: 
obj fun: -2134.077142510812
updated min: 
obj fun: -2134.078924719369
updated min: 
obj fun: -2134.080212880604
updated min: 
obj fun: -2134.0810910703026
updated min: 
obj fun: -2134.081718531566
updated min: 
obj fun: -2134.0823199883475
updated min: 
obj fun: -2134.082777167939
updated min: 
obj fun: -2134.0829568650533
updated min: 
obj fun: -2134.08315326936
updated min: 
obj fun: -2134.0835027636626
updated min: 
obj fun: -2134.084248525787
updated min: 
obj fun: -2134.0854338834797
updated min: 
obj fun: -2134.0871161264004
updated min: 
obj fun: -2134.0888345682156
updated min: 
obj fun: -2134.0911678885623
updated min: 
obj fun: -2134.0936722471492
updated min: 
obj fun: -2134.0964260353085
updated min: 
obj fun: -2134.099104470808
updated min: 
obj fun: -2134.1015942550544
updated min: 
obj fun: -2134.103895249191
updated min: 
obj fun: -2134.1074548390716
updated min: 
obj fun: -2134.1101435512387
updated min: 
obj fun: -2134.1130964068016
updated min: 
obj fun: -2134.1154720950162
updated min: 
obj fun: -2134.1174247578883
updated min: 
obj fun: -2134.1191553456933
updated min: 
obj fun: -2134.120358774149
updated min: 
obj fun: -2134.1213330639116
updated min: 
obj fun: -2134.122198052791
updated min: 
obj fun: -2134.123319650164
updated min: 
obj fun: -2134.1242217675413
updated min: 
obj fun: -2134.125493915055
updated min: 
obj fun: -2134.1262688110746
updated min: 
obj fun: -2134.1278289862526
updated min: 
obj fun: -2134.1298022655874
updated min: 
obj fun: -2134.1320083157143
updated min: 
obj fun: -2134.134350335733
updated min: 
obj fun: -2134.1365555973885
updated min: 
obj fun: -2134.138724431998
updated min: 
obj fun: -2134.1408576719336
updated min: 
obj fun: -2134.143414144268
updated min: 
obj fun: -2134.145650448622
updated min: 
obj fun: -2134.147986016343
updated min: 
obj fun: -2134.150367298619
updated min: 
obj fun: -2134.152950935265
updated min: 
obj fun: -2134.155163528263
updated min: 
obj fun: -2134.1575641420773
updated min: 
obj fun: -2134.1594528760074
updated min: 
obj fun: -2134.1609160305056
updated min: 
obj fun: -2134.1613407073078
updated min: 
obj fun: -2134.1616500852824
updated min: 
obj fun: -2134.162140436703
updated min: 
obj fun: -2134.1621836739628
updated min: 
obj fun: -2134.1622836909087
updated min: 
obj fun: -2134.163350339738
updated min: 
obj fun: -2134.164565185259
updated min: 
obj fun: -2134.1660639567785
updated min: 
obj fun: -2134.1672328375453
updated min: 
obj fun: -2134.168584274642
updated min: 
obj fun: -2134.170156230419
updated min: 
obj fun: -2134.171579975211
updated min: 
obj fun: -2134.1727028202104
updated min: 
obj fun: -2134.1733421561307
updated min: 
obj fun: -2134.173373128119
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2129.561408135807
updated min: 
obj fun: -2134.1733864615603
updated min: 
obj fun: -2134.1735274588045
updated min: 
obj fun: -2134.174214683188
updated min: 
obj fun: -2134.176096337958
updated min: 
obj fun: -2134.177996127518
updated min: 
obj fun: -2134.179878810749
updated min: 
obj fun: -2134.181486274666
updated min: 
obj fun: -2134.1831009057264
updated min: 
obj fun: -2134.184670942734
updated min: 
obj fun: -2134.1861806726047
updated min: 
obj fun: -2134.1877500872806
updated min: 
obj fun: -2134.1892301469584
updated min: 
obj fun: -2134.1907376369927
updated min: 
obj fun: -2134.191679219875
updated min: 
obj fun: -2134.1931537463483
updated min: 
obj fun: -2134.1947415531395
updated min: 
obj fun: -2134.196058025748
updated min: 
obj fun: -2134.197231924747
updated min: 
obj fun: -2134.1978746201357
updated min: 
obj fun: -2134.198542662407
updated min: 
obj fun: -2134.1994061998594
updated min: 
obj fun: -2134.199832812948
updated min: 
obj fun: -2134.2001730918514
updated min: 
obj fun: -2134.2006802579745
updated min: 
obj fun: -2134.201068254719
updated min: 
obj fun: -2134.201353033529
updated min: 
obj fun: -2134.2014966064444
updated min: 
obj fun: -2134.2019389695984
updated min: 
obj fun: -2134.2026146034577
updated min: 
obj fun: -2134.203502889501
updated min: 
obj fun: -2134.2043093264947
updated min: 
obj fun: -2134.205292770272
updated min: 
obj fun: -2134.2061250454
updated min: 
obj fun: -2134.206973569632
updated min: 
obj fun: -2134.2083097298664
updated min: 
obj fun: -2134.2095416336965
updated min: 
obj fun: -2134.211263601765
updated min: 
obj fun: -2134.212585153938
updated min: 
obj fun: -2134.2139745402415
updated min: 
obj fun: -2134.214949474396
updated min: 
obj fun: -2134.215688660676
updated min: 
obj fun: -2134.216172485946
updated min: 
obj fun: -2134.2164887186386
updated min: 
obj fun: -2134.2168202198873
updated min: 
obj fun: -2134.217159203587
updated min: 
obj fun: -2134.2190816359066
updated min: 
obj fun: -2134.2204511778123
updated min: 
obj fun: -2134.2220212358106
updated min: 
obj fun: -2134.223604663141
updated min: 
obj fun: -2134.224725097864
updated min: 
obj fun: -2134.2254153759036
updated min: 
obj fun: -2134.2256412265187
updated min: 
obj fun: -2134.2256978454357
updated min: 
obj fun: -2134.22630784632
updated min: 
obj fun: -2134.2270789751783
updated min: 
obj fun: -2134.228128039449
updated min: 
obj fun: -2134.229106282081
updated min: 
obj fun: -2134.230155307815
updated min: 
obj fun: -2134.231013748939
updated min: 
obj fun: -2134.231171961266
updated min: 
obj fun: -2134.232150888934
updated min: 
obj fun: -2134.2328440935694
updated min: 
obj fun: -2134.2333866648355
updated min: 
obj fun: -2134.23385863878
updated min: 
obj fun: -2134.2342506485093
updated min: 
obj fun: -2134.234729284194
updated min: 
obj fun: -2134.235089476472
updated min: 
obj fun: -2134.2359132264814
updated min: 
obj fun: -2134.236669122883
updated min: 
obj fun: -2134.2374501634845
updated min: 
obj fun: -2134.238216929685
updated min: 
obj fun: -2134.2387140324076
updated min: 
obj fun: -2134.2388060543617
updated min: 
obj fun: -2134.238922443459
updated min: 
obj fun: -2134.239967175768
updated min: 
obj fun: -2134.2410613220595
updated min: 
obj fun: -2134.2416889389756
updated min: 
obj fun: -2134.2420588746136
updated min: 
obj fun: -2134.242260330175
updated min: 
obj fun: -2134.2425270005338
updated min: 
obj fun: -2134.243270200591
updated min: 
obj fun: -2134.24406552216
updated min: 
obj fun: -2134.2451072290064
updated min: 
obj fun: -2134.245787179283
updated min: 
obj fun: -2134.2468159906166
updated min: 
obj fun: -2134.2473888785435
updated min: 
obj fun: -2134.248230838886
updated min: 
obj fun: -2134.248593610415
updated min: 
obj fun: -2134.2486435044066
updated min: 
obj fun: -2134.2487784780064
updated min: 
obj fun: -2134.2492092844536
updated min: 
obj fun: -2134.2498934631417
updated min: 
obj fun: -2134.2507525042056
updated min: 
obj fun: -2134.251638996471
updated min: 
obj fun: -2134.252408350142
updated min: 
obj fun: -2134.253040565917
updated min: 
obj fun: -2134.253499224047
updated min: 
obj fun: -2134.254222807238
updated min: 
obj fun: -2134.2547218880463
updated min: 
obj fun: -2134.254909351513
updated min: 
obj fun: -2134.255158977583
updated min: 
obj fun: -2134.2552140332896
updated min: 
obj fun: -2134.2558182968346
updated min: 
obj fun: -2134.256152579962
updated min: 
obj fun: -2134.256388830035
updated min: 
obj fun: -2134.2569028743123
updated min: 
obj fun: -2134.2572541194622
updated min: 
obj fun: -2134.2578785609953
updated min: 
obj fun: -2134.25863006193
updated min: 
obj fun: -2134.25951330307
updated min: 
obj fun: -2134.260470258389
updated min: 
obj fun: -2134.2613641423527
updated min: 
obj fun: -2134.2621751466713
updated min: 
obj fun: -2134.263013479619
updated min: 
obj fun: -2134.2634774538074
updated min: 
obj fun: -2134.263810378976
updated min: 
obj fun: -2134.2642720902313
updated min: 
obj fun: -2134.2649413421673
updated min: 
obj fun: -2134.2657811070408
updated min: 
obj fun: -2134.26652035713
updated min: 
obj fun: -2134.267182038353
updated min: 
obj fun: -2134.2674613726554
updated min: 
obj fun: -2134.2681162953913
updated min: 
obj fun: -2134.268689733879
updated min: 
obj fun: -2134.2693555941946
updated min: 
obj fun: -2134.2704647700234
updated min: 
obj fun: -2134.2712787445757
updated min: 
obj fun: -2134.2720975071556
updated min: 
obj fun: -2134.272963879564
updated min: 
obj fun: -2134.2733392072046
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2133.30071800025
updated min: 
obj fun: -2134.273728159243
updated min: 
obj fun: -2134.2738044937664
updated min: 
obj fun: -2134.2738360214244
updated min: 
obj fun: -2134.2738639315994
updated min: 
obj fun: -2134.2741584865053
updated min: 
obj fun: -2134.2744796604047
updated min: 
obj fun: -2134.274494806782
updated min: 
obj fun: -2134.274776296572
updated min: 
obj fun: -2134.2753832332637
updated min: 
obj fun: -2134.276588067799
updated min: 
obj fun: -2134.278048141749
updated min: 
obj fun: -2134.278605409443
updated min: 
obj fun: -2134.2803126152226
updated min: 
obj fun: -2134.2816227948547
updated min: 
obj fun: -2134.2829784189107
updated min: 
obj fun: -2134.2843334608724
updated min: 
obj fun: -2134.285455248685
updated min: 
obj fun: -2134.2865486406854
updated min: 
obj fun: -2134.2876517611826
updated min: 
obj fun: -2134.2886269714777
updated min: 
obj fun: -2134.289616456841
updated min: 
obj fun: -2134.290258325039
updated min: 
obj fun: -2134.2909911722654
updated min: 
obj fun: -2134.291528394701
updated min: 
obj fun: -2134.292193469212
updated min: 
obj fun: -2134.292632740815
updated min: 
obj fun: -2134.2928694838724
updated min: 
obj fun: -2134.293052554102
updated min: 
obj fun: -2134.2931615221173
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2120.493715353867
NN weights: [ 7.84954488e-01 -7.46844292e-01  4.23198414e+00  2.77168393e+00
  1.17953682e+01 -7.27469301e+00 -5.07466316e+00  4.39314556e+00
 -2.52823315e+01 -3.10578613e+01 -1.06400943e+00  4.84821737e-01
 -4.01899147e+00 -2.21389794e+00 -1.19480495e+01  7.65758991e+00
  5.11677980e+00 -4.26681232e+00  2.55464478e+01  3.08472347e+01
 -2.22757435e+01 -3.51506119e+01 -3.79661102e+01 -3.26750755e+01
 -3.45280151e+01 -5.71456909e+01 -4.56765251e+01 -2.20180473e+01
 -4.54817963e+01 -6.87243557e+00 -2.73388267e+00  1.55395264e+02
  6.92078934e+01  3.71697083e+01  1.09327972e+02  1.68485336e+02
  2.52289078e+02  2.04066376e+02  2.76818085e+02 -1.26641235e+01
 -2.27910328e+01 -3.35358772e+01 -7.83118591e+01 -3.39742050e+01
 -3.01613121e+01 -8.30528793e+01 -3.84930000e+01 -1.56803370e+01
 -3.84228821e+01 -1.47822723e+01 -2.28800907e+01 -3.36886978e+01
 -4.59520493e+01 -3.37390404e+01 -3.18450603e+01 -5.97590294e+01
 -4.10810471e+01 -1.82296715e+01 -4.14720497e+01 -1.30265741e+01
 -2.52284145e+00 -1.87076366e+00 -2.87721109e+00  1.91340792e+00
 -7.74481506e+01 -4.31962204e+00 -3.17038131e+00  9.53775406e+00
  1.04209671e+01  8.84981513e-01 -2.60453463e+00 -5.84534988e+01
 -1.10143036e+02  2.18297997e+01 -2.55094013e+01 -1.75201904e+02
 -9.77051086e+01 -7.92271042e+00 -7.01431885e+01  5.27199125e+00
 -2.91793561e+00  3.77224994e+00  1.12542003e-01  2.46649666e+01
  7.48322248e+00  6.73871398e-01  2.56542063e+00  1.29064703e+01
  1.22010317e+01 -2.48101692e+01 -2.26717834e+01 -3.36601105e+01
 -8.60235596e+01 -3.46877365e+01 -2.93305950e+01 -8.95122757e+01
 -3.77441788e+01 -1.51180067e+01 -3.72564468e+01 -1.45932102e+01
 -7.32162571e+00 -6.70499268e+01  3.89520121e+00 -2.92995358e+01
 -3.35587502e+01 -1.50772181e+01 -1.06219040e+02 -5.78642006e+01
 -1.05803352e+02 -4.58941498e+01 -3.51724124e+00 -6.76383820e+01
  4.51198387e+00 -2.73435802e+01 -3.11275139e+01 -1.07423954e+01
 -1.10436272e+02 -7.56123581e+01 -1.23425926e+02 -4.35170860e+01
  7.31608915e+00 -5.96585226e+00 -5.88518190e+00  3.13150930e+00
 -2.06441474e+00  2.81951809e+00 -8.54720001e+01  5.17626643e-01
 -5.82241554e+01  1.26698132e+01  7.54328549e-01  2.88604045e+00
 -4.60263538e+00  2.86798716e+00 -4.10340271e+01  5.53740597e+00
 -4.10638008e+01  5.34490442e+00 -7.16341352e+00 -1.36972654e+00]
Minimum obj value:-2134.293347305438
Optimal xi: 27.849012
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1451.1340792983713
W_T_median: 1238.491747181095
W_T_pctile_5: 777.5721549909163
W_T_CVAR_5_pct: 683.2103652992188
F value: -2134.293347305438
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
F value: -2134.293347305438
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[  7.3161,  -5.9659],
        [ -5.8852,   3.1315],
        [ -2.0644,   2.8195],
        [-85.4720,   0.5176],
        [-58.2242,  12.6698],
        [  0.7543,   2.8860],
        [ -4.6026,   2.8680],
        [-41.0340,   5.5374],
        [-41.0638,   5.3449],
        [ -7.1634,  -1.3697]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[-2.2276e+01, -3.5151e+01, -3.7966e+01, -3.2675e+01, -3.4528e+01,
         -5.7146e+01, -4.5677e+01, -2.2018e+01, -4.5482e+01, -6.8724e+00],
        [-2.7339e+00,  1.5540e+02,  6.9208e+01,  3.7170e+01,  1.0933e+02,
          1.6849e+02,  2.5229e+02,  2.0407e+02,  2.7682e+02, -1.2664e+01],
        [-2.2791e+01, -3.3536e+01, -7.8312e+01, -3.3974e+01, -3.0161e+01,
         -8.3053e+01, -3.8493e+01, -1.5680e+01, -3.8423e+01, -1.4782e+01],
        [-2.2880e+01, -3.3689e+01, -4.5952e+01, -3.3739e+01, -3.1845e+01,
         -5.9759e+01, -4.1081e+01, -1.8230e+01, -4.1472e+01, -1.3027e+01],
        [-2.5228e+00, -1.8708e+00, -2.8772e+00,  1.9134e+00, -7.7448e+01,
         -4.3196e+00, -3.1704e+00,  9.5378e+00,  1.0421e+01,  8.8498e-01],
        [-2.6045e+00, -5.8453e+01, -1.1014e+02,  2.1830e+01, -2.5509e+01,
         -1.7520e+02, -9.7705e+01, -7.9227e+00, -7.0143e+01,  5.2720e+00],
        [-2.9179e+00,  3.7722e+00,  1.1254e-01,  2.4665e+01,  7.4832e+00,
          6.7387e-01,  2.5654e+00,  1.2906e+01,  1.2201e+01, -2.4810e+01],
        [-2.2672e+01, -3.3660e+01, -8.6024e+01, -3.4688e+01, -2.9331e+01,
         -8.9512e+01, -3.7744e+01, -1.5118e+01, -3.7256e+01, -1.4593e+01],
        [-7.3216e+00, -6.7050e+01,  3.8952e+00, -2.9300e+01, -3.3559e+01,
         -1.5077e+01, -1.0622e+02, -5.7864e+01, -1.0580e+02, -4.5894e+01],
        [-3.5172e+00, -6.7638e+01,  4.5120e+00, -2.7344e+01, -3.1128e+01,
         -1.0742e+01, -1.1044e+02, -7.5612e+01, -1.2343e+02, -4.3517e+01]],
       device='cuda:0')), ('model.output_layer_3.weight', tensor([[  0.7850,  -0.7468,   4.2320,   2.7717,  11.7954,  -7.2747,  -5.0747,
           4.3931, -25.2823, -31.0579],
        [ -1.0640,   0.4848,  -4.0190,  -2.2139, -11.9480,   7.6576,   5.1168,
          -4.2668,  25.5464,  30.8472]], device='cuda:0'))])
loaded xi:  27.849012
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -2277.3664075759325
objective value function right now is: -2277.3664075759325
4.0% of gradient descent iterations done. Method = Adam
updated min: -2278.231167718208
objective value function right now is: -2278.231167718208
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2276.165827858801
8.0% of gradient descent iterations done. Method = Adam
updated min: -2278.905796281726
objective value function right now is: -2278.905796281726
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2277.986242915111
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2277.5361846626083
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -2276.9478091010114
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2277.069633404129
18.0% of gradient descent iterations done. Method = Adam
updated min: -2279.0207645210717
objective value function right now is: -2279.0207645210717
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2274.045652290711
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2275.3625298153115
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2268.961007552785
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2277.9380792990783
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -2275.671249714087
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2277.6942800428274
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2274.798603500578
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2275.588121911395
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2273.3108770919753
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2278.4227498997266
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2276.9257021503217
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2278.068133327864
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2277.5680249157035
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2276.003579176758
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2273.891825104732
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2278.4864177700165
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2278.326906690908
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2273.997128173669
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -2270.6373042798223
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -2277.692963969113
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2276.579744203734
62.0% of gradient descent iterations done. Method = Adam
updated min: -2279.6449718996446
objective value function right now is: -2279.6449718996446
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2277.3020595773755
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2276.602922209266
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2278.187643604498
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2269.2423216249786
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2279.0964600259513
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2275.9266378746656
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2278.1089048324015
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2273.049998307852
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2276.699048726186
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2274.8479336883825
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2273.4384083182717
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2275.794291700148
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2276.367425614691
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2267.991026562077
updated min: 
obj fun: -2279.6525364107843
updated min: 
obj fun: -2279.6636655861844
updated min: 
obj fun: -2279.6749425660582
updated min: 
obj fun: -2279.6845083439534
updated min: 
obj fun: -2279.6950333728373
updated min: 
obj fun: -2279.70490915283
updated min: 
obj fun: -2279.714191775516
updated min: 
obj fun: -2279.7226816761417
updated min: 
obj fun: -2279.729224333156
updated min: 
obj fun: -2279.736487486054
updated min: 
obj fun: -2279.7434676259886
updated min: 
obj fun: -2279.749855189032
updated min: 
obj fun: -2279.755097119907
updated min: 
obj fun: -2279.761882186901
updated min: 
obj fun: -2279.7693323308467
updated min: 
obj fun: -2279.7758103551478
updated min: 
obj fun: -2279.7838167833715
updated min: 
obj fun: -2279.790433191475
updated min: 
obj fun: -2279.798374878104
updated min: 
obj fun: -2279.805934443245
updated min: 
obj fun: -2279.812063187048
updated min: 
obj fun: -2279.8174932539523
updated min: 
obj fun: -2279.8236287783598
updated min: 
obj fun: -2279.8278510629593
updated min: 
obj fun: -2279.8327547598583
updated min: 
obj fun: -2279.8363134560736
updated min: 
obj fun: -2279.8398623498438
updated min: 
obj fun: -2279.8444523543994
updated min: 
obj fun: -2279.8477924018844
updated min: 
obj fun: -2279.851254069325
updated min: 
obj fun: -2279.8551848279044
updated min: 
obj fun: -2279.8587776411755
updated min: 
obj fun: -2279.8629537233114
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2277.8139291013176
updated min: 
obj fun: -2279.867699366811
updated min: 
obj fun: -2279.8724784979086
updated min: 
obj fun: -2279.8774746559625
updated min: 
obj fun: -2279.8821886637334
updated min: 
obj fun: -2279.8864987183074
updated min: 
obj fun: -2279.8907393842605
updated min: 
obj fun: -2279.8944866740826
updated min: 
obj fun: -2279.8983605275575
updated min: 
obj fun: -2279.9019111313482
updated min: 
obj fun: -2279.905697669161
updated min: 
obj fun: -2279.908350028507
updated min: 
obj fun: -2279.912485959161
updated min: 
obj fun: -2279.916895225592
updated min: 
obj fun: -2279.921929757284
updated min: 
obj fun: -2279.9266568442263
updated min: 
obj fun: -2279.932320051743
updated min: 
obj fun: -2279.935470943692
updated min: 
obj fun: -2279.938215423314
updated min: 
obj fun: -2279.9386247151624
updated min: 
obj fun: -2279.9400018925235
updated min: 
obj fun: -2279.940321154576
updated min: 
obj fun: -2279.942253343481
updated min: 
obj fun: -2279.9445231336476
updated min: 
obj fun: -2279.9475448141666
updated min: 
obj fun: -2279.950344124258
updated min: 
obj fun: -2279.952969411141
updated min: 
obj fun: -2279.9557557569424
updated min: 
obj fun: -2279.9594333650703
updated min: 
obj fun: -2279.9657703738358
updated min: 
obj fun: -2279.972165940686
updated min: 
obj fun: -2279.9790331021454
updated min: 
obj fun: -2279.9861343661623
updated min: 
obj fun: -2279.992291068252
updated min: 
obj fun: -2279.9970984577194
updated min: 
obj fun: -2280.0023277744895
updated min: 
obj fun: -2280.0046214076497
updated min: 
obj fun: -2280.0057884823204
updated min: 
obj fun: -2280.0059418329392
updated min: 
obj fun: -2280.0059941566456
updated min: 
obj fun: -2280.0066599225374
updated min: 
obj fun: -2280.00955471475
updated min: 
obj fun: -2280.0137544398995
updated min: 
obj fun: -2280.0181025865886
updated min: 
obj fun: -2280.0255591142854
updated min: 
obj fun: -2280.033582531609
updated min: 
obj fun: -2280.041442494112
updated min: 
obj fun: -2280.050475674609
updated min: 
obj fun: -2280.057850703045
updated min: 
obj fun: -2280.065862658163
updated min: 
obj fun: -2280.070404841235
updated min: 
obj fun: -2280.0729679333886
updated min: 
obj fun: -2280.0754708333925
updated min: 
obj fun: -2280.0756460021476
updated min: 
obj fun: -2280.0759560644487
updated min: 
obj fun: -2280.0780818567414
updated min: 
obj fun: -2280.080286914573
updated min: 
obj fun: -2280.0821870345612
updated min: 
obj fun: -2280.083592734301
updated min: 
obj fun: -2280.0845654859495
updated min: 
obj fun: -2280.085617018269
updated min: 
obj fun: -2280.086763372887
updated min: 
obj fun: -2280.0877960951025
updated min: 
obj fun: -2280.0889880928435
updated min: 
obj fun: -2280.0896930240915
updated min: 
obj fun: -2280.0900911860163
updated min: 
obj fun: -2280.0906666775154
updated min: 
obj fun: -2280.0927020849576
updated min: 
obj fun: -2280.09384021074
updated min: 
obj fun: -2280.094915308009
updated min: 
obj fun: -2280.0972922371193
updated min: 
obj fun: -2280.0981457662274
updated min: 
obj fun: -2280.0983696835297
updated min: 
obj fun: -2280.0991006903178
updated min: 
obj fun: -2280.0993971181188
updated min: 
obj fun: -2280.0994751826447
updated min: 
obj fun: -2280.100650945585
updated min: 
obj fun: -2280.1014557790577
updated min: 
obj fun: -2280.102418545766
updated min: 
obj fun: -2280.1028969465983
updated min: 
obj fun: -2280.1040177184045
updated min: 
obj fun: -2280.1042198081886
updated min: 
obj fun: -2280.10586294
updated min: 
obj fun: -2280.1081534923433
updated min: 
obj fun: -2280.109285960652
updated min: 
obj fun: -2280.1115070419323
updated min: 
obj fun: -2280.1124339014045
updated min: 
obj fun: -2280.1139767399527
updated min: 
obj fun: -2280.1149008666657
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2275.9630222278306
updated min: 
obj fun: -2280.1155363421226
updated min: 
obj fun: -2280.1165496448357
updated min: 
obj fun: -2280.117476074174
updated min: 
obj fun: -2280.1182429452883
updated min: 
obj fun: -2280.1185887341117
updated min: 
obj fun: -2280.1190832803027
updated min: 
obj fun: -2280.1196891182326
updated min: 
obj fun: -2280.1198008511283
updated min: 
obj fun: -2280.1213936588365
updated min: 
obj fun: -2280.1230038155113
updated min: 
obj fun: -2280.124622112179
updated min: 
obj fun: -2280.126471144273
updated min: 
obj fun: -2280.12795030256
updated min: 
obj fun: -2280.1292732705356
updated min: 
obj fun: -2280.1301471640822
updated min: 
obj fun: -2280.1304114838326
updated min: 
obj fun: -2280.1308222333155
updated min: 
obj fun: -2280.1320019246173
updated min: 
obj fun: -2280.1344877383567
updated min: 
obj fun: -2280.136282479424
updated min: 
obj fun: -2280.1387920021166
updated min: 
obj fun: -2280.142502539969
updated min: 
obj fun: -2280.1462818484783
updated min: 
obj fun: -2280.1496319388402
updated min: 
obj fun: -2280.1520612837576
updated min: 
obj fun: -2280.1536928668456
updated min: 
obj fun: -2280.15409274392
updated min: 
obj fun: -2280.154317678308
updated min: 
obj fun: -2280.1570350325405
updated min: 
obj fun: -2280.1593292064676
updated min: 
obj fun: -2280.1608107067404
updated min: 
obj fun: -2280.1616642626022
updated min: 
obj fun: -2280.16170289439
updated min: 
obj fun: -2280.1617957812023
updated min: 
obj fun: -2280.1622440026536
updated min: 
obj fun: -2280.162606501787
updated min: 
obj fun: -2280.162708992807
updated min: 
obj fun: -2280.162874904317
updated min: 
obj fun: -2280.1641981742164
updated min: 
obj fun: -2280.1653054822177
updated min: 
obj fun: -2280.168000406174
updated min: 
obj fun: -2280.169311639313
updated min: 
obj fun: -2280.1704666736705
updated min: 
obj fun: -2280.1714726701316
updated min: 
obj fun: -2280.1736585221283
updated min: 
obj fun: -2280.1744266291407
updated min: 
obj fun: -2280.175048238066
updated min: 
obj fun: -2280.1756322331357
updated min: 
obj fun: -2280.17656523403
updated min: 
obj fun: -2280.1790353706274
updated min: 
obj fun: -2280.1809707708835
updated min: 
obj fun: -2280.183258377419
updated min: 
obj fun: -2280.185697794122
updated min: 
obj fun: -2280.18899033455
updated min: 
obj fun: -2280.192020234309
updated min: 
obj fun: -2280.1952715678326
updated min: 
obj fun: -2280.1986317151386
updated min: 
obj fun: -2280.200669453079
updated min: 
obj fun: -2280.204071035861
updated min: 
obj fun: -2280.2072136057595
updated min: 
obj fun: -2280.2100390485903
updated min: 
obj fun: -2280.211177063502
updated min: 
obj fun: -2280.213563972622
updated min: 
obj fun: -2280.2154994589855
updated min: 
obj fun: -2280.2168706734337
updated min: 
obj fun: -2280.217886784097
updated min: 
obj fun: -2280.218409887909
updated min: 
obj fun: -2280.218879624039
updated min: 
obj fun: -2280.219743578843
updated min: 
obj fun: -2280.2208016047525
updated min: 
obj fun: -2280.2222063774366
updated min: 
obj fun: -2280.2227844927756
updated min: 
obj fun: -2280.224234841588
updated min: 
obj fun: -2280.2256300861563
updated min: 
obj fun: -2280.2266836518843
updated min: 
obj fun: -2280.2275265798685
updated min: 
obj fun: -2280.227972052951
updated min: 
obj fun: -2280.2282110939464
updated min: 
obj fun: -2280.228572112856
updated min: 
obj fun: -2280.2293680545167
updated min: 
obj fun: -2280.2301989553293
updated min: 
obj fun: -2280.230771221704
updated min: 
obj fun: -2280.231591173526
updated min: 
obj fun: -2280.2319561757863
updated min: 
obj fun: -2280.2324016341054
updated min: 
obj fun: -2280.2328585806285
updated min: 
obj fun: -2280.2333805742387
updated min: 
obj fun: -2280.2340450233246
updated min: 
obj fun: -2280.234642228402
updated min: 
obj fun: -2280.234854651575
updated min: 
obj fun: -2280.2349003078275
updated min: 
obj fun: -2280.2352517857144
updated min: 
obj fun: -2280.235315263639
updated min: 
obj fun: -2280.235756807838
updated min: 
obj fun: -2280.235873948502
updated min: 
obj fun: -2280.2361249154137
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2276.129337800225
updated min: 
obj fun: -2280.2374485345645
updated min: 
obj fun: -2280.2388895593294
updated min: 
obj fun: -2280.240480330241
updated min: 
obj fun: -2280.24190642983
updated min: 
obj fun: -2280.2429321100526
updated min: 
obj fun: -2280.2432868290844
updated min: 
obj fun: -2280.243544007343
updated min: 
obj fun: -2280.243645114561
updated min: 
obj fun: -2280.2448158372763
updated min: 
obj fun: -2280.2458004628793
updated min: 
obj fun: -2280.2471143329217
updated min: 
obj fun: -2280.248579210147
updated min: 
obj fun: -2280.2499791580403
updated min: 
obj fun: -2280.251190620541
updated min: 
obj fun: -2280.253471208433
updated min: 
obj fun: -2280.2542166321277
updated min: 
obj fun: -2280.2547390883483
updated min: 
obj fun: -2280.2553935797814
updated min: 
obj fun: -2280.2560028930643
updated min: 
obj fun: -2280.256929702617
updated min: 
obj fun: -2280.2579045225216
updated min: 
obj fun: -2280.258880026467
updated min: 
obj fun: -2280.2597653465386
updated min: 
obj fun: -2280.261298028026
updated min: 
obj fun: -2280.2616464540215
updated min: 
obj fun: -2280.262276342341
updated min: 
obj fun: -2280.2629315912427
updated min: 
obj fun: -2280.263775976061
updated min: 
obj fun: -2280.2647377193007
updated min: 
obj fun: -2280.2659367704623
updated min: 
obj fun: -2280.266772093541
updated min: 
obj fun: -2280.2685748443455
updated min: 
obj fun: -2280.2692265974574
updated min: 
obj fun: -2280.2693320111566
updated min: 
obj fun: -2280.2700071323743
updated min: 
obj fun: -2280.27076999674
updated min: 
obj fun: -2280.2729479980057
updated min: 
obj fun: -2280.2747609923717
updated min: 
obj fun: -2280.2769275759742
updated min: 
obj fun: -2280.278916182671
updated min: 
obj fun: -2280.2809356374487
updated min: 
obj fun: -2280.282601597227
updated min: 
obj fun: -2280.2835180941343
updated min: 
obj fun: -2280.2853234135982
updated min: 
obj fun: -2280.285779641944
updated min: 
obj fun: -2280.2859181681947
updated min: 
obj fun: -2280.286584188569
updated min: 
obj fun: -2280.2874527222216
updated min: 
obj fun: -2280.288994460348
updated min: 
obj fun: -2280.291015895104
updated min: 
obj fun: -2280.2937337549324
updated min: 
obj fun: -2280.296128013286
updated min: 
obj fun: -2280.2987387498097
updated min: 
obj fun: -2280.300860211679
updated min: 
obj fun: -2280.3027666527278
updated min: 
obj fun: -2280.304110931233
updated min: 
obj fun: -2280.305236105708
updated min: 
obj fun: -2280.3061004693905
updated min: 
obj fun: -2280.3064203126532
updated min: 
obj fun: -2280.306696759655
updated min: 
obj fun: -2280.3068983206217
updated min: 
obj fun: -2280.3071784076924
updated min: 
obj fun: -2280.3074306503217
updated min: 
obj fun: -2280.307705303934
updated min: 
obj fun: -2280.30841470313
updated min: 
obj fun: -2280.3087805194305
updated min: 
obj fun: -2280.309465157108
updated min: 
obj fun: -2280.3103532244995
updated min: 
obj fun: -2280.3114187340225
updated min: 
obj fun: -2280.3118789457462
updated min: 
obj fun: -2280.3125197427507
updated min: 
obj fun: -2280.312930916474
updated min: 
obj fun: -2280.3133470747302
updated min: 
obj fun: -2280.313670852653
updated min: 
obj fun: -2280.3141362487245
updated min: 
obj fun: -2280.3147466654414
updated min: 
obj fun: -2280.3155797653685
updated min: 
obj fun: -2280.3163493696934
updated min: 
obj fun: -2280.3167830595935
updated min: 
obj fun: -2280.3169836445245
updated min: 
obj fun: -2280.3172077917675
updated min: 
obj fun: -2280.3185293748434
updated min: 
obj fun: -2280.3187166208786
updated min: 
obj fun: -2280.3191786501807
updated min: 
obj fun: -2280.3195470662827
updated min: 
obj fun: -2280.320049949537
updated min: 
obj fun: -2280.3205310802855
updated min: 
obj fun: -2280.3214253399337
updated min: 
obj fun: -2280.321902078121
updated min: 
obj fun: -2280.3229657384463
updated min: 
obj fun: -2280.3235917616335
updated min: 
obj fun: -2280.3244762609716
updated min: 
obj fun: -2280.325130698571
updated min: 
obj fun: -2280.325979951879
updated min: 
obj fun: -2280.3269351545337
updated min: 
obj fun: -2280.3276778046957
updated min: 
obj fun: -2280.328331343266
updated min: 
obj fun: -2280.328761687587
updated min: 
obj fun: -2280.3293573520687
updated min: 
obj fun: -2280.330231564178
updated min: 
obj fun: -2280.3307417558713
updated min: 
obj fun: -2280.33143474501
updated min: 
obj fun: -2280.3322509375616
updated min: 
obj fun: -2280.3328381404954
updated min: 
obj fun: -2280.333011754983
updated min: 
obj fun: -2280.333127146192
updated min: 
obj fun: -2280.333258991465
updated min: 
obj fun: -2280.3343719232385
updated min: 
obj fun: -2280.334472915472
updated min: 
obj fun: -2280.3348879876976
updated min: 
obj fun: -2280.3353509304934
updated min: 
obj fun: -2280.33630335463
updated min: 
obj fun: -2280.337157695339
updated min: 
obj fun: -2280.3384270938004
updated min: 
obj fun: -2280.3394339918477
updated min: 
obj fun: -2280.342022848588
updated min: 
obj fun: -2280.3429559505694
updated min: 
obj fun: -2280.3433363845656
updated min: 
obj fun: -2280.343637052386
updated min: 
obj fun: -2280.343688367101
updated min: 
obj fun: -2280.3437631915435
updated min: 
obj fun: -2280.344463621362
updated min: 
obj fun: -2280.3445138429074
updated min: 
obj fun: -2280.3459069489654
updated min: 
obj fun: -2280.347194968162
updated min: 
obj fun: -2280.3483709792654
updated min: 
obj fun: -2280.3493511959455
updated min: 
obj fun: -2280.3500341951512
updated min: 
obj fun: -2280.350404974072
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2278.2171344159806
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2276.4820324107736
NN weights: [ 5.66603661e-01 -7.84073234e-01  4.09181309e+00  2.56201124e+00
  9.60659122e+00 -7.25510979e+00 -4.84627771e+00  4.26079941e+00
 -2.68072586e+01 -3.18694057e+01 -8.45662355e-01  5.22554576e-01
 -3.87883329e+00 -2.00422478e+00 -9.75883961e+00  7.64127159e+00
  4.89821053e+00 -4.13446903e+00  2.70713806e+01  3.16587811e+01
 -2.28153152e+01 -3.51506081e+01 -3.79663010e+01 -3.26750755e+01
 -3.45280151e+01 -5.71627769e+01 -4.56760292e+01 -2.20180473e+01
 -4.54817963e+01 -6.80011272e+00 -2.22432089e+00  1.85297729e+02
  8.57177505e+01  2.66330338e+01  1.13676003e+02  1.69929871e+02
  2.71275940e+02  2.57968964e+02  3.31970062e+02 -1.16896524e+01
 -2.38816051e+01 -3.35358772e+01 -7.83164139e+01 -3.39742050e+01
 -3.01613121e+01 -8.30858917e+01 -3.84930725e+01 -1.56803370e+01
 -3.84228821e+01 -1.46401138e+01 -2.38583717e+01 -3.36886978e+01
 -4.59554100e+01 -3.37390404e+01 -3.18450603e+01 -5.97831306e+01
 -4.10811195e+01 -1.82296715e+01 -4.14720497e+01 -1.29305391e+01
 -2.31816196e+00 -3.11775684e+00 -3.67568874e+00  3.11121106e+00
 -1.03309700e+02 -5.67266035e+00 -4.30528307e+00  1.17698832e+01
  1.39607563e+01  9.40318048e-01 -2.62720013e+00 -7.00795288e+01
 -1.27204636e+02  2.11036968e+01 -3.70599480e+01 -1.95975555e+02
 -1.12247795e+02 -1.30406294e+01 -7.12254944e+01  4.97826767e+00
 -2.77126980e+00  6.59418964e+00 -1.08169329e+00  5.90875664e+01
  1.50700703e+01 -1.64699644e-01  1.30444229e+00  3.01494865e+01
  2.93048687e+01 -2.15662518e+01 -2.37528896e+01 -3.36601105e+01
 -8.60289536e+01 -3.46877365e+01 -2.93305950e+01 -8.95496521e+01
 -3.77442818e+01 -1.51180067e+01 -3.72564468e+01 -1.44169683e+01
 -7.08564997e+00 -6.33819962e+01 -4.96219683e+00 -2.92995358e+01
 -3.35587502e+01 -1.68950195e+01 -9.60197220e+01 -5.78690300e+01
 -1.05807755e+02 -4.33979187e+01 -3.29282808e+00 -6.35212860e+01
 -2.79047298e+00 -2.73435802e+01 -3.11275139e+01 -1.24381571e+01
 -9.82367020e+01 -7.61092300e+01 -1.23896828e+02 -4.13280830e+01
 -9.65982556e-01 -2.52454739e+01 -4.67980814e+00  3.45462298e+00
 -3.48237723e-01  2.93530107e+00 -9.26368179e+01  2.20341422e-02
 -5.27938499e+01  1.35580082e+01  8.76454592e-01  2.92044306e+00
 -3.58081532e+00  2.70660067e+00 -3.11294270e+01  6.96639395e+00
 -2.99486141e+01  6.99333858e+00 -7.78341532e+00 -1.50203395e+00]
Minimum obj value:-2280.3501217614867
Optimal xi: 27.447403
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1474.8280494820967
W_T_median: 1275.8835329482863
W_T_pctile_5: 755.1215903241256
W_T_CVAR_5_pct: 658.082310921752
F value: -2280.3501217614867
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.1
F value: -2280.3501217614867
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[-9.6598e-01, -2.5245e+01],
        [-4.6798e+00,  3.4546e+00],
        [-3.4824e-01,  2.9353e+00],
        [-9.2637e+01,  2.2034e-02],
        [-5.2794e+01,  1.3558e+01],
        [ 8.7645e-01,  2.9204e+00],
        [-3.5808e+00,  2.7066e+00],
        [-3.1129e+01,  6.9664e+00],
        [-2.9949e+01,  6.9933e+00],
        [-7.7834e+00, -1.5020e+00]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[-2.2815e+01, -3.5151e+01, -3.7966e+01, -3.2675e+01, -3.4528e+01,
         -5.7163e+01, -4.5676e+01, -2.2018e+01, -4.5482e+01, -6.8001e+00],
        [-2.2243e+00,  1.8530e+02,  8.5718e+01,  2.6633e+01,  1.1368e+02,
          1.6993e+02,  2.7128e+02,  2.5797e+02,  3.3197e+02, -1.1690e+01],
        [-2.3882e+01, -3.3536e+01, -7.8316e+01, -3.3974e+01, -3.0161e+01,
         -8.3086e+01, -3.8493e+01, -1.5680e+01, -3.8423e+01, -1.4640e+01],
        [-2.3858e+01, -3.3689e+01, -4.5955e+01, -3.3739e+01, -3.1845e+01,
         -5.9783e+01, -4.1081e+01, -1.8230e+01, -4.1472e+01, -1.2931e+01],
        [-2.3182e+00, -3.1178e+00, -3.6757e+00,  3.1112e+00, -1.0331e+02,
         -5.6727e+00, -4.3053e+00,  1.1770e+01,  1.3961e+01,  9.4032e-01],
        [-2.6272e+00, -7.0080e+01, -1.2720e+02,  2.1104e+01, -3.7060e+01,
         -1.9598e+02, -1.1225e+02, -1.3041e+01, -7.1225e+01,  4.9783e+00],
        [-2.7713e+00,  6.5942e+00, -1.0817e+00,  5.9088e+01,  1.5070e+01,
         -1.6470e-01,  1.3044e+00,  3.0149e+01,  2.9305e+01, -2.1566e+01],
        [-2.3753e+01, -3.3660e+01, -8.6029e+01, -3.4688e+01, -2.9331e+01,
         -8.9550e+01, -3.7744e+01, -1.5118e+01, -3.7256e+01, -1.4417e+01],
        [-7.0856e+00, -6.3382e+01, -4.9622e+00, -2.9300e+01, -3.3559e+01,
         -1.6895e+01, -9.6020e+01, -5.7869e+01, -1.0581e+02, -4.3398e+01],
        [-3.2928e+00, -6.3521e+01, -2.7905e+00, -2.7344e+01, -3.1128e+01,
         -1.2438e+01, -9.8237e+01, -7.6109e+01, -1.2390e+02, -4.1328e+01]],
       device='cuda:0')), ('model.output_layer_3.weight', tensor([[  0.5666,  -0.7841,   4.0918,   2.5620,   9.6066,  -7.2551,  -4.8463,
           4.2608, -26.8073, -31.8694],
        [ -0.8457,   0.5226,  -3.8788,  -2.0042,  -9.7588,   7.6413,   4.8982,
          -4.1345,  27.0714,  31.6588]], device='cuda:0'))])
loaded xi:  27.447403
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -2420.1184253490624
objective value function right now is: -2420.1184253490624
4.0% of gradient descent iterations done. Method = Adam
updated min: -2428.207847424481
objective value function right now is: -2428.207847424481
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2426.392657779341
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2422.9177583635346
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2425.0785550110377
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2425.6035359031857
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -2424.7305753132387
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2426.3600381801766
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2424.691760200649
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2426.1816048293417
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2422.358974555049
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2427.9113219037567
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2425.437096084633
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -2424.1456405080967
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2425.9543003168947
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2427.7934356530263
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2424.437140126245
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2428.196545955398
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2422.2838535248584
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2425.7141113532625
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2425.0536388315745
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2426.250839156999
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2425.720946226013
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2427.3670969793952
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2424.5501912249383
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2426.8799453168235
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2427.5050952171177
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -2421.259173037831
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -2426.70411475321
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2426.0366142962876
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2419.711953480255
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2427.920667108272
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2427.1397345669648
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2426.692289225914
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2424.2245429719014
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2423.930547034803
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2424.1500265726477
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2424.581443617742
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2424.737372994479
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2422.65517287995
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2425.817784973979
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2423.3838958814918
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2425.5808421402694
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2426.1804454211347
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2420.0327735082337
updated min: 
obj fun: -2428.217546501657
updated min: 
obj fun: -2428.2284792739274
updated min: 
obj fun: -2428.2383180692664
updated min: 
obj fun: -2428.2461642661115
updated min: 
obj fun: -2428.251710585926
updated min: 
obj fun: -2428.2555221035022
updated min: 
obj fun: -2428.2587459189444
updated min: 
obj fun: -2428.2622364647073
updated min: 
obj fun: -2428.266581781314
updated min: 
obj fun: -2428.271758752411
updated min: 
obj fun: -2428.2770243374835
updated min: 
obj fun: -2428.281171376407
updated min: 
obj fun: -2428.283453539692
updated min: 
obj fun: -2428.2836498038882
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2425.944094599273
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2426.206336808134
updated min: 
obj fun: -2428.284562662768
updated min: 
obj fun: -2428.289055794498
updated min: 
obj fun: -2428.293074107093
updated min: 
obj fun: -2428.2964885463043
updated min: 
obj fun: -2428.298717933394
updated min: 
obj fun: -2428.30009522313
updated min: 
obj fun: -2428.301009803893
updated min: 
obj fun: -2428.3020824389887
updated min: 
obj fun: -2428.3038986544407
updated min: 
obj fun: -2428.306320621375
updated min: 
obj fun: -2428.3097204416927
updated min: 
obj fun: -2428.313438946265
updated min: 
obj fun: -2428.3171955913544
updated min: 
obj fun: -2428.320250750975
updated min: 
obj fun: -2428.3224049983405
updated min: 
obj fun: -2428.32332408981
updated min: 
obj fun: -2428.324469421313
updated min: 
obj fun: -2428.325973741647
updated min: 
obj fun: -2428.3274739673584
updated min: 
obj fun: -2428.3289047553058
updated min: 
obj fun: -2428.3300784218945
updated min: 
obj fun: -2428.3313192657356
updated min: 
obj fun: -2428.332307996575
updated min: 
obj fun: -2428.333259592884
updated min: 
obj fun: -2428.3342340504328
updated min: 
obj fun: -2428.335156551848
updated min: 
obj fun: -2428.3353094416802
updated min: 
obj fun: -2428.337003290052
updated min: 
obj fun: -2428.3385131435
updated min: 
obj fun: -2428.3391023365734
updated min: 
obj fun: -2428.3413996484114
updated min: 
obj fun: -2428.3429185334167
updated min: 
obj fun: -2428.3433177845613
updated min: 
obj fun: -2428.3436689057194
updated min: 
obj fun: -2428.345093255489
updated min: 
obj fun: -2428.3463426455223
updated min: 
obj fun: -2428.3475558988125
updated min: 
obj fun: -2428.348280539687
updated min: 
obj fun: -2428.3483849506288
updated min: 
obj fun: -2428.3484040432136
updated min: 
obj fun: -2428.349951268299
updated min: 
obj fun: -2428.351688590473
updated min: 
obj fun: -2428.3534613252104
updated min: 
obj fun: -2428.3551663681287
updated min: 
obj fun: -2428.357265186339
updated min: 
obj fun: -2428.359266036453
updated min: 
obj fun: -2428.361297089515
updated min: 
obj fun: -2428.362846680723
updated min: 
obj fun: -2428.3643791324066
updated min: 
obj fun: -2428.365195840572
updated min: 
obj fun: -2428.3658710730133
updated min: 
obj fun: -2428.3662929821153
updated min: 
obj fun: -2428.3671486783337
updated min: 
obj fun: -2428.3684488122576
updated min: 
obj fun: -2428.369999748956
updated min: 
obj fun: -2428.372161694711
updated min: 
obj fun: -2428.3742120425463
updated min: 
obj fun: -2428.376558388126
updated min: 
obj fun: -2428.3786090410445
updated min: 
obj fun: -2428.3797977177896
updated min: 
obj fun: -2428.381081202634
updated min: 
obj fun: -2428.3818555261487
updated min: 
obj fun: -2428.3826485194263
updated min: 
obj fun: -2428.382896392996
updated min: 
obj fun: -2428.3837066163096
updated min: 
obj fun: -2428.384730828085
updated min: 
obj fun: -2428.3855439746494
updated min: 
obj fun: -2428.3867905442216
updated min: 
obj fun: -2428.388211047396
updated min: 
obj fun: -2428.389746035181
updated min: 
obj fun: -2428.391690107896
updated min: 
obj fun: -2428.3936581006983
updated min: 
obj fun: -2428.3951939434664
updated min: 
obj fun: -2428.39593266131
updated min: 
obj fun: -2428.396516252542
updated min: 
obj fun: -2428.3968045382753
updated min: 
obj fun: -2428.3970193565665
updated min: 
obj fun: -2428.39740126881
updated min: 
obj fun: -2428.3980879220303
updated min: 
obj fun: -2428.399064710329
updated min: 
obj fun: -2428.40032339846
updated min: 
obj fun: -2428.4017173818147
updated min: 
obj fun: -2428.403091762369
updated min: 
obj fun: -2428.4043002136377
updated min: 
obj fun: -2428.405183130554
updated min: 
obj fun: -2428.4057805218613
updated min: 
obj fun: -2428.4057820181265
updated min: 
obj fun: -2428.4064251976174
updated min: 
obj fun: -2428.406629538531
updated min: 
obj fun: -2428.407188990841
updated min: 
obj fun: -2428.4079695846735
updated min: 
obj fun: -2428.4097525371194
updated min: 
obj fun: -2428.4122658154283
updated min: 
obj fun: -2428.4149530539885
updated min: 
obj fun: -2428.4177514300486
updated min: 
obj fun: -2428.4205539870236
updated min: 
obj fun: -2428.422195221275
updated min: 
obj fun: -2428.423035676374
updated min: 
obj fun: -2428.423373002387
updated min: 
obj fun: -2428.4237736971863
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2425.9379348042794
updated min: 
obj fun: -2428.425779327213
updated min: 
obj fun: -2428.427417091674
updated min: 
obj fun: -2428.4290405043257
updated min: 
obj fun: -2428.4296520826715
updated min: 
obj fun: -2428.4299658298664
updated min: 
obj fun: -2428.429981685526
updated min: 
obj fun: -2428.4302453959413
updated min: 
obj fun: -2428.4310173354493
updated min: 
obj fun: -2428.4317475163793
updated min: 
obj fun: -2428.4330449663926
updated min: 
obj fun: -2428.4343097739643
updated min: 
obj fun: -2428.435237676519
updated min: 
obj fun: -2428.4355333444505
updated min: 
obj fun: -2428.4356681005684
updated min: 
obj fun: -2428.4374693384166
updated min: 
obj fun: -2428.4390529411853
updated min: 
obj fun: -2428.440726921541
updated min: 
obj fun: -2428.4421037411475
updated min: 
obj fun: -2428.442758197522
updated min: 
obj fun: -2428.443257837141
updated min: 
obj fun: -2428.443272092048
updated min: 
obj fun: -2428.443341447851
updated min: 
obj fun: -2428.444340983039
updated min: 
obj fun: -2428.445778707964
updated min: 
obj fun: -2428.447394273412
updated min: 
obj fun: -2428.448906442065
updated min: 
obj fun: -2428.45032623946
updated min: 
obj fun: -2428.4515290147883
updated min: 
obj fun: -2428.4522088289596
updated min: 
obj fun: -2428.452899900454
updated min: 
obj fun: -2428.4534864409297
updated min: 
obj fun: -2428.4538846612963
updated min: 
obj fun: -2428.4548463769706
updated min: 
obj fun: -2428.4558655425226
updated min: 
obj fun: -2428.4573907114286
updated min: 
obj fun: -2428.4584477776875
updated min: 
obj fun: -2428.4595759193403
updated min: 
obj fun: -2428.4605879527858
updated min: 
obj fun: -2428.4616103957264
updated min: 
obj fun: -2428.4621981022838
updated min: 
obj fun: -2428.4628130091996
updated min: 
obj fun: -2428.4634417742845
updated min: 
obj fun: -2428.4644309879195
updated min: 
obj fun: -2428.4652334796415
updated min: 
obj fun: -2428.4661907906243
updated min: 
obj fun: -2428.4676217505557
updated min: 
obj fun: -2428.4689415964076
updated min: 
obj fun: -2428.4703487838615
updated min: 
obj fun: -2428.4721907958315
updated min: 
obj fun: -2428.4737993763015
updated min: 
obj fun: -2428.4751962465953
updated min: 
obj fun: -2428.4766412202557
updated min: 
obj fun: -2428.477531957488
updated min: 
obj fun: -2428.4784007698327
updated min: 
obj fun: -2428.478952544367
updated min: 
obj fun: -2428.480058037089
updated min: 
obj fun: -2428.48125887625
updated min: 
obj fun: -2428.4829054507163
updated min: 
obj fun: -2428.484247597131
updated min: 
obj fun: -2428.4857994479044
updated min: 
obj fun: -2428.4873669881113
updated min: 
obj fun: -2428.4888917526046
updated min: 
obj fun: -2428.490166153581
updated min: 
obj fun: -2428.491537127216
updated min: 
obj fun: -2428.4928528778432
updated min: 
obj fun: -2428.494256096302
updated min: 
obj fun: -2428.495767808081
updated min: 
obj fun: -2428.4971522716246
updated min: 
obj fun: -2428.498332634765
updated min: 
obj fun: -2428.4994118442355
updated min: 
obj fun: -2428.500213446293
updated min: 
obj fun: -2428.5011563191724
updated min: 
obj fun: -2428.501784801881
updated min: 
obj fun: -2428.502277491224
updated min: 
obj fun: -2428.502596986405
updated min: 
obj fun: -2428.5026853172835
updated min: 
obj fun: -2428.502705815604
updated min: 
obj fun: -2428.503605132304
updated min: 
obj fun: -2428.5049591593584
updated min: 
obj fun: -2428.5069388597235
updated min: 
obj fun: -2428.5086487058516
updated min: 
obj fun: -2428.5101927829965
updated min: 
obj fun: -2428.511370335917
updated min: 
obj fun: -2428.512186736058
updated min: 
obj fun: -2428.512735172374
updated min: 
obj fun: -2428.5133185870413
updated min: 
obj fun: -2428.5137005614793
updated min: 
obj fun: -2428.5145522101634
updated min: 
obj fun: -2428.5155816508664
updated min: 
obj fun: -2428.5166867152648
updated min: 
obj fun: -2428.5178280286796
updated min: 
obj fun: -2428.518763260397
updated min: 
obj fun: -2428.5194557748505
updated min: 
obj fun: -2428.5200247004527
updated min: 
obj fun: -2428.5206344671537
updated min: 
obj fun: -2428.521280972001
updated min: 
obj fun: -2428.521833002913
updated min: 
obj fun: -2428.522165901985
updated min: 
obj fun: -2428.522249614644
updated min: 
obj fun: -2428.522314264422
updated min: 
obj fun: -2428.522372906115
updated min: 
obj fun: -2428.5238185891167
updated min: 
obj fun: -2428.5260460393583
updated min: 
obj fun: -2428.5279073702213
updated min: 
obj fun: -2428.5291474649553
updated min: 
obj fun: -2428.529681932559
updated min: 
obj fun: -2428.52972082282
updated min: 
obj fun: -2428.5311634658224
updated min: 
obj fun: -2428.532455924168
updated min: 
obj fun: -2428.5331877758604
updated min: 
obj fun: -2428.5334265547003
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2427.028735634166
updated min: 
obj fun: -2428.534078069426
updated min: 
obj fun: -2428.5348588561446
updated min: 
obj fun: -2428.535205714731
updated min: 
obj fun: -2428.5353903706878
updated min: 
obj fun: -2428.535499601728
updated min: 
obj fun: -2428.535538359003
updated min: 
obj fun: -2428.5361162681347
updated min: 
obj fun: -2428.537056751048
updated min: 
obj fun: -2428.5380611862197
updated min: 
obj fun: -2428.5395448701365
updated min: 
obj fun: -2428.541073291972
updated min: 
obj fun: -2428.5420862868887
updated min: 
obj fun: -2428.542877180978
updated min: 
obj fun: -2428.543869016027
updated min: 
obj fun: -2428.544842097006
updated min: 
obj fun: -2428.5454963891693
updated min: 
obj fun: -2428.546520989366
updated min: 
obj fun: -2428.547376622552
updated min: 
obj fun: -2428.5486349752036
updated min: 
obj fun: -2428.5500767195645
updated min: 
obj fun: -2428.5515595514926
updated min: 
obj fun: -2428.5530688857634
updated min: 
obj fun: -2428.554530738732
updated min: 
obj fun: -2428.5558939929374
updated min: 
obj fun: -2428.557433122661
updated min: 
obj fun: -2428.55874159032
updated min: 
obj fun: -2428.559759455347
updated min: 
obj fun: -2428.560366009707
updated min: 
obj fun: -2428.5605033765855
updated min: 
obj fun: -2428.560618183493
updated min: 
obj fun: -2428.56075008382
updated min: 
obj fun: -2428.5609206840118
updated min: 
obj fun: -2428.5612427610577
updated min: 
obj fun: -2428.561642270821
updated min: 
obj fun: -2428.562129824274
updated min: 
obj fun: -2428.562787768741
updated min: 
obj fun: -2428.563579049979
updated min: 
obj fun: -2428.564338160938
updated min: 
obj fun: -2428.5652956165663
updated min: 
obj fun: -2428.5658094081664
updated min: 
obj fun: -2428.5661459217026
updated min: 
obj fun: -2428.5664310816696
updated min: 
obj fun: -2428.56664030949
updated min: 
obj fun: -2428.5668481373705
updated min: 
obj fun: -2428.567124347845
updated min: 
obj fun: -2428.567541402611
updated min: 
obj fun: -2428.567672675158
updated min: 
obj fun: -2428.5677590297787
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2423.566010185526
NN weights: [ 2.47996628e-01 -7.15199471e-01  3.98510504e+00  2.40447736e+00
  6.85245180e+00 -7.15125799e+00 -6.56295204e+00  4.15463400e+00
 -2.93518791e+01 -3.21498718e+01 -5.27055740e-01  4.54391599e-01
 -3.77212429e+00 -1.84668577e+00 -7.00421286e+00  7.54102707e+00
  6.63038683e+00 -4.02830458e+00  2.96160164e+01  3.19391689e+01
 -2.32468052e+01 -3.51506042e+01 -3.79673615e+01 -3.26750755e+01
 -3.45280151e+01 -5.71679802e+01 -4.56759834e+01 -2.20180473e+01
 -4.54817963e+01 -6.75384617e+00 -2.17705059e+00  2.12452942e+02
  9.52995529e+01  5.42015572e+01  1.21410240e+02  1.70799759e+02
  2.85032867e+02  3.45771454e+02  4.18342377e+02 -1.10995178e+01
 -2.47044983e+01 -3.35358772e+01 -7.83174744e+01 -3.39742050e+01
 -3.01613121e+01 -8.30951080e+01 -3.84931107e+01 -1.56803370e+01
 -3.84228821e+01 -1.45992002e+01 -2.45458374e+01 -3.36886978e+01
 -4.59578171e+01 -3.37390404e+01 -3.18450603e+01 -5.97902756e+01
 -4.10811501e+01 -1.82296715e+01 -4.14720497e+01 -1.28989849e+01
 -2.14574265e+00 -7.06449270e+00 -4.90310478e+00  4.72813654e+00
 -9.43548508e+01 -6.11122465e+00 -7.12637949e+00  1.62113380e+01
  1.94931736e+01  1.22327781e+00 -3.04783845e+00 -8.26791687e+01
 -1.45180328e+02  1.91539097e+01 -4.61323090e+01 -2.15301727e+02
 -1.27288475e+02 -1.92270641e+01 -7.62074051e+01  5.47049618e+00
 -2.66836715e+00  6.96999693e+00  3.82494003e-01  8.03304977e+01
  3.12039051e+01  1.87860096e+00  2.83866954e+00  4.29275017e+01
  4.16115799e+01 -2.49590092e+01 -2.46144714e+01 -3.36601105e+01
 -8.60305939e+01 -3.46877365e+01 -2.93305950e+01 -8.95601349e+01
 -3.77443161e+01 -1.51180067e+01 -3.72564468e+01 -1.43658047e+01
 -5.63207006e+00 -7.20005646e+01 -1.35086088e+01 -2.92995358e+01
 -3.35587502e+01 -2.21496677e+01 -9.54777679e+01 -5.79294777e+01
 -1.05894554e+02 -3.68999405e+01 -3.52993321e+00 -6.75169220e+01
 -4.94678783e+00 -2.73435802e+01 -3.11275139e+01 -1.21455116e+01
 -9.18391266e+01 -7.75790482e+01 -1.26098305e+02 -3.64706688e+01
 -1.02446175e+00 -3.66369133e+01 -4.30286980e+00  2.92599416e+00
 -1.86993763e-01  2.85643816e+00 -1.02172836e+02 -4.50265646e-01
 -4.57984962e+01  1.37873077e+01  7.17097104e-01  2.91681600e+00
 -2.54609752e+00  2.89748502e+00 -2.44763374e+01  5.30901384e+00
 -2.34641075e+01  5.47256470e+00 -8.16097546e+00 -1.61951280e+00]
Minimum obj value:-2428.568054866204
Optimal xi: 27.193022
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1490.0105444192734
W_T_median: 1301.233199481253
W_T_pctile_5: 741.7386291647667
W_T_CVAR_5_pct: 640.6152623231927
F value: -2428.568054866204
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.2
F value: -2428.568054866204
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[  -1.0245,  -36.6369],
        [  -4.3029,    2.9260],
        [  -0.1870,    2.8564],
        [-102.1728,   -0.4503],
        [ -45.7985,   13.7873],
        [   0.7171,    2.9168],
        [  -2.5461,    2.8975],
        [ -24.4763,    5.3090],
        [ -23.4641,    5.4726],
        [  -8.1610,   -1.6195]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[-2.3247e+01, -3.5151e+01, -3.7967e+01, -3.2675e+01, -3.4528e+01,
         -5.7168e+01, -4.5676e+01, -2.2018e+01, -4.5482e+01, -6.7538e+00],
        [-2.1771e+00,  2.1245e+02,  9.5300e+01,  5.4202e+01,  1.2141e+02,
          1.7080e+02,  2.8503e+02,  3.4577e+02,  4.1834e+02, -1.1100e+01],
        [-2.4704e+01, -3.3536e+01, -7.8317e+01, -3.3974e+01, -3.0161e+01,
         -8.3095e+01, -3.8493e+01, -1.5680e+01, -3.8423e+01, -1.4599e+01],
        [-2.4546e+01, -3.3689e+01, -4.5958e+01, -3.3739e+01, -3.1845e+01,
         -5.9790e+01, -4.1081e+01, -1.8230e+01, -4.1472e+01, -1.2899e+01],
        [-2.1457e+00, -7.0645e+00, -4.9031e+00,  4.7281e+00, -9.4355e+01,
         -6.1112e+00, -7.1264e+00,  1.6211e+01,  1.9493e+01,  1.2233e+00],
        [-3.0478e+00, -8.2679e+01, -1.4518e+02,  1.9154e+01, -4.6132e+01,
         -2.1530e+02, -1.2729e+02, -1.9227e+01, -7.6207e+01,  5.4705e+00],
        [-2.6684e+00,  6.9700e+00,  3.8249e-01,  8.0330e+01,  3.1204e+01,
          1.8786e+00,  2.8387e+00,  4.2928e+01,  4.1612e+01, -2.4959e+01],
        [-2.4614e+01, -3.3660e+01, -8.6031e+01, -3.4688e+01, -2.9331e+01,
         -8.9560e+01, -3.7744e+01, -1.5118e+01, -3.7256e+01, -1.4366e+01],
        [-5.6321e+00, -7.2001e+01, -1.3509e+01, -2.9300e+01, -3.3559e+01,
         -2.2150e+01, -9.5478e+01, -5.7929e+01, -1.0589e+02, -3.6900e+01],
        [-3.5299e+00, -6.7517e+01, -4.9468e+00, -2.7344e+01, -3.1128e+01,
         -1.2146e+01, -9.1839e+01, -7.7579e+01, -1.2610e+02, -3.6471e+01]],
       device='cuda:0')), ('model.output_layer_3.weight', tensor([[  0.2480,  -0.7152,   3.9851,   2.4045,   6.8525,  -7.1513,  -6.5630,
           4.1546, -29.3519, -32.1499],
        [ -0.5271,   0.4544,  -3.7721,  -1.8467,  -7.0042,   7.5410,   6.6304,
          -4.0283,  29.6160,  31.9392]], device='cuda:0'))])
loaded xi:  27.193022
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -2575.9360587933093
objective value function right now is: -2575.9360587933093
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2574.309642257146
6.0% of gradient descent iterations done. Method = Adam
updated min: -2576.369535020869
objective value function right now is: -2576.369535020869
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2576.009906249046
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2573.0992083308715
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2574.667708379975
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -2576.3608568753284
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2576.198846356362
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2573.1453424224164
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2571.964176610593
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2575.9910983226423
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2574.743547040813
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2572.3827273203265
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -2573.7071942893017
30.0% of gradient descent iterations done. Method = Adam
updated min: -2576.598929566234
objective value function right now is: -2576.598929566234
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2573.981845631336
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2575.4511359227686
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2573.787684949098
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2575.569006418795
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2576.2772884882625
42.0% of gradient descent iterations done. Method = Adam
updated min: -2576.922911473176
objective value function right now is: -2576.922911473176
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2575.665134613175
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2575.9318923874184
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2576.289771630317
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2574.1243583061587
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2571.249399274655
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2564.943089691869
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -2576.6541033613776
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -2576.7364208537915
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2575.278224803517
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2573.6879133449875
64.0% of gradient descent iterations done. Method = Adam
updated min: -2577.1843158613906
objective value function right now is: -2577.1843158613906
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2575.361954398847
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2574.487441660879
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2574.2811460690236
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2575.9929527428308
74.0% of gradient descent iterations done. Method = Adam
updated min: -2577.264773104365
objective value function right now is: -2577.264773104365
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2573.3957846003045
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2571.956526354838
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2575.5200297686215
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2567.7228438439447
84.0% of gradient descent iterations done. Method = Adam
updated min: -2577.4473034504067
objective value function right now is: -2577.4473034504067
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2563.3804988586285
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2575.617016533357
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2576.7680761551787
updated min: 
obj fun: -2577.4536337502955
updated min: 
obj fun: -2577.466159884324
updated min: 
obj fun: -2577.48264728051
updated min: 
obj fun: -2577.4975467031877
updated min: 
obj fun: -2577.506108740948
updated min: 
obj fun: -2577.5174731543043
updated min: 
obj fun: -2577.564403503592
updated min: 
obj fun: -2577.6013406992683
updated min: 
obj fun: -2577.627105265694
updated min: 
obj fun: -2577.641720190294
updated min: 
obj fun: -2577.651818760759
updated min: 
obj fun: -2577.6586243459033
updated min: 
obj fun: -2577.6611360803063
updated min: 
obj fun: -2577.6626082110133
updated min: 
obj fun: -2577.6626151455457
updated min: 
obj fun: -2577.6642578398046
updated min: 
obj fun: -2577.6790934833725
updated min: 
obj fun: -2577.69394121979
updated min: 
obj fun: -2577.7046561261304
updated min: 
obj fun: -2577.7110980513585
updated min: 
obj fun: -2577.7125696898047
updated min: 
obj fun: -2577.7127226645307
updated min: 
obj fun: -2577.715739388001
updated min: 
obj fun: -2577.7181857218075
updated min: 
obj fun: -2577.7187848850463
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2569.6216817745717
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2574.2163225347977
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2577.0964646467214
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2575.3094536200465
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2574.7592112190036
NN weights: [-5.2662846e-02 -8.4456331e-01  3.9265175e+00  2.3104563e+00
  2.5473146e+00 -7.2334032e+00 -6.7066479e+00  4.0968394e+00
 -3.1616621e+01 -3.2799641e+01 -2.2639456e-01  5.8531535e-01
 -3.7135363e+00 -1.7526600e+00 -2.6985784e+00  7.6267648e+00
  6.8164697e+00 -3.9705198e+00  3.1880764e+01  3.2589005e+01
 -2.3432880e+01 -3.5150604e+01 -3.7967609e+01 -3.2675076e+01
 -3.4528015e+01 -5.7168461e+01 -4.5675983e+01 -2.2018047e+01
 -4.5481796e+01 -6.7479577e+00 -1.9257909e+00  2.3405568e+02
  1.0438252e+02  4.3826508e+01  1.6340430e+02  1.7707486e+02
  3.0123251e+02  4.0811844e+02  4.8851678e+02 -8.9223108e+00
 -2.5149649e+01 -3.3535877e+01 -7.8317497e+01 -3.3974205e+01
 -3.0161312e+01 -8.3095192e+01 -3.8493111e+01 -1.5680337e+01
 -3.8422882e+01 -1.4588525e+01 -2.4933134e+01 -3.3688698e+01
 -4.5959080e+01 -3.3739040e+01 -3.1845060e+01 -5.9792667e+01
 -4.1081150e+01 -1.8229671e+01 -4.1472050e+01 -1.2891092e+01
 -1.0134983e+00 -6.1213374e+00 -5.7830262e+00  9.0926046e+00
 -7.8607933e+01 -7.7924914e+00 -8.2969465e+00  9.0284038e-01
  1.2960510e+01  1.6512554e+00 -3.1165924e+00 -9.5854752e+01
 -1.6125493e+02  1.8661804e+01 -8.8261826e+01 -2.3145372e+02
 -1.4159308e+02 -3.3202541e+01 -9.8827515e+01  5.0622659e+00
 -3.2901027e+00  8.4197149e+00  8.7652922e-01  9.9159813e+01
  4.8628395e+01  1.7327933e+00  6.2577729e+00  4.8511681e+01
  4.6081589e+01 -2.8412304e+01 -2.5077259e+01 -3.3660110e+01
 -8.6030663e+01 -3.4687737e+01 -2.9330595e+01 -8.9560379e+01
 -3.7744316e+01 -1.5118007e+01 -3.7256447e+01 -1.4351909e+01
 -4.2645130e+00 -8.1084694e+01 -2.0544910e+01 -2.9299536e+01
 -3.3558857e+01 -2.6992931e+01 -9.6774414e+01 -5.8239956e+01
 -1.0608416e+02 -3.4251503e+01 -3.4256358e+00 -6.4411049e+01
 -1.7457002e+00 -2.7343580e+01 -3.1131350e+01 -6.6589613e+00
 -8.0273926e+01 -8.0247643e+01 -1.2775670e+02 -3.5183563e+01
 -1.4735445e+01 -2.6832239e+01 -3.7913339e+00  3.1130800e+00
  2.7044860e-01  2.9125080e+00 -1.1287939e+02 -7.4058867e-01
 -3.9697826e+01  4.5581932e+00  7.5652325e-01  2.9162045e+00
 -2.9780505e+00  2.9884546e+00 -2.7073751e+01  5.1046834e+00
 -2.6587204e+01  5.6496005e+00 -8.3328991e+00 -1.6355504e+00]
Minimum obj value:-2577.719683779406
Optimal xi: 26.951397
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1498.4949943831687
W_T_median: 1323.2264373002881
W_T_pctile_5: 728.720088999934
W_T_CVAR_5_pct: 629.7494245037782
F value: -2577.719683779406
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.3
F value: -2577.719683779406
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ -14.7354,  -26.8322],
        [  -3.7913,    3.1131],
        [   0.2704,    2.9125],
        [-112.8794,   -0.7406],
        [ -39.6978,    4.5582],
        [   0.7565,    2.9162],
        [  -2.9781,    2.9885],
        [ -27.0738,    5.1047],
        [ -26.5872,    5.6496],
        [  -8.3329,   -1.6356]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[ -23.4329,  -35.1506,  -37.9676,  -32.6751,  -34.5280,  -57.1685,
          -45.6760,  -22.0180,  -45.4818,   -6.7480],
        [  -1.9258,  234.0557,  104.3825,   43.8265,  163.4043,  177.0749,
          301.2325,  408.1184,  488.5168,   -8.9223],
        [ -25.1496,  -33.5359,  -78.3175,  -33.9742,  -30.1613,  -83.0952,
          -38.4931,  -15.6803,  -38.4229,  -14.5885],
        [ -24.9331,  -33.6887,  -45.9591,  -33.7390,  -31.8451,  -59.7927,
          -41.0812,  -18.2297,  -41.4720,  -12.8911],
        [  -1.0135,   -6.1213,   -5.7830,    9.0926,  -78.6079,   -7.7925,
           -8.2969,    0.9028,   12.9605,    1.6513],
        [  -3.1166,  -95.8548, -161.2549,   18.6618,  -88.2618, -231.4537,
         -141.5931,  -33.2025,  -98.8275,    5.0623],
        [  -3.2901,    8.4197,    0.8765,   99.1598,   48.6284,    1.7328,
            6.2578,   48.5117,   46.0816,  -28.4123],
        [ -25.0773,  -33.6601,  -86.0307,  -34.6877,  -29.3306,  -89.5604,
          -37.7443,  -15.1180,  -37.2564,  -14.3519],
        [  -4.2645,  -81.0847,  -20.5449,  -29.2995,  -33.5589,  -26.9929,
          -96.7744,  -58.2400, -106.0842,  -34.2515],
        [  -3.4256,  -64.4110,   -1.7457,  -27.3436,  -31.1313,   -6.6590,
          -80.2739,  -80.2476, -127.7567,  -35.1836]], device='cuda:0')), ('model.output_layer_3.weight', tensor([[ -0.0527,  -0.8446,   3.9265,   2.3105,   2.5473,  -7.2334,  -6.7066,
           4.0968, -31.6166, -32.7996],
        [ -0.2264,   0.5853,  -3.7135,  -1.7527,  -2.6986,   7.6268,   6.8165,
          -3.9705,  31.8808,  32.5890]], device='cuda:0'))])
loaded xi:  26.951397
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -2725.0628013254723
objective value function right now is: -2725.0628013254723
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2718.7857945739825
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2723.7606722854084
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2722.7972156233664
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2723.119749561917
12.0% of gradient descent iterations done. Method = Adam
updated min: -2725.5256312951797
objective value function right now is: -2725.5256312951797
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -2724.7667848234237
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2722.7210698295135
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2724.1733075256766
20.0% of gradient descent iterations done. Method = Adam
updated min: -2725.8958661541938
objective value function right now is: -2725.8958661541938
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2724.774710290403
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2720.945983085665
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2723.242593454887
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -2725.0374797186732
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2724.3627441802014
32.0% of gradient descent iterations done. Method = Adam
updated min: -2726.302707609237
objective value function right now is: -2726.302707609237
34.0% of gradient descent iterations done. Method = Adam
updated min: -2726.471343582757
objective value function right now is: -2726.471343582757
36.0% of gradient descent iterations done. Method = Adam
updated min: -2727.3563991139854
objective value function right now is: -2727.3563991139854
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2723.8721611065157
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2726.5152341722755
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2724.8729607915293
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2725.5343439761737
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2722.204290203396
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2725.6496432255162
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2715.0023765977667
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2726.564416867328
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2723.2805838115473
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -2726.962136615377
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -2726.177386990835
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2726.644079817471
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2724.329598771057
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2724.7079862026894
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2724.4295044505166
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2725.497410927195
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2726.9318772632337
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2726.38293521329
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2727.2111204818175
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2721.49515063772
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2723.8084902555447
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2723.485781013263
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2726.801215601977
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2727.147736320073
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2726.721614546179
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2722.9814110743914
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2715.8065258081447
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2725.3479176389847
updated min: 
obj fun: -2727.3621290859137
updated min: 
obj fun: -2727.3697998419166
updated min: 
obj fun: -2727.3772894141807
updated min: 
obj fun: -2727.3844102842368
updated min: 
obj fun: -2727.390979994256
updated min: 
obj fun: -2727.396640307695
updated min: 
obj fun: -2727.401055507345
updated min: 
obj fun: -2727.4040969679163
updated min: 
obj fun: -2727.4064483312472
updated min: 
obj fun: -2727.4078971679714
updated min: 
obj fun: -2727.4089186180568
updated min: 
obj fun: -2727.409805623807
updated min: 
obj fun: -2727.4107037951094
updated min: 
obj fun: -2727.4113859558283
updated min: 
obj fun: -2727.4118884662375
updated min: 
obj fun: -2727.4144238730355
updated min: 
obj fun: -2727.417173015347
updated min: 
obj fun: -2727.4200909619826
updated min: 
obj fun: -2727.4228982763498
updated min: 
obj fun: -2727.4261621158516
updated min: 
obj fun: -2727.4305360798035
updated min: 
obj fun: -2727.435809041054
updated min: 
obj fun: -2727.441334211129
updated min: 
obj fun: -2727.4466690986915
updated min: 
obj fun: -2727.451416617294
updated min: 
obj fun: -2727.455297272675
updated min: 
obj fun: -2727.458342650837
updated min: 
obj fun: -2727.4607254116845
updated min: 
obj fun: -2727.462467006286
updated min: 
obj fun: -2727.4639219079827
updated min: 
obj fun: -2727.4659017116014
updated min: 
obj fun: -2727.4682155592695
updated min: 
obj fun: -2727.470554211179
updated min: 
obj fun: -2727.472879079949
updated min: 
obj fun: -2727.4748922164267
updated min: 
obj fun: -2727.476376310286
updated min: 
obj fun: -2727.4776691534207
updated min: 
obj fun: -2727.4785310272696
updated min: 
obj fun: -2727.478699315476
updated min: 
obj fun: -2727.4798026715202
updated min: 
obj fun: -2727.4827047803506
updated min: 
obj fun: -2727.4861465161016
updated min: 
obj fun: -2727.490050728918
updated min: 
obj fun: -2727.4943207571096
updated min: 
obj fun: -2727.498794515581
updated min: 
obj fun: -2727.5032657111055
updated min: 
obj fun: -2727.5074616492684
updated min: 
obj fun: -2727.5115251030074
updated min: 
obj fun: -2727.5157024830764
updated min: 
obj fun: -2727.5198534307474
updated min: 
obj fun: -2727.5240979904615
updated min: 
obj fun: -2727.5285264244326
updated min: 
obj fun: -2727.532826978676
updated min: 
obj fun: -2727.536977236558
updated min: 
obj fun: -2727.541021930548
updated min: 
obj fun: -2727.5452361454245
updated min: 
obj fun: -2727.5494448389672
updated min: 
obj fun: -2727.553370726362
updated min: 
obj fun: -2727.5568317625743
updated min: 
obj fun: -2727.5598850842744
updated min: 
obj fun: -2727.562370307989
updated min: 
obj fun: -2727.564236127528
updated min: 
obj fun: -2727.565665309754
updated min: 
obj fun: -2727.56684764802
updated min: 
obj fun: -2727.5680343013423
updated min: 
obj fun: -2727.56979409034
updated min: 
obj fun: -2727.5720969398303
updated min: 
obj fun: -2727.5753465742337
updated min: 
obj fun: -2727.5789021472297
updated min: 
obj fun: -2727.5825528606215
updated min: 
obj fun: -2727.585792551649
updated min: 
obj fun: -2727.58852194938
updated min: 
obj fun: -2727.590752807606
updated min: 
obj fun: -2727.592458000621
updated min: 
obj fun: -2727.593973301191
updated min: 
obj fun: -2727.5951754308912
updated min: 
obj fun: -2727.5964902617416
updated min: 
obj fun: -2727.5977109232376
updated min: 
obj fun: -2727.599015122271
updated min: 
obj fun: -2727.600022705178
updated min: 
obj fun: -2727.6004121219653
updated min: 
obj fun: -2727.6013588275023
updated min: 
obj fun: -2727.603539816272
updated min: 
obj fun: -2727.605811821268
updated min: 
obj fun: -2727.6080010526575
updated min: 
obj fun: -2727.6099231254434
updated min: 
obj fun: -2727.611563320985
updated min: 
obj fun: -2727.613100100913
updated min: 
obj fun: -2727.614419631946
updated min: 
obj fun: -2727.615609761862
updated min: 
obj fun: -2727.616719731421
updated min: 
obj fun: -2727.617907275672
updated min: 
obj fun: -2727.6195036237086
updated min: 
obj fun: -2727.6215075751074
updated min: 
obj fun: -2727.623604398583
updated min: 
obj fun: -2727.6257741944155
updated min: 
obj fun: -2727.627957549803
updated min: 
obj fun: -2727.630206558203
updated min: 
obj fun: -2727.6322863981845
updated min: 
obj fun: -2727.6338515996617
updated min: 
obj fun: -2727.635037284936
updated min: 
obj fun: -2727.6359879256484
updated min: 
obj fun: -2727.636507992584
updated min: 
obj fun: -2727.6373076831765
updated min: 
obj fun: -2727.6389658529065
updated min: 
obj fun: -2727.6404223411046
updated min: 
obj fun: -2727.6414265960184
updated min: 
obj fun: -2727.6416828722035
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2724.4689741413586
updated min: 
obj fun: -2727.642617025108
updated min: 
obj fun: -2727.6437421362925
updated min: 
obj fun: -2727.644712667537
updated min: 
obj fun: -2727.645325880559
updated min: 
obj fun: -2727.645740113069
updated min: 
obj fun: -2727.646449226693
updated min: 
obj fun: -2727.647798382925
updated min: 
obj fun: -2727.6496946801535
updated min: 
obj fun: -2727.6519278097903
updated min: 
obj fun: -2727.6540911257703
updated min: 
obj fun: -2727.6556471662325
updated min: 
obj fun: -2727.65641900491
updated min: 
obj fun: -2727.656439195149
updated min: 
obj fun: -2727.6578926938996
updated min: 
obj fun: -2727.6595751671257
updated min: 
obj fun: -2727.6608919081027
updated min: 
obj fun: -2727.66158814404
updated min: 
obj fun: -2727.6618035556935
updated min: 
obj fun: -2727.6618092256554
updated min: 
obj fun: -2727.661821185957
updated min: 
obj fun: -2727.6621409026757
updated min: 
obj fun: -2727.6628561563416
updated min: 
obj fun: -2727.6638916224465
updated min: 
obj fun: -2727.6651184729985
updated min: 
obj fun: -2727.666252818035
updated min: 
obj fun: -2727.667102297905
updated min: 
obj fun: -2727.6677723783887
updated min: 
obj fun: -2727.668237241692
updated min: 
obj fun: -2727.668616500058
updated min: 
obj fun: -2727.669060747486
updated min: 
obj fun: -2727.669595884262
updated min: 
obj fun: -2727.670249484567
updated min: 
obj fun: -2727.6712671180862
updated min: 
obj fun: -2727.672617549626
updated min: 
obj fun: -2727.6742827958815
updated min: 
obj fun: -2727.6762927396144
updated min: 
obj fun: -2727.678513440312
updated min: 
obj fun: -2727.680818043952
updated min: 
obj fun: -2727.683049357742
updated min: 
obj fun: -2727.6852350077647
updated min: 
obj fun: -2727.687302721091
updated min: 
obj fun: -2727.6890416191254
updated min: 
obj fun: -2727.690297307712
updated min: 
obj fun: -2727.6910024494687
updated min: 
obj fun: -2727.691321623892
updated min: 
obj fun: -2727.6917124361858
updated min: 
obj fun: -2727.6924181669356
updated min: 
obj fun: -2727.693656602499
updated min: 
obj fun: -2727.695439231989
updated min: 
obj fun: -2727.69769836603
updated min: 
obj fun: -2727.6999426056914
updated min: 
obj fun: -2727.7022211576113
updated min: 
obj fun: -2727.704194357097
updated min: 
obj fun: -2727.705797283368
updated min: 
obj fun: -2727.706955996941
updated min: 
obj fun: -2727.707835172525
updated min: 
obj fun: -2727.7082924373235
updated min: 
obj fun: -2727.7085587465735
updated min: 
obj fun: -2727.708783843796
updated min: 
obj fun: -2727.7092564556347
updated min: 
obj fun: -2727.7100019092823
updated min: 
obj fun: -2727.7109106868825
updated min: 
obj fun: -2727.711989424241
updated min: 
obj fun: -2727.713227768958
updated min: 
obj fun: -2727.7143886761723
updated min: 
obj fun: -2727.7155237404863
updated min: 
obj fun: -2727.7164539000846
updated min: 
obj fun: -2727.7172174199186
updated min: 
obj fun: -2727.7178340651562
updated min: 
obj fun: -2727.7182573036484
updated min: 
obj fun: -2727.7187612393773
updated min: 
obj fun: -2727.719414561987
updated min: 
obj fun: -2727.7201038024623
updated min: 
obj fun: -2727.720721594774
updated min: 
obj fun: -2727.7212429935603
updated min: 
obj fun: -2727.722023419939
updated min: 
obj fun: -2727.722995444088
updated min: 
obj fun: -2727.7243518076502
updated min: 
obj fun: -2727.7260340504554
updated min: 
obj fun: -2727.7276754601094
updated min: 
obj fun: -2727.729025662117
updated min: 
obj fun: -2727.730065152726
updated min: 
obj fun: -2727.7307397419063
updated min: 
obj fun: -2727.7310602534703
updated min: 
obj fun: -2727.7311311930875
updated min: 
obj fun: -2727.731245867996
updated min: 
obj fun: -2727.731668812524
updated min: 
obj fun: -2727.7320904297517
updated min: 
obj fun: -2727.7324734751305
updated min: 
obj fun: -2727.732779381129
updated min: 
obj fun: -2727.733029688912
updated min: 
obj fun: -2727.7331539794654
updated min: 
obj fun: -2727.733546863869
updated min: 
obj fun: -2727.7347802886525
updated min: 
obj fun: -2727.7358938383195
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2724.889880549168
updated min: 
obj fun: -2727.736717399006
updated min: 
obj fun: -2727.7374131001047
updated min: 
obj fun: -2727.738005280792
updated min: 
obj fun: -2727.7386190395505
updated min: 
obj fun: -2727.739183917293
updated min: 
obj fun: -2727.739507242192
updated min: 
obj fun: -2727.7407979385816
updated min: 
obj fun: -2727.741954170772
updated min: 
obj fun: -2727.742831662519
updated min: 
obj fun: -2727.743459384293
updated min: 
obj fun: -2727.743956614387
updated min: 
obj fun: -2727.74443107532
updated min: 
obj fun: -2727.7449849679892
updated min: 
obj fun: -2727.745640741291
updated min: 
obj fun: -2727.7464264824644
updated min: 
obj fun: -2727.7474357701126
updated min: 
obj fun: -2727.748656898739
updated min: 
obj fun: -2727.749954131971
updated min: 
obj fun: -2727.751432713465
updated min: 
obj fun: -2727.752880384233
updated min: 
obj fun: -2727.7540448749533
updated min: 
obj fun: -2727.755089025243
updated min: 
obj fun: -2727.7558048998017
updated min: 
obj fun: -2727.756234474728
updated min: 
obj fun: -2727.756704428531
updated min: 
obj fun: -2727.7572049732403
updated min: 
obj fun: -2727.7576806186034
updated min: 
obj fun: -2727.758198724651
updated min: 
obj fun: -2727.7586221655292
updated min: 
obj fun: -2727.758877547358
updated min: 
obj fun: -2727.7590009706737
updated min: 
obj fun: -2727.7590616211846
updated min: 
obj fun: -2727.75915578671
updated min: 
obj fun: -2727.75922193189
updated min: 
obj fun: -2727.7592521392035
updated min: 
obj fun: -2727.7596301829744
updated min: 
obj fun: -2727.7601627209874
updated min: 
obj fun: -2727.7606885613864
updated min: 
obj fun: -2727.7609197147563
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2727.4186441805186
updated min: 
obj fun: -2727.7610869947393
updated min: 
obj fun: -2727.7615543048455
updated min: 
obj fun: -2727.76227216367
updated min: 
obj fun: -2727.7632451268146
updated min: 
obj fun: -2727.7642494334914
updated min: 
obj fun: -2727.76534207042
updated min: 
obj fun: -2727.7665745002946
updated min: 
obj fun: -2727.767728513559
updated min: 
obj fun: -2727.768605941503
updated min: 
obj fun: -2727.769142831066
updated min: 
obj fun: -2727.769768111132
updated min: 
obj fun: -2727.7705919642076
updated min: 
obj fun: -2727.771980352435
updated min: 
obj fun: -2727.77369887359
updated min: 
obj fun: -2727.7753677926203
updated min: 
obj fun: -2727.7766197573224
updated min: 
obj fun: -2727.777384935332
updated min: 
obj fun: -2727.777617614717
updated min: 
obj fun: -2727.777954029021
updated min: 
obj fun: -2727.778445224709
updated min: 
obj fun: -2727.7786573062117
updated min: 
obj fun: -2727.779129482236
updated min: 
obj fun: -2727.77988819638
updated min: 
obj fun: -2727.7808295170385
updated min: 
obj fun: -2727.781839968918
updated min: 
obj fun: -2727.7827791957848
updated min: 
obj fun: -2727.783548440189
updated min: 
obj fun: -2727.7840634245135
updated min: 
obj fun: -2727.784433696691
updated min: 
obj fun: -2727.7847060553613
updated min: 
obj fun: -2727.784972948839
updated min: 
obj fun: -2727.7853370934286
updated min: 
obj fun: -2727.7858589121274
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2726.272366704165
NN weights: [-2.87139237e-01 -8.77931058e-01  3.89213252e+00  2.25306535e+00
  1.52449667e+00 -7.16366816e+00 -8.09536839e+00  4.06335497e+00
 -3.32377510e+01 -3.30423622e+01  7.79157225e-03  6.11582339e-01
 -3.67915177e+00 -1.69536972e+00 -1.67535031e+00  7.56067324e+00
  7.79004955e+00 -3.93702626e+00  3.35018501e+01  3.28304863e+01
 -2.34500713e+01 -3.51506042e+01 -3.79676437e+01 -3.26750755e+01
 -3.45280151e+01 -5.71685066e+01 -4.56759834e+01 -2.20180473e+01
 -4.54817963e+01 -6.75134230e+00 -2.20825267e+00  2.36451492e+02
  1.18610802e+02  2.73679566e+00  1.44474091e+02  1.87682785e+02
  3.05129761e+02  3.99397858e+02  4.81443817e+02 -9.49265289e+00
 -2.54090061e+01 -3.35358772e+01 -7.83174057e+01 -3.39742050e+01
 -3.01613121e+01 -8.30949097e+01 -3.84931107e+01 -1.56803370e+01
 -3.84228821e+01 -1.45817690e+01 -2.51636524e+01 -3.36886902e+01
 -4.59588051e+01 -3.37390404e+01 -3.18450603e+01 -5.97929573e+01
 -4.10811157e+01 -1.82296715e+01 -4.14720497e+01 -1.28861971e+01
 -1.35241434e-01 -1.30406370e+01 -8.12310505e+00  1.99980850e+01
 -8.69721451e+01 -1.02031231e+01 -1.64214516e+01 -1.32329121e+01
 -3.83439565e+00  2.54991388e+00 -2.80954814e+00 -1.15648514e+02
 -1.74820190e+02  7.98605967e+00 -1.07943756e+02 -2.45083481e+02
 -1.59507874e+02 -7.26625977e+01 -1.39104477e+02  4.94643736e+00
 -3.21052313e+00  1.31247349e+01  4.80779886e-01  1.21820137e+02
  6.10110512e+01  1.78300846e+00  1.22040081e+01  5.02168083e+01
  4.74239006e+01 -2.81401501e+01 -2.53439465e+01 -3.36601105e+01
 -8.60305481e+01 -3.46877365e+01 -2.93305950e+01 -8.95601196e+01
 -3.77443161e+01 -1.51180067e+01 -3.72564468e+01 -1.43432083e+01
 -3.31088257e+00 -8.91402588e+01 -2.67733631e+01 -2.92995358e+01
 -3.35619049e+01 -3.19142399e+01 -1.03221756e+02 -5.83847961e+01
 -1.06230515e+02 -3.41260605e+01 -3.89497542e+00 -5.87606850e+01
  4.40905762e+00 -2.73435802e+01 -3.10491772e+01  2.08448696e+00
 -7.01724701e+01 -8.09419250e+01 -1.28285202e+02 -3.90209236e+01
 -1.92471733e+01 -2.40568485e+01 -2.45321679e+00  2.76806259e+00
  7.46563792e-01  3.00478530e+00 -1.26748436e+02 -1.68405461e+00
 -4.41645393e+01  3.23020291e+00  1.17370832e+00  2.98274374e+00
 -2.32203841e+00  2.78423238e+00 -3.41538506e+01  7.05235243e+00
 -3.35585823e+01  7.46466970e+00 -8.73098373e+00 -1.65116870e+00]
Minimum obj value:-2727.784855614082
Optimal xi: 26.749016
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1508.0187845655769
W_T_median: 1344.9728929742105
W_T_pctile_5: 717.3773700202984
W_T_CVAR_5_pct: 616.6051361615613
F value: -2727.784855614082
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.4
F value: -2727.784855614082
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ -19.2472,  -24.0568],
        [  -2.4532,    2.7681],
        [   0.7466,    3.0048],
        [-126.7484,   -1.6841],
        [ -44.1645,    3.2302],
        [   1.1737,    2.9827],
        [  -2.3220,    2.7842],
        [ -34.1539,    7.0524],
        [ -33.5586,    7.4647],
        [  -8.7310,   -1.6512]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[-2.3450e+01, -3.5151e+01, -3.7968e+01, -3.2675e+01, -3.4528e+01,
         -5.7169e+01, -4.5676e+01, -2.2018e+01, -4.5482e+01, -6.7513e+00],
        [-2.2083e+00,  2.3645e+02,  1.1861e+02,  2.7368e+00,  1.4447e+02,
          1.8768e+02,  3.0513e+02,  3.9940e+02,  4.8144e+02, -9.4927e+00],
        [-2.5409e+01, -3.3536e+01, -7.8317e+01, -3.3974e+01, -3.0161e+01,
         -8.3095e+01, -3.8493e+01, -1.5680e+01, -3.8423e+01, -1.4582e+01],
        [-2.5164e+01, -3.3689e+01, -4.5959e+01, -3.3739e+01, -3.1845e+01,
         -5.9793e+01, -4.1081e+01, -1.8230e+01, -4.1472e+01, -1.2886e+01],
        [-1.3524e-01, -1.3041e+01, -8.1231e+00,  1.9998e+01, -8.6972e+01,
         -1.0203e+01, -1.6421e+01, -1.3233e+01, -3.8344e+00,  2.5499e+00],
        [-2.8095e+00, -1.1565e+02, -1.7482e+02,  7.9861e+00, -1.0794e+02,
         -2.4508e+02, -1.5951e+02, -7.2663e+01, -1.3910e+02,  4.9464e+00],
        [-3.2105e+00,  1.3125e+01,  4.8078e-01,  1.2182e+02,  6.1011e+01,
          1.7830e+00,  1.2204e+01,  5.0217e+01,  4.7424e+01, -2.8140e+01],
        [-2.5344e+01, -3.3660e+01, -8.6031e+01, -3.4688e+01, -2.9331e+01,
         -8.9560e+01, -3.7744e+01, -1.5118e+01, -3.7256e+01, -1.4343e+01],
        [-3.3109e+00, -8.9140e+01, -2.6773e+01, -2.9300e+01, -3.3562e+01,
         -3.1914e+01, -1.0322e+02, -5.8385e+01, -1.0623e+02, -3.4126e+01],
        [-3.8950e+00, -5.8761e+01,  4.4091e+00, -2.7344e+01, -3.1049e+01,
          2.0845e+00, -7.0172e+01, -8.0942e+01, -1.2829e+02, -3.9021e+01]],
       device='cuda:0')), ('model.output_layer_3.weight', tensor([[-2.8714e-01, -8.7793e-01,  3.8921e+00,  2.2531e+00,  1.5245e+00,
         -7.1637e+00, -8.0954e+00,  4.0634e+00, -3.3238e+01, -3.3042e+01],
        [ 7.7916e-03,  6.1158e-01, -3.6792e+00, -1.6954e+00, -1.6754e+00,
          7.5607e+00,  7.7900e+00, -3.9370e+00,  3.3502e+01,  3.2830e+01]],
       device='cuda:0'))])
loaded xi:  26.749016
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -2873.979243471933
objective value function right now is: -2873.979243471933
4.0% of gradient descent iterations done. Method = Adam
updated min: -2875.9197869800637
objective value function right now is: -2875.9197869800637
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2873.4701338871737
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2875.7431479886545
10.0% of gradient descent iterations done. Method = Adam
updated min: -2875.9272413075305
objective value function right now is: -2875.9272413075305
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2874.6573423732307
14.000000000000002% of gradient descent iterations done. Method = Adam
updated min: -2877.0060006619647
objective value function right now is: -2877.0060006619647
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2872.912789693447
18.0% of gradient descent iterations done. Method = Adam
updated min: -2877.150232017169
objective value function right now is: -2877.150232017169
20.0% of gradient descent iterations done. Method = Adam
updated min: -2877.6198781519684
objective value function right now is: -2877.6198781519684
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.2595903167544
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.614697861465
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.2737901635187
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -2875.4121730508086
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2877.5722110635716
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2875.7999383172664
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.7662262159442
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2874.9549564399904
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2877.4124781789174
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2877.1202003135395
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2875.7098899310663
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2871.6521790485235
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2873.4944990928516
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2874.1896953303153
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2874.5644367762957
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2872.4822667520893
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.5111682142388
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -2874.842199069906
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.9147984410024
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.794830683098
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.403335457329
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.5445029638504
66.0% of gradient descent iterations done. Method = Adam
updated min: -2877.8825992710413
objective value function right now is: -2877.8825992710413
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2875.776680417734
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2874.803222240799
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2874.470545759473
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2873.8875309082923
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2871.3980425398317
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2877.8060282154693
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2874.7494435189933
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.0418253062526
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2875.13589155442
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.0998631417847
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.987372484456
updated min: 
obj fun: -2878.6230836951
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2874.3957052384844
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.804915658703
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.320640822743
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2874.3709114257153
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.5859530884914
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.2589913172665
NN weights: [-2.87139237e-01 -8.77931058e-01  3.89213252e+00  2.25306535e+00
  1.52449667e+00 -7.16366816e+00 -8.09536839e+00  4.06335497e+00
 -3.32377510e+01 -3.30423622e+01  7.79157225e-03  6.11582339e-01
 -3.67915177e+00 -1.69536972e+00 -1.67535031e+00  7.56067324e+00
  7.79004955e+00 -3.93702626e+00  3.35018501e+01  3.28304863e+01
 -2.34500713e+01 -3.51506042e+01 -3.79676437e+01 -3.26750755e+01
 -3.45280151e+01 -5.71685066e+01 -4.56759834e+01 -2.20180473e+01
 -4.54817963e+01 -6.75134230e+00 -2.20825267e+00  2.36451492e+02
  1.18610802e+02  2.73679566e+00  1.44474091e+02  1.87682785e+02
  3.05129761e+02  3.99397858e+02  4.81443817e+02 -9.49265289e+00
 -2.54090061e+01 -3.35358772e+01 -7.83174057e+01 -3.39742050e+01
 -3.01613121e+01 -8.30949097e+01 -3.84931107e+01 -1.56803370e+01
 -3.84228821e+01 -1.45817690e+01 -2.51636524e+01 -3.36886902e+01
 -4.59588051e+01 -3.37390404e+01 -3.18450603e+01 -5.97929573e+01
 -4.10811157e+01 -1.82296715e+01 -4.14720497e+01 -1.28861971e+01
 -1.35241434e-01 -1.30406370e+01 -8.12310505e+00  1.99980850e+01
 -8.69721451e+01 -1.02031231e+01 -1.64214516e+01 -1.32329121e+01
 -3.83439565e+00  2.54991388e+00 -2.80954814e+00 -1.15648514e+02
 -1.74820190e+02  7.98605967e+00 -1.07943756e+02 -2.45083481e+02
 -1.59507874e+02 -7.26625977e+01 -1.39104477e+02  4.94643736e+00
 -3.21052313e+00  1.31247349e+01  4.80779886e-01  1.21820137e+02
  6.10110512e+01  1.78300846e+00  1.22040081e+01  5.02168083e+01
  4.74239006e+01 -2.81401501e+01 -2.53439465e+01 -3.36601105e+01
 -8.60305481e+01 -3.46877365e+01 -2.93305950e+01 -8.95601196e+01
 -3.77443161e+01 -1.51180067e+01 -3.72564468e+01 -1.43432083e+01
 -3.31088257e+00 -8.91402588e+01 -2.67733631e+01 -2.92995358e+01
 -3.35619049e+01 -3.19142399e+01 -1.03221756e+02 -5.83847961e+01
 -1.06230515e+02 -3.41260605e+01 -3.89497542e+00 -5.87606850e+01
  4.40905762e+00 -2.73435802e+01 -3.10491772e+01  2.08448696e+00
 -7.01724701e+01 -8.09419250e+01 -1.28285202e+02 -3.90209236e+01
 -1.92471733e+01 -2.40568485e+01 -2.45321679e+00  2.76806259e+00
  7.46563792e-01  3.00478530e+00 -1.26748436e+02 -1.68405461e+00
 -4.41645393e+01  3.23020291e+00  1.17370832e+00  2.98274374e+00
 -2.32203841e+00  2.78423238e+00 -3.41538506e+01  7.05235243e+00
 -3.35585823e+01  7.46466970e+00 -8.73098373e+00 -1.65116870e+00]
Minimum obj value:-2878.622008652622
Optimal xi: 26.801292
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1508.0187845655769
W_T_median: 1344.9728929742105
W_T_pctile_5: 717.3773700202984
W_T_CVAR_5_pct: 616.6051361615613
F value: -2878.622008652622
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
F value: -2878.622008652622
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ -19.2472,  -24.0568],
        [  -2.4532,    2.7681],
        [   0.7466,    3.0048],
        [-126.7484,   -1.6841],
        [ -44.1645,    3.2302],
        [   1.1737,    2.9827],
        [  -2.3220,    2.7842],
        [ -34.1539,    7.0524],
        [ -33.5586,    7.4647],
        [  -8.7310,   -1.6512]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[-2.3450e+01, -3.5151e+01, -3.7968e+01, -3.2675e+01, -3.4528e+01,
         -5.7169e+01, -4.5676e+01, -2.2018e+01, -4.5482e+01, -6.7513e+00],
        [-2.2083e+00,  2.3645e+02,  1.1861e+02,  2.7368e+00,  1.4447e+02,
          1.8768e+02,  3.0513e+02,  3.9940e+02,  4.8144e+02, -9.4927e+00],
        [-2.5409e+01, -3.3536e+01, -7.8317e+01, -3.3974e+01, -3.0161e+01,
         -8.3095e+01, -3.8493e+01, -1.5680e+01, -3.8423e+01, -1.4582e+01],
        [-2.5164e+01, -3.3689e+01, -4.5959e+01, -3.3739e+01, -3.1845e+01,
         -5.9793e+01, -4.1081e+01, -1.8230e+01, -4.1472e+01, -1.2886e+01],
        [-1.3524e-01, -1.3041e+01, -8.1231e+00,  1.9998e+01, -8.6972e+01,
         -1.0203e+01, -1.6421e+01, -1.3233e+01, -3.8344e+00,  2.5499e+00],
        [-2.8095e+00, -1.1565e+02, -1.7482e+02,  7.9861e+00, -1.0794e+02,
         -2.4508e+02, -1.5951e+02, -7.2663e+01, -1.3910e+02,  4.9464e+00],
        [-3.2105e+00,  1.3125e+01,  4.8078e-01,  1.2182e+02,  6.1011e+01,
          1.7830e+00,  1.2204e+01,  5.0217e+01,  4.7424e+01, -2.8140e+01],
        [-2.5344e+01, -3.3660e+01, -8.6031e+01, -3.4688e+01, -2.9331e+01,
         -8.9560e+01, -3.7744e+01, -1.5118e+01, -3.7256e+01, -1.4343e+01],
        [-3.3109e+00, -8.9140e+01, -2.6773e+01, -2.9300e+01, -3.3562e+01,
         -3.1914e+01, -1.0322e+02, -5.8385e+01, -1.0623e+02, -3.4126e+01],
        [-3.8950e+00, -5.8761e+01,  4.4091e+00, -2.7344e+01, -3.1049e+01,
          2.0845e+00, -7.0172e+01, -8.0942e+01, -1.2829e+02, -3.9021e+01]],
       device='cuda:0')), ('model.output_layer_3.weight', tensor([[-2.8714e-01, -8.7793e-01,  3.8921e+00,  2.2531e+00,  1.5245e+00,
         -7.1637e+00, -8.0954e+00,  4.0634e+00, -3.3238e+01, -3.3042e+01],
        [ 7.7916e-03,  6.1158e-01, -3.6792e+00, -1.6954e+00, -1.6754e+00,
          7.5607e+00,  7.7900e+00, -3.9370e+00,  3.3502e+01,  3.2830e+01]],
       device='cuda:0'))])
loaded xi:  26.801292
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -3028.929503486354
objective value function right now is: -3028.929503486354
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3024.610414702512
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3027.3134635358842
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3027.893972206804
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3025.3925364295255
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3027.031709510008
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -3028.174615386855
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3027.2317440232523
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3027.0011759927356
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3026.1762372877465
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3027.7619709743626
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3028.4531471240316
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3025.907277838611
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -3025.9407926327494
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3024.1163665585623
32.0% of gradient descent iterations done. Method = Adam
updated min: -3029.0695811129276
objective value function right now is: -3029.0695811129276
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3024.454280586779
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3029.009713348335
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3027.458840136001
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3026.0749056342897
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3027.2894546227662
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3025.366323304959
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3027.5195686497445
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3026.7243109310466
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3025.0416939350803
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3027.3864769860797
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3026.8906482656303
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -3027.0667972915585
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -3026.8150612535837
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3025.1679349338865
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3010.031374225276
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3015.831393274479
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3017.146121570579
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3027.2039189485913
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3021.90938209871
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3025.621584965257
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3028.171623880099
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3028.1998526375646
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3027.724172057062
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3026.174212601752
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3022.590389470931
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3026.6438896345135
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3028.014741550548
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3028.5651568598046
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3026.1363674979734
updated min: 
obj fun: -3029.0812309762996
updated min: 
obj fun: -3029.142794256267
updated min: 
obj fun: -3029.2049699077184
updated min: 
obj fun: -3029.2669190029424
updated min: 
obj fun: -3029.324240989664
updated min: 
obj fun: -3029.377697096351
updated min: 
obj fun: -3029.4257961889894
updated min: 
obj fun: -3029.466730033202
updated min: 
obj fun: -3029.5028617107273
updated min: 
obj fun: -3029.537361434107
updated min: 
obj fun: -3029.5722750916248
updated min: 
obj fun: -3029.608004980231
updated min: 
obj fun: -3029.6468231253803
updated min: 
obj fun: -3029.677555489365
updated min: 
obj fun: -3029.702534181363
updated min: 
obj fun: -3029.7239540273667
updated min: 
obj fun: -3029.7440102339583
updated min: 
obj fun: -3029.7667914255276
updated min: 
obj fun: -3029.7935604687377
updated min: 
obj fun: -3029.8213282383695
updated min: 
obj fun: -3029.847862351981
updated min: 
obj fun: -3029.869599144616
updated min: 
obj fun: -3029.884022564076
updated min: 
obj fun: -3029.8944280203273
updated min: 
obj fun: -3029.9013856571923
updated min: 
obj fun: -3029.9082810025507
updated min: 
obj fun: -3029.914409418863
updated min: 
obj fun: -3029.9209779587036
updated min: 
obj fun: -3029.9286601512813
updated min: 
obj fun: -3029.9370808583994
updated min: 
obj fun: -3029.9438723293338
updated min: 
obj fun: -3029.9470306777266
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3018.9377120991153
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3028.1352459961604
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3027.2895464161757
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3026.0207720282656
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3027.2842244418002
NN weights: [-7.97550380e-01 -8.68060529e-01  3.87213230e+00  2.08202958e+00
  1.49040639e+00 -7.38374233e+00 -8.21312428e+00  4.04776859e+00
 -3.30698318e+01 -3.42318230e+01  3.64950448e-01  5.92214108e-01
 -3.66591811e+00 -1.60978007e+00 -1.64083564e+00  7.78433371e+00
  7.14440298e+00 -3.92525721e+00  3.33339272e+01  3.40179291e+01
 -2.33083591e+01 -3.51472321e+01 -3.79573402e+01 -3.26750755e+01
 -3.45280151e+01 -5.71541901e+01 -4.56712952e+01 -2.20180473e+01
 -4.54817963e+01 -6.74982738e+00 -1.94915807e+00  2.38296707e+02
  1.36342361e+02 -1.43805563e+00  1.48389420e+02  2.03690598e+02
  3.06962982e+02  4.06542328e+02  4.88020020e+02 -8.07396984e+00
 -2.55261593e+01 -3.35366135e+01 -7.83189774e+01 -3.39742050e+01
 -3.01613121e+01 -8.30968170e+01 -3.84940186e+01 -1.56803370e+01
 -3.84228821e+01 -1.45780458e+01 -2.53921089e+01 -3.36973953e+01
 -4.59813499e+01 -3.37390404e+01 -3.18450603e+01 -5.98229980e+01
 -4.10926437e+01 -1.82296715e+01 -4.14720497e+01 -1.28871565e+01
 -3.37270796e-01 -1.83541756e+01 -8.67789555e+00  1.32956238e+01
 -1.06425667e+02 -1.03870573e+01 -2.19256897e+01 -2.43228302e+01
 -1.45600710e+01  2.65567160e+00 -2.99568295e+00 -1.35348541e+02
 -1.87447586e+02  6.37508154e+00 -1.14435837e+02 -2.57178223e+02
 -1.79008621e+02 -8.51441116e+01 -1.50985565e+02  4.88136196e+00
 -4.10357523e+00  2.55241451e+01  1.90062881e+00  7.60003281e+01
  4.60648155e+01  2.24132609e+00  2.47410793e+01  3.96436958e+01
  3.71199799e+01 -1.48052492e+01 -2.54545727e+01 -3.36604347e+01
 -8.60311890e+01 -3.46877365e+01 -2.93305950e+01 -8.95608978e+01
 -3.77447281e+01 -1.51180067e+01 -3.72564468e+01 -1.43381166e+01
 -3.98613667e+00 -8.09162598e+01 -2.07680187e+01 -2.92995358e+01
 -3.35619087e+01 -2.46174355e+01 -9.35331573e+01 -5.83847961e+01
 -1.06230515e+02 -3.75634346e+01 -3.17397380e+00 -5.88977814e+01
  1.23290849e+00 -2.73435802e+01 -3.10501881e+01  1.45379508e+00
 -6.81847687e+01 -8.09417267e+01 -1.28284592e+02 -4.16719208e+01
 -2.71314106e+01 -1.78043995e+01 -2.17347622e+00  2.80989504e+00
  4.26946253e-01  2.90660429e+00 -1.39873154e+02 -2.16489410e+00
 -5.73853073e+01  1.67760067e+01  6.87415242e-01  2.91652393e+00
 -1.93599963e+00  2.83865643e+00 -4.30866928e+01  2.02666283e+01
 -4.21100655e+01  2.04263458e+01 -9.87009239e+00 -1.83655286e+00]
Minimum obj value:-3029.944975317292
Optimal xi: 26.499569
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1518.413375614634
W_T_median: 1369.6210478577154
W_T_pctile_5: 703.4749270718856
W_T_CVAR_5_pct: 600.5018872757029
F value: -3029.944975317292
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.6
F value: -3029.944975317292
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ -27.1314,  -17.8044],
        [  -2.1735,    2.8099],
        [   0.4269,    2.9066],
        [-139.8732,   -2.1649],
        [ -57.3853,   16.7760],
        [   0.6874,    2.9165],
        [  -1.9360,    2.8387],
        [ -43.0867,   20.2666],
        [ -42.1101,   20.4263],
        [  -9.8701,   -1.8366]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[-2.3308e+01, -3.5147e+01, -3.7957e+01, -3.2675e+01, -3.4528e+01,
         -5.7154e+01, -4.5671e+01, -2.2018e+01, -4.5482e+01, -6.7498e+00],
        [-1.9492e+00,  2.3830e+02,  1.3634e+02, -1.4381e+00,  1.4839e+02,
          2.0369e+02,  3.0696e+02,  4.0654e+02,  4.8802e+02, -8.0740e+00],
        [-2.5526e+01, -3.3537e+01, -7.8319e+01, -3.3974e+01, -3.0161e+01,
         -8.3097e+01, -3.8494e+01, -1.5680e+01, -3.8423e+01, -1.4578e+01],
        [-2.5392e+01, -3.3697e+01, -4.5981e+01, -3.3739e+01, -3.1845e+01,
         -5.9823e+01, -4.1093e+01, -1.8230e+01, -4.1472e+01, -1.2887e+01],
        [-3.3727e-01, -1.8354e+01, -8.6779e+00,  1.3296e+01, -1.0643e+02,
         -1.0387e+01, -2.1926e+01, -2.4323e+01, -1.4560e+01,  2.6557e+00],
        [-2.9957e+00, -1.3535e+02, -1.8745e+02,  6.3751e+00, -1.1444e+02,
         -2.5718e+02, -1.7901e+02, -8.5144e+01, -1.5099e+02,  4.8814e+00],
        [-4.1036e+00,  2.5524e+01,  1.9006e+00,  7.6000e+01,  4.6065e+01,
          2.2413e+00,  2.4741e+01,  3.9644e+01,  3.7120e+01, -1.4805e+01],
        [-2.5455e+01, -3.3660e+01, -8.6031e+01, -3.4688e+01, -2.9331e+01,
         -8.9561e+01, -3.7745e+01, -1.5118e+01, -3.7256e+01, -1.4338e+01],
        [-3.9861e+00, -8.0916e+01, -2.0768e+01, -2.9300e+01, -3.3562e+01,
         -2.4617e+01, -9.3533e+01, -5.8385e+01, -1.0623e+02, -3.7563e+01],
        [-3.1740e+00, -5.8898e+01,  1.2329e+00, -2.7344e+01, -3.1050e+01,
          1.4538e+00, -6.8185e+01, -8.0942e+01, -1.2828e+02, -4.1672e+01]],
       device='cuda:0')), ('model.output_layer_3.weight', tensor([[ -0.7976,  -0.8681,   3.8721,   2.0820,   1.4904,  -7.3837,  -8.2131,
           4.0478, -33.0698, -34.2318],
        [  0.3650,   0.5922,  -3.6659,  -1.6098,  -1.6408,   7.7843,   7.1444,
          -3.9253,  33.3339,  34.0179]], device='cuda:0'))])
loaded xi:  26.499569
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -3178.912282080874
objective value function right now is: -3178.912282080874
4.0% of gradient descent iterations done. Method = Adam
updated min: -3180.717109012496
objective value function right now is: -3180.717109012496
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3179.378789739268
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3175.2172447137364
10.0% of gradient descent iterations done. Method = Adam
updated min: -3181.4325038896445
objective value function right now is: -3181.4325038896445
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3174.296357744211
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -3179.5357383951223
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3179.33188259243
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3176.4040687506167
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3177.16120978619
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3179.7608009232526
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3173.5412446223436
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3180.517764383953
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -3178.6414104745286
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3179.9322797103105
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3165.903709317338
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3179.681371423432
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3178.3984126861965
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3178.824386047967
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3179.2939637945874
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3178.8408781108164
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3171.6216511944544
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3177.8819924872564
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3173.823608497195
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3179.1295312759376
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3179.742984277053
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3175.6327754400813
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -3179.3655635712175
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -3177.635145207948
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3180.211246343778
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3178.0465868650695
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3179.5529236067796
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3176.512833222696
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3180.8296926540647
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3176.467580183361
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3179.0131286771707
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3178.4395964109126
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3166.2298323841806
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3179.8071439619357
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3177.597821989383
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3178.8024158350117
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3179.1314367712266
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3180.149696500427
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3177.639625959334
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3180.1092485301624
updated min: 
obj fun: -3181.4351023754775
updated min: 
obj fun: -3181.437802922832
updated min: 
obj fun: -3181.44343265068
updated min: 
obj fun: -3181.4467297512288
updated min: 
obj fun: -3181.447477562534
updated min: 
obj fun: -3181.457649881088
updated min: 
obj fun: -3181.4747858778305
updated min: 
obj fun: -3181.4911944951955
updated min: 
obj fun: -3181.505758405606
updated min: 
obj fun: -3181.5191431149215
updated min: 
obj fun: -3181.530246570816
updated min: 
obj fun: -3181.5429273817817
updated min: 
obj fun: -3181.5482503635267
updated min: 
obj fun: -3181.5514335444923
updated min: 
obj fun: -3181.5522647552443
updated min: 
obj fun: -3181.5522777456954
updated min: 
obj fun: -3181.552717104559
updated min: 
obj fun: -3181.5551394564177
updated min: 
obj fun: -3181.5570641239005
updated min: 
obj fun: -3181.5605769548815
updated min: 
obj fun: -3181.5662736977474
updated min: 
obj fun: -3181.5720993002783
updated min: 
obj fun: -3181.5800054606684
updated min: 
obj fun: -3181.5891221699962
updated min: 
obj fun: -3181.60235292688
updated min: 
obj fun: -3181.608773761994
updated min: 
obj fun: -3181.615093607448
updated min: 
obj fun: -3181.6208976307967
updated min: 
obj fun: -3181.626230418767
updated min: 
obj fun: -3181.6315426893607
updated min: 
obj fun: -3181.6377180793947
updated min: 
obj fun: -3181.6421308778713
updated min: 
obj fun: -3181.6497040012964
updated min: 
obj fun: -3181.651831092192
updated min: 
obj fun: -3181.6525068655087
updated min: 
obj fun: -3181.652527827602
updated min: 
obj fun: -3181.6543934904626
updated min: 
obj fun: -3181.658928019206
updated min: 
obj fun: -3181.6634785732895
updated min: 
obj fun: -3181.668042426568
updated min: 
obj fun: -3181.6719568442168
updated min: 
obj fun: -3181.6747314217996
updated min: 
obj fun: -3181.676565334223
updated min: 
obj fun: -3181.676942683352
updated min: 
obj fun: -3181.677800262113
updated min: 
obj fun: -3181.685664349311
updated min: 
obj fun: -3181.6941556720726
updated min: 
obj fun: -3181.7022953482783
updated min: 
obj fun: -3181.709187085878
updated min: 
obj fun: -3181.7155659065425
updated min: 
obj fun: -3181.7213501848887
updated min: 
obj fun: -3181.7265995950866
updated min: 
obj fun: -3181.7313887438922
updated min: 
obj fun: -3181.7354544211535
updated min: 
obj fun: -3181.739334912914
updated min: 
obj fun: -3181.743310433716
updated min: 
obj fun: -3181.747272946939
updated min: 
obj fun: -3181.7514584220075
updated min: 
obj fun: -3181.7558015308073
updated min: 
obj fun: -3181.7596883148058
updated min: 
obj fun: -3181.763125307114
updated min: 
obj fun: -3181.7663905312643
updated min: 
obj fun: -3181.7684926847764
updated min: 
obj fun: -3181.7698327696385
updated min: 
obj fun: -3181.770216148561
updated min: 
obj fun: -3181.775089091135
updated min: 
obj fun: -3181.7777719154114
updated min: 
obj fun: -3181.782024744696
updated min: 
obj fun: -3181.7865550517045
updated min: 
obj fun: -3181.7903243181313
updated min: 
obj fun: -3181.792675660716
updated min: 
obj fun: -3181.793553086568
updated min: 
obj fun: -3181.7943344860514
updated min: 
obj fun: -3181.7965328828627
updated min: 
obj fun: -3181.7987617671274
updated min: 
obj fun: -3181.800798343771
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3180.6355242987124
updated min: 
obj fun: -3181.802510684724
updated min: 
obj fun: -3181.8038702755994
updated min: 
obj fun: -3181.8048327743895
updated min: 
obj fun: -3181.8053471023463
updated min: 
obj fun: -3181.80540276117
updated min: 
obj fun: -3181.807330996288
updated min: 
obj fun: -3181.8089220031566
updated min: 
obj fun: -3181.810286441768
updated min: 
obj fun: -3181.8113082542563
updated min: 
obj fun: -3181.81195193767
updated min: 
obj fun: -3181.8122147010613
updated min: 
obj fun: -3181.812696108907
updated min: 
obj fun: -3181.81328382188
updated min: 
obj fun: -3181.8138316084464
updated min: 
obj fun: -3181.8142538564234
updated min: 
obj fun: -3181.8144005276367
updated min: 
obj fun: -3181.816860904437
updated min: 
obj fun: -3181.8172653752995
updated min: 
obj fun: -3181.817850208255
updated min: 
obj fun: -3181.818492357383
updated min: 
obj fun: -3181.8192224019363
updated min: 
obj fun: -3181.820110883595
updated min: 
obj fun: -3181.821095137164
updated min: 
obj fun: -3181.8219130975203
updated min: 
obj fun: -3181.8225554433234
updated min: 
obj fun: -3181.8229571532697
updated min: 
obj fun: -3181.8233097879042
updated min: 
obj fun: -3181.8236808175966
updated min: 
obj fun: -3181.824129317972
updated min: 
obj fun: -3181.824669414345
updated min: 
obj fun: -3181.8254948477634
updated min: 
obj fun: -3181.82656591492
updated min: 
obj fun: -3181.8278967481265
updated min: 
obj fun: -3181.8292582167937
updated min: 
obj fun: -3181.830134217788
updated min: 
obj fun: -3181.8306701669244
updated min: 
obj fun: -3181.83077937927
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3179.6191862016403
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3179.2659560015745
updated min: 
obj fun: -3181.8326042711765
updated min: 
obj fun: -3181.8342014911755
updated min: 
obj fun: -3181.8352089458963
updated min: 
obj fun: -3181.835577845662
updated min: 
obj fun: -3181.8356798464774
updated min: 
obj fun: -3181.8361446717254
updated min: 
obj fun: -3181.8366596192923
updated min: 
obj fun: -3181.837268156354
updated min: 
obj fun: -3181.8378661401216
updated min: 
obj fun: -3181.838327026248
updated min: 
obj fun: -3181.838544259131
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3176.7823504098424
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3180.3058127152967
NN weights: [-2.46068382e+00 -8.96170437e-01  3.73672962e+00  1.74597359e+00
  1.42304158e+00 -7.79800415e+00 -7.14974737e+00  3.94401598e+00
 -3.48752670e+01 -3.47624283e+01  5.61853647e-01  6.16154134e-01
 -3.59397936e+00 -2.17830396e+00 -1.57298374e+00  8.20251846e+00
  5.67814684e+00 -3.85735655e+00  3.51394196e+01  3.45458260e+01
 -2.17460537e+01 -3.51720619e+01 -3.80699806e+01 -3.26750755e+01
 -3.45280151e+01 -5.72658577e+01 -4.56987610e+01 -2.20180473e+01
 -4.54817963e+01 -6.80103254e+00 -3.07643247e+00  2.41865036e+02
  1.49052048e+02 -4.35642767e+00  1.48389420e+02  2.14886185e+02
  3.09730042e+02  4.06542328e+02  4.88020020e+02 -6.28645229e+00
 -2.60365582e+01 -3.35398331e+01 -7.83315048e+01 -3.39742050e+01
 -3.01613121e+01 -8.31104202e+01 -3.84981728e+01 -1.56803370e+01
 -3.84228821e+01 -1.45844221e+01 -2.62091103e+01 -3.36747360e+01
 -4.58673058e+01 -3.37390404e+01 -3.18450603e+01 -5.97057571e+01
 -4.10611076e+01 -1.82296715e+01 -4.14720497e+01 -1.28819351e+01
 -3.90673399e-01 -2.25565624e+01 -1.00213709e+01  1.50939159e+01
 -1.09006302e+02 -1.17258244e+01 -2.54279480e+01 -2.48373928e+01
 -1.50446253e+01  2.86335659e+00 -2.95973182e+00 -1.55948578e+02
 -2.01262360e+02  6.29881477e+00 -1.14435837e+02 -2.70927551e+02
 -1.99943817e+02 -8.51441116e+01 -1.50985565e+02  4.67300272e+00
 -6.01323891e+00  2.72446709e+01  6.99877143e-01  4.03884087e+01
  3.00662823e+01  1.01297569e+00  2.68473434e+01  2.50830708e+01
  2.26151791e+01 -6.98034573e+00 -2.58900833e+01 -3.36634827e+01
 -8.60434113e+01 -3.46877365e+01 -2.93305950e+01 -8.95739136e+01
 -3.77487984e+01 -1.51180067e+01 -3.72564468e+01 -1.43424406e+01
 -3.01642704e+00 -8.82974243e+01 -2.78796062e+01 -2.92995358e+01
 -3.35619087e+01 -3.15643005e+01 -9.94831772e+01 -5.83847961e+01
 -1.06230515e+02 -4.21691780e+01 -3.32870674e+00 -5.06317596e+01
  3.02098656e+00 -2.73435802e+01 -3.10501881e+01  3.68637419e+00
 -5.82845421e+01 -8.09417267e+01 -1.28284592e+02 -4.31439209e+01
 -2.80652637e+01 -1.84429398e+01 -2.25021291e+00  2.97741079e+00
  7.64583290e-01  2.87057185e+00 -1.49974487e+02 -2.63679385e+00
 -7.36701736e+01  3.14879608e+01  9.27868485e-01  2.86366081e+00
 -1.93667686e+00  2.96507406e+00 -5.79445534e+01  3.56706696e+01
 -5.68179817e+01  3.59202194e+01 -1.10422688e+01 -2.11259341e+00]
Minimum obj value:-3181.8387759921957
Optimal xi: 26.42687
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1520.3699883010888
W_T_median: 1372.6598578346916
W_T_pctile_5: 699.8711757921872
W_T_CVAR_5_pct: 597.2377858112517
F value: -3181.8387759921957
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.7
F value: -3181.8387759921957
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ -28.0653,  -18.4429],
        [  -2.2502,    2.9774],
        [   0.7646,    2.8706],
        [-149.9745,   -2.6368],
        [ -73.6702,   31.4880],
        [   0.9279,    2.8637],
        [  -1.9367,    2.9651],
        [ -57.9446,   35.6707],
        [ -56.8180,   35.9202],
        [ -11.0423,   -2.1126]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[-2.1746e+01, -3.5172e+01, -3.8070e+01, -3.2675e+01, -3.4528e+01,
         -5.7266e+01, -4.5699e+01, -2.2018e+01, -4.5482e+01, -6.8010e+00],
        [-3.0764e+00,  2.4187e+02,  1.4905e+02, -4.3564e+00,  1.4839e+02,
          2.1489e+02,  3.0973e+02,  4.0654e+02,  4.8802e+02, -6.2865e+00],
        [-2.6037e+01, -3.3540e+01, -7.8332e+01, -3.3974e+01, -3.0161e+01,
         -8.3110e+01, -3.8498e+01, -1.5680e+01, -3.8423e+01, -1.4584e+01],
        [-2.6209e+01, -3.3675e+01, -4.5867e+01, -3.3739e+01, -3.1845e+01,
         -5.9706e+01, -4.1061e+01, -1.8230e+01, -4.1472e+01, -1.2882e+01],
        [-3.9067e-01, -2.2557e+01, -1.0021e+01,  1.5094e+01, -1.0901e+02,
         -1.1726e+01, -2.5428e+01, -2.4837e+01, -1.5045e+01,  2.8634e+00],
        [-2.9597e+00, -1.5595e+02, -2.0126e+02,  6.2988e+00, -1.1444e+02,
         -2.7093e+02, -1.9994e+02, -8.5144e+01, -1.5099e+02,  4.6730e+00],
        [-6.0132e+00,  2.7245e+01,  6.9988e-01,  4.0388e+01,  3.0066e+01,
          1.0130e+00,  2.6847e+01,  2.5083e+01,  2.2615e+01, -6.9803e+00],
        [-2.5890e+01, -3.3663e+01, -8.6043e+01, -3.4688e+01, -2.9331e+01,
         -8.9574e+01, -3.7749e+01, -1.5118e+01, -3.7256e+01, -1.4342e+01],
        [-3.0164e+00, -8.8297e+01, -2.7880e+01, -2.9300e+01, -3.3562e+01,
         -3.1564e+01, -9.9483e+01, -5.8385e+01, -1.0623e+02, -4.2169e+01],
        [-3.3287e+00, -5.0632e+01,  3.0210e+00, -2.7344e+01, -3.1050e+01,
          3.6864e+00, -5.8285e+01, -8.0942e+01, -1.2828e+02, -4.3144e+01]],
       device='cuda:0')), ('model.output_layer_3.weight', tensor([[ -2.4607,  -0.8962,   3.7367,   1.7460,   1.4230,  -7.7980,  -7.1497,
           3.9440, -34.8753, -34.7624],
        [  0.5619,   0.6162,  -3.5940,  -2.1783,  -1.5730,   8.2025,   5.6781,
          -3.8574,  35.1394,  34.5458]], device='cuda:0'))])
loaded xi:  26.42687
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -3331.836277393208
objective value function right now is: -3331.836277393208
4.0% of gradient descent iterations done. Method = Adam
updated min: -3333.0212339166246
objective value function right now is: -3333.0212339166246
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3332.5285673743747
8.0% of gradient descent iterations done. Method = Adam
updated min: -3333.224243152503
objective value function right now is: -3333.224243152503
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3328.888872357525
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3331.446781214129
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -3332.5282734864736
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3332.0695136068016
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3331.261919115678
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3330.8526671513805
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3331.8645123132096
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3327.183045403178
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3332.1525830155465
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -3329.37284030801
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3322.463490099563
32.0% of gradient descent iterations done. Method = Adam
updated min: -3333.2290859807954
objective value function right now is: -3333.2290859807954
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3331.9157547115947
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3333.2244550501578
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3330.5494022178323
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3329.2230531438768
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3332.818867711542
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3332.7367093611297
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3328.895742756362
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3332.514330826237
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3330.4515007491646
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3331.8138679672034
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3333.0884363267915
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -3331.0551209596188
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -3331.9277840176806
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3330.7410720382754
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3328.141606714273
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3325.804101369739
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3329.7103284893783
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3331.958568797643
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3324.465230925877
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3330.922015561238
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3333.047357118325
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3330.6947589611805
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3330.1269229937393
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3332.0807870690855
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3330.84351774479
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3327.752225984286
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3332.297337864736
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3327.631253570524
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3332.098702908154
updated min: 
obj fun: -3333.2303696211093
updated min: 
obj fun: -3333.2359960786857
updated min: 
obj fun: -3333.240304604778
updated min: 
obj fun: -3333.243519236506
updated min: 
obj fun: -3333.24551403883
updated min: 
obj fun: -3333.2466196311334
updated min: 
obj fun: -3333.2469513394285
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3331.9368598298356
updated min: 
obj fun: -3333.247726709691
updated min: 
obj fun: -3333.2510698851283
updated min: 
obj fun: -3333.2548463233143
updated min: 
obj fun: -3333.259133772539
updated min: 
obj fun: -3333.2638297691838
updated min: 
obj fun: -3333.2691127874473
updated min: 
obj fun: -3333.2749543479963
updated min: 
obj fun: -3333.2809876928172
updated min: 
obj fun: -3333.2865962053324
updated min: 
obj fun: -3333.291650081847
updated min: 
obj fun: -3333.2958614370714
updated min: 
obj fun: -3333.2993501860115
updated min: 
obj fun: -3333.302380113778
updated min: 
obj fun: -3333.3051247840804
updated min: 
obj fun: -3333.307924134817
updated min: 
obj fun: -3333.310640132222
updated min: 
obj fun: -3333.3131843677256
updated min: 
obj fun: -3333.3157055206625
updated min: 
obj fun: -3333.3179145138192
updated min: 
obj fun: -3333.3205395156874
updated min: 
obj fun: -3333.323266606115
updated min: 
obj fun: -3333.3261676617517
updated min: 
obj fun: -3333.3287130993035
updated min: 
obj fun: -3333.330944723103
updated min: 
obj fun: -3333.3330821688705
updated min: 
obj fun: -3333.3347892502843
updated min: 
obj fun: -3333.3361172529617
updated min: 
obj fun: -3333.337422460411
updated min: 
obj fun: -3333.338673513778
updated min: 
obj fun: -3333.3398113881904
updated min: 
obj fun: -3333.340933569959
updated min: 
obj fun: -3333.3422287159574
updated min: 
obj fun: -3333.343901079284
updated min: 
obj fun: -3333.3455061675663
updated min: 
obj fun: -3333.3467084276126
updated min: 
obj fun: -3333.3475279282993
updated min: 
obj fun: -3333.348431525565
updated min: 
obj fun: -3333.3497190360736
updated min: 
obj fun: -3333.3513030435697
updated min: 
obj fun: -3333.353005956832
updated min: 
obj fun: -3333.35439480708
updated min: 
obj fun: -3333.3553091845915
updated min: 
obj fun: -3333.3556878511645
updated min: 
obj fun: -3333.3567433188427
updated min: 
obj fun: -3333.3586161272624
updated min: 
obj fun: -3333.361308197921
updated min: 
obj fun: -3333.3640698569166
updated min: 
obj fun: -3333.3662810029796
updated min: 
obj fun: -3333.3680130947596
updated min: 
obj fun: -3333.369185554412
updated min: 
obj fun: -3333.3702868577334
updated min: 
obj fun: -3333.3714896204347
updated min: 
obj fun: -3333.3731363051866
updated min: 
obj fun: -3333.3753238645495
updated min: 
obj fun: -3333.3784240532846
updated min: 
obj fun: -3333.3820113407223
updated min: 
obj fun: -3333.38567721807
updated min: 
obj fun: -3333.389164912605
updated min: 
obj fun: -3333.3924145206165
updated min: 
obj fun: -3333.395186619388
updated min: 
obj fun: -3333.3970502378374
updated min: 
obj fun: -3333.397655857635
updated min: 
obj fun: -3333.3978828375825
updated min: 
obj fun: -3333.401636049771
updated min: 
obj fun: -3333.40468500025
updated min: 
obj fun: -3333.406901053954
updated min: 
obj fun: -3333.408605660631
updated min: 
obj fun: -3333.4096915360046
updated min: 
obj fun: -3333.4103320806244
updated min: 
obj fun: -3333.41074166293
updated min: 
obj fun: -3333.4114067860733
updated min: 
obj fun: -3333.412602154795
updated min: 
obj fun: -3333.4143201787324
updated min: 
obj fun: -3333.4164451680967
updated min: 
obj fun: -3333.418497405356
updated min: 
obj fun: -3333.4202981985127
updated min: 
obj fun: -3333.4215938613956
updated min: 
obj fun: -3333.422344766903
updated min: 
obj fun: -3333.4225755026573
updated min: 
obj fun: -3333.4236637499043
updated min: 
obj fun: -3333.4257308472547
updated min: 
obj fun: -3333.42802231323
updated min: 
obj fun: -3333.430023426827
updated min: 
obj fun: -3333.431464432592
updated min: 
obj fun: -3333.432356436389
updated min: 
obj fun: -3333.4325075443753
updated min: 
obj fun: -3333.4327419597766
updated min: 
obj fun: -3333.4336576327923
updated min: 
obj fun: -3333.434856821794
updated min: 
obj fun: -3333.4361276721825
updated min: 
obj fun: -3333.437384361403
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3330.679732047445
updated min: 
obj fun: -3333.4385870636397
updated min: 
obj fun: -3333.439609376452
updated min: 
obj fun: -3333.440394855301
updated min: 
obj fun: -3333.4409279471333
updated min: 
obj fun: -3333.441197706635
updated min: 
obj fun: -3333.4412735797177
updated min: 
obj fun: -3333.4413064446403
updated min: 
obj fun: -3333.4413410615716
updated min: 
obj fun: -3333.4414657017114
updated min: 
obj fun: -3333.4415330861248
updated min: 
obj fun: -3333.4417592249924
updated min: 
obj fun: -3333.44260907703
updated min: 
obj fun: -3333.4437112885535
updated min: 
obj fun: -3333.445092411047
updated min: 
obj fun: -3333.4466438977925
updated min: 
obj fun: -3333.448056925807
updated min: 
obj fun: -3333.4491616634614
updated min: 
obj fun: -3333.4498285364693
updated min: 
obj fun: -3333.4501448157225
updated min: 
obj fun: -3333.4502903583034
updated min: 
obj fun: -3333.4504264230923
updated min: 
obj fun: -3333.4515596584483
updated min: 
obj fun: -3333.4547527433383
updated min: 
obj fun: -3333.458301344051
updated min: 
obj fun: -3333.4620570591765
updated min: 
obj fun: -3333.465758543474
updated min: 
obj fun: -3333.4692563465956
updated min: 
obj fun: -3333.472538629502
updated min: 
obj fun: -3333.475516846311
updated min: 
obj fun: -3333.4781942387913
updated min: 
obj fun: -3333.480611801203
updated min: 
obj fun: -3333.4827455935656
updated min: 
obj fun: -3333.4846505900873
updated min: 
obj fun: -3333.4863638877637
updated min: 
obj fun: -3333.48811974791
updated min: 
obj fun: -3333.4897695763616
updated min: 
obj fun: -3333.4912708364577
updated min: 
obj fun: -3333.4924858522595
updated min: 
obj fun: -3333.4933109441117
updated min: 
obj fun: -3333.4937789150085
updated min: 
obj fun: -3333.494038416698
updated min: 
obj fun: -3333.49421341392
updated min: 
obj fun: -3333.494353885248
updated min: 
obj fun: -3333.4945957681653
updated min: 
obj fun: -3333.494916054959
updated min: 
obj fun: -3333.4953544628324
updated min: 
obj fun: -3333.495872511848
updated min: 
obj fun: -3333.4964791048715
updated min: 
obj fun: -3333.497119280131
updated min: 
obj fun: -3333.497725546721
updated min: 
obj fun: -3333.49832999091
updated min: 
obj fun: -3333.498970077571
updated min: 
obj fun: -3333.4995814098747
updated min: 
obj fun: -3333.5000892761786
updated min: 
obj fun: -3333.5013953084685
updated min: 
obj fun: -3333.503647095461
updated min: 
obj fun: -3333.505799349359
updated min: 
obj fun: -3333.507685040649
updated min: 
obj fun: -3333.5093522156876
updated min: 
obj fun: -3333.510699100347
updated min: 
obj fun: -3333.5116783036947
updated min: 
obj fun: -3333.512403184194
updated min: 
obj fun: -3333.5132490949272
updated min: 
obj fun: -3333.5143039465
updated min: 
obj fun: -3333.515505188114
updated min: 
obj fun: -3333.5171233031783
updated min: 
obj fun: -3333.51893048211
updated min: 
obj fun: -3333.5209037930667
updated min: 
obj fun: -3333.5229895269003
updated min: 
obj fun: -3333.525037613373
updated min: 
obj fun: -3333.527004149374
updated min: 
obj fun: -3333.528840816721
updated min: 
obj fun: -3333.5306014735393
updated min: 
obj fun: -3333.532380022186
updated min: 
obj fun: -3333.5341730955065
updated min: 
obj fun: -3333.535985439866
updated min: 
obj fun: -3333.5378995948927
updated min: 
obj fun: -3333.53991762214
updated min: 
obj fun: -3333.5476403199987
updated min: 
obj fun: -3333.5499724354836
updated min: 
obj fun: -3333.55231282371
updated min: 
obj fun: -3333.554560545155
updated min: 
obj fun: -3333.556482438804
updated min: 
obj fun: -3333.5579463009285
updated min: 
obj fun: -3333.55897246299
updated min: 
obj fun: -3333.5595635258865
updated min: 
obj fun: -3333.5597374845634
updated min: 
obj fun: -3333.5597403461247
updated min: 
obj fun: -3333.5597864582414
updated min: 
obj fun: -3333.5598449307363
updated min: 
obj fun: -3333.5598911243483
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3331.020817207352
updated min: 
obj fun: -3333.560830935372
updated min: 
obj fun: -3333.5622755230174
updated min: 
obj fun: -3333.563674243126
updated min: 
obj fun: -3333.564874412657
updated min: 
obj fun: -3333.565837762531
updated min: 
obj fun: -3333.566570469591
updated min: 
obj fun: -3333.567139930605
updated min: 
obj fun: -3333.5676813206765
updated min: 
obj fun: -3333.568184475617
updated min: 
obj fun: -3333.568704666857
updated min: 
obj fun: -3333.569248669195
updated min: 
obj fun: -3333.569746601826
updated min: 
obj fun: -3333.5702553223678
updated min: 
obj fun: -3333.5706915170394
updated min: 
obj fun: -3333.5711266849544
updated min: 
obj fun: -3333.571429662187
updated min: 
obj fun: -3333.57177045877
updated min: 
obj fun: -3333.572117426916
updated min: 
obj fun: -3333.572444862336
updated min: 
obj fun: -3333.572747527617
updated min: 
obj fun: -3333.5729798835096
updated min: 
obj fun: -3333.573117373934
updated min: 
obj fun: -3333.5732484267287
updated min: 
obj fun: -3333.5733495241507
updated min: 
obj fun: -3333.573476864288
updated min: 
obj fun: -3333.573717991968
updated min: 
obj fun: -3333.5741534287586
updated min: 
obj fun: -3333.5747204823124
updated min: 
obj fun: -3333.5753072712973
updated min: 
obj fun: -3333.575920774299
updated min: 
obj fun: -3333.5766232971364
updated min: 
obj fun: -3333.577409699464
updated min: 
obj fun: -3333.57818654635
updated min: 
obj fun: -3333.5789376389066
updated min: 
obj fun: -3333.5796454146944
updated min: 
obj fun: -3333.5803543525244
updated min: 
obj fun: -3333.5811062378734
updated min: 
obj fun: -3333.581955133469
updated min: 
obj fun: -3333.582852134396
updated min: 
obj fun: -3333.583812342684
updated min: 
obj fun: -3333.584733259802
updated min: 
obj fun: -3333.585671258537
updated min: 
obj fun: -3333.5866373186414
updated min: 
obj fun: -3333.5876415685007
updated min: 
obj fun: -3333.5887676266434
updated min: 
obj fun: -3333.590001052279
updated min: 
obj fun: -3333.591384108663
updated min: 
obj fun: -3333.5928541747303
updated min: 
obj fun: -3333.594240964517
updated min: 
obj fun: -3333.595391484977
updated min: 
obj fun: -3333.596266254402
updated min: 
obj fun: -3333.5968481265495
updated min: 
obj fun: -3333.5971756273557
updated min: 
obj fun: -3333.5973742923893
updated min: 
obj fun: -3333.5976803849962
updated min: 
obj fun: -3333.5981737941543
updated min: 
obj fun: -3333.5988280655442
updated min: 
obj fun: -3333.599665190475
updated min: 
obj fun: -3333.600589953003
updated min: 
obj fun: -3333.601537811461
updated min: 
obj fun: -3333.6024799917436
updated min: 
obj fun: -3333.603370042499
updated min: 
obj fun: -3333.6041904014087
updated min: 
obj fun: -3333.6049780680064
updated min: 
obj fun: -3333.605760616841
updated min: 
obj fun: -3333.606423645868
updated min: 
obj fun: -3333.606864807626
updated min: 
obj fun: -3333.607187206806
updated min: 
obj fun: -3333.6075318575163
updated min: 
obj fun: -3333.607891203816
updated min: 
obj fun: -3333.6082323233827
updated min: 
obj fun: -3333.608533417197
updated min: 
obj fun: -3333.6087348886876
updated min: 
obj fun: -3333.608867563987
updated min: 
obj fun: -3333.6089663734697
updated min: 
obj fun: -3333.609026103218
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3330.4177841285336
updated min: 
obj fun: -3333.6099821979324
updated min: 
obj fun: -3333.6109839322353
updated min: 
obj fun: -3333.6118273505945
updated min: 
obj fun: -3333.6125487534505
updated min: 
obj fun: -3333.613114198364
updated min: 
obj fun: -3333.6136442655784
updated min: 
obj fun: -3333.614253759198
updated min: 
obj fun: -3333.6149878999668
updated min: 
obj fun: -3333.6157580038766
updated min: 
obj fun: -3333.6166018281865
updated min: 
obj fun: -3333.6175321238975
updated min: 
obj fun: -3333.618507738943
updated min: 
obj fun: -3333.619440523411
updated min: 
obj fun: -3333.620236920648
updated min: 
obj fun: -3333.620902776738
updated min: 
obj fun: -3333.6214131915394
updated min: 
obj fun: -3333.621704142961
updated min: 
obj fun: -3333.6218693528026
updated min: 
obj fun: -3333.6219347995857
updated min: 
obj fun: -3333.6219946711512
updated min: 
obj fun: -3333.6221185006334
updated min: 
obj fun: -3333.6222752074787
updated min: 
obj fun: -3333.6224599049056
updated min: 
obj fun: -3333.6226573914105
updated min: 
obj fun: -3333.6227973338796
updated min: 
obj fun: -3333.6229225768852
updated min: 
obj fun: -3333.6229812695674
updated min: 
obj fun: -3333.623027640166
updated min: 
obj fun: -3333.623456671832
updated min: 
obj fun: -3333.624150235537
updated min: 
obj fun: -3333.625026223748
updated min: 
obj fun: -3333.625927033311
updated min: 
obj fun: -3333.626845817477
updated min: 
obj fun: -3333.62767527018
updated min: 
obj fun: -3333.6283486807283
updated min: 
obj fun: -3333.629012486581
updated min: 
obj fun: -3333.6297129402437
updated min: 
obj fun: -3333.6304111004342
updated min: 
obj fun: -3333.631107353337
updated min: 
obj fun: -3333.6317851784497
updated min: 
obj fun: -3333.6324078907232
updated min: 
obj fun: -3333.6328965842813
updated min: 
obj fun: -3333.6331780652217
updated min: 
obj fun: -3333.63326580768
updated min: 
obj fun: -3333.6332871969707
updated min: 
obj fun: -3333.6365338208284
updated min: 
obj fun: -3333.636636443223
updated min: 
obj fun: -3333.6368891393877
updated min: 
obj fun: -3333.637202610331
updated min: 
obj fun: -3333.6376421552145
updated min: 
obj fun: -3333.6382523166453
updated min: 
obj fun: -3333.6390146734125
updated min: 
obj fun: -3333.6398147785535
updated min: 
obj fun: -3333.64061367861
updated min: 
obj fun: -3333.6414479660643
updated min: 
obj fun: -3333.6421689550994
updated min: 
obj fun: -3333.6428030024535
updated min: 
obj fun: -3333.6433219243363
updated min: 
obj fun: -3333.6437948603602
updated min: 
obj fun: -3333.6441991189286
updated min: 
obj fun: -3333.644574254456
updated min: 
obj fun: -3333.6450120779555
updated min: 
obj fun: -3333.6455840392796
updated min: 
obj fun: -3333.646119186544
updated min: 
obj fun: -3333.6465504232933
updated min: 
obj fun: -3333.646918980118
updated min: 
obj fun: -3333.6471959238334
updated min: 
obj fun: -3333.647389537023
updated min: 
obj fun: -3333.6475555634283
updated min: 
obj fun: -3333.6476653745576
updated min: 
obj fun: -3333.6485054890722
updated min: 
obj fun: -3333.6495421505133
updated min: 
obj fun: -3333.650523251394
updated min: 
obj fun: -3333.651334257909
updated min: 
obj fun: -3333.651922803793
updated min: 
obj fun: -3333.6522447772104
updated min: 
obj fun: -3333.652352344407
updated min: 
obj fun: -3333.652388638246
updated min: 
obj fun: -3333.6525974466385
updated min: 
obj fun: -3333.6529540867423
updated min: 
obj fun: -3333.65348920827
updated min: 
obj fun: -3333.654125527693
updated min: 
obj fun: -3333.654755038664
updated min: 
obj fun: -3333.655254115545
updated min: 
obj fun: -3333.6556156598162
updated min: 
obj fun: -3333.6558092190658
updated min: 
obj fun: -3333.6558501946884
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3332.2472454903464
NN weights: [-5.72342491e+00 -8.67791712e-01  3.79513860e+00  1.15764773e+00
  1.44892478e+00 -7.76839209e+00 -7.57666349e+00  4.14856243e+00
 -3.56333084e+01 -3.46189461e+01  3.04617405e+00  5.84511280e-01
 -3.94632053e+00 -2.88809705e+00 -1.59830594e+00  8.17712784e+00
  5.67965984e+00 -4.27081537e+00  3.58976936e+01  3.44000511e+01
 -1.86434078e+01 -3.52974739e+01 -3.73975449e+01 -3.26754684e+01
 -3.45280151e+01 -5.64280357e+01 -4.57000465e+01 -2.20180473e+01
 -4.54817963e+01 -7.83270979e+00 -3.78991199e+00  2.49029205e+02
  1.63576248e+02 -4.99162626e+00  1.48389420e+02  2.28298630e+02
  3.15472382e+02  4.06542328e+02  4.88020020e+02 -5.51000500e+00
 -2.68296204e+01 -3.34252586e+01 -7.82048035e+01 -3.39742050e+01
 -3.01613121e+01 -8.29954681e+01 -3.83813744e+01 -1.56803370e+01
 -3.84228821e+01 -1.43577824e+01 -2.73727188e+01 -3.38398476e+01
 -4.59169388e+01 -3.37390404e+01 -3.18450603e+01 -5.97524986e+01
 -4.12138824e+01 -1.82296715e+01 -4.14720497e+01 -1.33616753e+01
 -8.10413003e-01 -2.62592793e+01 -1.16666231e+01  1.20975065e+01
 -1.09006325e+02 -1.25695686e+01 -2.84089966e+01 -2.48373928e+01
 -1.50446253e+01  3.19406199e+00 -3.18107796e+00 -1.76426849e+02
 -2.14729172e+02  4.51103306e+00 -1.14435837e+02 -2.84072784e+02
 -2.20588074e+02 -8.51441116e+01 -1.50985565e+02  4.76685810e+00
 -6.62547255e+00  2.65503273e+01  2.70564270e+00  2.05004444e+01
  2.98737946e+01  3.08018446e+00  2.74767094e+01  2.50875015e+01
  2.26224079e+01 -5.95206213e+00 -2.66095772e+01 -3.34609833e+01
 -8.58174362e+01 -3.46877365e+01 -2.93305950e+01 -8.93560791e+01
 -3.75389557e+01 -1.51180067e+01 -3.72564468e+01 -1.39890871e+01
 -2.77698922e+00 -8.55401306e+01 -2.57144108e+01 -2.92995358e+01
 -3.35619087e+01 -2.86480179e+01 -9.50639191e+01 -5.83847961e+01
 -1.06230515e+02 -5.46458740e+01 -3.07468843e+00 -4.62035713e+01
 -3.89603525e-01 -2.73435802e+01 -3.10501881e+01  6.89563990e-01
 -5.29612885e+01 -8.09417267e+01 -1.28284592e+02 -5.14384079e+01
 -3.03499336e+01 -1.74289417e+01 -8.83533180e-01  2.87019658e+00
  1.07848203e+00  3.00246501e+00 -1.60375580e+02 -2.99528313e+00
 -7.38742065e+01  2.97889347e+01  1.19280171e+00  3.00919199e+00
 -6.32273078e-01  2.87866139e+00 -5.79425545e+01  3.52348824e+01
 -5.68140678e+01  3.55590744e+01 -1.18947582e+01 -2.31020117e+00]
Minimum obj value:-3333.653372205
Optimal xi: 26.278412
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1524.2247464918225
W_T_median: 1379.4499412456787
W_T_pctile_5: 692.0270208052623
W_T_CVAR_5_pct: 590.0741352117665
F value: -3333.653372205
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.8
F value: -3333.653372205
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ -30.3499,  -17.4289],
        [  -0.8835,    2.8702],
        [   1.0785,    3.0025],
        [-160.3756,   -2.9953],
        [ -73.8742,   29.7889],
        [   1.1928,    3.0092],
        [  -0.6323,    2.8787],
        [ -57.9426,   35.2349],
        [ -56.8141,   35.5591],
        [ -11.8948,   -2.3102]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[-1.8643e+01, -3.5297e+01, -3.7398e+01, -3.2675e+01, -3.4528e+01,
         -5.6428e+01, -4.5700e+01, -2.2018e+01, -4.5482e+01, -7.8327e+00],
        [-3.7899e+00,  2.4903e+02,  1.6358e+02, -4.9916e+00,  1.4839e+02,
          2.2830e+02,  3.1547e+02,  4.0654e+02,  4.8802e+02, -5.5100e+00],
        [-2.6830e+01, -3.3425e+01, -7.8205e+01, -3.3974e+01, -3.0161e+01,
         -8.2995e+01, -3.8381e+01, -1.5680e+01, -3.8423e+01, -1.4358e+01],
        [-2.7373e+01, -3.3840e+01, -4.5917e+01, -3.3739e+01, -3.1845e+01,
         -5.9752e+01, -4.1214e+01, -1.8230e+01, -4.1472e+01, -1.3362e+01],
        [-8.1041e-01, -2.6259e+01, -1.1667e+01,  1.2098e+01, -1.0901e+02,
         -1.2570e+01, -2.8409e+01, -2.4837e+01, -1.5045e+01,  3.1941e+00],
        [-3.1811e+00, -1.7643e+02, -2.1473e+02,  4.5110e+00, -1.1444e+02,
         -2.8407e+02, -2.2059e+02, -8.5144e+01, -1.5099e+02,  4.7669e+00],
        [-6.6255e+00,  2.6550e+01,  2.7056e+00,  2.0500e+01,  2.9874e+01,
          3.0802e+00,  2.7477e+01,  2.5088e+01,  2.2622e+01, -5.9521e+00],
        [-2.6610e+01, -3.3461e+01, -8.5817e+01, -3.4688e+01, -2.9331e+01,
         -8.9356e+01, -3.7539e+01, -1.5118e+01, -3.7256e+01, -1.3989e+01],
        [-2.7770e+00, -8.5540e+01, -2.5714e+01, -2.9300e+01, -3.3562e+01,
         -2.8648e+01, -9.5064e+01, -5.8385e+01, -1.0623e+02, -5.4646e+01],
        [-3.0747e+00, -4.6204e+01, -3.8960e-01, -2.7344e+01, -3.1050e+01,
          6.8956e-01, -5.2961e+01, -8.0942e+01, -1.2828e+02, -5.1438e+01]],
       device='cuda:0')), ('model.output_layer_3.weight', tensor([[ -5.7234,  -0.8678,   3.7951,   1.1576,   1.4489,  -7.7684,  -7.5767,
           4.1486, -35.6333, -34.6189],
        [  3.0462,   0.5845,  -3.9463,  -2.8881,  -1.5983,   8.1771,   5.6797,
          -4.2708,  35.8977,  34.4001]], device='cuda:0'))])
loaded xi:  26.278412
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -3484.9086272685186
objective value function right now is: -3484.9086272685186
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3482.425374328499
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3481.7743113222473
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3484.2551839947896
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3478.6201067428024
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3483.3278923005423
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -3483.17109242573
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3480.697457785033
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3482.780485557481
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3483.4899079075644
22.0% of gradient descent iterations done. Method = Adam
updated min: -3485.7724639371427
objective value function right now is: -3485.7724639371427
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3484.2126517066145
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3483.2913361674937
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -3485.0820361726314
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3478.945952611176
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3484.0497842908007
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3484.909500182232
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3485.205852941202
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3484.254842856237
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3481.4982802240243
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3482.1617505527706
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3484.896849941673
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3482.6542397255143
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3483.6617707231826
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3482.951136072176
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3485.5095294912003
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3481.935940285167
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -3482.3525178053187
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -3483.5439039209364
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3483.1494903380762
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3479.415952050903
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3483.8032817910575
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3482.7135272776363
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3482.1013185574247
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3473.448998759037
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3484.4092380588736
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3483.426519372305
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3480.8892521780813
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3483.77570915755
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3483.402265040864
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3476.712741163974
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3482.532910041446
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3483.2657264814025
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3483.6240294475265
updated min: 
obj fun: -3486.045572280368
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3485.3261561426225
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3483.72937306046
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3483.178511013045
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3485.777049038722
updated min: 
obj fun: -3486.0470163029386
updated min: 
obj fun: -3486.048483348107
updated min: 
obj fun: -3486.0496814034946
updated min: 
obj fun: -3486.050403575901
updated min: 
obj fun: -3486.0509065169217
updated min: 
obj fun: -3486.0514218718317
updated min: 
obj fun: -3486.052024292254
updated min: 
obj fun: -3486.05251909451
updated min: 
obj fun: -3486.052915410726
updated min: 
obj fun: -3486.053169346504
updated min: 
obj fun: -3486.053540506248
updated min: 
obj fun: -3486.053837090426
updated min: 
obj fun: -3486.0542870332424
updated min: 
obj fun: -3486.054768830442
updated min: 
obj fun: -3486.0552180424183
updated min: 
obj fun: -3486.0557181346926
updated min: 
obj fun: -3486.056183945897
updated min: 
obj fun: -3486.0568252192497
updated min: 
obj fun: -3486.057726544171
updated min: 
obj fun: -3486.058977694559
updated min: 
obj fun: -3486.0605598595494
updated min: 
obj fun: -3486.06224108191
updated min: 
obj fun: -3486.063713074858
updated min: 
obj fun: -3486.064537268133
updated min: 
obj fun: -3486.0647688526383
updated min: 
obj fun: -3486.065325185365
updated min: 
obj fun: -3486.0663545821144
updated min: 
obj fun: -3486.0672327014636
updated min: 
obj fun: -3486.0677164347926
updated min: 
obj fun: -3486.067929654441
updated min: 
obj fun: -3486.0700601055846
updated min: 
obj fun: -3486.071844997432
updated min: 
obj fun: -3486.0729558360777
updated min: 
obj fun: -3486.073132322995
updated min: 
obj fun: -3486.0736135287157
updated min: 
obj fun: -3486.0741657779804
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3485.4283498167406
updated min: 
obj fun: -3486.0754964639523
updated min: 
obj fun: -3486.0772599213824
updated min: 
obj fun: -3486.0787087800136
updated min: 
obj fun: -3486.079676999543
updated min: 
obj fun: -3486.0801952689326
updated min: 
obj fun: -3486.08044309627
updated min: 
obj fun: -3486.080631606457
updated min: 
obj fun: -3486.080814781748
updated min: 
obj fun: -3486.0811706350228
updated min: 
obj fun: -3486.081552165029
updated min: 
obj fun: -3486.0819239039756
updated min: 
obj fun: -3486.0822441389582
updated min: 
obj fun: -3486.0824500516374
updated min: 
obj fun: -3486.082664431328
updated min: 
obj fun: -3486.0832897381506
updated min: 
obj fun: -3486.0843113506853
updated min: 
obj fun: -3486.0855428306354
updated min: 
obj fun: -3486.0869038002747
updated min: 
obj fun: -3486.0884667025143
updated min: 
obj fun: -3486.0900517610244
updated min: 
obj fun: -3486.091427630632
updated min: 
obj fun: -3486.0924783350015
updated min: 
obj fun: -3486.0930725129956
updated min: 
obj fun: -3486.0930898098964
updated min: 
obj fun: -3486.093423869336
updated min: 
obj fun: -3486.0938555771645
updated min: 
obj fun: -3486.0941442809963
updated min: 
obj fun: -3486.094243731658
updated min: 
obj fun: -3486.0943163274187
updated min: 
obj fun: -3486.0943806774208
updated min: 
obj fun: -3486.0944270309337
updated min: 
obj fun: -3486.0944988745277
updated min: 
obj fun: -3486.0947862423
updated min: 
obj fun: -3486.095326777418
updated min: 
obj fun: -3486.096056690833
updated min: 
obj fun: -3486.09718593494
updated min: 
obj fun: -3486.0985068333625
updated min: 
obj fun: -3486.0998974174904
updated min: 
obj fun: -3486.101178788704
updated min: 
obj fun: -3486.1023305862473
updated min: 
obj fun: -3486.103049551478
updated min: 
obj fun: -3486.103198818455
updated min: 
obj fun: -3486.103551271133
updated min: 
obj fun: -3486.1043452379827
updated min: 
obj fun: -3486.10499319907
updated min: 
obj fun: -3486.1055935848467
updated min: 
obj fun: -3486.106094730525
updated min: 
obj fun: -3486.1063424293284
updated min: 
obj fun: -3486.1069650470777
updated min: 
obj fun: -3486.1078729749447
updated min: 
obj fun: -3486.108695151265
updated min: 
obj fun: -3486.1094222463707
updated min: 
obj fun: -3486.110007793895
updated min: 
obj fun: -3486.110334727348
updated min: 
obj fun: -3486.110433252827
updated min: 
obj fun: -3486.110493111049
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3485.1940052615714
NN weights: [  -4.372382     -0.8061677     3.7707288     1.2450818     1.4850769
   -7.453321     -7.640104      4.119932    -35.85828     -35.146763
    1.2023463     0.52647847   -4.0436935    -3.9929323    -1.6338241
    7.8668227     5.9748597    -4.3171725    36.12303      34.92594
  -21.083382    -35.398766    -37.64949     -32.6755      -34.528015
  -56.664883    -45.87965     -22.018047    -45.481796    -10.549525
   -4.4843435   256.00046     174.4284       -3.6216967   148.38942
  238.20097     321.56122     406.54233     488.02002      -4.308068
  -26.947433    -33.41563     -78.184525    -33.974205    -30.161312
  -82.97435     -38.36908     -15.680337    -38.422882    -14.357197
  -27.275074    -33.7422      -45.70337     -33.73904     -31.84506
  -59.528812    -41.088985    -18.229671    -41.47205     -13.344873
   -0.9997704   -30.79375     -14.666511      8.984684   -109.006325
  -15.466502    -33.49532     -24.837393    -15.044625      3.3040998
   -2.9676573  -193.90994    -227.36267       4.2748575  -114.43584
 -296.3193     -237.85757     -85.14411    -150.98557       4.5508966
   -6.576203     26.022223      5.045998      2.1920812    29.876547
    5.729048     27.88984      25.0917       22.626305     -3.571366
  -26.747719    -33.455784    -85.80633     -34.687737    -29.330595
  -89.34429     -37.532253    -15.118007    -37.256447    -13.989384
   -3.3273373   -88.87671     -24.292763    -29.299536    -33.56191
  -25.78022     -97.424515    -58.384796   -106.230515    -66.039825
   -3.122325    -34.272095      5.5776305   -27.34358     -31.050188
    6.791276    -40.162323    -80.94173    -128.28459     -55.417793
  -30.775087    -19.093557     -1.230656      2.8664167     1.2570434
    3.0559733  -169.77045      -3.3917105   -73.87012      29.71001
    1.5241827     3.0834894    -0.9956941     2.870783    -57.937714
   35.187523    -56.810074     35.518875    -12.294817     -2.337853  ]
Minimum obj value:-3486.1121739811742
Optimal xi: 26.239933
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1525.5634492612683
W_T_median: 1381.3134893816725
W_T_pctile_5: 690.7255568000884
W_T_CVAR_5_pct: 587.5989867837429
F value: -3486.1121739811742
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.9
F value: -3486.1121739811742
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ -30.7751,  -19.0936],
        [  -1.2307,    2.8664],
        [   1.2570,    3.0560],
        [-169.7704,   -3.3917],
        [ -73.8701,   29.7100],
        [   1.5242,    3.0835],
        [  -0.9957,    2.8708],
        [ -57.9377,   35.1875],
        [ -56.8101,   35.5189],
        [ -12.2948,   -2.3379]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[ -21.0834,  -35.3988,  -37.6495,  -32.6755,  -34.5280,  -56.6649,
          -45.8797,  -22.0180,  -45.4818,  -10.5495],
        [  -4.4843,  256.0005,  174.4284,   -3.6217,  148.3894,  238.2010,
          321.5612,  406.5423,  488.0200,   -4.3081],
        [ -26.9474,  -33.4156,  -78.1845,  -33.9742,  -30.1613,  -82.9743,
          -38.3691,  -15.6803,  -38.4229,  -14.3572],
        [ -27.2751,  -33.7422,  -45.7034,  -33.7390,  -31.8451,  -59.5288,
          -41.0890,  -18.2297,  -41.4720,  -13.3449],
        [  -0.9998,  -30.7938,  -14.6665,    8.9847, -109.0063,  -15.4665,
          -33.4953,  -24.8374,  -15.0446,    3.3041],
        [  -2.9677, -193.9099, -227.3627,    4.2749, -114.4358, -296.3193,
         -237.8576,  -85.1441, -150.9856,    4.5509],
        [  -6.5762,   26.0222,    5.0460,    2.1921,   29.8765,    5.7290,
           27.8898,   25.0917,   22.6263,   -3.5714],
        [ -26.7477,  -33.4558,  -85.8063,  -34.6877,  -29.3306,  -89.3443,
          -37.5323,  -15.1180,  -37.2564,  -13.9894],
        [  -3.3273,  -88.8767,  -24.2928,  -29.2995,  -33.5619,  -25.7802,
          -97.4245,  -58.3848, -106.2305,  -66.0398],
        [  -3.1223,  -34.2721,    5.5776,  -27.3436,  -31.0502,    6.7913,
          -40.1623,  -80.9417, -128.2846,  -55.4178]], device='cuda:0')), ('model.output_layer_3.weight', tensor([[ -4.3724,  -0.8062,   3.7707,   1.2451,   1.4851,  -7.4533,  -7.6401,
           4.1199, -35.8583, -35.1468],
        [  1.2023,   0.5265,  -4.0437,  -3.9929,  -1.6338,   7.8668,   5.9749,
          -4.3172,  36.1230,  34.9259]], device='cuda:0'))])
loaded xi:  26.239933
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -3637.14149882014
objective value function right now is: -3637.14149882014
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3631.9046105855004
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3636.3647421627898
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3633.89524878998
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3635.5208145331903
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3636.936684330793
14.000000000000002% of gradient descent iterations done. Method = Adam
updated min: -3637.2521880043196
objective value function right now is: -3637.2521880043196
16.0% of gradient descent iterations done. Method = Adam
updated min: -3637.658926106156
objective value function right now is: -3637.658926106156
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3634.6266383738202
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3635.722607810456
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3636.3665025928367
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3637.628724828378
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3636.083605789585
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -3637.0931259896515
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3637.373863080063
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3634.325917288229
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3636.7403028827875
36.0% of gradient descent iterations done. Method = Adam
updated min: -3638.172812044678
objective value function right now is: -3638.172812044678
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3636.704926789511
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3634.266840516251
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3635.1402390565772
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3636.819056497319
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3636.2589252588455
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3637.6830503354204
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3635.1891557208496
52.0% of gradient descent iterations done. Method = Adam
updated min: -3638.242568804312
objective value function right now is: -3638.242568804312
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3636.103263716943
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -3633.362117428273
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -3635.131393435469
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3636.120133960513
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3637.474303120884
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3636.320225012069
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3637.3554475774113
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3637.897592897705
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3637.640782370209
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3636.9560437271384
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3636.467456955244
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3636.28350012791
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3632.1552184866473
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3634.702289586029
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3637.618895510851
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3636.3812485677713
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3635.7585761698474
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3634.9989828447688
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3636.0011763661123
updated min: 
obj fun: -3638.243764059592
updated min: 
obj fun: -3638.2480161932067
updated min: 
obj fun: -3638.252295280845
updated min: 
obj fun: -3638.2557840625614
updated min: 
obj fun: -3638.258879361354
updated min: 
obj fun: -3638.2619253129255
updated min: 
obj fun: -3638.2649470803394
updated min: 
obj fun: -3638.2674697353013
updated min: 
obj fun: -3638.269456524205
updated min: 
obj fun: -3638.2713962198627
updated min: 
obj fun: -3638.2730687946723
updated min: 
obj fun: -3638.2740321671176
updated min: 
obj fun: -3638.27439482921
updated min: 
obj fun: -3638.2750014965764
updated min: 
obj fun: -3638.27617633309
updated min: 
obj fun: -3638.277748803816
updated min: 
obj fun: -3638.279781282683
updated min: 
obj fun: -3638.282497241275
updated min: 
obj fun: -3638.2857375242274
updated min: 
obj fun: -3638.2894324075323
updated min: 
obj fun: -3638.29311770392
updated min: 
obj fun: -3638.2964718719577
updated min: 
obj fun: -3638.2992242643336
updated min: 
obj fun: -3638.301080887713
updated min: 
obj fun: -3638.3018898437986
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3636.6175182032644
updated min: 
obj fun: -3638.3035197438535
updated min: 
obj fun: -3638.308605674827
updated min: 
obj fun: -3638.313465083381
updated min: 
obj fun: -3638.3180254526706
updated min: 
obj fun: -3638.3221000860144
updated min: 
obj fun: -3638.3260103420916
updated min: 
obj fun: -3638.3296770806573
updated min: 
obj fun: -3638.332935215856
updated min: 
obj fun: -3638.335970325878
updated min: 
obj fun: -3638.3388697792248
updated min: 
obj fun: -3638.3416793517304
updated min: 
obj fun: -3638.344552204829
updated min: 
obj fun: -3638.3475990574743
updated min: 
obj fun: -3638.35061077725
updated min: 
obj fun: -3638.353390324695
updated min: 
obj fun: -3638.3559868451043
updated min: 
obj fun: -3638.3585188476595
updated min: 
obj fun: -3638.3614989719504
updated min: 
obj fun: -3638.365042413786
updated min: 
obj fun: -3638.368605663866
updated min: 
obj fun: -3638.3723374829183
updated min: 
obj fun: -3638.3763327620436
updated min: 
obj fun: -3638.3806378038703
updated min: 
obj fun: -3638.385199294252
updated min: 
obj fun: -3638.3897011884724
updated min: 
obj fun: -3638.3937765884807
updated min: 
obj fun: -3638.397287788235
updated min: 
obj fun: -3638.400219556573
updated min: 
obj fun: -3638.4026347699028
updated min: 
obj fun: -3638.4045353469583
updated min: 
obj fun: -3638.406140226811
updated min: 
obj fun: -3638.4073137500463
updated min: 
obj fun: -3638.4075471999295
updated min: 
obj fun: -3638.4089146947704
updated min: 
obj fun: -3638.4107489546736
updated min: 
obj fun: -3638.4127312795094
updated min: 
obj fun: -3638.4148784005592
updated min: 
obj fun: -3638.4171972978734
updated min: 
obj fun: -3638.4195848848412
updated min: 
obj fun: -3638.4221316473454
updated min: 
obj fun: -3638.4247478796096
updated min: 
obj fun: -3638.4274046938567
updated min: 
obj fun: -3638.429855724089
updated min: 
obj fun: -3638.431881682572
updated min: 
obj fun: -3638.4336879756956
updated min: 
obj fun: -3638.435574128318
updated min: 
obj fun: -3638.4378207193718
updated min: 
obj fun: -3638.4404264917216
updated min: 
obj fun: -3638.4433591956395
updated min: 
obj fun: -3638.4464484508485
updated min: 
obj fun: -3638.4497458097735
updated min: 
obj fun: -3638.4531798681764
updated min: 
obj fun: -3638.4567273261077
updated min: 
obj fun: -3638.4605204742074
updated min: 
obj fun: -3638.464646761379
updated min: 
obj fun: -3638.4692484898924
updated min: 
obj fun: -3638.474075256443
updated min: 
obj fun: -3638.4788319626555
updated min: 
obj fun: -3638.483350776432
updated min: 
obj fun: -3638.48748065728
updated min: 
obj fun: -3638.491187518395
updated min: 
obj fun: -3638.494046351082
updated min: 
obj fun: -3638.4961273969293
updated min: 
obj fun: -3638.4975381619383
updated min: 
obj fun: -3638.4981979301906
updated min: 
obj fun: -3638.4997386883592
updated min: 
obj fun: -3638.50099686932
updated min: 
obj fun: -3638.501900407108
updated min: 
obj fun: -3638.502449874813
updated min: 
obj fun: -3638.5026009157123
updated min: 
obj fun: -3638.502791830058
updated min: 
obj fun: -3638.503876550226
updated min: 
obj fun: -3638.504825196031
updated min: 
obj fun: -3638.5054736916213
updated min: 
obj fun: -3638.505482185129
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3637.5693324200633
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3637.4758628258164
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3633.7941087151703
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3637.313946814798
NN weights: [-4.21213913e+00 -6.57580137e-01  3.77018571e+00  1.24284291e+00
  1.51243997e+00 -7.26049232e+00 -8.13636589e+00  4.11965036e+00
 -3.70518112e+01 -3.60643044e+01  1.03118074e+00  3.69643509e-01
 -4.04365015e+00 -3.99193764e+00 -1.66051185e+00  7.67897892e+00
  5.97538757e+00 -4.31729555e+00  3.73174019e+01  3.58420830e+01
 -2.13823204e+01 -3.53964272e+01 -3.76162109e+01 -3.26754990e+01
 -3.45280151e+01 -5.66230812e+01 -4.58760986e+01 -2.20180473e+01
 -4.54817963e+01 -1.07982187e+01 -6.75143719e+00  2.59675079e+02
  1.81254883e+02 -9.85372841e-01  1.48389420e+02  2.44475616e+02
  3.24795746e+02  4.06542328e+02  4.88020020e+02 -1.98532534e+00
 -2.69523163e+01 -3.34155655e+01 -7.81843033e+01 -3.39742050e+01
 -3.01613121e+01 -8.29741440e+01 -3.83690338e+01 -1.56803370e+01
 -3.84228821e+01 -1.43571978e+01 -2.72823772e+01 -3.37421837e+01
 -4.57034988e+01 -3.37390404e+01 -3.18450603e+01 -5.95289078e+01
 -4.10889626e+01 -1.82296715e+01 -4.14720497e+01 -1.33450928e+01
 -1.65973651e+00 -3.31363602e+01 -2.11622200e+01  9.01607227e+00
 -1.09006325e+02 -2.24335976e+01 -3.68169899e+01 -2.48373928e+01
 -1.50446253e+01  3.68631172e+00 -3.60682726e+00 -2.10867935e+02
 -2.39448166e+02  3.47056961e+00 -1.14435837e+02 -3.07951050e+02
 -2.54340332e+02 -8.51441116e+01 -1.50985565e+02  5.25926828e+00
 -6.14114618e+00  3.00408230e+01  7.00756693e+00  9.24247742e-01
  2.98777103e+01  7.41669559e+00  3.25325508e+01  2.50918255e+01
  2.26264019e+01 -2.35926008e+00 -2.67510967e+01 -3.34557037e+01
 -8.58060989e+01 -3.46877365e+01 -2.93305950e+01 -8.93440628e+01
 -3.75321960e+01 -1.51180067e+01 -3.72564468e+01 -1.39892349e+01
 -2.96396971e+00 -8.36254807e+01 -1.27882776e+01 -2.92995358e+01
 -3.35619087e+01 -1.26835670e+01 -9.09950333e+01 -5.83847961e+01
 -1.06230515e+02 -8.28877335e+01 -2.87227225e+00 -3.13417110e+01
  5.58531857e+00 -2.73435802e+01 -3.10501881e+01  7.31604004e+00
 -3.65752792e+01 -8.09417267e+01 -1.28284592e+02 -6.23736000e+01
 -3.18896751e+01 -1.98442020e+01 -2.45685279e-01  2.90525627e+00
  1.89099133e+00  3.06220484e+00 -1.79426895e+02 -3.50551724e+00
 -7.38690186e+01  2.96841660e+01  2.10700417e+00  3.09366274e+00
 -1.19330712e-01  2.90674090e+00 -5.79375305e+01  3.51860542e+01
 -5.68099518e+01  3.55176048e+01 -1.30886307e+01 -2.59602404e+00]
Minimum obj value:-3638.5040433246195
Optimal xi: 26.096209
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1528.128877375213
W_T_median: 1385.7640479395086
W_T_pctile_5: 682.9631395015361
W_T_CVAR_5_pct: 582.2910065238805
F value: -3638.5040433246195
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.0
F value: -3638.5040433246195
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[-3.1890e+01, -1.9844e+01],
        [-2.4569e-01,  2.9053e+00],
        [ 1.8910e+00,  3.0622e+00],
        [-1.7943e+02, -3.5055e+00],
        [-7.3869e+01,  2.9684e+01],
        [ 2.1070e+00,  3.0937e+00],
        [-1.1933e-01,  2.9067e+00],
        [-5.7938e+01,  3.5186e+01],
        [-5.6810e+01,  3.5518e+01],
        [-1.3089e+01, -2.5960e+00]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[ -21.3823,  -35.3964,  -37.6162,  -32.6755,  -34.5280,  -56.6231,
          -45.8761,  -22.0180,  -45.4818,  -10.7982],
        [  -6.7514,  259.6751,  181.2549,   -0.9854,  148.3894,  244.4756,
          324.7957,  406.5423,  488.0200,   -1.9853],
        [ -26.9523,  -33.4156,  -78.1843,  -33.9742,  -30.1613,  -82.9741,
          -38.3690,  -15.6803,  -38.4229,  -14.3572],
        [ -27.2824,  -33.7422,  -45.7035,  -33.7390,  -31.8451,  -59.5289,
          -41.0890,  -18.2297,  -41.4720,  -13.3451],
        [  -1.6597,  -33.1364,  -21.1622,    9.0161, -109.0063,  -22.4336,
          -36.8170,  -24.8374,  -15.0446,    3.6863],
        [  -3.6068, -210.8679, -239.4482,    3.4706, -114.4358, -307.9510,
         -254.3403,  -85.1441, -150.9856,    5.2593],
        [  -6.1411,   30.0408,    7.0076,    0.9242,   29.8777,    7.4167,
           32.5326,   25.0918,   22.6264,   -2.3593],
        [ -26.7511,  -33.4557,  -85.8061,  -34.6877,  -29.3306,  -89.3441,
          -37.5322,  -15.1180,  -37.2564,  -13.9892],
        [  -2.9640,  -83.6255,  -12.7883,  -29.2995,  -33.5619,  -12.6836,
          -90.9950,  -58.3848, -106.2305,  -82.8877],
        [  -2.8723,  -31.3417,    5.5853,  -27.3436,  -31.0502,    7.3160,
          -36.5753,  -80.9417, -128.2846,  -62.3736]], device='cuda:0')), ('model.output_layer_3.weight', tensor([[ -4.2121,  -0.6576,   3.7702,   1.2428,   1.5124,  -7.2605,  -8.1364,
           4.1197, -37.0518, -36.0643],
        [  1.0312,   0.3696,  -4.0437,  -3.9919,  -1.6605,   7.6790,   5.9754,
          -4.3173,  37.3174,  35.8421]], device='cuda:0'))])
loaded xi:  26.096209
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -3783.787021029651
objective value function right now is: -3783.787021029651
4.0% of gradient descent iterations done. Method = Adam
updated min: -3790.626905000752
objective value function right now is: -3790.626905000752
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3790.4936980569587
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3785.759096806085
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3789.344621331579
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3788.9319378506116
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -3788.99976526986
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3787.3381264730106
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3788.517086623735
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3778.2822517652344
22.0% of gradient descent iterations done. Method = Adam
updated min: -3790.6388878356765
objective value function right now is: -3790.6388878356765
24.0% of gradient descent iterations done. Method = Adam
updated min: -3790.8219590584163
objective value function right now is: -3790.8219590584163
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3789.756638593031
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -3779.587064067156
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3788.1326743909735
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3788.9439893534077
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3789.3562467022866
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3784.572997384231
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3789.39907285811
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3789.5766817942394
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3788.348031004018
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3787.592289875346
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3790.088265790242
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3789.7341400426785
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3789.6716388226487
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3787.732889357708
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3789.5795933517416
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -3786.992999032604
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -3790.118503167309
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3785.68804740861
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3785.165508216017
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3787.533763659835
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3787.975979767474
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3787.748195757961
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3788.8413783416627
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3786.522806443491
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3789.384281039754
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3789.2009901658666
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3787.5716312119957
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3789.8011921991524
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3790.176166051359
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3788.0797139343385
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3790.0456733913034
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3787.1123725577377
updated min: 
obj fun: -3791.36314961584
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3788.267584419817
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3789.9011663411648
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3784.4475149851896
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3790.1135986594877
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3788.203518012958
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3784.642402803249
NN weights: [-4.21213913e+00 -6.57580137e-01  3.77018571e+00  1.24284291e+00
  1.51243997e+00 -7.26049232e+00 -8.13636589e+00  4.11965036e+00
 -3.70518112e+01 -3.60643044e+01  1.03118074e+00  3.69643509e-01
 -4.04365015e+00 -3.99193764e+00 -1.66051185e+00  7.67897892e+00
  5.97538757e+00 -4.31729555e+00  3.73174019e+01  3.58420830e+01
 -2.13823204e+01 -3.53964272e+01 -3.76162109e+01 -3.26754990e+01
 -3.45280151e+01 -5.66230812e+01 -4.58760986e+01 -2.20180473e+01
 -4.54817963e+01 -1.07982187e+01 -6.75143719e+00  2.59675079e+02
  1.81254883e+02 -9.85372841e-01  1.48389420e+02  2.44475616e+02
  3.24795746e+02  4.06542328e+02  4.88020020e+02 -1.98532534e+00
 -2.69523163e+01 -3.34155655e+01 -7.81843033e+01 -3.39742050e+01
 -3.01613121e+01 -8.29741440e+01 -3.83690338e+01 -1.56803370e+01
 -3.84228821e+01 -1.43571978e+01 -2.72823772e+01 -3.37421837e+01
 -4.57034988e+01 -3.37390404e+01 -3.18450603e+01 -5.95289078e+01
 -4.10889626e+01 -1.82296715e+01 -4.14720497e+01 -1.33450928e+01
 -1.65973651e+00 -3.31363602e+01 -2.11622200e+01  9.01607227e+00
 -1.09006325e+02 -2.24335976e+01 -3.68169899e+01 -2.48373928e+01
 -1.50446253e+01  3.68631172e+00 -3.60682726e+00 -2.10867935e+02
 -2.39448166e+02  3.47056961e+00 -1.14435837e+02 -3.07951050e+02
 -2.54340332e+02 -8.51441116e+01 -1.50985565e+02  5.25926828e+00
 -6.14114618e+00  3.00408230e+01  7.00756693e+00  9.24247742e-01
  2.98777103e+01  7.41669559e+00  3.25325508e+01  2.50918255e+01
  2.26264019e+01 -2.35926008e+00 -2.67510967e+01 -3.34557037e+01
 -8.58060989e+01 -3.46877365e+01 -2.93305950e+01 -8.93440628e+01
 -3.75321960e+01 -1.51180067e+01 -3.72564468e+01 -1.39892349e+01
 -2.96396971e+00 -8.36254807e+01 -1.27882776e+01 -2.92995358e+01
 -3.35619087e+01 -1.26835670e+01 -9.09950333e+01 -5.83847961e+01
 -1.06230515e+02 -8.28877335e+01 -2.87227225e+00 -3.13417110e+01
  5.58531857e+00 -2.73435802e+01 -3.10501881e+01  7.31604004e+00
 -3.65752792e+01 -8.09417267e+01 -1.28284592e+02 -6.23736000e+01
 -3.18896751e+01 -1.98442020e+01 -2.45685279e-01  2.90525627e+00
  1.89099133e+00  3.06220484e+00 -1.79426895e+02 -3.50551724e+00
 -7.38690186e+01  2.96841660e+01  2.10700417e+00  3.09366274e+00
 -1.19330712e-01  2.90674090e+00 -5.79375305e+01  3.51860542e+01
 -5.68099518e+01  3.55176048e+01 -1.30886307e+01 -2.59602404e+00]
Minimum obj value:-3791.361639346051
Optimal xi: 26.132929
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1528.128877375213
W_T_median: 1385.7640479395086
W_T_pctile_5: 682.9631395015361
W_T_CVAR_5_pct: 582.2910065238805
F value: -3791.361639346051
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.1
F value: -3791.361639346051
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[-3.1890e+01, -1.9844e+01],
        [-2.4569e-01,  2.9053e+00],
        [ 1.8910e+00,  3.0622e+00],
        [-1.7943e+02, -3.5055e+00],
        [-7.3869e+01,  2.9684e+01],
        [ 2.1070e+00,  3.0937e+00],
        [-1.1933e-01,  2.9067e+00],
        [-5.7938e+01,  3.5186e+01],
        [-5.6810e+01,  3.5518e+01],
        [-1.3089e+01, -2.5960e+00]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[ -21.3823,  -35.3964,  -37.6162,  -32.6755,  -34.5280,  -56.6231,
          -45.8761,  -22.0180,  -45.4818,  -10.7982],
        [  -6.7514,  259.6751,  181.2549,   -0.9854,  148.3894,  244.4756,
          324.7957,  406.5423,  488.0200,   -1.9853],
        [ -26.9523,  -33.4156,  -78.1843,  -33.9742,  -30.1613,  -82.9741,
          -38.3690,  -15.6803,  -38.4229,  -14.3572],
        [ -27.2824,  -33.7422,  -45.7035,  -33.7390,  -31.8451,  -59.5289,
          -41.0890,  -18.2297,  -41.4720,  -13.3451],
        [  -1.6597,  -33.1364,  -21.1622,    9.0161, -109.0063,  -22.4336,
          -36.8170,  -24.8374,  -15.0446,    3.6863],
        [  -3.6068, -210.8679, -239.4482,    3.4706, -114.4358, -307.9510,
         -254.3403,  -85.1441, -150.9856,    5.2593],
        [  -6.1411,   30.0408,    7.0076,    0.9242,   29.8777,    7.4167,
           32.5326,   25.0918,   22.6264,   -2.3593],
        [ -26.7511,  -33.4557,  -85.8061,  -34.6877,  -29.3306,  -89.3441,
          -37.5322,  -15.1180,  -37.2564,  -13.9892],
        [  -2.9640,  -83.6255,  -12.7883,  -29.2995,  -33.5619,  -12.6836,
          -90.9950,  -58.3848, -106.2305,  -82.8877],
        [  -2.8723,  -31.3417,    5.5853,  -27.3436,  -31.0502,    7.3160,
          -36.5753,  -80.9417, -128.2846,  -62.3736]], device='cuda:0')), ('model.output_layer_3.weight', tensor([[ -4.2121,  -0.6576,   3.7702,   1.2428,   1.5124,  -7.2605,  -8.1364,
           4.1197, -37.0518, -36.0643],
        [  1.0312,   0.3696,  -4.0437,  -3.9919,  -1.6605,   7.6790,   5.9754,
          -4.3173,  37.3174,  35.8421]], device='cuda:0'))])
loaded xi:  26.132929
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -3942.3011697478964
objective value function right now is: -3942.3011697478964
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3941.917178004198
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3937.4849619382862
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3937.6205720814205
10.0% of gradient descent iterations done. Method = Adam
updated min: -3942.813654333305
objective value function right now is: -3942.813654333305
12.0% of gradient descent iterations done. Method = Adam
updated min: -3943.3175737583742
objective value function right now is: -3943.3175737583742
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -3941.8046483299468
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3940.494095045928
18.0% of gradient descent iterations done. Method = Adam
updated min: -3943.6776234072477
objective value function right now is: -3943.6776234072477
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3940.6419715953302
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3942.613945962583
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3942.226664863637
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3942.1841186611523
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -3943.5537537485875
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3941.9851873487833
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3943.4824363653315
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3940.154896202139
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3940.7244889522517
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3942.1119375504068
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3939.759844363888
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3939.749950494149
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3940.1110882573157
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3941.465050081342
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3940.9386889620955
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3938.599686532139
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3942.642488451765
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3942.030334846888
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -3939.4754756579905
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -3943.236067340454
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3942.612268710274
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3942.386801442604
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3943.4962199491038
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3942.217350404103
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3939.0848478204166
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3942.544800952882
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3939.2236108983616
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3941.660919986073
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3940.767823660164
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3941.8650526427523
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3939.55347461682
82.0% of gradient descent iterations done. Method = Adam
updated min: -3943.691700687861
objective value function right now is: -3943.691700687861
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3940.536614468856
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3942.7674218609855
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3942.4376253343535
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3943.429570287678
updated min: 
obj fun: -3943.842021532666
updated min: 
obj fun: -3943.849788041588
updated min: 
obj fun: -3943.85870820652
updated min: 
obj fun: -3943.867221273606
updated min: 
obj fun: -3943.875377794445
updated min: 
obj fun: -3943.8832905293857
updated min: 
obj fun: -3943.891176972312
updated min: 
obj fun: -3943.8988596242743
updated min: 
obj fun: -3943.9064945822843
updated min: 
obj fun: -3943.921330408496
updated min: 
obj fun: -3943.9293593400903
updated min: 
obj fun: -3943.937041207973
updated min: 
obj fun: -3943.9440760113707
updated min: 
obj fun: -3943.9504159220355
updated min: 
obj fun: -3943.9562085368125
updated min: 
obj fun: -3943.9621032038804
updated min: 
obj fun: -3943.968378799046
updated min: 
obj fun: -3943.9750534981536
updated min: 
obj fun: -3943.981747618326
updated min: 
obj fun: -3943.9885002131614
updated min: 
obj fun: -3943.9956623055314
updated min: 
obj fun: -3944.002205543938
updated min: 
obj fun: -3944.0082719989755
updated min: 
obj fun: -3944.013544582531
updated min: 
obj fun: -3944.0182426916845
updated min: 
obj fun: -3944.0223588218328
updated min: 
obj fun: -3944.026319158369
updated min: 
obj fun: -3944.029982126224
updated min: 
obj fun: -3944.033271381342
updated min: 
obj fun: -3944.0362273669803
updated min: 
obj fun: -3944.0392015105854
updated min: 
obj fun: -3944.0424740965245
updated min: 
obj fun: -3944.0461943658165
updated min: 
obj fun: -3944.050713629964
updated min: 
obj fun: -3944.056081082194
updated min: 
obj fun: -3944.062124843263
updated min: 
obj fun: -3944.0685494535114
updated min: 
obj fun: -3944.08207785309
updated min: 
obj fun: -3944.088836838727
updated min: 
obj fun: -3944.095464967136
updated min: 
obj fun: -3944.1019945173466
updated min: 
obj fun: -3944.1085134656414
updated min: 
obj fun: -3944.1148607354453
updated min: 
obj fun: -3944.1206963698864
updated min: 
obj fun: -3944.126006222073
updated min: 
obj fun: -3944.1307628489835
updated min: 
obj fun: -3944.1349084259064
updated min: 
obj fun: -3944.138513922344
updated min: 
obj fun: -3944.142044917929
updated min: 
obj fun: -3944.1456583130807
updated min: 
obj fun: -3944.1493034429363
updated min: 
obj fun: -3944.1534202906287
updated min: 
obj fun: -3944.1581890505713
updated min: 
obj fun: -3944.1634298825106
updated min: 
obj fun: -3944.169032660852
updated min: 
obj fun: -3944.1743800092713
updated min: 
obj fun: -3944.179699829225
updated min: 
obj fun: -3944.18456761537
updated min: 
obj fun: -3944.1890948106425
updated min: 
obj fun: -3944.193013126707
updated min: 
obj fun: -3944.1966466385247
updated min: 
obj fun: -3944.199919639365
updated min: 
obj fun: -3944.202990719405
updated min: 
obj fun: -3944.205798249498
updated min: 
obj fun: -3944.208830145678
updated min: 
obj fun: -3944.211907740728
updated min: 
obj fun: -3944.2152187211414
updated min: 
obj fun: -3944.2182275320915
updated min: 
obj fun: -3944.220823277019
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3940.5611239903888
updated min: 
obj fun: -3944.2227951537548
updated min: 
obj fun: -3944.224093987138
updated min: 
obj fun: -3944.2247093873825
updated min: 
obj fun: -3944.2248823852447
updated min: 
obj fun: -3944.225094960632
updated min: 
obj fun: -3944.2278690439166
updated min: 
obj fun: -3944.230533730437
updated min: 
obj fun: -3944.232920753029
updated min: 
obj fun: -3944.234864386067
updated min: 
obj fun: -3944.236290522187
updated min: 
obj fun: -3944.2370922940645
updated min: 
obj fun: -3944.2374661281883
updated min: 
obj fun: -3944.23871864571
updated min: 
obj fun: -3944.2405377472146
updated min: 
obj fun: -3944.242169306364
updated min: 
obj fun: -3944.243376184149
updated min: 
obj fun: -3944.2444259469667
updated min: 
obj fun: -3944.2456263434547
updated min: 
obj fun: -3944.2469870068267
updated min: 
obj fun: -3944.2484742244965
updated min: 
obj fun: -3944.2499161535497
updated min: 
obj fun: -3944.251127772211
updated min: 
obj fun: -3944.2519577055004
updated min: 
obj fun: -3944.252249596975
updated min: 
obj fun: -3944.253665502374
updated min: 
obj fun: -3944.255848039786
updated min: 
obj fun: -3944.257522168796
updated min: 
obj fun: -3944.258828805764
updated min: 
obj fun: -3944.259657882973
updated min: 
obj fun: -3944.259917750137
updated min: 
obj fun: -3944.260946255281
updated min: 
obj fun: -3944.2623155894785
updated min: 
obj fun: -3944.2636399622043
updated min: 
obj fun: -3944.2647860969346
updated min: 
obj fun: -3944.2657811812787
updated min: 
obj fun: -3944.2664502461007
updated min: 
obj fun: -3944.2671229071216
updated min: 
obj fun: -3944.2680398884986
updated min: 
obj fun: -3944.2689503494275
updated min: 
obj fun: -3944.2699573394693
updated min: 
obj fun: -3944.271044448449
updated min: 
obj fun: -3944.2722638990176
updated min: 
obj fun: -3944.2742822648215
updated min: 
obj fun: -3944.276461731454
updated min: 
obj fun: -3944.278306814626
updated min: 
obj fun: -3944.2796108199555
updated min: 
obj fun: -3944.280367276484
updated min: 
obj fun: -3944.2807081060846
updated min: 
obj fun: -3944.280856662214
updated min: 
obj fun: -3944.2814013176458
updated min: 
obj fun: -3944.2824361194225
updated min: 
obj fun: -3944.2830052940935
updated min: 
obj fun: -3944.283112894714
updated min: 
obj fun: -3944.28343628563
updated min: 
obj fun: -3944.284277570113
updated min: 
obj fun: -3944.2861716835837
updated min: 
obj fun: -3944.288486841791
updated min: 
obj fun: -3944.2907173517488
updated min: 
obj fun: -3944.2926194968663
updated min: 
obj fun: -3944.2940139715097
updated min: 
obj fun: -3944.29467937625
updated min: 
obj fun: -3944.294704034215
updated min: 
obj fun: -3944.29521538955
updated min: 
obj fun: -3944.296054775628
updated min: 
obj fun: -3944.296591046289
updated min: 
obj fun: -3944.296941696929
updated min: 
obj fun: -3944.297389601432
updated min: 
obj fun: -3944.2979021454535
updated min: 
obj fun: -3944.2984260228914
updated min: 
obj fun: -3944.299300984824
updated min: 
obj fun: -3944.30060606163
updated min: 
obj fun: -3944.302229413687
updated min: 
obj fun: -3944.30388811831
updated min: 
obj fun: -3944.3053744904455
updated min: 
obj fun: -3944.306582528395
updated min: 
obj fun: -3944.3073860046293
updated min: 
obj fun: -3944.3075696486785
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3941.5680667689776
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3940.860132342016
updated min: 
obj fun: -3944.307653901313
updated min: 
obj fun: -3944.3076960635085
updated min: 
obj fun: -3944.307850864068
updated min: 
obj fun: -3944.308058657669
updated min: 
obj fun: -3944.308196408689
updated min: 
obj fun: -3944.308715385344
updated min: 
obj fun: -3944.3093827030707
updated min: 
obj fun: -3944.3099686000273
updated min: 
obj fun: -3944.310488799328
updated min: 
obj fun: -3944.3107499037715
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3943.4846193918784
updated min: 
obj fun: -3944.3114922929126
updated min: 
obj fun: -3944.3122159276436
updated min: 
obj fun: -3944.31284200243
updated min: 
obj fun: -3944.313369626462
updated min: 
obj fun: -3944.31393517154
updated min: 
obj fun: -3944.314506541594
updated min: 
obj fun: -3944.3149687254113
updated min: 
obj fun: -3944.3152148735303
updated min: 
obj fun: -3944.3152518738043
updated min: 
obj fun: -3944.3153771764614
updated min: 
obj fun: -3944.3157983243677
updated min: 
obj fun: -3944.31618833513
updated min: 
obj fun: -3944.3164813075955
updated min: 
obj fun: -3944.316567492581
updated min: 
obj fun: -3944.3168295820747
updated min: 
obj fun: -3944.317109080355
updated min: 
obj fun: -3944.317202835621
updated min: 
obj fun: -3944.317591756695
updated min: 
obj fun: -3944.3183358453616
updated min: 
obj fun: -3944.3191187860816
updated min: 
obj fun: -3944.3197909902265
updated min: 
obj fun: -3944.320139565452
updated min: 
obj fun: -3944.320198977229
updated min: 
obj fun: -3944.3203627529133
updated min: 
obj fun: -3944.3207619860154
updated min: 
obj fun: -3944.3211864995387
updated min: 
obj fun: -3944.3215723045223
updated min: 
obj fun: -3944.3219097610026
updated min: 
obj fun: -3944.322219591659
updated min: 
obj fun: -3944.3224560998256
updated min: 
obj fun: -3944.32260960754
updated min: 
obj fun: -3944.322617978415
updated min: 
obj fun: -3944.3228686119664
updated min: 
obj fun: -3944.3234298384423
updated min: 
obj fun: -3944.324057173825
updated min: 
obj fun: -3944.3246211473515
updated min: 
obj fun: -3944.325063446235
updated min: 
obj fun: -3944.3253481863508
updated min: 
obj fun: -3944.3254216018486
updated min: 
obj fun: -3944.3258296233707
updated min: 
obj fun: -3944.326146641645
updated min: 
obj fun: -3944.3261546984854
updated min: 
obj fun: -3944.3264355373726
updated min: 
obj fun: -3944.326510133855
updated min: 
obj fun: -3944.3266760870506
updated min: 
obj fun: -3944.3270423040603
updated min: 
obj fun: -3944.327533080955
updated min: 
obj fun: -3944.3280088388165
updated min: 
obj fun: -3944.328352275675
updated min: 
obj fun: -3944.328465323147
updated min: 
obj fun: -3944.3286478523205
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3939.2884084856532
NN weights: [ 4.74083281e+00 -6.05231225e-01  9.19196510e+00  6.95839357e+00
  1.50236368e+00 -7.21371889e+00 -8.65140438e+00  9.42954826e+00
 -3.68419228e+01 -3.41400528e+01 -8.08153725e+00  3.08324784e-01
 -9.60280800e+00 -9.97284603e+00 -1.64957035e+00  7.63792706e+00
  5.87632370e+00 -9.75020504e+00  3.71071587e+01  3.39121284e+01
 -2.05172062e+01 -3.31611977e+01 -3.50131264e+01 -3.26754990e+01
 -3.45280151e+01 -5.40978355e+01 -4.35600357e+01 -2.20180473e+01
 -4.54817963e+01 -8.98295689e+00 -1.01033278e+01  2.66133636e+02
  1.91419907e+02 -2.57619476e+00  1.48389420e+02  2.54628021e+02
  3.30656036e+02  4.06542328e+02  4.88020020e+02  1.45188773e+00
 -2.20893211e+01 -3.03388405e+01 -7.56949158e+01 -3.39742050e+01
 -3.01613121e+01 -8.04823990e+01 -3.51542091e+01 -1.56803370e+01
 -3.84228821e+01 -1.06275110e+01 -2.43525887e+01 -3.30575523e+01
 -4.55980606e+01 -3.37390404e+01 -3.18450603e+01 -5.93704529e+01
 -4.03181038e+01 -1.82296715e+01 -4.14720497e+01 -1.27730618e+01
 -2.05518365e+00 -3.17586575e+01 -2.60999794e+01  1.12376785e+01
 -1.09006325e+02 -2.82771416e+01 -3.58156853e+01 -2.48373928e+01
 -1.50446253e+01  3.93215418e+00 -3.41294265e+00 -2.26741562e+02
 -2.50890579e+02  2.77781343e+00 -1.14435837e+02 -3.18964050e+02
 -2.70099091e+02 -8.51441116e+01 -1.50985565e+02  5.11528015e+00
 -8.55922794e+00  3.53682251e+01  1.39431763e+01 -6.48027468e+00
  2.98776054e+01  1.46226006e+01  3.82417679e+01  2.50918255e+01
  2.26264019e+01  2.43915632e-01 -2.19410820e+01 -3.02408257e+01
 -8.31430130e+01 -3.46877365e+01 -2.93305950e+01 -8.66858368e+01
 -3.41804504e+01 -1.51180067e+01 -3.72564468e+01 -1.01605186e+01
 -3.48951864e+00 -8.86092300e+01 -1.69248562e+01 -2.92995358e+01
 -3.35619087e+01 -1.61173458e+01 -9.56122665e+01 -5.83847961e+01
 -1.06230515e+02 -9.25983963e+01 -3.51588750e+00 -2.74503117e+01
  1.08130779e+01 -2.73435802e+01 -3.10501881e+01  1.31404057e+01
 -3.21683044e+01 -8.09417267e+01 -1.28284592e+02 -6.59228516e+01
 -3.81901016e+01 -1.42198238e+01  9.11260307e-01  2.92288423e+00
  1.71668804e+00  3.07651901e+00 -1.90218109e+02 -3.94000244e+00
 -7.38694382e+01  2.96784973e+01  1.87244821e+00  3.09457755e+00
  1.00547111e+00  2.93178511e+00 -5.79375305e+01  3.51862602e+01
 -5.68099594e+01  3.55177612e+01 -1.59638758e+01 -3.06151843e+00]
Minimum obj value:-3944.331084817474
Optimal xi: 26.021206
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1531.545467669449
W_T_median: 1390.6909153611855
W_T_pctile_5: 678.7087660114726
W_T_CVAR_5_pct: 574.957753588692
F value: -3944.331084817474
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.2
F value: -3944.331084817474
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ -38.1901,  -14.2198],
        [   0.9113,    2.9229],
        [   1.7167,    3.0765],
        [-190.2181,   -3.9400],
        [ -73.8694,   29.6785],
        [   1.8724,    3.0946],
        [   1.0055,    2.9318],
        [ -57.9375,   35.1863],
        [ -56.8100,   35.5178],
        [ -15.9639,   -3.0615]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[-2.0517e+01, -3.3161e+01, -3.5013e+01, -3.2675e+01, -3.4528e+01,
         -5.4098e+01, -4.3560e+01, -2.2018e+01, -4.5482e+01, -8.9830e+00],
        [-1.0103e+01,  2.6613e+02,  1.9142e+02, -2.5762e+00,  1.4839e+02,
          2.5463e+02,  3.3066e+02,  4.0654e+02,  4.8802e+02,  1.4519e+00],
        [-2.2089e+01, -3.0339e+01, -7.5695e+01, -3.3974e+01, -3.0161e+01,
         -8.0482e+01, -3.5154e+01, -1.5680e+01, -3.8423e+01, -1.0628e+01],
        [-2.4353e+01, -3.3058e+01, -4.5598e+01, -3.3739e+01, -3.1845e+01,
         -5.9370e+01, -4.0318e+01, -1.8230e+01, -4.1472e+01, -1.2773e+01],
        [-2.0552e+00, -3.1759e+01, -2.6100e+01,  1.1238e+01, -1.0901e+02,
         -2.8277e+01, -3.5816e+01, -2.4837e+01, -1.5045e+01,  3.9322e+00],
        [-3.4129e+00, -2.2674e+02, -2.5089e+02,  2.7778e+00, -1.1444e+02,
         -3.1896e+02, -2.7010e+02, -8.5144e+01, -1.5099e+02,  5.1153e+00],
        [-8.5592e+00,  3.5368e+01,  1.3943e+01, -6.4803e+00,  2.9878e+01,
          1.4623e+01,  3.8242e+01,  2.5092e+01,  2.2626e+01,  2.4392e-01],
        [-2.1941e+01, -3.0241e+01, -8.3143e+01, -3.4688e+01, -2.9331e+01,
         -8.6686e+01, -3.4180e+01, -1.5118e+01, -3.7256e+01, -1.0161e+01],
        [-3.4895e+00, -8.8609e+01, -1.6925e+01, -2.9300e+01, -3.3562e+01,
         -1.6117e+01, -9.5612e+01, -5.8385e+01, -1.0623e+02, -9.2598e+01],
        [-3.5159e+00, -2.7450e+01,  1.0813e+01, -2.7344e+01, -3.1050e+01,
          1.3140e+01, -3.2168e+01, -8.0942e+01, -1.2828e+02, -6.5923e+01]],
       device='cuda:0')), ('model.output_layer_3.weight', tensor([[  4.7408,  -0.6052,   9.1920,   6.9584,   1.5024,  -7.2137,  -8.6514,
           9.4295, -36.8419, -34.1401],
        [ -8.0815,   0.3083,  -9.6028,  -9.9728,  -1.6496,   7.6379,   5.8763,
          -9.7502,  37.1072,  33.9121]], device='cuda:0'))])
loaded xi:  26.021206
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -4095.3059175274307
objective value function right now is: -4095.3059175274307
4.0% of gradient descent iterations done. Method = Adam
updated min: -4095.7459254572514
objective value function right now is: -4095.7459254572514
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4093.9828652447673
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4094.460425598932
10.0% of gradient descent iterations done. Method = Adam
updated min: -4096.213733970568
objective value function right now is: -4096.213733970568
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4094.362522864483
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -4094.3738215184558
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4090.72847674135
18.0% of gradient descent iterations done. Method = Adam
updated min: -4096.988800814111
objective value function right now is: -4096.988800814111
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4094.6905566770247
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4093.315104211501
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4093.4393977742707
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4093.68166181706
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -4095.14772387012
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4096.07887960932
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4096.886229388994
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4095.581295931909
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4096.5013994070005
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4093.250173883741
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4096.448501026734
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4088.3470732060932
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4096.885535025353
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4095.614456111093
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4096.358399117195
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4092.823295123296
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4094.840957673668
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4094.5008567305526
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -4096.394443326472
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -4095.9906232186736
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4095.9356773820773
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4096.038668117904
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4096.42235796137
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4096.58962459656
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4093.4633145367316
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4095.7346447786176
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4096.225233990483
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4092.8221861633892
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4094.11330089545
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4096.872463551301
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4095.6738786830365
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4095.3341872194515
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4096.709865252651
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4092.127364901701
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4091.7528546068447
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4092.5464582172103
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4085.632747089398
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4094.9860991064293
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4090.9997367986534
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4094.085816939006
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4091.54501105838
NN weights: [ 3.40519261e+00 -7.00377941e-01  7.71266174e+00  5.36422205e+00
  1.52152252e+00 -7.37437105e+00 -7.98379135e+00  7.91103268e+00
 -3.78614464e+01 -3.47151794e+01 -6.76042032e+00  4.04440284e-01
 -8.13390636e+00 -8.39004230e+00 -1.66856027e+00  7.79965448e+00
  5.26165581e+00 -8.24144459e+00  3.81266212e+01  3.44862900e+01
 -2.19252834e+01 -3.46238327e+01 -3.61688118e+01 -3.26754990e+01
 -3.45280151e+01 -5.52990150e+01 -4.50666351e+01 -2.20180473e+01
 -4.54817963e+01 -1.13486567e+01 -9.35760307e+00  2.66749451e+02
  1.94366150e+02 -1.78205538e+00  1.48389420e+02  2.57853241e+02
  3.31271881e+02  4.06542328e+02  4.88020020e+02  2.83677697e-01
 -2.32888088e+01 -3.23442459e+01 -7.73664322e+01 -3.39742050e+01
 -3.01613121e+01 -8.21728058e+01 -3.71889687e+01 -1.56803370e+01
 -3.84228821e+01 -1.30427341e+01 -2.55158405e+01 -3.50151062e+01
 -4.72031555e+01 -3.37390404e+01 -3.18450603e+01 -6.09963837e+01
 -4.23037300e+01 -1.82296715e+01 -4.14720497e+01 -1.51775713e+01
 -2.22888160e+00 -3.26365433e+01 -2.91629848e+01  1.18163347e+01
 -1.09006325e+02 -3.17276917e+01 -3.68489265e+01 -2.48373928e+01
 -1.50446253e+01  4.31980038e+00 -3.30362773e+00 -2.29915466e+02
 -2.52733047e+02  1.95890927e+00 -1.14435837e+02 -3.20637604e+02
 -2.73227722e+02 -8.51441116e+01 -1.50985565e+02  5.30330372e+00
 -9.80874825e+00  3.49167366e+01  1.37226658e+01 -2.46764684e+00
  2.98776913e+01  1.43908567e+01  3.77623367e+01  2.50918255e+01
  2.26264019e+01  1.19307792e+00 -2.31969395e+01 -3.23399696e+01
 -8.49076233e+01 -3.46877365e+01 -2.93305950e+01 -8.84673615e+01
 -3.63079376e+01 -1.51180067e+01 -3.72564468e+01 -1.26277647e+01
 -2.75561023e+00 -8.73802795e+01 -1.64060478e+01 -2.92995358e+01
 -3.35619087e+01 -1.55947132e+01 -9.43464813e+01 -5.83847961e+01
 -1.06230515e+02 -9.64058075e+01 -2.71088696e+00 -2.60892410e+01
  1.11791420e+01 -2.73435802e+01 -3.10501881e+01  1.33964148e+01
 -3.09370708e+01 -8.09417267e+01 -1.28284592e+02 -6.47573853e+01
 -3.84862289e+01 -1.41766157e+01  1.03937912e+00  2.99017596e+00
  2.33743095e+00  3.00783324e+00 -1.91976700e+02 -3.75926828e+00
 -7.38694153e+01  2.96753330e+01  2.58031750e+00  3.01764488e+00
  1.25924933e+00  2.98861980e+00 -5.79375305e+01  3.51862602e+01
 -5.68099594e+01  3.55177612e+01 -1.60924664e+01 -3.12627268e+00]
Minimum obj value:-4092.6262409217707
Optimal xi: 26.324556
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1533.6747606915803
W_T_median: 1393.5327984216192
W_T_pctile_5: 674.5293119527929
W_T_CVAR_5_pct: 569.6349599187931
F value: -4092.6262409217707
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.3
F value: -4092.6262409217707
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ -38.4862,  -14.1766],
        [   1.0394,    2.9902],
        [   2.3374,    3.0078],
        [-191.9767,   -3.7593],
        [ -73.8694,   29.6753],
        [   2.5803,    3.0176],
        [   1.2592,    2.9886],
        [ -57.9375,   35.1863],
        [ -56.8100,   35.5178],
        [ -16.0925,   -3.1263]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[-2.1925e+01, -3.4624e+01, -3.6169e+01, -3.2675e+01, -3.4528e+01,
         -5.5299e+01, -4.5067e+01, -2.2018e+01, -4.5482e+01, -1.1349e+01],
        [-9.3576e+00,  2.6675e+02,  1.9437e+02, -1.7821e+00,  1.4839e+02,
          2.5785e+02,  3.3127e+02,  4.0654e+02,  4.8802e+02,  2.8368e-01],
        [-2.3289e+01, -3.2344e+01, -7.7366e+01, -3.3974e+01, -3.0161e+01,
         -8.2173e+01, -3.7189e+01, -1.5680e+01, -3.8423e+01, -1.3043e+01],
        [-2.5516e+01, -3.5015e+01, -4.7203e+01, -3.3739e+01, -3.1845e+01,
         -6.0996e+01, -4.2304e+01, -1.8230e+01, -4.1472e+01, -1.5178e+01],
        [-2.2289e+00, -3.2637e+01, -2.9163e+01,  1.1816e+01, -1.0901e+02,
         -3.1728e+01, -3.6849e+01, -2.4837e+01, -1.5045e+01,  4.3198e+00],
        [-3.3036e+00, -2.2992e+02, -2.5273e+02,  1.9589e+00, -1.1444e+02,
         -3.2064e+02, -2.7323e+02, -8.5144e+01, -1.5099e+02,  5.3033e+00],
        [-9.8087e+00,  3.4917e+01,  1.3723e+01, -2.4676e+00,  2.9878e+01,
          1.4391e+01,  3.7762e+01,  2.5092e+01,  2.2626e+01,  1.1931e+00],
        [-2.3197e+01, -3.2340e+01, -8.4908e+01, -3.4688e+01, -2.9331e+01,
         -8.8467e+01, -3.6308e+01, -1.5118e+01, -3.7256e+01, -1.2628e+01],
        [-2.7556e+00, -8.7380e+01, -1.6406e+01, -2.9300e+01, -3.3562e+01,
         -1.5595e+01, -9.4346e+01, -5.8385e+01, -1.0623e+02, -9.6406e+01],
        [-2.7109e+00, -2.6089e+01,  1.1179e+01, -2.7344e+01, -3.1050e+01,
          1.3396e+01, -3.0937e+01, -8.0942e+01, -1.2828e+02, -6.4757e+01]],
       device='cuda:0')), ('model.output_layer_3.weight', tensor([[  3.4052,  -0.7004,   7.7127,   5.3642,   1.5215,  -7.3744,  -7.9838,
           7.9110, -37.8614, -34.7152],
        [ -6.7604,   0.4044,  -8.1339,  -8.3900,  -1.6686,   7.7997,   5.2617,
          -8.2414,  38.1266,  34.4863]], device='cuda:0'))])
loaded xi:  26.324556
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -4247.487809327249
objective value function right now is: -4247.487809327249
4.0% of gradient descent iterations done. Method = Adam
updated min: -4249.294073708255
objective value function right now is: -4249.294073708255
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4248.9132413446805
8.0% of gradient descent iterations done. Method = Adam
updated min: -4249.353302785773
objective value function right now is: -4249.353302785773
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4248.283403907111
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4248.058758285236
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -4248.585544422623
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4249.3523832215715
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4249.233009072322
20.0% of gradient descent iterations done. Method = Adam
updated min: -4249.685662058324
objective value function right now is: -4249.685662058324
22.0% of gradient descent iterations done. Method = Adam
updated min: -4249.921429442887
objective value function right now is: -4249.921429442887
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4249.033197557777
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4249.578484251897
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -4249.063536088212
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4245.300364982067
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4246.008610485541
34.0% of gradient descent iterations done. Method = Adam
updated min: -4250.183507269444
objective value function right now is: -4250.183507269444
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4248.952699525456
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4249.010088881514
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4247.722045660788
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4249.094947125718
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4248.7225109357705
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4249.134759779365
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4247.671538946647
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4248.108216382735
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4248.319889103597
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4250.128323718384
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -4249.142084817272
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -4243.763527007755
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4248.946815015213
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4249.998402341653
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4248.886733276576
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4246.110198636464
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4249.0816484781035
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4249.405476109732
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4244.446350125177
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4248.94136817144
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4249.852065683983
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4249.4894017780125
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4249.258091131856
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4243.06444463409
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4247.9642105814855
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4249.940378193035
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4248.17438781574
updated min: 
obj fun: -4250.451547504783
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4247.19796442258
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4249.163779881127
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4246.750179269437
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4245.080573251154
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4245.235231203822
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4248.168380316522
NN weights: [ 3.40519261e+00 -7.00377941e-01  7.71266174e+00  5.36422205e+00
  1.52152252e+00 -7.37437105e+00 -7.98379135e+00  7.91103268e+00
 -3.78614464e+01 -3.47151794e+01 -6.76042032e+00  4.04440284e-01
 -8.13390636e+00 -8.39004230e+00 -1.66856027e+00  7.79965448e+00
  5.26165581e+00 -8.24144459e+00  3.81266212e+01  3.44862900e+01
 -2.19252834e+01 -3.46238327e+01 -3.61688118e+01 -3.26754990e+01
 -3.45280151e+01 -5.52990150e+01 -4.50666351e+01 -2.20180473e+01
 -4.54817963e+01 -1.13486567e+01 -9.35760307e+00  2.66749451e+02
  1.94366150e+02 -1.78205538e+00  1.48389420e+02  2.57853241e+02
  3.31271881e+02  4.06542328e+02  4.88020020e+02  2.83677697e-01
 -2.32888088e+01 -3.23442459e+01 -7.73664322e+01 -3.39742050e+01
 -3.01613121e+01 -8.21728058e+01 -3.71889687e+01 -1.56803370e+01
 -3.84228821e+01 -1.30427341e+01 -2.55158405e+01 -3.50151062e+01
 -4.72031555e+01 -3.37390404e+01 -3.18450603e+01 -6.09963837e+01
 -4.23037300e+01 -1.82296715e+01 -4.14720497e+01 -1.51775713e+01
 -2.22888160e+00 -3.26365433e+01 -2.91629848e+01  1.18163347e+01
 -1.09006325e+02 -3.17276917e+01 -3.68489265e+01 -2.48373928e+01
 -1.50446253e+01  4.31980038e+00 -3.30362773e+00 -2.29915466e+02
 -2.52733047e+02  1.95890927e+00 -1.14435837e+02 -3.20637604e+02
 -2.73227722e+02 -8.51441116e+01 -1.50985565e+02  5.30330372e+00
 -9.80874825e+00  3.49167366e+01  1.37226658e+01 -2.46764684e+00
  2.98776913e+01  1.43908567e+01  3.77623367e+01  2.50918255e+01
  2.26264019e+01  1.19307792e+00 -2.31969395e+01 -3.23399696e+01
 -8.49076233e+01 -3.46877365e+01 -2.93305950e+01 -8.84673615e+01
 -3.63079376e+01 -1.51180067e+01 -3.72564468e+01 -1.26277647e+01
 -2.75561023e+00 -8.73802795e+01 -1.64060478e+01 -2.92995358e+01
 -3.35619087e+01 -1.55947132e+01 -9.43464813e+01 -5.83847961e+01
 -1.06230515e+02 -9.64058075e+01 -2.71088696e+00 -2.60892410e+01
  1.11791420e+01 -2.73435802e+01 -3.10501881e+01  1.33964148e+01
 -3.09370708e+01 -8.09417267e+01 -1.28284592e+02 -6.47573853e+01
 -3.84862289e+01 -1.41766157e+01  1.03937912e+00  2.99017596e+00
  2.33743095e+00  3.00783324e+00 -1.91976700e+02 -3.75926828e+00
 -7.38694153e+01  2.96753330e+01  2.58031750e+00  3.01764488e+00
  1.25924933e+00  2.98861980e+00 -5.79375305e+01  3.51862602e+01
 -5.68099594e+01  3.55177612e+01 -1.60924664e+01 -3.12627268e+00]
Minimum obj value:-4250.452201947017
Optimal xi: 25.964022
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1533.6747606915803
W_T_median: 1393.5327984216192
W_T_pctile_5: 674.5293119527929
W_T_CVAR_5_pct: 569.6349599187931
F value: -4250.452201947017
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.4
F value: -4250.452201947017
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ -38.4862,  -14.1766],
        [   1.0394,    2.9902],
        [   2.3374,    3.0078],
        [-191.9767,   -3.7593],
        [ -73.8694,   29.6753],
        [   2.5803,    3.0176],
        [   1.2592,    2.9886],
        [ -57.9375,   35.1863],
        [ -56.8100,   35.5178],
        [ -16.0925,   -3.1263]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[-2.1925e+01, -3.4624e+01, -3.6169e+01, -3.2675e+01, -3.4528e+01,
         -5.5299e+01, -4.5067e+01, -2.2018e+01, -4.5482e+01, -1.1349e+01],
        [-9.3576e+00,  2.6675e+02,  1.9437e+02, -1.7821e+00,  1.4839e+02,
          2.5785e+02,  3.3127e+02,  4.0654e+02,  4.8802e+02,  2.8368e-01],
        [-2.3289e+01, -3.2344e+01, -7.7366e+01, -3.3974e+01, -3.0161e+01,
         -8.2173e+01, -3.7189e+01, -1.5680e+01, -3.8423e+01, -1.3043e+01],
        [-2.5516e+01, -3.5015e+01, -4.7203e+01, -3.3739e+01, -3.1845e+01,
         -6.0996e+01, -4.2304e+01, -1.8230e+01, -4.1472e+01, -1.5178e+01],
        [-2.2289e+00, -3.2637e+01, -2.9163e+01,  1.1816e+01, -1.0901e+02,
         -3.1728e+01, -3.6849e+01, -2.4837e+01, -1.5045e+01,  4.3198e+00],
        [-3.3036e+00, -2.2992e+02, -2.5273e+02,  1.9589e+00, -1.1444e+02,
         -3.2064e+02, -2.7323e+02, -8.5144e+01, -1.5099e+02,  5.3033e+00],
        [-9.8087e+00,  3.4917e+01,  1.3723e+01, -2.4676e+00,  2.9878e+01,
          1.4391e+01,  3.7762e+01,  2.5092e+01,  2.2626e+01,  1.1931e+00],
        [-2.3197e+01, -3.2340e+01, -8.4908e+01, -3.4688e+01, -2.9331e+01,
         -8.8467e+01, -3.6308e+01, -1.5118e+01, -3.7256e+01, -1.2628e+01],
        [-2.7556e+00, -8.7380e+01, -1.6406e+01, -2.9300e+01, -3.3562e+01,
         -1.5595e+01, -9.4346e+01, -5.8385e+01, -1.0623e+02, -9.6406e+01],
        [-2.7109e+00, -2.6089e+01,  1.1179e+01, -2.7344e+01, -3.1050e+01,
          1.3396e+01, -3.0937e+01, -8.0942e+01, -1.2828e+02, -6.4757e+01]],
       device='cuda:0')), ('model.output_layer_3.weight', tensor([[  3.4052,  -0.7004,   7.7127,   5.3642,   1.5215,  -7.3744,  -7.9838,
           7.9110, -37.8614, -34.7152],
        [ -6.7604,   0.4044,  -8.1339,  -8.3900,  -1.6686,   7.7997,   5.2617,
          -8.2414,  38.1266,  34.4863]], device='cuda:0'))])
loaded xi:  25.964022
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -4402.255698182793
objective value function right now is: -4402.255698182793
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4400.274691351512
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4400.727009247601
8.0% of gradient descent iterations done. Method = Adam
updated min: -4402.60970555302
objective value function right now is: -4402.60970555302
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4399.248509684007
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4400.7981058346295
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -4399.095883772591
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4401.609479876413
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4401.552325941187
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4401.48757411143
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4401.13045565365
24.0% of gradient descent iterations done. Method = Adam
updated min: -4402.867110512802
objective value function right now is: -4402.867110512802
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4400.714046625309
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -4402.569363243953
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4401.94450379848
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4402.745435000394
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4399.9128298963715
36.0% of gradient descent iterations done. Method = Adam
updated min: -4403.307519956419
objective value function right now is: -4403.307519956419
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4401.811399421306
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4401.879683572326
42.0% of gradient descent iterations done. Method = Adam
updated min: -4403.5468038305435
objective value function right now is: -4403.5468038305435
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4401.750598211138
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4401.264684370507
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4402.796340503566
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4400.934741317753
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4401.153085072944
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4402.818174853108
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -4397.043653865838
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -4399.418259903465
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4400.617144991478
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4397.092688347358
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4401.728632685463
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4402.25512303428
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4399.207458146661
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4396.6940145423205
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4400.960100571193
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4398.911629448761
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4401.112854553192
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4402.843315210106
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4403.013779495316
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4403.006293959623
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4403.2498142095465
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4400.867161862291
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4398.283833681906
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4398.030578434987
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4402.625310619417
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4402.574258014128
updated min: 
obj fun: -4403.54759112124
updated min: 
obj fun: -4403.550556196531
updated min: 
obj fun: -4403.5530173002135
updated min: 
obj fun: -4403.5546878065525
updated min: 
obj fun: -4403.555241052352
updated min: 
obj fun: -4403.555560764696
updated min: 
obj fun: -4403.558301890485
updated min: 
obj fun: -4403.561586753093
updated min: 
obj fun: -4403.564971891243
updated min: 
obj fun: -4403.568027582274
updated min: 
obj fun: -4403.570422127526
updated min: 
obj fun: -4403.571457449835
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4402.75508275703
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4400.898503534252
updated min: 
obj fun: -4403.571492730479
updated min: 
obj fun: -4403.573164742317
updated min: 
obj fun: -4403.57514601825
updated min: 
obj fun: -4403.577271410277
updated min: 
obj fun: -4403.579197863175
updated min: 
obj fun: -4403.580815766744
updated min: 
obj fun: -4403.582069090549
updated min: 
obj fun: -4403.582992143635
updated min: 
obj fun: -4403.583629861335
updated min: 
obj fun: -4403.58409688987
updated min: 
obj fun: -4403.584441717124
updated min: 
obj fun: -4403.584734428235
updated min: 
obj fun: -4403.585171853565
updated min: 
obj fun: -4403.585871416828
updated min: 
obj fun: -4403.586922994065
updated min: 
obj fun: -4403.588278963255
updated min: 
obj fun: -4403.589707872579
updated min: 
obj fun: -4403.590957704534
updated min: 
obj fun: -4403.59175305476
updated min: 
obj fun: -4403.592196344664
updated min: 
obj fun: -4403.592256544032
updated min: 
obj fun: -4403.592513489832
updated min: 
obj fun: -4403.5933576192
updated min: 
obj fun: -4403.5944154603885
updated min: 
obj fun: -4403.595677272688
updated min: 
obj fun: -4403.597031631946
updated min: 
obj fun: -4403.598602449969
updated min: 
obj fun: -4403.600297064
updated min: 
obj fun: -4403.601849001875
updated min: 
obj fun: -4403.603043791218
updated min: 
obj fun: -4403.603811052082
updated min: 
obj fun: -4403.604063184295
updated min: 
obj fun: -4403.604302017633
updated min: 
obj fun: -4403.604935549813
updated min: 
obj fun: -4403.605928903417
updated min: 
obj fun: -4403.607241916897
updated min: 
obj fun: -4403.608957664721
updated min: 
obj fun: -4403.610677584457
updated min: 
obj fun: -4403.612139008
updated min: 
obj fun: -4403.613214762442
updated min: 
obj fun: -4403.613941646138
updated min: 
obj fun: -4403.614304957052
updated min: 
obj fun: -4403.614483125754
updated min: 
obj fun: -4403.61465663164
updated min: 
obj fun: -4403.615440477284
updated min: 
obj fun: -4403.616745691497
updated min: 
obj fun: -4403.618655351894
updated min: 
obj fun: -4403.620926028507
updated min: 
obj fun: -4403.623234995308
updated min: 
obj fun: -4403.6252970510295
updated min: 
obj fun: -4403.626974991605
updated min: 
obj fun: -4403.628101559
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4402.359859687083
NN weights: [-4.02062953e-01 -8.27236235e-01  3.78178906e+00  2.03745317e+00
  1.32489896e+00 -7.21711683e+00 -1.01183815e+01  3.82262325e+00
 -3.67778282e+01 -3.22719040e+01 -3.13824606e+00  4.97961640e-01
 -4.37293768e+00 -5.31002474e+00 -1.47107494e+00  7.64858007e+00
  4.89197206e+00 -4.31408978e+00  3.70432281e+01  3.20381317e+01
 -2.95323830e+01 -3.99804420e+01 -4.17266579e+01 -3.26754990e+01
 -3.45280151e+01 -6.06772041e+01 -5.03108482e+01 -2.20180473e+01
 -4.54817963e+01 -1.82867279e+01 -1.26047277e+01  2.66939301e+02
  2.01073318e+02 -8.57253134e-01  1.48389420e+02  2.64975006e+02
  3.30810608e+02  4.06542328e+02  4.88028717e+02  3.77627945e+00
 -2.89210358e+01 -3.57457809e+01 -8.10492783e+01 -3.39742050e+01
 -3.01613121e+01 -8.57250290e+01 -4.05282364e+01 -1.56803370e+01
 -3.84228821e+01 -1.91055489e+01 -3.01999531e+01 -3.81010208e+01
 -5.08002167e+01 -3.37390404e+01 -3.18450603e+01 -6.44668427e+01
 -4.53354530e+01 -1.82296715e+01 -4.14720497e+01 -2.07357006e+01
 -2.67352104e+00 -2.96781254e+01 -3.34440727e+01  1.69196358e+01
 -1.09006325e+02 -3.71203995e+01 -3.38480759e+01 -2.48373928e+01
 -1.55486879e+01  5.09576225e+00 -3.02136588e+00 -2.45771957e+02
 -2.63234406e+02  2.09954858e+00 -1.14435837e+02 -3.30894409e+02
 -2.89269348e+02 -8.51441116e+01 -1.50985565e+02  4.90716982e+00
 -9.72238255e+00  4.11674080e+01  2.14794197e+01 -1.85281582e+01
  2.98776951e+01  2.27859592e+01  4.45725822e+01  2.50918255e+01
  2.39775639e+01  7.50564456e-01 -2.90339718e+01 -3.57306023e+01
 -8.85770874e+01 -3.46877365e+01 -2.93305950e+01 -9.20045776e+01
 -3.96375008e+01 -1.51180067e+01 -3.72564468e+01 -1.87933617e+01
 -3.19844007e+00 -8.35382843e+01 -1.68798847e+01 -2.92995358e+01
 -3.35619087e+01 -1.62315006e+01 -9.04971848e+01 -5.83847961e+01
 -1.06216133e+02 -1.12339149e+02 -2.62538743e+00 -1.48060503e+01
  1.83441696e+01 -2.73435802e+01 -3.10566120e+01  2.06558323e+01
 -1.96948185e+01 -8.13981705e+01 -1.67843933e+02 -6.01826668e+01
 -3.87515411e+01 -1.53636265e+01  9.24551070e-01  2.98225117e+00
  2.13597608e+00  3.00905466e+00 -2.00451340e+02 -3.99949956e+00
 -7.38362961e+01  2.96851349e+01  2.11539268e+00  3.00248551e+00
  9.85454082e-01  2.98631120e+00 -5.17788048e+01  3.96251183e+01
  1.43951073e+01  1.88731461e+01 -1.84065514e+01 -3.62680840e+00]
Minimum obj value:-4403.630703148744
Optimal xi: 25.927706
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1535.8105250075982
W_T_median: 1395.9707040007593
W_T_pctile_5: 674.1544873648155
W_T_CVAR_5_pct: 564.1435865105705
F value: -4403.630703148744
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.5
F value: -4403.630703148744
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ -38.7515,  -15.3636],
        [   0.9246,    2.9823],
        [   2.1360,    3.0091],
        [-200.4513,   -3.9995],
        [ -73.8363,   29.6851],
        [   2.1154,    3.0025],
        [   0.9855,    2.9863],
        [ -51.7788,   39.6251],
        [  14.3951,   18.8731],
        [ -18.4066,   -3.6268]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[ -29.5324,  -39.9804,  -41.7267,  -32.6755,  -34.5280,  -60.6772,
          -50.3108,  -22.0180,  -45.4818,  -18.2867],
        [ -12.6047,  266.9393,  201.0733,   -0.8573,  148.3894,  264.9750,
          330.8106,  406.5423,  488.0287,    3.7763],
        [ -28.9210,  -35.7458,  -81.0493,  -33.9742,  -30.1613,  -85.7250,
          -40.5282,  -15.6803,  -38.4229,  -19.1055],
        [ -30.2000,  -38.1010,  -50.8002,  -33.7390,  -31.8451,  -64.4668,
          -45.3355,  -18.2297,  -41.4720,  -20.7357],
        [  -2.6735,  -29.6781,  -33.4441,   16.9196, -109.0063,  -37.1204,
          -33.8481,  -24.8374,  -15.5487,    5.0958],
        [  -3.0214, -245.7720, -263.2344,    2.0995, -114.4358, -330.8944,
         -289.2693,  -85.1441, -150.9856,    4.9072],
        [  -9.7224,   41.1674,   21.4794,  -18.5282,   29.8777,   22.7860,
           44.5726,   25.0918,   23.9776,    0.7506],
        [ -29.0340,  -35.7306,  -88.5771,  -34.6877,  -29.3306,  -92.0046,
          -39.6375,  -15.1180,  -37.2564,  -18.7934],
        [  -3.1984,  -83.5383,  -16.8799,  -29.2995,  -33.5619,  -16.2315,
          -90.4972,  -58.3848, -106.2161, -112.3391],
        [  -2.6254,  -14.8061,   18.3442,  -27.3436,  -31.0566,   20.6558,
          -19.6948,  -81.3982, -167.8439,  -60.1827]], device='cuda:0')), ('model.output_layer_3.weight', tensor([[ -0.4021,  -0.8272,   3.7818,   2.0375,   1.3249,  -7.2171, -10.1184,
           3.8226, -36.7778, -32.2719],
        [ -3.1382,   0.4980,  -4.3729,  -5.3100,  -1.4711,   7.6486,   4.8920,
          -4.3141,  37.0432,  32.0381]], device='cuda:0'))])
loaded xi:  25.927706
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -4554.08768084064
objective value function right now is: -4554.08768084064
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4553.135129723732
6.0% of gradient descent iterations done. Method = Adam
updated min: -4555.562772180706
objective value function right now is: -4555.562772180706
8.0% of gradient descent iterations done. Method = Adam
updated min: -4556.204381017387
objective value function right now is: -4556.204381017387
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4555.783621985065
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4555.197621511896
14.000000000000002% of gradient descent iterations done. Method = Adam
updated min: -4556.715890038143
objective value function right now is: -4556.715890038143
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4555.161156224552
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4554.804886420069
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4556.098241581355
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4552.067442788051
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4552.2043766923025
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4555.298982101084
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -4554.571231346727
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4553.719376055625
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4555.337057207203
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4554.298845540575
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4556.383224738882
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4555.326839494275
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4555.118793987598
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4555.521625068461
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4553.3288025167285
46.0% of gradient descent iterations done. Method = Adam
updated min: -4556.754940051638
objective value function right now is: -4556.754940051638
48.0% of gradient descent iterations done. Method = Adam
updated min: -4557.066918140365
objective value function right now is: -4557.066918140365
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4556.254647347707
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4555.871319097709
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4553.672892449994
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -4552.463175280882
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -4554.552444558255
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4554.6062691939005
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4554.745243662935
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4554.436480924899
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4554.1572739785015
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4555.875704771843
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4557.066438009472
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4554.814787718378
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4556.523731909379
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4554.973809456906
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4555.556078979293
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4555.294496471125
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4551.77635637126
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4554.2304458609
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4555.214042022033
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4550.982465547053
updated min: 
obj fun: -4557.128175142782
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4553.665716865947
updated min: 
obj fun: -4557.129891314772
updated min: 
obj fun: -4557.156238848644
updated min: 
obj fun: -4557.172653495293
updated min: 
obj fun: -4557.181589112062
updated min: 
obj fun: -4557.188363422845
updated min: 
obj fun: -4557.19533372682
updated min: 
obj fun: -4557.204820084201
updated min: 
obj fun: -4557.218539599493
updated min: 
obj fun: -4557.230353910697
updated min: 
obj fun: -4557.251982648462
updated min: 
obj fun: -4557.273406508073
updated min: 
obj fun: -4557.291947774917
updated min: 
obj fun: -4557.307496985465
updated min: 
obj fun: -4557.3215650258235
updated min: 
obj fun: -4557.333293701589
updated min: 
obj fun: -4557.344214413328
updated min: 
obj fun: -4557.351520384338
updated min: 
obj fun: -4557.356591334568
updated min: 
obj fun: -4557.358324487255
updated min: 
obj fun: -4557.362016028586
updated min: 
obj fun: -4557.3698732091825
updated min: 
obj fun: -4557.3778148194015
updated min: 
obj fun: -4557.38313221696
updated min: 
obj fun: -4557.385612180776
updated min: 
obj fun: -4557.392061288731
updated min: 
obj fun: -4557.413861320378
updated min: 
obj fun: -4557.434793257861
updated min: 
obj fun: -4557.453259008158
updated min: 
obj fun: -4557.468705238663
updated min: 
obj fun: -4557.4813661538
updated min: 
obj fun: -4557.491027616615
updated min: 
obj fun: -4557.497336083143
updated min: 
obj fun: -4557.502021180011
updated min: 
obj fun: -4557.505066285971
updated min: 
obj fun: -4557.507483765416
updated min: 
obj fun: -4557.508692123315
updated min: 
obj fun: -4557.508849790236
updated min: 
obj fun: -4557.509786094432
updated min: 
obj fun: -4557.515345883179
updated min: 
obj fun: -4557.5238093527005
updated min: 
obj fun: -4557.5329711211325
updated min: 
obj fun: -4557.541268806517
updated min: 
obj fun: -4557.548345364938
updated min: 
obj fun: -4557.554265054211
updated min: 
obj fun: -4557.559619278069
updated min: 
obj fun: -4557.564155410256
updated min: 
obj fun: -4557.567271909119
updated min: 
obj fun: -4557.569380279332
updated min: 
obj fun: -4557.570160650926
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4556.103917575284
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4556.794870200723
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4551.826879151096
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4549.257759699795
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4552.003273523921
NN weights: [-2.99214745e+00 -8.79227638e-01  1.35894060e+00 -6.17133796e-01
  1.35159969e+00 -7.79794550e+00 -1.16310406e+01  1.33976042e+00
 -3.65037193e+01 -2.82738247e+01 -4.46652460e+00  5.13911009e-01
 -3.34609866e+00 -6.11233282e+00 -1.49689853e+00  8.23547554e+00
  3.43088007e+00 -2.96150422e+00  3.67691040e+01  2.80387497e+01
 -3.08783379e+01 -4.02307320e+01 -4.20372391e+01 -3.26754990e+01
 -3.45280151e+01 -6.09781342e+01 -5.05395432e+01 -2.20180473e+01
 -4.59907379e+01 -1.97781429e+01 -1.37059212e+01  2.73503876e+02
  2.10067245e+02  3.42634916e-01  1.48389420e+02  2.74045807e+02
  3.36361755e+02  4.06542328e+02  5.05204865e+02  6.30230761e+00
 -2.99748554e+01 -3.65615273e+01 -8.21597824e+01 -3.39742050e+01
 -3.01613121e+01 -8.68330078e+01 -4.13518257e+01 -1.56803370e+01
 -4.00088806e+01 -2.04881401e+01 -3.05422268e+01 -3.82736893e+01
 -5.09524956e+01 -3.37390404e+01 -3.18450603e+01 -6.46164551e+01
 -4.54990196e+01 -1.82296715e+01 -4.21843605e+01 -2.17751560e+01
 -2.35397553e+00 -2.63269653e+01 -3.45671463e+01  2.52064590e+01
 -1.09006325e+02 -3.92695999e+01 -3.03161736e+01 -2.48373928e+01
 -4.12336807e+01  4.45160341e+00 -3.09110594e+00 -2.59145233e+02
 -2.73618591e+02  1.69648790e+00 -1.14435837e+02 -3.40876190e+02
 -3.02921417e+02 -8.51441116e+01 -1.49832214e+02  4.72473383e+00
 -1.06354418e+01  4.68584099e+01  2.78283234e+01 -3.06900158e+01
  2.98776951e+01  2.97357998e+01  5.07440300e+01  2.50918255e+01
  3.12199802e+01  6.89389110e-01 -3.02373924e+01 -3.66481552e+01
 -8.98221207e+01 -3.46877365e+01 -2.93305950e+01 -9.32458878e+01
 -4.05642891e+01 -1.51180067e+01 -3.88005447e+01 -2.02825489e+01
 -3.95180202e+00 -8.20923233e+01 -1.98181629e+01 -2.92995358e+01
 -3.35619087e+01 -1.89681225e+01 -8.86963196e+01 -5.83847961e+01
 -9.97557678e+01 -1.33838562e+02 -3.46849847e+00 -9.58416557e+00
  1.86402950e+01 -2.73435802e+01 -3.10566120e+01  2.11504078e+01
 -1.43598824e+01 -8.13981705e+01 -1.73942169e+02 -5.22703400e+01
 -4.06413536e+01 -1.48501854e+01  1.02114069e+00  3.04262567e+00
  1.65617323e+00  2.97883177e+00 -2.09305084e+02 -4.33150911e+00
 -7.38362961e+01  2.96849232e+01  1.59862316e+00  2.97851586e+00
  9.90007699e-01  3.04308867e+00 -5.17788048e+01  3.96251183e+01
  2.46784344e+01  1.67033367e+01 -1.96001968e+01 -3.52529478e+00]
Minimum obj value:-4557.573732889684
Optimal xi: 25.880514
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1536.3909660848321
W_T_median: 1396.0682966059885
W_T_pctile_5: 670.4962075168628
W_T_CVAR_5_pct: 562.9615546051158
F value: -4557.573732889684
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.6
F value: -4557.573732889684
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ -40.6414,  -14.8502],
        [   1.0211,    3.0426],
        [   1.6562,    2.9788],
        [-209.3051,   -4.3315],
        [ -73.8363,   29.6849],
        [   1.5986,    2.9785],
        [   0.9900,    3.0431],
        [ -51.7788,   39.6251],
        [  24.6784,   16.7033],
        [ -19.6002,   -3.5253]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[-3.0878e+01, -4.0231e+01, -4.2037e+01, -3.2675e+01, -3.4528e+01,
         -6.0978e+01, -5.0540e+01, -2.2018e+01, -4.5991e+01, -1.9778e+01],
        [-1.3706e+01,  2.7350e+02,  2.1007e+02,  3.4263e-01,  1.4839e+02,
          2.7405e+02,  3.3636e+02,  4.0654e+02,  5.0520e+02,  6.3023e+00],
        [-2.9975e+01, -3.6562e+01, -8.2160e+01, -3.3974e+01, -3.0161e+01,
         -8.6833e+01, -4.1352e+01, -1.5680e+01, -4.0009e+01, -2.0488e+01],
        [-3.0542e+01, -3.8274e+01, -5.0952e+01, -3.3739e+01, -3.1845e+01,
         -6.4616e+01, -4.5499e+01, -1.8230e+01, -4.2184e+01, -2.1775e+01],
        [-2.3540e+00, -2.6327e+01, -3.4567e+01,  2.5206e+01, -1.0901e+02,
         -3.9270e+01, -3.0316e+01, -2.4837e+01, -4.1234e+01,  4.4516e+00],
        [-3.0911e+00, -2.5915e+02, -2.7362e+02,  1.6965e+00, -1.1444e+02,
         -3.4088e+02, -3.0292e+02, -8.5144e+01, -1.4983e+02,  4.7247e+00],
        [-1.0635e+01,  4.6858e+01,  2.7828e+01, -3.0690e+01,  2.9878e+01,
          2.9736e+01,  5.0744e+01,  2.5092e+01,  3.1220e+01,  6.8939e-01],
        [-3.0237e+01, -3.6648e+01, -8.9822e+01, -3.4688e+01, -2.9331e+01,
         -9.3246e+01, -4.0564e+01, -1.5118e+01, -3.8801e+01, -2.0283e+01],
        [-3.9518e+00, -8.2092e+01, -1.9818e+01, -2.9300e+01, -3.3562e+01,
         -1.8968e+01, -8.8696e+01, -5.8385e+01, -9.9756e+01, -1.3384e+02],
        [-3.4685e+00, -9.5842e+00,  1.8640e+01, -2.7344e+01, -3.1057e+01,
          2.1150e+01, -1.4360e+01, -8.1398e+01, -1.7394e+02, -5.2270e+01]],
       device='cuda:0')), ('model.output_layer_3.weight', tensor([[ -2.9921,  -0.8792,   1.3589,  -0.6171,   1.3516,  -7.7979, -11.6310,
           1.3398, -36.5037, -28.2738],
        [ -4.4665,   0.5139,  -3.3461,  -6.1123,  -1.4969,   8.2355,   3.4309,
          -2.9615,  36.7691,  28.0387]], device='cuda:0'))])
loaded xi:  25.880514
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -4708.155184099639
objective value function right now is: -4708.155184099639
4.0% of gradient descent iterations done. Method = Adam
updated min: -4709.711820564992
objective value function right now is: -4709.711820564992
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4707.064370763245
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.63673853463
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4708.877994524155
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.681538462421
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -4708.538412248319
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4705.057171057554
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.639456074728
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.591919979585
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.411152643962
24.0% of gradient descent iterations done. Method = Adam
updated min: -4709.932274997531
objective value function right now is: -4709.932274997531
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.123630851513
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -4705.118618863052
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.336809211675
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4708.843688531058
34.0% of gradient descent iterations done. Method = Adam
updated min: -4710.79223429165
objective value function right now is: -4710.79223429165
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4710.137644069478
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.790395613456
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.624672878057
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.104177238297
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4710.564837781917
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4703.331160787411
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4707.577939400803
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4707.602420418896
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4708.26059765093
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4705.374375790166
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.058437387031
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -4708.545683878465
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4708.1582091067285
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.945205974244
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4708.986937320857
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4707.636279113571
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.5031078561415
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4708.490644043859
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4710.675546024068
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.130481922219
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.782426768426
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.475897161407
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4710.590718122516
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4707.937306849415
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4707.421764905133
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.923443353141
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.279338781189
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.093833616581
updated min: 
obj fun: -4710.798025208058
updated min: 
obj fun: -4710.8101661992705
updated min: 
obj fun: -4710.82502592017
updated min: 
obj fun: -4710.841261533277
updated min: 
obj fun: -4710.856468503561
updated min: 
obj fun: -4710.868533991716
updated min: 
obj fun: -4710.87746234742
updated min: 
obj fun: -4710.883478667033
updated min: 
obj fun: -4710.886800373206
updated min: 
obj fun: -4710.887717585811
updated min: 
obj fun: -4710.8920097444525
updated min: 
obj fun: -4710.90523206002
updated min: 
obj fun: -4710.917700807843
updated min: 
obj fun: -4710.928520448807
updated min: 
obj fun: -4710.935714297514
updated min: 
obj fun: -4710.939803103925
updated min: 
obj fun: -4710.941265708104
updated min: 
obj fun: -4710.946001693817
updated min: 
obj fun: -4710.952645711081
updated min: 
obj fun: -4710.95765225576
updated min: 
obj fun: -4710.9600527604325
updated min: 
obj fun: -4710.960138000375
updated min: 
obj fun: -4710.961755401462
updated min: 
obj fun: -4710.969435708786
updated min: 
obj fun: -4710.976971507187
updated min: 
obj fun: -4710.984329727633
updated min: 
obj fun: -4710.990598530317
updated min: 
obj fun: -4710.995754304995
updated min: 
obj fun: -4710.999864122861
updated min: 
obj fun: -4711.0026992412095
updated min: 
obj fun: -4711.00428804099
updated min: 
obj fun: -4711.005444109115
updated min: 
obj fun: -4711.006990359505
updated min: 
obj fun: -4711.008970190042
updated min: 
obj fun: -4711.011346476132
updated min: 
obj fun: -4711.0149164771
updated min: 
obj fun: -4711.019583905311
updated min: 
obj fun: -4711.024931874617
updated min: 
obj fun: -4711.030725748714
updated min: 
obj fun: -4711.035395726065
updated min: 
obj fun: -4711.038776242711
updated min: 
obj fun: -4711.040574958362
updated min: 
obj fun: -4711.041182593357
updated min: 
obj fun: -4711.044610415387
updated min: 
obj fun: -4711.050175234316
updated min: 
obj fun: -4711.056002751794
updated min: 
obj fun: -4711.062276424749
updated min: 
obj fun: -4711.069403958978
updated min: 
obj fun: -4711.076802108137
updated min: 
obj fun: -4711.0843309117945
updated min: 
obj fun: -4711.091812828886
updated min: 
obj fun: -4711.098835441019
updated min: 
obj fun: -4711.10547107531
updated min: 
obj fun: -4711.112511528541
updated min: 
obj fun: -4711.119659380014
updated min: 
obj fun: -4711.1267075556225
updated min: 
obj fun: -4711.133087489659
updated min: 
obj fun: -4711.138791852962
updated min: 
obj fun: -4711.143498810593
updated min: 
obj fun: -4711.147449239334
updated min: 
obj fun: -4711.150881490834
updated min: 
obj fun: -4711.1534268030555
updated min: 
obj fun: -4711.155467336394
updated min: 
obj fun: -4711.157497169951
updated min: 
obj fun: -4711.15927031311
updated min: 
obj fun: -4711.160975441309
updated min: 
obj fun: -4711.16296912906
updated min: 
obj fun: -4711.165142940285
updated min: 
obj fun: -4711.166617283597
updated min: 
obj fun: -4711.167371818986
updated min: 
obj fun: -4711.167618621409
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.458813902595
updated min: 
obj fun: -4711.167688567058
updated min: 
obj fun: -4711.16816870714
updated min: 
obj fun: -4711.169468409406
updated min: 
obj fun: -4711.17121350917
updated min: 
obj fun: -4711.172761802376
updated min: 
obj fun: -4711.1740897602
updated min: 
obj fun: -4711.175797830634
updated min: 
obj fun: -4711.17779176099
updated min: 
obj fun: -4711.179877882492
updated min: 
obj fun: -4711.182337233677
updated min: 
obj fun: -4711.184982011726
updated min: 
obj fun: -4711.18769600156
updated min: 
obj fun: -4711.190077333581
updated min: 
obj fun: -4711.192115358573
updated min: 
obj fun: -4711.193574416513
updated min: 
obj fun: -4711.194821800137
updated min: 
obj fun: -4711.196358128659
updated min: 
obj fun: -4711.198215158618
updated min: 
obj fun: -4711.200133476403
updated min: 
obj fun: -4711.202268586289
updated min: 
obj fun: -4711.2044807939665
updated min: 
obj fun: -4711.206759114142
updated min: 
obj fun: -4711.209023699763
updated min: 
obj fun: -4711.21119012446
updated min: 
obj fun: -4711.212967191215
updated min: 
obj fun: -4711.214144744678
updated min: 
obj fun: -4711.214582704402
updated min: 
obj fun: -4711.214816824805
updated min: 
obj fun: -4711.217373152633
updated min: 
obj fun: -4711.219830662077
updated min: 
obj fun: -4711.221814740021
updated min: 
obj fun: -4711.2232762646545
updated min: 
obj fun: -4711.224218635974
updated min: 
obj fun: -4711.224797838536
updated min: 
obj fun: -4711.225633209294
updated min: 
obj fun: -4711.226935201941
updated min: 
obj fun: -4711.227703738669
updated min: 
obj fun: -4711.227990528321
updated min: 
obj fun: -4711.228050320663
updated min: 
obj fun: -4711.229589805476
updated min: 
obj fun: -4711.231539849174
updated min: 
obj fun: -4711.2334419738745
updated min: 
obj fun: -4711.234972483197
updated min: 
obj fun: -4711.23619250816
updated min: 
obj fun: -4711.237213409167
updated min: 
obj fun: -4711.237966667144
updated min: 
obj fun: -4711.238568746914
updated min: 
obj fun: -4711.239105190415
updated min: 
obj fun: -4711.239437551414
updated min: 
obj fun: -4711.239634770248
updated min: 
obj fun: -4711.2397116894
updated min: 
obj fun: -4711.2397684863
updated min: 
obj fun: -4711.239916111233
updated min: 
obj fun: -4711.240168607811
updated min: 
obj fun: -4711.240528984639
updated min: 
obj fun: -4711.2409176885085
updated min: 
obj fun: -4711.241190653559
updated min: 
obj fun: -4711.2418946873395
updated min: 
obj fun: -4711.243691219285
updated min: 
obj fun: -4711.245163952125
updated min: 
obj fun: -4711.2463766860965
updated min: 
obj fun: -4711.247377395772
updated min: 
obj fun: -4711.248223575249
updated min: 
obj fun: -4711.249106637533
updated min: 
obj fun: -4711.249982334652
updated min: 
obj fun: -4711.251012452279
updated min: 
obj fun: -4711.2521579530685
updated min: 
obj fun: -4711.253341151966
updated min: 
obj fun: -4711.254554561297
updated min: 
obj fun: -4711.255736956007
updated min: 
obj fun: -4711.256872296765
updated min: 
obj fun: -4711.258091120995
updated min: 
obj fun: -4711.2594345912075
updated min: 
obj fun: -4711.260770772769
updated min: 
obj fun: -4711.261963278365
updated min: 
obj fun: -4711.263001518532
updated min: 
obj fun: -4711.263797727319
updated min: 
obj fun: -4711.264500321207
updated min: 
obj fun: -4711.265130055361
updated min: 
obj fun: -4711.265763299979
updated min: 
obj fun: -4711.266227125523
updated min: 
obj fun: -4711.266574443795
updated min: 
obj fun: -4711.266802196965
updated min: 
obj fun: -4711.266987270297
updated min: 
obj fun: -4711.266995720203
updated min: 
obj fun: -4711.267110245665
updated min: 
obj fun: -4711.267693699198
updated min: 
obj fun: -4711.268114131856
updated min: 
obj fun: -4711.268481548793
updated min: 
obj fun: -4711.268797128106
updated min: 
obj fun: -4711.26926464079
updated min: 
obj fun: -4711.27004154439
updated min: 
obj fun: -4711.271269976126
updated min: 
obj fun: -4711.272954871148
updated min: 
obj fun: -4711.274889941864
updated min: 
obj fun: -4711.276940494251
updated min: 
obj fun: -4711.279074363252
updated min: 
obj fun: -4711.281116490976
updated min: 
obj fun: -4711.283010310274
updated min: 
obj fun: -4711.284562349204
updated min: 
obj fun: -4711.285454870701
updated min: 
obj fun: -4711.285991853401
updated min: 
obj fun: -4711.286286025328
updated min: 
obj fun: -4711.28649670069
updated min: 
obj fun: -4711.286591551722
updated min: 
obj fun: -4711.286973696756
updated min: 
obj fun: -4711.2874914722315
updated min: 
obj fun: -4711.287916070906
updated min: 
obj fun: -4711.288132413515
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4707.122934215639
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4708.892450567301
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4708.720817507488
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4709.139471046381
NN weights: [-1.15262241e+01 -8.37724030e-01 -2.44266248e+00 -9.14515400e+00
  1.51315868e+00 -7.70955086e+00 -1.36708889e+01 -2.14748597e+00
 -3.79729958e+01 -2.60961304e+01 -1.24421654e+01  4.26460028e-01
 -6.34640026e+00 -1.37903166e+01 -1.65760875e+00  8.15294456e+00
  1.20709324e+00 -5.19137907e+00  3.82383614e+01  2.58601418e+01
 -3.29134789e+01 -3.84802361e+01 -3.91106606e+01 -3.26754990e+01
 -3.45280151e+01 -5.80226898e+01 -4.85599365e+01 -2.20180473e+01
 -4.02312164e+01 -2.14013367e+01 -1.47845383e+01  2.79449554e+02
  2.17063004e+02 -3.12585163e+00  1.48389420e+02  2.81084534e+02
  3.41258606e+02  4.06542328e+02  5.18313293e+02  7.36778545e+00
 -3.13796997e+01 -3.79143372e+01 -8.34951401e+01 -3.39742050e+01
 -3.01613121e+01 -8.82309723e+01 -4.27473946e+01 -1.56803370e+01
 -3.90297356e+01 -2.19183807e+01 -3.19409275e+01 -3.66312256e+01
 -4.82791672e+01 -3.37390404e+01 -3.18450603e+01 -6.19506950e+01
 -4.36544418e+01 -1.82296715e+01 -3.69181366e+01 -2.34333153e+01
 -3.29682446e+00 -2.72217808e+01 -3.79958076e+01  2.47597790e+01
 -1.09006325e+02 -4.34039192e+01 -3.10718536e+01 -2.48373928e+01
 -6.33330040e+01  5.15984821e+00 -2.99225950e+00 -2.71290497e+02
 -2.83899445e+02  3.26806712e+00 -1.14435837e+02 -3.50835876e+02
 -3.15323303e+02 -8.51441116e+01 -1.46965912e+02  4.78180647e+00
 -1.28759747e+01  4.90237770e+01  3.11012955e+01 -4.26617393e+01
  2.98776951e+01  3.33450699e+01  5.30092545e+01  2.50918255e+01
  3.12947235e+01  2.55294633e+00 -3.19119091e+01 -3.81950912e+01
 -9.14264908e+01 -3.46877365e+01 -2.93305950e+01 -9.49094391e+01
 -4.21681137e+01 -1.51180067e+01 -3.80547981e+01 -2.17798996e+01
 -2.93237257e+00 -8.21330185e+01 -2.38731995e+01 -2.92995358e+01
 -3.35619087e+01 -2.32496929e+01 -8.83914490e+01 -5.83847961e+01
 -8.59580231e+01 -1.61513428e+02 -3.88405132e+00 -4.90646410e+00
  2.02161369e+01 -2.73435802e+01 -3.10566120e+01  2.28453560e+01
 -9.38616562e+00 -8.13981705e+01 -1.78849213e+02 -4.60665550e+01
 -4.33969536e+01 -1.41330490e+01  1.60130763e+00  3.02273560e+00
  2.30389476e+00  3.05023837e+00 -2.17209564e+02 -4.27586555e+00
 -7.38362961e+01  2.96839523e+01  2.25468230e+00  3.04646492e+00
  1.58780932e+00  3.02084541e+00 -5.17788048e+01  3.96251183e+01
  3.04580593e+01  2.02263527e+01 -1.92283268e+01 -3.51827335e+00]
Minimum obj value:-4711.289798916868
Optimal xi: 25.798056
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1537.2206860617134
W_T_median: 1396.9264328899246
W_T_pctile_5: 667.0285650516618
W_T_CVAR_5_pct: 560.8183066579732
F value: -4711.289798916868
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.7
F value: -4711.289798916868
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ -43.3970,  -14.1330],
        [   1.6013,    3.0227],
        [   2.3039,    3.0502],
        [-217.2096,   -4.2759],
        [ -73.8363,   29.6840],
        [   2.2547,    3.0465],
        [   1.5878,    3.0208],
        [ -51.7788,   39.6251],
        [  30.4581,   20.2264],
        [ -19.2283,   -3.5183]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[ -32.9135,  -38.4802,  -39.1107,  -32.6755,  -34.5280,  -58.0227,
          -48.5599,  -22.0180,  -40.2312,  -21.4013],
        [ -14.7845,  279.4496,  217.0630,   -3.1259,  148.3894,  281.0845,
          341.2586,  406.5423,  518.3133,    7.3678],
        [ -31.3797,  -37.9143,  -83.4951,  -33.9742,  -30.1613,  -88.2310,
          -42.7474,  -15.6803,  -39.0297,  -21.9184],
        [ -31.9409,  -36.6312,  -48.2792,  -33.7390,  -31.8451,  -61.9507,
          -43.6544,  -18.2297,  -36.9181,  -23.4333],
        [  -3.2968,  -27.2218,  -37.9958,   24.7598, -109.0063,  -43.4039,
          -31.0719,  -24.8374,  -63.3330,    5.1598],
        [  -2.9923, -271.2905, -283.8994,    3.2681, -114.4358, -350.8359,
         -315.3233,  -85.1441, -146.9659,    4.7818],
        [ -12.8760,   49.0238,   31.1013,  -42.6617,   29.8777,   33.3451,
           53.0093,   25.0918,   31.2947,    2.5529],
        [ -31.9119,  -38.1951,  -91.4265,  -34.6877,  -29.3306,  -94.9094,
          -42.1681,  -15.1180,  -38.0548,  -21.7799],
        [  -2.9324,  -82.1330,  -23.8732,  -29.2995,  -33.5619,  -23.2497,
          -88.3914,  -58.3848,  -85.9580, -161.5134],
        [  -3.8841,   -4.9065,   20.2161,  -27.3436,  -31.0566,   22.8454,
           -9.3862,  -81.3982, -178.8492,  -46.0666]], device='cuda:0')), ('model.output_layer_3.weight', tensor([[-11.5262,  -0.8377,  -2.4427,  -9.1452,   1.5132,  -7.7096, -13.6709,
          -2.1475, -37.9730, -26.0961],
        [-12.4422,   0.4265,  -6.3464, -13.7903,  -1.6576,   8.1529,   1.2071,
          -5.1914,  38.2384,  25.8601]], device='cuda:0'))])
loaded xi:  25.798056
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -4862.361159741157
objective value function right now is: -4862.361159741157
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4861.215091616353
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4860.562980296494
8.0% of gradient descent iterations done. Method = Adam
updated min: -4863.70501515806
objective value function right now is: -4863.70501515806
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4860.164736373897
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4859.0696404815435
14.000000000000002% of gradient descent iterations done. Method = Adam
updated min: -4864.378572501227
objective value function right now is: -4864.378572501227
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4863.089270957161
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4864.096835661036
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4862.372960437019
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4862.3188873612
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4861.725559231701
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4861.691597148449
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -4862.386260339543
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4863.170077803398
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4862.900190524796
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4863.450234973054
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4859.774028133957
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4863.194842441045
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4862.5540824331165
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4862.411833400147
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4862.383002263583
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4861.370676376211
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4863.140866422793
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4862.603333550051
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4863.28908510978
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4863.433152755706
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -4860.135941746695
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -4861.645813251372
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4861.911292648379
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4859.44331789949
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4863.321994995421
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4860.077859860784
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4863.241842147233
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4863.3453464694585
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4863.137488934694
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4862.796003056251
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4863.595034224004
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4862.909121281107
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4860.147985478119
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4863.133960450227
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4851.11053073868
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4862.020976586247
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4864.108720323967
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4861.835088146285
updated min: 
obj fun: -4864.391184058431
updated min: 
obj fun: -4864.418804543965
updated min: 
obj fun: -4864.444429151764
updated min: 
obj fun: -4864.468681872995
updated min: 
obj fun: -4864.4921099141775
updated min: 
obj fun: -4864.510582931133
updated min: 
obj fun: -4864.519813359066
updated min: 
obj fun: -4864.53642551439
updated min: 
obj fun: -4864.551132065985
updated min: 
obj fun: -4864.560851081686
updated min: 
obj fun: -4864.566822166739
updated min: 
obj fun: -4864.5711196622815
updated min: 
obj fun: -4864.574946719868
updated min: 
obj fun: -4864.578674528019
updated min: 
obj fun: -4864.581865910508
updated min: 
obj fun: -4864.584574843809
updated min: 
obj fun: -4864.586907781022
updated min: 
obj fun: -4864.588483913005
updated min: 
obj fun: -4864.589684070343
updated min: 
obj fun: -4864.589943578899
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4854.8297608193425
updated min: 
obj fun: -4864.595825900324
updated min: 
obj fun: -4864.603719256773
updated min: 
obj fun: -4864.610826493761
updated min: 
obj fun: -4864.616962967206
updated min: 
obj fun: -4864.622529716386
updated min: 
obj fun: -4864.627535133867
updated min: 
obj fun: -4864.632122418413
updated min: 
obj fun: -4864.636323620805
updated min: 
obj fun: -4864.639950985274
updated min: 
obj fun: -4864.643050698102
updated min: 
obj fun: -4864.645924638141
updated min: 
obj fun: -4864.649159009595
updated min: 
obj fun: -4864.652345250297
updated min: 
obj fun: -4864.655322116691
updated min: 
obj fun: -4864.658145549998
updated min: 
obj fun: -4864.660979238715
updated min: 
obj fun: -4864.664168059451
updated min: 
obj fun: -4864.667674341841
updated min: 
obj fun: -4864.67164304764
updated min: 
obj fun: -4864.675530104834
updated min: 
obj fun: -4864.679327712331
updated min: 
obj fun: -4864.682688182198
updated min: 
obj fun: -4864.685290280839
updated min: 
obj fun: -4864.686859390333
updated min: 
obj fun: -4864.687594994961
updated min: 
obj fun: -4864.68933414972
updated min: 
obj fun: -4864.692132075493
updated min: 
obj fun: -4864.695492572677
updated min: 
obj fun: -4864.699751878569
updated min: 
obj fun: -4864.704762572817
updated min: 
obj fun: -4864.71007189862
updated min: 
obj fun: -4864.715445711276
updated min: 
obj fun: -4864.720480897939
updated min: 
obj fun: -4864.724920542826
updated min: 
obj fun: -4864.729309117504
updated min: 
obj fun: -4864.733541206503
updated min: 
obj fun: -4864.737424567496
updated min: 
obj fun: -4864.741195081705
updated min: 
obj fun: -4864.744624274395
updated min: 
obj fun: -4864.747619801562
updated min: 
obj fun: -4864.750032594724
updated min: 
obj fun: -4864.751910458764
updated min: 
obj fun: -4864.753285929978
updated min: 
obj fun: -4864.754317450922
updated min: 
obj fun: -4864.755116896611
updated min: 
obj fun: -4864.7559044496775
updated min: 
obj fun: -4864.756654336263
updated min: 
obj fun: -4864.757399317467
updated min: 
obj fun: -4864.758318284466
updated min: 
obj fun: -4864.759149302919
updated min: 
obj fun: -4864.759760213541
updated min: 
obj fun: -4864.760119573146
updated min: 
obj fun: -4864.760574410959
updated min: 
obj fun: -4864.761025943926
updated min: 
obj fun: -4864.761139186865
updated min: 
obj fun: -4864.761990059169
updated min: 
obj fun: -4864.7631530514245
updated min: 
obj fun: -4864.764585202066
updated min: 
obj fun: -4864.7663200608695
updated min: 
obj fun: -4864.768463622407
updated min: 
obj fun: -4864.771010968117
updated min: 
obj fun: -4864.773546552207
updated min: 
obj fun: -4864.776197828828
updated min: 
obj fun: -4864.778856352139
updated min: 
obj fun: -4864.781146887264
updated min: 
obj fun: -4864.78316421858
updated min: 
obj fun: -4864.784929281035
updated min: 
obj fun: -4864.786376544909
updated min: 
obj fun: -4864.787182074098
updated min: 
obj fun: -4864.787442147276
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4862.842934013835
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4862.903271649296
updated min: 
obj fun: -4864.787887037637
updated min: 
obj fun: -4864.789217998192
updated min: 
obj fun: -4864.7906068527345
updated min: 
obj fun: -4864.792066585914
updated min: 
obj fun: -4864.793471446119
updated min: 
obj fun: -4864.7948380889875
updated min: 
obj fun: -4864.7961048704165
updated min: 
obj fun: -4864.797237141953
updated min: 
obj fun: -4864.798322577908
updated min: 
obj fun: -4864.79941552552
updated min: 
obj fun: -4864.800543564538
updated min: 
obj fun: -4864.801539658704
updated min: 
obj fun: -4864.802388971291
updated min: 
obj fun: -4864.803146324829
updated min: 
obj fun: -4864.803800781955
updated min: 
obj fun: -4864.804357912086
updated min: 
obj fun: -4864.8048517197785
updated min: 
obj fun: -4864.805302477228
updated min: 
obj fun: -4864.80580588819
updated min: 
obj fun: -4864.806411228775
updated min: 
obj fun: -4864.807123371768
updated min: 
obj fun: -4864.808050366767
updated min: 
obj fun: -4864.80899688883
updated min: 
obj fun: -4864.809901155139
updated min: 
obj fun: -4864.810602956189
updated min: 
obj fun: -4864.811138833799
updated min: 
obj fun: -4864.811532716455
updated min: 
obj fun: -4864.811813672356
updated min: 
obj fun: -4864.8121041187405
updated min: 
obj fun: -4864.81245703345
updated min: 
obj fun: -4864.813039930861
updated min: 
obj fun: -4864.813874297529
updated min: 
obj fun: -4864.814878220041
updated min: 
obj fun: -4864.815858430038
updated min: 
obj fun: -4864.816707572302
updated min: 
obj fun: -4864.817301949281
updated min: 
obj fun: -4864.817664398872
updated min: 
obj fun: -4864.817793596562
updated min: 
obj fun: -4864.818057512379
updated min: 
obj fun: -4864.818576477157
updated min: 
obj fun: -4864.819291714622
updated min: 
obj fun: -4864.820125280293
updated min: 
obj fun: -4864.820963523254
updated min: 
obj fun: -4864.821630562364
updated min: 
obj fun: -4864.822146784937
updated min: 
obj fun: -4864.822428848762
updated min: 
obj fun: -4864.82248964335
updated min: 
obj fun: -4864.822569603468
updated min: 
obj fun: -4864.822886128281
updated min: 
obj fun: -4864.823222694093
updated min: 
obj fun: -4864.82361649839
updated min: 
obj fun: -4864.82400582422
updated min: 
obj fun: -4864.8243690119425
updated min: 
obj fun: -4864.824750772649
updated min: 
obj fun: -4864.825053164234
updated min: 
obj fun: -4864.825295508768
updated min: 
obj fun: -4864.825465312818
updated min: 
obj fun: -4864.825533626523
updated min: 
obj fun: -4864.825569292308
updated min: 
obj fun: -4864.825622156623
updated min: 
obj fun: -4864.8256644649155
updated min: 
obj fun: -4864.825864918791
updated min: 
obj fun: -4864.825984037218
updated min: 
obj fun: -4864.8260343440625
updated min: 
obj fun: -4864.826073950628
updated min: 
obj fun: -4864.826158720138
updated min: 
obj fun: -4864.826391120802
updated min: 
obj fun: -4864.826974361602
updated min: 
obj fun: -4864.827866996425
updated min: 
obj fun: -4864.829064898251
updated min: 
obj fun: -4864.830565402398
updated min: 
obj fun: -4864.832145822467
updated min: 
obj fun: -4864.8336941346915
updated min: 
obj fun: -4864.83509633425
updated min: 
obj fun: -4864.836231277639
updated min: 
obj fun: -4864.837213212867
updated min: 
obj fun: -4864.838040917632
updated min: 
obj fun: -4864.838725810744
updated min: 
obj fun: -4864.83928538768
updated min: 
obj fun: -4864.839721889472
updated min: 
obj fun: -4864.8401269572005
updated min: 
obj fun: -4864.840524888834
updated min: 
obj fun: -4864.840974777103
updated min: 
obj fun: -4864.841497817425
updated min: 
obj fun: -4864.842156057734
updated min: 
obj fun: -4864.842878439009
updated min: 
obj fun: -4864.843567874642
updated min: 
obj fun: -4864.844072484496
updated min: 
obj fun: -4864.8444211457745
updated min: 
obj fun: -4864.844517377656
updated min: 
obj fun: -4864.844797993398
updated min: 
obj fun: -4864.845462369593
updated min: 
obj fun: -4864.845984705557
updated min: 
obj fun: -4864.846323300775
updated min: 
obj fun: -4864.846462192041
updated min: 
obj fun: -4864.84658802319
updated min: 
obj fun: -4864.847163872853
updated min: 
obj fun: -4864.847744834134
updated min: 
obj fun: -4864.848379354337
updated min: 
obj fun: -4864.849013791785
updated min: 
obj fun: -4864.849648758717
updated min: 
obj fun: -4864.850312468436
updated min: 
obj fun: -4864.850957106255
updated min: 
obj fun: -4864.851566906294
updated min: 
obj fun: -4864.852085311053
updated min: 
obj fun: -4864.852434908143
updated min: 
obj fun: -4864.852597997245
updated min: 
obj fun: -4864.85278338995
updated min: 
obj fun: -4864.853040698844
updated min: 
obj fun: -4864.853043844018
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4863.476189479971
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -4862.242569791217
NN weights: [-1.78541203e+01 -9.43132997e-01 -2.43994284e+00 -1.31417313e+01
  1.35518479e+00 -7.20928288e+00 -1.61878777e+01 -2.36486387e+00
 -3.95481949e+01 -2.28484268e+01 -2.86387348e+01  4.91010517e-01
 -1.26688585e+01 -2.69824886e+01 -1.49875414e+00  7.65861225e+00
 -6.60825908e-01 -1.11363182e+01  3.98136902e+01  2.26122456e+01
 -2.71748848e+01 -5.22397900e+00 -4.11414528e+00 -3.26754990e+01
 -3.45130463e+01 -2.30501080e+01 -1.48539896e+01 -2.16949806e+01
 -1.65366573e+01 -1.79263287e+01 -1.41435261e+01  2.87615906e+02
  2.27182938e+02  8.88084126e+00  1.48389420e+02  2.91321350e+02
  3.48693634e+02  4.06542328e+02  5.23876343e+02  5.39597750e+00
 -2.57272263e+01 -3.45279388e+01 -7.97493057e+01 -3.39742050e+01
 -3.01613121e+01 -8.45022812e+01 -3.92545891e+01 -1.56803370e+01
 -4.13548203e+01 -1.80965137e+01 -2.57219543e+01 -1.27048826e+01
 -2.28710213e+01 -3.37390404e+01 -3.18450603e+01 -3.65849495e+01
 -1.93838139e+01 -1.82296715e+01 -2.25763683e+01 -1.92168522e+01
 -3.41136193e+00 -2.75939426e+01 -3.99363556e+01  2.63422604e+01
 -1.09006325e+02 -4.57732468e+01 -3.10414104e+01 -2.48373928e+01
 -7.07157669e+01  5.49511814e+00 -3.13907838e+00 -2.82034271e+02
 -2.93144501e+02  3.43212628e+00 -1.14435837e+02 -3.59735046e+02
 -3.26249359e+02 -8.51441116e+01 -1.44530304e+02  5.08587217e+00
 -1.26116590e+01  5.57717590e+01  3.87552910e+01 -5.18392525e+01
  2.98776951e+01  4.16835022e+01  6.03162308e+01  2.50918255e+01
  3.35043411e+01  8.08542252e-01 -2.65441303e+01 -3.60591316e+01
 -8.90424881e+01 -3.46877365e+01 -2.93305950e+01 -9.25512466e+01
 -3.99545860e+01 -1.51180067e+01 -4.10981903e+01 -1.84918709e+01
 -2.29584956e+00 -8.31927872e+01 -2.80686283e+01 -2.92995358e+01
 -3.35619087e+01 -2.71074829e+01 -8.89240265e+01 -5.83847961e+01
 -7.39627609e+01 -1.82056931e+02 -3.70153093e+00 -4.11260462e+00
  1.93864250e+01 -2.73448792e+01 -3.10569954e+01  2.24968357e+01
 -7.99159718e+00 -8.13982468e+01 -1.85737106e+02 -4.24053307e+01
 -4.49074745e+01 -1.41538095e+01  1.48753285e+00  2.98950672e+00
  1.86877644e+00  3.01284766e+00 -2.25597244e+02 -4.64805794e+00
 -7.39050369e+01  2.96332779e+01  1.98128104e+00  3.01462746e+00
  1.62241101e+00  2.99428082e+00 -5.21191597e+01  3.92592354e+01
  3.53174744e+01  2.17214947e+01 -1.98672352e+01 -3.71117306e+00]
Minimum obj value:-4864.849574174062
Optimal xi: 25.776018
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1537.9932952746874
W_T_median: 1397.8009813380836
W_T_pctile_5: 665.9059848748592
W_T_CVAR_5_pct: 558.4915909081623
F value: -4864.849574174062
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.8
F value: -4864.849574174062
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ -44.9075,  -14.1538],
        [   1.4875,    2.9895],
        [   1.8688,    3.0128],
        [-225.5972,   -4.6481],
        [ -73.9050,   29.6333],
        [   1.9813,    3.0146],
        [   1.6224,    2.9943],
        [ -52.1192,   39.2592],
        [  35.3175,   21.7215],
        [ -19.8672,   -3.7112]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[ -27.1749,   -5.2240,   -4.1141,  -32.6755,  -34.5130,  -23.0501,
          -14.8540,  -21.6950,  -16.5367,  -17.9263],
        [ -14.1435,  287.6159,  227.1829,    8.8808,  148.3894,  291.3214,
          348.6936,  406.5423,  523.8763,    5.3960],
        [ -25.7272,  -34.5279,  -79.7493,  -33.9742,  -30.1613,  -84.5023,
          -39.2546,  -15.6803,  -41.3548,  -18.0965],
        [ -25.7220,  -12.7049,  -22.8710,  -33.7390,  -31.8451,  -36.5849,
          -19.3838,  -18.2297,  -22.5764,  -19.2169],
        [  -3.4114,  -27.5939,  -39.9364,   26.3423, -109.0063,  -45.7732,
          -31.0414,  -24.8374,  -70.7158,    5.4951],
        [  -3.1391, -282.0343, -293.1445,    3.4321, -114.4358, -359.7350,
         -326.2494,  -85.1441, -144.5303,    5.0859],
        [ -12.6117,   55.7718,   38.7553,  -51.8393,   29.8777,   41.6835,
           60.3162,   25.0918,   33.5043,    0.8085],
        [ -26.5441,  -36.0591,  -89.0425,  -34.6877,  -29.3306,  -92.5512,
          -39.9546,  -15.1180,  -41.0982,  -18.4919],
        [  -2.2958,  -83.1928,  -28.0686,  -29.2995,  -33.5619,  -27.1075,
          -88.9240,  -58.3848,  -73.9628, -182.0569],
        [  -3.7015,   -4.1126,   19.3864,  -27.3449,  -31.0570,   22.4968,
           -7.9916,  -81.3982, -185.7371,  -42.4053]], device='cuda:0')), ('model.output_layer_3.weight', tensor([[-17.8541,  -0.9431,  -2.4399, -13.1417,   1.3552,  -7.2093, -16.1879,
          -2.3649, -39.5482, -22.8484],
        [-28.6387,   0.4910, -12.6689, -26.9825,  -1.4988,   7.6586,  -0.6608,
         -11.1363,  39.8137,  22.6122]], device='cuda:0'))])
loaded xi:  25.776018
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -5016.676599868275
objective value function right now is: -5016.676599868275
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5015.069374593783
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5015.277537757771
8.0% of gradient descent iterations done. Method = Adam
updated min: -5016.717341402559
objective value function right now is: -5016.717341402559
10.0% of gradient descent iterations done. Method = Adam
updated min: -5016.849651089243
objective value function right now is: -5016.849651089243
12.0% of gradient descent iterations done. Method = Adam
updated min: -5017.5452996490285
objective value function right now is: -5017.5452996490285
14.000000000000002% of gradient descent iterations done. Method = Adam
updated min: -5017.737576149151
objective value function right now is: -5017.737576149151
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.303098979857
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.069448765367
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5017.637224496332
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.705365392941
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5017.274644483576
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5014.154704063827
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -5017.226398026027
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5013.47223843107
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.425861096805
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5012.458818794042
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.7167753972935
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5015.5493931640485
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5015.514742990735
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.808301355709
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.164406612052
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5010.409722473707
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5013.1837958341
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5015.4331984830615
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.682869149078
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5014.737718874384
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.475693528476
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -5013.629448219335
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.57145151694
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.571684355684
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5015.137245011092
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5015.469976573374
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.872146168424
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5013.918319924223
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5017.1935030102795
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5017.43366914358
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5015.331601258177
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5014.181603216369
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.05855538046
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.362236727495
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.779144005321
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.983279363487
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5013.30067146246
updated min: 
obj fun: -5018.667089989382
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.925600750762
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.228759964537
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5015.686165398979
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5015.553722566376
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5014.8317611211705
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5016.533834509914
NN weights: [-1.78541203e+01 -9.43132997e-01 -2.43994284e+00 -1.31417313e+01
  1.35518479e+00 -7.20928288e+00 -1.61878777e+01 -2.36486387e+00
 -3.95481949e+01 -2.28484268e+01 -2.86387348e+01  4.91010517e-01
 -1.26688585e+01 -2.69824886e+01 -1.49875414e+00  7.65861225e+00
 -6.60825908e-01 -1.11363182e+01  3.98136902e+01  2.26122456e+01
 -2.71748848e+01 -5.22397900e+00 -4.11414528e+00 -3.26754990e+01
 -3.45130463e+01 -2.30501080e+01 -1.48539896e+01 -2.16949806e+01
 -1.65366573e+01 -1.79263287e+01 -1.41435261e+01  2.87615906e+02
  2.27182938e+02  8.88084126e+00  1.48389420e+02  2.91321350e+02
  3.48693634e+02  4.06542328e+02  5.23876343e+02  5.39597750e+00
 -2.57272263e+01 -3.45279388e+01 -7.97493057e+01 -3.39742050e+01
 -3.01613121e+01 -8.45022812e+01 -3.92545891e+01 -1.56803370e+01
 -4.13548203e+01 -1.80965137e+01 -2.57219543e+01 -1.27048826e+01
 -2.28710213e+01 -3.37390404e+01 -3.18450603e+01 -3.65849495e+01
 -1.93838139e+01 -1.82296715e+01 -2.25763683e+01 -1.92168522e+01
 -3.41136193e+00 -2.75939426e+01 -3.99363556e+01  2.63422604e+01
 -1.09006325e+02 -4.57732468e+01 -3.10414104e+01 -2.48373928e+01
 -7.07157669e+01  5.49511814e+00 -3.13907838e+00 -2.82034271e+02
 -2.93144501e+02  3.43212628e+00 -1.14435837e+02 -3.59735046e+02
 -3.26249359e+02 -8.51441116e+01 -1.44530304e+02  5.08587217e+00
 -1.26116590e+01  5.57717590e+01  3.87552910e+01 -5.18392525e+01
  2.98776951e+01  4.16835022e+01  6.03162308e+01  2.50918255e+01
  3.35043411e+01  8.08542252e-01 -2.65441303e+01 -3.60591316e+01
 -8.90424881e+01 -3.46877365e+01 -2.93305950e+01 -9.25512466e+01
 -3.99545860e+01 -1.51180067e+01 -4.10981903e+01 -1.84918709e+01
 -2.29584956e+00 -8.31927872e+01 -2.80686283e+01 -2.92995358e+01
 -3.35619087e+01 -2.71074829e+01 -8.89240265e+01 -5.83847961e+01
 -7.39627609e+01 -1.82056931e+02 -3.70153093e+00 -4.11260462e+00
  1.93864250e+01 -2.73448792e+01 -3.10569954e+01  2.24968357e+01
 -7.99159718e+00 -8.13982468e+01 -1.85737106e+02 -4.24053307e+01
 -4.49074745e+01 -1.41538095e+01  1.48753285e+00  2.98950672e+00
  1.86877644e+00  3.01284766e+00 -2.25597244e+02 -4.64805794e+00
 -7.39050369e+01  2.96332779e+01  1.98128104e+00  3.01462746e+00
  1.62241101e+00  2.99428082e+00 -5.21191597e+01  3.92592354e+01
  3.53174744e+01  2.17214947e+01 -1.98672352e+01 -3.71117306e+00]
Minimum obj value:-5018.663500363659
Optimal xi: 25.787363
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1537.9932952746874
W_T_median: 1397.8009813380836
W_T_pctile_5: 665.9059848748592
W_T_CVAR_5_pct: 558.4915909081623
F value: -5018.663500363659
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.9
F value: -5018.663500363659
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ -44.9075,  -14.1538],
        [   1.4875,    2.9895],
        [   1.8688,    3.0128],
        [-225.5972,   -4.6481],
        [ -73.9050,   29.6333],
        [   1.9813,    3.0146],
        [   1.6224,    2.9943],
        [ -52.1192,   39.2592],
        [  35.3175,   21.7215],
        [ -19.8672,   -3.7112]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[ -27.1749,   -5.2240,   -4.1141,  -32.6755,  -34.5130,  -23.0501,
          -14.8540,  -21.6950,  -16.5367,  -17.9263],
        [ -14.1435,  287.6159,  227.1829,    8.8808,  148.3894,  291.3214,
          348.6936,  406.5423,  523.8763,    5.3960],
        [ -25.7272,  -34.5279,  -79.7493,  -33.9742,  -30.1613,  -84.5023,
          -39.2546,  -15.6803,  -41.3548,  -18.0965],
        [ -25.7220,  -12.7049,  -22.8710,  -33.7390,  -31.8451,  -36.5849,
          -19.3838,  -18.2297,  -22.5764,  -19.2169],
        [  -3.4114,  -27.5939,  -39.9364,   26.3423, -109.0063,  -45.7732,
          -31.0414,  -24.8374,  -70.7158,    5.4951],
        [  -3.1391, -282.0343, -293.1445,    3.4321, -114.4358, -359.7350,
         -326.2494,  -85.1441, -144.5303,    5.0859],
        [ -12.6117,   55.7718,   38.7553,  -51.8393,   29.8777,   41.6835,
           60.3162,   25.0918,   33.5043,    0.8085],
        [ -26.5441,  -36.0591,  -89.0425,  -34.6877,  -29.3306,  -92.5512,
          -39.9546,  -15.1180,  -41.0982,  -18.4919],
        [  -2.2958,  -83.1928,  -28.0686,  -29.2995,  -33.5619,  -27.1075,
          -88.9240,  -58.3848,  -73.9628, -182.0569],
        [  -3.7015,   -4.1126,   19.3864,  -27.3449,  -31.0570,   22.4968,
           -7.9916,  -81.3982, -185.7371,  -42.4053]], device='cuda:0')), ('model.output_layer_3.weight', tensor([[-17.8541,  -0.9431,  -2.4399, -13.1417,   1.3552,  -7.2093, -16.1879,
          -2.3649, -39.5482, -22.8484],
        [-28.6387,   0.4910, -12.6689, -26.9825,  -1.4988,   7.6586,  -0.6608,
         -11.1363,  39.8137,  22.6122]], device='cuda:0'))])
loaded xi:  25.787363
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -5168.282381585612
objective value function right now is: -5168.282381585612
4.0% of gradient descent iterations done. Method = Adam
updated min: -5169.723297188556
objective value function right now is: -5169.723297188556
6.0% of gradient descent iterations done. Method = Adam
updated min: -5171.692740870341
objective value function right now is: -5171.692740870341
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5170.116717878668
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5170.320384053416
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5170.0023642281485
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -5170.591965981436
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5170.438390269332
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5161.005087881062
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5170.869878237913
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5171.544293879987
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5168.410850725033
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5171.205782523034
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -5171.212728913763
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5171.221681946476
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5163.616625990971
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5170.584037734896
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5170.65056868022
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5169.174463689779
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5170.967848651536
42.0% of gradient descent iterations done. Method = Adam
updated min: -5171.964548618876
objective value function right now is: -5171.964548618876
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5171.0027538615395
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5171.345823807532
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5170.8389146305035
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5170.413382645383
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5170.216695460576
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5168.84842141207
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -5171.230146611236
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -5166.1959052594475
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5169.908302574141
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5171.341390980939
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5170.8093331444825
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5170.806159296338
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5171.636966203156
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5171.449478664215
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5167.780930363148
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5169.2385865874385
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5171.14369111944
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5169.558565831519
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5171.218145484794
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5168.544251584139
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5168.039695327905
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5170.994673310379
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5167.2374624274935
90.0% of gradient descent iterations done. Method = Adam
updated min: -5172.04115710727
objective value function right now is: -5172.04115710727
updated min: 
obj fun: -5172.04130096109
updated min: 
obj fun: -5172.047580870418
updated min: 
obj fun: -5172.053919134139
updated min: 
obj fun: -5172.060106594172
updated min: 
obj fun: -5172.065208039275
updated min: 
obj fun: -5172.069922791707
updated min: 
obj fun: -5172.074380286827
updated min: 
obj fun: -5172.077645986557
updated min: 
obj fun: -5172.0795863298235
updated min: 
obj fun: -5172.079669853738
updated min: 
obj fun: -5172.085389695361
updated min: 
obj fun: -5172.093037872836
updated min: 
obj fun: -5172.1011021323375
updated min: 
obj fun: -5172.107630465184
updated min: 
obj fun: -5172.112721690232
updated min: 
obj fun: -5172.116497659955
updated min: 
obj fun: -5172.11915325742
updated min: 
obj fun: -5172.121377703275
updated min: 
obj fun: -5172.123461667007
updated min: 
obj fun: -5172.125419861399
updated min: 
obj fun: -5172.127249857215
updated min: 
obj fun: -5172.129791571244
updated min: 
obj fun: -5172.132938923867
updated min: 
obj fun: -5172.136960713696
updated min: 
obj fun: -5172.141907986323
updated min: 
obj fun: -5172.147041938176
updated min: 
obj fun: -5172.152704655698
updated min: 
obj fun: -5172.156836640896
updated min: 
obj fun: -5172.159476212907
updated min: 
obj fun: -5172.160780910685
updated min: 
obj fun: -5172.162001099164
updated min: 
obj fun: -5172.162482000405
updated min: 
obj fun: -5172.1631451118365
updated min: 
obj fun: -5172.164045882519
updated min: 
obj fun: -5172.166765858565
updated min: 
obj fun: -5172.170126284691
updated min: 
obj fun: -5172.1729393702335
updated min: 
obj fun: -5172.1758931663435
updated min: 
obj fun: -5172.179742085054
updated min: 
obj fun: -5172.182306888643
updated min: 
obj fun: -5172.183264671298
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5170.089609614104
updated min: 
obj fun: -5172.190082693629
updated min: 
obj fun: -5172.196309473424
updated min: 
obj fun: -5172.20139732493
updated min: 
obj fun: -5172.206504756224
updated min: 
obj fun: -5172.211487995658
updated min: 
obj fun: -5172.215911575211
updated min: 
obj fun: -5172.219258403821
updated min: 
obj fun: -5172.2215166686165
updated min: 
obj fun: -5172.223403850673
updated min: 
obj fun: -5172.2251658007435
updated min: 
obj fun: -5172.226868528768
updated min: 
obj fun: -5172.229159492689
updated min: 
obj fun: -5172.232809358617
updated min: 
obj fun: -5172.237255167676
updated min: 
obj fun: -5172.2426093209315
updated min: 
obj fun: -5172.249556920973
updated min: 
obj fun: -5172.25801257354
updated min: 
obj fun: -5172.266853128199
updated min: 
obj fun: -5172.276142692602
updated min: 
obj fun: -5172.284567745449
updated min: 
obj fun: -5172.29119173875
updated min: 
obj fun: -5172.295961106859
updated min: 
obj fun: -5172.298696241536
updated min: 
obj fun: -5172.3006184064225
updated min: 
obj fun: -5172.301804724948
updated min: 
obj fun: -5172.303070174318
updated min: 
obj fun: -5172.3039767867
updated min: 
obj fun: -5172.304737465346
updated min: 
obj fun: -5172.305876904597
updated min: 
obj fun: -5172.307939704724
updated min: 
obj fun: -5172.310728629008
updated min: 
obj fun: -5172.314355278765
updated min: 
obj fun: -5172.318715141708
updated min: 
obj fun: -5172.323541392477
updated min: 
obj fun: -5172.32831114841
updated min: 
obj fun: -5172.333341242093
updated min: 
obj fun: -5172.338211048915
updated min: 
obj fun: -5172.343278568911
updated min: 
obj fun: -5172.348111398512
updated min: 
obj fun: -5172.353382222096
updated min: 
obj fun: -5172.358904796034
updated min: 
obj fun: -5172.364123388816
updated min: 
obj fun: -5172.369070299345
updated min: 
obj fun: -5172.3735568886195
updated min: 
obj fun: -5172.3780379587415
updated min: 
obj fun: -5172.3823207759915
updated min: 
obj fun: -5172.386509254873
updated min: 
obj fun: -5172.390449890148
updated min: 
obj fun: -5172.394615250007
updated min: 
obj fun: -5172.39878968562
updated min: 
obj fun: -5172.4027589288635
updated min: 
obj fun: -5172.406099897882
updated min: 
obj fun: -5172.408352722097
updated min: 
obj fun: -5172.410284695831
updated min: 
obj fun: -5172.412191999422
updated min: 
obj fun: -5172.414196767599
updated min: 
obj fun: -5172.41639795979
updated min: 
obj fun: -5172.418667878328
updated min: 
obj fun: -5172.421477814475
updated min: 
obj fun: -5172.4250361973
updated min: 
obj fun: -5172.429888266999
updated min: 
obj fun: -5172.43585881739
updated min: 
obj fun: -5172.4420384650675
updated min: 
obj fun: -5172.447950328378
updated min: 
obj fun: -5172.45273120825
updated min: 
obj fun: -5172.456284890806
updated min: 
obj fun: -5172.458367651488
updated min: 
obj fun: -5172.459611304603
updated min: 
obj fun: -5172.46045490329
updated min: 
obj fun: -5172.460893160583
updated min: 
obj fun: -5172.461557094481
updated min: 
obj fun: -5172.463096149571
updated min: 
obj fun: -5172.465289504513
updated min: 
obj fun: -5172.468412528337
updated min: 
obj fun: -5172.472010427153
updated min: 
obj fun: -5172.475505495984
updated min: 
obj fun: -5172.478451737674
updated min: 
obj fun: -5172.4806072328765
updated min: 
obj fun: -5172.482219271887
updated min: 
obj fun: -5172.483474661452
updated min: 
obj fun: -5172.484958707242
updated min: 
obj fun: -5172.486885520338
updated min: 
obj fun: -5172.48912864897
updated min: 
obj fun: -5172.492532826312
updated min: 
obj fun: -5172.497148049015
updated min: 
obj fun: -5172.502723706436
updated min: 
obj fun: -5172.508678371711
updated min: 
obj fun: -5172.514841843663
updated min: 
obj fun: -5172.520216439689
updated min: 
obj fun: -5172.524872188193
updated min: 
obj fun: -5172.528722627813
updated min: 
obj fun: -5172.531660042526
updated min: 
obj fun: -5172.533198337778
updated min: 
obj fun: -5172.534229917275
updated min: 
obj fun: -5172.535224768406
updated min: 
obj fun: -5172.536128193489
updated min: 
obj fun: -5172.537535124773
updated min: 
obj fun: -5172.539531891614
updated min: 
obj fun: -5172.541675529377
updated min: 
obj fun: -5172.543243086561
updated min: 
obj fun: -5172.543884168031
updated min: 
obj fun: -5172.545604518551
updated min: 
obj fun: -5172.54761953805
updated min: 
obj fun: -5172.549738580983
updated min: 
obj fun: -5172.552038283537
updated min: 
obj fun: -5172.554591152141
updated min: 
obj fun: -5172.5573505393995
updated min: 
obj fun: -5172.55997413041
updated min: 
obj fun: -5172.562591171952
updated min: 
obj fun: -5172.565126440084
updated min: 
obj fun: -5172.567629211783
updated min: 
obj fun: -5172.5699293979
updated min: 
obj fun: -5172.571960324577
updated min: 
obj fun: -5172.573718789143
updated min: 
obj fun: -5172.575352696249
updated min: 
obj fun: -5172.576485473659
updated min: 
obj fun: -5172.577130684226
updated min: 
obj fun: -5172.577783896772
updated min: 
obj fun: -5172.578455745159
updated min: 
obj fun: -5172.579263933981
updated min: 
obj fun: -5172.580361208585
updated min: 
obj fun: -5172.581825025865
updated min: 
obj fun: -5172.58372684581
updated min: 
obj fun: -5172.586126965342
updated min: 
obj fun: -5172.588714757772
updated min: 
obj fun: -5172.591208058612
updated min: 
obj fun: -5172.593541991917
updated min: 
obj fun: -5172.5953470573695
updated min: 
obj fun: -5172.59669137341
updated min: 
obj fun: -5172.597208573047
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5166.960914205591
updated min: 
obj fun: -5172.597810833176
updated min: 
obj fun: -5172.598295391362
updated min: 
obj fun: -5172.598437995346
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5171.163730115189
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5171.404210999813
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5169.321130182326
NN weights: [-5.24071007e+01 -1.01140821e+00 -2.18176723e+00 -1.44873505e+01
  1.31359613e+00 -7.58136368e+00 -1.80800743e+01 -1.77993464e+00
 -4.08590851e+01 -2.05762482e+01 -4.68400116e+01  5.49176693e-01
 -1.29911661e+01 -2.87532158e+01 -1.45631337e+00  8.03698921e+00
  2.33838648e-01 -1.18123207e+01  4.11248245e+01  2.03399773e+01
 -1.89616508e+01  4.27149773e+01  4.34011116e+01 -4.12956810e+01
 -8.94109631e+00  2.42512894e+01  3.27707748e+01 -1.88365424e+00
 -1.81496906e+00  5.64966053e-02 -1.33709412e+01  2.91141205e+02
  2.32299637e+02  2.95071840e+00  1.69257751e+02  2.96255157e+02
  3.51442078e+02  4.06542328e+02  5.24426147e+02  5.47057152e+00
 -2.75899487e+01 -3.28052902e+01 -7.78763504e+01 -3.39742050e+01
 -3.01613121e+01 -8.26683044e+01 -3.75844612e+01 -1.56803370e+01
 -4.14468002e+01 -1.61945572e+01 -2.48293381e+01 -5.39786863e+00
 -1.59140749e+01 -3.37388763e+01 -3.18450603e+01 -2.96438293e+01
 -1.19501934e+01 -1.82296715e+01 -2.30307980e+01 -1.47049828e+01
 -3.42716837e+00 -2.67783337e+01 -4.00976143e+01  3.43546143e+01
 -9.07768173e+01 -4.63832207e+01 -3.01729622e+01 -2.48373928e+01
 -7.12447739e+01  5.86588335e+00 -2.73895431e+00 -2.93874146e+02
 -3.03566711e+02  2.53043318e+00 -1.36094910e+02 -3.70092010e+02
 -3.38459900e+02 -8.51441116e+01 -1.44529678e+02  4.74585676e+00
 -1.14212914e+01  5.93477631e+01  4.47051697e+01 -5.75486526e+01
  3.01425686e+01  4.79220619e+01  6.40654984e+01  2.50918255e+01
  3.55031128e+01 -2.00471354e+00 -2.80160465e+01 -3.52641525e+01
 -8.80773163e+01 -3.46877365e+01 -2.93305950e+01 -9.16029358e+01
 -3.91624107e+01 -1.51180067e+01 -4.11569710e+01 -1.68421173e+01
 -2.74849391e+00 -8.99688110e+01 -3.90899200e+01 -2.92995358e+01
 -3.35619087e+01 -3.78997955e+01 -9.50389175e+01 -5.83847961e+01
 -7.37455368e+01 -2.07175690e+02 -3.58049941e+00  2.81772447e+00
  2.28199100e+01 -2.73481560e+01 -3.12095833e+01  2.60289249e+01
 -7.00719178e-01 -8.14017563e+01 -1.93362579e+02 -2.56694412e+01
 -4.53034286e+01 -1.48914433e+01  1.84451652e+00  2.99764419e+00
  2.42875409e+00  3.08691502e+00 -2.33201019e+02 -4.65451956e+00
 -1.05420364e+02  1.00159426e+01  2.37388110e+00  3.06953716e+00
  1.82850063e+00  2.98911929e+00 -5.64140167e+01  3.49071655e+01
  2.30027084e+01  3.48118477e+01 -2.05964127e+01 -3.77923274e+00]
Minimum obj value:-5172.598435953475
Optimal xi: 25.735079
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1539.3199794347347
W_T_median: 1399.2289549928769
W_T_pctile_5: 663.9767335228789
W_T_CVAR_5_pct: 554.6662298213132
F value: -5172.598435953475
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
F value: -5172.598435953475
-----------------------------------------------
loaded NN:  OrderedDict([('model.hidden_layer_1.weight', tensor([[ -45.3034,  -14.8914],
        [   1.8445,    2.9976],
        [   2.4288,    3.0869],
        [-233.2010,   -4.6545],
        [-105.4204,   10.0159],
        [   2.3739,    3.0695],
        [   1.8285,    2.9891],
        [ -56.4140,   34.9072],
        [  23.0027,   34.8118],
        [ -20.5964,   -3.7792]], device='cuda:0')), ('model.hidden_layer_2.weight', tensor([[-1.8962e+01,  4.2715e+01,  4.3401e+01, -4.1296e+01, -8.9411e+00,
          2.4251e+01,  3.2771e+01, -1.8837e+00, -1.8150e+00,  5.6497e-02],
        [-1.3371e+01,  2.9114e+02,  2.3230e+02,  2.9507e+00,  1.6926e+02,
          2.9626e+02,  3.5144e+02,  4.0654e+02,  5.2443e+02,  5.4706e+00],
        [-2.7590e+01, -3.2805e+01, -7.7876e+01, -3.3974e+01, -3.0161e+01,
         -8.2668e+01, -3.7584e+01, -1.5680e+01, -4.1447e+01, -1.6195e+01],
        [-2.4829e+01, -5.3979e+00, -1.5914e+01, -3.3739e+01, -3.1845e+01,
         -2.9644e+01, -1.1950e+01, -1.8230e+01, -2.3031e+01, -1.4705e+01],
        [-3.4272e+00, -2.6778e+01, -4.0098e+01,  3.4355e+01, -9.0777e+01,
         -4.6383e+01, -3.0173e+01, -2.4837e+01, -7.1245e+01,  5.8659e+00],
        [-2.7390e+00, -2.9387e+02, -3.0357e+02,  2.5304e+00, -1.3609e+02,
         -3.7009e+02, -3.3846e+02, -8.5144e+01, -1.4453e+02,  4.7459e+00],
        [-1.1421e+01,  5.9348e+01,  4.4705e+01, -5.7549e+01,  3.0143e+01,
          4.7922e+01,  6.4065e+01,  2.5092e+01,  3.5503e+01, -2.0047e+00],
        [-2.8016e+01, -3.5264e+01, -8.8077e+01, -3.4688e+01, -2.9331e+01,
         -9.1603e+01, -3.9162e+01, -1.5118e+01, -4.1157e+01, -1.6842e+01],
        [-2.7485e+00, -8.9969e+01, -3.9090e+01, -2.9300e+01, -3.3562e+01,
         -3.7900e+01, -9.5039e+01, -5.8385e+01, -7.3746e+01, -2.0718e+02],
        [-3.5805e+00,  2.8177e+00,  2.2820e+01, -2.7348e+01, -3.1210e+01,
          2.6029e+01, -7.0072e-01, -8.1402e+01, -1.9336e+02, -2.5669e+01]],
       device='cuda:0')), ('model.output_layer_3.weight', tensor([[-52.4071,  -1.0114,  -2.1818, -14.4874,   1.3136,  -7.5814, -18.0801,
          -1.7799, -40.8591, -20.5762],
        [-46.8400,   0.5492, -12.9912, -28.7532,  -1.4563,   8.0370,   0.2338,
         -11.8123,  41.1248,  20.3400]], device='cuda:0'))])
loaded xi:  25.735079
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
updated min: -16007.351121767044
objective value function right now is: -16007.351121767044
4.0% of gradient descent iterations done. Method = Adam
updated min: -16007.419401587124
objective value function right now is: -16007.419401587124
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16006.401315555182
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.870665442106
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.151735078332
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.155819387594
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.085985348987
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.126955454809
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.865610132332
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.154991406416
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.124687017507
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.824616876309
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.850095838156
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.112739158963
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.799407253107
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.126931518571
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.081640566033
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16002.632223808687
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.16579330918
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.59546752032
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.733850348048
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.06586533787
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.149836649112
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.747627144665
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.905466416629
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.460108698098
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.949193045511
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.567271089081
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.15801996297
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.809519737388
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.151179103374
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.141507567047
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.155525204014
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.716118449034
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.907137498209
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.168143228348
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.765641412352
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.888402271185
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.136098994803
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.671761120702
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.616212256267
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.115950961776
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.000448532875
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.156363272179
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.841012687863
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.052539007042
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.162159344987
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.671374743353
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.153408221044
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.019191536308
NN weights: [-5.68944664e+01 -2.79207659e+00 -2.18086696e+00 -1.44764385e+01
  1.26066196e+00 -1.00483913e+01 -2.08142452e+01 -1.77934206e+00
 -4.25358467e+01 -2.50666084e+01 -4.23507347e+01  2.32985854e+00
 -1.29921513e+01 -2.87649899e+01 -1.40340757e+00  1.05034904e+01
  2.96804357e+00 -1.18129816e+01  4.28014679e+01  2.48301868e+01
 -1.40854139e+01  4.88693733e+01  4.96943779e+01 -3.84247513e+01
 -8.87985229e+00  3.05195007e+01  3.89256248e+01 -1.88365424e+00
 -1.81496906e+00  3.57678175e+00 -1.28438807e+01  2.92312042e+02
  2.33831909e+02 -1.92565788e-02  1.70838486e+02  2.97748444e+02
  3.52595428e+02  4.06542328e+02  5.24426147e+02  4.30333805e+00
 -2.75842915e+01 -3.28052254e+01 -7.78762741e+01 -3.39742050e+01
 -3.01613121e+01 -8.26682281e+01 -3.75843964e+01 -1.56803370e+01
 -4.14468002e+01 -1.61946640e+01 -2.48006973e+01 -5.39668703e+00
 -1.59125900e+01 -3.37388763e+01 -3.18450603e+01 -2.96423149e+01
 -1.19489899e+01 -1.82296715e+01 -2.30307980e+01 -1.47116642e+01
 -4.40974665e+00 -2.78895321e+01 -4.12658081e+01  3.73622627e+01
 -9.19876556e+01 -4.75437851e+01 -3.12848301e+01 -2.48373928e+01
 -7.12447739e+01  5.26519299e+00 -2.41027325e-01 -2.94045563e+02
 -3.03300537e+02  1.01053762e+00 -1.38823105e+02 -3.69891052e+02
 -3.38679291e+02 -8.51441116e+01 -1.44529678e+02  4.51947165e+00
 -8.39130116e+00  6.26153107e+01  4.80673866e+01 -4.91726608e+01
  3.35949211e+01  5.12465439e+01  6.73130569e+01  2.50918255e+01
  3.55031128e+01  2.48787448e-01 -2.80124817e+01 -3.52641144e+01
 -8.80772781e+01 -3.46877365e+01 -2.93305950e+01 -9.16028976e+01
 -3.91623726e+01 -1.51180067e+01 -4.11569710e+01 -1.68421688e+01
 -1.06704843e+00 -8.80984802e+01 -3.71785278e+01 -2.92995358e+01
 -3.35619087e+01 -3.60010490e+01 -9.31720581e+01 -5.83847961e+01
 -7.37455368e+01 -2.05547958e+02  1.02245975e+00  7.07975531e+00
  2.70337391e+01 -2.59470139e+01 -3.13772964e+01  3.02274952e+01
  3.52980447e+00 -8.14017563e+01 -1.93362595e+02 -1.94236412e+01
 -4.67036171e+01 -1.34799280e+01  1.83794343e+00  2.60502601e+00
  2.30701804e+00  2.62143230e+00 -2.33488892e+02 -4.74635887e+00
 -9.99533081e+01  3.88799453e+00  2.27695560e+00  2.62382841e+00
  1.86209869e+00  2.61283875e+00 -5.64140167e+01  3.49071655e+01
  2.30024929e+01  3.48119202e+01 -2.36309776e+01 -1.38897192e+00]
Minimum obj value:-16005.540009387587
Optimal xi: 25.205286
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:236: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/pytorch_1/researchcode/fun_train_NN.py:248: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1550.9907618806392
W_T_median: 1405.3578503411634
W_T_pctile_5: 616.3641306251106
W_T_CVAR_5_pct: 497.5803784772386
F value: -16005.540009387587
-----------------------------------------------
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/pytorch_1/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 10.0
F value: -16005.540009387587
-----------------------------------------------
