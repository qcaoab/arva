Starting at: 
24-11-22_23:03

 Random seed:  2  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       8  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       8  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1  logistic_sigmoid   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 8)    False        None  
2       (8, 8)    False        None  
3       (8, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/pytorch_decumulation_mc/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       8  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       8  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 8)    False        None  
2       (8, 8)    False        None  
3       (8, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer        8  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer        8  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1  logistic_sigmoid   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0       (2, 8)     False        None  
0       (8, 8)     False        None  
0       (8, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer        8  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer        8  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0       (2, 8)     False        None  
0       (8, 8)     False        None  
0       (8, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.4409635]
objective value function right now is: -1680.781563286589
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02338349]
objective value function right now is: -1716.5062636736802
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.7305624e-07]
objective value function right now is: -1714.3333408489605
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.7518343e-12]
objective value function right now is: -1719.1968388905564
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.7520924e-16]
objective value function right now is: -1719.491622722529
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.5237631e-20]
objective value function right now is: -1719.7783235689228
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-7.4434857e-26]
objective value function right now is: -1720.401152676383
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.5644541e-30]
objective value function right now is: -1717.9892912384971
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.3084687e-33]
objective value function right now is: -1720.3703778920774
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00267604]
objective value function right now is: -1720.6488958819532
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.4101026e-07]
objective value function right now is: -1721.0154991584573
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0013578]
objective value function right now is: -1718.8801334609561
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00903595]
objective value function right now is: -1717.82301470443
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00022326]
objective value function right now is: -1720.3184295097217
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00139748]
objective value function right now is: -1720.8648398299085
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00198103]
objective value function right now is: -1720.9084144310784
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00225608]
objective value function right now is: -1720.4885644122846
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00184126]
objective value function right now is: -1721.2269450563301
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00161772]
objective value function right now is: -1719.284756122367
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00043598]
objective value function right now is: -1720.5670325721624
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.716678e-05]
objective value function right now is: -1720.6412130705648
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.8044391e-05]
objective value function right now is: -1720.2853083355963
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00106247]
objective value function right now is: -1720.7837422314547
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00041658]
objective value function right now is: -1720.5695672985746
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00086791]
objective value function right now is: -1720.2345098552619
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0034832]
objective value function right now is: -1720.510015051006
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00670116]
objective value function right now is: -1721.1810837254498
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00046663]
objective value function right now is: -1721.3414643411793
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00018441]
objective value function right now is: -1720.9061954821364
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00352349]
objective value function right now is: -1720.6948291874035
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.0220212e-05]
objective value function right now is: -1719.9102965310058
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00277596]
objective value function right now is: -1720.0517693908137
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00075771]
objective value function right now is: -1720.989282716834
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00031892]
objective value function right now is: -1721.0028312800014
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00160178]
objective value function right now is: -1720.5072657307885
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00656236]
objective value function right now is: -1702.3133211405639
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00880607]
objective value function right now is: -1721.4342339919674
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00161104]
objective value function right now is: -1720.7084803162095
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00012898]
objective value function right now is: -1720.0535137088639
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.0400274e-05]
objective value function right now is: -1720.502992888875
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00028212]
objective value function right now is: -1718.4682527419006
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.469517e-05]
objective value function right now is: -1720.976462370017
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00231389]
objective value function right now is: -1719.7567834310446
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00264168]
objective value function right now is: -1720.7306540575569
new min fval:  -1456.3396617170315
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00158468]
objective value function right now is: -1721.094643677312
new min fval:  -1720.6332626247456
new min fval:  -1720.847951048527
new min fval:  -1721.105553395573
new min fval:  -1721.3068331014715
new min fval:  -1721.4700633216335
new min fval:  -1721.5794820751212
new min fval:  -1721.639959171793
new min fval:  -1721.6635013052103
new min fval:  -1721.664572853767
new min fval:  -1721.6955081071344
new min fval:  -1721.7138661958618
new min fval:  -1721.7295195649144
new min fval:  -1721.737859250777
new min fval:  -1721.745144899997
new min fval:  -1721.7467435165838
new min fval:  -1721.7485122188787
new min fval:  -1721.750204363859
new min fval:  -1721.7531143678245
new min fval:  -1721.7568633804826
new min fval:  -1721.7601936116184
new min fval:  -1721.7701064944642
new min fval:  -1721.774884615951
new min fval:  -1721.778066618222
new min fval:  -1721.7835131899499
new min fval:  -1721.7923488509498
new min fval:  -1721.7972496834966
new min fval:  -1721.7980448237267
new min fval:  -1721.8009788354454
new min fval:  -1721.8031537959348
new min fval:  -1721.804210607789
new min fval:  -1721.805895720123
new min fval:  -1721.8096545603057
new min fval:  -1721.8138579987317
new min fval:  -1721.8141274771547
new min fval:  -1721.8152805457012
new min fval:  -1721.8178185671738
new min fval:  -1721.8200294848507
new min fval:  -1721.8226725973104
new min fval:  -1721.8254829808347
new min fval:  -1721.8286092312987
new min fval:  -1721.8299956284811
new min fval:  -1721.8319520408613
new min fval:  -1721.8347581367636
new min fval:  -1721.8371064073476
new min fval:  -1721.8399152169009
new min fval:  -1721.841777102972
new min fval:  -1721.8431814357825
new min fval:  -1721.8438965672349
new min fval:  -1721.8440202319948
new min fval:  -1721.8451620362396
new min fval:  -1721.8456445712256
new min fval:  -1721.8456819224475
new min fval:  -1721.846451179892
new min fval:  -1721.8464713209446
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00712029]
objective value function right now is: -1720.9261255329102
new min fval:  -1721.8468358951473
new min fval:  -1721.846915970284
new min fval:  -1721.8472052759105
new min fval:  -1721.8484714485435
new min fval:  -1721.8503664786826
new min fval:  -1721.8516011131271
new min fval:  -1721.8522517483887
new min fval:  -1721.8532371109316
new min fval:  -1721.8542304014638
new min fval:  -1721.855090285339
new min fval:  -1721.855960448652
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00124942]
objective value function right now is: -1721.042926571116
new min fval:  -1721.8563494553214
new min fval:  -1721.8573747356174
new min fval:  -1721.857987885538
new min fval:  -1721.8585585813546
new min fval:  -1721.8590011898223
new min fval:  -1721.8596054804475
new min fval:  -1721.8598682284548
new min fval:  -1721.8610276572
new min fval:  -1721.8618021272202
new min fval:  -1721.8623706764695
new min fval:  -1721.863531561159
new min fval:  -1721.864292221347
new min fval:  -1721.8647421522592
new min fval:  -1721.8647746547915
new min fval:  -1721.86478120165
new min fval:  -1721.865173557038
new min fval:  -1721.8657463560558
new min fval:  -1721.8660985602014
new min fval:  -1721.866238802734
new min fval:  -1721.8662781764838
new min fval:  -1721.8663504524122
new min fval:  -1721.8666545503233
new min fval:  -1721.8670058142543
new min fval:  -1721.8674480948298
new min fval:  -1721.8680854681731
new min fval:  -1721.8681930335747
new min fval:  -1721.8682198019187
new min fval:  -1721.868541640676
new min fval:  -1721.8685987245258
new min fval:  -1721.8687485123937
new min fval:  -1721.8695155784849
new min fval:  -1721.8696230251091
new min fval:  -1721.8698700443306
new min fval:  -1721.8704064240396
new min fval:  -1721.8706672043363
new min fval:  -1721.8707557264702
new min fval:  -1721.8707894859085
new min fval:  -1721.8709769669667
new min fval:  -1721.8714953420194
new min fval:  -1721.8720409088298
new min fval:  -1721.8726617306359
new min fval:  -1721.8732462067726
new min fval:  -1721.873388035647
new min fval:  -1721.8738944917548
new min fval:  -1721.8743332577264
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00063889]
objective value function right now is: -1720.9884655193905
new min fval:  -1721.8743505024538
new min fval:  -1721.8747876759905
new min fval:  -1721.8748387592527
new min fval:  -1721.8752014896863
new min fval:  -1721.8753077532879
new min fval:  -1721.8757218565847
new min fval:  -1721.8762364443755
new min fval:  -1721.8766886709539
new min fval:  -1721.877013795522
new min fval:  -1721.877173513646
new min fval:  -1721.8772949465092
new min fval:  -1721.8780377486999
new min fval:  -1721.8786606718447
new min fval:  -1721.8789147913308
new min fval:  -1721.8791374059274
new min fval:  -1721.879453843267
new min fval:  -1721.8798909064938
new min fval:  -1721.8804104920146
new min fval:  -1721.8807476370675
new min fval:  -1721.8810432238258
new min fval:  -1721.8815634290336
new min fval:  -1721.8820559740145
new min fval:  -1721.8823180353731
new min fval:  -1721.8823711703703
new min fval:  -1721.8824892752446
new min fval:  -1721.882652702081
new min fval:  -1721.882749983425
new min fval:  -1721.882780012689
new min fval:  -1721.8828770612072
new min fval:  -1721.88337365586
new min fval:  -1721.884213186788
new min fval:  -1721.88518971011
new min fval:  -1721.8861003144232
new min fval:  -1721.8862768607696
new min fval:  -1721.8864081115119
new min fval:  -1721.886830631063
new min fval:  -1721.8874832815654
new min fval:  -1721.8876150830215
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00071874]
objective value function right now is: -1720.5498266384996
new min fval:  -1721.8876173922965
new min fval:  -1721.887808805359
new min fval:  -1721.8879259360451
new min fval:  -1721.8879686825987
new min fval:  -1721.8880337075345
new min fval:  -1721.888116702584
new min fval:  -1721.8884766282295
new min fval:  -1721.888626612174
new min fval:  -1721.888641065035
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.0597585e-05]
objective value function right now is: -1721.5865414724192
min fval:  -1721.888641065035
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 359.6193479524028
W_T_median: 165.62162286537654
W_T_pctile_5: -234.41785233215128
W_T_CVAR_5_pct: -435.5208194393344
Average q (qsum/M+1):  56.58846159904234
Optimal xi:  [-7.6745355e-06]
Expected(across Rb) median(across samples) p_equity:  0.279584832303226
obj fun:  tensor(-1721.8886, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.831094]
objective value function right now is: -1660.4199415013813
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.31120825]
objective value function right now is: -1662.4307065663565
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.430458e-05]
objective value function right now is: -1666.5793121143176
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.866605e-10]
objective value function right now is: -1666.55465796599
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.5593202e-14]
objective value function right now is: -1666.54766899272
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.5397311e-18]
objective value function right now is: -1658.1784459577193
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [1.96084e-12]
objective value function right now is: -1662.577148936694
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.3983015e-08]
objective value function right now is: -1667.485826356204
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.15386052]
objective value function right now is: -1655.4255950089648
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.6699278e-05]
objective value function right now is: -1667.0678355440261
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.04793387]
objective value function right now is: -1667.9571380114435
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00063479]
objective value function right now is: -1668.0773072606592
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06000909]
objective value function right now is: -1667.1020335282508
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [0.05374414]
objective value function right now is: -1665.8421250841477
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00272991]
objective value function right now is: -1662.4202725306332
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.05052554]
objective value function right now is: -1665.725289444612
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00015978]
objective value function right now is: -1663.4207481034825
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0034213]
objective value function right now is: -1628.1903019692154
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.07948411]
objective value function right now is: -1664.3023279144309
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-8.8200744e-05]
objective value function right now is: -1665.497537996926
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00306807]
objective value function right now is: -1668.4697276563513
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.2916245e-05]
objective value function right now is: -1665.403298390356
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.1253067]
objective value function right now is: -1666.7908439073535
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.1134495e-05]
objective value function right now is: -1666.153330942727
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0507442]
objective value function right now is: -1664.1180828299434
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00019332]
objective value function right now is: -1653.64706765908
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.04364574]
objective value function right now is: -1666.6707832019476
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00034363]
objective value function right now is: -1664.8040929980255
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00205017]
objective value function right now is: -1666.7098435523628
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05765218]
objective value function right now is: -1661.142894208544
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04730213]
objective value function right now is: -1668.4884070852586
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.03646792]
objective value function right now is: -1667.4472851262706
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0526432]
objective value function right now is: -1668.7672327315133
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00306239]
objective value function right now is: -1665.3452274662675
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0064231]
objective value function right now is: -1668.742397414595
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.10896431]
objective value function right now is: -1667.562734167623
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [3.206143e-05]
objective value function right now is: -1668.6566568891553
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.3335847e-07]
objective value function right now is: -1667.1408064403688
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00142739]
objective value function right now is: -1665.6283599106744
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00019429]
objective value function right now is: -1664.244637099147
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0005197]
objective value function right now is: -1665.524737838586
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02632529]
objective value function right now is: -1667.0557191877622
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [5.5687953e-05]
objective value function right now is: -1668.9176477456358
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00137543]
objective value function right now is: -1666.8550056284355
new min fval:  -1626.8892987542363
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00030178]
objective value function right now is: -1667.4496000992465
new min fval:  -1667.7734208775519
new min fval:  -1668.0452334463
new min fval:  -1668.1443725694978
new min fval:  -1668.3102392893518
new min fval:  -1668.5589810057763
new min fval:  -1668.9584412680174
new min fval:  -1669.077639591078
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00095391]
objective value function right now is: -1665.0973919391035
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01992445]
objective value function right now is: -1668.8708602652234
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.5222076e-05]
objective value function right now is: -1667.0355444685633
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.12137903]
objective value function right now is: -1651.482124511078
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.866199e-05]
objective value function right now is: -1630.668564741841
min fval:  -1669.077639591078
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 420.6918777011478
W_T_median: 165.26273822470307
W_T_pctile_5: -36.983379043321484
W_T_CVAR_5_pct: -223.94832606762856
Average q (qsum/M+1):  55.32112367691532
Optimal xi:  [0.00013091]
Expected(across Rb) median(across samples) p_equity:  0.3228090576827526
obj fun:  tensor(-1669.0776, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.2721705]
objective value function right now is: -1599.6086419278704
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [2.0533946]
objective value function right now is: -1588.0390183959025
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.08311516]
objective value function right now is: -1598.8636893783464
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [6.0626717e-06]
objective value function right now is: -1615.8384707209475
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.2949563e-10]
objective value function right now is: -1610.5889730457275
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.5559075e-14]
objective value function right now is: -1609.1616754124595
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-1.8708886e-12]
objective value function right now is: -1605.0667767438604
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.16561611]
objective value function right now is: -1609.0746133328548
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [4.3005937e-05]
objective value function right now is: -1612.3642808789768
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02797646]
objective value function right now is: -1615.6097475515196
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.03864724]
objective value function right now is: -1614.1260523801595
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.20904951]
objective value function right now is: -1571.3439974306566
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00901413]
objective value function right now is: -1576.7393249804759
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [0.00287093]
objective value function right now is: -1596.269576412689
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0263051]
objective value function right now is: -1598.016007047263
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00290985]
objective value function right now is: -1596.9945130688182
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00472882]
objective value function right now is: -1611.3675331554753
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04238288]
objective value function right now is: -1618.0248440115602
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01458923]
objective value function right now is: -1585.0977819117447
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01567755]
objective value function right now is: -1614.3103587970681
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00621944]
objective value function right now is: -1617.8744185016662
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00088489]
objective value function right now is: -1617.362964549567
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00511464]
objective value function right now is: -1620.1775611430946
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.05605312]
objective value function right now is: -1601.7054489470022
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00079079]
objective value function right now is: -1615.4219617702968
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.12518279]
objective value function right now is: -1616.2168344566355
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.07497282]
objective value function right now is: -1586.7707425424094
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0149505]
objective value function right now is: -1596.9269982876956
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [0.01715185]
objective value function right now is: -1621.488410150934
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01714391]
objective value function right now is: -1623.2912927012892
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02214717]
objective value function right now is: -1622.691275430815
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00070088]
objective value function right now is: -1619.7280429136329
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00171809]
objective value function right now is: -1541.8636706862671
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02980011]
objective value function right now is: -1604.820541404326
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00289181]
objective value function right now is: -1620.2680311799024
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03485655]
objective value function right now is: -1623.8185393561866
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.10751604]
objective value function right now is: -1617.5253382489675
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02191135]
objective value function right now is: -1620.1653716389942
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01575784]
objective value function right now is: -1617.1355918300883
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0014016]
objective value function right now is: -1612.0871910909634
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00140919]
objective value function right now is: -1621.0516729035003
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03792324]
objective value function right now is: -1618.332042650674
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00167127]
objective value function right now is: -1618.7324941487657
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00823909]
objective value function right now is: -1623.1799668240726
new min fval:  -1504.322524382681
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01004831]
objective value function right now is: -1624.2129052113671
new min fval:  -1624.1966959216406
new min fval:  -1624.4564575189563
new min fval:  -1625.3960898424566
new min fval:  -1625.7757524525025
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05019521]
objective value function right now is: -1620.737968121293
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.1262126]
objective value function right now is: -1621.6682205088991
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01601772]
objective value function right now is: -1618.8460654169514
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.03832449]
objective value function right now is: -1618.9443447387237
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.08529338]
objective value function right now is: -1617.4392734595153
min fval:  -1625.7757524525025
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 455.6363887555861
W_T_median: 218.40301625022892
W_T_pctile_5: 2.022005873223171
W_T_CVAR_5_pct: -107.46041542378555
Average q (qsum/M+1):  54.179876512096776
Optimal xi:  [0.00530322]
Expected(across Rb) median(across samples) p_equity:  0.34977842221657435
obj fun:  tensor(-1625.7758, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.385971]
objective value function right now is: -1592.0127388963897
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.841737]
objective value function right now is: -1594.2909321626432
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.490654]
objective value function right now is: -1594.3025428873361
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.199989]
objective value function right now is: -1594.9975076333358
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.85471]
objective value function right now is: -1596.8802944399404
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.248919]
objective value function right now is: -1587.5363719003892
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [8.811761]
objective value function right now is: -1598.2425010305128
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.730511]
objective value function right now is: -1585.117939562411
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.937114]
objective value function right now is: -1596.6464954729822
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.739533]
objective value function right now is: -1596.9554718725785
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.858143]
objective value function right now is: -1593.8699039252378
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.723902]
objective value function right now is: -1598.8660751509392
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.939113]
objective value function right now is: -1595.9307006466688
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [8.9302025]
objective value function right now is: -1599.6337064361637
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.960789]
objective value function right now is: -1592.5438364757183
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.09429]
objective value function right now is: -1594.1867847679364
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.115615]
objective value function right now is: -1595.10165961368
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.975716]
objective value function right now is: -1596.3509098378802
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.311891]
objective value function right now is: -1597.8170174689033
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.276524]
objective value function right now is: -1596.3879601714584
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.997282]
objective value function right now is: -1599.4571110427034
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.217713]
objective value function right now is: -1597.7702630903432
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.16801]
objective value function right now is: -1592.8758067007711
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.776767]
objective value function right now is: -1594.2979885813716
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.523146]
objective value function right now is: -1598.0718650406636
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.399892]
objective value function right now is: -1597.4828720137718
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.193491]
objective value function right now is: -1598.5608115517664
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [9.478861]
objective value function right now is: -1585.8252733603092
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [9.356506]
objective value function right now is: -1598.175594520846
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.24857]
objective value function right now is: -1597.2854517933533
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.287246]
objective value function right now is: -1592.630200619915
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.966939]
objective value function right now is: -1591.750911089958
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.730498]
objective value function right now is: -1597.1220740799276
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.237237]
objective value function right now is: -1597.5416045068446
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.008594]
objective value function right now is: -1596.3444517382957
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.049759]
objective value function right now is: -1598.4943988633738
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.1480665]
objective value function right now is: -1599.3259596883283
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.236656]
objective value function right now is: -1599.3019276609602
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.522012]
objective value function right now is: -1600.803939987549
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.187444]
objective value function right now is: -1598.1410056633217
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.410395]
objective value function right now is: -1597.4191795889521
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.1847725]
objective value function right now is: -1577.468406771721
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.631822]
objective value function right now is: -1599.510571380428
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.089134]
objective value function right now is: -1595.5536747968592
new min fval:  -1087.461550645152
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.26862]
objective value function right now is: -1596.7266560094058
new min fval:  -1597.351022129788
new min fval:  -1597.622936617618
new min fval:  -1597.9517540564505
new min fval:  -1598.7352581266962
new min fval:  -1599.326758125088
new min fval:  -1599.3656028427872
new min fval:  -1599.4691059009788
new min fval:  -1599.6488718779642
new min fval:  -1599.8116153079152
new min fval:  -1599.9780176454165
new min fval:  -1600.0997091121865
new min fval:  -1600.2780154906861
new min fval:  -1600.4634633375315
new min fval:  -1600.5645893936307
new min fval:  -1600.618189933795
new min fval:  -1600.6551512028357
new min fval:  -1600.68367845195
new min fval:  -1600.7226635815327
new min fval:  -1600.7861559848973
new min fval:  -1600.8823487743173
new min fval:  -1600.9746560216176
new min fval:  -1601.0492843066736
new min fval:  -1601.1080947172927
new min fval:  -1601.1499993952034
new min fval:  -1601.178407603396
new min fval:  -1601.206036096114
new min fval:  -1601.2570344054886
new min fval:  -1601.3237144261202
new min fval:  -1601.421579625418
new min fval:  -1601.5031896718197
new min fval:  -1601.55551213319
new min fval:  -1601.5732332392886
new min fval:  -1601.5956978591416
new min fval:  -1601.610330025682
new min fval:  -1601.657004692295
new min fval:  -1601.681920810395
new min fval:  -1601.690348741698
new min fval:  -1601.6945506017194
new min fval:  -1601.7121517785893
new min fval:  -1601.7168706520652
new min fval:  -1601.7185106268457
new min fval:  -1601.725197444201
new min fval:  -1601.7315541835235
new min fval:  -1601.7462632805264
new min fval:  -1601.755583172257
new min fval:  -1601.7600876947238
new min fval:  -1601.7810303391514
new min fval:  -1601.7982597126602
new min fval:  -1601.806919789963
new min fval:  -1601.811120737617
new min fval:  -1601.8154716075865
new min fval:  -1601.8204872769466
new min fval:  -1601.8215151331158
new min fval:  -1601.8335781075689
new min fval:  -1601.8443137053969
new min fval:  -1601.847054375249
new min fval:  -1601.8487085345892
new min fval:  -1601.8519019704743
new min fval:  -1601.8576169838598
new min fval:  -1601.8639191861955
new min fval:  -1601.868104651497
new min fval:  -1601.8705183681411
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.097449]
objective value function right now is: -1600.9517853294928
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.172275]
objective value function right now is: -1598.040403849275
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.251133]
objective value function right now is: -1599.9509884314382
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.213142]
objective value function right now is: -1598.2360786642669
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [8.90502]
objective value function right now is: -1597.8214602863864
min fval:  -1601.8705183681411
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 628.4237547111295
W_T_median: 313.0229346827666
W_T_pctile_5: 86.27864523429807
W_T_CVAR_5_pct: -12.895940693041473
Average q (qsum/M+1):  52.092348160282256
Optimal xi:  [9.180298]
Expected(across Rb) median(across samples) p_equity:  0.32725917994976045
obj fun:  tensor(-1601.8705, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.528103]
objective value function right now is: -1617.7338442266428
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.249668]
objective value function right now is: -1610.1602874744
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.555348]
objective value function right now is: -1613.3765727722066
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.911872]
objective value function right now is: -1615.8358918140623
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.904499]
objective value function right now is: -1619.7157490940097
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.9293]
objective value function right now is: -1607.5434507347786
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [13.278547]
objective value function right now is: -1605.5650341455787
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.9783945]
objective value function right now is: -1615.1688302304383
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.219053]
objective value function right now is: -1612.9638023739094
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.861952]
objective value function right now is: -1615.8670583854605
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.136969]
objective value function right now is: -1615.2587958815939
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.352965]
objective value function right now is: -1616.569656460834
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.036336]
objective value function right now is: -1612.9528733833838
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [13.132899]
objective value function right now is: -1617.1235453757122
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.843224]
objective value function right now is: -1593.0109262186934
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.390784]
objective value function right now is: -1615.4215764858957
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.027724]
objective value function right now is: -1617.6123151001823
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.068937]
objective value function right now is: -1610.3512306142763
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.134842]
objective value function right now is: -1615.403276588812
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.193204]
objective value function right now is: -1604.2442654986442
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.958497]
objective value function right now is: -1612.1834391726754
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.007645]
objective value function right now is: -1617.1036153194411
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.337741]
objective value function right now is: -1614.7241326400806
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.092331]
objective value function right now is: -1608.5771535989386
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.144112]
objective value function right now is: -1617.0934881520463
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.22379]
objective value function right now is: -1606.4267338457867
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.004496]
objective value function right now is: -1613.1187945374
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [13.043837]
objective value function right now is: -1603.373663570933
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [13.081439]
objective value function right now is: -1606.1061684658216
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.982048]
objective value function right now is: -1614.807700061354
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.268683]
objective value function right now is: -1604.720719349886
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.105319]
objective value function right now is: -1610.3334732556273
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.53524]
objective value function right now is: -1585.7690132985954
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.373202]
objective value function right now is: -1612.3079788837001
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.636506]
objective value function right now is: -1611.561358857162
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.660333]
objective value function right now is: -1558.5173159878334
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.935333]
objective value function right now is: -1601.5123434207176
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.898341]
objective value function right now is: -1618.6151245776928
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.575687]
objective value function right now is: -1606.7329266015715
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.959898]
objective value function right now is: -1610.0229794072468
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.215522]
objective value function right now is: -1613.1547233233387
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.034671]
objective value function right now is: -1617.905200135052
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.068277]
objective value function right now is: -1610.9579342374968
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.364985]
objective value function right now is: -1609.1674852826638
new min fval:  -798.5456815156026
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.054974]
objective value function right now is: -1597.8024799545967
new min fval:  -1602.9948264334596
new min fval:  -1606.9267968667834
new min fval:  -1612.0409926944023
new min fval:  -1616.7532260649969
new min fval:  -1619.2880827362467
new min fval:  -1619.7529844103617
new min fval:  -1620.0596396762603
new min fval:  -1620.375847709659
new min fval:  -1620.4015292345316
new min fval:  -1620.4905639414044
new min fval:  -1620.6015237305212
new min fval:  -1620.7097103077774
new min fval:  -1620.7394583275347
new min fval:  -1620.7594555200808
new min fval:  -1620.8096268054794
new min fval:  -1620.85474328274
new min fval:  -1620.887919045539
new min fval:  -1620.918242757693
new min fval:  -1620.940589292915
new min fval:  -1620.9727152304297
new min fval:  -1620.986424468559
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.146864]
objective value function right now is: -1611.8493988414182
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.929644]
objective value function right now is: -1615.9870655151424
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.366883]
objective value function right now is: -1606.6494384041066
new min fval:  -1620.9886201658614
new min fval:  -1620.9920616316506
new min fval:  -1620.9958014898536
new min fval:  -1621.0002347546708
new min fval:  -1621.003870399524
new min fval:  -1621.006660907326
new min fval:  -1621.008802440723
new min fval:  -1621.0109697791204
new min fval:  -1621.013522371563
new min fval:  -1621.015615782639
new min fval:  -1621.0186654351307
new min fval:  -1621.0205725092626
new min fval:  -1621.0224710776458
new min fval:  -1621.0233615869777
new min fval:  -1621.0236160411803
new min fval:  -1621.0239919382502
new min fval:  -1621.0254878895298
new min fval:  -1621.0271079277024
new min fval:  -1621.0297057399964
new min fval:  -1621.032506901084
new min fval:  -1621.0341617838421
new min fval:  -1621.0351074347323
new min fval:  -1621.036111383129
new min fval:  -1621.0374741654937
new min fval:  -1621.0394175140245
new min fval:  -1621.0419142935727
new min fval:  -1621.0433877220644
new min fval:  -1621.0451484378616
new min fval:  -1621.0470098693715
new min fval:  -1621.050522263874
new min fval:  -1621.0543263730144
new min fval:  -1621.057034769617
new min fval:  -1621.0586209487587
new min fval:  -1621.059397299149
new min fval:  -1621.0595697098408
new min fval:  -1621.0607854186767
new min fval:  -1621.0631178079154
new min fval:  -1621.0662303770998
new min fval:  -1621.0686886019805
new min fval:  -1621.0696703289207
new min fval:  -1621.070792198431
new min fval:  -1621.071492447243
new min fval:  -1621.0731966527478
new min fval:  -1621.0758398489943
new min fval:  -1621.0776544334167
new min fval:  -1621.0786117099253
new min fval:  -1621.0788928471356
new min fval:  -1621.0790992175328
new min fval:  -1621.0794461603527
new min fval:  -1621.0804948314444
new min fval:  -1621.0823464616542
new min fval:  -1621.0838854315937
new min fval:  -1621.085322900302
new min fval:  -1621.0858723607003
new min fval:  -1621.0870086353402
new min fval:  -1621.0888889531727
new min fval:  -1621.0907641758274
new min fval:  -1621.0926682413115
new min fval:  -1621.0946259115829
new min fval:  -1621.0955838756483
new min fval:  -1621.096043524074
new min fval:  -1621.0967647945267
new min fval:  -1621.0982941617824
new min fval:  -1621.1001490158046
new min fval:  -1621.101639187545
new min fval:  -1621.1017024446228
new min fval:  -1621.1023039875727
new min fval:  -1621.1044589625476
new min fval:  -1621.1070430496652
new min fval:  -1621.1089472633075
new min fval:  -1621.1095226400314
new min fval:  -1621.1106450256584
new min fval:  -1621.1145609409648
new min fval:  -1621.11718519744
new min fval:  -1621.1188644997753
new min fval:  -1621.1201249985809
new min fval:  -1621.1216874753582
new min fval:  -1621.1235113981968
new min fval:  -1621.1256015099912
new min fval:  -1621.12821114747
new min fval:  -1621.129697412801
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.007142]
objective value function right now is: -1615.2495196030802
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [12.968143]
objective value function right now is: -1606.1439494486046
min fval:  -1621.129697412801
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 695.4366535558481
W_T_median: 422.5733294007657
W_T_pctile_5: 175.1715225514931
W_T_CVAR_5_pct: 37.276255917688744
Average q (qsum/M+1):  49.89759088331653
Optimal xi:  [13.097367]
Expected(across Rb) median(across samples) p_equity:  0.2965019484361013
obj fun:  tensor(-1621.1297, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.948184]
objective value function right now is: -1655.0091432999636
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.119426]
objective value function right now is: -1654.910861617587
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.393082]
objective value function right now is: -1644.3698803861141
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.548325]
objective value function right now is: -1637.9159125193032
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.290244]
objective value function right now is: -1657.1603047753724
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.2136345]
objective value function right now is: -1657.73701642128
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [13.694023]
objective value function right now is: -1654.745249494349
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.9172125]
objective value function right now is: -1660.8273553181832
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.353255]
objective value function right now is: -1652.2295136845596
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.884703]
objective value function right now is: -1657.9994752617781
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.100153]
objective value function right now is: -1645.409556380221
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.694272]
objective value function right now is: -1650.2955441561955
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.254131]
objective value function right now is: -1652.0541642094781
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [14.414586]
objective value function right now is: -1656.1045107445013
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.736379]
objective value function right now is: -1652.9221598661488
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.140595]
objective value function right now is: -1657.2625409984728
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.9980345]
objective value function right now is: -1644.8755714991999
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.265117]
objective value function right now is: -1659.528995496654
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.336862]
objective value function right now is: -1654.9818296196313
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.218709]
objective value function right now is: -1636.5200403126455
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.127715]
objective value function right now is: -1660.945993217246
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.107174]
objective value function right now is: -1626.756058969584
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.63127]
objective value function right now is: -1637.3160358581363
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.869759]
objective value function right now is: -1650.4258977376205
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.136628]
objective value function right now is: -1659.2736162765987
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.313532]
objective value function right now is: -1661.9983651875934
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.011362]
objective value function right now is: -1653.2747731520512
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [14.127735]
objective value function right now is: -1651.7669932879437
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [14.435247]
objective value function right now is: -1662.838422292002
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.916444]
objective value function right now is: -1659.395520733411
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.400565]
objective value function right now is: -1652.834601500274
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.387287]
objective value function right now is: -1661.37919046098
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.524181]
objective value function right now is: -1656.7429738696412
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.505315]
objective value function right now is: -1651.252533018381
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.019323]
objective value function right now is: -1657.515652050489
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.8873625]
objective value function right now is: -1634.6729210254327
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.259246]
objective value function right now is: -1656.564021160291
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.439645]
objective value function right now is: -1646.0469072835851
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.514016]
objective value function right now is: -1659.0147263366525
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.155273]
objective value function right now is: -1653.6404549809527
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.38966]
objective value function right now is: -1653.438507351496
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.1497755]
objective value function right now is: -1657.4463260785199
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.944803]
objective value function right now is: -1660.5841977672205
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.2096405]
objective value function right now is: -1654.7314633259903
new min fval:  -1600.3669287828575
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.905033]
objective value function right now is: -1642.7305466083778
new min fval:  -1644.7669738221337
new min fval:  -1645.3131271375614
new min fval:  -1645.932401447656
new min fval:  -1646.985763229289
new min fval:  -1648.7166373261489
new min fval:  -1650.7199633177959
new min fval:  -1652.5859000950538
new min fval:  -1654.8715936983688
new min fval:  -1656.844159835929
new min fval:  -1658.2045935708788
new min fval:  -1659.1244433354373
new min fval:  -1659.7708190905885
new min fval:  -1660.1239883528558
new min fval:  -1660.376697856458
new min fval:  -1660.609760685006
new min fval:  -1660.817512639442
new min fval:  -1660.9663059431366
new min fval:  -1661.0605626463428
new min fval:  -1661.1375641105494
new min fval:  -1661.201244028645
new min fval:  -1661.27542970652
new min fval:  -1661.4265807208014
new min fval:  -1661.6363272838003
new min fval:  -1661.8500562908816
new min fval:  -1662.0472568541406
new min fval:  -1662.2316759371977
new min fval:  -1662.4300422828142
new min fval:  -1662.6808851199485
new min fval:  -1662.9337588576564
new min fval:  -1663.1470393243096
new min fval:  -1663.3170794052307
new min fval:  -1663.4443907414386
new min fval:  -1663.5460789688868
new min fval:  -1663.6397301909149
new min fval:  -1663.735222083584
new min fval:  -1663.8369837764371
new min fval:  -1663.917376752391
new min fval:  -1663.9743866817569
new min fval:  -1664.0227822793815
new min fval:  -1664.0885598935438
new min fval:  -1664.1836510361268
new min fval:  -1664.2869222026854
new min fval:  -1664.371226851748
new min fval:  -1664.4362642969568
new min fval:  -1664.4933901220234
new min fval:  -1664.5503402508523
new min fval:  -1664.5948931705227
new min fval:  -1664.6283358162748
new min fval:  -1664.6430911631292
new min fval:  -1664.6512301160205
new min fval:  -1664.6610920834871
new min fval:  -1664.6702895621381
new min fval:  -1664.6790001891513
new min fval:  -1664.6829133553556
new min fval:  -1664.6854598416992
new min fval:  -1664.689250278866
new min fval:  -1664.6907186997282
new min fval:  -1664.6943303713729
new min fval:  -1664.7091770834702
new min fval:  -1664.7360625758809
new min fval:  -1664.7701425303799
new min fval:  -1664.810527058854
new min fval:  -1664.8576581580712
new min fval:  -1664.9137262709005
new min fval:  -1664.963457761216
new min fval:  -1665.0069812073964
new min fval:  -1665.0435024103554
new min fval:  -1665.072519681272
new min fval:  -1665.1044210070745
new min fval:  -1665.1419703991135
new min fval:  -1665.1814991455815
new min fval:  -1665.2218571571323
new min fval:  -1665.2501851612933
new min fval:  -1665.2661969795715
new min fval:  -1665.2737929439215
new min fval:  -1665.2770579873027
new min fval:  -1665.2940641307575
new min fval:  -1665.3210621812107
new min fval:  -1665.343774431185
new min fval:  -1665.3625218448024
new min fval:  -1665.3808947676057
new min fval:  -1665.40056479149
new min fval:  -1665.4265798027939
new min fval:  -1665.4556694670405
new min fval:  -1665.4829665133536
new min fval:  -1665.504731939356
new min fval:  -1665.5188389082791
new min fval:  -1665.5290753093086
new min fval:  -1665.5391111548313
new min fval:  -1665.5468510756602
new min fval:  -1665.5574394821786
new min fval:  -1665.569181945938
new min fval:  -1665.5835738160588
new min fval:  -1665.599727980875
new min fval:  -1665.6101968479536
new min fval:  -1665.6161134507772
new min fval:  -1665.6327028852575
new min fval:  -1665.651608625212
new min fval:  -1665.6625513059134
new min fval:  -1665.6651124618918
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.172521]
objective value function right now is: -1651.8520182778118
new min fval:  -1665.6690413064375
new min fval:  -1665.687290362728
new min fval:  -1665.7050995687357
new min fval:  -1665.7202061850305
new min fval:  -1665.7324638607167
new min fval:  -1665.743714901722
new min fval:  -1665.7565837762695
new min fval:  -1665.7700157409402
new min fval:  -1665.7808179927204
new min fval:  -1665.7912652657842
new min fval:  -1665.7992153046462
new min fval:  -1665.805732773916
new min fval:  -1665.81115034938
new min fval:  -1665.8183109748884
new min fval:  -1665.8275654183524
new min fval:  -1665.838574099289
new min fval:  -1665.8519717116649
new min fval:  -1665.8637872534646
new min fval:  -1665.8737520099535
new min fval:  -1665.8819029847414
new min fval:  -1665.8870212485274
new min fval:  -1665.8924869876225
new min fval:  -1665.897467555897
new min fval:  -1665.9042425618695
new min fval:  -1665.9129835078704
new min fval:  -1665.9230421086315
new min fval:  -1665.9323828286333
new min fval:  -1665.9414212783058
new min fval:  -1665.946789999879
new min fval:  -1665.9493622362254
new min fval:  -1665.9499778569752
new min fval:  -1665.9582018864057
new min fval:  -1665.96242985706
new min fval:  -1665.9642083895542
new min fval:  -1665.9699119283496
new min fval:  -1665.9765736388351
new min fval:  -1665.9821492636386
new min fval:  -1665.9828294799345
new min fval:  -1665.9873897120842
new min fval:  -1665.988692144063
new min fval:  -1665.991278129636
new min fval:  -1665.9917930258534
new min fval:  -1665.9931436883116
new min fval:  -1665.9960995525419
new min fval:  -1666.0001267810296
new min fval:  -1666.0048674426682
new min fval:  -1666.0099267423432
new min fval:  -1666.0126959030542
new min fval:  -1666.0221018598827
new min fval:  -1666.0325126133691
new min fval:  -1666.040510029739
new min fval:  -1666.0469553137468
new min fval:  -1666.0522473613194
new min fval:  -1666.0554660811197
new min fval:  -1666.0575199032571
new min fval:  -1666.0589825855043
new min fval:  -1666.0610237669791
new min fval:  -1666.0653449794333
new min fval:  -1666.070677467667
new min fval:  -1666.0768977464745
new min fval:  -1666.081420110501
new min fval:  -1666.0847363305213
new min fval:  -1666.0867689942713
new min fval:  -1666.088147060708
new min fval:  -1666.0900238515578
new min fval:  -1666.0920278868714
new min fval:  -1666.0951948180004
new min fval:  -1666.0975912306083
new min fval:  -1666.0997246469994
new min fval:  -1666.102639118827
new min fval:  -1666.1070008516099
new min fval:  -1666.112914448397
new min fval:  -1666.1179203920626
new min fval:  -1666.1239846447338
new min fval:  -1666.1291015168554
new min fval:  -1666.1332280214463
new min fval:  -1666.1364763508573
new min fval:  -1666.1394921397928
new min fval:  -1666.143214907896
new min fval:  -1666.147891333275
new min fval:  -1666.1531389003997
new min fval:  -1666.1577631209805
new min fval:  -1666.1622267167324
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.003613]
objective value function right now is: -1657.519733046609
new min fval:  -1666.1652379646366
new min fval:  -1666.1678415874414
new min fval:  -1666.1709070956242
new min fval:  -1666.175539053321
new min fval:  -1666.1821809119215
new min fval:  -1666.1890056800974
new min fval:  -1666.1950915576388
new min fval:  -1666.1995807234564
new min fval:  -1666.2028065754628
new min fval:  -1666.2050584793567
new min fval:  -1666.2081369430575
new min fval:  -1666.212857001397
new min fval:  -1666.218126745381
new min fval:  -1666.2234373878339
new min fval:  -1666.2285886239729
new min fval:  -1666.2332949341396
new min fval:  -1666.238620639207
new min fval:  -1666.243533926643
new min fval:  -1666.2480090897902
new min fval:  -1666.2511932908226
new min fval:  -1666.252623874778
new min fval:  -1666.2548937324007
new min fval:  -1666.2596876545929
new min fval:  -1666.2671120791972
new min fval:  -1666.2754763356686
new min fval:  -1666.2809006847392
new min fval:  -1666.2823200394712
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.19929]
objective value function right now is: -1654.3564839945707
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.95976]
objective value function right now is: -1646.485720533494
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.009892]
objective value function right now is: -1663.744576798315
min fval:  -1666.2823200394712
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 706.4081876041737
W_T_median: 456.91885129726927
W_T_pctile_5: 203.21017139161805
W_T_CVAR_5_pct: 49.64910497065395
Average q (qsum/M+1):  48.956318516885084
Optimal xi:  [14.124531]
Expected(across Rb) median(across samples) p_equity:  0.2780815233786901
obj fun:  tensor(-1666.2823, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.871027]
objective value function right now is: -1742.8836379469567
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.808595]
objective value function right now is: -1741.0590130083394
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.83887]
objective value function right now is: -1758.3216367999162
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.860844]
objective value function right now is: -1761.5769427075331
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.09072]
objective value function right now is: -1756.0830595875927
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.041792]
objective value function right now is: -1760.2761287412766
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [14.735753]
objective value function right now is: -1723.7397597800089
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.103026]
objective value function right now is: -1754.222804183706
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.142877]
objective value function right now is: -1559.6275995803792
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.71297]
objective value function right now is: -1737.6149546545944
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.460708]
objective value function right now is: -1564.0463672805006
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.758587]
objective value function right now is: -1727.620316015321
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.121946]
objective value function right now is: -1767.219530160633
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [14.30317]
objective value function right now is: -1744.3419510178235
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.086788]
objective value function right now is: -1762.4121572305905
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.51391]
objective value function right now is: -1754.6960469241026
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.883773]
objective value function right now is: -1749.4511520317863
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.05346]
objective value function right now is: -1758.019739737452
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.667556]
objective value function right now is: -1754.655033850335
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.505582]
objective value function right now is: -1750.589774678622
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.813739]
objective value function right now is: -1751.0057320647948
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.937037]
objective value function right now is: -1763.941998170331
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.358566]
objective value function right now is: -1752.558404345149
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.871551]
objective value function right now is: -1746.6920447936277
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.910018]
objective value function right now is: -1738.6998882396035
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.735805]
objective value function right now is: -1751.3527828374997
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.704619]
objective value function right now is: -1704.8548098060498
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [14.914426]
objective value function right now is: -1692.801422880398
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [15.200402]
objective value function right now is: -1765.7952109324021
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.564291]
objective value function right now is: -1748.3859602484802
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.053121]
objective value function right now is: -1752.3232446863215
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.822478]
objective value function right now is: -1765.8120415507872
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.823466]
objective value function right now is: -1758.4229263399502
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.819552]
objective value function right now is: -1765.5552160881732
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.853082]
objective value function right now is: -1755.0944514094642
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.345093]
objective value function right now is: -1747.1283179336992
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.818392]
objective value function right now is: -1755.4037284411315
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.880198]
objective value function right now is: -1763.3790576157146
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.455971]
objective value function right now is: -1754.0950419227406
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.473964]
objective value function right now is: -1726.8039275352148
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.891913]
objective value function right now is: -1752.093001289653
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.898681]
objective value function right now is: -1751.9275786228166
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.827849]
objective value function right now is: -1733.7597385546994
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.680473]
objective value function right now is: -1755.0450709955783
new min fval:  -1736.109354123683
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.880909]
objective value function right now is: -1743.4674559031791
new min fval:  -1743.7427232322245
new min fval:  -1745.8308461463153
new min fval:  -1749.0559247520584
new min fval:  -1752.3552448393596
new min fval:  -1754.9492252930038
new min fval:  -1757.502599705508
new min fval:  -1760.1251838282808
new min fval:  -1762.676326875182
new min fval:  -1764.8309059495325
new min fval:  -1766.4998252684006
new min fval:  -1767.5936208013786
new min fval:  -1768.277672861974
new min fval:  -1768.766024250425
new min fval:  -1769.1321924762253
new min fval:  -1769.1383762262558
new min fval:  -1769.3033114631637
new min fval:  -1769.517286863029
new min fval:  -1769.7215300587072
new min fval:  -1769.7989467351051
new min fval:  -1769.8262658481624
new min fval:  -1769.850295526629
new min fval:  -1769.9166001043372
new min fval:  -1769.9748276499402
new min fval:  -1770.0305997872424
new min fval:  -1770.0909412608935
new min fval:  -1770.1713959573742
new min fval:  -1770.2074206336993
new min fval:  -1770.2240824989035
new min fval:  -1770.235355709714
new min fval:  -1770.2388858324673
new min fval:  -1770.2484807885505
new min fval:  -1770.2746010914393
new min fval:  -1770.306910856163
new min fval:  -1770.3479085826925
new min fval:  -1770.4000048264938
new min fval:  -1770.4649572539079
new min fval:  -1770.5227117440386
new min fval:  -1770.5618879737763
new min fval:  -1770.5806958890428
new min fval:  -1770.5862129649436
new min fval:  -1770.6297008918636
new min fval:  -1770.659404804984
new min fval:  -1770.677972745183
new min fval:  -1770.689945319379
new min fval:  -1770.7104095120164
new min fval:  -1770.7564012065604
new min fval:  -1770.791391537938
new min fval:  -1770.8141142956429
new min fval:  -1770.8227795866426
new min fval:  -1770.8263321834386
new min fval:  -1770.8293346457065
new min fval:  -1770.8345119169528
new min fval:  -1770.8504443151542
new min fval:  -1770.8740568776402
new min fval:  -1770.9098470870379
new min fval:  -1770.985780718002
new min fval:  -1770.9881608682067
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.635603]
objective value function right now is: -1740.761043927483
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.046331]
objective value function right now is: -1709.48614518838
new min fval:  -1770.9891603348474
new min fval:  -1770.9910089330408
new min fval:  -1770.9948270400557
new min fval:  -1770.9948976335595
new min fval:  -1770.9972149948073
new min fval:  -1770.9995583621283
new min fval:  -1771.0035323256473
new min fval:  -1771.0063943297234
new min fval:  -1771.01029132648
new min fval:  -1771.0153403089282
new min fval:  -1771.01880553651
new min fval:  -1771.0214727477496
new min fval:  -1771.0236057570187
new min fval:  -1771.0256502322263
new min fval:  -1771.026996024591
new min fval:  -1771.0318032479245
new min fval:  -1771.0367038712563
new min fval:  -1771.0410336284992
new min fval:  -1771.0493005310789
new min fval:  -1771.0580767870024
new min fval:  -1771.0653792028838
new min fval:  -1771.0704201324786
new min fval:  -1771.0753673320983
new min fval:  -1771.076171300544
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.360993]
objective value function right now is: -1732.725994151018
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.861649]
objective value function right now is: -1758.5263886856283
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.957662]
objective value function right now is: -1742.1119244096117
min fval:  -1771.076171300544
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 736.5537171013441
W_T_median: 495.10091819492567
W_T_pctile_5: 220.3244325583954
W_T_CVAR_5_pct: 56.16249814267262
Average q (qsum/M+1):  48.082291141633064
Optimal xi:  [14.742583]
Expected(across Rb) median(across samples) p_equity:  0.26631189535061517
obj fun:  tensor(-1771.0762, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.447526]
objective value function right now is: -4485.083718088493
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.69135]
objective value function right now is: -4465.375711417564
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.884987]
objective value function right now is: -4341.53707560184
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.42222]
objective value function right now is: -4389.45356093639
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.404162]
objective value function right now is: -4260.720696842184
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.458211]
objective value function right now is: -4283.4556121339765
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [15.880581]
objective value function right now is: -4451.559777046885
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.392711]
objective value function right now is: -4123.908679492075
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.425386]
objective value function right now is: -4381.357851940224
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.519451]
objective value function right now is: -4302.109758220749
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.5239725]
objective value function right now is: -4356.426471931931
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.619286]
objective value function right now is: -4276.642062080096
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.801798]
objective value function right now is: -4101.952590666591
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [15.64276]
objective value function right now is: -4446.205105151709
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.3029175]
objective value function right now is: -4434.617576724634
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.287381]
objective value function right now is: -4332.4435173542015
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.548561]
objective value function right now is: -4371.103166746003
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.596678]
objective value function right now is: -4388.327054521516
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.688264]
objective value function right now is: -4210.379336233628
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.704158]
objective value function right now is: -4511.054134305069
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.859471]
objective value function right now is: -4150.720412401272
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.67219]
objective value function right now is: -4341.527232709364
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.59865]
objective value function right now is: -4448.207367024105
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.700479]
objective value function right now is: -4384.848040660678
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.69403]
objective value function right now is: -4446.837787766669
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.461807]
objective value function right now is: -4390.595844575632
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.871492]
objective value function right now is: -4427.675348289
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [15.502909]
objective value function right now is: -4493.158255211551
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [15.695701]
objective value function right now is: -4039.026964650992
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.575649]
objective value function right now is: -4418.8077684370055
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.230519]
objective value function right now is: -4408.869310026634
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.70053]
objective value function right now is: -4229.742055315023
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.773937]
objective value function right now is: -4474.822605769491
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.813057]
objective value function right now is: -4353.108592392154
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.487383]
objective value function right now is: -4403.8196585872465
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.486062]
objective value function right now is: -4311.090382992173
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.633323]
objective value function right now is: -4017.33954895834
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.576777]
objective value function right now is: -4437.3944727413955
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.342454]
objective value function right now is: -4264.494223694087
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.267796]
objective value function right now is: -4005.2565674965576
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.297098]
objective value function right now is: -4361.417442248752
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.868798]
objective value function right now is: -4435.096497720904
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.685947]
objective value function right now is: -4513.846542344702
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.495254]
objective value function right now is: -4451.877875688071
new min fval:  -3463.3378128097597
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.865289]
objective value function right now is: -4506.402937264427
new min fval:  -4485.249346569126
new min fval:  -4487.192572196249
new min fval:  -4489.313025004804
new min fval:  -4490.174741298879
new min fval:  -4492.673443211984
new min fval:  -4492.84958766322
new min fval:  -4494.554713463394
new min fval:  -4494.829305615224
new min fval:  -4495.132982516252
new min fval:  -4496.759496797981
new min fval:  -4498.511051428627
new min fval:  -4500.98703698032
new min fval:  -4503.435873033692
new min fval:  -4505.711768466368
new min fval:  -4507.565616407231
new min fval:  -4509.121476525741
new min fval:  -4510.280251235418
new min fval:  -4511.21927011239
new min fval:  -4511.8358186011965
new min fval:  -4511.884834992887
new min fval:  -4512.331917548163
new min fval:  -4513.081974315418
new min fval:  -4514.340448002556
new min fval:  -4515.195121685173
new min fval:  -4515.921672120469
new min fval:  -4516.9197972208185
new min fval:  -4517.871719901756
new min fval:  -4518.752079182506
new min fval:  -4519.634170596814
new min fval:  -4520.4474439794485
new min fval:  -4521.105132118207
new min fval:  -4521.681490160653
new min fval:  -4522.260611380161
new min fval:  -4523.448903899379
new min fval:  -4524.492248735899
new min fval:  -4525.329919100719
new min fval:  -4525.800592241581
new min fval:  -4527.356128747473
new min fval:  -4529.262623014133
new min fval:  -4531.423590287802
new min fval:  -4531.497370046323
new min fval:  -4531.539880989596
new min fval:  -4531.585100615852
new min fval:  -4531.64817276701
new min fval:  -4531.820392161131
new min fval:  -4531.9863570698
new min fval:  -4532.097236565128
new min fval:  -4532.340676886611
new min fval:  -4532.696594697386
new min fval:  -4533.070685499549
new min fval:  -4533.337526305748
new min fval:  -4533.725106974495
new min fval:  -4534.272457826175
new min fval:  -4534.94130333751
new min fval:  -4535.649030006761
new min fval:  -4536.3179605289415
new min fval:  -4536.937180538934
new min fval:  -4537.498679326706
new min fval:  -4538.014245387513
new min fval:  -4538.391195291702
new min fval:  -4538.706841394469
new min fval:  -4538.951787781698
new min fval:  -4539.081787402709
new min fval:  -4539.155946073558
new min fval:  -4539.180280652031
new min fval:  -4539.201444158584
new min fval:  -4539.241591565872
new min fval:  -4539.336394635016
new min fval:  -4539.3789308234245
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.021855]
objective value function right now is: -3607.525374601973
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.769269]
objective value function right now is: -4441.085564262956
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.20377]
objective value function right now is: -4368.308779362181
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.907149]
objective value function right now is: -4410.87460204219
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.873447]
objective value function right now is: 9193.906229623954
min fval:  -4539.3789308234245
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 835.7898191969565
W_T_median: 618.4280874273707
W_T_pctile_5: 246.72690124401717
W_T_CVAR_5_pct: 62.570498981900265
Average q (qsum/M+1):  45.55219490297379
Optimal xi:  [15.6383095]
Expected(across Rb) median(across samples) p_equity:  0.246182781457901
obj fun:  tensor(-4539.3789, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.5456609]
objective value function right now is: 3746591.4532114314
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00387475]
objective value function right now is: 1908771.039265444
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.664668e-08]
objective value function right now is: 1720062.8985837584
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.138859e-12]
objective value function right now is: 1496872.0257304711
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [9.850812e-18]
objective value function right now is: 1273462.7575372958
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.0233526e-21]
objective value function right now is: 907857.5234076155
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [2.6510921e-05]
objective value function right now is: 296788.00467408553
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.203384]
objective value function right now is: 116186.92378853656
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.794982]
objective value function right now is: 115479.0559731508
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.309549]
objective value function right now is: 116352.60371995896
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.709449]
objective value function right now is: 116246.27493578485
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.633725]
objective value function right now is: 121782.34662664204
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.6667185]
objective value function right now is: 114822.87612321998
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [5.126231]
objective value function right now is: 1336413.9044011587
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [10.968861]
objective value function right now is: 105929.26190042752
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.492366]
objective value function right now is: 96070.54306782807
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.380281]
objective value function right now is: 102350.05617743373
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.656365]
objective value function right now is: 97774.01023856807
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [11.595917]
objective value function right now is: 97465.45017508912
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [14.121308]
objective value function right now is: -37663.16783330211
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.9918585]
objective value function right now is: 6070608.3496904615
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.02627335]
objective value function right now is: 4719052.992034617
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.2275385e-07]
objective value function right now is: 4537902.255055002
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.5031487e-12]
objective value function right now is: 2451774.23139345
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.4183918e-16]
objective value function right now is: 1444357.0807064695
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [1.0373862e-17]
objective value function right now is: 469933.7235007048
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00079908]
objective value function right now is: 260118.26947401784
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [13.476908]
objective value function right now is: -30600.53485103485
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [13.50266]
objective value function right now is: -89882.90294424616
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.2912655]
objective value function right now is: -150016.4161754284
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.996722]
objective value function right now is: -233594.448504044
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [13.598315]
objective value function right now is: -707.424975607615
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.207018]
objective value function right now is: -259181.1827124502
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.433565]
objective value function right now is: -196810.38051584148
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.622345]
objective value function right now is: -272937.74099506624
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.669379]
objective value function right now is: -286372.854896706
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.576798]
objective value function right now is: -273555.14491742157
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.570388]
objective value function right now is: -284992.3545427343
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.226598]
objective value function right now is: -260547.11570745046
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.7279]
objective value function right now is: -303302.81822978065
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.502146]
objective value function right now is: -298693.7590910392
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.427844]
objective value function right now is: -200983.573867859
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.618115]
objective value function right now is: -292161.64856287284
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [15.4205265]
objective value function right now is: -282415.807811961
new min fval:  1114062.263054213
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [16.058205]
objective value function right now is: -226055.78253347575
new min fval:  -236787.09166521602
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.5937937]
objective value function right now is: 1846747.824721547
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00956442]
objective value function right now is: 1055038.9290187394
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.701849e-07]
objective value function right now is: 806205.649258693
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00074789]
objective value function right now is: 295994.55342625704
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.208615]
objective value function right now is: 118425.8975877274
min fval:  -236787.09166521602
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1118.8809333491415
W_T_median: 813.3709803089112
W_T_pctile_5: 272.05342494537257
W_T_CVAR_5_pct: 47.85781066025779
Average q (qsum/M+1):  42.78263215095766
Optimal xi:  [16.070438]
Expected(across Rb) median(across samples) p_equity:  0.2763110101222992
obj fun:  tensor(-236787.0917, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5000.0
-----------------------------------------------
