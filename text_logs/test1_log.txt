Starting at: 
17-10-23_12:15:50

 numpy seed:  2  


 pytorch seed:  2  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.

 NN training settings: 
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 20, 'itbound_SGD_algorithms': 2000, 'nit_IterateAveragingStart': 1800, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
Intialized NN model structure for withdrawal: 
Sequential(
  (hidden_layer_0): Linear(in_features=2, out_features=8, bias=True)
  (hidden_layer_0_activation): Sigmoid()
  (hidden_layer_1): Linear(in_features=8, out_features=8, bias=True)
  (hidden_layer_1_activation): Sigmoid()
  (hidden_layer_2): Linear(in_features=8, out_features=1, bias=True)
  (hidden_layer_2_activation): Identity()
)
Intialized NN model structure for allocation: 
Sequential(
  (hidden_layer_0): Linear(in_features=2, out_features=8, bias=True)
  (hidden_layer_0_activation): Sigmoid()
  (hidden_layer_1): Linear(in_features=8, out_features=8, bias=True)
  (hidden_layer_1_activation): Sigmoid()
  (hidden_layer_2): Linear(in_features=8, out_features=2, bias=True)
  (hidden_layer_2_activation): Softmax(dim=1)
)
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.5 0.5]
W_T_mean: 2160.553598397474
W_T_median: 1449.009936889302
W_T_pctile_5: -155.17696299638737
W_T_CVAR_5_pct: -354.9955379746938
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1032.3375611159652
Current xi:  [99.44794]
objective value function right now is: -1032.3375611159652
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1041.5464022871142
Current xi:  [99.606125]
objective value function right now is: -1041.5464022871142
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1049.0774410158153
Current xi:  [100.13736]
objective value function right now is: -1049.0774410158153
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1052.00933288867
Current xi:  [100.90298]
objective value function right now is: -1052.00933288867
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1056.1045510083686
Current xi:  [101.929]
objective value function right now is: -1056.1045510083686
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [102.91062]
objective value function right now is: -1053.2954304569514
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1056.7049333677157
Current xi:  [104.02696]
objective value function right now is: -1056.7049333677157
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.143745]
objective value function right now is: -1047.2220865319487
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1060.4736531169542
Current xi:  [106.172005]
objective value function right now is: -1060.4736531169542
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1062.0899956712587
Current xi:  [107.32941]
objective value function right now is: -1062.0899956712587
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.378716]
objective value function right now is: -1061.0262671654154
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1062.6537826615267
Current xi:  [109.477806]
objective value function right now is: -1062.6537826615267
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1063.5992293598936
Current xi:  [110.71984]
objective value function right now is: -1063.5992293598936
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1067.5652970167575
Current xi:  [111.97965]
objective value function right now is: -1067.5652970167575
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [112.64665]
objective value function right now is: -1066.1058366660893
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [113.641426]
objective value function right now is: -1063.1329000909152
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [114.7019]
objective value function right now is: -1066.5192765359307
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1068.503771202618
Current xi:  [116.12174]
objective value function right now is: -1068.503771202618
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1070.0619070397927
Current xi:  [117.70833]
objective value function right now is: -1070.0619070397927
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1073.8688762525944
Current xi:  [118.96343]
objective value function right now is: -1073.8688762525944
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1083.3441860260132
Current xi:  [120.149605]
objective value function right now is: -1083.3441860260132
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1154.1397076703395
Current xi:  [121.14706]
objective value function right now is: -1154.1397076703395
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1187.9376982478536
Current xi:  [121.20316]
objective value function right now is: -1187.9376982478536
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1210.6996258161603
Current xi:  [121.067955]
objective value function right now is: -1210.6996258161603
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1308.480893489331
Current xi:  [120.6582]
objective value function right now is: -1308.480893489331
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1350.3624286724341
Current xi:  [118.91463]
objective value function right now is: -1350.3624286724341
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1435.145862225987
Current xi:  [116.85529]
objective value function right now is: -1435.145862225987
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1469.0395991913122
Current xi:  [115.254005]
objective value function right now is: -1469.0395991913122
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1488.423305296707
Current xi:  [113.942665]
objective value function right now is: -1488.423305296707
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1495.2600081921244
Current xi:  [112.7067]
objective value function right now is: -1495.2600081921244
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1506.3291651711654
Current xi:  [111.24048]
objective value function right now is: -1506.3291651711654
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [109.99771]
objective value function right now is: -1503.9491733573846
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1511.6501961968047
Current xi:  [108.70499]
objective value function right now is: -1511.6501961968047
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [107.57535]
objective value function right now is: -1510.627422820651
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -1518.3523234226247
Current xi:  [106.48149]
objective value function right now is: -1518.3523234226247
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1519.294556619136
Current xi:  [106.30956]
objective value function right now is: -1519.294556619136
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1520.449680692118
Current xi:  [106.09975]
objective value function right now is: -1520.449680692118
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1520.6713708004763
Current xi:  [105.88693]
objective value function right now is: -1520.6713708004763
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1521.7064576415078
Current xi:  [105.67448]
objective value function right now is: -1521.7064576415078
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [105.4676]
objective value function right now is: -1520.9013154370396
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1522.3972546534671
Current xi:  [105.28192]
objective value function right now is: -1522.3972546534671
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1522.708910239836
Current xi:  [105.05915]
objective value function right now is: -1522.708910239836
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.84764]
objective value function right now is: -1521.936174398849
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1523.3892962968403
Current xi:  [104.63979]
objective value function right now is: -1523.3892962968403
90.0% of gradient descent iterations done. Method = Adam
new min fval:  -1523.769150266324
Current xi:  [104.43489]
objective value function right now is: -1523.769150266324
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -1524.716338259363
Current xi:  [104.207054]
objective value function right now is: -1524.716338259363
94.0% of gradient descent iterations done. Method = Adam
new min fval:  -1525.1433644339363
Current xi:  [103.99812]
objective value function right now is: -1525.1433644339363
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.80064]
objective value function right now is: -1524.3841463420276
98.0% of gradient descent iterations done. Method = Adam
new min fval:  -1525.7996097006276
Current xi:  [103.68589]
objective value function right now is: -1525.7996097006276
new min fval from sgd:  -1525.81342665414
new min fval from sgd:  -1525.8160112280184
new min fval from sgd:  -1525.8409916271175
new min fval from sgd:  -1525.882556515175
new min fval from sgd:  -1525.9201201197532
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [103.642235]
objective value function right now is: -1525.9201201197532
min fval:  -1525.9201201197532
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 669.3494535304568
W_T_median: 331.69935497645616
W_T_pctile_5: 96.67497854745656
W_T_CVAR_5_pct: -42.14059736312364
Average q (qsum/M+1):  50.601015152469756
Optimal xi:  103.6422348022461
Expected(across Rb) median(across samples) p_equity:  0.3210372765858968
obj fun:  -1525.9201201197532
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
Intialized NN model structure for withdrawal: 
Sequential(
  (hidden_layer_0): Linear(in_features=2, out_features=8, bias=True)
  (hidden_layer_0_activation): Sigmoid()
  (hidden_layer_1): Linear(in_features=8, out_features=8, bias=True)
  (hidden_layer_1_activation): Sigmoid()
  (hidden_layer_2): Linear(in_features=8, out_features=1, bias=True)
  (hidden_layer_2_activation): Identity()
)
Intialized NN model structure for allocation: 
Sequential(
  (hidden_layer_0): Linear(in_features=2, out_features=8, bias=True)
  (hidden_layer_0_activation): Sigmoid()
  (hidden_layer_1): Linear(in_features=8, out_features=8, bias=True)
  (hidden_layer_1_activation): Sigmoid()
  (hidden_layer_2): Linear(in_features=8, out_features=2, bias=True)
  (hidden_layer_2_activation): Softmax(dim=1)
)
Transfer learning model loaded from kappa_1.0.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.5 0.5]
W_T_mean: 2160.553598397474
W_T_median: 1449.009936889302
W_T_pctile_5: -155.17696299638737
W_T_CVAR_5_pct: -354.9955379746938
-----------------------------------------------

 Side loaded standardization params from pre-trained model. 

2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1487.1168874303598
Current xi:  [[103.94639]]
objective value function right now is: -1487.1168874303598
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1495.0810677229747
Current xi:  [[104.420525]]
objective value function right now is: -1495.0810677229747
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1497.4079748140741
Current xi:  [[104.896164]]
objective value function right now is: -1497.4079748140741
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [[105.15274]]
objective value function right now is: -1495.2379308093148
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [[105.732025]]
objective value function right now is: -1495.744146514798
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [[106.197556]]
objective value function right now is: -1485.174982117436
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [[106.49397]]
objective value function right now is: -1490.1833084921902
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [[107.00761]]
objective value function right now is: -1491.4390280743733
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [[107.662636]]
objective value function right now is: -1471.6433242762096
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1500.5874302357745
Current xi:  [[107.71908]]
objective value function right now is: -1500.5874302357745
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [[108.09782]]
objective value function right now is: -1498.509775002942
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [[108.49709]]
objective value function right now is: -1492.1400229627063
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [[108.86742]]
objective value function right now is: -1478.5956201838894
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [[109.36689]]
objective value function right now is: -1499.0471354737238
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [[109.75531]]
objective value function right now is: -1494.8072000471311
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [[109.85301]]
objective value function right now is: -1495.1925770370005
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [[110.17875]]
objective value function right now is: -1500.5214184068907
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1511.839787685758
Current xi:  [[110.59313]]
objective value function right now is: -1511.839787685758
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1523.9490004477047
Current xi:  [[110.8363]]
objective value function right now is: -1523.9490004477047
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [[111.164246]]
objective value function right now is: -1516.8634358179586
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1532.0128003698878
Current xi:  [[111.63467]]
objective value function right now is: -1532.0128003698878
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [[112.167885]]
objective value function right now is: -1525.431075087133
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1533.0255095150007
Current xi:  [[112.697784]]
objective value function right now is: -1533.0255095150007
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [[113.32115]]
objective value function right now is: -1528.726557483122
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1533.689952261676
Current xi:  [[114.059944]]
objective value function right now is: -1533.689952261676
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [[114.400955]]
objective value function right now is: -1522.975705170166
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [[114.76024]]
objective value function right now is: -1532.7769726596528
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1537.7208383376699
Current xi:  [[115.44007]]
objective value function right now is: -1537.7208383376699
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [[115.93264]]
objective value function right now is: -1531.7629140456854
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [[116.40114]]
objective value function right now is: -1533.0372919769363
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [[116.94896]]
objective value function right now is: -1522.4440032250097
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [[117.32187]]
objective value function right now is: -1529.2149163671988
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [[117.649666]]
objective value function right now is: -1536.2970957385526
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1539.3393721318484
Current xi:  [[118.21019]]
objective value function right now is: -1539.3393721318484
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [[118.6345]]
objective value function right now is: -1537.5583398308872
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1542.1467189552911
Current xi:  [[118.728935]]
objective value function right now is: -1542.1467189552911
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [[118.85525]]
objective value function right now is: -1541.7571468632332
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [[118.973854]]
objective value function right now is: -1541.8162840378568
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [[119.12932]]
objective value function right now is: -1542.104430758043
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1542.9504647537221
Current xi:  [[119.27098]]
objective value function right now is: -1542.9504647537221
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1543.0620733214305
Current xi:  [[119.391815]]
objective value function right now is: -1543.0620733214305
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [[119.505325]]
objective value function right now is: -1541.9300051334276
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1543.4102111810373
Current xi:  [[119.62341]]
objective value function right now is: -1543.4102111810373
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [[119.74326]]
objective value function right now is: -1538.8601066828683
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [[119.86114]]
objective value function right now is: -1540.1735601346395
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [[119.96541]]
objective value function right now is: -1541.016566545111
94.0% of gradient descent iterations done. Method = Adam
new min fval:  -1544.8539951589732
Current xi:  [[120.06579]]
objective value function right now is: -1544.8539951589732
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [[120.19362]]
objective value function right now is: -1537.052014413118
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [[120.24919]]
objective value function right now is: -1544.786972886199
new min fval from sgd:  -1545.1225123027812
new min fval from sgd:  -1545.2842991791624
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [[120.270584]]
objective value function right now is: -1545.2842991791624
min fval:  -1545.2842991791624
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 666.5006620374947
W_T_median: 355.56098485795036
W_T_pctile_5: 121.7019204264877
W_T_CVAR_5_pct: -2.5169057736069815
Average q (qsum/M+1):  50.01539267263105
Optimal xi:  120.27058410644531
Expected(across Rb) median(across samples) p_equity:  0.3018801689147949
obj fun:  -1545.2842991791624
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.0
-----------------------------------------------
