Starting at: 
08-06-23_15:20

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = MC_everything
timeseries_basket['basket_desc'] = marc_test1_all_assets_longfactors
timeseries_basket['basket_columns'] = 
['Size_Lo30_real_ret', 'Value_Hi30_real_ret', 'Oprof_Hi30_real_ret', 'Inv_Lo30_real_ret', 'Mom_Hi30_real_ret', 'EP_Hi30_real_ret', 'Vol_Lo20_real_ret', 'Div_Hi30_real_ret', 'EQWFact_real_ret', 'T30_real_ret', 'T90_real_ret', 'B10_real_ret', 'VWD_real_ret', 'EWD_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = MC_everything
timeseries_basket['basket_desc'] = marc_test1_all_assets_longfactors
timeseries_basket['basket_columns'] = 
['Size_Lo30_nom_ret', 'Value_Hi30_nom_ret', 'Oprof_Hi30_nom_ret', 'Inv_Lo30_nom_ret', 'Mom_Hi30_nom_ret', 'EP_Hi30_nom_ret', 'Vol_Lo20_nom_ret', 'Div_Hi30_nom_ret', 'EQWFact_nom_ret', 'T30_nom_ret', 'T90_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'EWD_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.031411     0.013051
192608                    0.0319              0.0561  ...     0.028647     0.031002
192609                   -0.0173             -0.0071  ...     0.005787    -0.006499
192610                   -0.0294             -0.0355  ...    -0.028996    -0.034630
192611                   -0.0038              0.0294  ...     0.028554     0.024776

[5 rows x 15 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.036240    -0.011556
202209                   -0.0955             -0.0871  ...    -0.091324    -0.099903
202210                    0.0883              0.1486  ...     0.077403     0.049863
202211                   -0.0076              0.0462  ...     0.052365     0.028123
202212                   -0.0457             -0.0499  ...    -0.057116    -0.047241

[5 rows x 15 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'Oprof_Hi30_nom_ret_ind', 'Inv_Lo30_nom_ret_ind',
       'Mom_Hi30_nom_ret_ind', 'EP_Hi30_nom_ret_ind', 'Vol_Lo20_nom_ret_ind',
       'Div_Hi30_nom_ret_ind', 'EQWFact_nom_ret_ind', 'CPI_nom_ret_ind',
       'T30_nom_ret_ind', 'T90_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind', 'EWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
Oprof_Hi30_real_ret    0.003948
Inv_Lo30_real_ret      0.004513
Mom_Hi30_real_ret      0.011386
EP_Hi30_real_ret       0.007033
Vol_Lo20_real_ret      0.003529
Div_Hi30_real_ret      0.007888
EQWFact_real_ret       0.004508
T30_real_ret           0.000229
T90_real_ret           0.000501
B10_real_ret           0.001637
VWD_real_ret           0.006759
EWD_real_ret           0.009545
dtype: float64


timeseries_basket['data_df_stdev'] = 
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
Oprof_Hi30_real_ret    0.037444
Inv_Lo30_real_ret      0.037639
Mom_Hi30_real_ret      0.061421
EP_Hi30_real_ret       0.041390
Vol_Lo20_real_ret      0.030737
Div_Hi30_real_ret      0.056728
EQWFact_real_ret       0.037131
T30_real_ret           0.005227
T90_real_ret           0.005373
B10_real_ret           0.019258
VWD_real_ret           0.053610
EWD_real_ret           0.071360
dtype: float64


timeseries_basket['data_df_corr'] = 
                     Size_Lo30_real_ret  ...  EWD_real_ret
Size_Lo30_real_ret             1.000000  ...      0.977206
Value_Hi30_real_ret            0.908542  ...      0.919912
Oprof_Hi30_real_ret            0.433774  ...      0.462336
Inv_Lo30_real_ret              0.455237  ...      0.479661
Mom_Hi30_real_ret              0.903222  ...      0.912002
EP_Hi30_real_ret               0.476966  ...      0.506661
Vol_Lo20_real_ret              0.360014  ...      0.382411
Div_Hi30_real_ret              0.816292  ...      0.849068
EQWFact_real_ret               0.494572  ...      0.513359
T30_real_ret                   0.014412  ...      0.029084
T90_real_ret                   0.021968  ...      0.037909
B10_real_ret                   0.012916  ...      0.024853
VWD_real_ret                   0.865290  ...      0.907369
EWD_real_ret                   0.977206  ...      1.000000

[14 rows x 14 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      22  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      22  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 22)     True          22  
2     (22, 22)     True          22  
3      (22, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer      14       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/factor_decumulation/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      22  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      22  logistic_sigmoid   
3        obj.layers[3]        3  output_layer      14           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 22)     True          22  
2     (22, 22)     True          22  
3     (22, 14)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0      (22, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer       14           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0     (22, 14)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0      (22, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       22  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       22  logistic_sigmoid   
0        obj.layers[3]         3  output_layer       14           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 22)      True          22  
0     (22, 22)      True          22  
0     (22, 14)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857
 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857
 0.07142857 0.07142857]
W_T_mean: 8576.841924009083
W_T_median: 5042.380921625826
W_T_pctile_5: 103.64169553244135
W_T_CVAR_5_pct: -371.9331258181762
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2202.071385247156
Current xi:  [115.684494]
objective value function right now is: -2202.071385247156
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -2235.1815154951987
Current xi:  [135.9319]
objective value function right now is: -2235.1815154951987
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -2265.624783211362
Current xi:  [157.68762]
objective value function right now is: -2265.624783211362
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -2295.722376032385
Current xi:  [180.09973]
objective value function right now is: -2295.722376032385
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [202.51611]
objective value function right now is: -2258.9322760669666
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -2328.4991145158488
Current xi:  [225.20566]
objective value function right now is: -2328.4991145158488
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -2337.8757705801113
Current xi:  [249.35619]
objective value function right now is: -2337.8757705801113
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -2340.7940792011427
Current xi:  [269.56195]
objective value function right now is: -2340.7940792011427
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -2364.5589181675196
Current xi:  [290.20023]
objective value function right now is: -2364.5589181675196
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [311.20575]
objective value function right now is: -2358.655975725604
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [332.5713]
objective value function right now is: -2360.0050285409297
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -2411.7751971253306
Current xi:  [356.87122]
objective value function right now is: -2411.7751971253306
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [379.6036]
objective value function right now is: -2406.37284340604
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [401.49152]
objective value function right now is: -2386.669579926016
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -2441.5114794623228
Current xi:  [422.6274]
objective value function right now is: -2441.5114794623228
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -2453.5725879235806
Current xi:  [444.45395]
objective value function right now is: -2453.5725879235806
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [465.52576]
objective value function right now is: -2450.917408779919
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -2466.0626417742
Current xi:  [486.49673]
objective value function right now is: -2466.0626417742
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -2477.368953935863
Current xi:  [507.2592]
objective value function right now is: -2477.368953935863
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2489.1101849883376
Current xi:  [528.6127]
objective value function right now is: -2489.1101849883376
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [548.39185]
objective value function right now is: -2445.283789038888
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [568.1022]
objective value function right now is: -2480.830675763166
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2494.8972177332753
Current xi:  [587.3734]
objective value function right now is: -2494.8972177332753
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -2529.8867586511833
Current xi:  [607.0568]
objective value function right now is: -2529.8867586511833
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -2532.1268960918646
Current xi:  [626.6127]
objective value function right now is: -2532.1268960918646
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -2540.8287844841466
Current xi:  [645.85223]
objective value function right now is: -2540.8287844841466
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [664.1683]
objective value function right now is: -2537.861090462923
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -2559.7525667888276
Current xi:  [682.2452]
objective value function right now is: -2559.7525667888276
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -2563.1368279496432
Current xi:  [699.51404]
objective value function right now is: -2563.1368279496432
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -2564.2646849700855
Current xi:  [716.26044]
objective value function right now is: -2564.2646849700855
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -2576.093708506642
Current xi:  [734.73145]
objective value function right now is: -2576.093708506642
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -2576.536902915977
Current xi:  [753.3906]
objective value function right now is: -2576.536902915977
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -2588.0662334956955
Current xi:  [770.8207]
objective value function right now is: -2588.0662334956955
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [788.1291]
objective value function right now is: -2582.649115452464
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -2589.979238427866
Current xi:  [804.56494]
objective value function right now is: -2589.979238427866
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2605.523832966501
Current xi:  [808.04333]
objective value function right now is: -2605.523832966501
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -2606.9936807246777
Current xi:  [811.64325]
objective value function right now is: -2606.9936807246777
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [815.4749]
objective value function right now is: -2606.1245077116373
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2609.8339993982477
Current xi:  [819.28156]
objective value function right now is: -2609.8339993982477
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -2611.1363117109145
Current xi:  [822.9656]
objective value function right now is: -2611.1363117109145
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -2612.324933852421
Current xi:  [826.6931]
objective value function right now is: -2612.324933852421
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -2614.858848905694
Current xi:  [830.4228]
objective value function right now is: -2614.858848905694
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -2615.2604908456124
Current xi:  [834.2772]
objective value function right now is: -2615.2604908456124
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -2616.2608112007256
Current xi:  [837.93475]
objective value function right now is: -2616.2608112007256
new min fval from sgd:  -2618.6635244470135
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [841.57227]
objective value function right now is: -2618.6635244470135
new min fval from sgd:  -2618.8404009288915
new min fval from sgd:  -2618.9239974730062
new min fval from sgd:  -2618.9827243052546
new min fval from sgd:  -2619.1356815172694
new min fval from sgd:  -2619.3366179920567
new min fval from sgd:  -2619.473080076425
new min fval from sgd:  -2619.6641362962005
new min fval from sgd:  -2619.835832998827
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [845.25635]
objective value function right now is: -2616.6315918070813
new min fval from sgd:  -2619.8793224369288
new min fval from sgd:  -2619.9967057424255
new min fval from sgd:  -2620.1147094303274
new min fval from sgd:  -2620.1276268963475
new min fval from sgd:  -2620.272321426048
new min fval from sgd:  -2620.3696459512266
new min fval from sgd:  -2620.457788455071
new min fval from sgd:  -2620.5174143633953
new min fval from sgd:  -2620.5491322179873
new min fval from sgd:  -2620.821712453592
new min fval from sgd:  -2621.100838523712
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [848.9639]
objective value function right now is: -2616.431136282712
new min fval from sgd:  -2621.113570955612
new min fval from sgd:  -2621.432123286161
new min fval from sgd:  -2621.441992889327
new min fval from sgd:  -2621.6720946059863
new min fval from sgd:  -2621.7113968425133
new min fval from sgd:  -2621.888929000781
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [852.7434]
objective value function right now is: -2620.0634850486795
new min fval from sgd:  -2621.9127401069886
new min fval from sgd:  -2622.2662822182792
new min fval from sgd:  -2622.3777115583507
new min fval from sgd:  -2622.4416227275124
new min fval from sgd:  -2622.514825111546
new min fval from sgd:  -2622.5910884130412
new min fval from sgd:  -2622.668383826908
new min fval from sgd:  -2622.7379513129845
new min fval from sgd:  -2622.8236541480355
new min fval from sgd:  -2622.8647331316906
new min fval from sgd:  -2622.886309917777
new min fval from sgd:  -2622.9294763720154
new min fval from sgd:  -2622.9568410612214
new min fval from sgd:  -2622.9829766520747
new min fval from sgd:  -2622.9997626277145
new min fval from sgd:  -2623.0056684214974
new min fval from sgd:  -2623.057385627131
new min fval from sgd:  -2623.0674203842595
new min fval from sgd:  -2623.073745809816
new min fval from sgd:  -2623.093944986324
new min fval from sgd:  -2623.1173017101737
new min fval from sgd:  -2623.1439853063303
new min fval from sgd:  -2623.1587788424054
new min fval from sgd:  -2623.1883668862074
new min fval from sgd:  -2623.198567318612
new min fval from sgd:  -2623.2244054093053
new min fval from sgd:  -2623.2263455461175
new min fval from sgd:  -2623.2394489586327
new min fval from sgd:  -2623.268532874371
new min fval from sgd:  -2623.2886389707755
new min fval from sgd:  -2623.3086180347
new min fval from sgd:  -2623.312385821429
new min fval from sgd:  -2623.31961278054
new min fval from sgd:  -2623.3384062580208
new min fval from sgd:  -2623.3589661165506
new min fval from sgd:  -2623.36866282898
new min fval from sgd:  -2623.3951544839615
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [854.8985]
objective value function right now is: -2623.211035799377
new min fval from sgd:  -2623.4008506629
new min fval from sgd:  -2623.400882128479
new min fval from sgd:  -2623.43299884899
new min fval from sgd:  -2623.4422261233235
new min fval from sgd:  -2623.4532869024556
new min fval from sgd:  -2623.469155426002
new min fval from sgd:  -2623.4722829744323
new min fval from sgd:  -2623.478465911391
new min fval from sgd:  -2623.502939359332
new min fval from sgd:  -2623.5083314966123
new min fval from sgd:  -2623.595875344856
new min fval from sgd:  -2623.6558073109873
new min fval from sgd:  -2623.676058684509
new min fval from sgd:  -2623.6805574777
new min fval from sgd:  -2623.6980573481706
new min fval from sgd:  -2623.7042214883054
new min fval from sgd:  -2623.724019305544
new min fval from sgd:  -2623.7463823530143
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [855.6763]
objective value function right now is: -2623.39248442189
min fval:  -2623.7463823530143
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -8.5715,   6.4621],
        [ -8.3669,  -9.0182],
        [ -5.1223,   3.2445],
        [  8.0669,   4.1508],
        [-11.4280,  -6.0457],
        [ -6.4858,   4.0851],
        [  6.0779,   4.9126],
        [  9.1747,   0.7713],
        [ -8.3119,   6.0846],
        [ -8.3147,   6.0977],
        [ -8.4882,   6.6116],
        [ -8.6931,   5.7529],
        [  7.8819,   3.3034],
        [ -1.2974,   1.0853],
        [  5.4594,  -0.3288],
        [  5.2299,   5.2824],
        [-11.7613,  -7.4762],
        [  7.7973,   4.2640],
        [ -8.5799,   5.8746],
        [ -7.8535,   5.7472],
        [ -1.2944,   1.0845],
        [-21.4386,  -5.7294]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 4.8764, -5.7404, -3.0406, -9.5323, -3.7687, -2.3429, -8.4043, -9.1228,
         4.2727,  3.0356,  5.1757,  1.5461, -8.9288, -3.8883, -8.9159, -8.5427,
        -4.0692, -9.2749,  1.8685,  2.4405, -3.8889, -4.6902], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.2585e-01, -1.6547e-01,  1.7509e-03, -1.2930e-02, -1.1901e-01,
          1.1574e-03, -1.6502e-02, -2.2728e-03, -1.8736e-01, -6.9325e-02,
         -2.5298e-01, -1.2688e-02, -9.1088e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7960e-01, -1.2929e-02, -1.9678e-02, -4.9674e-02,
          1.2308e-02, -3.1398e-02],
        [-9.2453e+00,  1.2482e+01, -7.2066e-01,  7.0332e+00,  4.6981e+00,
         -1.2007e+00,  4.5523e+00,  6.2080e+00, -7.5674e+00, -3.0869e+00,
         -1.0021e+01, -1.8126e+00,  5.8578e+00, -1.6237e-01,  6.8999e-01,
          5.4520e+00,  8.4865e+00,  8.0426e+00, -2.2762e+00, -3.8127e+00,
         -1.9618e-01,  8.7135e+00],
        [-2.2586e-01, -1.6548e-01,  1.7510e-03, -1.2930e-02, -1.1901e-01,
          1.1575e-03, -1.6502e-02, -2.2751e-03, -1.8737e-01, -6.9327e-02,
         -2.5298e-01, -1.2689e-02, -9.1085e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7961e-01, -1.2929e-02, -1.9679e-02, -4.9675e-02,
          1.2307e-02, -3.1398e-02],
        [-9.6515e+00,  1.3280e+01, -9.1434e-01,  9.3871e+00,  5.7620e+00,
         -1.3190e+00,  6.4927e+00,  6.8045e+00, -8.3770e+00, -3.3906e+00,
         -1.0939e+01, -2.1716e+00,  6.5179e+00, -2.1870e-01,  2.0491e+00,
          4.1453e+00,  9.2888e+00,  7.8384e+00, -2.6898e+00, -4.4743e+00,
         -1.9003e-01,  9.7752e+00],
        [-7.9773e+00,  1.0315e+01,  2.5895e-01,  4.6608e+00,  4.9211e+00,
          1.7041e-02,  2.1882e+00,  6.8647e+00, -6.6734e+00, -4.8263e+00,
         -8.8073e+00, -2.8795e+00,  6.4311e+00,  1.6629e-01,  1.1111e+00,
          5.0411e+00,  7.6765e+00,  1.1081e+01, -3.5739e+00, -4.0927e+00,
          1.6082e-01,  8.5887e+00],
        [-7.6589e+00,  9.2592e+00,  7.9739e-02,  3.9257e+00,  4.5700e+00,
          1.0005e-01,  2.1053e+00,  4.6166e+00, -6.0015e+00, -3.8545e+00,
         -9.2106e+00, -1.7570e+00,  5.7099e+00, -1.2986e-02,  5.9836e-02,
          4.9159e+00,  7.0161e+00,  9.4703e+00, -2.3319e+00, -3.1763e+00,
         -1.3208e-02,  7.3285e+00],
        [-2.2581e-01, -1.6549e-01,  1.7511e-03, -1.2938e-02, -1.1903e-01,
          1.1578e-03, -1.6509e-02, -2.2839e-03, -1.8738e-01, -6.9338e-02,
         -2.5288e-01, -1.2692e-02, -9.1165e-03,  1.2292e-02,  1.9855e-02,
         -1.7662e-02, -1.7966e-01, -1.2937e-02, -1.9683e-02, -4.9682e-02,
          1.2307e-02, -3.1400e-02],
        [-2.2584e-01, -1.6546e-01,  1.7508e-03, -1.2931e-02, -1.1901e-01,
          1.1573e-03, -1.6502e-02, -2.2704e-03, -1.8735e-01, -6.9322e-02,
         -2.5298e-01, -1.2688e-02, -9.1089e-03,  1.2292e-02,  1.9856e-02,
         -1.7657e-02, -1.7959e-01, -1.2929e-02, -1.9678e-02, -4.9672e-02,
          1.2308e-02, -3.1398e-02],
        [-2.2586e-01, -1.6548e-01,  1.7510e-03, -1.2930e-02, -1.1901e-01,
          1.1575e-03, -1.6502e-02, -2.2752e-03, -1.8737e-01, -6.9327e-02,
         -2.5298e-01, -1.2689e-02, -9.1085e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7961e-01, -1.2929e-02, -1.9679e-02, -4.9675e-02,
          1.2307e-02, -3.1398e-02],
        [-2.2585e-01, -1.6547e-01,  1.7509e-03, -1.2930e-02, -1.1901e-01,
          1.1575e-03, -1.6502e-02, -2.2730e-03, -1.8736e-01, -6.9325e-02,
         -2.5298e-01, -1.2688e-02, -9.1088e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7960e-01, -1.2929e-02, -1.9678e-02, -4.9674e-02,
          1.2308e-02, -3.1398e-02],
        [-7.4230e+00,  9.2761e+00,  8.2691e-02,  4.6883e+00,  4.4150e+00,
          1.2211e-01,  2.9895e+00,  3.5408e+00, -6.2114e+00, -3.5923e+00,
         -9.4198e+00, -1.8627e+00,  4.7011e+00, -1.9985e-02, -6.0281e-02,
          5.2032e+00,  7.0797e+00,  8.3206e+00, -2.3235e+00, -3.3515e+00,
         -1.9947e-02,  7.4397e+00],
        [ 7.9326e+00, -1.0706e+01,  4.0837e-01, -9.2089e+00, -5.7725e+00,
          6.6403e-01, -4.8629e+00, -5.8385e+00,  6.6214e+00,  5.0109e+00,
          9.6232e+00,  3.3999e+00, -5.8456e+00, -1.3669e-01, -1.4453e+00,
         -3.3601e+00, -8.4017e+00, -9.5444e+00,  3.9084e+00,  4.3412e+00,
         -1.3169e-01, -9.7519e+00],
        [-2.2583e-01, -1.6545e-01,  1.7507e-03, -1.2931e-02, -1.1900e-01,
          1.1572e-03, -1.6503e-02, -2.2678e-03, -1.8733e-01, -6.9320e-02,
         -2.5297e-01, -1.2687e-02, -9.1093e-03,  1.2293e-02,  1.9856e-02,
         -1.7657e-02, -1.7958e-01, -1.2930e-02, -1.9677e-02, -4.9670e-02,
          1.2308e-02, -3.1399e-02],
        [-2.2585e-01, -1.6547e-01,  1.7509e-03, -1.2931e-02, -1.1901e-01,
          1.1575e-03, -1.6502e-02, -2.2732e-03, -1.8736e-01, -6.9325e-02,
         -2.5298e-01, -1.2688e-02, -9.1089e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7960e-01, -1.2929e-02, -1.9678e-02, -4.9674e-02,
          1.2308e-02, -3.1398e-02],
        [ 6.5891e-01,  3.0868e-01,  8.3566e-03,  3.5747e-02,  2.1159e-01,
          5.3475e-03,  5.7464e-02,  2.6629e-01,  5.5272e-01,  2.6083e-01,
          7.4008e-01,  8.5701e-02,  7.1654e-02,  3.7717e-02,  5.6900e-02,
          8.1005e-02,  2.9146e-01,  3.6840e-02,  1.1119e-01,  2.1066e-01,
          3.7716e-02,  1.3910e-01],
        [-2.2585e-01, -1.6548e-01,  1.7509e-03, -1.2930e-02, -1.1901e-01,
          1.1575e-03, -1.6502e-02, -2.2733e-03, -1.8736e-01, -6.9325e-02,
         -2.5298e-01, -1.2688e-02, -9.1087e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7961e-01, -1.2929e-02, -1.9678e-02, -4.9674e-02,
          1.2307e-02, -3.1398e-02],
        [-7.5648e+00,  9.6323e+00,  3.4567e-02,  4.2023e+00,  4.4635e+00,
         -1.3328e-01,  2.0401e+00,  5.3264e+00, -6.2784e+00, -3.7048e+00,
         -9.3470e+00, -2.0562e+00,  5.8618e+00, -3.3635e-02,  2.0363e-01,
          5.0433e+00,  7.2814e+00,  9.5323e+00, -2.4148e+00, -3.3678e+00,
         -3.5727e-02,  7.7623e+00],
        [-2.2586e-01, -1.6548e-01,  1.7510e-03, -1.2930e-02, -1.1901e-01,
          1.1575e-03, -1.6502e-02, -2.2750e-03, -1.8737e-01, -6.9327e-02,
         -2.5298e-01, -1.2689e-02, -9.1085e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7961e-01, -1.2929e-02, -1.9679e-02, -4.9675e-02,
          1.2307e-02, -3.1398e-02],
        [-2.2585e-01, -1.6548e-01,  1.7510e-03, -1.2930e-02, -1.1901e-01,
          1.1575e-03, -1.6502e-02, -2.2744e-03, -1.8737e-01, -6.9326e-02,
         -2.5298e-01, -1.2689e-02, -9.1087e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7961e-01, -1.2929e-02, -1.9679e-02, -4.9675e-02,
          1.2307e-02, -3.1398e-02],
        [-2.2585e-01, -1.6547e-01,  1.7509e-03, -1.2931e-02, -1.1901e-01,
          1.1574e-03, -1.6502e-02, -2.2722e-03, -1.8735e-01, -6.9324e-02,
         -2.5298e-01, -1.2688e-02, -9.1089e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7960e-01, -1.2929e-02, -1.9678e-02, -4.9673e-02,
          1.2308e-02, -3.1398e-02],
        [ 6.6450e-01,  3.1201e-01,  8.4127e-03,  3.4696e-02,  2.1235e-01,
          5.3877e-03,  5.6376e-02,  2.6891e-01,  5.5678e-01,  2.6166e-01,
          7.4701e-01,  8.5769e-02,  7.1346e-02,  3.8211e-02,  5.7275e-02,
          8.0058e-02,  2.9347e-01,  3.5786e-02,  1.1132e-01,  2.1139e-01,
          3.8209e-02,  1.3913e-01],
        [-2.2586e-01, -1.6548e-01,  1.7510e-03, -1.2930e-02, -1.1901e-01,
          1.1575e-03, -1.6502e-02, -2.2753e-03, -1.8737e-01, -6.9327e-02,
         -2.5298e-01, -1.2689e-02, -9.1085e-03,  1.2292e-02,  1.9855e-02,
         -1.7657e-02, -1.7961e-01, -1.2929e-02, -1.9679e-02, -4.9675e-02,
          1.2307e-02, -3.1398e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.4313,  0.0078, -2.4313,  0.0873, -0.2625, -0.5234, -2.4313, -2.4314,
        -2.4313, -2.4313, -0.4369, -0.2571, -2.4314, -2.4313,  4.5769, -2.4313,
        -0.4006, -2.4313, -2.4313, -2.4313,  4.6668, -2.4313], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0334, -10.4940,   0.0334, -15.8008,  -9.7979,  -6.9452,   0.0334,
           0.0334,   0.0334,   0.0334,  -7.0927,  14.9535,   0.0334,   0.0334,
           4.2936,   0.0334,  -7.0565,   0.0334,   0.0334,   0.0334,   4.7631,
           0.0334]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 1.0813e+01,  2.9656e+00],
        [ 5.5664e-01,  1.1757e+01],
        [ 1.8618e+01,  2.3664e+00],
        [ 1.0637e+01,  1.5273e-02],
        [-1.4941e+01, -2.2181e+00],
        [-1.2849e+00,  1.3570e+00],
        [-1.2800e+00,  1.3116e+00],
        [-1.7122e+00,  6.5637e-01],
        [-2.0284e+00,  2.5259e-01],
        [ 9.3477e+00, -5.2188e-01],
        [-1.9314e+00,  2.3251e-01],
        [-1.8788e+00,  4.0167e-01],
        [-1.0523e+01, -1.6251e+00],
        [-8.3398e+00,  5.3496e+00],
        [-7.4426e+00,  1.0247e+01],
        [-1.2446e+01, -2.8588e+00],
        [-1.4240e+01, -3.1016e+00],
        [-1.0901e+01, -2.0063e+00],
        [-1.0106e+01, -2.4687e+00],
        [ 3.1771e+00,  1.1429e+01],
        [ 1.0907e+01,  2.2476e+00],
        [ 1.1102e+01,  1.0847e+01]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-5.5365,  7.0663,  2.9709, -9.6018, -1.5848, -4.5127, -4.5185, -4.9090,
        -4.9531, -9.4875, -4.9847, -4.9560,  5.4223,  3.0397,  6.7011, -3.5168,
        -2.9913,  3.2472,  0.4201,  7.7652, -4.3018,  6.4326], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 6.1957e-01, -3.4634e+00, -1.7479e+00,  1.1905e+00,  1.2460e-02,
          1.1948e-01,  1.3226e-01,  4.3984e-02, -1.1598e-02,  1.0215e+00,
         -6.1706e-03,  2.4414e-03, -2.5313e+00,  1.1746e+00, -7.8526e-01,
         -9.2734e-03,  2.7572e-03, -2.7562e+00, -1.8971e+00, -2.6434e+00,
         -4.2419e-01,  3.0293e-02],
        [-4.6968e-01, -1.5418e-01, -2.2179e+00, -4.7229e-01, -1.4815e-02,
         -2.6861e-02, -2.4100e-02,  6.5293e-03, -1.5585e-02, -3.8766e-01,
         -1.6068e-02, -1.1227e-02, -1.0214e+00, -1.1716e-01, -1.0616e-01,
         -5.8386e-03, -7.9498e-03, -5.2026e-01, -1.9652e-01, -4.4158e-01,
         -1.2068e+00, -1.2289e+00],
        [-9.0043e-01, -3.1328e-01, -2.4749e+00, -1.0214e+00, -5.5072e-03,
         -2.2407e-02, -1.9453e-02, -7.7448e-03,  4.4606e-03, -9.3610e-01,
          4.1020e-03,  2.5876e-03, -1.2488e+00, -4.6992e-02, -9.2699e-02,
         -2.2626e-03, -3.5666e-03, -6.7176e-01, -2.4800e-01, -8.7567e-01,
         -1.3802e+00, -7.7155e-01],
        [-4.7184e-01, -1.5540e-01, -2.2143e+00, -4.7815e-01, -1.4473e-02,
         -2.7281e-02, -2.4476e-02,  6.5809e-03, -1.5495e-02, -3.9563e-01,
         -1.5975e-02, -1.1107e-02, -1.0396e+00, -1.1822e-01, -1.0605e-01,
         -5.8775e-03, -7.9716e-03, -5.2621e-01, -1.9589e-01, -4.4421e-01,
         -1.2127e+00, -1.2341e+00],
        [-4.8450e-01, -1.6921e-01, -2.2399e+00, -4.9569e-01, -1.4212e-02,
         -2.5472e-02, -2.2971e-02,  5.3544e-03, -1.0943e-02, -4.2767e-01,
         -1.1482e-02, -7.9784e-03, -1.0434e+00, -1.0826e-01, -9.0719e-02,
         -6.2509e-03, -8.3116e-03, -5.2892e-01, -2.1129e-01, -4.5420e-01,
         -1.2241e+00, -1.0613e+00],
        [ 3.0119e-01,  9.1885e-01, -1.5837e+00, -8.5542e-01, -1.1953e-02,
         -1.9274e+00, -1.8752e+00, -1.4092e+00, -5.5076e-01,  1.0780e+00,
         -4.5548e-01, -9.9188e-01, -3.9286e+00,  1.1415e+00,  2.2999e+00,
         -1.2239e-02, -1.3323e-02, -2.7068e+00, -9.7625e-01,  4.2057e-01,
         -1.0808e+00, -1.2393e+00],
        [ 2.4582e-01, -1.1721e+01,  5.7285e-01, -6.6182e+00,  3.3458e+00,
          1.7362e-01,  1.7999e-01,  2.0080e-01,  2.8786e-01, -4.5146e+00,
          2.9196e-01,  1.9339e-01,  3.6973e+00, -2.2846e+00, -8.2839e+00,
          2.5629e+00,  2.7897e+00,  3.3397e+00,  3.6998e+00, -1.2216e+01,
          1.0269e-01, -7.8724e+00],
        [-2.7710e+00,  3.8384e+00, -3.8614e+00, -5.2497e+00,  1.4955e-01,
         -5.1488e-01, -5.0931e-01, -4.0831e-01, -1.3933e-01, -4.9036e+00,
         -1.0681e-01, -2.8754e-01,  2.6586e+00,  3.1018e+00,  6.0454e+00,
          5.3721e-01,  5.1709e-01,  2.4000e+00,  1.7467e+00,  2.8980e+00,
         -1.8615e+00,  1.3298e+00],
        [ 5.3996e+00,  1.7695e+01,  2.7980e+00,  6.7002e+00, -6.8933e+00,
          1.9716e-02,  1.5955e-02,  1.6031e-02, -1.9886e-01,  4.1219e+00,
         -1.8883e-01,  2.3240e-02, -8.0362e+00,  3.0849e+00,  1.4710e+01,
         -3.3033e+00, -3.8057e+00, -7.4740e+00, -6.2744e+00,  1.9113e+01,
          5.0855e+00,  2.1580e+01],
        [-4.6054e-01, -1.5864e-01, -2.1828e+00, -4.7962e-01, -1.3132e-02,
         -2.7434e-02, -2.4527e-02,  7.6509e-03, -1.3661e-02, -3.9854e-01,
         -1.4154e-02, -9.5939e-03, -1.0591e+00, -1.2246e-01, -9.9910e-02,
         -5.2767e-03, -7.3107e-03, -5.5050e-01, -2.1282e-01, -4.5037e-01,
         -1.2105e+00, -1.2384e+00],
        [-9.3210e-01, -5.0637e-01, -2.5650e+00, -1.2297e+00,  5.1007e-03,
         -1.3266e-02, -1.0812e-02, -6.5037e-03, -1.3910e-03, -1.1288e+00,
         -1.7836e-03, -1.9456e-03, -7.9371e-01, -5.5349e-02, -8.6527e-02,
          5.2059e-03,  5.8080e-03, -8.8163e-01, -4.5943e-01, -1.6628e+00,
         -1.1149e+00, -5.6400e-01],
        [-2.8460e+00, -1.8825e+01, -9.7046e+00, -3.0631e+00,  6.2381e+00,
          2.2284e-02,  2.2216e-02,  3.8058e-02,  1.2254e-02, -4.6393e+00,
          2.0356e-03,  4.3669e-02,  7.6060e+00, -3.1393e+00, -1.1899e+01,
          2.9927e+00,  3.5772e+00,  7.5127e+00,  7.0486e+00, -2.2653e+01,
         -1.0357e+01, -2.5603e+01],
        [-4.7448e-01, -1.5413e-01, -2.2021e+00, -4.7761e-01, -1.5017e-02,
         -2.6936e-02, -2.4189e-02,  6.2971e-03, -1.5709e-02, -3.9518e-01,
         -1.6194e-02, -1.1365e-02, -1.0353e+00, -1.1675e-01, -1.0676e-01,
         -6.0131e-03, -8.0879e-03, -5.2030e-01, -1.9318e-01, -4.4328e-01,
         -1.2192e+00, -1.2259e+00],
        [-6.9231e-01,  3.0014e+00,  2.7786e+00, -1.3175e+00,  3.0620e-04,
         -1.3308e-01, -1.3059e-01, -4.6677e-03, -1.5150e-02, -1.3574e+00,
         -1.8928e-02, -2.6791e-03,  2.1880e+00, -4.4040e-01,  8.1953e-01,
         -1.8323e-02, -1.1176e-02,  1.4720e+00,  4.3629e-01,  2.4554e+00,
          8.6002e-02, -2.4341e-01],
        [ 3.0175e+00, -1.5929e-01,  3.1082e+00,  1.6836e+00, -3.7655e+00,
          1.7569e-01,  1.8810e-01,  2.3150e-01, -5.4313e-01,  3.8072e+00,
         -5.3327e-01,  1.6719e-01,  6.0672e-02, -2.5586e+00, -3.6155e+00,
         -4.5190e+00, -4.8662e+00,  1.6404e-01, -1.7814e-01, -2.9936e-01,
          2.7050e+00, -1.8862e+00],
        [-6.1343e-01, -2.0997e-01, -2.3786e+00, -7.6186e-01, -1.9840e-02,
         -3.2273e-02, -3.0458e-02, -4.7454e-03,  5.8161e-03, -6.4208e-01,
          6.0534e-03,  3.5540e-03, -1.0330e+00, -6.3106e-02, -1.0050e-01,
         -7.6629e-03, -1.3412e-02, -4.8643e-01, -1.8407e-01, -5.2743e-01,
         -1.3866e+00, -9.2565e-01],
        [ 2.3232e-01,  3.1081e+00,  2.1861e+00,  1.7924e+00,  8.3474e-01,
         -4.2057e-01, -3.7750e-01, -8.6959e-02, -6.3586e-02,  1.4179e+00,
         -6.1088e-02, -4.4329e-02,  1.9436e-01, -4.4091e-01,  1.1123e+00,
          4.9760e-01,  7.0823e-01, -1.6870e-01, -3.9429e-01, -1.1351e+00,
          1.7979e-02, -1.2019e+00],
        [-9.3911e-03, -3.8280e+00, -3.1866e-01, -2.0025e+00, -3.0075e-02,
          4.3973e-02,  5.3990e-02,  1.0353e-01,  1.4874e-01, -1.8588e+00,
          1.5683e-01,  1.3914e-01, -1.0725e-01, -2.1520e-01, -2.7783e-01,
          5.4541e-02,  7.3731e-03, -1.3880e+00, -8.0744e-01, -5.4071e+00,
         -9.3933e-01,  6.6525e-02],
        [-2.0923e+00,  1.6783e+00,  2.1031e+00, -6.6800e+00,  1.9109e-04,
         -8.2756e-01, -7.5222e-01, -4.3503e-01, -1.2713e-01, -3.5556e+00,
         -7.5366e-02, -3.3382e-01,  9.4887e+00, -1.2212e+00,  5.7869e+00,
         -6.4339e-04, -3.5999e-03,  9.1765e+00,  6.6382e+00,  6.8362e-01,
          2.7993e-01, -2.2562e+00],
        [-5.5183e+00, -1.5095e+01, -2.8824e-01, -4.9346e+00,  5.3321e+00,
          1.5448e-01,  1.5735e-01,  1.6104e-01,  5.9928e-01, -2.7075e+00,
          5.5855e-01,  1.3923e-01,  3.0535e+00,  8.4196e-01, -6.5453e+00,
          4.6053e+00,  5.0922e+00,  3.6158e+00,  3.8865e+00, -1.4997e+01,
         -2.8857e+00, -4.2056e+00],
        [ 4.0156e-01, -1.0952e+00, -1.5738e+00, -4.3123e-01, -8.5229e-03,
         -6.6402e-01, -6.0136e-01, -2.4497e-01, -5.3144e-02, -2.8255e-01,
         -5.5575e-02, -9.1214e-02, -2.1121e+00,  1.5978e+00,  1.4435e+00,
         -1.0813e-03, -1.8711e-03, -2.1573e+00, -8.6345e-01, -1.7321e+00,
         -7.5512e-01,  5.2300e-01],
        [-1.9858e+00,  2.9033e+00, -2.5269e+00, -2.1485e+00,  2.2962e+00,
         -1.4441e+00, -1.3739e+00, -7.3427e-01,  6.2891e-02,  2.1944e-01,
          6.1358e-02, -2.6440e-01,  7.4516e-02,  1.7016e+00,  3.0581e+00,
          1.5196e+00,  1.8766e+00,  6.5632e-01,  1.1298e+00,  1.3664e+00,
         -2.7325e+00, -7.5516e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.8466, -2.2256, -2.5068, -2.1943, -2.2514, -1.8834,  0.6255, -4.9456,
        -2.2024, -2.2158, -2.5964,  0.0491, -2.2135,  3.0825,  3.0734, -2.4047,
         2.5296, -1.0040,  2.4991, -0.5295, -2.0766, -2.4070], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.5234e-02,  7.2296e-05, -1.5502e-04,  5.9106e-05,  9.2680e-05,
         -2.2228e-02, -7.7352e-03, -1.5990e-01, -1.5103e+00,  6.1218e-05,
         -1.5306e-04, -1.3007e-05,  6.3929e-05, -1.5060e+00, -1.5008e+00,
         -2.1007e-04, -1.4999e+00, -5.4381e-03, -6.9054e-01, -3.7369e-03,
         -4.7102e-03, -3.5212e-02],
        [-8.7292e-03, -1.4222e-03,  7.6021e-04, -1.3898e-03, -1.5771e-03,
         -1.3307e-02, -5.1882e-03, -1.0878e-01, -8.2132e-01, -1.3260e-03,
          8.8522e-04, -2.6449e-04, -1.4244e-03, -8.2289e-01, -8.1366e-01,
          5.1250e-04, -8.1735e-01,  1.2221e-03, -3.1415e-01, -6.8756e-03,
          4.4122e-03, -3.5269e-02],
        [-2.1229e-02, -5.1093e-04, -3.4543e-04, -5.1941e-04, -5.4632e-04,
         -1.8385e-02, -9.4420e-03, -1.3731e-01, -1.4218e+00, -5.2075e-04,
         -3.2211e-04,  2.3184e-05, -5.1689e-04, -1.4173e+00, -1.4127e+00,
         -3.8173e-04, -1.4125e+00, -6.1909e-03, -6.2055e-01, -3.7929e-03,
         -7.1839e-03, -2.7972e-02],
        [-9.4172e-03, -2.1938e-04, -2.2482e-04, -2.0759e-04, -2.8288e-04,
         -1.6622e-02, -7.4112e-03, -1.3137e-01, -1.2415e+00, -2.0252e-04,
         -2.1593e-04, -7.4338e-05, -2.1553e-04, -1.2385e+00, -1.2343e+00,
         -1.4845e-04, -1.2328e+00, -5.0361e-03, -5.7108e-01, -3.7365e-03,
         -7.0540e-03, -2.5170e-02],
        [-1.0969e+00,  7.0714e-02,  2.8031e-01,  4.2693e-02,  2.8583e-01,
          5.4877e-01,  3.6379e+00, -1.3225e+00, -3.0535e+00,  2.8854e-01,
          9.2331e-01, -1.4907e+01,  2.7428e-02,  3.6135e+00,  3.4661e+00,
         -2.3553e-02,  3.3911e+00,  1.2464e+00,  2.0600e+00,  5.6362e+00,
         -4.8358e-01,  4.4222e-01],
        [-1.1325e-02, -1.4138e-04,  1.4500e-04, -1.5008e-04, -1.0196e-03,
         -8.7395e-03, -5.0212e-04, -1.4891e-01, -9.6424e-01, -1.5847e-04,
          8.4481e-05, -6.7668e-04, -1.4540e-04, -9.6261e-01, -9.4804e-01,
         -9.4017e-05, -9.5700e-01,  4.4294e-03, -4.5741e-01, -5.5136e-03,
          2.2288e-03, -3.6784e-02],
        [-2.4271e-02, -7.7962e-04, -2.2040e-04, -7.9007e-04, -5.4691e-04,
         -1.4880e-02, -9.2166e-03, -1.1032e-01, -1.2944e+00, -7.8678e-04,
         -1.4609e-04, -3.3668e-05, -7.9327e-04, -1.2897e+00, -1.2879e+00,
         -3.7474e-04, -1.2861e+00, -9.7262e-03, -5.3746e-01, -3.8536e-03,
         -1.0548e-02, -2.4508e-02],
        [ 3.9237e+00, -6.2937e-02,  8.2976e-01, -8.9553e-02,  1.8773e-01,
          2.5280e+00,  4.4063e+00, -5.1176e-01,  2.7121e+00,  1.5038e-01,
          2.1898e+00, -6.3975e+00, -1.0291e-01,  9.8757e-01,  1.1535e+00,
          5.7770e-02,  9.4780e-01,  2.9512e+00,  2.3504e+00,  1.6411e+00,
          3.1498e+00,  1.2727e-01],
        [-1.4494e-02, -1.9430e-04, -1.7724e-04, -2.0127e-04, -2.0117e-04,
         -1.5493e-02, -8.1680e-03, -1.3258e-01, -1.3345e+00, -2.0053e-04,
         -1.8098e-04, -1.8463e-05, -1.9899e-04, -1.3303e+00, -1.3262e+00,
         -2.7675e-04, -1.3255e+00, -6.2074e-03, -5.8710e-01, -4.1919e-03,
         -7.5895e-03, -2.9301e-02],
        [-2.1892e-02, -3.5780e-04, -4.8405e-04, -3.6317e-04, -3.6356e-04,
         -2.3657e-02, -1.6951e-02, -9.6588e-02, -1.1849e+00, -3.5150e-04,
         -4.0838e-04, -1.7243e-05, -3.6464e-04, -1.1843e+00, -1.1795e+00,
         -3.3804e-04, -1.1790e+00, -7.4055e-03, -5.7283e-01, -5.4422e-03,
         -1.0076e-02, -3.1398e-02],
        [-2.3028e+00, -2.1890e-02, -8.3502e-03, -5.3786e-02,  9.5685e-02,
         -8.9290e-01, -1.4767e+01,  7.5055e+00,  3.0752e+00,  1.1812e-01,
         -7.0438e-02,  2.1906e-03, -5.7019e-02, -7.6423e-01, -1.3833e+00,
         -7.3209e-03, -7.3795e-01, -2.2229e+00,  1.8670e+00, -1.4910e+01,
         -1.6032e+00,  1.4609e+00],
        [ 2.4319e+00,  1.1747e-01, -1.6596e-01,  8.9778e-02,  3.1240e-01,
         -1.2941e+00, -9.2455e-01, -5.3120e-01,  5.2163e+00,  3.3676e-01,
         -2.0670e-01,  2.2187e+01,  7.3275e-02,  3.7788e-01,  7.5700e-01,
         -1.2275e-01,  4.8364e-01, -2.5823e+00,  1.5965e+00, -2.2668e+00,
         -6.5383e-01, -2.8611e+00],
        [-2.1115e-02, -3.0744e-04, -1.4460e-04, -2.9289e-04, -3.5131e-04,
         -2.2539e-02, -1.0971e-02, -1.5511e-01, -1.5086e+00, -2.8377e-04,
         -1.2594e-04,  2.8039e-05, -3.0274e-04, -1.5044e+00, -1.4994e+00,
         -3.7557e-04, -1.4986e+00, -6.5951e-03, -6.8458e-01, -3.5928e-03,
         -7.8039e-03, -3.0722e-02],
        [-1.4210e-02,  2.7210e-04, -1.6876e-04,  2.6752e-04,  2.9104e-04,
         -2.2712e-02, -5.7647e-03, -1.5704e-01, -1.4740e+00,  2.7486e-04,
         -1.6774e-04, -4.4364e-05,  2.6776e-04, -1.4699e+00, -1.4648e+00,
         -1.4749e-04, -1.4640e+00, -5.0059e-03, -6.6920e-01, -3.9010e-03,
         -5.3125e-03, -3.3932e-02]], device='cuda:0'))])
xi:  [855.6613]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1346.528421096153
W_T_median: 1269.666313381941
W_T_pctile_5: 857.0691692284117
W_T_CVAR_5_pct: 549.11237330707
Average q (qsum/M+1):  66.92983713457662
Optimal xi:  [855.6613]
Expected(across Rb) median(across samples) p_equity:  1.293834924354087e-06
obj fun:  tensor(-2623.7464, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/factor_decumulation/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/factor_decumulation/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: MC_everything
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
