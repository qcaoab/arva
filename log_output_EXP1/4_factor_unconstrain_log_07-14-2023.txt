/home/marcchen/Documents/constrain_factor/researchcode/exp_config_json_files/multi_portfolio_exp1.json
Starting at: 
14-07-23_17:46

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor4
timeseries_basket['basket_desc'] = Factor4 portfolio for paper: Basic, size, value, vol, mom
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret', 'Vol_Lo20_real_ret', 'Mom_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor4
timeseries_basket['basket_desc'] = Factor4 portfolio for paper: Basic, size, value, vol, mom
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret', 'Vol_Lo20_nom_ret', 'Mom_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.005383     0.031411
192608                    0.0319              0.0561  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0355  ...     0.005323    -0.028996
192611                   -0.0038              0.0294  ...     0.005303     0.028554

[5 rows x 8 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.043289    -0.036240
202209                   -0.0955             -0.0871  ...    -0.050056    -0.091324
202210                    0.0883              0.1486  ...    -0.014968     0.077403
202211                   -0.0076              0.0462  ...     0.040789     0.052365
202212                   -0.0457             -0.0499  ...    -0.018566    -0.057116

[5 rows x 8 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'Mom_Hi30_nom_ret_ind', 'Vol_Lo20_nom_ret_ind', 'CPI_nom_ret_ind',
       'T30_nom_ret_ind', 'B10_nom_ret_ind', 'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
Vol_Lo20_real_ret      0.003529
Mom_Hi30_real_ret      0.011386
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
Vol_Lo20_real_ret      0.030737
Mom_Hi30_real_ret      0.061421
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Mom_Hi30_real_ret
T30_real_ret             1.000000  ...           0.055142
B10_real_ret             0.351722  ...           0.066570
VWD_real_ret             0.068448  ...           0.936115
Size_Lo30_real_ret       0.014412  ...           0.903222
Value_Hi30_real_ret      0.018239  ...           0.869469
Vol_Lo20_real_ret        0.081282  ...           0.482682
Mom_Hi30_real_ret        0.055142  ...           1.000000

[7 rows x 7 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      15  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      15  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 15)     True          15  
2     (15, 15)     True          15  
3      (15, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       7       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      15  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      15  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       7           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 15)     True          15  
2     (15, 15)     True          15  
3      (15, 7)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       15  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       15  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 15)      True          15  
0     (15, 15)      True          15  
0      (15, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       15  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       15  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        7           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 15)      True          15  
0     (15, 15)      True          15  
0      (15, 7)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 1500, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       15  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       15  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 15)      True          15  
0     (15, 15)      True          15  
0      (15, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       15  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       15  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        7           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 15)      True          15  
0     (15, 15)      True          15  
0      (15, 7)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0, 0.5, 0.1, 0.1, 0.1, 0.1, 0.1]
W_T_mean: 1080.663625928201
W_T_median: 808.5408506516229
W_T_pctile_5: -163.24259466603715
W_T_CVAR_5_pct: -300.4117296963052
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1824.178049245032
Current xi:  [121.81486]
objective value function right now is: -1824.178049245032
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1848.6899548026047
Current xi:  [145.83229]
objective value function right now is: -1848.6899548026047
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1861.1706445669959
Current xi:  [169.44397]
objective value function right now is: -1861.1706445669959
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [192.97778]
objective value function right now is: -1850.444822136723
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1885.4035296275401
Current xi:  [216.08452]
objective value function right now is: -1885.4035296275401
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1911.9781666420934
Current xi:  [239.2938]
objective value function right now is: -1911.9781666420934
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1928.6464238575188
Current xi:  [262.24997]
objective value function right now is: -1928.6464238575188
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1936.574750085036
Current xi:  [285.40378]
objective value function right now is: -1936.574750085036
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1951.2958875297118
Current xi:  [308.03568]
objective value function right now is: -1951.2958875297118
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1968.0264091164515
Current xi:  [330.5468]
objective value function right now is: -1968.0264091164515
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1980.2606946175815
Current xi:  [353.29416]
objective value function right now is: -1980.2606946175815
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1995.5160686711363
Current xi:  [375.85034]
objective value function right now is: -1995.5160686711363
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -2006.055299502499
Current xi:  [397.97144]
objective value function right now is: -2006.055299502499
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -2017.7310258087618
Current xi:  [420.31534]
objective value function right now is: -2017.7310258087618
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -2030.6669635001576
Current xi:  [442.73367]
objective value function right now is: -2030.6669635001576
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -2043.4853989713788
Current xi:  [465.21933]
objective value function right now is: -2043.4853989713788
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -2058.541127507683
Current xi:  [489.42096]
objective value function right now is: -2058.541127507683
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -2073.6748103941045
Current xi:  [510.61258]
objective value function right now is: -2073.6748103941045
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -2087.0716411142907
Current xi:  [534.26385]
objective value function right now is: -2087.0716411142907
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2098.378872371383
Current xi:  [557.519]
objective value function right now is: -2098.378872371383
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -2111.1493830396175
Current xi:  [580.0006]
objective value function right now is: -2111.1493830396175
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -2120.2197580245474
Current xi:  [602.26654]
objective value function right now is: -2120.2197580245474
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2127.962326502407
Current xi:  [624.24384]
objective value function right now is: -2127.962326502407
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -2138.5032362770644
Current xi:  [646.33704]
objective value function right now is: -2138.5032362770644
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -2149.5224241393685
Current xi:  [668.1947]
objective value function right now is: -2149.5224241393685
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -2158.4305312016327
Current xi:  [690.3915]
objective value function right now is: -2158.4305312016327
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -2167.840057808131
Current xi:  [712.1697]
objective value function right now is: -2167.840057808131
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -2176.9805647357052
Current xi:  [733.2296]
objective value function right now is: -2176.9805647357052
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -2187.0043139422432
Current xi:  [754.8416]
objective value function right now is: -2187.0043139422432
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -2188.459082688884
Current xi:  [776.02527]
objective value function right now is: -2188.459082688884
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -2195.5345900996263
Current xi:  [796.90857]
objective value function right now is: -2195.5345900996263
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -2208.2297735833463
Current xi:  [818.5499]
objective value function right now is: -2208.2297735833463
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -2214.178704309808
Current xi:  [839.39044]
objective value function right now is: -2214.178704309808
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -2217.5386622278907
Current xi:  [860.04565]
objective value function right now is: -2217.5386622278907
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -2226.20777130755
Current xi:  [880.3132]
objective value function right now is: -2226.20777130755
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2233.122135761129
Current xi:  [884.3719]
objective value function right now is: -2233.122135761129
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -2234.037368916529
Current xi:  [888.7216]
objective value function right now is: -2234.037368916529
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -2235.93524618769
Current xi:  [893.1008]
objective value function right now is: -2235.93524618769
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2236.965483867197
Current xi:  [897.4603]
objective value function right now is: -2236.965483867197
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -2238.0419412309743
Current xi:  [901.7157]
objective value function right now is: -2238.0419412309743
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [906.0505]
objective value function right now is: -2238.012906336381
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -2241.1161146330896
Current xi:  [910.3602]
objective value function right now is: -2241.1161146330896
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -2242.094268724087
Current xi:  [914.69324]
objective value function right now is: -2242.094268724087
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -2243.374946559068
Current xi:  [918.99225]
objective value function right now is: -2243.374946559068
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [923.34766]
objective value function right now is: -2242.8141872630595
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -2243.9096426611663
Current xi:  [927.5372]
objective value function right now is: -2243.9096426611663
94.0% of gradient descent iterations done. Method = Adam
new min fval:  -2245.5737825067
Current xi:  [931.77277]
objective value function right now is: -2245.5737825067
new min fval from sgd:  -2247.184238286027
new min fval from sgd:  -2247.222127428255
new min fval from sgd:  -2247.275780757435
new min fval from sgd:  -2247.3228897293775
new min fval from sgd:  -2247.348425220102
new min fval from sgd:  -2247.3883983753435
new min fval from sgd:  -2247.4421545523323
new min fval from sgd:  -2247.486394987757
new min fval from sgd:  -2247.5220766113657
new min fval from sgd:  -2247.554058727067
new min fval from sgd:  -2247.6030290821122
new min fval from sgd:  -2247.6322860735563
new min fval from sgd:  -2247.6476001221786
new min fval from sgd:  -2247.648699801798
new min fval from sgd:  -2247.683055904379
new min fval from sgd:  -2247.7925610018833
new min fval from sgd:  -2247.808539783993
new min fval from sgd:  -2247.8143436367372
new min fval from sgd:  -2247.8417802995937
new min fval from sgd:  -2247.879229564937
new min fval from sgd:  -2247.929671457846
new min fval from sgd:  -2247.9591792923666
new min fval from sgd:  -2247.9690517334743
new min fval from sgd:  -2248.01076290551
new min fval from sgd:  -2248.059993837367
new min fval from sgd:  -2248.0945906254606
new min fval from sgd:  -2248.095317443214
new min fval from sgd:  -2248.112313472745
new min fval from sgd:  -2248.115327997467
new min fval from sgd:  -2248.144254393134
new min fval from sgd:  -2248.144878004059
new min fval from sgd:  -2248.1501464067196
new min fval from sgd:  -2248.2025009339136
new min fval from sgd:  -2248.292099748171
new min fval from sgd:  -2248.305666273611
new min fval from sgd:  -2248.312503347911
new min fval from sgd:  -2248.314831678448
new min fval from sgd:  -2248.326251375134
new min fval from sgd:  -2248.336571981325
new min fval from sgd:  -2248.3468546302124
new min fval from sgd:  -2248.3494700670876
new min fval from sgd:  -2248.363401535708
new min fval from sgd:  -2248.374274515625
new min fval from sgd:  -2248.3849896335114
new min fval from sgd:  -2248.4088275653135
new min fval from sgd:  -2248.4195627267363
new min fval from sgd:  -2248.43273621407
new min fval from sgd:  -2248.4451913523158
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [936.1053]
objective value function right now is: -2247.4967676580086
new min fval from sgd:  -2248.451913785111
new min fval from sgd:  -2248.5179228543625
new min fval from sgd:  -2248.6013596547073
new min fval from sgd:  -2248.665123386925
new min fval from sgd:  -2248.715237195914
new min fval from sgd:  -2248.7365641746574
new min fval from sgd:  -2248.754037847736
new min fval from sgd:  -2248.846723470177
new min fval from sgd:  -2248.938478326086
new min fval from sgd:  -2249.0024184272497
new min fval from sgd:  -2249.0405813937896
new min fval from sgd:  -2249.087738509479
new min fval from sgd:  -2249.1157363614243
new min fval from sgd:  -2249.125635153008
new min fval from sgd:  -2249.148286802853
new min fval from sgd:  -2249.162356428767
new min fval from sgd:  -2249.1862473078504
new min fval from sgd:  -2249.1892473603966
new min fval from sgd:  -2249.191128481283
new min fval from sgd:  -2249.1976909277846
new min fval from sgd:  -2249.205202920078
new min fval from sgd:  -2249.213833350231
new min fval from sgd:  -2249.221315349614
new min fval from sgd:  -2249.2332894192236
new min fval from sgd:  -2249.24750955885
new min fval from sgd:  -2249.260199575239
new min fval from sgd:  -2249.271437807734
new min fval from sgd:  -2249.2823151046
new min fval from sgd:  -2249.290703823719
new min fval from sgd:  -2249.2950421443497
new min fval from sgd:  -2249.297631550974
new min fval from sgd:  -2249.2982872099224
new min fval from sgd:  -2249.303302146816
new min fval from sgd:  -2249.3066861781817
new min fval from sgd:  -2249.312863584316
new min fval from sgd:  -2249.315659177167
new min fval from sgd:  -2249.323673175416
new min fval from sgd:  -2249.3313213297097
new min fval from sgd:  -2249.3357606297795
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [938.62476]
objective value function right now is: -2249.335170289391
new min fval from sgd:  -2249.342761073787
new min fval from sgd:  -2249.3486561874615
new min fval from sgd:  -2249.3532907669814
new min fval from sgd:  -2249.356989494713
new min fval from sgd:  -2249.3584330242597
new min fval from sgd:  -2249.358676929991
new min fval from sgd:  -2249.371400714622
new min fval from sgd:  -2249.375733615671
new min fval from sgd:  -2249.3772214745172
new min fval from sgd:  -2249.3782911321846
new min fval from sgd:  -2249.3823386404792
new min fval from sgd:  -2249.385084781596
new min fval from sgd:  -2249.3904634085243
new min fval from sgd:  -2249.3947321089418
new min fval from sgd:  -2249.4034056597984
new min fval from sgd:  -2249.41210275556
new min fval from sgd:  -2249.417465705066
new min fval from sgd:  -2249.4213958803093
new min fval from sgd:  -2249.4245082574416
new min fval from sgd:  -2249.428677155633
new min fval from sgd:  -2249.432427441917
new min fval from sgd:  -2249.4384104553437
new min fval from sgd:  -2249.4414449679125
new min fval from sgd:  -2249.4434868026124
new min fval from sgd:  -2249.447103869762
new min fval from sgd:  -2249.452272689609
new min fval from sgd:  -2249.456140159314
new min fval from sgd:  -2249.4573506132524
new min fval from sgd:  -2249.4614661344967
new min fval from sgd:  -2249.4653794278497
new min fval from sgd:  -2249.466898857669
new min fval from sgd:  -2249.4695343975286
new min fval from sgd:  -2249.4736565060866
new min fval from sgd:  -2249.4796912623524
new min fval from sgd:  -2249.485222994876
new min fval from sgd:  -2249.491478041934
new min fval from sgd:  -2249.49218692643
new min fval from sgd:  -2249.494219407187
new min fval from sgd:  -2249.4961907653374
new min fval from sgd:  -2249.497958304843
new min fval from sgd:  -2249.4987855314484
new min fval from sgd:  -2249.499765898051
new min fval from sgd:  -2249.5055116265535
new min fval from sgd:  -2249.5098806785254
new min fval from sgd:  -2249.513801395772
new min fval from sgd:  -2249.5184451036257
new min fval from sgd:  -2249.5231817184963
new min fval from sgd:  -2249.5268313037827
new min fval from sgd:  -2249.5297771428964
new min fval from sgd:  -2249.5353221366618
new min fval from sgd:  -2249.5409463311184
new min fval from sgd:  -2249.5463665878206
new min fval from sgd:  -2249.5495137645808
new min fval from sgd:  -2249.549969685573
new min fval from sgd:  -2249.5501164270554
new min fval from sgd:  -2249.5511800580184
new min fval from sgd:  -2249.5597869599032
new min fval from sgd:  -2249.5642235588934
new min fval from sgd:  -2249.5661842173936
new min fval from sgd:  -2249.568935665804
new min fval from sgd:  -2249.572491257873
new min fval from sgd:  -2249.5763613637464
new min fval from sgd:  -2249.5796253917283
new min fval from sgd:  -2249.5820575247008
new min fval from sgd:  -2249.584596900582
new min fval from sgd:  -2249.5904499520484
new min fval from sgd:  -2249.5998511316498
new min fval from sgd:  -2249.6038254917025
new min fval from sgd:  -2249.6041053413505
new min fval from sgd:  -2249.6041940458363
new min fval from sgd:  -2249.6049506418735
new min fval from sgd:  -2249.6050865530497
new min fval from sgd:  -2249.6071368911125
new min fval from sgd:  -2249.6196440646536
new min fval from sgd:  -2249.631557804298
new min fval from sgd:  -2249.6370264805782
new min fval from sgd:  -2249.6424462231653
new min fval from sgd:  -2249.649675225149
new min fval from sgd:  -2249.6523886625237
new min fval from sgd:  -2249.6544241291376
new min fval from sgd:  -2249.6599891334495
new min fval from sgd:  -2249.6639626262768
new min fval from sgd:  -2249.665552669865
new min fval from sgd:  -2249.667817399986
new min fval from sgd:  -2249.6702384369623
new min fval from sgd:  -2249.6767703038545
new min fval from sgd:  -2249.684396879054
new min fval from sgd:  -2249.6940361183556
new min fval from sgd:  -2249.699489991213
new min fval from sgd:  -2249.7029120999746
new min fval from sgd:  -2249.7037831130783
new min fval from sgd:  -2249.7045934176704
new min fval from sgd:  -2249.707330430984
new min fval from sgd:  -2249.7081688936337
new min fval from sgd:  -2249.7102750323265
new min fval from sgd:  -2249.7118795892
new min fval from sgd:  -2249.7123140901444
new min fval from sgd:  -2249.7132901931022
new min fval from sgd:  -2249.7141934747856
new min fval from sgd:  -2249.716132585285
new min fval from sgd:  -2249.7198579613255
new min fval from sgd:  -2249.7228027427927
new min fval from sgd:  -2249.729905943512
new min fval from sgd:  -2249.735967041729
new min fval from sgd:  -2249.7417786886685
new min fval from sgd:  -2249.746059679872
new min fval from sgd:  -2249.748637043532
new min fval from sgd:  -2249.7515027504614
new min fval from sgd:  -2249.753073120985
new min fval from sgd:  -2249.754186167639
new min fval from sgd:  -2249.75723530972
new min fval from sgd:  -2249.7606557678832
new min fval from sgd:  -2249.761920286847
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [939.4978]
objective value function right now is: -2249.6445427562735
min fval:  -2249.761920286847
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  7.5853,  -2.4158],
        [-11.2703,   5.0152],
        [  9.9490,  -0.7813],
        [ -5.2532,  10.7978],
        [ -0.9049,  -0.3972],
        [  9.7710,  -0.9981],
        [ -1.6154,   0.2263],
        [ -2.4890,  11.9357],
        [-15.4529,  -9.2488],
        [ 10.0907,  -0.7558],
        [-11.0426,   5.0470],
        [ 10.2122,  -0.8103],
        [ -0.9049,  -0.3972],
        [ -0.9467,  13.8227],
        [  7.7564,  -2.1683]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-8.6100,  0.4023, -8.7903, -0.0091, -3.3355, -8.7333, -3.2129,  0.8343,
         5.1041, -8.8837,  0.1267, -8.8976, -3.3355,  1.5231, -8.7892],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 4.5794e-01, -4.5401e+00,  4.6484e+00, -2.3711e+00, -1.4731e-01,
          3.4503e+00, -1.1651e-01, -1.7946e+00,  4.7787e+00,  5.0336e+00,
         -4.2806e+00,  5.5352e+00, -1.4744e-01, -1.5902e+00,  5.4051e-01],
        [-5.9536e-02, -2.6107e-02, -5.3493e-02, -7.9777e-01, -4.1391e-02,
         -2.0935e-02, -1.0359e-02, -5.3479e-01, -5.2845e-01, -5.8833e-02,
         -6.5195e-03, -5.3934e-02, -4.1392e-02, -5.4903e-01, -4.9991e-02],
        [-6.3178e-02, -2.3072e-02, -2.2490e-02, -7.9225e-01, -4.7377e-02,
          8.9707e-04, -1.3464e-02, -4.9145e-01, -2.8756e-01, -2.4828e-02,
         -6.0819e-03, -1.9377e-02, -4.7378e-02, -4.7941e-01, -5.2529e-02],
        [-1.6246e-01,  1.5479e+00, -1.9365e-01, -3.2436e+00,  3.5504e-02,
         -1.9659e-01,  1.3514e-01, -4.0840e+00,  5.4427e+00, -1.8758e-01,
          1.7880e+00, -2.0151e-01,  3.5520e-02, -4.7788e+00, -1.4841e-01],
        [-5.4629e-02,  4.3674e-02,  5.6218e-01, -1.1173e+00, -5.6825e-02,
          3.6666e-01, -2.2125e-02, -7.9788e-01, -7.2299e-02,  6.5892e-01,
          4.9833e-02,  7.1048e-01, -5.6825e-02, -5.6026e-01, -4.2799e-02],
        [ 8.4784e-01, -4.7861e+00,  6.1889e+00, -2.8612e+00, -6.1410e-02,
          4.3638e+00, -2.0339e-01, -2.0713e+00,  6.8916e+00,  6.6928e+00,
         -4.4884e+00,  7.4215e+00, -6.1464e-02, -1.8598e+00,  1.2486e+00],
        [-1.9504e-01,  1.0148e+00, -4.1804e-01, -4.1012e+00,  3.6645e-02,
         -4.2285e-01,  2.1457e-01, -4.9066e+00,  6.9624e+00, -4.0721e-01,
          1.3263e+00, -4.3230e-01,  3.6709e-02, -5.7814e+00, -1.7827e-01],
        [ 1.8183e-01,  6.9756e-02,  4.0797e+00, -2.0958e+00, -1.4983e-02,
          3.2340e+00, -1.2090e-02, -1.8987e+00, -6.4879e-03,  4.5711e+00,
          7.4153e-02,  4.6731e+00, -1.4983e-02, -1.3424e+00,  1.8142e-01],
        [-5.9514e-02, -2.6169e-02, -5.3407e-02, -7.9729e-01, -4.1014e-02,
         -2.1056e-02, -1.0200e-02, -5.3169e-01, -5.2481e-01, -5.8707e-02,
         -6.6648e-03, -5.3814e-02, -4.1016e-02, -5.4455e-01, -4.9991e-02],
        [ 2.0563e-01,  2.6594e+00,  3.2773e-01,  3.5166e+00,  5.6551e-02,
          3.7418e-01, -6.3300e-05,  3.8221e+00, -7.8716e+00,  2.9208e-01,
          2.3704e+00,  2.8500e-01,  5.6749e-02,  4.5574e+00,  1.8497e-01],
        [ 1.3958e-01,  4.2657e+00, -1.6442e+00,  3.3046e+00, -5.3697e-02,
         -1.3324e+00, -6.0006e-02,  3.0370e+00, -1.0699e+01, -1.9740e+00,
          3.9441e+00, -1.9610e+00, -5.4075e-02,  4.1353e+00,  9.6603e-02],
        [ 5.0471e+00, -1.6574e+00,  1.0791e+01, -5.9458e+00,  1.4029e-01,
          9.6805e+00,  2.4305e-02, -5.3917e+00, -1.6123e+00,  1.1477e+01,
         -1.5293e+00,  1.1719e+01,  1.4032e-01, -4.0970e+00,  4.3942e+00],
        [ 9.5327e-01, -4.7789e+00,  6.3886e+00, -2.7667e+00, -3.1029e-02,
          4.5855e+00, -2.2168e-01, -1.9909e+00,  7.1586e+00,  6.9213e+00,
         -4.4812e+00,  7.5774e+00, -3.1035e-02, -1.8317e+00,  1.3877e+00],
        [-6.2744e-02, -2.1177e-02, -3.6872e-02, -7.8205e-01, -4.3049e-02,
         -1.0704e-02, -1.1199e-02, -5.1509e-01, -3.7102e-01, -4.0443e-02,
         -5.0260e-03, -3.5324e-02, -4.3050e-02, -5.4398e-01, -5.2808e-02],
        [ 1.3272e-01,  4.3319e+00, -1.2802e+00,  3.5993e+00, -2.7607e-02,
         -1.1133e+00,  1.4604e-02,  3.2222e+00, -1.0221e+01, -1.7027e+00,
          4.1238e+00, -1.6542e+00, -2.7400e-02,  4.3304e+00,  1.1722e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 0.1036, -1.2523, -2.0086, -1.7029, -3.3248, -1.3729, -1.4975, -5.7875,
        -1.2657,  0.5232,  0.5890, -4.2458, -1.7312, -1.5660,  0.1889],
       device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -3.3399,  -0.1084,  -0.2994,  -4.4105,  -1.0311,  -5.1279,  -5.8858,
          -2.8673,  -0.1087,   3.6576,   7.7776, -13.0918,  -5.1789,  -0.1642,
           7.0091]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.1710,   0.2032],
        [ -2.1762,   0.1873],
        [ 11.2387,   0.2672],
        [ -0.6025,   4.8412],
        [ -2.1728,   0.2039],
        [-22.3047,   3.6989],
        [ -2.1712,   0.2031],
        [ -2.1711,   0.2032],
        [ -2.1712,   0.2031],
        [ -7.5783,  11.4133],
        [ -2.1712,   0.2031],
        [-14.1485,  -3.6735],
        [  3.9173,  -2.8464],
        [ -8.4588,   2.9460],
        [  9.1526,   4.4924]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -4.7924,  -4.7593, -10.5870,   2.1966,  -4.7844,   1.0276,  -4.7917,
         -4.7897,  -4.7917,   1.6115,  -4.7917,   1.3229, -11.5095,   2.4089,
         -2.0863], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 4.8220e-02,  5.0614e-02, -5.7704e-01, -1.2763e-01,  4.8547e-02,
         -8.7817e-01,  4.8245e-02,  4.8302e-02,  4.8247e-02,  1.8107e+00,
          4.8245e-02, -5.3373e-01, -8.2827e-01,  2.6728e-01, -1.8613e+00],
        [ 4.9516e-02,  5.2076e-02, -5.7830e-01, -1.2936e-01,  4.9859e-02,
         -8.8747e-01,  4.9542e-02,  4.9603e-02,  4.9545e-02,  1.8263e+00,
          4.9542e-02, -5.3623e-01, -8.3612e-01,  2.7001e-01, -1.8684e+00],
        [-2.0117e-01, -1.7380e-01, -4.8770e+00, -1.5537e+00, -1.9731e-01,
          5.3964e+00, -2.0055e-01, -1.9889e-01, -2.0049e-01, -1.4269e+01,
         -2.0055e-01,  3.9824e+00,  3.9332e-01, -1.4417e+00, -1.7285e-01],
        [-1.3727e-01, -1.1132e-01,  2.8035e+00, -7.1535e+00, -1.3510e-01,
          1.5234e-02, -1.3661e-01, -1.3567e-01, -1.3655e-01, -1.4952e+00,
         -1.3661e-01, -1.8737e+01,  1.1501e+00, -7.3523e-03,  3.8200e-01],
        [-7.0190e-02, -7.4248e-02,  3.7607e+00,  6.2280e-01, -7.1072e-02,
         -8.6881e-01, -7.0409e-02, -7.0735e-02, -7.0426e-02, -1.6853e+00,
         -7.0409e-02, -2.6154e+00, -1.6065e+00, -1.3981e+00,  2.6575e-01],
        [ 2.1577e-01,  2.3436e-01, -6.7538e-01,  3.7751e+00,  2.1788e-01,
         -6.6052e-02,  2.1563e-01,  2.1594e-01,  2.1562e-01,  8.6462e-01,
          2.1563e-01,  2.2246e+00,  2.8678e+00, -1.3162e+01, -7.9664e-01],
        [ 5.3008e-02,  5.6055e-02, -5.7966e-01, -1.2907e-01,  5.3401e-02,
         -9.0894e-01,  5.3040e-02,  5.3111e-02,  5.3043e-02,  1.8562e+00,
          5.3040e-02, -5.3982e-01, -8.4745e-01,  2.8741e-01, -1.8795e+00],
        [ 3.2342e-01,  2.9995e-01, -8.9214e+00, -6.5263e+00,  3.1484e-01,
          5.5681e+00,  3.2302e-01,  3.2044e-01,  3.2298e-01, -1.3595e+01,
          3.2303e-01,  1.0702e+01, -5.6913e+00, -4.4991e+00, -9.9170e+00],
        [ 4.9491e-02,  5.2049e-02, -5.7828e-01, -1.2933e-01,  4.9834e-02,
         -8.8730e-01,  4.9518e-02,  4.9578e-02,  4.9520e-02,  1.8260e+00,
          4.9518e-02, -5.3618e-01, -8.3598e-01,  2.6995e-01, -1.8683e+00],
        [ 4.8554e-02,  5.0994e-02, -5.7720e-01, -1.2775e-01,  4.8886e-02,
         -8.8057e-01,  4.8580e-02,  4.8638e-02,  4.8582e-02,  1.8144e+00,
          4.8580e-02, -5.3417e-01, -8.2996e-01,  2.6834e-01, -1.8627e+00],
        [ 4.9282e-02,  5.1803e-02, -5.7853e-01, -1.3006e-01,  4.9622e-02,
         -8.8605e-01,  4.9308e-02,  4.9368e-02,  4.9311e-02,  1.8250e+00,
          4.9308e-02, -5.3643e-01, -8.3611e-01,  2.6800e-01, -1.8686e+00],
        [ 4.9312e-02,  5.1853e-02, -5.7780e-01, -1.2842e-01,  4.9654e-02,
         -8.8587e-01,  4.9339e-02,  4.9399e-02,  4.9341e-02,  1.8229e+00,
          4.9339e-02, -5.3541e-01, -8.3401e-01,  2.7056e-01, -1.8663e+00],
        [ 5.2204e-02, -5.9867e-02, -7.0299e+00,  2.9029e+00,  3.4688e-02,
          1.0013e+01,  5.0741e-02,  4.6147e-02,  5.0595e-02,  2.0321e-01,
          5.0744e-02, -4.1707e+00,  1.3132e+00,  2.8252e+00, -1.3940e-01],
        [-5.3612e-01, -5.9654e-01, -6.9262e+00,  1.0360e+00, -5.3167e-01,
          7.3902e+00, -5.3614e-01, -5.3462e-01, -5.3613e-01,  7.6851e+00,
         -5.3614e-01, -2.0049e+00, -6.8949e-02,  2.4033e+00, -2.5729e+00],
        [ 4.9213e-02,  5.1723e-02, -5.7859e-01, -1.3026e-01,  4.9552e-02,
         -8.8563e-01,  4.9239e-02,  4.9299e-02,  4.9242e-02,  1.8246e+00,
          4.9239e-02, -5.3649e-01, -8.3610e-01,  2.6741e-01, -1.8687e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-6.0304, -6.0244, -0.7618, -4.7533, -1.6404, -6.0481, -6.0117, -2.6986,
        -6.0245, -6.0291, -6.0248, -6.0258, -3.4212, -9.3874, -6.0249],
       device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.4111,  -0.4160, -28.7209,  -0.1482, -14.0728,  -1.9424,  -0.4208,
           0.1198,  -0.4159,  -0.4121,  -0.4164,  -0.4145,   3.3763,   4.2996,
          -0.4164],
        [ -0.7252,  -0.7458,   0.9875,   4.7715,   2.0354,  -1.8527,  -0.7575,
          -8.4980,  -0.7455,  -0.7290,  -0.7485,  -0.7386,   1.5057,  -4.6754,
          -0.7492],
        [ -0.2608,  -0.2631, -15.8732,  -0.2194, -24.6945,  -1.1986,  -0.2700,
          -0.3949,  -0.2630,  -0.2615,  -0.2625,  -0.2629, -16.5993,  -0.5841,
          -0.2623],
        [ -0.2654,  -0.2677, -16.0805,  -0.2084, -25.3724,  -1.1812,  -0.2748,
          -0.3785,  -0.2677,  -0.2660,  -0.2671,  -0.2675, -17.1200,  -0.5412,
          -0.2669],
        [ -0.1588,  -0.1614,   0.3450,   0.9318,   0.0870,   0.4675,  -0.1590,
           5.4108,  -0.1614,  -0.1592,  -0.1621,  -0.1603,   0.4061,   3.9730,
          -0.1623],
        [ -0.1760,  -0.1769, -14.8053,  -0.2921, -20.5974,  -0.5081,  -0.1798,
          -0.3778,  -0.1769,  -0.1763,  -0.1766,  -0.1768, -12.8273,  -0.1961,
          -0.1766],
        [  1.4981,   1.5122,   3.4660,  -3.5390,   2.1709,   5.5695,   1.5757,
          11.1673,   1.5119,   1.5027,   1.5057,   1.5124,   1.3572,  -1.7034,
           1.5038]], device='cuda:0'))])
xi:  [939.40314]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1291.6524470159554
W_T_median: 1256.2089968219816
W_T_pctile_5: 943.0086789172855
W_T_CVAR_5_pct: 609.4104655488426
Average q (qsum/M+1):  52.92786432081653
Optimal xi:  [939.40314]
Expected(across Rb) median(across samples) p_equity:  0.17154846308291477
obj fun:  tensor(-2249.7619, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor4
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
