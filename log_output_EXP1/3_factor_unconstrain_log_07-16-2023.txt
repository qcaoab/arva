/home/marcchen/Documents/constrain_factor/researchcode/exp_config_json_files/multi_portfolio_exp1.json
Starting at: 
16-07-23_10:15

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = 3factor_mc
timeseries_basket['basket_desc'] = 3 factors : Basic, size, value, mom
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret', 'Mom_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = 3factor_mc
timeseries_basket['basket_desc'] = 3 factors : Basic, size, value, mom
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret', 'Mom_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.005383     0.031411
192608                    0.0319              0.0561  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0355  ...     0.005323    -0.028996
192611                   -0.0038              0.0294  ...     0.005303     0.028554

[5 rows x 7 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.043289    -0.036240
202209                   -0.0955             -0.0871  ...    -0.050056    -0.091324
202210                    0.0883              0.1486  ...    -0.014968     0.077403
202211                   -0.0076              0.0462  ...     0.040789     0.052365
202212                   -0.0457             -0.0499  ...    -0.018566    -0.057116

[5 rows x 7 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'Mom_Hi30_nom_ret_ind', 'CPI_nom_ret_ind', 'T30_nom_ret_ind',
       'B10_nom_ret_ind', 'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
Mom_Hi30_real_ret      0.011386
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
Mom_Hi30_real_ret      0.061421
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Mom_Hi30_real_ret
T30_real_ret             1.000000  ...           0.055142
B10_real_ret             0.351722  ...           0.066570
VWD_real_ret             0.068448  ...           0.936115
Size_Lo30_real_ret       0.014412  ...           0.903222
Value_Hi30_real_ret      0.018239  ...           0.869469
Mom_Hi30_real_ret        0.055142  ...           1.000000

[6 rows x 6 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      14  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      14  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 14)     True          14  
2     (14, 14)     True          14  
3      (14, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       6       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      14  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      14  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       6           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 14)     True          14  
2     (14, 14)     True          14  
3      (14, 6)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        6           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 6)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 1500, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        6           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 6)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.6, 0.1, 0.1, 0.1, 0.1]
W_T_mean: 949.9717120279637
W_T_median: 709.5227420395738
W_T_pctile_5: -172.4431898348041
W_T_CVAR_5_pct: -301.59287550049254
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1827.793337299374
Current xi:  [123.31879]
objective value function right now is: -1827.793337299374
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1852.3401673357384
Current xi:  [147.73106]
objective value function right now is: -1852.3401673357384
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1870.9027881686386
Current xi:  [171.53851]
objective value function right now is: -1870.9027881686386
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1887.1411285530273
Current xi:  [195.1485]
objective value function right now is: -1887.1411285530273
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1896.0732437025974
Current xi:  [218.21135]
objective value function right now is: -1896.0732437025974
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1917.1394712767851
Current xi:  [241.56367]
objective value function right now is: -1917.1394712767851
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1931.4004463450926
Current xi:  [264.4462]
objective value function right now is: -1931.4004463450926
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1944.5414188525865
Current xi:  [287.0037]
objective value function right now is: -1944.5414188525865
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1957.0013651326146
Current xi:  [309.6203]
objective value function right now is: -1957.0013651326146
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1969.3022044244537
Current xi:  [332.0555]
objective value function right now is: -1969.3022044244537
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1982.4978023344308
Current xi:  [354.68246]
objective value function right now is: -1982.4978023344308
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1995.182033512693
Current xi:  [377.5097]
objective value function right now is: -1995.182033512693
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -2007.9230539553025
Current xi:  [399.80453]
objective value function right now is: -2007.9230539553025
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -2026.120957769874
Current xi:  [423.7375]
objective value function right now is: -2026.120957769874
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -2041.709907092645
Current xi:  [447.2669]
objective value function right now is: -2041.709907092645
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -2054.159668411221
Current xi:  [470.29123]
objective value function right now is: -2054.159668411221
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -2063.192796820965
Current xi:  [493.02988]
objective value function right now is: -2063.192796820965
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -2077.9120474175807
Current xi:  [515.3463]
objective value function right now is: -2077.9120474175807
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -2089.2449487060376
Current xi:  [537.87695]
objective value function right now is: -2089.2449487060376
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2101.2573533503014
Current xi:  [560.2237]
objective value function right now is: -2101.2573533503014
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -2111.806717279209
Current xi:  [582.65607]
objective value function right now is: -2111.806717279209
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -2121.951286482487
Current xi:  [604.62524]
objective value function right now is: -2121.951286482487
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2132.7979898099466
Current xi:  [626.44867]
objective value function right now is: -2132.7979898099466
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -2140.9588377563387
Current xi:  [648.4059]
objective value function right now is: -2140.9588377563387
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -2153.327779529473
Current xi:  [670.1272]
objective value function right now is: -2153.327779529473
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -2159.9513324195045
Current xi:  [691.894]
objective value function right now is: -2159.9513324195045
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -2169.0293052636916
Current xi:  [713.82886]
objective value function right now is: -2169.0293052636916
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -2176.1339975455576
Current xi:  [735.04407]
objective value function right now is: -2176.1339975455576
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -2186.348671323227
Current xi:  [756.044]
objective value function right now is: -2186.348671323227
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -2194.683882851182
Current xi:  [777.3146]
objective value function right now is: -2194.683882851182
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -2202.214169570095
Current xi:  [798.32623]
objective value function right now is: -2202.214169570095
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -2206.6426067448474
Current xi:  [820.08307]
objective value function right now is: -2206.6426067448474
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -2215.641724989653
Current xi:  [841.0106]
objective value function right now is: -2215.641724989653
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -2220.365952015887
Current xi:  [861.6094]
objective value function right now is: -2220.365952015887
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -2228.379497505696
Current xi:  [882.0605]
objective value function right now is: -2228.379497505696
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2233.581369053927
Current xi:  [886.1013]
objective value function right now is: -2233.581369053927
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -2234.117652691027
Current xi:  [890.4255]
objective value function right now is: -2234.117652691027
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -2236.1756318113516
Current xi:  [894.82794]
objective value function right now is: -2236.1756318113516
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2237.0654219978915
Current xi:  [899.1831]
objective value function right now is: -2237.0654219978915
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -2237.0925172447014
Current xi:  [903.38074]
objective value function right now is: -2237.0925172447014
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -2239.788346422997
Current xi:  [907.7112]
objective value function right now is: -2239.788346422997
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [912.03357]
objective value function right now is: -2239.505599515552
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -2242.001247707706
Current xi:  [916.3259]
objective value function right now is: -2242.001247707706
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -2243.033210262497
Current xi:  [920.66705]
objective value function right now is: -2243.033210262497
90.0% of gradient descent iterations done. Method = Adam
new min fval:  -2244.6326910832518
Current xi:  [924.9187]
objective value function right now is: -2244.6326910832518
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [929.1581]
objective value function right now is: -2243.2622363912974
94.0% of gradient descent iterations done. Method = Adam
new min fval:  -2244.9119414889897
Current xi:  [933.44586]
objective value function right now is: -2244.9119414889897
new min fval from sgd:  -2247.6688484955057
new min fval from sgd:  -2247.71694220812
new min fval from sgd:  -2247.8959041576413
new min fval from sgd:  -2248.027852914767
new min fval from sgd:  -2248.158688202049
new min fval from sgd:  -2248.2604639579936
new min fval from sgd:  -2248.3063550590036
new min fval from sgd:  -2248.3203307221106
new min fval from sgd:  -2248.3385211964996
new min fval from sgd:  -2248.4730077546806
new min fval from sgd:  -2248.546973181895
new min fval from sgd:  -2248.68739700093
new min fval from sgd:  -2248.771569477337
new min fval from sgd:  -2248.788376463863
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [937.74524]
objective value function right now is: -2248.740788764176
new min fval from sgd:  -2248.788389458999
new min fval from sgd:  -2248.8386630991745
new min fval from sgd:  -2248.857742300031
new min fval from sgd:  -2248.8931306630748
new min fval from sgd:  -2248.9128250832305
new min fval from sgd:  -2248.92690590644
new min fval from sgd:  -2249.0282410607606
new min fval from sgd:  -2249.077870496357
new min fval from sgd:  -2249.113420263688
new min fval from sgd:  -2249.1606644295807
new min fval from sgd:  -2249.23202857267
new min fval from sgd:  -2249.3062838564624
new min fval from sgd:  -2249.362429114327
new min fval from sgd:  -2249.39144579608
new min fval from sgd:  -2249.4029354595314
new min fval from sgd:  -2249.4170243265926
new min fval from sgd:  -2249.4328706951
new min fval from sgd:  -2249.4426675281397
new min fval from sgd:  -2249.4593244173298
new min fval from sgd:  -2249.474651418045
new min fval from sgd:  -2249.4823522645365
new min fval from sgd:  -2249.4946878745022
new min fval from sgd:  -2249.509076233346
new min fval from sgd:  -2249.5247712305923
new min fval from sgd:  -2249.542411058698
new min fval from sgd:  -2249.560039014259
new min fval from sgd:  -2249.573602816348
new min fval from sgd:  -2249.594871479949
new min fval from sgd:  -2249.61238646655
new min fval from sgd:  -2249.6317292249573
new min fval from sgd:  -2249.6318021723896
new min fval from sgd:  -2249.6413110287403
new min fval from sgd:  -2249.653133945288
new min fval from sgd:  -2249.661229535308
new min fval from sgd:  -2249.667191826118
new min fval from sgd:  -2249.6687074488023
new min fval from sgd:  -2249.6703167860624
new min fval from sgd:  -2249.6724112901047
new min fval from sgd:  -2249.6833910517003
new min fval from sgd:  -2249.6941192302847
new min fval from sgd:  -2249.703595363026
new min fval from sgd:  -2249.7137296679484
new min fval from sgd:  -2249.7241764853525
new min fval from sgd:  -2249.7350222550303
new min fval from sgd:  -2249.742427019512
new min fval from sgd:  -2249.744075185078
new min fval from sgd:  -2249.7449696173717
new min fval from sgd:  -2249.748043665577
new min fval from sgd:  -2249.7535787154857
new min fval from sgd:  -2249.7576358727783
new min fval from sgd:  -2249.761570374848
new min fval from sgd:  -2249.766756193994
new min fval from sgd:  -2249.7710375900447
new min fval from sgd:  -2249.7788872863157
new min fval from sgd:  -2249.7873502958464
new min fval from sgd:  -2249.7945007199514
new min fval from sgd:  -2249.801619168594
new min fval from sgd:  -2249.810207813425
new min fval from sgd:  -2249.8153415814145
new min fval from sgd:  -2249.817175354139
new min fval from sgd:  -2249.8177344144974
new min fval from sgd:  -2249.8202588392114
new min fval from sgd:  -2249.8236384813367
new min fval from sgd:  -2249.826190349699
new min fval from sgd:  -2249.830245415008
new min fval from sgd:  -2249.834433367244
new min fval from sgd:  -2249.8366869646134
new min fval from sgd:  -2249.838353374205
new min fval from sgd:  -2249.839835577441
new min fval from sgd:  -2249.841440206762
new min fval from sgd:  -2249.843692998337
new min fval from sgd:  -2249.8508471218843
new min fval from sgd:  -2249.861044688704
new min fval from sgd:  -2249.86921986701
new min fval from sgd:  -2249.877255975828
new min fval from sgd:  -2249.877400327459
new min fval from sgd:  -2249.8822626931565
new min fval from sgd:  -2249.8845114117275
new min fval from sgd:  -2249.889897068928
new min fval from sgd:  -2249.8971120225688
new min fval from sgd:  -2249.9023842972633
new min fval from sgd:  -2249.9027045362163
new min fval from sgd:  -2249.9031181377723
new min fval from sgd:  -2249.913773871012
new min fval from sgd:  -2249.9244422955544
new min fval from sgd:  -2249.9319928734535
new min fval from sgd:  -2249.9356209369917
new min fval from sgd:  -2249.9379483656285
new min fval from sgd:  -2249.9403424199863
new min fval from sgd:  -2249.9432640814557
new min fval from sgd:  -2249.948441089427
new min fval from sgd:  -2249.9528636484365
new min fval from sgd:  -2249.956349508072
new min fval from sgd:  -2249.957560399511
new min fval from sgd:  -2249.9577322250675
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [940.2571]
objective value function right now is: -2249.87953083444
new min fval from sgd:  -2249.9585288299295
new min fval from sgd:  -2249.959680156058
new min fval from sgd:  -2249.960456283104
new min fval from sgd:  -2249.9622857205354
new min fval from sgd:  -2249.9626666818885
new min fval from sgd:  -2249.96503925465
new min fval from sgd:  -2249.9668931388755
new min fval from sgd:  -2249.968680278888
new min fval from sgd:  -2249.96970228503
new min fval from sgd:  -2249.9763669689514
new min fval from sgd:  -2249.9827994393454
new min fval from sgd:  -2249.993370297961
new min fval from sgd:  -2249.9984312640227
new min fval from sgd:  -2250.007404242651
new min fval from sgd:  -2250.0134511005085
new min fval from sgd:  -2250.013748064951
new min fval from sgd:  -2250.0193826820037
new min fval from sgd:  -2250.030257338201
new min fval from sgd:  -2250.0390581029956
new min fval from sgd:  -2250.04408368887
new min fval from sgd:  -2250.049144568855
new min fval from sgd:  -2250.0529891236088
new min fval from sgd:  -2250.055844938689
new min fval from sgd:  -2250.0577731424914
new min fval from sgd:  -2250.0602610370565
new min fval from sgd:  -2250.060609791047
new min fval from sgd:  -2250.069399368657
new min fval from sgd:  -2250.074876703336
new min fval from sgd:  -2250.079040486545
new min fval from sgd:  -2250.0795628389887
new min fval from sgd:  -2250.0829993819693
new min fval from sgd:  -2250.094608188759
new min fval from sgd:  -2250.106457276192
new min fval from sgd:  -2250.1171614355044
new min fval from sgd:  -2250.129211052438
new min fval from sgd:  -2250.142405288967
new min fval from sgd:  -2250.156191007154
new min fval from sgd:  -2250.167909864436
new min fval from sgd:  -2250.1763779618163
new min fval from sgd:  -2250.178586759841
new min fval from sgd:  -2250.18713489128
new min fval from sgd:  -2250.1912441684885
new min fval from sgd:  -2250.1919144415283
new min fval from sgd:  -2250.192382809401
new min fval from sgd:  -2250.1953728849517
new min fval from sgd:  -2250.1965859296783
new min fval from sgd:  -2250.2022358450954
new min fval from sgd:  -2250.206023942426
new min fval from sgd:  -2250.2066026012
new min fval from sgd:  -2250.2148453625555
new min fval from sgd:  -2250.224970377069
new min fval from sgd:  -2250.2298791210105
new min fval from sgd:  -2250.232231310446
new min fval from sgd:  -2250.2341925748997
new min fval from sgd:  -2250.2350878158613
new min fval from sgd:  -2250.2403320547573
new min fval from sgd:  -2250.2431341278034
new min fval from sgd:  -2250.2485571730485
new min fval from sgd:  -2250.2535314634265
new min fval from sgd:  -2250.260206645944
new min fval from sgd:  -2250.268407795869
new min fval from sgd:  -2250.2738643990892
new min fval from sgd:  -2250.278520318443
new min fval from sgd:  -2250.2807195040227
new min fval from sgd:  -2250.2830843065967
new min fval from sgd:  -2250.284346779106
new min fval from sgd:  -2250.28810317468
new min fval from sgd:  -2250.2893353885606
new min fval from sgd:  -2250.2909072259404
new min fval from sgd:  -2250.2964951210356
new min fval from sgd:  -2250.3001584196536
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [941.11285]
objective value function right now is: -2250.247428765719
min fval:  -2250.3001584196536
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -9.5555,   5.0842],
        [ -1.0734,  -0.4082],
        [  9.3349,  -4.1510],
        [ -1.0747,  -0.4060],
        [-12.2872, -15.3430],
        [  7.0310,   3.4772],
        [ -8.6206,   5.5298],
        [  9.8476,  -2.5891],
        [ -1.0734,  -0.4082],
        [ -1.7316,   0.3317],
        [ -8.4766,   4.4021],
        [-22.6181, -15.4338],
        [ -9.4901,   4.9535],
        [ -9.6252,   5.0340]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -0.0973,  -3.1050,  -7.6495,  -3.1055,   5.5295, -10.9175,   0.4118,
         -7.6343,  -3.1050,  -3.2198,  -1.5195,  -1.0606,  -0.2549,  -0.2883],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.5962e+00,  2.1649e-01,  5.3124e+00,  2.3759e-01,  9.4669e+00,
          6.7983e+00, -4.1206e+00,  6.0656e+00,  2.1653e-01,  1.5946e-01,
         -1.9911e+00,  7.4548e+00, -3.1814e+00, -3.1830e+00],
        [-2.6973e+00,  2.1675e-01,  2.8139e+00,  2.1520e-01,  7.3211e+00,
          3.3604e+00, -3.0704e+00,  3.4944e+00,  2.1675e-01,  2.6225e-01,
         -1.1402e+00,  3.7038e-01, -2.4408e+00, -2.4176e+00],
        [-7.4315e-02, -1.1365e-02, -1.3900e-01, -1.1358e-02, -1.2822e-01,
         -2.1961e-01, -2.0559e-01, -4.3951e-01, -1.1365e-02, -8.3361e-03,
         -1.7481e-02,  3.9876e-03, -6.3042e-02, -6.0880e-02],
        [-7.4315e-02, -1.1365e-02, -1.3900e-01, -1.1358e-02, -1.2822e-01,
         -2.1961e-01, -2.0559e-01, -4.3951e-01, -1.1365e-02, -8.3361e-03,
         -1.7481e-02,  3.9876e-03, -6.3042e-02, -6.0880e-02],
        [-7.4315e-02, -1.1365e-02, -1.3900e-01, -1.1358e-02, -1.2822e-01,
         -2.1961e-01, -2.0559e-01, -4.3951e-01, -1.1365e-02, -8.3361e-03,
         -1.7481e-02,  3.9876e-03, -6.3042e-02, -6.0880e-02],
        [ 3.3377e+00,  7.9926e-02, -5.4945e+00,  9.0084e-02, -9.2670e+00,
         -8.8495e+00,  3.9064e+00, -5.9680e+00,  7.9503e-02,  1.7832e-01,
          1.7583e+00, -9.9342e+00,  2.9240e+00,  2.8349e+00],
        [-7.4315e-02, -1.1365e-02, -1.3900e-01, -1.1358e-02, -1.2822e-01,
         -2.1961e-01, -2.0559e-01, -4.3951e-01, -1.1365e-02, -8.3361e-03,
         -1.7480e-02,  3.9876e-03, -6.3042e-02, -6.0880e-02],
        [-7.4315e-02, -1.1365e-02, -1.3900e-01, -1.1358e-02, -1.2822e-01,
         -2.1961e-01, -2.0559e-01, -4.3951e-01, -1.1365e-02, -8.3361e-03,
         -1.7481e-02,  3.9876e-03, -6.3042e-02, -6.0880e-02],
        [-7.4315e-02, -1.1365e-02, -1.3900e-01, -1.1358e-02, -1.2822e-01,
         -2.1961e-01, -2.0559e-01, -4.3951e-01, -1.1365e-02, -8.3361e-03,
         -1.7481e-02,  3.9876e-03, -6.3042e-02, -6.0880e-02],
        [-8.5310e-01,  6.8750e-02,  4.9639e+00,  8.1095e-02,  8.7836e+00,
          6.5561e+00, -1.3952e+00,  5.6896e+00,  6.8719e-02,  2.7239e-01,
          1.0095e-02,  9.3107e-01, -6.5354e-01, -6.4375e-01],
        [-7.4315e-02, -1.1365e-02, -1.3900e-01, -1.1358e-02, -1.2822e-01,
         -2.1961e-01, -2.0559e-01, -4.3951e-01, -1.1365e-02, -8.3360e-03,
         -1.7480e-02,  3.9876e-03, -6.3042e-02, -6.0880e-02],
        [-7.4315e-02, -1.1365e-02, -1.3900e-01, -1.1358e-02, -1.2822e-01,
         -2.1961e-01, -2.0559e-01, -4.3951e-01, -1.1365e-02, -8.3361e-03,
         -1.7481e-02,  3.9876e-03, -6.3042e-02, -6.0880e-02],
        [ 3.1390e+00,  5.8964e-02, -5.2384e+00,  7.0882e-02, -9.0898e+00,
         -8.7831e+00,  3.8818e+00, -5.8479e+00,  5.9418e-02,  2.1544e-01,
          1.7464e+00, -1.0067e+01,  3.1462e+00,  3.0836e+00],
        [-7.4315e-02, -1.1365e-02, -1.3900e-01, -1.1358e-02, -1.2822e-01,
         -2.1961e-01, -2.0559e-01, -4.3951e-01, -1.1365e-02, -8.3361e-03,
         -1.7481e-02,  3.9876e-03, -6.3042e-02, -6.0880e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-3.1711, -3.3215, -1.8740, -1.8740, -1.8740,  2.8212, -1.8740, -1.8740,
        -1.8740, -3.2303, -1.8740, -1.8740,  2.6787, -1.8740], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-8.0813, -3.7064, -0.0245, -0.0245, -0.0245, 11.1748, -0.0245, -0.0245,
         -0.0245, -5.6831, -0.0245, -0.0245, 11.0442, -0.0245]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.0264,   0.2699],
        [-10.3679,  11.4611],
        [ -1.9255,   0.0424],
        [ -1.9113,   0.0585],
        [ 14.0362,   7.0365],
        [ -2.4067,   0.3546],
        [ -1.9113,   0.0585],
        [ -8.5657,   2.7204],
        [ 15.7394,   3.1464],
        [ 11.2297,  -0.3112],
        [ -2.1661,   0.2053],
        [ -1.9299,   0.0386],
        [ -1.9113,   0.0585],
        [  6.9499,   0.7200]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -4.9104,   1.4213,  -4.9786,  -4.9796,  -0.6095,  -4.9860,  -4.9796,
          1.5072,  -4.2360, -10.8423,  -4.8597,  -4.9780,  -4.9796,  -7.7906],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 7.2761e-02,  8.7936e+00, -2.6750e-01, -2.5481e-01, -2.6369e+00,
          8.9771e-02, -2.5481e-01,  2.3386e+00, -9.6426e-01, -9.1439e+00,
         -2.2974e-01, -2.6983e-01, -2.5481e-01, -3.1948e+00],
        [ 1.9907e-01,  1.6638e+00,  2.1192e-01,  2.1875e-01, -2.1385e+00,
          9.1400e-02,  2.1873e-01,  1.6107e+00, -4.7228e+00,  1.3019e+00,
          2.5882e-01,  2.0985e-01,  2.1873e-01,  5.2614e-01],
        [ 2.2909e-01, -2.2416e+00,  2.9626e-01,  2.9197e-01, -2.3556e+01,
          8.0747e-02,  2.9199e-01, -1.9138e+00, -8.7132e+00, -5.8105e+00,
          3.0780e-01,  2.9698e-01,  2.9198e-01, -1.9760e+00],
        [ 1.1412e-01, -5.2638e+00,  7.7411e-02,  8.0456e-02, -3.1668e+00,
         -2.4809e-02,  8.0458e-02, -4.7159e+00, -4.1031e+00, -3.7256e+00,
          1.4235e-01,  7.6836e-02,  8.0457e-02, -1.9023e+00],
        [ 1.1093e-01, -1.8885e+00,  2.5268e-01,  2.2635e-01,  9.0346e-02,
          1.3585e-01,  2.2643e-01, -3.0154e+00,  7.2426e-01,  1.0055e+00,
          9.0153e-02,  2.6057e-01,  2.2640e-01,  4.8338e+00],
        [ 2.4684e-03,  9.3635e-01,  6.7723e-03,  7.3086e-03, -2.0537e+00,
          1.0373e-03,  7.3068e-03,  3.0744e-01, -3.5208e+00,  1.0836e+00,
         -3.0785e-04,  6.5987e-03,  7.3075e-03, -2.5543e-01],
        [-4.2036e-01, -1.3980e+01, -1.8450e-01, -1.8704e-01, -1.2409e+00,
         -5.7513e-01, -1.8707e-01, -1.4232e+01, -1.8245e+00, -4.7042e+00,
         -4.0481e-01, -1.8437e-01, -1.8705e-01, -1.2511e+00],
        [ 1.0081e-01,  2.2085e+00,  2.0137e-01,  1.9104e-01,  4.8138e-01,
         -6.9271e-02,  1.9107e-01, -2.4234e+00, -2.1044e+00,  6.0985e-01,
          1.7079e-01,  2.0428e-01,  1.9106e-01, -2.0113e-01],
        [-1.0527e-02,  1.5779e+00, -2.7237e-01, -2.5163e-01,  5.3403e-01,
          2.4292e-01, -2.5169e-01,  3.3992e+00,  7.0846e-01, -9.8189e+00,
          3.4311e-01, -2.7608e-01, -2.5167e-01, -3.0462e+00],
        [ 2.7439e-01, -7.8406e+00,  1.8162e-01,  1.8153e-01, -4.3400e+00,
          1.8750e-01,  1.8154e-01,  8.9019e-01, -7.4015e+00,  4.7671e-01,
          3.6071e-01,  1.8178e-01,  1.8154e-01, -2.5495e+00],
        [ 6.9679e-02, -1.8299e+00,  5.0292e-02,  5.1679e-02, -3.1308e+00,
         -1.0371e-02,  5.1682e-02, -5.5886e+00, -4.0519e+00, -3.6376e+00,
          7.4849e-02,  5.0094e-02,  5.1681e-02, -6.9548e-01],
        [ 1.5431e-01, -3.8480e+00,  2.5061e-01,  2.4758e-01, -1.6321e+01,
          1.1146e-01,  2.4758e-01, -1.2610e+00, -7.4732e+00, -6.0067e+00,
          2.1352e-01,  2.5109e-01,  2.4758e-01, -7.0115e-01],
        [ 2.0385e-01,  1.7234e+00,  2.2429e-01,  2.3059e-01, -2.1333e+00,
          9.5237e-02,  2.3057e-01,  1.4085e+00, -5.1651e+00,  1.5198e+00,
          2.7556e-01,  2.2234e-01,  2.3057e-01,  3.3757e-01],
        [-2.7386e-01, -8.3864e+00, -1.8482e-01, -1.8408e-01,  3.0398e+00,
         -1.4152e-01, -1.8412e-01, -2.8565e+00, -2.7707e+00,  5.3875e+00,
         -2.9751e-01, -1.8584e-01, -1.8410e-01, -6.9902e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-7.8784, -3.8210,  4.0881, -0.1182, -1.7355, -4.3738,  1.4655, -3.6533,
        -2.3797,  3.0322, -0.6680,  3.7178, -3.7635, -4.9849], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 5.7055e+00,  1.9495e+00, -1.3668e-01, -8.0028e-01, -1.8246e+01,
          1.6585e+00, -1.7871e+01,  1.3905e+00,  2.6762e+00, -2.1108e+01,
         -3.8633e-01, -1.2771e+00,  2.1196e+00, -3.8336e+00],
        [-4.5167e+00,  1.9204e+00, -8.0914e+00, -3.8672e+00,  1.7342e+00,
          5.9025e-01,  7.9638e-02, -1.7087e+00,  1.8428e+00,  2.2972e+00,
         -4.2352e+00, -4.8278e+00,  1.8661e+00,  4.3333e+00],
        [-3.3405e-01, -7.8340e-02, -2.5418e-02, -2.5354e-01, -2.4484e+01,
         -3.7482e-02, -4.7211e+00, -2.0492e+00, -1.5793e+01, -2.2782e+00,
         -1.4796e-01, -7.0686e-02, -6.2188e-02, -3.7687e-01],
        [-3.3451e-01, -7.0456e-02, -2.4538e-02, -2.5663e-01, -2.5064e+01,
         -3.6986e-02, -4.6027e+00, -2.1714e+00, -1.6149e+01, -2.2760e+00,
         -1.4781e-01, -6.5672e-02, -5.5616e-02, -3.8940e-01],
        [ 3.1517e+00, -2.4465e+00,  6.9310e-01,  4.9924e+00, -3.2308e-01,
         -1.7650e+00,  1.1146e+00,  7.0840e-01,  7.9733e-01,  2.9341e+00,
          4.7949e+00,  6.3302e+00, -2.6758e+00, -1.5801e+00],
        [-1.6606e+00, -5.0299e-01,  1.1066e+01,  4.7660e+00,  2.1293e+00,
          3.8452e-01,  3.4074e+00,  2.1820e+00,  1.7034e+00,  4.2121e+00,
          4.3702e+00,  5.1412e+00, -3.5571e-01, -4.6453e-01]], device='cuda:0'))])
xi:  [941.03925]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1345.0699025110848
W_T_median: 1267.0404412149649
W_T_pctile_5: 944.8542391290805
W_T_CVAR_5_pct: 608.7893063258308
Average q (qsum/M+1):  52.966568485383064
Optimal xi:  [941.03925]
Expected(across Rb) median(across samples) p_equity:  0.17230259295999228
obj fun:  tensor(-2250.3002, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: 3factor_mc
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
