Starting at: 
07-07-23_14:20

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.005383     0.031411
192608                    0.0319              0.0561  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0355  ...     0.005323    -0.028996
192611                   -0.0038              0.0294  ...     0.005303     0.028554

[5 rows x 6 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.043289    -0.036240
202209                   -0.0955             -0.0871  ...    -0.050056    -0.091324
202210                    0.0883              0.1486  ...    -0.014968     0.077403
202211                   -0.0076              0.0462  ...     0.040789     0.052365
202212                   -0.0457             -0.0499  ...    -0.018566    -0.057116

[5 rows x 6 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'CPI_nom_ret_ind', 'T30_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Value_Hi30_real_ret
T30_real_ret             1.000000  ...             0.018239
B10_real_ret             0.351722  ...             0.031301
VWD_real_ret             0.068448  ...             0.909335
Size_Lo30_real_ret       0.014412  ...             0.908542
Value_Hi30_real_ret      0.018239  ...             1.000000

[5 rows x 5 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       5       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       5           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 5)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 1500, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.6, 0.1, 0.15, 0.15]
W_T_mean: 885.7020923209269
W_T_median: 652.7542771298704
W_T_pctile_5: -199.01786215571934
W_T_CVAR_5_pct: -325.26341987729825
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1742.422936917936
Current xi:  [118.72226]
objective value function right now is: -1742.422936917936
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1761.8516815926648
Current xi:  [139.7076]
objective value function right now is: -1761.8516815926648
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1774.77578371986
Current xi:  [161.1416]
objective value function right now is: -1774.77578371986
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1790.262153904901
Current xi:  [182.75667]
objective value function right now is: -1790.262153904901
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1805.3893810685126
Current xi:  [203.6402]
objective value function right now is: -1805.3893810685126
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1822.228915435777
Current xi:  [224.93315]
objective value function right now is: -1822.228915435777
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [247.87866]
objective value function right now is: -1807.012851634753
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1826.7853245287154
Current xi:  [265.51828]
objective value function right now is: -1826.7853245287154
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1847.1261385888001
Current xi:  [287.85806]
objective value function right now is: -1847.1261385888001
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1853.8714973077472
Current xi:  [308.48312]
objective value function right now is: -1853.8714973077472
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1856.2680916983018
Current xi:  [328.76566]
objective value function right now is: -1856.2680916983018
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1869.615105821718
Current xi:  [348.26315]
objective value function right now is: -1869.615105821718
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1873.3291646090502
Current xi:  [367.33838]
objective value function right now is: -1873.3291646090502
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1879.466023813347
Current xi:  [385.47003]
objective value function right now is: -1879.466023813347
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1885.4866496655204
Current xi:  [403.64008]
objective value function right now is: -1885.4866496655204
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1890.4236456004191
Current xi:  [421.01245]
objective value function right now is: -1890.4236456004191
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1892.8454028856563
Current xi:  [437.67383]
objective value function right now is: -1892.8454028856563
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1896.1880925700582
Current xi:  [453.08646]
objective value function right now is: -1896.1880925700582
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1896.5693939726846
Current xi:  [468.20886]
objective value function right now is: -1896.5693939726846
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [483.5661]
objective value function right now is: -1895.778337875717
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1900.8495340315505
Current xi:  [497.0637]
objective value function right now is: -1900.8495340315505
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1904.05548092091
Current xi:  [509.94913]
objective value function right now is: -1904.05548092091
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1907.3996405415144
Current xi:  [521.3858]
objective value function right now is: -1907.3996405415144
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [532.3723]
objective value function right now is: -1905.2393510688596
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [542.9248]
objective value function right now is: -1906.8358746467004
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [552.3033]
objective value function right now is: -1902.9086122921915
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1908.6493513987944
Current xi:  [560.6058]
objective value function right now is: -1908.6493513987944
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [567.1128]
objective value function right now is: -1901.5936653225594
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1910.1349781767685
Current xi:  [572.6608]
objective value function right now is: -1910.1349781767685
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1910.7396865389394
Current xi:  [577.44006]
objective value function right now is: -1910.7396865389394
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1913.0987637572596
Current xi:  [581.6233]
objective value function right now is: -1913.0987637572596
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [586.59344]
objective value function right now is: -1899.801387943914
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [590.63116]
objective value function right now is: -1911.2066529185754
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [594.2869]
objective value function right now is: -1909.1074821642126
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [597.51184]
objective value function right now is: -1910.6136130839684
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1915.3792605479714
Current xi:  [598.28064]
objective value function right now is: -1915.3792605479714
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1916.0564396258033
Current xi:  [599.10596]
objective value function right now is: -1916.0564396258033
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [600.0785]
objective value function right now is: -1915.9379034440674
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1916.3371261010193
Current xi:  [601.0474]
objective value function right now is: -1916.3371261010193
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [601.4975]
objective value function right now is: -1916.0099123082412
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [602.0845]
objective value function right now is: -1916.1120201930992
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1916.7043371146003
Current xi:  [602.9203]
objective value function right now is: -1916.7043371146003
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [603.7426]
objective value function right now is: -1913.917956236214
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [604.19385]
objective value function right now is: -1916.281128027036
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [604.89813]
objective value function right now is: -1916.5186822006317
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [605.4095]
objective value function right now is: -1916.181253044643
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [605.75745]
objective value function right now is: -1916.3554807227822
new min fval from sgd:  -1916.7074799069487
new min fval from sgd:  -1916.732544817781
new min fval from sgd:  -1916.7548780746727
new min fval from sgd:  -1916.7920431210116
new min fval from sgd:  -1916.8010060599652
new min fval from sgd:  -1916.826840169555
new min fval from sgd:  -1916.8343863071666
new min fval from sgd:  -1916.8493738144796
new min fval from sgd:  -1916.9103884104272
new min fval from sgd:  -1916.923102579517
new min fval from sgd:  -1916.9474836053637
new min fval from sgd:  -1916.9609780060878
new min fval from sgd:  -1916.9681309216285
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [606.5032]
objective value function right now is: -1916.1709227860888
new min fval from sgd:  -1916.973525407545
new min fval from sgd:  -1916.9864221099558
new min fval from sgd:  -1917.0025502591293
new min fval from sgd:  -1917.0241561190114
new min fval from sgd:  -1917.0336813634733
new min fval from sgd:  -1917.0347892706486
new min fval from sgd:  -1917.035138073275
new min fval from sgd:  -1917.035783954199
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [606.7461]
objective value function right now is: -1916.9412708751959
new min fval from sgd:  -1917.0446748348434
new min fval from sgd:  -1917.0521675469997
new min fval from sgd:  -1917.064795638876
new min fval from sgd:  -1917.0715851429766
new min fval from sgd:  -1917.0794521569037
new min fval from sgd:  -1917.091808355319
new min fval from sgd:  -1917.095600225597
new min fval from sgd:  -1917.0974445181444
new min fval from sgd:  -1917.1042706396977
new min fval from sgd:  -1917.1098102075737
new min fval from sgd:  -1917.1152779490417
new min fval from sgd:  -1917.119398328069
new min fval from sgd:  -1917.120721551925
new min fval from sgd:  -1917.1218718138966
new min fval from sgd:  -1917.1229515962893
new min fval from sgd:  -1917.1237098115832
new min fval from sgd:  -1917.1237243227952
new min fval from sgd:  -1917.1347813901111
new min fval from sgd:  -1917.1440900790687
new min fval from sgd:  -1917.1454392966607
new min fval from sgd:  -1917.1473855625843
new min fval from sgd:  -1917.1479280399153
new min fval from sgd:  -1917.1482369233652
new min fval from sgd:  -1917.14913961029
new min fval from sgd:  -1917.1507065624369
new min fval from sgd:  -1917.1530350305623
new min fval from sgd:  -1917.1563333963943
new min fval from sgd:  -1917.1602985987138
new min fval from sgd:  -1917.1659223402116
new min fval from sgd:  -1917.1703892273576
new min fval from sgd:  -1917.1751234256535
new min fval from sgd:  -1917.1800314609989
new min fval from sgd:  -1917.18486576394
new min fval from sgd:  -1917.1849288290302
new min fval from sgd:  -1917.1867819597517
new min fval from sgd:  -1917.1944420767259
new min fval from sgd:  -1917.2015499941374
new min fval from sgd:  -1917.2074496170949
new min fval from sgd:  -1917.2090854106941
new min fval from sgd:  -1917.2100134858936
new min fval from sgd:  -1917.2163451888343
new min fval from sgd:  -1917.2222261501079
new min fval from sgd:  -1917.2296400085477
new min fval from sgd:  -1917.2329955934238
new min fval from sgd:  -1917.2375352064817
new min fval from sgd:  -1917.2400244220485
new min fval from sgd:  -1917.2415662433436
new min fval from sgd:  -1917.2426838334375
new min fval from sgd:  -1917.244035287555
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [606.83606]
objective value function right now is: -1917.1393061295112
min fval:  -1917.244035287555
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-13.9122,   4.9054],
        [  4.7122,   4.8818],
        [-13.9629,   4.8175],
        [ -1.9438, -12.5956],
        [ 10.4388,  -1.9117],
        [ -1.0417,  -0.2464],
        [ 10.2260,  -1.4627],
        [-21.3678,   2.0147],
        [ -4.1939,  10.1806],
        [  7.6684,   1.1450],
        [ -1.0298,  -0.2524],
        [  9.7587,  -1.9370],
        [ -1.0239,  -0.2555]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  0.8757, -10.8309,   0.8539,  -2.8587,  -8.2065,  -3.3017,  -8.2520,
          0.6209,   2.6952,  -9.3335,  -3.3128,  -8.0199,  -3.3166],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.1622e+00,  4.4692e+00, -1.8614e+00, -1.0962e+00,  1.0703e+01,
         -2.8084e-01,  1.0247e+01, -6.9291e-03, -8.0824e+00,  5.5012e+00,
         -3.0952e-01,  7.1734e+00, -3.2144e-01],
        [ 1.0040e-01, -7.2547e-01,  9.9795e-02,  5.9990e-01, -6.6586e-01,
          3.8011e-02, -6.3192e-01, -1.4489e-01, -1.9918e-01, -2.7622e-01,
          3.7264e-02, -5.5984e-01,  3.7440e-02],
        [ 2.1854e-01, -9.8078e-01,  2.1872e-01,  1.1070e+00, -8.2626e-01,
          1.6362e-01, -7.6109e-01, -2.6745e-02,  2.1822e-01, -3.1581e-01,
          1.6279e-01, -6.9234e-01,  1.6348e-01],
        [-8.1495e+00, -1.6970e-01, -8.0108e+00,  1.1480e+01, -1.2737e+01,
         -1.3839e-01, -1.0165e+01, -9.6729e+00, -9.8620e-01, -2.3156e-01,
         -1.7291e-01, -1.0093e+01, -1.8427e-01],
        [ 1.0040e-01, -7.2547e-01,  9.9795e-02,  5.9990e-01, -6.6586e-01,
          3.8011e-02, -6.3193e-01, -1.4489e-01, -1.9918e-01, -2.7622e-01,
          3.7264e-02, -5.5984e-01,  3.7440e-02],
        [ 4.5631e-01, -1.1292e+00,  4.5604e-01,  1.7346e+00, -8.9622e-01,
          3.5488e-01, -8.2123e-01,  2.7359e-01,  4.6734e-01, -3.4684e-01,
          3.5459e-01, -7.6160e-01,  3.5575e-01],
        [ 1.6145e+00, -1.7488e-03,  2.0810e+00,  1.3621e+01,  1.2299e-01,
         -2.1875e-01,  9.2799e-02,  5.4810e+00, -6.2541e+00, -5.2697e-03,
         -2.5770e-01,  9.5701e-02, -2.7342e-01],
        [ 1.0040e-01, -7.2547e-01,  9.9795e-02,  5.9990e-01, -6.6586e-01,
          3.8011e-02, -6.3192e-01, -1.4489e-01, -1.9918e-01, -2.7622e-01,
          3.7264e-02, -5.5984e-01,  3.7440e-02],
        [ 1.0040e-01, -7.2547e-01,  9.9795e-02,  5.9990e-01, -6.6586e-01,
          3.8011e-02, -6.3193e-01, -1.4489e-01, -1.9918e-01, -2.7622e-01,
          3.7264e-02, -5.5984e-01,  3.7440e-02],
        [ 1.9567e+00, -2.3625e+00,  2.1352e+00, -1.5094e+01, -4.6210e+00,
         -2.3666e-01, -4.5915e+00,  5.3407e-01,  2.5376e+00, -2.2099e+00,
         -2.6569e-01, -2.9384e+00, -2.7779e-01],
        [ 1.0040e-01, -7.2549e-01,  9.9794e-02,  5.9993e-01, -6.6587e-01,
          3.8014e-02, -6.3193e-01, -1.4490e-01, -1.9915e-01, -2.7622e-01,
          3.7267e-02, -5.5984e-01,  3.7443e-02],
        [ 1.0040e-01, -7.2547e-01,  9.9795e-02,  5.9990e-01, -6.6586e-01,
          3.8011e-02, -6.3193e-01, -1.4489e-01, -1.9918e-01, -2.7622e-01,
          3.7264e-02, -5.5984e-01,  3.7440e-02],
        [ 1.0040e-01, -7.2551e-01,  9.9795e-02,  5.9996e-01, -6.6588e-01,
          3.8018e-02, -6.3194e-01, -1.4491e-01, -1.9910e-01, -2.7622e-01,
          3.7271e-02, -5.5985e-01,  3.7447e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-5.2603, -1.6041, -1.4088,  0.8044, -1.6041, -1.1238, -8.5832, -1.6041,
        -1.6041,  2.2977, -1.6042, -1.6041, -1.6043], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-11.9523,   0.9290,   1.3294, -13.3323,   0.9290,   1.5260, -14.5252,
           0.9290,   0.9290,  14.6813,   0.9291,   0.9290,   0.9292]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-11.8662,  -3.1095],
        [ -9.4166,   0.3461],
        [  5.1798,  11.4286],
        [ 11.4494,   0.1543],
        [ 12.1747,   3.0110],
        [  8.3729,   0.5480],
        [-11.4132,   5.5411],
        [  8.8890,   5.8551],
        [ -4.9948,  10.9547],
        [-12.6036,  -4.3758],
        [ 10.8569,   0.1460],
        [  7.7188,   7.7133],
        [ -1.2005,   1.7395]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 0.4758,  9.3977,  3.2018, -9.9957, -2.7220, -9.8529,  3.0042, -2.5848,
         2.7656, -1.8413, -9.5001,  4.9925, -4.2733], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 4.4199e+00,  1.5787e+00, -5.1968e+00, -1.7603e+00, -1.5591e+00,
         -1.4894e+00, -2.1233e+00, -7.1132e-01, -1.0902e+01,  3.9971e+00,
         -1.2840e+00, -2.6509e+00,  3.7686e-02],
        [ 5.6982e-01, -1.2914e+00,  2.4633e-01, -1.0385e+00, -2.1180e+00,
         -1.6812e-01,  5.4870e-01, -7.8261e-01, -1.4467e-01,  5.3831e-01,
         -9.8452e-01, -8.9978e-01, -2.4335e-02],
        [ 9.4538e-01, -3.4477e+00, -1.1205e+00,  5.9169e+00,  1.1413e+00,
          1.2536e+00, -5.0760e+00,  1.1913e+00, -4.4234e+00,  7.5391e-01,
          4.5287e+00,  2.1057e+00, -1.0386e+00],
        [ 5.7097e-01, -1.2810e+00,  2.5471e-01, -1.0491e+00, -2.1060e+00,
         -1.7333e-01,  5.4893e-01, -7.8062e-01, -1.4534e-01,  5.4247e-01,
         -9.9364e-01, -8.9288e-01, -2.7085e-02],
        [ 9.5015e+00,  5.0316e+00, -1.5858e+01, -8.0303e+00, -5.5463e+00,
         -5.0312e+00, -2.6887e+00, -2.9531e+00, -9.6141e+00,  1.0873e+01,
         -6.8370e+00, -1.1365e+01,  1.0903e-01],
        [ 5.7058e-01, -1.2878e+00,  2.4912e-01, -1.0420e+00, -2.1140e+00,
         -1.6984e-01,  5.4902e-01, -7.8221e-01, -1.4476e-01,  5.4002e-01,
         -9.8751e-01, -8.9732e-01, -2.5233e-02],
        [ 5.7035e-01, -1.2910e+00,  2.4659e-01, -1.0387e+00, -2.1179e+00,
         -1.6821e-01,  5.4905e-01, -7.8290e-01, -1.4457e-01,  5.3889e-01,
         -9.8465e-01, -8.9941e-01, -2.4368e-02],
        [ 5.7146e-01, -1.2802e+00,  2.5535e-01, -1.0498e+00, -2.1054e+00,
         -1.7366e-01,  5.4923e-01, -7.8076e-01, -1.4530e-01,  5.4316e-01,
         -9.9418e-01, -8.9225e-01, -2.7248e-02],
        [ 5.7048e-01, -1.2841e+00,  2.5215e-01, -1.0460e+00, -2.1094e+00,
         -1.7179e-01,  5.4875e-01, -7.8114e-01, -1.4514e-01,  5.4106e-01,
         -9.9096e-01, -8.9498e-01, -2.6276e-02],
        [-1.1221e+00,  3.7830e+00,  1.1449e+00, -1.0362e+00, -2.1514e+00,
         -4.2351e+00,  2.2195e+00, -1.3381e+00,  4.3268e+00, -3.2124e+00,
         -9.8680e-01, -2.2021e+00, -7.0490e-01],
        [ 5.3716e-01, -1.2203e+00,  3.1941e-01, -1.1425e+00, -1.9966e+00,
         -2.1538e-01,  5.2342e-01, -7.2231e-01, -1.6526e-01,  5.3247e-01,
         -1.0747e+00, -8.5706e-01, -5.1027e-02],
        [ 2.2832e+00, -5.1829e+00,  9.3714e-01, -8.7062e+00, -1.5939e+00,
         -3.0103e+00,  4.5931e+00, -2.1618e-01,  6.2434e+00,  6.0534e+00,
         -8.7893e+00, -1.0390e+00,  3.0093e-01],
        [ 1.5133e+00,  3.5567e+00,  2.2109e-01, -5.6266e+00, -5.2284e+00,
          7.9446e-02,  1.1016e+00, -9.3386e-02,  1.4685e-01,  5.6581e+00,
         -5.4696e+00, -2.7861e+01, -3.1702e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 1.1240, -2.9323,  1.5711, -2.9373, -0.2741, -2.9340, -2.9323, -2.9374,
        -2.9360, -2.3102, -2.9734, -2.1530, -7.1720], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-2.0009e+01,  1.3344e+00, -9.0494e+00,  1.3471e+00,  1.0917e-01,
          1.3394e+00,  1.3349e+00,  1.3479e+00,  1.3438e+00,  1.9415e+00,
          1.3884e+00,  4.8686e+00,  4.7205e-08],
        [ 8.5735e-02, -3.1391e-01,  7.8256e-01, -3.1604e-01, -5.8658e+00,
         -3.1413e-01, -3.1429e-01, -3.1668e-01, -3.1468e-01,  1.2768e+00,
         -3.6604e-01,  1.4754e+00,  1.5959e+01],
        [-1.6830e+00, -8.9977e-02, -1.2589e+01, -9.1562e-02, -9.4533e-02,
         -9.0540e-02, -9.0073e-02, -9.1721e-02, -9.1064e-02, -1.1778e+01,
         -1.0050e-01, -2.8079e-01, -8.7679e-03],
        [-1.6076e+00, -7.7301e-02, -1.2498e+01, -7.8629e-02, -7.8253e-02,
         -7.7773e-02, -7.7383e-02, -7.8763e-02, -7.8211e-02, -1.1850e+01,
         -8.6007e-02, -2.9695e-01, -1.1448e-02],
        [ 1.7386e+00, -1.0462e-01,  1.0445e+00, -9.7904e-02,  7.2360e+00,
         -1.0198e-01, -1.0490e-01, -9.8026e-02, -9.9165e-02,  1.6031e+00,
         -6.2002e-02,  2.1925e+00, -1.5602e+01]], device='cuda:0'))])
xi:  [606.80634]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 825.4623575007319
W_T_median: 783.0860624834904
W_T_pctile_5: 606.8456103198255
W_T_CVAR_5_pct: 307.4874717749237
Average q (qsum/M+1):  51.92764380670363
Optimal xi:  [606.80634]
Expected(across Rb) median(across samples) p_equity:  0.22348343641497195
obj fun:  tensor(-1917.2440, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
