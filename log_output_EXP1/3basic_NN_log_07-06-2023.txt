Starting at: 
06-07-23_21:03

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Basic
timeseries_basket['basket_desc'] = Basic portfolio for paper: T30, B10 and VWD
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Basic
timeseries_basket['basket_desc'] = Basic portfolio for paper: T30, B10 and VWD
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret']
############# End: defined asset  basket #################
               CPI_nom_ret  T30_nom_ret  B10_nom_ret  VWD_nom_ret
My identifier                                                    
192607           -0.011299     0.002243     0.005383     0.031411
192608           -0.005714     0.002536     0.005363     0.028647
192609            0.005747     0.002273     0.005343     0.005787
192610            0.005714     0.003195     0.005323    -0.028996
192611            0.005682     0.003093     0.005303     0.028554
               CPI_nom_ret  T30_nom_ret  B10_nom_ret  VWD_nom_ret
My identifier                                                    
202208           -0.000354     0.001901    -0.043289    -0.036240
202209            0.002151     0.001929    -0.050056    -0.091324
202210            0.004056     0.002327    -0.014968     0.077403
202211           -0.001010     0.002856     0.040789     0.052365
202212           -0.003070     0.003379    -0.018566    -0.057116
Indices constructed with column names:
Index(['date_for_plt', 'CPI_nom_ret_ind', 'T30_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret    0.000229
B10_real_ret    0.001637
VWD_real_ret    0.006759
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret    0.005227
B10_real_ret    0.019258
VWD_real_ret    0.053610
dtype: float64


timeseries_basket['data_df_corr'] = 
              T30_real_ret  B10_real_ret  VWD_real_ret
T30_real_ret      1.000000      0.351722      0.068448
B10_real_ret      0.351722      1.000000      0.090987
VWD_real_ret      0.068448      0.090987      1.000000


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      11  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      11  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 11)     True          11  
2     (11, 11)     True          11  
3      (11, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       3       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      11  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      11  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       3           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 11)     True          11  
2     (11, 11)     True          11  
3      (11, 3)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       11  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       11  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 11)      True          11  
0     (11, 11)      True          11  
0      (11, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       11  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       11  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        3           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 11)      True          11  
0     (11, 11)      True          11  
0      (11, 3)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       11  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       11  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 11)      True          11  
0     (11, 11)      True          11  
0      (11, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       11  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       11  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        3           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 11)      True          11  
0     (11, 11)      True          11  
0      (11, 3)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0, 0.7, 0.3]
W_T_mean: 550.8097796014655
W_T_median: 376.5715594428238
W_T_pctile_5: -286.81006287847805
W_T_CVAR_5_pct: -395.2724688746242
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1211.0126776466104
Current xi:  [95.30027]
objective value function right now is: -1211.0126776466104
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1335.7251447362532
Current xi:  [83.786354]
objective value function right now is: -1335.7251447362532
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1418.3211320943474
Current xi:  [60.26278]
objective value function right now is: -1418.3211320943474
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1442.747194976451
Current xi:  [43.255653]
objective value function right now is: -1442.747194976451
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1452.6821316252317
Current xi:  [29.580645]
objective value function right now is: -1452.6821316252317
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1455.9929326909692
Current xi:  [20.218773]
objective value function right now is: -1455.9929326909692
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1459.9366535531483
Current xi:  [13.439188]
objective value function right now is: -1459.9366535531483
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1461.462800333118
Current xi:  [7.5970097]
objective value function right now is: -1461.462800333118
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1463.8959099970987
Current xi:  [0.03357042]
objective value function right now is: -1463.8959099970987
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1464.8466591716663
Current xi:  [0.00427362]
objective value function right now is: -1464.8466591716663
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1466.3729283937976
Current xi:  [-0.22999452]
objective value function right now is: -1466.3729283937976
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.0877547]
objective value function right now is: -1463.896721985897
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1467.246252585836
Current xi:  [-0.10051869]
objective value function right now is: -1467.246252585836
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [0.00040194]
objective value function right now is: -1459.1938182383692
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.08294001]
objective value function right now is: -1466.221546915346
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1467.6407441506678
Current xi:  [-0.02341714]
objective value function right now is: -1467.6407441506678
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1468.4439346869005
Current xi:  [-0.06949185]
objective value function right now is: -1468.4439346869005
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1468.4685113503454
Current xi:  [-0.10730332]
objective value function right now is: -1468.4685113503454
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.10071803]
objective value function right now is: -1467.9588150288175
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.19507538]
objective value function right now is: -1467.291440336845
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.08542212]
objective value function right now is: -1468.268358164674
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.13620342]
objective value function right now is: -1466.7873748830325
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.14595564]
objective value function right now is: -1468.0024012229271
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.12999852]
objective value function right now is: -1468.1568485649275
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1468.8832133918158
Current xi:  [-0.02988796]
objective value function right now is: -1468.8832133918158
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.12756565]
objective value function right now is: -1467.5943439611751
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1469.2127256804085
Current xi:  [-0.02227437]
objective value function right now is: -1469.2127256804085
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-0.07871059]
objective value function right now is: -1466.6351975879568
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-0.15459895]
objective value function right now is: -1468.58322446727
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.18389277]
objective value function right now is: -1466.9567426541864
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06013369]
objective value function right now is: -1468.8458861141737
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.08801228]
objective value function right now is: -1468.9904445008704
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.14860292]
objective value function right now is: -1468.7124030257594
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.22887374]
objective value function right now is: -1466.8649223412367
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04309934]
objective value function right now is: -1467.6526853816947
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1469.5710547630526
Current xi:  [-0.04953662]
objective value function right now is: -1469.5710547630526
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1470.3007177451627
Current xi:  [-0.01174671]
objective value function right now is: -1470.3007177451627
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1470.3351656265556
Current xi:  [-0.01600181]
objective value function right now is: -1470.3351656265556
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00144543]
objective value function right now is: -1470.104426202321
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02187981]
objective value function right now is: -1469.7594011365934
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1470.5094363246271
Current xi:  [-0.00150315]
objective value function right now is: -1470.5094363246271
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1470.5258306872813
Current xi:  [-0.0018714]
objective value function right now is: -1470.5258306872813
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03184197]
objective value function right now is: -1470.3668212197706
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01758587]
objective value function right now is: -1470.384275764507
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02646414]
objective value function right now is: -1470.4755599158757
new min fval from sgd:  -1470.526620598721
new min fval from sgd:  -1470.5556144298987
new min fval from sgd:  -1470.5867110812144
new min fval from sgd:  -1470.6013229274417
new min fval from sgd:  -1470.607009493311
new min fval from sgd:  -1470.620762745453
new min fval from sgd:  -1470.6220739781666
new min fval from sgd:  -1470.652421395016
new min fval from sgd:  -1470.6662484783521
new min fval from sgd:  -1470.6839853238193
new min fval from sgd:  -1470.6976073824442
new min fval from sgd:  -1470.7050352792842
new min fval from sgd:  -1470.7137768688149
new min fval from sgd:  -1470.7199965150082
new min fval from sgd:  -1470.7308342264746
new min fval from sgd:  -1470.7478636815385
new min fval from sgd:  -1470.749482194411
new min fval from sgd:  -1470.7676046364672
new min fval from sgd:  -1470.7706604238879
new min fval from sgd:  -1470.772288827457
new min fval from sgd:  -1470.7803357277346
new min fval from sgd:  -1470.7830583400666
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03626673]
objective value function right now is: -1470.5992253688407
new min fval from sgd:  -1470.7859745192818
new min fval from sgd:  -1470.8064217374385
new min fval from sgd:  -1470.8197203066513
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00375276]
objective value function right now is: -1470.6181511837076
new min fval from sgd:  -1470.8279492984573
new min fval from sgd:  -1470.8307627120178
new min fval from sgd:  -1470.8424311506926
new min fval from sgd:  -1470.842862077763
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02778589]
objective value function right now is: -1470.677248820733
new min fval from sgd:  -1470.8689665954696
new min fval from sgd:  -1470.8887105075673
new min fval from sgd:  -1470.894322834345
new min fval from sgd:  -1470.8975313894548
new min fval from sgd:  -1470.8994510810091
new min fval from sgd:  -1470.899924301946
new min fval from sgd:  -1470.902392355147
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00010639]
objective value function right now is: -1470.898480197262
new min fval from sgd:  -1470.9026679709186
new min fval from sgd:  -1470.9036481096427
new min fval from sgd:  -1470.9044865929532
new min fval from sgd:  -1470.905648145747
new min fval from sgd:  -1470.9081150586103
new min fval from sgd:  -1470.9104577775304
new min fval from sgd:  -1470.9116608277084
new min fval from sgd:  -1470.9129672974213
new min fval from sgd:  -1470.9147634122537
new min fval from sgd:  -1470.9163500092109
new min fval from sgd:  -1470.918932023529
new min fval from sgd:  -1470.9211119007239
new min fval from sgd:  -1470.9222271593576
new min fval from sgd:  -1470.9240225909161
new min fval from sgd:  -1470.9245274901573
new min fval from sgd:  -1470.925147699733
new min fval from sgd:  -1470.9255937386665
new min fval from sgd:  -1470.9260248717933
new min fval from sgd:  -1470.926496349442
new min fval from sgd:  -1470.9284909573164
new min fval from sgd:  -1470.9292086387973
new min fval from sgd:  -1470.9295886834543
new min fval from sgd:  -1470.930786513269
new min fval from sgd:  -1470.9311264071468
new min fval from sgd:  -1470.931789290535
new min fval from sgd:  -1470.9321574970402
new min fval from sgd:  -1470.933933518111
new min fval from sgd:  -1470.9353952470935
new min fval from sgd:  -1470.9384657259943
new min fval from sgd:  -1470.9398443410857
new min fval from sgd:  -1470.9407478640935
new min fval from sgd:  -1470.9413454722692
new min fval from sgd:  -1470.94310016986
new min fval from sgd:  -1470.945894466213
new min fval from sgd:  -1470.9492541781278
new min fval from sgd:  -1470.951096226427
new min fval from sgd:  -1470.9527451242996
new min fval from sgd:  -1470.9540375607319
new min fval from sgd:  -1470.9542026987401
new min fval from sgd:  -1470.9600205743563
new min fval from sgd:  -1470.9716855475149
new min fval from sgd:  -1470.9835469051814
new min fval from sgd:  -1470.9912243171411
new min fval from sgd:  -1470.9953978843987
new min fval from sgd:  -1470.9979093061288
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0004768]
objective value function right now is: -1470.9379673336016
min fval:  -1470.9979093061288
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -6.1374,   7.1165],
        [ -0.4893,  -5.9248],
        [ 11.3207,  -0.3785],
        [ 12.0960,  -1.2442],
        [ -5.4714,   7.5151],
        [ -1.0774,   0.1959],
        [ -0.4277,  -7.7703],
        [  0.4708,  -6.9255],
        [-10.0346,   3.1377],
        [ -4.5756,  -9.6533],
        [  1.0733,  -7.3917]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  4.9771,  -4.8706,  -9.9812, -10.1202,   5.3694,  -2.2021,  -5.6230,
         -5.6765,   4.5636,  -5.9980,  -5.5692], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-3.6131e-01, -8.3056e-02, -1.4073e-01, -1.5350e-01, -4.0420e-01,
         -2.9217e-02, -1.2983e-01, -1.0093e-01, -1.6282e-01, -7.5652e-02,
         -1.8828e-01],
        [-5.0529e+00,  2.2926e+00,  1.2740e+01,  8.6022e+00, -5.7548e+00,
          4.6794e-02,  4.1070e+00,  2.1142e+00, -2.1558e+00,  7.2697e+00,
          2.7444e+00],
        [-3.6131e-01, -8.3056e-02, -1.4073e-01, -1.5350e-01, -4.0420e-01,
         -2.9217e-02, -1.2983e-01, -1.0093e-01, -1.6282e-01, -7.5652e-02,
         -1.8828e-01],
        [-3.6131e-01, -8.3056e-02, -1.4073e-01, -1.5350e-01, -4.0420e-01,
         -2.9217e-02, -1.2983e-01, -1.0093e-01, -1.6282e-01, -7.5652e-02,
         -1.8828e-01],
        [-3.6131e-01, -8.3056e-02, -1.4073e-01, -1.5350e-01, -4.0420e-01,
         -2.9217e-02, -1.2983e-01, -1.0093e-01, -1.6282e-01, -7.5652e-02,
         -1.8828e-01],
        [-2.9534e+00,  1.3852e+00,  6.4436e+00,  5.0214e+00, -3.7098e+00,
          9.9677e-03,  2.8676e+00,  1.2223e+00,  1.3419e-01,  4.3099e+00,
          1.7998e+00],
        [-3.6131e-01, -8.3056e-02, -1.4073e-01, -1.5350e-01, -4.0420e-01,
         -2.9217e-02, -1.2983e-01, -1.0093e-01, -1.6282e-01, -7.5652e-02,
         -1.8828e-01],
        [-6.2375e+00,  3.0738e+00,  4.5810e-01,  1.1685e+00, -6.5824e+00,
          1.0092e-01,  5.0467e+00,  3.4680e+00, -1.3920e+00, -9.7184e-02,
          4.8348e+00],
        [-3.6131e-01, -8.3056e-02, -1.4073e-01, -1.5350e-01, -4.0420e-01,
         -2.9217e-02, -1.2983e-01, -1.0093e-01, -1.6282e-01, -7.5652e-02,
         -1.8828e-01],
        [ 4.1089e+00, -9.6945e-01, -1.3059e+01, -1.0034e+01,  5.2048e+00,
          1.9518e-02, -2.6143e+00, -2.6721e+00,  4.6764e+00, -8.4026e+00,
         -2.5812e+00],
        [-3.6131e-01, -8.3056e-02, -1.4073e-01, -1.5350e-01, -4.0420e-01,
         -2.9217e-02, -1.2983e-01, -1.0093e-01, -1.6282e-01, -7.5652e-02,
         -1.8828e-01]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.1882, -0.6897, -1.1882, -1.1882, -1.1882, -1.7230, -1.1882,  1.7042,
        -1.1882,  2.7038, -1.1882], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.0353e-03, -7.3363e+00,  1.0352e-03,  1.0352e-03,  1.0352e-03,
         -3.2970e+00,  1.0352e-03, -4.4714e+00,  1.0352e-03,  1.3978e+01,
          1.0352e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -4.9173,   5.4114],
        [ 14.8926,   3.9020],
        [-10.2255,  -0.1484],
        [-10.2469,   3.3221],
        [ 12.0316,   2.9731],
        [-12.1564,   2.9461],
        [  3.8518, -14.9139],
        [  1.6896,   2.3556],
        [ -1.7842,  14.7554],
        [ 12.0768,  10.0582],
        [  2.1504,  12.9238]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  2.3840,  -1.3395,   8.2298,   9.9624,  -3.1403,   4.1537, -12.1577,
         -8.3154,   8.6926,   5.9757,   9.9016], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-9.6762e-03, -1.5380e+00,  1.7949e-01, -1.9895e+00, -7.2720e-01,
         -3.1578e-01, -1.0874e+00, -1.7208e-02, -1.1444e+00, -1.5275e+00,
          5.1908e-01],
        [-3.1689e-01, -5.3499e+00,  3.5931e+00, -2.2859e+00, -2.3906e+00,
          3.9035e+00,  4.3182e+00,  1.0288e-02, -6.2427e+00, -1.9425e+00,
         -1.1699e+01],
        [-3.1051e+00, -2.1432e+00,  2.8045e-02,  1.9477e+00,  1.4227e+00,
         -2.1653e-01, -1.6335e+01, -2.8955e-02, -3.2302e+01, -1.0948e+00,
          1.0713e+01],
        [-1.0124e+00, -9.6034e+00,  5.0830e+00,  1.7694e+00, -5.5816e+00,
          5.3876e+00,  4.5221e+00, -5.8999e-03, -5.5883e+00, -1.4673e+01,
         -1.7164e+01],
        [ 6.8336e-01, -8.0090e-01,  2.8918e-01, -1.4434e+00, -1.5272e-01,
         -1.0553e+00, -3.5017e-01, -3.0075e-02, -1.8140e+00,  8.7710e-02,
          1.3973e+00],
        [ 5.0782e-02, -1.9373e+00, -8.1476e-01,  5.2497e+00, -1.4952e+00,
         -8.1668e+00, -1.2688e+01,  5.3950e+00, -2.8703e+00,  5.6686e-01,
          2.8966e+00],
        [-1.6449e-01, -1.6025e+00, -1.0061e+00, -6.5049e-01, -1.2938e+00,
         -1.2593e-01, -1.3409e+00, -2.8287e-02, -2.9120e-01, -8.8697e-01,
         -4.6845e-01],
        [-1.9466e-01, -3.6602e-01, -2.8431e+00, -3.9564e+00,  2.1458e-02,
         -1.2777e-01,  6.7233e-01, -1.1755e-01, -7.0976e-01, -2.2199e+00,
         -3.0891e+00],
        [ 5.2517e+00, -1.1129e+00,  7.5590e+00,  2.8857e+00, -1.5506e+00,
          5.1004e-01, -1.2398e+00, -3.9447e+00,  4.8063e+00, -3.2227e-01,
         -4.0259e+00],
        [-6.7638e-01, -8.8058e+00,  4.6671e+00,  5.7961e-03, -5.4775e+00,
          1.5078e+00,  3.5115e+00,  2.6612e-03, -8.6278e-01, -1.2614e+01,
         -1.1139e+01],
        [ 1.1967e-01, -4.0915e+00, -1.4597e-01,  7.8347e-01, -3.0959e+00,
          3.5948e+00, -1.7439e+00, -1.3912e+00,  9.4628e-01, -3.6229e+00,
          1.0068e+00]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.7971, -0.4525, -1.5754, -0.0520, -2.1906, -1.1099, -1.9513, -0.8971,
        -7.8956, -0.9209, -1.7064], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-8.2110e-01, -2.0014e+01, -1.6855e+01, -1.1789e+01, -1.7157e+00,
         -2.3752e+00, -7.4843e-02,  5.6116e+00,  2.2097e+00, -1.7525e+00,
          3.8741e+00],
        [-4.0968e-01,  1.3963e+00,  4.2168e+00, -1.0600e+00, -8.5653e-02,
          6.2958e-01,  8.6336e-04, -1.8213e+00, -1.2913e+00, -1.4504e+00,
         -1.1795e+00],
        [ 1.4809e+00,  2.1854e+00,  4.1710e+00,  5.1564e+00,  9.6392e-01,
          8.4187e-01,  1.3351e-02, -5.5527e-01, -8.6510e-01,  4.0123e+00,
         -5.9779e-01]], device='cuda:0'))])
xi:  [0.00015741]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 54.623439877941145
W_T_median: 25.48104372471201
W_T_pctile_5: 0.0002597439161835524
W_T_CVAR_5_pct: -106.3507235377101
Average q (qsum/M+1):  50.88221396169355
Optimal xi:  [0.00015741]
Expected(across Rb) median(across samples) p_equity:  0.2183851301825295
obj fun:  tensor(-1470.9979, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Basic
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
