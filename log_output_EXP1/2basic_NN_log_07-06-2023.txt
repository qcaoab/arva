Starting at: 
06-07-23_21:03

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_nom_ret', 'VWD_nom_ret']
############# End: defined asset  basket #################
               CPI_nom_ret  B10_nom_ret  VWD_nom_ret
My identifier                                       
192607           -0.011299     0.005383     0.031411
192608           -0.005714     0.005363     0.028647
192609            0.005747     0.005343     0.005787
192610            0.005714     0.005323    -0.028996
192611            0.005682     0.005303     0.028554
               CPI_nom_ret  B10_nom_ret  VWD_nom_ret
My identifier                                       
202208           -0.000354    -0.043289    -0.036240
202209            0.002151    -0.050056    -0.091324
202210            0.004056    -0.014968     0.077403
202211           -0.001010     0.040789     0.052365
202212           -0.003070    -0.018566    -0.057116
Indices constructed with column names:
Index(['date_for_plt', 'CPI_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
B10_real_ret    0.001637
VWD_real_ret    0.006759
dtype: float64


timeseries_basket['data_df_stdev'] = 
B10_real_ret    0.019258
VWD_real_ret    0.053610
dtype: float64


timeseries_basket['data_df_corr'] = 
              B10_real_ret  VWD_real_ret
B10_real_ret      1.000000      0.090987
VWD_real_ret      0.090987      1.000000


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.7, 0.3]
W_T_mean: 806.8030976991514
W_T_median: 590.5096833413396
W_T_pctile_5: -245.69811644652623
W_T_CVAR_5_pct: -375.85021235410403
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1358.91310204424
Current xi:  [85.39248]
objective value function right now is: -1358.91310204424
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1410.9940972602242
Current xi:  [62.72724]
objective value function right now is: -1410.9940972602242
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1424.6632799855145
Current xi:  [43.39528]
objective value function right now is: -1424.6632799855145
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1431.2734455312661
Current xi:  [26.781246]
objective value function right now is: -1431.2734455312661
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1435.5827782661183
Current xi:  [12.653444]
objective value function right now is: -1435.5827782661183
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1441.6428013290615
Current xi:  [0.03325499]
objective value function right now is: -1441.6428013290615
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1441.8990270079842
Current xi:  [-1.7650185]
objective value function right now is: -1441.8990270079842
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.8923213]
objective value function right now is: -1441.1640642917407
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1442.684486421576
Current xi:  [-4.2222443]
objective value function right now is: -1442.684486421576
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.1262016]
objective value function right now is: -1439.46903034541
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.8530664]
objective value function right now is: -1441.9278983754102
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.5034266]
objective value function right now is: -1441.6804564872805
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.9907694]
objective value function right now is: -1442.454294935374
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-6.1345387]
objective value function right now is: -1442.1083229378385
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.096858]
objective value function right now is: -1441.962139798881
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.6294646]
objective value function right now is: -1439.529211274288
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.009318]
objective value function right now is: -1442.4950448440536
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-6.1656623]
objective value function right now is: -1440.585100853412
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.5427604]
objective value function right now is: -1440.9474007851725
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.6481586]
objective value function right now is: -1440.1176722708983
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1443.016464620395
Current xi:  [-8.412282]
objective value function right now is: -1443.016464620395
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.685168]
objective value function right now is: -1442.7849953315017
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.716421]
objective value function right now is: -1441.3111986517386
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.937974]
objective value function right now is: -1442.0752631163077
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1443.353480116219
Current xi:  [-13.098493]
objective value function right now is: -1443.353480116219
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-20.142052]
objective value function right now is: -1442.4110993725028
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1444.4518428207077
Current xi:  [-28.876396]
objective value function right now is: -1444.4518428207077
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1445.0355347526447
Current xi:  [-34.94129]
objective value function right now is: -1445.0355347526447
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1445.5254727183783
Current xi:  [-34.946827]
objective value function right now is: -1445.5254727183783
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.90776]
objective value function right now is: -1444.7003852425776
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -1445.5475857083677
Current xi:  [-35.003292]
objective value function right now is: -1445.5475857083677
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1466.321523729872
Current xi:  [-35.030678]
objective value function right now is: -1466.321523729872
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.982018]
objective value function right now is: -1465.934067913446
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1467.7928256296846
Current xi:  [-35.001797]
objective value function right now is: -1467.7928256296846
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.019382]
objective value function right now is: -1467.3993073577728
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1468.5762786754951
Current xi:  [-34.961674]
objective value function right now is: -1468.5762786754951
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.96534]
objective value function right now is: -1468.5546650950005
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1468.7535237612465
Current xi:  [-34.983562]
objective value function right now is: -1468.7535237612465
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.009335]
objective value function right now is: -1468.643415732589
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1468.87405428637
Current xi:  [-35.005325]
objective value function right now is: -1468.87405428637
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.97028]
objective value function right now is: -1468.8550284233283
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.96986]
objective value function right now is: -1468.846035532677
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1468.9730038124821
Current xi:  [-34.969795]
objective value function right now is: -1468.9730038124821
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1468.998014373826
Current xi:  [-34.98661]
objective value function right now is: -1468.998014373826
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.975315]
objective value function right now is: -1468.5582359653358
new min fval from sgd:  -1468.9984105139101
new min fval from sgd:  -1469.009694716329
new min fval from sgd:  -1469.0675672236914
new min fval from sgd:  -1469.0760687412478
new min fval from sgd:  -1469.0799737963755
new min fval from sgd:  -1469.096951482787
new min fval from sgd:  -1469.1036455414792
new min fval from sgd:  -1469.1390223271642
new min fval from sgd:  -1469.1625622899755
new min fval from sgd:  -1469.1716987406417
new min fval from sgd:  -1469.1727794340136
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.991436]
objective value function right now is: -1469.130039643814
new min fval from sgd:  -1469.178449321576
new min fval from sgd:  -1469.1787547754482
new min fval from sgd:  -1469.1882726170327
new min fval from sgd:  -1469.2010153408405
new min fval from sgd:  -1469.2166509676163
new min fval from sgd:  -1469.2214823179922
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.98441]
objective value function right now is: -1468.7611983868705
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.02121]
objective value function right now is: -1469.0690099093777
new min fval from sgd:  -1469.2216685286314
new min fval from sgd:  -1469.223537430336
new min fval from sgd:  -1469.2249746254208
new min fval from sgd:  -1469.2259360571054
new min fval from sgd:  -1469.226655942773
new min fval from sgd:  -1469.2274185095723
new min fval from sgd:  -1469.230104188554
new min fval from sgd:  -1469.2319018941062
new min fval from sgd:  -1469.2331613780905
new min fval from sgd:  -1469.2346408665492
new min fval from sgd:  -1469.234804686366
new min fval from sgd:  -1469.234954502037
new min fval from sgd:  -1469.2365827658818
new min fval from sgd:  -1469.2378287450588
new min fval from sgd:  -1469.2390034303203
new min fval from sgd:  -1469.2425256358072
new min fval from sgd:  -1469.2456332470183
new min fval from sgd:  -1469.2481709012256
new min fval from sgd:  -1469.2503100016452
new min fval from sgd:  -1469.2514795451066
new min fval from sgd:  -1469.2522921487027
new min fval from sgd:  -1469.2528529291844
new min fval from sgd:  -1469.2540314538196
new min fval from sgd:  -1469.2560201962967
new min fval from sgd:  -1469.2570941278325
new min fval from sgd:  -1469.2586246996598
new min fval from sgd:  -1469.2601505981859
new min fval from sgd:  -1469.2620780524057
new min fval from sgd:  -1469.2636740925323
new min fval from sgd:  -1469.2652533300268
new min fval from sgd:  -1469.2660318105823
new min fval from sgd:  -1469.2667573055871
new min fval from sgd:  -1469.2673749187659
new min fval from sgd:  -1469.2680688038586
new min fval from sgd:  -1469.2682140145632
new min fval from sgd:  -1469.2701015387272
new min fval from sgd:  -1469.2714682209007
new min fval from sgd:  -1469.2736748536279
new min fval from sgd:  -1469.2754772735511
new min fval from sgd:  -1469.2762458908103
new min fval from sgd:  -1469.277366610582
new min fval from sgd:  -1469.2782573426543
new min fval from sgd:  -1469.2786735086559
new min fval from sgd:  -1469.2789308605088
new min fval from sgd:  -1469.279120717577
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.977177]
objective value function right now is: -1469.2673128537924
new min fval from sgd:  -1469.27997495546
new min fval from sgd:  -1469.2831603154134
new min fval from sgd:  -1469.2853753537122
new min fval from sgd:  -1469.2869492613993
new min fval from sgd:  -1469.2884033830883
new min fval from sgd:  -1469.2908617908988
new min fval from sgd:  -1469.2932903342848
new min fval from sgd:  -1469.2955534405114
new min fval from sgd:  -1469.2970118247001
new min fval from sgd:  -1469.2972904761714
new min fval from sgd:  -1469.2978726048195
new min fval from sgd:  -1469.2981137074094
new min fval from sgd:  -1469.2982615871142
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.983067]
objective value function right now is: -1469.2674003238935
min fval:  -1469.2982615871142
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -0.3090,  -0.2996],
        [ -8.2125,  -1.3415],
        [  4.8993,  -8.0650],
        [  4.2855,  -5.5835],
        [  9.9086,  -1.7623],
        [ -7.5685, -10.2031],
        [  6.7487,  -6.9886],
        [  6.9401,  -1.0866],
        [  4.5645,  -8.3236],
        [  6.2603,  -6.3102]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-3.3119,  7.2361, -7.9787, -6.3224, -7.9563, -7.3049, -6.4213, -8.1441,
        -8.2019, -5.7900], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -0.0158,  -0.4441,  -0.1215,  -0.1028,  -0.5352,   0.0139,  -0.2447,
          -0.0422,  -0.1078,  -0.2566],
        [ -0.0158,  -0.4441,  -0.1215,  -0.1028,  -0.5352,   0.0139,  -0.2447,
          -0.0422,  -0.1078,  -0.2566],
        [ -0.0158,  -0.4441,  -0.1215,  -0.1028,  -0.5352,   0.0139,  -0.2447,
          -0.0422,  -0.1078,  -0.2566],
        [ -0.0158,  -0.4441,  -0.1215,  -0.1028,  -0.5352,   0.0139,  -0.2447,
          -0.0422,  -0.1078,  -0.2566],
        [ -0.1323, -10.3703,   6.3535,   2.3825,   8.1414,   6.5415,   4.3062,
           1.8093,   3.7883,   1.5778],
        [ -0.0158,  -0.4441,  -0.1215,  -0.1028,  -0.5352,   0.0139,  -0.2447,
          -0.0422,  -0.1078,  -0.2566],
        [ -0.0158,  -0.4441,  -0.1215,  -0.1028,  -0.5352,   0.0139,  -0.2447,
          -0.0422,  -0.1078,  -0.2566],
        [ -0.1886,  -9.8153,   6.0248,   2.1659,   7.3247,   5.9239,   4.1243,
           1.4336,   3.4230,   1.4319],
        [ -0.0158,  -0.4441,  -0.1215,  -0.1028,  -0.5352,   0.0139,  -0.2447,
          -0.0422,  -0.1078,  -0.2566],
        [ -0.1603,   9.9063,  -6.7750,  -2.7165,  -9.5062,  -5.3157,  -5.7677,
          -2.9930,  -3.6259,  -4.6379]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.2068, -1.2068, -1.2068, -1.2068, -3.4596, -1.2069, -1.2068, -3.1565,
        -1.2068,  3.7821], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 0.0162,  0.0162,  0.0162,  0.0162, -9.1561,  0.0162,  0.0162, -7.4387,
          0.0162, 16.0294]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  8.7243,   7.5288],
        [-10.5185,   6.3446],
        [-14.2015,   6.7173],
        [-11.0035,  -1.9771],
        [-11.6275,  -2.1634],
        [  2.0417,  10.5580],
        [-12.8446,  -2.6453],
        [ -8.6301,  -1.6786],
        [ -9.8767,  -1.3408],
        [  2.7792,   5.1464]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 6.7068,  9.7441,  8.0727, -0.3902, -0.4621,  9.2216, -0.6874, -1.6785,
         4.0415, -5.2584], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.5286e+01, -8.9236e-01, -1.1211e+00,  6.1594e+00,  6.2567e+00,
         -1.0438e+01,  7.2059e+00,  3.4267e+00,  3.1398e+00, -2.0001e-03],
        [ 1.6478e+00, -2.5098e+00,  5.0196e+00, -7.6478e-01, -2.8334e+00,
          7.3523e-01, -6.4059e+00, -1.5221e+00,  1.7163e+00, -2.6630e+00],
        [-1.5198e+01, -2.0230e-01, -8.0172e-01,  5.3483e+00,  5.4730e+00,
         -9.6774e+00,  6.1841e+00,  4.4920e+00,  2.7243e+00, -1.8681e-03],
        [-1.2624e+00,  6.8582e-01, -1.6115e-01,  8.5830e-02,  9.7965e-02,
          1.2538e-01,  1.1134e-01,  3.0222e-02, -1.1724e+00,  7.0348e-01],
        [-1.2630e+00,  6.8484e-01, -1.6129e-01,  8.2835e-02,  9.4856e-02,
          1.2429e-01,  1.0806e-01,  2.8767e-02, -1.1727e+00,  7.0296e-01],
        [-2.2808e+00, -2.0932e+00, -6.9221e+00,  5.1430e+00,  6.6695e+00,
         -1.9188e+01,  6.8772e+00,  3.0597e+00,  4.3196e+00, -6.9956e-03],
        [-1.2663e+00,  6.7923e-01, -1.6208e-01,  6.6533e-02,  7.7935e-02,
          1.1810e-01,  9.0249e-02,  2.0988e-02, -1.1743e+00,  7.0001e-01],
        [-2.1690e+00, -2.4939e+00,  2.6408e+00, -5.7633e+00, -4.6963e+00,
         -1.8939e+00, -2.0893e+00,  3.3918e-01, -1.5719e+01, -2.6886e+00],
        [-1.2632e+00,  6.8450e-01, -1.6134e-01,  8.1838e-02,  9.3821e-02,
          1.2392e-01,  1.0697e-01,  2.8284e-02, -1.1728e+00,  7.0279e-01],
        [-1.2643e+00,  6.8264e-01, -1.6161e-01,  7.6319e-02,  8.8093e-02,
          1.2187e-01,  1.0094e-01,  2.5629e-02, -1.1733e+00,  7.0182e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-5.6204, -4.5453, -4.9713, -4.5869, -4.5873, -3.2502, -4.5898, -0.8258,
        -4.5875, -4.5883], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 8.7824,  1.1042,  8.3480, -0.2922, -0.2995, -7.0388, -0.2993,  5.8714,
         -0.2992, -0.2925],
        [-8.8525, -1.1431, -8.3321,  0.3044,  0.2961,  7.2005,  0.2906, -6.2635,
          0.2961,  0.3009]], device='cuda:0'))])
xi:  [-34.983982]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 454.0872256306885
W_T_median: 168.27568538889506
W_T_pctile_5: -34.98220443454485
W_T_CVAR_5_pct: -148.27261455434157
Average q (qsum/M+1):  52.17969537550403
Optimal xi:  [-34.983982]
Expected(across Rb) median(across samples) p_equity:  0.43740422228972115
obj fun:  tensor(-1469.2983, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
