/home/marcchen/Documents/constrain_factor/researchcode/exp_config_json_files/multi_portfolio_exp1.json
Starting at: 
17-07-23_10:23

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = 3factor_mc
timeseries_basket['basket_desc'] = 3 factors : Basic, size, value, mom
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret', 'Mom_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = 3factor_mc
timeseries_basket['basket_desc'] = 3 factors : Basic, size, value, mom
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret', 'Mom_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.005383     0.031411
192608                    0.0319              0.0561  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0355  ...     0.005323    -0.028996
192611                   -0.0038              0.0294  ...     0.005303     0.028554

[5 rows x 7 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.043289    -0.036240
202209                   -0.0955             -0.0871  ...    -0.050056    -0.091324
202210                    0.0883              0.1486  ...    -0.014968     0.077403
202211                   -0.0076              0.0462  ...     0.040789     0.052365
202212                   -0.0457             -0.0499  ...    -0.018566    -0.057116

[5 rows x 7 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'Mom_Hi30_nom_ret_ind', 'CPI_nom_ret_ind', 'T30_nom_ret_ind',
       'B10_nom_ret_ind', 'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
Mom_Hi30_real_ret      0.011386
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
Mom_Hi30_real_ret      0.061421
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Mom_Hi30_real_ret
T30_real_ret             1.000000  ...           0.055142
B10_real_ret             0.351722  ...           0.066570
VWD_real_ret             0.068448  ...           0.936115
Size_Lo30_real_ret       0.014412  ...           0.903222
Value_Hi30_real_ret      0.018239  ...           0.869469
Mom_Hi30_real_ret        0.055142  ...           1.000000

[6 rows x 6 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      14  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      14  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 14)     True          14  
2     (14, 14)     True          14  
3      (14, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       7       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      14  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      14  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       7              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 14)     True          14  
2     (14, 14)     True          14  
3      (14, 7)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        7              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 7)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 1500, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       14  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       14  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        7              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 14)      True          14  
0     (14, 14)      True          14  
0      (14, 7)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.6, 0.1, 0.1, 0.1, 0.1]
W_T_mean: 949.9717120279637
W_T_median: 709.5227420395738
W_T_pctile_5: -172.4431898348041
W_T_CVAR_5_pct: -301.59287550049254
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1806.8334059775088
Current xi:  [120.18629]
objective value function right now is: -1806.8334059775088
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1818.9367218402163
Current xi:  [142.39954]
objective value function right now is: -1818.9367218402163
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1834.1700508461267
Current xi:  [164.87141]
objective value function right now is: -1834.1700508461267
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1846.7298436981926
Current xi:  [187.08138]
objective value function right now is: -1846.7298436981926
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1858.3518833850449
Current xi:  [208.91191]
objective value function right now is: -1858.3518833850449
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1869.876962809985
Current xi:  [230.65498]
objective value function right now is: -1869.876962809985
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1886.7879576569335
Current xi:  [254.41667]
objective value function right now is: -1886.7879576569335
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1897.5380074855693
Current xi:  [277.24097]
objective value function right now is: -1897.5380074855693
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1907.2171774890526
Current xi:  [298.9911]
objective value function right now is: -1907.2171774890526
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1915.3207552341532
Current xi:  [320.02353]
objective value function right now is: -1915.3207552341532
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1923.9626181852352
Current xi:  [340.68668]
objective value function right now is: -1923.9626181852352
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1931.3917716739495
Current xi:  [361.02945]
objective value function right now is: -1931.3917716739495
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1939.6856451518718
Current xi:  [380.97375]
objective value function right now is: -1939.6856451518718
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1945.4268085810445
Current xi:  [400.46494]
objective value function right now is: -1945.4268085810445
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1951.4496125336607
Current xi:  [419.40698]
objective value function right now is: -1951.4496125336607
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1957.71536938642
Current xi:  [437.9738]
objective value function right now is: -1957.71536938642
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1962.660132904131
Current xi:  [456.0018]
objective value function right now is: -1962.660132904131
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1966.7064753238108
Current xi:  [473.22394]
objective value function right now is: -1966.7064753238108
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1969.2109360863385
Current xi:  [490.50916]
objective value function right now is: -1969.2109360863385
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1974.2315009450094
Current xi:  [507.0065]
objective value function right now is: -1974.2315009450094
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1977.7529336923897
Current xi:  [522.1697]
objective value function right now is: -1977.7529336923897
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1978.751993154823
Current xi:  [536.7445]
objective value function right now is: -1978.751993154823
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1981.4511553566992
Current xi:  [550.33875]
objective value function right now is: -1981.4511553566992
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1982.65727568497
Current xi:  [562.75726]
objective value function right now is: -1982.65727568497
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1983.7527573040857
Current xi:  [573.9027]
objective value function right now is: -1983.7527573040857
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1985.1845017467347
Current xi:  [583.5676]
objective value function right now is: -1985.1845017467347
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1986.5562319659093
Current xi:  [593.0601]
objective value function right now is: -1986.5562319659093
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [600.4737]
objective value function right now is: -1986.2553068658972
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [607.23724]
objective value function right now is: -1986.3058052855051
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1986.8008265592146
Current xi:  [612.29706]
objective value function right now is: -1986.8008265592146
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [616.30084]
objective value function right now is: -1984.8146397906291
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1989.0649420819764
Current xi:  [620.888]
objective value function right now is: -1989.0649420819764
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [624.37335]
objective value function right now is: -1987.699215278728
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [626.6699]
objective value function right now is: -1988.4752997858693
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [628.3772]
objective value function right now is: -1987.3879425777682
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1990.046486563798
Current xi:  [628.51245]
objective value function right now is: -1990.046486563798
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1990.1705425403788
Current xi:  [629.0484]
objective value function right now is: -1990.1705425403788
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1990.3053370862472
Current xi:  [629.83704]
objective value function right now is: -1990.3053370862472
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1990.6228142038133
Current xi:  [630.2326]
objective value function right now is: -1990.6228142038133
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [630.6092]
objective value function right now is: -1990.3111063450315
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [630.8681]
objective value function right now is: -1990.5429266797603
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [631.3218]
objective value function right now is: -1990.2740993506527
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [631.5923]
objective value function right now is: -1990.6095075798887
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [631.9289]
objective value function right now is: -1990.5044819631369
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [632.3139]
objective value function right now is: -1990.4827890806139
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [632.4539]
objective value function right now is: -1990.5041269499459
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [633.0327]
objective value function right now is: -1990.5993209037902
new min fval from sgd:  -1990.6247619941464
new min fval from sgd:  -1990.6263069620188
new min fval from sgd:  -1990.6304451596136
new min fval from sgd:  -1990.6387155403897
new min fval from sgd:  -1990.6397739005452
new min fval from sgd:  -1990.6528869911185
new min fval from sgd:  -1990.6646837038709
new min fval from sgd:  -1990.6751348471507
new min fval from sgd:  -1990.6758305283531
new min fval from sgd:  -1990.714439673401
new min fval from sgd:  -1990.7769922365026
new min fval from sgd:  -1990.8107096683052
new min fval from sgd:  -1990.82692015165
new min fval from sgd:  -1990.8307436981543
new min fval from sgd:  -1990.849160221339
new min fval from sgd:  -1990.860062763637
new min fval from sgd:  -1990.8762168078672
new min fval from sgd:  -1990.8955197129726
new min fval from sgd:  -1990.924714220595
new min fval from sgd:  -1990.9537436113974
new min fval from sgd:  -1990.981471766838
new min fval from sgd:  -1991.0005063616868
new min fval from sgd:  -1991.0139533370625
new min fval from sgd:  -1991.0307877516975
new min fval from sgd:  -1991.0573971654317
new min fval from sgd:  -1991.0867585408002
new min fval from sgd:  -1991.1121135734725
new min fval from sgd:  -1991.1404621192896
new min fval from sgd:  -1991.168473540227
new min fval from sgd:  -1991.1737855485408
new min fval from sgd:  -1991.2184680307616
new min fval from sgd:  -1991.274141940856
new min fval from sgd:  -1991.314158792138
new min fval from sgd:  -1991.3558018921176
new min fval from sgd:  -1991.4197975985694
new min fval from sgd:  -1991.4952815671531
new min fval from sgd:  -1991.5763292925553
new min fval from sgd:  -1991.68268677238
new min fval from sgd:  -1991.7874178150223
new min fval from sgd:  -1991.8749421399493
new min fval from sgd:  -1991.9553158582758
new min fval from sgd:  -1992.0318299366536
new min fval from sgd:  -1992.0946946800389
new min fval from sgd:  -1992.1634318523415
new min fval from sgd:  -1992.2289423378115
new min fval from sgd:  -1992.3008504607328
new min fval from sgd:  -1992.3697243590723
new min fval from sgd:  -1992.442852866484
new min fval from sgd:  -1992.5138394532162
new min fval from sgd:  -1992.5609348063144
new min fval from sgd:  -1992.596254212185
new min fval from sgd:  -1992.6276212098549
new min fval from sgd:  -1992.6731833174617
new min fval from sgd:  -1992.7213145527621
new min fval from sgd:  -1992.7639069642926
new min fval from sgd:  -1992.7883571846594
new min fval from sgd:  -1992.7922239487743
new min fval from sgd:  -1992.8409048751112
new min fval from sgd:  -1992.8686024765266
new min fval from sgd:  -1992.8932788202917
new min fval from sgd:  -1992.9062079146986
new min fval from sgd:  -1992.9223560388423
new min fval from sgd:  -1992.9426624652638
new min fval from sgd:  -1992.9686528327081
new min fval from sgd:  -1992.9818870816343
new min fval from sgd:  -1992.9978128952682
new min fval from sgd:  -1993.017316127892
new min fval from sgd:  -1993.0352019336824
new min fval from sgd:  -1993.0486956792056
new min fval from sgd:  -1993.059881494003
new min fval from sgd:  -1993.064541016568
new min fval from sgd:  -1993.0786111128443
new min fval from sgd:  -1993.123556022721
new min fval from sgd:  -1993.1822909968234
new min fval from sgd:  -1993.2304051778162
new min fval from sgd:  -1993.2344793833504
new min fval from sgd:  -1993.240998245855
new min fval from sgd:  -1993.253987979654
new min fval from sgd:  -1993.2570431846991
new min fval from sgd:  -1993.262477382733
new min fval from sgd:  -1993.2690513364753
new min fval from sgd:  -1993.278394840795
new min fval from sgd:  -1993.2981539048972
new min fval from sgd:  -1993.3140729557151
new min fval from sgd:  -1993.33106252586
new min fval from sgd:  -1993.3511051673024
new min fval from sgd:  -1993.3710349822668
new min fval from sgd:  -1993.3924592612707
new min fval from sgd:  -1993.4126054666388
new min fval from sgd:  -1993.4165939304805
new min fval from sgd:  -1993.422694315115
new min fval from sgd:  -1993.4350306606964
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [633.4315]
objective value function right now is: -1993.4350306606964
new min fval from sgd:  -1993.4354453988487
new min fval from sgd:  -1993.4394043324653
new min fval from sgd:  -1993.4439776240454
new min fval from sgd:  -1993.4484499395048
new min fval from sgd:  -1993.452934375378
new min fval from sgd:  -1993.456868283599
new min fval from sgd:  -1993.4594833590406
new min fval from sgd:  -1993.4610395616698
new min fval from sgd:  -1993.4642987935629
new min fval from sgd:  -1993.467933866164
new min fval from sgd:  -1993.4699655785118
new min fval from sgd:  -1993.4707212455278
new min fval from sgd:  -1993.470952448457
new min fval from sgd:  -1993.4716294139903
new min fval from sgd:  -1993.4767026162713
new min fval from sgd:  -1993.4818753051586
new min fval from sgd:  -1993.4858343721455
new min fval from sgd:  -1993.489102859725
new min fval from sgd:  -1993.492231854697
new min fval from sgd:  -1993.4957739478982
new min fval from sgd:  -1993.4994034616366
new min fval from sgd:  -1993.5026831579185
new min fval from sgd:  -1993.503007632916
new min fval from sgd:  -1993.5072780069265
new min fval from sgd:  -1993.5076651599004
new min fval from sgd:  -1993.5099897972718
new min fval from sgd:  -1993.5132786438978
new min fval from sgd:  -1993.5159000431952
new min fval from sgd:  -1993.5175386569942
new min fval from sgd:  -1993.5216754463338
new min fval from sgd:  -1993.525817765833
new min fval from sgd:  -1993.5273493289167
new min fval from sgd:  -1993.530918485764
new min fval from sgd:  -1993.53580841428
new min fval from sgd:  -1993.5377229462154
new min fval from sgd:  -1993.5390294311185
new min fval from sgd:  -1993.5390604582112
new min fval from sgd:  -1993.5404724218304
new min fval from sgd:  -1993.5431014293927
new min fval from sgd:  -1993.5450744580035
new min fval from sgd:  -1993.5456122317055
new min fval from sgd:  -1993.5466233387078
new min fval from sgd:  -1993.5494588194654
new min fval from sgd:  -1993.5523384006249
new min fval from sgd:  -1993.557632969015
new min fval from sgd:  -1993.5645067399334
new min fval from sgd:  -1993.570597191389
new min fval from sgd:  -1993.576560457382
new min fval from sgd:  -1993.5782521975684
new min fval from sgd:  -1993.5827391300495
new min fval from sgd:  -1993.5866797300732
new min fval from sgd:  -1993.5889050338571
new min fval from sgd:  -1993.595101672505
new min fval from sgd:  -1993.5959194206978
new min fval from sgd:  -1993.6032058897065
new min fval from sgd:  -1993.606746052859
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [633.43274]
objective value function right now is: -1993.568785624604
new min fval from sgd:  -1993.6074053553286
new min fval from sgd:  -1993.609162143835
new min fval from sgd:  -1993.611164900717
new min fval from sgd:  -1993.61124857566
new min fval from sgd:  -1993.6133564344163
new min fval from sgd:  -1993.6159734426387
new min fval from sgd:  -1993.619100086805
new min fval from sgd:  -1993.6213414967824
new min fval from sgd:  -1993.6219756797564
new min fval from sgd:  -1993.6235629844327
new min fval from sgd:  -1993.625811430973
new min fval from sgd:  -1993.6272382245036
new min fval from sgd:  -1993.6299955883908
new min fval from sgd:  -1993.6322684552474
new min fval from sgd:  -1993.632997804935
new min fval from sgd:  -1993.6356099555246
new min fval from sgd:  -1993.6392639131664
new min fval from sgd:  -1993.6449384644945
new min fval from sgd:  -1993.6489391336027
new min fval from sgd:  -1993.6584305789504
new min fval from sgd:  -1993.6635910385207
new min fval from sgd:  -1993.6670332182832
new min fval from sgd:  -1993.6697481279277
new min fval from sgd:  -1993.670077230271
new min fval from sgd:  -1993.6769492410863
new min fval from sgd:  -1993.6840692009218
new min fval from sgd:  -1993.6900016720463
new min fval from sgd:  -1993.6920134627997
new min fval from sgd:  -1993.6921191668423
new min fval from sgd:  -1993.6941985284202
new min fval from sgd:  -1993.6962073688792
new min fval from sgd:  -1993.6969945092435
new min fval from sgd:  -1993.698368265533
new min fval from sgd:  -1993.6988464196709
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [633.5072]
objective value function right now is: -1993.673299755403
min fval:  -1993.6988464196709
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -0.9778,   0.1040],
        [ -9.0668,   5.0131],
        [ 10.7690,  -1.1673],
        [ -8.5068,   4.9309],
        [ -0.9819,   0.0954],
        [ -4.2509,   8.6159],
        [ -7.4486,   5.8198],
        [  7.2797,   3.5896],
        [-11.1804, -14.4428],
        [ -0.2231,  -0.4024],
        [ -0.9794,   0.1009],
        [  8.5040,   1.3198],
        [ -6.8459,   6.1434],
        [-32.8998, -10.7952]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -2.8814,   0.0808,  -8.7293,   1.7248,  -2.8762,   3.1333,   2.0427,
        -10.7964,   0.6250,  -4.2581,  -2.8796,  -9.1716,   2.0375,  -4.3289],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-2.0019e-03, -2.4562e-02, -4.0013e-01, -9.6502e-02, -1.9637e-03,
         -7.5447e-01, -1.8526e-01, -7.3105e-02, -1.9804e-01,  1.0220e-02,
         -1.9809e-03, -1.3015e-01, -2.2714e-01,  2.8482e-02],
        [-2.0019e-03, -2.4562e-02, -4.0013e-01, -9.6502e-02, -1.9636e-03,
         -7.5447e-01, -1.8526e-01, -7.3105e-02, -1.9804e-01,  1.0220e-02,
         -1.9809e-03, -1.3015e-01, -2.2714e-01,  2.8482e-02],
        [-2.0019e-03, -2.4562e-02, -4.0013e-01, -9.6502e-02, -1.9636e-03,
         -7.5447e-01, -1.8526e-01, -7.3105e-02, -1.9804e-01,  1.0220e-02,
         -1.9809e-03, -1.3015e-01, -2.2714e-01,  2.8482e-02],
        [-1.0634e-02,  3.2738e+00, -9.4999e+00,  3.5590e+00,  6.9888e-03,
          3.8951e+00,  3.3433e+00, -8.4084e+00, -9.8686e+00, -2.3233e-01,
         -2.9598e-03, -5.5577e+00,  3.3881e+00, -1.0153e+01],
        [-2.0019e-03, -2.4561e-02, -4.0013e-01, -9.6502e-02, -1.9636e-03,
         -7.5447e-01, -1.8526e-01, -7.3105e-02, -1.9804e-01,  1.0220e-02,
         -1.9809e-03, -1.3015e-01, -2.2714e-01,  2.8482e-02],
        [-9.5646e-02, -3.2355e+00,  9.4995e+00, -4.0926e+00, -7.4128e-02,
         -4.7912e+00, -3.9483e+00,  9.1120e+00,  1.0074e+01,  1.8830e-01,
         -8.7294e-02,  6.5001e+00, -3.1126e+00,  9.2872e+00],
        [-2.0019e-03, -2.4562e-02, -4.0013e-01, -9.6502e-02, -1.9637e-03,
         -7.5447e-01, -1.8526e-01, -7.3105e-02, -1.9804e-01,  1.0220e-02,
         -1.9810e-03, -1.3015e-01, -2.2714e-01,  2.8482e-02],
        [-2.0019e-03, -2.4562e-02, -4.0013e-01, -9.6502e-02, -1.9636e-03,
         -7.5447e-01, -1.8526e-01, -7.3105e-02, -1.9804e-01,  1.0220e-02,
         -1.9810e-03, -1.3015e-01, -2.2714e-01,  2.8482e-02],
        [-2.0019e-03, -2.4561e-02, -4.0013e-01, -9.6502e-02, -1.9636e-03,
         -7.5447e-01, -1.8526e-01, -7.3105e-02, -1.9804e-01,  1.0220e-02,
         -1.9809e-03, -1.3015e-01, -2.2714e-01,  2.8482e-02],
        [-2.0019e-03, -2.4561e-02, -4.0013e-01, -9.6502e-02, -1.9636e-03,
         -7.5447e-01, -1.8526e-01, -7.3105e-02, -1.9804e-01,  1.0220e-02,
         -1.9809e-03, -1.3015e-01, -2.2714e-01,  2.8482e-02],
        [ 5.2420e-02, -2.5364e+00,  6.7778e+00, -3.1809e+00,  5.6607e-02,
         -4.5643e+00, -3.2607e+00,  6.5004e+00,  8.6576e+00, -1.7664e-01,
          5.3585e-02,  4.7895e+00, -2.2216e+00,  4.2428e+00],
        [-2.0019e-03, -2.4562e-02, -4.0013e-01, -9.6502e-02, -1.9636e-03,
         -7.5447e-01, -1.8526e-01, -7.3105e-02, -1.9804e-01,  1.0220e-02,
         -1.9810e-03, -1.3015e-01, -2.2714e-01,  2.8482e-02],
        [ 1.8902e-02,  2.7496e+00, -8.7820e+00,  3.0624e+00,  3.1128e-02,
          3.2500e+00,  3.0686e+00, -6.9989e+00, -9.0591e+00,  2.0603e-01,
          2.1792e-02, -4.9375e+00,  2.7138e+00, -7.6858e+00],
        [-2.0020e-03, -2.4562e-02, -4.0013e-01, -9.6502e-02, -1.9638e-03,
         -7.5446e-01, -1.8526e-01, -7.3105e-02, -1.9803e-01,  1.0219e-02,
         -1.9811e-03, -1.3015e-01, -2.2714e-01,  2.8482e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.5076, -1.5076, -1.5076,  0.8683, -1.5076, -1.2756, -1.5076, -1.5076,
        -1.5076, -1.5076, -1.6105, -1.5076,  0.4782, -1.5076], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0867,   0.0867,   0.0867,  11.2976,   0.0867, -10.9669,   0.0867,
           0.0867,   0.0867,   0.0867,  -6.1044,   0.0867,   7.6163,   0.0867]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  8.5033,  11.3650],
        [-10.5121,   5.5489],
        [ -1.3512,   0.2673],
        [ -1.3513,   0.2674],
        [-12.4290,  -4.7159],
        [  9.1855,  -0.8363],
        [ 12.0302,   1.1777],
        [ -5.1428,   2.8921],
        [ 12.5533,   0.4089],
        [  9.9981, -13.4814],
        [-12.3791,  -3.1758],
        [ -1.3513,   0.2674],
        [ -6.9591,  -8.7609],
        [ 10.8028,   6.8153]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  3.8622,   2.2683,  -3.8331,  -3.8339,  -3.4414, -10.4025,  -4.2090,
         -8.9572, -10.3991,  -5.1419,  -0.4534,  -3.8339,  -8.4566,  -1.3346],
       device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.8975e+00,  1.7424e-01, -1.8473e-01, -1.8447e-01, -6.2015e-01,
          1.4595e+00, -7.3995e-01,  9.2966e-01,  8.0675e-01, -5.7848e-01,
         -3.0858e-01, -1.8446e-01, -2.0975e+00, -5.2271e-01],
        [-2.9049e+00,  1.7955e-01, -1.8470e-01, -1.8439e-01, -6.3698e-01,
          1.4775e+00, -7.2484e-01,  9.3166e-01,  8.2778e-01, -5.5813e-01,
         -3.1222e-01, -1.8439e-01, -2.1129e+00, -5.1440e-01],
        [-4.4682e+00, -5.4629e+00,  4.4731e-02,  4.5198e-02, -1.4272e+00,
          1.9810e+00,  1.0381e+00,  3.2499e+00,  7.8567e+00,  4.0033e+00,
         -2.0654e+00,  4.5203e-02, -3.6073e+00,  1.8091e+00],
        [-1.0155e+01, -4.3047e+00,  1.6656e-01,  1.6886e-01,  2.5881e+00,
         -6.2992e+00, -5.9328e+00,  7.7973e-03, -1.3816e+01,  3.9605e+00,
          5.6280e+00,  1.6889e-01,  5.7009e+00, -5.5699e+00],
        [ 1.3585e+00,  5.3566e+00, -7.6669e-02, -7.8672e-02, -1.6131e+00,
         -2.5677e+00, -1.4413e+00, -4.9653e+00, -6.2510e+00, -3.8257e+00,
          2.8083e+00, -7.8692e-02, -2.4603e+00, -1.2337e+00],
        [ 2.6161e+00,  4.1939e+00, -1.4725e-01, -1.4773e-01,  9.0343e+00,
          1.6358e-01, -5.4973e+00,  5.7769e-01, -1.7631e+00, -7.7338e+00,
          3.5930e+00, -1.4773e-01,  1.1040e+01,  2.0267e-01],
        [-2.9250e+00,  1.3963e-01, -2.0317e-01, -2.0306e-01, -2.7731e-01,
          1.2783e+00, -8.1136e-01,  9.0652e-01,  7.2499e-01, -6.5530e-01,
          2.0985e-01, -2.0306e-01, -2.0241e+00, -5.8244e-01],
        [-2.9131e+00,  1.6517e-01, -1.8929e-01, -1.8907e-01, -5.5638e-01,
          1.4147e+00, -7.5277e-01,  9.2446e-01,  7.9693e-01, -5.9105e-01,
         -1.4960e-01, -1.8906e-01, -2.0939e+00, -5.3817e-01],
        [-2.9248e+00,  1.1647e-01, -2.2019e-01, -2.2013e-01,  4.4668e-01,
          1.1579e+00, -8.6575e-01,  8.9205e-01,  6.5050e-01, -7.0247e-01,
          6.3012e-01, -2.2013e-01, -1.8627e+00, -6.2255e-01],
        [-1.1846e+01, -2.5143e+00,  9.2169e-02,  9.0949e-02,  5.3208e+00,
         -3.0355e+00, -4.5474e+00,  1.7764e-03, -4.5594e+00,  7.2779e+00,
          5.8383e+00,  9.0935e-02,  7.0966e-01, -4.3105e+00],
        [-2.9265e+00,  1.4005e-01, -2.0351e-01, -2.0341e-01, -2.7855e-01,
          1.2808e+00, -8.0925e-01,  9.0776e-01,  7.2685e-01, -6.5219e-01,
          2.1354e-01, -2.0340e-01, -2.0270e+00, -5.8081e-01],
        [-2.1011e+00,  9.3641e-01, -2.6803e-01, -2.6870e-01, -2.4947e+00,
          1.8335e+00, -2.4227e+00,  1.6468e+00,  3.5792e+00,  4.7116e-01,
         -1.1401e+00, -2.6870e-01,  2.1229e+00, -1.2976e+00],
        [ 8.2257e-01,  3.1160e+00,  6.4431e-02,  6.4063e-02,  5.4282e+00,
         -1.5950e+00, -8.2627e-01, -1.1248e+00, -7.2612e+00, -8.0243e+00,
          4.9717e+00,  6.4057e-02,  5.1946e+00, -4.4675e-01],
        [-2.9191e+00,  1.5705e-01, -1.9290e-01, -1.9272e-01, -4.9386e-01,
          1.3749e+00, -7.6917e-01,  9.1941e-01,  7.7790e-01, -6.0958e-01,
         -4.2559e-02, -1.9272e-01, -2.0797e+00, -5.5128e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.8111, -2.7974, -0.7722, -0.7840,  1.3418, -6.8413, -2.8363, -2.8115,
        -2.8554,  0.0893, -2.8342, -4.0119, -1.1539, -2.8181], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.8609,  -0.8730,   0.0805,  -0.2946,   1.8756,   9.0644,  -0.8438,
          -0.8625,  -0.8291,  -4.2983,  -0.8465,  -4.2507,   1.7366,  -0.8576],
        [  0.8933,   0.9054,   1.4179,   0.8653,   0.7282,  -8.2955,   0.8773,
           0.8951,   0.8634,   4.3878,   0.8799,   4.4892,  -0.5154,   0.8904],
        [ -0.0614,  -0.0633, -13.9338,  -0.6509, -12.8509,  -0.7916,  -0.0542,
          -0.0597,  -0.0557,  -0.7720,  -0.0545,  -0.0366,  -1.4431,  -0.0578],
        [ -0.0367,  -0.0387, -14.2624,   0.4385, -13.0166,   5.0706,  -0.0170,
          -0.0320,   0.0184,  -0.4000,  -0.0172,  -0.0280,   1.7596,  -0.0279],
        [  0.6967,   0.7114,   0.0976,  -6.3834,   0.5845,   3.7481,   0.6192,
           0.6831,   0.5564,   0.1106,   0.6209,   1.8165,   0.3097,   0.6655],
        [ -0.6867,  -0.7009,   1.3434,   6.2709,   1.5523, -10.6809,  -0.6067,
          -0.6718,  -0.5430,   0.7554,  -0.6083,  -0.3801,   0.3063,  -0.6538],
        [ -0.2025,  -0.1998,  10.2899,  -6.9879,   0.9484,  -5.2112,  -0.4969,
          -0.2980,  -0.7245,   9.9450,  -0.4986,  -0.1661,  -2.8260,  -0.3592]],
       device='cuda:0'))])
xi:  [633.4675]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 885.3255875048924
W_T_median: 835.2039481983675
W_T_pctile_5: 633.4448531942614
W_T_CVAR_5_pct: 368.78586638226
Average q (qsum/M+1):  52.416527532762096
Optimal xi:  [633.4675]
Expected(across Rb) median(across samples) p_equity:  0.19182680498591556
obj fun:  tensor(-1993.6988, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: 3factor_mc
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
