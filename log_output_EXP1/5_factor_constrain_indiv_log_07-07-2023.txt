Starting at: 
07-07-23_16:21

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = 5_Factor_plusEWD
timeseries_basket['basket_desc'] = 5_Factor_plusEWD
timeseries_basket['basket_columns'] = 
['Size_Lo30_real_ret', 'Value_Hi30_real_ret', 'Mom_Hi30_real_ret', 'Vol_Lo20_real_ret', 'Div_Hi30_real_ret', 'T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'EWD_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = 5_Factor_plusEWD
timeseries_basket['basket_desc'] = 5_Factor_plusEWD
timeseries_basket['basket_columns'] = 
['Size_Lo30_nom_ret', 'Value_Hi30_nom_ret', 'Mom_Hi30_nom_ret', 'Vol_Lo20_nom_ret', 'Div_Hi30_nom_ret', 'T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'EWD_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.031411     0.013051
192608                    0.0319              0.0561  ...     0.028647     0.031002
192609                   -0.0173             -0.0071  ...     0.005787    -0.006499
192610                   -0.0294             -0.0355  ...    -0.028996    -0.034630
192611                   -0.0038              0.0294  ...     0.028554     0.024776

[5 rows x 10 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.036240    -0.011556
202209                   -0.0955             -0.0871  ...    -0.091324    -0.099903
202210                    0.0883              0.1486  ...     0.077403     0.049863
202211                   -0.0076              0.0462  ...     0.052365     0.028123
202212                   -0.0457             -0.0499  ...    -0.057116    -0.047241

[5 rows x 10 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'Mom_Hi30_nom_ret_ind', 'Vol_Lo20_nom_ret_ind', 'Div_Hi30_nom_ret_ind',
       'CPI_nom_ret_ind', 'T30_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind', 'EWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
Mom_Hi30_real_ret      0.011386
Vol_Lo20_real_ret      0.003529
Div_Hi30_real_ret      0.007888
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
EWD_real_ret           0.009545
dtype: float64


timeseries_basket['data_df_stdev'] = 
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
Mom_Hi30_real_ret      0.061421
Vol_Lo20_real_ret      0.030737
Div_Hi30_real_ret      0.056728
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
EWD_real_ret           0.071360
dtype: float64


timeseries_basket['data_df_corr'] = 
                     Size_Lo30_real_ret  ...  EWD_real_ret
Size_Lo30_real_ret             1.000000  ...      0.977206
Value_Hi30_real_ret            0.908542  ...      0.919912
Mom_Hi30_real_ret              0.903222  ...      0.912002
Vol_Lo20_real_ret              0.360014  ...      0.382411
Div_Hi30_real_ret              0.816292  ...      0.849068
T30_real_ret                   0.014412  ...      0.029084
B10_real_ret                   0.012916  ...      0.024853
VWD_real_ret                   0.865290  ...      0.907369
EWD_real_ret                   0.977206  ...      1.000000

[9 rows x 9 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 202212
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      17  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      17  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 17)     True          17  
2     (17, 17)     True          17  
3      (17, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       9       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      17  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      17  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       9              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 17)     True          17  
2     (17, 17)     True          17  
3      (17, 9)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       17  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       17  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 17)      True          17  
0     (17, 17)      True          17  
0      (17, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       17  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       17  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        9              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 17)      True          17  
0     (17, 17)      True          17  
0      (17, 9)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       17  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       17  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 17)      True          17  
0     (17, 17)      True          17  
0      (17, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       17  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       17  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        9              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 17)      True          17  
0     (17, 17)      True          17  
0      (17, 9)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.05, 0.05, 0.05, 0.05, 0.05, 0, 0.5, 0.25, 0]
W_T_mean: 5354.986675536277
W_T_median: 3311.3440043263445
W_T_pctile_5: 34.87264995404438
W_T_CVAR_5_pct: -295.5643377607271
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1675.4792631656621
Current xi:  [105.573616]
objective value function right now is: -1675.4792631656621
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1709.464406737709
Current xi:  [120.508]
objective value function right now is: -1709.464406737709
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1738.2550674546856
Current xi:  [142.0532]
objective value function right now is: -1738.2550674546856
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1747.8137949080356
Current xi:  [162.8708]
objective value function right now is: -1747.8137949080356
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1756.4636557651927
Current xi:  [181.69691]
objective value function right now is: -1756.4636557651927
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1779.5029880383395
Current xi:  [200.18428]
objective value function right now is: -1779.5029880383395
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [210.44534]
objective value function right now is: -1740.4863272634402
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [220.69276]
objective value function right now is: -1746.3197242947151
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [229.95383]
objective value function right now is: -1747.8369736421614
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [237.85913]
objective value function right now is: -1750.7151444160677
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [244.71744]
objective value function right now is: -1739.4179403072987
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [251.71683]
objective value function right now is: -1751.6634013396238
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [257.16022]
objective value function right now is: -1739.3862229797805
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [261.9744]
objective value function right now is: -1749.2326670181133
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1786.920632401612
Current xi:  [276.43442]
objective value function right now is: -1786.920632401612
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1798.6726687525245
Current xi:  [292.17487]
objective value function right now is: -1798.6726687525245
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1802.7273385156993
Current xi:  [306.40494]
objective value function right now is: -1802.7273385156993
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [319.98444]
objective value function right now is: -1799.7864530570614
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1805.5426311390977
Current xi:  [331.96628]
objective value function right now is: -1805.5426311390977
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1807.7370718079176
Current xi:  [343.5273]
objective value function right now is: -1807.7370718079176
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1809.0024074026878
Current xi:  [354.73026]
objective value function right now is: -1809.0024074026878
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1816.1823388535593
Current xi:  [364.7971]
objective value function right now is: -1816.1823388535593
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1817.7799452147974
Current xi:  [373.5374]
objective value function right now is: -1817.7799452147974
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [380.58026]
objective value function right now is: -1807.7513612532296
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [388.227]
objective value function right now is: -1816.6955202193158
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [395.6728]
objective value function right now is: -1815.6153033420253
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1820.137232081966
Current xi:  [402.0993]
objective value function right now is: -1820.137232081966
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1820.8784565562707
Current xi:  [407.19208]
objective value function right now is: -1820.8784565562707
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [411.98166]
objective value function right now is: -1820.3666560365807
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [415.2348]
objective value function right now is: -1810.7401628481375
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [416.81903]
objective value function right now is: -1816.8551889879418
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [420.02255]
objective value function right now is: -1815.8564463726552
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1822.6938186367513
Current xi:  [423.08582]
objective value function right now is: -1822.6938186367513
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [425.56946]
objective value function right now is: -1821.1416022232534
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [427.3155]
objective value function right now is: -1818.444912212883
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1825.5239119307384
Current xi:  [427.56207]
objective value function right now is: -1825.5239119307384
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [428.06378]
objective value function right now is: -1824.7108686472686
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1826.8765298247888
Current xi:  [428.79602]
objective value function right now is: -1826.8765298247888
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [429.28778]
objective value function right now is: -1825.1890499560309
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [429.6969]
objective value function right now is: -1824.9951111653945
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [429.99368]
objective value function right now is: -1825.1020363146733
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [430.4135]
objective value function right now is: -1824.5472960091483
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1828.8502668509138
Current xi:  [431.1497]
objective value function right now is: -1828.8502668509138
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1829.5401540958815
Current xi:  [432.07217]
objective value function right now is: -1829.5401540958815
new min fval from sgd:  -1829.982376584721
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [432.40628]
objective value function right now is: -1829.982376584721
new min fval from sgd:  -1830.0917856646963
new min fval from sgd:  -1830.3085391511752
new min fval from sgd:  -1830.3916472514798
new min fval from sgd:  -1830.4136871556325
new min fval from sgd:  -1830.4177784411709
new min fval from sgd:  -1830.4880128684795
new min fval from sgd:  -1830.5061768332273
new min fval from sgd:  -1830.5597601776533
new min fval from sgd:  -1830.724379796101
new min fval from sgd:  -1830.7249780886752
new min fval from sgd:  -1830.7832597220997
new min fval from sgd:  -1830.8783541912715
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [432.7627]
objective value function right now is: -1830.6369717748496
new min fval from sgd:  -1830.8806602846137
new min fval from sgd:  -1830.9032078530015
new min fval from sgd:  -1830.9447299026554
new min fval from sgd:  -1831.0354099378665
new min fval from sgd:  -1831.1066364162589
new min fval from sgd:  -1831.1952119572754
new min fval from sgd:  -1831.2870562707346
new min fval from sgd:  -1831.3377527958094
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [433.28955]
objective value function right now is: -1827.966162043518
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [434.00922]
objective value function right now is: -1829.6479308696444
new min fval from sgd:  -1831.3422590690718
new min fval from sgd:  -1831.3437499089432
new min fval from sgd:  -1831.3442887226906
new min fval from sgd:  -1831.3610533422568
new min fval from sgd:  -1831.3689551487594
new min fval from sgd:  -1831.3868076874908
new min fval from sgd:  -1831.4068680067771
new min fval from sgd:  -1831.429586237803
new min fval from sgd:  -1831.4514250675272
new min fval from sgd:  -1831.4692739779555
new min fval from sgd:  -1831.481322104249
new min fval from sgd:  -1831.4928133717908
new min fval from sgd:  -1831.5097311752343
new min fval from sgd:  -1831.52734706081
new min fval from sgd:  -1831.540684438219
new min fval from sgd:  -1831.549389839666
new min fval from sgd:  -1831.5562776036504
new min fval from sgd:  -1831.5725497704254
new min fval from sgd:  -1831.5730140408234
new min fval from sgd:  -1831.5827324391532
new min fval from sgd:  -1831.6008988233818
new min fval from sgd:  -1831.616877911483
new min fval from sgd:  -1831.6294182947988
new min fval from sgd:  -1831.6343117641113
new min fval from sgd:  -1831.6571020753713
new min fval from sgd:  -1831.6778384996085
new min fval from sgd:  -1831.6831192471548
new min fval from sgd:  -1831.6947538668444
new min fval from sgd:  -1831.6983556575017
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [434.34338]
objective value function right now is: -1831.4376879410725
new min fval from sgd:  -1831.6997968000749
new min fval from sgd:  -1831.7096341667443
new min fval from sgd:  -1831.710092010815
new min fval from sgd:  -1831.713260609493
new min fval from sgd:  -1831.7252843565284
new min fval from sgd:  -1831.7313405638188
new min fval from sgd:  -1831.7529510357394
new min fval from sgd:  -1831.7648961935108
new min fval from sgd:  -1831.7745676374886
new min fval from sgd:  -1831.7773105593658
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [434.44125]
objective value function right now is: -1831.5551520658573
min fval:  -1831.7773105593658
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -8.8681,   6.8105],
        [  6.8839,  -4.3926],
        [ -6.5586,   4.2847],
        [  7.1713,  -1.6796],
        [ 25.1825,   5.2798],
        [ -1.0229,   1.0002],
        [  6.6666,  -3.1742],
        [  9.1696,   7.4222],
        [  7.7882,  -5.6922],
        [  6.5383,   3.0290],
        [  6.9496,  -4.4010],
        [-16.3306,  -6.0318],
        [ -1.5512,   1.2879],
        [  6.3022,   4.9589],
        [  6.6159,   4.0027],
        [ -9.0058,   6.4589],
        [  6.9907,  -3.6899]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 4.5711, -4.3168, -1.9184, -5.1799,  4.0974, -3.6766, -4.2767,  3.8448,
        -4.5544, -7.1425, -4.3901, -2.4287, -3.4790, -9.0257, -8.0519,  3.7022,
        -4.5497], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 7.5022e-01,  6.2844e-01,  1.6806e-02,  1.5068e-01,  1.6150e+00,
          2.1301e-02,  4.0176e-01,  8.0360e-01,  8.8735e-01,  8.2506e-02,
          6.0560e-01,  1.3384e+00,  2.1418e-02,  4.5259e-02,  7.1334e-02,
          4.7143e-01,  4.1765e-01],
        [-6.7225e+00,  4.9282e+00, -7.3842e-01,  3.3917e+00, -1.0587e+01,
         -1.4546e-01,  3.5468e+00, -1.3256e+01,  6.2921e+00,  4.4076e+00,
          4.3520e+00,  5.9428e+00, -1.0410e-01,  2.0666e+01,  7.6950e+00,
         -5.5486e+00,  3.8882e+00],
        [-2.1315e-01, -1.5972e-01,  2.6467e-03, -7.7545e-02, -4.8542e-01,
         -1.3476e-03, -9.5683e-02, -1.7272e-01, -2.7348e-01, -6.5948e-02,
         -1.5164e-01, -5.2924e-01, -6.4729e-04, -4.2995e-02, -5.7137e-02,
         -1.0954e-01, -9.7205e-02],
        [-7.1074e+00,  4.8802e+00, -8.0058e-01,  3.5974e+00, -1.1122e+01,
         -2.5772e-02,  3.4618e+00, -1.3536e+01,  6.7721e+00,  4.9860e+00,
          4.6669e+00,  6.2550e+00, -1.1627e-01,  2.1375e+01,  8.0302e+00,
         -5.8505e+00,  3.9818e+00],
        [ 7.5252e-01,  6.3015e-01,  1.6861e-02,  1.5100e-01,  1.6199e+00,
          2.1383e-02,  4.0282e-01,  8.0564e-01,  8.8991e-01,  8.2773e-02,
          6.0724e-01,  1.3434e+00,  2.1500e-02,  4.5416e-02,  7.1576e-02,
          4.7268e-01,  4.1874e-01],
        [-2.1315e-01, -1.5972e-01,  2.6467e-03, -7.7545e-02, -4.8542e-01,
         -1.3476e-03, -9.5683e-02, -1.7272e-01, -2.7348e-01, -6.5948e-02,
         -1.5164e-01, -5.2924e-01, -6.4729e-04, -4.2995e-02, -5.7137e-02,
         -1.0954e-01, -9.7205e-02],
        [-9.4536e+00,  5.1187e+00,  1.6941e-01,  3.9501e+00, -1.0189e+01,
          1.9401e-01,  4.0591e+00, -1.3723e+01,  7.4981e+00,  8.6341e+00,
          5.0515e+00,  7.6170e+00,  2.0633e-01,  1.5056e+01,  1.5259e+01,
         -8.2363e+00,  4.7254e+00],
        [ 7.4586e-01,  6.2522e-01,  1.6702e-02,  1.5008e-01,  1.6059e+00,
          2.1146e-02,  3.9978e-01,  7.9973e-01,  8.8251e-01,  8.2000e-02,
          6.0250e-01,  1.3289e+00,  2.1265e-02,  4.4954e-02,  7.0874e-02,
          4.6904e-01,  4.1559e-01],
        [-2.1315e-01, -1.5972e-01,  2.6467e-03, -7.7545e-02, -4.8542e-01,
         -1.3476e-03, -9.5683e-02, -1.7272e-01, -2.7348e-01, -6.5948e-02,
         -1.5164e-01, -5.2924e-01, -6.4729e-04, -4.2995e-02, -5.7137e-02,
         -1.0954e-01, -9.7205e-02],
        [-2.1315e-01, -1.5972e-01,  2.6467e-03, -7.7545e-02, -4.8542e-01,
         -1.3476e-03, -9.5683e-02, -1.7272e-01, -2.7348e-01, -6.5948e-02,
         -1.5164e-01, -5.2924e-01, -6.4729e-04, -4.2995e-02, -5.7137e-02,
         -1.0954e-01, -9.7205e-02],
        [ 7.5319e-01,  6.3064e-01,  1.6877e-02,  1.5109e-01,  1.6213e+00,
          2.1407e-02,  4.0311e-01,  8.0623e-01,  8.9065e-01,  8.2856e-02,
          6.0771e-01,  1.3449e+00,  2.1524e-02,  4.5469e-02,  7.1652e-02,
          4.7305e-01,  4.1905e-01],
        [ 7.4345e-01,  6.2343e-01,  1.6645e-02,  1.4974e-01,  1.6009e+00,
          2.1061e-02,  3.9868e-01,  7.9759e-01,  8.7983e-01,  8.1724e-02,
          6.0079e-01,  1.3237e+00,  2.1180e-02,  4.4794e-02,  7.0626e-02,
          4.6772e-01,  4.1445e-01],
        [-2.1315e-01, -1.5972e-01,  2.6467e-03, -7.7545e-02, -4.8542e-01,
         -1.3476e-03, -9.5683e-02, -1.7272e-01, -2.7348e-01, -6.5948e-02,
         -1.5164e-01, -5.2924e-01, -6.4730e-04, -4.2995e-02, -5.7137e-02,
         -1.0954e-01, -9.7205e-02],
        [-2.1315e-01, -1.5972e-01,  2.6467e-03, -7.7545e-02, -4.8542e-01,
         -1.3476e-03, -9.5683e-02, -1.7272e-01, -2.7348e-01, -6.5948e-02,
         -1.5164e-01, -5.2924e-01, -6.4729e-04, -4.2995e-02, -5.7137e-02,
         -1.0954e-01, -9.7205e-02],
        [ 7.4452e-01,  6.2422e-01,  1.6671e-02,  1.4988e-01,  1.6031e+00,
          2.1099e-02,  3.9916e-01,  7.9854e-01,  8.8102e-01,  8.1845e-02,
          6.0154e-01,  1.3260e+00,  2.1218e-02,  4.4864e-02,  7.0735e-02,
          4.6831e-01,  4.1495e-01],
        [-6.4263e+00,  4.2372e+00, -3.5101e-01,  2.6779e+00, -9.2234e+00,
         -8.6173e-02,  3.0202e+00, -1.2101e+01,  5.5940e+00,  3.5648e+00,
          3.8290e+00,  5.3966e+00,  8.1847e-03,  1.7304e+01,  6.3954e+00,
         -5.5550e+00,  3.4885e+00],
        [-2.1315e-01, -1.5972e-01,  2.6467e-03, -7.7545e-02, -4.8542e-01,
         -1.3476e-03, -9.5683e-02, -1.7272e-01, -2.7348e-01, -6.5948e-02,
         -1.5164e-01, -5.2924e-01, -6.4730e-04, -4.2995e-02, -5.7137e-02,
         -1.0954e-01, -9.7205e-02]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 2.1688,  1.4679, -0.8022,  1.5631,  2.1758, -0.8022,  0.8314,  2.1554,
        -0.8022, -0.8022,  2.1779,  2.1480, -0.8022, -0.8022,  2.1513,  1.4251,
        -0.8022], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 4.6964e+00, -1.7196e+01,  1.6726e-02, -2.0632e+01,  4.7950e+00,
          1.6726e-02, -2.2584e+01,  4.5151e+00,  1.6726e-02,  1.6726e-02,
          4.8238e+00,  4.4182e+00,  1.6726e-02,  1.6726e-02,  4.4610e+00,
         -1.1895e+01,  1.6726e-02]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-11.8093,  -1.8651],
        [ -7.8426,   5.9341],
        [  5.8497,   2.2517],
        [ -9.7422,  -3.1751],
        [  5.7686,   2.1915],
        [ -0.5920,   2.5681],
        [-10.6474,  -1.2246],
        [ 10.1358,   4.2854],
        [-10.4323,  -4.5364],
        [ -0.3527,   9.6768],
        [-10.9050,  -3.7218],
        [ -0.2826,   2.5052],
        [  4.1891,  10.6591],
        [ -6.4819,  -2.3548],
        [-11.5481,  -3.8011],
        [ 10.5269,  -0.0187],
        [ -5.1306,   8.4596]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 4.2323,  3.3210,  6.0701, -0.9482,  5.9927, -3.7002,  6.0750, -3.0475,
        -4.6739,  6.8180, -1.8969, -3.8819,  7.5061, -4.0795, -1.5117, -9.3873,
         6.6869], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-8.5738e-01, -2.3058e-01, -1.5176e+00, -5.4615e-01, -1.5174e+00,
         -1.3799e-01, -3.4944e-01, -6.5342e-01, -7.0464e-02, -1.1356e+00,
         -3.9319e-01, -1.3276e-01, -1.3539e+00, -3.1032e-02, -4.7148e-01,
         -6.5022e-01, -2.6596e-01],
        [-8.5804e-01, -2.3074e-01, -1.5167e+00, -5.4692e-01, -1.5165e+00,
         -1.3780e-01, -3.4785e-01, -6.5379e-01, -7.0547e-02, -1.1370e+00,
         -3.9386e-01, -1.3257e-01, -1.3549e+00, -3.1057e-02, -4.7228e-01,
         -6.5081e-01, -2.6707e-01],
        [-8.5747e-01, -2.3052e-01, -1.5176e+00, -5.4605e-01, -1.5174e+00,
         -1.3790e-01, -3.4960e-01, -6.5371e-01, -7.0440e-02, -1.1354e+00,
         -3.9309e-01, -1.3267e-01, -1.3539e+00, -3.1032e-02, -4.7136e-01,
         -6.5029e-01, -2.6615e-01],
        [-5.6392e+00,  4.7225e+00,  3.4846e+00, -6.9326e+00,  3.5338e+00,
          1.3071e-02, -4.8161e+00,  2.4344e+00, -2.7351e+00,  2.0999e+01,
         -6.0648e+00,  2.1947e-02,  2.1192e+01, -1.7658e+00, -9.0481e+00,
          2.8098e+00,  1.2919e+01],
        [-8.5747e-01, -2.3052e-01, -1.5176e+00, -5.4604e-01, -1.5174e+00,
         -1.3790e-01, -3.4961e-01, -6.5371e-01, -7.0440e-02, -1.1354e+00,
         -3.9309e-01, -1.3267e-01, -1.3538e+00, -3.1032e-02, -4.7135e-01,
         -6.5028e-01, -2.6614e-01],
        [ 3.8425e+00,  4.4053e+00, -2.6080e+00, -1.5689e-01, -2.8475e+00,
          1.2414e+00,  2.3388e+00, -7.3870e+00, -6.4293e+00,  2.8966e+00,
         -3.2157e+00,  1.1558e+00,  1.6278e+00, -1.1480e+00, -1.6672e+00,
         -1.1658e+01,  1.1826e+01],
        [-8.5748e-01, -2.3053e-01, -1.5176e+00, -5.4609e-01, -1.5174e+00,
         -1.3790e-01, -3.4953e-01, -6.5369e-01, -7.0446e-02, -1.1355e+00,
         -3.9313e-01, -1.3267e-01, -1.3539e+00, -3.1033e-02, -4.7140e-01,
         -6.5030e-01, -2.6617e-01],
        [-9.2326e+00,  2.0267e+00, -3.4962e-01, -2.7195e+00, -2.8629e-01,
          1.5037e+00, -7.7029e+00,  2.6809e+00, -8.8484e-02,  8.0338e-01,
         -8.8741e-01,  1.4355e+00,  8.3564e-02, -3.9000e-01, -8.5990e-01,
          2.8718e+00, -9.8453e+00],
        [-2.3190e+00,  9.4171e-01,  2.0023e+00, -1.2445e+00,  1.9521e+00,
          7.5992e-01,  3.8850e-01,  7.5042e-01, -1.1169e+00, -3.5914e-01,
         -9.8540e-01,  7.7263e-01,  2.5409e-01, -7.5469e-01, -1.0889e+00,
          8.1464e-01,  1.2097e+00],
        [ 2.8412e+00, -1.6057e+00, -5.1148e+00,  5.8575e+00, -5.0170e+00,
         -4.4284e-01,  3.8632e+00, -1.4425e+01,  2.7584e+00, -1.1536e+01,
          5.9315e+00, -4.8364e-01, -1.9072e+01,  1.6362e+00,  7.8139e+00,
         -6.8053e+00, -5.0446e+00],
        [-8.5752e-01, -2.3053e-01, -1.5175e+00, -5.4610e-01, -1.5173e+00,
         -1.3789e-01, -3.4948e-01, -6.5374e-01, -7.0446e-02, -1.1355e+00,
         -3.9314e-01, -1.3265e-01, -1.3539e+00, -3.1034e-02, -4.7141e-01,
         -6.5032e-01, -2.6623e-01],
        [-8.5749e-01, -2.3053e-01, -1.5176e+00, -5.4608e-01, -1.5174e+00,
         -1.3790e-01, -3.4953e-01, -6.5371e-01, -7.0444e-02, -1.1354e+00,
         -3.9312e-01, -1.3267e-01, -1.3539e+00, -3.1033e-02, -4.7139e-01,
         -6.5030e-01, -2.6618e-01],
        [-8.5768e-01, -2.3066e-01, -1.5172e+00, -5.4650e-01, -1.5169e+00,
         -1.3791e-01, -3.4874e-01, -6.5358e-01, -7.0503e-02, -1.1363e+00,
         -3.9350e-01, -1.3268e-01, -1.3544e+00, -3.1043e-02, -4.7184e-01,
         -6.5053e-01, -2.6649e-01],
        [-8.5907e-01, -2.3129e-01, -1.5147e+00, -5.4882e-01, -1.5145e+00,
         -1.3775e-01, -3.4420e-01, -6.5350e-01, -7.0804e-02, -1.1411e+00,
         -3.9559e-01, -1.3250e-01, -1.3571e+00, -3.1105e-02, -4.7434e-01,
         -6.5204e-01, -2.6882e-01],
        [-8.5750e-01, -2.3061e-01, -1.5174e+00, -5.4628e-01, -1.5172e+00,
         -1.3795e-01, -3.4920e-01, -6.5350e-01, -7.0478e-02, -1.1359e+00,
         -3.9330e-01, -1.3272e-01, -1.3541e+00, -3.1036e-02, -4.7161e-01,
         -6.5036e-01, -2.6619e-01],
        [-8.5750e-01, -2.3056e-01, -1.5175e+00, -5.4617e-01, -1.5173e+00,
         -1.3792e-01, -3.4938e-01, -6.5362e-01, -7.0459e-02, -1.1356e+00,
         -3.9320e-01, -1.3269e-01, -1.3540e+00, -3.1035e-02, -4.7149e-01,
         -6.5034e-01, -2.6620e-01],
        [-8.5753e-01, -2.3068e-01, -1.5172e+00, -5.4647e-01, -1.5170e+00,
         -1.3799e-01, -3.4884e-01, -6.5335e-01, -7.0508e-02, -1.1363e+00,
         -3.9348e-01, -1.3276e-01, -1.3542e+00, -3.1040e-02, -4.7182e-01,
         -6.5040e-01, -2.6622e-01]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.5204, -1.5196, -1.5205, -5.4345, -1.5205, -5.1913, -1.5204, -1.8349,
        -0.1439,  2.5138, -1.5204, -1.5204, -1.5200, -1.5176, -1.5203, -1.5204,
        -1.5201], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.6438e-02, -1.6507e-02, -1.6432e-02, -3.9004e+00, -1.6432e-02,
         -1.9982e+00, -1.6436e-02, -7.7636e-02, -7.9382e+00, -3.0406e+00,
         -1.6437e-02, -1.6435e-02, -1.6471e-02, -1.6676e-02, -1.6451e-02,
         -1.6443e-02, -1.6466e-02],
        [ 2.2024e-01,  2.2045e-01,  2.2021e-01, -4.3339e+00,  2.2021e-01,
         -9.4564e+00,  2.2022e-01,  3.4160e+00,  1.3172e+01,  9.8064e+00,
          2.2023e-01,  2.2022e-01,  2.2034e-01,  2.2103e-01,  2.2028e-01,
          2.2025e-01,  2.2034e-01],
        [-6.6681e-02, -6.6852e-02, -6.6677e-02,  2.7254e-01, -6.6675e-02,
          1.7590e+01, -6.6683e-02,  1.4626e-02,  1.5919e+01, -7.6507e+00,
         -6.6689e-02, -6.6683e-02, -6.6764e-02, -6.7246e-02, -6.6714e-02,
         -6.6698e-02, -6.6743e-02],
        [-2.2043e-01, -2.2114e-01, -2.2039e-01, -6.8913e+00, -2.2038e-01,
         -1.3137e+01, -2.2041e-01,  1.5041e+00,  1.4983e+01, -2.3886e+01,
         -2.2044e-01, -2.2041e-01, -2.2076e-01, -2.2281e-01, -2.2056e-01,
         -2.2048e-01, -2.2070e-01],
        [ 1.2054e-01,  1.2072e-01,  1.2052e-01, -2.9027e+00,  1.2051e-01,
         -8.8589e+00,  1.2052e-01,  2.5322e+00,  1.1874e+01,  1.1804e+01,
          1.2053e-01,  1.2052e-01,  1.2062e-01,  1.2118e-01,  1.2057e-01,
          1.2055e-01,  1.2062e-01],
        [-1.5258e+00, -1.5287e+00, -1.5267e+00,  4.8380e+00, -1.5267e+00,
          4.4843e+00, -1.5267e+00,  1.1441e+00, -2.9734e+00, -2.1373e+01,
         -1.5269e+00, -1.5268e+00, -1.5272e+00, -1.5321e+00, -1.5264e+00,
         -1.5267e+00, -1.5261e+00],
        [ 3.3165e-01,  3.3061e-01,  3.3022e-01, -4.7450e-02,  3.3022e-01,
         -7.4881e-01,  3.3037e-01,  4.3485e-01,  3.3556e+00, -4.2367e+00,
          3.3017e-01,  3.3025e-01,  3.3112e-01,  3.3253e-01,  3.3133e-01,
          3.3067e-01,  3.3227e-01],
        [-1.5262e-02, -1.5330e-02, -1.5255e-02, -3.1856e+00, -1.5254e-02,
         -8.0260e-01, -1.5258e-02, -1.2650e-01, -7.0987e+00, -2.9554e-01,
         -1.5260e-02, -1.5258e-02, -1.5294e-02, -1.5500e-02, -1.5274e-02,
         -1.5265e-02, -1.5290e-02],
        [ 2.3175e-01,  2.3060e-01,  2.3033e-01, -9.8386e-01,  2.3033e-01,
         -4.5920e+00,  2.3047e-01, -1.7315e-01,  3.1017e+00,  1.1570e+01,
          2.3026e-01,  2.3035e-01,  2.3117e-01,  2.3226e-01,  2.3141e-01,
          2.3076e-01,  2.3233e-01]], device='cuda:0'))])
xi:  [434.40906]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 680.9366916453475
W_T_median: 610.4600360611878
W_T_pctile_5: 434.9230742739952
W_T_CVAR_5_pct: 230.70495851107455
Average q (qsum/M+1):  51.64767357610887
Optimal xi:  [434.40906]
Expected(across Rb) median(across samples) p_equity:  0.06893593427278878
obj fun:  tensor(-1831.7773, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: 5_Factor_plusEWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
