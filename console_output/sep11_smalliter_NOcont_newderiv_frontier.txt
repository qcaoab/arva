Starting at: 
2022-09-11 22:24:03

 Random seed:  1  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = basic_T30_VWD
timeseries_basket['basket_desc'] = CRSP data: T30 and VWD
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
5.0% of MC simulations done.
10.0% of MC simulations done.
15.0% of MC simulations done.
20.0% of MC simulations done.
25.0% of MC simulations done.
30.0% of MC simulations done.
35.0% of MC simulations done.
40.0% of MC simulations done.
45.0% of MC simulations done.
50.0% of MC simulations done.
55.00000000000001% of MC simulations done.
60.0% of MC simulations done.
65.0% of MC simulations done.
70.0% of MC simulations done.
75.0% of MC simulations done.
80.0% of MC simulations done.
85.0% of MC simulations done.
90.0% of MC simulations done.
95.0% of MC simulations done.
100.0% of MC simulations done.
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  hidden_layer    None       None         None   
4        obj.layers[4]        4  hidden_layer    None       None         None   
5        obj.layers[5]        5  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  
4    False        None  
5    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  hidden_layer       4  logistic_sigmoid   
4        obj.layers[4]        4  hidden_layer       4  logistic_sigmoid   
5        obj.layers[5]        5  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 4)    False        None  
4       (4, 4)    False        None  
5       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [  0.87854119   1.28047768   0.80492869   1.81079033  -0.61778981
  -2.6443001   -1.10656003  -2.06091428  42.79359675  52.75264031
  97.79494131  68.21545733   1.17567369   3.84267234   2.53501517
   7.24132694 -23.37883686 -24.7957267   -5.94328966   6.96839638
  48.33561121  56.31648959  52.6651913   55.02386534   8.81016547
  61.81176387 -13.61556394  29.70412403  12.83589148  51.68048692
 -16.29523253  22.55541369  -0.91118819  17.12790488  -8.34810503
  26.64831418   8.04697657 -12.41158813   4.11366468  -6.46607084
   1.81975488 -30.73365862 -31.20620983 -27.89922946  12.44388252
  33.63244291  31.92101736  32.88679737  12.19761835 -28.57757412
 -28.82978604 -25.33570256   4.39881427  17.82244512  17.03341574
  17.30404476  -3.4797117   -2.93422218   2.35621446  -5.16071364
   2.3990635   -5.17121313   2.31838487  -5.16766962]
Minimum obj value:-1046.0315380385077
Optimal xi: 31.098315981074872
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1060.1896240829183
W_T_median: 1051.5345761169833
W_T_pctile_5: 967.576213261727
W_T_CVAR_5_pct: 940.0177333190285
F value: -1046.0315380385077
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.1
F value: -1046.0315380385077
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ -4.7665309    2.15721647 -10.51493398  -6.4036965    2.98799564
  -0.87286249  10.41540838   6.67156189  27.52595806  32.61638445
   1.85167911  28.5218022   -1.91174952 -14.2386921    0.87907625
 -16.89926533  58.26162296  23.78678735  19.28378826  24.4242036
  45.11822075  26.55498494  20.15292655  24.01494203  -4.29453979
   6.93036092 -10.64398831  12.32845552  -0.8982001   13.46869274
  -7.349098    23.50196243 -24.42400908 -13.47127565  -2.9293511
   5.88107907  -1.75195776  14.64062893  -7.83863222  23.14668465
 -35.81147164 -40.78981906 -41.85562883   4.28686714 -19.18096987
  -7.22859543  -8.19159863   4.18117126   6.98340488   0.25581122
   0.59870004   1.76125682  45.97423334  44.66809768  44.17371054
  -2.8685756    3.41395715  -3.76874442   1.65781083  -3.61712777
   1.87034174  -3.63119322   4.79916839   1.62392713]
Minimum obj value:-1208.2204925878668
Optimal xi: 31.084537107795526
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1075.3587920652694
W_T_median: 1051.8808526665234
W_T_pctile_5: 966.725534302689
W_T_CVAR_5_pct: 939.3870435553239
F value: -1208.2204925878668
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.25
F value: -1208.2204925878668
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [  4.03039351   5.96497599  -3.49190815   2.56942913  -4.84080727
  -4.2711592    3.29059674  -2.56149255  56.45108878  31.81560125
  56.05244335  75.51200695  56.12953077  29.72288607  54.22924262
  73.6739973   44.9969433   40.84214481  -1.05105412  -9.6013592
   6.01984616   1.82111277   8.97994652   6.5771421   -5.74097169
  -0.11938838  -7.14932268   9.04257043 -12.85285963  -2.46567804
 -23.40659505   2.86941465   1.5072478   26.9911717   22.6309841
   0.64278114  -2.38801857  36.10371801  24.11319534   0.33414657
 -24.28532557   3.72457      6.8382665    2.45168912 -41.82933313
 -11.9564279    1.71785388   2.88494272 -13.66651611   0.90943185
   4.1616682   11.292465    14.52547132  -1.50602316  -4.30407285
 -12.67590779   4.35023905  -3.62945269   0.78079841  -2.60700166
   5.74515269   1.25391652  -2.57167452   1.94838174]
Minimum obj value:-1373.147677258318
Optimal xi: 30.98853826048785
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1103.2558278796037
W_T_median: 1061.3089976113824
W_T_pctile_5: 960.9940292396622
W_T_CVAR_5_pct: 931.857612794376
F value: -1373.147677258318
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.4
F value: -1373.147677258318
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [  2.48709021  -1.76379175   4.70514571  -2.552409    -3.48760661
   3.07787636  -3.63623637   3.94327441   9.76939527   0.32516602
   1.98403315  33.03267013  15.37265011  44.3493772    3.23203389
  -4.24961814  64.44403773  25.96798093  22.01910638  19.29543134
   6.17080593  42.92102381  68.06805492  14.17992102   2.64193926
  24.92359537  27.37268046  -2.38782861  11.36880682   1.45045118
  24.57036243   5.37974574 -10.91526656  -1.65483238 -21.76581211
   8.97706049  -7.36081513   3.22765941  -2.77478342  50.48420764
   3.37573006  22.74745433  -4.40676484  10.02391422 -35.68490543
   2.21458182 -30.42392817   1.76955631   1.67217398   3.04962054
  -7.73481443  10.74705188   3.67935      1.62690187  18.3736724
  -3.15787063   1.55215406  -3.09175396  -9.3968755    3.34457129
   3.85943513  -3.03453472   4.69037757   1.37266752]
Minimum obj value:-1600.0383730066358
Optimal xi: 30.55584363055789
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1174.4924833955997
W_T_median: 1077.929610241517
W_T_pctile_5: 933.2585746445449
W_T_CVAR_5_pct: 895.3465876010464
F value: -1600.0383730066358
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.6
F value: -1600.0383730066358
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [  1.17650071  -1.3768505   -5.31972698  -2.95132907  -0.78539592
   1.18908241   4.99295189   3.19612058 -19.97906226  -4.83512139
   3.54027319  -2.44090028   4.47932768   9.728418    54.36744254
   4.76824252   4.79607609   9.56187273  50.03508304   3.77209912
  66.14875254  42.62781142   1.90835048  13.64397491  24.09988693
   5.80324216  63.01716242  20.78584737   1.66705596  30.60785748
   1.96271514  10.47634306  -7.081234     9.84145621   0.10993141
   7.68181829  -3.09730982   1.82007863  14.72212684  45.96279078
 -17.42557237  -4.21168115   3.3372184   10.66037774  -1.20842319
  15.53610657  26.97354728  36.28428717 -35.96601889   4.11663306
   1.89768954   3.84119887  -6.05012693  10.62049439   4.71333208
  25.98072979   3.80736963  -1.88716074  14.1870286    2.62328172
  -9.27015304   2.74855238   5.22374482   4.38787753]
Minimum obj value:-1705.9658591021766
Optimal xi: 30.155465149037866
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1193.5149673597552
W_T_median: 1091.931535007786
W_T_pctile_5: 910.17506679131
W_T_CVAR_5_pct: 870.5128646711045
F value: -1705.9658591021766
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.7
F value: -1705.9658591021766
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [  1.4895023   -4.95186031 -20.34667984  -0.66389943  -1.36068438
   4.5487898   19.43431626   1.016852    -4.37153083   6.25495608
  12.86737428  -1.35618206  15.69100256   1.60706425 -17.16572498
   1.46911668  21.27514818   1.86130448  -1.93992344  90.70569323
   2.97058527  12.51717829   8.28523207  13.81928727  -3.12973652
  20.98297804  14.85358011   3.8107099    1.03371381 -47.74861538
 -20.46710711 -41.62018407  24.36087845  -3.81296317  11.64281964
  -3.05665769  -5.9421676    7.2005318   17.9306255    0.83192173
 -25.0016167    5.18744754 -18.61871923  -0.76852123   6.5749797
  -5.00690302   6.72689174  -5.76316801  -5.13386756  90.8767408
   0.63546012   4.2781487    3.14817011 -35.06125868   5.38494289
  -3.73205739   6.64869195   1.16872576   2.22092571  -2.05148531
  -5.22675318   3.70856933  -3.52963301  -2.0622883 ]
Minimum obj value:-1854.026289641737
Optimal xi: 29.28377957368116
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1336.5975549189395
W_T_median: 1151.2998664835932
W_T_pctile_5: 859.1524253122119
W_T_CVAR_5_pct: 784.7912860667943
F value: -1854.026289641737
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.8
F value: -1854.026289641737
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [  2.21739169  -1.99461619  -4.15374509   2.63261936  -1.95592365
   3.23949727   4.42063104  -1.37682046  33.1332211   29.91583648
   7.20941029   7.2149849    1.70361135 -14.17254671  11.98127773
  11.82922416  36.82953977 -13.92236925   4.65435706   4.74125261
  38.26585781   2.62126825   8.92708553   4.10614588  -0.55797143
 -70.75431558   2.15639564  -7.26591233  20.69594251   1.35288407
  -1.07592162  10.20692086   3.13954243  -4.49499857  44.99522645
  14.48958402   8.23787822 -15.14034578  34.39108415   4.98669998
   0.22321904 -20.23588453   8.96236134 -20.38615516  38.36958944
   3.16470918  61.23802107  -5.60539547  -5.79851696   6.12759819
  -9.07202654   1.89906557   4.56902769   2.3723529   25.50989896
  24.66810327  -2.89677129  -7.67415274  -6.92356454   8.64200966
   3.68120813  -3.33898463   9.44783511   5.55145202]
Minimum obj value:-1948.0174134677916
Optimal xi: 27.1320229081844
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1455.2596578671398
W_T_median: 1248.3928949074923
W_T_pctile_5: 734.6510703962645
W_T_CVAR_5_pct: 638.2928559857504
F value: -1948.0174134677916
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.9
F value: -1948.0174134677916
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-3.32822235e+00 -5.92574005e-01 -6.54825201e+00 -1.48287593e+00
  2.34389149e+00  2.02330801e+00  7.44778433e+00  1.45601746e+00
  7.95598114e+00 -1.39744065e+00  4.44191101e+00  3.18643934e-01
 -1.82081270e+00  1.26815400e+01  6.74705826e+01  6.28452820e+01
 -2.20868101e+00  1.16822295e+01  7.17817010e+01  6.05757752e+01
 -2.18647461e+00  1.26184876e+01  7.08323835e+01  5.94920836e+01
  4.75987652e+01  1.03833717e+01  2.39158354e+01 -2.30754514e+01
  4.63522609e+01  2.36416551e+01  1.53668562e+01  1.28852685e+01
  1.29068697e+00  5.24183793e+01  2.51456478e+01 -1.29401997e+01
  1.95738533e+00  6.06035823e+00  7.97901892e-01  2.16649855e+01
 -1.72779712e+01  3.18002552e+00 -2.58583336e+01 -2.94902140e+00
 -5.80538700e+00  1.69582157e+01 -1.03078496e+01  7.73097896e+00
 -3.20186895e+00  9.97795936e+00 -6.76812074e+00  3.16644949e+00
  5.40159272e+01 -6.72378330e-02  8.41375879e+01  2.74163776e+01
  4.91107851e+00 -3.97179372e+00  4.49208814e+00  6.31359905e+00
  4.55918737e+00 -3.84165260e+00 -1.12903004e+01 -7.08629878e+00]
Minimum obj value:-2094.425330645055
Optimal xi: 26.79045459359173
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1473.3670829589312
W_T_median: 1274.2894880898884
W_T_pctile_5: 718.1427770276856
W_T_CVAR_5_pct: 621.0588365330618
F value: -2094.425330645055
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
F value: -2094.425330645055
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ 4.44557056e+00 -4.16503586e+00 -6.15343730e+00  1.51836335e+00
 -3.75072289e+00  3.28217495e+00  5.09394714e+00 -2.07661065e+00
  3.05663110e+01  3.58336436e+00  1.31446972e+01  2.06794130e+01
  2.12140045e+01 -2.67292798e+01 -4.77744541e+00  1.71328466e+00
  4.09559816e+01 -3.94434961e+00 -2.18686695e+01  2.51527717e+01
 -6.34886673e+00  1.45304108e+01  2.33893006e+01  5.90957312e+00
  5.75198978e+00  4.23718066e+00 -6.02031128e+00  4.40514238e+00
 -4.83338048e-01 -1.42638762e+01 -5.10503343e+00  2.43791057e+01
 -2.51847644e+00 -9.38658175e-01  4.34200282e+01  1.13634797e-01
  2.09329754e+00  1.98934719e+01 -1.42948431e+01  2.83361669e-01
 -1.76051976e+01  2.84685591e+00 -1.73483578e+01  2.08953349e+00
  1.94793776e+01  6.82929097e+00  2.22210010e+01  7.15003800e+00
  8.05351973e+00 -6.50712354e+00  8.61252579e+00 -6.72639407e+00
 -7.05485348e+01  1.97118647e+00 -6.28705507e+01  1.79625143e+00
 -6.71732583e-02 -2.26179765e+00  5.80639755e+00  1.78477976e+00
  2.24130270e-01 -2.24787152e+00  5.82113214e+00  1.76659749e+00]
Minimum obj value:-2420.8531758045624
Optimal xi: 27.02130813244911
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1491.7631245277844
W_T_median: 1300.831570839232
W_T_pctile_5: 741.4729345245273
W_T_CVAR_5_pct: 632.0450653843569
F value: -2420.8531758045624
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.2
F value: -2420.8531758045624
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-11.73177339  -9.73502543  -1.26041143  -9.87309716  12.49980311
  10.89475675   1.18440943   9.92003965  47.92533151  -3.13803394
   0.29453292  25.62307481  47.00643908  -2.21677156   0.17233653
  25.79215727   9.85373794  -3.14765523  -5.37185151  -0.41393499
  47.50026963  -3.04451776   0.80316445  25.02362697  13.85804895
   0.7657726   -8.79833898   6.05531764  -5.84261424   6.28621759
  14.82983385  -3.87081875 -12.07911229   5.86971237  13.56871646
 -26.83235542   4.21406086   0.8404346   31.25019732   4.77446102
  33.28292897   1.73803926  35.49669132  51.85526217  39.29535598
  22.04333682  28.87596917   0.3421832   71.44092125   5.88334417
  57.42366512  -5.92353745 -20.61461745   6.51335426 -21.65781342
   1.0524021   -0.13221091  -3.17102127  11.30820209   7.64965762
  -0.12168578  -3.14186327  14.51891135   4.93551373]
Minimum obj value:-2845.9418434513173
Optimal xi: 25.792272242905625
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1520.199928886252
W_T_median: 1366.8946615392742
W_T_pctile_5: 667.2943054579252
W_T_CVAR_5_pct: 565.6588846336562
F value: -2845.9418434513173
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
F value: -2845.9418434513173
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-4.12955636 -0.36856283 -3.55739653 -4.10413373  4.58076805  0.33803194
  4.91348616  3.86302618  0.04604769 -1.40092872 -1.63115892 -2.60175859
  1.49996851  2.07912524  2.00019934  1.79117154 -0.73748332 -1.7222466
 -2.02280954 -1.61520837 -0.8863304  -1.69122094 -2.05122448 -1.90859041
  0.38280666  0.54412321 -0.74781852  1.02628065 -1.26069229 -0.72788795
 -2.11864679 -0.88955352 -1.82349729 -1.76854692 -1.06450656 -2.2867808
 -1.53054327 -1.17319457 -1.69132957 -1.82703433  0.96078421  0.78019861
 -0.70340555 -0.65765852  0.48820446 -0.04354827 -0.58727424  0.91748032
 -0.90813219 -1.57331755 -0.75224019 -1.05355335  0.58818488  0.6842344
 -1.16383934  0.40160305  1.71019821  2.55166663  1.2570748   1.98608426
  1.26326835 -1.60660202 -0.34479527  1.85096891]
Minimum obj value:-3592.7005999582125
Optimal xi: 24.960280637413025
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1551.5577480553873
W_T_median: 1405.4383972885003
W_T_pctile_5: 624.6754498300529
W_T_CVAR_5_pct: 489.59103520411713
F value: -3592.7005999582125
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.0
F value: -3592.7005999582125
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-4.41952144 -1.96322708 -2.48093162 -3.90237178  4.12634912  2.83164845
  3.531668    3.01560408 -1.41597142 -1.89843187 -0.57363793 -2.56402666
 -0.26293546 -1.90747016  1.05926445 -1.08878742 -0.80054007 -1.42242766
  0.13250206 -2.75486326 -1.73149355 -2.84938043 -1.81142948 -1.50934275
 -0.19486527 -0.82557917  0.21060489  0.22465091 -0.87786097 -1.824728
 -2.14258047 -1.41571092  0.5095953   1.22758526  0.38572591  1.70457663
 -1.76211107 -1.97786659 -0.84786316 -1.45298624  0.49825533 -0.40507813
  1.01463452 -0.33684414 -0.68235556 -1.8553076  -0.86850718 -2.0485956
  0.87702735  0.23697393  0.81153132  0.07433294 -1.4617405  -2.30512594
 -0.97304801 -1.14263591  0.95991926  1.97356034 -1.82192968 -1.56423327
  0.8004837   1.53675407  0.3852557  -1.33849282]
Minimum obj value:-5144.26344769833
Optimal xi: 24.980714217076756
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1551.557748060324
W_T_median: 1405.4383972912642
W_T_pctile_5: 624.6754498228327
W_T_CVAR_5_pct: 489.59103520011587
F value: -5144.26344769833
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
F value: -5144.26344769833
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-2.93099567 -3.84700731 -3.74006778 -3.41194374  2.79010747  3.62299304
  4.18068568  3.02401849  0.43954534 -1.28605902 -1.46357917 -1.61405951
 -1.13831873 -1.92641564 -1.8013381  -1.90871778 -0.6007402  -1.26599191
 -1.60936796 -2.41631793 -0.05759192 -1.17035099 -2.1056326  -1.67732193
  1.46713307  1.34645107  1.69402105  0.6299103  -1.17092879 -0.76391825
 -1.22447306 -1.65116383 -2.38515985 -0.88926979 -1.46318143 -1.94756556
 -1.07284337 -1.42500051 -2.45348135 -1.73632623 -1.89370912 -1.33460075
 -0.71575734 -1.4613319  -0.54098803  0.27091676 -0.81921701 -0.35067103
 -1.86944208 -1.73509055 -1.08573531 -2.0456424  -1.08538861 -1.47229266
 -1.36525131 -1.12079369 -1.28988152 -0.66245219 -1.98727948 -0.90129458
 -0.31155623 -0.26855931 -2.14612813 -1.63472738]
Minimum obj value:-16005.167540326487
Optimal xi: 24.97968166111574
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1551.5577480629784
W_T_median: 1405.4383972927872
W_T_pctile_5: 624.6754498189422
W_T_CVAR_5_pct: 489.59103519794144
F value: -16005.167540326487
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 10.0
F value: -16005.167540326487
-----------------------------------------------
