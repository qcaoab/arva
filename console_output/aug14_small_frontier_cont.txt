Starting at: 
2022-08-14 10:15:19

 Random seed:  1  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = basic_T30_VWD
timeseries_basket['basket_desc'] = CRSP data: T30 and VWD
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
5.0% of MC simulations done.
10.0% of MC simulations done.
15.0% of MC simulations done.
20.0% of MC simulations done.
25.0% of MC simulations done.
30.0% of MC simulations done.
35.0% of MC simulations done.
40.0% of MC simulations done.
45.0% of MC simulations done.
50.0% of MC simulations done.
55.00000000000001% of MC simulations done.
60.0% of MC simulations done.
65.0% of MC simulations done.
70.0% of MC simulations done.
75.0% of MC simulations done.
80.0% of MC simulations done.
85.0% of MC simulations done.
90.0% of MC simulations done.
95.0% of MC simulations done.
100.0% of MC simulations done.
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  hidden_layer    None       None         None   
4        obj.layers[4]        4  hidden_layer    None       None         None   
5        obj.layers[5]        5  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  
4    False        None  
5    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  hidden_layer       4  logistic_sigmoid   
4        obj.layers[4]        4  hidden_layer       4  logistic_sigmoid   
5        obj.layers[5]        5  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 4)    False        None  
4       (4, 4)    False        None  
5       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ 3.71033082e+00 -4.91403129e+00  2.62013615e+00  7.46821479e-01
 -3.44957945e+00  3.55020888e+00 -2.92176749e+00 -9.96945429e-01
  2.88356052e+01  2.03740004e+01  2.03781945e+01  1.67334020e+01
  2.44678603e+01  2.61053923e+01  3.50244735e+01  4.21425968e+01
  1.55723348e+00 -4.00701864e-01  6.98159664e+00  2.61749362e+00
  3.92181527e+01  3.67649953e+01  2.40698315e+01  2.32645843e+01
  1.28667076e+00  1.46147950e+01 -2.66398673e+00  1.29944455e+01
  3.02959670e+00  2.42603520e-01  1.59383980e-01  2.20819316e+01
  1.03391063e+01 -6.37602343e+00  5.96579812e+00 -1.79200782e+01
  7.11522940e+00 -1.08504275e+01  4.52215023e+00 -1.02285873e+01
 -4.17884315e-02 -2.03448729e+01 -1.99432655e+01 -1.93778316e+01
  2.58401000e+00  1.64780719e+01  1.56941215e+01  1.61271513e+01
  5.22575104e+00 -1.02565542e+01 -7.84176436e+00 -7.41639882e+00
  2.74177370e+01  1.51685679e+01  1.54452708e+01  1.69559547e+01
 -2.39386388e+00 -3.48866053e+00  3.37662917e+00 -4.72166010e+00
  3.22376898e+00 -4.71774416e+00  3.16251389e+00 -4.71486796e+00]
Minimum obj value:-1046.3429495413893
Optimal xi: 31.107729902534253
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING-dataset
W_T_mean: 1063.9086938090534
W_T_median: 1050.173627478382
W_T_pctile_5: 967.9542455785548
W_T_CVAR_5_pct: 939.9539796444536
F value: -1046.3429495413893
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.1
F value: -1046.3429495413893
-----------------------------------------------
{'NN': [3.7103308192656153, -4.914031292321556, 2.620136152041739, 0.7468214793302334, -3.449579446231488, 3.5502088798292735, -2.921767489083037, -0.9969454294993951, 28.83560515193701, 20.37400036533778, 20.378194516062745, 16.733402047863652, 24.46786027037998, 26.10539234261853, 35.02447354454463, 42.142596771033816, 1.557233484420495, -0.40070186400564023, 6.981596642298996, 2.6174936166618283, 39.21815265119752, 36.764995308918, 24.069831498662975, 23.264584312705356, 1.2866707609930286, 14.614795007812948, -2.6639867332027274, 12.994445466019203, 3.029596697449311, 0.24260351957355394, 0.15938397964313658, 22.08193163745939, 10.339106283043167, -6.376023431580758, 5.965798121924041, -17.92007817831716, 7.115229401850721, -10.850427516558241, 4.522150227381071, -10.228587292779004, -0.04178843145586324, -20.344872938376813, -19.943265546940356, -19.37783163858078, 2.584009996707078, 16.478071923524798, 15.694121536438452, 16.127151291872064, 5.225751043368103, -10.256554175846755, -7.841764357663035, -7.416398824094467, 27.41773700138588, 15.168567886968459, 15.44527083443973, 16.955954745869914, -2.3938638837934954, -3.48866053136798, 3.376629174089268, -4.721660099031384, 3.223768977509618, -4.717744158275796, 3.162513894357063, -4.7148679602817625]}
[ 3.71033082e+00 -4.91403129e+00  2.62013615e+00  7.46821479e-01
 -3.44957945e+00  3.55020888e+00 -2.92176749e+00 -9.96945429e-01
  2.88356052e+01  2.03740004e+01  2.03781945e+01  1.67334020e+01
  2.44678603e+01  2.61053923e+01  3.50244735e+01  4.21425968e+01
  1.55723348e+00 -4.00701864e-01  6.98159664e+00  2.61749362e+00
  3.92181527e+01  3.67649953e+01  2.40698315e+01  2.32645843e+01
  1.28667076e+00  1.46147950e+01 -2.66398673e+00  1.29944455e+01
  3.02959670e+00  2.42603520e-01  1.59383980e-01  2.20819316e+01
  1.03391063e+01 -6.37602343e+00  5.96579812e+00 -1.79200782e+01
  7.11522940e+00 -1.08504275e+01  4.52215023e+00 -1.02285873e+01
 -4.17884315e-02 -2.03448729e+01 -1.99432655e+01 -1.93778316e+01
  2.58401000e+00  1.64780719e+01  1.56941215e+01  1.61271513e+01
  5.22575104e+00 -1.02565542e+01 -7.84176436e+00 -7.41639882e+00
  2.74177370e+01  1.51685679e+01  1.54452708e+01  1.69559547e+01
 -2.39386388e+00 -3.48866053e+00  3.37662917e+00 -4.72166010e+00
  3.22376898e+00 -4.71774416e+00  3.16251389e+00 -4.71486796e+00]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ 3.46623380e+00 -3.85499168e+00  2.05785493e+00  1.12886540e+00
 -3.20548243e+00  2.49116927e+00 -2.35948627e+00 -1.37898935e+00
  3.72192405e+01  2.42864096e+01  4.81613588e+01  1.69654500e+01
  3.00473143e+01  3.52528576e+01  4.67603793e+01  6.37636693e+01
  3.68447235e+00 -1.77622188e+00  1.08615643e+01  2.15093924e+00
  1.09162003e+02  1.31186822e+02  3.10711706e+01  8.10626349e+01
  2.66937170e+00  1.18418765e+01 -3.45264082e+00  2.43964637e+01
  7.35664895e-01 -2.34714892e+00  6.63326790e+00  2.22161399e+01
  1.16525070e+01 -3.67653145e+00  7.75118037e+00 -2.94077824e+01
  8.42694287e+00 -1.21774408e+01  4.48830773e+00 -1.24870387e+01
  4.74581796e-02 -3.38234891e+01 -3.44520085e+01 -3.45982138e+01
  1.74762221e+00  1.49788620e+01  1.36725411e+01  1.32730721e+01
  1.30732903e+00 -5.24506723e+00 -7.18148861e-01  3.95096498e-01
  3.70403835e+01  1.66080220e+01  1.94721203e+01  2.18232941e+01
 -2.06572069e+00 -3.62798062e+00  3.96230363e+00 -4.82057451e+00
  3.17545571e+00 -4.87590573e+00  2.89064155e+00 -4.95529386e+00]
Minimum obj value:-1207.6346671171395
Optimal xi: 31.070224052695746
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING-dataset
W_T_mean: 1081.3413613604284
W_T_median: 1054.7700109301895
W_T_pctile_5: 965.7862304667003
W_T_CVAR_5_pct: 937.3040651440883
F value: -1207.6346671171395
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.25
F value: -1207.6346671171395
-----------------------------------------------
{'NN': [3.4662338033561073, -3.8549916823783787, 2.0578549323552084, 1.1288653981678218, -3.205482430322153, 2.4911692698858996, -2.3594862693964314, -1.3789893483372697, 37.21924049067763, 24.2864095974025, 48.161358847824495, 16.965449972033163, 30.047314278373424, 35.25285755169039, 46.760379269758424, 63.76366927290762, 3.684472350001922, -1.7762218793863636, 10.861564318254752, 2.1509392425421217, 109.16200302658602, 131.18682202636083, 31.07117058191567, 81.06263487605831, 2.6693716976651007, 11.841876454685416, -3.4526408224336356, 24.39646372289338, 0.7356648946246431, -2.3471489213580656, 6.63326789529434, 22.216139898062544, 11.652507018746759, -3.6765314477088653, 7.751180370372557, -29.40778236389316, 8.426942872708697, -12.177440817997832, 4.488307728797942, -12.487038677052897, 0.047458179643137144, -33.8234891174605, -34.45200851242702, -34.5982137964784, 1.7476222089617335, 14.978862036352744, 13.672541078953529, 13.273072056966434, 1.3073290261701405, -5.245067225186539, -0.7181488610254233, 0.3950964982105043, 37.04038350569646, 16.6080219939768, 19.472120293634624, 21.823294085973565, -2.0657206876052823, -3.627980616279764, 3.962303625569453, -4.820574509929044, 3.1754557101722427, -4.875905732692168, 2.8906415535198406, -4.955293860333762]}
[ 3.46623380e+00 -3.85499168e+00  2.05785493e+00  1.12886540e+00
 -3.20548243e+00  2.49116927e+00 -2.35948627e+00 -1.37898935e+00
  3.72192405e+01  2.42864096e+01  4.81613588e+01  1.69654500e+01
  3.00473143e+01  3.52528576e+01  4.67603793e+01  6.37636693e+01
  3.68447235e+00 -1.77622188e+00  1.08615643e+01  2.15093924e+00
  1.09162003e+02  1.31186822e+02  3.10711706e+01  8.10626349e+01
  2.66937170e+00  1.18418765e+01 -3.45264082e+00  2.43964637e+01
  7.35664895e-01 -2.34714892e+00  6.63326790e+00  2.22161399e+01
  1.16525070e+01 -3.67653145e+00  7.75118037e+00 -2.94077824e+01
  8.42694287e+00 -1.21774408e+01  4.48830773e+00 -1.24870387e+01
  4.74581796e-02 -3.38234891e+01 -3.44520085e+01 -3.45982138e+01
  1.74762221e+00  1.49788620e+01  1.36725411e+01  1.32730721e+01
  1.30732903e+00 -5.24506723e+00 -7.18148861e-01  3.95096498e-01
  3.70403835e+01  1.66080220e+01  1.94721203e+01  2.18232941e+01
 -2.06572069e+00 -3.62798062e+00  3.96230363e+00 -4.82057451e+00
  3.17545571e+00 -4.87590573e+00  2.89064155e+00 -4.95529386e+00]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ 3.29041060e+00 -3.77355778e+00  1.94392421e+00 -4.23597995e-01
 -3.02965923e+00  2.40973537e+00 -2.24555555e+00  1.73474045e-01
  4.45878431e+01  2.79625584e+01  8.93693188e+01  1.73484872e+01
  3.16613926e+01  3.89026508e+01  4.97647709e+01  9.67800202e+01
  7.20388380e+00 -2.39675289e+00  1.19474880e+01  2.38510226e+00
  1.17255383e+02  1.86114554e+02  3.11110458e+01  1.23976582e+02
  2.88551043e+00  9.71031371e+00 -2.91909413e+00  4.43770160e+01
  7.00216600e-01 -5.75027918e+00  9.61182379e+00  2.13190326e+01
  1.08817047e+01 -1.35141455e+00  9.28866401e+00 -3.68776524e+01
  9.11489558e+00 -1.31546760e+01  3.99106275e+00 -1.32454552e+01
 -5.78681104e-01 -5.00252846e+01 -4.59434903e+01 -4.59212215e+01
  2.02534576e+00  4.32759283e+00  1.11439254e+01  1.05768247e+01
  1.35743334e+00 -3.74488676e+00 -5.34907372e-01 -3.96444995e-01
  4.34711779e+01  2.42677061e+01  2.63426485e+01  2.87521485e+01
 -2.45799096e+00 -3.80216365e+00  3.95909338e+00 -5.23208410e+00
  2.63476332e+00 -4.65857269e+00  2.61041381e+00 -4.69133492e+00]
Minimum obj value:-1371.609682560675
Optimal xi: 30.98184628479063
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING-dataset
W_T_mean: 1102.8148376966808
W_T_median: 1059.7539428776884
W_T_pctile_5: 960.2589114562489
W_T_CVAR_5_pct: 930.4879658930065
F value: -1371.609682560675
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.4
F value: -1371.609682560675
-----------------------------------------------
{'NN': [3.2904106020652635, -3.7735577791744213, 1.9439242086345458, -0.4235979947061513, -3.0296592290313353, 2.4097353666818866, -2.245555545675901, 0.173474044536715, 44.587843115177144, 27.96255839908441, 89.36931880121637, 17.348487160502977, 31.66139255865442, 38.90265082464621, 49.76477086174057, 96.78002023064786, 7.203883800484585, -2.3967528897449877, 11.947488001977172, 2.385102264396142, 117.25538322777271, 186.11455449224607, 31.111045782754893, 123.97658229384868, 2.885510429951835, 9.710313713766835, -2.9190941347611563, 44.37701599320966, 0.7002166003254644, -5.750279181975022, 9.611823788223676, 21.319032594726124, 10.881704661280503, -1.3514145529548218, 9.288664010730425, -36.877652419304816, 9.114895580723894, -13.154676031045803, 3.9910627475134794, -13.2454551848644, -0.5786811036598013, -50.0252845521391, -45.94349025050436, -45.92122147724791, 2.025345764256792, 4.327592831153411, 11.143925432098582, 10.576824690232433, 1.3574333428883996, -3.74488676262848, -0.5349073723094814, -0.39644499480849865, 43.471177888397996, 24.267706110131062, 26.342648453758226, 28.75214853176295, -2.4579909578671346, -3.802163649626616, 3.9590933772048666, -5.232084098837494, 2.634763315655787, -4.658572693658148, 2.6104138123138036, -4.691334915829442]}
[ 3.29041060e+00 -3.77355778e+00  1.94392421e+00 -4.23597995e-01
 -3.02965923e+00  2.40973537e+00 -2.24555555e+00  1.73474045e-01
  4.45878431e+01  2.79625584e+01  8.93693188e+01  1.73484872e+01
  3.16613926e+01  3.89026508e+01  4.97647709e+01  9.67800202e+01
  7.20388380e+00 -2.39675289e+00  1.19474880e+01  2.38510226e+00
  1.17255383e+02  1.86114554e+02  3.11110458e+01  1.23976582e+02
  2.88551043e+00  9.71031371e+00 -2.91909413e+00  4.43770160e+01
  7.00216600e-01 -5.75027918e+00  9.61182379e+00  2.13190326e+01
  1.08817047e+01 -1.35141455e+00  9.28866401e+00 -3.68776524e+01
  9.11489558e+00 -1.31546760e+01  3.99106275e+00 -1.32454552e+01
 -5.78681104e-01 -5.00252846e+01 -4.59434903e+01 -4.59212215e+01
  2.02534576e+00  4.32759283e+00  1.11439254e+01  1.05768247e+01
  1.35743334e+00 -3.74488676e+00 -5.34907372e-01 -3.96444995e-01
  4.34711779e+01  2.42677061e+01  2.63426485e+01  2.87521485e+01
 -2.45799096e+00 -3.80216365e+00  3.95909338e+00 -5.23208410e+00
  2.63476332e+00 -4.65857269e+00  2.61041381e+00 -4.69133492e+00]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ 3.02775046e+00 -3.78426964e+00  1.84286252e+00  4.88248282e-01
 -2.76699909e+00  2.42044723e+00 -2.14449386e+00 -7.38372233e-01
  4.82741731e+01  2.79464122e+01  1.23475989e+02  1.72324990e+01
  3.38223991e+01  3.98664877e+01  5.30180618e+01  1.08654146e+02
  1.11012744e+01 -7.02226404e-02  1.19128185e+01  2.49025463e+00
  1.61325111e+02  2.60342710e+02  3.11799265e+01  1.67182311e+02
  1.89816065e+00  1.07705115e+01 -2.35053235e+00  3.50778604e+01
  3.49424334e+00 -8.72513854e+00  9.12086726e+00  1.77554135e+01
  9.43840722e+00  1.25547459e-01  1.03358266e+01 -4.46010646e+01
  1.00848516e+01 -1.44160776e+01  2.30599410e+00 -1.62364888e+01
 -2.63660568e+00 -6.92944395e+01 -5.83489766e+01 -5.84951374e+01
  1.75775446e+00 -1.22546286e+01  8.03079099e+00  7.44426089e+00
  2.49614406e+00 -4.83634962e+00 -6.14948142e-01 -7.30082217e-01
  4.99381966e+01  2.58049796e+01  3.01450960e+01  3.24855275e+01
 -4.40739661e+00 -4.07418251e+00  6.91380996e+00 -5.60371144e+00
  2.89502244e+00 -3.98074679e+00  2.93745377e+00 -4.05443251e+00]
Minimum obj value:-1595.7818004054561
Optimal xi: 30.74332102978899
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING-dataset
W_T_mean: 1140.6032962918891
W_T_median: 1074.7391755799304
W_T_pctile_5: 945.5588394330205
W_T_CVAR_5_pct: 911.4242472375507
F value: -1595.7818004054561
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.6
F value: -1595.7818004054561
-----------------------------------------------
{'NN': [3.027750463850484, -3.7842696428690292, 1.842862522210524, 0.4882482824787697, -2.766999090816561, 2.420447230376454, -2.144493859251896, -0.7383722326482023, 48.27417309724927, 27.946412162912726, 123.4759891676887, 17.232499031721808, 33.82239913890338, 39.86648766668088, 53.018061794456074, 108.6541462587545, 11.101274373170584, -0.07022264044019273, 11.912818473967471, 2.490254631011722, 161.32511097947372, 260.3427097622689, 31.179926514955476, 167.18231067913192, 1.8981606462079474, 10.770511467229074, -2.3505323472319195, 35.07786035627767, 3.494243336025015, -8.725138539430404, 9.120867261805703, 17.755413479187872, 9.438407215467425, 0.12554745860112967, 10.335826604843936, -44.60106462575418, 10.084851564402857, -14.416077597243612, 2.3059940959365934, -16.236488847219455, -2.6366056847385213, -69.29443953292828, -58.34897658877464, -58.495137381379436, 1.7577544642648155, -12.254628609681218, 8.030790986259113, 7.444260894662553, 2.4961440598153994, -4.83634962291077, -0.6149481422538933, -0.7300822168487434, 49.93819656298926, 25.804979564517076, 30.145095997648905, 32.48552749700309, -4.407396609213811, -4.074182505138301, 6.913809957133184, -5.603711442306127, 2.8950224376074782, -3.9807467860303603, 2.9374537689650833, -4.0544325126145075]}
[ 3.02775046e+00 -3.78426964e+00  1.84286252e+00  4.88248282e-01
 -2.76699909e+00  2.42044723e+00 -2.14449386e+00 -7.38372233e-01
  4.82741731e+01  2.79464122e+01  1.23475989e+02  1.72324990e+01
  3.38223991e+01  3.98664877e+01  5.30180618e+01  1.08654146e+02
  1.11012744e+01 -7.02226404e-02  1.19128185e+01  2.49025463e+00
  1.61325111e+02  2.60342710e+02  3.11799265e+01  1.67182311e+02
  1.89816065e+00  1.07705115e+01 -2.35053235e+00  3.50778604e+01
  3.49424334e+00 -8.72513854e+00  9.12086726e+00  1.77554135e+01
  9.43840722e+00  1.25547459e-01  1.03358266e+01 -4.46010646e+01
  1.00848516e+01 -1.44160776e+01  2.30599410e+00 -1.62364888e+01
 -2.63660568e+00 -6.92944395e+01 -5.83489766e+01 -5.84951374e+01
  1.75775446e+00 -1.22546286e+01  8.03079099e+00  7.44426089e+00
  2.49614406e+00 -4.83634962e+00 -6.14948142e-01 -7.30082217e-01
  4.99381966e+01  2.58049796e+01  3.01450960e+01  3.24855275e+01
 -4.40739661e+00 -4.07418251e+00  6.91380996e+00 -5.60371144e+00
  2.89502244e+00 -3.98074679e+00  2.93745377e+00 -4.05443251e+00]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ 3.15202536e+00 -3.77238459e+00  1.74222391e+00  6.29980236e-02
 -2.89127398e+00  2.40856218e+00 -2.04385524e+00 -3.13121974e-01
  5.01652930e+01  2.73955289e+01  1.61809576e+02  1.95812967e+01
  3.68571126e+01  4.36551412e+01  5.00500980e+01  1.21821276e+02
  1.18475049e+01 -2.75828671e-01  1.13869225e+01  2.44978228e+00
  2.10534082e+02  3.19827985e+02  3.12007282e+01  2.05478029e+02
  1.88850182e+00  1.02703129e+01 -2.28339203e+00  3.25910873e+01
  3.66143093e+00 -9.29046057e+00  9.58325109e+00  1.24698051e+01
  9.18991130e+00  4.41756851e-01  1.07221367e+01 -4.79122161e+01
  1.08189410e+01 -1.44289086e+01  1.26118611e+00 -1.46301431e+01
 -2.19911442e+00 -1.00094049e+02 -6.69387131e+01 -6.74448840e+01
  1.88856222e+00 -1.18349118e+01  8.21629324e+00  7.70688033e+00
  2.65599744e+00 -7.94854054e+00 -1.91454839e-01 -5.52826162e-01
  5.59191334e+01  4.35931133e+01  4.27201787e+01  4.50462543e+01
 -5.06665297e+00 -4.28095297e+00  5.42279473e+00 -6.21500901e+00
  2.44832730e+00 -4.01618438e+00  2.46519044e+00 -4.06207590e+00]
Minimum obj value:-1710.7596101683025
Optimal xi: 30.56779303086356
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING-dataset
W_T_mean: 1162.2949460930286
W_T_median: 1085.3251853297477
W_T_pctile_5: 934.8990040541842
W_T_CVAR_5_pct: 897.1595907091681
F value: -1710.7596101683025
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.7
F value: -1710.7596101683025
-----------------------------------------------
{'NN': [3.1520253570115337, -3.7723845890023817, 1.7422239052711528, 0.06299802362555383, -2.8912739839776367, 2.408562176509746, -2.0438552423126057, -0.313121973795004, 50.16529296840766, 27.395528857819244, 161.8095755130782, 19.58129671918397, 36.85711257595465, 43.655141223827094, 50.050098021746614, 121.82127648970464, 11.847504879905742, -0.2758286714415755, 11.386922535836673, 2.4497822750205995, 210.53408220890668, 319.8279848265459, 31.200728158992753, 205.47802914996367, 1.8885018238092317, 10.27031294670465, -2.283392030894426, 32.59108734699427, 3.661430929261969, -9.290460568064816, 9.583251085946355, 12.46980505420618, 9.189911304951472, 0.4417568509163365, 10.722136690895166, -47.91221613067931, 10.818940975142493, -14.42890861024859, 1.2611861052347004, -14.63014312021066, -2.1991144151513455, -100.094048983409, -66.9387130785206, -67.44488403839516, 1.8885622246652618, -11.834911760626618, 8.21629324036721, 7.706880328377297, 2.655997437898943, -7.948540537755238, -0.1914548388740713, -0.5528261623579432, 55.91913342340217, 43.59311331172668, 42.7201787464952, 45.04625431880441, -5.06665296766134, -4.280952971111478, 5.422794730517519, -6.2150090144890235, 2.4483273036354296, -4.016184383664292, 2.4651904355261975, -4.062075902853292]}
[ 3.15202536e+00 -3.77238459e+00  1.74222391e+00  6.29980236e-02
 -2.89127398e+00  2.40856218e+00 -2.04385524e+00 -3.13121974e-01
  5.01652930e+01  2.73955289e+01  1.61809576e+02  1.95812967e+01
  3.68571126e+01  4.36551412e+01  5.00500980e+01  1.21821276e+02
  1.18475049e+01 -2.75828671e-01  1.13869225e+01  2.44978228e+00
  2.10534082e+02  3.19827985e+02  3.12007282e+01  2.05478029e+02
  1.88850182e+00  1.02703129e+01 -2.28339203e+00  3.25910873e+01
  3.66143093e+00 -9.29046057e+00  9.58325109e+00  1.24698051e+01
  9.18991130e+00  4.41756851e-01  1.07221367e+01 -4.79122161e+01
  1.08189410e+01 -1.44289086e+01  1.26118611e+00 -1.46301431e+01
 -2.19911442e+00 -1.00094049e+02 -6.69387131e+01 -6.74448840e+01
  1.88856222e+00 -1.18349118e+01  8.21629324e+00  7.70688033e+00
  2.65599744e+00 -7.94854054e+00 -1.91454839e-01 -5.52826162e-01
  5.59191334e+01  4.35931133e+01  4.27201787e+01  4.50462543e+01
 -5.06665297e+00 -4.28095297e+00  5.42279473e+00 -6.21500901e+00
  2.44832730e+00 -4.01618438e+00  2.46519044e+00 -4.06207590e+00]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [   3.00571142   -3.32931529    1.59047841    2.39636456   -2.74496004
    1.96549288   -1.89210975   -2.64648851   54.17087677   28.72044771
  189.4569236    18.46519237   42.12110296   46.2598762    51.47196886
  134.78818617   11.47675387   -1.56068536    8.9232791     2.40802346
  202.79942655  344.46645249   31.17034227  234.84410208    1.57539643
   10.76842342   -2.47220727   32.19437408    4.83576261  -10.7106126
    9.94784363   19.886886      7.58690109    1.92138549   12.03603065
  -50.24757294   10.97232441  -13.78909516    0.35115222  -14.42228323
   -1.73458775 -156.4868023   -74.79077667  -75.68565705    1.81411798
  -10.83725894    9.41833713    8.97613451    3.20057109  -24.76645161
    3.64513938    2.97408976   58.10367131   81.92609715   52.85026762
   55.23701215   -5.42158392   -4.29649306    5.86503305   -8.40118641
    2.25567185   -3.98242393    2.23107949   -4.03051352]
Minimum obj value:-1827.769563141227
Optimal xi: 30.414181959575448
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING-dataset
W_T_mean: 1179.3509514404284
W_T_median: 1097.9287386691992
W_T_pctile_5: 925.3089786087562
W_T_CVAR_5_pct: 884.2907335967511
F value: -1827.769563141227
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.8
F value: -1827.769563141227
-----------------------------------------------
{'NN': [3.0057114166345618, -3.3293152891588007, 1.5904784133743344, 2.396364556345109, -2.744960043600623, 1.9654928766661817, -1.8921097504157767, -2.6464885065145696, 54.170876765657354, 28.72044770872177, 189.45692359836127, 18.46519237218393, 42.121102961346324, 46.259876198374094, 51.47196885552118, 134.7881861653685, 11.476753868527249, -1.5606853609189266, 8.92327909687509, 2.408023461096229, 202.7994265544889, 344.46645249380805, 31.170342270104936, 234.84410207984706, 1.5753964303380912, 10.768423421833266, -2.4722072683883787, 32.19437407720044, 4.835762605226511, -10.710612597505982, 9.947843627826733, 19.886886004593855, 7.5869010918026385, 1.921385488697973, 12.03603064840679, -50.247572943122336, 10.972324414188671, -13.789095159255472, 0.3511522239654481, -14.422283230616165, -1.7345877473654157, -156.48680229960468, -74.79077666932704, -75.68565704807104, 1.8141179793598263, -10.83725894186545, 9.418337125848534, 8.976134514332339, 3.2005710949814707, -24.766451606274195, 3.645139380582636, 2.9740897557084973, 58.10367131193131, 81.92609715033046, 52.850267619945264, 55.23701215334135, -5.421583920696857, -4.296493061146081, 5.865033051442954, -8.401186407139617, 2.255671851034643, -3.982423927237016, 2.2310794927711894, -4.030513517679759]}
[   3.00571142   -3.32931529    1.59047841    2.39636456   -2.74496004
    1.96549288   -1.89210975   -2.64648851   54.17087677   28.72044771
  189.4569236    18.46519237   42.12110296   46.2598762    51.47196886
  134.78818617   11.47675387   -1.56068536    8.9232791     2.40802346
  202.79942655  344.46645249   31.17034227  234.84410208    1.57539643
   10.76842342   -2.47220727   32.19437408    4.83576261  -10.7106126
    9.94784363   19.886886      7.58690109    1.92138549   12.03603065
  -50.24757294   10.97232441  -13.78909516    0.35115222  -14.42228323
   -1.73458775 -156.4868023   -74.79077667  -75.68565705    1.81411798
  -10.83725894    9.41833713    8.97613451    3.20057109  -24.76645161
    3.64513938    2.97408976   58.10367131   81.92609715   52.85026762
   55.23701215   -5.42158392   -4.29649306    5.86503305   -8.40118641
    2.25567185   -3.98242393    2.23107949   -4.03051352]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [   3.1035469    -3.21788848    1.41289404   -2.61899331   -2.84279553
    1.85406607   -1.71452538    2.36886936   57.76946789   31.76365355
  195.02941336   17.73048087   49.97287055   49.27907425   69.5544223
  149.19793895   10.59215463   -2.63668049   11.60005122    2.02831926
  245.26410171  354.62661196   31.29763104  245.98745219    1.31444963
   11.29195975   -2.95033224   36.60081522    5.84713621  -11.34982837
   10.27502943   17.6050143     6.20758404    1.91915234   12.21632988
  -51.3588445    10.94025216  -12.95759564   -0.84848647  -14.62494375
   -1.96949383 -213.73255385  -82.11770727  -83.34225992    1.7690277
  -10.33176476   10.1838816     9.81566164    3.68525782  -50.07562532
    7.00710587    5.78720671   58.97066277  114.22655152   61.08844646
   63.7549663    -5.83790821   -4.26535765    5.1859397    -9.14685365
    2.18965796   -3.89152401    2.12297704   -3.92729776]
Minimum obj value:-1946.4584121094733
Optimal xi: 30.240275087800093
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING-dataset
W_T_mean: 1194.3589684830765
W_T_median: 1113.8489586612377
W_T_pctile_5: 914.9072957795116
W_T_CVAR_5_pct: 871.5403098892722
F value: -1946.4584121094733
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.9
F value: -1946.4584121094733
-----------------------------------------------
{'NN': [3.103546900365823, -3.217888478367022, 1.4128940431248738, -2.618993306549624, -2.8427955273319347, 1.854066065874379, -1.7145253801663614, 2.3688693563801473, 57.76946789026988, 31.763653550417445, 195.02941335896958, 17.73048086932908, 49.97287055457502, 49.279074251165156, 69.55442230312754, 149.19793894871006, 10.592154627350268, -2.6366804917962177, 11.600051217190094, 2.028319258325344, 245.26410171159657, 354.62661195553574, 31.297631041328554, 245.9874521862275, 1.3144496316610779, 11.291959749374119, -2.950332244187786, 36.600815218231986, 5.847136206231689, -11.34982837379309, 10.275029434092566, 17.605014296780293, 6.2075840425135596, 1.9191523357652256, 12.21632987719125, -51.358844504699796, 10.940252156164183, -12.957595638341129, -0.8484864696368798, -14.624943752765835, -1.9694938266258015, -213.73255385306422, -82.11770726516632, -83.3422599183328, 1.7690277041841085, -10.331764764155523, 10.183881597446259, 9.815661637487613, 3.685257820391294, -50.075625317464414, 7.007105871467417, 5.787206707617969, 58.97066276944352, 114.22655151827696, 61.08844646287586, 63.75496630128074, -5.837908213963081, -4.2653576530839485, 5.185939700280084, -9.146853651471172, 2.189657957604428, -3.8915240078094593, 2.122977035047002, -3.9272977621104386]}
[   3.1035469    -3.21788848    1.41289404   -2.61899331   -2.84279553
    1.85406607   -1.71452538    2.36886936   57.76946789   31.76365355
  195.02941336   17.73048087   49.97287055   49.27907425   69.5544223
  149.19793895   10.59215463   -2.63668049   11.60005122    2.02831926
  245.26410171  354.62661196   31.29763104  245.98745219    1.31444963
   11.29195975   -2.95033224   36.60081522    5.84713621  -11.34982837
   10.27502943   17.6050143     6.20758404    1.91915234   12.21632988
  -51.3588445    10.94025216  -12.95759564   -0.84848647  -14.62494375
   -1.96949383 -213.73255385  -82.11770727  -83.34225992    1.7690277
  -10.33176476   10.1838816     9.81566164    3.68525782  -50.07562532
    7.00710587    5.78720671   58.97066277  114.22655152   61.08844646
   63.7549663    -5.83790821   -4.26535765    5.1859397    -9.14685365
    2.18965796   -3.89152401    2.12297704   -3.92729776]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [   3.12378958   -3.03530212    1.3316079    -8.14803852   -2.86303821
    1.67147971   -1.63323923    7.89791456   62.61511378   34.55636583
  197.83243137   16.29526915   56.22825334   60.59602156   91.68399717
  172.09880157   10.39859765   -3.34095501   11.45626741    2.00898067
  256.69813528  314.76251602   31.65498714  230.70391973    1.20775485
   11.09790167   -3.13976619   36.43331121    7.64184148  -12.37889244
    8.76369303   21.58830604    6.14394249    0.84208662   13.60896485
  -49.60561635   10.7440663   -11.68625594   -1.66400804  -15.60679658
   -1.44436698 -266.29530244  -87.15119501  -88.3813223     1.86500811
   -6.51059933   10.29763962    9.95541577    4.23023282  -71.46606891
    8.74895785    5.48210238   57.41983676  149.28748464   72.51416036
   77.14382211   -5.92879807   -4.24049531    4.20968855   -8.87150826
    1.93481052   -3.87708937    1.57621978   -3.93629685]
Minimum obj value:-2066.438772160597
Optimal xi: 30.085431703057026
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING-dataset
W_T_mean: 1207.0201855684102
W_T_median: 1133.300748514061
W_T_pctile_5: 905.672696317603
W_T_CVAR_5_pct: 859.4249308307822
F value: -2066.438772160597
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
F value: -2066.438772160597
-----------------------------------------------
{'NN': [3.1237895798930726, -3.035302122609188, 1.3316078960840667, -8.148038515152882, -2.8630382068592324, 1.671479710116595, -1.633239233125569, 7.897914564983409, 62.615113784641636, 34.55636582743195, 197.8324313689218, 16.295269145988563, 56.22825334442116, 60.59602156462959, 91.68399716988594, 172.09880157459483, 10.398597645292622, -3.340955014178998, 11.45626740681281, 2.0089806675031316, 256.6981352828126, 314.76251601843506, 31.65498713810512, 230.70391973178258, 1.2077548464314531, 11.0979016739137, -3.1397661948180633, 36.43331121495756, 7.64184148068619, -12.378892436488695, 8.76369303466704, 21.588306035408223, 6.14394249155861, 0.8420866217644016, 13.608964845557372, -49.60561634556747, 10.744066299162544, -11.686255940435272, -1.6640080448486674, -15.606796577131986, -1.4443669752757704, -266.29530244226993, -87.15119500950685, -88.38132229823607, 1.8650081070759348, -6.510599331977046, 10.297639617556975, 9.95541577489845, 4.230232817631159, -71.46606890629715, 8.748957845634584, 5.482102384559878, 57.419836756830065, 149.28748464312918, 72.51416035819268, 77.14382210881428, -5.928798066217028, -4.240495307993631, 4.2096885539458455, -8.8715082636565, 1.9348105174005952, -3.877089367788406, 1.5762197842698382, -3.9362968470808215]}
[   3.12378958   -3.03530212    1.3316079    -8.14803852   -2.86303821
    1.67147971   -1.63323923    7.89791456   62.61511378   34.55636583
  197.83243137   16.29526915   56.22825334   60.59602156   91.68399717
  172.09880157   10.39859765   -3.34095501   11.45626741    2.00898067
  256.69813528  314.76251602   31.65498714  230.70391973    1.20775485
   11.09790167   -3.13976619   36.43331121    7.64184148  -12.37889244
    8.76369303   21.58830604    6.14394249    0.84208662   13.60896485
  -49.60561635   10.7440663   -11.68625594   -1.66400804  -15.60679658
   -1.44436698 -266.29530244  -87.15119501  -88.3813223     1.86500811
   -6.51059933   10.29763962    9.95541577    4.23023282  -71.46606891
    8.74895785    5.48210238   57.41983676  149.28748464   72.51416036
   77.14382211   -5.92879807   -4.24049531    4.20968855   -8.87150826
    1.93481052   -3.87708937    1.57621978   -3.93629685]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ 3.11872151e+00 -2.89196615e+00  1.34133215e+00 -1.41507373e+01
 -2.85797013e+00  1.52814374e+00 -1.64296349e+00  1.39006133e+01
  6.88342761e+01  3.98297074e+01  1.83356255e+02  1.58630842e+01
  5.87055601e+01  8.09957199e+01  1.17017258e+02  1.94161157e+02
  1.10088498e+01 -3.57010020e+00  1.68629530e+01  2.12450609e+00
  2.49077536e+02  2.77799970e+02  3.94279372e+01  2.12349007e+02
  1.27593247e+00  1.09859859e+01 -3.65171401e+00  3.47934824e+01
  1.20970729e+01 -1.29031005e+01  3.13914992e+00  2.44570698e+01
  6.71646739e+00 -2.47490536e-01  1.44247028e+01 -5.02592336e+01
  1.00109930e+01 -1.10360508e+01 -1.26361549e+00 -1.08122056e+01
 -4.77226077e-01 -2.98993474e+02 -9.21480579e+01 -9.23591936e+01
  2.30582741e+00  1.18582858e+01  1.27318814e+01  1.24035967e+01
  5.75196935e+00 -9.12469138e+01  1.37353084e+01  7.08113722e+00
  6.04094312e+01  1.85876692e+02  8.07594618e+01  8.94811385e+01
 -6.63512046e+00 -4.35077427e+00  9.28256865e-01 -8.84543711e+00
  1.77808836e+00 -3.80817251e+00  9.81285583e-01 -3.98073353e+00]
Minimum obj value:-2309.840247318507
Optimal xi: 29.85187196167731
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING-dataset
W_T_mean: 1223.9585567605372
W_T_median: 1164.9088487021108
W_T_pctile_5: 891.7072047416534
W_T_CVAR_5_pct: 841.0970799458155
F value: -2309.840247318507
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.2
F value: -2309.840247318507
-----------------------------------------------
{'NN': [3.1187215065912897, -2.891966150042753, 1.3413321525363187, -14.150737273378855, -2.8579701335574206, 1.528143737550173, -1.642963489577841, 13.900613323209377, 68.8342760867363, 39.82970735510622, 183.3562546666066, 15.863084167605443, 58.70556010410523, 80.99571990455404, 117.01725771856442, 194.16115674376434, 11.008849792312986, -3.5701002000158026, 16.86295300320735, 2.124506085171857, 249.07753595259484, 277.7999695916719, 39.42793724260859, 212.34900696135566, 1.275932471587454, 10.985985895752151, -3.6517140070023344, 34.79348235930285, 12.097072930548505, -12.903100520363664, 3.139149917605941, 24.457069768343725, 6.716467386609732, -0.24749053553509148, 14.424702825528353, -50.25923357343895, 10.010992979689224, -11.036050821809125, -1.2636154941522733, -10.81220564063712, -0.47722607651195853, -298.9934736789896, -92.14805791697015, -92.35919360745197, 2.3058274078893373, 11.858285822739184, 12.731881365412908, 12.403596713854379, 5.751969347479565, -91.2469137858349, 13.735308385410963, 7.081137215572339, 60.40943123749906, 185.8766917127515, 80.75946178822477, 89.48113853297804, -6.635120459368918, -4.3507742729965315, 0.9282568651888525, -8.845437107277906, 1.7780883603748094, -3.808172505120851, 0.9812855831805387, -3.980733532994208]}
[ 3.11872151e+00 -2.89196615e+00  1.34133215e+00 -1.41507373e+01
 -2.85797013e+00  1.52814374e+00 -1.64296349e+00  1.39006133e+01
  6.88342761e+01  3.98297074e+01  1.83356255e+02  1.58630842e+01
  5.87055601e+01  8.09957199e+01  1.17017258e+02  1.94161157e+02
  1.10088498e+01 -3.57010020e+00  1.68629530e+01  2.12450609e+00
  2.49077536e+02  2.77799970e+02  3.94279372e+01  2.12349007e+02
  1.27593247e+00  1.09859859e+01 -3.65171401e+00  3.47934824e+01
  1.20970729e+01 -1.29031005e+01  3.13914992e+00  2.44570698e+01
  6.71646739e+00 -2.47490536e-01  1.44247028e+01 -5.02592336e+01
  1.00109930e+01 -1.10360508e+01 -1.26361549e+00 -1.08122056e+01
 -4.77226077e-01 -2.98993474e+02 -9.21480579e+01 -9.23591936e+01
  2.30582741e+00  1.18582858e+01  1.27318814e+01  1.24035967e+01
  5.75196935e+00 -9.12469138e+01  1.37353084e+01  7.08113722e+00
  6.04094312e+01  1.85876692e+02  8.07594618e+01  8.94811385e+01
 -6.63512046e+00 -4.35077427e+00  9.28256865e-01 -8.84543711e+00
  1.77808836e+00 -3.80817251e+00  9.81285583e-01 -3.98073353e+00]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ 2.95519999e+00 -2.48970360e+00  1.47244429e+00 -2.16907105e+01
 -2.69444862e+00  1.12588119e+00 -1.77407563e+00  2.14405866e+01
  7.28536012e+01  5.24585608e+01  1.76786649e+02  1.48646935e+01
  6.22155439e+01  1.09371269e+02  1.54106628e+02  2.21869800e+02
  1.18747426e+01 -3.75890565e+00  1.79620802e+01  2.42490740e+00
  2.34708230e+02  2.36483262e+02  4.46632455e+01  1.86821440e+02
  1.38729820e+00  1.04467080e+01 -3.60596796e+00  3.28243170e+01
  1.18207888e+01 -1.25103783e+01  3.32681737e+00  2.91354096e+01
  7.28752735e+00 -1.57882425e+00  1.62877335e+01 -5.13007419e+01
  1.00104344e+01 -1.17988992e+01 -1.73970401e+00 -7.19334280e+00
  1.11399069e-01 -3.09979946e+02 -9.77887250e+01 -9.64948833e+01
  3.31456800e+00  1.55805274e+01  1.68637740e+01  1.66020356e+01
  9.23947875e+00 -1.01319590e+02  2.03119176e+01  8.57950109e+00
  6.52334438e+01  2.36923405e+02  8.33219624e+01  9.73914748e+01
 -7.18002923e+00 -4.47964355e+00 -2.23143319e+00 -7.35030325e+00
  1.51434358e+00 -3.71162249e+00  4.20709261e-01 -3.98540282e+00]
Minimum obj value:-2679.0658471741294
Optimal xi: 29.61984526436995
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING-dataset
W_T_mean: 1236.7570124965841
W_T_median: 1192.7211961450055
W_T_pctile_5: 878.0483514742712
W_T_CVAR_5_pct: 823.9393014833629
F value: -2679.0658471741294
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
F value: -2679.0658471741294
-----------------------------------------------
{'NN': [2.9551999937363727, -2.4897036045973953, 1.472444288760528, -21.690710502544924, -2.694448620702595, 1.1258811921048109, -1.7740756258021204, 21.440586552375457, 72.85360115140557, 52.45856079590202, 176.7866493523576, 14.864693542725462, 62.21554385314789, 109.37126882693946, 154.10662781444, 221.86979994165475, 11.874742601142701, -3.7589056543659973, 17.962080223244186, 2.424907400302699, 234.70823008436804, 236.48326193981666, 44.663245543735094, 186.82144003256394, 1.3872982003136562, 10.44670800636939, -3.6059679610273294, 32.82431701361823, 11.820788775739357, -12.510378258306515, 3.3268173744307483, 29.1354095539817, 7.287527348374134, -1.578824250353722, 16.287733537454077, -51.300741945883374, 10.010434444906998, -11.798899243236448, -1.7397040064680025, -7.193342799826018, 0.11139906890370277, -309.979945686688, -97.78872503831758, -96.49488328836856, 3.314567997089062, 15.580527407340165, 16.86377397071917, 16.602035570060885, 9.239478753960524, -101.31959013057948, 20.311917643688723, 8.579501085168236, 65.23344381374994, 236.9234046536741, 83.32196242787631, 97.39147478812815, -7.18002923421627, -4.479643554805622, -2.2314331896696094, -7.350303254241207, 1.5143435751465202, -3.711622486670821, 0.4207092610668092, -3.9854028163396515]}
[ 2.95519999e+00 -2.48970360e+00  1.47244429e+00 -2.16907105e+01
 -2.69444862e+00  1.12588119e+00 -1.77407563e+00  2.14405866e+01
  7.28536012e+01  5.24585608e+01  1.76786649e+02  1.48646935e+01
  6.22155439e+01  1.09371269e+02  1.54106628e+02  2.21869800e+02
  1.18747426e+01 -3.75890565e+00  1.79620802e+01  2.42490740e+00
  2.34708230e+02  2.36483262e+02  4.46632455e+01  1.86821440e+02
  1.38729820e+00  1.04467080e+01 -3.60596796e+00  3.28243170e+01
  1.18207888e+01 -1.25103783e+01  3.32681737e+00  2.91354096e+01
  7.28752735e+00 -1.57882425e+00  1.62877335e+01 -5.13007419e+01
  1.00104344e+01 -1.17988992e+01 -1.73970401e+00 -7.19334280e+00
  1.11399069e-01 -3.09979946e+02 -9.77887250e+01 -9.64948833e+01
  3.31456800e+00  1.55805274e+01  1.68637740e+01  1.66020356e+01
  9.23947875e+00 -1.01319590e+02  2.03119176e+01  8.57950109e+00
  6.52334438e+01  2.36923405e+02  8.33219624e+01  9.73914748e+01
 -7.18002923e+00 -4.47964355e+00 -2.23143319e+00 -7.35030325e+00
  1.51434358e+00 -3.71162249e+00  4.20709261e-01 -3.98540282e+00]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [   2.96615322   -2.63321944    1.40697616  -11.56775336   -2.70540184
    1.26939703   -1.7086075    11.31762941   81.43636038   83.18607782
  198.58871189   16.28628433   70.49335913   98.01512257  153.08251467
  235.78674863   12.58602862   -1.84639203    9.80186917    2.40515919
  224.63385933  217.50062222   45.67123711  177.80073015    1.13688164
   10.08736861   -3.04090227   22.35075665   12.74962724  -12.21078078
    1.6613669    22.61841246    7.61087359   -0.9087727    18.79596836
  -50.77059238   11.10572968  -12.56218612   -2.50918397   -2.88386453
    1.14244669 -313.34307688 -103.15724832 -102.69841152    3.63432582
   14.3235487    19.81999788   18.65138672   11.4561511  -108.24664017
   28.7319235    15.84033161   70.14547702  277.60105523   85.63969024
  103.33230178   -8.68468969   -4.90747167   -3.27368288   -6.14011194
    1.33808836   -3.82694066    0.65928594   -4.02982827]
Minimum obj value:-3299.9310729401122
Optimal xi: 29.392535634322584
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING-dataset
W_T_mean: 1245.9073019272705
W_T_median: 1209.3374374894238
W_T_pctile_5: 864.6386169888046
W_T_CVAR_5_pct: 808.1265993589759
F value: -3299.9310729401122
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.0
F value: -3299.9310729401122
-----------------------------------------------
{'NN': [2.9661532161898077, -2.633219444333703, 1.4069761623597006, -11.567753363801938, -2.705401843155819, 1.269397031841083, -1.7086074994012894, 11.317629413632549, 81.43636037733896, 83.18607782087079, 198.58871188529938, 16.286284327959628, 70.49335912794395, 98.01512257133027, 153.08251467253896, 235.78674862883506, 12.586028617872485, -1.8463920342484843, 9.801869171756719, 2.4051591931419254, 224.63385933223995, 217.50062222464078, 45.67123711414847, 177.80073014933853, 1.136881635235208, 10.087368614296638, -3.0409022699780057, 22.35075665043181, 12.749627237535915, -12.210780784881834, 1.6613669040339976, 22.618412461290173, 7.610873589155795, -0.908772699859054, 18.795968355191814, -50.77059238382455, 11.105729683990877, -12.562186121569438, -2.5091839681579673, -2.8838645339051983, 1.142446689273615, -313.34307687873195, -103.15724831881256, -102.69841151590302, 3.6343258189528242, 14.32354869798212, 19.819997880776313, 18.651386717087835, 11.456151100815392, -108.24664017279166, 28.73192349588909, 15.840331611560567, 70.14547702167829, 277.6010552260936, 85.63969024197233, 103.33230178132821, -8.684689692590029, -4.907471671287251, -3.273682883777308, -6.140111939756316, 1.3380883556319845, -3.8269406618028805, 0.65928593877112, -4.0298282665660805]}
[   2.96615322   -2.63321944    1.40697616  -11.56775336   -2.70540184
    1.26939703   -1.7086075    11.31762941   81.43636038   83.18607782
  198.58871189   16.28628433   70.49335913   98.01512257  153.08251467
  235.78674863   12.58602862   -1.84639203    9.80186917    2.40515919
  224.63385933  217.50062222   45.67123711  177.80073015    1.13688164
   10.08736861   -3.04090227   22.35075665   12.74962724  -12.21078078
    1.6613669    22.61841246    7.61087359   -0.9087727    18.79596836
  -50.77059238   11.10572968  -12.56218612   -2.50918397   -2.88386453
    1.14244669 -313.34307688 -103.15724832 -102.69841152    3.63432582
   14.3235487    19.81999788   18.65138672   11.4561511  -108.24664017
   28.7319235    15.84033161   70.14547702  277.60105523   85.63969024
  103.33230178   -8.68468969   -4.90747167   -3.27368288   -6.14011194
    1.33808836   -3.82694066    0.65928594   -4.02982827]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [   2.86805601   -1.71175725    1.45218967   -5.00599106   -2.60730464
    0.34793484   -1.75382101    4.75586711  105.43544946   88.45943169
  195.21437793   19.84407397   79.52197773   98.68135167  153.60447835
  244.91836574   14.12574843   10.76561425   11.89277037    3.25168575
  218.46752387  215.83200166   51.39636395  166.18186521   -0.44965869
   10.47293936   -2.19592162   12.91012941   16.50376939  -10.8320315
   10.77143282   36.38167356   10.62741092    1.27189222   23.02542392
  -49.45378804   13.84451914  -12.5349869    -2.90735422   -2.15861756
    6.39908536 -308.07991222 -112.51119968 -109.05571419    5.52031105
   17.37040009   19.08457025   19.07274267   13.41954099 -115.29623484
   48.39395996   27.56980329   81.45988769  299.48801452   77.67196897
  104.01605931   -8.02484238   -5.12047872   -1.51230552   -5.39552191
    2.25513669   -4.18529288    0.48998735   -4.66796081]
Minimum obj value:-4549.86889259978
Optimal xi: 29.204716499244114
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING-dataset
W_T_mean: 1253.4114159859132
W_T_median: 1218.2766051711299
W_T_pctile_5: 853.6029941334828
W_T_CVAR_5_pct: 789.6431103230434
F value: -4549.86889259978
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
F value: -4549.86889259978
-----------------------------------------------
{'NN': [2.868056011099238, -1.7117572499609737, 1.45218966989537, -5.005991059826288, -2.6073046380652385, 0.3479348374683463, -1.753821006936969, 4.75586710965691, 105.43544945910409, 88.45943169366318, 195.21437793434433, 19.8440739651835, 79.52197773136403, 98.68135167146983, 153.60447835343547, 244.9183657448738, 14.12574842625149, 10.765614253490092, 11.892770367091869, 3.25168575161184, 218.46752387187345, 215.83200166485614, 51.396363946975434, 166.18186520927878, -0.4496586855627716, 10.472939358883107, -2.1959216167714204, 12.910129413203158, 16.503769394307465, -10.832031503491645, 10.771432816510394, 36.381673563441375, 10.627410923679873, 1.2718922226946234, 23.025423917056404, -49.45378804139566, 13.844519143471636, -12.53498690114713, -2.9073542181807346, -2.158617559726405, 6.3990853568358474, -308.0799122247985, -112.5111996845409, -109.05571418630299, 5.5203110525213015, 17.370400090093323, 19.08457025353542, 19.072742669718057, 13.419540993206034, -115.29623484388527, 48.39395996432535, 27.569803294047155, 81.45988768958044, 299.48801452086406, 77.67196897078156, 104.01605930610665, -8.024842382327108, -5.12047871686106, -1.5123055193566113, -5.395521910601737, 2.255136687963916, -4.185292883393578, 0.4899873518624249, -4.66796081159517]}
[   2.86805601   -1.71175725    1.45218967   -5.00599106   -2.60730464
    0.34793484   -1.75382101    4.75586711  105.43544946   88.45943169
  195.21437793   19.84407397   79.52197773   98.68135167  153.60447835
  244.91836574   14.12574843   10.76561425   11.89277037    3.25168575
  218.46752387  215.83200166   51.39636395  166.18186521   -0.44965869
   10.47293936   -2.19592162   12.91012941   16.50376939  -10.8320315
   10.77143282   36.38167356   10.62741092    1.27189222   23.02542392
  -49.45378804   13.84451914  -12.5349869    -2.90735422   -2.15861756
    6.39908536 -308.07991222 -112.51119968 -109.05571419    5.52031105
   17.37040009   19.08457025   19.07274267   13.41954099 -115.29623484
   48.39395996   27.56980329   81.45988769  299.48801452   77.67196897
  104.01605931   -8.02484238   -5.12047872   -1.51230552   -5.39552191
    2.25513669   -4.18529288    0.48998735   -4.66796081]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [  -4.52723117   -9.92535635   -8.4374775   -12.34845329    4.78798218
    8.56153358    8.13584588   12.09832894  105.42986926   88.49452021
  192.91509489   22.47424804   72.30747341   98.6705893   153.41955485
  240.85732187    9.03427781   10.73966545    4.00952066   -1.62021142
  212.04473603  215.8311201    48.99632845  160.24142458    4.67619796
   13.82128545    2.16048697   15.59852177   16.53677155  -10.74938993
   11.03179483   35.96395928   17.3049735     8.31248027   30.20785557
  -42.03136477   16.9369221    -9.21207216    0.63715785    2.85977409
    4.52501407 -311.09782431 -112.2253829  -113.04318911    0.81895638
   13.83644759   16.41243178   15.66059376   16.51403306 -112.42411733
   49.44466283   30.87141791   81.17765887  298.69637221   78.43136978
  104.08076015   -3.29308487   -9.2558823     1.12542711   -7.30114096
    4.23977277   -5.31314839    3.56582313   -5.03228854]
Minimum obj value:-16005.166170461434
Optimal xi: 25.01074392780787
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/pieter_code_all_branches/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING-dataset
W_T_mean: 1551.5577480633933
W_T_median: 1405.4383972928833
W_T_pctile_5: 624.6754498172772
W_T_CVAR_5_pct: 489.5910351982233
F value: -16005.166170461434
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 10.0
F value: -16005.166170461434
-----------------------------------------------
