Starting at: 
2022-09-21 12:00:10

 Random seed:  1  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = basic_T30_VWD
timeseries_basket['basket_desc'] = CRSP data: T30 and VWD
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
5.0% of MC simulations done.
10.0% of MC simulations done.
15.0% of MC simulations done.
20.0% of MC simulations done.
25.0% of MC simulations done.
30.0% of MC simulations done.
35.0% of MC simulations done.
40.0% of MC simulations done.
45.0% of MC simulations done.
50.0% of MC simulations done.
55.00000000000001% of MC simulations done.
60.0% of MC simulations done.
65.0% of MC simulations done.
70.0% of MC simulations done.
75.0% of MC simulations done.
80.0% of MC simulations done.
85.0% of MC simulations done.
90.0% of MC simulations done.
95.0% of MC simulations done.
100.0% of MC simulations done.
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  hidden_layer    None       None         None   
4        obj.layers[4]        4  hidden_layer    None       None         None   
5        obj.layers[5]        5  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  
4    False        None  
5    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  hidden_layer       4  logistic_sigmoid   
4        obj.layers[4]        4  hidden_layer       4  logistic_sigmoid   
5        obj.layers[5]        5  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 4)    False        None  
4       (4, 4)    False        None  
5       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ -1.37958928   2.71412946  -1.48691026   0.70356448   1.63415799
  -2.82282115   1.88584753  -1.41959286 115.78598589  11.77091291
  25.39444607 128.53712184  21.77274391   0.82056782   1.31551464
  22.14535872  20.16581138 125.21024102  73.04018228  14.34339332
 -21.11643366  -1.76256982  35.72750291 -25.56367463  12.75971146
   3.25145586  -0.61001683 -13.68452175   3.44496168   1.20947329
  -1.14405306  -6.74159139   6.79033327   6.7871995  -11.17539462
 -15.25352256  12.62083897   3.1193189   -0.20949746 -14.21205674
 -72.25321365 -66.65984718  27.20617823   0.92050797 -29.21251396
 -59.04904697  -4.28147086 -28.91273795  -3.90013677  -1.97894829
  46.35359075  37.78477863   3.35626991   3.57989198  17.92732502
  14.974637    -2.7207267   -5.14509305  -1.26542984  -5.32123418
   4.92175912  -3.35662974   4.71394576  -4.73059338]
Minimum obj value:-1045.947992317105
Optimal xi: 31.098836465970624
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1060.8126289818495
W_T_median: 1049.416344820822
W_T_pctile_5: 967.619297098326
W_T_CVAR_5_pct: 939.8726381439236
F value: -1045.947992317105
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.1
F value: -1045.947992317105
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ -3.13600193   3.71954811   4.70489248   1.29020732   2.17196475
  -4.0326188   -5.61101707  -1.20845718  29.81228639  15.34299743
   8.52371886  38.59551558  52.60674305  39.29979876   7.33582656
  49.61973465  50.91528384  38.31296424   6.1750384   44.02768335
   2.9706646   -6.50271653 -29.89231107  -2.6841216    2.28801366
  -1.9510795   -5.7138597   -6.11325397   2.10525067  23.07007494
  43.21145207  44.81970831  27.91943171  16.48543564   4.48875218
  13.71474003   6.38084879   3.88719889  20.65208671  28.16896478
 -49.24999203   2.52460362 -30.16879291   5.77649639 -22.50343485
   6.01662916  -1.64776787   5.40034994 -13.54761594  13.48112546
   0.36247261   4.29476209 -12.99499009  11.00025871   0.2669778
   9.07957923   4.41376622  -3.08287228  -3.48379656   1.48741455
   4.39182719  -3.58981988   6.39370968   2.91870163]
Minimum obj value:-1199.6902507151158
Optimal xi: 30.897291581864092
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1081.3063326739796
W_T_median: 1054.4026976113983
W_T_pctile_5: 955.3684545815215
W_T_CVAR_5_pct: 929.3726162427549
F value: -1199.6902507151158
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.25
F value: -1199.6902507151158
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ -2.90778414   2.38367591   3.3746765    2.47511109   2.49094839
  -2.24339431  -2.65004587  -2.57202451  10.69479511  91.75220575
  22.77591918  48.98957648  27.26663085   1.24770656   7.74405113
   1.89377835   3.18995019  28.10648116  56.97183151  44.26634763
   3.49439861  87.6663279   65.1330344  110.6077306   44.81330305
  19.12106396   0.52957582  73.77306896   7.84090801   5.50097418
   0.76080395   2.96688058  -1.24768694 -10.31577066  13.63251077
   3.63008775   5.49704797  -1.73130741  -5.02708063   5.34899673
  14.01290574   1.07521607  -6.36461349   6.980113    62.38982073
  64.72808307  -3.44484352  -0.13549839 -31.44129701 -28.32521843
  -0.5732874    4.6866761   15.48585962  -2.30813303   0.92323048
  -7.56830577   5.39988166  -3.38622889   2.86894086  -3.51750877
  12.97423487   0.99875176   4.43442031   1.54920203]
Minimum obj value:-1371.6443663169182
Optimal xi: 30.968170223539143
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1100.8949131251222
W_T_median: 1060.4099574694594
W_T_pctile_5: 959.5996913475914
W_T_CVAR_5_pct: 931.2948856007181
F value: -1371.6443663169182
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.4
F value: -1371.6443663169182
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ -4.92945451 -14.5700583    4.03343362   4.57968965   5.42075056
  13.65818146  -3.47637625  -3.98411193  -6.61034556   2.44227166
  25.0407551    6.47893942  12.85094199 110.14013694 144.81828992
  69.74634934   7.42715054 -22.98996018  48.07545756  55.50794743
   1.41970128   1.73589389  -0.65631375   3.00759458  -1.8936869
 -22.96032319  80.47159939  46.07995637  -6.96477899  11.2060126
  16.13523217  20.8763724    3.80221027  84.8376399    6.62300992
  16.35930142 -29.00063673 -62.42621584   5.14566249   2.78089695
 -17.64047749  -3.23238298  12.35380418   6.69604036  -8.46197067
   5.75955771  25.1482573    4.31701991  19.30820016   4.58944619
  -3.42976129   2.36804524  47.4310701   59.99883795  -2.15593099
   0.8745645    3.63055807  -2.63541816   1.42155627  -3.23270767
   4.94138162   1.41246479  -9.48688978   3.4149731 ]
Minimum obj value:-1602.8593848102794
Optimal xi: 30.525193225819702
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1185.065467586914
W_T_median: 1078.7907302776862
W_T_pctile_5: 930.2624350598232
W_T_CVAR_5_pct: 891.8701929818633
F value: -1602.8593848102794
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.6
F value: -1602.8593848102794
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [  5.29195691   1.5305186   -6.89167734 -12.37825137  -5.47551104
  -1.35792073   6.45315975  12.53972661   4.1500026    2.24196745
  -8.0842898   10.87041915  43.42270602  26.28974902   2.30392393
   8.30954543   8.28065422  45.13940545  -9.62837802  -6.25924867
  69.62540782  90.97290034   3.30540384   5.50637356 -24.02438699
  -7.1205914    3.13676891   4.15194394  40.80651817  12.92183001
   4.92362299  33.77042338 -12.11404206   9.95017238  68.00531257
  29.82573472 -28.78504023  -2.01642929  71.62284685  38.76576639
  11.4468512  -14.12651459  34.37177928  11.43576635   1.82040904
 -12.57262268   2.22138452  15.74549312   7.10796443  11.39930894
   0.58631091  -2.20571144  48.80830144  50.53930942   1.09528779
  -3.10194513   0.49605582  -2.59973242   4.12379238  -2.49604177
   2.18128684   3.12904114   5.65245985   1.11939554]
Minimum obj value:-1726.5291426740778
Optimal xi: 30.02465084823279
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1257.4009255440046
W_T_median: 1103.6953824302075
W_T_pctile_5: 903.5894823769415
W_T_CVAR_5_pct: 846.4367060056385
F value: -1726.5291426740778
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.7
F value: -1726.5291426740778
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ -1.53576277 -16.58447932  -0.61727407   1.49604867   1.04063808
  15.4933965    1.70856387  -0.31758995   4.01495168  23.02674059
  13.72495091   3.24086821   0.21206343  46.96522568   7.05832089
  -2.05378357  -2.86491568   4.64848847  47.38704416  -2.22536258
  14.11113387   0.25266151  -2.20739777  33.77617828 -46.24982613
  67.57109903   2.43082325 -19.21836606  -1.00276159  -0.6972831
 -28.32290418  12.0737826    7.33349777 -11.5145697    3.81419143
  84.61516369  -3.92294113  24.48374976   4.52237985 -97.87146174
   2.08326053   3.35720981 -18.44626158   6.33002918   2.0401946
   1.66680432  46.07830084  -7.80920193  -4.70889386  -2.12017018
   7.36705807 -10.9036042   24.65814365  21.76584348  -1.73277329
  14.93771467  -9.30691601   3.70007594  -5.66439699   3.30591901
   2.78539894  -2.3708393    4.51961012   3.12646916]
Minimum obj value:-1828.670338100591
Optimal xi: 29.362795986113866
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1276.7651938390652
W_T_median: 1127.0787182673384
W_T_pctile_5: 862.8836183735867
W_T_CVAR_5_pct: 807.262562896443
F value: -1828.670338100591
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.8
F value: -1828.670338100591
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-1.75919396e+00  2.99568998e+00  7.05797562e+00 -3.49668328e+00
  3.64571134e-01 -2.84919809e+00 -8.31913170e+00  3.88756917e+00
  3.40408557e+00 -3.36089603e+00 -2.77802762e+01 -4.38088767e+01
  2.80765697e+00  2.02300000e+01  2.77981142e+01  4.40066856e+01
  2.85162264e+01  7.71944356e+00  1.95698341e+02  7.73121390e+00
  2.29032538e+01  1.47637385e+00 -4.55314746e+01 -3.90728902e+01
  5.46989928e+00 -1.54357409e+01  4.80415942e+00 -4.73952763e+00
 -1.33718465e+01 -1.68234588e+01  1.64486933e+01 -7.45579717e-01
  4.08923511e+01  1.03975082e+01  6.65378331e+00  1.01696081e+01
 -2.19342722e+01 -3.54067002e+00  2.04968330e+01  5.67468959e+01
  6.06866185e+00 -1.06497000e+01  1.58143832e-01  1.81039566e+01
  1.59014002e+00  1.11153323e-01 -2.03729656e+01  3.29111967e+00
  4.08143104e+00  4.90633164e+01 -7.15078093e+01  6.55073416e-01
  2.81507854e+01  3.24880999e+01  1.63774018e+01  3.37822364e-01
  7.54324307e+00  1.19135471e+00  6.88128047e+00 -1.04668346e+01
  1.19098031e+00 -1.72406219e+00  5.03991710e+00  2.92739264e+00]
Minimum obj value:-1986.8708883963607
Optimal xi: 28.38195395546426
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1405.0913180588302
W_T_median: 1177.9243822589206
W_T_pctile_5: 816.7365567547065
W_T_CVAR_5_pct: 723.9959511884352
F value: -1986.8708883963607
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.9
F value: -1986.8708883963607
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [  6.38635679  -1.64961891 -12.9016339  -10.79283402  -6.47035177
   0.41333396  12.78162593  10.87655759  -0.61318361  32.84691297
   6.71322085   3.34074303  74.50446964  -1.48362692 -74.39899484
   1.94425535   4.76908984   3.40537071  -3.58660336  42.9427713
   6.01581859  10.82604678 -14.51506013  29.17599263  32.80639768
   7.48257947  14.89894487  -9.68870002  -7.96031596  -5.04055675
  -4.02469541  12.42492916   1.63891034  -7.06657147   0.96335509
  66.58438576   0.4557974    5.74323634   4.89644122 -11.74238312
   4.37890294 -75.54317653   3.11275056 -73.75959461 -42.26344148
  -7.84591445  58.77518125   0.85681131  -1.12467913 -35.45557337
   6.83900201 -16.12770215  -1.21397464   3.24123082  -2.94973178
   4.28097163  28.10433314   3.5295871    5.75851282  -2.95367639
   7.64252487   3.1066088    4.58824485  -2.64206146]
Minimum obj value:-2101.118613403616
Optimal xi: 27.348453104958065
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1437.9176223909813
W_T_median: 1232.31156372108
W_T_pctile_5: 748.1491149492725
W_T_CVAR_5_pct: 663.2012388612154
F value: -2101.118613403616
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
F value: -2101.118613403616
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ -1.44923276  -0.45139298  -4.32197413   4.93813224   1.13121242
   2.16840224   4.20524361  -5.10296694  28.72205588  55.35134263
   0.42026001  35.71550352  -5.08385562   0.52357598 -10.19426194
   3.26628823 -11.68734957  -5.81998698  13.38253288  26.40094251
  40.71251682   8.32629529   0.32685619   3.5382421   49.18240046
  -0.14417472 -55.13984406   8.67744077   4.19623725   8.40050058
 -14.57920533 -24.3837514   42.29388714  27.63269544   3.1774255
   1.32787131   4.8088696   -3.30595538   5.11209478  10.40191943
  13.56155977  -2.46872159   4.79067946  15.10045122 -84.07276003
   2.23664737   0.74438582 -95.42324151  -4.49806868  16.43921144
   2.92591027  -6.29057379  31.59384194   0.21678059  46.42222431
  42.56050653   1.83409311  -2.4294837    4.94012297   2.64164661
  -7.85185012  -2.73594665   0.91867312  -2.48188484]
Minimum obj value:-2278.66850061693
Optimal xi: 27.538742466972565
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1468.8335386013125
W_T_median: 1261.8005532342227
W_T_pctile_5: 760.1931060725601
W_T_CVAR_5_pct: 662.9981049681904
F value: -2278.66850061693
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.1
F value: -2278.66850061693
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-1.07853835e+01 -1.20687006e+00  3.69444611e+00 -2.30696990e+01
  9.49701033e+00  1.98862323e+00 -2.31431824e+00  2.21671197e+01
 -1.11648456e+01  2.29036497e+01 -1.34748872e+01  7.92199737e+00
 -1.01686483e+01  5.93381137e-01 -4.68762413e+00  4.23554029e+01
  3.16581999e+01  3.40777265e+00  9.43988488e+01 -4.23779039e-02
  5.18571829e+01  9.34158519e+00 -3.07158602e-01  1.25853268e+01
 -3.70559449e+00 -7.39285587e+00 -1.56018106e+01  1.64173013e+01
  7.76913738e+00 -7.90672035e+01 -1.08888019e+01 -6.35304634e+00
  4.75689090e+00 -7.95384561e+00 -1.92264092e+00  3.82157987e+00
 -4.30245692e+00  2.84932977e+01  9.01529962e+00  4.14733302e+00
 -1.61648637e+00  1.64847116e+01  2.58982424e+01 -3.00625380e+00
  3.18848035e+01  1.88632981e-01 -2.10602336e+01  2.69680183e+01
  1.07202581e+01 -6.91131861e-02 -2.73748413e+01  8.43047955e+00
  1.75140212e+00 -7.18381795e+01 -1.03925853e+02  1.59843208e+00
  6.19778978e+00  1.53544190e+00 -9.56076291e-02 -2.40594443e+00
  1.77155053e+00 -2.60211349e+00  4.97171712e+00  7.65830007e-01]
Minimum obj value:-2426.265801217368
Optimal xi: 27.23621954849074
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1486.9137764167178
W_T_median: 1293.7556760437606
W_T_pctile_5: 745.9197962354905
W_T_CVAR_5_pct: 642.2018442589813
F value: -2426.265801217368
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.2
F value: -2426.265801217368
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [   2.49087456   -5.0344293    -4.38185304   -1.2183361    -3.86891712
    6.08110599    5.19696608    1.30353811    1.72144091    4.70917087
   46.47513121   22.20813702   51.51097992    7.86993479   -5.26109584
   -7.35258689   12.09758608    8.54838228   -6.34992665   -6.19816363
    9.66297843   -3.04145238   -3.90855875   -2.74300113    6.16429531
    0.86974461    4.21296201    6.02857356  -46.33679891    4.12420386
 -104.33133055   85.70291503    5.59081547   59.71471993   -3.48522785
   95.83416388    0.23316878   29.45684644   -3.59821563  170.95171635
    5.49211034   35.13133783    0.91248418   23.85083147    8.2900005
   18.94650379    1.35710339   -1.42993075   -0.73377902   -6.4672301
   17.61290781   -8.74956186   65.69059497   85.66554964    3.74158686
   -4.96964627   -3.90341746   -2.53139186    0.86510562   -2.27819867
   10.37042062    6.88343567   16.43178406    5.4316033 ]
Minimum obj value:-2572.010659352047
Optimal xi: 26.85769746187858
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1501.2971115069447
W_T_median: 1327.3164926869395
W_T_pctile_5: 725.507644532942
W_T_CVAR_5_pct: 620.5135839860185
F value: -2572.010659352047
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.3
F value: -2572.010659352047
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [  3.27355299 -12.186248    -9.26651469 -10.58264353  -2.61166943
  11.89162748   8.59253159  11.09311206 -10.35605046  62.50663181
  10.49365357  17.45172875   8.47209956  -3.59185115   0.44111345
  -5.13176506   2.70658524  -0.98237531  30.26110169   0.55721466
   6.39322249  -0.60460278  32.32659915   2.38776137  19.84327848
 -22.22711069 -17.31108152   7.08085272  -4.60290433 -16.37761997
   6.71301555   3.91278657  10.68180364  26.59586247   3.36292034
 -14.43806768 -12.71156657 -37.12749117  -3.69544412   8.74016446
  14.17338959  -2.18185016 -17.8252963   -2.85663524  36.63939146
  -0.22031521   8.18709562   0.750943     2.39210555 -76.05724121
 -86.59383019 -78.58219439  -2.17976799  10.95925183  14.57824392
  11.30258944   6.41761767   1.43501585  -0.59456484  -2.69173099
   1.16643243  -2.79659397  -0.52814739  -2.68619348]
Minimum obj value:-2725.194457183192
Optimal xi: 26.77498656937972
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1504.7392642344714
W_T_median: 1338.1900692476127
W_T_pctile_5: 721.1054753916109
W_T_CVAR_5_pct: 618.7897753842786
F value: -2725.194457183192
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.4
F value: -2725.194457183192
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ -2.3250515   -9.52875336  -9.53937989  -9.27202588   0.96203309
   9.07180977   9.05472177   9.18664245 -20.69886203  20.13310497
  -2.09758667   1.06849779 -14.66623757  17.56139392   2.22681008
   9.53563611 -14.20995879  17.69233381   2.1630717    9.13124902
 -15.94587623  13.89360785   1.93647564  12.41245383  13.81583843
  -8.620806     0.80315046  -4.45494112  -1.52941317  13.21442915
  18.8265432   15.56386      6.33666003  -6.58338701  -5.77379648
  -3.17243748 -17.26042648   7.37178633   4.21175059   2.78934652
  28.33839371  19.47288495 -20.37998522   6.38250119 -36.37709114
 -24.83872518   6.5659403   -1.78369164 -47.17740758 -29.97382409
   2.90622335 -59.38616349  42.47184836  32.2442848   22.34770727
  21.38744286   2.78495672  -3.76514654   2.96734995  -3.79132909
   9.85451704   3.44478576  -1.17885269  -4.15760329]
Minimum obj value:-2844.938470898362
Optimal xi: 25.841672870091966
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1518.8052448298183
W_T_median: 1363.6894693939632
W_T_pctile_5: 669.1730508530785
W_T_CVAR_5_pct: 566.7382196824783
F value: -2844.938470898362
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
F value: -2844.938470898362
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ -3.89123888   5.59383551  -3.10338404  -4.5726536    2.73020381
  -5.36445769   3.56181642   4.40415683  49.84550396  32.43464849
   1.72990843   3.1570932   16.2310278    4.11326056  40.25576441
  65.22243974  -6.77544331  14.14177085 -32.24462048 -39.17852495
  23.88598264  12.09176887 -27.77563143 -39.91955987 -11.93334168
   7.01209336  -1.14204935   4.95121201   5.03605524 -63.58799168
 -16.13125897  15.55004448   3.75620025 -25.70897694   0.67751627
  29.92841508  13.1421261  -13.94487737  -1.40370581  46.31349234
  33.01020076  22.637895    25.96114196  12.56325355  14.26246724
  21.03232366  -6.02748552  33.86929823 -78.44629306 -63.43129466
 -62.35516196  -0.79302159   8.78575316  29.82164216   4.28964786
  -4.54803833  -5.38760445  -2.81818402   1.46644755  -2.17697715
 -10.43668433  -4.68042213  15.12032914   5.13906445]
Minimum obj value:-2999.068306717853
Optimal xi: 26.117009582501964
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1517.5246040496413
W_T_median: 1371.8835192493268
W_T_pctile_5: 653.4615647851303
W_T_CVAR_5_pct: 575.5100216647548
F value: -2999.068306717853
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.6
F value: -2999.068306717853
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [ -0.52456124   0.07031856  -0.53752256  -1.75518461   0.30374644
   1.05152974   1.21801774   0.81163966  -9.08649012  -8.46016713
   5.19425806  -9.27784294  -9.32353948  -8.53078848   8.31660391
 -10.27466865 -14.50971204 -16.32523288  -4.00336698 -15.11153803
 -17.44462328 -17.69011584 -11.76525506 -17.18168928 -25.63875381
 -21.22341897  21.35585285  18.23295268 -26.58567551 -19.91683079
  21.12811345  18.3596066  -32.88143163 -28.21187284  13.97249635
  10.5776033  -25.83157515 -21.10042918  21.03880886  18.39874858
  14.1939477    4.46365771  27.74060267  -7.28193079  -4.25419102
  -6.08175343  29.81818204 -12.69361815  48.28848288  45.91206062
  -3.22076384  36.68401311  48.27252228  46.43270052  -2.65763891
  36.3969987    1.75435296  -5.02684962  -2.54032059  -6.318865
  18.94260378   7.43312743  -8.6277584   -8.36566064]
Minimum obj value:-3127.9069684664237
Optimal xi: 25.09758244546627
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1547.2935307907871
W_T_median: 1401.1467064253197
W_T_pctile_5: 630.1542073514339
W_T_CVAR_5_pct: 497.50817186786975
F value: -3127.9069684664237
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.7
F value: -3127.9069684664237
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-4.69083166 -2.88580518 -4.54253141 -0.31385689  5.19348961  3.57739111
  4.55259108  0.60364508 -2.55531297 -2.30211022 -1.1241517  -0.75724823
 -1.9032209  -1.27336336 -1.20322709  0.78571179 -2.40649942 -1.85198874
 -1.76356494 -2.2502121   1.1422381   1.69548746  1.60735009  0.5095059
 -2.14127874 -0.42085282 -1.31175217 -1.52965584 -2.11037062 -1.15912638
 -1.37107143 -1.88662948 -0.76092839 -1.81149462 -0.49421447 -1.64116579
  0.63770714  0.58981824  1.83663431  0.50939395 -1.05829484  0.42595944
 -0.32808585 -1.27359579  1.5668375   1.22118932  0.10069795 -0.05713882
 -0.6447141  -1.43109169 -2.01567099 -1.63706977 -1.1245879  -0.59980901
 -1.40892055 -1.619346    0.59923784 -1.37971696  1.24835707  1.61672486
 -2.22893185 -0.80428246 -0.786012   -1.15095629]
Minimum obj value:-3280.8515978899873
Optimal xi: 24.967512969303687
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1551.6347824755474
W_T_median: 1406.0687944567135
W_T_pctile_5: 624.1985510211185
W_T_CVAR_5_pct: 487.9109330118708
F value: -3280.8515978899873
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.8
F value: -3280.8515978899873
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-3.54103467 -1.64950479 -4.79360486 -2.80896426  5.2575835   1.24370901
  4.12222172  3.60392614 -0.40197228 -2.9695036   0.18562731 -1.91095328
  0.68309867 -0.46301738  0.44095961  0.04295893 -0.18942274 -2.57965122
 -0.95003038 -2.24776365  0.49610066 -1.84219399  0.33626208 -2.45158285
  1.73819923  2.29148478  1.09671774  1.76183632 -2.03135537 -1.75160495
 -0.0651631  -1.25194843  1.8152135   2.10599514  1.01892583  1.69452844
 -1.38288413 -1.7696427  -0.86228337 -1.19872931 -1.19219781 -2.10004841
 -2.37810828 -1.10042377 -2.19951755 -1.9330552  -1.54998661 -0.48929625
  0.83578872  0.99912895  1.17103209  0.99954368 -1.0096023  -1.98622326
 -1.13011405 -0.60890767 -2.01100736 -1.09734568 -1.15684687 -1.92462236
 -0.50452504 -1.94328435  2.10261208 -0.29100302]
Minimum obj value:-3436.0166757294187
Optimal xi: 24.990774311827515
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1551.6347824773009
W_T_median: 1406.0687944574602
W_T_pctile_5: 624.1985510193865
W_T_CVAR_5_pct: 487.9109330104782
F value: -3436.0166757294187
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.9
F value: -3436.0166757294187
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-3.06601983 -4.06319293 -3.67219139 -1.99807973  3.74684973  3.74785418
  3.5065988   2.22527053 -1.32120515 -2.01045526 -1.52902996 -1.11145447
 -2.03374707 -1.26509696 -1.54093464 -0.36336003 -1.7811789  -1.72634264
 -2.36141812  0.60344938 -1.81928152 -1.3557867  -0.65256622 -0.09850153
 -1.97127811 -1.2055609  -2.30601954 -1.27128025 -0.88076464 -1.73314985
 -1.85189092 -1.91574509 -0.29985303 -2.06462541 -1.70039285 -0.47674934
  0.28379544  0.31847778  1.54902961  0.17924481  1.06707703  1.29313163
  1.11918609  0.45804136  0.28489933 -1.44349898  0.47871059 -0.79489516
  0.45516371 -2.00519225 -0.75698915 -1.88917655  1.24346413  1.29231089
  0.99945482  0.33709616  1.48447413  1.61248311 -0.60502017 -2.10878745
 -0.53150061  0.79912143 -0.67016127 -1.9342207 ]
Minimum obj value:-3591.180229679119
Optimal xi: 24.97793479424022
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1551.634782477914
W_T_median: 1406.0687944576682
W_T_pctile_5: 624.1985510188409
W_T_CVAR_5_pct: 487.9109330100428
F value: -3591.180229679119
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.0
F value: -3591.180229679119
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-4.97385538 -3.64639372 -1.59969621 -2.72235026  4.38039249  4.10613933
  1.46252416  3.77750027 -1.99581419 -2.0375768  -2.77454951 -1.97461963
 -2.71927541 -2.76784863 -1.77150803 -1.85983329 -0.50129673 -0.51862039
 -0.56477734  0.61801895 -2.71410513 -0.84695068 -1.45652061  0.20508638
 -0.78286889 -0.79335579 -2.60308151 -1.21311401 -0.33967934  0.04603522
 -0.77139828 -0.77670774  0.71392176  0.57147323 -1.23338377 -0.75940176
 -0.63124469 -0.30743992  2.25585793 -0.58057811  2.02617235 -0.44882924
 -0.84459532  0.20235861  1.87777655 -0.63769043 -0.18984369 -0.54224063
 -2.94520704  0.6338236   0.65748864 -0.3709253   1.12847534  1.76667864
  1.30794322  1.64415678 -3.32566626 -1.9164785   0.02804087  2.1862421
  0.30770435  2.60020833  0.88963054  1.98529702]
Minimum obj value:-3746.3437207089373
Optimal xi: 24.978082334214534
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1551.6347824777597
W_T_median: 1406.0687944572624
W_T_pctile_5: 624.1985510187342
W_T_CVAR_5_pct: 487.9109330106125
F value: -3746.3437207089373
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.1
F value: -3746.3437207089373
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-3.91697896 -3.7556099  -3.21836943 -3.47334425  3.63701935  3.59990982
  2.46831747  2.25339119 -1.08509637 -0.91875182 -0.69748462 -2.5688452
 -2.06443101 -0.58434666 -0.56865199 -1.93724664 -2.2817196  -0.38547179
 -1.02698237 -2.35472854 -2.14840993 -1.23800398 -0.64349648 -2.56550462
  1.05022517  0.9703141  -1.7525091  -1.43671585  0.21623829  1.46200004
  1.29570286  1.44160557  1.28015858  0.61335043  1.01218119 -0.08730047
 -0.8504641  -0.86801654 -1.61243153 -2.35898697  1.75276782  1.27459953
  1.00966937  0.37006725  1.16221355  2.11466922  1.61978634  1.58568978
 -1.5267874  -2.09888693 -2.09891899 -1.87668341 -0.35630639 -1.03703095
 -1.74227638 -1.03139763 -1.69876697 -1.02244012 -2.42591899 -1.2806406
 -1.14548756 -0.68979583 -0.95140882  0.12662303]
Minimum obj value:-3901.5072378271834
Optimal xi: 24.978547169873625
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1551.6347824785566
W_T_median: 1406.0687944579786
W_T_pctile_5: 624.1985510182485
W_T_CVAR_5_pct: 487.9109330094442
F value: -3901.5072378271834
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.2
F value: -3901.5072378271834
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-3.31003146 -2.78196589 -3.43231874 -2.79791063  3.24720694  3.23165165
  2.91235312  4.65064366 -2.24851059 -2.18782532 -0.26943909 -1.20850963
 -1.81690368 -1.93524505  0.77195986  1.12289826 -1.67100948 -2.50323346
 -0.81404459 -0.24248321 -2.60908016 -2.57614539 -1.32209009 -1.16377781
 -1.15886984 -0.01119824 -1.95906717 -1.67495422 -1.37540926 -1.40544856
 -2.65717728 -1.57414244  1.42454275  0.60746435  0.5495734   1.3610873
  2.20281858  0.68298756  1.68641859  1.26525262  1.0538456  -1.43447975
 -0.29713152  0.50597609  2.04504286  1.26694481  2.22052114  1.24345313
 -0.68669847 -2.53805503 -0.14993855 -1.21503078  1.00306148 -2.38607418
  0.76070504 -0.03882992  0.77269703  2.82800638 -3.34079404 -1.85646045
  1.08217639  1.7942883   0.38609688  1.35774248]
Minimum obj value:-4056.670757689093
Optimal xi: 24.988693675440093
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1551.6347824787258
W_T_median: 1406.0687944580313
W_T_pctile_5: 624.1985510180834
W_T_CVAR_5_pct: 487.9109330093456
F value: -4056.670757689093
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.3
F value: -4056.670757689093
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-3.27792233 -2.51771233 -2.53925326 -4.89705227  3.64880276  1.76563875
  3.63124082  4.05566382 -0.49497136 -2.00763407 -2.12165438 -1.71526899
  0.30449347 -1.2265976  -0.45847746 -1.58098385  0.00776308 -2.23219351
 -2.15432424 -1.6387188  -0.63919095 -1.52169037 -1.16112967 -2.29926216
  1.21574569  0.26254788  1.37369133  1.35262849 -1.36343606 -0.75185373
 -1.15941902 -2.139813   -1.5720904  -0.7491223  -2.04774044 -1.63559549
 -1.91374821 -0.70188455 -1.85563814 -1.49390899 -0.87252315 -1.79858089
 -1.82887532 -2.04508229  1.67579675  1.78333089  1.157412    0.93520338
 -1.89957962 -1.41674417 -1.73969641 -0.66649397 -0.89818434 -0.95708439
 -0.90246748 -2.11130255 -2.07256026 -0.76323265 -0.82969163 -2.18449888
 -0.52841203 -1.70331213 -0.61016958 -1.76069783]
Minimum obj value:-4211.833272443755
Optimal xi: 24.971403262561527
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1551.6347824789875
W_T_median: 1406.068794458175
W_T_pctile_5: 624.1985510178735
W_T_CVAR_5_pct: 487.91093300908153
F value: -4211.833272443755
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.4
F value: -4211.833272443755
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-5.4085281  -3.70784284 -0.74761297 -3.71275799  4.43147363  4.10528262
  0.95439642  4.54351175 -0.93749368 -2.45480607 -1.33893917 -2.08073054
 -0.81542746 -2.0994837  -2.00686641 -2.01081224  0.74079422  0.81062367
  1.24360926  1.30925394  1.02063758 -0.85062477 -2.65897179 -1.3632839
  0.86156565  1.93144787  1.52343679  0.42091649 -1.59040686 -2.22946505
 -2.18988338 -1.0996166  -1.99608936 -1.06576222 -2.48027711 -0.40066488
 -1.54313851 -1.617015   -1.72483844 -1.42852072 -0.5752955  -1.02008832
 -0.72295716 -0.54397532 -0.6616658  -2.19005837 -2.33880723 -1.6283883
 -1.49364454 -1.02047377 -2.24713659 -0.76054825  0.33732783  0.54535966
  0.09416314  1.08136063  1.07677567 -0.17095327 -1.78396488 -1.3110983
 -2.31392706 -1.95970147  1.70363055  0.86833042]
Minimum obj value:-4366.997756870664
Optimal xi: 24.98801591453511
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1551.6347824782572
W_T_median: 1406.0687944578694
W_T_pctile_5: 624.1985510185835
W_T_CVAR_5_pct: 487.9109330096595
F value: -4366.997756870664
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.5
F value: -4366.997756870664
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-0.28072415 -4.83985226 -3.9337765  -4.62697013  0.45550378  4.83069246
  4.10359403  3.700637    0.59192047  1.89064444  1.33414539  1.95721802
 -1.11101128 -1.6556699  -2.13051912 -1.45485351  1.36196011 -1.38000752
 -1.51820775 -2.30286108 -0.64329422 -0.96133111 -2.28975397 -2.50422107
  1.15095266  1.5322566   1.88956205  1.22051067 -0.84288804 -1.3666842
 -0.85978729 -0.97763745 -1.21277331 -1.66857283 -1.85678291 -2.02374838
 -1.95266735 -0.47865985 -1.56320586 -1.46181565 -0.97911727 -0.88673559
 -2.05018447 -1.23314959 -0.87528744  0.71021514 -0.86269626 -1.30677533
 -1.84748494 -0.93304878 -0.54040804 -0.81288527 -0.77953892 -1.67434031
 -1.04914218 -2.13156727 -1.36860686 -0.57174742 -0.18521935  1.08417911
  0.6955033   0.49476153  0.22184627 -0.9713065 ]
Minimum obj value:-4522.159962886687
Optimal xi: 24.96998573607154
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1551.6347824784632
W_T_median: 1406.0687944579545
W_T_pctile_5: 624.1985510183939
W_T_CVAR_5_pct: 487.9109330094922
F value: -4522.159962886687
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.6
F value: -4522.159962886687
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-4.07763884e+00 -4.32707345e+00 -7.03537047e-01 -3.92975896e+00
  3.82047564e+00  4.25786460e+00  3.76915371e-01  5.57168570e+00
 -1.84618678e+00 -2.29783092e+00 -1.88042123e+00 -1.14410673e+00
 -1.93915806e+00 -6.93194782e-01 -1.96516777e+00 -2.06399938e+00
  3.82134120e-02  1.75192124e+00  2.32515399e-01  1.84287800e+00
 -2.74319472e+00 -1.91080303e+00 -9.39617667e-01 -9.55568121e-01
 -5.14829734e-01 -7.62188899e-01 -9.75760092e-01 -8.97732356e-01
 -1.15018716e+00 -2.01534047e+00 -9.52922462e-01 -1.94401709e+00
 -2.65621673e-01 -9.74474949e-01 -1.53781317e+00 -1.39452578e+00
 -1.78134328e+00 -2.00963929e+00 -1.71361298e+00 -9.75242341e-01
  2.12793189e-01 -3.41227103e-01 -2.39171406e-01  5.14424456e-02
 -1.20730803e+00 -1.61876892e+00 -8.15956997e-01 -1.27842896e+00
 -8.12216159e-01 -9.52821591e-01 -1.97431968e+00 -1.35845255e+00
 -1.02046907e+00 -8.33741479e-01 -1.48277579e+00 -2.33412910e+00
  1.49319309e+00 -2.44326264e-01 -7.47972682e-01 -9.18748941e-01
 -5.11511841e-01  3.09006017e-04 -1.63627177e-01 -9.96632430e-01]
Minimum obj value:-4677.324460051201
Optimal xi: 24.97673942067942
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1551.634782478666
W_T_median: 1406.0687944580409
W_T_pctile_5: 624.1985510182
W_T_CVAR_5_pct: 487.91093300932727
F value: -4677.324460051201
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.7
F value: -4677.324460051201
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
36.0% of gradient descent iterations done. Method = Adam
38.0% of gradient descent iterations done. Method = Adam
40.0% of gradient descent iterations done. Method = Adam
42.0% of gradient descent iterations done. Method = Adam
44.0% of gradient descent iterations done. Method = Adam
46.0% of gradient descent iterations done. Method = Adam
48.0% of gradient descent iterations done. Method = Adam
50.0% of gradient descent iterations done. Method = Adam
52.0% of gradient descent iterations done. Method = Adam
54.0% of gradient descent iterations done. Method = Adam
56.00000000000001% of gradient descent iterations done. Method = Adam
57.99999999999999% of gradient descent iterations done. Method = Adam
60.0% of gradient descent iterations done. Method = Adam
62.0% of gradient descent iterations done. Method = Adam
64.0% of gradient descent iterations done. Method = Adam
66.0% of gradient descent iterations done. Method = Adam
68.0% of gradient descent iterations done. Method = Adam
70.0% of gradient descent iterations done. Method = Adam
72.0% of gradient descent iterations done. Method = Adam
74.0% of gradient descent iterations done. Method = Adam
76.0% of gradient descent iterations done. Method = Adam
78.0% of gradient descent iterations done. Method = Adam
80.0% of gradient descent iterations done. Method = Adam
82.0% of gradient descent iterations done. Method = Adam
84.0% of gradient descent iterations done. Method = Adam
86.0% of gradient descent iterations done. Method = Adam
88.0% of gradient descent iterations done. Method = Adam
90.0% of gradient descent iterations done. Method = Adam
92.0% of gradient descent iterations done. Method = Adam
94.0% of gradient descent iterations done. Method = Adam
96.0% of gradient descent iterations done. Method = Adam
98.0% of gradient descent iterations done. Method = Adam
100.0% of gradient descent iterations done. Method = Adam
NN weights: [-4.49533148 -1.67785046 -3.30231665 -4.10292138  3.0964407   2.54316538
  4.13803732  3.20373791  1.67019064 -3.46494257 -1.60234655 -0.2046162
  0.88713897 -2.49342975 -0.13800935 -0.39771861 -0.80019883 -2.51026154
 -2.52442878 -1.46020011 -0.67550864 -1.88134274 -2.18802998 -2.26921064
  1.72002421  1.28399914  2.36533177  1.08689085 -0.90315384 -1.89569119
 -0.86505039 -0.9395458  -1.64807879 -1.55254618 -1.77432124 -2.17814199
 -1.75836822 -1.72805838 -2.03387585 -1.35490432 -2.06614077 -1.56098803
 -1.05177317 -0.83013898 -0.85058624 -1.22628324 -1.26296075 -2.1322064
 -1.68623355 -2.07522365 -0.54064429 -0.98997483 -1.92823969 -1.88959919
 -1.82605117 -1.32924279 -1.84356429 -1.63373016 -1.24121675 -0.93850133
  0.21367943  0.19005701 -0.45088583 -1.16929047]
Minimum obj value:-4832.488252711328
Optimal xi: 24.98078776791905
/home/marcchen/Documents/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/marcchen/Documents/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset
W_T_mean: 1551.634782479597
W_T_median: 1406.0687944584329
W_T_pctile_5: 624.1985510173159
W_T_CVAR_5_pct: 487.91093300857915
F value: -4832.488252711328
-----------------------------------------------
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:153: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
/home/marcchen/Documents/researchcode/fun_Plot_NN_control_DataHeatmaps.py:157: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.8
F value: -4832.488252711328
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0877712339752
W_T_median: 1222.733226526609
W_T_pctile_5: 834.028380700438
W_T_CVAR_5_pct: 747.2411321494716
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
4.0% of gradient descent iterations done. Method = Adam
6.0% of gradient descent iterations done. Method = Adam
8.0% of gradient descent iterations done. Method = Adam
10.0% of gradient descent iterations done. Method = Adam
12.0% of gradient descent iterations done. Method = Adam
14.000000000000002% of gradient descent iterations done. Method = Adam
16.0% of gradient descent iterations done. Method = Adam
18.0% of gradient descent iterations done. Method = Adam
20.0% of gradient descent iterations done. Method = Adam
22.0% of gradient descent iterations done. Method = Adam
24.0% of gradient descent iterations done. Method = Adam
26.0% of gradient descent iterations done. Method = Adam
28.000000000000004% of gradient descent iterations done. Method = Adam
30.0% of gradient descent iterations done. Method = Adam
32.0% of gradient descent iterations done. Method = Adam
34.0% of gradient descent iterations done. Method = Adam
