Starting at: 
13-02-23_11:51

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 50000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 5000, 'itbound_SGD_algorithms': 50000, 'nit_IterateAveragingStart': 45000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1565.1597049785923
Current xi:  [64.040306]
objective value function right now is: -1565.1597049785923
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1582.525855375417
Current xi:  [34.462067]
objective value function right now is: -1582.525855375417
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1592.7530812484176
Current xi:  [8.891811]
objective value function right now is: -1592.7530812484176
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1599.4374198910755
Current xi:  [-0.19682358]
objective value function right now is: -1599.4374198910755
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-2.2127695]
objective value function right now is: -1597.8784084253293
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00479232]
objective value function right now is: -1563.0515234347288
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1599.5019304195316
Current xi:  [-0.05208683]
objective value function right now is: -1599.5019304195316
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.3850618]
objective value function right now is: -1597.727415229742
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1600.1801597679198
Current xi:  [-0.29857683]
objective value function right now is: -1600.1801597679198
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.19621123]
objective value function right now is: -1599.7173156554366
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05874553]
objective value function right now is: -1592.6868523897604
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1601.146093806597
Current xi:  [-0.0050487]
objective value function right now is: -1601.146093806597
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.6393589883037
Current xi:  [0.01525674]
objective value function right now is: -1602.6393589883037
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1603.4790425619115
Current xi:  [-0.03218995]
objective value function right now is: -1603.4790425619115
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06304395]
objective value function right now is: -1601.844002504294
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.162748301916
Current xi:  [-0.06473421]
objective value function right now is: -1604.162748301916
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00557796]
objective value function right now is: -1602.0386366588502
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06688282]
objective value function right now is: -1602.8705565485313
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.13274917]
objective value function right now is: -1603.6659852826624
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.5679228]
objective value function right now is: -1596.2177332907488
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.5131962]
objective value function right now is: -1599.513584546946
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.4282916]
objective value function right now is: -1595.4013214936735
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02497708]
objective value function right now is: -1601.2899857899715
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.2023827]
objective value function right now is: -1595.347145223595
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.89556575]
objective value function right now is: -1600.1177381634857
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.15345524]
objective value function right now is: -1599.3695922323707
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.1801875]
objective value function right now is: -1591.5123265513232
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-24.105717]
objective value function right now is: -1545.3384053660814
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-43.98643]
objective value function right now is: -1549.0782347130344
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-65.829056]
objective value function right now is: -1548.721278791474
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.90932]
objective value function right now is: -1603.8290290871084
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.77999]
objective value function right now is: -1596.7865234433004
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.596146]
objective value function right now is: -1603.3698795467067
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.246783833521
Current xi:  [-73.64545]
objective value function right now is: -1604.246783833521
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.6124771534276
Current xi:  [-73.63445]
objective value function right now is: -1604.6124771534276
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.5728409975582
Current xi:  [-73.34656]
objective value function right now is: -1605.5728409975582
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.56421]
objective value function right now is: -1605.298709398485
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1606.0164271624922
Current xi:  [-73.32626]
objective value function right now is: -1606.0164271624922
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.38665]
objective value function right now is: -1605.675745023843
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.48261]
objective value function right now is: -1605.706711107292
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.33771]
objective value function right now is: -1605.823247614074
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.522484]
objective value function right now is: -1605.8270154908719
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.49059]
objective value function right now is: -1605.7090663298463
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.339294]
objective value function right now is: -1605.8071268514657
new min fval from sgd:  -1606.0174226997347
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.24787]
objective value function right now is: -1606.0174226997347
new min fval from sgd:  -1606.029809533974
new min fval from sgd:  -1606.1155931016467
new min fval from sgd:  -1606.1560596986615
new min fval from sgd:  -1606.1738226109144
new min fval from sgd:  -1606.1778449192407
new min fval from sgd:  -1606.1846097517523
new min fval from sgd:  -1606.1857719639095
new min fval from sgd:  -1606.1947972949554
new min fval from sgd:  -1606.2099930992179
new min fval from sgd:  -1606.220043134405
new min fval from sgd:  -1606.2303474929067
new min fval from sgd:  -1606.233964661095
new min fval from sgd:  -1606.2435154202567
new min fval from sgd:  -1606.2436666315546
new min fval from sgd:  -1606.246129266566
new min fval from sgd:  -1606.2553610959787
new min fval from sgd:  -1606.2682985562412
new min fval from sgd:  -1606.284449746882
new min fval from sgd:  -1606.2883774038194
new min fval from sgd:  -1606.3020140199408
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.312935]
objective value function right now is: -1605.8271428007968
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.47187]
objective value function right now is: -1606.061387730413
new min fval from sgd:  -1606.3060341735188
new min fval from sgd:  -1606.3129427595172
new min fval from sgd:  -1606.3323734751898
new min fval from sgd:  -1606.3648033106342
new min fval from sgd:  -1606.3820255911612
new min fval from sgd:  -1606.3980104121106
new min fval from sgd:  -1606.404474342896
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.4816]
objective value function right now is: -1606.1925507752183
new min fval from sgd:  -1606.404878812703
new min fval from sgd:  -1606.4063634217841
new min fval from sgd:  -1606.4079399395246
new min fval from sgd:  -1606.4104354021033
new min fval from sgd:  -1606.4139009146709
new min fval from sgd:  -1606.4172311758068
new min fval from sgd:  -1606.4205216843302
new min fval from sgd:  -1606.421057604358
new min fval from sgd:  -1606.421626091154
new min fval from sgd:  -1606.423121071071
new min fval from sgd:  -1606.424883850533
new min fval from sgd:  -1606.4271463155058
new min fval from sgd:  -1606.4281492003888
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.26015]
objective value function right now is: -1606.3956352947118
new min fval from sgd:  -1606.429936957156
new min fval from sgd:  -1606.4332253368154
new min fval from sgd:  -1606.435688868178
new min fval from sgd:  -1606.4376937085347
new min fval from sgd:  -1606.439297759333
new min fval from sgd:  -1606.4404980540355
new min fval from sgd:  -1606.4419662541472
new min fval from sgd:  -1606.4430650365807
new min fval from sgd:  -1606.4436752037668
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-73.28692]
objective value function right now is: -1606.3417175628372
min fval:  -1606.4436752037668
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  3.6047,  -7.4220],
        [ -1.3573, -10.3820],
        [-10.1818,  -1.6791],
        [  7.7994,  -4.2249],
        [  7.2278,  -4.0491],
        [ -2.3685,   4.0330],
        [  6.7288,  -4.2840],
        [ -5.7483,   1.6286],
        [ -1.9271,   3.4546],
        [ -0.7611,   1.2675]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-7.2764, -9.5770,  9.6802, -8.9838, -8.6172,  0.8839, -8.8531,  5.4692,
         0.2073, -1.6732], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 2.4012e+00,  7.7635e+00, -2.1767e+00,  3.4378e+00,  3.2429e+00,
         -3.2225e-02,  2.2607e+00, -1.9369e+00,  2.1469e-02,  1.4306e-02],
        [-3.8577e-01, -4.3400e-02, -6.0550e-01, -2.8460e-01, -2.4917e-01,
         -1.6165e-03, -1.6889e-01, -3.0348e-01, -1.9240e-03, -6.3184e-03],
        [-3.8577e-01, -4.3400e-02, -6.0550e-01, -2.8460e-01, -2.4917e-01,
         -1.6165e-03, -1.6889e-01, -3.0348e-01, -1.9240e-03, -6.3185e-03],
        [ 2.8185e+00,  1.0262e+01, -2.8036e+00,  4.3237e+00,  4.1031e+00,
         -7.5924e-02,  2.9942e+00, -2.3890e+00,  4.5086e-03,  6.8206e-03],
        [ 3.1653e+00,  7.7718e+00, -2.9909e+00,  3.3680e+00,  3.2450e+00,
         -2.0752e-02,  2.2741e+00, -1.3925e+00,  3.7884e-02,  2.1884e-02],
        [ 3.3789e+00,  1.0350e+01, -3.5647e+00,  4.1684e+00,  4.2645e+00,
         -7.4885e-02,  3.0637e+00, -1.9619e+00,  1.1568e-02,  1.4476e-02],
        [ 2.9625e+00,  1.0208e+01, -3.0404e+00,  4.3260e+00,  4.1157e+00,
         -7.1965e-02,  3.0143e+00, -2.1393e+00,  9.3760e-03,  1.0272e-02],
        [-3.8577e-01, -4.3400e-02, -6.0550e-01, -2.8460e-01, -2.4917e-01,
         -1.6165e-03, -1.6889e-01, -3.0348e-01, -1.9240e-03, -6.3184e-03],
        [-2.6260e+00, -1.7447e+01,  1.0644e+01, -4.2463e+00, -3.6043e+00,
          1.0746e+00, -4.0734e+00, -1.4817e-01,  8.0548e-01,  1.3071e-02],
        [-3.8577e-01, -4.3400e-02, -6.0550e-01, -2.8460e-01, -2.4917e-01,
         -1.6165e-03, -1.6889e-01, -3.0348e-01, -1.9240e-03, -6.3184e-03]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.4457, -0.7023, -0.7023, -1.9794, -1.5069, -1.9321, -2.0205, -0.7023,
         0.6363, -0.7023], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-2.4386e+00,  1.6175e-02,  1.6175e-02, -4.0245e+00, -2.6772e+00,
         -4.3228e+00, -4.0631e+00,  1.6175e-02,  1.6816e+01,  1.6175e-02]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 12.0445,   4.3339],
        [ 10.9021,   3.2606],
        [  0.1720,   4.9713],
        [ 11.0546,  -0.9470],
        [ -1.7577,   0.9840],
        [  8.2158,  -3.0134],
        [ -8.4184,   4.6442],
        [ 13.3147,   3.7069],
        [ -4.0006, -15.2089],
        [-13.8870,  -4.1283]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -0.9947,   1.5613,   4.6805, -11.2062,  -3.4314, -16.8982,  10.3050,
         -0.5258, -14.4599,  -2.2945], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-5.0679e+00, -7.8096e+00, -1.3701e+01, -1.4158e+01,  2.1456e-03,
         -1.4597e+01, -3.8456e+00, -9.2662e+00,  1.0135e+01,  1.1240e+01],
        [-8.8368e-01, -1.1921e+00, -6.8067e-01, -8.9658e-01, -2.5037e-03,
         -7.0148e-01, -7.2208e-01, -1.0496e+00, -1.0992e+00, -7.5538e-01],
        [-8.8356e-01, -1.1918e+00, -6.8048e-01, -8.9635e-01, -2.5022e-03,
         -7.0123e-01, -7.2176e-01, -1.0492e+00, -1.0987e+00, -7.5505e-01],
        [-2.3416e+00, -1.4477e+00, -5.8778e-01, -3.0381e+00,  1.5986e-01,
         -2.7104e+00,  1.6072e+00, -2.8529e+00,  5.4614e-01,  1.0880e+00],
        [-2.5223e+00, -2.1241e+00,  2.8412e+00, -8.5615e+00, -1.1457e-01,
          2.5042e-02,  6.8016e+00, -2.6333e+00, -2.4494e+01,  5.7613e-01],
        [-4.2957e+00, -8.0025e+00, -2.1785e+00,  1.7246e+00, -1.2540e-02,
          7.1917e-01, -2.9080e+00, -2.8251e+00,  8.1750e+00,  4.5708e+00],
        [ 9.9060e-01,  1.0070e+01,  3.5669e+00,  1.0178e+00,  8.1598e-02,
          9.2599e-01,  6.7723e-01,  2.7826e+00, -8.5483e+00, -6.9087e+00],
        [-2.3801e+00,  1.3370e+00, -9.8929e-01, -2.1112e+00,  8.4335e-02,
         -1.2100e+00,  7.6110e-01, -1.6485e+00, -1.0104e+01,  5.6613e+00],
        [ 5.8898e+00,  6.8627e+00,  8.2755e+00,  4.2360e+00,  2.1044e-01,
          9.3667e+00,  1.1703e+00,  7.1831e+00, -1.9289e+01, -2.2393e+00],
        [-7.1554e-02, -7.6552e+00,  3.1938e-01,  1.4059e+00,  7.3634e-02,
         -1.4762e+00, -2.3958e+00, -1.9714e+00,  3.2326e-01,  6.6110e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-0.2544, -2.0825, -2.0840, -1.7369, -3.1082, -2.9133,  4.6702,  0.3742,
         2.8690, -1.1093], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 1.0694e+01,  5.0685e-03,  4.9645e-03,  1.5434e+00,  1.9209e+00,
          2.1109e+00, -3.1863e+00, -1.9433e+00,  3.4522e+00, -2.0233e+00],
        [-1.0694e+01, -5.1069e-03, -5.2150e-03, -1.5445e+00, -2.0569e+00,
         -2.1325e+00,  3.1435e+00,  1.9080e+00, -3.2805e+00,  2.0224e+00]],
       device='cuda:0'))])
xi:  [-73.26509]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 324.82573546546274
W_T_median: 105.0513543308007
W_T_pctile_5: -73.2128154633418
W_T_CVAR_5_pct: -160.14424191402352
Average q (qsum/M+1):  54.403741651965724
Optimal xi:  [-73.26509]
Observed VAR:  105.0513543308007
Expected(across Rb) median(across samples) p_equity:  0.30664635027448334
obj fun:  tensor(-1606.4437, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
