Starting at: 
13-02-23_11:51

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 50000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 5000, 'itbound_SGD_algorithms': 50000, 'nit_IterateAveragingStart': 45000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1689.7644589314816
Current xi:  [66.22325]
objective value function right now is: -1689.7644589314816
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1699.8449888772986
Current xi:  [29.229387]
objective value function right now is: -1699.8449888772986
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1707.9563072325197
Current xi:  [-6.9489613]
objective value function right now is: -1707.9563072325197
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1712.7794350687489
Current xi:  [-40.480328]
objective value function right now is: -1712.7794350687489
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1718.0106429562643
Current xi:  [-74.30581]
objective value function right now is: -1718.0106429562643
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1721.9499496178134
Current xi:  [-108.190384]
objective value function right now is: -1721.9499496178134
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1725.5422523095115
Current xi:  [-142.05109]
objective value function right now is: -1725.5422523095115
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1728.8706649203455
Current xi:  [-175.98793]
objective value function right now is: -1728.8706649203455
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1730.899430323265
Current xi:  [-209.44963]
objective value function right now is: -1730.899430323265
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1733.8724040353172
Current xi:  [-242.96513]
objective value function right now is: -1733.8724040353172
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1735.3753178084705
Current xi:  [-275.25943]
objective value function right now is: -1735.3753178084705
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1736.7080225650016
Current xi:  [-306.92258]
objective value function right now is: -1736.7080225650016
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1738.0554833833241
Current xi:  [-337.92725]
objective value function right now is: -1738.0554833833241
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1739.5250866457357
Current xi:  [-367.43872]
objective value function right now is: -1739.5250866457357
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1740.5096015833865
Current xi:  [-394.42838]
objective value function right now is: -1740.5096015833865
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-420.30844]
objective value function right now is: -1740.450105879074
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.0533501923549
Current xi:  [-441.36356]
objective value function right now is: -1741.0533501923549
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-460.42554]
objective value function right now is: -1740.42436616051
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.1700458339542
Current xi:  [-472.98224]
objective value function right now is: -1741.1700458339542
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-481.5469]
objective value function right now is: -1740.6883506607546
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-488.29993]
objective value function right now is: -1740.1962830791279
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.83585]
objective value function right now is: -1741.0193726690934
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.29722057304
Current xi:  [-492.08627]
objective value function right now is: -1741.29722057304
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.3771520530527
Current xi:  [-490.68906]
objective value function right now is: -1741.3771520530527
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-493.18198]
objective value function right now is: -1741.1580305099878
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-494.6535]
objective value function right now is: -1740.9968648018896
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-493.7994]
objective value function right now is: -1740.5171284517735
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-492.57483]
objective value function right now is: -1741.337024158505
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-491.24417]
objective value function right now is: -1740.7135318835187
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.49048]
objective value function right now is: -1741.3594274345203
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-490.25507]
objective value function right now is: -1741.239785412321
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.1503]
objective value function right now is: -1741.196015208592
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.79544]
objective value function right now is: -1741.1666038196306
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.34207]
objective value function right now is: -1741.0939548883994
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-493.11172]
objective value function right now is: -1740.059123928844
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.4843958510046
Current xi:  [-492.92474]
objective value function right now is: -1741.4843958510046
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.568217874024
Current xi:  [-492.59378]
objective value function right now is: -1741.568217874024
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.16498]
objective value function right now is: -1741.5230127167506
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.6318055872946
Current xi:  [-491.75516]
objective value function right now is: -1741.6318055872946
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.5295]
objective value function right now is: -1741.570369923273
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-490.42554]
objective value function right now is: -1741.5932411776296
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-490.6355]
objective value function right now is: -1741.573385426674
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-490.7092]
objective value function right now is: -1741.6153547583065
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-490.23386]
objective value function right now is: -1741.6192174767352
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-490.1893]
objective value function right now is: -1741.6209184305314
new min fval from sgd:  -1741.6353403721218
new min fval from sgd:  -1741.6595156843312
new min fval from sgd:  -1741.6673105165175
new min fval from sgd:  -1741.6712897794475
new min fval from sgd:  -1741.6806323198443
new min fval from sgd:  -1741.6835353117847
new min fval from sgd:  -1741.685012564008
new min fval from sgd:  -1741.685921440639
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.57285]
objective value function right now is: -1741.632247504176
new min fval from sgd:  -1741.6869082361584
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.70892]
objective value function right now is: -1741.4314754845495
new min fval from sgd:  -1741.6934093190753
new min fval from sgd:  -1741.6949961552318
new min fval from sgd:  -1741.6950546255257
new min fval from sgd:  -1741.6998445818497
new min fval from sgd:  -1741.701222101972
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.36566]
objective value function right now is: -1741.6628032121323
new min fval from sgd:  -1741.7015486416292
new min fval from sgd:  -1741.7028740954074
new min fval from sgd:  -1741.7036584900604
new min fval from sgd:  -1741.7050485421782
new min fval from sgd:  -1741.7055750723891
new min fval from sgd:  -1741.7056762115253
new min fval from sgd:  -1741.7057892943012
new min fval from sgd:  -1741.7059189787797
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-489.0026]
objective value function right now is: -1741.6855718912952
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-488.9981]
objective value function right now is: -1741.6892077140974
min fval:  -1741.7059189787797
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.4577,  1.2653],
        [16.0850,  2.3697],
        [-2.7263,  5.1040],
        [-4.0217,  7.8175],
        [-0.4577,  1.2653],
        [-0.4577,  1.2653],
        [-0.4578,  1.2654],
        [-0.4577,  1.2653],
        [-0.4577,  1.2653],
        [-0.4577,  1.2653]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-0.7873, -8.9837,  6.8428, 13.6857, -0.7873, -0.7873, -0.7875, -0.7873,
        -0.7873, -0.7873], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-7.4377e-03, -2.2978e-01, -3.1465e-02, -2.0806e-01, -7.4377e-03,
         -7.4377e-03, -7.4321e-03, -7.4377e-03, -7.4377e-03, -7.4377e-03],
        [-7.4377e-03, -2.2978e-01, -3.1465e-02, -2.0806e-01, -7.4377e-03,
         -7.4377e-03, -7.4321e-03, -7.4377e-03, -7.4377e-03, -7.4377e-03],
        [ 7.3508e-02,  5.0414e+00,  1.0943e+00,  6.2456e+00,  7.3508e-02,
          7.3509e-02,  7.0200e-02,  7.3508e-02,  7.3509e-02,  7.3509e-02],
        [ 5.7226e-02, -8.0734e+00, -1.8193e+00, -9.5966e+00,  5.7226e-02,
          5.7226e-02,  5.2952e-02,  5.7226e-02,  5.7226e-02,  5.7226e-02],
        [-7.4377e-03, -2.2978e-01, -3.1465e-02, -2.0806e-01, -7.4377e-03,
         -7.4377e-03, -7.4321e-03, -7.4377e-03, -7.4377e-03, -7.4377e-03],
        [-7.4377e-03, -2.2978e-01, -3.1465e-02, -2.0806e-01, -7.4377e-03,
         -7.4377e-03, -7.4321e-03, -7.4377e-03, -7.4377e-03, -7.4377e-03],
        [-7.4377e-03, -2.2978e-01, -3.1465e-02, -2.0806e-01, -7.4377e-03,
         -7.4377e-03, -7.4321e-03, -7.4377e-03, -7.4377e-03, -7.4377e-03],
        [-7.4377e-03, -2.2978e-01, -3.1465e-02, -2.0806e-01, -7.4377e-03,
         -7.4377e-03, -7.4321e-03, -7.4377e-03, -7.4377e-03, -7.4377e-03],
        [ 5.3337e-02,  5.5206e+00,  1.1622e+00,  6.7651e+00,  5.3337e-02,
          5.3337e-02,  4.9471e-02,  5.3337e-02,  5.3337e-02,  5.3337e-02],
        [-7.4377e-03, -2.2978e-01, -3.1465e-02, -2.0806e-01, -7.4377e-03,
         -7.4377e-03, -7.4321e-03, -7.4377e-03, -7.4377e-03, -7.4377e-03]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.8237, -0.8237, -3.5452,  5.2447, -0.8237, -0.8237, -0.8237, -0.8237,
        -3.7974, -0.8237], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0208,  -0.0208,   6.9678, -12.6609,  -0.0208,  -0.0208,  -0.0208,
          -0.0208,   7.7604,  -0.0208]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-10.7263, -13.1469],
        [-17.7377,  -5.0206],
        [-11.1023,   0.8577],
        [ 11.6619,   8.6886],
        [-10.5392,   2.6417],
        [ -2.4395,  -0.6981],
        [ -0.4844,   2.9186],
        [  0.8597,  11.6175],
        [ 11.2700,   4.0051],
        [ -0.9265,   5.5959]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-13.9266,  -0.7646,  11.5515,   4.1831,   6.7658,  -4.1519,  -2.1829,
          8.8985,  -0.7432,   2.8173], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 3.0475e+00,  1.1022e+00, -6.3736e+00, -2.5142e-01, -3.9344e+00,
          1.6201e+00, -3.1178e-02, -1.5480e-02, -9.8576e+00, -4.3613e-02],
        [-1.2498e+01,  7.0245e-01,  1.7777e+00, -5.0157e+00, -2.8531e-01,
          6.2946e-01, -1.4079e-01, -1.6630e+01, -2.0693e+00, -1.2411e+00],
        [-2.1999e+00, -1.2114e+01,  5.8741e+00, -6.9786e+00, -3.0692e+00,
          1.0144e-01, -8.9981e-02, -2.2412e+00, -1.9764e+00, -9.6344e-01],
        [ 3.1513e+00,  3.6666e+00, -2.6663e+00,  3.3714e+00,  1.2162e+00,
          5.2361e-01, -7.5250e-02,  4.0641e+00,  1.4277e+00,  6.2683e-01],
        [-4.6047e+00, -2.5371e+00, -9.5873e-01,  1.1041e+00, -6.3282e+00,
         -3.0936e+00,  7.2113e-01, -6.3383e+00,  1.7360e+00, -8.7781e-01],
        [-1.1099e+00, -1.0486e+00, -9.2523e-01, -1.0886e+00, -6.4904e-02,
         -1.0421e+00, -2.7735e-02, -5.6701e-01, -1.1340e+00, -2.7431e-01],
        [-2.1940e+00, -1.2594e+00,  6.4763e-01, -2.3597e+00, -7.8884e-01,
         -8.8805e-01,  2.9522e-02, -1.1478e+00, -1.7458e+00, -3.2159e-01],
        [-1.1098e+00, -1.0485e+00, -9.2522e-01, -1.0885e+00, -6.4893e-02,
         -1.0421e+00, -2.7735e-02, -5.6702e-01, -1.1339e+00, -2.7431e-01],
        [-1.1098e+00, -1.0485e+00, -9.2522e-01, -1.0885e+00, -6.4893e-02,
         -1.0421e+00, -2.7735e-02, -5.6702e-01, -1.1339e+00, -2.7431e-01],
        [-8.2422e+00, -5.0868e+00, -5.9050e+00, -2.1319e+00, -1.7731e+00,
         -6.6556e+00,  1.2061e+00, -4.2185e+00,  2.3940e+00, -5.2436e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ 1.5410,  1.4908, -0.4017,  0.3101,  5.4848, -2.2349, -1.3846, -2.2351,
        -2.2351, 14.2815], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-4.7582, -8.0263, -6.5886,  4.6334, -0.5517,  0.0259, -0.1988,  0.0238,
          0.0239, -4.0167],
        [ 4.3899,  8.0259,  6.5914, -4.5695,  0.4899, -0.0218,  0.1972, -0.0239,
         -0.0239,  4.3113]], device='cuda:0'))])
xi:  [-489.0211]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 262.27017054367593
W_T_median: 106.22981688869966
W_T_pctile_5: -490.1104022057652
W_T_CVAR_5_pct: -599.8157496467351
Average q (qsum/M+1):  57.15152075982863
Optimal xi:  [-489.0211]
Observed VAR:  106.22981688869966
Expected(across Rb) median(across samples) p_equity:  0.2772803933553708
obj fun:  tensor(-1741.7059, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
