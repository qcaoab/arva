Starting at: 
09-03-23_17:07

 numpy seed:  3  


 pytorch seed:  3  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.2734e+00,  4.8709e-01],
        [-2.0934e+00,  2.5409e+00],
        [-2.2876e+01,  1.6497e-02],
        [ 1.1707e+01, -1.9355e+01],
        [ 2.7174e+01, -2.8451e+00],
        [ 5.2974e-01, -2.0164e+01],
        [-1.4523e+01, -1.4006e+01],
        [ 2.5252e+01, -6.6165e+00],
        [ 2.1479e+01, -1.2247e+01],
        [ 1.4506e+00,  1.0643e+01]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.8504,  -4.9451,  19.2859, -14.5092, -18.2183, -14.7456, -12.5980,
        -15.0216, -12.3807,  -2.9477], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 7.7663e-03,  5.1054e-02, -1.6077e+00, -3.6082e-01, -6.8995e-02,
         -2.2112e-01, -4.0013e-02, -1.5366e-01, -4.8100e-01, -2.2317e-01],
        [ 7.7667e-03,  5.1052e-02, -1.6077e+00, -3.6081e-01, -6.8996e-02,
         -2.2111e-01, -4.0013e-02, -1.5366e-01, -4.8100e-01, -2.2316e-01],
        [ 7.7593e-03,  5.1074e-02, -1.6075e+00, -3.6085e-01, -6.8988e-02,
         -2.2113e-01, -4.0012e-02, -1.5365e-01, -4.8096e-01, -2.2318e-01],
        [ 7.6082e-03,  5.1231e-02, -1.5976e+00, -3.5942e-01, -6.9857e-02,
         -2.2033e-01, -3.9888e-02, -1.5423e-01, -4.7853e-01, -2.1877e-01],
        [ 2.9546e-01, -7.0587e-01, -1.4249e+01,  1.5561e+01,  2.1216e+01,
          1.5625e+01,  9.1560e+00,  1.3380e+01,  1.4731e+01, -5.4137e-01],
        [ 7.5223e-03,  5.1595e-02, -1.5973e+00, -3.6067e-01, -6.9622e-02,
         -2.2059e-01, -3.9908e-02, -1.5389e-01, -4.7752e-01, -2.2053e-01],
        [ 7.5219e-03,  5.1596e-02, -1.5972e+00, -3.6067e-01, -6.9623e-02,
         -2.2059e-01, -3.9907e-02, -1.5389e-01, -4.7752e-01, -2.2053e-01],
        [ 7.6052e-03,  5.1337e-02, -1.5997e+00, -3.6001e-01, -6.9697e-02,
         -2.2043e-01, -3.9915e-02, -1.5401e-01, -4.7806e-01, -2.2001e-01],
        [ 7.7425e-03,  5.1127e-02, -1.6069e+00, -3.6098e-01, -6.8970e-02,
         -2.2117e-01, -4.0010e-02, -1.5363e-01, -4.8091e-01, -2.2327e-01],
        [ 2.0968e-01,  5.7453e-01,  1.2352e+01, -1.5429e+01, -1.5342e+01,
         -1.3502e+01, -1.4275e+00, -1.1646e+01, -1.2296e+01,  1.0666e+00]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.6527, -1.6527, -1.6529, -1.6660, -4.2043, -1.6636, -1.6636, -1.6626,
        -1.6532,  0.4355], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0579,  -0.0579,  -0.0579,  -0.0577, -19.8162,  -0.0575,  -0.0575,
          -0.0577,  -0.0579,  13.0785]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 16.5454,  10.1505],
        [-27.7295,  -3.4493],
        [ 12.0052,  -2.8082],
        [ -7.3754,   2.4268],
        [-15.6389,   6.7443],
        [-22.1360,   2.1224],
        [ -5.7880,  13.9104],
        [ -7.6202,   5.0118],
        [-13.9450,   4.8303],
        [ 10.1323,  23.6041]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  5.3889,  16.1790, -13.2207,  -3.5134,   1.1029,  24.0271,  10.9445,
         -8.9956,   0.0942,  18.2169], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.5210e+00,  2.8651e+00, -8.2776e+00, -2.5787e+00, -2.1681e-02,
         -2.7665e+00,  1.5492e+00, -2.4621e+00, -2.3149e+00,  2.5708e+00],
        [ 9.2009e+00, -9.2805e-01,  1.4710e-01,  1.1929e+00,  1.9313e+01,
         -2.2385e+00,  2.0543e+01,  1.7633e-02,  8.2694e+00,  2.2845e+00],
        [-1.7957e+00,  5.8038e-01, -1.7285e+01, -1.5858e+00, -2.2805e+00,
         -2.5971e+00,  1.0729e+00, -8.8275e-01, -2.9741e+00,  2.8343e+00],
        [-1.7300e+01,  1.2026e+01, -7.3663e+00,  8.1434e-01,  3.3758e+00,
          5.4412e+00,  2.3406e+00, -4.6183e-02,  1.2460e+00, -3.0484e+01],
        [-2.7516e+00,  5.2446e-02, -2.1913e+01, -1.2601e+00, -1.3140e+00,
         -2.0667e+00,  6.3349e-01, -4.2871e+00, -3.1028e+00,  5.1743e+00],
        [-3.1050e+00,  3.8474e-02, -1.8430e+01, -1.1562e+00, -1.0966e+00,
         -1.7040e+00,  6.8963e-01, -1.5615e+00, -4.1952e+00,  4.5244e+00],
        [-2.9802e+00,  2.0621e+00, -5.9482e+00, -8.5396e-01, -1.3693e+00,
         -5.0158e+00,  7.8727e-01, -9.7172e-01, -1.5423e+00,  3.8791e+00],
        [-5.2476e-01, -1.5870e+01,  8.7069e-01, -4.4496e-01, -5.1603e-01,
          1.6685e+00, -3.4716e-01, -5.3196e-01, -5.1674e-01, -9.1086e+00],
        [-8.3235e+00,  6.3334e+00, -8.6171e-01, -7.5219e-01, -2.3934e+00,
          4.6890e+00, -7.6771e+00, -1.5385e-02, -1.9552e+00, -8.6234e+00],
        [-8.7035e+01,  4.5951e+00, -4.8541e+01, -2.4205e+00,  1.6555e+00,
          3.3116e+00, -1.1850e+00,  3.9256e-04,  7.4916e-01, -7.9892e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-6.1122, -7.9929, -6.3331, -4.7742, -5.0293, -7.2396, -6.8996, -4.5070,
        -3.3346, -2.3465], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  4.9421,   0.4776,   5.4848,  -9.4060,   7.3509,   7.5357,   6.4729,
           4.4710,  -0.8656,  31.1138],
        [ -4.9421,  -0.5178,  -5.4848,   9.4228,  -7.3509,  -7.5358,  -6.4819,
          -4.4711,   0.7832, -31.1109]], device='cuda:0'))])
loaded xi:  214.20367
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1583.6866209228313
W_T_median: 1138.6857387434734
W_T_pctile_5: -127.4429131675796
W_T_CVAR_5_pct: -297.80727420155023
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -28.399767391953297
Current xi:  [213.46942]
objective value function right now is: -28.399767391953297
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.77934]
objective value function right now is: -19.781362065641947
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.3046]
objective value function right now is: -22.77981407503028
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -29.027645364227034
Current xi:  [212.84363]
objective value function right now is: -29.027645364227034
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -29.32177858445841
Current xi:  [213.89355]
objective value function right now is: -29.32177858445841
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.0008]
objective value function right now is: -29.13193953390366
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [213.17921]
objective value function right now is: -28.87800344856673
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.38562]
objective value function right now is: -28.107742253059158
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.53275]
objective value function right now is: -26.574196634743846
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -29.483818288808028
Current xi:  [214.03946]
objective value function right now is: -29.483818288808028
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.40117]
objective value function right now is: -26.18768719122365
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.57526]
objective value function right now is: -27.21742906495108
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.03113]
objective value function right now is: -26.895886688040026
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [211.91287]
objective value function right now is: -28.292258965938263
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.37622]
objective value function right now is: -28.347677472181644
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.90195]
objective value function right now is: -23.27641980641427
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.35875]
objective value function right now is: -25.39016471779254
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.83247]
objective value function right now is: -28.1133259947275
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.05222]
objective value function right now is: -28.161001461375314
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.61632]
objective value function right now is: -29.262512748013265
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.1739]
objective value function right now is: -28.572296839994575
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.4855]
objective value function right now is: -29.2050746105317
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.32649]
objective value function right now is: -28.23703641164323
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.49702]
objective value function right now is: -28.16730858587343
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.96336]
objective value function right now is: -27.51753622352242
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.85434]
objective value function right now is: -29.47903547509925
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.02814]
objective value function right now is: -29.165990031012132
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -29.941861352065988
Current xi:  [214.9703]
objective value function right now is: -29.941861352065988
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [212.88896]
objective value function right now is: -27.659227028047884
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.91821]
objective value function right now is: -27.376780198243594
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.44316]
objective value function right now is: -27.76058140347548
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -29.945972926602263
Current xi:  [214.15273]
objective value function right now is: -29.945972926602263
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.35449]
objective value function right now is: -29.50932250303226
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -30.02009901019292
Current xi:  [212.42783]
objective value function right now is: -30.02009901019292
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.14757]
objective value function right now is: -28.58084374772185
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -30.465378003220536
Current xi:  [215.38838]
objective value function right now is: -30.465378003220536
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.21986]
objective value function right now is: -30.241626689099622
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [215.65672]
objective value function right now is: -30.434617433575188
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -30.91803327984143
Current xi:  [215.88696]
objective value function right now is: -30.91803327984143
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [216.1946]
objective value function right now is: -30.89375644471432
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [216.58353]
objective value function right now is: -30.504250694561744
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [216.553]
objective value function right now is: -30.777728602135824
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [216.70932]
objective value function right now is: -30.722966445497683
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [216.66241]
objective value function right now is: -30.654465130950598
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [216.8301]
objective value function right now is: -30.74450119083045
new min fval from sgd:  -30.93203568572703
new min fval from sgd:  -30.943380052882624
new min fval from sgd:  -30.985331685982565
new min fval from sgd:  -30.9993892431084
new min fval from sgd:  -31.001289858193022
new min fval from sgd:  -31.003475730990278
new min fval from sgd:  -31.01127662024086
new min fval from sgd:  -31.01523219902203
new min fval from sgd:  -31.015471300918602
new min fval from sgd:  -31.020474137052716
new min fval from sgd:  -31.025229759081636
new min fval from sgd:  -31.03047258120143
new min fval from sgd:  -31.042085422286355
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [216.74617]
objective value function right now is: -29.56317596188601
new min fval from sgd:  -31.04647264503976
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [217.32921]
objective value function right now is: -30.454238302007216
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [217.27109]
objective value function right now is: -30.37594229081663
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [217.09348]
objective value function right now is: -30.975942147631965
new min fval from sgd:  -31.047039173313365
new min fval from sgd:  -31.04879755650014
new min fval from sgd:  -31.04945905335201
new min fval from sgd:  -31.04962931901489
new min fval from sgd:  -31.05021939028298
new min fval from sgd:  -31.051202747113205
new min fval from sgd:  -31.051562932735134
new min fval from sgd:  -31.05514364230348
new min fval from sgd:  -31.057216590639477
new min fval from sgd:  -31.057928866386472
new min fval from sgd:  -31.0586022797641
new min fval from sgd:  -31.059910575222027
new min fval from sgd:  -31.06204960695177
new min fval from sgd:  -31.064222567681885
new min fval from sgd:  -31.06583646903647
new min fval from sgd:  -31.06783907104518
new min fval from sgd:  -31.06888867472486
new min fval from sgd:  -31.069929218788435
new min fval from sgd:  -31.07061507165716
new min fval from sgd:  -31.070618177570903
new min fval from sgd:  -31.070795809532612
new min fval from sgd:  -31.07080675802704
new min fval from sgd:  -31.07182137972109
new min fval from sgd:  -31.072449678554037
new min fval from sgd:  -31.07604730605627
new min fval from sgd:  -31.076097557758622
new min fval from sgd:  -31.076625452269386
new min fval from sgd:  -31.077400549525173
new min fval from sgd:  -31.077875922940905
new min fval from sgd:  -31.07809449964197
new min fval from sgd:  -31.08216056813772
new min fval from sgd:  -31.084980389837554
new min fval from sgd:  -31.08797098116261
new min fval from sgd:  -31.089421738859787
new min fval from sgd:  -31.09049403332051
new min fval from sgd:  -31.09068476247131
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [217.11127]
objective value function right now is: -31.081593023334023
min fval:  -31.09068476247131
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 0.1286, -0.1987],
        [ 0.1286, -0.1987],
        [ 0.1286, -0.1987],
        [ 0.1286, -0.1987],
        [ 0.1286, -0.1987],
        [ 0.1286, -0.1987],
        [ 0.1286, -0.1987],
        [ 0.1286, -0.1987],
        [ 0.1286, -0.1987],
        [ 0.1286, -0.1987]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([0.2330, 0.2330, 0.2330, 0.2330, 0.2330, 0.2330, 0.2330, 0.2330, 0.2330,
        0.2330], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470,
         0.2470],
        [0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470,
         0.2470],
        [0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470,
         0.2470],
        [0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470,
         0.2470],
        [0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470,
         0.2470],
        [0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470,
         0.2470],
        [0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470,
         0.2470],
        [0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470,
         0.2470],
        [0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470,
         0.2470],
        [0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470, 0.2470,
         0.2470]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([0.4005, 0.4005, 0.4005, 0.4005, 0.4005, 0.4005, 0.4005, 0.4005, 0.4005,
        0.4005], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.7688, -1.7688, -1.7688, -1.7688, -1.7688, -1.7688, -1.7688, -1.7688,
         -1.7688, -1.7688]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 14.3071,   8.9954],
        [-27.3132,  -3.1232],
        [ 12.0633,  -2.6311],
        [ -1.7024,   0.4812],
        [-11.4305,   3.2591],
        [-22.9057,   2.3158],
        [ -3.0329,  16.1110],
        [ -2.1153,   3.9878],
        [ -1.7022,   0.4661],
        [ 14.3706,  23.1320]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  4.1046,  16.1419, -12.4736,  -3.7949,   1.7351,  23.9689,  12.8677,
         -5.0090,  -3.8115,  16.4730], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.7733e+00,  1.5692e+00, -1.4278e+00, -9.5870e-02, -1.6441e+00,
         -2.1698e+00,  1.9237e+00, -4.2273e-01, -8.6045e-02,  1.0862e+00],
        [ 1.0069e+01, -9.8813e-01,  6.7221e-01,  3.8218e-02,  4.1513e+00,
         -1.6795e+00,  1.7912e+01,  2.3897e-02,  3.8220e-02,  1.7208e+00],
        [-3.2569e+00,  1.9828e+00,  1.4897e+00,  1.4046e-02, -1.1228e+00,
         -2.1014e+00,  4.9266e+00, -8.2885e-01,  3.1819e-02, -2.7258e-02],
        [-1.7446e+01,  1.0915e+01, -7.1271e+00, -4.5272e-02, -1.1678e+00,
          4.4786e+00, -1.0501e+01, -5.0717e-03, -4.9866e-02, -2.0494e+01],
        [-3.1059e+00, -4.5329e-01, -1.1084e+01,  8.0550e-02, -5.1100e+00,
         -2.0669e+00,  2.2020e+00,  1.9195e+00,  4.8091e-02,  4.9407e+00],
        [-2.7151e+00,  1.2689e+00, -5.9608e-01, -1.8189e-01, -5.0935e+00,
         -2.0358e+00,  2.9646e+00,  3.2088e+00, -1.3078e-01,  2.8209e+00],
        [-1.3108e-01, -3.1295e-01,  1.0464e+00,  1.6180e-01,  6.3027e-01,
         -2.4032e+00,  9.5382e-01,  5.6659e-01,  1.5917e-01, -3.2053e+00],
        [-2.0771e+00,  7.0294e-01, -3.3688e-01, -5.2928e-02,  6.9871e-01,
         -8.8556e-01,  1.6547e+00,  2.8458e-01, -3.4137e-02, -1.5123e+00],
        [-8.0526e+00,  2.8233e+00, -4.9481e-01,  4.0104e-01, -1.7294e+00,
          7.8355e+00, -8.5186e+00,  9.8750e-02,  3.7656e-01, -6.7861e+00],
        [-3.8605e+01,  3.0973e+00, -1.2227e+01, -5.1517e-01,  3.0994e+00,
          1.9489e+00, -1.0392e+01, -1.2594e-02, -4.7543e-01, -1.1490e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-5.0432, -9.1847, -4.9500, -5.8588, -5.5584, -6.3836, -4.1148, -4.1329,
        -3.2904, -3.2890], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  2.1611,   0.4604,   3.5708,  -8.5474,   4.9514,   3.1938,   1.8529,
           0.8438,  -0.5662,  20.3362],
        [ -2.1611,  -0.5005,  -3.5709,   8.5432,  -4.9514,  -3.1938,  -1.8530,
          -0.8438,   0.4854, -20.3335]], device='cuda:0'))])
xi:  [217.09328]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1194.8133372050047
W_T_median: 947.6025802945303
W_T_pctile_5: 217.67882799258084
W_T_CVAR_5_pct: 29.89744778955821
Average q (qsum/M+1):  35.0
Optimal xi:  [217.09328]
Observed VAR:  947.6025802945303
Expected(across Rb) median(across samples) p_equity:  0.22726692284146946
obj fun:  tensor(-31.0907, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 999.0
-----------------------------------------------
