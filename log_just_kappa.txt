Starting at: 
2022-06-29 17:24:27

 Random seed:  1  

tracing parameter entered from terminal:  1.5 2.0 3.0 10.0


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = basic_T30_VWD
timeseries_basket['basket_desc'] = CRSP data: T30 and VWD
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
5.0% of MC simulations done.
10.0% of MC simulations done.
15.0% of MC simulations done.
20.0% of MC simulations done.
25.0% of MC simulations done.
30.0% of MC simulations done.
35.0% of MC simulations done.
40.0% of MC simulations done.
45.0% of MC simulations done.
50.0% of MC simulations done.
55.00000000000001% of MC simulations done.
60.0% of MC simulations done.
65.0% of MC simulations done.
70.0% of MC simulations done.
75.0% of MC simulations done.
80.0% of MC simulations done.
85.0% of MC simulations done.
90.0% of MC simulations done.
95.0% of MC simulations done.
100.0% of MC simulations done.
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  hidden_layer    None       None         None   
4        obj.layers[4]        4  hidden_layer    None       None         None   
5        obj.layers[5]        5  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  
4    False        None  
5    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  hidden_layer       4  logistic_sigmoid   
4        obj.layers[4]        4  hidden_layer       4  logistic_sigmoid   
5        obj.layers[5]        5  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 4)    False        None  
4       (4, 4)    False        None  
5       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.861114840684
gradient value of function right now is: [ 1.74924904e-02  1.74581903e-02  2.80291262e-03  1.73551215e-02
 -1.74924904e-02 -1.74581903e-02 -2.80291262e-03 -1.73551215e-02
  2.30235606e-04  2.88143154e-04  3.00905202e-04  2.89447847e-04
  2.54109032e-04  3.18097639e-04  3.32156879e-04  3.19490005e-04
  1.24512618e-03  1.45706376e-03  1.56007404e-03  1.52689642e-03
  6.75399041e-04  8.35914215e-04  8.76503083e-04  8.45550040e-04
 -1.00066324e-04  2.82483202e-04  2.95522627e-04 -1.04876139e-04
 -1.51324802e-04  1.65400993e-04  1.83363046e-04 -1.51003326e-04
 -1.09827384e-04  3.32479838e-05  4.56606961e-05 -1.07007840e-04
 -5.14238665e-05  2.86558740e-06  8.58425120e-06 -4.97240402e-05
  3.86801795e-06  1.06220337e-05 -3.76221265e-05  7.39724584e-06
 -8.22314192e-06  1.35023799e-05 -1.60125838e-04  3.42515401e-06
 -3.96438209e-06  1.66653326e-05 -1.40360555e-04  6.92708128e-06
  2.81981498e-07  9.81776532e-07 -3.91563274e-06  6.45485882e-07
 -3.11499314e-06  1.15384176e-04  8.29779513e-07  2.71686337e-05
 -1.49457213e-06 -5.03147072e-05  1.46915672e-06  8.24381606e-05
 -1.19464422e+00]
supnorm grad right now is: 1.194644215487687
Weights right now are: 
[-1.65044615e+00 -2.04969355e+00 -4.78713745e-01 -2.04102559e+00
  1.91119752e+00  6.85871138e-01  1.77082408e-01  1.79090164e+00
 -1.43182987e+00 -8.55349582e-01 -1.61903224e+00 -1.75470046e+00
 -1.14055820e+00 -1.27237623e+00 -1.53647300e+00 -1.30618019e+00
  3.58215428e-01  5.03168837e-01  1.44687339e-01  7.76576993e-01
 -1.27493764e+00 -1.87635659e+00 -9.54395746e-01 -5.72992793e-01
 -5.21145120e-01 -5.77815522e-01 -7.50734510e-01 -3.72979907e-01
 -1.83987466e-01 -1.00137130e+00 -1.05425708e+00  3.62062441e-01
 -3.39530168e-01 -1.71493775e+00 -1.53174682e+00  1.09625529e-01
  1.02085146e-02 -1.63795552e+00 -1.41003519e+00 -7.75557278e-01
  5.31896463e-01  1.98123929e+00 -1.07799127e+00  9.94445043e-01
 -1.81209526e+00 -9.06796933e-01  1.13671569e+00 -1.67715798e+00
 -1.42885062e+00 -1.21978925e+00  1.45829863e+00 -1.74817210e+00
  1.74330103e+00  1.87604219e+00 -1.10177643e+00  1.06867099e+00
  3.26199151e-01 -1.01584788e+00 -1.18905843e+00 -1.77938592e+00
  7.76039107e-01  1.57050013e+00 -2.31715468e-01 -1.36960485e+00
  2.49021379e+01]
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.6567643983667
gradient value of function right now is: [ 5.28639582e-03  5.28543239e-03  1.13193681e-03  5.27035775e-03
 -5.28639582e-03 -5.28543239e-03 -1.13193681e-03 -5.27035775e-03
  2.61503072e-05  5.88250479e-05  5.66170351e-05  4.07412161e-05
  3.07274829e-05  6.58069614e-05  6.37231530e-05  4.67692620e-05
  5.56548326e-04  6.50033831e-04  6.95374997e-04  6.66509021e-04
  1.16855247e-04  1.95046182e-04  1.95646912e-04  1.59500678e-04
 -1.04284408e-04  8.15865258e-05  8.81256046e-05 -1.07711154e-04
 -1.35309409e-04  4.98448210e-05  5.81550391e-05 -1.37982229e-04
 -1.19524115e-04  7.43165820e-06  1.46689854e-05 -1.20628655e-04
 -6.91069175e-05 -9.19080330e-06 -5.04717679e-06 -6.92786099e-05
 -4.27263722e-07  1.72470711e-06 -2.60717002e-05  6.76271857e-07
 -2.16213767e-06  6.49696603e-06 -8.90807140e-05  2.08523801e-06
  1.06558001e-06  8.69962412e-06 -6.40932893e-05  4.70094425e-06
  6.17674859e-07 -1.63112605e-07  1.01277457e-05  2.20161315e-07
  2.16680560e-06  4.58814068e-05  3.08089171e-06  1.34663908e-05
 -7.06733423e-06 -4.35420950e-05  4.35918700e-06  3.42603011e-05
 -2.71564237e+00]
supnorm grad right now is: 2.715642372256006
Weights right now are: 
[-1.80534706 -2.2384409  -0.51239239 -2.26167817  2.06609843  0.87461848
  0.21076106  2.01155422 -1.44547161 -0.87462367 -1.62954838 -1.76406527
 -1.17285095 -1.31615909 -1.56186253 -1.32878745  0.2254378   0.34847663
  0.05730283  0.69289363 -1.3066425  -1.91798635 -0.97803301 -0.5944598
 -0.44144499 -0.66561596 -0.82396583 -0.23853608 -0.09056784 -1.04139187
 -1.09118301  0.51685625 -0.22754573 -1.72150792 -1.54327882  0.2899608
  0.05126145 -1.63205248 -1.40846552 -0.70878233  0.53153092  1.97654991
 -0.98899269  0.99037743 -1.78288306 -0.92230673  2.30250898 -1.67453291
 -1.41161604 -1.2497877   2.6815744  -1.76552867  1.74203297  1.87550432
 -1.11887568  1.06729554  0.35651622 -1.23277152 -1.2012414  -1.81642437
  0.79634495  1.67214182 -0.25104478 -1.50432703 24.80450337]
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.8375131856974
gradient value of function right now is: [ 3.65943009e-04  4.36004020e-04 -1.73262568e-04  5.18204665e-04
 -3.65943009e-04 -4.36004020e-04  1.73262568e-04 -5.18204665e-04
 -3.63576889e-04 -1.83663059e-04 -1.74854403e-04 -4.87747585e-04
 -3.52277155e-04 -1.72426144e-04 -1.62712713e-04 -4.74365554e-04
  1.83239317e-04  2.81886087e-04  3.15024573e-04  1.82758854e-04
 -5.28178596e-04 -2.34876118e-04 -2.15288114e-04 -7.18835870e-04
 -8.03308896e-04  2.34154483e-05  3.03168807e-05 -1.05857894e-03
 -6.61257598e-04  2.12255615e-05  2.68779028e-05 -8.71379769e-04
 -6.02315820e-04  1.28340311e-06  6.34349417e-06 -7.93134335e-04
 -7.05673556e-04 -2.03115105e-05 -1.44178990e-05 -9.28639833e-04
  1.21657877e-05  8.60112125e-06  1.95495089e-04  1.08024418e-05
  2.49836617e-06  1.07699124e-05 -1.05612716e-04  5.97914309e-06
  4.42878117e-06  1.20891131e-05 -7.62981362e-05  7.67542500e-06
  1.33515831e-05  4.26533364e-06  2.77583580e-04  9.72933289e-06
  4.70153695e-06  9.07074805e-05  5.98905385e-06  9.09452054e-05
 -3.23399088e-05 -1.21178298e-04  8.96366538e-06  7.49727481e-05
 -1.59355942e+00]
supnorm grad right now is: 1.5935594239748618
Weights right now are: 
[-1.92734569e+00 -2.38797743e+00 -5.45998751e-01 -2.43808161e+00
  2.18809707e+00  1.02415502e+00  2.44367414e-01  2.18795766e+00
 -1.41725051e+00 -8.68323710e-01 -1.62503969e+00 -1.74810714e+00
 -1.11342831e+00 -1.30625233e+00 -1.55380414e+00 -1.29465452e+00
  6.19538644e-02  1.49834206e-01 -5.54811659e-02  5.92752554e-01
 -1.28244191e+00 -1.92557865e+00 -9.81317014e-01 -5.81545342e-01
 -6.82392288e-03 -7.44598486e-01 -8.92247056e-01  6.00613609e-01
  2.96014887e-01 -1.08211321e+00 -1.12947541e+00  1.25948320e+00
  3.03446210e-01 -1.72653078e+00 -1.55564481e+00  1.29211162e+00
  3.33196396e-01 -1.61226501e+00 -1.39904178e+00 -1.61063348e-01
  5.30816458e-01  1.97418032e+00 -9.74095405e-01  9.87918735e-01
 -1.76602825e+00 -9.51522614e-01  4.13982704e+00 -1.68889378e+00
 -1.41996034e+00 -1.30323342e+00  4.42200630e+00 -1.81532391e+00
  1.69237896e+00  1.85457628e+00 -2.43911073e+00  1.01343182e+00
  3.32060589e-01 -1.57887092e+00 -1.22938751e+00 -1.94263253e+00
  9.77365236e-01  1.96148967e+00 -3.72744478e-01 -1.72873487e+00
  2.48831697e+01]
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2842.6310540317318
gradient value of function right now is: [-0.15902029 -0.15562725 -0.20222074 -0.15743768  0.15902029  0.15562725
  0.20222074  0.15743768 -0.00842176 -0.01033973 -0.01226918 -0.01526896
 -0.00572838 -0.00703538 -0.0083511  -0.01039417  0.02078596  0.02555006
  0.03033468  0.03778992 -0.00827449 -0.01015525 -0.01204776 -0.01498888
 -0.04562396 -0.01977774 -0.02018506 -0.05323989 -0.05900097 -0.0250209
 -0.02553718 -0.06893909 -0.06372068 -0.02679946 -0.02735164 -0.07457283
 -0.0841081  -0.03425961 -0.03498348 -0.09787808  0.00528469  0.04626653
  0.11631018  0.01810249 -0.01107323 -0.03284615 -0.08249765 -0.01740378
 -0.01078472 -0.03165735 -0.07973888 -0.01684005  0.01547171  0.08760299
  0.19365473  0.03786641 -0.19969101  0.36893175 -0.41493592  0.93914304
  0.65609118 -1.62997816 -0.25665406  0.5382506   3.42802458]
supnorm grad right now is: 3.428024581191827
Weights right now are: 
[-1.41655957 -1.71030758  3.98341516 -1.62388885  1.67731094  0.34648517
 -4.28504649  1.3737649  -2.28667453 -1.83462827 -2.4811745  -2.4141345
 -1.90616517 -2.20666066 -2.37856102 -1.7820112   2.71286007  2.82385263
  2.55466461  3.26848117 -2.46957604 -3.19320191 -2.01939076 -1.45051203
  2.59494192 -1.88230709 -1.96092958  4.81394866  2.32414902 -2.12952743
 -2.12327612  4.7803215   2.22058413 -2.4736688  -2.28701806  4.70348989
  2.84304178 -2.63144721 -2.37203956  3.80883794  3.87821746  4.46483877
 -2.33529742  3.9301995  -3.38280238 -1.84801218  5.22635047 -3.09955421
 -3.04920971 -2.21654752  5.29748035 -3.25138626  4.99011986  4.40698863
 -3.37066205  4.18561439 -1.02166222 -2.42905311 -3.58930172 -2.64938182
  4.79604129  1.69726686 -2.27283967 -2.61770147 25.87057854]
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2837.513615952236
gradient value of function right now is: [-6.13013887e+00 -6.07377906e+00 -5.45260744e+00 -6.10304248e+00
  6.13013887e+00  6.07377906e+00  5.45260744e+00  6.10304248e+00
 -8.24411366e-02 -1.04088057e-01 -1.19311354e-01 -1.80692177e-01
 -5.84984878e-02 -7.38969902e-02 -8.47434646e-02 -1.28416804e-01
  1.92221509e-01  2.42644652e-01  2.77891067e-01  4.21262956e-01
 -8.04896842e-02 -1.01632316e-01 -1.16512894e-01 -1.76443706e-01
 -5.57179042e-01 -6.86166512e-02 -6.75398860e-02 -6.62446255e-01
 -7.48178591e-01 -9.39319677e-02 -9.24829179e-02 -8.88502479e-01
 -7.82363935e-01 -1.01002254e-01 -9.94853492e-02 -9.28187642e-01
 -1.27964099e+00 -1.60385835e-01 -1.57877562e-01 -1.51762694e+00
  6.04317302e-02  8.12648553e-02  1.68499437e+00  5.18485067e-02
 -7.91095016e-03 -1.62928503e-02 -1.71944699e-01 -8.23839589e-03
 -6.87265712e-03 -1.42262845e-02 -1.47347191e-01 -7.17866981e-03
  9.39470450e-02  1.33520011e-01  2.38799060e+00  8.35485579e-02
 -2.88222272e-01  1.13864437e+00 -3.34353801e-01  1.03416158e+00
  1.92911045e+00 -6.62477481e+00 -2.25180175e-01  7.87757168e-01
  7.81009884e+00]
supnorm grad right now is: 7.810098837657821
Weights right now are: 
[-1.63343506 -1.92856499  4.09046957 -1.84179163  1.89418643  0.56474258
 -4.39210091  1.59166768 -1.89024205 -1.44234881 -2.07860856 -2.07040932
 -1.38580063 -1.66806087 -1.81537223 -1.24832984  3.01801172  3.15702855
  2.88327151  3.69079327 -2.02573738 -2.75289455 -1.57407    -1.05805191
  2.52834829 -0.74297449 -0.78385771  4.64607948  2.24098182 -1.07705776
 -1.0330273   4.60421175  2.14228592 -1.43712475 -1.21490795  4.54364112
  2.6815181  -1.84671724 -1.54641788  3.55004241  5.89137153  4.58485722
 -1.24849013  5.16991311 -2.22619807 -0.15168298  6.08039464 -1.73310882
 -1.92301802 -0.56931418  6.17017354 -1.92572161  6.54861931  3.99325419
 -2.22882192  4.89610907  1.35377664 -2.64890026 -2.15697959 -3.35685904
  5.32668916  1.45803986 -0.12479151 -3.15550189 25.80046671]
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2842.796351444967
gradient value of function right now is: [-2.57680165e+00 -2.56376875e+00 -2.43650871e+00 -2.56667662e+00
  2.57680165e+00  2.56376875e+00  2.43650871e+00  2.56667662e+00
 -1.95310849e-02 -2.62768529e-02 -3.15069879e-02 -5.35715130e-02
 -1.26508409e-02 -1.73553015e-02 -2.10279239e-02 -3.66160511e-02
  4.49786468e-02  6.00043400e-02  7.14190995e-02  1.20498886e-01
 -1.90047151e-02 -2.56462575e-02 -3.08082261e-02 -5.25524584e-02
 -9.75055686e-02 -2.17227974e-02 -2.16943886e-02 -1.13112388e-01
 -1.44322794e-01 -3.30462897e-02 -3.30562023e-02 -1.66956171e-01
 -1.60188626e-01 -3.78541962e-02 -3.78988319e-02 -1.85050075e-01
 -3.01344193e-01 -6.72050840e-02 -6.73211124e-02 -3.46110467e-01
  8.39925272e-03  4.77789698e-02  3.19832894e-01  1.46940926e-02
 -3.52019208e-03 -1.94521951e-02 -5.86023346e-02 -5.53357906e-03
 -3.22923250e-03 -1.78656134e-02 -5.34535417e-02 -5.07383093e-03
  1.90256156e-02  8.56020755e-02  4.25544468e-01  2.98461823e-02
 -1.32541128e-01  2.28584685e-01 -2.94233632e-01  5.70118569e-01
  6.69566986e-01 -1.55273540e+00 -1.47854148e-01  2.69366602e-01
  8.04708219e+00]
supnorm grad right now is: 8.047082189035905
Weights right now are: 
[-1.55397904 -1.84969775  4.3479958  -1.76221722  1.81473041  0.48587534
 -4.64962713  1.51209326 -1.20805881 -0.82625423 -1.54006617 -1.59031775
 -0.9114304  -1.19527215 -1.38276735 -0.76932322  3.13133884  3.32749773
  3.11930431  4.00236392 -1.3410143  -2.13070851 -1.02801947 -0.5634796
  3.12787117 -1.30192604 -1.34275416  5.22975176  2.79604026 -1.7599001
 -1.71502992  5.15931173  2.66906701 -2.17760695 -1.95403768  5.08453249
  3.11978849 -2.8364751  -2.53585071  3.99851923  7.47009295  5.37534023
 -1.48929295  6.55616422 -0.96098449  0.97808976  6.91548887 -0.49209751
 -0.63722655  0.59869161  7.03213185 -0.67114427  7.78501197  4.29817986
 -2.355252    5.81386284  2.30491507 -2.57853417 -1.99402599 -3.28305322
  6.03135617  1.88535519  0.67748278 -3.00714521 26.14407001]
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -2843.8129335452495
gradient value of function right now is: [ 1.5591091   1.54253604  1.18104541  1.53638749 -1.5591091  -1.54253604
 -1.18104541 -1.53638749  0.03749582  0.04823559  0.05721853  0.12107526
  0.02694439  0.03452078  0.04085206  0.08568066 -0.07672003 -0.10320409
 -0.12547255 -0.28889224  0.03737444  0.04794233  0.05677779  0.11937512
  0.14756542  0.03320442  0.03448642  0.17588088  0.21268385  0.05117949
  0.05203765  0.25106292  0.23804586  0.06010052  0.0603889   0.27902395
  0.59951599  0.1696508   0.16716782  0.69116326 -0.07438412 -0.04783801
 -0.76034445 -0.05297568  0.00892942  0.01117664  0.10695518  0.00879319
  0.00938629  0.0124459   0.09497625  0.00972092 -0.11255338 -0.07244502
 -1.15285719 -0.07993467  0.29857172 -1.40676192  0.11024225 -0.41915281
 -1.31096721  3.47707454  0.17724558 -0.78649664 -5.5300882 ]
supnorm grad right now is: 5.5300881953135645
Weights right now are: 
[-1.59051866 -1.88808443  4.48354351 -1.80220061  1.85127004  0.52426202
 -4.78517485  1.55207666 -1.14322357 -0.60867012 -1.23812359 -1.08411886
 -1.16862285 -1.22570238 -1.287438   -0.34091925  2.99898865  3.23114938
  3.04743575  4.0697182  -1.35594349 -1.96933959 -0.76824641 -0.05973335
  3.56274265 -1.06438532 -1.12857916  5.60846236  3.19281571 -1.4905312
 -1.4500486   5.51197773  3.02885304 -1.88083542 -1.65070036  5.40961912
  3.2425197  -2.87423714 -2.54767777  4.09329207  8.82951     4.87004167
 -1.38017779  7.37660586 -1.50201926  2.59780118  7.30157531 -0.39175884
 -1.5361413   2.02828614  7.46027383 -0.92504381  8.77429051  3.45691912
 -2.04079593  6.20157894  3.67596696 -2.21415561 -0.55257254 -3.73377786
  6.42583762  1.78559048  2.3140473  -2.90348996 25.69288771]
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2844.8766102523145
gradient value of function right now is: [ 1.21330289e-01  1.07410720e-01 -7.52870611e-02  1.02247778e-01
 -1.21330289e-01 -1.07410720e-01  7.52870611e-02 -1.02247778e-01
  1.18845222e-02  1.71633400e-02  2.25287974e-02  7.88734732e-02
  8.83346631e-03  1.25918083e-02  1.64055885e-02  5.63738745e-02
 -1.97780823e-02 -3.33730702e-02 -4.72001125e-02 -1.93044686e-01
  1.21926815e-02  1.74018615e-02  2.26891317e-02  7.81107325e-02
  5.63586961e-02  4.40783342e-02  5.02816115e-02  5.54844133e-02
  8.87182704e-02  6.88857525e-02  7.35299861e-02  8.84589243e-02
  1.07393421e-01  8.25336406e-02  8.54325591e-02  1.07744373e-01
  3.65397453e-01  2.76054710e-01  2.75076319e-01  3.71637012e-01
 -2.98173576e-04 -3.36125493e-03 -4.36628208e-02 -8.50351940e-04
  8.94027730e-05  2.09719006e-03  4.09405212e-02  2.35607669e-04
  1.58643666e-03  4.44535979e-03  3.59042737e-02  2.06822309e-03
  4.54074638e-03  6.75460045e-04 -6.07333439e-02  3.92122302e-03
  3.64788669e-02 -6.27647057e-02  4.48162732e-02 -1.06765506e-01
 -7.03153478e-01  1.57932935e+00  3.54971230e-02 -8.82827925e-02
 -1.76872364e+00]
supnorm grad right now is: 1.76872363895872
Weights right now are: 
[-1.58284478 -1.8857361   4.62203027 -1.80238753  1.84359615  0.52191369
 -4.92366161  1.55226357 -1.65112885 -0.83398752 -1.28449241 -0.59274363
 -1.74649388 -1.48736792 -1.35043454  0.19309074  3.21175045  3.38791386
  3.15716238  4.00189025 -1.90389214 -2.2152977  -0.82357804  0.45167622
  3.94895974 -0.38339629 -0.77864895  5.9065653   3.61499377 -0.87634285
 -0.98324291  5.85061845  3.47851741 -1.29834512 -1.1322798   5.78263678
  3.7267537  -2.55717295 -2.18680374  4.51809363 10.2619125   5.70286743
 -1.50143341  8.91704027 -0.61330205  4.05675978  7.545926    0.703255
 -2.42584907  2.0072955   7.73973    -1.86687061 10.00284563  3.97528104
 -1.8915022   7.39491144  3.38896783 -1.96847411 -0.22963535 -3.05209509
  6.70679745  1.66042265  2.38391238 -2.28232002 25.6935519 ]
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2841.274066078919
gradient value of function right now is: [-4.88204973 -4.85569418 -4.59610654 -4.85101365  4.88204973  4.85569418
  4.59610654  4.85101365 -0.02744341 -0.03689015 -0.04426974 -0.1489809
 -0.01990326 -0.02685246 -0.03227691 -0.109208    0.06628629  0.08742213
  0.10394089  0.34089357 -0.02702923 -0.03636693 -0.04366103 -0.14713806
 -0.15877713 -0.0349727  -0.03969415 -0.19224988 -0.20112188 -0.04476413
 -0.05228309 -0.24050703 -0.20569952 -0.0468003  -0.05544361 -0.24419127
 -0.54702042 -0.13467284 -0.15870583 -0.6286244   0.0589208   0.06212794
  0.76656823  0.05817921 -0.00555603 -0.00961893 -0.06435231 -0.00633084
 -0.00655458 -0.0108226  -0.07354316 -0.00733803  0.09451573  0.09927633
  1.08600061  0.09350424 -0.40641979  1.69574741 -0.2036713   0.71002075
  1.07598567 -3.72002264 -0.33807486  1.32490229  3.26155409]
supnorm grad right now is: 4.8820497273719266
Weights right now are: 
[-1.77185356 -2.08160056  4.50730408 -1.99971659  2.03260493  0.71777815
 -4.80893542  1.74959264 -2.08173474 -0.99571776 -1.29703662 -0.10182198
 -2.26027152 -1.678121   -1.36163182  0.78357213  3.14914708  3.4380601
  3.27037992  4.28459352 -2.38836986 -2.41100636 -0.85835857  0.95820142
  4.25084631  0.08811616 -1.01327298  6.2486994   3.878306   -0.57105313
 -0.96784599  6.12145592  3.71995418 -1.07220596 -1.00821596  6.02019959
  3.69028641 -2.89964014 -2.4087695   4.45376111 10.88475908  6.1113048
 -1.57149283  9.59878606  0.91125651  5.87458178  7.60283504  2.46178568
 -2.90359902  2.3287583   7.7655877  -2.19052403  9.92014356  3.8522745
 -2.08159436  7.34030695  2.86236062 -2.03880743 -0.18476253 -2.81203852
  7.25834026  1.53770447  2.09412959 -2.23697094 25.71221268]
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2841.760823517852
gradient value of function right now is: [-4.64758791e+00 -4.62080575e+00 -4.46134086e+00 -4.61729730e+00
  4.64758791e+00  4.62080575e+00  4.46134086e+00  4.61729730e+00
 -1.32008416e-02 -2.01760603e-02 -2.57076510e-02 -1.29006679e-01
 -9.55696294e-03 -1.47468100e-02 -1.88564116e-02 -9.58630901e-02
  3.65939795e-02  5.06620680e-02  6.23640782e-02  3.01320084e-01
 -1.30795597e-02 -2.00177230e-02 -2.55221753e-02 -1.28955635e-01
 -7.53332905e-02 -1.43572803e-02 -1.57419708e-02 -1.02408315e-01
 -9.54543180e-02 -1.68075426e-02 -2.15067410e-02 -1.25910075e-01
 -9.81857431e-02 -1.65702608e-02 -2.32388761e-02 -1.27307820e-01
 -3.69472578e-01 -5.27769028e-02 -7.63839145e-02 -4.42119933e-01
  4.87598010e-02  5.12538159e-02  5.67620275e-01  4.84590291e-02
 -3.52556494e-03 -6.65216665e-03 -3.90813070e-02 -4.06318456e-03
 -5.81707148e-03 -9.18242726e-03 -5.31344011e-02 -6.46605761e-03
  8.27106032e-02  8.53266252e-02  8.14149402e-01  8.22166587e-02
 -3.33096653e-01  1.45846658e+00 -1.49235920e-01  6.07212268e-01
  6.78882341e-01 -2.30558769e+00 -2.76944059e-01  1.15463182e+00
  2.13720652e+00]
supnorm grad right now is: 4.647587914761123
Weights right now are: 
[-1.77290431 -2.09812542  4.53521395 -2.01861442  2.03365569  0.73430301
 -4.83684528  1.76849047 -2.49788574 -0.96069109 -1.05475612  0.70200076
 -2.79455612 -1.69858655 -1.1447982   1.64779546  2.82747615  3.26600903
  3.18770148  4.5049133  -2.83146149 -2.3941439  -0.63042843  1.75993785
  4.94298463  0.67202457 -1.36799338  6.9802449   4.44259322 -0.24806015
 -1.02164827  6.67104196  4.20176484 -0.87757003 -0.92753141  6.46452462
  3.66424802 -3.45689416 -2.86273044  4.33154786 11.67106697  6.55827881
 -1.69672054 10.36531396  1.64945266  6.96497069  7.66192958  3.32346056
 -3.68201155  2.33976323  7.90421852 -2.86594862 10.14033542  3.78804756
 -2.14160463  7.50313273  2.95737808 -1.99394716 -0.03379565 -2.69798772
  7.49703146  1.7178731   2.24199905 -2.1660668  25.67024628]
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2840.175735563501
gradient value of function right now is: [ 6.50868388e+00  6.38887359e+00  5.73479275e+00  6.37684857e+00
 -6.50868388e+00 -6.38887359e+00 -5.73479275e+00 -6.37684857e+00
  4.29442115e-02  6.14068939e-02  7.66135022e-02  4.61463033e-01
  3.10666877e-02  4.40658860e-02  5.47829099e-02  3.26532072e-01
 -8.49544101e-02 -1.36815050e-01 -1.78736056e-01 -1.16382211e+00
  4.28465537e-02  6.09639545e-02  7.58940500e-02  4.53821990e-01
  1.41192390e-01  5.15489451e-02  7.31773804e-02  2.08196424e-01
  2.11524385e-01  7.39459526e-02  9.69288355e-02  3.08848638e-01
  2.37260233e-01  8.22108031e-02  1.03172555e-01  3.43612628e-01
  1.48168847e+00  6.01478808e-01  6.86667045e-01  1.96888174e+00
 -9.18152853e-02 -2.55451058e-01 -1.29181272e+00 -1.02303710e-01
  7.17372897e-03  5.79796302e-02  1.65613071e-01  1.01595259e-02
  1.20215197e-02  7.31957537e-02  1.77522037e-01  1.61137405e-02
 -1.45155250e-01 -3.79657945e-01 -1.90441139e+00 -1.59990545e-01
  6.60959197e-01 -2.56300178e+00  9.23287660e-01 -2.57624213e+00
 -3.06796079e+00  7.69224389e+00  6.43661623e-01 -2.28259509e+00
 -1.07328628e+01]
supnorm grad right now is: 10.732862799905325
Weights right now are: 
[-1.57895082 -1.89426971  4.82780058 -1.81342649  1.83970219  0.5304473
 -5.12943192  1.56330254 -2.52403583 -0.9113509  -1.02638872  0.47631164
 -2.79429928 -1.60152081 -1.05919504  1.4882115   2.03813175  2.77256625
  2.86414784  4.68326496 -2.78250478 -2.27465982 -0.53494769  1.57948235
  5.00383118  0.08164018 -2.31868076  7.44937619  4.44979773 -0.63523423
 -1.63867877  6.89267442  4.20536924 -1.14500472 -1.2999579   6.57591514
  3.82633581 -3.37643761 -2.6825412   4.24979839 12.33345351  7.22348361
 -1.48805179 11.03415362  2.24255349  7.79334746  8.12426862  4.00411834
 -2.96006563  2.85331813  8.65522293 -2.17315252  9.62127108  3.31472279
 -2.28571391  6.94450039  3.01602124 -2.21819742 -0.69883908 -2.51019858
  7.44687064  2.12158965  2.22327848 -2.33419287 25.536356  ]
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2844.5215881538643
gradient value of function right now is: [ 1.93863926e+00  1.86406260e+00  1.68288448e+00  1.86591220e+00
 -1.93863926e+00 -1.86406260e+00 -1.68288448e+00 -1.86591220e+00
  8.09895603e-03  1.28896634e-02  1.76212415e-02  2.87322277e-01
  4.96757745e-03  8.04157136e-03  1.10719475e-02  1.63387879e-01
 -1.04205639e-02 -2.31611587e-02 -3.46553616e-02 -3.39279649e-01
  6.25860777e-03  1.06166620e-02  1.48969077e-02  2.27318965e-01
  2.51415700e-02  1.21250351e-02  1.81648986e-02  3.83290111e-02
  3.58780319e-02  1.06079152e-02  1.57703411e-02  6.18332886e-02
  3.92222892e-02  8.78439865e-03  1.24784497e-02  7.11415888e-02
  7.17726439e-02 -9.24044348e-02 -1.10544472e-01  2.51296528e-01
 -4.31822720e-02 -5.86363436e-02 -4.64342344e-01 -4.64151684e-02
  1.23975686e-03 -1.53886646e-02  4.53465987e-03  8.11280458e-04
  1.18985200e-03 -7.62523478e-03  1.10286863e-02  9.23542681e-04
 -5.93706866e-02 -8.40742460e-02 -6.96973871e-01 -6.28321971e-02
  1.53834574e-01 -1.13156414e+00  2.89115084e-01 -8.75938881e-01
 -4.84076622e-02  5.70685088e-01  1.43473997e-01 -9.80672322e-01
 -2.70104693e+00]
supnorm grad right now is: 2.7010469306822342
Weights right now are: 
[-1.5708396  -1.91477002  4.82005376 -1.83431704  1.83159098  0.5509476
 -5.1216851   1.58419309 -3.87186081 -1.41405324 -1.13285253  1.55161774
 -4.24955879 -2.16458436 -1.20023664  2.60182194  1.38970878  2.39879844
  2.64261555  4.93248776 -4.21455838 -2.85778583 -0.71891029  2.57369174
  5.86310529  0.88296283 -2.66727036  8.45197011  5.02208652 -0.02577105
 -1.49367889  7.55111489  4.64199452 -0.73050883 -0.8703857   7.03928023
  3.51949915 -4.30178988 -3.4700807   3.64571873 13.24729444  8.05667128
 -1.73884199 11.98773198  2.10460683  7.49583957  7.60938704  3.89337403
 -3.43519129  2.55753429  8.57435815 -2.61723948  9.77049432  3.36505817
 -2.1606949   7.02315719  3.25964339 -2.30481142 -0.90732916 -2.64863123
  8.20448314  1.87438236  2.33389837 -2.37155756 25.76563216]
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2844.0445144429696
gradient value of function right now is: [ 2.20854434e+00  2.07776247e+00  1.68784795e+00  2.08226496e+00
 -2.20854434e+00 -2.07776247e+00 -1.68784795e+00 -2.08226496e+00
  3.33576716e-02  4.09188584e-02  5.13304358e-02  3.88310370e-01
  2.50239489e-02  2.99326643e-02  3.68657073e-02  2.64627601e-01
 -1.23734748e-02 -2.94343276e-02 -5.50811578e-02 -6.80693131e-01
  3.37467915e-02  4.06375378e-02  5.03110054e-02  3.64179072e-01
  8.98402116e-03  7.40009413e-03  4.14015531e-02  3.86804540e-02
  1.73928742e-02  6.87989450e-03  2.53726740e-02  4.88353519e-02
  2.54965306e-02  6.07864589e-03  9.64426055e-03  6.08657520e-02
  5.98442639e-01  1.64680422e-01  1.50699768e-01  1.11273152e+00
 -4.59722827e-02 -1.30673851e-01 -6.76731055e-01 -5.06515406e-02
  1.80724822e-03  1.15154512e-02  8.04523521e-02  2.23859030e-03
  5.22019655e-03  3.01894020e-02  6.40662711e-02  6.61280347e-03
 -7.21477140e-02 -2.77653947e-01 -9.41078902e-01 -8.12838140e-02
  2.68381393e-01 -1.26655653e+00  5.26566789e-01 -1.40700135e+00
 -1.01226678e+00  3.00213375e+00  2.67606411e-01 -1.12399009e+00
 -6.99858103e+00]
supnorm grad right now is: 6.998581027460425
Weights right now are: 
[-1.68839296 -1.99578385  4.80359442 -1.91666704  1.94914434  0.63196143
 -5.10522576  1.66654309 -3.61128952 -0.64403348 -0.22365902  0.98899246
 -4.20876483 -1.45476854 -0.25770438  2.35432    -0.0884383   1.59459691
  2.24807207  5.70407397 -4.04422686 -2.07983239  0.25390227  2.29607376
  6.7068586   1.81414425 -2.14327062 10.21308642  5.26211386  0.63052584
 -0.98127302  8.45754886  4.63012021 -0.3469334  -0.18355561  7.34774381
  4.06452406 -3.94049365 -3.0230563   3.47195081 14.79062227  8.86730815
 -1.37595408 13.48172148  0.7348592   8.04133077  7.61695341  2.9740194
 -4.09270532  3.9970296   8.89585916 -2.71442825  9.23323501  2.67106417
 -2.07081043  6.37801824  3.62295139 -2.31673161 -1.27839897 -2.50879295
  8.0638898   2.13286824  2.77833968 -2.38764452 25.63081913]
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -2842.847997815556
gradient value of function right now is: [-2.74421494e+00 -2.52973531e+00 -1.99924637e+00 -2.54037945e+00
  2.74421494e+00  2.52973531e+00  1.99924637e+00  2.54037945e+00
 -2.13662305e-02 -4.16977274e-02 -7.39411633e-02 -5.97853470e-01
 -1.48579786e-02 -2.87905414e-02 -5.10772459e-02 -4.11393767e-01
  1.91620674e-02  4.57324089e-02  9.75113412e-02  1.00476989e+00
 -2.02976861e-02 -3.92929934e-02 -6.96493627e-02 -5.61143607e-01
 -1.44337290e-02 -1.01090881e-02 -2.92826673e-02 -8.18173256e-02
 -1.76239851e-02 -7.54126380e-03 -1.80639040e-02 -7.07502464e-02
 -2.39418181e-02 -4.08661146e-03 -6.86018074e-03 -6.25453457e-02
 -9.02443731e-01 -1.03199093e-01 -1.22685079e-01 -1.58673111e+00
  8.41671870e-02  2.41934281e-01  1.60423187e+00  9.40210072e-02
 -2.31364537e-03 -2.71067060e-02 -5.85929104e-02 -3.43396559e-03
 -3.98173618e-03 -4.60196134e-02 -5.63851775e-02 -6.08562055e-03
  1.32804831e-01  4.54205383e-01  1.63722678e+00  1.55066589e-01
 -5.06284942e-01  3.01489426e+00 -9.71397938e-01  3.54460646e+00
  9.24843646e-01 -3.13621475e+00 -5.22069571e-01  2.78758712e+00
  8.17797917e+00]
supnorm grad right now is: 8.177979170513584
Weights right now are: 
[-1.68287898e+00 -1.98701132e+00  4.91060497e+00 -1.90808267e+00
  1.94363035e+00  6.23188906e-01 -5.21223631e+00  1.65795872e+00
 -5.14680749e+00 -9.14875417e-01  6.99672226e-02  9.99453682e-01
 -6.00046874e+00 -1.86551833e+00 -2.48878144e-02  2.45693914e+00
 -8.42503785e-01  8.27742222e-01  1.87349647e+00  6.16104969e+00
 -5.73099878e+00 -2.44532303e+00  4.86651230e-01  2.35635881e+00
  7.61775424e+00  2.41025809e+00 -2.56050099e+00  1.19782720e+01
  5.66512553e+00  1.11289113e+00 -1.09132615e+00  9.72431466e+00
  4.76903194e+00  2.79904095e-02  5.61182480e-01  7.80489892e+00
  4.64584338e+00 -3.97494179e+00 -2.82670566e+00  2.89010617e+00
  1.66934636e+01  9.94461944e+00 -1.86932890e+00  1.53947352e+01
  6.46613783e-01  8.55211940e+00  7.55069749e+00  3.09388773e+00
 -5.56825014e+00  4.55636795e+00  9.35668373e+00 -3.86592618e+00
  9.23169632e+00  2.90595939e+00 -1.65759277e+00  6.27761906e+00
  3.64542691e+00 -1.99036213e+00 -1.68946306e+00 -2.22414502e+00
  8.17072254e+00  2.17057397e+00  2.57603357e+00 -2.06905407e+00
  2.58978488e+01]
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2843.415886851179
gradient value of function right now is: [ 5.31225879e+00  5.00615404e+00  4.49559441e+00  5.03359119e+00
 -5.31225879e+00 -5.00615404e+00 -4.49559441e+00 -5.03359119e+00
  1.92835985e-02  2.51378407e-02  3.63527658e-02  7.33384574e-01
  1.19655196e-02  1.54669658e-02  2.25516139e-02  4.63901418e-01
 -1.47758222e-02 -2.16770273e-02 -4.13102808e-02 -1.36209398e+00
  1.67647226e-02  2.17915639e-02  3.19103324e-02  6.54772107e-01
  5.84514030e-03  8.64433211e-03  3.14812315e-02  4.70359765e-02
  4.17097341e-03  4.50074510e-03  1.45269848e-02  2.42256346e-02
  2.22494904e-03  4.21681996e-05 -1.16134879e-03  5.23277947e-03
  8.47102884e-01  2.58332336e-01  2.29480781e-01  1.71860900e+00
 -6.61871238e-02 -1.93697026e-01 -1.05355988e+00 -7.87361189e-02
  2.96397792e-03  1.74665103e-02  1.09082889e-01  4.30348622e-03
  3.03864838e-03  3.13953463e-02  6.49500289e-02  4.99763805e-03
 -6.65477081e-02 -3.36596435e-01 -8.35475434e-01 -8.41320790e-02
  3.75447645e-01 -1.85356278e+00  9.88191263e-01 -2.59975633e+00
 -1.38373768e+00  4.15644070e+00  4.42254132e-01 -1.89097546e+00
 -5.10591490e+00]
supnorm grad right now is: 5.312258794545244
Weights right now are: 
[-1.5859657  -1.93935159  4.94113763 -1.85662908  1.84671708  0.57552918
 -5.24276897  1.60650513 -6.04899833 -0.9722322   0.53118483  1.83191884
 -7.0349716  -2.02697377  0.37288622  3.38518612 -0.59435682  0.40822408
  1.34330092  6.03317353 -6.70635956 -2.58180464  0.87283769  3.22899474
  9.04883725  3.4199046  -2.36157099 13.43667311  6.60546496  1.99287198
 -0.78905486 11.53963069  5.16025062  0.50346089  1.64216328  9.16442695
  5.25211667 -3.92831862 -2.53488358  2.55709711 18.59080775 10.46503525
 -1.98821132 17.16374618  0.02611038  9.23581353  7.36958851  2.73730536
 -6.30633656  5.53763757  9.61027888 -4.38444078  8.22147824  3.09817905
 -2.24636315  5.35717966  3.48111366 -2.14298996 -1.94789042 -2.53637371
  8.39643758  2.17902094  2.27933717 -2.21603684 25.84193819]
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2844.447723518338
gradient value of function right now is: [ 3.42464136e+00  3.22157196e+00  2.96526618e+00  3.24933306e+00
 -3.42464136e+00 -3.22157196e+00 -2.96526618e+00 -3.24933306e+00
  1.94065332e-03  3.03994851e-03  4.35422791e-03  3.94223604e-01
  1.27148519e-03  1.88906616e-03  2.79043020e-03  2.53113865e-01
 -2.16484616e-03 -4.11167650e-03 -8.43879235e-03 -8.19413310e-01
  1.66566137e-03  2.56463425e-03  3.86223986e-03  3.54581355e-01
  2.73925939e-03  1.94992588e-03  4.52536846e-03  7.52990553e-03
  2.32057244e-03  1.28837554e-03  2.80268319e-03  6.71732717e-03
  1.31511407e-03  2.19998649e-04  3.12489676e-04  3.95778201e-03
  5.75181407e-01  1.27714594e-01  1.13663487e-01  1.12533529e+00
 -5.70801864e-02 -1.72853414e-01 -8.75952536e-01 -5.63838080e-02
  1.51627762e-03  2.18394806e-02  7.29163819e-02  1.95060444e-03
  9.51742402e-04  1.62537876e-02  3.79230316e-02  1.29014783e-03
 -3.33590261e-02 -1.37009369e-01 -5.27662169e-01 -3.16090842e-02
  2.35553535e-01 -1.41303400e+00  6.12864503e-01 -1.96746313e+00
 -1.05884250e+00  3.18211495e+00  2.25249291e-01 -1.21691725e+00
 -3.05754746e+00]
supnorm grad right now is: 3.42464135842596
Weights right now are: 
[-1.63198989 -1.97129683  4.9640663  -1.88850782  1.89274126  0.60747442
 -5.26569764  1.63838387 -6.94548709 -1.51262346  0.27440149  1.73522568
 -7.92782469 -2.54009994  0.16176167  3.65924988  0.51664555  0.95107265
  1.43580672  6.08308777 -7.59465706 -3.10680131  0.62823828  3.37999624
  9.12173208  2.81444554 -2.89006279 13.87332622  6.88817256  1.48948634
 -1.27335669 12.42128352  5.51969428  1.30330866  2.91628166 10.46366567
  5.8357032  -4.97350673 -3.05001667  2.46650012 19.92939141 10.76723231
 -2.60427303 18.38110039  1.28989714 10.86662104  7.63464219  4.25334635
 -6.23413287  7.09849658 10.24794233 -3.95165387  8.62262215  3.09014682
 -2.17812435  5.56086576  3.76313937 -2.19764574 -2.47820456 -2.60822482
  8.401954    2.30589465  2.52369276 -2.43922889 25.93596095]
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2841.868694028639
gradient value of function right now is: [ 5.26925917e+00  4.40529807e+00  3.76907883e+00  4.53479425e+00
 -5.26925917e+00 -4.40529807e+00 -3.76907883e+00 -4.53479425e+00
  4.93957716e-02  6.11754811e-02  7.36414792e-02  1.58623565e+00
  2.92077872e-02  3.57662801e-02  4.28483846e-02  9.17052953e-01
 -4.15219242e-02 -5.33898134e-02 -6.66985024e-02 -2.44195077e+00
  4.16358380e-02  5.11774790e-02  6.14977241e-02  1.31369242e+00
  1.28836621e-02  8.19339635e-03  2.54123825e-02  1.86783012e-01
  6.96571864e-03  4.12219254e-03  1.18622037e-02  8.30863593e-02
  1.31402342e-03  4.49443083e-04  8.74561860e-04  6.56512377e-03
  7.60153104e-01  2.10631578e-01  1.95932490e-01  1.28534991e+00
 -8.11324833e-02 -2.63091628e-01 -1.07182991e+00 -8.12444714e-02
  3.46061676e-03  3.91380897e-02  1.10001858e-01  4.39795461e-03
  3.55590671e-03  4.09667352e-02  6.37641341e-02  4.67527946e-03
 -1.05564423e-01 -1.09223910e+00 -7.23035525e-01 -1.30764580e-01
  6.58230017e-01 -2.42793609e+00  1.88451139e+00 -3.35852566e+00
 -1.66292042e+00  4.58067281e+00  6.58068655e-01 -2.14015560e+00
 -9.44258689e+00]
supnorm grad right now is: 9.442586886974986
Weights right now are: 
[-1.58654238 -1.97422385  4.9584113  -1.88160599  1.84729375  0.61040144
 -5.26004264  1.63148204 -7.18280568 -1.54610163  0.32325588  2.12900359
 -8.27428364 -2.64197062  0.16780725  4.33034013  0.78699691  0.95966802
  1.27792608  5.90902943 -7.91555835 -3.20180735  0.61995889  3.96271906
  9.95559827  2.94337934 -2.54748656 14.71772425  7.95491756  1.71555649
 -0.91504845 13.64238639  6.20775257  2.03845456  3.78737354 12.22465835
  6.68726201 -5.65311573 -3.35370304  2.15803531 21.56954528 10.76314562
 -2.89900225 19.92838211  1.89591926 12.55693951  8.05292124  4.95397426
 -6.95135185  9.4452125  10.71837597 -4.1012744   8.73288259  2.84623781
 -2.01170657  5.55086538  4.11862266 -2.12265852 -2.45768154 -2.91215367
  8.5979488   2.31856275  2.89358254 -2.39798094 25.72292654]
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2843.3003036748455
gradient value of function right now is: [ 5.51322166e+00  5.30901164e+00  5.07866933e+00  5.34502040e+00
 -5.51322166e+00 -5.30901164e+00 -5.07866933e+00 -5.34502040e+00
  3.31321598e-03  3.90362088e-03  3.39354834e-03  3.12183990e-01
  1.88084738e-03  2.09392743e-03  1.77506533e-03  1.81011600e-01
 -2.64079450e-03 -3.17578770e-03 -2.83761322e-03 -6.29843163e-01
  2.55250047e-03  2.92117174e-03  2.53719591e-03  2.63547659e-01
  8.85897741e-04  5.08119874e-04  1.62236547e-03  1.10981173e-02
  3.88754037e-04  2.15432665e-04  8.30134033e-04  4.82083296e-03
  3.20597121e-05  9.74329803e-06  2.60056519e-05  4.13231628e-04
  6.12832000e-01  9.39261736e-02  6.95773891e-02  1.28515517e+00
 -6.07351332e-02 -1.70697555e-01 -1.19625707e+00 -6.60987348e-02
  6.45308792e-04  1.09368680e-02  6.50093683e-02  9.83648046e-04
  5.26370439e-04  4.40120461e-03  2.84494058e-02  7.30574799e-04
 -2.19381318e-02 -8.71231585e-02 -3.37534850e-01 -2.32494756e-02
  2.47547334e-01 -1.72571465e+00  6.11855354e-01 -2.34459018e+00
 -1.04933407e+00  3.55744983e+00  2.68657961e-01 -1.69843312e+00
 -3.79500014e+00]
supnorm grad right now is: 5.513221660702761
Weights right now are: 
[-1.58427881 -1.87773238  5.13617691 -1.7973993   1.84503019  0.51390997
 -5.43780825  1.54727535 -7.13826278 -1.44747719  0.33047334  1.65441056
 -8.34938975 -2.61522336  0.14578395  4.1221362   0.91715542  1.02939733
  1.43256704  6.71916166 -7.91699912 -3.12349013  0.62434344  3.65272529
 10.41366347  2.65749656 -2.9193181  15.27362432  8.76306934  1.63483118
 -1.26110524 14.44526252  7.43323692  2.70740665  4.20014095 14.32480036
  6.97003695 -5.70525597 -3.22827145  1.5659354  23.21447751 11.43150822
 -2.45108279 21.56028945  3.55964462 13.31035214  8.48383155  6.44630014
 -8.76006212  9.98431185 11.64345263 -5.25483846  7.47478381  2.87071679
 -2.5253538   4.35734208  4.11186666 -2.13216184 -3.21485654 -2.65705995
  8.8769501   2.19876957  2.68227541 -2.2694781  25.79819148]
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2844.978819406293
gradient value of function right now is: [ 5.04260034e-01  9.43852694e-01  1.20399303e+00  8.63340430e-01
 -5.04260034e-01 -9.43852694e-01 -1.20399303e+00 -8.63340430e-01
 -1.30223342e-02 -1.45212273e-02 -9.52334464e-03 -6.52787798e-01
 -6.42881312e-03 -7.18088178e-03 -4.80422859e-03 -3.24559882e-01
  5.61462973e-03  6.39956407e-03  3.91020874e-03  7.43029736e-01
 -1.00443765e-02 -1.11913882e-02 -7.44241240e-03 -4.91643338e-01
 -1.65324917e-03 -1.06026181e-03 -4.18965247e-03 -4.71663777e-02
 -7.68228678e-04 -4.66193514e-04 -1.58964736e-03 -1.79095368e-02
 -6.81563529e-05 -3.58655760e-05 -1.21098315e-04 -1.23746902e-03
 -4.42607005e-01 -1.93135788e-01 -1.75051338e-01 -5.69802856e-01
  1.03814988e-03  8.05578334e-02  1.40432452e-01  4.78585638e-03
 -2.24415833e-03 -3.62777252e-02 -1.04547332e-01 -3.52038434e-03
 -1.51456603e-03 -2.34258824e-02 -5.59407557e-02 -2.35797320e-03
  3.26315178e-02  2.81714439e-01  1.28880177e-01  4.64395014e-02
 -1.59406493e-01  2.59006841e-01 -3.32387385e-01  5.85452934e-01
  1.25065366e+00 -3.79041024e+00 -1.80038585e-01  3.22265128e-01
  4.01142435e+00]
supnorm grad right now is: 4.011424350238319
Weights right now are: 
[-1.71132628 -1.96637716  5.08421962 -1.88845067  1.97207765  0.60255475
 -5.38585095  1.63832672 -7.2811967  -1.58851383  0.02724868  1.3983518
 -8.48883368 -2.7098191  -0.0975474   4.27926104  1.51408346  1.61933669
  2.08702687  6.99137577 -7.9929241  -3.17635223  0.39984541  3.67133673
 11.25629032  2.82315572 -3.06190928 15.44519597  9.7112627   1.91118978
 -1.50697722 14.83387883  8.20973188  3.24234761  5.11149423 16.45236284
  7.37802519 -6.36642186 -3.73309425  1.68087266 23.89702978 11.73168464
 -2.44073845 22.33617605  3.74975869 14.07979196  8.60734769  6.89533383
 -9.6802937  11.00801012 11.64777725 -5.67731509  7.80406198  2.76635946
 -2.27493718  4.70280816  3.71788605 -2.12363769 -3.01771765 -2.78366057
  9.24681224  1.96492438  2.45215918 -2.28130151 25.92343419]
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2845.464792527007
gradient value of function right now is: [-2.02257109e+00 -2.13606642e+00 -2.19311523e+00 -2.11601060e+00
  2.02257109e+00  2.13606642e+00  2.19311523e+00  2.11601060e+00
  2.70404252e-03  3.06573314e-03  1.53207202e-03  1.66527756e-01
  1.41422544e-03  1.57089260e-03  8.79595179e-04  8.06959084e-02
 -2.34616834e-03 -2.69033031e-03 -1.20924338e-03 -2.01392311e-01
  2.05182858e-03  2.29444317e-03  1.25615102e-03  1.20503260e-01
  9.56754704e-04  4.54528192e-04  8.79555001e-04  1.25467634e-02
  4.44843658e-04  2.24201884e-04  4.40872538e-04  5.54555764e-03
  4.77950573e-05  2.28544561e-05  4.74895827e-05  8.37107636e-04
  3.01465737e-02  4.46259151e-02  4.25566552e-02  3.27894269e-02
  8.29057616e-04 -8.61820715e-03  6.30411849e-02  1.25891455e-03
 -3.36920852e-05  7.90253946e-03  1.17630607e-02  4.90821384e-07
  3.88447481e-05  5.88562513e-03  5.92529725e-03  5.91180678e-05
 -5.36425564e-03 -6.66787928e-02 -2.69977773e-02 -3.55407321e-03
 -1.88865993e-02  2.85062830e-02  8.69666737e-02 -3.77387237e-02
 -1.78006406e-01  4.74321356e-01 -1.89010487e-02  4.70539402e-02
  1.33453879e+00]
supnorm grad right now is: 2.193115231876228
Weights right now are: 
[-1.75982691 -2.11090077  4.91971898 -2.00930117  2.02057829  0.74707836
 -5.22135032  1.75917722 -7.39316206 -1.69099277 -0.12430018  1.86759683
 -8.88716844 -3.05048276 -0.5433795   4.9972107   1.78077355  1.87154724
  2.28732691  6.47087974 -8.26754305 -3.41143911  0.0562337   4.32674137
 11.00117005  1.73503981 -3.91809529 15.62153011  9.62548975  0.85381122
 -2.6138421  15.16931473  8.6146164   2.77278678  4.83904338 17.56700072
  7.90355465 -6.23724806 -3.50097413  1.94921204 24.61139491 11.79005002
 -2.22465185 23.19019696  4.85383079 15.31020693  9.48823399  8.45067476
 -9.79726258 12.31418256 12.75924195 -4.81431089  7.69970918  2.72283578
 -2.33673757  4.43983876  3.57955785 -2.28903774 -3.13337974 -2.68745557
  8.94946334  2.2398646   2.30566582 -2.37443301 25.96455871]
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2841.994298634097
gradient value of function right now is: [-4.84160781e+00 -4.58297629e+00 -4.46322129e+00 -4.63552279e+00
  4.84160781e+00  4.58297629e+00  4.46322129e+00  4.63552279e+00
 -1.16173945e-02 -1.26617281e-02 -6.58482154e-03 -3.50725336e-01
 -6.22601289e-03 -6.77043022e-03 -3.61479354e-03 -2.03672047e-01
  6.76648381e-03  7.56108166e-03  3.52484090e-03  4.79030007e-01
 -9.14998256e-03 -9.95196606e-03 -5.28398495e-03 -2.87469445e-01
 -3.74269492e-03 -8.81241599e-04 -2.08308927e-03 -5.44411038e-02
 -2.03830981e-03 -4.66893404e-04 -1.03311772e-03 -2.58734164e-02
 -3.73874512e-04 -8.52793294e-05 -2.06735675e-04 -5.58460603e-03
 -1.71205637e-01 -1.42265729e-02 -1.34028204e-02 -3.32668133e-01
  7.43905235e-03  1.14723619e-01  2.82383653e-01  1.02745082e-02
 -1.85532730e-04 -1.20296717e-02 -1.55314771e-02 -3.30026647e-04
 -1.30863700e-04 -9.33851312e-03 -8.19468967e-03 -2.49465910e-04
  1.44653284e-02  3.52620874e-01  1.85789815e-01  2.43511598e-02
 -1.01624790e-01  3.35670373e-01 -8.32711922e-01  2.25676285e+00
  3.96930242e-01 -1.25632867e+00 -1.21270483e-01  3.98468431e-01
  2.22075322e+00]
supnorm grad right now is: 4.841607814172401
Weights right now are: 
[-1.91288815 -2.21159019  4.89882939 -2.11514795  2.17363952  0.84776778
 -5.20046072  1.865024   -8.08841376 -2.36249263 -0.84532611  1.48860924
 -9.75873911 -3.87776535 -1.50487838  4.93644538  2.61641023  2.69239589
  3.01056018  6.67247205 -9.07504131 -4.18147443 -0.83165225  4.15710412
 10.89642475  0.86812346 -4.05667577 15.20135515  9.69370003 -0.05304305
 -2.91958041 14.8541258   9.02645615  2.38726402  5.01122361 17.64381154
  8.24227055 -6.70851184 -3.99152394  2.04863244 25.29031783 12.0932531
 -2.49649269 23.77101542  5.83381639 14.81979853  9.52881911  9.38795709
 -9.46368986 11.87319549 12.8378087  -4.26429822  7.82902264  3.58588477
 -2.07555532  4.47800984  4.25963104 -2.43328422 -3.96612823 -2.62371416
  9.29206031  2.10195624  2.69377019 -2.56915046 25.65948845]
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2846.1269277477145
gradient value of function right now is: [-9.53859104e-01 -9.35888431e-01 -9.45805757e-01 -9.40172981e-01
  9.53859104e-01  9.35888431e-01  9.45805757e-01  9.40172981e-01
 -1.83144119e-03 -1.95413306e-03 -1.17287295e-03 -1.70547547e-02
 -4.55776627e-04 -5.27477643e-04 -1.47057401e-04 -1.17636263e-02
  5.01276533e-04  5.56632696e-04  3.16299693e-04  2.47927770e-02
 -9.76378377e-04 -1.08518401e-03 -4.53292224e-04 -1.71471944e-02
 -1.60553978e-04  4.81343507e-05  8.84846268e-05 -6.04254140e-03
 -1.01781434e-04  2.05427161e-05  4.11273251e-05 -2.33378475e-03
 -1.51053428e-05  1.70139342e-07 -6.16739074e-08 -1.89495582e-04
  3.24883561e-02  3.17609466e-02  2.47697488e-02 -1.18645346e-03
  4.78955116e-04  6.46096480e-03  2.49548156e-02  6.14968228e-04
 -4.97612996e-05 -4.17199982e-04  1.33934768e-02 -9.96347151e-05
  1.21968967e-04 -9.69646276e-04  6.21260354e-03  7.20681532e-05
 -5.13708502e-03  6.59186663e-02 -1.70615737e-02 -2.23514907e-03
 -9.11992055e-03  1.51797968e-02 -6.95947066e-02  1.07710380e-01
 -2.09662916e-01  6.50202513e-01 -1.62810324e-02  2.88655338e-02
 -4.19582061e-01]
supnorm grad right now is: 0.9538591038458339
Weights right now are: 
[ -1.80688093  -2.0609171    5.03106885  -1.96298219   2.0676323
   0.69709468  -5.33270019   1.71285824  -7.31783791  -1.63426117
   0.08954832   1.40400992  -8.81662055  -2.9501024   -0.57526943
   5.47580772   3.23791575   3.34810855   3.23394759   7.02583576
  -8.09357898  -3.2247686    0.17480647   4.49466002  10.58460964
   0.28312751  -5.10764107  15.74482086   9.6486104   -0.39267711
  -3.8788159   15.37046415   8.57597125   1.60933049   4.13904835
  18.47316463   8.59907511  -7.02816842  -4.09162917   1.66906106
  26.55583472  12.12588764  -1.77754063  24.99964025   9.02996158
  16.51390863  10.68876643  12.82676113 -10.79768952  13.70545171
  14.09471775  -3.59253399   7.11890415   2.969416    -2.14171392
   3.73019413   4.31032558  -2.40932592  -3.58448517  -2.89473098
   9.35607664   2.21480147   2.84865289  -2.45669341  25.83149235]
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2846.030741590539
gradient value of function right now is: [ 5.09572561e-01  5.58930380e-01  5.22222532e-01  5.43732606e-01
 -5.09572561e-01 -5.58930380e-01 -5.22222532e-01 -5.43732606e-01
  1.82140906e-02  1.88960964e-02  1.20653901e-02 -1.52262848e-02
  1.10124307e-02  1.13261350e-02  7.96838517e-03 -2.53127660e-02
 -4.08992277e-03 -4.27961470e-03 -2.30775643e-03  2.15751356e-01
  1.62005729e-02  1.66949529e-02  1.15139674e-02 -2.99408176e-02
 -2.10377692e-04 -7.49922166e-05  2.19352474e-04  4.04700044e-02
 -1.70657834e-04 -6.72307288e-05  3.27419494e-05  1.39098826e-02
 -1.71600155e-06  2.24654957e-07  1.10400526e-05  1.21531207e-03
 -1.32385910e-01 -3.68485483e-02 -4.01810774e-02 -3.64326098e-01
  1.15363825e-02  3.30136468e-02  2.69058185e-01  9.79900999e-03
 -2.68396603e-04 -1.69978472e-02 -1.41988412e-02 -4.56286821e-04
 -5.81245483e-05 -1.16702247e-02 -8.74429392e-03 -1.61117487e-04
 -1.18063759e-02 -4.24554713e-01  5.00885857e-02 -2.07708452e-02
  2.48487196e-02  3.82777007e-01  2.39307340e-01 -2.37343740e-01
  1.83212609e-01 -5.10753407e-01  3.23306400e-02  2.90156153e-01
  1.41447684e-01]
supnorm grad right now is: 0.5589303795461794
Weights right now are: 
[ -1.74604915  -1.99432585   5.12324251  -1.89121469   2.00680052
   0.63050343  -5.42487385   1.64109074  -6.99556277  -1.31065506
   0.31633618   1.34492301  -8.80992701  -2.91953739  -0.81071151
   5.67806432   3.20126513   3.31617926   3.00039895   7.13545743
  -7.9764108   -3.08967227   0.07123122   4.63387561   9.8547938
  -1.72320068  -6.52851491  16.21498373   8.97660795  -2.43749568
  -5.49570913  15.90935504   8.51328762   0.34309611   3.1895728
  19.58082982   8.48790982  -7.53136047  -4.71979614   1.69371968
  27.45125028  12.61668242  -1.55531219  25.79818791   9.76086098
  15.59954875  10.80260469  13.59035562 -11.66632312  12.59763028
  14.2636628   -3.81886371   8.05850485   2.39230881  -1.78920201
   4.25701736   4.67085407  -2.12281753  -3.76238075  -2.97683178
   9.5133177    2.11780998   3.38378016  -2.30938605  25.8518159 ]
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2845.3791373766094
gradient value of function right now is: [ 7.03351246e-01  7.57902425e-01  7.57573593e-01  7.51492692e-01
 -7.03351246e-01 -7.57902425e-01 -7.57573593e-01 -7.51492692e-01
 -8.50511181e-03 -8.86719802e-03 -4.34431682e-03 -7.78179783e-02
 -3.86520190e-03 -4.00686655e-03 -2.10211856e-03 -1.33471370e-02
  2.70019782e-03  2.78630112e-03  1.33291967e-03 -5.34382719e-02
 -6.22809770e-03 -6.46878766e-03 -3.32021448e-03 -3.47395334e-02
  9.69509039e-04  4.60758920e-04  5.13524898e-04 -2.85135384e-02
  7.63966099e-04  3.04223827e-04  3.48968007e-04 -1.17774203e-02
  1.99203083e-05  1.73372848e-05  1.68625581e-05 -2.59794960e-03
  1.63853126e-01  3.37579682e-02  3.41112264e-02  2.44249617e-01
  9.10907345e-04 -1.01894774e-01 -1.53599501e-01 -1.50924176e-03
  8.28859238e-05  1.93328001e-02  2.02655906e-02  2.88088725e-04
  8.08424679e-05  1.29568919e-02  1.07291666e-02  1.83400276e-04
  2.04708578e-02  1.86811654e-01  9.95426651e-02  3.22711954e-02
 -1.78506269e-02  1.23836715e-01  3.82781709e-01 -1.09691173e+00
 -4.69106315e-01  1.20048021e+00  1.58042425e-02  5.12726818e-03
 -4.96565774e-01]
supnorm grad right now is: 1.200480208578146
Weights right now are: 
[ -1.83100105  -2.03483171   5.12003907  -1.93208134   2.09175242
   0.67100929  -5.4216704    1.68195739  -7.70389067  -2.00082599
  -0.53132262   1.19971716  -9.80492889  -3.88373855  -2.07013827
   5.87968268   4.04893058   4.1442859    3.64530911   7.20397722
  -8.89558498  -3.98121338  -1.08937641   4.70895828   9.01926519
  -3.49965567  -8.35641948  15.81652099   8.34896875  -3.97156435
  -7.31332574  15.5734334    7.53500456  -1.36835152   1.32107614
  19.71021481   9.3744501   -7.08381262  -4.16041098   1.98787231
  28.07715029  12.49106023  -1.7731246   26.62783206  11.85629509
  16.95307968  11.90984965  15.88607022 -10.98829655  13.63121174
  15.63696788  -2.52366757   8.20837107   3.25758893  -1.70791298
   4.44844132   4.09878473  -2.26874829  -4.10726955  -2.93214588
   9.01270455   2.63592344   2.58303116  -2.26138697  25.84138189]
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2843.703826364394
gradient value of function right now is: [-3.64969800e+00 -3.06556478e+00 -2.95241243e+00 -3.18823235e+00
  3.64969800e+00  3.06556478e+00  2.95241243e+00  3.18823235e+00
 -2.00059759e-02 -2.04979188e-02 -8.91460974e-03 -5.71608313e-01
 -8.17372378e-03 -8.37087934e-03 -3.83713162e-03 -2.67875099e-01
  8.16100395e-03  8.49681300e-03  2.89469988e-03  7.44040315e-01
 -1.34590225e-02 -1.37810801e-02 -6.24630633e-03 -4.06385325e-01
 -3.02473878e-03 -3.06116176e-04 -1.12119300e-03 -7.43441010e-02
 -1.68157341e-03 -1.81690946e-04 -5.41211254e-04 -3.24877259e-02
 -1.83900117e-04 -1.81130555e-05 -7.93684159e-05 -5.92738477e-03
 -2.68205291e-01 -1.94050311e-02 -2.42533711e-02 -6.07648148e-01
  4.76563608e-02  8.31373178e-02  1.09903306e+00  4.88482874e-02
 -3.88751121e-04 -1.23274000e-02 -5.26575596e-03 -7.11789681e-04
 -4.05133796e-04 -1.12755836e-02 -5.69679646e-03 -7.68766138e-04
  8.31007572e-02  3.81531033e-01  7.84156772e-01  9.88681120e-02
 -3.30724167e-01  2.24647769e+00 -4.77565775e-01  1.41478451e+00
  2.00103990e-01 -7.05660229e-01 -3.19146706e-01  1.96995080e+00
  7.68451375e+00]
supnorm grad right now is: 7.684513751514605
Weights right now are: 
[ -1.85328412  -2.06876899   5.0860182   -1.95362757   2.11403549
   0.70494658  -5.38764953   1.70350362  -7.59252391  -1.88440712
  -0.51092605   1.27282627  -9.65315017  -3.71707511  -2.17670361
   6.25292311   4.14731312   4.22630691   3.72380617   7.23592452
  -8.72665011  -3.80032756  -1.13855639   5.00029017  10.20407993
  -3.37978348  -8.22091972  16.16197214   9.44921638  -3.92029854
  -7.20343588  16.04731778   9.03754133  -1.17786918   1.42647347
  20.41621741   9.10318261  -8.06863284  -5.13706064   2.1185108
  28.34581953  12.94195922  -1.4264792   26.8505528   14.44339587
  16.61695745  12.26745533  18.22275901 -10.92736526  13.49369131
  16.23114345  -1.53759361   9.23735013   2.94439444  -0.95337732
   5.29453672   4.38637609  -2.22545754  -3.72279793  -3.29113327
   9.45745331   2.33032365   2.98840654  -2.3818033   26.05450588]
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2845.242603006189
gradient value of function right now is: [-1.85972497e+00 -1.95952540e+00 -2.00149044e+00 -1.95985001e+00
  1.85972497e+00  1.95952540e+00  2.00149044e+00  1.95985001e+00
  1.44201216e-02  1.44783480e-02  7.29819416e-03  1.61059424e-01
  6.94236608e-03  6.95703240e-03  4.65380594e-03 -3.73314207e-03
 -5.14411762e-03 -5.16947393e-03 -1.69342343e-03  7.00856740e-02
  1.06709140e-02  1.06969922e-02  6.76994645e-03  2.87390025e-02
  6.46129236e-04  3.17918249e-04  6.18367740e-04  3.44291462e-02
  2.92296130e-04  1.54017609e-04  2.80179297e-04  1.24614651e-02
  1.35656567e-05  5.50345307e-06  6.61870557e-05  1.51898981e-03
 -6.72634838e-02  2.87933909e-02  2.89566329e-02 -3.76893665e-01
  1.46624621e-02  2.74821447e-02  5.46852736e-01  2.27823301e-02
 -1.95919960e-05  1.00644219e-02  8.58022154e-03  1.05379272e-05
  1.55733344e-04  8.69744581e-03  4.88475415e-03  2.05444312e-04
 -1.32215127e-02 -2.78237947e-01  9.06751516e-02 -2.08772312e-02
 -3.99854681e-02  5.13838336e-01  2.76045202e-02  5.22144648e-01
 -1.38718796e-01  2.55347059e-01 -7.32588597e-02  7.81337462e-01
  2.78257399e+00]
supnorm grad right now is: 2.7825739935188913
Weights right now are: 
[ -1.79731573  -2.11383471   5.04845717  -1.96168084   2.0580671
   0.7500123   -5.3500885    1.71155689  -6.76357865  -1.0681604
   0.47571766   1.53696978  -8.61575851  -2.69064081  -1.51342621
   7.25752343   4.2305035    4.29397673   3.56616677   6.92354758
  -7.64919029  -2.73532145  -0.30011149   5.83756426  10.64448836
  -4.3091272   -9.27130497  17.03825544   9.96673997  -4.57487012
  -8.05209219  17.00484865   9.83320454  -1.90971187   0.21507415
  21.93780717   9.48484693  -7.83088138  -4.77599196   1.95163117
  29.13674012  13.54473484  -1.18231168  27.87939704  14.70978583
  17.75420928  12.8212836   18.91703699 -13.18503179  14.3461131
  16.84025228  -2.25538058   7.99025399   2.46925604  -1.72461605
   4.04712148   3.94672939  -2.45731911  -4.11192476  -2.84890104
   9.49476035   2.35665473   2.35053005  -2.2991752   26.01965283]
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2844.145761260951
gradient value of function right now is: [-2.67303309e+00 -2.58830940e+00 -2.66338921e+00 -2.62750332e+00
  2.67303309e+00  2.58830940e+00  2.66338921e+00  2.62750332e+00
  2.48117734e-02  2.50492178e-02  1.29615511e-02  2.36336132e-02
  1.15741920e-02  1.16536739e-02  7.65812856e-03 -5.75950266e-02
 -5.15054159e-03 -5.23003258e-03 -1.80942173e-03  3.22687887e-01
  1.76366914e-02  1.77710656e-02  1.11506053e-02 -5.86959821e-02
 -1.86952463e-03 -1.17360628e-03 -4.63719136e-04  5.10297392e-02
 -1.12227788e-03 -6.95472930e-04 -3.95223579e-04  1.79181400e-02
  1.93894143e-05  5.14078990e-06  3.67594587e-05  3.25976646e-03
 -3.29263118e-01 -1.51352763e-01 -1.33125150e-01 -4.04369404e-01
  2.11975174e-03  3.36070296e-02  1.84287846e-01  1.94327233e-03
 -4.91548394e-04 -1.84946245e-02 -6.82863870e-02 -6.60598149e-04
 -2.36083931e-04 -8.98504043e-03 -3.92891643e-02 -3.14444525e-04
 -4.81753227e-03 -5.86487815e-01  2.12783240e-02 -1.30530412e-02
  1.57507104e-02  7.67227896e-02  3.60075417e-01 -4.22326156e-01
  1.05557829e+00 -3.74955391e+00  3.64975071e-02  4.75625808e-02
  2.25855778e-01]
supnorm grad right now is: 3.749553909268448
Weights right now are: 
[ -1.85767826  -2.16198657   5.03461337  -1.99376871   2.11842963
   0.79816416  -5.33624471   1.74364476  -6.80350236  -1.09719948
   0.56102506   1.25850414  -9.07053049  -3.13086186  -2.59158484
   7.80637426   5.57817222   5.6085243    4.53949844   7.06484506
  -7.87145594  -2.94314092  -0.99366451   6.18245453  10.5961518
  -6.14646125 -10.94339265  16.88218063   9.95319331  -6.13303738
  -9.66452097  16.79199165  10.1668797   -2.92612071  -2.62663705
  22.07545614   9.5496337   -8.72197278  -5.67623959   1.81800931
  29.61579193  13.84440083  -1.85553975  28.01523038  15.41694146
  16.66090626  12.23527686  19.6361746  -14.28189249  13.20562841
  16.2483093   -2.78815416   9.51372193   2.51436441  -0.83432259
   5.26092212   4.87673367  -2.32934941  -4.39931065  -3.23828593
  10.36693379   1.79144825   3.23077332  -2.68219624  25.81964251]
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -2843.4651409285684
gradient value of function right now is: [-2.97168489e+00 -1.84661361e+00 -1.87081357e+00 -2.02724467e+00
  2.97168489e+00  1.84661361e+00  1.87081357e+00  2.02724467e+00
 -3.37990079e-02 -3.46784725e-02 -1.28688226e-02 -9.39213988e-01
 -7.55782553e-03 -7.73350742e-03 -3.28301916e-03 -2.58415388e-01
  7.02552442e-03  7.31681307e-03  1.83923893e-03  8.82095351e-01
 -1.46270955e-02 -1.49783210e-02 -6.07365564e-03 -4.42746246e-01
 -2.19905810e-03 -1.41898369e-04 -6.21076562e-04 -8.32305624e-02
 -1.24480196e-03 -7.05600030e-05 -2.63267791e-04 -3.22924639e-02
 -6.97678474e-05 -6.60605845e-06 -2.51046416e-05 -4.11922373e-03
 -2.34810704e-01 -5.11025312e-05 -8.37553796e-04 -6.14074386e-01
  2.66546833e-02  1.64672021e-01  9.66235952e-01  2.81907768e-02
 -7.28594584e-05 -5.69103906e-03 -5.11323543e-03 -1.46507751e-04
 -6.06604620e-05 -6.83325834e-03 -4.07243008e-03 -1.40083007e-04
  3.50427823e-02  6.36580832e-01  4.42445428e-01  5.29498436e-02
 -2.29846609e-01  1.45230572e+00 -1.27459688e+00  4.46243696e+00
  2.80815985e-01 -1.11157121e+00 -2.60177217e-01  1.43898592e+00
  7.95288857e-03]
supnorm grad right now is: 4.462436958379556
Weights right now are: 
[ -1.93739731  -2.12924577   5.09627137  -1.96400563   2.19814868
   0.76542336  -5.3979027    1.71388168  -7.15674005  -1.43936029
   0.2676359    0.91110282  -9.37535503  -3.42500252  -3.25770683
   8.02785623   6.69619078   6.69396418   5.21460745   7.52149978
  -8.14242467  -3.20323139  -1.54842624   6.23463822  10.29086495
  -7.23789179 -12.06062635  16.4880635    9.87221138  -7.39483711
 -10.83431446  16.34204818  10.66827892  -3.4895552   -4.1283788
  22.28308828  10.12966798  -8.95760059  -5.99390214   1.34416812
  30.85745514  14.51407035  -1.431852    29.17677935  17.1344558
  16.94858099  13.46413043  21.54378477 -15.02827449  12.87241245
  17.33023644  -2.87889404   9.97346631   2.9345915   -1.02532744
   6.1541719    4.11067097  -2.17101004  -5.21927508  -2.55606115
  10.18848346   2.22181491   2.22998236  -2.3657301   25.47551637]
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -2845.23924393384
gradient value of function right now is: [ 3.33848211e+00  2.34576398e+00  2.28625193e+00  2.44780983e+00
 -3.33848211e+00 -2.34576398e+00 -2.28625193e+00 -2.44780983e+00
  8.38602858e-02  8.89796305e-02  4.57748098e-02  8.69555213e-01
  2.90107458e-02  3.00193428e-02  2.01546857e-02  1.79308692e-01
 -7.21266675e-03 -8.09301750e-03 -1.98383282e-03 -5.29058189e-01
  4.70506352e-02  4.88331761e-02  3.20461330e-02  3.10430101e-01
  1.14706957e-03  5.02875955e-04  9.84217787e-04  1.01998510e-01
  7.17386165e-04  3.12366688e-04  4.91218161e-04  2.82706755e-02
  9.60446757e-06  2.76204156e-06  2.06869920e-05  2.35505083e-03
  2.01971195e-01  6.44167281e-02  6.43713340e-02  3.17193157e-01
 -8.14325081e-03 -8.71039439e-02 -3.90512984e-01 -1.94790702e-02
  6.52508475e-05  1.96155605e-02  2.48844615e-02  6.08297163e-04
  4.88050202e-04  1.64502705e-02  1.54078789e-02  1.12639743e-03
 -4.42114516e-02 -1.05592247e+00 -2.29122838e-01 -1.32612948e-01
  1.64582341e-01 -5.30076424e-01  7.60698218e-01 -1.67760514e+00
 -6.16171215e-01  1.86843521e+00  3.94194525e-01 -1.01580258e+00
 -4.00824959e+00]
supnorm grad right now is: 4.008249591974668
Weights right now are: 
[ -1.73359904  -2.04000465   5.21765296  -1.83515014   1.99435042
   0.67618224  -5.5192843    1.58502619  -5.96378838  -0.25574633
   1.47967187   1.11186536  -8.52654709  -2.5486582   -3.26910013
   8.87622973   7.93633744   7.97863027   5.72197893   7.5490444
  -7.10253696  -2.1383907   -1.33838198   7.06890991  11.47388724
  -6.38020786 -11.11797063  17.28480423  11.16439422  -6.38183398
  -9.88081046  16.71739859  11.13375529  -3.26829578  -4.37571973
  23.82064615  10.92196739  -8.76244101  -5.48998122   1.22024967
  31.9606022   14.32256765  -1.40610259  30.84875981  17.9888083
  18.6645465   14.0494767   23.42740749 -17.03480464  14.56158362
  18.11309067  -2.71583176   8.40486536   2.50064556  -1.16349241
   4.03851356   4.51008507  -2.52440705  -4.57274956  -3.05900025
  10.08673186   2.30249464   1.80771284  -2.35576175  25.81032881]
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2844.0984690060673
gradient value of function right now is: [-1.90840635e+00 -2.66236357e+00 -2.65684376e+00 -2.64590166e+00
  1.90840635e+00  2.66236357e+00  2.65684376e+00  2.64590166e+00
  2.15575446e-02  2.23659803e-02  1.09540468e-03  9.41842550e-01
 -1.18080961e-03 -1.16238099e-03 -1.30575982e-03 -1.77762307e-03
 -1.10281163e-02 -1.12854823e-02 -1.67263947e-03 -2.97317938e-01
  2.45904247e-04  3.34613023e-04 -1.47812811e-03  8.04066108e-02
  1.80765142e-03  1.77834849e-03  4.61670002e-03  7.15781919e-02
  4.89330035e-04  8.10858393e-04  2.28139899e-03  3.24543307e-02
 -3.76153865e-06  1.63697189e-05  5.21228989e-05  1.77975372e-03
  1.22401369e-03  1.69486769e-02  2.20150932e-02 -5.05632627e-02
  8.35857902e-03 -1.58939264e-02  1.54517780e-01  7.04089459e-03
  9.30431389e-05  8.52149225e-03  2.46731296e-03  4.78116280e-04
  1.81447881e-04  2.14719823e-02  2.79492345e-03  1.22079441e-03
  4.63747583e-02 -6.53406738e-01  3.03779614e-01  2.28974632e-02
 -2.66275300e-02  6.29660483e-01  7.09835428e-01 -9.27247209e-01
 -7.12458361e-02  1.32211282e-01  1.28943446e-01  2.34730121e-01
 -1.16481867e+00]
supnorm grad right now is: 2.662363571622298
Weights right now are: 
[ -1.69037384  -2.31927258   5.03598159  -2.06639333   1.95112521
   0.95545017  -5.33761293   1.81626938  -6.70094175  -0.96810245
   0.35066928   1.8139366   -9.19130666  -3.1862105   -4.50661642
   9.62766931   8.6803095    8.69165959   6.57701622   6.94625774
  -7.88718993  -2.88815331  -2.80041163   7.89786356  12.49197938
  -6.28547857 -11.59699612  17.25117171  12.10992091  -6.33469584
 -10.1929099   17.04080908  11.91627462  -3.32646229  -5.64067607
  25.07541775  10.87058868  -8.9956426   -5.61734014   1.23975256
  32.4246529   14.74335294  -1.85878761  31.38014095  18.43733103
  18.39922889  13.83908599  24.08270879 -21.73659515  14.14591476
  18.11127128  -5.20892314  10.37376274   2.64418801  -0.31244717
   5.73911155   3.76825653  -1.98778414  -4.60703407  -3.17806988
  10.38176011   2.06702656   1.10860829  -2.25075982  25.80564607]
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2845.8888175928732
gradient value of function right now is: [ 1.50513722e+00  8.08416660e-01  8.35416052e-01  8.27907154e-01
 -1.50513722e+00 -8.08416660e-01 -8.35416052e-01 -8.27907154e-01
  1.88873264e-02  2.20488188e-02  7.66729558e-04  7.40311820e-01
  2.08522239e-04  2.20298257e-04  6.74347402e-04  1.39706731e-02
 -6.55234980e-03 -7.56638268e-03 -7.30535489e-04 -2.99644933e-01
  7.68598444e-04  9.04758483e-04  8.51147550e-04  5.37058651e-02
  1.63805872e-03  9.04900435e-04  2.39459449e-03  5.14749871e-02
  9.63433536e-04  5.07720292e-04  1.26310068e-03  2.60711219e-02
  1.19779540e-05  4.43010097e-06  3.08987273e-05  3.38099648e-04
  2.19562284e-02  9.02481446e-03  8.28787503e-03  1.04761590e-01
  1.27953902e-03 -2.43189720e-02  1.13075662e-02 -2.89129242e-03
  2.89232824e-05  1.35463301e-03  3.15647257e-03  4.74009796e-05
  2.31653887e-04  1.14630560e-02  3.26568975e-03  5.75546455e-04
 -3.94343262e-02 -2.27577981e-01 -2.63434159e-01 -3.97174184e-02
  8.01034411e-02 -2.17937389e-01  3.83211077e-01 -1.01996899e+00
 -1.32701291e-01  4.89266387e-01  8.49911041e-02 -2.56713452e-01
 -3.72008839e+00]
supnorm grad right now is: 3.720088386279815
Weights right now are: 
[ -1.65839312  -2.19566113   5.27153945  -1.8920728    1.9191445
   0.83183872  -5.57317079   1.64194885  -6.63315316  -0.93817562
   0.77684487   1.26323126  -8.88916002  -2.86722822  -5.00604124
  11.09580027  10.96270238  10.93838574   8.30147952   6.77928824
  -7.61338946  -2.602511    -3.32306171   8.89654795  12.62646172
  -7.4285482  -12.90027588  16.85824895  12.2826721   -8.47680623
 -12.40160743  16.26885605  12.43721256  -3.77449568  -6.76503185
  26.32262339  10.8216502   -9.93931911  -6.8464796    0.80409846
  34.27173253  14.95743344  -1.16836358  33.09821188  18.64810184
  17.01666146  13.96650257  23.91395732 -21.80557451  12.18103486
  18.34139881  -5.69323251   9.33697748   3.1890866   -1.07260113
   4.49181052   4.73613647  -2.0109578   -5.35887181  -3.08691175
  10.56909547   2.16328143   1.58333881  -2.48121974  25.70693294]
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2870.488615844679
gradient value of function right now is: [ 5.91523942e-01 -5.45200025e-01 -1.22544812e+00 -7.13422346e-01
 -5.91523942e-01  5.45200025e-01  1.22544812e+00  7.13422346e-01
  5.48786119e-01  6.52016066e-01  2.78542035e-01  1.89783099e+00
  2.64913356e-01  2.82723649e-01  1.94913365e-01 -3.88809724e-02
 -6.90143138e-02 -1.18634983e-01 -6.73884953e-03 -1.07291638e+00
  2.34331610e-01  2.52588499e-01  1.67827201e-01 -1.13823093e-01
  3.01464431e-03  7.36285915e-04  4.99415132e-03  5.15651219e-01
  4.93190872e-03  1.43860533e-03  6.35271159e-03  3.87743359e-01
  6.69718192e-05  1.01485950e-05  1.36202097e-04  2.52001022e-02
  1.77011851e+00  7.92519809e-03  6.44008281e-03  5.09091632e+00
 -1.99325711e-01 -7.75162149e-01 -1.25827834e+01 -4.09756839e-01
  1.31890957e-05  1.08624667e-02  1.15768483e-02  1.93472970e-04
  3.20352172e-04  5.59957296e-02  1.11439943e-02  2.17143247e-03
 -2.85141959e-01 -5.21095723e+00 -4.60874927e+00 -7.87198277e-01
  1.45120630e+00 -1.54843024e+01  6.00871169e+00 -3.21789357e+01
 -6.99146243e-01  4.18358157e+00  3.25004221e+00 -2.91113324e+01
 -1.45957645e+01]
supnorm grad right now is: 32.17893570063756
Weights right now are: 
[ -1.23486885  -4.51712043   4.44413375  -3.84397879   1.49562023
   3.15329802  -4.74576509   3.59385484  -5.28467367   0.3756576
   2.35762223   2.86097053  -8.95020656  -2.841003    -5.98903723
  15.82589637  13.98196557  14.09354256  10.02827675   3.01773239
  -7.103879    -2.00556312  -3.80585208  13.46785415  13.34830429
  -9.28865866 -15.06622526  16.65455803  13.38941543 -10.93788481
 -15.39573348  14.22383434  11.89133079  -4.53731588 -10.38444743
  28.24669705  10.80704481  -9.12044741  -5.37636527   0.68524715
  35.0127965   15.5108369   -0.52822423  34.86467147  20.92997837
  18.5749295   14.85775997  27.35158839 -19.83324607  11.94123938
  19.95507186  -3.3874056    8.8288343    3.42957094  -0.3543479
   3.92239568   3.70660791  -1.99264791  -5.89741315  -2.12910005
  10.63747393   1.92522218   0.54337462  -1.82679215  26.43485316]
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2871.258693994182
gradient value of function right now is: [-3.26312960e+00  5.04892224e-02 -6.34517337e-01  6.41951827e-02
  3.26312960e+00 -5.04892224e-02  6.34517337e-01 -6.41951827e-02
 -6.58121144e-01 -9.11102664e-01 -2.51062846e-01 -2.98397467e+00
 -2.45825864e-01 -2.81478095e-01 -1.59057827e-01 -3.49131797e-01
  7.81810720e-02  1.99460194e-01  2.42446061e-03  2.56547457e+00
 -1.88835234e-01 -2.24780921e-01 -1.14385096e-01 -3.32617911e-01
 -3.89332751e-02 -1.04761656e-02 -4.35897449e-02 -7.14384608e-01
 -1.19476582e-01 -3.96693662e-02 -9.15771877e-02 -8.86359590e-01
 -1.21784879e-04 -7.52819656e-06 -2.97469892e-04 -1.87792417e-02
 -3.03433793e-01  5.44606991e-02  7.16227556e-02 -4.10409394e-01
  4.33333243e-02  3.78982083e-01  2.15435895e+00  9.80009319e-02
 -4.99792457e-04 -3.91923978e-02 -2.86682752e-02 -3.45172443e-03
 -6.17808327e-03 -3.77139947e-01 -7.27954427e-02 -3.19526195e-02
  4.50553268e-01  4.44149746e+00  5.56856711e+00  1.04584307e+00
 -7.96405008e-01  6.45323933e+00 -4.90226095e+00  1.90768032e+01
  9.99537966e-01 -3.53318016e+00 -1.45726092e+00  9.62215895e+00
  1.44144291e+01]
supnorm grad right now is: 19.076803234066762
Weights right now are: 
[-1.38553743e+00 -5.29308762e+00  4.84709168e+00 -4.45004151e+00
  1.64628880e+00  3.92926520e+00 -5.14872302e+00  4.19991756e+00
 -5.50513935e+00 -3.29987788e-02  2.47765303e+00  2.58858494e+00
 -8.96970872e+00 -2.76035303e+00 -6.57135416e+00  1.66277781e+01
  1.58986643e+01  1.63347945e+01  1.09123524e+01  2.74593318e+00
 -7.17765127e+00 -1.99913475e+00 -4.40451907e+00  1.43865982e+01
  1.31096339e+01 -1.17445074e+01 -1.72018212e+01  1.58229329e+01
  1.26242905e+01 -1.31532994e+01 -1.79352589e+01  1.21974623e+01
  1.38039070e+01 -4.97158540e+00 -9.93180827e+00  3.09241992e+01
  1.11623284e+01 -7.47512789e+00 -1.06222187e+00  4.67423368e-01
  3.55856347e+01  1.48603762e+01 -3.48955529e-01  3.55436218e+01
  2.36592650e+01  1.92350166e+01  1.54535593e+01  2.98253416e+01
 -1.33897659e+01  1.03094241e+01  2.02773000e+01  9.82725301e-01
  7.29785737e+00  4.06176109e+00  3.17039841e-01  2.83238621e+00
  3.01484502e+00 -2.13956457e+00 -5.73331878e+00 -2.28039649e+00
  1.08833040e+01  1.64675197e+00  3.64458881e-01 -1.91477643e+00
  2.66748775e+01]
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2874.792914726016
gradient value of function right now is: [-7.60111419e-01 -1.54549744e-01 -7.02559739e-01 -1.89677564e-01
  7.60111419e-01  1.54549744e-01  7.02559739e-01  1.89677564e-01
  4.12961584e-02 -7.94598657e-02 -5.39336224e-03 -2.28838270e-01
 -3.01213906e-02 -2.82985567e-02 -1.93643903e-02 -1.87843696e-01
  1.93665189e-03  6.98757199e-03 -1.07109452e-04  7.79073044e-01
 -1.21011739e-02 -9.49648792e-03 -7.13385800e-03 -1.90837844e-01
 -1.18835952e-02 -3.74173252e-03 -9.98760371e-03  4.29333958e-03
  2.10990965e-01  1.45919999e-01  6.45490227e-02  4.94079686e-02
 -1.01971492e-05 -8.90942692e-07  5.20668431e-05 -7.39533107e-03
  2.33671571e-01 -3.84122152e-01 -3.73863204e-01  1.07426489e+00
 -5.94134975e-02 -5.60722119e-02 -4.79409500e+00 -1.94521764e-01
 -9.86571093e-04 -6.24296185e-02 -9.70997054e-02 -1.28402467e-02
  1.44160847e-03 -1.45566058e-01  2.26342046e-02 -2.42162031e-02
  6.19937614e-02  5.32949591e-02  8.25341003e-01  1.27766491e-01
  2.40188374e-01 -3.51869871e+00  1.02371990e+00 -6.35474105e+00
  5.22217736e-01 -1.07174116e+00  1.05324594e+00 -1.32145763e+01
 -4.04262233e-01]
supnorm grad right now is: 13.214576332310386
Weights right now are: 
[ -1.19212403  -5.82745993   5.14630497  -4.84736417   1.4528754
   4.46363752  -5.44793631   4.59724022  -6.31751373   0.16464794
   1.93137439   2.64564896  -8.87204373  -1.86320318  -6.90638291
  17.26721526  16.15626464  17.0117142   10.94527818   3.11018785
  -7.24559793  -1.00709754  -4.8328305   15.03356575  15.62774971
  -9.37182806 -14.76553935  16.19450393  12.57220472 -13.16656464
 -17.44562938  11.48216815  16.31519744  -5.03943873  -7.09611016
  33.33586719  10.73975998  -9.61569787  -2.61366142  -0.14654272
  35.70435558  14.42211469  -0.34421402  35.74371729  30.60859301
  19.834081    16.72754157  36.50761971 -11.45626595  11.09811689
  21.96825689   6.05330088   6.89185583   4.32055757   0.38952103
   2.32790466   2.46268451  -2.41030092  -5.15011844  -2.48630293
  10.81653095   1.30805501  -0.09649194  -1.93648464  26.69729991]
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.4076266465263
gradient value of function right now is: [ 5.41802134e-02 -8.73993000e-02 -1.00432277e-01 -9.97607340e-02
 -5.41802134e-02  8.73993000e-02  1.00432277e-01  9.97607340e-02
  4.22876856e-03 -4.29633393e-02 -1.54383586e-02  2.95342905e-01
 -2.57133730e-02 -3.50682489e-02 -1.57043119e-02 -1.03761191e-01
 -1.45327771e-02 -6.53629002e-02 -1.11003555e-04 -5.25726843e-01
 -1.22295538e-02 -2.03243718e-02 -6.96154987e-03 -8.81564691e-02
  4.47032768e-03  9.95555683e-04  2.50907694e-03  9.01412254e-02
  8.73869736e-02  2.98585619e-02  4.95220281e-02  3.44978764e-01
  3.54998813e-06  5.84215656e-07 -3.28921716e-07 -4.42847408e-03
  3.58988253e-01 -2.52398180e-02 -6.39138631e-02  9.11612616e-01
 -3.84725999e-02 -1.13415965e-01 -2.45067920e+00 -1.00506696e-01
 -5.18021056e-04  4.07892972e-05 -2.05874628e-02 -3.00534002e-03
 -1.04638413e-04  6.31344994e-02  9.19035184e-03  4.89834323e-04
 -1.94183187e-02 -3.94089898e-01 -6.15071899e-01 -4.99302836e-02
  2.97851346e-01 -2.81572436e+00  2.67875258e-01 -3.70989769e+00
  9.13840306e-03  1.86496446e-01  6.69035245e-01 -6.66852160e+00
 -2.95977076e+00]
supnorm grad right now is: 6.668521599037164
Weights right now are: 
[ -1.26202271  -6.55200517   5.35613031  -5.39942568   1.52277409
   5.18818276  -5.65776165   5.14930173  -7.20430202   1.062749
   2.07132162   2.53056572  -9.32397408  -0.44403418  -8.0976463
  17.39196018  16.97257223  16.92497899  11.94547731   2.68868088
  -7.7460755    0.66097824  -6.06876982  15.10796329  17.48498155
  -8.59808993 -12.83541469  15.33460847  10.60386806 -15.35606011
 -17.47727494  11.30118286  17.23756501  -4.94572357  -7.01936514
  36.1146356   11.0289852   -9.56982838  -2.94141276  -0.07977808
  36.11601118  13.58753115  -0.16401564  35.68082806  36.27557254
  20.13570334  18.95459228  40.47379183  -8.13101625  14.59122357
  21.8447281   11.03716958   6.23997995   4.21651795   0.27869737
   1.99129437   2.46187907  -2.280254    -5.41356086  -2.27562974
  10.5717433    1.26451931   0.19195636  -1.97122407  26.57545186]
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2873.054654306259
gradient value of function right now is: [ 1.74580857e+00 -2.28719080e-01 -1.78685979e-01 -2.61243271e-01
 -1.74580857e+00  2.28719080e-01  1.78685979e-01  2.61243271e-01
  3.64642763e-01  5.11036293e-01  7.14419990e-02  1.55592962e+00
  5.62212998e-02  7.52170743e-02  1.65072860e-02 -9.87387576e-02
 -3.55838770e-02 -1.25520952e-01 -2.51456553e-04 -1.65277334e+00
  5.64822119e-02  6.98167324e-02  1.63081280e-02 -8.07947838e-02
  9.31091611e-03  3.66622626e-04  6.26179266e-03  4.49383364e-01
  2.43859413e-02 -9.66172404e-03 -1.92132733e-02  5.42754421e-01
  2.14150642e-05  1.14779117e-06  2.82452135e-05  1.14892790e-02
  1.23003092e+00 -4.55813443e-02 -1.02765954e-02  2.95377376e+00
 -9.83463784e-02 -4.26758257e-01 -4.76138525e+00 -2.42373370e-01
  1.99757995e-04  1.64199922e-02 -7.27754347e-03  1.02094871e-03
  7.56581705e-04  4.29683644e-02  1.33475630e-02  1.55583037e-03
 -3.05402005e-01 -2.43062239e+00 -1.90127157e+00 -6.51799341e-01
  1.06115660e+00 -8.27375294e+00  4.63596010e+00 -1.85870627e+01
  1.55560031e-01 -6.59720026e-01  2.09601812e+00 -1.65350252e+01
 -1.13065976e+01]
supnorm grad right now is: 18.58706273970168
Weights right now are: 
[-8.25940627e-01 -6.62847310e+00  5.93023036e+00 -5.21269509e+00
  1.08669200e+00  5.26465069e+00 -6.23186169e+00  4.96257114e+00
 -7.51767527e+00  1.38442910e+00  1.28816706e+00  3.17458207e+00
 -1.02837603e+01 -1.04799738e+00 -9.58700803e+00  1.71459205e+01
  1.82427155e+01  1.76364986e+01  1.26593629e+01  3.24862853e+00
 -8.94779714e+00  2.12480002e-03 -7.83043408e+00  1.51356311e+01
  1.93743154e+01 -8.88343430e+00 -1.30079644e+01  1.51408554e+01
  1.09316899e+01 -1.53053016e+01 -1.73162958e+01  1.05406747e+01
  1.74365216e+01 -5.01147988e+00 -7.26842271e+00  3.75565325e+01
  1.08580186e+01 -9.78030142e+00 -2.69502728e+00 -4.21467639e-01
  3.62363418e+01  1.31459115e+01 -1.23047782e-01  3.56700937e+01
  4.07334339e+01  2.08588252e+01  1.99367494e+01  4.43586867e+01
 -4.59046756e+00  1.42913388e+01  2.29564829e+01  1.39953321e+01
  6.35389055e+00  4.61267035e+00 -1.35303857e-01  1.81862646e+00
  1.89518284e+00 -2.35428382e+00 -5.12786145e+00 -2.47145126e+00
  1.07197652e+01  1.05549969e+00 -6.87781952e-02 -2.05828350e+00
  2.65627045e+01]
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2871.8094227268793
gradient value of function right now is: [-2.81056766e+00  3.46216494e-01  1.21124697e-01  3.92276387e-01
  2.81056766e+00 -3.46216494e-01 -1.21124697e-01 -3.92276387e-01
 -4.89682210e-01 -9.46647015e-01 -8.81888729e-02 -3.04520604e+00
 -9.39256664e-02 -8.66342144e-02 -4.48106522e-02  1.02720036e-01
  2.95645051e-02  1.36023081e-01  6.64605007e-05  2.15329532e+00
 -7.53455223e-02 -6.87593787e-02 -3.32436735e-02  9.05262301e-02
 -2.40433047e-02 -3.54944786e-03 -2.16877518e-02 -5.77958496e-01
  1.66630403e-01  9.68339539e-02  2.63569420e-02 -5.37366269e-01
 -2.82314972e-06 -1.20252653e-07 -3.09302574e-05 -1.70465765e-02
 -2.19200244e+00 -3.44465682e-01 -3.54772581e-01 -3.43124581e+00
  1.10427683e-01  4.93522563e-01  7.33055006e+00  2.71703175e-01
 -2.28148088e-03 -6.61128185e-02 -1.27483467e-01 -1.26510215e-02
 -7.17758154e-03 -2.27357258e-01 -9.40951381e-03 -3.60186843e-02
  3.79470666e-01  2.78915020e+00  2.19921465e+00  8.78529342e-01
 -1.10549871e+00  9.20824592e+00 -4.08971695e+00  1.60088690e+01
  1.50068232e+00 -6.64280944e+00 -1.76805183e+00  1.66830948e+01
  1.96271821e+01]
supnorm grad right now is: 19.62718211494794
Weights right now are: 
[ -1.35984139  -6.28426184   5.90283825  -4.90404427   1.62059276
   4.92043942  -6.20446959   4.65392032  -8.52060552   0.78696881
   1.07842334   2.70112552 -10.23457282  -0.14565458 -10.04412972
  17.23607741  18.20512003  18.08704948  12.81118235   3.55332646
  -8.90367672   0.9067418   -8.24150175  15.07998803  19.47048815
 -10.01883066 -14.26748427  14.42643384  10.54729571 -14.97197164
 -17.52768686   9.95619601  17.91163621  -5.03125677  -6.5420809
  39.7743063   10.32307779 -10.19089923  -2.85964195  -0.61543998
  36.32611929  13.41566372  -0.21126208  35.7251604   42.58122312
  19.76198614  20.55023872  45.64044451  -2.95070332  12.42042318
  23.45312761  14.91133023   6.66391777   4.70238164   0.1124154
   1.93904396   1.85042121  -2.45859913  -4.92177813  -2.5218059
  10.64599477   1.36608281  -0.10045192  -2.1237491   26.90562922]
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2870.7559723012664
gradient value of function right now is: [ 2.74837194e+00 -5.17050085e-01 -6.59964169e-01 -6.03891087e-01
 -2.74837194e+00  5.17050085e-01  6.59964169e-01  6.03891087e-01
  7.79663167e-01  1.07113470e+00  1.67303644e-01  2.88004670e+00
  1.75012626e-01  1.81943719e-01  1.03038057e-01 -1.88070548e-01
 -4.32617242e-02 -1.63920590e-01 -6.19216115e-05 -2.07288674e+00
  1.38088726e-01  1.45245278e-01  7.50967313e-02 -1.92503084e-01
  1.76278245e-02  1.55232797e-03  9.63826261e-03  7.91066150e-01
  4.36496736e-02 -4.51343856e-03  2.33459678e-02  6.61614506e-01
 -1.83086055e-06  3.57147340e-08 -1.63939144e-06  2.23355449e-02
  3.22301822e+00  9.56985353e-02  8.12073829e-02  6.01917597e+00
 -1.58506709e-01 -7.64819491e-01 -1.59636827e+01 -5.13182760e-01
  2.21342807e-04  3.27674860e-02  5.83254900e-02  2.89121369e-03
  9.93982201e-04  1.20048634e-01  2.32560295e-02  1.01587075e-02
 -4.38151832e-01 -4.21698236e+00 -1.73540035e+00 -1.16970022e+00
  1.34719849e+00 -1.22659639e+01  6.74493599e+00 -2.79224256e+01
 -1.48342058e+00  7.52161063e+00  3.26500446e+00 -3.31000013e+01
 -2.13617682e+01]
supnorm grad right now is: 33.10000131020202
Weights right now are: 
[ -1.12196373  -6.75636972   5.97905732  -5.29095171   1.38271511
   5.39254731  -6.28068866   5.04082776  -9.11538274   1.86443309
   1.27358227   3.25726615  -9.61558502   0.74830192 -10.06897675
  18.0186889   19.46875627  17.80721691  13.29777421   3.47602241
  -8.42081797   1.77570873  -8.30861486  15.63451993  21.06848213
  -8.54787829 -13.75465139  13.80580644  10.08030915 -15.67746289
 -17.38138871  10.02202443  18.28285004  -5.02758887  -4.92773842
  42.76834808  10.32633864 -10.30730525  -2.87378076  -0.65470132
  36.57240574  12.99064737  -0.36821665  35.72240786  46.5603091
  20.18571604  21.31181163  48.75759108   2.61647844  13.52630625
  24.55642469  20.12364745   5.44057382   4.59709037  -0.12454518
   1.03932022   1.70086619  -2.57468085  -5.10949274  -2.41769411
  10.39989709   1.54849968  -0.34601746  -2.08587283  26.37395402]
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2874.5527647464696
gradient value of function right now is: [-1.52156397e+00  3.26263211e-01  9.01185931e-01  4.13940593e-01
  1.52156397e+00 -3.26263211e-01 -9.01185931e-01 -4.13940593e-01
 -5.37743887e-01 -6.66072064e-01 -1.03119120e-01 -1.65991956e+00
 -1.48903608e-01 -1.44882951e-01 -8.33926151e-02  1.77145882e-01
  2.84120731e-02  3.49364834e-02  2.06487485e-05  1.06194810e-01
 -1.04584221e-01 -9.92309273e-02 -5.45669287e-02  2.37291102e-01
 -1.82665192e-02  3.87004825e-06 -6.41576014e-03 -5.72136876e-01
  1.61237152e-01  4.95295940e-02  5.50605164e-02  3.62068420e-02
  3.23400163e-06  4.83947289e-08  1.63593243e-06 -1.95979798e-02
 -2.36837952e+00 -2.36859233e-01 -2.97539880e-01 -3.81597187e+00
  4.66544932e-02  9.82666256e-01  4.46499133e+00  2.03590612e-01
 -6.50478380e-04 -7.41144432e-02 -1.17456455e-01 -6.70702575e-03
 -8.31401401e-04 -4.45785279e-02 -1.47986833e-02 -6.02917076e-03
  2.00382810e-01  2.58428691e+00  9.17782947e-01  5.88684933e-01
 -7.74433444e-01  4.21801970e+00 -7.46906508e+00  2.86010652e+01
  1.88370072e+00 -8.12772972e+00 -2.51403463e+00  1.39388191e+01
  1.48142433e+01]
supnorm grad right now is: 28.601065165746114
Weights right now are: 
[ -1.22771112  -6.60452236   5.9593935   -5.12854248   1.48846249
   5.24069995  -6.26102483   4.87841853  -9.18690851   1.89967179
   1.92020842   2.68720696  -9.77979137   0.92272141 -10.91429531
  18.11838649  20.6641168   17.64756417  14.35992334   3.56374991
  -8.47236056   2.16861364  -9.09238079  15.66897735  21.30466755
 -10.35257237 -16.43304032  12.5644138   10.33543664 -15.62766097
 -16.91560819  10.35864803  18.78785681  -5.02190903  -4.36727466
  44.08394792  10.0595674  -10.4444968   -2.84201681  -0.55467092
  36.77413659  12.53870883  -0.3142057   35.75030889  50.29605214
  21.02056228  21.87300274  51.82301385   4.27248333  13.06261837
  25.43299321  21.70021123   5.89319216   4.7743352    0.06403953
   0.91927205   2.0306961   -2.47743594  -5.24983504  -2.33355854
  10.3060327    1.50017439  -0.27500465  -2.10022484  26.7763535 ]
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2872.5992844902567
gradient value of function right now is: [-2.69502822e+00  5.79500363e-01  8.85562750e-01  6.90951359e-01
  2.69502822e+00 -5.79500363e-01 -8.85562750e-01 -6.90951359e-01
 -6.50249752e-01 -9.18340513e-01 -7.19470102e-02 -3.17674388e+00
 -1.02046466e-01 -8.63264198e-02 -4.92146020e-02  3.46646905e-01
  5.34351741e-02  1.37359058e-01  2.01999987e-05  1.60800549e+00
 -7.51845220e-02 -6.31690717e-02 -3.27661458e-02  3.51688435e-01
 -4.92885997e-02 -2.90448169e-03 -2.16278207e-02 -1.01222604e+00
  6.28129115e-02  3.00662820e-02  3.56759017e-03 -4.37855695e-01
  2.59103436e-06  3.72217255e-08  2.30500038e-06 -1.46042625e-02
 -3.42379766e+00 -1.73999456e-01 -2.34840009e-01 -5.82120097e+00
  1.39843599e-01  1.14194644e+00  1.16137495e+01  4.80963733e-01
 -9.14408434e-04 -5.54459019e-02 -7.93570651e-02 -8.40623874e-03
 -2.29528799e-03 -1.73575501e-01 -1.01431874e-02 -2.00110838e-02
  3.02766223e-01  3.46549910e+00  1.88348430e+00  9.30300681e-01
 -1.35606915e+00  1.16933925e+01 -8.08497548e+00  3.62127510e+01
  1.24387483e+00 -6.44976750e+00 -3.72625454e+00  3.17373889e+01
  1.75201313e+01]
supnorm grad right now is: 36.21275103143201
Weights right now are: 
[ -1.43237116  -6.45127921   5.95729095  -4.98272038   1.69312253
   5.08745679  -6.25892229   4.73259643 -10.07322634   1.68251892
   1.7303092    2.66224042  -9.95674454   0.9359095  -11.62603719
  18.05132043  20.25038552  17.16943917  14.44459791   3.6746754
  -8.69070087   2.41899247  -9.79971592  15.54417062  22.76104955
  -8.37748281 -16.18124645  12.2006659    9.17859227 -16.66748662
 -17.51586651  10.11163881  19.23996042  -5.01759335  -3.91545054
  45.34377962  10.05822633  -9.55324456  -1.90522493  -0.59471401
  37.09741175  12.37276129  -0.20424076  35.89526395  53.31142736
  21.12342891  22.73460948  53.92899846   7.41139774  13.20057021
  25.02199183  24.16269013   6.76167722   4.7884895    0.20946297
   0.6173616    2.00352129  -2.38597865  -5.38213832  -2.3163466
  10.7149396    1.30150428  -0.47860149  -2.00667294  26.52054569]
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2875.496973196237
gradient value of function right now is: [-2.31749699e+00  2.51897436e-01  1.17593917e-01  2.92965768e-01
  2.31749699e+00 -2.51897436e-01 -1.17593917e-01 -2.92965768e-01
 -3.92866863e-01 -5.33650724e-01 -5.78445866e-02 -1.79332854e+00
 -9.38327726e-02 -8.71860175e-02 -5.56612880e-02  4.46383211e-02
  2.64173608e-02  7.23058491e-02  5.88764921e-06  1.33779583e+00
 -5.34690423e-02 -4.78999853e-02 -2.90311579e-02  6.38339528e-02
 -2.35705698e-02 -1.37288261e-03 -8.57964645e-03 -5.48015135e-01
  2.95161735e-02  2.53302007e-02 -4.47135726e-03 -2.44692585e-01
  1.62902971e-06  6.23128671e-10  1.72185198e-06 -1.01316974e-02
 -1.57251340e+00 -1.43617844e-01 -1.85500434e-01 -2.55290227e+00
  6.32516294e-02  4.32972161e-01  4.66494472e+00  2.17572369e-01
 -1.02879127e-03 -4.45665612e-02 -5.62506110e-02 -8.68720085e-03
 -2.02266104e-03 -9.39255714e-02 -1.72464922e-03 -1.56204270e-02
  2.19565468e-01  1.70924129e+00  1.24813253e+00  5.99040105e-01
 -7.31875099e-01  5.40991109e+00 -3.16740321e+00  1.25158442e+01
  8.34360943e-01 -3.85083709e+00 -1.56937219e+00  1.29796439e+01
  8.56541845e+00]
supnorm grad right now is: 12.979643923105051
Weights right now are: 
[ -1.27034812  -6.47003212   5.92125826  -5.02445918   1.53109949
   5.10620971  -6.2228896    4.77433523 -10.07572846   1.8633441
   2.0971703    2.59602678  -9.54579274   1.74435697 -11.58942954
  18.39655819  21.25822415  17.0443735   14.92597433   3.87562294
  -8.22405622   3.47555587  -9.73473839  15.77396167  23.81704044
  -9.06107128 -16.50873275  11.60689387   9.08053465 -16.21190162
 -16.90828604   9.97951592  19.67993497  -5.01365129  -3.43491521
  46.35411094   9.88440411  -9.80891995  -1.8368678   -0.65501133
  37.23639619  11.85318396  -0.33174803  35.82765344  56.44157346
  21.24000428  23.38453465  56.93475588  10.60270923  13.11591796
  25.82169712  27.01501637   6.30759556   5.0868694    0.13551677
   0.60822263   1.59634075  -2.46086542  -5.2270153   -2.5089051
  10.78520119   1.35774052  -0.44214418  -2.04048042  26.57096697]
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2875.153360235769
gradient value of function right now is: [-1.84745279e+00  3.39819229e-01  5.50180747e-01  4.06888444e-01
  1.84745279e+00 -3.39819229e-01 -5.50180747e-01 -4.06888444e-01
 -4.22508412e-01 -4.85665651e-01 -6.57631672e-02 -1.53322652e+00
 -1.13025981e-01 -6.75205475e-02 -7.02012369e-02  1.46997052e-01
  2.90604363e-02  6.28571555e-02  6.08849335e-06  6.44116760e-01
 -6.71721616e-02 -3.41102910e-02 -3.86464575e-02  1.81649775e-01
 -2.95558063e-02 -3.14737955e-03 -1.65368876e-02 -5.90344664e-01
  7.84082317e-02  2.07395416e-02 -1.56366335e-02 -1.47621585e-01
  1.01091833e-06  1.49656470e-08  1.08844253e-06 -1.34335447e-02
 -2.46156753e+00 -3.06754074e-01 -3.68339691e-01 -3.55157796e+00
  1.01882617e-01  4.61046418e-01  6.11958511e+00  2.70356753e-01
 -2.19260044e-03 -5.30120093e-02 -1.18271817e-01 -1.28327804e-02
 -4.22890895e-03 -1.42209441e-01 -1.35353302e-03 -2.48049892e-02
  2.33149815e-01  1.84070614e+00  5.54541693e-01  5.82883619e-01
 -9.47811995e-01  8.05297003e+00 -3.18484031e+00  1.23497590e+01
  1.84351352e+00 -1.02734857e+01 -1.58913007e+00  1.55053537e+01
  1.02381269e+01]
supnorm grad right now is: 15.505353655325642
Weights right now are: 
[ -1.213696    -6.68726781   5.97153715  -5.20958873   1.47444738
   5.3234454   -6.27316849   4.95946478 -10.65690853   2.43319145
   1.58789052   2.77282915  -9.52854296   2.23586494 -11.68192028
  18.59928796  21.28995228  16.01446103  15.2094278    3.54840429
  -8.38331207   4.17110386 -10.05370524  15.85467357  24.63751437
  -8.79015495 -17.31372874  11.19101705   8.09604677 -17.57467767
 -17.42996045   9.96305145  20.29757392  -5.00588817  -3.05202776
  46.81254684   9.71382816  -9.57339004  -1.58943041  -0.70930672
  37.60841695  11.76950355  -0.35861404  35.83153078  59.19150195
  20.73771712  23.70544428  58.6166901   14.15217848  13.27473222
  25.31950364  29.38750698   6.47396867   5.21234851   0.22636399
   0.31370972   1.81161595  -2.32285165  -5.43706792  -2.60170913
  11.18917684   1.1724152   -0.39952997  -2.05216952  26.65664274]
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2876.4069167526577
gradient value of function right now is: [ 3.83495834e-01  3.65369452e-01  1.10235388e+00  4.60113327e-01
 -3.83495834e-01 -3.65369452e-01 -1.10235388e+00 -4.60113327e-01
 -1.66787501e-01 -1.88157408e-01 -4.31669680e-02 -5.04202150e-02
 -9.70709888e-02 -8.74678579e-02 -6.03144865e-02  2.33433521e-01
 -5.41182045e-03 -3.38381720e-02  1.13182629e-06 -1.05048909e+00
 -5.95850911e-02 -4.86602967e-02 -3.54902701e-02  2.94281847e-01
 -3.06107442e-03  3.60128502e-04 -7.73436925e-04 -4.08726842e-02
  1.52754185e-01  4.74137506e-02  5.06447760e-02  2.45469471e-01
 -1.09599528e-06  1.22047069e-09 -1.01949610e-06 -1.43767094e-02
 -2.58080151e+00 -2.31290561e-01 -2.98527576e-01 -3.85047156e+00
  5.89319650e-02  4.95003174e-01  7.13379976e+00  2.36455670e-01
 -6.08580747e-04 -4.65145998e-02 -9.70347284e-02 -7.86099862e-03
 -6.00865649e-04 -1.28182280e-02 -1.53906443e-04 -4.89126266e-03
 -7.01534019e-02  8.94690170e-01 -1.73894732e+00  1.91581080e-02
 -4.22342900e-01  3.93835116e+00 -2.96897247e+00  1.09891121e+01
  1.51767898e+00 -6.96885915e+00 -1.78685734e+00  1.56817764e+01
  3.32901170e+00]
supnorm grad right now is: 15.681776429739214
Weights right now are: 
[ -0.96487879  -6.67061014   5.94049503  -5.20430036   1.22563016
   5.30678773  -6.24212636   4.95417641 -10.68199567   2.90796005
   1.18216472   3.24425242  -9.71032027   2.57475276 -12.3653398
  18.58505033  22.88489731  15.09912828  15.23042497   3.71763716
  -8.69071012   4.68626822 -10.83447042  15.75435698  22.59976263
 -10.0499529  -20.32472069  10.98588729   9.00024319 -16.24848378
 -15.97608006  10.40002447  19.80141487  -5.00566915  -3.1937139
  47.94917244   9.48635523 -10.31156004  -1.95005476  -0.73302378
  37.8716731   11.48103421  -0.28781042  35.82929571  62.9403563
  21.92255465  24.0144758   61.54730619  17.08031022  12.27589764
  26.81187111  31.34251365   6.0933376    5.1403706   -0.15206327
  -0.48469621   1.97721795  -2.46080349  -5.67337158  -2.53981599
  10.78159326   1.53298585  -0.45252887  -2.00761416  26.54461611]
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2874.8279584349334
gradient value of function right now is: [-1.42548861e+00  5.44245846e-01  1.25628359e+00  6.87285123e-01
  1.42548861e+00 -5.44245846e-01 -1.25628359e+00 -6.87285123e-01
 -6.18558441e-01 -6.72181493e-01 -1.03815615e-01 -1.84207414e+00
 -1.69286100e-01 -1.31124855e-01 -9.79415726e-02  3.70045212e-01
  4.50341560e-02  2.17063737e-02  2.80515636e-06  4.21582201e-01
 -1.03766817e-01 -6.47319002e-02 -5.67031735e-02  4.50883427e-01
 -6.33315508e-02 -3.75875490e-03 -3.12253706e-02 -8.99943615e-01
  2.87593815e-01  1.00500202e-01  8.48912754e-02  1.54525348e-01
 -2.68710336e-06  1.02668659e-08 -4.90792052e-06 -2.54094846e-02
 -4.52371482e+00 -4.21896262e-01 -5.12458644e-01 -6.60846159e+00
  1.63585249e-01  9.63367393e-01  1.11888960e+01  5.08008265e-01
 -1.66637423e-03 -6.48682332e-02 -1.62133221e-01 -1.31424944e-02
 -3.22499102e-03 -1.61570337e-01  1.28396090e-02 -2.27754809e-02
  2.50332781e-01  2.61250711e+00  6.19462485e-01  7.17996751e-01
 -1.54076994e+00  1.26241228e+01 -6.30749597e+00  2.55646677e+01
  2.37486588e+00 -1.24532553e+01 -3.78516675e+00  3.12655773e+01
  1.63511002e+01]
supnorm grad right now is: 31.26557726419711
Weights right now are: 
[-1.12222423e+00 -6.91264105e+00  5.97162467e+00 -5.37400290e+00
  1.38297560e+00  5.54881864e+00 -6.27325601e+00  5.12387895e+00
 -1.11487629e+01  3.57390118e+00  1.33338958e+00  3.39957552e+00
 -9.95556570e+00  2.91521862e+00 -1.34078173e+01  1.86512027e+01
  2.40761973e+01  1.48507922e+01  1.52436234e+01  3.91208022e+00
 -8.96007335e+00  5.17159129e+00 -1.18891267e+01  1.58360509e+01
  2.30459520e+01 -9.94019828e+00 -2.15111282e+01  1.08838057e+01
  8.18628317e+00 -1.72744977e+01 -1.61972875e+01  9.93445825e+00
  2.01813292e+01 -5.00459713e+00 -2.85985444e+00  4.96823693e+01
  9.45678187e+00 -1.05615357e+01 -1.94621303e+00 -7.62854490e-01
  3.79771018e+01  1.15307844e+01 -4.09425256e-01  3.57400928e+01
  6.57762777e+01  2.22406653e+01  2.41027563e+01  6.33919584e+01
  2.01224087e+01  1.14277179e+01  2.68164482e+01  3.34336679e+01
  6.52296734e+00  5.25623556e+00  3.39586283e-01  3.78736158e-02
  1.68410020e+00 -2.39900798e+00 -5.62462858e+00 -2.58046370e+00
  1.12229866e+01  1.42479014e+00 -5.56383007e-01 -2.06658036e+00
  2.66676732e+01]
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2875.2219094243246
gradient value of function right now is: [ 9.58804378e-01 -4.95391485e-01 -8.59633299e-01 -6.05322453e-01
 -9.58804378e-01  4.95391485e-01  8.59633299e-01  6.05322453e-01
  3.32223782e-01  3.27869971e-01  4.84635007e-02  1.31146526e+00
  8.80674882e-02  6.25493487e-02  5.52736354e-02 -3.07893573e-01
 -5.10989047e-02 -5.61854283e-02 -1.50016084e-06 -8.81636334e-01
  6.19456676e-02  4.07003380e-02  3.64454863e-02 -3.19198735e-01
  8.11279856e-02  5.04243404e-03  4.56725658e-02  8.07854927e-01
 -1.25797783e-02 -1.75801924e-02  6.59311829e-03  1.46856669e-01
 -4.24495319e-07 -7.02765001e-10 -4.29453395e-07  1.22848828e-02
  3.40041859e+00  1.74478256e-01  1.91700975e-01  5.49006959e+00
 -1.60480984e-01 -7.21089517e-01 -1.31559719e+01 -5.27748069e-01
  3.50322891e-04  2.69121405e-02  6.61155744e-02  4.92586363e-03
  1.74350193e-03  2.10114482e-01  6.67169760e-03  2.19313837e-02
 -1.56312676e-01 -1.81974539e+00 -3.10475192e-01 -5.13634800e-01
  1.16365667e+00 -1.17279096e+01  4.05143846e+00 -1.68316105e+01
 -1.47511852e+00  7.90885543e+00  3.06352653e+00 -3.15175820e+01
 -1.42559306e+01]
supnorm grad right now is: 31.51758196543488
Weights right now are: 
[ -1.14293724  -6.70790414   6.07181861  -5.23240975   1.40368861
   5.34408173  -6.37344995   4.9822858  -11.43254754   3.66127777
   1.37592593   3.12480923  -9.64046676   3.48489361 -13.69940519
  18.87067129  24.77770812  13.78630384  15.3257934    3.5824713
  -8.70257737   5.75130129 -12.29534872  15.79567188  23.53429378
  -9.6840837  -21.50991626  10.28935626   8.09707048 -17.63152522
 -15.43048319  10.5874305   20.10995892  -5.00381696  -2.76275406
  51.29214193   9.4736099   -9.69941842  -0.83675701  -0.55274023
  38.1064288   10.88223011  -0.42847834  35.63524404  68.24929892
  24.02567225  24.53542742  65.6418791   22.19622096  11.61153662
  26.77628135  35.38396323   6.40614821   4.88516705   0.15628755
  -0.36315817   1.69044848  -2.47233422  -5.51948151  -2.66976336
  10.99492174   1.49588144  -0.6587449   -2.03018437  26.45599733]
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2877.0361288787876
gradient value of function right now is: [-2.80033792e-01 -1.11525498e-01 -4.96258249e-01 -1.51026732e-01
  2.80033792e-01  1.11525498e-01  4.96258249e-01  1.51026732e-01
  6.51400169e-02  5.65088287e-02 -1.57729000e-03  8.95142729e-02
 -2.11027748e-02 -3.26866672e-02 -1.44985862e-02 -1.64947231e-01
 -1.17745523e-03  3.72315125e-02 -4.52121273e-07  5.88375603e-01
 -1.40924060e-02 -2.81346624e-02 -1.03126125e-02 -1.81348999e-01
  2.75302305e-03 -1.33723274e-03  7.24335415e-03  9.57524440e-02
 -8.87679389e-02 -2.65931043e-02 -4.69583857e-02 -1.67273563e-01
  2.35238203e-06  7.54953517e-10  2.67075414e-06 -3.67460314e-03
  8.15444368e-01  4.85415284e-02  6.60892263e-02  1.31193745e+00
 -2.46171713e-02 -1.38476173e-01 -1.33134027e+00 -8.81880382e-02
  6.34305797e-05  1.41388527e-03  5.75465015e-03  7.55299138e-04
 -8.06773983e-04  2.46859821e-02 -1.39445672e-02 -4.11685865e-03
  1.29220885e-02 -1.83857145e-02  2.63086681e-01  4.54476849e-02
  2.37047018e-01 -1.78364315e+00  7.09401678e-01 -3.32694418e+00
  1.55652472e-01 -5.33449083e-01  9.87351200e-01 -6.32709173e+00
 -2.23851158e+00]
supnorm grad right now is: 6.327091727971006
Weights right now are: 
[ -0.97371972  -6.59071368   5.88650665  -5.18628874   1.23447109
   5.22689127  -6.18813799   4.93616479 -11.12932983   4.32955283
  -0.18448685   3.72025883  -9.87586624   3.87275555 -13.77550655
  18.81650035  25.60899931  13.35383285  15.33728806   3.94271341
  -9.09651092   6.02509592 -12.40751451  15.54135374  23.16079881
  -8.77074633 -22.33722886  10.58146691   7.33047837 -18.40117865
 -15.39323747   9.81070912  19.69185414  -5.00419717  -3.29859031
  51.03403282   9.29091215  -9.57759131  -0.87282306  -0.65546826
  38.53085569  11.57391995  -0.27155989  35.89891572  68.96991155
  23.79455997  24.779976    65.76393593  23.42394426  10.04661035
  25.7053364   35.59992263   6.8637255    4.8909133    0.0985743
  -0.45194759   1.71702517  -2.40284762  -5.45066103  -2.85631928
  11.411601     1.42966965  -0.59590634  -2.05678015  26.51502348]
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2877.23041100864
gradient value of function right now is: [-2.15007656e-01 -1.09978167e-01 -4.15814563e-01 -1.41638116e-01
  2.15007656e-01  1.09978167e-01  4.15814563e-01  1.41638116e-01
  5.97101175e-02  3.79310857e-02 -4.62927608e-03  1.29018910e-01
 -2.21854865e-02 -4.30525714e-02 -1.45796448e-02 -1.45394417e-01
 -3.76386450e-03  3.04078475e-02 -3.20545479e-07  4.41615993e-01
 -1.51520743e-02 -3.37081679e-02 -1.05127157e-02 -1.53281445e-01
  1.20434895e-02 -1.98883991e-04  1.76360188e-02  1.36002993e-01
 -4.82088325e-02 -1.06105871e-02 -2.87140853e-02 -1.04397573e-01
  2.69823645e-06  1.48586131e-09  4.79209055e-06 -4.09689810e-03
  7.80857910e-01  8.70845775e-02  1.01285019e-01  1.15166259e+00
 -1.48484005e-02 -1.38193103e-01 -8.33186903e-01 -6.87830708e-02
  1.52795188e-04  4.27109527e-03  2.99215267e-02  1.76201441e-03
 -2.37716472e-04  5.45083274e-02 -5.05635018e-03  8.71278566e-04
  6.95870616e-03 -2.58360973e-02  1.55238673e-02  3.36514293e-02
  1.81998821e-01 -1.13388567e+00  6.63454453e-01 -2.94045982e+00
 -4.43483309e-01  2.32661741e+00  8.32990554e-01 -4.60627576e+00
 -2.29343813e+00]
supnorm grad right now is: 4.606275756348585
Weights right now are: 
[-8.31123462e-01 -6.87848544e+00  5.93634659e+00 -5.41827236e+00
  1.09187483e+00  5.51466303e+00 -6.23797793e+00  5.16814841e+00
 -1.08482212e+01  4.67869289e+00 -5.96403178e-01  3.66167401e+00
 -9.96450329e+00  4.23630366e+00 -1.41677191e+01  1.88887401e+01
  2.65775665e+01  1.25752052e+01  1.53689885e+01  4.31066386e+00
 -9.09759658e+00  6.50854493e+00 -1.26894951e+01  1.56797457e+01
  2.30527704e+01 -8.40471223e+00 -2.29743242e+01  1.00562492e+01
  7.47277993e+00 -1.82009839e+01 -1.41267411e+01  9.87510028e+00
  1.96013126e+01 -5.00442872e+00 -3.56718541e+00  5.17751362e+01
  9.37748286e+00 -9.40466438e+00 -3.57448868e-01 -6.38422157e-01
  3.87916254e+01  1.12932068e+01 -4.37896697e-01  3.59438204e+01
  7.08546827e+01  2.50122602e+01  2.51921228e+01  6.70020964e+01
  2.56734381e+01  9.35850236e+00  2.62297063e+01  3.71228680e+01
  5.73228651e+00  4.98267490e+00 -1.38952027e-02 -1.02535434e+00
  1.53191136e+00 -2.40527124e+00 -5.33328080e+00 -3.03283380e+00
  1.12295779e+01  1.67013163e+00 -7.63006267e-01 -1.98728008e+00
  2.64892394e+01]
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2877.3088989374046
gradient value of function right now is: [-1.10946273e-01 -9.78402505e-02 -3.08121040e-01 -1.20702870e-01
  1.10946273e-01  9.78402505e-02  3.08121040e-01  1.20702870e-01
  5.66787061e-02  3.00941224e-02 -6.84876023e-03  1.83464616e-01
 -2.22993489e-02 -4.33571003e-02 -1.40253211e-02 -1.18479144e-01
 -5.21738094e-03  2.97702140e-02 -2.74004439e-07  2.58556948e-01
 -1.53122794e-02 -3.37894724e-02 -1.01066680e-02 -1.18566068e-01
  1.55727892e-02  1.46682905e-04  2.23019769e-02  1.67329701e-01
 -4.45149527e-02 -1.39114326e-02 -3.16767876e-02 -8.64533540e-02
  2.13482938e-06  1.31869768e-09  5.78325560e-06 -4.38512484e-03
  6.80478411e-01  8.74494256e-02  8.21613230e-02  9.44483940e-01
 -1.42027404e-02 -9.93624938e-02 -9.56854231e-01 -6.10533909e-02
  2.88266931e-05 -3.61042369e-04  2.69512101e-02  8.57518291e-04
  1.67593402e-04  6.65130104e-02 -4.46508381e-03  4.07924094e-03
  9.05826850e-03 -4.54504469e-02 -6.01705388e-03  2.75812811e-02
  1.42434617e-01 -1.07685795e+00  5.78926226e-01 -2.40623182e+00
 -4.34917853e-01  2.29233948e+00  6.12180301e-01 -3.87015989e+00
 -2.29514296e+00]
supnorm grad right now is: 3.8701598866290414
Weights right now are: 
[-1.39645060e+00 -6.72966335e+00  5.99507071e+00 -5.32160201e+00
  1.65720197e+00  5.36584094e+00 -6.29670205e+00  5.07147806e+00
 -1.15926097e+01  4.12021869e+00 -6.84145546e-01  3.27146563e+00
 -9.97926365e+00  4.86768691e+00 -1.46448411e+01  1.90597527e+01
  2.66838647e+01  1.11819530e+01  1.53955632e+01  4.25538538e+00
 -9.06676229e+00  7.24688394e+00 -1.30608295e+01  1.57124594e+01
  2.32813684e+01 -7.83090002e+00 -2.36010058e+01  9.52453280e+00
  7.65003825e+00 -1.80989113e+01 -1.29855031e+01  1.07799174e+01
  1.94520129e+01 -5.00453337e+00 -4.69071980e+00  5.27373001e+01
  9.19356093e+00 -9.27342409e+00 -1.93477166e-02 -5.68101500e-01
  3.87137593e+01  1.15709197e+01 -5.45905382e-01  3.58534866e+01
  7.21649132e+01  2.65513528e+01  2.55771782e+01  6.83660799e+01
  2.70265147e+01  7.82167622e+00  2.67330501e+01  3.69706000e+01
  6.99785478e+00  4.97981931e+00  2.22625836e-01 -2.76108635e-01
  1.54107661e+00 -2.42278265e+00 -5.53085567e+00 -2.86802860e+00
  1.11434933e+01  1.76009435e+00 -7.02650029e-01 -2.04030798e+00
  2.67361561e+01]
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2877.401046228707
gradient value of function right now is: [-1.04772148e-01 -9.42399917e-02 -3.02682429e-01 -1.16583082e-01
  1.04772148e-01  9.42399917e-02  3.02682429e-01  1.16583082e-01
  5.96144095e-02  2.75322341e-02 -6.51104761e-03  1.84846881e-01
 -1.93686821e-02 -4.25185697e-02 -1.20167548e-02 -1.14727132e-01
 -4.39315556e-03  4.19748473e-02 -2.16923935e-07  2.59597601e-01
 -1.28124291e-02 -3.34307395e-02 -8.41992147e-03 -1.15573992e-01
  1.17973177e-02 -6.74984898e-04  2.09490438e-02  1.67475852e-01
 -7.01741731e-02 -2.30175977e-02 -5.02914426e-02 -1.24104315e-01
  1.69217270e-06  8.46543571e-10  6.47634019e-06 -3.88264748e-03
  6.90683932e-01  1.02889567e-01  1.04833207e-01  9.35300374e-01
 -1.38657254e-02 -8.18548801e-02 -9.91585411e-01 -5.65891025e-02
 -1.03006149e-05 -2.98763486e-03  2.87792275e-02  3.32848445e-04
 -2.39366694e-05  5.83504582e-02 -1.02344854e-02  2.27992702e-03
  5.91282073e-04 -4.41778609e-02 -4.41359438e-02  1.29583134e-02
  1.23876672e-01 -1.08364925e+00  5.32902552e-01 -2.22415203e+00
 -3.79506902e-01  2.15024369e+00  5.73516206e-01 -3.77949369e+00
 -2.36541161e+00]
supnorm grad right now is: 3.7794936897258546
Weights right now are: 
[ -1.04738804  -7.22410688   6.12776035  -5.6569734    1.30813941
   5.86028447  -6.42939169   5.40684945 -11.31159595   4.7778088
  -0.72886835   3.5251831  -10.03165323   5.12195395 -15.08671303
  19.27693127  27.19483014  10.66808903  15.41031324   4.26744764
  -9.12609349   7.41220736 -13.54495065  16.03034548  24.2576479
  -6.83310061 -23.0717862    9.39938873   6.53135621 -19.21542567
 -13.39186997  10.01228612  19.20202269  -5.00458523  -6.7497571
  53.5837243    9.15485461  -8.69689654   0.37643573  -0.7150835
  39.05466698  11.52294287  -0.56081463  36.09920377  72.44175372
  26.6517218   25.60106469  68.14937062  27.91115625   7.50583748
  25.98251038  37.30762981   6.25084933   5.14735701  -0.27465993
  -0.39113561   1.51816079  -2.40932448  -5.63056106  -2.96913142
  11.44479245   1.52417768  -0.55564876  -2.0212202   26.64221747]
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2877.4146801710262
gradient value of function right now is: [-9.21143228e-02 -1.05918957e-01 -3.31591101e-01 -1.30366873e-01
  9.21143228e-02  1.05918957e-01  3.31591101e-01  1.30366873e-01
  6.99219540e-02  3.76439737e-02 -5.93009514e-03  2.04835363e-01
 -1.83316836e-02 -4.28918830e-02 -1.16292344e-02 -1.22371749e-01
 -4.66336495e-03  4.17142708e-02 -2.24466537e-07  2.75819292e-01
 -1.17274126e-02 -3.28388202e-02 -8.04409865e-03 -1.23270546e-01
  1.58802477e-02  1.54442326e-04  2.59383288e-02  1.87760543e-01
 -5.69688007e-02 -1.79330353e-02 -4.35226885e-02 -1.11945707e-01
  1.96659983e-06  7.93737241e-10  9.15646123e-06 -3.69776528e-03
  7.61552195e-01  1.06241580e-01  1.07576541e-01  1.03464704e+00
 -1.54770159e-02 -9.52288329e-02 -1.11470529e+00 -6.57505374e-02
  8.59623952e-05 -1.23782406e-03  3.30898324e-02  1.21793037e-03
  2.07944872e-04  6.99535256e-02 -7.01854718e-03  4.40134440e-03
 -1.89195115e-03 -7.07558812e-02 -2.83125408e-02  3.40069329e-03
  1.50379479e-01 -1.20366856e+00  5.79593965e-01 -2.38754232e+00
 -5.11562995e-01  2.68954979e+00  6.22373025e-01 -4.14466415e+00
 -2.47032638e+00]
supnorm grad right now is: 4.144664154162167
Weights right now are: 
[ -1.20225834  -7.02484111   6.18929621  -5.50388242   1.46300971
   5.66101869  -6.49092755   5.25375847 -11.6122895    4.63962638
  -1.04443603   3.65709133 -10.5009152    5.13992728 -15.86582823
  18.9649924   27.21942437   9.25449945  15.4323166    4.42356768
  -9.5209685    7.51404884 -14.24992469  15.76740782  24.97550721
  -5.98228691 -22.90615283   9.29410164   6.75520316 -19.02667729
 -11.85116405  10.65561494  18.90270368  -5.0046503   -8.2818309
  53.97586756   9.07144379  -8.45248693   0.57208009  -0.69640549
  39.13929076  11.40192356  -0.45068768  36.06656519  73.44235981
  28.28985117  25.61694944  69.19660687  29.15551046   7.39729654
  26.61170076  37.94963174   7.10920815   5.30961079   0.07911222
   0.23423358   1.30497788  -2.46484234  -5.83244478  -3.01199571
  11.28277033   1.64767505  -0.50941424  -2.06695578  26.6795203 ]
NN weights: [-1.13577149e+00 -6.89219514e+00  6.08236324e+00 -5.40867583e+00
  1.39652287e+00  5.52837273e+00 -6.38399458e+00  5.15855188e+00
 -1.13378985e+01  4.36522108e+00 -5.34844463e-01  3.48466931e+00
 -1.00617429e+01  4.38444400e+00 -1.45617295e+01  1.89537060e+01
  2.64659206e+01  1.17843759e+01  1.53816426e+01  4.00982905e+00
 -9.19297808e+00  6.65626215e+00 -1.30769234e+01  1.57445358e+01
  2.35901310e+01 -7.85625860e+00 -2.28738569e+01  9.81593582e+00
  7.38441191e+00 -1.83407944e+01 -1.37930245e+01  1.03581625e+01
  1.94786266e+01 -5.00440110e+00 -4.70765491e+00  5.22779965e+01
  9.28001012e+00 -9.19168542e+00 -1.91482069e-01 -6.03588651e-01
  3.87166529e+01  1.14814203e+01 -4.57792404e-01  3.59086609e+01
  7.11564527e+01  2.57134054e+01  2.52715251e+01  6.74905923e+01
  2.59467221e+01  8.77475402e+00  2.62636436e+01  3.67095808e+01
  6.81416001e+00  5.06713660e+00 -2.24357198e-02 -2.31196965e-01
  1.48250960e+00 -2.41495013e+00 -5.55393624e+00 -2.85253836e+00
  1.12759032e+01  1.55618921e+00 -6.42380882e-01 -2.03940580e+00]
Minimum obj value:-2877.4146801710262
Optimal xi: 26.661195326483803
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1512.1308334429493
W_T_median: 1357.1229318833216
W_T_pctile_5: 712.542292143636
W_T_CVAR_5_pct: 609.2572838071865
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
F value: -2877.4146801710262
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.358131295244
gradient value of function right now is: [ 4.17958529e-02  2.38375175e-03  4.46962127e-02  4.33666289e-02
 -4.17958529e-02 -2.38375175e-03 -4.46962127e-02 -4.33666289e-02
  7.49657933e-03  7.47804702e-03  1.66855576e-04  6.89357204e-04
  2.32453539e-03  2.31832555e-03  5.42117786e-05  2.15682131e-04
  1.95198489e-03  1.94678209e-03  4.53442175e-05  1.80783402e-04
  7.94487790e-03  7.92366447e-03  1.84957761e-04  7.36459730e-04
  9.70147099e-05  1.38350044e-04  1.20377062e-04  1.03205016e-04
  1.88712193e-04  2.56450742e-04  2.28364906e-04  1.91221431e-04
  4.66733040e-05  5.20255772e-05  5.13976802e-05  3.80100295e-05
  4.64733283e-04  4.70782874e-04  4.90490494e-04  3.41168925e-04
 -4.71863442e-05 -1.46146174e-05 -9.01545228e-06 -3.65003722e-05
 -4.23570854e-05  3.98159188e-05  5.70619901e-05 -1.87156509e-05
 -8.96537830e-05  2.44527357e-05  4.53940406e-05 -6.01273444e-05
 -1.45406088e-05  3.45875300e-05  4.72243096e-05  2.80416706e-06
 -1.92349927e-05 -2.35163068e-06  1.42950586e-05  5.39619337e-05
  3.38563899e-05  3.99048460e-05 -2.38785106e-06  7.87882095e-06
  2.91735870e+00]
supnorm grad right now is: 2.917358702586391
Weights right now are: 
[-0.43143422 -0.86462727 -1.02778142 -2.20494998  1.90557169  0.17067649
  2.33817971  2.1386943  -1.28280004 -1.25912952  0.26153157  0.43531831
  1.81284857  1.0428615   0.84953614  0.37751908 -2.02852514 -2.18352314
 -1.02390487 -1.30646782 -1.25748734 -1.77484744 -0.53191679 -1.02130019
 -2.27894841 -1.09860774 -0.88200824 -2.01125655 -0.9410993  -1.4051157
 -2.21198211 -0.91391564  1.38798302  1.64106798  0.72773737  0.72011095
  1.25165354  0.26644101  0.67628173  0.67298394 -0.70959192 -0.8216496
 -1.22633917 -0.25285493  1.50706211 -0.30069936 -1.47551505 -0.11063773
  0.43521914 -1.34642977 -1.04954798 -0.42371969  1.60504671  0.04974777
 -0.65853212  0.88225781  2.69016277  2.19505596  0.68721022 -0.90955994
 -0.22831532 -1.46194964  0.61899795  1.17810794 25.20007801]
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.609858527889
gradient value of function right now is: [ 1.66884194e-02  9.70460517e-04  1.71896809e-02  1.67787561e-02
 -1.66884194e-02 -9.70460517e-04 -1.71896809e-02 -1.67787561e-02
  1.73936307e-03  1.73629974e-03  3.89331504e-05  1.79849423e-04
  9.60802947e-04  9.58775503e-04  2.30394125e-05  1.00499327e-04
  7.16069401e-04  7.14580503e-04  1.70024856e-05  7.46758164e-05
  2.79721178e-03  2.79139750e-03  6.65060842e-05  2.91916859e-04
  3.08402855e-05  4.38543166e-05  3.86617587e-05  3.18890308e-05
  6.10505650e-05  8.49461988e-05  7.55651562e-05  6.20044096e-05
  1.99153467e-05  2.23774950e-05  2.20761010e-05  1.61370195e-05
  2.17896753e-04  2.21489444e-04  2.30085054e-04  1.59492147e-04
 -1.45186360e-05 -1.36613823e-05 -1.41657308e-05 -1.34773169e-05
 -1.14717392e-05  1.08308086e-05  1.55834318e-05 -4.23719111e-06
 -2.86211626e-05  4.33037160e-06  9.88838699e-06 -1.97761032e-05
 -4.52275064e-06  1.47861208e-06  3.60234726e-06 -5.77820280e-07
 -6.38105995e-06  9.84920243e-07  2.71489753e-06  1.29031746e-05
  8.12099421e-06  8.00117168e-06 -5.39046365e-07  2.57870300e-06
 -1.49681515e+00]
supnorm grad right now is: 1.4968151525606945
Weights right now are: 
[-0.55768412 -0.874363   -1.14978173 -2.41205173  2.03182159  0.18041221
  2.46018002  2.34579605 -1.56686836 -1.66646527  0.25244161  0.3983309
  1.7858795   1.00276902  0.84876896  0.37447618 -2.06974499 -2.24445416
 -1.02517459 -1.31150506 -1.33038979 -1.88218511 -0.53421577 -1.03039925
 -2.28779026 -1.11149896 -0.88849314 -2.0182324  -0.95732762 -1.43090991
 -2.22621179 -0.9284227   1.38411683  1.63673017  0.72535592  0.71781557
  1.20194651  0.21611555  0.64732298  0.6464828  -0.69276571 -0.81500766
 -1.21939426 -0.24497362  1.64364406 -0.34867355 -1.54114514 -0.08866704
  0.53177518 -1.35584459 -1.06812052 -0.39406775  1.62518132  0.03068978
 -0.68923169  0.88061992  2.97882502  2.19329329  0.66285956 -0.95284711
 -0.34696538 -1.48882493  0.62149277  1.17256746 24.90132551]
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.5656812160723
gradient value of function right now is: [ 6.26830857e-03  3.82570594e-04  6.36842316e-03  6.24552825e-03
 -6.26830857e-03 -3.82570594e-04 -6.36842316e-03 -6.24552825e-03
  4.60959116e-04  4.60625618e-04  9.57778690e-06  5.40359826e-05
  3.85121614e-04  3.84611827e-04  8.95523653e-06  4.58619260e-05
  2.51500290e-04  2.51186709e-04  5.73694147e-06  2.98232857e-05
  9.47032775e-04  9.45860409e-04  2.16079842e-05  1.12358937e-04
  8.69253762e-06  1.38944466e-05  1.18020491e-05  9.89748227e-06
  1.71979193e-05  2.75146213e-05  2.33370553e-05  1.97019282e-05
  6.44039313e-06  7.58344232e-06  7.33381168e-06  5.38092593e-06
  8.81276338e-05  9.10981107e-05  9.37050504e-05  6.47244425e-05
 -5.61068180e-06 -6.58647884e-06 -7.14326838e-06 -5.65064694e-06
 -4.14075941e-06  3.51120692e-06  5.08389655e-06 -1.59582429e-06
 -1.12031420e-05  3.70229449e-07  2.07784364e-06 -8.21066136e-06
 -1.92296429e-06 -1.60332346e-06 -1.25991313e-06 -9.23445490e-07
 -2.14237897e-06  3.06468732e-07  4.16138379e-07  4.21120054e-06
  2.11209368e-06  2.55676834e-06 -1.62891918e-07  9.70760651e-07
 -1.92199263e+00]
supnorm grad right now is: 1.9219926250895905
Weights right now are: 
[-0.69080796 -0.88498897 -1.27532922 -2.62631535  2.16494544  0.19103819
  2.58572752  2.56005968 -1.7574585  -1.94001656  0.24693238  0.370771
  1.75595403  0.9582446   0.84796093  0.37070416 -2.11012106 -2.30418928
 -1.02634565 -1.31700781 -1.39904434 -1.98335534 -0.5362513  -1.03995241
 -2.29434272 -1.1223447  -0.89370448 -2.02403558 -0.96965199 -1.45342066
 -2.23798665 -0.94096376  1.38034342  1.63228842  0.72297058  0.71548463
  1.14378001  0.15610898  0.61318066  0.61507997 -0.67537018 -0.80365399
 -1.2057836  -0.23551594  1.77819465 -0.38568632 -1.59263394 -0.06264004
  0.6345815  -1.35874728 -1.07812332 -0.35923092  1.64985632  0.03551405
 -0.68845897  0.88638716  3.1926178   2.19733437  0.6542731  -0.98812579
 -0.41682632 -1.51076025  0.62349686  1.16682785 24.89463426]
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.679921644347
gradient value of function right now is: [ 2.24898550e-03  1.45854527e-04  2.26995052e-03  2.23433653e-03
 -2.24898550e-03 -1.45854527e-04 -2.26995052e-03 -2.23433652e-03
  1.28145577e-04  1.28267352e-04  2.11121333e-06  1.69866292e-05
  1.49501276e-04  1.49478147e-04  3.09777929e-06  2.03017342e-05
  8.45588339e-05  8.45609494e-05  1.68118341e-06  1.14121639e-05
  3.08387664e-04  3.08403046e-04  6.11632703e-06  4.16277664e-05
  1.85428528e-06  4.23600954e-06  3.24401642e-06  2.95743419e-06
  3.53769392e-06  8.54783741e-06  6.43701927e-06  6.00159865e-06
  1.69663132e-06  2.23713330e-06  2.06650061e-06  1.55724109e-06
  3.31390622e-05  3.53474884e-05  3.57476061e-05  2.46423506e-05
 -2.45660977e-06 -2.77110468e-06 -3.01510380e-06 -2.50925372e-06
 -1.83986842e-06  1.20586499e-06  1.77537964e-06 -8.96721074e-07
 -4.96490965e-06 -2.97276090e-07  2.80245973e-07 -3.88320934e-06
 -9.52446371e-07 -1.09253184e-06 -1.04842017e-06 -6.79863962e-07
 -6.40925232e-07 -2.36771174e-07 -3.03709410e-08  1.77515697e-06
  5.17649083e-07  1.26802510e-06 -6.05546997e-08  3.81327427e-07
  8.21961670e-01]
supnorm grad right now is: 0.8219616696549267
Weights right now are: 
[-0.82673748 -0.89644751 -1.40225926 -2.84383872  2.30087495  0.20249672
  2.71265756  2.77758305 -1.90115874 -2.14651785  0.24325479  0.34707843
  1.72312924  0.90936215  0.8471163   0.36596027 -2.14872384 -2.36135683
 -1.02739561 -1.32302642 -1.46245818 -2.07689491 -0.53801123 -1.05004439
 -2.29906233 -1.13185606 -0.89801852 -2.02900254 -0.97838292 -1.47352475
 -2.24777998 -0.95190943  1.37721714  1.6283476   0.72091179  0.71345643
  1.08059583  0.08959649  0.57575544  0.58086873 -0.65570932 -0.78977885
 -1.18881123 -0.22442447  1.91681767 -0.4206369  -1.64164241 -0.03467708
  0.75146527 -1.35750368 -1.08332822 -0.31788669  1.67901542  0.05311738
 -0.67123657  0.89667022  3.35993662  2.20123642  0.65262008 -1.02380527
 -0.4686862  -1.53254353  0.62528131  1.1607593  25.04946986]
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.633825339189
gradient value of function right now is: [ 8.39542642e-04  5.85383367e-05  8.44473643e-04  8.33616371e-04
 -8.39542642e-04 -5.85383367e-05 -8.44473643e-04 -8.33616371e-04
  3.94488766e-05  3.95302017e-05  6.39985413e-07  6.19117089e-06
  6.10257320e-05  6.10560802e-05  1.30870014e-06  9.80338176e-06
  2.94886043e-05  2.95132360e-05  5.93509228e-07  4.70253772e-06
  1.04689556e-04  1.04780429e-04  2.10163333e-06  1.66984070e-05
  5.75093289e-07  1.42567072e-06  1.10556504e-06  9.51115259e-07
  1.03075396e-06  2.94149541e-06  2.20877404e-06  1.97223096e-06
  4.47403048e-07  6.20857616e-07  5.68081952e-07  4.17132520e-07
  1.29736229e-05  1.39107592e-05  1.40181699e-05  9.41127841e-06
 -9.47891746e-07 -1.06058599e-06 -1.15865076e-06 -9.70137855e-07
 -5.97616826e-07  5.26014181e-07  7.42653805e-07 -2.29529896e-07
 -1.81163960e-06 -1.20816073e-07  7.30615748e-08 -1.41717817e-06
 -3.31801945e-07 -4.84059570e-07 -4.85615805e-07 -2.49707863e-07
 -2.75206391e-07 -2.95257969e-08 -9.21653936e-10  6.01079320e-07
  2.12801559e-07  3.93419377e-07 -1.89762019e-08  1.26219070e-07
 -1.43101606e+00]
supnorm grad right now is: 1.4310160634128906
Weights right now are: 
[-9.66259157e-01 -9.09016586e-01 -1.53190441e+00 -3.06673000e+00
  2.44039663e+00  2.15065801e-01  2.84230271e+00  3.00047433e+00
 -2.01897466e+00 -2.31602612e+00  2.40491843e-01  3.24446823e-01
  1.68649869e+00  8.54766517e-01  8.46192957e-01  3.59784663e-01
 -2.18581340e+00 -2.41633553e+00 -1.02836512e+00 -1.32976215e+00
 -1.52160537e+00 -2.16422435e+00 -5.39587890e-01 -1.06100904e+00
 -2.30264892e+00 -1.14048570e+00 -9.01825831e-01 -2.03334837e+00
 -9.84649269e-01 -1.49208881e+00 -2.25641813e+00 -9.61659169e-01
  1.37483053e+00  1.62513806e+00  7.19267618e-01  7.11851159e-01
  1.01241053e+00  1.67895569e-02  5.35107199e-01  5.44343117e-01
 -6.33923652e-01 -7.74366420e-01 -1.16990537e+00 -2.12049633e-01
  2.03361777e+00 -4.57956591e-01 -1.69312123e+00 -1.07600427e-02
  8.75235684e-01 -1.35407406e+00 -1.08593622e+00 -2.72759590e-01
  1.70893947e+00  7.85603694e-02 -6.44127413e-01  9.08912392e-01
  3.46138630e+00  2.19618056e+00  6.51364558e-01 -1.05783599e+00
 -5.13693988e-01 -1.55197262e+00  6.26664010e-01  1.15502957e+00
  2.48407026e+01]
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.1428990387612
gradient value of function right now is: [ 2.89353168e-04  2.18252378e-05  2.90431416e-04  2.87354641e-04
 -2.89353168e-04 -2.18252377e-05 -2.90431416e-04 -2.87354641e-04
  1.14742325e-05  1.15319530e-05  9.14421145e-08  2.04811337e-06
  2.32054446e-05  2.32576281e-05  3.92462926e-07  4.28680752e-06
  9.46259697e-06  9.49046951e-06  1.36998333e-07  1.72968731e-06
  3.28359050e-05  3.29346935e-05  4.70586786e-07  6.00103989e-06
  1.22588533e-08  4.03217562e-07  2.57163792e-07  2.62826830e-07
 -6.16845502e-08  8.61323626e-07  5.12915255e-07  5.64111632e-07
  5.90593317e-08  1.25122755e-07  1.02162480e-07  8.22289820e-08
  4.56026067e-06  5.10380373e-06  5.04058231e-06  3.36602307e-06
 -4.39014072e-07 -4.02073352e-07 -4.30786050e-07 -4.35145402e-07
 -3.08726743e-07  1.87398910e-07  2.70137656e-07 -1.69263150e-07
 -8.29711115e-07 -9.25769510e-08 -2.10424784e-08 -6.79208210e-07
 -1.66992362e-07 -2.04011637e-07 -2.06115901e-07 -1.40518692e-07
 -6.99268421e-08 -1.05776948e-07 -1.76292040e-08  2.93949182e-07
  5.21937904e-08  2.39923999e-07 -6.73473255e-09  4.30624360e-08
  4.05525844e+00]
supnorm grad right now is: 4.0552584402201495
Weights right now are: 
[-1.10435553e+00 -9.22431480e-01 -1.65987517e+00 -3.28729535e+00
  2.57849301e+00  2.28480695e-01  2.97027347e+00  3.22103967e+00
 -2.11654089e+00 -2.45658103e+00  2.38516985e-01  3.02480716e-01
  1.64670757e+00  7.95407144e-01  8.45218795e-01  3.51903431e-01
 -2.22006474e+00 -2.46716120e+00 -1.02920479e+00 -1.33705637e+00
 -1.57489099e+00 -2.24298731e+00 -5.40917646e-01 -1.07259206e+00
 -2.30493656e+00 -1.14802572e+00 -9.05021349e-01 -2.03700065e+00
 -9.88201773e-01 -1.50876329e+00 -2.26375388e+00 -9.70074585e-01
  1.37339905e+00  1.62295072e+00  7.18186927e-01  7.10791762e-01
  9.42453793e-01 -5.91358955e-02  4.93075923e-01  5.07376906e-01
 -6.10146003e-01 -7.58661559e-01 -1.15069626e+00 -1.98445870e-01
  2.10705345e+00 -4.94107089e-01 -1.74265391e+00  9.35529872e-03
  9.91859638e-01 -1.34953070e+00 -1.08696582e+00 -2.25397149e-01
  1.73611277e+00  1.05045932e-01 -6.15595985e-01  9.22519847e-01
  3.49754786e+00  2.20510493e+00  6.51915008e-01 -1.09115430e+00
 -5.37734748e-01 -1.57329326e+00  6.27987825e-01  1.14992677e+00
  2.52774668e+01]
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.3124474388687
gradient value of function right now is: [ 1.16970827e-04  9.66645197e-06  1.17290915e-04  1.16244808e-04
 -1.16970828e-04 -9.66645202e-06 -1.17290916e-04 -1.16244809e-04
  4.14856608e-06  4.16659947e-06  6.01118688e-08  8.99261411e-07
  1.04272615e-05  1.04444538e-05  2.29908122e-07  2.30696374e-06
  3.56776234e-06  3.57666153e-06  6.93412365e-08  7.82814550e-07
  1.21700139e-05  1.22008916e-05  2.35924492e-07  2.67085779e-06
  3.79998009e-08  1.51698195e-07  1.15536080e-07  9.47135768e-08
  6.37698560e-08  3.48852817e-07  2.56983933e-07  2.18379900e-07
 -1.31331314e-09  1.23575793e-08  7.75440972e-09  7.91950078e-09
  1.88456788e-06  2.05606536e-06  2.04964425e-06  1.32261676e-06
 -1.48292253e-07 -1.40224753e-07 -1.50979143e-07 -1.45951092e-07
 -7.67683640e-08  9.88328975e-08  1.33262116e-07 -1.74946433e-08
 -2.49975417e-07 -1.27621559e-08  1.22583727e-08 -1.93715036e-07
 -4.46966657e-08 -7.08976326e-08 -7.19554057e-08 -3.45419344e-08
 -4.78113896e-08  5.91625598e-09  6.29505390e-09  7.99143356e-08
  4.07420445e-08  4.80844041e-08 -2.62896458e-09  1.29771348e-08
 -3.29877295e+00]
supnorm grad right now is: 3.2987729549209517
Weights right now are: 
[-1.24179780e+00 -9.36955425e-01 -1.78703763e+00 -3.50689119e+00
  2.71593527e+00  2.43004641e-01  3.09743592e+00  3.44063552e+00
 -2.20039018e+00 -2.57734723e+00  2.37087864e-01  2.80415282e-01
  1.60286481e+00  7.29958469e-01  8.44179309e-01  3.41742519e-01
 -2.25180913e+00 -2.51429917e+00 -1.02992600e+00 -1.34495816e+00
 -1.62329353e+00 -2.31460509e+00 -5.42036110e-01 -1.08489248e+00
 -2.30609219e+00 -1.15443615e+00 -9.07653096e-01 -2.04001932e+00
 -9.89519270e-01 -1.52365063e+00 -2.27005383e+00 -9.77371651e-01
  1.37294237e+00  1.62190847e+00  7.17714899e-01  7.10298059e-01
  8.71805335e-01 -1.36911460e-01  4.50143701e-01  4.70329288e-01
 -5.86841747e-01 -7.43758755e-01 -1.13286208e+00 -1.84428876e-01
  2.13892451e+00 -5.18965531e-01 -1.77673512e+00  2.05192225e-02
  1.06913441e+00 -1.34534649e+00 -1.08744046e+00 -1.83966477e-01
  1.75203548e+00  1.24542242e-01 -5.95176710e-01  9.32973282e-01
  3.51164058e+00  2.20944393e+00  6.51766302e-01 -1.11563752e+00
 -5.48701728e-01 -1.58993763e+00  6.28743398e-01  1.14656219e+00
  2.47742127e+01]
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.5744431840853
gradient value of function right now is: [ 4.21399501e-05  3.86012328e-06  4.22226010e-05  4.19058502e-05
 -4.21399494e-05 -3.86012321e-06 -4.22226002e-05 -4.19058494e-05
  1.33485337e-06  1.34228877e-06  1.54167400e-08  3.32821679e-07
  4.23874785e-06  4.24830693e-06  8.83317755e-08  1.07781699e-06
  1.19548980e-06  1.19958899e-06  2.06964832e-08  3.01428757e-07
  4.01392192e-06  4.02789089e-06  6.91821747e-08  1.01218267e-06
  1.58811393e-09  4.35555541e-08  3.02072008e-08  2.69407973e-08
  1.30903003e-09  1.12491888e-07  7.70064909e-08  6.97567895e-08
 -1.38956719e-08 -1.18794394e-08 -1.29172185e-08 -7.41812791e-09
  6.69601003e-07  7.41311708e-07  7.34567608e-07  4.72812151e-07
 -5.81785641e-08 -4.74997991e-08 -5.02791235e-08 -5.58009999e-08
 -3.20664059e-08  3.95709233e-08  5.30476011e-08 -9.17571231e-09
 -9.67184881e-08 -4.89520939e-09  4.43307325e-09 -7.58350658e-08
 -1.78490682e-08 -2.41571070e-08 -2.42460162e-08 -1.40037220e-08
 -1.65583115e-08 -1.54444222e-09  2.70699751e-09  3.27488927e-08
  1.55075598e-08  2.19160232e-08 -9.44207068e-10  3.98313462e-09
 -1.90267326e+00]
supnorm grad right now is: 1.9026732648311087
Weights right now are: 
[-1.38058926e+00 -9.53093143e-01 -1.91532572e+00 -3.72875117e+00
  2.85472673e+00  2.59142358e-01  3.22572401e+00  3.66249550e+00
 -2.27449322e+00 -2.68290386e+00  2.36015345e-01  2.57900108e-01
  1.55332470e+00  6.56029540e-01  8.43022624e-01  3.28391343e-01
 -2.28155358e+00 -2.55837425e+00 -1.03055758e+00 -1.35355612e+00
 -1.66797474e+00 -2.38073424e+00 -5.43001535e-01 -1.09809375e+00
 -2.30634069e+00 -1.15955486e+00 -9.09728223e-01 -2.04243335e+00
 -9.89366852e-01 -1.53656329e+00 -2.27552579e+00 -9.83707456e-01
  1.37348559e+00  1.62213029e+00  7.17914073e-01  7.10400338e-01
  8.03533597e-01 -2.12738229e-01  4.07139473e-01  4.34001214e-01
 -5.70476036e-01 -7.32527637e-01 -1.12005134e+00 -1.72488006e-01
  2.15106219e+00 -5.31330927e-01 -1.79344582e+00  2.49042473e-02
  1.10349648e+00 -1.34325304e+00 -1.08807435e+00 -1.59559604e-01
  1.75863827e+00  1.33434465e-01 -5.86097160e-01  9.38091170e-01
  3.51685259e+00  2.21128009e+00  6.51402765e-01 -1.12701738e+00
 -5.53270188e-01 -1.59800972e+00  6.29054054e-01  1.14511595e+00
  2.49188492e+01]
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.0315215746064
gradient value of function right now is: [ 1.41757762e-05  1.46360538e-06  1.41958195e-05  1.41058813e-05
 -1.41757737e-05 -1.46360512e-06 -1.41958170e-05 -1.41058788e-05
  4.06331215e-07  4.09632419e-07  9.97204278e-10  1.11385385e-07
  1.63828027e-06  1.64416845e-06  2.60562929e-08  4.59860513e-07
  3.76227830e-07  3.78232770e-07  3.96849771e-09  1.04537659e-07
  1.24154015e-06  1.24825930e-06  1.28594658e-08  3.44926278e-07
 -6.68343168e-09  9.99649349e-09  4.36281419e-09  6.25409991e-09
 -1.61032255e-08  3.15004643e-08  1.54977738e-08  1.97483318e-08
 -8.99449606e-09 -1.01654734e-08 -1.00096384e-08 -6.37309738e-09
  2.15274015e-07  2.46980257e-07  2.41586436e-07  1.57535342e-07
 -2.42271944e-08 -1.57675052e-08 -1.61778270e-08 -2.26464914e-08
 -1.67629944e-08  1.35318280e-08  1.84924009e-08 -8.50753442e-09
 -4.19947866e-08 -3.28942987e-09  2.92224929e-10 -3.42300902e-08
 -8.35248702e-09 -7.89034822e-09 -7.72747017e-09 -6.87497312e-09
 -3.69899813e-09 -6.40112642e-09  1.97365589e-10  1.58767630e-08
  4.19033964e-09  1.34023475e-08 -2.95824437e-10  1.13862833e-09
  4.43102287e+00]
supnorm grad right now is: 4.431022869809364
Weights right now are: 
[-1.5202381  -0.97123476 -2.04433924 -3.9519937   2.99437557  0.27728397
  3.35473754  3.88573802 -2.33708063 -2.7665176   0.23519079  0.23719637
  1.4969549   0.57242264  0.84168353  0.31108404 -2.30902024 -2.59827701
 -1.03112368 -1.3625386  -1.70919017 -2.44149815 -0.54387333 -1.11197725
 -2.30606929 -1.16276695 -0.91113839 -2.04406918 -0.98878206 -1.54571958
 -2.27989516 -0.98854147  1.37484946  1.62348255  0.71880723  0.7110765
  0.75031791 -0.27205898  0.36855128  0.40324546 -0.56299236 -0.72707268
 -1.1142165  -0.16597083  2.15560434 -0.53658212 -1.80049432  0.02640978
  1.11657807 -1.34254003 -1.08859087 -0.14945172  1.76108844  0.13648874
 -0.58304971  0.94001434  3.51889026  2.21183483  0.65105076 -1.13164455
 -0.55527185 -1.60127879  0.62917312  1.1446283  25.27909361]
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.706452840724
gradient value of function right now is: [ 5.55723469e-06  6.55993533e-07  5.56456544e-06  5.53365377e-06
 -5.55723664e-06 -6.55993759e-07 -5.56456739e-06 -5.53365570e-06
  1.56711004e-07  1.57783859e-07  1.22176816e-09  4.62047443e-08
  7.44109942e-07  7.46189769e-07  1.43974533e-08  2.23111353e-07
  1.43211992e-07  1.43856271e-07  2.04438098e-09  4.25950172e-08
  4.61780652e-07  4.63894819e-07  6.51391777e-09  1.37336992e-07
 -1.99339013e-09  3.14155166e-09  1.45890737e-09  1.93711249e-09
 -2.98206873e-09  1.28121386e-08  7.72139509e-09  7.92503871e-09
 -5.56478759e-09 -6.54605075e-09 -6.37747470e-09 -4.08649306e-09
  8.03548038e-08  9.05473765e-08  8.91545341e-08  5.75538763e-08
 -8.27496353e-09 -4.95741809e-09 -5.01177884e-09 -7.57088140e-09
 -5.05185745e-09  5.91994875e-09  7.90192500e-09 -1.74416502e-09
 -1.38907227e-08 -3.06787171e-10  1.07496418e-09 -1.09274666e-08
 -2.57172293e-09 -2.25055821e-09 -2.11853151e-09 -1.93887577e-09
 -2.03240004e-09 -9.86207957e-10  5.47786587e-10  5.19843152e-09
  2.32675588e-09  3.84954526e-09 -1.45271931e-10  4.01156498e-10
 -2.73325609e-02]
supnorm grad right now is: 0.027332560859332104
Weights right now are: 
[-1.65697247 -0.99136153 -2.17066919 -4.16980517  3.13110993  0.29741075
  3.48106748  4.10354948 -2.37811154 -2.81384228  0.23479569  0.22405679
  1.43536942  0.48452105  0.84029689  0.29072454 -2.33191251 -2.62862421
 -1.03152893 -1.37031828 -1.74592643 -2.49421349 -0.54456001 -1.12525924
 -2.30561042 -1.16400449 -0.91171988 -2.04478185 -0.98804038 -1.54999107
 -2.28225193 -0.99104865  1.3761774   1.62498827  0.72000848  0.71193136
  0.72365099 -0.3019927   0.34372952  0.38559606 -0.56014347 -0.72517545
 -1.11225494 -0.16335551  2.15732995 -0.53862546 -1.80321832  0.02700816
  1.12138262 -1.34236829 -1.08888245 -0.14566609  1.76198652  0.13741521
 -0.58214882  0.94071285  3.51959004  2.21215273  0.6508985  -1.13341738
 -0.55602253 -1.60260874  0.62921811  1.14447821 25.01294595]
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.5954357734895
gradient value of function right now is: [ 2.13193071e-06  2.84008201e-07  2.13470686e-06  2.12409218e-06
 -2.13193117e-06 -2.84008262e-07 -2.13470732e-06 -2.12409264e-06
  6.13457034e-08  6.17356649e-08  6.03030485e-10  1.86011557e-08
  3.27515938e-07  3.28321840e-07  6.78920745e-09  1.00768568e-07
  5.50815299e-08  5.53117047e-08  8.65001543e-10  1.68222987e-08
  1.72168050e-07  1.72903991e-07  2.66744423e-09  5.25754172e-08
 -8.13740739e-10  9.42343180e-10  3.70919902e-10  5.76978328e-10
 -6.81508177e-10  5.06175591e-09  3.24780631e-09  3.11614021e-09
 -2.90342625e-09 -3.45645510e-09 -3.35867841e-09 -2.15390758e-09
  2.96550388e-08  3.32180401e-08  3.27810003e-08  2.10874914e-08
 -3.05478347e-09 -1.61843207e-09 -1.59455902e-09 -2.72767482e-09
 -1.74476922e-09  2.42555832e-09  3.21359408e-09 -4.25482977e-10
 -5.04901800e-09  8.07547032e-11  6.32489554e-10 -3.87896225e-09
 -8.92758667e-10 -6.55458043e-10 -5.78435829e-10 -6.20481788e-10
 -9.08640660e-10 -1.23206441e-10  3.15688438e-10  1.90616917e-09
  1.05857855e-09  1.29904989e-09 -6.51369860e-11  1.45416749e-10
 -1.77146285e+00]
supnorm grad right now is: 1.771462852349667
Weights right now are: 
[-1.79386307 -1.01400067 -2.29748192 -4.38207047  3.26800059  0.3200499
  3.60788026  4.31581486 -2.39793117 -2.83440546  0.23460062  0.21797381
  1.37557716  0.40973001  0.8389619   0.27095891 -2.34695313 -2.64552962
 -1.03177961 -1.37518063 -1.77676143 -2.53382744 -0.54512131 -1.13617275
 -2.305375   -1.16438202 -0.91188898 -2.04501192 -0.98779503 -1.55169347
 -2.28330061 -0.99209272  1.37699699  1.62596074  0.72090764  0.71252649
  0.71324828 -0.31366504  0.33262091  0.37828097 -0.55911721 -0.72454676
 -1.11162503 -0.16243355  2.15796421 -0.53942472 -1.80428547  0.02719308
  1.12312488 -1.34233877 -1.08904047 -0.14432641  1.7623066   0.13769213
 -0.58189329  0.94094372  3.51987502  2.21226666  0.65081564 -1.13410689
 -0.55634747 -1.60312369  0.62923839  1.14442752 24.86118973]
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.3300764152536
gradient value of function right now is: [ 8.00405418e-07  1.14793306e-07  8.01339470e-07  7.97678284e-07
 -8.00405593e-07 -1.14793331e-07 -8.01339646e-07 -7.97678458e-07
  2.32071155e-08  2.34216748e-08 -3.25249741e-11  6.96522605e-09
  1.35610565e-07  1.36147765e-07  1.96806335e-09  4.16199566e-08
  2.09609677e-08  2.10922185e-08  1.54959491e-10  6.36538981e-09
  6.37277874e-08  6.41374602e-08  4.42088110e-10  1.93432553e-08
 -6.54910481e-10  2.16496061e-10 -8.60084923e-11  1.37730849e-10
 -1.35667272e-09  1.66577314e-09  6.40106432e-10  1.04882207e-09
 -1.12498908e-09 -1.53622124e-09 -1.42849189e-09 -9.64840769e-10
  1.08842609e-08  1.27407075e-08  1.23782933e-08  8.12357268e-09
 -1.56259963e-09 -6.91256385e-10 -6.57178759e-10 -1.39349507e-09
 -1.16183036e-09  8.65385701e-10  1.18773157e-09 -6.29411593e-10
 -2.72043509e-09 -1.28157397e-10  1.18199189e-10 -2.20995504e-09
 -5.29198469e-10 -2.75265456e-10 -2.35336252e-10 -4.11179766e-10
 -1.96334846e-10 -5.15227245e-10  4.35934589e-11  1.11169598e-09
  2.94370140e-10  9.73283519e-10 -2.39876686e-11  5.13239875e-11
  6.37666054e+00]
supnorm grad right now is: 6.376660536057233
Weights right now are: 
[-1.92392433 -1.03617229 -2.41977205 -4.56065625  3.39806187  0.34222152
  3.73017042  4.49440068 -2.40620513 -2.84277185  0.23452939  0.21545044
  1.3353122   0.36657065  0.83812936  0.25818383 -2.35412578 -2.6528928
 -1.03188698 -1.37740768 -1.79616044 -2.55504187 -0.54542066 -1.14246703
 -2.30524616 -1.16449639 -0.91192288 -2.04508194 -0.98765795 -1.55236783
 -2.28371395 -0.99250789  1.37740929  1.62646134  0.72138747  0.7128376
  0.70928628 -0.31812598  0.32824825  0.37545498 -0.55868695 -0.72432935
 -1.11141351 -0.16205136  2.15822213 -0.53975474 -1.80472387  0.02727005
  1.12384348 -1.34234505 -1.08912427 -0.14377038  1.7624356   0.13777737
 -0.58181965  0.94103461  3.51998855  2.21231192  0.6507755  -1.13438804
 -0.55648664 -1.6033267   0.62924731  1.14440821 25.45980296]
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.677903402685
gradient value of function right now is: [ 4.45949872e-07  6.65308084e-08  4.46503828e-07  4.44515104e-07
 -4.45950029e-07 -6.65308319e-08 -4.46503986e-07 -4.44515261e-07
  1.35559308e-08  1.36560721e-08  8.06134388e-11  4.13902261e-09
  7.97572854e-08  7.99933726e-08  1.49056148e-09  2.47673970e-08
  1.20180809e-08  1.20775036e-08  1.53452505e-10  3.70048687e-09
  3.58317454e-08  3.60139749e-08  4.44792818e-10  1.10294425e-08
 -2.73600080e-10  1.36409050e-10 -1.37230581e-12  8.42229239e-11
 -3.73159319e-10  1.05122741e-09  5.88115980e-10  6.51131649e-10
 -7.46328763e-10 -9.43298583e-10 -8.99350485e-10 -5.89125889e-10
  6.12131031e-09  6.97277363e-09  6.84068860e-09  4.43159047e-09
 -7.55785973e-10 -3.27535431e-10 -3.08562497e-10 -6.63967957e-10
 -4.83186033e-10  5.31269231e-10  7.11754268e-10 -1.82308845e-10
 -1.26924251e-09  4.40456256e-12  1.36774345e-10 -9.93961641e-10
 -2.31406642e-10 -1.16122181e-10 -9.21206543e-11 -1.63456357e-10
 -1.74214095e-10 -1.17287296e-10  6.32061265e-11  5.03714585e-10
  2.24834606e-10  3.83998384e-10 -1.56042226e-11  2.90933376e-11
  8.96509670e-01]
supnorm grad right now is: 0.8965096699468342
Weights right now are: 
[-2.02766992 -1.05265481 -2.52069686 -4.67725865  3.50180747  0.35870404
  3.83109522  4.61100309 -2.41008216 -2.84667752  0.23449898  0.21426513
  1.31334468  0.34430274  0.83769664  0.25133384 -2.3575515  -2.6563445
 -1.03193544 -1.3784646  -1.8062725  -2.56535203 -0.54556193 -1.14561244
 -2.30517849 -1.16453991 -0.9119292  -2.04510877 -0.98758191 -1.55267452
 -2.28389678 -0.9926977   1.37761879  1.62672104  0.72163621  0.7129997
  0.70750864 -0.32013758  0.32627214  0.37417722 -0.55848454 -0.7242347
 -1.11132316 -0.1618735   2.15834884 -0.53990654 -1.80492686  0.0273114
  1.12418344 -1.34234769 -1.08916374 -0.14350746  1.76249709  0.13781207
 -0.5817913   0.9410773   3.52004124  2.2123366   0.65075638 -1.13452342
 -0.55655218 -1.60342743  0.62925167  1.14439983 25.05388995]
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.5528650493347
gradient value of function right now is: [ 3.00295768e-07  4.58659760e-08  3.00680591e-07  2.99361392e-07
 -3.00296063e-07 -4.58660210e-08 -3.00680886e-07 -2.99361686e-07
  9.40234010e-09  9.46254218e-09  9.21072356e-11  2.89644278e-09
  5.55268754e-08  5.56611911e-08  1.16125071e-09  1.73504838e-08
  8.25306007e-09  8.28800483e-09  1.28986152e-10  2.55978061e-09
  2.42695652e-08  2.43754215e-08  3.71993859e-10  7.52575185e-09
 -1.51855165e-10  9.73525404e-11  1.54851418e-11  5.93296877e-11
 -1.07933051e-10  7.55667294e-10  4.83665334e-10  4.64644344e-10
 -5.53373196e-10 -6.72209964e-10 -6.49477162e-10 -4.18650388e-10
  4.14999410e-09  4.65770322e-09  4.59395163e-09  2.95553979e-09
 -4.65878022e-10 -1.99354670e-10 -1.86354403e-10 -4.04173672e-10
 -2.64023849e-10  3.76886455e-10  4.99024532e-10 -5.91491022e-11
 -7.63600590e-10  3.10758057e-11  1.18757373e-10 -5.80482673e-10
 -1.32245979e-10 -6.43942487e-11 -4.74584668e-11 -8.53073318e-11
 -1.43276181e-10 -1.37881025e-11  5.74989803e-11  2.96687432e-10
  1.74977592e-10  1.99557500e-10 -1.14061666e-11  1.98556955e-11
 -2.02215226e+00]
supnorm grad right now is: 2.022152263302958
Weights right now are: 
[-2.09814662 -1.06345142 -2.5908278  -4.74909822  3.57228418  0.36950064
  3.90122617  4.68284266 -2.41232456 -2.8489372   0.23448617  0.21357948
  1.30009913  0.33099736  0.8374473   0.24721088 -2.35953461 -2.65833852
 -1.03196063 -1.3790763  -1.81213099 -2.57125318 -0.54563428 -1.14742104
 -2.30513139 -1.16456206 -0.91192822 -2.04512237 -0.98751702 -1.55285
 -2.28399468 -0.99280607  1.37774488  1.6268803   0.72178819  0.71309904
  0.70650522 -0.32128122  0.3251501   0.37345079 -0.55835596 -0.72418217
 -1.11127416 -0.161761    2.15842882 -0.53999543 -1.80504605  0.02734015
  1.12439755 -1.34234999 -1.08918841 -0.14334027  1.76253569  0.13782995
 -0.58177764  0.94110403  3.52007269  2.21235258  0.65074495 -1.13460697
 -0.55659196 -1.60348826  0.62925446  1.14439478 24.86580184]
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.5299205149227
gradient value of function right now is: [ 2.21061827e-07  3.43348701e-08  2.21345147e-07  2.20387654e-07
 -2.21062103e-07 -3.43349133e-08 -2.21345425e-07 -2.20387930e-07
  7.02626845e-09  7.07087232e-09  7.04463440e-11  2.16892168e-09
  4.18885671e-08  4.19883188e-08  8.82562156e-10  1.31132908e-08
  6.14905128e-09  6.17485553e-09  9.71333407e-11  1.91086500e-09
  1.78988783e-08  1.79763178e-08  2.77235548e-10  5.56090048e-09
 -1.13688924e-10  6.93565634e-11  9.25606897e-12  4.22060015e-11
 -7.38745237e-11  5.63898426e-10  3.63538419e-10  3.46535602e-10
 -4.23605200e-10 -5.14213549e-10 -4.96994695e-10 -3.20174944e-10
  3.06086310e-09  3.43289311e-09  3.38683266e-09  2.17802376e-09
 -3.45102381e-10 -1.43477900e-10 -1.32972929e-10 -2.98120841e-10
 -1.93620951e-10  2.82149395e-10  3.73320557e-10 -4.06117810e-11
 -5.64003062e-10  2.60401546e-11  9.15918513e-11 -4.27319987e-10
 -9.71003432e-11 -4.42349045e-11 -3.11133977e-11 -6.15844778e-11
 -1.08261201e-10 -6.90669617e-12  4.43226092e-11  2.19527671e-10
  1.32610669e-10  1.46053541e-10 -8.68022325e-12  1.45974452e-11
 -2.22301717e+00]
supnorm grad right now is: 2.2230171737545663
Weights right now are: 
[-2.14880551 -1.07126365 -2.64150473 -4.79975749  3.62294307  0.37731288
  3.95190311  4.73350193 -2.41391691 -2.85054119  0.23447582  0.21309137
  1.29057581  0.32144485  0.83726473  0.24424219 -2.36093769 -2.65974851
 -1.0319792  -1.37950998 -1.81623498 -2.57537887 -0.54568712 -1.14868931
 -2.30509945 -1.16457677 -0.9119274  -2.04513136 -0.98747779 -1.55297365
 -2.28406615 -0.99288224  1.37783726  1.62699608  0.72189901  0.71317122
  0.70580345 -0.32207778  0.32436734  0.372945   -0.55826836 -0.72414702
 -1.11124133 -0.16168382  2.15848218 -0.54005939 -1.80513079  0.02735949
  1.12454228 -1.34235365 -1.08920711 -0.14322628  1.76256175  0.1378413
 -0.58176916  0.9411224   3.52009113  2.21236761  0.65073749 -1.13466436
 -0.55661794 -1.60353165  0.62925631  1.14439148 24.86238838]
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.7061689327347
gradient value of function right now is: [ 1.69643754e-07  2.66965377e-08  1.69855343e-07  1.69131905e-07
 -1.69643919e-07 -2.66965634e-08 -1.69855509e-07 -1.69132069e-07
  5.41391432e-09  5.45242291e-09  3.82723757e-11  1.66508816e-09
  3.27938784e-08  3.28852156e-08  6.36431639e-10  1.02486246e-08
  4.75073372e-09  4.77331712e-09  6.45304741e-11  1.47258113e-09
  1.37152693e-08  1.37825014e-08  1.81206161e-10  4.24990181e-09
 -1.06599890e-10  4.68094191e-11 -4.54314739e-12  2.87439324e-11
 -1.22650729e-10  4.19706588e-10  2.45091629e-10  2.59271529e-10
 -3.22851774e-10 -4.05595283e-10 -3.87619569e-10 -2.53059998e-10
  2.34357800e-09  2.65972296e-09  2.61298563e-09  1.68941138e-09
 -2.91079089e-10 -1.15891831e-10 -1.06490605e-10 -2.52542754e-10
 -1.79553369e-10  2.13953125e-10  2.85490241e-10 -6.00440387e-11
 -4.83419027e-10  1.04284352e-11  6.29941688e-11 -3.74530457e-10
 -8.64114339e-11 -3.60202387e-11 -2.53492039e-11 -5.83045541e-11
 -7.37956363e-11 -3.42965911e-11  2.92573823e-11  1.92353633e-10
  9.51952116e-11  1.41544159e-10 -6.63293292e-12  1.10416813e-11
 -8.19822815e-02]
supnorm grad right now is: 0.08198228150275556
Weights right now are: 
[-2.18797107 -1.07738449 -2.68071481 -4.83882022  3.66210862  0.38343372
  3.99111319  4.77256466 -2.41516252 -2.85179535  0.23446598  0.21270818
  1.28308107  0.31392999  0.83711628  0.2419008  -2.36203075 -2.66084657
 -1.0319947  -1.37984876 -1.81940434 -2.57856326 -0.54573094 -1.14967134
 -2.30507634 -1.16458807 -0.91192723 -2.04513829 -0.98745397 -1.5530709
 -2.28412462 -0.99294231  1.37791123  1.62708812  0.72198723  0.71322866
  0.70526146 -0.3226908   0.32376437  0.37255544 -0.55820334 -0.72412038
 -1.11121672 -0.16162751  2.15852163 -0.54010858 -1.8051962   0.0273716
  1.12464997 -1.34235642 -1.08922172 -0.14314336  1.76258087  0.1378498
 -0.58176304  0.94113509  3.52010884  2.2123738   0.65073057 -1.13470672
 -0.55664016 -1.60356197  0.62925784  1.14438891 24.98265122]
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.7058750441015
gradient value of function right now is: [ 1.40579367e-07  2.23570844e-08  1.40754501e-07  1.40160285e-07
 -1.40579780e-07 -2.23571500e-08 -1.40754914e-07 -1.40160697e-07
  4.52466996e-09  4.55686419e-09  3.19899328e-11  1.39302758e-09
  2.75918789e-08  2.76686192e-08  5.35924514e-10  8.63155679e-09
  3.96439871e-09  3.98325705e-09  5.38372413e-11  1.23007103e-09
  1.13754030e-08  1.14312292e-08  1.50178577e-10  3.52835347e-09
 -8.98152612e-11  3.75875380e-11 -5.08545435e-12  2.30731890e-11
 -1.02315377e-10  3.50065833e-10  2.04452085e-10  2.16232650e-10
 -2.73523268e-10 -3.44038818e-10 -3.28681654e-10 -2.14644150e-10
  1.94368839e-09  2.20620845e-09  2.16735043e-09  1.40129843e-09
 -2.43130766e-10 -9.50087869e-11 -8.68053380e-11 -2.10499680e-10
 -1.49870488e-10  1.78939801e-10  2.38766344e-10 -4.99137548e-11
 -4.03427120e-10  9.41819336e-12  5.34504411e-11 -3.12317556e-10
 -7.20108042e-11 -2.87172254e-11 -1.96018031e-11 -4.83203804e-11
 -6.18192209e-11 -2.84362420e-11  2.47945151e-11  1.60845196e-10
  8.00327136e-11  1.18254745e-10 -5.60155799e-12  9.13283361e-12
 -1.13200469e-01]
supnorm grad right now is: 0.11320046874551687
Weights right now are: 
[-2.218828   -1.08226321 -2.71160936 -4.86958609  3.69296555  0.38831244
  4.02200773  4.80333053 -2.41614864 -2.85278888  0.2344601   0.21240534
  1.27707365  0.30790485  0.83700241  0.24002435 -2.3628969  -2.66171707
 -1.03200582 -1.38011698 -1.82189779 -2.58106964 -0.54576198 -1.15044323
 -2.3050554  -1.1645963  -0.91192574 -2.04514333 -0.98742714 -1.55314697
 -2.2841677  -0.99298928  1.37797016  1.62716267  0.72205838  0.71327519
  0.7048355  -0.32317577  0.32328826  0.37224726 -0.55814776 -0.72409932
 -1.11119745 -0.16157912  2.15855554 -0.54014777 -1.80524841  0.02738374
  1.12474167 -1.34235843 -1.08923338 -0.1430717   1.76259719  0.13785631
 -0.5817585   0.94114625  3.52012265  2.21237938  0.65072516 -1.13474199
 -0.55665761 -1.60358762  0.62925906  1.14438686 24.93039467]
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.574830241905
gradient value of function right now is: [ 1.16901443e-07  1.87601496e-08  1.17043315e-07  1.16554969e-07
 -1.16901671e-07 -1.87601859e-08 -1.17043544e-07 -1.16555197e-07
  3.76425057e-09  3.79388883e-09  1.57770279e-11  1.15440092e-09
  2.32638938e-08  2.33376976e-08  4.15142442e-10  7.26361156e-09
  3.30869497e-09  3.32624083e-09  3.78902911e-11  1.02378992e-09
  9.44154600e-09  9.49320613e-09  1.03962348e-10  2.92017547e-09
 -8.72332619e-11  2.73472057e-11 -1.15965429e-11  1.69722904e-11
 -1.29589287e-10  2.81932012e-10  1.46967346e-10  1.75047825e-10
 -2.23923414e-10 -2.91317816e-10 -2.75331772e-10 -1.82097773e-10
  1.61181853e-09  1.85027811e-09  1.81048773e-09  1.17647889e-09
 -2.19812618e-10 -8.31048745e-11 -7.54406604e-11 -1.91035193e-10
 -1.45159113e-10  1.46659637e-10  1.97385254e-10 -6.07036281e-11
 -3.69329537e-10  1.24204005e-12  3.93509134e-11 -2.90826195e-10
 -6.77663402e-11 -2.54891789e-11 -1.75579810e-11 -4.75323110e-11
 -4.57020738e-11 -4.18818999e-11  1.74419806e-11  1.49541654e-10
  6.21510658e-11  1.17548513e-10 -4.62115711e-12  7.52428323e-12
  1.91195572e+00]
supnorm grad right now is: 1.9119557196222616
Weights right now are: 
[-2.24504338 -1.08644914 -2.73785705 -4.89572375  3.71918094  0.39249837
  4.04825542  4.8294682  -2.41699312 -2.85363964  0.234455    0.2121457
  1.27189699  0.30271307  0.83690436  0.23840573 -2.36363751 -2.66246137
 -1.03201534 -1.38034658 -1.82401779 -2.58320052 -0.5457884  -1.15110021
 -2.30503753 -1.16460302 -0.91192422 -2.04514746 -0.98740457 -1.55321178
 -2.28420434 -0.99302933  1.37802117  1.62722741  0.72212007  0.71331559
  0.70447337 -0.3235882   0.32288351  0.37198524 -0.55810069 -0.72408153
 -1.11118124 -0.16153814  2.15858485 -0.54018124 -1.80529308  0.02739439
  1.12481962 -1.34236009 -1.08924331 -0.14301073  1.7626112   0.13786167
 -0.58175487  0.94115584  3.52013356  2.21238619  0.65072061 -1.13477253
 -0.55667242 -1.60361035  0.62926011  1.14438515 25.10878877]
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.0220369413514
gradient value of function right now is: [ 9.85885337e-08  1.59475266e-08  9.87044925e-08  9.82973572e-08
 -9.85890649e-08 -1.59476125e-08 -9.87050245e-08 -9.82978869e-08
  3.16900948e-09  3.19670491e-09  2.48974206e-12  9.66972443e-10
  1.98632047e-08  1.99351503e-08  3.17033780e-10  6.18520816e-09
  2.79741252e-09  2.81398469e-09  2.49989337e-11  8.62428191e-10
  7.94341166e-09  7.99197446e-09  6.68928038e-11  2.44757268e-09
 -8.54240793e-11  1.93196286e-11 -1.68559454e-11  1.22192697e-11
 -1.53207572e-10  2.27663553e-10  1.00237651e-10  1.42390970e-10
 -1.83662104e-10 -2.49192180e-10 -2.32424546e-10 -1.56167374e-10
  1.35487335e-09  1.57605659e-09  1.53495916e-09  1.00356668e-09
 -2.01890682e-10 -7.47133476e-11 -6.74636349e-11 -1.76032832e-10
 -1.43009209e-10  1.20821381e-10  1.64388771e-10 -7.06798435e-11
 -3.44168332e-10 -6.24770598e-12  2.72963474e-11 -2.75299141e-10
 -6.49418329e-11 -2.35426139e-11 -1.65694736e-11 -4.74108485e-11
 -3.25119636e-11 -5.42770537e-11  1.12187208e-11  1.42059975e-10
  4.74028643e-11  1.18952327e-10 -3.82648550e-12  6.27029473e-12
  4.46302135e+00]
supnorm grad right now is: 4.463021352646288
Weights right now are: 
[-2.26777359 -1.09011006 -2.76061541 -4.9183871   3.74191114  0.39615928
  4.07101378  4.85213155 -2.41773131 -2.85438316  0.23445006  0.21191838
  1.26735362  0.29815688  0.83681705  0.23698382 -2.36428367 -2.66311065
 -1.03202393 -1.38054714 -1.82585852 -2.58505043 -0.54581213 -1.15167133
 -2.30502243 -1.16460879 -0.911923   -2.04515101 -0.98738676 -1.55326848
 -2.28423706 -0.99306441  1.37806631  1.62728446  0.72217451  0.71335123
  0.7041588  -0.32394585  0.32253234  0.37175785 -0.55806058 -0.72406618
 -1.11116733 -0.16150356  2.15860962 -0.54021029 -1.80533186  0.02740284
  1.12488616 -1.34236154 -1.089252   -0.14295924  1.76262304  0.13786624
 -0.58175181  0.94116375  3.52014402  2.21239006  0.65071653 -1.13479909
 -0.55668554 -1.6036296   0.62926103  1.14438365 25.30265807]
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.63238131123
gradient value of function right now is: [ 9.06981558e-08  1.47735591e-08  9.08085794e-08  9.04339893e-08
 -9.06980295e-08 -1.47735384e-08 -9.08084530e-08 -9.04338634e-08
  2.95755146e-09  2.98040116e-09  1.40500151e-11  9.09130412e-10
  1.84337514e-08  1.84906980e-08  3.34933769e-10  5.76645474e-09
  2.59221203e-09  2.60569117e-09  3.07296457e-11  8.03748770e-10
  7.33671388e-09  7.37610304e-09  8.37485831e-11  2.27386803e-09
 -6.72863914e-11  2.04983503e-11 -9.29090939e-12  1.26931835e-11
 -9.47387944e-11  2.22254343e-10  1.18653199e-10  1.37852599e-10
 -1.80254525e-10 -2.33450564e-10 -2.20968319e-10 -1.45871897e-10
  1.25262496e-09  1.43520671e-09  1.40528774e-09  9.12361107e-10
 -1.69860691e-10 -6.28346752e-11 -5.65933289e-11 -1.47077588e-10
 -1.10808611e-10  1.15706924e-10  1.55464696e-10 -4.45729911e-11
 -2.84409333e-10  2.65912969e-12  3.24743713e-11 -2.23050717e-10
 -5.18379674e-11 -1.83992164e-11 -1.20236715e-11 -3.58237583e-11
 -3.68802031e-11 -2.99904766e-11  1.45215125e-11  1.15221239e-10
  4.99573164e-11  8.94751598e-11 -3.69323687e-12  5.82247602e-12
  1.49380847e+00]
supnorm grad right now is: 1.4938084737516355
Weights right now are: 
[-2.28741479 -1.09329694 -2.78028082 -4.937971    3.76155233  0.39934617
  4.09067919  4.87171544 -2.41837187 -2.85502849  0.23444619  0.21172105
  1.2633852   0.29417681  0.83674174  0.2357407  -2.36484412 -2.6636739
 -1.03203113 -1.38072121 -1.82744846 -2.58664858 -0.54583191 -1.15216495
 -2.30500869 -1.16461356 -0.91192155 -2.04515394 -0.98736952 -1.55331761
 -2.28426473 -0.99309475  1.37810583  1.62733479  0.72222241  0.71338261
  0.70388736 -0.32425518  0.32222884  0.37156144 -0.55802482 -0.72405305
 -1.11115548 -0.16147262  2.15863202 -0.54023573 -1.80536588  0.02741085
  1.12494549 -1.34236284 -1.08925964 -0.14291305  1.7626337   0.13786998
 -0.58174945  0.94117093  3.52015262  2.21239507  0.65071307 -1.13482256
 -0.5566969  -1.60364731  0.62926185  1.14438238 25.12954879]
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.317280585376
gradient value of function right now is: [ 8.60933827e-08  1.41132382e-08  8.62042857e-08  8.58470220e-08
 -8.60933951e-08 -1.41132405e-08 -8.62042979e-08 -8.58470342e-08
  2.86446903e-09  2.88166134e-09  3.25539044e-11  8.90694823e-10
  1.76221371e-08  1.76604572e-08  3.85769999e-10  5.55140683e-09
  2.48286757e-09  2.49267957e-09  4.16617154e-11  7.76722584e-10
  7.00940651e-09  7.03804510e-09  1.15419944e-10  2.19225627e-09
 -4.48560112e-11  2.42202494e-11  1.64240678e-12  1.46377016e-11
 -1.41122885e-11  2.30693274e-10  1.54811292e-10  1.41413249e-10
 -1.87590652e-10 -2.25628897e-10 -2.18801467e-10 -1.40338438e-10
  1.19907216e-09  1.33855562e-09  1.32283803e-09  8.48726692e-10
 -1.33853632e-10 -5.08799089e-11 -4.57047499e-11 -1.13783690e-10
 -7.11559146e-11  1.16272923e-10  1.53260542e-10 -8.92541976e-12
 -2.16178259e-10  1.52070091e-11  4.17794425e-11 -1.60965947e-10
 -3.61748702e-11 -1.28593946e-11 -6.96126828e-12 -2.11847830e-11
 -4.65711229e-11  3.93537617e-12  2.03042269e-11  8.44563986e-11
  5.71961812e-11  5.29340985e-11 -3.75001424e-12  5.66037765e-12
 -3.27189207e+00]
supnorm grad right now is: 3.2718920731392873
Weights right now are: 
[-2.30523299 -1.0962074  -2.79812089 -4.95573757  3.77937054  0.40225663
  4.10851926  4.889482   -2.4189553  -2.85561635  0.23444291  0.21154132
  1.25975041  0.2905311   0.83667352  0.2346019  -2.36535448 -2.66418686
 -1.03203753 -1.38087973 -1.82889095 -2.58809866 -0.54584939 -1.15261282
 -2.30499582 -1.16461775 -0.91191997 -2.04515651 -0.98735267 -1.55336222
 -2.28428938 -0.99312231  1.37814199  1.62738112  0.72226643  0.7134115
  0.70364115 -0.32453626  0.32195321  0.37138292 -0.55799177 -0.724041
 -1.11114473 -0.16144424  2.15865293 -0.54025881 -1.80539687  0.02741838
  1.1250006  -1.34236375 -1.0892665  -0.14287033  1.76264358  0.13787337
 -0.58174736  0.94117753  3.52016086  2.21239951  0.65070986 -1.13484475
 -0.55670745 -1.60366384  0.62926261  1.14438119 24.74587186]
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.539175584651
gradient value of function right now is: [ 7.71405421e-08  1.27174161e-08  7.72384837e-08  7.69206119e-08
 -7.71404883e-08 -1.27174071e-08 -7.72384298e-08 -7.69205582e-08
  2.56747691e-09  2.58400344e-09  2.50248921e-11  7.96663548e-10
  1.59242748e-08  1.59625042e-08  3.34310187e-10  5.01130499e-09
  2.22941068e-09  2.23892665e-09  3.47123041e-11  6.96384233e-10
  6.27335255e-09  6.30103244e-09  9.54961006e-11  1.95899942e-09
 -4.50383008e-11  2.01861026e-11 -1.35368989e-12  1.22514775e-11
 -2.96626742e-11  2.03350991e-10  1.30082264e-10  1.24963653e-10
 -1.66933417e-10 -2.04436357e-10 -1.97080787e-10 -1.27287019e-10
  1.07258890e-09  1.20515293e-09  1.18828696e-09  7.64578092e-10
 -1.26882303e-10 -4.70398490e-11 -4.20662813e-11 -1.08265878e-10
 -7.15010039e-11  1.03483574e-10  1.36998821e-10 -1.52316152e-11
 -2.06736147e-10  1.12601503e-11  3.56420407e-11 -1.56234970e-10
 -3.53988649e-11 -1.20586010e-11 -6.63419541e-12 -2.17624585e-11
 -3.96841426e-11 -2.94246371e-12  1.70698691e-11  8.15585795e-11
  4.96970199e-11  5.44402093e-11 -3.34796423e-12  5.04656342e-12
 -2.16131159e+00]
supnorm grad right now is: 2.1613115923098314
Weights right now are: 
[-2.32107586 -1.09881116 -2.81398354 -4.97153496  3.7952134   0.40486039
  4.1243819   4.9052794  -2.41947893 -2.85614357  0.23443884  0.2113794
  1.25649224  0.2872642   0.83660928  0.233579   -2.36581079 -2.66464528
 -1.03204392 -1.38102186 -1.83017673 -2.58939055 -0.54586691 -1.15301319
 -2.30498564 -1.1646216  -0.91191906 -2.04515888 -0.98734222 -1.55340263
 -2.28431326 -0.9931473   1.37817502  1.62742277  0.72230616  0.71343749
  0.7034215  -0.32478537  0.3217085   0.37122467 -0.55796458 -0.72403052
 -1.1111354  -0.16142111  2.15867008 -0.54027935 -1.80542444  0.02742365
  1.12504618 -1.34236475 -1.08927276 -0.14283561  1.7626517   0.13787627
 -0.58174563  0.94118273  3.52016834  2.21240277  0.65070676 -1.13486369
 -0.55671717 -1.60367774  0.6292633   1.14438018 24.82978436]
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.2663427164343
gradient value of function right now is: [ 6.58144284e-08  1.09073426e-08  6.58923914e-08  6.56254728e-08
 -6.58142976e-08 -1.09073208e-08 -6.58922604e-08 -6.56253423e-08
  2.15898710e-09  2.17728033e-09  4.06712347e-12  6.61544519e-10
  1.37186602e-08  1.37662279e-08  2.27973779e-10  4.28618266e-09
  1.89657061e-09  1.90745097e-09  1.84670315e-11  5.86842352e-10
  5.31607850e-09  5.34758278e-09  4.90504401e-11  1.64403982e-09
 -5.62845203e-11  1.21636373e-11 -1.13806526e-11  7.64766437e-12
 -9.42884030e-11  1.56699867e-10  7.33189563e-11  9.77350803e-11
 -1.30602053e-10 -1.75329061e-10 -1.64106326e-10 -1.09776227e-10
  9.06853753e-10  1.05083640e-09  1.02487581e-09  6.68774915e-10
 -1.33902471e-10 -4.76221847e-11 -4.24356186e-11 -1.16108914e-10
 -9.26088752e-11  8.32129480e-11  1.12779621e-10 -4.34573671e-11
 -2.26632480e-10 -1.69602407e-12  2.09742372e-11 -1.80145850e-10
 -4.22432247e-11 -1.38903343e-11 -8.96908069e-12 -3.01628433e-11
 -2.37826497e-11 -3.22285898e-11  8.86380941e-12  9.33439700e-11
  3.40435841e-11  7.64516681e-11 -2.69523742e-12  4.19220864e-12
  3.57686992e+00]
supnorm grad right now is: 3.576869919391484
Weights right now are: 
[-2.33563734 -1.1012175  -2.8285634  -4.98605499  3.80977488  0.40726673
  4.13896176  4.91979943 -2.41996363 -2.85663143  0.23443443  0.2112291
  1.25347547  0.28423993  0.83654753  0.23263015 -2.36623201 -2.66506832
 -1.03205024 -1.38115336 -1.83136036 -2.5905795  -0.54588423 -1.15338258
 -2.30497687 -1.16462524 -0.91191854 -2.04516111 -0.98733525 -1.55344051
 -2.28433676 -0.99317068  1.37820621  1.6274615   0.7223433   0.71346164
  0.70321922 -0.3250136   0.3214839   0.3710798  -0.55794044 -0.72402129
 -1.11112715 -0.16140061  2.15868463 -0.54029853 -1.80545005  0.02742736
  1.1250862  -1.34236623 -1.08927895 -0.1428054   1.76265871  0.13787871
 -0.58174424  0.94118709  3.52017571  2.21240432  0.65070372 -1.13487998
 -0.55672633 -1.60368935  0.62926393  1.14437925 25.21740278]
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.530949982161
gradient value of function right now is: [ 6.50515547e-08  1.08334029e-08  6.51340795e-08  6.48681607e-08
 -6.50515771e-08 -1.08334067e-08 -6.51341019e-08 -6.48681830e-08
  2.18106879e-09  2.19508291e-09  2.13705709e-11  6.77436562e-10
  1.36211117e-08  1.36536685e-08  2.86524169e-10  4.29052480e-09
  1.89136435e-09  1.89942400e-09  2.95151364e-11  5.91352769e-10
  5.29376707e-09  5.31709634e-09  8.07452675e-11  1.65466392e-09
 -3.85304489e-11  1.64621438e-11 -1.70675091e-12  9.98215742e-12
 -2.46769838e-11  1.72615132e-10  1.10622317e-10  1.06057659e-10
 -1.43764278e-10 -1.76116581e-10 -1.69772313e-10 -1.09646383e-10
  9.05116517e-10  1.01688643e-09  1.00270158e-09  6.45109995e-10
 -1.07567304e-10 -3.91250348e-11 -3.47489229e-11 -9.15664110e-11
 -6.04542355e-11  8.80584801e-11  1.16549953e-10 -1.26076689e-11
 -1.75094027e-10  9.94725890e-12  3.07017456e-11 -1.32145795e-10
 -2.99082446e-11 -9.63396780e-12 -4.94166305e-12 -1.82372955e-11
 -3.38616656e-11 -2.19571943e-12  1.46933487e-11  6.92104624e-11
  4.24943680e-11  4.60547610e-11 -2.86993665e-12  4.24969789e-12
 -2.21141965e+00]
supnorm grad right now is: 2.2114196531110073
Weights right now are: 
[-2.34902543 -1.10344107 -2.84196804 -4.99940505  3.82316298  0.4094903
  4.15236641  4.93314949 -2.42040842 -2.85707943  0.23443146  0.21109166
  1.25068022  0.28143676  0.83649358  0.23175234 -2.36661967 -2.66545787
 -1.03205539 -1.38127409 -1.83244647 -2.59167106 -0.54589821 -1.1537207
 -2.30496752 -1.16462829 -0.9119174  -2.04516299 -0.98732446 -1.55347466
 -2.28435639 -0.99319178  1.37823456  1.62749751  0.7223776   0.7134841
  0.70303376 -0.32522462  0.32127677  0.37094579 -0.5579162  -0.72401256
 -1.1111194  -0.16137986  2.15869973 -0.54031613 -1.80547363  0.02743238
  1.12512645 -1.34236724 -1.08928442 -0.1427744   1.76266587  0.13788103
 -0.58174292  0.94119176  3.52018205  2.21240716  0.65070109 -1.13489611
 -0.55673456 -1.60370113  0.62926452  1.14437837 24.83420438]
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.675554717213
gradient value of function right now is: [ 5.80555917e-08  9.71217592e-09  5.81265289e-08  5.78915739e-08
 -5.80556726e-08 -9.71218971e-09 -5.81266099e-08 -5.78916545e-08
  1.93328503e-09  1.94784238e-09  1.06734286e-11  5.96523638e-10
  1.22490006e-08  1.22854450e-08  2.28234206e-10  3.84386374e-09
  1.68676006e-09  1.69530145e-09  2.09354912e-11  5.24772031e-10
  4.70718931e-09  4.73182823e-09  5.63461122e-11  1.46384325e-09
 -4.31105794e-11  1.21394299e-11 -6.57694678e-12  7.48911975e-12
 -5.55979394e-11  1.46006745e-10  8.04520249e-11  9.04157756e-11
 -1.22850614e-10 -1.58186640e-10 -1.50023902e-10 -9.87811327e-11
  8.03759641e-10  9.18557269e-10  9.00239548e-10  5.83709114e-10
 -1.08573637e-10 -3.84787151e-11 -3.40930971e-11 -9.33649233e-11
 -6.95293682e-11  7.62065981e-11  1.02171314e-10 -2.61903756e-11
 -1.80863319e-10  3.40692462e-12  2.28656929e-11 -1.40882674e-10
 -3.26054575e-11 -1.02573728e-11 -5.90074054e-12 -2.19489941e-11
 -2.51571980e-11 -1.68419093e-11  1.03633188e-11  7.34379136e-11
  3.39267823e-11  5.59179464e-11 -2.49171709e-12  3.71921803e-12
  9.20094153e-01]
supnorm grad right now is: 0.9200941526166695
Weights right now are: 
[-2.36138934 -1.10550425 -2.85434719 -5.01173399  3.83552689  0.41155348
  4.16474556  4.94547843 -2.4208204  -2.85749443  0.23442877  0.21096436
  1.24808156  0.27883072  0.83644357  0.23093618 -2.36697866 -2.66581861
 -1.03206013 -1.38138589 -1.83344973 -2.5926794  -0.54591103 -1.15403304
 -2.30495876 -1.16463104 -0.9119163  -2.04516467 -0.98731432 -1.5535062
 -2.28437455 -0.99321127  1.37826097  1.62753105  0.72240955  0.71350503
  0.7028624  -0.32541962  0.32108534  0.37082194 -0.5578936  -0.72400463
 -1.11111233 -0.16136042  2.15871363 -0.5403325  -1.80549546  0.02743711
  1.12516367 -1.34236836 -1.08928957 -0.14274556  1.76267248  0.13788312
 -0.58174173  0.94119613  3.52018794  2.21240939  0.65069871 -1.13491066
 -0.55674205 -1.6037118   0.62926505  1.14437757 25.08492989]
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.265288522026
gradient value of function right now is: [ 5.70639031e-08  9.58631591e-09  5.71373056e-08  5.69050177e-08
 -5.70640640e-08 -9.58634318e-09 -5.71374667e-08 -5.69051781e-08
  1.93286966e-09  1.94438168e-09  2.23339057e-11  6.02520993e-10
  1.20890700e-08  1.21149377e-08  2.66278460e-10  3.81721391e-09
  1.66982629e-09  1.67637403e-09  2.82439231e-11  5.23619421e-10
  4.65375412e-09  4.67264181e-09  7.72149209e-11  1.45895343e-09
 -3.07668002e-11  1.48966596e-11 -3.85500971e-14  8.98087685e-12
 -7.92836556e-12  1.55479280e-10  1.04955799e-10  9.52676689e-11
 -1.30939058e-10 -1.57483721e-10 -1.52737339e-10 -9.79339161e-11
  7.96141832e-10  8.88326195e-10  8.78063959e-10  5.63183523e-10
 -8.96298647e-11 -3.24987131e-11 -2.86977807e-11 -7.57051487e-11
 -4.71769729e-11  7.87806160e-11  1.03779405e-10 -5.08608486e-12
 -1.44345632e-10  1.11356659e-11  2.91375876e-11 -1.07011049e-10
 -2.39741341e-11 -7.36226644e-12 -3.20627767e-12 -1.36753118e-11
 -3.18700021e-11  3.62123357e-12  1.41656383e-11  5.65967529e-11
  3.92676030e-11  3.50331828e-11 -2.58634500e-12  3.73801695e-12
 -3.46697580e+00]
supnorm grad right now is: 3.4669758017539025
Weights right now are: 
[-2.37280949 -1.10741819 -2.8657813  -5.02312195  3.84694705  0.41346742
  4.17617968  4.9568664  -2.42120119 -2.85787814  0.23442668  0.21084681
  1.245666    0.27640791  0.83639815  0.23017766 -2.36731073 -2.66615239
 -1.03206427 -1.38148926 -1.83437561 -2.59361021 -0.54592216 -1.15432112
 -2.30495012 -1.16463346 -0.91191501 -2.04516616 -0.98730321 -1.55353525
 -2.28439066 -0.99322922  1.3782854   1.62756238  0.72243932  0.71352459
  0.70270441 -0.32560011  0.32090838  0.37070725 -0.55787181 -0.72399722
 -1.1111058  -0.16134169  2.15872716 -0.54034765 -1.80551576  0.02744197
  1.12519962 -1.34236928 -1.08929434 -0.14271759  1.76267888  0.13788504
 -0.58174068  0.94120037  3.52019338  2.21241176  0.65069649 -1.13492478
 -0.55674912 -1.60372208  0.62926556  1.14437681 24.73550038]
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.6977419447867
gradient value of function right now is: [ 5.15425268e-08  8.69317289e-09  5.16064636e-08  5.13986227e-08
 -5.15428804e-08 -8.69323241e-09 -5.16068177e-08 -5.13989754e-08
  1.73355359e-09  1.74576320e-09  1.27923738e-11  5.36906394e-10
  1.09935109e-08  1.10233720e-08  2.16269779e-10  3.45840576e-09
  1.50669957e-09  1.51380120e-09  2.07711719e-11  4.70162150e-10
  4.18816610e-09  4.20858313e-09  5.60275197e-11  1.30641965e-09
 -3.54170106e-11  1.12611073e-11 -4.40818158e-12  6.88902679e-12
 -3.65838543e-11  1.33405151e-10  7.88719711e-11  8.23335705e-11
 -1.13375787e-10 -1.42986412e-10 -1.36524907e-10 -8.91793101e-11
  7.15566945e-10  8.11901705e-10  7.97743909e-10  5.15575735e-10
 -9.20841652e-11 -3.24228988e-11 -2.85819318e-11 -7.87364397e-11
 -5.61078055e-11  6.91202345e-11  9.21385058e-11 -1.76312599e-11
 -1.51908583e-10  5.38644922e-12  2.24407931e-11 -1.16829866e-10
 -2.67964930e-11 -8.08218437e-12 -4.19329794e-12 -1.73197517e-11
 -2.43413947e-11 -9.44510236e-12  1.04059124e-11  6.12240372e-11
  3.19467706e-11  4.43829727e-11 -2.26851572e-12  3.31427727e-12
 -4.52281543e-01]
supnorm grad right now is: 0.45228154262655434
Weights right now are: 
[-2.38352204 -1.10922113 -2.87650718 -5.03380453  3.8576596   0.41527036
  4.18690556  4.96754898 -2.4215613  -2.85824071  0.23442387  0.21073525
  1.24338814  0.27412405  0.83635315  0.2294612  -2.36762366 -2.66646675
 -1.03206866 -1.38158691 -1.83524643 -2.59448514 -0.54593404 -1.15459278
 -2.30494303 -1.16463582 -0.91191419 -2.04516761 -0.9872963  -1.55356287
 -2.28440714 -0.99324631  1.37830882  1.6275919   0.72246749  0.71354301
  0.70255556 -0.32576886  0.32074258  0.37060004 -0.5578532  -0.72399044
 -1.11109976 -0.16132578  2.15873876 -0.54036185 -1.80553471  0.0274456
  1.12523047 -1.34237028 -1.08929882 -0.14269393  1.76268437  0.13788679
 -0.58173972  0.94120391  3.52019838  2.21241393  0.65069435 -1.13493711
 -0.55675569 -1.60373124  0.62926603  1.14437615 24.96583577]
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.9393724068213
gradient value of function right now is: [ 5.08904967e-08  8.61502770e-09  5.09567222e-08  5.07503288e-08
 -5.08903201e-08 -8.61499768e-09 -5.09565454e-08 -5.07501527e-08
  1.73893130e-09  1.74858135e-09  2.26981675e-11  5.43718921e-10
  1.08923865e-08  1.09133281e-08  2.49060722e-10  3.44636372e-09
  1.49749066e-09  1.50292009e-09  2.70006136e-11  4.70739352e-10
  4.15797708e-09  4.17359394e-09  7.37379313e-11  1.30679946e-09
 -2.50971641e-11  1.36154501e-11  1.07407061e-12  8.17393524e-12
  3.55049172e-12  1.41723556e-10  9.96643189e-11  8.66640232e-11
 -1.20544216e-10 -1.42768363e-10 -1.39175709e-10 -8.87065428e-11
  7.11722621e-10  7.89483634e-10  7.81980093e-10  5.00262895e-10
 -7.61942166e-11 -2.76285037e-11 -2.42575357e-11 -6.38386743e-11
 -3.75422685e-11  7.13746723e-11  9.36583359e-11  6.54320007e-14
 -1.21504474e-10  1.17016635e-11  2.76064081e-11 -8.84468531e-11
 -1.96153417e-11 -5.76815985e-12 -2.03319625e-12 -1.03865117e-11
 -3.02021555e-11  7.84406822e-12  1.36229458e-11  4.73465544e-11
  3.65363445e-11  2.70934329e-11 -2.35350890e-12  3.33815361e-12
 -4.49942665e+00]
supnorm grad right now is: 4.49942664721815
Weights right now are: 
[-2.39354517 -1.11091459 -2.88654276 -5.04379972  3.86768273  0.41696382
  4.19694115  4.97754418 -2.42189901 -2.85858073  0.23442128  0.21063065
  1.24124525  0.27197551  0.83631099  0.2287873  -2.3679171  -2.66676154
 -1.03207275 -1.38167847 -1.83606139 -2.59530397 -0.54594506 -1.15484695
 -2.30493633 -1.16463798 -0.91191337 -2.04516894 -0.98728972 -1.55358866
 -2.28442254 -0.99326226  1.37833087  1.62761968  0.72249401  0.71356036
  0.70241619 -0.32592686  0.32058733  0.37049963 -0.5578357  -0.72398411
 -1.11109415 -0.16131079  2.1587495  -0.54037522 -1.80555249  0.02744907
  1.12525939 -1.3423713  -1.08930308 -0.14267165  1.76268949  0.1378884
 -0.58173884  0.94120725  3.52020285  2.21241589  0.65069243 -1.13494895
 -0.55676164 -1.60374     0.62926645  1.14437553 24.65187695]
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.550151750322
gradient value of function right now is: [ 4.66675489e-08  7.92775761e-09  4.67264685e-08  4.65387361e-08
 -4.66674889e-08 -7.92774770e-09 -4.67264083e-08 -4.65386761e-08
  1.58519195e-09  1.59549598e-09  1.50983837e-11  4.93009256e-10
  1.00475428e-08  1.00718727e-08  2.10043327e-10  3.16950189e-09
  1.37197277e-09  1.37789749e-09  2.11193045e-11  4.29552592e-10
  3.80092465e-09  3.81791263e-09  5.71077901e-11  1.18966483e-09
 -2.90260066e-11  1.08348071e-11 -2.37789738e-12  6.56618169e-12
 -1.96047102e-11  1.24838086e-10  7.93574736e-11  7.67300871e-11
 -1.06942647e-10 -1.31641674e-10 -1.26709250e-10 -8.19680610e-11
  6.49812137e-10  7.31034301e-10  7.20504263e-10  4.63796114e-10
 -7.88496842e-11 -2.75508072e-11 -2.41270257e-11 -6.68852987e-11
 -4.46955407e-11  6.40982630e-11  8.49000628e-11 -9.76217423e-12
 -1.28394268e-10  7.37926356e-12  2.26024845e-11 -9.69615569e-11
 -2.19505635e-11 -6.27977677e-12 -2.73791660e-12 -1.33198040e-11
 -2.45149208e-11 -2.18479744e-12  1.07758366e-11  5.10374970e-11
  3.10442896e-11  3.42399738e-11 -2.11981317e-12  3.03540068e-12
 -2.06085488e+00]
supnorm grad right now is: 2.0608548791339354
Weights right now are: 
[-2.40291854 -1.11250382 -2.89592768 -5.05314704  3.87705611  0.41855305
  4.20632607  4.9868915  -2.42221485 -2.85889888  0.23441919  0.2105329
  1.23923092  0.26995549  0.83627203  0.22815363 -2.36819173 -2.66703751
 -1.03207639 -1.38176414 -1.83682266 -2.59606909 -0.54595483 -1.15508434
 -2.30492955 -1.16463994 -0.9119124  -2.04517014 -0.98728208 -1.55361286
 -2.28443653 -0.9932772   1.37835159  1.62764599  0.72251908  0.71357677
  0.70228619 -0.32607475  0.32044213  0.37040569 -0.5578184  -0.72397823
 -1.11108898 -0.16129594  2.15876008 -0.54038785 -1.80556934  0.02745263
  1.12528784 -1.34237228 -1.0893072  -0.1426496   1.76269452  0.13788984
 -0.58173812  0.94121055  3.52020717  2.21241781  0.65069053 -1.13496023
 -0.55676747 -1.60374824  0.62926687  1.14437493 24.82337052]
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.7062732311174
gradient value of function right now is: [ 4.30495893e-08  7.33765252e-09  4.31025834e-08  4.29306044e-08
 -4.30493300e-08 -7.33760836e-09 -4.31023237e-08 -4.29303457e-08
  1.45569474e-09  1.46623335e-09  9.66875897e-12  4.50732308e-10
  9.31954667e-09  9.34577917e-09  1.79676427e-10  2.93243045e-09
  1.26509129e-09  1.27123590e-09  1.67440791e-11  3.94769306e-10
  3.49753605e-09  3.51511143e-09  4.47883375e-11  1.09095928e-09
 -3.11301654e-11  8.69066904e-12 -4.73965827e-12  5.32922114e-12
 -3.50376581e-11  1.11056140e-10  6.39412740e-11  6.86159962e-11
 -9.58201528e-11 -1.21993316e-10 -1.16133721e-10 -7.61150545e-11
  5.97430627e-10  6.79897321e-10  6.67348966e-10  4.31841402e-10
 -7.90806126e-11 -2.72069587e-11 -2.38091886e-11 -6.75964703e-11
 -4.91452276e-11  5.79424439e-11  7.73883116e-11 -1.66407138e-11
 -1.30852758e-10  4.05075663e-12  1.85519500e-11 -1.01071449e-10
 -2.32462454e-11 -6.61674765e-12 -3.26691916e-12 -1.51778672e-11
 -1.98937393e-11 -9.83670793e-12  8.49784219e-12  5.30440537e-11
  2.65161867e-11  3.92336507e-11 -1.91774987e-12  2.75771260e-12
  8.59366620e-02]
supnorm grad right now is: 0.0859366619534996
Weights right now are: 
[-2.41180107 -1.11401513 -2.90482126 -5.06200501  3.88593864  0.42006436
  4.21521964  4.99574947 -2.42251569 -2.85920176  0.23441682  0.21043962
  1.23731336  0.26803291  0.83623406  0.22755001 -2.36845282 -2.66729978
 -1.03208007 -1.38184568 -1.83754523 -2.59679505 -0.54596472 -1.15530992
 -2.30492362 -1.16464181 -0.91191163 -2.04517128 -0.98727646 -1.55363589
 -2.28445034 -0.99329142  1.37837146  1.627671    0.72254297  0.71359238
  0.70216263 -0.32621473  0.32030455  0.37031678 -0.55780294 -0.72397267
 -1.11108409 -0.16128275  2.15876961 -0.54039981 -1.80558525  0.02745562
  1.12531347 -1.34237322 -1.08931105 -0.14262993  1.76269906  0.1378912
 -0.58173743  0.94121349  3.52021108  2.21241984  0.6506888  -1.13497089
 -0.55677283 -1.60375616  0.62926725  1.14437437 25.00664102]
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.3162220684612
gradient value of function right now is: [ 3.92838546e-08  6.71739136e-09  3.93303097e-08  3.91748211e-08
 -3.92841315e-08 -6.71743857e-09 -3.93305871e-08 -3.91750973e-08
  1.31724463e-09  1.32834078e-09  2.77127214e-12  4.04935923e-10
  8.55336261e-09  8.58266065e-09  1.43593354e-10  2.68038953e-09
  1.15243365e-09  1.15901022e-09  1.13915155e-11  3.57680919e-10
  3.17897895e-09  3.19774172e-09  2.97468841e-11  9.86116510e-10
 -3.45350052e-11  6.20132243e-12 -7.81982676e-12  3.90144015e-12
 -5.60682530e-11  9.55486006e-11  4.52740933e-11  5.95537871e-11
 -8.30867601e-11 -1.11655754e-10 -1.04493470e-10 -6.98921203e-11
  5.42270358e-10  6.28146923e-10  6.12725898e-10  3.99700968e-10
 -8.12661005e-11 -2.74148648e-11 -2.39691424e-11 -7.00656458e-11
 -5.58782848e-11  5.10975473e-11  6.91913685e-11 -2.57809879e-11
 -1.37076138e-10 -2.35653225e-13  1.36722509e-11 -1.08630412e-10
 -2.53992137e-11 -7.26147693e-12 -4.09802205e-12 -1.78628483e-11
 -1.48765795e-11 -1.88630174e-11  5.81512278e-12  5.67029244e-11
  2.13658258e-11  4.61429473e-11 -1.69667266e-12  2.48787141e-12
  3.35805781e+00]
supnorm grad right now is: 3.358057810507335
Weights right now are: 
[-2.42026128 -1.11545928 -2.91329209 -5.07044196  3.89439884  0.42150851
  4.22369047  5.00418641 -2.42280364 -2.85949157  0.23441421  0.21035011
  1.23547933  0.26619433  0.83619658  0.22697184 -2.36870216 -2.66755021
 -1.03208379 -1.3819237  -1.83823425 -2.59748717 -0.54597475 -1.15552545
 -2.30491827 -1.16464366 -0.91191105 -2.04517241 -0.98727231 -1.55365824
 -2.28446416 -0.99330521  1.37839075  1.62769504  0.722566    0.71360737
  0.70204481 -0.32634769  0.32017369  0.37023235 -0.55778868 -0.72396744
 -1.11107953 -0.1612708   2.15877826 -0.54041124 -1.8056005   0.0274578
  1.12533721 -1.34237413 -1.08931482 -0.14261212  1.7627032   0.13789244
 -0.58173685  0.941216    3.52021544  2.21242097  0.650687   -1.13498091
 -0.55677827 -1.60376333  0.62926764  1.14437383 25.21205232]
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.6848984608714
gradient value of function right now is: [ 3.85899708e-08  6.61844516e-09  3.86370394e-08  3.84838405e-08
 -3.85898156e-08 -6.61841880e-09 -3.86368838e-08 -3.84836856e-08
  1.30726314e-09  1.31705778e-09  7.40700565e-12  4.04355111e-10
  8.42853063e-09  8.45335519e-09  1.57970010e-10  2.65102431e-09
  1.13700971e-09  1.14274139e-09  1.42202825e-11  3.54563774e-10
  3.13332948e-09  3.14967277e-09  3.77656175e-11  9.76665820e-10
 -2.94180339e-11  7.21568026e-12 -5.20805239e-12  4.44645009e-12
 -3.66375577e-11  9.86195381e-11  5.46913001e-11  6.10485656e-11
 -8.58667857e-11 -1.10668759e-10 -1.04937524e-10 -6.90980194e-11
  5.35042765e-10  6.11331963e-10  5.99194411e-10  3.88441835e-10
 -7.30716033e-11 -2.47913011e-11 -2.16087144e-11 -6.25223437e-11
 -4.66143719e-11  5.17729892e-11  6.93741670e-11 -1.72269124e-11
 -1.21455660e-10  2.86983446e-12  1.60885517e-11 -9.43573676e-11
 -2.17945766e-11 -6.03005153e-12 -2.96071685e-12 -1.44624106e-11
 -1.72196772e-11 -1.09842458e-11  7.29395445e-12  4.95430788e-11
  2.33518709e-11  3.75562899e-11 -1.72332779e-12  2.45836486e-12
  7.78801513e-01]
supnorm grad right now is: 0.7788015128360413
Weights right now are: 
[-2.42825407 -1.11682789 -2.92129461 -5.07841271  3.90239163  0.42287712
  4.23169299  5.01215716 -2.423074   -2.85976395  0.23441272  0.21026656
  1.23373739  0.26444729  0.83616429  0.22642435 -2.36893747 -2.6677867
 -1.0320867  -1.38199703 -1.83888321 -2.59813949 -0.54598246 -1.15572758
 -2.30491221 -1.16464513 -0.91190996 -2.04517332 -0.98726468 -1.55367843
 -2.2844753  -0.99331775  1.37840836  1.6277178   0.72258755  0.7136216
  0.70193394 -0.32647448  0.32004949  0.37015173 -0.55777366 -0.72396226
 -1.11107498 -0.16125786  2.15878797 -0.54042186 -1.80561473  0.02746161
  1.12536217 -1.34237468 -1.08931805 -0.14259257  1.76270772  0.13789374
 -0.58173617  0.94121907  3.52021866  2.21242356  0.65068557 -1.13499118
 -0.55678288 -1.60377123  0.62926798  1.14437334 25.03547329]
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3591.910886385957
gradient value of function right now is: [ 3.50470346e-08  6.02843178e-09  3.50877245e-08  3.49500345e-08
 -3.50472956e-08 -6.02847664e-09 -3.50879859e-08 -3.49502948e-08
  1.17434352e-09  1.18480665e-09  1.54406297e-13  3.59991387e-10
  7.69768633e-09  7.72600796e-09  1.20792949e-10  2.40869836e-09
  1.02992453e-09  1.03616248e-09  8.67522011e-12  3.19009863e-10
  2.83197082e-09  2.84971315e-09  2.22273761e-11  8.76631733e-10
 -3.32233186e-11  4.68450109e-12 -8.48661341e-12  3.00260667e-12
 -5.93227425e-11  8.30472087e-11  3.52879956e-11  5.19991398e-11
 -7.29459830e-11 -1.00594553e-10 -9.33854201e-11 -6.30659817e-11
  4.82845599e-10  5.63654167e-10  5.48301514e-10  3.58968391e-10
 -7.59755647e-11 -2.54351666e-11 -2.21856747e-11 -6.56590978e-11
 -5.43776240e-11  4.49765270e-11  6.12999246e-11 -2.73196221e-11
 -1.29246341e-10 -1.81121466e-12  1.09025385e-11 -1.03335262e-10
 -2.43389529e-11 -6.87832387e-12 -3.98958973e-12 -1.75283422e-11
 -1.17804506e-11 -2.13055732e-11  4.39798390e-12  5.41586850e-11
  1.78816212e-11  4.57306996e-11 -1.49498097e-12  2.18900678e-12
  4.83521850e+00]
supnorm grad right now is: 4.8352185043821665
Weights right now are: 
[-2.43579071 -1.11812211 -2.92884059 -5.08592872  3.90992827  0.42417134
  4.23923896  5.01967317 -2.42333054 -2.86002228  0.23441082  0.21018695
  1.23208916  0.2627946   0.83613198  0.22590495 -2.36915996 -2.66801023
 -1.03208975 -1.38206658 -1.8394961  -2.59875534 -0.54599063 -1.1559191
 -2.30490694 -1.16464662 -0.91190916 -2.04517424 -0.98725929 -1.55369806
 -2.28448683 -0.99332987  1.37842547  1.6277395   0.72260822  0.71363513
  0.70182925 -0.32659338  0.31993274  0.37007627 -0.55776017 -0.72395756
 -1.11107088 -0.16124637  2.15879642 -0.54043207 -1.80562838  0.02746433
  1.12538459 -1.34237541 -1.08932134 -0.14257536  1.7627117   0.13789483
 -0.58173568  0.94122164  3.52022206  2.21242543  0.65068405 -1.13500049
 -0.55678756 -1.60377824  0.62926832  1.14437287 25.35376034]
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.7036609676293
gradient value of function right now is: [ 3.57084550e-08  6.15836954e-09  3.57524869e-08  3.56110478e-08
 -3.57082759e-08 -6.15833862e-09 -3.57523075e-08 -3.56108691e-08
  1.21782980e-09  1.22653535e-09  8.53100797e-12  3.77697371e-10
  7.85622413e-09  7.87790930e-09  1.53191978e-10  2.47536483e-09
  1.05636869e-09  1.06143092e-09  1.42633676e-11  3.30120947e-10
  2.90368511e-09  2.91808860e-09  3.79605158e-11  9.07054551e-10
 -2.58080916e-11  6.92946476e-12 -4.09601675e-12  4.23951689e-12
 -2.74324037e-11  9.31936209e-11  5.44167627e-11  5.75397149e-11
 -8.16857614e-11 -1.03664380e-10 -9.87928370e-11 -6.46632068e-11
  4.96052409e-10  5.63808064e-10  5.53655264e-10  3.58052996e-10
 -6.54594620e-11 -2.20338320e-11 -1.91234188e-11 -5.57886902e-11
 -4.02253784e-11  4.86756441e-11  6.49392956e-11 -1.30385839e-11
 -1.07989966e-10  3.90522653e-12  1.60127967e-11 -8.31434846e-11
 -1.90682952e-11 -5.05574389e-12 -2.20781113e-12 -1.22794163e-11
 -1.69670729e-11 -7.31287428e-12  7.37275879e-12  4.37257654e-11
  2.25611870e-11  3.19660929e-11 -1.62452778e-12  2.28425762e-12
 -2.53603173e-01]
supnorm grad right now is: 0.2536031725735553
Weights right now are: 
[-2.44295392 -1.1193556  -2.9360127  -5.09307239  3.91709147  0.42540483
  4.24641108  5.02681684 -2.42357516 -2.86026858  0.23440887  0.21011094
  1.23051695  0.26121821  0.83610045  0.22540902 -2.36937183 -2.66822307
 -1.03209277 -1.38213288 -1.84007897 -2.59934101 -0.54599869 -1.15610145
 -2.304902   -1.16464809 -0.91190847 -2.04517514 -0.98725465 -1.55371704
 -2.28449821 -0.99334156  1.378442    1.62776031  0.7226281   0.71364809
  0.7017297  -0.32670615  0.31982187  0.37000471 -0.5577474  -0.72395316
 -1.1110671  -0.16123563  2.15880418 -0.54044188 -1.80564149  0.02746647
  1.12540579 -1.34237621 -1.08932462 -0.14255933  1.76271541  0.13789581
 -0.58173529  0.94122392  3.52022577  2.21242661  0.65068248 -1.13500927
 -0.55679231 -1.60378453  0.62926866  1.1443724  24.94083628]
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.5381944143305
gradient value of function right now is: [ 3.50186245e-08  6.05522628e-09  3.50627712e-08  3.49237967e-08
 -3.50183382e-08 -6.05517684e-09 -3.50624845e-08 -3.49235111e-08
  1.20365227e-09  1.21145819e-09  1.15651846e-11  3.74979871e-10
  7.72647496e-09  7.74505460e-09  1.62097044e-10  2.44123320e-09
  1.03952773e-09  1.04400723e-09  1.60639027e-11  3.25995560e-10
  2.85477818e-09  2.86751949e-09  4.30402681e-11  8.94970730e-10
 -2.23245959e-11  7.56998624e-12 -2.34521181e-12  4.57770947e-12
 -1.43979948e-11  9.47673093e-11  6.04451402e-11  5.82269005e-11
 -8.31821859e-11 -1.02430620e-10 -9.85913823e-11 -6.37696477e-11
  4.88085859e-10  5.49006708e-10  5.41142780e-10  3.48278017e-10
 -5.97301247e-11 -2.01256495e-11 -1.73843628e-11 -5.04687895e-11
 -3.36783930e-11  4.88734033e-11  6.47077128e-11 -7.08236655e-12
 -9.70733615e-11  5.99715659e-12  1.76033541e-11 -7.31501291e-11
 -1.65221430e-11 -4.18668378e-12 -1.41608020e-12 -9.88355563e-12
 -1.87749266e-11 -1.34776056e-12  8.38291578e-12  3.86772543e-11
  2.38770904e-11  2.57927665e-11 -1.63587954e-12  2.27185782e-12
 -2.16903261e+00]
supnorm grad right now is: 2.169032605398475
Weights right now are: 
[-2.44991851 -1.1205581  -2.94298581 -5.10001798  3.92405607  0.42660733
  4.25338419  5.03376244 -2.42381235 -2.86050757  0.23440743  0.21003748
  1.22898191  0.2596787   0.83607101  0.2249256  -2.36957781 -2.6684301
 -1.03209542 -1.38219719 -1.84064481 -2.59990981 -0.54600572 -1.15627803
 -2.30489664 -1.16464942 -0.91190756 -2.04517594 -0.98724833 -1.5537352
 -2.28450856 -0.99335275  1.37845792  1.6277806   0.72264744  0.71366074
  0.70163307 -0.32681624  0.31971379  0.36993481 -0.55773402 -0.72394887
 -1.1110634  -0.16122421  2.15881221 -0.54045146 -1.80565424  0.02746918
  1.1254277  -1.34237701 -1.08932782 -0.14254237  1.76271924  0.13789678
 -0.58173488  0.94122641  3.5202293   2.21242759  0.65068101 -1.13501795
 -0.55679677 -1.60379067  0.62926897  1.14437193 24.79296298]
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.6665306079776
gradient value of function right now is: [ 3.22893840e-08  5.59797259e-09  3.23285343e-08  3.22015092e-08
 -3.22896706e-08 -5.59802196e-09 -3.23288214e-08 -3.22017952e-08
  1.10012306e-09  1.10852597e-09  5.64678376e-12  3.40313615e-10
  7.15926917e-09  7.18084759e-09  1.32229089e-10  2.25277906e-09
  9.56478346e-10  9.61400949e-10  1.15840632e-11  2.98348802e-10
  2.62173801e-09  2.63570408e-09  3.05155810e-11  8.17405421e-10
 -2.55587072e-11  5.55116825e-12 -5.02861126e-12  3.42844153e-12
 -3.31582498e-11  8.24774244e-11  4.48148162e-11  5.10987034e-11
 -7.28875147e-11 -9.46113608e-11 -8.95156209e-11 -5.90914374e-11
  4.47613086e-10  5.12566218e-10  5.02008791e-10  3.25742783e-10
 -6.24919035e-11 -2.07174439e-11 -1.79248031e-11 -5.34371315e-11
 -4.03073127e-11  4.35500517e-11  5.84381629e-11 -1.54669723e-11
 -1.03994706e-10  2.23615303e-12  1.34664869e-11 -8.10088279e-11
 -1.87271731e-11 -4.88385845e-12 -2.24273425e-12 -1.24869459e-11
 -1.42414811e-11 -1.01206672e-11  6.04832723e-12  4.25650704e-11
  1.95445168e-11  3.26050718e-11 -1.46136379e-12  2.05084993e-12
  1.12016876e+00]
supnorm grad right now is: 1.1201687593866745
Weights right now are: 
[-2.45663977 -1.12172173 -2.9497154  -5.106721    3.93077733  0.42777096
  4.26011378  5.04046545 -2.42404242 -2.86073924  0.2344057   0.20996605
  1.22749553  0.25818835  0.83604175  0.22445701 -2.36977716 -2.66863038
 -1.03209818 -1.38225954 -1.84119177 -2.60045941 -0.54601306 -1.15644903
 -2.30489192 -1.16465072 -0.91190683 -2.04517674 -0.98724365 -1.55375281
 -2.28451899 -0.99336364  1.37847343  1.62780023  0.72266615  0.713673
  0.7015396  -0.32692235  0.31960958  0.3698674  -0.55772195 -0.72394473
 -1.11105981 -0.16121398  2.15881969 -0.54046057 -1.80566642  0.0274715
  1.12544764 -1.34237772 -1.0893308  -0.14252712  1.76272276  0.13789773
 -0.58173446  0.94122864  3.52023263  2.21242873  0.65067961 -1.13502605
 -0.55680108 -1.60379657  0.62926928  1.14437151 25.12538023]
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.6219881729917
gradient value of function right now is: [ 3.20322961e-08  5.56699400e-09  3.20723504e-08  3.19458819e-08
 -3.20319557e-08 -5.56693507e-09 -3.20720094e-08 -3.19455424e-08
  1.10231749e-09  1.10970537e-09  9.63676982e-12  3.43059955e-10
  7.11880211e-09  7.13673804e-09  1.45829824e-10  2.24817270e-09
  9.52799011e-10  9.57056527e-10  1.40986687e-11  2.98585552e-10
  2.60985628e-09  2.62193527e-09  3.75822175e-11  8.17583097e-10
 -2.15124936e-11  6.49968661e-12 -2.84572227e-12  3.94324817e-12
 -1.70100120e-11  8.59077501e-11  5.32979194e-11  5.28717541e-11
 -7.59910483e-11 -9.45589402e-11 -9.07015895e-11 -5.89118420e-11
  4.46127548e-10  5.03598838e-10  4.95749789e-10  3.19606578e-10
 -5.61800036e-11 -1.87272186e-11 -1.61320028e-11 -4.75534254e-11
 -3.26723194e-11  4.45579243e-11  5.91345603e-11 -8.23759678e-12
 -9.17588367e-11  4.91990794e-12  1.56708563e-11 -6.96482934e-11
 -1.58113387e-11 -3.92781956e-12 -1.35573564e-12 -9.69151166e-12
 -1.65902624e-11 -2.97823488e-12  7.37332766e-12  3.68607964e-11
  2.14189741e-11  2.54134471e-11 -1.49365568e-12  2.06551800e-12
 -1.51604172e+00]
supnorm grad right now is: 1.5160417184399082
Weights right now are: 
[-2.46313292 -1.12284872 -2.95621654 -5.11319655  3.93727048  0.42889795
  4.26661493  5.046941   -2.42426481 -2.86096319  0.23440412  0.20989705
  1.2260544   0.25674327  0.83601371  0.22400284 -2.36996995 -2.66882408
 -1.03210079 -1.3823198  -1.84172003 -2.60099027 -0.54601998 -1.1566141
 -2.30488724 -1.16465193 -0.91190605 -2.04517749 -0.98723874 -1.55376974
 -2.28452889 -0.99337411  1.37848842  1.62781929  0.72268429  0.71368488
  0.70144931 -0.32702497  0.31950881  0.36980222 -0.55771011 -0.72394071
 -1.11105633 -0.16120392  2.15882703 -0.54046943 -1.80567822  0.0274739
  1.12546722 -1.34237839 -1.08933367 -0.14251207  1.76272622  0.13789865
 -0.58173407  0.94123086  3.52023576  2.21242995  0.65067829 -1.13503409
 -0.55680512 -1.60380249  0.62926957  1.1443711  24.87484961]
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.6580639299805
gradient value of function right now is: [ 2.98256833e-08  5.19613934e-09  2.98617558e-08  2.97448970e-08
 -2.98261205e-08 -5.19621550e-09 -2.98621937e-08 -2.97453331e-08
  1.01881062e-09  1.02665246e-09  5.00485550e-12  3.15181824e-10
  6.65789220e-09  6.67814458e-09  1.22218312e-10  2.09545550e-09
  8.85610823e-10  8.90206612e-10  1.05807250e-11  2.76283605e-10
  2.42170765e-09  2.43471751e-09  2.77720656e-11  7.55141618e-10
 -2.39773567e-11  4.92629701e-12 -4.91502470e-12  3.04562685e-12
 -3.15845235e-11  7.61787006e-11  4.10385438e-11  4.72121859e-11
 -6.77745172e-11 -8.82387987e-11 -8.34093047e-11 -5.51181933e-11
  4.13439632e-10  4.73864564e-10  4.63958672e-10  3.01166721e-10
 -5.82472120e-11 -1.91141527e-11 -1.64840951e-11 -4.98001516e-11
 -3.77445602e-11  4.03325453e-11  5.41491365e-11 -1.47057983e-11
 -9.69753649e-11  2.01340937e-12  1.24512915e-11 -7.56335706e-11
 -1.74905217e-11 -4.43737926e-12 -1.96861165e-12 -1.16875775e-11
 -1.30557476e-11 -9.77952780e-12  5.56330392e-12  3.97540941e-11
  1.80466435e-11  3.05995021e-11 -1.35740690e-12  1.89012968e-12
  1.23410488e+00]
supnorm grad right now is: 1.234104883413711
Weights right now are: 
[-2.4693437  -1.1239293  -2.96243491 -5.11939051  3.94348126  0.42997853
  4.2728333   5.05313496 -2.4244774  -2.86117737  0.23440282  0.20983117
  1.22467106  0.25535596  0.83598727  0.22356703 -2.37015442 -2.66900948
 -1.03210317 -1.38237743 -1.8422249  -2.60149775 -0.54602626 -1.15677174
 -2.30488247 -1.16465306 -0.9119052  -2.04517818 -0.98723318 -1.55378593
 -2.2845381  -0.99338412  1.37850275  1.62783761  0.72270172  0.71369633
  0.70136308 -0.3271233   0.31941234  0.36973973 -0.55769825 -0.72393688
 -1.11105303 -0.16119382  2.15883429 -0.54047789 -1.80568953  0.02747637
  1.12548668 -1.34237904 -1.08933645 -0.14249704  1.76272964  0.13789952
 -0.5817337   0.94123307  3.52023899  2.2124307   0.65067695 -1.13504175
 -0.55680915 -1.60380792  0.62926986  1.14437069 25.10842758]
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.6547583574256
gradient value of function right now is: [ 2.87461668e-08  5.01956248e-09  2.87809095e-08  2.86684837e-08
 -2.87458113e-08 -5.01950037e-09 -2.87805534e-08 -2.86681291e-08
  9.83225420e-10  9.90805874e-10  4.78368494e-12  3.04208199e-10
  6.43713894e-09  6.45675378e-09  1.18020087e-10  2.02626663e-09
  8.54532325e-10  8.58974841e-10  1.01789076e-11  2.66622362e-10
  2.33414878e-09  2.34671186e-09  2.66791477e-11  7.27930256e-10
 -2.32207701e-11  4.67120434e-12 -4.82861135e-12  2.88841800e-12
 -3.06539708e-11  7.34694556e-11  3.95098158e-11  4.55359188e-11
 -6.55680101e-11 -8.54285380e-11 -8.07356025e-11 -5.33644111e-11
  3.98491450e-10  4.56825148e-10  4.47245336e-10  2.90342477e-10
 -5.62907816e-11 -1.83914226e-11 -1.58353241e-11 -4.81127412e-11
 -3.65050969e-11  3.89344080e-11  5.22750392e-11 -1.42620822e-11
 -9.37186477e-11  1.94816785e-12  1.20327090e-11 -7.31039459e-11
 -1.69041692e-11 -4.23389193e-12 -1.84258709e-12 -1.12943795e-11
 -1.25854662e-11 -9.49590677e-12  5.36852977e-12  3.84464089e-11
  1.74146649e-11  2.96146315e-11 -1.31188973e-12  1.82102071e-12
  1.26558099e+00]
supnorm grad right now is: 1.2655809901468167
Weights right now are: 
[-2.47528411 -1.12496529 -2.96838261 -5.12531488  3.94942167  0.43101452
  4.27878099  5.05905934 -2.42468122 -2.86138267  0.23440148  0.20976796
  1.22334374  0.25402492  0.83596176  0.22314872 -2.37033116 -2.66918707
 -1.03210549 -1.38243267 -1.84270804 -2.60198332 -0.54603239 -1.15692267
 -2.30487804 -1.16465412 -0.91190441 -2.04517883 -0.98722827 -1.55380139
 -2.28454699 -0.99339368  1.37851653  1.62785521  0.72271846  0.71370732
  0.70128052 -0.32721731  0.31932009  0.36967998 -0.5576872  -0.72393322
 -1.11104987 -0.16118438  2.15884119 -0.54048602 -1.80570038  0.02747875
  1.12550494 -1.34237967 -1.08933911 -0.14248291  1.76273289  0.13790034
 -0.58173336  0.94123518  3.5202417   2.2124322   0.65067574 -1.13504917
 -0.55681285 -1.60381342  0.62927013  1.14437032 25.05372563]
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.678130060322
gradient value of function right now is: [ 2.78664847e-08  4.87676710e-09  2.79003143e-08  2.77914233e-08
 -2.78663027e-08 -4.87673515e-09 -2.79001320e-08 -2.77912418e-08
  9.55668255e-10  9.62901540e-10  5.16750281e-12  2.96002594e-10
  6.25819756e-09  6.27680771e-09  1.16618488e-10  1.97132360e-09
  8.29687724e-10  8.33917553e-10  1.02141058e-11  2.59093307e-10
  2.26405352e-09  2.27600512e-09  2.68047592e-11  7.06689091e-10
 -2.20638404e-11  4.60551446e-12 -4.45846262e-12  2.83911589e-12
 -2.76887608e-11  7.18293236e-11  3.94795919e-11  4.44742309e-11
 -6.42634663e-11 -8.32191787e-11 -7.88004627e-11 -5.19634085e-11
  3.86593453e-10  4.42266787e-10  4.33309715e-10  2.81026795e-10
 -5.38735963e-11 -1.75634227e-11 -1.50875518e-11 -4.59560972e-11
 -3.45046067e-11  3.79696231e-11  5.09033404e-11 -1.29128200e-11
 -8.94884737e-11  2.24208335e-12  1.19934602e-11 -6.95412296e-11
 -1.60477592e-11 -3.94777979e-12 -1.62288200e-12 -1.05965136e-11
 -1.25577861e-11 -8.31304001e-12  5.40847522e-12  3.66487846e-11
  1.72027724e-11  2.78723250e-11 -1.28227936e-12  1.76843185e-12
  8.96500640e-01]
supnorm grad right now is: 0.8965006403818255
Weights right now are: 
[-2.48104245 -1.1259717  -2.97414797 -5.13105771  3.95518     0.43202092
  4.28454635  5.06480216 -2.42487875 -2.86158171  0.23440033  0.20970672
  1.22205309  0.25273046  0.83593711  0.22274181 -2.37050251 -2.6693593
 -1.03210767 -1.38248622 -1.84317596 -2.60245372 -0.54603813 -1.15706885
 -2.30487349 -1.16465513 -0.91190355 -2.04517945 -0.98722283 -1.55381649
 -2.28455551 -0.993403    1.37852998  1.62787243  0.72273484  0.71371805
  0.70120065 -0.32730844  0.31923068  0.36962212 -0.55767597 -0.72392972
 -1.11104688 -0.16117481  2.15884803 -0.540494   -1.80571103  0.02748106
  1.12552336 -1.34238034 -1.08934179 -0.14246866  1.76273613  0.13790109
 -0.58173309  0.94123727  3.52024459  2.21243339  0.65067451 -1.13505637
 -0.55681665 -1.60381862  0.62927041  1.14436993 25.02914282]
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.6694458052934
gradient value of function right now is: [ 2.68812418e-08  4.71447687e-09  2.69137950e-08  2.68089645e-08
 -2.68812270e-08 -4.71447410e-09 -2.69137803e-08 -2.68089498e-08
  9.22572679e-10  9.29617311e-10  4.75514367e-12  2.85687156e-10
  6.05515750e-09  6.07336584e-09  1.12016615e-10  1.90725897e-09
  8.01083314e-10  8.05206035e-10  9.71190191e-12  2.50128123e-10
  2.18368823e-09  2.19532545e-09  2.54295546e-11  6.81508731e-10
 -2.15722149e-11  4.32455117e-12 -4.48736372e-12  2.66940931e-12
 -2.76642830e-11  6.91461101e-11  3.76297985e-11  4.28315140e-11
 -6.20503610e-11 -8.06058934e-11 -7.62510887e-11 -5.03400435e-11
  3.72845340e-10  4.26969702e-10  4.18175330e-10  2.71331279e-10
 -5.23950070e-11 -1.69947984e-11 -1.45799429e-11 -4.47096319e-11
 -3.37487340e-11  3.66253731e-11  4.91337713e-11 -1.28757242e-11
 -8.71054413e-11  2.05109357e-12  1.14964937e-11 -6.77935886e-11
 -1.56558248e-11 -3.80902716e-12 -1.55293074e-12 -1.03788876e-11
 -1.19961264e-11 -8.41970564e-12  5.15797043e-12  3.57171048e-11
  1.65202777e-11  2.73209491e-11 -1.23856873e-12  1.70337081e-12
  1.06522211e+00]
supnorm grad right now is: 1.065222110831261
Weights right now are: 
[-2.4865846  -1.12694248 -2.97969693 -5.13658499  3.96072216  0.43299171
  4.2900953   5.07032944 -2.42506948 -2.86177382  0.23439907  0.20964751
  1.22080731  0.25148118  0.835913    0.2223488  -2.37066774 -2.66952533
 -1.03210986 -1.38253791 -1.84362674 -2.60290677 -0.5460439  -1.15720982
 -2.30486932 -1.1646561  -0.91190279 -2.04518004 -0.98721828 -1.55383104
 -2.28456392 -0.99341197  1.37854301  1.62788905  0.72275065  0.71372841
  0.70112366 -0.32739607  0.31914467  0.36956649 -0.55766558 -0.72392638
 -1.111044   -0.16116593  2.15885445 -0.54050167 -1.80572126  0.02748322
  1.12554045 -1.34238102 -1.08934436 -0.14245543  1.76273916  0.1379018
 -0.58173284  0.94123924  3.52024711  2.21243478  0.65067336 -1.13506326
 -0.55682016 -1.60382374  0.62927066  1.14436958 25.05838948]
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.615289235759
gradient value of function right now is: [ 2.68566029e-08  4.71979022e-09  2.68901686e-08  2.67850081e-08
 -2.68573271e-08 -4.71991748e-09 -2.68908939e-08 -2.67857304e-08
  9.30924390e-10  9.37144711e-10  8.23365410e-12  2.90047076e-10
  6.06022845e-09  6.07539409e-09  1.24604852e-10  1.91586024e-09
  8.03557814e-10  8.07137272e-10  1.19503806e-11  2.52085529e-10
  2.18941240e-09  2.19951850e-09  3.16830296e-11  6.86601428e-10
 -1.82581141e-11  5.18823691e-12 -2.63363616e-12  3.14263907e-12
 -1.39456464e-11  7.26107385e-11  4.52234318e-11  4.46766385e-11
 -6.51798008e-11 -8.10773163e-11 -7.77837396e-11 -5.05072579e-11
  3.74289085e-10  4.22390659e-10  4.15854589e-10  2.68051023e-10
 -4.73270543e-11 -1.54171247e-11 -1.31549644e-11 -3.99468598e-11
 -2.73851307e-11  3.77384502e-11  5.00713633e-11 -6.69430507e-12
 -7.71806903e-11  4.36776709e-12  1.34729312e-11 -5.84627182e-11
 -1.32490090e-11 -3.03092201e-12 -8.18290032e-13 -8.03230312e-12
 -1.41899698e-11 -2.15975982e-12  6.36122356e-12  3.10145686e-11
  1.83020032e-11  2.12240070e-11 -1.27612107e-12  1.73167595e-12
 -1.60517242e+00]
supnorm grad right now is: 1.6051724220943198
Weights right now are: 
[-2.49199599 -1.12789242 -2.98511492 -5.14198186  3.96613354  0.43394165
  4.2955133   5.07572631 -2.42525573 -2.86196144  0.23439793  0.20958975
  1.21958715  0.25025753  0.83588976  0.22196412 -2.37082921 -2.66968761
 -1.03211193 -1.38258839 -1.84406676 -2.60334905 -0.54604935 -1.15734731
 -2.30486514 -1.16465702 -0.91190199 -2.0451806  -0.98721344 -1.55384515
 -2.28457192 -0.9934207   1.37855568  1.62790531  0.7227661   0.71373856
  0.70104851 -0.3274818   0.31906058  0.36951201 -0.55765527 -0.72392306
 -1.11104116 -0.16115714  2.15886086 -0.54050911 -1.8057312   0.02748549
  1.12555744 -1.34238159 -1.08934681 -0.14244225  1.76274217  0.13790252
 -0.58173257  0.94124121  3.52024962  2.21243605  0.65067225 -1.13507018
 -0.55682359 -1.60382882  0.62927091  1.14436924 24.91954339]
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.6187713713944
gradient value of function right now is: [ 2.50237912e-08  4.40693448e-09  2.50538532e-08  2.49566962e-08
 -2.50242545e-08 -4.40701627e-09 -2.50543171e-08 -2.49571582e-08
  8.59307539e-10  8.66067986e-10  3.68823672e-12  2.65815154e-10
  5.67010725e-09  5.68780884e-09  1.02295804e-10  1.78515541e-09
  7.46812561e-10  7.50779586e-10  8.58074796e-12  2.33015681e-10
  2.03152975e-09  2.04270486e-09  2.23275507e-11  6.33547569e-10
 -2.09212468e-11  3.73286668e-12 -4.68525351e-12  2.31469355e-12
 -2.87463056e-11  6.38110035e-11  3.35398918e-11  3.95817275e-11
 -5.76104234e-11 -7.56079856e-11 -7.12974574e-11 -4.72438286e-11
  3.46785258e-10  3.98450691e-10  3.89797757e-10  2.53284371e-10
 -5.00477295e-11 -1.60380464e-11 -1.37284982e-11 -4.27789959e-11
 -3.28020988e-11  3.40150985e-11  4.57277909e-11 -1.32862428e-11
 -8.34265753e-11  1.53008655e-12  1.04213884e-11 -6.52737384e-11
 -1.51036367e-11 -3.60551227e-12 -1.47898402e-12 -1.01559201e-11
 -1.07787917e-11 -9.05679243e-12  4.59837073e-12  3.42985727e-11
  1.50919357e-11  2.67049752e-11 -1.15199387e-12  1.58126133e-12
  1.60415206e+00]
supnorm grad right now is: 1.604152057308453
Weights right now are: 
[-2.49720281 -1.12880843 -2.99032819 -5.14717478  3.97134036  0.43485766
  4.30072656  5.08091923 -2.42543589 -2.86214281  0.23439651  0.20953374
  1.21841016  0.2490775   0.83586658  0.22159271 -2.37098502 -2.66984412
 -1.03211412 -1.38263718 -1.844491   -2.60377527 -0.54605511 -1.15748009
 -2.30486154 -1.16465793 -0.91190136 -2.04518116 -0.98721014 -1.5538588
 -2.28458008 -0.99342916  1.37856802  1.62792095  0.72278099  0.71374834
  0.70097594 -0.32756413  0.31897974  0.36945968 -0.55764614 -0.72391988
 -1.11103842 -0.16114939  2.15886664 -0.54051622 -1.8057407   0.02748736
  1.12557263 -1.34238213 -1.08934912 -0.1424306   1.76274487  0.13790322
 -0.58173229  0.94124294  3.5202519   2.21243733  0.65067118 -1.13507664
 -0.55682679 -1.6038337   0.62927114  1.14436893 25.09143425]
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.5797687539766
gradient value of function right now is: [ 2.52991840e-08  4.46384557e-09  2.53308722e-08  2.52320593e-08
 -2.52993717e-08 -4.46387898e-09 -2.53310601e-08 -2.52322465e-08
  8.79724675e-10  8.85531685e-10  8.05485374e-12  2.74327605e-10
  5.73939314e-09  5.75350283e-09  1.19036616e-10  1.81558539e-09
  7.58676193e-10  7.62011563e-10  1.14586619e-11  2.38177834e-10
  2.06334066e-09  2.07274242e-09  3.03457840e-11  6.47538133e-10
 -1.70317571e-11  4.85973856e-12 -2.43300980e-12  2.93866898e-12
 -1.20492045e-11  6.88291285e-11  4.33068388e-11  4.23252745e-11
 -6.21045773e-11 -7.70025265e-11 -7.39537952e-11 -4.79560350e-11
  3.52782662e-10  3.97641031e-10  3.91656983e-10  2.52302186e-10
 -4.42776312e-11 -1.43216396e-11 -1.21755740e-11 -3.73079900e-11
 -2.53524166e-11  3.57579451e-11  4.73999947e-11 -5.81458989e-12
 -7.20686345e-11  4.35104414e-12  1.29320120e-11 -5.44314181e-11
 -1.23096914e-11 -2.71353858e-12 -6.14568136e-13 -7.37591906e-12
 -1.35765771e-11 -1.55902747e-12  6.12800196e-12  2.89294360e-11
  1.74552814e-11  1.95851566e-11 -1.21111868e-12  1.62957111e-12
 -1.86793620e+00]
supnorm grad right now is: 1.8679361952961469
Weights right now are: 
[-2.50224067 -1.12969637 -2.99537218 -5.15219918  3.97637823  0.4357456
  4.30577056  5.08594363 -2.42560962 -2.86231785  0.23439549  0.20947986
  1.21726791  0.24793193  0.83584482  0.22123248 -2.37113562 -2.66999548
 -1.03211603 -1.38268426 -1.8449006  -2.60418701 -0.54606013 -1.1576081
 -2.30485755 -1.16465876 -0.91190058 -2.04518167 -0.98720547 -1.55387199
 -2.28458756 -0.9934373   1.37857994  1.62793622  0.72279551  0.71375787
  0.70090598 -0.32764393  0.31890144  0.36940897 -0.55763636 -0.72391687
 -1.11103583 -0.161141    2.15887262 -0.54052323 -1.80575003  0.02748947
  1.12558861 -1.34238279 -1.08935149 -0.14241814  1.7627477   0.13790384
 -0.58173208  0.94124479  3.52025422  2.21243858  0.65067014 -1.13508291
 -0.55683001 -1.60383828  0.62927138  1.1443686  24.84712428]
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.565002254452
gradient value of function right now is: [ 2.34203497e-08  4.14075564e-09  2.34483401e-08  2.33577490e-08
 -2.34203631e-08 -4.14075804e-09 -2.34483534e-08 -2.33577623e-08
  8.05153833e-10  8.11603767e-10  3.02185618e-12  2.48930039e-10
  5.33596847e-09  5.35301031e-09  9.47330761e-11  1.67966126e-09
  7.00042172e-10  7.03833239e-10  7.76434818e-12  2.18350026e-10
  1.90066853e-09  1.91132846e-09  2.01049015e-11  5.92528340e-10
 -2.00989669e-11  3.29090961e-12 -4.71633031e-12  2.04798384e-12
 -2.86926592e-11  5.94365623e-11  3.05218902e-11  3.69032667e-11
 -5.39479689e-11 -7.12853913e-11 -6.70796544e-11 -4.45579059e-11
  3.24396640e-10  3.73529114e-10  3.65144830e-10  2.37486578e-10
 -4.76188996e-11 -1.51357389e-11 -1.29189653e-11 -4.07122000e-11
 -3.15633675e-11  3.18056531e-11  4.28280137e-11 -1.32031403e-11
 -7.95434435e-11  1.18679247e-12  9.59302648e-12 -6.23933766e-11
 -1.44597940e-11 -3.39223410e-12 -1.37699017e-12 -9.79017528e-12
 -9.91429849e-12 -9.15881597e-12  4.20096308e-12  3.28195191e-11
  1.39949499e-11  2.58115664e-11 -1.08038437e-12  1.47834885e-12
  1.98302087e+00]
supnorm grad right now is: 1.9830208737303825
Weights right now are: 
[-2.50720767 -1.13057354 -3.00034529 -5.15715297  3.98134524  0.43662277
  4.31074367  5.09089742 -2.42578154 -2.86249102  0.2343943   0.20942644
  1.21613905  0.24679995  0.8358227   0.2208761  -2.37128438 -2.67014496
 -1.03211804 -1.38273083 -1.84530488 -2.60459333 -0.54606541 -1.15773462
 -2.3048538  -1.16465963 -0.91189989 -2.0451822  -0.98720154 -1.5538852
 -2.2845953  -0.99344545  1.37859187  1.62795134  0.72280994  0.71376731
  0.70083691 -0.32772245  0.31882433  0.36935909 -0.557627   -0.72391392
 -1.11103333 -0.16113308  2.15887824 -0.54053015 -1.80575926  0.0274912
  1.12560395 -1.34238344 -1.08935388 -0.14240639  1.76275037  0.13790445
 -0.5817319   0.94124647  3.52025685  2.21243917  0.65066901 -1.13508903
 -0.55683337 -1.60384254  0.62927162  1.14436827 25.18307997]
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.699879978986
gradient value of function right now is: [ 2.37523484e-08  4.20314982e-09  2.37815817e-08  2.36892932e-08
 -2.37522812e-08 -4.20313779e-09 -2.37815144e-08 -2.36892262e-08
  8.23517919e-10  8.29375064e-10  5.89385515e-12  2.56033158e-10
  5.41296757e-09  5.42774416e-09  1.06174492e-10  1.70944390e-09
  7.12201659e-10  7.15598475e-10  9.68931452e-12  2.23086461e-10
  1.93377662e-09  1.94333321e-09  2.54569457e-11  6.05477147e-10
 -1.76601916e-11  4.04521365e-12 -3.27214714e-12  2.46892626e-12
 -1.79332407e-11  6.30308246e-11  3.70475861e-11  3.89020394e-11
 -5.71718569e-11 -7.25902990e-11 -6.91758100e-11 -4.52735063e-11
  3.30424571e-10  3.75458318e-10  3.68738631e-10  2.38416840e-10
 -4.40765469e-11 -1.41288750e-11 -1.20198855e-11 -3.73581202e-11
 -2.69357964e-11  3.31128649e-11  4.41524320e-11 -8.47582560e-12
 -7.25378767e-11  3.01371354e-12  1.12501906e-11 -5.56730461e-11
 -1.27346331e-11 -2.85059837e-12 -8.45084046e-13 -8.05889225e-12
 -1.16615544e-11 -4.57664107e-12  5.18869231e-12  2.95027414e-11
  1.55607152e-11  2.14128989e-11 -1.12389983e-12  1.51191714e-12
 -4.01674971e-01]
supnorm grad right now is: 0.4016749710121167
Weights right now are: 
[-2.51197455 -1.13141694 -3.00511798 -5.16190717  3.98611212  0.43746617
  4.31551637  5.09565163 -2.4259464  -2.86265712  0.23439329  0.20937525
  1.21505266  0.24571038  0.83580182  0.22053314 -2.37142713 -2.67028844
 -1.03211988 -1.38277552 -1.84569249 -2.60498297 -0.54607022 -1.15785588
 -2.30485002 -1.16466042 -0.91189914 -2.04518268 -0.98719715 -1.55389783
 -2.28460246 -0.99345323  1.37860328  1.62796596  0.72282384  0.71377641
  0.70077075 -0.32779789  0.3187503   0.36931121 -0.55761771 -0.72391105
 -1.11103092 -0.16112522  2.1588839  -0.54053684 -1.80576819  0.02749306
  1.12561923 -1.342384   -1.08935615 -0.14239463  1.76275305  0.13790502
 -0.58173175  0.94124817  3.52025932  2.21244003  0.65066796 -1.13509522
 -0.55683656 -1.603847    0.62927185  1.14436796 25.16156116]
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.704791911914
gradient value of function right now is: [ 2.33535729e-08  4.13642911e-09  2.33822376e-08  2.32916036e-08
 -2.33534004e-08 -4.13639847e-09 -2.33820649e-08 -2.32914315e-08
  8.09572188e-10  8.15396594e-10  5.54865870e-12  2.51595262e-10
  5.32927198e-09  5.34404332e-09  1.03669534e-10  1.68267916e-09
  7.00397516e-10  7.03779528e-10  9.37227909e-12  2.19325690e-10
  1.90082567e-09  1.91033594e-09  2.45851699e-11  5.94982213e-10
 -1.76317293e-11  3.89189875e-12 -3.37516150e-12  2.37800560e-12
 -1.86223489e-11  6.17759941e-11  3.59212180e-11  3.81427211e-11
 -5.61122239e-11 -7.14941333e-11 -6.80566082e-11 -4.45959943e-11
  3.24763674e-10  3.69456102e-10  3.62699412e-10  2.34624658e-10
 -4.37456549e-11 -1.39714704e-11 -1.18800075e-11 -3.71072543e-11
 -2.69420660e-11  3.25252254e-11  4.33987240e-11 -8.76223976e-12
 -7.20840190e-11  2.83374370e-12  1.09612961e-11 -5.54482478e-11
 -1.26979118e-11 -2.82951539e-12 -8.49866842e-13 -8.08889170e-12
 -1.13139766e-11 -4.97268676e-12  5.02583768e-12  2.93463237e-11
  1.51976766e-11  2.14845885e-11 -1.10510031e-12  1.48547184e-12
 -2.02924055e-01]
supnorm grad right now is: 0.20292405490943674
Weights right now are: 
[-2.51661929 -1.13224031 -3.00976848 -5.16653962  3.99075685  0.43828954
  4.32016686  5.10028407 -2.42610793 -2.86281976  0.234392    0.20932496
  1.21399187  0.24464684  0.83578057  0.22019789 -2.37156664 -2.67042858
 -1.03212186 -1.38281926 -1.846071   -2.60536328 -0.54607545 -1.15797452
 -2.30484671 -1.16466123 -0.91189857 -2.04518318 -0.9871942  -1.55391027
 -2.28460998 -0.99346091  1.37861458  1.6279802   0.72283744  0.71378529
  0.70070605 -0.3278712   0.31867824  0.36926465 -0.55760933 -0.72390831
 -1.1110286  -0.16111818  2.15888894 -0.54054331 -1.80577683  0.02749449
  1.12563302 -1.34238462 -1.08935839 -0.14238416  1.76275544  0.13790557
 -0.5817316   0.94124964  3.52026174  2.21244056  0.65066689 -1.13510088
 -0.55683969 -1.60385099  0.62927207  1.14436766 25.3255094 ]
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.7055848763066
gradient value of function right now is: [ 2.30086717e-08  4.07903400e-09  2.30368891e-08  2.29476658e-08
 -2.30082767e-08 -4.07896423e-09 -2.30364934e-08 -2.29472717e-08
  7.97886937e-10  8.03646625e-10  5.39969431e-12  2.47949096e-10
  5.25714971e-09  5.27178392e-09  1.02031946e-10  1.65990498e-09
  6.90313974e-10  6.93659422e-10  9.19340964e-12  2.16162131e-10
  1.87265057e-09  1.88205411e-09  2.40971739e-11  5.86145063e-10
 -1.74628556e-11  3.79636311e-12 -3.38470596e-12  2.32021436e-12
 -1.86333090e-11  6.08337509e-11  3.52645505e-11  3.75646249e-11
 -5.53233916e-11 -7.05624389e-11 -6.71482315e-11 -4.40161910e-11
  3.19941920e-10  3.64094566e-10  3.57395186e-10  2.31224019e-10
 -4.32358290e-11 -1.37773303e-11 -1.17053806e-11 -3.66747640e-11
 -2.66805933e-11  3.20533192e-11  4.27783465e-11 -8.74879425e-12
 -7.12671113e-11  2.76167249e-12  1.07852520e-11 -5.48463730e-11
 -1.25627749e-11 -2.78326965e-12 -8.27377433e-13 -8.01336934e-12
 -1.11253333e-11 -5.00218599e-12  4.93863348e-12  2.90345010e-11
  1.49605762e-11  2.12977652e-11 -1.08975802e-12  1.46359446e-12
 -1.44414912e-01]
supnorm grad right now is: 0.144414912360946
Weights right now are: 
[-2.52107872 -1.13303223 -3.01423335 -5.17098724  3.99521628  0.43908146
  4.32463173  5.10473169 -2.42626255 -2.86297552  0.23439103  0.20927693
  1.21297045  0.24362248  0.835761    0.21987542 -2.37170047 -2.67056308
 -1.03212359 -1.38286116 -1.84643376 -2.6057279  -0.54607997 -1.15808803
 -2.30484322 -1.16466195 -0.91189786 -2.04518361 -0.98719023 -1.55392203
 -2.28461665 -0.99346817  1.37862528  1.62799393  0.72285048  0.71379386
  0.7006441  -0.32794185  0.31860894  0.36921977 -0.55760079 -0.72390561
 -1.11102631 -0.16111095  2.15889429 -0.54054951 -1.80578513  0.0274963
  1.12564715 -1.34238511 -1.08936045 -0.14237329  1.76275795  0.13790611
 -0.58173144  0.94125124  3.52026394  2.21244164  0.65066597 -1.13510669
 -0.55684258 -1.60385531  0.62927229  1.14436737 24.85548308]
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.705526845777
gradient value of function right now is: [ 2.26960907e-08  4.02718171e-09  2.27239252e-08  2.26359696e-08
 -2.26961751e-08 -4.02719707e-09 -2.27240096e-08 -2.26360536e-08
  7.87507764e-10  7.93187910e-10  5.34277813e-12  2.44746787e-10
  5.19197847e-09  5.20641531e-09  1.00810434e-10  1.63945767e-09
  6.81257659e-10  6.84556488e-10  9.08052379e-12  2.13344462e-10
  1.84732289e-09  1.85659194e-09  2.37919356e-11  5.78266668e-10
 -1.72296754e-11  3.72745978e-12 -3.35177802e-12  2.27795598e-12
 -1.83288151e-11  6.00437095e-11  3.48286998e-11  3.70767238e-11
 -5.46696636e-11 -6.97222442e-11 -6.63504832e-11 -4.34922235e-11
  3.15623842e-10  3.59161580e-10  3.52558765e-10  2.28091903e-10
 -4.26441342e-11 -1.35710112e-11 -1.15230014e-11 -3.61666307e-11
 -2.63092137e-11  3.16418451e-11  4.22264297e-11 -8.61411494e-12
 -7.02856998e-11  2.74058790e-12  1.06584945e-11 -5.40840020e-11
 -1.23871221e-11 -2.72877846e-12 -7.96591012e-13 -7.89625450e-12
 -1.09813620e-11 -4.92468669e-12  4.88108207e-12  2.86406217e-11
  1.47721704e-11  2.10043535e-11 -1.07597410e-12  1.44281214e-12
 -1.56121850e-01]
supnorm grad right now is: 0.15612185024624103
Weights right now are: 
[-2.52535677 -1.1337933  -3.01851672 -5.17525402  3.99949432  0.43984253
  4.32891509  5.10899846 -2.42641173 -2.8631257   0.23438979  0.20923044
  1.21198872  0.24263824  0.83574134  0.21956501 -2.37182922 -2.6706924
 -1.03212544 -1.38290155 -1.8467825  -2.60607827 -0.54608483 -1.15819742
 -2.30484024 -1.16466268 -0.91189733 -2.04518406 -0.98718766 -1.55393346
 -2.28462356 -0.99347524  1.37863573  1.62800713  0.72286306  0.71380209
  0.70058448 -0.3280094   0.31854257  0.36917689 -0.55759323 -0.72390305
 -1.11102415 -0.16110463  2.158899   -0.54055544 -1.80579307  0.02749764
  1.12565974 -1.34238558 -1.08936243 -0.14236379  1.76276015  0.13790662
 -0.5817313   0.94125259  3.52026613  2.21244235  0.65066502 -1.13511206
 -0.5568454  -1.60385927  0.62927249  1.14436711 25.17675338]
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -3592.706113373696
gradient value of function right now is: [ 2.23763978e-08  3.97389499e-09  2.24038191e-08  2.23171684e-08
 -2.23765627e-08 -3.97392423e-09 -2.24039843e-08 -2.23173329e-08
  7.76682137e-10  7.82301955e-10  5.20967172e-12  2.41370195e-10
  5.12497989e-09  5.13928536e-09  9.93124850e-11  1.61830407e-09
  6.71911315e-10  6.75175859e-10  8.91823811e-12  2.10412942e-10
  1.82122936e-09  1.83039848e-09  2.33497538e-11  5.70084436e-10
 -1.70695060e-11  3.64248113e-12 -3.35604561e-12  2.22637078e-12
 -1.83211622e-11  5.91790569e-11  3.42358122e-11  3.65456430e-11
 -5.39421583e-11 -6.88549268e-11 -6.55081999e-11 -4.29526675e-11
  3.11160081e-10  3.54187696e-10  3.47643102e-10  2.24937993e-10
 -4.21702239e-11 -1.33884145e-11 -1.13583375e-11 -3.57623279e-11
 -2.60507317e-11  3.12059359e-11  4.16515566e-11 -8.58484379e-12
 -6.95183278e-11  2.67920356e-12  1.05007346e-11 -5.35126630e-11
 -1.22564456e-11 -2.68525862e-12 -7.75446415e-13 -7.82021586e-12
 -1.08260945e-11 -4.90591480e-12  4.80626743e-12  2.83352216e-11
  1.45621491e-11  2.08012910e-11 -1.06194959e-12  1.42362203e-12
 -9.75954450e-02]
supnorm grad right now is: 0.09759544500327624
Weights right now are: 
[-2.52959388 -1.13454827 -3.02275902 -5.17947995  4.00373143  0.4405975
  4.33315739  5.1132244  -2.42655907 -2.86327412  0.23438882  0.20918463
  1.21101392  0.24166067  0.83572233  0.21925695 -2.37195661 -2.67082043
 -1.03212713 -1.38294147 -1.84712729 -2.60642483 -0.54608925 -1.15830542
 -2.30483693 -1.16466337 -0.91189667 -2.04518448 -0.98718407 -1.5539448
 -2.2846301  -0.99348222  1.37864606  1.62802031  0.72287562  0.7138103
  0.70052561 -0.3280764   0.31847678  0.36913438 -0.55758508 -0.72390055
 -1.11102206 -0.16109775  2.15890394 -0.54056143 -1.80580106  0.02749918
  1.12567314 -1.34238613 -1.08936449 -0.14235352  1.76276249  0.13790709
 -0.5817312   0.94125405  3.52026837  2.21244308  0.65066406 -1.13511745
 -0.55684829 -1.60386312  0.6292727   1.14436683 25.21463911]
NN weights: [-2.51872188 -1.132615   -3.01187363 -5.16863666  3.99285944  0.43866423
  4.32227201  5.10238111 -2.42618091 -2.86289329  0.23439157  0.2093023
  1.21350794  0.24416149  0.83577144  0.22004513 -2.37162983 -2.67049209
 -1.03212266 -1.38283904 -1.84624199 -2.60553516 -0.54607752 -1.15802802
 -2.30484504 -1.16466155 -0.91189821 -2.04518337 -0.98719221 -1.55391577
 -2.28461304 -0.99346432  1.37861962  1.62798671  0.72284361  0.71378936
  0.70067685 -0.32790457  0.31864554  0.36924345 -0.55760528 -0.72390702
 -1.1110275  -0.16111475  2.15889155 -0.54054622 -1.80578073  0.02749542
  1.12563978 -1.34238481 -1.08935932 -0.14237896  1.76275665  0.13790583
 -0.58173152  0.94125042  3.52026276  2.21244118  0.65066648 -1.1351037
 -0.55684102 -1.60385316  0.62927218  1.14436753]
Minimum obj value:-3592.706113373696
Optimal xi: 24.984433920837425
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1551.5577480446295
W_T_median: 1405.438397282097
W_T_pctile_5: 624.6754498455748
W_T_CVAR_5_pct: 489.5910352131688
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 2.0
F value: -3592.706113373696
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.079514533228
gradient value of function right now is: [ 2.40776623e-03  8.98403612e-02  9.02585523e-02  6.65521692e-02
 -2.40776623e-03 -8.98403612e-02 -9.02585523e-02 -6.65521692e-02
  4.88166979e-04  3.29010794e-03  3.34497185e-03  1.77789993e-03
  4.25196007e-04  2.86502814e-03  2.91282068e-03  1.54814927e-03
  2.13294455e-04  1.43670751e-03  1.46068566e-03  7.76308432e-04
  6.21438133e-03  4.18974700e-02  4.25957635e-02  2.26413995e-02
 -3.78050644e-04 -9.70463034e-05 -2.64258135e-03 -2.66852329e-03
  1.81815781e-04  4.98680172e-05  1.21561959e-03  1.22992975e-03
  4.10685281e-05  1.17747373e-05  2.64761079e-04  2.68314285e-04
  1.78874731e-05  5.33280298e-06  1.12108872e-04  1.13755368e-04
  2.39296585e-04  7.27487562e-05  2.51952806e-04  3.30006085e-04
  4.42611391e-05  2.17931433e-05  4.54426252e-05  7.79589798e-05
  1.40465270e-04  7.29435380e-05  1.42491899e-04  2.60216510e-04
  8.86673292e-05  4.23748566e-05  9.06422774e-05  1.54263903e-04
  2.75474056e-05 -4.61458655e-06 -1.57716282e-05 -4.63704297e-05
  5.77105525e-05 -6.10368366e-06  2.92088456e-05 -2.70343414e-05
  1.67893164e+00]
supnorm grad right now is: 1.6789316387345237
Weights right now are: 
[-0.7287094  -2.78325541 -1.97160894 -1.57409979  0.72262589  1.01440833
  2.07609112  0.87504366  0.58488789  1.33471079  2.04833126  0.40616864
 -1.47861864 -1.13013438 -2.24803495 -2.33771159 -0.72835933 -2.17947218
 -2.32814471 -1.91203915  0.61021453 -0.90140584 -0.32734052  0.18434098
  0.46934526  1.11020994  1.15011599  0.6395722  -1.55018017 -0.08100116
 -1.97692162 -1.39791713 -0.98212083 -0.63360827 -2.22144314 -1.9576205
  0.05993954  0.4092388   0.15390087 -0.27108816  0.39824517  1.5900429
  0.85370905  0.93423909  1.72768029  1.25368217  0.73108108  2.23055587
 -1.31943686 -0.69085912 -1.80309623 -1.07907478 -0.50266906 -1.08807763
 -1.82347359 -1.67990643  0.95749773 -1.51546959  2.05204017  0.48876857
  0.93527575 -2.45891362 -1.46146048 -0.868862   25.09921194]
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5143.93269892577
gradient value of function right now is: [ 8.43750795e-04  3.03234003e-02  3.04409055e-02  2.71149947e-02
 -8.43750795e-04 -3.03234003e-02 -3.04409055e-02 -2.71149947e-02
  1.51953049e-04  1.16383800e-03  1.17815320e-03  6.60216869e-04
  1.18220574e-04  9.05078909e-04  9.16217505e-04  5.13393264e-04
  6.16473910e-05  4.71813147e-04  4.77621966e-04  2.67616344e-04
  1.03860864e-03  7.95528598e-03  8.05312699e-03  4.51287509e-03
 -5.57571948e-05 -1.34904450e-05 -3.70860583e-04 -3.74544859e-04
  4.57302955e-05  1.18687580e-05  2.91686163e-04  2.95164507e-04
  1.74416352e-05  4.69744736e-06  1.08193446e-04  1.09628753e-04
  1.27831197e-04  3.04782985e-05  8.58890943e-04  8.67027349e-04
  5.24861947e-05  1.54614734e-05  5.53764673e-05  7.21008598e-05
  2.27357499e-06  1.22519946e-06  2.31823462e-06  4.29450967e-06
  2.78395658e-05  1.46358568e-05  2.83167651e-05  5.24558404e-05
  2.54391865e-05  1.19496617e-05  2.61478303e-05  4.41642906e-05
  9.49017804e-06 -2.95103837e-07  3.23832749e-07 -1.01562430e-05
  1.54714889e-05 -1.38744840e-06  1.42842164e-05 -7.57594447e-06
  2.91333902e+00]
supnorm grad right now is: 2.913339016768014
Weights right now are: 
[-0.73283204 -2.91281846 -2.08932452 -1.68978458  0.72674852  1.14397137
  2.19380669  0.99072845  0.58270725  1.31464321  2.02684015  0.39177825
 -1.48379609 -1.16819601 -2.28033884 -2.37385297 -0.73014199 -2.19420366
 -2.34137022 -1.92433059  0.56243222 -1.3246926  -0.79009451 -0.12261641
  0.480036    1.11290715  1.23238485  0.6990928  -1.56774694 -0.0853447
 -2.05196764 -1.46708538 -0.98317472 -0.63389846 -2.22786285 -1.96271546
  0.04886655  0.40655562  0.08803649 -0.32423978  0.37337411  1.581367
  0.82312701  0.90304104  1.72536845  1.25229446  0.728842    2.22616163
 -1.33081147 -0.69793706 -1.81430535 -1.10096894 -0.5178402  -1.09679286
 -1.83709456 -1.71058044  0.93229684 -1.51171421  2.06058001  0.5273612
  0.88504114 -2.45114383 -1.47545328 -0.86347137 25.23262544]
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.234195854027
gradient value of function right now is: [ 3.57476787e-04  1.24408132e-02  1.24777764e-02  1.17495770e-02
 -3.57476787e-04 -1.24408132e-02 -1.24777764e-02 -1.17495770e-02
  6.10116478e-05  4.97008527e-04  5.01832336e-04  3.06693086e-04
  4.00488314e-05  3.25981829e-04  3.29148670e-04  2.01122742e-04
  2.20712253e-05  1.79604752e-04  1.81350134e-04  1.10805966e-04
  2.54729479e-04  2.07467724e-03  2.09481727e-03  1.28019026e-03
 -1.24403330e-05 -2.93397625e-06 -7.95902662e-05 -8.04436431e-05
  1.32586690e-05  3.38859000e-06  8.11029753e-05  8.21612840e-05
  6.00302875e-06  1.58731033e-06  3.58411184e-05  3.63540887e-05
  5.81221419e-05  1.35620740e-05  3.74368186e-04  3.78255171e-04
  1.48419001e-05  4.45494122e-06  1.56453187e-05  2.04049128e-05
 -4.33405470e-07 -1.60484388e-07 -4.59255314e-07 -6.39708270e-07
  8.17506251e-06  4.56807663e-06  8.27902890e-06  1.55577136e-05
  8.54846414e-06  4.22277932e-06  8.76019498e-06  1.49313299e-05
  3.52513460e-06 -8.99190004e-08  9.35383605e-07 -3.01868819e-06
  5.26313090e-06 -5.22941006e-07  5.77200333e-06 -2.90142155e-06
  6.76897327e-01]
supnorm grad right now is: 0.6768973271463524
Weights right now are: 
[-0.73723696 -3.0467777  -2.21092649 -1.82339145  0.73115345  1.27793061
  2.31540867  1.12433531  0.58056849  1.29301068  2.00375393  0.37510552
 -1.4881832  -1.20364504 -2.31032027 -2.41000054 -0.73172693 -2.20859935
 -2.35424912 -1.93723243  0.53618096 -1.58130655 -1.0696068  -0.32240295
  0.48522521  1.11412507  1.27079292  0.72688284 -1.58048924 -0.08827684
 -2.10459142 -1.51558998 -0.98421892 -0.63416507 -2.23406378 -1.96763578
  0.02841336  0.40190114 -0.03027932 -0.41971768  0.35644755  1.57573553
  0.80228922  0.88192909  1.72523932  1.25217985  0.72872576  2.22582277
 -1.3382405  -0.70241592 -1.82163617 -1.11534506 -0.53066212 -1.10378353
 -1.84864795 -1.73631177  0.90794144 -1.5104195   2.05509744  0.55306665
  0.84342852 -2.44528188 -1.49113083 -0.85952965 25.0347015 ]
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5143.905591484892
gradient value of function right now is: [ 1.50312730e-04  5.01333272e-03  5.02451366e-03  4.84831924e-03
 -1.50312730e-04 -5.01333272e-03 -5.02451366e-03 -4.84831924e-03
  2.48726694e-05  2.10545282e-04  2.12220170e-04  1.40643696e-04
  1.35552237e-05  1.14603492e-04  1.15516437e-04  7.65324497e-05
  7.92333114e-06  6.69754948e-05  6.75091614e-05  4.47244284e-05
  6.88757761e-05  5.82769256e-04  5.87407500e-04  3.89247684e-04
 -3.22334056e-06 -7.53087918e-07 -1.99435672e-05 -2.01777284e-05
  3.92185315e-06  1.00663344e-06  2.30702181e-05  2.34069047e-05
  1.92614534e-06  5.10884583e-07  1.10788728e-05  1.12546765e-05
  2.17489069e-05  5.07765229e-06  1.34647459e-04  1.36222248e-04
  4.47808790e-06  1.40191497e-06  4.71059153e-06  6.17804186e-06
 -3.10705619e-07 -1.49244804e-07 -3.22182809e-07 -5.15908396e-07
  2.70298542e-06  1.63610495e-06  2.71516742e-06  5.19305673e-06
  2.91150282e-06  1.56093493e-06  2.96156474e-06  5.15406439e-06
  1.29791608e-06 -9.09200342e-08  5.18699129e-07 -1.02707448e-06
  1.85766717e-06 -2.37117002e-07  2.21641865e-06 -1.24864239e-06
 -3.12633692e+00]
supnorm grad right now is: 3.1263369244413526
Weights right now are: 
[-7.42271008e-01 -3.19446057e+00 -2.34487520e+00 -1.97636030e+00
  7.36187495e-01  1.42561349e+00  2.44935738e+00  1.27730416e+00
  5.78228607e-01  1.26809435e+00  1.97722123e+00  3.54227032e-01
 -1.49219407e+00 -1.23776153e+00 -2.33911159e+00 -2.44781667e+00
 -7.33261430e-01 -2.22327152e+00 -2.36734664e+00 -1.95152661e+00
  5.17995862e-01 -1.76861306e+00 -1.27316918e+00 -4.81060504e-01
  4.88539485e-01  1.11487094e+00  1.29462294e+00  7.44131453e-01
 -1.59043717e+00 -9.04797181e-02 -2.14445071e+00 -1.55234945e+00
 -9.85142998e-01 -6.34391387e-01 -2.23940754e+00 -1.97187779e+00
  5.97108763e-03  3.96979334e-01 -1.56019517e-01 -5.21242579e-01
  3.43041783e-01  1.57135288e+00  7.85773841e-01  8.65291489e-01
  1.72574383e+00  1.25241055e+00  7.29226684e-01  2.22666588e+00
 -1.34446006e+00 -7.06099787e-01 -1.82778007e+00 -1.12736509e+00
 -5.42370553e-01 -1.11004750e+00 -1.85920808e+00 -1.75977998e+00
  8.83924718e-01 -1.50976066e+00  2.04352459e+00  5.72984774e-01
  8.04835465e-01 -2.44007098e+00 -1.50745971e+00 -8.55744690e-01
  2.47347929e+01]
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.176435263606
gradient value of function right now is: [ 5.87939151e-05  1.85641165e-03  1.85953796e-03  1.81688646e-03
 -5.87939151e-05 -1.85641165e-03 -1.85953796e-03 -1.81688646e-03
  9.48892169e-06  8.29717325e-05  8.35247604e-05  5.91601738e-05
  4.29209503e-06  3.74805145e-05  3.77306707e-05  2.67150349e-05
  2.65158063e-06  2.31528207e-05  2.33073677e-05  1.65023254e-05
  1.85067647e-05  1.61730261e-04  1.62808866e-04  1.15299112e-04
 -8.72990260e-07 -1.88707186e-07 -5.38973549e-06 -5.44930072e-06
  1.06347676e-06  2.53817047e-07  6.25222848e-06  6.33892281e-06
  5.33808289e-07  1.31555786e-07  3.07480763e-06  3.12105607e-06
  6.96926106e-06  1.52459236e-06  4.27788872e-05  4.32647895e-05
  1.34293428e-06  3.86190717e-07  1.41905840e-06  1.81504786e-06
 -1.15407009e-07 -5.35854476e-08 -1.20226860e-07 -1.92782032e-07
  8.48688293e-07  4.65679275e-07  8.65657521e-07  1.58905859e-06
  9.10241788e-07  4.46508187e-07  9.37017438e-07  1.57787202e-06
  4.21480851e-07  5.88418231e-09  1.83901332e-07 -2.87732819e-07
  5.99621550e-07 -5.35717431e-08  7.01843095e-07 -3.02480865e-07
  1.59231495e+00]
supnorm grad right now is: 1.5923149455174612
Weights right now are: 
[-7.47864668e-01 -3.35082660e+00 -2.48660929e+00 -2.14118264e+00
  7.41781154e-01  1.58197951e+00  2.59109147e+00  1.44212651e+00
  5.75695969e-01  1.24020536e+00  1.94756681e+00  3.29098667e-01
 -1.49579783e+00 -1.26945548e+00 -2.36581894e+00 -2.48559712e+00
 -7.34722117e-01 -2.23771163e+00 -2.38021797e+00 -1.96665448e+00
  5.04580698e-01 -1.91155594e+00 -1.42828283e+00 -6.11365753e-01
  4.90934015e-01  1.11540195e+00  1.31143043e+00  7.56304596e-01
 -1.59824762e+00 -9.22282204e-02 -2.17482973e+00 -1.58039951e+00
 -9.85910324e-01 -6.34581631e-01 -2.24372045e+00 -1.97530566e+00
 -1.54101936e-02  3.92288227e-01 -2.72218821e-01 -6.15156438e-01
  3.32061893e-01  1.56763246e+00  7.72249779e-01  8.51734603e-01
  1.72640922e+00  1.25276677e+00  7.29881657e-01  2.22783331e+00
 -1.34996782e+00 -7.09698805e-01 -1.83322130e+00 -1.13799432e+00
 -5.52848044e-01 -1.11623836e+00 -1.86865057e+00 -1.78084767e+00
  8.61504546e-01 -1.50908314e+00  2.02896891e+00  5.92529921e-01
  7.69825053e-01 -2.43510747e+00 -1.52346315e+00 -8.51465885e-01
  2.51461519e+01]
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.123885252314
gradient value of function right now is: [ 2.40603768e-05  7.08693439e-04  7.09611621e-04  6.98254271e-04
 -2.40603767e-05 -7.08693439e-04 -7.09611621e-04 -6.98254271e-04
  3.82550632e-06  3.41879214e-05  3.43852500e-05  2.56177845e-05
  1.44652509e-06  1.29061584e-05  1.29807755e-05  9.66677609e-06
  9.39900697e-07  8.38564062e-06  8.43412578e-06  6.28083334e-06
  5.47947332e-06  4.89284596e-05  4.92111083e-05  3.66553951e-05
 -2.84014149e-07 -6.12676414e-08 -1.71184874e-06 -1.73220921e-06
  3.19331848e-07  7.70746815e-08  1.82332645e-06  1.85111155e-06
  1.57140228e-07  3.92727907e-08  8.78294308e-07  8.92842686e-07
  2.33921684e-06  5.16514621e-07  1.39398806e-05  1.41148053e-05
  4.26460112e-07  1.27632138e-07  4.49768947e-07  5.78146131e-07
 -3.91896068e-08 -2.00534040e-08 -4.04801501e-08 -6.72330322e-08
  2.90031420e-07  1.71882658e-07  2.93418013e-07  5.49025416e-07
  3.02525651e-07  1.61204743e-07  3.09001976e-07  5.32809210e-07
  1.46890862e-07 -6.78287001e-09  7.26327066e-08 -1.03994733e-07
  2.06232127e-07 -2.54430923e-08  2.49886274e-07 -1.30239624e-07
 -1.94895870e+00]
supnorm grad right now is: 1.9489587002710451
Weights right now are: 
[-0.75401356 -3.51246804 -2.63305795 -2.31314847  0.74793005  1.74362095
  2.73754012  1.61409233  0.57296636  1.20932773  1.91477044  0.29962363
 -1.49903198 -1.29867182 -2.39041158 -2.5225     -0.73610477 -2.25175243
 -2.39271966 -1.98224006  0.49420127 -2.0251697  -1.55143269 -0.7211529
  0.49288059  1.1158159   1.32486482  0.76603795 -1.60438148 -0.0935506
 -2.19826763 -1.60205141 -0.98651669 -0.63472644 -2.24706841 -1.97796779
 -0.03472252  0.38817734 -0.37484176 -0.69815017  0.32277807  1.56458597
  0.7608139   0.84030725  1.72705625  1.25311623  0.7305172   2.22899411
 -1.35493926 -0.71282158 -1.83814423 -1.14760189 -0.56211445 -1.12154784
 -1.87701731 -1.79956541  0.84178766 -1.50859863  2.01574984  0.60770053
  0.73970068 -2.43100556 -1.53833068 -0.84775586 24.86269372]
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.2510685679745
gradient value of function right now is: [ 9.55295202e-06  2.59017593e-04  2.59279712e-04  2.56244912e-04
 -9.55295200e-06 -2.59017593e-04 -2.59279711e-04 -2.56244912e-04
  1.49610425e-06  1.36665290e-05  1.37359584e-05  1.06465852e-05
  4.73952514e-07  4.32196422e-06  4.34395705e-06  3.36543468e-06
  3.22129467e-07  2.93752097e-06  2.95246928e-06  2.28740264e-06
  1.61604179e-06  1.47478827e-05  1.48228750e-05  1.14861672e-05
 -9.72299749e-08 -2.02203063e-08 -5.80640858e-07 -5.87493108e-07
  9.14098485e-08  2.14177091e-08  5.16908738e-07  5.24803400e-07
  4.15350775e-08  1.01403670e-08  2.29361449e-07  2.33214472e-07
  7.47594543e-07  1.60589675e-07  4.39757491e-06  4.45343632e-06
  1.33867015e-07  3.88571150e-08  1.41397393e-07  1.79892079e-07
 -1.19841469e-08 -6.16028411e-09 -1.23911206e-08 -2.07128786e-08
  9.47698623e-08  5.48466669e-08  9.63166992e-08  1.78062245e-07
  9.63922361e-08  5.04930543e-08  9.87805892e-08  1.69121392e-07
  4.81099007e-08 -1.21284398e-09  2.45970066e-08 -3.27131232e-08
  6.76172745e-08 -7.76030799e-09  8.09674762e-08 -3.95663767e-08
 -5.92453312e-01]
supnorm grad right now is: 0.5924533124884528
Weights right now are: 
[-0.76075513 -3.67684598 -2.78193711 -2.48896874  0.75467161  1.90799889
  2.88641929  1.7899126   0.57002281  1.17523964  1.87859317  0.26558971
 -1.50194802 -1.32563681 -2.41309197 -2.55811872 -0.73741289 -2.26535031
 -2.4048175  -1.99802714  0.48588502 -2.11834989 -1.65234997 -0.81535572
  0.49462234  1.11617201  1.33680277  0.77469244 -1.6091645  -0.09456057
 -2.21649503 -1.6189019  -0.9869722  -0.63483328 -2.24956283 -1.9799518
 -0.05171906  0.38465736 -0.46421023 -0.77047021  0.31485718  1.5620537
  0.75107497  0.830677    1.72761509  1.25342775  0.73106765  2.22999718
 -1.35935605 -0.71563439 -1.8425437  -1.15600728 -0.57003067 -1.12614704
 -1.88423286 -1.81526238  0.82895383 -1.50860605  2.00823686  0.61747652
  0.72080357 -2.42897783 -1.55027299 -0.84466799 24.91325233]
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.23677517186
gradient value of function right now is: [ 3.87108486e-06  9.52338728e-05  9.53103103e-05  9.44631149e-05
 -3.87108495e-06 -9.52338751e-05 -9.53103126e-05 -9.44631171e-05
  5.98399017e-07  5.57381547e-06  5.59930243e-06  4.47429734e-06
  1.58961837e-07  1.47790437e-06  1.48467366e-06  1.18581573e-06
  1.12452281e-07  1.04553918e-06  1.05032808e-06  8.38911857e-07
  4.95966707e-07  4.61449365e-06  4.63561575e-06  3.70318076e-06
 -3.64782468e-08 -7.47327230e-09 -2.15208544e-07 -2.17783942e-07
  2.67039603e-08  6.21440451e-09  1.48846747e-07  1.51181484e-07
  1.01775242e-08  2.51008805e-09  5.49353631e-08  5.59154217e-08
  2.44947863e-07  5.22018130e-08  1.41936761e-06  1.43792248e-06
  4.27477920e-08  1.23554348e-08  4.51574494e-08  5.72783329e-08
 -3.57396923e-09 -1.90605238e-09 -3.68411270e-09 -6.28123115e-09
  3.17178532e-08  1.85029085e-08  3.22162874e-08  5.96608474e-08
  3.14977860e-08  1.67242334e-08  3.22439901e-08  5.54960783e-08
  1.60881499e-08 -5.13185295e-10  8.58431741e-09 -1.09552915e-08
  2.26416542e-08 -2.69362562e-09  2.71411477e-08 -1.35344190e-08
 -8.56821784e-01]
supnorm grad right now is: 0.8568217838680333
Weights right now are: 
[-0.76817607 -3.84232544 -2.93177887 -2.66656178  0.76209256  2.07347836
  3.03626105  1.96750565  0.56682717  1.13747465  1.83853882  0.226566
 -1.50459948 -1.35065689 -2.43413073 -2.59225826 -0.73865504 -2.27852573
 -2.41653403 -2.0138492   0.47902558 -2.19675527 -1.73720021 -0.89736407
  0.49631978  1.11651084  1.34825137  0.7830427  -1.61275447 -0.09530831
 -2.23042366 -1.63183936 -0.98729479 -0.63490775 -2.25129868 -1.98133448
 -0.06662953  0.38162094 -0.54157079 -0.83333508  0.30850247  1.56008276
  0.74345394  0.82288374  1.72805882  1.25367655  0.73150494  2.23079875
 -1.36317832 -0.71800689 -1.84635169 -1.16323782 -0.57612489 -1.12956315
 -1.88992248 -1.82694528  0.82325777 -1.50852463  2.00522154  0.62131214
  0.71269517 -2.42804342 -1.55779825 -0.84234697 24.92311914]
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.13842843917
gradient value of function right now is: [ 1.56390372e-06  3.43863089e-05  3.44085448e-05  3.41689118e-05
 -1.56390349e-06 -3.43863040e-05 -3.44085398e-05 -3.41689069e-05
  2.38441215e-07  2.26569173e-06  2.27526946e-06  1.86118682e-06
  5.30132794e-08  5.02817041e-07  5.04945710e-07  4.12862950e-07
  3.88288182e-08  3.68305977e-07  3.69865165e-07  3.02421175e-07
  1.53250644e-07  1.45448825e-06  1.46064262e-06  1.19447064e-06
 -1.41514091e-08 -2.80506787e-09 -8.34203848e-08 -8.43776521e-08
  7.50042806e-09  1.69503644e-09  4.17714942e-08  4.24051755e-08
  1.87390373e-09  4.74051967e-10  9.82093371e-09  1.00107726e-08
  7.97714402e-08  1.65075808e-08  4.61311997e-07  4.67146468e-07
  1.35856097e-08  3.74633214e-09  1.43813505e-08  1.80298730e-08
 -1.01175759e-09 -5.28922987e-10 -1.04683713e-09 -1.79246016e-09
  1.04856634e-08  5.75928681e-09  1.07302209e-08  1.94951629e-08
  1.02625902e-08  5.15126478e-09  1.05725166e-08  1.79029853e-08
  5.27560575e-09  7.17136335e-11  2.83726717e-09 -3.29324798e-09
  7.51031462e-09 -7.16425083e-10  8.76624956e-09 -3.70813754e-09
  1.88816679e+00]
supnorm grad right now is: 1.888166792608652
Weights right now are: 
[-0.77640376 -4.00766611 -3.0814717  -2.84439586  0.77032025  2.23881902
  3.18595387  2.14533973  0.56333294  1.09540391  1.79394546  0.18196973
 -1.50701212 -1.37385574 -2.45367724 -2.62426259 -0.73983379 -2.2912565
 -2.42786195 -2.02945421  0.4732655  -2.26369586 -1.80952667 -0.96888924
  0.49798699  1.11683923  1.35919938  0.791326   -1.61490055 -0.09576684
 -2.23971547 -1.6406573  -0.98748937 -0.63495329 -2.25232028 -1.98215424
 -0.07886909  0.37914144 -0.60537324 -0.88643808  0.3045866   1.55892195
  0.73903451  0.81790322  1.72833626  1.25382923  0.73178283  2.2312987
 -1.365855   -0.71958523 -1.84904541 -1.16826229 -0.57946611 -1.13133213
 -1.89321635 -1.83299447  0.82122046 -1.50850103  2.00416019  0.62260187
  0.70979425 -2.42773462 -1.56099035 -0.84109814 25.15592536]
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.238102227051
gradient value of function right now is: [ 6.63637060e-07  1.28203335e-05  1.28272629e-05  1.27545742e-05
 -6.63636924e-07 -1.28203309e-05 -1.28272603e-05 -1.27545716e-05
  1.00457300e-07  9.66695359e-07  9.70640146e-07  8.05461054e-07
  1.88150893e-08  1.80679159e-07  1.81417732e-07  1.50470379e-07
  1.41509228e-08  1.35901101e-07  1.36456613e-07  1.13181353e-07
  5.08555148e-08  4.88681835e-07  4.90678384e-07  4.07039742e-07
 -5.98749602e-09 -1.21220814e-09 -3.49275018e-08 -3.53463614e-08
  2.22137370e-09  5.21154330e-10  1.21447150e-08  1.23430959e-08
  7.84875014e-11  4.26614845e-11  1.54279986e-10  1.75896171e-10
  2.84089268e-08  6.02805675e-09  1.62275484e-07  1.64435676e-07
  4.48148477e-09  1.27907224e-09  4.73670048e-09  5.97586756e-09
 -3.01002556e-10 -1.74808701e-10 -3.08062225e-10 -5.53724642e-10
  3.73538750e-09  2.17336507e-09  3.79596891e-09  7.01588190e-09
  3.62056050e-09  1.93118998e-09  3.70544750e-09  6.39467296e-09
  1.87661023e-09 -5.88693111e-11  1.08834320e-09 -1.24259867e-09
  2.68238091e-09 -3.09334792e-10  3.18606881e-09 -1.57434636e-09
 -8.45187546e-01]
supnorm grad right now is: 0.845187546444055
Weights right now are: 
[-0.78570298 -4.17339518 -3.23151719 -3.02289297  0.77961946  2.4045481
  3.33599936  2.32383684  0.55944926  1.04810865  1.74389428  0.13129855
 -1.50916049 -1.39479184 -2.47158952 -2.65127983 -0.74094727 -2.30337808
 -2.43872076 -2.04401912  0.46840424 -2.32014136 -1.8700537  -1.02836273
  0.49935643  1.11711207  1.36769913  0.79855905 -1.61576176 -0.09596184
 -2.24406309 -1.64495312 -0.98756063 -0.63497241 -2.25266583 -1.9824415
 -0.08674221  0.37750996 -0.64799107 -0.92479839  0.30294661  1.55845298
  0.73727363  0.81573951  1.72845052  1.25389329  0.73189903  2.23150701
 -1.3671293  -0.72032588 -1.8503358  -1.1706638  -0.5807794  -1.13202392
 -1.89454997 -1.83532919  0.82051581 -1.50847999  2.00377109  0.62304723
  0.70879333 -2.42761936 -1.56216113 -0.84056112 24.93469342]
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.06540515809
gradient value of function right now is: [ 2.83545541e-07  4.78296749e-06  4.78528067e-06  4.76174837e-06
 -2.83545638e-07 -4.78296912e-06 -4.78528230e-06 -4.76174999e-06
  4.28510575e-08  4.15278631e-07  4.16951341e-07  3.48289465e-07
  6.92457929e-09  6.69559489e-08  6.72261506e-08  5.61261161e-08
  5.27674320e-09  5.10272279e-08  5.12331403e-08  4.27747477e-08
  1.78776452e-08  1.72977432e-07  1.73675118e-07  1.45021003e-07
 -2.59336189e-09 -5.31678945e-10 -1.50455438e-08 -1.52306023e-08
  6.77581515e-10  1.63522550e-10  3.65300531e-09  3.71615966e-09
 -1.76887719e-10 -3.03860100e-11 -1.08913552e-09 -1.09830891e-09
  1.07124338e-08  2.30384155e-09  6.08213682e-08  6.16529339e-08
  1.55697492e-09  4.50874918e-10  1.64446578e-09  2.07936799e-09
 -9.37527984e-11 -5.88795156e-11 -9.50681283e-11 -1.78609042e-10
  1.38943577e-09  8.28205566e-10  1.40699698e-09  2.61954974e-09
  1.34214403e-09  7.35245308e-10  1.36884820e-09  2.38351547e-09
  6.95752125e-10 -3.76458785e-11  4.25656948e-10 -4.68211037e-10
  1.00087048e-09 -1.24274347e-10  1.19629644e-09 -6.28378598e-10
 -2.33848532e+00]
supnorm grad right now is: 2.3384853175927516
Weights right now are: 
[-0.79622595 -4.33777118 -3.38046802 -3.19994375  0.79014244  2.56892413
  3.48495023  2.50088765  0.55527272  0.99805972  1.69134519  0.07948193
 -1.51074619 -1.41030777 -2.48564292 -2.66759401 -0.74188506 -2.31335303
 -2.44793792 -2.05458836  0.46476415 -2.36023216 -1.91196703 -1.06691942
  0.50012369  1.11726351  1.37226232  0.80295316 -1.61602435 -0.09602066
 -2.24548718 -1.64639137 -0.98754446 -0.634971   -2.25254874 -1.98233302
 -0.09025416  0.37678825 -0.66786696 -0.94426036  0.30237804  1.55829559
  0.73667047  0.81498686  1.72848709  1.25391277  0.73193651  2.23157387
 -1.36761379 -0.72058458 -1.85082981 -1.17156029 -0.58125257 -1.13225555
 -1.8950354  -1.83615391  0.82027204 -1.50847987  2.003634    0.62317832
  0.70844173 -2.42758277 -1.56256573 -0.84040484 24.80206101]
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.21590271392
gradient value of function right now is: [ 1.16095464e-07  1.75862385e-06  1.75943124e-06  1.75137853e-06
 -1.16095470e-07 -1.75862394e-06 -1.75943133e-06 -1.75137862e-06
  1.75718434e-08  1.71315253e-07  1.71998998e-07  1.44140123e-07
  2.57469887e-09  2.50488938e-08  2.51490308e-08  2.10652612e-08
  1.96846276e-09  1.91526740e-08  1.92292371e-08  1.61070972e-08
  6.53478733e-09  6.36135274e-08  6.38677196e-08  5.35041120e-08
 -1.07111271e-09 -2.12928894e-10 -6.26680236e-09 -6.33971165e-09
  2.19293883e-10  5.16013362e-11  1.19137536e-09  1.21111965e-09
 -1.15026610e-10 -2.17241050e-11 -6.83658076e-10 -6.90866147e-10
  4.06883994e-09  8.46773664e-10  2.33269867e-08  2.36277759e-08
  5.75519709e-10  1.57651280e-10  6.09423155e-10  7.60158527e-10
 -3.04693679e-11 -1.84955916e-11 -3.11482573e-11 -5.82909567e-11
  5.22882542e-10  2.88516211e-10  5.34764746e-10  9.69014784e-10
  5.05457090e-10  2.56497664e-10  5.20144082e-10  8.82322471e-10
  2.58923427e-10  2.05041051e-12  1.55416485e-10 -1.54932553e-10
  3.77534878e-10 -3.44125233e-11  4.35245745e-10 -1.85741846e-10
  1.23018340e+00]
supnorm grad right now is: 1.230183404955179
Weights right now are: 
[-0.80784579 -4.49971054 -3.52813709 -3.37342772  0.80176228  2.73086351
  3.63261932  2.67437165  0.55150141  0.95651388  1.64862792  0.04069741
 -1.51158333 -1.41845269 -2.49355989 -2.67482781 -0.74247092 -2.31924662
 -2.45367077 -2.05991575  0.462719   -2.38094525 -1.93301257 -1.08498918
  0.50047836  1.11733594  1.37433486  0.80503384 -1.61610922 -0.09604159
 -2.2459444  -1.6468562  -0.98751486 -0.6349657  -2.25236996 -1.98215652
 -0.09167288  0.37648307 -0.67594224 -0.9523996   0.30217302  1.55823643
  0.73645354  0.81471508  1.72849873  1.25392059  0.73194838  2.23159614
 -1.36779677 -0.72069567 -1.85101635 -1.17190295 -0.58142986 -1.13235464
 -1.89521729 -1.83646652  0.82018114 -1.50847849  2.00357705  0.62324578
  0.70831021 -2.42756963 -1.56272288 -0.84032129 25.08617654]
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.084774989451
gradient value of function right now is: [ 5.20573113e-08  7.44067142e-07  7.44401615e-07  7.41096148e-07
 -5.20573198e-08 -7.44067263e-07 -7.44401737e-07 -7.41096269e-07
  7.93136427e-09  7.75015516e-08  7.78098339e-08  6.52890197e-08
  1.11618606e-09  1.08842163e-08  1.09275781e-08  9.16472824e-09
  8.53929729e-10  8.32765844e-09  8.36083396e-09  7.01220964e-09
  2.82337399e-09  2.75471498e-08  2.76568495e-08  2.31983223e-08
 -4.85004758e-10 -9.55503426e-11 -2.84434098e-09 -2.87689445e-09
  9.16154098e-11  2.14036841e-11  4.98630531e-10  5.06800088e-10
 -5.74377539e-11 -1.09520266e-11 -3.39814940e-10 -3.43484966e-10
  1.79384427e-09  3.69686932e-10  1.03127753e-08  1.04434387e-08
  2.51779191e-10  6.78351348e-11  2.66803582e-10  3.31466184e-10
 -1.26399683e-11 -7.60708884e-12 -1.29508090e-11 -2.42728539e-11
  2.30144595e-10  1.23946330e-10  2.36028565e-10  4.24428806e-10
  2.22440774e-10  1.10191648e-10  2.29478197e-10  3.86404994e-10
  1.13461459e-10  2.91390090e-12  6.79197304e-11 -6.55692900e-11
  1.66062255e-10 -1.35879021e-11  1.89798780e-10 -7.57472434e-11
  2.25557403e+00]
supnorm grad right now is: 2.2555740281416896
Weights right now are: 
[-8.18428855e-01 -4.64192506e+00 -3.66142907e+00 -3.52220840e+00
  8.12345346e-01  2.87307805e+00  3.76591131e+00  2.82315235e+00
  5.49203952e-01  9.33332893e-01  1.62521843e+00  2.07105125e-02
 -1.51194249e+00 -1.42194803e+00 -2.49705048e+00 -2.67779306e+00
 -7.42741418e-01 -2.32189575e+00 -2.45631504e+00 -2.06217165e+00
  4.61816352e-01 -2.38979420e+00 -1.94191381e+00 -1.09247836e+00
  5.00633180e-01  1.11736714e+00  1.37523622e+00  8.05944988e-01
 -1.61613996e+00 -9.60489851e-02 -2.24611011e+00 -1.64702474e+00
 -9.87497482e-01 -6.34962348e-01 -2.25226758e+00 -1.98205337e+00
 -9.22545680e-02  3.76360236e-01 -6.79258081e-01 -9.55756924e-01
  3.02091960e-01  1.55821373e+00  7.36367714e-01  8.14607443e-01
  1.72850297e+00  1.25392330e+00  7.31952704e-01  2.23160434e+00
 -1.36787152e+00 -7.20738338e-01 -1.85109268e+00 -1.17204238e+00
 -5.81501959e-01 -1.13239255e+00 -1.89529138e+00 -1.83659328e+00
  8.20143738e-01 -1.50847830e+00  2.00355451e+00  6.23268944e-01
  7.08255637e-01 -2.42756450e+00 -1.56278557e+00 -8.40293157e-01
  2.51973133e+01]
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.233285241562
gradient value of function right now is: [ 2.97852257e-08  4.13086016e-07  4.13270110e-07  4.11459896e-07
 -2.97852473e-08 -4.13086316e-07 -4.13270410e-07 -4.11460195e-07
  4.57354541e-09  4.46382893e-08  4.48161860e-08  3.76095606e-08
  6.34413337e-10  6.17782004e-09  6.20248582e-09  5.20236552e-09
  4.85820233e-10  4.73129526e-09  4.75018480e-09  3.98433170e-09
  1.60255040e-09  1.56149172e-08  1.56772321e-08  1.31512234e-08
 -2.81368068e-10 -5.69698366e-11 -1.63704338e-09 -1.65674161e-09
  5.27613741e-11  1.27526297e-11  2.83570861e-10  2.88507269e-10
 -3.40418239e-11 -6.69304485e-12 -1.99674166e-10 -2.01955256e-10
  1.03679471e-09  2.19894917e-10  5.90840484e-09  5.98722110e-09
  1.42930233e-10  4.02386347e-11  1.51164723e-10  1.89580359e-10
 -7.23288102e-12 -4.68262529e-12 -7.32802146e-12 -1.41361416e-11
  1.33088607e-10  7.66475594e-11  1.35389368e-10  2.48528287e-10
  1.28322299e-10  6.80193886e-11  1.31399061e-10  2.25884002e-10
  6.58875011e-11 -1.74764436e-12  4.07201987e-11 -4.18728681e-11
  9.59467114e-11 -1.03207029e-11  1.12272356e-10 -5.41152541e-11
 -9.03410371e-01]
supnorm grad right now is: 0.9034103713499472
Weights right now are: 
[-8.25839123e-01 -4.74336815e+00 -3.76060269e+00 -3.62489826e+00
  8.19755615e-01  2.97452113e+00  3.86508493e+00  2.92584220e+00
  5.47973749e-01  9.21259630e-01  1.61308640e+00  1.05040283e-02
 -1.51211671e+00 -1.42364474e+00 -2.49875274e+00 -2.67922318e+00
 -7.42874471e-01 -2.32319280e+00 -2.45761622e+00 -2.06326551e+00
  4.61376468e-01 -2.39408482e+00 -1.94622268e+00 -1.09609419e+00
  5.00709680e-01  1.11738258e+00  1.37568282e+00  8.06396824e-01
 -1.61615447e+00 -9.60525369e-02 -2.24618821e+00 -1.64710419e+00
 -9.87488356e-01 -6.34960577e-01 -2.25221379e+00 -1.98199900e+00
 -9.25373509e-02  3.76300251e-01 -6.80874412e-01 -9.57394437e-01
  3.02052713e-01  1.55820277e+00  7.36326145e-01  8.14555553e-01
  1.72850494e+00  1.25392462e+00  7.31954707e-01  2.23160819e+00
 -1.36790748e+00 -7.20759268e-01 -1.85112945e+00 -1.17210951e+00
 -5.81536712e-01 -1.13241118e+00 -1.89532713e+00 -1.83665438e+00
  8.20125957e-01 -1.50847825e+00  2.00354336e+00  6.23281301e-01
  7.08229739e-01 -2.42756224e+00 -1.56281612e+00 -8.40277637e-01
  2.49322723e+01]
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5143.964707069492
gradient value of function right now is: [ 1.98159709e-08  2.69850995e-07  2.69970342e-07  2.68799439e-07
 -1.98159749e-08 -2.69851050e-07 -2.69970396e-07 -2.68799493e-07
  3.04815705e-09  2.98344920e-08  2.99528847e-08  2.51569672e-08
  4.18328863e-10  4.08603011e-09  4.10226948e-09  3.44377690e-09
  3.20648684e-10  3.13222889e-09  3.14467706e-09  2.63995481e-09
  1.05627910e-09  1.03229869e-08  1.03639975e-08  8.70152048e-09
 -1.86454279e-10 -3.65473420e-11 -1.09494075e-09 -1.10735410e-09
  3.41497839e-11  7.94427662e-12  1.86128247e-10  1.89154084e-10
 -2.28644381e-11 -4.36247913e-12 -1.35145097e-10 -1.36609495e-10
  6.83095541e-10  1.40019067e-10  3.93325225e-09  3.98259067e-09
  9.57491081e-11  2.55692760e-11  1.01505356e-10  1.25825973e-10
 -4.70359863e-12 -2.81421360e-12 -4.82742161e-12 -9.03587751e-12
  8.76518509e-11  4.66209700e-11  9.00388232e-11  1.61212218e-10
  8.47274607e-11  4.14540044e-11  8.75371989e-11  1.46778851e-10
  4.31213012e-11  1.55854976e-12  2.57484337e-11 -2.44819696e-11
  6.32148595e-11 -4.82824888e-12  7.19323659e-11 -2.75994784e-11
  2.90540757e+00]
supnorm grad right now is: 2.9054075668274217
Weights right now are: 
[-8.30718300e-01 -4.81015476e+00 -3.82714908e+00 -3.69160478e+00
  8.24634791e-01  3.04130775e+00  3.93163132e+00  2.99254872e+00
  5.47214001e-01  9.13838488e-01  1.60563483e+00  4.24776016e-03
 -1.51222185e+00 -1.42466868e+00 -2.49978067e+00 -2.68008567e+00
 -7.42954998e-01 -2.32397717e+00 -2.45840364e+00 -2.06392626e+00
  4.61111004e-01 -2.39667193e+00 -1.94882020e+00 -1.09827357e+00
  5.00756521e-01  1.11739209e+00  1.37595524e+00  8.06672523e-01
 -1.61616323e+00 -9.60546642e-02 -2.24623522e+00 -1.64715203e+00
 -9.87482663e-01 -6.34959455e-01 -2.25218041e+00 -1.98196524e+00
 -9.27098078e-02  3.76263571e-01 -6.81856481e-01 -9.58389639e-01
  3.02028977e-01  1.55819609e+00  7.36301042e-01  8.14524058e-01
  1.72850613e+00  1.25392539e+00  7.31955920e-01  2.23161054e+00
 -1.36792958e+00 -7.20772000e-01 -1.85115191e+00 -1.17215088e+00
 -5.81558024e-01 -1.13242248e+00 -1.89534894e+00 -1.83669198e+00
  8.20115044e-01 -1.50847796e+00  2.00353643e+00  6.23288521e-01
  7.08213891e-01 -2.42756043e+00 -1.56283505e+00 -8.40268156e-01
  2.51711059e+01]
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.263611010232
gradient value of function right now is: [ 1.52638068e-08  2.05063056e-07  2.05153458e-07  2.04268638e-07
 -1.52638387e-08 -2.05063484e-07 -2.05153887e-07 -2.04269064e-07
  2.35852527e-09  2.30534371e-08  2.31451152e-08  1.94369818e-08
  3.21834795e-10  3.13874733e-09  3.15125153e-09  2.64501803e-09
  2.46852073e-10  2.40769470e-09  2.41728614e-09  2.02900659e-09
  8.11889604e-10  7.92279207e-09  7.95434064e-09  6.67743711e-09
 -1.44938376e-10 -2.90948104e-11 -8.45346776e-10 -8.55359817e-10
  2.66110953e-11  6.37284634e-12  1.43519156e-10  1.45976449e-10
 -1.79151576e-11 -3.50480864e-12 -1.05172435e-10 -1.06364973e-10
  5.30893150e-10  1.11574240e-10  3.03388610e-09  3.07369634e-09
  7.33528719e-11  2.03652113e-11  7.76304032e-11  9.70269370e-11
 -3.65324606e-12 -2.32639421e-12 -3.71331940e-12 -7.11509507e-12
  6.81715255e-11  3.84751356e-11  6.95346517e-11  1.26731769e-10
  6.57638333e-11  3.41591468e-11  6.75053411e-11  1.15223721e-10
  3.36791135e-11 -3.20407715e-13  2.06123965e-11 -2.07086217e-11
  4.91580565e-11 -4.86635243e-12  5.69974053e-11 -2.58608200e-11
  1.48457448e-01]
supnorm grad right now is: 0.14845744762522797
Weights right now are: 
[-8.34307260e-01 -4.85869658e+00 -3.87568510e+00 -3.73997596e+00
  8.28223751e-01  3.08984957e+00  3.98016734e+00  3.04091990e+00
  5.46659053e-01  9.08418631e-01  1.60019331e+00 -3.20850619e-04
 -1.51229793e+00 -1.42540984e+00 -2.50052478e+00 -2.68071007e+00
 -7.43013322e-01 -2.32454546e+00 -2.45897420e+00 -2.06440505e+00
  4.60919054e-01 -2.39854324e+00 -1.95069898e+00 -1.09985029e+00
  5.00790709e-01  1.11739902e+00  1.37615396e+00  8.06873641e-01
 -1.61616957e+00 -9.60562059e-02 -2.24626921e+00 -1.64718661e+00
 -9.87478464e-01 -6.34958628e-01 -2.25215581e+00 -1.98194037e+00
 -9.28353487e-02  3.76236893e-01 -6.82570990e-01 -9.59113744e-01
  3.02011724e-01  1.55819125e+00  7.36282802e-01  8.14501140e-01
  1.72850700e+00  1.25392596e+00  7.31956794e-01  2.23161225e+00
 -1.36794562e+00 -7.20781235e-01 -1.85116824e+00 -1.17218103e+00
 -5.81573495e-01 -1.13243067e+00 -1.89536480e+00 -1.83671937e+00
  8.20107045e-01 -1.50847778e+00  2.00353142e+00  6.23293727e-01
  7.08202245e-01 -2.42755921e+00 -1.56284882e+00 -8.40261421e-01
  2.50023966e+01]
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5143.678771382508
gradient value of function right now is: [ 1.20492688e-08  1.60243242e-07  1.60313534e-07  1.59626223e-07
 -1.20492830e-08 -1.60243431e-07 -1.60313723e-07 -1.59626411e-07
  1.86166411e-09  1.82458422e-08  1.83181197e-08  1.53939426e-08
  2.52252890e-10  2.46730104e-09  2.47708935e-09  2.08068465e-09
  1.93591435e-10  1.89370399e-09  1.90121647e-09  1.59700285e-09
  6.36245851e-10  6.22655198e-09  6.25124421e-09  5.25154009e-09
 -1.13691107e-10 -2.20688374e-11 -6.69663800e-10 -6.77111615e-10
  2.04416785e-11  4.71201491e-12  1.11873249e-10  1.13657302e-10
 -1.41826481e-11 -2.68550996e-12 -8.39825415e-11 -8.48805163e-11
  4.14384872e-10  8.40943134e-11  2.39405150e-09  2.42349928e-09
  5.82900733e-11  1.53235856e-11  6.18415598e-11  7.63472282e-11
 -2.81683672e-12 -1.65979219e-12 -2.90259145e-12 -5.38689740e-12
  5.31394401e-11  2.76681142e-11  5.47571708e-11  9.72268559e-11
  5.14090221e-11  2.46233507e-11  5.32656206e-11  8.85744624e-11
  2.60640917e-11  1.47008886e-12  1.54078122e-11 -1.43885445e-11
  3.83018545e-11 -2.53462849e-12  4.31799258e-11 -1.53243132e-11
  4.13906695e+00]
supnorm grad right now is: 4.139066947226931
Weights right now are: 
[-8.37054773e-01 -4.89542984e+00 -3.91243179e+00 -3.77656891e+00
  8.30971265e-01  3.12658282e+00  4.01691403e+00  3.07751285e+00
  5.46233124e-01  9.04264061e-01  1.59602214e+00 -3.82241258e-03
 -1.51235601e+00 -1.42597491e+00 -2.50109211e+00 -2.68118604e+00
 -7.43057880e-01 -2.32497901e+00 -2.45940948e+00 -2.06477024e+00
  4.60772629e-01 -2.39996876e+00 -1.95213021e+00 -1.10105123e+00
  5.00817001e-01  1.11740448e+00  1.37630614e+00  8.07027729e-01
 -1.61617447e+00 -9.60574497e-02 -2.24629523e+00 -1.64721312e+00
 -9.87475215e-01 -6.34957975e-01 -2.25213685e+00 -1.98192118e+00
 -9.29319104e-02  3.76215777e-01 -6.83117718e-01 -9.59668087e-01
  3.01998612e-01  1.55818741e+00  7.36268942e-01  8.14483665e-01
  1.72850765e+00  1.25392643e+00  7.31957459e-01  2.23161357e+00
 -1.36795790e+00 -7.20788815e-01 -1.85118071e+00 -1.17220429e+00
 -5.81585333e-01 -1.13243741e+00 -1.89537689e+00 -1.83674051e+00
  8.20100952e-01 -1.50847750e+00  2.00352739e+00  6.23298361e-01
  7.08193431e-01 -2.42755822e+00 -1.56285966e+00 -8.40255028e-01
  2.52752265e+01]
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.26224473899
gradient value of function right now is: [ 1.03834100e-08  1.36890233e-07  1.36950211e-07  1.36364796e-07
 -1.03834331e-08 -1.36890538e-07 -1.36950517e-07 -1.36365100e-07
  1.61075279e-09  1.57531988e-08  1.58157956e-08  1.32865606e-08
  2.17582773e-10  2.12319635e-09  2.13164809e-09  1.78983393e-09
  1.67051323e-10  1.63026030e-09  1.63674956e-09  1.37432455e-09
  5.48336851e-10  5.35389722e-09  5.37519959e-09  4.51389658e-09
 -9.90113571e-11 -1.98597781e-11 -5.77587525e-10 -5.84418816e-10
  1.79456178e-11  4.29797702e-12  9.67803516e-11  9.84372882e-11
 -1.24172507e-11 -2.43286688e-12 -7.28432464e-11 -7.36721811e-11
  3.61290111e-10  7.58678479e-11  2.06514310e-09  2.09219845e-09
  4.98408729e-11  1.38137882e-11  5.27522619e-11  6.59014783e-11
 -2.46294899e-12 -1.57075671e-12 -2.50400688e-12 -4.80196188e-12
  4.63839125e-11  2.61309610e-11  4.73284055e-11  8.61896296e-11
  4.47448458e-11  2.32004756e-11  4.59445270e-11  7.83620279e-11
  2.29068636e-11 -1.68682324e-13  1.40294326e-11 -1.40624534e-11
  3.34519870e-11 -3.26856012e-12  3.87564294e-11 -1.74949311e-11
  2.81358733e-01]
supnorm grad right now is: 0.28135873257013533
Weights right now are: 
[-8.39338632e-01 -4.92567800e+00 -3.94269294e+00 -3.80670080e+00
  8.33255123e-01  3.15683098e+00  4.04717518e+00  3.10764474e+00
  5.45878569e-01  9.00803324e-01  1.59254758e+00 -6.73998539e-03
 -1.51240409e+00 -1.42644306e+00 -2.50156214e+00 -2.68158049e+00
 -7.43094787e-01 -2.32533838e+00 -2.45977029e+00 -2.06507305e+00
  4.60651457e-01 -2.40114930e+00 -1.95331546e+00 -1.10204606e+00
  5.00838853e-01  1.11740899e+00  1.37643294e+00  8.07156092e-01
 -1.61617850e+00 -9.60584651e-02 -2.24631672e+00 -1.64723500e+00
 -9.87472492e-01 -6.34957431e-01 -2.25212094e+00 -1.98190508e+00
 -9.30119326e-02  3.76198416e-01 -6.83572174e-01 -9.60128789e-01
  3.01987697e-01  1.55818425e+00  7.36257411e-01  8.14469166e-01
  1.72850820e+00  1.25392682e+00  7.31958010e-01  2.23161464e+00
 -1.36796811e+00 -7.20795092e-01 -1.85119107e+00 -1.17222350e+00
 -5.81595177e-01 -1.13244299e+00 -1.89538695e+00 -1.83675796e+00
  8.20095894e-01 -1.50847732e+00  2.00352415e+00  6.23302023e-01
  7.08186093e-01 -2.42755736e+00 -1.56286845e+00 -8.40250391e-01
  2.49916235e+01]
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.069675617547
gradient value of function right now is: [ 9.11347477e-09  1.19255275e-07  1.19307461e-07  1.18798840e-07
 -9.11351111e-09 -1.19255750e-07 -1.19307936e-07 -1.18799313e-07
  1.41808174e-09  1.38492179e-08  1.39043611e-08  1.16783766e-08
  1.90983363e-10  1.86070832e-09  1.86813165e-09  1.56819767e-09
  1.46680543e-10  1.42921549e-09  1.43491712e-09  1.20456494e-09
  4.80976286e-10  4.68896721e-09  4.70766473e-09  3.95240547e-09
 -8.75609355e-11 -1.79468885e-11 -5.07426294e-10 -5.13673899e-10
  1.59361334e-11  3.91211381e-12  8.50803993e-11  8.66054600e-11
 -1.10377929e-11 -2.21262479e-12 -6.43310442e-11 -6.50940807e-11
  3.19631092e-10  6.86442774e-11  1.81372239e-09  1.83848176e-09
  4.35012952e-11  1.24795230e-11  4.59664505e-11  5.78947980e-11
 -2.18032251e-12 -1.46006973e-12 -2.19622731e-12 -4.30876024e-12
  4.10504674e-11  2.42893450e-11  4.16040772e-11  7.70923510e-11
  3.95261548e-11  2.15348233e-11  4.03339997e-11  7.00033975e-11
  2.03568380e-11 -1.02158963e-12  1.28023088e-11 -1.34050207e-11
  2.95877048e-11 -3.53021572e-12  3.50009253e-11 -1.80755779e-11
 -2.31926614e+00]
supnorm grad right now is: 2.319266142188081
Weights right now are: 
[-8.41296623e-01 -4.95140602e+00 -3.96843219e+00 -3.83233024e+00
  8.35213114e-01  3.18255900e+00  4.07291442e+00  3.13327417e+00
  5.45574527e-01  8.97830500e-01  1.58956294e+00 -9.24727085e-03
 -1.51244510e+00 -1.42684310e+00 -2.50196376e+00 -2.68191771e+00
 -7.43126275e-01 -2.32564559e+00 -2.46007872e+00 -2.06533202e+00
  4.60548143e-01 -2.40215778e+00 -1.95432796e+00 -1.10289629e+00
  5.00857572e-01  1.11741276e+00  1.37654192e+00  8.07266368e-01
 -1.61618189e+00 -9.60592825e-02 -2.24633496e+00 -1.64725356e+00
 -9.87470139e-01 -6.34956968e-01 -2.25210716e+00 -1.98189114e+00
 -9.30802299e-02  3.76184001e-01 -6.83961662e-01 -9.60523425e-01
  3.01978308e-01  1.55818163e+00  7.36247479e-01  8.14456729e-01
  1.72850866e+00  1.25392712e+00  7.31958481e-01  2.23161556e+00
 -1.36797687e+00 -7.20800048e-01 -1.85120000e+00 -1.17223984e+00
 -5.81603626e-01 -1.13244739e+00 -1.89539562e+00 -1.83677281e+00
  8.20091563e-01 -1.50847725e+00  2.00352143e+00  6.23304829e-01
  7.08179775e-01 -2.42755670e+00 -1.56287587e+00 -8.40246834e-01
  2.48419400e+01]
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.042639568935
gradient value of function right now is: [ 7.79244175e-09  1.01346055e-07  1.01390221e-07  1.00959863e-07
 -7.79240399e-09 -1.01345564e-07 -1.01389731e-07 -1.00959375e-07
  1.21062148e-09  1.18612305e-08  1.19082306e-08  1.00093221e-08
  1.62131239e-10  1.58516347e-09  1.59145442e-09  1.33702582e-09
  1.24566285e-10  1.21800348e-09  1.22283713e-09  1.02736291e-09
  4.08351153e-10  3.99469881e-09  4.01054593e-09  3.36981377e-09
 -7.41715145e-11 -1.45795860e-11 -4.35131645e-10 -4.40092476e-10
  1.31873531e-11  3.08119801e-12  7.17268690e-11  7.29034706e-11
 -9.41851055e-12 -1.81272563e-12 -5.54992180e-11 -5.61114806e-11
  2.69336496e-10  5.53571768e-11  1.54927622e-09  1.56881342e-09
  3.75038284e-11  1.00558680e-11  3.97509589e-11  4.93098212e-11
 -1.81579286e-12 -1.09925994e-12 -1.86030719e-12 -3.50474556e-12
  3.45730615e-11  1.85049137e-11  3.54818585e-11  6.36463566e-11
  3.34010435e-11  1.64472113e-11  3.44790801e-11  5.79260087e-11
  1.70043366e-11  5.11233371e-13  1.01970255e-11 -9.68991946e-12
  2.49383735e-11 -1.97266148e-12  2.83972667e-11 -1.11063473e-11
  2.49311615e+00]
supnorm grad right now is: 2.493116149160691
Weights right now are: 
[-8.42973505e-01 -4.97328819e+00 -3.99032392e+00 -3.85412880e+00
  8.36889996e-01  3.20444117e+00  4.09480615e+00  3.15507273e+00
  5.45313637e-01  8.95280461e-01  1.58700276e+00 -1.13980690e-02
 -1.51248016e+00 -1.42718501e+00 -2.50230704e+00 -2.68220593e+00
 -7.43153208e-01 -2.32590826e+00 -2.46034243e+00 -2.06555345e+00
  4.60459845e-01 -2.40301934e+00 -1.95519296e+00 -1.10362269e+00
  5.00873632e-01  1.11741602e+00  1.37663538e+00  8.07360961e-01
 -1.61618480e+00 -9.60599903e-02 -2.24635055e+00 -1.64726943e+00
 -9.87468109e-01 -6.34956565e-01 -2.25209528e+00 -1.98187913e+00
 -9.31387571e-02  3.76171548e-01 -6.84295309e-01 -9.60861531e-01
  3.01970285e-01  1.55817936e+00  7.36238998e-01  8.14446104e-01
  1.72850906e+00  1.25392739e+00  7.31958884e-01  2.23161633e+00
 -1.36798439e+00 -7.20804441e-01 -1.85120764e+00 -1.17225385e+00
 -5.81610872e-01 -1.13245129e+00 -1.89540303e+00 -1.83678555e+00
  8.20087854e-01 -1.50847715e+00  2.00351914e+00  6.23307198e-01
  7.08174366e-01 -2.42755611e+00 -1.56288218e+00 -8.40243800e-01
  2.51314657e+01]
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.163615577287
gradient value of function right now is: [ 7.00543113e-09  9.05958039e-08  9.06352271e-08  9.02514449e-08
 -7.00542430e-09 -9.05957153e-08 -9.06351385e-08 -9.02513567e-08
  1.09009487e-09  1.06769878e-08  1.07193158e-08  9.01002074e-09
  1.45565258e-10  1.42268241e-09  1.42833165e-09  1.19997410e-09
  1.11869420e-10  1.09345903e-09  1.09780082e-09  9.22308252e-10
  3.66472796e-10  3.58375805e-09  3.59798263e-09  3.02314830e-09
 -6.68812570e-11 -1.32319514e-11 -3.91613048e-10 -3.96132333e-10
  1.18691317e-11  2.79542529e-12  6.43679417e-11  6.54391191e-11
 -8.53284171e-12 -1.65424567e-12 -5.01749409e-11 -5.07360390e-11
  2.42671899e-10  5.02149059e-11  1.39298055e-09  1.41076583e-09
  3.36441921e-11  9.11524165e-12  3.56424316e-11  4.43142123e-11
 -1.63303640e-12 -1.00634947e-12 -1.66844663e-12 -3.16411131e-12
  3.11582009e-11  1.69521290e-11  3.19139692e-11  5.75224927e-11
  3.00835090e-11  1.50601878e-11  3.09979892e-11  5.23298826e-11
  1.53462516e-11  2.66018077e-13  9.25728167e-12 -8.92827112e-12
  2.24866331e-11 -1.92312571e-12  2.57071678e-11 -1.05204100e-11
  1.73019082e+00]
supnorm grad right now is: 1.7301908176624794
Weights right now are: 
[-8.44469141e-01 -4.99268854e+00 -4.00973272e+00 -3.87345524e+00
  8.38385632e-01  3.22384151e+00  4.11421496e+00  3.17439917e+00
  5.45080667e-01  8.93002828e-01  1.58471608e+00 -1.33192896e-02
 -1.51251136e+00 -1.42748935e+00 -2.50261259e+00 -2.68246252e+00
 -7.43177183e-01 -2.32614213e+00 -2.46057724e+00 -2.06575062e+00
  4.60381298e-01 -2.40378595e+00 -1.95596261e+00 -1.10426909e+00
  5.00888004e-01  1.11741893e+00  1.37671886e+00  8.07445446e-01
 -1.61618738e+00 -9.60606144e-02 -2.24636440e+00 -1.64728352e+00
 -9.87466282e-01 -6.34956202e-01 -2.25208462e+00 -1.98186834e+00
 -9.31910680e-02  3.76160459e-01 -6.84592874e-01 -9.61163092e-01
  3.01963143e-01  1.55817736e+00  7.36231442e-01  8.14436607e-01
  1.72850941e+00  1.25392761e+00  7.31959242e-01  2.23161703e+00
 -1.36799109e+00 -7.20808248e-01 -1.85121446e+00 -1.17226644e+00
 -5.81617329e-01 -1.13245466e+00 -1.89540965e+00 -1.83679698e+00
  8.20084516e-01 -1.50847707e+00  2.00351704e+00  6.23309263e-01
  7.08169504e-01 -2.42755562e+00 -1.56288794e+00 -8.40241040e-01
  2.51314518e+01]
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.166657095634
gradient value of function right now is: [ 6.33384647e-09  8.14903713e-08  8.15257742e-08  8.11813874e-08
 -6.33380805e-09 -8.14898773e-08 -8.15252799e-08 -8.11808953e-08
  9.86629441e-10  9.66487003e-09  9.70317862e-09  8.15665375e-09
  1.31357673e-10  1.28399063e-09  1.28908824e-09  1.08308990e-09
  1.00976422e-10  9.87114036e-10  9.91032859e-10  8.32682907e-10
  3.30595368e-10  3.23332798e-09  3.24615934e-09  2.72777926e-09
 -6.05429637e-11 -1.19786691e-11 -3.54492663e-10 -3.58583994e-10
  1.06981756e-11  2.52040820e-12  5.80098261e-11  5.89757652e-11
 -7.75992680e-12 -1.50561613e-12 -4.56163329e-11 -4.61273222e-11
  2.19395702e-10  4.54003939e-11  1.25935488e-09  1.27543507e-09
  3.03951483e-11  8.23479871e-12  3.21999849e-11  4.00324037e-11
 -1.47181173e-12 -9.08036351e-13 -1.50352707e-12 -2.85308694e-12
  2.81683028e-11  1.53288182e-11  2.88503932e-11  5.20002581e-11
  2.71958870e-11  1.36177101e-11  2.80214903e-11  4.73052126e-11
  1.38709635e-11  2.37036976e-13  8.37016075e-12 -8.06939811e-12
  2.03269562e-11 -1.74039366e-12  2.32360825e-11 -9.51302153e-12
  1.71437683e+00]
supnorm grad right now is: 1.7143768277129308
Weights right now are: 
[-8.45832822e-01 -5.01028224e+00 -4.02733408e+00 -3.89098211e+00
  8.39749313e-01  3.24143522e+00  4.13181632e+00  3.19192605e+00
  5.44868141e-01  8.90923473e-01  1.58262846e+00 -1.50736853e-02
 -1.51253973e+00 -1.42776623e+00 -2.50289057e+00 -2.68269601e+00
 -7.43198983e-01 -2.32635496e+00 -2.46079091e+00 -2.06593010e+00
  4.60309912e-01 -2.40448321e+00 -1.95666265e+00 -1.10485716e+00
  5.00901079e-01  1.11742156e+00  1.37679509e+00  8.07522586e-01
 -1.61618972e+00 -9.60611754e-02 -2.24637696e+00 -1.64729630e+00
 -9.87464612e-01 -6.34955874e-01 -2.25207484e+00 -1.98185845e+00
 -9.32385485e-02  3.76150472e-01 -6.84864115e-01 -9.61437908e-01
  3.01956612e-01  1.55817555e+00  7.36224535e-01  8.14427969e-01
  1.72850973e+00  1.25392782e+00  7.31959565e-01  2.23161766e+00
 -1.36799717e+00 -7.20811706e-01 -1.85122066e+00 -1.17227777e+00
 -5.81623196e-01 -1.13245773e+00 -1.89541567e+00 -1.83680728e+00
  8.20081501e-01 -1.50847702e+00  2.00351521e+00  6.23311102e-01
  7.08165095e-01 -2.42755518e+00 -1.56289302e+00 -8.40238717e-01
  2.50621316e+01]
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.25410176067
gradient value of function right now is: [ 5.86710170e-09  7.51290708e-08  7.51616930e-08  7.48446676e-08
 -5.86707027e-09 -7.51286687e-08 -7.51612908e-08 -7.48442670e-08
  9.15985383e-10  8.96150505e-09  8.99709486e-09  7.56152901e-09
  1.21736145e-10  1.18826900e-09  1.19299674e-09  1.00211547e-09
  9.35998281e-11  9.13717852e-10  9.17353090e-10  7.70592841e-10
  3.06221245e-10  2.99081807e-09  3.00271209e-09  2.52262322e-09
 -5.64209584e-11 -1.13812463e-11 -3.28508584e-10 -3.32436818e-10
  1.00155774e-11  2.41642833e-12  5.38330781e-11  5.47681988e-11
 -7.25662217e-12 -1.43614430e-12 -4.24254763e-11 -4.29179577e-11
  2.04574882e-10  4.32059017e-11  1.16698175e-09  1.18243825e-09
  2.80159967e-11  7.82960593e-12  2.96411969e-11  3.71031655e-11
 -1.37319637e-12 -8.90160553e-13 -1.39205182e-12 -2.69369946e-12
  2.62624492e-11  1.49763053e-11  2.67505097e-11  4.89316645e-11
  2.53162939e-11  1.32899679e-11  2.59529832e-11  4.44659023e-11
  1.29812520e-11 -2.43521933e-13  8.01469264e-12 -8.10106526e-12
  1.89472414e-11 -1.95197505e-12  2.20466134e-11 -1.03260132e-11
 -4.87315518e-01]
supnorm grad right now is: 0.4873155183246932
Weights right now are: 
[-8.47064078e-01 -5.02608882e+00 -4.04314753e+00 -3.90672877e+00
  8.40980569e-01  3.25724180e+00  4.14762977e+00  3.20767271e+00
  5.44675987e-01  8.89043978e-01  1.58074149e+00 -1.66594493e-02
 -1.51256531e+00 -1.42801585e+00 -2.50314119e+00 -2.68290651e+00
 -7.43218650e-01 -2.32654688e+00 -2.46098360e+00 -2.06609195e+00
  4.60245556e-01 -2.40511159e+00 -1.95729353e+00 -1.10538712e+00
  5.00912903e-01  1.11742395e+00  1.37686398e+00  8.07592303e-01
 -1.61619182e+00 -9.60616924e-02 -2.24638830e+00 -1.64730783e+00
 -9.87463095e-01 -6.34955573e-01 -2.25206597e+00 -1.98184947e+00
 -9.32814582e-02  3.76141343e-01 -6.85109053e-01 -9.61686093e-01
  3.01950727e-01  1.55817389e+00  7.36218311e-01  8.14420195e-01
  1.72851002e+00  1.25392802e+00  7.31959858e-01  2.23161822e+00
 -1.36800268e+00 -7.20814956e-01 -1.85122627e+00 -1.17228798e+00
 -5.81628509e-01 -1.13246062e+00 -1.89542111e+00 -1.83681657e+00
  8.20078797e-01 -1.50847696e+00  2.00351354e+00  6.23312903e-01
  7.08161138e-01 -2.42755477e+00 -1.56289758e+00 -8.40236477e-01
  2.49792217e+01]
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.09803158915
gradient value of function right now is: [ 5.31369968e-09  6.77617197e-08  6.77910698e-08  6.75059127e-08
 -5.31366535e-09 -6.77612828e-08 -6.77906327e-08 -6.75054775e-08
  8.28996644e-10  8.12487436e-09  8.15705305e-09  6.85841743e-09
  1.09775391e-10  1.07360232e-09  1.07786105e-09  9.05814598e-10
  8.44230964e-11  8.25734932e-10  8.29010305e-10  6.96700616e-10
  2.76128133e-10  2.70204769e-09  2.71276170e-09  2.28005404e-09
 -5.08520850e-11 -1.00226706e-11 -2.98048262e-10 -3.01464289e-10
  8.90155418e-12  2.08795538e-12  4.83319358e-11  4.91307638e-11
 -6.57224668e-12 -1.27195426e-12 -3.86520620e-11 -3.90833268e-11
  1.83798867e-10  3.78792319e-11  1.05620955e-09  1.06959846e-09
  2.54808632e-11  6.85782257e-12  2.70020630e-11  3.35250481e-11
 -1.22491460e-12 -7.48387011e-13 -1.25324030e-12 -2.37247393e-12
  2.35920991e-11  1.27003808e-11  2.41927953e-11  4.34826153e-11
  2.27830676e-11  1.12850623e-11  2.35008752e-11  3.95632922e-11
  1.16085276e-11  2.90185730e-13  6.99332000e-12 -6.66895385e-12
  1.70258803e-11 -1.39110163e-12  1.94198388e-11 -7.74115715e-12
  2.16074835e+00]
supnorm grad right now is: 2.1607483466972535
Weights right now are: 
[-8.48185675e-01 -5.04042390e+00 -4.05748884e+00 -3.92100964e+00
  8.42102166e-01  3.27157688e+00  4.16197107e+00  3.22195357e+00
  5.44500801e-01  8.87330086e-01  1.57902079e+00 -1.81056317e-02
 -1.51258857e+00 -1.42824286e+00 -2.50336910e+00 -2.68309795e+00
 -7.43236533e-01 -2.32672145e+00 -2.46115887e+00 -2.06623918e+00
  4.60187062e-01 -2.40568288e+00 -1.95786710e+00 -1.10586900e+00
  5.00923696e-01  1.11742613e+00  1.37692681e+00  8.07655884e-01
 -1.61619374e+00 -9.60621512e-02 -2.24639857e+00 -1.64731828e+00
 -9.87461704e-01 -6.34955297e-01 -2.25205784e+00 -1.98184125e+00
 -9.33205702e-02  3.76133093e-01 -6.85332137e-01 -9.61912131e-01
  3.01945369e-01  1.55817239e+00  7.36212647e-01  8.14413103e-01
  1.72851029e+00  1.25392818e+00  7.31960124e-01  2.23161874e+00
 -1.36800771e+00 -7.20817818e-01 -1.85123139e+00 -1.17229735e+00
 -5.81633358e-01 -1.13246316e+00 -1.89542607e+00 -1.83682508e+00
  8.20076323e-01 -1.50847689e+00  2.00351202e+00  6.23314401e-01
  7.08157527e-01 -2.42755437e+00 -1.56290179e+00 -8.40234496e-01
  2.51609660e+01]
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.1366480022925
gradient value of function right now is: [ 5.05443595e-09  6.41923419e-08  6.42201600e-08  6.39501944e-08
 -5.05441308e-09 -6.41920523e-08 -6.42198702e-08 -6.39499059e-08
  7.91076793e-10  7.73425704e-09  7.76500287e-09  6.52575842e-09
  1.04689100e-10  1.02109382e-09  1.02516080e-09  8.61082237e-10
  8.05240828e-11  7.85475194e-10  7.88603570e-10  6.62401725e-10
  2.63160977e-10  2.56834112e-09  2.57856581e-09  2.16617029e-09
 -4.88546082e-11 -9.97307451e-12 -2.83427677e-10 -2.86892619e-10
  8.65574432e-12  2.11736918e-12  4.62583783e-11  4.70830548e-11
 -6.32931740e-12 -1.26951128e-12 -3.68612641e-11 -3.72997111e-11
  1.76956160e-10  3.78377582e-11  1.00540208e-09  1.01902359e-09
  2.40371810e-11  6.84451827e-12  2.54081087e-11  3.19447880e-11
 -1.18439695e-12 -7.89105856e-13 -1.19433762e-12 -2.34266332e-12
  2.27155203e-11  1.33052905e-11  2.30510813e-11  4.25757160e-11
  2.18753015e-11  1.17974242e-11  2.23481939e-11  3.86643771e-11
  1.12499659e-11 -4.77251914e-13  7.05855670e-12 -7.29808637e-12
  1.63747811e-11 -1.88331273e-12  1.92964061e-11 -9.73987543e-12
 -1.88343485e+00]
supnorm grad right now is: 1.883434850420407
Weights right now are: 
[-8.49228210e-01 -5.05369410e+00 -4.07076478e+00 -3.93422975e+00
  8.43144701e-01  3.28484708e+00  4.17524702e+00  3.23517368e+00
  5.44337918e-01  8.85735481e-01  1.57741986e+00 -1.94514077e-02
 -1.51261013e+00 -1.42845349e+00 -2.50358057e+00 -2.68327563e+00
 -7.43253119e-01 -2.32688347e+00 -2.46132153e+00 -2.06637585e+00
  4.60132834e-01 -2.40621286e+00 -1.95839918e+00 -1.10631610e+00
  5.00933709e-01  1.11742814e+00  1.37698528e+00  8.07715040e-01
 -1.61619550e+00 -9.60625780e-02 -2.24640809e+00 -1.64732796e+00
 -9.87460410e-01 -6.34955042e-01 -2.25205025e+00 -1.98183357e+00
 -9.33568096e-02  3.76125465e-01 -6.85539429e-01 -9.62122135e-01
  3.01940388e-01  1.55817101e+00  7.36207373e-01  8.14406528e-01
  1.72851053e+00  1.25392834e+00  7.31960369e-01  2.23161921e+00
 -1.36801234e+00 -7.20820437e-01 -1.85123612e+00 -1.17230596e+00
 -5.81637830e-01 -1.13246548e+00 -1.89543067e+00 -1.83683292e+00
  8.20074050e-01 -1.50847689e+00  2.00351059e+00  6.23315867e-01
  7.08154212e-01 -2.42755407e+00 -1.56290571e+00 -8.40232598e-01
  2.48503707e+01]
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.263843912013
gradient value of function right now is: [ 4.65503255e-09  5.89067160e-08  5.89321938e-08  5.86850061e-08
 -4.65501601e-09 -5.89065066e-08 -5.89319844e-08 -5.86847976e-08
  7.28323655e-10  7.12950077e-09  7.15779260e-09  6.01727724e-09
  9.60977618e-11  9.38553978e-10  9.42285109e-10  7.91728553e-10
  7.39304495e-11  7.22122778e-10  7.24993391e-10  6.09169182e-10
  2.41546799e-10  2.36049925e-09  2.36987895e-09  1.99149708e-09
 -4.48484801e-11 -9.01635482e-12 -2.61395148e-10 -2.64501406e-10
  7.86400564e-12  1.89034958e-12  4.23245570e-11  4.30551607e-11
 -5.83424989e-12 -1.15298936e-12 -3.41177323e-11 -3.45129711e-11
  1.62051129e-10  3.41002901e-11  9.25486604e-10  9.37663958e-10
  2.21973884e-11  6.16951378e-12  2.34896091e-11  2.93607818e-11
 -1.07885258e-12 -6.95080860e-13 -1.09476073e-12 -2.11410156e-12
  2.08035061e-11  1.17702113e-11  2.12085173e-11  3.86857845e-11
  2.00582087e-11  1.04460840e-11  2.05790575e-11  3.51605485e-11
  1.02683351e-11 -1.32657156e-13  6.31646465e-12 -6.31530233e-12
  1.50007476e-11 -1.50501570e-12  1.73943749e-11 -7.95628916e-12
 -9.75947450e-02]
supnorm grad right now is: 0.09759474500592248
Weights right now are: 
[-8.50200153e-01 -5.06601902e+00 -4.08309504e+00 -3.94650825e+00
  8.44116644e-01  3.29717200e+00  4.18757727e+00  3.24745218e+00
  5.44186007e-01  8.84247505e-01  1.57592598e+00 -2.07073709e-02
 -1.51263019e+00 -1.42864954e+00 -2.50377740e+00 -2.68344102e+00
 -7.43268549e-01 -2.32703429e+00 -2.46147295e+00 -2.06650310e+00
  4.60082404e-01 -2.40670604e+00 -1.95889431e+00 -1.10673223e+00
  5.00943049e-01  1.11743000e+00  1.37703985e+00  8.07770245e-01
 -1.61619714e+00 -9.60629645e-02 -2.24641692e+00 -1.64733694e+00
 -9.87459197e-01 -6.34954805e-01 -2.25204314e+00 -1.98182638e+00
 -9.33905468e-02  3.76118439e-01 -6.85732647e-01 -9.62317841e-01
  3.01935742e-01  1.55816974e+00  7.36202453e-01  8.14400401e-01
  1.72851075e+00  1.25392848e+00  7.31960598e-01  2.23161965e+00
 -1.36801669e+00 -7.20822832e-01 -1.85124056e+00 -1.17231398e+00
 -5.81642023e-01 -1.13246761e+00 -1.89543498e+00 -1.83684021e+00
  8.20071920e-01 -1.50847688e+00  2.00350929e+00  6.23317156e-01
  7.08151092e-01 -2.42755378e+00 -1.56290928e+00 -8.40231008e-01
  2.50093607e+01]
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5143.303864330717
gradient value of function right now is: [ 4.50524612e-09  5.68073910e-08  5.68319903e-08  5.65936180e-08
 -4.50521674e-09 -5.68070205e-08 -5.68316197e-08 -5.65932489e-08
  7.07538103e-10  6.90405013e-09  6.93158398e-09  5.82346732e-09
  9.33731851e-11  9.08746342e-10  9.12378849e-10  7.66068045e-10
  7.18427507e-11  6.99273809e-10  7.02068851e-10  5.89497829e-10
  2.34523677e-10  2.28399191e-09  2.29311656e-09  1.92568054e-09
 -4.39389341e-11 -9.23067306e-12 -2.52790859e-10 -2.56043063e-10
  7.83702418e-12  1.98776959e-12  4.13357921e-11  4.21193846e-11
 -5.72452650e-12 -1.18195087e-12 -3.30682282e-11 -3.34821785e-11
  1.59270192e-10  3.51067453e-11  8.96549397e-10  9.09355241e-10
  2.12580547e-11  6.34142162e-12  2.24261819e-11  2.84849576e-11
 -1.06547380e-12 -7.65460132e-13 -1.06237903e-12 -2.14504315e-12
  2.04229410e-11  1.28098238e-11  2.05589106e-11  3.88055880e-11
  1.96247218e-11  1.13454590e-11  1.99006995e-11  3.51890979e-11
  1.01705868e-11 -9.56586716e-13  6.60244116e-12 -7.30645983e-12
  1.47175788e-11 -2.05816035e-12  1.78073777e-11 -1.05064716e-11
 -5.01891148e+00]
supnorm grad right now is: 5.018911484431617
Weights right now are: 
[-8.51095768e-01 -5.07733566e+00 -4.09441657e+00 -3.95778234e+00
  8.45012259e-01  3.30848864e+00  4.19889880e+00  3.25872627e+00
  5.44045876e-01  8.82875302e-01  1.57454835e+00 -2.18655667e-02
 -1.51264866e+00 -1.42882999e+00 -2.50395857e+00 -2.68359326e+00
 -7.43282758e-01 -2.32717314e+00 -2.46161235e+00 -2.06662023e+00
  4.60035985e-01 -2.40715984e+00 -1.95934991e+00 -1.10711511e+00
  5.00951691e-01  1.11743173e+00  1.37709016e+00  8.07821151e-01
 -1.61619865e+00 -9.60633230e-02 -2.24642504e+00 -1.64734520e+00
 -9.87458071e-01 -6.34954583e-01 -2.25203656e+00 -1.98181972e+00
 -9.34217566e-02  3.76111910e-01 -6.85910684e-01 -9.62498210e-01
  3.01931475e-01  1.55816856e+00  7.36197934e-01  8.14394743e-01
  1.72851096e+00  1.25392861e+00  7.31960810e-01  2.23162006e+00
 -1.36802070e+00 -7.20825018e-01 -1.85124466e+00 -1.17232145e+00
 -5.81645887e-01 -1.13246955e+00 -1.89543896e+00 -1.83684700e+00
  8.20069935e-01 -1.50847689e+00  2.00350805e+00  6.23318355e-01
  7.08148189e-01 -2.42755351e+00 -1.56291268e+00 -8.40229493e-01
  2.46271412e+01]
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.25227282085
gradient value of function right now is: [ 4.08437096e-09  5.13457899e-08  5.13679460e-08  5.11531734e-08
 -4.08439557e-09 -5.13460994e-08 -5.13682556e-08 -5.11534817e-08
  6.39665302e-10  6.26498011e-09  6.28982269e-09  5.28861535e-09
  8.40419819e-11  8.21270429e-10  8.24532795e-10  6.92926223e-10
  6.46767848e-11  6.32091386e-10  6.34602168e-10  5.33323194e-10
  2.11158285e-10  2.06467505e-09  2.07287298e-09  1.74225076e-09
 -3.93623454e-11 -7.87597170e-12 -2.29741195e-10 -2.32447075e-10
  6.84479593e-12  1.63808489e-12  3.69096443e-11  3.75411532e-11
 -5.15410290e-12 -1.01448931e-12 -3.01689489e-11 -3.05160738e-11
  1.41907619e-10  2.97169730e-11  8.11708632e-10  8.22292229e-10
  1.94708387e-11  5.36866630e-12  2.06125999e-11  2.57168017e-11
 -9.39020427e-13 -6.00261169e-13 -9.54922611e-13 -1.83668162e-12
  1.82082960e-11  1.01915872e-11  1.85940847e-11  3.37836006e-11
  1.75624725e-11  9.04836375e-12  1.80463133e-11  3.07125304e-11
  8.98077262e-12 -2.23231910e-14  5.49990341e-12 -5.45007536e-12
  1.31371755e-11 -1.24420892e-12  1.51648673e-11 -6.72197410e-12
  5.71053960e-01]
supnorm grad right now is: 0.5710539600372646
Weights right now are: 
[-8.51946175e-01 -5.08804607e+00 -4.10513160e+00 -3.96845255e+00
  8.45862666e-01  3.31919905e+00  4.20961383e+00  3.26939648e+00
  5.43912818e-01  8.81571329e-01  1.57323921e+00 -2.29664448e-02
 -1.51266615e+00 -1.42900103e+00 -2.50413029e+00 -2.68373759e+00
 -7.43296217e-01 -2.32730477e+00 -2.46174450e+00 -2.06673131e+00
  4.59992030e-01 -2.40758994e+00 -1.95978172e+00 -1.10747810e+00
  5.00959866e-01  1.11743335e+00  1.37713799e+00  8.07869540e-01
 -1.61620006e+00 -9.60636504e-02 -2.24643271e+00 -1.64735300e+00
 -9.87457000e-01 -6.34954374e-01 -2.25203028e+00 -1.98181338e+00
 -9.34512013e-02  3.76105845e-01 -6.86079674e-01 -9.62669352e-01
  3.01927409e-01  1.55816746e+00  7.36193632e-01  8.14389391e-01
  1.72851116e+00  1.25392872e+00  7.31961010e-01  2.23162044e+00
 -1.36802451e+00 -7.20827066e-01 -1.85124854e+00 -1.17232846e+00
 -5.81649562e-01 -1.13247137e+00 -1.89544273e+00 -1.83685337e+00
  8.20068070e-01 -1.50847688e+00  2.00350694e+00  6.23319346e-01
  7.08145456e-01 -2.42755323e+00 -1.56291577e+00 -8.40228310e-01
  2.50134668e+01]
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.177490030825
gradient value of function right now is: [ 3.90976790e-09  4.89985409e-08  4.90196823e-08  4.88148917e-08
 -3.90979752e-09 -4.89989115e-08 -4.90200532e-08 -4.88152610e-08
  6.13424794e-10  6.00080888e-09  6.02464489e-09  5.06457195e-09
  8.05136046e-11  7.85756312e-10  7.88883424e-10  6.62809531e-10
  6.19693417e-11  6.04836335e-10  6.07243325e-10  5.10209691e-10
  2.02210834e-10  1.97463861e-09  1.98249344e-09  1.66589966e-09
 -3.78791508e-11 -7.71031842e-12 -2.19945482e-10 -2.22619816e-10
  6.61815254e-12  1.61492119e-12  3.54056103e-11  3.60337730e-11
 -4.97216201e-12 -9.96487659e-13 -2.89568598e-11 -2.93010303e-11
  1.36657980e-10  2.91319304e-11  7.77223685e-10  7.87693659e-10
  1.85564872e-11  5.25809508e-12  1.96190906e-11  2.46337443e-11
 -9.05877895e-13 -6.01749728e-13 -9.14284748e-13 -1.79060638e-12
  1.75399497e-11  1.02109255e-11  1.78141888e-11  3.28156019e-11
  1.68935500e-11  9.05518775e-12  1.72721155e-11  2.98040677e-11
  8.67648912e-12 -3.20279230e-13  5.42663676e-12 -5.57328499e-12
  1.26425178e-11 -1.41992450e-12  1.48424757e-11 -7.35283238e-12
 -1.53927465e+00]
supnorm grad right now is: 1.5392746538002051
Weights right now are: 
[-8.52746208e-01 -5.09808958e+00 -4.11517944e+00 -3.97845840e+00
  8.46662698e-01  3.32924256e+00  4.21966168e+00  3.27940233e+00
  5.43787447e-01  8.80343829e-01  1.57200684e+00 -2.40025714e-02
 -1.51268261e+00 -1.42916186e+00 -2.50429175e+00 -2.68387327e+00
 -7.43308888e-01 -2.32742856e+00 -2.46186878e+00 -2.06683575e+00
  4.59950671e-01 -2.40799418e+00 -1.96018757e+00 -1.10781918e+00
  5.00967593e-01  1.11743490e+00  1.37718300e+00  8.07915078e-01
 -1.61620141e+00 -9.60639777e-02 -2.24643994e+00 -1.64736036e+00
 -9.87455988e-01 -6.34954173e-01 -2.25202437e+00 -1.98180739e+00
 -9.34790738e-02  3.76099958e-01 -6.86238678e-01 -9.62830455e-01
  3.01923603e-01  1.55816640e+00  7.36189601e-01  8.14384350e-01
  1.72851134e+00  1.25392884e+00  7.31961196e-01  2.23162080e+00
 -1.36802808e+00 -7.20829090e-01 -1.85125219e+00 -1.17233510e+00
 -5.81652999e-01 -1.13247317e+00 -1.89544626e+00 -1.83685941e+00
  8.20066304e-01 -1.50847688e+00  2.00350583e+00  6.23320499e-01
  7.08142874e-01 -2.42755299e+00 -1.56291879e+00 -8.40226863e-01
  2.48609922e+01]
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.195366679002
gradient value of function right now is: [ 3.70086440e-09  4.62484465e-08  4.62683825e-08  4.60753450e-08
 -3.70083010e-09 -4.62480181e-08 -4.62679538e-08 -4.60749182e-08
  5.80927526e-10  5.68378596e-09  5.70635749e-09  4.79732378e-09
  7.61103177e-11  7.42902465e-10  7.45858325e-10  6.26702615e-10
  5.85882382e-11  5.71927657e-10  5.74203141e-10  4.82481620e-10
  1.91115068e-10  1.86658211e-09  1.87400535e-09  1.57484207e-09
 -3.58698658e-11 -7.29382792e-12 -2.08337067e-10 -2.10865501e-10
  6.24672373e-12  1.52285250e-12  3.34301152e-11  3.40221863e-11
 -4.72191919e-12 -9.45753609e-13 -2.75020476e-11 -2.78286306e-11
  1.29291648e-10  2.75322912e-11  7.35561527e-10  7.45451074e-10
  1.75577922e-11  4.96609743e-12  1.85649941e-11  2.33008764e-11
 -8.55027703e-13 -5.66998257e-13 -8.63346688e-13 -1.68979740e-12
  1.65924728e-11  9.63491064e-12  1.68582622e-11  3.10282240e-11
  1.59819043e-11  8.54490491e-12  1.63456724e-11  2.81818342e-11
  8.20680852e-12 -2.84000873e-13  5.12901078e-12 -5.25664344e-12
  1.19624250e-11 -1.32856715e-12  1.40295692e-11 -6.91123853e-12
 -1.40780166e+00]
supnorm grad right now is: 1.407801657173915
Weights right now are: 
[-0.85349965 -5.10752012 -4.12461404 -3.98785362  0.84741614  3.3386731
  4.22909628  3.28879756  0.54366928  0.87918707  1.57084549 -0.02497903
 -1.5126981  -1.42931315 -2.50444365 -2.68400091 -0.74332081 -2.32754503
 -2.46198571 -2.06693401  0.45991176 -2.40837439 -1.96056929 -1.10814
  0.50097486  1.11743637  1.37722541  0.80795799 -1.61620267 -0.09606429
 -2.24644675 -1.64736728 -0.98745503 -0.63495398 -2.25201878 -1.98180174
 -0.09350527  0.3760944  -0.68638841 -0.96298216  0.30192002  1.5581654
  0.73618581  0.81437962  1.72851151  1.25392896  0.73196137  2.23162114
 -1.36803142 -0.72083104 -1.85125559 -1.17234133 -0.58165623 -1.1324749
 -1.89544957 -1.83686507  0.82006466 -1.50847686  2.0035048   0.6233216
  0.70814048 -2.42755276 -1.56292162 -0.84022544 24.9136407 ]
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.255527762022
gradient value of function right now is: [ 3.47196758e-09  4.32730237e-08  4.32916460e-08  4.31113506e-08
 -3.47197452e-09 -4.32731105e-08 -4.32917328e-08 -4.31114371e-08
  5.44683860e-10  5.33587244e-09  5.35702359e-09  4.50496899e-09
  7.11843799e-11  6.95772529e-10  6.98535406e-10  5.87126877e-10
  5.48045659e-11  5.35723787e-10  5.37851033e-10  4.52080123e-10
  1.78743708e-10  1.74810124e-09  1.75503979e-09  1.47533038e-09
 -3.35307191e-11 -6.70964271e-12 -1.95684335e-10 -1.97990072e-10
  5.78145837e-12  1.38374122e-12  3.11660726e-11  3.16998374e-11
 -4.42820035e-12 -8.72996909e-13 -2.59035406e-11 -2.62026292e-11
  1.20586634e-10  2.52511060e-11  6.89695655e-10  6.98690725e-10
  1.65196036e-11  4.55435249e-12  1.74879989e-11  2.18188078e-11
 -7.93138487e-13 -5.07296923e-13 -8.06301795e-13 -1.55302686e-12
  1.54720886e-11  8.65615197e-12  1.57979983e-11  2.87079229e-11
  1.49218555e-11  7.68430037e-12  1.53312201e-11  2.60964854e-11
  7.63006628e-12 -2.42469096e-14  4.67537449e-12 -4.61938430e-12
  1.11622714e-11 -1.06155765e-12  1.28844738e-11 -5.70837706e-12
  5.16171816e-01]
supnorm grad right now is: 0.5161718158016739
Weights right now are: 
[-0.85421698 -5.11647401 -4.13357179 -3.99677404  0.84813347  3.34762699
  4.23805403  3.29771797  0.54355677  0.87808502  1.56973907 -0.02590941
 -1.51271282 -1.429457   -2.50458807 -2.68412229 -0.74333214 -2.32765578
 -2.4620969  -2.06702746  0.4598748  -2.40873583 -1.96093216 -1.10844502
  0.5009818   1.11743776  1.37726582  0.80799888 -1.61620387 -0.09606458
 -2.2464532  -1.64737384 -0.98745412 -0.6349538  -2.25201344 -1.98179634
 -0.09353024  0.37608916 -0.68653092 -0.96312655  0.30191661  1.55816446
  0.7361822   0.8143751   1.72851168  1.25392906  0.73196154  2.23162146
 -1.36803462 -0.72083281 -1.85125887 -1.1723473  -0.58165931 -1.13247647
 -1.89545274 -1.8368705   0.82006307 -1.50847686  2.00350381  0.62332258
  0.70813815 -2.42755255 -1.56292433 -0.84022418 25.08213293]
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.178212463924
gradient value of function right now is: [ 3.28510895e-09  4.08400229e-08  4.08575750e-08  4.06876757e-08
 -3.28506777e-09 -4.08395113e-08 -4.08570632e-08 -4.06871661e-08
  5.15268834e-10  5.05178585e-09  5.07178569e-09  4.26594715e-09
  6.71976521e-11  6.57384863e-10  6.59991704e-10  5.54849372e-10
  5.17420702e-11  5.06233169e-10  5.08240547e-10  4.27283091e-10
  1.68720603e-10  1.65150471e-09  1.65805098e-09  1.39409316e-09
 -3.16636520e-11 -6.27090058e-12 -1.85326561e-10 -1.87469288e-10
  5.41994891e-12  1.28063511e-12  2.93497175e-11  2.98411169e-11
 -4.19389470e-12 -8.18746401e-13 -2.45961886e-11 -2.48752896e-11
  1.13680492e-10  2.35451527e-11  6.52315735e-10  6.60657216e-10
  1.56554065e-11  4.24496111e-12  1.65848469e-11  2.06172379e-11
 -7.45018626e-13 -4.63802529e-13 -7.60400407e-13 -1.45015529e-12
  1.45889965e-11  7.95369666e-12  1.49393586e-11  2.69322430e-11
  1.40805447e-11  7.06474778e-12  1.45051922e-11  2.44953839e-11
  7.17858040e-12  1.12609986e-13  4.34774895e-12 -4.18570368e-12
  1.05272312e-11 -9.05248858e-13  1.20320617e-11 -4.95943951e-12
  1.59231841e+00]
supnorm grad right now is: 1.5923184104700887
Weights right now are: 
[-0.85489898 -5.12496371 -4.14206515 -4.00523201  0.84881547  3.3561167
  4.24654739  3.30617596  0.54344964  0.8770367   1.56868658 -0.0267943
 -1.51272683 -1.42959369 -2.5047253  -2.68423761 -0.74334292 -2.32776103
 -2.46220257 -2.06711626  0.45983965 -2.40907918 -1.96127688 -1.10873473
  0.50098841  1.11743911  1.37730425  0.80803778 -1.61620502 -0.09606486
 -2.24645934 -1.64738009 -0.98745325 -0.63495363 -2.25200835 -1.98179119
 -0.09355404  0.37608408 -0.68666643 -0.96326388  0.30191337  1.55816354
  0.73617878  0.81437082  1.72851183  1.25392917  0.7319617   2.23162177
 -1.36803766 -0.72083459 -1.85126196 -1.17235299 -0.58166224 -1.13247805
 -1.89545575 -1.83687567  0.82006157 -1.50847682  2.00350286  0.62332358
  0.70813597 -2.42755231 -1.56292692 -0.84022289 25.08404559]
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.23679847577
gradient value of function right now is: [ 3.18627144e-09  3.95102818e-08  3.95272675e-08  3.93629648e-08
 -3.18622057e-09 -3.95096513e-08 -3.95266367e-08 -3.93623367e-08
  5.00741202e-10  4.90202354e-09  4.92147564e-09  4.13835207e-09
  6.52721308e-11  6.37491900e-10  6.40026338e-10  5.37895163e-10
  5.02641874e-11  4.90961873e-10  4.92913677e-10  4.14267308e-10
  1.63817046e-10  1.60090910e-09  1.60727077e-09  1.35098075e-09
 -3.08970772e-11 -6.25314308e-12 -1.79722558e-10 -1.81884047e-10
  5.32736143e-12  1.29278810e-12  2.85673829e-11  2.90687524e-11
 -4.09974262e-12 -8.18158764e-13 -2.39007904e-11 -2.41828126e-11
  1.11064946e-10  2.35358610e-11  6.32920007e-10  6.41350619e-10
  1.51055021e-11  4.24033676e-12  1.59783122e-11  2.00137519e-11
 -7.29470647e-13 -4.80002735e-13 -7.38013507e-13 -1.43864206e-12
  1.42499307e-11  8.18898902e-12  1.45008404e-11  2.65782314e-11
  1.37305002e-11  7.26517479e-12  1.40631635e-11  2.41461540e-11
  7.03806522e-12 -1.74626731e-13  4.37592648e-12 -4.44737400e-12
  1.02730401e-11 -1.08871750e-12  1.19870326e-11 -5.74194950e-12
 -8.60713522e-01]
supnorm grad right now is: 0.8607135222756844
Weights right now are: 
[-0.85554849 -5.1330287  -4.15013361 -4.01326692  0.84946498  3.36418169
  4.25461585  3.31421086  0.54334761  0.87603773  1.56768365 -0.02763766
 -1.51274013 -1.4297237  -2.50485583 -2.68434731 -0.74335317 -2.32786115
 -2.46230309 -2.06720074  0.45980624 -2.40940571 -1.9616047  -1.10901029
  0.5009947   1.11744038  1.37734087  0.80807484 -1.61620611 -0.09606512
 -2.24646517 -1.64738602 -0.98745241 -0.63495346 -2.25200348 -1.98178626
 -0.09357665  0.3760793  -0.68679545 -0.96339461  0.30191029  1.55816268
  0.73617552  0.81436674  1.72851198  1.25392927  0.73196185  2.23162206
 -1.36804056 -0.72083626 -1.85126491 -1.17235839 -0.58166504 -1.13247953
 -1.89545861 -1.83688058  0.82006014 -1.50847679  2.00350197  0.62332448
  0.70813387 -2.4275521  -1.56292934 -0.84022176 24.93312658]
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.225356356899
gradient value of function right now is: [ 3.05463898e-09  3.77896034e-08  3.78058388e-08  3.76488496e-08
 -3.05467418e-09 -3.77900390e-08 -3.78062746e-08 -3.76492835e-08
  4.80333168e-10  4.70196788e-09  4.72062712e-09  3.96952664e-09
  6.25248067e-11  6.10618376e-10  6.13046113e-10  5.15227467e-10
  4.81539661e-11  4.70318270e-10  4.72188111e-10  3.96854146e-10
  1.56892727e-10  1.53313958e-09  1.53923230e-09  1.29380992e-09
 -2.96509938e-11 -6.01008482e-12 -1.72384383e-10 -1.74463919e-10
  5.10345787e-12  1.24026189e-12  2.73453487e-11  2.78268616e-11
 -3.94372008e-12 -7.88644556e-13 -2.29760116e-11 -2.32481928e-11
  1.06525711e-10  2.26077277e-11  6.06713605e-10  6.14819441e-10
  1.44684807e-11  4.07110156e-12  1.53022491e-11  1.91790045e-11
 -6.98791799e-13 -4.61109135e-13 -7.06396198e-13 -1.37974548e-12
  1.36692715e-11  7.87953082e-12  1.39022067e-11  2.55141125e-11
  1.31687051e-11  6.98939617e-12  1.34808006e-11  2.31767475e-11
  6.75309740e-12 -1.91161698e-13  4.20553090e-12 -4.27707541e-12
  9.85452545e-12 -1.06183246e-12  1.15130788e-11 -5.56160130e-12
 -1.06272090e+00]
supnorm grad right now is: 1.0627208991347992
Weights right now are: 
[-0.85616255 -5.14063514 -4.15774331 -4.02084501  0.85007904  3.37178812
  4.26222556  3.32178895  0.54325108  0.87509279  1.56673496 -0.0284354
 -1.51275271 -1.42984651 -2.50497913 -2.68445094 -0.74336286 -2.32795574
 -2.46239806 -2.06728056  0.45977468 -2.4097141  -1.96191432 -1.10927054
  0.50100065  1.11744159  1.37737552  0.80810991 -1.61620713 -0.09606537
 -2.24647067 -1.64739163 -0.98745162 -0.63495331 -2.25199887 -1.9817816
 -0.09359803  0.37607475 -0.68691743 -0.96351822  0.30190738  1.55816185
  0.73617244  0.81436289  1.72851212  1.25392937  0.73196199  2.23162234
 -1.3680433  -0.72083787 -1.8512677  -1.17236349 -0.58166768 -1.13248096
 -1.89546132 -1.83688521  0.82005878 -1.50847676  2.00350114  0.62332535
  0.70813189 -2.42755189 -1.56293162 -0.84022067 24.93571036]
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.260817216751
gradient value of function right now is: [ 2.91221779e-09  3.59464852e-08  3.59619125e-08  3.58127693e-08
 -2.91217001e-09 -3.59458958e-08 -3.59613228e-08 -3.58121820e-08
  4.57920797e-10  4.48506643e-09  4.50285011e-09  3.78694363e-09
  5.95014605e-11  5.81443649e-10  5.83753317e-10  4.90684070e-10
  4.58306943e-11  4.47897234e-10  4.49676342e-10  3.77991970e-10
  1.49292397e-10  1.45973615e-09  1.46553203e-09  1.23204724e-09
 -2.82341764e-11 -5.68428435e-12 -1.64469779e-10 -1.66429799e-10
  4.83296739e-12  1.16526030e-12  2.59746715e-11  2.64256988e-11
 -3.76481032e-12 -7.48037821e-13 -2.19710273e-11 -2.22284277e-11
  1.01302406e-10  2.13461677e-11  5.78235218e-10  5.85863601e-10
  1.38058414e-11  3.84303656e-12  1.46087831e-11  1.82652203e-11
 -6.62466897e-13 -4.30439171e-13 -6.71559529e-13 -1.30307700e-12
  1.29987015e-11  7.37521348e-12  1.32475829e-11  2.41841108e-11
  1.25288757e-11  6.54433134e-12  1.28502684e-11  2.19754121e-11
  6.41553085e-12 -9.79371211e-14  3.95883347e-12 -3.95725137e-12
  9.37676712e-12 -9.47389229e-13  1.08758071e-11 -5.01457982e-12
 -2.73086761e-01]
supnorm grad right now is: 0.2730867608673991
Weights right now are: 
[-0.85676393 -5.14806751 -4.16517888 -4.02824972  0.85068042  3.3792205
  4.26966112  3.32919366  0.54315655  0.87416687  1.56580536 -0.02921721
 -1.512765   -1.42996664 -2.50509974 -2.68455231 -0.74337233 -2.32804827
 -2.46249096 -2.06735865  0.45974384 -2.41001572 -1.96221714 -1.10952511
  0.50100647  1.11744276  1.37740947  0.80814426 -1.61620813 -0.09606562
 -2.24647604 -1.64739709 -0.98745085 -0.63495315 -2.25199434 -1.98177702
 -0.09361892  0.37607033 -0.68703684 -0.9636392   0.30190453  1.55816106
  0.73616942  0.81435912  1.72851226  1.25392946  0.73196213  2.23162261
 -1.36804597 -0.72083942 -1.85127043 -1.17236845 -0.58167025 -1.13248234
 -1.89546397 -1.83688972  0.82005746 -1.50847675  2.00350034  0.6233262
  0.70812996 -2.42755172 -1.56293383 -0.84021965 25.00702119]
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.260303104251
gradient value of function right now is: [ 2.78366476e-09  3.42848486e-08  3.42995494e-08  3.41574634e-08
 -2.78369821e-09 -3.42852605e-08 -3.42999615e-08 -3.41578737e-08
  4.37763745e-10  4.28929780e-09  4.30629637e-09  3.62202628e-09
  5.67905845e-11  5.55182952e-10  5.57387101e-10  4.68574063e-10
  4.37475759e-11  4.27716035e-10  4.29414055e-10  3.61000228e-10
  1.42474533e-10  1.39364170e-09  1.39917216e-09  1.17639021e-09
 -2.69696120e-11 -5.40616998e-12 -1.57315782e-10 -1.59174985e-10
  4.59712576e-12  1.10367074e-12  2.47543489e-11  2.51804598e-11
 -3.60456141e-12 -7.13145637e-13 -2.10604906e-11 -2.13053408e-11
  9.66627808e-11  2.02785346e-11  5.52572762e-10  5.59801129e-10
  1.32025349e-11  3.64913511e-12  1.39751160e-11  1.74426186e-11
 -6.30222395e-13 -4.06242246e-13 -6.40152526e-13 -1.23689870e-12
  1.23992809e-11  6.96676799e-12  1.26551297e-11  2.30193587e-11
  1.19557837e-11  6.18416662e-12  1.22789461e-11  2.09225699e-11
  6.11346342e-12 -3.55894251e-14  3.75854389e-12 -3.72817900e-12
  8.94347733e-12 -8.62417284e-13  1.03377495e-11 -4.63222831e-12
  3.63509108e-01]
supnorm grad right now is: 0.36350910840112555
Weights right now are: 
[-0.85734251 -5.15520191 -4.17231634 -4.03535758  0.851259    3.3863549
  4.27679858  3.33630153  0.54306554  0.87327563  1.56491058 -0.02996971
 -1.51277682 -1.43008213 -2.50521568 -2.68464977 -0.74338143 -2.32813723
 -2.46258028 -2.06743372  0.45971418 -2.41030561 -1.96250819 -1.10976978
  0.50101208  1.11744389  1.37744215  0.80817733 -1.61620909 -0.09606585
 -2.2464812  -1.64740234 -0.9874501  -0.634953   -2.25198997 -1.9817726
 -0.09363907  0.37606607 -0.68715172 -0.9637556   0.30190179  1.55816029
  0.73616652  0.81435549  1.72851239  1.25392954  0.73196226  2.23162287
 -1.36804855 -0.7208409  -1.85127307 -1.17237327 -0.58167274 -1.13248365
 -1.89546652 -1.83689409  0.82005619 -1.50847673  2.00349955  0.62332699
  0.70812809 -2.42755154 -1.56293601 -0.84021861 25.02161842]
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.051932791428
gradient value of function right now is: [ 2.64288765e-09  3.24851082e-08  3.24990151e-08  3.23646016e-08
 -2.64285068e-09 -3.24846540e-08 -3.24985606e-08 -3.23641491e-08
  4.15252040e-10  4.07452711e-09  4.09063850e-09  3.44174393e-09
  5.37477566e-11  5.26259089e-10  5.28343275e-10  4.44313680e-10
  4.14083956e-11  4.05479280e-10  4.07085074e-10  3.42348480e-10
  1.34847877e-10  1.32106637e-09  1.32629620e-09  1.11550181e-09
 -2.54952138e-11 -5.01324937e-12 -1.49525778e-10 -1.51231891e-10
  4.29916641e-12  1.00767282e-12  2.33449415e-11  2.37303743e-11
 -3.41659550e-12 -6.63397044e-13 -2.00628029e-11 -2.02883893e-11
  9.11648563e-11  1.87392840e-11  5.24304071e-10  5.30917451e-10
  1.25788936e-11  3.37027593e-12  1.33329079e-11  1.65306107e-11
 -5.91196239e-13 -3.62038697e-13 -6.05089505e-13 -1.14811111e-12
  1.16934654e-11  6.25914390e-12  1.20003441e-11  2.15166647e-11
  1.12918650e-11  5.56177300e-12  1.16555845e-11  1.95770595e-11
  5.74280614e-12  1.70473167e-13  3.45898515e-12 -3.26399277e-12
  8.43373632e-12 -6.64989377e-13  9.59189322e-12 -3.75333307e-12
  2.42590112e+00]
supnorm grad right now is: 2.425901116208879
Weights right now are: 
[-0.85789543 -5.16200573 -4.17912307 -4.04213614  0.85181192  3.39315871
  4.28360531  3.34308008  0.54297858  0.87242345  1.56405503 -0.03068934
 -1.5127881  -1.43019235 -2.50532634 -2.6847428  -0.74339012 -2.32822215
 -2.46266553 -2.0675054   0.4596859  -2.41058229 -1.96278596 -1.11000334
  0.50101744  1.11744496  1.37747341  0.80820896 -1.61621    -0.09606607
 -2.24648611 -1.64740733 -0.98744938 -0.63495286 -2.25198578 -1.98176836
 -0.09365826  0.37606207 -0.68726147 -0.96386678  0.30189917  1.55815957
  0.73616374  0.81435203  1.72851251  1.25392962  0.73196239  2.23162311
 -1.36805103 -0.72084226 -1.85127558 -1.17237785 -0.58167513 -1.13248486
 -1.89546896 -1.83689826  0.82005497 -1.50847671  2.00349881  0.62332768
  0.70812632 -2.42755135 -1.56293805 -0.84021774 25.15206311]
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.176372276795
gradient value of function right now is: [ 2.61311907e-09  3.20507485e-08  3.20644847e-08  3.19318321e-08
 -2.61314330e-09 -3.20510458e-08 -3.20647822e-08 -3.19321283e-08
  4.11718213e-10  4.02985657e-09  4.04585120e-09  3.40238549e-09
  5.33143218e-11  5.20591925e-10  5.22662157e-10  4.39296952e-10
  4.10769266e-11  4.01138228e-10  4.02733359e-10  3.38504782e-10
  1.33688823e-10  1.30620931e-09  1.31140124e-09  1.10238605e-09
 -2.54475232e-11 -5.18148743e-12 -1.47738473e-10 -1.49535899e-10
  4.34955934e-12  1.06311672e-12  2.32498848e-11  2.36637167e-11
 -3.41423796e-12 -6.86876462e-13 -1.98538214e-11 -2.00917017e-11
  9.12239898e-11  1.94513088e-11  5.18763294e-10  5.25754321e-10
  1.23383727e-11  3.49546570e-12  1.30451337e-11  1.63763991e-11
 -5.94941782e-13 -3.97350808e-13 -6.00128852e-13 -1.17940890e-12
  1.17024936e-11  6.81459198e-12  1.18853717e-11  2.18930004e-11
  1.12692358e-11  6.04281853e-12  1.15215368e-11  1.98817022e-11
  5.78546674e-12 -2.14748418e-13  3.62739955e-12 -3.71919496e-12
  8.43409388e-12 -9.45895717e-13  9.90229721e-12 -4.91507747e-12
 -1.55866470e+00]
supnorm grad right now is: 1.558664698175674
Weights right now are: 
[-0.85842024 -5.16844979 -4.1855699  -4.04855628  0.85233673  3.39960277
  4.29005214  3.34950022  0.54289591  0.87161431  1.56324267 -0.03137249
 -1.51279881 -1.43029695 -2.50543135 -2.68483107 -0.74339837 -2.32830275
 -2.46274645 -2.06757341  0.45965904 -2.41084476 -1.96304948 -1.11022485
  0.50102255  1.117446    1.37750308  0.80823898 -1.61621088 -0.09606628
 -2.24649079 -1.64741209 -0.9874487  -0.63495272 -2.2519818  -1.98176433
 -0.09367657  0.37605816 -0.68736566 -0.96397238  0.30189669  1.55815886
  0.73616112  0.81434874  1.72851263  1.2539297   0.73196251  2.23162335
 -1.36805337 -0.72084363 -1.85127797 -1.17238224 -0.58167739 -1.13248607
 -1.89547128 -1.83690224  0.82005382 -1.50847669  2.00349808  0.62332843
  0.70812463 -2.42755118 -1.56294005 -0.84021674 24.89242292]
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.245694520401
gradient value of function right now is: [ 2.50889606e-09  3.07146077e-08  3.07277588e-08  3.06007771e-08
 -2.50886854e-09 -3.07142710e-08 -3.07274219e-08 -3.06004416e-08
  3.95238506e-10  3.87082154e-09  3.88617178e-09  3.26857331e-09
  5.10995839e-11  4.99282412e-10  5.01266084e-10  4.21378495e-10
  3.93742964e-11  3.84754757e-10  3.86283339e-10  3.24728220e-10
  1.28126832e-10  1.25264692e-09  1.25762144e-09  1.05733849e-09
 -2.43974815e-11 -4.93247604e-12 -1.41941643e-10 -1.43646093e-10
  4.14837413e-12  1.00615312e-12  2.22436231e-11  2.26338937e-11
 -3.28028310e-12 -6.55296346e-13 -1.91113468e-11 -1.93375161e-11
  8.73522369e-11  1.84892382e-11  4.97907108e-10  5.04527832e-10
  1.18586500e-11  3.32087657e-12  1.25452212e-11  1.57071895e-11
 -5.67753766e-13 -3.73454564e-13 -5.74539891e-13 -1.12138906e-12
  1.12019469e-11  6.41575710e-12  1.14038948e-11  2.08880750e-11
  1.07933525e-11  5.69186172e-12  1.10589953e-11  1.89758839e-11
  5.53299718e-12 -1.24262402e-13  3.44144492e-12 -3.48133935e-12
  8.08043348e-12 -8.43980581e-13  9.42049145e-12 -4.48361636e-12
 -7.20830207e-01]
supnorm grad right now is: 0.7208302071748535
Weights right now are: 
[-0.85892216 -5.17460129 -4.19172403 -4.05468498  0.85283865  3.40575428
  4.29620627  3.35562892  0.54281692  0.87084006  1.56246536 -0.03202638
 -1.51280902 -1.43039683 -2.50553163 -2.68491538 -0.74340624 -2.32837972
 -2.46282372 -2.06763838  0.45963343 -2.41109541 -1.96330111 -1.11043645
  0.50102742  1.11744697  1.37753148  0.80826772 -1.6162117  -0.09606647
 -2.24649522 -1.6474166  -0.98744804 -0.63495259 -2.25197798 -1.98176046
 -0.09369399  0.37605453 -0.68746525 -0.96407326  0.30189431  1.55815821
  0.73615861  0.8143456   1.72851275  1.25392977  0.73196262  2.23162357
 -1.36805562 -0.72084486 -1.85128026 -1.1723864  -0.58167955 -1.13248716
 -1.8954735  -1.83690602  0.82005271 -1.50847668  2.00349741  0.62332905
  0.70812301 -2.42755101 -1.56294191 -0.84021598 24.90190159]
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.004759211673
gradient value of function right now is: [ 2.45550796e-09  3.00031075e-08  3.00159578e-08  2.98919475e-08
 -2.45548121e-09 -3.00027805e-08 -3.00156307e-08 -2.98916218e-08
  3.87434874e-10  3.78971103e-09  3.80476736e-09  3.19934850e-09
  5.00745832e-11  4.88602841e-10  4.90548003e-10  4.12260452e-10
  3.85871185e-11  3.76551225e-10  3.78050230e-10  3.17723877e-10
  1.25514455e-10  1.22547140e-09  1.23034766e-09  1.03414249e-09
 -2.39988019e-11 -4.93618685e-12 -1.38898119e-10 -1.40619515e-10
  4.10408692e-12  1.01566330e-12  2.18332653e-11  2.22304253e-11
 -3.23165114e-12 -6.57105405e-13 -1.87324873e-11 -1.89613064e-11
  8.60101188e-11  1.85353533e-11  4.87451586e-10  4.94146403e-10
  1.15557362e-11  3.32752317e-12  1.22090468e-11  1.53843770e-11
 -5.60341303e-13 -3.83868919e-13 -5.62967565e-13 -1.11798026e-12
  1.10328501e-11  6.57545225e-12  1.11737624e-11  2.07404188e-11
  1.06149293e-11  5.82796792e-12  1.08243778e-11  1.88236358e-11
  5.46794491e-12 -3.01580167e-13  3.46878557e-12 -3.63640374e-12
  7.95715633e-12 -9.62891052e-13  9.42219929e-12 -4.95207134e-12
 -2.67714679e+00]
supnorm grad right now is: 2.677146792173385
Weights right now are: 
[-0.85941092 -5.18058029 -4.19770558 -4.06064184  0.85332741  3.41173328
  4.30218783  3.36158578  0.54273998  0.87008578  1.56170811 -0.03266341
 -1.51281896 -1.43049401 -2.5056292  -2.68499741 -0.74341389 -2.32845461
 -2.46289891 -2.0677016   0.45960851 -2.41133924 -1.96354591 -1.11064231
  0.50103216  1.11744792  1.37755915  0.80829572 -1.6162125  -0.09606666
 -2.24649954 -1.64742099 -0.9874474  -0.63495247 -2.25197425 -1.98175669
 -0.09371095  0.37605101 -0.68756222 -0.96417148  0.301892    1.55815758
  0.73615616  0.81434254  1.72851286  1.25392984  0.73196274  2.23162379
 -1.36805781 -0.72084602 -1.85128249 -1.17239046 -0.58168166 -1.1324882
 -1.89547566 -1.83690971  0.82005163 -1.50847667  2.00349674  0.62332967
  0.70812145 -2.42755085 -1.56294375 -0.84021519 24.787655  ]
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5143.341658239029
gradient value of function right now is: [ 2.41084284e-09  2.94016750e-08  2.94142741e-08  2.92927606e-08
 -2.41087864e-09 -2.94021117e-08 -2.94147109e-08 -2.92931956e-08
  3.81072950e-10  3.72195178e-09  3.73677530e-09  3.14127151e-09
  4.92473000e-11  4.79739304e-10  4.81654396e-10  4.04656064e-10
  3.79521522e-11  3.69745833e-10  3.71221759e-10  3.11884893e-10
  1.23394807e-10  1.20283303e-09  1.20763202e-09  1.01473024e-09
 -2.36945294e-11 -4.97448893e-12 -1.36332727e-10 -1.38084887e-10
  4.08343297e-12  1.03686093e-12  2.15211750e-11  2.19301979e-11
 -3.19467747e-12 -6.63014436e-13 -1.84138130e-11 -1.86469040e-11
  8.50342255e-11  1.87273179e-11  4.78762221e-10  4.85590743e-10
  1.12880748e-11  3.35988699e-12  1.19095089e-11  1.51162310e-11
 -5.54839477e-13 -4.00885315e-13 -5.52875835e-13 -1.12119718e-12
  1.08959909e-11  6.81970321e-12  1.09713978e-11  2.06891794e-11
  1.04681351e-11  6.03968524e-12  1.06176862e-11  1.87588534e-11
  5.42037192e-12 -5.01609602e-13  3.52440716e-12 -3.87876081e-12
  7.85182853e-12 -1.09023400e-12  9.48625710e-12 -5.57267636e-12
 -4.93197658e+00]
supnorm grad right now is: 4.9319765795091675
Weights right now are: 
[-0.85988545 -5.1863748  -4.20350257 -4.06641491  0.85380195  3.41752779
  4.30798482  3.36735886  0.54266528  0.86935315  1.56097258 -0.03328228
 -1.51282858 -1.43058826 -2.50572382 -2.68507699 -0.74342131 -2.32852725
 -2.46297184 -2.06776294  0.45958437 -2.4115757  -1.96378331 -1.11084199
  0.50103675  1.11744883  1.37758603  0.80832291 -1.61621327 -0.09606684
 -2.24650371 -1.64742523 -0.98744678 -0.63495235 -2.25197062 -1.98175301
 -0.09372735  0.37604764 -0.68765633 -0.9642668   0.30188975  1.55815697
  0.73615377  0.81433958  1.72851296  1.2539299   0.73196284  2.231624
 -1.36805992 -0.72084715 -1.85128465 -1.17239436 -0.5816837  -1.1324892
 -1.89547776 -1.83691326  0.8200506  -1.50847667  2.00349612  0.6233302
  0.70811994 -2.42755071 -1.56294548 -0.84021451 24.6082724 ]
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.240733084352
gradient value of function right now is: [ 2.27304021e-09  2.76772930e-08  2.76891241e-08  2.75749750e-08
 -2.27300870e-09 -2.76769094e-08 -2.76887403e-08 -2.75745928e-08
  3.58468296e-10  3.51104616e-09  3.52496810e-09  2.96502303e-09
  4.61860499e-11  4.51313627e-10  4.53106535e-10  3.80925828e-10
  3.55972204e-11  3.47877256e-10  3.49259189e-10  2.93628326e-10
  1.15758389e-10  1.13182682e-09  1.13632108e-09  9.55435203e-10
 -2.21353285e-11 -4.47819352e-12 -1.28756341e-10 -1.30304196e-10
  3.74257577e-12  9.08801858e-13  2.00592017e-11  2.04118329e-11
 -2.99290179e-12 -5.98825598e-13 -1.74278405e-11 -1.76347044e-11
  7.91227246e-11  1.67588769e-11  4.50913414e-10  4.56915725e-10
  1.07269641e-11  3.00715975e-12  1.13475307e-11  1.42092148e-11
 -5.12138399e-13 -3.37985261e-13 -5.18051586e-13 -1.01241480e-12
  1.01463002e-11  5.82159611e-12  1.03272087e-11  1.89209936e-11
  9.77522112e-12  5.16454227e-12  1.00139657e-11  1.71879877e-11
  5.01025810e-12 -1.18538758e-13  3.11904254e-12 -3.15846792e-12
  7.31739313e-12 -7.68804486e-13  8.53242901e-12 -4.07498905e-12
 -8.02445958e-01]
supnorm grad right now is: 0.8024459582615923
Weights right now are: 
[-0.86034139 -5.19193142 -4.20906157 -4.07195097  0.85425788  3.42308441
  4.31354382  3.37289492  0.54259334  0.86864907  1.5602657  -0.03387674
 -1.51283787 -1.43067887 -2.50581479 -2.68515345 -0.74342847 -2.32859709
 -2.46304195 -2.06782187  0.45956111 -2.41180292 -1.96401143 -1.11103375
  0.50104121  1.11744974  1.37761184  0.80834904 -1.61621403 -0.09606703
 -2.24650775 -1.64742934 -0.98744618 -0.63495222 -2.25196713 -1.98174948
 -0.09374328  0.37604421 -0.68774681 -0.96435851  0.3018876   1.55815636
  0.7361515   0.81433672  1.72851307  1.25392997  0.73196295  2.2316242
 -1.36806196 -0.72084836 -1.85128673 -1.17239818 -0.58168566 -1.13249027
 -1.89547977 -1.83691673  0.8200496  -1.50847664  2.00349547  0.6233309
  0.70811847 -2.42755054 -1.56294723 -0.8402136  24.98494459]
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.1514661247875
gradient value of function right now is: [ 2.16758619e-09  2.63511916e-08  2.63624370e-08  2.62539242e-08
 -2.16755430e-09 -2.63508042e-08 -2.63620493e-08 -2.62535382e-08
  3.41411002e-10  3.34970452e-09  3.36295128e-09  2.82980871e-09
  4.38896367e-11  4.29682864e-10  4.31384838e-10  3.62814059e-10
  3.38307845e-11  3.31237398e-10  3.32549381e-10  2.79695288e-10
  1.10017010e-10  1.07767982e-09  1.08194677e-09  9.10084815e-10
 -2.09940659e-11 -4.14956455e-12 -1.22926339e-10 -1.24343087e-10
  3.50544923e-12  8.26958300e-13  1.89848748e-11  1.93022031e-11
 -2.84529446e-12 -5.56387084e-13 -1.66704749e-11 -1.68605140e-11
  7.48454914e-11  1.54662730e-11  4.29696388e-10  4.35171108e-10
  1.02753182e-11  2.77524249e-12  1.08869175e-11  1.35228600e-11
 -4.81736021e-13 -2.99397319e-13 -4.91844713e-13 -9.39371830e-13
  9.60143675e-12  5.20320380e-12  9.83744246e-12  1.77070899e-11
  9.26612997e-12  4.62142033e-12  9.55031099e-12  1.61040343e-11
  4.72075736e-12  9.11386469e-14  2.85753968e-12 -2.72062542e-12
  6.92953357e-12 -5.82953541e-13  7.90244815e-12 -3.19938897e-12
  1.80130128e+00]
supnorm grad right now is: 1.8013012782444227
Weights right now are: 
[-0.86078427 -5.19732051 -4.21445296 -4.07732016  0.85470077  3.4284735
  4.31893521  3.37826411  0.54252357  0.8679648   1.55957872 -0.03445477
 -1.51284684 -1.43076671 -2.50590298 -2.68522761 -0.74343538 -2.3286648
 -2.46310993 -2.06787904  0.45953861 -2.41202324 -1.96423263 -1.1112198
  0.5010455   1.11745059  1.37763695  0.80837444 -1.61621474 -0.0960672
 -2.24651163 -1.64743329 -0.9874456  -0.63495211 -2.25196372 -1.98174604
 -0.0937586   0.37604104 -0.68783462 -0.96444745  0.3018855   1.55815579
  0.73614928  0.81433396  1.72851317  1.25393003  0.73196305  2.23162439
 -1.36806394 -0.72084944 -1.85128874 -1.17240181 -0.58168757 -1.13249122
 -1.89548172 -1.83692004  0.82004863 -1.50847664  2.00349489  0.62333144
  0.70811705 -2.42755041 -1.56294883 -0.84021294 25.0794608 ]
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.258418506086
gradient value of function right now is: [ 2.12126071e-09  2.57439960e-08  2.57549838e-08  2.56490032e-08
 -2.12122039e-09 -2.57435068e-08 -2.57544944e-08 -2.56485158e-08
  3.34516254e-10  3.27910232e-09  3.29208907e-09  2.76971579e-09
  4.29853121e-11  4.20407100e-10  4.22075084e-10  3.54917049e-10
  3.31358254e-11  3.24107889e-10  3.25393750e-10  2.73625306e-10
  1.07720550e-10  1.05414835e-09  1.05832887e-09  8.90054224e-10
 -2.06202746e-11 -4.13075586e-12 -1.20291968e-10 -1.21712202e-10
  3.45783277e-12  8.30169649e-13  1.86150543e-11  1.89356687e-11
 -2.79845836e-12 -5.54648296e-13 -1.63372212e-11 -1.65279486e-11
  7.35622597e-11  1.54205350e-11  4.20584176e-10  4.26079342e-10
  1.00230042e-11  2.76587580e-12  1.06102932e-11  1.32380074e-11
 -4.73829004e-13 -3.05832139e-13 -4.81277754e-13 -9.31562869e-13
  9.43230450e-12  5.28846694e-12  9.62917770e-12  1.75052172e-11
  9.09422674e-12  4.69415781e-12  9.34197031e-12  1.59098748e-11
  4.64892815e-12 -2.09066449e-14  2.85988358e-12 -2.82481055e-12
  6.80457473e-12 -6.49949769e-13  7.85885067e-12 -3.50623001e-12
  4.18310592e-01]
supnorm grad right now is: 0.4183105919470232
Weights right now are: 
[-0.86121916 -5.20260314 -4.21973784 -4.08258329  0.85513565  3.43375613
  4.32422009  3.38352724  0.54245499  0.86729268  1.55890394 -0.03502245
 -1.51285566 -1.43085294 -2.50598955 -2.68530041 -0.74344218 -2.32873128
 -2.46317667 -2.06793516  0.45951651 -2.41223948 -1.96444972 -1.11140236
  0.50104973  1.11745144  1.37766161  0.80839939 -1.61621546 -0.09606737
 -2.24651545 -1.64743718 -0.98744503 -0.634952   -2.25196038 -1.98174265
 -0.09377371  0.37603787 -0.68792086 -0.96453482  0.30188345  1.55815522
  0.73614711  0.81433124  1.72851326  1.2539301   0.73196315  2.23162459
 -1.36806588 -0.72085052 -1.85129072 -1.17240542 -0.58168944 -1.13249219
 -1.89548364 -1.83692331  0.82004767 -1.50847662  2.00349431  0.623332
  0.70811564 -2.42755026 -1.56295046 -0.84021222 25.04843573]
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5143.842701314091
gradient value of function right now is: [ 2.02010862e-09  2.44795991e-08  2.44900284e-08  2.43894141e-08
 -2.02012471e-09 -2.44797942e-08 -2.44902235e-08 -2.43896085e-08
  3.18095374e-10  3.12420074e-09  3.13653842e-09  2.63996610e-09
  4.07795461e-11  3.99684217e-10  4.01265016e-10  3.37574072e-10
  3.14388205e-11  3.08163737e-10  3.09382519e-10  2.60281440e-10
  1.02210066e-10  1.00231572e-09  1.00627844e-09  8.46664055e-10
 -1.95108749e-11 -3.80603445e-12 -1.14700551e-10 -1.15989526e-10
  3.22763601e-12  7.50907332e-13  1.75822387e-11  1.78682074e-11
 -2.65366787e-12 -5.12073684e-13 -1.56054298e-11 -1.57790608e-11
  6.94052789e-11  1.41493326e-11  4.00223931e-10  4.05193713e-10
  9.59583744e-12  2.53701805e-12  1.01769985e-11  1.25776229e-11
 -4.43788708e-13 -2.68466757e-13 -4.55701756e-13 -8.59311249e-13
  8.89463748e-12  4.67623678e-12  9.15084740e-12  1.63000493e-11
  8.59443909e-12  4.15776118e-12  8.89153305e-12  1.48369763e-11
  4.35887591e-12  2.00744374e-13  2.60517140e-12 -2.41561867e-12
  6.41418487e-12 -4.54360014e-13  7.24039450e-12 -2.65451413e-12
  3.50121189e+00]
supnorm grad right now is: 3.5012118863583503
Weights right now are: 
[-0.86163989 -5.20770569 -4.22484258 -4.08766703  0.85555639  3.43885869
  4.32932483  3.38861097  0.54238866  0.86664217  1.55825086 -0.03557196
 -1.51286418 -1.43093628 -2.50607322 -2.68537077 -0.74344875 -2.32879553
 -2.46324118 -2.06798941  0.45949516 -2.41244845 -1.96465952 -1.11157882
  0.50105381  1.11745225  1.37768547  0.80842353 -1.61621614 -0.09606753
 -2.24651914 -1.64744093 -0.98744447 -0.63495189 -2.25195714 -1.98173937
 -0.09378825  0.37603483 -0.68800427 -0.9646193   0.30188146  1.55815467
  0.736145    0.81432862  1.72851336  1.25393016  0.73196324  2.23162477
 -1.36806774 -0.72085157 -1.85129262 -1.17240886 -0.58169123 -1.13249311
 -1.89548549 -1.83692644  0.82004675 -1.50847664  2.00349375  0.62333258
  0.7081143  -2.42755015 -1.56295199 -0.84021153 25.22186772]
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.237232608544
gradient value of function right now is: [ 2.02545019e-09  2.45231067e-08  2.45335635e-08  2.44327314e-08
 -2.02541720e-09 -2.45227074e-08 -2.45331640e-08 -2.44323336e-08
  3.19459226e-10  3.13267001e-09  3.14506950e-09  2.64630313e-09
  4.09779137e-11  4.00935926e-10  4.02525636e-10  3.38516016e-10
  3.15920481e-11  3.09132447e-10  3.10358107e-10  2.61010897e-10
  1.02676000e-10  1.00518094e-09  1.00916476e-09  8.48800822e-10
 -1.96793442e-11 -3.92532204e-12 -1.14938676e-10 -1.16285214e-10
  3.28473075e-12  7.84439898e-13  1.77152704e-11  1.80176079e-11
 -2.67772890e-12 -5.28653909e-13 -1.56476737e-11 -1.58291297e-11
  7.01248825e-11  1.46327140e-11  4.01464238e-10  4.06667253e-10
  9.57254491e-12  2.62313588e-12  1.01364410e-11  1.26281054e-11
 -4.50437043e-13 -2.87504671e-13 -4.58291284e-13 -8.83641301e-13
  8.99192458e-12  4.98640807e-12  9.19097690e-12  1.66540030e-11
  8.67211967e-12  4.42706057e-12  8.91849361e-12  1.51391459e-11
  4.42811968e-12  1.55469897e-14  2.71132065e-12 -2.64768696e-12
  6.48836598e-12 -5.93938964e-13  7.46328134e-12 -3.23218827e-12
  8.65128769e-01]
supnorm grad right now is: 0.8651287687318877
Weights right now are: 
[-0.86204877 -5.21265611 -4.22979511 -4.09259918  0.85596526  3.44380911
  4.33427736  3.39354314  0.54232408  0.86600985  1.55761602 -0.03610595
 -1.51287247 -1.43101729 -2.50615455 -2.68543915 -0.74345514 -2.32885799
 -2.46330389 -2.06804213  0.45947438 -2.4126515  -1.96486338 -1.11175023
  0.5010578   1.11745307  1.37770866  0.808447   -1.61621681 -0.0960677
 -2.24652274 -1.64744459 -0.98744393 -0.63495178 -2.25195398 -1.98173618
 -0.0938025   0.37603178 -0.68808535 -0.96470148  0.30187953  1.55815413
  0.73614296  0.81432607  1.72851345  1.25393022  0.73196334  2.23162495
 -1.36806956 -0.72085265 -1.85129447 -1.17241227 -0.58169299 -1.13249407
 -1.89548729 -1.83692954  0.82004585 -1.50847661  2.00349318  0.62333319
  0.70811299 -2.42755001 -1.56295354 -0.84021074 25.12130494]
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.263091996856
gradient value of function right now is: [ 2.01111008e-09  2.43301850e-08  2.43405614e-08  2.42405281e-08
 -2.01108798e-09 -2.43299175e-08 -2.43402938e-08 -2.42402616e-08
  3.17428659e-10  3.11089985e-09  3.12322452e-09  2.62762226e-09
  4.07141109e-11  3.98093196e-10  3.99673217e-10  3.36074718e-10
  3.13895378e-11  3.06949353e-10  3.08167576e-10  2.59135956e-10
  1.01999701e-10  9.97916288e-10  1.00187521e-09  8.42565233e-10
 -1.95853342e-11 -3.93967672e-12 -1.14113652e-10 -1.15471364e-10
  3.27844673e-12  7.90652129e-13  1.76159906e-11  1.79220450e-11
 -2.66670886e-12 -5.31124483e-13 -1.55462355e-11 -1.57293507e-11
  6.98243519e-11  1.46985828e-11  3.98678087e-10  4.03927609e-10
  9.48553091e-12  2.63535472e-12  1.00380037e-11  1.25423976e-11
 -4.49175577e-13 -2.92676252e-13 -4.55304571e-13 -8.85276724e-13
  8.95566029e-12  5.06976352e-12  9.12969491e-12  1.66494329e-11
  8.63123146e-12  4.49860403e-12  8.85484305e-12  1.51281806e-11
  4.41586907e-12 -5.90978514e-14  2.72689157e-12 -2.71354507e-12
  6.45893065e-12 -6.46415614e-13  7.48146237e-12 -3.42677348e-12
 -1.63920408e-01]
supnorm grad right now is: 0.163920407697768
Weights right now are: 
[-0.86243847 -5.21736769 -4.2345087  -4.09729342  0.85635496  3.44852069
  4.33899095  3.39823737  0.54226261  0.86540691  1.55701071 -0.0366153
 -1.51288035 -1.43109437 -2.50623194 -2.68550423 -0.74346121 -2.32891742
 -2.46336356 -2.06809232  0.45945465 -2.41284474 -1.96505738 -1.11191341
  0.50106159  1.11745382  1.37773078  0.80846938 -1.61621744 -0.09606785
 -2.24652614 -1.64744805 -0.98744341 -0.63495168 -2.25195096 -1.98173313
 -0.09381599  0.37602898 -0.68816258 -0.9647797   0.30187769  1.55815362
  0.73614101  0.81432364  1.72851354  1.25393027  0.73196342  2.23162512
 -1.3680713  -0.7208536  -1.85129625 -1.17241547 -0.58169466 -1.13249492
 -1.89548901 -1.83693245  0.820045   -1.50847662  2.00349266  0.62333367
  0.70811173 -2.42754989 -1.56295497 -0.84021017 24.77305409]
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.260535754609
gradient value of function right now is: [ 1.98548280e-09  2.40020947e-08  2.40123293e-08  2.39136747e-08
 -1.98548881e-09 -2.40021670e-08 -2.40124017e-08 -2.39137468e-08
  3.13451378e-10  3.07176251e-09  3.08393283e-09  2.59455920e-09
  4.01862185e-11  3.92908346e-10  3.94467893e-10  3.31696386e-10
  3.09836300e-11  3.02962204e-10  3.04164683e-10  2.55768944e-10
  1.00670252e-10  9.84852750e-10  9.88760103e-10  8.31532533e-10
 -1.93450441e-11 -3.89514278e-12 -1.12675772e-10 -1.14018962e-10
  3.23692411e-12  7.81549156e-13  1.73837643e-11  1.76864766e-11
 -2.63602314e-12 -5.25600569e-13 -1.53616613e-11 -1.55429941e-11
  6.89573426e-11  1.45308140e-11  3.93583093e-10  3.98775741e-10
  9.36051040e-12  2.60435423e-12  9.90520007e-12  1.23814822e-11
 -4.43383672e-13 -2.89542733e-13 -4.49255878e-13 -8.74584510e-13
  8.84390066e-12  5.01677141e-12  9.01348212e-12  1.64511869e-11
  8.52266681e-12  4.45125648e-12  8.74141280e-12  1.49469537e-11
  4.36243466e-12 -6.61509384e-14  2.69730451e-12 -2.68942725e-12
  6.37963088e-12 -6.43067939e-13  7.39655515e-12 -3.41031509e-12
 -2.84778783e-01]
supnorm grad right now is: 0.2847787833804897
Weights right now are: 
[-0.86282683 -5.22205588 -4.23919888 -4.10196435  0.85674332  3.45320888
  4.34368114  3.4029083   0.5422013   0.86480589  1.5564073  -0.03712299
 -1.5128882  -1.43117117 -2.50630904 -2.68556907 -0.74346727 -2.32897664
 -2.46342301 -2.06814232  0.45943498 -2.41303722 -1.96525062 -1.11207593
  0.50106537  1.11745458  1.37775283  0.80849169 -1.61621807 -0.096068
 -2.24652953 -1.6474515  -0.9874429  -0.63495157 -2.25194795 -1.98173008
 -0.09382946  0.37602615 -0.68823955 -0.96485768  0.30187586  1.55815311
  0.73613908  0.81432122  1.72851362  1.25393033  0.73196351  2.23162529
 -1.36807303 -0.72085457 -1.85129802 -1.17241868 -0.58169633 -1.13249578
 -1.89549072 -1.83693536  0.82004415 -1.50847661  2.00349213  0.6233342
  0.70811048 -2.42754977 -1.56295641 -0.84020953 24.97118468]
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.263502485474
gradient value of function right now is: [ 1.95700147e-09  2.36404338e-08  2.36505111e-08  2.35533802e-08
 -1.95702408e-09 -2.36407066e-08 -2.36507840e-08 -2.35536519e-08
  3.08968448e-10  3.02821298e-09  3.04020853e-09  2.55786400e-09
  3.95891917e-11  3.87124530e-10  3.88660805e-10  3.26825772e-10
  3.05244546e-11  2.98513467e-10  2.99698047e-10  2.52022543e-10
  9.91707293e-11  9.70314324e-10  9.74163227e-10  8.19287384e-10
 -1.90633635e-11 -3.83268999e-12 -1.11084910e-10 -1.12405475e-10
  3.18480189e-12  7.67501179e-13  1.71158773e-11  1.74129304e-11
 -2.59980056e-12 -5.17680383e-13 -1.51562383e-11 -1.53347177e-11
  6.79271668e-11  1.42907182e-11  3.87899357e-10  3.93002144e-10
  9.22738851e-12  2.56145577e-12  9.76510339e-12  1.21994705e-11
 -4.36423512e-13 -2.83934192e-13 -4.42440567e-13 -8.60014471e-13
  8.71290104e-12  4.92488219e-12  8.88327832e-12  1.61937121e-11
  8.39730199e-12  4.37002925e-12  8.61579033e-12  1.47141666e-11
  4.29561920e-12 -5.39038131e-14  2.65091776e-12 -2.63131102e-12
  6.28407177e-12 -6.26669964e-13  7.27444447e-12 -3.31728014e-12
 -1.21002464e-01]
supnorm grad right now is: 0.1210024637451289
Weights right now are: 
[-0.86320547 -5.22662003 -4.24376497 -4.1065117   0.85712196  3.45777302
  4.34824722  3.40745565  0.54214149  0.86421973  1.55581882 -0.03761812
 -1.51289585 -1.431246   -2.50638417 -2.68563225 -0.74347317 -2.32903435
 -2.46348095 -2.06819104  0.45941581 -2.41322475 -1.9654389  -1.11223429
  0.50106906  1.11745532  1.37777433  0.80851345 -1.61621869 -0.09606815
 -2.24653284 -1.64745486 -0.98744239 -0.63495147 -2.25194501 -1.98172711
 -0.0938426   0.37602338 -0.68831459 -0.96493371  0.30187408  1.55815262
  0.73613719  0.81431886  1.72851371  1.25393039  0.7319636   2.23162546
 -1.3680747  -0.72085553 -1.85129972 -1.17242181 -0.58169794 -1.13249663
 -1.89549238 -1.8369382   0.82004332 -1.5084766   2.00349162  0.62333473
  0.70810928 -2.42754965 -1.56295783 -0.84020885 24.8519535 ]
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.2636621260135
gradient value of function right now is: [ 1.93123353e-09  2.33121932e-08  2.33221283e-08  2.32263775e-08
 -1.93119266e-09 -2.33117000e-08 -2.33216349e-08 -2.32258861e-08
  3.04935108e-10  2.98876076e-09  3.00059965e-09  2.52457529e-09
  3.90536397e-11  3.81897668e-10  3.83413152e-10  3.22417612e-10
  3.01124405e-11  2.94491964e-10  2.95660548e-10  2.48630880e-10
  9.78235694e-11  9.57158432e-10  9.60955027e-10  8.08190589e-10
 -1.88145105e-11 -3.78233203e-12 -1.09639284e-10 -1.10942397e-10
  3.14053231e-12  7.56787506e-13  1.68783433e-11  1.71712502e-11
 -2.56782511e-12 -5.11332461e-13 -1.49697729e-11 -1.51460639e-11
  6.70242368e-11  1.40994402e-11  3.82759837e-10  3.87793992e-10
  9.10409861e-12  2.52686329e-12  9.63470014e-12  1.20357716e-11
 -4.30348454e-13 -2.79972450e-13 -4.36286636e-13 -8.48096608e-13
  8.59681329e-12  4.85839593e-12  8.76511613e-12  1.59769626e-11
  8.28552228e-12  4.31103292e-12  8.50127733e-12  1.45174103e-11
  4.23775387e-12 -5.25792452e-14  2.61538239e-12 -2.59517396e-12
  6.19927661e-12 -6.17714845e-13  7.17691490e-12 -3.27199057e-12
 -1.09298539e-01]
supnorm grad right now is: 0.10929853865278716
Weights right now are: 
[-0.86357401 -5.231056   -4.24820283 -4.11093136  0.8574905   3.46220899
  4.35268508  3.41187531  0.54208324  0.86364904  1.55524587 -0.03810016
 -1.5129033  -1.43131881 -2.50645727 -2.68569371 -0.74347891 -2.3290905
 -2.46353733 -2.06823844  0.45939716 -2.41340717 -1.96562205 -1.11238831
  0.50107266  1.11745605  1.37779527  0.80853463 -1.61621929 -0.09606829
 -2.24653606 -1.64745814 -0.9874419  -0.63495137 -2.25194215 -1.98172421
 -0.09385541  0.37602066 -0.68838762 -0.96500771  0.30187234  1.55815214
  0.73613535  0.81431657  1.72851379  1.25393044  0.73196368  2.23162562
 -1.36807634 -0.72085647 -1.85130139 -1.17242487 -0.58169952 -1.13249747
 -1.895494   -1.83694098  0.82004252 -1.50847658  2.0034911   0.62333526
  0.7081081  -2.42754952 -1.56295922 -0.84020818 25.0198496 ]
NN weights: [-0.86262791 -5.21964826 -4.23679023 -4.09956559  0.85654441  3.45080125
  4.34127248  3.40050955  0.54223267  0.86511357  1.55671619 -0.03686309
 -1.51288418 -1.4311318  -2.50626951 -2.68553583 -0.74346417 -2.32894629
 -2.46339254 -2.06811669  0.45944507 -2.41293851 -1.96515153 -1.11199259
  0.50106344  1.11745419  1.37774154  0.80848027 -1.61621775 -0.09606792
 -2.24652779 -1.64744973 -0.98744316 -0.63495163 -2.25194949 -1.98173164
 -0.09382257  0.37602759 -0.68820012 -0.96481774  0.3018768   1.55815337
  0.73614007  0.81432246  1.72851358  1.2539303   0.73196347  2.2316252
 -1.36807214 -0.72085408 -1.85129711 -1.17241704 -0.58169547 -1.13249535
 -1.89548984 -1.83693387  0.82004459 -1.50847661  2.0034924   0.62333394
  0.70811113 -2.42754983 -1.56295568 -0.84020983]
Minimum obj value:-5144.2636621260135
Optimal xi: 24.98252312063571
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1551.5577480531542
W_T_median: 1405.4383972872631
W_T_pctile_5: 624.6754498333082
W_T_CVAR_5_pct: 489.59103520591236
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
F value: -5144.2636621260135
-----------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.71540385706
gradient value of function right now is: [ 3.99818621e-01  1.51313424e-01  3.92366005e-01  4.27859510e-03
 -3.99818621e-01 -1.51313424e-01 -3.92366005e-01 -4.27859510e-03
  5.06672964e-03  5.09683381e-05  5.33302352e-03  5.39618760e-03
  2.08453981e-01  2.07273444e-03  2.19361283e-01  2.21938036e-01
  2.81286067e-02  2.82132260e-04  2.96053071e-02  2.99552134e-02
  6.14487356e-03  6.17834722e-05  6.46777378e-03  6.54435169e-03
 -5.66154533e-03 -5.57947829e-03 -2.01263314e-03 -5.46020285e-03
 -1.06246881e-03 -1.04699954e-03 -3.78983877e-04 -1.01952781e-03
  2.28470344e-03  2.25127808e-03  8.18069674e-04  2.19548814e-03
 -2.13567671e-04 -2.10450061e-04 -7.63511420e-05 -2.04715752e-04
 -1.33898832e-04 -4.47113213e-05 -8.99782880e-05 -6.75905521e-05
 -6.48729987e-05  5.83411242e-07 -3.88390260e-05 -3.36727787e-05
 -7.13913632e-05  6.29091465e-06 -4.50331140e-05 -4.31009504e-05
  1.06681297e-04  8.65597922e-05  8.35940639e-05  5.38866786e-05
  5.44370596e-06 -2.79782897e-05  1.58306603e-06 -1.24193437e-05
 -3.59388581e-05 -7.93816008e-05  1.08168910e-05 -3.68296228e-05
  1.26559829e+00]
supnorm grad right now is: 1.2655982868666882
Weights right now are: 
[-2.46899183 -1.79837222 -1.10505188 -0.78625552  2.68858207  0.56680254
  2.50643593  0.76644138 -1.3674241  -0.83471394 -2.39724224 -2.30308772
  0.61729402  0.5334954  -0.618599    0.54114426 -1.1765903   0.44344636
 -0.8394029  -1.91608599  1.75640784  0.5814806   1.53772395  1.37408537
 -1.37579483 -1.1228388  -0.51562635 -0.33135222  1.42911553  1.69388575
  1.46319017  1.69370708 -1.84084112 -1.71049927 -0.6211774  -0.9254022
 -2.00298043 -1.70959598 -1.51055945 -2.00860017 -1.57703189 -1.08723834
 -1.48133679 -0.90608363 -1.4469822  -1.07881156 -1.39887934 -0.79372884
  0.42597281  1.52192278  0.18376683  0.24633451 -1.60183179 -0.77340903
 -0.40772913 -1.58093733 -0.28123294 -1.5748219   2.05198938  1.17430373
  0.91155646 -0.37020424  2.00177875 -0.48107577 25.09226064]
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.919654114374
gradient value of function right now is: [ 8.19795942e-02  6.41647640e-02  8.07078722e-02  9.02882882e-04
 -8.19795942e-02 -6.41647640e-02 -8.07078722e-02 -9.02882882e-04
  1.05767016e-03  1.03647513e-05  1.11884745e-03  1.13120757e-03
  3.37668857e-02  3.27714168e-04  3.57130003e-02  3.61048433e-02
  5.29685854e-03  5.17885064e-05  5.60297428e-03  5.66477001e-03
  1.29388778e-03  1.26737978e-05  1.36871615e-03  1.38383189e-03
  2.86959846e-04  2.82899351e-04  1.02556813e-04  2.76658726e-04
 -1.69152169e-04 -1.66752479e-04 -6.04903482e-05 -1.62334703e-04
  7.05805728e-04  6.95741872e-04  2.53482022e-04  6.78371547e-04
  4.14304371e-05  4.08417096e-05  1.48524450e-05  3.96642283e-05
  1.08277381e-04  7.13444935e-05  8.24490028e-05  5.39288399e-05
  1.15068441e-04  7.09238199e-05  8.65305977e-05  5.76865227e-05
  1.38203709e-04  4.82358720e-05  9.75384573e-05  7.49534099e-05
  7.38359658e-05  5.02102776e-05  5.67930491e-05  3.72743336e-05
  9.01371321e-05 -2.91680442e-05  3.51767359e-05 -4.36966392e-05
  9.29251976e-05 -5.55005782e-06  5.07000372e-05  1.35885623e-05
  2.16076502e+00]
supnorm grad right now is: 2.1607650248963943
Weights right now are: 
[-2.55536899e+00 -1.90325734e+00 -1.24888051e+00 -7.88029686e-01
  2.77495923e+00  6.71687667e-01  2.65026456e+00  7.68215549e-01
 -1.37206585e+00 -8.34786030e-01 -2.40278947e+00 -2.30748831e+00
  3.25539595e-02  5.26779675e-01 -1.33642318e+00  1.77512450e-02
 -1.23278693e+00  4.42137770e-01 -9.01533221e-01 -1.97068884e+00
  1.74866505e+00  5.81389449e-01  1.52812402e+00  1.36710796e+00
 -1.35012131e+00 -1.09367570e+00 -5.07179333e-01 -3.08137821e-01
  1.43442729e+00  1.69991369e+00  1.46499680e+00  1.69857027e+00
 -1.85583569e+00 -1.72747832e+00 -6.27104466e-01 -9.40576813e-01
 -2.00281835e+00 -1.70941297e+00 -1.51049303e+00 -2.00843286e+00
 -1.58023714e+00 -1.08992167e+00 -1.48371781e+00 -9.07512377e-01
 -1.45236049e+00 -1.08291540e+00 -1.40281968e+00 -7.96166708e-01
  4.14170936e-01  1.51666189e+00  1.76457107e-01  2.40118082e-01
 -1.61088336e+00 -7.81346532e-01 -4.15177351e-01 -1.58532986e+00
 -3.09459670e-01 -1.56613628e+00  2.03642455e+00  1.18796766e+00
  8.37850546e-01 -3.46515763e-01  1.99136697e+00 -4.82858207e-01
  2.51538616e+01]
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.070534748973
gradient value of function right now is: [ 3.27129892e-02  2.94873302e-02  3.22776892e-02  3.64338170e-04
 -3.27129892e-02 -2.94873302e-02 -3.22776892e-02 -3.64338170e-04
  4.26847873e-04  4.01757418e-06  4.49868663e-04  4.54520272e-04
  7.88759667e-03  7.37565900e-05  8.31194215e-03  8.39749828e-03
  1.97717848e-03  1.85772625e-05  2.08374101e-03  2.10526025e-03
  5.25447374e-04  4.94326336e-06  5.53780955e-04  5.59505170e-04
  2.50131704e-04  2.46739589e-04  9.01215435e-05  2.41062908e-04
 -4.05728838e-05 -4.00212611e-05 -1.45850846e-05 -3.89048522e-05
  2.06481193e-04  2.03660074e-04  7.45915658e-05  1.98299912e-04
  2.36584201e-05  2.33362725e-05  8.51119301e-06  2.26386966e-05
  4.31624954e-05  2.79773514e-05  3.40828107e-05  2.16582135e-05
  4.40905289e-05  2.67649764e-05  3.43591458e-05  2.22679762e-05
  5.55307260e-05  1.87993306e-05  4.02257379e-05  3.04158790e-05
  2.53912994e-05  1.73133633e-05  2.03231872e-05  1.28625683e-05
  3.33151871e-05 -1.10049630e-05  1.28010071e-05 -1.59950314e-05
  3.63318523e-05 -2.00041834e-06  1.86494449e-05  5.58704406e-06
 -1.37683817e+00]
supnorm grad right now is: 1.3768381690900293
Weights right now are: 
[-2.62690925 -2.03197269 -1.36833262 -0.78952786  2.8464995   0.80040301
  2.76971667  0.76971373 -1.37596677 -0.8348444  -2.40745242 -2.31118439
 -0.26991343  0.52341641 -1.70783901 -0.25294242 -1.27585256  0.44117094
 -0.94916162 -2.01251283  1.74210938  0.5813151   1.51999413  1.36120381
 -1.36932326 -1.11550193 -0.5135595  -0.32548438  1.43719354  1.70305461
  1.46594222  1.7011018  -1.86764013 -1.74085212 -0.63179646 -0.95251842
 -2.00391592 -1.71065371 -1.51094581 -2.00955893 -1.587651   -1.09500274
 -1.48913323 -0.91083237 -1.46228608 -1.08938998 -1.41006427 -0.8006901
  0.3914964   1.5083136   0.16237263  0.22792692 -1.62107991 -0.7898939
 -0.42374155 -1.59026639 -0.3532134  -1.55488562  2.01264428  1.20791053
  0.70488273 -0.33849738  1.97584063 -0.49210259 24.89553023]
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.145022056073
gradient value of function right now is: [ 1.37152481e-02  1.29782787e-02  1.35607646e-02  1.54660133e-04
 -1.37152481e-02 -1.29782787e-02 -1.35607646e-02 -1.54660133e-04
  1.81137170e-04  1.63552713e-06  1.89955140e-04  1.91806029e-04
  2.15906619e-03  1.94102545e-05  2.26399560e-03  2.28599035e-03
  7.72335405e-04  6.96486548e-06  8.09915166e-04  8.17799938e-04
  2.24797881e-04  2.02870818e-06  2.35739204e-04  2.38035418e-04
  1.01981060e-04  1.00654328e-04  3.71163056e-05  9.82235870e-05
 -1.14546852e-05 -1.13053780e-05 -4.14694405e-06 -1.09767050e-05
  6.43620902e-05  6.35191929e-05  2.34308233e-05  6.17764247e-05
  9.66531934e-06  9.53916737e-06  3.49731937e-06  9.24358082e-06
  1.49858236e-05  9.69144055e-06  1.22849726e-05  7.55722179e-06
  1.51775794e-05  9.18543655e-06  1.22631726e-05  7.70444451e-06
  1.99728147e-05  6.66680270e-06  1.48361359e-05  1.10164273e-05
  8.60642502e-06  5.88327993e-06  7.15736976e-06  4.36553327e-06
  1.13065034e-05 -3.83389794e-06  4.25433276e-06 -5.34588902e-06
  1.27514981e-05 -9.17666227e-07  6.35663387e-06  2.02949816e-06
  5.31853619e-01]
supnorm grad right now is: 0.5318536187041676
Weights right now are: 
[-2.70793394 -2.19120618 -1.50391104 -0.79124423  2.92752418  0.9596365
  2.90529509  0.77143009 -1.38043677 -0.8349087  -2.41277168 -2.315398
 -0.47887588  0.52117712 -1.96328664 -0.43901923 -1.3213962   0.44018744
 -0.99930595 -2.05651825  1.73454472  0.58123261  1.51065501  1.35442583
 -1.39681765 -1.14677133 -0.52276792 -0.35030465  1.43916567  1.70529523
  1.46661998  1.70290488 -1.87740763 -1.75192497 -0.63570252 -0.96239084
 -2.00531625 -1.71223763 -1.51152571 -2.01099457 -1.59510245 -1.10013592
 -1.4948078  -0.91420308 -1.47205053 -1.09579167 -1.41748632 -0.80518484
  0.36816818  1.49979587  0.14745627  0.21524112 -1.63057729 -0.79797447
 -0.43207637 -1.59489421 -0.39543783 -1.5433381   1.98986449  1.22721574
  0.57097599 -0.32669505  1.96086296 -0.50124107 25.05822057]
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.882807865904
gradient value of function right now is: [ 5.50504306e-03  5.32455930e-03  5.45314478e-03  6.31202473e-05
 -5.50504306e-03 -5.32455930e-03 -5.45314478e-03 -6.31202473e-05
  7.36752117e-05  6.41396949e-07  7.68982341e-05  7.76101119e-05
  6.23544121e-04  5.41286367e-06  6.50790907e-04  6.56803968e-04
  2.87929944e-04  2.50452668e-06  3.00521546e-04  3.03301969e-04
  9.24498260e-05  8.04392563e-07  9.64932948e-05  9.73862435e-05
  3.60058312e-05  3.55554594e-05  1.32390805e-05  3.46567275e-05
 -3.40676704e-06 -3.36409929e-06 -1.24286543e-06 -3.26262547e-06
  2.03414660e-05  2.00855552e-05  7.46699284e-06  1.95137008e-05
  3.57216615e-06  3.52737631e-06  1.30119479e-06  3.41438705e-06
  4.93735874e-06  3.21716467e-06  4.19977588e-06  2.50646960e-06
  4.99278562e-06  3.04320839e-06  4.18058167e-06  2.55102026e-06
  6.82402795e-06  2.27890992e-06  5.19585235e-06  3.78851555e-06
  2.85603640e-06  1.97119284e-06  2.46620548e-06  1.45352342e-06
  3.70117494e-06 -1.31740896e-06  1.36728792e-06 -1.74149877e-06
  4.26942306e-06 -4.24911792e-07  2.09287863e-06  6.73199266e-07
 -2.76928255e+00]
supnorm grad right now is: 2.7692825495764812
Weights right now are: 
[-2.7991851  -2.37673812 -1.6568983  -0.79320461  3.01877534  1.14516844
  3.05828235  0.77339047 -1.3855344  -0.83497897 -2.4188078  -2.32017679
 -0.63946763  0.519525   -2.1586236  -0.58123909 -1.36911359  0.43919951
 -1.05158406 -2.10237093  1.72583637  0.58114163  1.49995717  1.34666606
 -1.42520802 -1.17907603 -0.53238184 -0.37592403  1.44072511  1.7070679
  1.46716043  1.70433017 -1.88578428 -1.76142608 -0.63908267 -0.97085509
 -2.00679812 -1.71391466 -1.51214378 -2.01251347 -1.60202448 -1.10482146
 -1.50024001 -0.91733376 -1.48107903 -1.10160558 -1.42455216 -0.80934112
  0.34569608  1.49178132  0.13278268  0.20298679 -1.63933236 -0.80531742
 -0.43999831 -1.59914355 -0.43384918 -1.53307065  1.96992322  1.24400436
  0.44620236 -0.3159908   1.94715389 -0.51063162 24.82016711]
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.082437423589
gradient value of function right now is: [ 2.10215243e-03  2.05648564e-03  2.08561368e-03  2.46016874e-05
 -2.10215243e-03 -2.05648564e-03 -2.08561368e-03 -2.46016874e-05
  2.85150752e-05  2.40037012e-07  2.96416107e-05  2.99038514e-05
  1.84530297e-04  1.55046590e-06  1.91814978e-04  1.93509889e-04
  1.02190865e-04  8.59788159e-07  1.06227220e-04  1.07166680e-04
  3.62894618e-05  3.05294878e-07  3.77227923e-05  3.80563963e-05
  1.19229171e-05  1.17790233e-05  4.43073032e-06  1.14721669e-05
 -1.03075102e-06 -1.01830274e-06 -3.79303652e-07 -9.86860063e-07
  6.42081573e-06  6.34293632e-06  2.37873757e-06  6.15817932e-06
  1.24522760e-06  1.23017524e-06  4.57148914e-07  1.18992965e-06
  1.57883854e-06  1.02940616e-06  1.38280814e-06  8.05246268e-07
  1.59833634e-06  9.74368388e-07  1.37662195e-06  8.20500797e-07
  2.26172018e-06  7.51177833e-07  1.75571695e-06  1.26248787e-06
  9.28001669e-07  6.42406898e-07  8.25879213e-07  4.72942096e-07
  1.17815083e-06 -4.29849730e-07  4.25885569e-07 -5.46568404e-07
  1.38108968e-06 -1.58761846e-07  6.70810992e-07  2.24451772e-07
 -1.51606470e+00]
supnorm grad right now is: 1.516064704392898
Weights right now are: 
[-2.8968076  -2.57848397 -1.82084366 -0.79534071  3.11639785  1.34691429
  3.22222771  0.77552657 -1.39106133 -0.83505247 -2.4253233  -2.32533281
 -0.76744493  0.51825341 -2.3135949  -0.69402343 -1.41654369  0.43825173
 -1.10331779 -2.14772598  1.71627692  0.58104527  1.48826571  1.33818935
 -1.45168654 -1.20921958 -0.54144606 -0.39980674  1.44200032  1.70851818
  1.46760627  1.70549517 -1.89300099 -1.76961559 -0.6420219  -0.97814454
 -2.00824922 -1.71555766 -1.51275382 -2.01400025 -1.60815259 -1.10897211
 -1.50521541 -0.92012161 -1.48907083 -1.10675276 -1.43101523 -0.81304184
  0.3250463   1.48446362  0.1189905   0.19164878 -1.64717376 -0.81191269
 -0.44734486 -1.60295902 -0.46767196 -1.52358375  1.95272123  1.25867482
  0.33657157 -0.30428161  1.93501039 -0.51907071 24.85597439]
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.261116418531
gradient value of function right now is: [ 7.75977121e-04  7.64132197e-04  7.70872715e-04  9.29895414e-06
 -7.75977124e-04 -7.64132199e-04 -7.70872717e-04 -9.29895416e-06
  1.06647811e-05  8.71165400e-08  1.10485134e-05  1.11424629e-05
  5.56209830e-05  4.53817474e-07  5.76213349e-05  5.81109381e-05
  3.51606662e-05  2.87139832e-07  3.64256496e-05  3.67353328e-05
  1.38053606e-05  1.12696240e-07  1.43019629e-05  1.44235266e-05
  3.83750541e-06  3.79263182e-06  1.44200611e-06  3.69251104e-06
 -3.15745219e-07 -3.12052888e-07 -1.17316843e-07 -3.02331312e-07
  2.03069047e-06  2.00684173e-06  7.59971629e-07  1.94793467e-06
  4.21134293e-07  4.16206102e-07  1.56006777e-07  4.02484072e-07
  4.99731533e-07  3.24082465e-07  4.46021284e-07  2.55451494e-07
  5.06903364e-07  3.07178196e-07  4.44597294e-07  2.60861866e-07
  7.39552750e-07  2.42725299e-07  5.81162746e-07  4.14629396e-07
  2.98638090e-07  2.06217811e-07  2.71133546e-07  1.52092563e-07
  3.71181729e-07 -1.35397232e-07  1.31158100e-07 -1.68432700e-07
  4.39851304e-07 -5.20989357e-08  2.12854480e-07  7.59376252e-08
  5.12857863e+00]
supnorm grad right now is: 5.128578627397423
Weights right now are: 
[-2.99794462 -2.7893504  -1.9909296  -0.79760308  3.21753486  1.55778072
  3.39231365  0.77778894 -1.3968645  -0.83512738 -2.43213904 -2.33072445
 -0.87272979  0.51723737 -2.44060944 -0.78643066 -1.46227655  0.43736453
 -1.1530149  -2.19127981  1.70608406  0.58094555  1.47584589  1.32918784
 -1.47553834 -1.23638286 -0.54968827 -0.42131356  1.44307207  1.70973757
  1.46798382  1.70647404 -1.89929433 -1.77676025 -0.64460598 -0.98450006
 -2.00962645 -1.71711765 -1.51333676 -2.01541102 -1.61351831 -1.11264008
 -1.50969976 -0.92258583 -1.49607586 -1.11130558 -1.4368409  -0.81631609
  0.30641224  1.47781655  0.10630328  0.18133734 -1.65414482 -0.81784055
 -0.45407373 -1.60637351 -0.49660151 -1.51505866  1.9385193   1.27117614
  0.25199449 -0.29340557  1.92443591 -0.52627541 25.35131342]
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.057868517599
gradient value of function right now is: [ 2.87858029e-04  2.84634654e-04  2.86267458e-04  3.54228332e-06
 -2.87858021e-04 -2.84634646e-04 -2.86267450e-04 -3.54228323e-06
  4.00624271e-06  3.20372480e-08  4.13907310e-06  4.17320121e-06
  1.74891019e-05  1.39764094e-07  1.80688047e-05  1.82177236e-05
  1.22074028e-05  9.76151110e-08  1.26121377e-05  1.27161236e-05
  5.28909700e-06  4.22655942e-08  5.46440976e-06  5.50944532e-06
  1.25636026e-06  1.24203467e-06  4.77040019e-07  1.20898673e-06
 -1.00930609e-07 -9.97802624e-08 -3.78544892e-08 -9.66613884e-08
  6.64876217e-07  6.57265950e-07  2.51272722e-07  6.37919803e-07
  1.44827582e-07  1.43176050e-07  5.41318923e-08  1.38444887e-07
  1.63065863e-07  1.06303478e-07  1.47267573e-07  8.36112440e-08
  1.65605549e-07  1.00909189e-07  1.46927264e-07  8.54790684e-08
  2.46240872e-07  8.12399559e-08  1.95033627e-07  1.38500961e-07
  9.88657055e-08  6.87328003e-08  9.08974354e-08  5.04317502e-08
  1.21755310e-07 -4.58539095e-08  4.24956897e-08 -5.49744822e-08
  1.44664929e-07 -1.85538025e-08  7.00111970e-08  2.47258822e-08
  1.79335499e+00]
supnorm grad right now is: 1.793354985478052
Weights right now are: 
[-3.10056008 -3.00443011 -2.16369916 -0.79995632  3.32015032  1.77286043
  3.56508321  0.78014219 -1.4028284  -0.83520224 -2.43912171 -2.33624644
 -0.96146915  0.51640444 -2.54732433 -0.86405038 -1.50560315  0.43654729
 -1.19995003 -2.23240076  1.69541642  0.58084408  1.46288816  1.3197993
 -1.4967115  -1.26049678 -0.55708704 -0.44041036  1.44398834  1.71078037
  1.46830976  1.70731111 -1.90483114 -1.78304795 -0.64690257 -0.9900929
 -2.01091047 -1.7185725  -1.51388517 -2.01672655 -1.61820382 -1.11584158
 -1.51368345 -0.92474553 -1.50219402 -1.11527478 -1.44201421 -0.81918774
  0.28981351  1.47198742  0.09482886  0.1720846  -1.66026189 -0.82301948
 -0.46007262 -1.60937544 -0.51887521 -1.50802854  1.92859105  1.28093207
  0.20539969 -0.2876988   1.91564106 -0.53174189 25.11065142]
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.882853106112
gradient value of function right now is: [ 1.05599465e-04  1.04705108e-04  1.05107127e-04  1.33742230e-06
 -1.05599463e-04 -1.04705107e-04 -1.05107125e-04 -1.33742227e-06
  1.48702965e-06  1.16953221e-08  1.53289137e-06  1.54521029e-06
  5.58450032e-06  4.39086941e-08  5.75671091e-06  5.80296517e-06
  4.20796197e-06  3.30988517e-08  4.33774570e-06  4.37260738e-06
  2.00740702e-06  1.57755675e-08  2.06929738e-06  2.08591885e-06
  4.11045390e-07  4.06450907e-07  1.57623970e-07  3.95645061e-07
 -3.27332723e-08 -3.23677145e-08 -1.23910847e-08 -3.13599250e-08
  2.19552203e-07  2.17089607e-07  8.37700644e-08  2.10727752e-07
  4.96391421e-08  4.90845671e-08  1.87206379e-08  4.74693263e-08
  5.36388711e-08  3.50764733e-08  4.86210430e-08  2.75241906e-08
  5.45322534e-08  3.33347570e-08  4.85559526e-08  2.81688260e-08
  8.21984764e-08  2.71481406e-08  6.52513592e-08  4.63139569e-08
  3.29122320e-08  2.29669742e-08  3.03804002e-08  1.67900775e-08
  4.03227757e-08 -1.53630245e-08  1.39634831e-08 -1.81113626e-08
  4.79311474e-08 -6.37242550e-09  2.32280903e-08  8.19446388e-09
  2.84974090e+00]
supnorm grad right now is: 2.849740895601653
Weights right now are: 
[-3.203867   -3.2216864  -2.33779248 -0.80239194  3.42345724  1.99011672
  3.73917654  0.78257781 -1.40890642 -0.83527726 -2.44622031 -2.34185907
 -1.03808057  0.5156971  -2.63920538 -0.93089498 -1.54648921  0.43578991
 -1.24413057 -2.27110367  1.68431657  0.58074026  1.44943934  1.31005622
 -1.51549022 -1.28183767 -0.5637216  -0.45736243  1.44479263  1.71169533
  1.46859847  1.70804608 -1.90979547 -1.78868371 -0.64898024 -0.99510707
 -2.01211349 -1.719935   -1.51440294 -2.01795884 -1.62231408 -1.1186786
 -1.51721422 -0.92664821 -1.50749479 -1.11874141 -1.44654433 -0.82169027
  0.27623625  1.46721959  0.08519207  0.16451028 -1.66533867 -0.8272183
 -0.46503732 -1.61188054 -0.53164565 -1.50353529  1.92373267  1.28657669
  0.18722365 -0.28511133  1.90961449 -0.53444291 25.20634811]
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.898659612625
gradient value of function right now is: [ 3.92553801e-05  3.89977194e-05  3.91001265e-05  5.12779134e-07
 -3.92553935e-05 -3.89977327e-05 -3.91001397e-05 -5.12779308e-07
  5.58873337e-07  4.35630931e-09  5.75113273e-07  5.79645291e-07
  1.84523492e-06  1.43833944e-08  1.89885441e-06  1.91381774e-06
  1.47630634e-06  1.15104684e-08  1.51920980e-06  1.53118322e-06
  7.73259858e-07  6.02220688e-09  7.95721279e-07  8.01988347e-07
  1.38133303e-07  1.36607521e-07  5.32631785e-08  1.32943888e-07
 -1.10250129e-08 -1.09033647e-08 -4.19531601e-09 -1.05623551e-08
  7.47948084e-08  7.39659268e-08  2.86920788e-08  7.17872566e-08
  1.74388329e-08  1.72463495e-08  6.61009890e-09  1.66765526e-08
  1.81887445e-08  1.20215971e-08  1.65608360e-08  9.35078090e-09
  1.85050582e-08  1.14380765e-08  1.65488566e-08  9.57436173e-09
  2.80500007e-08  9.38116823e-09  2.23280866e-08  1.58151970e-08
  1.12922999e-08  7.95885649e-09  1.04702954e-08  5.77220620e-09
  1.38292543e-08 -5.43088224e-09  4.78820190e-09 -6.26236193e-09
  1.64079147e-08 -2.38819896e-09  7.96226996e-09  2.67117199e-09
 -2.72322022e+00]
supnorm grad right now is: 2.7232202193229567
Weights right now are: 
[-3.30700103 -3.43904925 -2.51171529 -0.80489664  3.52659127  2.20747957
  3.91309934  0.78508251 -1.41504068 -0.83535185 -2.45336905 -2.34751173
 -1.10496286  0.5150891  -2.71912083 -0.98920491 -1.58481543  0.43509785
 -1.28545049 -2.30731977  1.67285476  0.5806346   1.43558522  1.30001485
 -1.53146219 -1.29972435 -0.56945014 -0.47186626  1.44549875  1.71249486
  1.46885433  1.70869243 -1.91423036 -1.79369835 -0.65084618 -0.99958144
 -2.01322225 -1.72118485 -1.5148815  -2.0190923  -1.62557448 -1.12090797
 -1.52005449 -0.92819131 -1.51145629 -1.12128855 -1.44997996 -0.82361078
  0.26798314  1.46439609  0.07899137  0.15987766 -1.66853769 -0.82965208
 -0.46808665 -1.6134864  -0.53672145 -1.50160076  1.92194712  1.28885242
  0.18100937 -0.28421966  1.90683727 -0.53543836 24.77381176]
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.100186363474
gradient value of function right now is: [ 1.44090039e-05  1.43339250e-05  1.43603399e-05  1.94510463e-07
 -1.44090066e-05 -1.43339277e-05 -1.43603426e-05 -1.94510500e-07
  2.07238804e-07  1.60365547e-09  2.13014875e-07  2.14665369e-07
  6.14369656e-07  4.75531085e-09  6.31494891e-07  6.36388603e-07
  5.14164478e-07  3.98023214e-09  5.28497330e-07  5.32593179e-07
  2.94526507e-07  2.27696376e-09  3.02732115e-07  3.05076381e-07
  4.66823003e-08  4.61690785e-08  1.80553714e-08  4.49304404e-08
 -3.73710985e-09 -3.69606969e-09 -1.42644391e-09 -3.58067672e-09
  2.55207891e-08  2.52393019e-08  9.82007870e-09  2.44968507e-08
  6.09130739e-09  6.02439051e-09  2.31579078e-09  5.82564572e-09
  6.19799051e-09  4.08966149e-09  5.63951062e-09  3.18259205e-09
  6.31213675e-09  3.89384520e-09  5.64095841e-09  3.26232518e-09
  9.62213067e-09  3.20369852e-09  7.65455618e-09  5.42539678e-09
  3.88207664e-09  2.72976715e-09  3.59620458e-09  1.98165464e-09
  4.72957629e-09 -1.83836514e-09  1.63162100e-09 -2.12808315e-09
  5.60833799e-09 -8.11969505e-10  2.72525356e-09  9.28914742e-10
  1.43084967e+00]
supnorm grad right now is: 1.430849665335398
Weights right now are: 
[-3.41004321 -3.656394   -2.68551762 -0.80747876  3.62963348  2.42482437
  4.08690171  0.78766462 -1.42121275 -0.835426   -2.46054362 -2.35319472
 -1.16286826  0.51457193 -2.78745324 -1.04009787 -1.62035048  0.43450477
 -1.32363695 -2.34095274  1.66106245  0.58052693  1.42139049  1.28967926
 -1.54275016 -1.31176065 -0.57361809 -0.48233064  1.44608344  1.71314018
  1.46906876  1.70923164 -1.91797895 -1.79784163 -0.65240567 -1.00333324
 -2.01417204 -1.72222878 -1.5152828  -2.02005136 -1.62747281 -1.1221801
 -1.52174966 -0.92912983 -1.51352999 -1.12258763 -1.45181436 -0.82465648
  0.26453808  1.46323127  0.07628552  0.15794145 -1.66992128 -0.83064157
 -0.46937763 -1.6141896  -0.53850324 -1.50090581  1.92132864  1.28965613
  0.17889327 -0.28391137  1.90582173 -0.53577873 25.05677839]
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.168072978655
gradient value of function right now is: [ 5.32339817e-06  5.30030634e-06  5.30774434e-06  7.43398248e-08
 -5.32339707e-06 -5.30030524e-06 -5.30774325e-06 -7.43398093e-08
  7.73468460e-08  5.97174258e-10  7.94622039e-08  8.00720155e-08
  2.13274418e-07  1.64732804e-09  2.19108300e-07  2.20790219e-07
  1.82608765e-07  1.41055909e-09  1.87603954e-07  1.89044087e-07
  1.12956686e-07  8.71224200e-10  1.16044585e-07  1.16934572e-07
  1.64182362e-08  1.62379444e-08  6.35429915e-09  1.58006068e-08
 -1.31765994e-09 -1.30320647e-09 -5.03282918e-10 -1.26243367e-09
  9.01786825e-09  8.91852086e-09  3.47226358e-09  8.65543285e-09
  2.18397768e-09  2.16001297e-09  8.30822925e-10  2.08859976e-09
  2.18940306e-09  1.45021557e-09  1.99484595e-09  1.12480258e-09
  2.23024217e-09  1.38117322e-09  1.99570477e-09  1.15316804e-09
  3.40369230e-09  1.13823660e-09  2.70986987e-09  1.91924851e-09
  1.37809964e-09  9.72315833e-10  1.27826999e-09  7.03896072e-10
  1.67710718e-09 -6.58038089e-10  5.78912235e-10 -7.57085999e-10
  1.98691371e-09 -2.97475439e-10  9.66191767e-10  3.23207526e-10
 -9.75946608e-02]
supnorm grad right now is: 0.09759466076899333
Weights right now are: 
[-3.51389369 -3.87444637 -2.86023983 -0.8101588   3.73348397  2.64287676
  4.26162394  0.79034467 -1.4273365  -0.83549663 -2.46760486 -2.35885925
 -1.20739322  0.51419377 -2.83770074 -1.08093425 -1.65072606  0.43411444
 -1.35595676 -2.37016782  1.64930076  0.58042203  1.40747692  1.27924227
 -1.54827838 -1.31734795 -0.57572901 -0.48758958  1.44645584  1.71352802
  1.46920838  1.70958169 -1.92044973 -1.80042396 -0.65339655 -1.00575633
 -2.0147876  -1.72286825 -1.51552913 -2.02065556 -1.62825653 -1.1226952
 -1.52245865 -0.9295277  -1.51433926 -1.12308472 -1.45253498 -0.82507152
  0.26328646  1.46281551  0.0752927   0.15723797 -1.67042723 -0.8309954
 -0.46984583 -1.61444682 -0.53912469 -1.50066627  1.92111436  1.28993322
  0.17815692 -0.28380972  1.90546453 -0.53590417 24.98962321]
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.160025801097
gradient value of function right now is: [ 2.00432077e-06  1.99647063e-06  1.99892484e-06  2.87874410e-08
 -2.00432098e-06 -1.99647084e-06 -1.99892505e-06 -2.87874440e-08
  2.94754821e-08  2.27294032e-10  3.02757213e-08  3.05070223e-08
  7.94895467e-08  6.13276128e-10  8.16480984e-08  8.22720673e-08
  6.80546310e-08  5.25075467e-10  6.99026949e-08  7.04369131e-08
  4.38817510e-08  3.38020615e-10  4.50725554e-08  4.54166691e-08
  6.14449581e-09  6.07705355e-09  2.37896487e-09  5.91325829e-09
 -4.93725520e-10 -4.88312414e-10 -1.88653005e-10 -4.73033139e-10
  3.38120769e-09  3.34397565e-09  1.30239573e-09  3.24529072e-09
  8.22215209e-10  8.13197423e-10  3.12898914e-10  7.86304243e-10
  8.20806434e-10  5.43945667e-10  7.47939023e-10  4.21602264e-10
  8.36229023e-10  5.18087893e-10  7.48348622e-10  4.32296653e-10
  1.27715565e-09  4.27095069e-10  1.01682026e-09  7.20107157e-10
  5.17573015e-10  3.65280431e-10  4.80100927e-10  2.64304555e-10
  6.29397135e-10 -2.46854222e-10  2.17207723e-10 -2.84020098e-10
  7.45548609e-10 -1.12381891e-10  3.62614408e-10  1.21206641e-10
  5.12250925e-01]
supnorm grad right now is: 0.5122509248396022
Weights right now are: 
[-3.6182437  -4.08548187 -3.03221435 -0.81286109  3.83783397  2.85391226
  4.43359845  0.79304696 -1.43270807 -0.83555091 -2.47360315 -2.36396087
 -1.23204137  0.51399706 -2.86383621 -1.1052827  -1.67007574  0.43393632
 -1.37614573 -2.38951253  1.63963097  0.58034057  1.39665623  1.27021202
 -1.55043379 -1.31948719 -0.57656063 -0.48965963  1.4466233   1.71369554
  1.469272    1.7097414  -1.92158743 -1.80156352 -0.65383836 -1.00685371
 -2.01506626 -1.72314681 -1.51563618 -2.02092354 -1.62854609 -1.12288927
 -1.52272335 -0.92967673 -1.5146348  -1.12326987 -1.45280034 -0.82522468
  0.26283414  1.46266289  0.07493183  0.15698215 -1.67061038 -0.83112613
 -0.47001642 -1.61454065 -0.53934834 -1.500576    1.92103659  1.29003569
  0.17789175 -0.28376645  1.90533585 -0.53594524 25.00393031]
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.087729135466
gradient value of function right now is: [ 8.41433139e-07  8.38280020e-07  8.39257525e-07  1.22825682e-08
 -8.41432604e-07 -8.38279486e-07 -8.39256991e-07 -1.22825604e-08
  1.25858728e-08  9.69767759e-11  1.29265353e-08  1.30250921e-08
  3.39525272e-08  2.61751515e-10  3.48717328e-08  3.51376955e-08
  2.89490942e-08  2.23185870e-10  2.97328504e-08  2.99596224e-08
  1.87759647e-08  1.44511520e-10  1.92839295e-08  1.94308548e-08
  2.63548668e-09  2.60656589e-09  1.02064856e-09  2.53635443e-09
 -2.10490312e-10 -2.08183053e-10 -8.04510984e-11 -2.01673861e-10
  1.44777086e-09  1.43183232e-09  5.57810762e-10  1.38960525e-09
  3.52309115e-10  3.48445965e-10  1.34109794e-10  3.36929756e-10
  3.51845038e-10  2.32901958e-10  3.20472260e-10  1.80660020e-10
  3.58453052e-10  2.21812007e-10  3.20646947e-10  1.85247483e-10
  5.47657261e-10  1.82828419e-10  4.35893637e-10  3.08775030e-10
  2.21848228e-10  1.56381307e-10  2.05688644e-10  1.13243894e-10
  2.69687258e-10 -1.05358654e-10  9.30240198e-11 -1.21497720e-10
  3.19515140e-10 -4.77795605e-11  1.55391478e-10  5.23051510e-11
  1.54504767e+00]
supnorm grad right now is: 1.5450476677189555
Weights right now are: 
[-3.7220715  -4.26194913 -3.18612505 -0.81525758  3.94166178  3.03037952
  4.58750916  0.79544344 -1.4361858  -0.83558026 -2.47728799 -2.36743549
 -1.24289687  0.51391276 -2.87504726 -1.11642056 -1.67919649  0.43386364
 -1.38554421 -2.39888145  1.63406948  0.5802966   1.39080208  1.26464637
 -1.55129108 -1.32033546 -0.57689235 -0.49048434  1.44669179  1.71376338
  1.46929814  1.70980696 -1.9220564  -1.80202819 -0.65401921 -1.00730411
 -2.01518053 -1.72326    -1.51567973 -2.0210329  -1.62866069 -1.12296555
 -1.522828   -0.92973565 -1.51475154 -1.12334255 -1.45290505 -0.82528511
  0.262656    1.46260294  0.07478977  0.15688154 -1.67068269 -0.83117738
 -0.47008365 -1.61457762 -0.5394368  -1.50054047  1.92100593  1.29007577
  0.17778728 -0.28375007  1.90528512 -0.53596172 25.08421656]
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.167652361039
gradient value of function right now is: [ 4.47755795e-07  4.46112340e-07  4.46619914e-07  6.58997348e-09
 -4.47757124e-07 -4.46113664e-07 -4.46621239e-07 -6.58999325e-09
  6.82064942e-09  5.25701836e-11  7.00505775e-09  7.05843786e-09
  1.83532793e-08  1.41535783e-10  1.88496097e-08  1.89932963e-08
  1.56350289e-08  1.20576993e-10  1.60578539e-08  1.61802609e-08
  1.00936718e-08  7.77088320e-11  1.03664380e-08  1.04453764e-08
  1.43331858e-09  1.41758980e-09  5.55002946e-10  1.37934247e-09
 -1.13433798e-10 -1.12190411e-10 -4.33482887e-11 -1.08677873e-10
  7.85471891e-10  7.76824632e-10  3.02588120e-10  7.53881324e-10
  1.91388552e-10  1.89289937e-10  7.28418063e-11  1.83025967e-10
  1.91087957e-10  1.26736848e-10  1.74173434e-10  9.81627070e-11
  1.94636730e-10  1.20686143e-10  1.74228794e-10  1.00629602e-10
  2.97137608e-10  9.94513977e-11  2.36605507e-10  1.67540417e-10
  1.20432860e-10  8.50522670e-11  1.11741587e-10  6.15085668e-11
  1.46573175e-10 -5.75872786e-11  5.06162822e-11 -6.62016219e-11
  1.73630592e-10 -2.63493850e-11  8.44265878e-11  2.81227271e-11
  1.64092749e-01]
supnorm grad right now is: 0.1640927489838661
Weights right now are: 
[-3.81460893 -4.37699396 -3.29620343 -0.81691422  4.03419922  3.14542436
  4.69758755  0.79710009 -1.43804926 -0.83559488 -2.47921277 -2.36935085
 -1.24804114  0.51387297 -2.88033481 -1.12173808 -1.68357002  0.43382969
 -1.39003827 -2.40340283  1.63126368  0.58027486  1.38790906  1.26175911
 -1.55169278 -1.32073279 -0.57704766 -0.49087084  1.44672377  1.71379502
  1.46931035  1.7098376  -1.9222768  -1.80224622 -0.654104   -1.00751563
 -2.01523426 -1.72331315 -1.51570015 -2.02108428 -1.62871415 -1.12300153
 -1.52287693 -0.92976322 -1.51480599 -1.1233768  -1.45295399 -0.82531335
  0.26257295  1.46257476  0.07472344  0.1568346  -1.67071641 -0.83120154
 -0.47011507 -1.61459491 -0.5394779  -1.50052393  1.92099167  1.29009458
  0.17773856 -0.28374176  1.90526145 -0.53596909 24.96082247]
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.078584242829
gradient value of function right now is: [ 2.92065946e-07  2.91006992e-07  2.91333321e-07  4.31940256e-09
 -2.92067313e-07 -2.91008354e-07 -2.91334684e-07 -4.31942297e-09
  4.51215410e-09  3.47558973e-11  4.63402335e-09  4.66930733e-09
  1.20914436e-08  9.31890981e-11  1.24181008e-08  1.25126860e-08
  1.03015214e-08  7.93963667e-11  1.05798256e-08  1.06604100e-08
  6.62438666e-09  5.09674188e-11  6.80321660e-09  6.85497985e-09
  9.48499007e-10  9.38091803e-10  3.67362708e-10  9.12815815e-10
 -7.44799789e-11 -7.36636769e-11 -2.84696960e-11 -7.13601427e-11
  5.18771331e-10  5.13060861e-10  1.99896698e-10  4.97926736e-10
  1.26505040e-10  1.25118050e-10  4.81599976e-11  1.20982270e-10
  1.26344933e-10  8.36322595e-11  1.15079809e-10  6.48718212e-11
  1.28686830e-10  7.96288298e-11  1.15114059e-10  6.65031797e-11
  1.96539125e-10  6.56025359e-11  1.56428007e-10  1.10811461e-10
  7.95885016e-11  5.60967531e-11  7.37898139e-11  4.06249616e-11
  9.68240273e-11 -3.78218586e-11  3.34108914e-11 -4.36219712e-11
  1.14727392e-10 -1.71534968e-11  5.57805759e-11  1.87828181e-11
  1.61992745e+00]
supnorm grad right now is: 1.6199274507790766
Weights right now are: 
[-3.8839737  -4.44925029 -3.36799097 -0.8179802   4.103564    3.21768071
  4.76937511  0.79816607 -1.43917225 -0.83560356 -2.48036702 -2.37051189
 -1.25106732  0.51384962 -2.88344312 -1.12486929 -1.68614725  0.4338098
 -1.39268534 -2.40606954  1.62960479  0.58026208  1.38620446  1.26004374
 -1.55192973 -1.32096714 -0.57713939 -0.49109886  1.44674247  1.71381351
  1.46931749  1.70985551 -1.92240655 -1.80237455 -0.65415397 -1.00764017
 -2.01526589 -1.72334444 -1.51571219 -2.02111453 -1.62874575 -1.12302251
 -1.52290573 -0.92977946 -1.51483819 -1.12339678 -1.45298279 -0.82533
  0.26252388  1.4625583   0.07468441  0.15680698 -1.67073633 -0.83121562
 -0.47013354 -1.61460509 -0.53950208 -1.50051447  1.92098334  1.2901055
  0.17770988 -0.28373744  1.9052475  -0.5359738  25.09906648]
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.134640876493
gradient value of function right now is: [ 2.17168583e-07  2.16387804e-07  2.16628037e-07  3.22267483e-09
 -2.17167560e-07 -2.16386785e-07 -2.16627017e-07 -3.22265952e-09
  3.38914221e-09  2.61255517e-11  3.48066449e-09  3.50717103e-09
  9.05184407e-09  6.98164221e-11  9.29634420e-09  9.36716377e-09
  7.71311839e-09  5.94924926e-11  7.92146006e-09  7.98180630e-09
  4.94669400e-09  3.80881413e-11  5.08021046e-09  5.11887016e-09
  7.12655430e-10  7.04834998e-10  2.75930167e-10  6.85795195e-10
 -5.56797457e-11 -5.50694237e-11 -2.12759286e-11 -5.33436079e-11
  3.89365035e-10  3.85078549e-10  1.49983366e-10  3.73693293e-10
  9.50475312e-11  9.40053183e-11  3.61715030e-11  9.08917177e-11
  9.48532420e-11  6.29884579e-11  8.64992044e-11  4.87409120e-11
  9.65919112e-11  5.99699527e-11  8.65047970e-11  4.99520667e-11
  1.47354423e-10  4.94030631e-11  1.17371519e-10  8.30856155e-11
  5.97415165e-11  4.22396208e-11  5.54572572e-11  3.05222105e-11
  7.27816834e-11 -2.87166707e-11  2.51590771e-11 -3.29442460e-11
  8.62226334e-11 -1.31894259e-11  4.19154989e-11  1.38697719e-11
 -9.65594355e-01]
supnorm grad right now is: 0.9655943552462398
Weights right now are: 
[-3.93409249 -4.49950626 -3.41825058 -0.81872695  4.15368279  3.26793668
  4.81963472  0.79891282 -1.43995609 -0.83560961 -2.48117212 -2.37132294
 -1.25316561  0.51383342 -2.88559815 -1.12704067 -1.68793497  0.433796
 -1.3945214  -2.40791954  1.62845677  0.58025323  1.38502536  1.25885586
 -1.55209468 -1.32113027 -0.57720325 -0.49125759  1.44675539  1.71382629
  1.46932243  1.70986789 -1.92249674 -1.80246374 -0.6541887  -1.00772672
 -2.0152879  -1.72336621 -1.51572056 -2.02113558 -1.62876774 -1.12303709
 -1.52292577 -0.92979076 -1.51486057 -1.12341067 -1.45300283 -0.82534157
  0.26248977  1.46254684  0.07465725  0.15678779 -1.67075018 -0.8312254
 -0.47014639 -1.61461217 -0.53951894 -1.50050783  1.92097751  1.29011312
  0.17768992 -0.28373442  1.90523778 -0.53597702 24.95628034]
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.095871269516
gradient value of function right now is: [ 1.72094644e-07  1.71479993e-07  1.71668883e-07  2.56050616e-09
 -1.72096102e-07 -1.71481446e-07 -1.71670338e-07 -2.56052809e-09
  2.70665776e-09  2.08667353e-11  2.77972399e-09  2.80088919e-09
  7.20986065e-09  5.56153527e-11  7.40453827e-09  7.46093709e-09
  6.14448394e-09  4.73984948e-11  6.31039624e-09  6.35846161e-09
  3.93301856e-09  3.02861124e-11  4.03913700e-09  4.06986917e-09
  5.69116362e-10  5.62870888e-10  2.20347073e-10  5.47657405e-10
 -4.42845783e-11 -4.37991467e-11 -1.69211356e-11 -4.24259104e-11
  3.10657339e-10  3.07237241e-10  1.19661376e-10  2.98148921e-10
  7.58822351e-11  7.50501457e-11  2.88769790e-11  7.25633334e-11
  7.57126830e-11  5.03035837e-11  6.90587848e-11  3.89085779e-11
  7.70938206e-11  4.78902618e-11  6.90569561e-11  3.98713523e-11
  1.17571674e-10  3.94488731e-11  9.36620314e-11  6.62901913e-11
  4.76772057e-11  3.37252717e-11  4.42672473e-11  2.43609343e-11
  5.81112392e-11 -2.29734788e-11  2.00967546e-11 -2.63250449e-11
  6.88399927e-11 -1.05690937e-11  3.34626721e-11  1.10386542e-11
 -1.43487290e+00]
supnorm grad right now is: 1.4348729046396762
Weights right now are: 
[-3.97317021 -4.53847743 -3.45725938 -0.81930798  4.19276051  3.30690785
  4.85864352  0.79949384 -1.44056869 -0.83561433 -2.48180126 -2.37195685
 -1.25479991  0.51382083 -2.88727658 -1.12873187 -1.68932765  0.43378526
 -1.39595168 -2.40936071  1.62756439  0.58024637  1.38410889  1.25793244
 -1.55222346 -1.32125764 -0.57725314 -0.49138152  1.44676543  1.71383622
  1.46932626  1.70987751 -1.92256706 -1.80253329 -0.65421581 -1.00779422
 -2.01530506 -1.72338318 -1.5157271  -2.02115199 -1.6287849  -1.12304842
 -1.52294139 -0.92979956 -1.51487805 -1.12342145 -1.45301846 -0.82535059
  0.26246312  1.46253795  0.07463603  0.15677277 -1.67076099 -0.83123299
 -0.47015641 -1.61461768 -0.53953212 -1.5005027   1.92097297  1.29011902
  0.17767433 -0.28373212  1.9052302  -0.53597955 24.87723704]
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.021504827195
gradient value of function right now is: [ 1.42641299e-07  1.42134568e-07  1.42290140e-07  2.12680360e-09
 -1.42642037e-07 -1.42135304e-07 -1.42290877e-07 -2.12681466e-09
  2.25737163e-09  1.74052762e-11  2.31829310e-09  2.33594298e-09
  6.00008341e-09  4.62895925e-11  6.16205212e-09  6.20898218e-09
  5.11413897e-09  3.94556725e-11  5.25219333e-09  5.29219424e-09
  3.26864853e-09  2.51733126e-11  3.35681757e-09  3.38235539e-09
  4.74615143e-10  4.69406577e-10  1.83746893e-10  4.56712195e-10
 -3.68130023e-11 -3.64094592e-11 -1.40652929e-11 -3.52674034e-11
  2.58888919e-10  2.56038675e-10  9.97143168e-11  2.48461132e-10
  6.32707121e-11  6.25768927e-11  2.40760411e-11  6.05025415e-11
  6.31147618e-11  4.19604455e-11  5.75829009e-11  3.24409568e-11
  6.42612732e-11  3.99458886e-11  5.75767076e-11  3.32405233e-11
  9.79709626e-11  3.29046012e-11  7.80608259e-11  5.52403575e-11
  3.97384439e-11  2.81268444e-11  3.69060198e-11  2.03091330e-11
  4.84536969e-11 -1.91972996e-11  1.67644226e-11 -2.19719462e-11
  5.73950065e-11 -8.85001535e-12  2.78985901e-11  9.16670667e-12
 -1.99524891e+00]
supnorm grad right now is: 1.995248910018421
Weights right now are: 
[-4.00475577 -4.56995406 -3.48876999 -0.8197784   4.22434608  3.33838448
  4.89015413  0.79996426 -1.44106689 -0.83561816 -2.48231291 -2.37247239
 -1.25612572  0.51381061 -2.88863817 -1.13010383 -1.69045762  0.43377655
 -1.39711214 -2.41053001  1.62684159  0.5802408   1.3833666   1.25718451
 -1.55232818 -1.32136121 -0.57729368 -0.49148231  1.44677357  1.71384427
  1.46932937  1.70988531 -1.92262419 -1.80258979 -0.65423781 -1.00784905
 -2.01531902 -1.72339699 -1.51573241 -2.02116534 -1.62879882 -1.12305765
 -1.52295408 -0.92980672 -1.51489222 -1.12343025 -1.45303115 -0.82535794
  0.26244148  1.46253069  0.0746188   0.15676054 -1.67076975 -0.83123918
 -0.47016454 -1.61462216 -0.53954279 -1.5004985   1.9209693   1.29012382
  0.17766172 -0.28373024  1.90522406 -0.53598159 24.81467337]
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.162526294638
gradient value of function right now is: [ 1.21464034e-07  1.21034498e-07  1.21166261e-07  1.81423457e-09
 -1.21463053e-07 -1.21033521e-07 -1.21165283e-07 -1.81421979e-09
  1.93209714e-09  1.48887267e-11  1.98421190e-09  1.99931057e-09
  5.12614052e-09  3.95247143e-11  5.26444284e-09  5.30451623e-09
  4.36973752e-09  3.36933366e-11  4.48763319e-09  4.52179368e-09
  2.78959031e-09  2.14714477e-11  2.86479622e-09  2.88657962e-09
  4.06076659e-10  4.01620892e-10  1.57246584e-10  3.90776157e-10
 -3.14038388e-11 -3.10596413e-11 -1.20013661e-11 -3.00866513e-11
  2.21337827e-10  2.18901340e-10  8.52700404e-11  2.12431811e-10
  5.41042500e-11  5.35110368e-11  2.05927021e-11  5.17393114e-11
  5.39858017e-11  3.58242290e-11  4.92180112e-11  2.77356765e-11
  5.49689750e-11  3.41019238e-11  4.92156263e-11  2.84221308e-11
  8.38558315e-11  2.80850140e-11  6.67812805e-11  4.72812662e-11
  3.39825623e-11  2.40091940e-11  3.15364881e-11  1.73580133e-11
  4.14078572e-11 -1.62980774e-11  1.43105486e-11 -1.87205479e-11
  4.90575001e-11 -7.46807763e-12  2.38457488e-11  7.91992839e-12
 -3.78301927e-01]
supnorm grad right now is: 0.3783019272094757
Weights right now are: 
[-4.0312448  -4.59634958 -3.51519427 -0.82017366  4.25083511  3.36478
  4.91657841  0.80035953 -1.44148705 -0.8356214  -2.48274441 -2.37290717
 -1.25724161  0.51380201 -2.88978417 -1.13125855 -1.69140878  0.43376922
 -1.39808897 -2.41151427  1.62623398  0.58023613  1.3827426   1.25655577
 -1.55241649 -1.32144855 -0.57732786 -0.49156728  1.44678041  1.71385103
  1.46933198  1.70989186 -1.92267234 -1.80263741 -0.65425635 -1.00789526
 -2.01533079 -1.72340863 -1.51573688 -2.02117659 -1.62881053 -1.12306547
 -1.52296477 -0.92981275 -1.51490415 -1.12343768 -1.45304184 -0.82536411
  0.26242325  1.46252458  0.07460426  0.15675024 -1.67077712 -0.83124442
 -0.47017139 -1.61462593 -0.53955179 -1.50049495  1.92096619  1.29012788
  0.17765106 -0.28372853  1.90521889 -0.53598328 24.98022095]
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.077236496369
gradient value of function right now is: [ 1.05684009e-07  1.05311750e-07  1.05425862e-07  1.58091963e-09
 -1.05685435e-07 -1.05313170e-07 -1.05427283e-07 -1.58094117e-09
  1.68846394e-09  1.30023564e-11  1.73398190e-09  1.74716914e-09
  4.47267165e-09  3.44625943e-11  4.59327669e-09  4.62822174e-09
  3.81310594e-09  2.93812442e-11  3.91592674e-09  3.94571881e-09
  2.43188588e-09  1.87052709e-11  2.49741181e-09  2.51639109e-09
  3.54724320e-10  3.50832590e-10  1.37399123e-10  3.41376903e-10
 -2.73610846e-11 -2.70612387e-11 -1.04595106e-11 -2.62148629e-11
  1.93216170e-10  1.91089534e-10  7.44573432e-11  1.85451537e-10
  4.72348832e-11  4.67170587e-11  1.79835402e-11  4.51725617e-11
  4.71506631e-11  3.12123305e-11  4.29477625e-11  2.42092814e-11
  4.80124758e-11  2.97099789e-11  4.29493569e-11  2.48117959e-11
  7.32982496e-11  2.44647563e-11  5.83391372e-11  4.13269763e-11
  2.96733942e-11  2.09143553e-11  2.75115844e-11  1.51461690e-11
  3.61270232e-11 -1.41131253e-11  1.24715134e-11 -1.62778240e-11
  4.28120238e-11 -6.40272412e-12  2.08097965e-11  7.00789813e-12
  1.65920523e+00]
supnorm grad right now is: 1.6592052299633082
Weights right now are: 
[-4.05403235 -4.61905673 -3.53792609 -0.82051426  4.27362265  3.38748715
  4.93931022  0.80070012 -1.44185025 -0.8356242  -2.48311739 -2.37328299
 -1.25820453  0.51379458 -2.89077307 -1.13225497 -1.69222966  0.43376289
 -1.39893199 -2.41236371  1.62571017  0.5802321   1.38220468  1.25601375
 -1.55249282 -1.32152404 -0.57735742 -0.49164074  1.4467863   1.71385687
  1.46933424  1.70989751 -1.92271393 -1.80267854 -0.65427238 -1.00793518
 -2.01534095 -1.72341868 -1.51574076 -2.02118631 -1.62882069 -1.12307219
 -1.52297403 -0.92981796 -1.51491449 -1.12344408 -1.4530511  -0.82536945
  0.26240749  1.46251931  0.0745917   0.15674135 -1.67078352 -0.83124892
 -0.47017732 -1.61462919 -0.53955959 -1.50049189  1.9209635   1.2901314
  0.17764183 -0.28372714  1.9052144  -0.53598477 25.08762141]
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.168515741188
gradient value of function right now is: [ 9.40581571e-08  9.37279719e-08  9.38291229e-08  1.40893607e-09
 -9.40574058e-08 -9.37272234e-08 -9.38283735e-08 -1.40892474e-09
  1.50846455e-09  1.16220882e-11  1.54913079e-09  1.56091432e-09
  3.99035822e-09  3.07618676e-11  4.09795985e-09  4.12914227e-09
  3.40223135e-09  2.62285689e-11  3.49397462e-09  3.52056132e-09
  2.16804914e-09  1.66842784e-11  2.22646711e-09  2.24339035e-09
  3.16931701e-10  3.13454357e-10  1.22733173e-10  3.04992616e-10
 -2.44033498e-11 -2.41359001e-11 -9.32656333e-12 -2.33799884e-11
  1.72574762e-10  1.70675199e-10  6.64876948e-11  1.65632229e-10
  4.22096493e-11  4.17468859e-11  1.60663561e-11  4.03649613e-11
  4.21119135e-11  2.79364334e-11  3.83882902e-11  2.16344996e-11
  4.28771311e-11  2.65914333e-11  3.83848722e-11  2.21693129e-11
  6.54148725e-11  2.18966616e-11  5.20906847e-11  3.68854449e-11
  2.65018787e-11  1.87182757e-11  2.45911968e-11  1.35361527e-11
  3.22916437e-11 -1.26964041e-11  1.11596315e-11 -1.45930554e-11
  3.82606863e-11 -5.81038042e-12  1.85970317e-11  6.19117252e-12
 -4.99866815e-15]
supnorm grad right now is: 9.405815707050868e-08
Weights right now are: 
[-4.07396203 -4.63891632 -3.55780715 -0.82081258  4.29355233  3.40734674
  4.95919128  0.80099845 -1.44216921 -0.83562666 -2.48344496 -2.37361305
 -1.25904892  0.51378807 -2.89164024 -1.13312875 -1.69294956  0.43375734
 -1.39967132 -2.41310867  1.62525121  0.58022856  1.38173335  1.25553884
 -1.55255986 -1.32159035 -0.5773834  -0.49170525  1.44679147  1.71386198
  1.46933622  1.70990246 -1.92275045 -1.80271466 -0.65428646 -1.00797023
 -2.01534988 -1.72342751 -1.51574416 -2.02119485 -1.62882964 -1.12307807
 -1.52298216 -0.92982254 -1.5149236  -1.12344968 -1.45305923 -0.82537414
  0.26239364  1.46251468  0.07458069  0.15673358 -1.67078915 -0.83125286
 -0.47018253 -1.61463206 -0.53956647 -1.50048922  1.92096114  1.29013445
  0.17763371 -0.283726    1.90521045 -0.5359861  24.99099392]
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.895417220034
gradient value of function right now is: [ 8.48438561e-08  8.45469228e-08  8.46378352e-08  1.27250879e-09
 -8.48425876e-08 -8.45456588e-08 -8.46365698e-08 -1.27248951e-09
  1.36537444e-09  1.05292642e-11  1.40219169e-09  1.41286242e-09
  3.60734611e-09  2.78347117e-11  3.70464233e-09  3.73284485e-09
  3.07594151e-09  2.37348540e-11  3.15890546e-09  3.18295356e-09
  1.95869738e-09  1.50869620e-11  2.01148654e-09  2.02678270e-09
  2.86938177e-10  2.83789266e-10  1.11079734e-10  2.76106757e-10
 -2.20640186e-11 -2.18221581e-11 -8.42936412e-12 -2.11370833e-11
  1.56215686e-10  1.54495855e-10  6.01637096e-11  1.49919322e-10
  3.82316527e-11  3.78124176e-11  1.45467728e-11  3.65579833e-11
  3.81164983e-11  2.53708520e-11  3.47899704e-11  1.95963776e-11
  3.88024439e-11  2.41495856e-11  3.47799824e-11  2.00754795e-11
  5.91254490e-11  1.98874041e-11  4.71220193e-11  3.33384427e-11
  2.39893378e-11  1.69987634e-11  2.22887126e-11  1.22634930e-11
  2.92743644e-11 -1.16331407e-11  1.01352205e-11 -1.32931397e-11
  3.46749897e-11 -5.39599247e-12  1.68515012e-11  5.50145964e-12
 -2.74245582e+00]
supnorm grad right now is: 2.7424558195805995
Weights right now are: 
[-4.09192992 -4.65682122 -3.57573134 -0.82108188  4.31152023  3.42525165
  4.97711549  0.80126774 -1.44245781 -0.83562888 -2.48374134 -2.37391169
 -1.25981194  0.51378219 -2.89242383 -1.13391831 -1.69360015  0.43375232
 -1.40033945 -2.41378188  1.62483676  0.58022537  1.38130773  1.25510999
 -1.55262049 -1.32165031 -0.57740689 -0.49176359  1.44679614  1.71386659
  1.469338    1.70990693 -1.92278346 -1.80274731 -0.65429918 -1.00800192
 -2.01535796 -1.7234355  -1.51574723 -2.02120257 -1.6288377  -1.12308342
 -1.52298951 -0.92982667 -1.51493181 -1.12345477 -1.45306657 -0.82537838
  0.26238112  1.4625105   0.07457073  0.15672654 -1.67079422 -0.83125645
 -0.47018723 -1.61463465 -0.53957264 -1.50048682  1.92095902  1.29013723
  0.17762637 -0.28372488  1.90520689 -0.53598732 24.82681766]
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.105064008296
gradient value of function right now is: [ 7.68100364e-08  7.65419842e-08  7.66240118e-08  1.15326224e-09
 -7.68088063e-08 -7.65407585e-08 -7.66227848e-08 -1.15324362e-09
  1.23987402e-09  9.55636096e-12  1.27329319e-09  1.28297886e-09
  3.27204423e-09  2.52340872e-11  3.36025997e-09  3.38582991e-09
  2.79026359e-09  2.15190513e-11  2.86549086e-09  2.88729597e-09
  1.77568160e-09  1.36699531e-11  1.82351822e-09  1.83737905e-09
  2.60473633e-10  2.57615476e-10  1.00855422e-10  2.50651642e-10
 -1.99923540e-11 -1.97732272e-11 -7.63958780e-12 -1.91532507e-11
  1.41739513e-10  1.40179225e-10  5.45999530e-11  1.36032275e-10
  3.46908476e-11  3.43104821e-11  1.32024421e-11  3.31735371e-11
  3.45966240e-11  2.29854467e-11  3.15556398e-11  1.77790350e-11
  3.52211937e-11  2.18780818e-11  3.15487728e-11  1.82156038e-11
  5.37003458e-11  1.80150804e-11  4.27785963e-11  3.02782793e-11
  2.17705151e-11  1.53985756e-11  2.02127967e-11  1.11236330e-11
  2.65473151e-11 -1.04909071e-11  9.18303739e-12 -1.20256915e-11
  3.14523109e-11 -4.82728947e-12  1.52859974e-11  5.04560253e-12
 -1.35746043e+00]
supnorm grad right now is: 1.357460430679715
Weights right now are: 
[-4.10822061 -4.67305497 -3.59198252 -0.82132633  4.32781092  3.4414854
  4.99336667  0.80151219 -1.44272033 -0.8356309  -2.48401094 -2.37418334
 -1.26050518  0.51377684 -2.89313576 -1.13463565 -1.69419128  0.43374777
 -1.40094652 -2.41439357  1.62446044  0.58022248  1.38092127  1.25472059
 -1.55267565 -1.32170486 -0.57742826 -0.49181667  1.44680037  1.71387078
  1.46933962  1.70991099 -1.92281348 -1.802777   -0.65431076 -1.00803073
 -2.0153653  -1.72344276 -1.51575003 -2.0212096  -1.62884504 -1.12308827
 -1.52299619 -0.92983044 -1.51493928 -1.12345939 -1.45307326 -0.82538224
  0.26236974  1.46250669  0.07456167  0.15672013 -1.67079884 -0.8312597
 -0.47019152 -1.614637   -0.5395783  -1.50048459  1.92095706  1.29013977
  0.1776197  -0.28372387  1.90520364 -0.5359884  24.86524833]
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.7414230004
gradient value of function right now is: [ 6.98254074e-08  6.95823723e-08  6.96567101e-08  1.04937257e-09
 -6.98250408e-08 -6.95820069e-08 -6.96563443e-08 -1.04936701e-09
  1.13020115e-09  8.69825686e-12  1.16063852e-09  1.16945838e-09
  2.97952606e-09  2.29444751e-11  3.05978764e-09  3.08304778e-09
  2.54101357e-09  1.95680368e-11  2.60946313e-09  2.62930006e-09
  1.61620488e-09  1.24239033e-11  1.65970841e-09  1.67231141e-09
  2.37258747e-10  2.34656427e-10  9.19192750e-11  2.28339591e-10
 -1.81724257e-11 -1.79733333e-11 -6.94838018e-12 -1.74117280e-11
  1.29020574e-10  1.27600889e-10  4.97293981e-11  1.23840051e-10
  3.15685097e-11  3.12225334e-11  1.20214137e-11  3.01911434e-11
  3.15090252e-11  2.08324103e-11  2.86840864e-11  1.61718079e-11
  3.20848370e-11  1.98265367e-11  2.86849558e-11  1.65750470e-11
  4.90121871e-11  1.63175449e-11  3.89922763e-11  2.76348835e-11
  1.98210121e-11  1.39536393e-11  1.83661606e-11  1.01124843e-11
  2.41167039e-11 -9.36922448e-12  8.32213642e-12 -1.08470930e-11
  2.85950556e-11 -4.24138861e-12  1.38976734e-11  4.72727863e-12
  3.52114495e+00]
supnorm grad right now is: 3.5211449454233184
Weights right now are: 
[-4.12293651 -4.68771957 -3.60666281 -0.82154738  4.34252683  3.45615001
  5.00804698  0.80173325 -1.44295818 -0.83563274 -2.4842552  -2.37442946
 -1.26113257  0.513772   -2.89378008 -1.13528488 -1.69472632  0.43374364
 -1.40149599 -2.41494722  1.62412004  0.58021985  1.3805717   1.25436836
 -1.55272563 -1.3217543  -0.5774476  -0.49186476  1.44680421  1.71387457
  1.46934108  1.70991466 -1.92284068 -1.8028039  -0.65432123 -1.00805683
 -2.01537196 -1.72344935 -1.51575256 -2.02121597 -1.62885167 -1.1230927
 -1.52300225 -0.92983385 -1.51494603 -1.12346361 -1.45307931 -0.82538573
  0.26235945  1.46250322  0.07455347  0.15671432 -1.67080301 -0.83126267
 -0.4701954  -1.61463913 -0.53958339 -1.50048257  1.9209553   1.29014209
  0.17761366 -0.2837229   1.90520071 -0.53598935 25.23604908]
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.260231047601
gradient value of function right now is: [ 6.52457774e-08  6.50191780e-08  6.50884578e-08  9.81565604e-10
 -6.52453838e-08 -6.50187858e-08 -6.50880652e-08 -9.81559627e-10
  1.05883668e-09  8.17142051e-12  1.08738412e-09  1.09566059e-09
  2.78878232e-09  2.15346623e-11  2.86399002e-09  2.88579675e-09
  2.37851649e-09  1.83670262e-11  2.44266062e-09  2.46125940e-09
  1.51204704e-09  1.16551749e-11  1.55279254e-09  1.56460262e-09
  2.22501156e-10  2.20059223e-10  8.61092873e-11  2.14086961e-10
 -1.70413247e-11 -1.68545128e-11 -6.50832860e-12 -1.63242252e-11
  1.21040319e-10  1.19707678e-10  4.66020690e-11  1.16153444e-10
  2.96517028e-11  2.93265388e-11  1.12783887e-11  2.83516113e-11
  2.95363777e-11  1.97245525e-11  2.69903852e-11  1.51960253e-11
  3.00627655e-11  1.87739352e-11  2.69769817e-11  1.55635914e-11
  4.57633249e-11  1.54569837e-11  3.64987051e-11  2.58053208e-11
  1.85875697e-11  1.32137203e-11  1.72910136e-11  9.50993445e-12
  2.27133091e-11 -9.11025373e-12  7.87797732e-12 -1.03624041e-11
  2.69008464e-11 -4.28634004e-12  1.30701673e-11  4.19164297e-12
 -4.89023845e+00]
supnorm grad right now is: 4.8902384518049
Weights right now are: 
[-4.13646187 -4.70119791 -3.62015554 -0.82175074  4.35605223  3.46962838
  5.02153973  0.8019366  -1.44317735 -0.83563443 -2.48448027 -2.37465625
 -1.26171013  0.51376755 -2.89437319 -1.1358825  -1.69521889  0.43373984
 -1.40200183 -2.41545691  1.62380681  0.58021745  1.38025004  1.25404426
 -1.55277164 -1.3217998  -0.57746543 -0.49190905  1.44680773  1.71387806
  1.46934243  1.70991803 -1.9228657  -1.80282864 -0.65433087 -1.00808084
 -2.01537809 -1.72345541 -1.51575489 -2.02122182 -1.62885778 -1.12309674
 -1.52300782 -0.92983699 -1.51495225 -1.12346745 -1.45308488 -0.82538895
  0.26234995  1.46250006  0.07454591  0.15670897 -1.67080686 -0.83126537
 -0.47019896 -1.6146411  -0.53958807 -1.50048074  1.92095368  1.2901442
  0.17760812 -0.28372209  1.90519802 -0.53599026 24.6549539 ]
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.04828015643
gradient value of function right now is: [ 6.02357691e-08  6.00270408e-08  6.00908315e-08  9.06923987e-10
 -6.02350864e-08 -6.00263605e-08 -6.00901505e-08 -9.06913635e-10
  9.79781429e-10  7.55240602e-12  1.00617979e-09  1.01383213e-09
  2.57827816e-09  1.98857197e-11  2.64776255e-09  2.66790696e-09
  2.19912699e-09  1.69617582e-11  2.25839369e-09  2.27557583e-09
  1.39740926e-09  1.07588004e-11  1.43504040e-09  1.44594623e-09
  2.05766855e-10  2.03508962e-10  7.96695681e-11  1.98004724e-10
 -1.57322230e-11 -1.55597869e-11 -6.01140797e-12 -1.50717048e-11
  1.11875582e-10  1.10644025e-10  4.30940697e-11  1.07369238e-10
  2.73999512e-11  2.70995215e-11  1.04272346e-11  2.62011570e-11
  2.73176348e-11  1.81599488e-11  2.49223678e-11  1.40404715e-11
  2.78085893e-11  1.72841683e-11  2.49147845e-11  1.43838520e-11
  4.23866322e-11  1.42315463e-11  3.37709658e-11  2.38994437e-11
  1.71872442e-11  1.21634729e-11  1.59613229e-11  8.78329558e-12
  2.09668530e-11 -8.30252037e-12  7.25607075e-12 -9.50728374e-12
  2.48397184e-11 -3.82735903e-12  1.20717407e-11  3.97085174e-12
 -1.82159350e+00]
supnorm grad right now is: 1.8215935025940524
Weights right now are: 
[-4.14902499 -4.71371744 -3.63268839 -0.82193979  4.36861536  3.48214793
  5.0340726   0.80212566 -1.44338143 -0.835636   -2.48468985 -2.37486742
 -1.26224743  0.51376341 -2.89492497 -1.13643847 -1.69567716  0.43373631
 -1.40247244 -2.4159311   1.62351554  0.58021521  1.37995092  1.25374287
 -1.55281449 -1.32184218 -0.57748203 -0.49195028  1.44681101  1.7138813
  1.46934368  1.70992117 -1.922889   -1.80285168 -0.65433985 -1.0081032
 -2.01538379 -1.72346105 -1.51575707 -2.02122728 -1.62886349 -1.1231005
 -1.523013   -0.92983991 -1.51495806 -1.12347103 -1.45309007 -0.82539194
  0.26234111  1.46249711  0.07453887  0.15670399 -1.67081045 -0.83126789
 -0.47020228 -1.61464293 -0.53959246 -1.50047903  1.92095217  1.29014616
  0.17760294 -0.28372133  1.9051955  -0.53599111 24.86414898]
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.681713552205
gradient value of function right now is: [ 5.57293083e-08  5.55366048e-08  5.55954767e-08  8.39674036e-10
 -5.57286273e-08 -5.55359262e-08 -5.55947974e-08 -8.39663608e-10
  9.08379427e-10  6.99012327e-12  9.32831482e-10  9.39917978e-10
  2.38842052e-09  1.83901308e-11  2.45272911e-09  2.47136866e-09
  2.03731721e-09  1.56870533e-11  2.09217264e-09  2.10807220e-09
  1.29410253e-09  9.94646209e-12  1.32891960e-09  1.33900754e-09
  1.90614761e-10  1.88524161e-10  7.38523988e-11  1.83449866e-10
 -1.45464682e-11 -1.43871101e-11 -5.56228230e-12 -1.39376126e-11
  1.03570185e-10  1.02430609e-10  3.99220370e-11  9.94119820e-11
  2.53549784e-11  2.50771160e-11  9.65581260e-12  2.42488046e-11
  2.53040072e-11  1.67263429e-11  2.30333350e-11  1.29859445e-11
  2.57654975e-11  1.59177003e-11  2.30331680e-11  1.33093967e-11
  3.93606106e-11  1.30985555e-11  3.13116010e-11  2.21928325e-11
  1.59146831e-11  1.12011585e-11  1.47451318e-11  8.11864842e-12
  1.93645027e-11 -7.51710646e-12  6.68258204e-12 -8.70759137e-12
  2.29629324e-11 -3.40112470e-12  1.11595947e-11  3.80322520e-12
  3.79171559e+00]
supnorm grad right now is: 3.7917155865653323
Weights right now are: 
[-4.16061246 -4.72526479 -3.644248   -0.82211432  4.38020286  3.49369531
  5.04563224  0.80230018 -1.44357009 -0.83563745 -2.48488359 -2.37506263
 -1.2627437   0.51375958 -2.89543461 -1.13695199 -1.69610046  0.43373305
 -1.40290715 -2.41636912  1.6232466   0.58021314  1.37967475  1.25346459
 -1.5528541  -1.32188136 -0.57749736 -0.4919884   1.44681403  1.71388429
  1.46934484  1.70992407 -1.92291053 -1.80287298 -0.65434814 -1.00812387
 -2.01538906 -1.72346626 -1.51575907 -2.02123232 -1.62886874 -1.123104
 -1.5230178  -0.92984261 -1.51496341 -1.12347436 -1.45309486 -0.82539471
  0.26233295  1.46249437  0.07453238  0.15669939 -1.67081375 -0.83127023
 -0.47020535 -1.61464462 -0.53959647 -1.50047745  1.92095079  1.29014798
  0.17759817 -0.2837206   1.90519318 -0.53599188 25.26509055]
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.64650505818
gradient value of function right now is: [ 5.22431769e-08  5.20628682e-08  5.21179344e-08  7.87725126e-10
 -5.22414286e-08 -5.20611259e-08 -5.21161903e-08 -7.87698510e-10
  8.53240624e-10  6.56540894e-12  8.76205085e-10  8.82860683e-10
  2.24174455e-09  1.72597087e-11  2.30209510e-09  2.31958803e-09
  1.91231205e-09  1.47236186e-11  1.96379418e-09  1.97871652e-09
  1.21426139e-09  9.33220101e-12  1.24692560e-09  1.25639008e-09
  1.79020222e-10  1.77056838e-10  6.93610848e-11  1.72291762e-10
 -1.36474039e-11 -1.34978995e-11 -5.21856015e-12 -1.30762275e-11
  9.72470923e-11  9.61771170e-11  3.74852251e-11  9.33430952e-11
  2.38105021e-11  2.35495733e-11  9.06773552e-12  2.27717914e-11
  2.37617505e-11  1.57052833e-11  2.16283740e-11  1.21943330e-11
  2.41949833e-11  1.49457308e-11  2.16280950e-11  1.24980286e-11
  3.69630966e-11  1.22981787e-11  2.94031955e-11  2.08412329e-11
  1.49438457e-11  1.05168601e-11  1.38449196e-11  7.62328000e-12
  1.81816754e-11 -7.05373578e-12  6.27379033e-12 -8.17428057e-12
  2.15615243e-11 -3.19057072e-12  1.04784954e-11  3.57390583e-12
  3.90755190e+00]
supnorm grad right now is: 3.9075519038540003
Weights right now are: 
[-4.17138966 -4.73600476 -3.65499934 -0.82227677  4.39098007  3.50443529
  5.05638359  0.80246263 -1.44374593 -0.83563881 -2.48506417 -2.37524458
 -1.26320588  0.51375602 -2.89590924 -1.13743023 -1.69649471  0.43373001
 -1.40331203 -2.41677708  1.62299623  0.58021121  1.37941762  1.25320552
 -1.55289103 -1.32191788 -0.57751166 -0.49202394  1.44681685  1.71388708
  1.46934592  1.70992678 -1.9229306  -1.80289283 -0.65435588 -1.00814314
 -2.01539398 -1.72347113 -1.51576094 -2.02123702 -1.62887365 -1.12310726
 -1.52302227 -0.92984513 -1.51496841 -1.12347746 -1.45309933 -0.82539729
  0.26232534  1.46249181  0.07452633  0.15669512 -1.67081684 -0.83127242
 -0.47020822 -1.61464619 -0.53960023 -1.50047599  1.92094949  1.29014967
  0.17759371 -0.2837199   1.90519101 -0.5359926  25.21292822]
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.041450192788
gradient value of function right now is: [ 4.92811646e-08  4.91113786e-08  4.91632140e-08  7.43602476e-10
 -4.92799082e-08 -4.91101265e-08 -4.91619606e-08 -7.43583369e-10
  8.06403673e-10  6.20826927e-12  8.28110668e-10  8.34402679e-10
  2.11715789e-09  1.63090447e-11  2.17416263e-09  2.19068800e-09
  1.80614197e-09  1.39134757e-11  1.85477287e-09  1.86887068e-09
  1.14644887e-09  8.81564769e-12  1.17729331e-09  1.18623173e-09
  1.69213461e-10  1.67357195e-10  6.55484611e-11  1.62846244e-10
 -1.28903892e-11 -1.27491414e-11 -4.92809398e-12 -1.23503909e-11
  9.19120304e-11  9.09005109e-11  3.54218036e-11  8.82184315e-11
  2.25118808e-11  2.22651167e-11  8.57151117e-12  2.15289942e-11
  2.24600909e-11  1.48666999e-11  2.04571600e-11  1.15314408e-11
  2.28669382e-11  1.41483385e-11  2.04544987e-11  1.18165991e-11
  3.49037925e-11  1.16463280e-11  2.77789814e-11  1.96796223e-11
  1.41260648e-11  9.95509298e-12  1.30961643e-11  7.20988786e-12
  1.72046864e-11 -6.71657038e-12  5.94074308e-12 -7.75164471e-12
  2.03915333e-11 -3.04845256e-12  9.91004765e-12  3.34142422e-12
  1.89214207e+00]
supnorm grad right now is: 1.8921420725104412
Weights right now are: 
[-4.18166534 -4.746245   -3.66525041 -0.82243176  4.40125574  3.51467552
  5.06663464  0.80261762 -1.4439139  -0.8356401  -2.48523666 -2.37541839
 -1.26364705  0.51375262 -2.8963623  -1.13788674 -1.69687107  0.4337271
 -1.40369852 -2.41716651  1.62275729  0.58020937  1.37917226  1.25295829
 -1.55292629 -1.32195275 -0.57752532 -0.49205787  1.44681954  1.71388974
  1.46934694  1.70992935 -1.92294976 -1.80291177 -0.65436326 -1.00816152
 -2.01539867 -1.72347577 -1.51576273 -2.02124151 -1.62887834 -1.12311036
 -1.52302654 -0.92984753 -1.51497318 -1.12348041 -1.45310359 -0.82539976
  0.26231806  1.46248939  0.07452054  0.15669102 -1.67081979 -0.83127449
 -0.47021095 -1.6146477  -0.53960382 -1.50047459  1.92094825  1.2901513
  0.17758946 -0.28371927  1.90518894 -0.53599329 25.13698159]
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.150675150473
gradient value of function right now is: [ 4.66799629e-08  4.65194046e-08  4.65684073e-08  7.04846090e-10
 -4.66787682e-08 -4.65182141e-08 -4.65672155e-08 -7.04827864e-10
  7.65222057e-10  5.89604298e-12  7.85826056e-10  7.91799414e-10
  2.00766646e-09  1.54782712e-11  2.06173771e-09  2.07741546e-09
  1.71283161e-09  1.32054699e-11  1.75896253e-09  1.77233799e-09
  1.08686634e-09  8.36431196e-12  1.11611560e-09  1.12459325e-09
  1.60608673e-10  1.58846528e-10  6.21953108e-11  1.54554258e-10
 -1.22283671e-11 -1.20943538e-11 -4.67335767e-12 -1.17152710e-11
  8.72379230e-11  8.62777064e-11  3.36093337e-11  8.37262357e-11
  2.13767137e-11  2.11423590e-11  8.13642092e-12  2.04419666e-11
  2.13133345e-11  1.41518069e-11  1.94352026e-11  1.09505173e-11
  2.16964477e-11  1.34681439e-11  1.94294544e-11  1.12188027e-11
  3.30832909e-11  1.10866319e-11  2.63496920e-11  1.86533053e-11
  1.34061136e-11  9.47671519e-12  1.24436892e-11  6.84817667e-12
  1.63484762e-11 -6.44479553e-12  5.65401126e-12 -7.39912095e-12
  1.93723202e-11 -2.96220261e-12  9.41375984e-12  3.11986944e-12
 -7.01377654e-01]
supnorm grad right now is: 0.7013776537424158
Weights right now are: 
[-4.19139604 -4.7559422  -3.67495783 -0.82257862  4.41098644  3.52437272
  5.07634207  0.80276449 -1.44407325 -0.83564133 -2.4854003  -2.37558327
 -1.26406528  0.5137494  -2.89679179 -1.13831948 -1.69722787  0.43372436
 -1.40406492 -2.41753569  1.62253084  0.58020763  1.37893972  1.25272398
 -1.5529597  -1.3219858  -0.57753828 -0.49209003  1.44682208  1.71389225
  1.46934792  1.70993179 -1.9229679  -1.80292972 -0.65437026 -1.00817894
 -2.01540311 -1.72348016 -1.51576442 -2.02124576 -1.62888277 -1.12311328
 -1.52303057 -0.92984981 -1.51497769 -1.12348319 -1.45310763 -0.82540209
  0.26231116  1.4624871   0.07451504  0.15668712 -1.67082257 -0.83127645
 -0.47021353 -1.61464912 -0.53960722 -1.50047326  1.92094708  1.29015283
  0.17758543 -0.28371869  1.90518699 -0.53599396 24.9108331 ]
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.542769087813
gradient value of function right now is: [ 4.38739891e-08  4.37233359e-08  4.37693024e-08  6.62847482e-10
 -4.38752363e-08 -4.37245787e-08 -4.37705465e-08 -6.62866548e-10
  7.20408524e-10  5.54267644e-12  7.39790742e-10  7.45408740e-10
  1.88885386e-09  1.45410995e-11  1.93968546e-09  1.95442088e-09
  1.61156508e-09  1.24066782e-11  1.65493476e-09  1.66750705e-09
  1.02233053e-09  7.85617944e-12  1.04982151e-09  1.05778790e-09
  1.51095802e-10  1.49438751e-10  5.85445445e-11  1.45417488e-10
 -1.14867249e-11 -1.13608971e-11 -4.39254904e-12 -1.10060217e-11
  8.20267922e-11  8.11243249e-11  3.16197867e-11  7.87341379e-11
  2.00922811e-11  1.98721108e-11  7.65206551e-12  1.92158392e-11
  2.00492052e-11  1.32491057e-11  1.82473713e-11  1.02880796e-11
  2.04142666e-11  1.26076662e-11  1.82466675e-11  1.05441398e-11
  3.11887890e-11  1.03727111e-11  2.48077271e-11  1.75849068e-11
  1.26071666e-11  8.87086535e-12  1.16788903e-11  6.43055659e-12
  1.53388849e-11 -5.94421251e-12  5.29227120e-12 -6.89334556e-12
  1.81916608e-11 -2.69008838e-12  8.84015018e-12  3.01971042e-12
  4.28681318e+00]
supnorm grad right now is: 4.286813182879206
Weights right now are: 
[-4.20050352 -4.76501838 -3.68404357 -0.82271616  4.42009394  3.53344892
  5.08542782  0.80290203 -1.44422265 -0.83564247 -2.48555372 -2.37573785
 -1.26445714  0.51374639 -2.89719419 -1.13872494 -1.69756219  0.43372179
 -1.40440824 -2.41788161  1.62231872  0.580206    1.3787219   1.25250451
 -1.55299103 -1.32201679 -0.57755043 -0.49212018  1.44682446  1.71389461
  1.46934883  1.70993407 -1.92298491 -1.80294654 -0.65437682 -1.00819526
 -2.01540728 -1.72348428 -1.51576601 -2.02124974 -1.62888693 -1.123116
 -1.52303436 -0.92985194 -1.51498193 -1.12348578 -1.45311142 -0.82540427
  0.26230468  1.46248496  0.07450989  0.15668347 -1.67082519 -0.83127827
 -0.47021595 -1.61465045 -0.53961043 -1.50047201  1.92094597  1.29015426
  0.17758164 -0.28371819  1.90518515 -0.53599459 25.29065238]
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.161919162278
gradient value of function right now is: [ 4.19293888e-08  4.17856194e-08  4.18294732e-08  6.33869519e-10
 -4.19277007e-08 -4.17839371e-08 -4.18277891e-08 -6.33843754e-10
  6.89550242e-10  5.31102637e-12  7.08110009e-10  7.13490789e-10
  1.80687221e-09  1.39251117e-11  1.85551799e-09  1.86962285e-09
  1.54168059e-09  1.18815642e-11  1.58318694e-09  1.59522169e-09
  9.77730066e-10  7.52161495e-12  1.00403273e-09  1.01165643e-09
  1.44675602e-10  1.43088429e-10  5.60325717e-11  1.39225827e-10
 -1.09954093e-11 -1.08749203e-11 -4.20274575e-12 -1.05343522e-11
  7.85484620e-11  7.76839727e-11  3.02656679e-11  7.53887328e-11
  1.92502033e-11  1.90391827e-11  7.32805169e-12  1.84089629e-11
  1.91955189e-11  1.27309317e-11  1.74961326e-11  9.86005883e-12
  1.95411681e-11  1.21154356e-11  1.74916633e-11  1.01022881e-11
  2.98090410e-11  9.97217393e-12  2.37348882e-11  1.68079673e-11
  1.20723438e-11  8.52423769e-12  1.12004208e-11  6.16507366e-12
  1.47149918e-11 -5.77795248e-12  5.08598275e-12 -6.64705585e-12
  1.74380209e-11 -2.64487439e-12  8.47444296e-12  2.82730442e-12
  4.53525964e-01]
supnorm grad right now is: 0.4535259635422207
Weights right now are: 
[-4.20905485 -4.77354036 -3.6925745  -0.82284541  4.42864529  3.54197092
  5.09395877  0.80303128 -1.44436318 -0.83564356 -2.48569803 -2.37588326
 -1.26482549  0.51374354 -2.89757247 -1.1391061  -1.69787648  0.43371936
 -1.40473099 -2.41820683  1.62211938  0.58020446  1.37851719  1.25229824
 -1.55302055 -1.32204598 -0.57756185 -0.49214858  1.44682671  1.71389683
  1.46934969  1.70993622 -1.92300094 -1.8029624  -0.654383   -1.00821065
 -2.01541121 -1.72348817 -1.51576751 -2.0212535  -1.62889085 -1.12311863
 -1.52303794 -0.92985395 -1.51498592 -1.12348828 -1.45311499 -0.82540633
  0.26229862  1.46248291  0.07450506  0.15668006 -1.67082766 -0.83128003
 -0.47021825 -1.61465171 -0.53961345 -1.50047081  1.92094493  1.29015563
  0.17757807 -0.2837176   1.90518341 -0.53599514 25.04092296]
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.160054373888
gradient value of function right now is: [ 3.99126973e-08  3.97760409e-08  3.98177140e-08  6.03721414e-10
 -3.99115531e-08 -3.97749006e-08 -3.98165725e-08 -6.03703951e-10
  6.57370065e-10  5.06298246e-12  6.75061814e-10  6.80191100e-10
  1.72154188e-09  1.32670120e-11  1.76788548e-09  1.78132323e-09
  1.46894864e-09  1.13206179e-11  1.50849269e-09  1.51995883e-09
  9.31370440e-10  7.16469899e-12  9.56423290e-10  9.63684962e-10
  1.37908634e-10  1.36395699e-10  5.34126727e-11  1.32713962e-10
 -1.04729861e-11 -1.03582217e-11 -4.00313917e-12 -1.00338539e-11
  7.48614603e-11  7.40375490e-11  2.88455521e-11  7.18501654e-11
  1.83487229e-11  1.81475837e-11  6.98502142e-12  1.75469103e-11
  1.82964894e-11  1.21330867e-11  1.66760534e-11  9.39791875e-12
  1.86257935e-11  1.15463744e-11  1.66716776e-11  9.62874123e-12
  2.84127301e-11  9.50372731e-12  2.26225970e-11  1.60204587e-11
  1.15064975e-11  8.12359800e-12  1.06749977e-11  5.87587408e-12
  1.40256561e-11 -5.50582486e-12  4.84749145e-12 -6.33457278e-12
  1.66208946e-11 -2.51852191e-12  8.07721857e-12  2.69607761e-12
  5.12249862e-01]
supnorm grad right now is: 0.5122498617700939
Weights right now are: 
[-4.21728739 -4.78174469 -3.70078743 -0.8229699   4.43687781  3.55017523
  5.10217169  0.80315576 -1.44449866 -0.8356446  -2.48583716 -2.37602344
 -1.26518041  0.51374081 -2.89793694 -1.13947334 -1.69817931  0.43371703
 -1.40504198 -2.41852017  1.62192735  0.58020299  1.37831999  1.25209955
 -1.55304897 -1.32207409 -0.57757286 -0.49217593  1.44682887  1.71389897
  1.46935051  1.70993829 -1.92301637 -1.80297766 -0.65438894 -1.00822546
 -2.01541499 -1.72349191 -1.51576895 -2.02125712 -1.62889462 -1.12312113
 -1.52304137 -0.92985589 -1.51498975 -1.12349066 -1.45311843 -0.82540831
  0.26229277  1.46248095  0.07450039  0.15667675 -1.67083003 -0.83128171
 -0.47022045 -1.61465292 -0.53961633 -1.50046968  1.92094393  1.29015693
  0.17757465 -0.28371705  1.90518175 -0.53599569 24.99453344]
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.00064163327
gradient value of function right now is: [ 3.82343746e-08  3.81036395e-08  3.81434965e-08  5.78668485e-10
 -3.82335160e-08 -3.81027839e-08 -3.81426401e-08 -5.78655398e-10
  6.30646135e-10  4.86120762e-12  6.47623807e-10  6.52546925e-10
  1.65063873e-09  1.27312095e-11  1.69508710e-09  1.70797757e-09
  1.40851736e-09  1.08639524e-11  1.44644612e-09  1.45744581e-09
  8.92833752e-10  6.87397290e-12  9.16857272e-10  9.23821805e-10
  1.32335672e-10  1.30883548e-10  5.12376150e-11  1.27341849e-10
 -1.00463059e-11 -9.93619154e-12 -3.83872529e-12 -9.62440117e-12
  7.18393197e-11  7.10484961e-11  2.76719241e-11  6.89447703e-11
  1.76154290e-11  1.74222825e-11  6.70358266e-12  1.68445352e-11
  1.75551219e-11  1.16750688e-11  1.60186271e-11  9.02373793e-12
  1.78684571e-11  1.11109421e-11  1.60117492e-11  9.24319105e-12
  2.72265591e-11  9.14754480e-12  2.16950435e-11  1.53516280e-11
  1.10416025e-11  7.81707812e-12  1.02557427e-11  5.64331194e-12
  1.34766003e-11 -5.34415419e-12  4.66548897e-12 -6.11382186e-12
  1.59647293e-11 -2.46776568e-12  7.75780906e-12  2.54473550e-12
 -2.16905124e+00]
supnorm grad right now is: 2.169051238029491
Weights right now are: 
[-4.22517556 -4.78960587 -3.70865684 -0.82308923  4.44476599  3.55803642
  5.11004111  0.80327509 -1.44462865 -0.8356456  -2.48597064 -2.37615794
 -1.26552074  0.51373819 -2.89828642 -1.13982547 -1.69846971  0.4337148
 -1.40534019 -2.41882065  1.62174323  0.58020158  1.37813093  1.25190905
 -1.5530762  -1.32210102 -0.57758341 -0.49220214  1.44683094  1.71390101
  1.4693513   1.70994027 -1.92303114 -1.80299226 -0.65439463 -1.00823964
 -2.01541861 -1.72349548 -1.51577032 -2.02126058 -1.62889823 -1.1231235
 -1.52304465 -0.92985774 -1.51499343 -1.12349292 -1.45312171 -0.82541022
  0.26228713  1.46247909  0.07449591  0.15667357 -1.6708323  -0.83128329
 -0.47022255 -1.61465408 -0.53961908 -1.50046862  1.92094298  1.29015816
  0.17757138 -0.28371663  1.90518016 -0.53599625 24.85640118]
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.156218092017
gradient value of function right now is: [ 3.63949851e-08  3.62707107e-08  3.63085890e-08  5.51093636e-10
 -3.63964944e-08 -3.62722147e-08 -3.63100946e-08 -5.51116714e-10
  6.01127530e-10  4.62972451e-12  6.17302769e-10  6.21992690e-10
  1.57252166e-09  1.21183815e-11  1.61484619e-09  1.62711939e-09
  1.34193272e-09  1.03415871e-11  1.37805117e-09  1.38852472e-09
  8.50445283e-10  6.54203215e-12  8.73317262e-10  8.79947235e-10
  1.26085243e-10  1.24702037e-10  4.88340195e-11  1.21335734e-10
 -9.56149432e-12 -9.45672018e-12 -3.65477267e-12 -9.16056594e-12
  6.84218963e-11  6.76688701e-11  2.63645586e-11  6.56695274e-11
  1.67743789e-11  1.65905013e-11  6.38576037e-12  1.60413325e-11
  1.67253481e-11  1.10916517e-11  1.52440293e-11  8.59054729e-12
  1.70260335e-11  1.05550164e-11  1.52397107e-11  8.80138397e-12
  2.59716173e-11  8.68703358e-12  2.06788713e-11  1.46438082e-11
  1.05177378e-11  7.42580720e-12  9.75765065e-12  5.37071766e-12
  1.28217034e-11 -5.03194338e-12  4.43125888e-12 -5.79026759e-12
  1.51943447e-11 -2.30435979e-12  7.38345147e-12  2.46415602e-12
  5.78888300e-01]
supnorm grad right now is: 0.578888300334128
Weights right now are: 
[-4.23270493 -4.79710952 -3.71616833 -0.82320319  4.45229536  3.56554007
  5.1175526   0.80338906 -1.44475291 -0.83564656 -2.48609824 -2.3762865
 -1.26584588  0.51373569 -2.8986203  -1.14016189 -1.69874717  0.43371266
 -1.4056251  -2.41910773  1.62156737  0.58020023  1.37795034  1.25172709
 -1.55310224 -1.32212678 -0.5775935  -0.49222721  1.44683291  1.71390296
  1.46935206  1.70994216 -1.92304527 -1.80300623 -0.65440008 -1.0082532
 -2.01542207 -1.72349891 -1.51577164 -2.02126389 -1.62890168 -1.12312578
 -1.52304779 -0.92985952 -1.51499694 -1.12349508 -1.45312485 -0.82541203
  0.26228176  1.46247731  0.07449164  0.15667053 -1.67083447 -0.83128482
 -0.47022455 -1.61465519 -0.53962171 -1.5004676   1.92094208  1.29015935
  0.17756827 -0.28371618  1.90517865 -0.53599677 25.04360901]
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.007333790241
gradient value of function right now is: [ 3.47871653e-08  3.46685349e-08  3.47046846e-08  5.26995453e-10
 -3.47863423e-08 -3.46677148e-08 -3.47038636e-08 -5.26982821e-10
  5.75286509e-10  4.42828771e-12  5.90761325e-10  5.95247900e-10
  1.50415967e-09  1.15852617e-11  1.54463096e-09  1.55636608e-09
  1.28363827e-09  9.88694638e-12  1.31817634e-09  1.32819103e-09
  8.13344474e-10  6.25321536e-12  8.35211462e-10  8.41549741e-10
  1.20627532e-10  1.19304346e-10  4.67300718e-11  1.16088802e-10
 -9.13937817e-12 -9.03924011e-12 -3.49423756e-12 -8.75653953e-12
  6.54425481e-11  6.47223867e-11  2.52220607e-11  6.28128153e-11
  1.60425065e-11  1.58666703e-11  6.10857133e-12  1.53421100e-11
  1.60012955e-11  1.05904263e-11  1.45734817e-11  8.21488435e-12
  1.62901446e-11  1.00777815e-11  1.45706421e-11  8.41757331e-12
  2.48649394e-11  8.29404275e-12  1.97882662e-11  1.40195916e-11
  1.00612317e-11  7.08961330e-12  9.32705691e-12  5.13483796e-12
  1.22549997e-11 -4.78115356e-12  4.23216405e-12 -5.52117510e-12
  1.45270529e-11 -2.17045484e-12  7.05927416e-12  2.38387779e-12
  2.12519127e+00]
supnorm grad right now is: 2.1251912673147464
Weights right now are: 
[-4.23988631 -4.8042664  -3.72333268 -0.82331195  4.45947676  3.57269696
  5.12471696  0.80349782 -1.44487158 -0.83564747 -2.48622011 -2.3764093
 -1.26615626  0.5137333  -2.89893903 -1.14048304 -1.69901204  0.43371062
 -1.4058971  -2.41938178  1.62139952  0.58019894  1.37777798  1.25155343
 -1.55312712 -1.32215139 -0.57760314 -0.49225115  1.44683479  1.71390483
  1.46935278  1.70994397 -1.92305876 -1.80301958 -0.65440528 -1.00826615
 -2.01542538 -1.72350218 -1.5157729  -2.02126705 -1.62890498 -1.12312795
 -1.5230508  -0.92986121 -1.5150003  -1.12349716 -1.45312785 -0.82541377
  0.26227662  1.4624756   0.07448755  0.15666763 -1.67083654 -0.83128627
 -0.47022648 -1.61465624 -0.53962424 -1.50046663  1.92094121  1.29016048
  0.17756527 -0.28371574  1.90517719 -0.53599727 25.11245128]
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.779206067782
gradient value of function right now is: [ 3.33444534e-08  3.32308808e-08  3.32654817e-08  5.05367475e-10
 -3.33438876e-08 -3.32303170e-08 -3.32649174e-08 -5.05358709e-10
  5.52095854e-10  4.24844598e-12  5.66943498e-10  5.71248137e-10
  1.44282664e-09  1.11093998e-11  1.48163893e-09  1.49289274e-09
  1.23134964e-09  9.48124290e-12  1.26447333e-09  1.27407768e-09
  7.80067187e-10  5.99548079e-12  8.01034747e-10  8.07112175e-10
  1.15739264e-10  1.14469884e-10  4.48417717e-11  1.11387181e-10
 -8.76240617e-12 -8.66641370e-12 -3.35051665e-12 -8.39554442e-12
  6.27775111e-11  6.20867799e-11  2.41978011e-11  6.02562659e-11
  1.53891997e-11  1.52205518e-11  5.86049530e-12  1.47176300e-11
  1.53507596e-11  1.01518056e-11  1.39759711e-11  7.87894574e-12
  1.56286414e-11  9.65995970e-12  1.39739271e-11  8.07399380e-12
  2.38665841e-11  7.94811271e-12  1.89884819e-11  1.34569102e-11
  9.65134593e-12  6.79568674e-12  8.94378987e-12  4.92420800e-12
  1.17498369e-11 -4.56823969e-12  4.05630733e-12 -5.28715407e-12
  1.39322648e-11 -2.07055672e-12  6.77004280e-12  2.30000819e-12
  3.35407683e+00]
supnorm grad right now is: 3.3540768339125306
Weights right now are: 
[-4.24665944 -4.81101643 -3.73008975 -0.82341459  4.46624988  3.579447
  5.13147403  0.80360046 -1.44498366 -0.83564833 -2.48633521 -2.37652527
 -1.26644925  0.51373104 -2.89923991 -1.14078621 -1.69926208  0.43370869
 -1.40615387 -2.41964051  1.62124111  0.58019772  1.3776153   1.25138952
 -1.55315065 -1.32217465 -0.57761225 -0.49227379  1.44683658  1.71390659
  1.46935346  1.70994567 -1.92307153 -1.80303221 -0.6544102  -1.0082784
 -2.01542851 -1.72350528 -1.51577409 -2.02127004 -1.62890811 -1.12313003
 -1.52305365 -0.92986281 -1.51500348 -1.12349913 -1.4531307  -0.82541541
  0.26227179  1.46247397  0.0744837   0.15666491 -1.67083851 -0.83128766
 -0.4702283  -1.61465725 -0.53962665 -1.50046568  1.92094037  1.29016157
  0.17756241 -0.28371529  1.90517581 -0.53599773 25.21607087]
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16003.121082586928
gradient value of function right now is: [ 3.18388874e-08  3.17305768e-08  3.17635678e-08  4.82743730e-10
 -3.18400607e-08 -3.17317461e-08 -3.17647383e-08 -4.82761639e-10
  5.27788146e-10  4.05673164e-12  5.41973295e-10  5.46085198e-10
  1.37863936e-09  1.06029956e-11  1.41570206e-09  1.42644682e-09
  1.17662776e-09  9.04950892e-12  1.20825986e-09  1.21743024e-09
  7.45272652e-10  5.72145836e-12  7.65292548e-10  7.71094355e-10
  1.10580169e-10  1.09367881e-10  4.28630806e-11  1.06431857e-10
 -8.36246053e-12 -8.27089191e-12 -3.19910929e-12 -8.01302970e-12
  5.99549610e-11  5.92955665e-11  2.31206432e-11  5.75521815e-11
  1.46929240e-11  1.45319837e-11  5.59792980e-12  1.40528828e-11
  1.46649607e-11  9.66464383e-12  1.33311806e-11  7.51827273e-12
  1.49333072e-11  9.19521062e-12  1.33319896e-11  7.70682664e-12
  2.28423592e-11  7.55935997e-12  1.81534387e-11  1.28781087e-11
  9.21793901e-12  6.46916485e-12  8.52875845e-12  4.69684011e-12
  1.12025835e-11 -4.29059356e-12  3.85797373e-12 -5.00863171e-12
  1.32934037e-11 -1.93202642e-12  6.45920503e-12  2.24405632e-12
  7.85982424e+00]
supnorm grad right now is: 7.859824235417216
Weights right now are: 
[-4.25326722 -4.81760172 -3.73668188 -0.82351476  4.47285769  3.58603231
  5.13806619  0.80370062 -1.44509313 -0.83564918 -2.48644761 -2.37663853
 -1.26673526  0.51372884 -2.89953362 -1.14108214 -1.69950617  0.43370681
 -1.40640454 -2.41989308  1.62108648  0.58019653  1.37745652  1.25122953
 -1.55317359 -1.32219735 -0.57762114 -0.49229587  1.44683831  1.71390831
  1.46935412  1.70994734 -1.92308398 -1.80304452 -0.654415   -1.00829035
 -2.01543156 -1.72350829 -1.51577525 -2.02127296 -1.62891115 -1.12313204
 -1.52305642 -0.92986438 -1.51500658 -1.12350105 -1.45313347 -0.82541701
  0.26226706  1.4624724   0.07447993  0.15666224 -1.67084042 -0.83128901
 -0.47023008 -1.61465822 -0.53962898 -1.50046476  1.92093957  1.29016263
  0.17755966 -0.28371489  1.90517446 -0.53599818 25.53157607]
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.035368768937
gradient value of function right now is: [ 3.09367011e-08  3.08315672e-08  3.08635837e-08  4.69298461e-10
 -3.09365881e-08 -3.08314546e-08 -3.08634711e-08 -4.69296618e-10
  5.13424030e-10  3.95216391e-12  5.27231954e-10  5.31235612e-10
  1.34053147e-09  1.03251642e-11  1.37659278e-09  1.38705012e-09
  1.14414260e-09  8.81266963e-12  1.17492106e-09  1.18384641e-09
  7.24559305e-10  5.57068516e-12  7.44035256e-10  7.49680984e-10
  1.07629927e-10  1.06449292e-10  4.16945827e-11  1.03579727e-10
 -8.14018025e-12 -8.05098815e-12 -3.11219137e-12 -7.79916660e-12
  5.83688726e-11  5.77265400e-11  2.24956484e-11  5.60231406e-11
  1.43129698e-11  1.41560865e-11  5.44996914e-12  1.36880340e-11
  1.42747008e-11  9.44863561e-12  1.30017141e-11  7.32872635e-12
  1.45318701e-11  8.99109078e-12  1.29987249e-11  7.50924016e-12
  2.21783761e-11  7.39974919e-12  1.76510427e-11  1.25046798e-11
  8.97497247e-12  6.32473517e-12  8.32054188e-12  4.58065890e-12
  1.09338899e-11 -4.26796851e-12  3.77601558e-12 -4.92638071e-12
  1.29599057e-11 -1.93763556e-12  6.29769005e-12  2.12389356e-12
  1.92379151e+00]
supnorm grad right now is: 1.9237915119420765
Weights right now are: 
[-4.25956195 -4.82387504 -3.74296173 -0.82361023  4.47915244  3.59230565
  5.14434605  0.80379609 -1.44519753 -0.83564998 -2.48655483 -2.37674656
 -1.26700793  0.51372674 -2.89981362 -1.14136428 -1.69973889  0.43370502
 -1.40664351 -2.42013388  1.6209391   0.58019539  1.37730517  1.25107703
 -1.55319549 -1.322219   -0.57762962 -0.49231694  1.44683997  1.71390995
  1.46935476  1.70994893 -1.92309585 -1.80305626 -0.65441957 -1.00830175
 -2.01543447 -1.72351117 -1.51577636 -2.02127575 -1.62891406 -1.12313396
 -1.52305907 -0.92986587 -1.51500954 -1.12350288 -1.45313612 -0.82541855
  0.26226255  1.46247089  0.07447635  0.15665971 -1.67084225 -0.8312903
 -0.47023177 -1.61465916 -0.5396312  -1.50046388  1.9209388   1.29016363
  0.17755702 -0.28371451  1.90517318 -0.53599861 25.13157787]
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.155154750362
gradient value of function right now is: [ 2.98863432e-08  2.97848855e-08  2.98157763e-08  4.53564140e-10
 -2.98875638e-08 -2.97861020e-08 -2.98169940e-08 -4.53582881e-10
  4.96551631e-10  3.82408651e-12  5.09907758e-10  5.13780820e-10
  1.29591215e-09  9.98619198e-12  1.33077828e-09  1.34089009e-09
  1.10611040e-09  8.52375521e-12  1.13587016e-09  1.14450101e-09
  7.00352155e-10  5.38710146e-12  7.19180182e-10  7.24638657e-10
  1.04104389e-10  1.02962346e-10  4.03213424e-11  1.00182779e-10
 -7.87091879e-12 -7.78467172e-12 -3.00862394e-12 -7.54087656e-12
  5.64565183e-11  5.58351919e-11  2.17544051e-11  5.41854382e-11
  1.38478101e-11  1.36960163e-11  5.27175068e-12  1.32426501e-11
  1.38050823e-11  9.15501010e-12  1.25823796e-11  7.09057427e-12
  1.40527023e-11  8.71168023e-12  1.25782950e-11  7.26430818e-12
  2.14348947e-11  7.16941051e-12  1.70666177e-11  1.20858155e-11
  8.68015300e-12  6.12832164e-12  8.05281382e-12  4.43234291e-12
  1.05827154e-11 -4.15295916e-12  3.65763737e-12 -4.77909052e-12
  1.25412814e-11 -1.90199580e-12  6.09397448e-12  2.03404068e-12
  5.94577110e-01]
supnorm grad right now is: 0.5945771099957133
Weights right now are: 
[-4.26566667 -4.82995903 -3.74905203 -0.82370285  4.48525716  3.59838964
  5.15043636  0.80388872 -1.4452989  -0.83565076 -2.48665892 -2.37685145
 -1.26727254  0.5137247  -2.90008535 -1.14163808 -1.69996474  0.43370328
 -1.40687544 -2.42036757  1.62079608  0.58019429  1.37715832  1.25092906
 -1.55321674 -1.32224002 -0.57763785 -0.4923374   1.44684158  1.71391154
  1.46935537  1.70995047 -1.92310738 -1.80306767 -0.65442401 -1.00831281
 -2.0154373  -1.72351397 -1.51577744 -2.02127845 -1.62891688 -1.12313583
 -1.52306164 -0.92986732 -1.51501241 -1.12350466 -1.45313869 -0.82542003
  0.26225818  1.46246942  0.07447287  0.15665724 -1.67084402 -0.83129155
 -0.47023342 -1.61466006 -0.53963336 -1.50046304  1.92093806  1.2901646
  0.17755446 -0.28371412  1.90517194 -0.53599902 25.06682429]
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.133190842422
gradient value of function right now is: [ 2.89286678e-08  2.88305606e-08  2.88604255e-08  4.39215780e-10
 -2.89282596e-08 -2.88301538e-08 -2.88600183e-08 -4.39209541e-10
  4.81135762e-10  3.70698163e-12  4.94079025e-10  4.97832720e-10
  1.25516253e-09  9.67641174e-12  1.28893700e-09  1.29873314e-09
  1.07136017e-09  8.25956879e-12  1.10018896e-09  1.10855060e-09
  6.78243213e-10  5.21931496e-12  6.96479386e-10  7.01766778e-10
  1.00882727e-10  9.97759080e-11  3.90662592e-11  9.70788572e-11
 -7.62488271e-12 -7.54132238e-12 -2.91399861e-12 -7.30489431e-12
  5.47089580e-11  5.41068020e-11  2.10770143e-11  5.25062599e-11
  1.34224925e-11  1.32753444e-11  5.10882367e-12  1.28354729e-11
  1.33763901e-11  8.88489341e-12  1.21993589e-11  6.87352030e-12
  1.36152706e-11  8.45482260e-12  1.21943022e-11  7.04102773e-12
  2.07554961e-11  6.95900763e-12  1.65326400e-11  1.17030139e-11
  8.41110291e-12  5.94755978e-12  7.80830257e-12  4.29722524e-12
  1.02608667e-11 -4.04989251e-12  3.54972374e-12 -4.64621479e-12
  1.21585609e-11 -1.86293844e-12  5.90789333e-12  1.95419466e-12
 -1.00057667e+00]
supnorm grad right now is: 1.0005766722010043
Weights right now are: 
[-4.27153286 -4.83580531 -3.75490438 -0.8237919   4.49112333  3.6042359
  5.15628868  0.80397777 -1.44539641 -0.83565151 -2.48675906 -2.37695235
 -1.26752699  0.51372273 -2.90034665 -1.14190136 -1.70018192  0.4337016
 -1.40709847 -2.42059229  1.62065858  0.58019323  1.37701712  1.25078679
 -1.5532372  -1.32226026 -0.57764577 -0.49235708  1.44684313  1.71391307
  1.46935596  1.70995195 -1.92311848 -1.80307864 -0.65442829 -1.00832346
 -2.01544002 -1.72351667 -1.51577848 -2.02128106 -1.62891959 -1.12313765
 -1.52306411 -0.92986871 -1.51501517 -1.12350638 -1.45314117 -0.82542146
  0.26225397  1.462468    0.07446952  0.15665487 -1.67084573 -0.83129276
 -0.470235   -1.61466093 -0.53963545 -1.50046222  1.92093733  1.29016555
  0.17755199 -0.28371371  1.90517074 -0.53599941 24.93299883]
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.308922422972
gradient value of function right now is: [ 2.81288194e-08  2.80335129e-08  2.80625197e-08  4.27263790e-10
 -2.81269529e-08 -2.80316528e-08 -2.80606577e-08 -4.27235206e-10
  4.68312358e-10  3.61312756e-12  4.80917719e-10  4.84574338e-10
  1.22122807e-09  9.42771829e-12  1.25410788e-09  1.26364693e-09
  1.04242507e-09  8.04751363e-12  1.07049100e-09  1.07863342e-09
  6.59817881e-10  5.08448132e-12  6.77568583e-10  6.82716512e-10
  9.82408334e-11  9.71627629e-11  3.80227862e-11  9.45256002e-11
 -7.42612339e-12 -7.34472510e-12 -2.83637280e-12 -7.11364191e-12
  5.32880705e-11  5.27014384e-11  2.05182026e-11  5.11366226e-11
  1.30816201e-11  1.29381815e-11  4.97613738e-12  1.25080615e-11
  1.30221642e-11  8.69564637e-12  1.18992521e-11  6.69956324e-12
  1.32519854e-11  8.27491575e-12  1.18912581e-11  6.86046796e-12
  2.01690320e-11  6.81072859e-12  1.60852895e-11  1.13732587e-11
  8.18996746e-12  5.82142194e-12  7.61827091e-12  4.19008391e-12
  1.00114427e-11 -4.01349801e-12  3.47327292e-12 -4.56725803e-12
  1.18589168e-11 -1.88844637e-12  5.76088592e-12  1.84968151e-12
 -4.74982114e+00]
supnorm grad right now is: 4.74982113601659
Weights right now are: 
[-4.27720572 -4.84145893 -3.76056385 -0.82387806  4.49679617  3.60988951
  5.16194814  0.80406392 -1.44549081 -0.83565224 -2.486856   -2.37705003
 -1.26777321  0.51372083 -2.90059951 -1.14215614 -1.70039209  0.43369998
 -1.40731431 -2.42080977  1.62052554  0.58019221  1.3768805   1.25064913
 -1.55325701 -1.32227985 -0.57765345 -0.49237614  1.44684463  1.71391455
  1.46935654  1.70995338 -1.92312923 -1.80308927 -0.65443243 -1.00833378
 -2.01544266 -1.72351928 -1.51577948 -2.02128358 -1.62892222 -1.1231394
 -1.52306652 -0.92987006 -1.51501785 -1.12350805 -1.45314357 -0.82542284
  0.2622499   1.46246663  0.07446628  0.15665258 -1.67084738 -0.83129394
 -0.47023654 -1.61466178 -0.53963749 -1.50046141  1.92093663  1.29016647
  0.17754958 -0.28371332  1.90516957 -0.53599978 24.66381446]
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.276033371025
gradient value of function right now is: [ 2.68066059e-08  2.67158851e-08  2.67434917e-08  4.07279832e-10
 -2.68058788e-08 -2.67151605e-08 -2.67427663e-08 -4.07268683e-10
  4.46702198e-10  3.43563743e-12  4.58707745e-10  4.62188628e-10
  1.16439397e-09  8.96090527e-12  1.19569630e-09  1.20477316e-09
  9.93952049e-10  7.64935050e-12  1.02067253e-09  1.02842075e-09
  6.29073145e-10  4.83241076e-12  6.45971196e-10  6.50869394e-10
  9.35796467e-11  9.25535077e-11  3.62627507e-11  9.00641549e-11
 -7.06022809e-12 -6.98290026e-12 -2.70012330e-12 -6.76487266e-12
  5.07162116e-11  5.01583035e-11  1.95521423e-11  4.86811429e-11
  1.24373206e-11  1.23010536e-11  4.73716005e-12  1.18949711e-11
  1.24066594e-11  8.19433283e-12  1.12887225e-11  6.36546484e-12
  1.26317627e-11  7.79658729e-12  1.12875670e-11  6.52358560e-12
  1.93018617e-11  6.41249096e-12  1.53498387e-11  1.08828911e-11
  7.79852570e-12  5.48442067e-12  7.22232144e-12  3.97712738e-12
  9.48627480e-12 -3.66588475e-12  3.27174913e-12 -4.25851946e-12
  1.12530562e-11 -1.65739681e-12  5.46845859e-12  1.87536223e-12
  5.07653968e+00]
supnorm grad right now is: 5.076539680402539
Weights right now are: 
[-4.28272932 -4.84696383 -3.76607444 -0.82396197  4.50231978  3.61539442
  5.16745874  0.80414784 -1.44558282 -0.83565295 -2.48695048 -2.37714522
 -1.26801309  0.51371898 -2.90084584 -1.14240434 -1.70059685  0.4336984
 -1.40752457 -2.42102164  1.62039595  0.58019121  1.37674741  1.25051503
 -1.5532763  -1.32229893 -0.57766092 -0.4923947   1.44684608  1.71391599
  1.46935709  1.70995478 -1.92313969 -1.80309962 -0.65443646 -1.00834382
 -2.01544523 -1.72352181 -1.51578046 -2.02128603 -1.62892478 -1.1231411
 -1.52306885 -0.92987138 -1.51502046 -1.12350967 -1.4531459  -0.82542418
  0.26224593  1.4624653   0.07446311  0.15665035 -1.67084899 -0.83129507
 -0.47023803 -1.6146626  -0.53963946 -1.50046063  1.92093594  1.29016736
  0.17754725 -0.28371297  1.90516844 -0.53600016 25.33279468]
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.143292162835
gradient value of function right now is: [ 2.61305676e-08  2.60422122e-08  2.60690941e-08  3.97175556e-10
 -2.61313470e-08 -2.60429889e-08 -2.60698717e-08 -3.97187571e-10
  4.35878548e-10  3.35639023e-12  4.47599132e-10  4.50998182e-10
  1.13574725e-09  8.75086217e-12  1.16629492e-09  1.17515500e-09
  9.69540976e-10  7.47038758e-12  9.95618425e-10  1.00318194e-09
  6.13526896e-10  4.71861275e-12  6.30015689e-10  6.34796368e-10
  9.13513047e-11  9.03491871e-11  3.53837076e-11  8.79107029e-11
 -6.89252146e-12 -6.81699691e-12 -2.63478821e-12 -6.60355388e-12
  4.95176763e-11  4.89727260e-11  1.90816952e-11  4.75260588e-11
  1.21495191e-11  1.20163439e-11  4.62549364e-12  1.16186584e-11
  1.21116785e-11  8.02886825e-12  1.10374690e-11  6.22015348e-12
  1.23287445e-11  7.63983509e-12  1.10337595e-11  6.37253704e-12
  1.88064310e-11  6.28713583e-12  1.49725283e-11  1.06036040e-11
  7.61461479e-12  5.37389470e-12  7.06326708e-12  3.88777652e-12
  9.28326210e-12 -3.63905109e-12  3.20818119e-12 -4.19032968e-12
  1.10018196e-11 -1.66380491e-12  5.34570605e-12  1.78828778e-12
  8.33740446e-01]
supnorm grad right now is: 0.8337404456907255
Weights right now are: 
[-4.28807646 -4.85229288 -3.77140899 -0.82404323  4.50766692  3.62072346
  5.17279329  0.80422909 -1.44567196 -0.83565364 -2.48704202 -2.37723746
 -1.26824542  0.51371719 -2.90108442 -1.14264474 -1.70079518  0.43369687
 -1.40772824 -2.42122685  1.62027043  0.58019024  1.37661853  1.25038517
 -1.55329499 -1.32231741 -0.57766815 -0.49241268  1.44684749  1.71391738
  1.46935763  1.70995613 -1.92314982 -1.80310964 -0.65444037 -1.00835354
 -2.01544772 -1.72352427 -1.5157814  -2.02128841 -1.62892726 -1.12314274
 -1.52307111 -0.92987265 -1.51502297 -1.12351123 -1.45314816 -0.82542549
  0.26224209  1.46246401  0.07446005  0.15664818 -1.67085055 -0.83129617
 -0.47023948 -1.6146634  -0.53964136 -1.50045988  1.92093529  1.29016822
  0.177545   -0.28371262  1.90516735 -0.53600051 25.04433814]
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.168054979786
gradient value of function right now is: [ 2.57695490e-08  2.56824528e-08  2.57089494e-08  3.91761735e-10
 -2.57680143e-08 -2.56809233e-08 -2.57074183e-08 -3.91738142e-10
  4.30035339e-10  3.31217942e-12  4.41599743e-10  4.44953647e-10
  1.12032581e-09  8.63409683e-12  1.15046118e-09  1.15920207e-09
  9.56379396e-10  7.37071238e-12  9.82104919e-10  9.89566687e-10
  6.05161905e-10  4.65538083e-12  6.21427211e-10  6.26143299e-10
  9.01323623e-11  8.91435672e-11  3.49072328e-11  8.67360121e-11
 -6.79978170e-12 -6.72526902e-12 -2.59899625e-12 -6.51458491e-12
  4.88573422e-11  4.83196314e-11  1.88248680e-11  4.68914179e-11
  1.19889861e-11  1.18575628e-11  4.56377883e-12  1.14649447e-11
  1.19488797e-11  7.92838929e-12  1.08932874e-11  6.13881999e-12
  1.21625411e-11  7.54436951e-12  1.08891028e-11  6.28874434e-12
  1.85472117e-11  6.20936460e-12  1.47699221e-11  1.04583212e-11
  7.51251206e-12  5.30670435e-12  6.97133510e-12  3.83724558e-12
  9.16108979e-12 -3.60387101e-12  3.16771382e-12 -4.14165531e-12
  1.08558790e-11 -1.65072774e-12  5.27526691e-12  1.75488728e-12
 -1.01497489e-01]
supnorm grad right now is: 0.10149748884096209
Weights right now are: 
[-4.29321152 -4.85741059 -3.77653198 -0.82412129  4.51280199  3.62584117
  5.17791629  0.80430716 -1.44575765 -0.8356543  -2.48713002 -2.37732613
 -1.26846867  0.51371547 -2.90131368 -1.14287574 -1.70098576  0.4336954
 -1.40792395 -2.42142405  1.62014984  0.58018932  1.37649469  1.2502604
 -1.55331295 -1.32233518 -0.57767511 -0.49242997  1.44684885  1.71391873
  1.46935815  1.70995743 -1.92315956 -1.80311927 -0.65444412 -1.00836289
 -2.01545011 -1.72352664 -1.51578231 -2.0212907  -1.62892964 -1.12314433
 -1.52307328 -0.92987387 -1.5150254  -1.12351274 -1.45315033 -0.82542674
  0.2622384   1.46246277  0.07445711  0.1566461  -1.67085205 -0.83129723
 -0.47024087 -1.61466416 -0.53964318 -1.50045917  1.92093466  1.29016904
  0.17754284 -0.28371229  1.9051663  -0.53600086 24.90291185]
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.163287354597
gradient value of function right now is: [ 2.53912278e-08  2.53054484e-08  2.53315422e-08  3.86079134e-10
 -2.53918635e-08 -2.53060820e-08 -2.53321765e-08 -3.86088908e-10
  4.23931566e-10  3.26544654e-12  4.35331984e-10  4.38638431e-10
  1.10421635e-09  8.51066390e-12  1.13391877e-09  1.14253435e-09
  9.42655057e-10  7.26556961e-12  9.68011774e-10  9.75366800e-10
  5.96435084e-10  4.58863647e-12  6.12466046e-10  6.17114329e-10
  8.88530936e-11  8.78782897e-11  3.44114333e-11  8.55042227e-11
 -6.70199073e-12 -6.62854665e-12 -2.56158727e-12 -6.42084297e-12
  4.81625179e-11  4.76324321e-11  1.85569484e-11  4.62241686e-11
  1.18193346e-11  1.16897655e-11  4.49914870e-12  1.13026174e-11
  1.17796904e-11  7.81759164e-12  1.07397889e-11  6.05164073e-12
  1.19900855e-11  7.43892366e-12  1.07354494e-11  6.19928187e-12
  1.82815007e-11  6.12251720e-12  1.45590826e-11  1.03077060e-11
  7.40619546e-12  5.23251377e-12  6.87315720e-12  3.78281346e-12
  9.03359629e-12 -3.55565668e-12  3.12367840e-12 -4.08467323e-12
  1.07040697e-11 -1.63087678e-12  5.20114076e-12  1.72745361e-12
 -3.47130944e-01]
supnorm grad right now is: 0.34713094363538227
Weights right now are: 
[-4.29818293 -4.8623652  -3.7814917  -0.8241969   4.5177734   3.63079579
  5.18287601  0.80438277 -1.44584069 -0.83565494 -2.48721529 -2.37741204
 -1.26868492  0.5137138  -2.90153574 -1.1430995  -1.70117037  0.43369398
 -1.40811353 -2.42161506  1.62003304  0.58018842  1.37637475  1.25013955
 -1.55333036 -1.32235239 -0.57768185 -0.49244672  1.44685016  1.71392002
  1.46935865  1.70995869 -1.923169   -1.8031286  -0.65444775 -1.00837194
 -2.01545242 -1.72352893 -1.5157832  -2.02129291 -1.62893195 -1.12314586
 -1.52307539 -0.92987506 -1.51502775 -1.1235142  -1.45315243 -0.82542796
  0.26223482  1.46246157  0.07445426  0.15664408 -1.6708535  -0.83129826
 -0.47024222 -1.6146649  -0.53964496 -1.50045847  1.92093405  1.29016984
  0.17754075 -0.28371196  1.90516528 -0.53600119 24.70216116]
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.168135940568
gradient value of function right now is: [ 2.50076112e-08  2.49231655e-08  2.49488516e-08  3.80309708e-10
 -2.50080434e-08 -2.49235962e-08 -2.49492827e-08 -3.80316320e-10
  4.17706796e-10  3.21719980e-12  4.28939020e-10  4.32196662e-10
  1.08780970e-09  8.38343677e-12  1.11706873e-09  1.12555564e-09
  9.28662826e-10  7.15705919e-12  9.53641389e-10  9.60886676e-10
  5.87547222e-10  4.51983677e-12  6.03338174e-10  6.07916832e-10
  8.75419928e-11  8.65816040e-11  3.39044619e-11  8.42431966e-11
 -6.60138285e-12 -6.52904341e-12 -2.52320091e-12 -6.32450498e-12
  4.74485117e-11  4.69263002e-11  1.82822908e-11  4.55392527e-11
  1.16441917e-11  1.15165463e-11  4.43259686e-12  1.11352168e-11
  1.16053080e-11  7.69984029e-12  1.05798332e-11  5.96198671e-12
  1.18127244e-11  7.32684533e-12  1.05756963e-11  6.10755642e-12
  1.80132415e-11  6.03030548e-12  1.43445736e-11  1.01569214e-11
  7.29636939e-12  5.15360864e-12  6.77059472e-12  3.72663378e-12
  8.89825045e-12 -3.49983423e-12  3.07663452e-12 -4.02218240e-12
  1.05440433e-11 -1.60290266e-12  5.12355237e-12  1.70455291e-12
 -8.58854738e-02]
supnorm grad right now is: 0.08588547382053585
Weights right now are: 
[-4.30304907 -4.86721493 -3.78634643 -0.82427092  4.52263953  3.63564551
  5.18773072  0.80445679 -1.44592204 -0.83565556 -2.48729882 -2.37749621
 -1.26889669  0.51371217 -2.90175321 -1.14331861 -1.70135117  0.43369259
 -1.40829918 -2.42180213  1.61991867  0.58018754  1.37625731  1.25002121
 -1.5533474  -1.32236925 -0.57768845 -0.49246312  1.44685145  1.71392129
  1.46935914  1.70995992 -1.92317823 -1.80313774 -0.65445131 -1.00838081
 -2.01545469 -1.72353117 -1.51578406 -2.02129508 -1.62893421 -1.12314736
 -1.52307745 -0.92987622 -1.51503005 -1.12351562 -1.45315449 -0.82542914
  0.26223131  1.4624604   0.07445146  0.1566421  -1.67085492 -0.83129926
 -0.47024353 -1.61466563 -0.53964669 -1.50045779  1.92093345  1.29017062
  0.17753869 -0.28371164  1.90516428 -0.53600153 25.13258127]
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.167244555543
gradient value of function right now is: [ 2.46492907e-08  2.45660908e-08  2.45913959e-08  3.74923794e-10
 -2.46474157e-08 -2.45642223e-08 -2.45895254e-08 -3.74895044e-10
  4.11879080e-10  3.17237067e-12  4.22954375e-10  4.26166541e-10
  1.07245460e-09  8.26525575e-12  1.10130008e-09  1.10966712e-09
  9.15555835e-10  7.05616772e-12  9.40181357e-10  9.47324319e-10
  5.79224649e-10  4.45589146e-12  5.94791612e-10  5.99305376e-10
  8.63181941e-11  8.53712402e-11  3.34299508e-11  8.30653798e-11
 -6.50778282e-12 -6.43646987e-12 -2.48737883e-12 -6.23481997e-12
  4.67833831e-11  4.62684971e-11  1.80257057e-11  4.49008135e-11
  1.14814423e-11  1.13555826e-11  4.37055489e-12  1.09795628e-11
  1.14424786e-11  7.59298413e-12  1.04318696e-11  5.87867824e-12
  1.16469416e-11  7.22513650e-12  1.04277253e-11  6.02216166e-12
  1.77601843e-11  5.94648021e-12  1.41433687e-11  1.00144036e-11
  7.19394785e-12  5.08205748e-12  6.67587779e-12  3.67455570e-12
  8.77313905e-12 -3.45182792e-12  3.03362269e-12 -3.96653704e-12
  1.03961338e-11 -1.58181763e-12  5.05178370e-12  1.67984048e-12
 -1.67819909e-01]
supnorm grad right now is: 0.16781990931897434
Weights right now are: 
[-4.30773041 -4.87188048 -3.79101677 -0.82434217  4.52732085  3.64031104
  5.19240105  0.80452803 -1.44600036 -0.83565617 -2.48737926 -2.37757726
 -1.26910053  0.5137106  -2.90196254 -1.14352953 -1.7015252   0.43369124
 -1.4084779  -2.4219822   1.61980859  0.58018669  1.37614427  1.24990731
 -1.55336382 -1.32238549 -0.57769481 -0.49247892  1.44685268  1.71392252
  1.46935962  1.7099611  -1.92318713 -1.80314654 -0.65445474 -1.00838935
 -2.01545687 -1.72353333 -1.51578489 -2.02129717 -1.62893639 -1.12314881
 -1.52307943 -0.92987734 -1.51503227 -1.123517   -1.45315648 -0.82543029
  0.26222793  1.46245927  0.07444878  0.1566402  -1.67085629 -0.83130023
 -0.47024481 -1.61466633 -0.53964836 -1.50045714  1.92093287  1.29017138
  0.17753671 -0.28371133  1.90516332 -0.53600185 25.21648986]
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.1685155766
gradient value of function right now is: [ 2.42938141e-08  2.42118491e-08  2.42367767e-08  3.69576834e-10
 -2.42950168e-08 -2.42130478e-08 -2.42379766e-08 -3.69595339e-10
  4.06135555e-10  3.12800601e-12  4.17055913e-10  4.20223161e-10
  1.05730736e-09  8.14817794e-12  1.08574411e-09  1.09399264e-09
  9.02655913e-10  6.95647579e-12  9.26933390e-10  9.33975441e-10
  5.71024754e-10  4.39262894e-12  5.86370639e-10  5.90820323e-10
  8.51101503e-11  8.41764604e-11  3.29628356e-11  8.19030843e-11
 -6.41520778e-12 -6.34491032e-12 -2.45205142e-12 -6.14614358e-12
  4.61259095e-11  4.56182664e-11  1.77727712e-11  4.42699145e-11
  1.13204227e-11  1.11963298e-11  4.30936317e-12  1.08256062e-11
  1.12822012e-11  7.48559125e-12  1.02851135e-11  5.79593419e-12
  1.14838286e-11  7.12288752e-12  1.02810673e-11  5.93745544e-12
  1.75122549e-11  5.86210887e-12  1.39453700e-11  9.87450958e-12
  7.09305876e-12  5.01010918e-12  6.58183086e-12  3.62274056e-12
  8.65009224e-12 -3.40134846e-12  2.99072465e-12 -3.90967104e-12
  1.02503292e-11 -1.55871971e-12  4.98077821e-12  1.65768720e-12
  3.90524617e-03]
supnorm grad right now is: 0.003905246172356289
Weights right now are: 
[-4.31231149 -4.87644613 -3.79558711 -0.82441191  4.53190193  3.64487669
  5.19697139  0.80459778 -1.44607708 -0.83565676 -2.48745804 -2.37765664
 -1.26930012  0.51370906 -2.9021675  -1.14373606 -1.7016956   0.43368993
 -1.40865289 -2.42215853  1.61970082  0.58018586  1.3760336   1.2497958
 -1.55337992 -1.32240141 -0.57770105 -0.49249441  1.4468539   1.71392372
  1.46936008  1.70996227 -1.92319586 -1.80315517 -0.65445811 -1.00839772
 -2.01545902 -1.72353545 -1.51578571 -2.02129922 -1.62893852 -1.12315023
 -1.52308139 -0.92987843 -1.51503444 -1.12351836 -1.45315843 -0.82543141
  0.26222463  1.46245815  0.07444615  0.15663834 -1.67085764 -0.83130119
 -0.47024605 -1.61466701 -0.53965002 -1.50045648  1.9209323   1.29017212
  0.17753475 -0.28371101  1.90516237 -0.53600215 24.92358806]
NN weights: [-4.30050761 -4.86468206 -3.78381094 -0.82423229  4.52009807  3.63311264
  5.18519524  0.80441815 -1.44587962 -0.83565524 -2.48725526 -2.37745232
 -1.26878619  0.51371302 -2.90163974 -1.14320428 -1.70125684  0.43369331
 -1.40820231 -2.42170453  1.61997836  0.580188    1.3763186   1.25008296
 -1.55333852 -1.32236046 -0.57768501 -0.49245457  1.44685078  1.71392063
  1.46935889  1.70995928 -1.92317342 -1.80313297 -0.65444946 -1.00837619
 -2.01545351 -1.72353    -1.51578361 -2.02129395 -1.62893303 -1.12314658
 -1.52307637 -0.92987561 -1.51502885 -1.12351488 -1.45315342 -0.82542852
  0.26223314  1.46246101  0.07445292  0.15664313 -1.67085418 -0.83129874
 -0.47024285 -1.61466525 -0.53964579 -1.50045815  1.92093376  1.29017021
  0.17753976 -0.2837118   1.9051648  -0.53600135]
Minimum obj value:-16005.1685155766
Optimal xi: 24.99357550310524
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1551.5577480611685
W_T_median: 1405.438397291775
W_T_pctile_5: 624.6754498215942
W_T_CVAR_5_pct: 489.59103519940646
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 10.0
F value: -16005.1685155766
-----------------------------------------------
