/home/marcchen/Documents/constrain_factor/researchcode/exp_config_json_files/multi_portfolio_exp2_30yrsplit.json
Starting at: 
13-07-23_15:23

 numpy seed:  2  


 pytorch seed:  2  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'B10_real_ret', 'VWD_real_ret', 'Size_Lo30_real_ret', 'Value_Hi30_real_ret']
############# End: defined asset  basket #################
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
192601                  0.0                NaN  ...     0.000561     0.023174
192602                  0.0                NaN  ...    -0.033046    -0.053510
192603                  0.0                NaN  ...    -0.064002    -0.096824
192604                  0.0                NaN  ...     0.037029     0.032975
192605                  0.0                NaN  ...     0.012095     0.001035

[5 rows x 16 columns]
               Cash_nom_ret  Size_Lo30_nom_ret  ...  VWD_nom_ret  EWD_nom_ret
My identifier                                   ...                          
202208                  NaN            -0.0020  ...    -0.036240    -0.011556
202209                  NaN            -0.0955  ...    -0.091324    -0.099903
202210                  NaN             0.0883  ...     0.077403     0.049863
202211                  NaN            -0.0076  ...     0.052365     0.028123
202212                  NaN            -0.0457  ...    -0.057116    -0.047241

[5 rows x 16 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = Paper_FactorInv_Factor2
timeseries_basket['basket_desc'] = Factor2 portfolio for paper: Basic, size and value
timeseries_basket['basket_columns'] = 
['T30_nom_ret', 'B10_nom_ret', 'VWD_nom_ret', 'Size_Lo30_nom_ret', 'Value_Hi30_nom_ret']
############# End: defined asset  basket #################
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
192607                    0.0014              0.0154  ...     0.005383     0.031411
192608                    0.0319              0.0561  ...     0.005363     0.028647
192609                   -0.0173             -0.0071  ...     0.005343     0.005787
192610                   -0.0294             -0.0355  ...     0.005323    -0.028996
192611                   -0.0038              0.0294  ...     0.005303     0.028554

[5 rows x 6 columns]
               Size_Lo30_nom_ret  Value_Hi30_nom_ret  ...  B10_nom_ret  VWD_nom_ret
My identifier                                         ...                          
202208                   -0.0020             -0.0164  ...    -0.043289    -0.036240
202209                   -0.0955             -0.0871  ...    -0.050056    -0.091324
202210                    0.0883              0.1486  ...    -0.014968     0.077403
202211                   -0.0076              0.0462  ...     0.040789     0.052365
202212                   -0.0457             -0.0499  ...    -0.018566    -0.057116

[5 rows x 6 columns]
Indices constructed with column names:
Index(['date_for_plt', 'Size_Lo30_nom_ret_ind', 'Value_Hi30_nom_ret_ind',
       'CPI_nom_ret_ind', 'T30_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
T30_real_ret           0.000229
B10_real_ret           0.001637
VWD_real_ret           0.006759
Size_Lo30_real_ret     0.009939
Value_Hi30_real_ret    0.010052
dtype: float64


timeseries_basket['data_df_stdev'] = 
T30_real_ret           0.005227
B10_real_ret           0.019258
VWD_real_ret           0.053610
Size_Lo30_real_ret     0.082598
Value_Hi30_real_ret    0.072063
dtype: float64


timeseries_basket['data_df_corr'] = 
                     T30_real_ret  ...  Value_Hi30_real_ret
T30_real_ret             1.000000  ...             0.018239
B10_real_ret             0.351722  ...             0.031301
VWD_real_ret             0.068448  ...             0.909335
Size_Lo30_real_ret       0.014412  ...             0.908542
Value_Hi30_real_ret      0.018239  ...             1.000000

[5 rows x 5 columns]


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192607
End: 202212
-----------------------------------------------
Bootstrap block size: 6
-----------------------------------------------
Dates USED bootstrapping:
Start: 196307
End: 199307
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       5       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/constrain_factor/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      13  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      13  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       5           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 13)     True          13  
2     (13, 13)     True          13  
3      (13, 5)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 1500, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       13  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       13  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        5           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 13)      True          13  
0     (13, 13)      True          13  
0      (13, 5)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.0, 0.6, 0.1, 0.15, 0.15]
W_T_mean: 1048.790547539696
W_T_median: 718.9540226887534
W_T_pctile_5: -260.92985278814115
W_T_CVAR_5_pct: -399.7608019453169
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1855.0692344333647
Current xi:  [123.19793]
objective value function right now is: -1855.0692344333647
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1878.6466655695588
Current xi:  [147.14073]
objective value function right now is: -1878.6466655695588
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1899.9933208488867
Current xi:  [170.88193]
objective value function right now is: -1899.9933208488867
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1914.1744453770327
Current xi:  [194.3372]
objective value function right now is: -1914.1744453770327
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1928.1224124345745
Current xi:  [217.55959]
objective value function right now is: -1928.1224124345745
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1946.7119399999967
Current xi:  [240.68365]
objective value function right now is: -1946.7119399999967
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1956.6574697043484
Current xi:  [263.4462]
objective value function right now is: -1956.6574697043484
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1976.4756806722708
Current xi:  [286.36932]
objective value function right now is: -1976.4756806722708
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1989.689711328801
Current xi:  [309.09094]
objective value function right now is: -1989.689711328801
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1996.8964925929629
Current xi:  [331.59222]
objective value function right now is: -1996.8964925929629
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -2018.0354214582478
Current xi:  [354.19974]
objective value function right now is: -2018.0354214582478
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -2031.6018845995725
Current xi:  [376.74246]
objective value function right now is: -2031.6018845995725
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -2040.0425915040735
Current xi:  [399.28864]
objective value function right now is: -2040.0425915040735
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -2057.593074570092
Current xi:  [421.83704]
objective value function right now is: -2057.593074570092
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -2068.105326297817
Current xi:  [444.31473]
objective value function right now is: -2068.105326297817
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -2081.476588653229
Current xi:  [466.5363]
objective value function right now is: -2081.476588653229
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -2094.899717496588
Current xi:  [488.71356]
objective value function right now is: -2094.899717496588
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [510.68262]
objective value function right now is: -2084.2921522432625
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -2111.9814634125996
Current xi:  [532.0779]
objective value function right now is: -2111.9814634125996
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -2124.9308610034022
Current xi:  [554.2367]
objective value function right now is: -2124.9308610034022
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -2150.4054234329706
Current xi:  [578.30304]
objective value function right now is: -2150.4054234329706
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -2159.2840362288107
Current xi:  [601.74225]
objective value function right now is: -2159.2840362288107
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -2172.737729312845
Current xi:  [624.4593]
objective value function right now is: -2172.737729312845
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -2174.998783034753
Current xi:  [646.7639]
objective value function right now is: -2174.998783034753
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -2193.865564664825
Current xi:  [668.7079]
objective value function right now is: -2193.865564664825
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -2204.3212124703414
Current xi:  [691.0581]
objective value function right now is: -2204.3212124703414
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -2214.7147359184596
Current xi:  [713.0886]
objective value function right now is: -2214.7147359184596
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -2222.780732200599
Current xi:  [734.8281]
objective value function right now is: -2222.780732200599
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -2233.2484529355547
Current xi:  [756.55426]
objective value function right now is: -2233.2484529355547
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -2243.0044522174885
Current xi:  [778.00214]
objective value function right now is: -2243.0044522174885
62.0% of gradient descent iterations done. Method = Adam
new min fval:  -2246.413580542687
Current xi:  [799.06726]
objective value function right now is: -2246.413580542687
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -2258.1443554370835
Current xi:  [820.42737]
objective value function right now is: -2258.1443554370835
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -2264.95166245876
Current xi:  [841.0101]
objective value function right now is: -2264.95166245876
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -2270.840050854742
Current xi:  [861.5467]
objective value function right now is: -2270.840050854742
70.0% of gradient descent iterations done. Method = Adam
new min fval:  -2277.243386221787
Current xi:  [881.9712]
objective value function right now is: -2277.243386221787
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -2283.5533479678083
Current xi:  [886.2664]
objective value function right now is: -2283.5533479678083
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -2284.984952503274
Current xi:  [890.5081]
objective value function right now is: -2284.984952503274
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -2286.910172379596
Current xi:  [894.8446]
objective value function right now is: -2286.910172379596
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -2287.9855629510357
Current xi:  [899.1558]
objective value function right now is: -2287.9855629510357
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -2289.4929099746173
Current xi:  [903.564]
objective value function right now is: -2289.4929099746173
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -2290.143409053895
Current xi:  [907.8594]
objective value function right now is: -2290.143409053895
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -2292.7891464983095
Current xi:  [912.1338]
objective value function right now is: -2292.7891464983095
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -2293.7554405092187
Current xi:  [916.5356]
objective value function right now is: -2293.7554405092187
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [920.95386]
objective value function right now is: -2293.355857940267
90.0% of gradient descent iterations done. Method = Adam
new min fval:  -2296.954523264386
Current xi:  [925.30005]
objective value function right now is: -2296.954523264386
92.0% of gradient descent iterations done. Method = Adam
new min fval:  -2297.946857837515
Current xi:  [929.63226]
objective value function right now is: -2297.946857837515
94.0% of gradient descent iterations done. Method = Adam
new min fval:  -2299.233236349415
Current xi:  [933.9413]
objective value function right now is: -2299.233236349415
new min fval from sgd:  -2300.360861793994
new min fval from sgd:  -2300.4224555887895
new min fval from sgd:  -2300.530892154609
new min fval from sgd:  -2300.5852446655977
new min fval from sgd:  -2300.5938714385584
new min fval from sgd:  -2300.6133190305363
new min fval from sgd:  -2300.658829745665
new min fval from sgd:  -2300.673911288865
new min fval from sgd:  -2300.696710551276
new min fval from sgd:  -2300.708748073426
new min fval from sgd:  -2300.749099744051
new min fval from sgd:  -2300.8039767062014
new min fval from sgd:  -2300.8173335527545
new min fval from sgd:  -2300.835524431101
new min fval from sgd:  -2300.837136109772
new min fval from sgd:  -2300.8535241543873
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [938.32153]
objective value function right now is: -2300.556636131088
new min fval from sgd:  -2300.898967472894
new min fval from sgd:  -2300.9601021999056
new min fval from sgd:  -2301.00429120016
new min fval from sgd:  -2301.028682651091
new min fval from sgd:  -2301.0391114801437
new min fval from sgd:  -2301.0843438833226
new min fval from sgd:  -2301.1175675538043
new min fval from sgd:  -2301.148260109031
new min fval from sgd:  -2301.1719928518573
new min fval from sgd:  -2301.1775914099526
new min fval from sgd:  -2301.1937475504383
new min fval from sgd:  -2301.2085539459517
new min fval from sgd:  -2301.2676072736153
new min fval from sgd:  -2301.3272653394256
new min fval from sgd:  -2301.3526732437667
new min fval from sgd:  -2301.4227555413677
new min fval from sgd:  -2301.496712668365
new min fval from sgd:  -2301.539179568865
new min fval from sgd:  -2301.562513355256
new min fval from sgd:  -2301.600525974958
new min fval from sgd:  -2301.6059879849704
new min fval from sgd:  -2301.62449868783
new min fval from sgd:  -2301.646673029192
new min fval from sgd:  -2301.6779753439223
new min fval from sgd:  -2301.6805421127365
new min fval from sgd:  -2301.742715076747
new min fval from sgd:  -2301.828344644242
new min fval from sgd:  -2301.8355038709633
new min fval from sgd:  -2301.8433169610075
new min fval from sgd:  -2301.875318355331
new min fval from sgd:  -2301.9029690421244
new min fval from sgd:  -2301.9248867203346
new min fval from sgd:  -2301.942254296638
new min fval from sgd:  -2301.9534242119407
new min fval from sgd:  -2301.958359131019
new min fval from sgd:  -2301.959961293659
new min fval from sgd:  -2301.9610205683216
new min fval from sgd:  -2301.9666421025368
new min fval from sgd:  -2301.9750624203766
new min fval from sgd:  -2301.9846389874024
new min fval from sgd:  -2301.9927799502543
new min fval from sgd:  -2301.995330338964
new min fval from sgd:  -2302.0016815278714
new min fval from sgd:  -2302.004878047499
new min fval from sgd:  -2302.005222616714
new min fval from sgd:  -2302.0079172169253
new min fval from sgd:  -2302.0214676765204
new min fval from sgd:  -2302.0224068440793
new min fval from sgd:  -2302.0224640309934
new min fval from sgd:  -2302.024787022182
new min fval from sgd:  -2302.026594790389
new min fval from sgd:  -2302.0299041595817
new min fval from sgd:  -2302.033850269422
new min fval from sgd:  -2302.0380895932212
new min fval from sgd:  -2302.04245652697
new min fval from sgd:  -2302.045686853692
new min fval from sgd:  -2302.0493046532542
new min fval from sgd:  -2302.051685252149
new min fval from sgd:  -2302.053941844304
new min fval from sgd:  -2302.055374805253
new min fval from sgd:  -2302.060279875425
new min fval from sgd:  -2302.0649420255118
new min fval from sgd:  -2302.0669356225344
new min fval from sgd:  -2302.071240678058
new min fval from sgd:  -2302.0751830506874
new min fval from sgd:  -2302.075866437014
new min fval from sgd:  -2302.079233209292
new min fval from sgd:  -2302.080048668678
new min fval from sgd:  -2302.081357421168
new min fval from sgd:  -2302.0829631202864
new min fval from sgd:  -2302.08413097668
new min fval from sgd:  -2302.084879665406
new min fval from sgd:  -2302.0929934304777
new min fval from sgd:  -2302.0974050109094
new min fval from sgd:  -2302.100603347134
new min fval from sgd:  -2302.1017865799167
new min fval from sgd:  -2302.1037982762514
new min fval from sgd:  -2302.1057989249002
new min fval from sgd:  -2302.1088983584896
new min fval from sgd:  -2302.114920907631
new min fval from sgd:  -2302.122505341367
new min fval from sgd:  -2302.1264099079845
new min fval from sgd:  -2302.129449430526
new min fval from sgd:  -2302.132714405314
new min fval from sgd:  -2302.136002937168
new min fval from sgd:  -2302.14006787428
new min fval from sgd:  -2302.142801584477
new min fval from sgd:  -2302.144661274687
new min fval from sgd:  -2302.1512831456316
new min fval from sgd:  -2302.1594718890574
new min fval from sgd:  -2302.1613572472884
new min fval from sgd:  -2302.1658389165673
new min fval from sgd:  -2302.1678238516356
new min fval from sgd:  -2302.1694019151096
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [940.86285]
objective value function right now is: -2302.0825307910463
new min fval from sgd:  -2302.170683028012
new min fval from sgd:  -2302.175470449169
new min fval from sgd:  -2302.1803250353887
new min fval from sgd:  -2302.187936414115
new min fval from sgd:  -2302.1916673409405
new min fval from sgd:  -2302.1939937063034
new min fval from sgd:  -2302.1961811818546
new min fval from sgd:  -2302.1985623209052
new min fval from sgd:  -2302.2045601687487
new min fval from sgd:  -2302.213738785559
new min fval from sgd:  -2302.2183882603713
new min fval from sgd:  -2302.2232567714855
new min fval from sgd:  -2302.22986845357
new min fval from sgd:  -2302.2346000340162
new min fval from sgd:  -2302.239700902446
new min fval from sgd:  -2302.244486381828
new min fval from sgd:  -2302.249821722948
new min fval from sgd:  -2302.2558619241863
new min fval from sgd:  -2302.263553206561
new min fval from sgd:  -2302.267096053562
new min fval from sgd:  -2302.2687792671845
new min fval from sgd:  -2302.2715789390413
new min fval from sgd:  -2302.2737593886973
new min fval from sgd:  -2302.2854312536406
new min fval from sgd:  -2302.2862793251197
new min fval from sgd:  -2302.290667597633
new min fval from sgd:  -2302.2960438211467
new min fval from sgd:  -2302.2973779465556
new min fval from sgd:  -2302.299361640059
new min fval from sgd:  -2302.300684217572
new min fval from sgd:  -2302.3026417808837
new min fval from sgd:  -2302.3062057329525
new min fval from sgd:  -2302.311362243813
new min fval from sgd:  -2302.3119134996377
new min fval from sgd:  -2302.314661572314
new min fval from sgd:  -2302.324523831343
new min fval from sgd:  -2302.338421518699
new min fval from sgd:  -2302.3396491758385
new min fval from sgd:  -2302.341230995267
new min fval from sgd:  -2302.343001999847
new min fval from sgd:  -2302.3446871013157
new min fval from sgd:  -2302.34566379077
new min fval from sgd:  -2302.346977767124
new min fval from sgd:  -2302.3502103218652
new min fval from sgd:  -2302.3545681391583
new min fval from sgd:  -2302.362461231826
new min fval from sgd:  -2302.368641515758
new min fval from sgd:  -2302.37210843904
new min fval from sgd:  -2302.377497199282
new min fval from sgd:  -2302.3804222342596
new min fval from sgd:  -2302.381710692054
new min fval from sgd:  -2302.3839402770536
new min fval from sgd:  -2302.385092820143
new min fval from sgd:  -2302.39032358856
new min fval from sgd:  -2302.398390391542
new min fval from sgd:  -2302.4043568653165
new min fval from sgd:  -2302.4067738502536
new min fval from sgd:  -2302.408951371205
new min fval from sgd:  -2302.4117763695613
new min fval from sgd:  -2302.4144304296597
new min fval from sgd:  -2302.4146917002445
new min fval from sgd:  -2302.415085099698
new min fval from sgd:  -2302.41543389743
new min fval from sgd:  -2302.4191359274137
new min fval from sgd:  -2302.4213469828073
new min fval from sgd:  -2302.4257457435915
new min fval from sgd:  -2302.433343656573
new min fval from sgd:  -2302.437617323396
new min fval from sgd:  -2302.4403438558247
new min fval from sgd:  -2302.4490884490883
new min fval from sgd:  -2302.4629529126037
new min fval from sgd:  -2302.472498825379
new min fval from sgd:  -2302.4812915884704
new min fval from sgd:  -2302.486380155288
new min fval from sgd:  -2302.4878922998805
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [941.7332]
objective value function right now is: -2302.479701929954
min fval:  -2302.4878922998805
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -7.0111,   8.0128],
        [-29.6492, -13.4595],
        [ -7.7529,   7.9579],
        [-12.7375, -15.4874],
        [  9.4727,  -2.3281],
        [ -9.1099,   6.4311],
        [  7.2585,   4.8530],
        [ -8.9766,   7.0414],
        [ -7.4396,   7.6680],
        [  5.9949,  -9.1661],
        [ -9.1651,   6.4715],
        [  2.4847,  -0.9547],
        [ -1.2522,  -0.1818]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([  0.9016,  -2.1639,   0.7198,   4.3561,  -7.2314,  -0.6340, -10.9764,
          0.2972,   0.7264,  -6.9177,  -0.6102,  -6.7993,  -3.1841],
       device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -0.3439,   0.0376,  -0.2294,  -0.1038,  -0.6376,  -0.0132,  -0.1154,
          -0.0673,  -0.2420,  -0.0549,  -0.0135,  -0.0187,  -0.0241],
        [ -0.3439,   0.0376,  -0.2294,  -0.1038,  -0.6376,  -0.0132,  -0.1154,
          -0.0673,  -0.2420,  -0.0549,  -0.0135,  -0.0187,  -0.0241],
        [ -0.3419,   0.0378,  -0.2285,  -0.1023,  -0.6315,  -0.0131,  -0.1153,
          -0.0670,  -0.2410,  -0.0549,  -0.0134,  -0.0187,  -0.0241],
        [ -4.0287,   5.8353,  -3.9539,  10.1489,   4.1564,  -2.1933,   2.6531,
          -3.5018,  -4.0820,   2.0870,  -2.2608,  -0.1683,   0.1392],
        [ -0.3439,   0.0376,  -0.2294,  -0.1038,  -0.6376,  -0.0132,  -0.1154,
          -0.0673,  -0.2420,  -0.0549,  -0.0135,  -0.0187,  -0.0241],
        [ -4.3054,   6.4384,  -4.3660,  10.7595,   4.5477,  -2.4398,   3.2682,
          -3.6917,  -4.3178,   2.4777,  -2.5074,  -0.1988,   0.1145],
        [ -4.5563,   8.9903,  -4.2862,  10.9260,   4.8506,  -2.3607,   4.5927,
          -3.6638,  -4.4679,   2.7410,  -2.2923,  -0.2184,   0.0942],
        [ -0.3439,   0.0376,  -0.2294,  -0.1038,  -0.6376,  -0.0132,  -0.1154,
          -0.0673,  -0.2420,  -0.0549,  -0.0135,  -0.0187,  -0.0241],
        [ -0.3439,   0.0376,  -0.2294,  -0.1038,  -0.6376,  -0.0132,  -0.1154,
          -0.0673,  -0.2420,  -0.0549,  -0.0135,  -0.0187,  -0.0241],
        [  2.7451, -11.0373,   2.8160,  -9.4122,  -7.7156,   2.1020, -12.5538,
           2.2665,   2.8776,  -4.1666,   2.1337,  -1.0095,   0.1577],
        [ -0.3439,   0.0376,  -0.2294,  -0.1038,  -0.6376,  -0.0132,  -0.1154,
          -0.0673,  -0.2420,  -0.0549,  -0.0135,  -0.0187,  -0.0241],
        [  1.3100,   0.2444,   1.0026,   0.2593,   6.0808,  -1.9615,  10.0017,
          -1.0393,   0.9885,   0.5938,  -1.9710,   0.1932,   0.2564],
        [ -0.3439,   0.0376,  -0.2294,  -0.1038,  -0.6376,  -0.0132,  -0.1154,
          -0.0673,  -0.2420,  -0.0549,  -0.0135,  -0.0187,  -0.0241]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.9366, -1.9366, -1.9471, -2.2120, -1.9366, -2.1322, -2.0729, -1.9366,
        -1.9366,  2.6872, -1.9366, -5.0711, -1.9366], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-9.6046e-03, -9.6046e-03, -9.7986e-03, -5.5344e+00, -9.6046e-03,
         -6.3237e+00, -7.2827e+00, -9.6046e-03, -9.6046e-03,  1.8412e+01,
         -9.6046e-03, -7.6158e+00, -9.6046e-03]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-12.4781,  -7.1362],
        [ -8.1635,  -0.5569],
        [ -1.4703,   0.1743],
        [  8.6153,  -0.6641],
        [ -1.4600,   0.1723],
        [-24.8952,   3.9363],
        [ -3.0003,  -7.4720],
        [-10.0305,  -0.1271],
        [ -1.5890,   0.2092],
        [  4.8278,  -9.1135],
        [ 12.8565,   4.7038],
        [  5.7030,  11.7599],
        [ 10.9121,  -7.4170]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-1.3293,  6.2922, -4.5290, -9.6458, -4.5241, -0.0988, -5.9501,  9.5844,
        -4.6037, -4.1407, -1.4261,  1.0900, -1.9118], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-0.2466,  0.6029, -0.1166, -3.3229, -0.1085,  2.2354,  1.8952,  4.5531,
         -0.2822,  3.1208, -8.9846, -7.5094,  1.7904],
        [-0.0531, -0.5196, -0.0229, -0.3080, -0.0229, -0.0794, -0.2758, -0.9634,
         -0.0245, -1.0973, -2.1977, -1.8739, -0.5358],
        [-0.0532, -0.5200, -0.0228, -0.3067, -0.0228, -0.0786, -0.2747, -0.9641,
         -0.0244, -1.0973, -2.1985, -1.8727, -0.5355],
        [-0.0533, -0.5204, -0.0227, -0.3051, -0.0227, -0.0777, -0.2735, -0.9648,
         -0.0243, -1.0970, -2.1993, -1.8715, -0.5354],
        [ 2.2232,  1.5169,  0.1234, -2.2685,  0.1183, -0.3201, -1.0971,  1.0463,
          0.1805, -1.4205,  1.1834, -0.7379, -0.4231],
        [-0.0533, -0.5205, -0.0227, -0.3048, -0.0226, -0.0775, -0.2732, -0.9648,
         -0.0243, -1.0969, -2.1993, -1.8713, -0.5355],
        [-0.0531, -0.5197, -0.0229, -0.3080, -0.0229, -0.0794, -0.2758, -0.9634,
         -0.0245, -1.0974, -2.1976, -1.8738, -0.5358],
        [-8.1936,  1.0060,  0.0284, -8.0436,  0.0277,  8.8328, -6.7341,  3.0728,
          0.0346, -5.6495, -1.2902,  0.7456, -1.9981],
        [-0.0511, -0.5193, -0.0153, -0.2226, -0.0152, -0.0315, -0.2081, -0.9780,
         -0.0164, -1.0169, -2.2355, -1.8116, -0.5814],
        [-0.0531, -0.5196, -0.0229, -0.3080, -0.0229, -0.0794, -0.2758, -0.9634,
         -0.0245, -1.0973, -2.1977, -1.8739, -0.5358],
        [-0.0531, -0.5197, -0.0229, -0.3078, -0.0229, -0.0793, -0.2757, -0.9635,
         -0.0245, -1.0974, -2.1977, -1.8736, -0.5358],
        [-0.0533, -0.5206, -0.0227, -0.3045, -0.0226, -0.0773, -0.2729, -0.9650,
         -0.0242, -1.0969, -2.1992, -1.8709, -0.5355],
        [-0.0532, -0.5199, -0.0229, -0.3072, -0.0228, -0.0789, -0.2752, -0.9638,
         -0.0244, -1.0974, -2.1980, -1.8731, -0.5356]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.8676, -2.4926, -2.4927, -2.4928,  2.1796, -2.4929, -2.4927, -3.3140,
        -2.5178, -2.4926, -2.4927, -2.4933, -2.4928], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-5.0962e+00, -2.2064e-01, -2.1911e-01, -2.1850e-01,  4.5389e-01,
         -2.1841e-01, -2.2072e-01,  3.8885e+00, -1.4633e-01, -2.2035e-01,
         -2.2021e-01, -2.1724e-01, -2.1971e-01],
        [-2.2887e-01, -3.3748e-03, -3.3759e-03, -3.3779e-03, -1.1426e+01,
         -3.3784e-03, -3.3745e-03, -2.8214e-01, -3.5970e-03, -3.3749e-03,
         -3.3745e-03, -3.3780e-03, -3.3752e-03],
        [-1.7935e-01, -6.8075e-03, -6.8074e-03, -6.8082e-03, -1.3027e+01,
         -6.8084e-03, -6.8071e-03, -1.7596e-01, -6.9672e-03, -6.8076e-03,
         -6.8070e-03, -6.8078e-03, -6.8072e-03],
        [-1.9187e-01, -7.5793e-03, -7.5797e-03, -7.5813e-03, -1.3084e+01,
         -7.5817e-03, -7.5789e-03, -1.7234e-01, -7.8130e-03, -7.5795e-03,
         -7.5787e-03, -7.5810e-03, -7.5793e-03],
        [ 5.3203e+00,  2.2770e-01,  2.2722e-01,  2.2536e-01,  1.4674e+00,
          2.2496e-01,  2.2760e-01, -2.4965e+00,  1.5768e-01,  2.2795e-01,
          2.2787e-01,  2.2570e-01,  2.2742e-01]], device='cuda:0'))])
xi:  [941.7299]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1256.001069894321
W_T_median: 1218.109266137063
W_T_pctile_5: 944.4190456502064
W_T_CVAR_5_pct: 656.2580526342059
Average q (qsum/M+1):  53.114442886844756
Optimal xi:  [941.7299]
Expected(across Rb) median(across samples) p_equity:  1.775474738489417e-06
obj fun:  tensor(-2302.4879, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/constrain_factor/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/constrain_factor/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: Paper_FactorInv_Factor2
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
