Starting at: 
2022-06-30 10:23:25

 Random seed:  1  

tracing parameter entered from terminal:  10.0


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = basic_T30_VWD
timeseries_basket['basket_desc'] = CRSP data: T30 and VWD
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
5.0% of MC simulations done.
10.0% of MC simulations done.
15.0% of MC simulations done.
20.0% of MC simulations done.
25.0% of MC simulations done.
30.0% of MC simulations done.
35.0% of MC simulations done.
40.0% of MC simulations done.
45.0% of MC simulations done.
50.0% of MC simulations done.
55.00000000000001% of MC simulations done.
60.0% of MC simulations done.
65.0% of MC simulations done.
70.0% of MC simulations done.
75.0% of MC simulations done.
80.0% of MC simulations done.
85.0% of MC simulations done.
90.0% of MC simulations done.
95.0% of MC simulations done.
100.0% of MC simulations done.
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  hidden_layer    None       None         None   
4        obj.layers[4]        4  hidden_layer    None       None         None   
5        obj.layers[5]        5  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  
4    False        None  
5    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  hidden_layer       4  logistic_sigmoid   
4        obj.layers[4]        4  hidden_layer       4  logistic_sigmoid   
5        obj.layers[5]        5  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 4)    False        None  
4       (4, 4)    False        None  
5       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.986923996214
gradient value of function right now is: [ 1.34330430e-01  1.34147989e-01  8.32179476e-03  1.33733799e-01
 -1.34330430e-01 -1.34147989e-01 -8.32179476e-03 -1.33733799e-01
  1.07839243e-03  1.15257543e-03  1.22364889e-03  1.21676926e-03
  1.28619740e-03  1.37459744e-03  1.45934708e-03  1.45114519e-03
  7.10723381e-03  7.59403848e-03  8.06190785e-03  8.01665903e-03
  3.13836617e-03  3.35416876e-03  3.56098733e-03  3.54096990e-03
  4.41442266e-05  4.54158560e-04  4.48774958e-04  4.90066219e-05
  2.99783372e-05  2.96042426e-04  2.92315368e-04  3.30892187e-05
  1.14467296e-05  1.12359986e-04  1.10933179e-04  1.26319892e-05
 -3.56154381e-06 -3.21146182e-05 -3.16539778e-05 -3.87168366e-06
  3.26044553e-05  4.98069291e-05  2.78977508e-05  4.18568501e-05
  1.64155710e-05  2.61172013e-05  1.24644998e-05  2.18531217e-05
  2.02218751e-05  3.18562289e-05  1.68467165e-05  2.65576197e-05
  3.69045932e-05  5.76044077e-05  2.97914746e-05  4.82918381e-05
 -3.37355662e-07  3.73399680e-07 -1.33327173e-05  4.57739152e-06
  1.03833784e-05  2.04303538e-06  9.86878536e-07  1.28908067e-06
 -1.16748609e+00]
supnorm grad right now is: 1.1674860867305956
Weights right now are: 
[-1.76678082 -2.1414411  -0.67856539 -2.10149918  2.02753219  0.77761869
  0.37693405  1.85137523 -1.62391721 -0.98192876 -1.82263974 -1.97317274
 -1.32550657 -1.37635788 -1.72337617 -1.5103596   0.64906008  0.75747013
  0.42813442  1.0732335  -1.46098785 -1.98326759 -1.14784884 -0.78243254
 -1.15953257 -0.78027659 -0.86764238 -1.39066833 -0.80665301 -1.23495119
 -1.19212353 -0.6408704  -0.97612481 -2.04013263 -1.74804297 -0.91138531
 -0.45886913 -1.93069454 -1.59848628 -1.52455844  0.38730009  1.83001556
  0.28025306  0.97050597 -1.68956485 -0.78102773 -2.13907562 -1.6337154
 -1.34680038 -0.98160311 -1.48475815 -1.59789555  1.24327454  1.29457796
  0.34041928  0.70392644  0.82882718 -0.44366941 -1.32744738 -1.2295118
  0.55058996  0.88313791 -0.10583847 -0.97836386 24.90386919]
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.867136926205
gradient value of function right now is: [ 6.86505248e-02  6.85681990e-02  4.89540583e-03  6.83643788e-02
 -6.86505248e-02 -6.85681990e-02 -4.89540583e-03 -6.83643788e-02
  5.64074425e-04  6.01401853e-04  6.37882282e-04  6.34205215e-04
  6.68094495e-04  7.12268503e-04  7.55466899e-04  7.51113469e-04
  4.21045138e-03  4.48778684e-03  4.75975280e-03  4.73236517e-03
  1.63241493e-03  1.74039916e-03  1.84596257e-03  1.83532314e-03
  2.15875933e-05  2.21378824e-04  2.18872152e-04  2.43986582e-05
  1.45077867e-05  1.42877395e-04  1.41157203e-04  1.62976585e-05
  5.74136094e-06  5.60816785e-05  5.53982404e-05  6.44522349e-06
 -3.07699643e-06 -2.82627595e-05 -2.78853443e-05 -3.41366737e-06
  1.57730941e-05  2.40858805e-05  1.34101047e-05  2.02680710e-05
  7.56785705e-06  1.20368723e-05  5.71206637e-06  1.00837008e-05
  9.31695126e-06  1.46751337e-05  7.73416950e-06  1.22466714e-05
  1.76167339e-05  2.74861830e-05  1.39876855e-05  2.30897066e-05
 -3.20542523e-07 -1.36366104e-07 -6.39614178e-06  2.07182697e-06
  4.77309487e-06  9.67489720e-07  3.21425070e-07  5.28277289e-07
 -2.53097795e+00]
supnorm grad right now is: 2.530977947795098
Weights right now are: 
[-1.85701781 -2.25178736 -0.68565065 -2.23190853  2.11776918  0.88796495
  0.38401932  1.98178458 -1.63059409 -0.9891931  -1.82660089 -1.97701449
 -1.34355128 -1.39562508 -1.73426356 -1.52094763  0.59804822  0.701707
  0.39801575  1.04404431 -1.47539104 -1.99881189 -1.1564545  -0.79078809
 -1.16081748 -0.79269698 -0.87732488 -1.392903   -0.80739761 -1.24177182
 -1.19747442 -0.64216028 -0.97655879 -2.04402889 -1.75114086 -0.91213833
 -0.45874877 -1.92951464 -1.59764165 -1.52435489  0.38491174  1.8266996
  0.27749609  0.96693668 -1.69278742 -0.78499247 -2.1428218  -1.63866532
 -1.3531806  -0.98922991 -1.49299253 -1.60738207  1.23334713  1.28059519
  0.32999124  0.6889779   0.82912887 -0.4437183  -1.32164436 -1.23124792
  0.54676235  0.8824783  -0.10663898 -0.97888389 24.81668186]
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.04240252529
gradient value of function right now is: [ 2.86473722e-02  2.86184392e-02  2.53254844e-03  2.85374529e-02
 -2.86473722e-02 -2.86184392e-02 -2.53254844e-03 -2.85374529e-02
  2.41748045e-04  2.57010895e-04  2.72303110e-04  2.70670139e-04
  2.82428207e-04  3.00245657e-04  3.18107672e-04  3.16200598e-04
  2.19097182e-03  2.32859460e-03  2.46700278e-03  2.45223777e-03
  6.92525528e-04  7.36233812e-04  7.80037201e-04  7.75360033e-04
  8.37380615e-06  8.58182470e-05  8.49014187e-05  9.66864969e-06
  5.47250719e-06  5.39531728e-05  5.33404641e-05  6.27964296e-06
  2.29874392e-06  2.23820793e-05  2.21229477e-05  2.63338656e-06
 -2.24791114e-06 -2.08517203e-05 -2.05914923e-05 -2.54929498e-06
  5.95635279e-06  9.09933775e-06  4.98792531e-06  7.67056509e-06
  2.58120541e-06  4.10802239e-06  1.90757224e-06  3.44817736e-06
  3.20995844e-06  5.06107652e-06  2.61947484e-06  4.23058054e-06
  6.33713488e-06  9.89266807e-06  4.86789200e-06  8.33434832e-06
 -2.14940701e-07 -2.15122043e-07 -2.36961880e-06  6.82428815e-07
  1.60203769e-06  3.74728673e-07 -3.00469997e-09  1.49674824e-07
 -1.66321975e+00]
supnorm grad right now is: 1.6632197520778833
Weights right now are: 
[-1.97320255 -2.39388654 -0.69649505 -2.39986292  2.23395393  1.03006413
  0.39486371  2.14973897 -1.63940788 -0.9987574  -1.8318109  -1.98206655
 -1.36713597 -1.42074222 -1.74844231 -1.53473403  0.51944485  0.61600894
  0.35177543  0.99923917 -1.49426004 -2.01912285 -1.16768777 -0.80169291
 -1.16237786 -0.80777103 -0.88908329 -1.39567255 -0.80828614 -1.24991681
 -1.20386844 -0.64373076 -0.97710098 -2.04888601 -1.75500504 -0.91309764
 -0.45848354 -1.92687161 -1.59574774 -1.52389618  0.38205599  1.82272861
  0.27424231  0.96265726 -1.69639118 -0.7894307  -2.14695534 -1.64421316
 -1.36033602 -0.9977958  -1.50212272 -1.61804756  1.221775    1.26427594
  0.31814025  0.67149532  0.82979482 -0.44327303 -1.31479615 -1.23303877
  0.54251634  0.881648   -0.1070822  -0.97937907 24.87962393]
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.032630309435
gradient value of function right now is: [ 1.07621135e-02  1.07532637e-02  1.28180295e-03  1.07244420e-02
 -1.07621135e-02 -1.07532637e-02 -1.28180295e-03 -1.07244420e-02
  9.31862062e-05  9.87930249e-05  1.04563745e-04  1.03899720e-04
  1.06748972e-04  1.13167344e-04  1.19776814e-04  1.19016386e-04
  1.10635813e-03  1.17254535e-03  1.24095743e-03  1.23309399e-03
  2.63188640e-04  2.79019571e-04  2.95316870e-04  2.93441695e-04
  2.99928485e-06  3.07756043e-05  3.04650019e-05  3.53114454e-06
  1.84770663e-06  1.83119118e-05  1.81162859e-05  2.16283936e-06
  8.20185835e-07  7.98063429e-06  7.89281318e-06  9.57134751e-07
 -1.45920407e-06 -1.35978298e-05 -1.34374176e-05 -1.68659225e-06
  2.00398742e-06  3.06608530e-06  1.63818766e-06  2.58940507e-06
  6.93183813e-07  1.10625327e-06  4.84028212e-07  9.31510651e-07
  9.02000729e-07  1.42693537e-06  7.03180238e-07  1.19579163e-06
  1.87343792e-06  2.93164361e-06  1.35303795e-06  2.47910682e-06
 -1.24110071e-07 -1.42566998e-07 -7.72306914e-07  1.83142103e-07
  4.07065362e-07  1.34156472e-07 -8.54488409e-08  2.77991550e-08
  1.88813825e+00]
supnorm grad right now is: 1.8881382502033008
Weights right now are: 
[-2.10086734 -2.55005317 -0.71180851 -2.58446765  2.36161872  1.18623076
  0.41017717  2.3343437  -1.64934937 -1.00951431 -1.83766424 -1.98774083
 -1.39330571 -1.44853219 -1.76411294 -1.54996674  0.40813231  0.49500506
  0.28655535  0.93606124 -1.51528986 -2.04169459 -1.18015788 -0.81379501
 -1.16397985 -0.82320668 -0.90113072 -1.39857102 -0.80916253 -1.25794734
 -1.21017655 -0.64530978 -0.97767044 -2.05395803 -1.75904233 -0.91412345
 -0.45797621 -1.92180536 -1.59211494 -1.52300217  0.37924462  1.81882817
  0.27108467  0.95844275 -1.69945269 -0.79319695 -2.15036217 -1.64893566
 -1.36655174 -1.00523044 -1.50987005 -1.62732891  1.21126016  1.24947034
  0.30779146  0.65557145  0.83092131 -0.44222653 -1.30823408 -1.23462816
  0.53902462  0.88084648 -0.10646719 -0.97972053 25.13432453]
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.134496212107
gradient value of function right now is: [ 3.88316783e-03  3.88063192e-03  6.90496095e-04  3.87079939e-03
 -3.88316783e-03 -3.88063192e-03 -6.90496095e-04 -3.87079939e-03
  3.43846672e-05  3.63482229e-05  3.84336446e-05  3.81694375e-05
  3.84831043e-05  4.06793224e-05  4.30129580e-05  4.27173410e-05
  5.80461989e-04  6.13393712e-04  6.48540511e-04  6.44092957e-04
  9.54963174e-05  1.00948337e-04  1.06739837e-04  1.06006138e-04
  1.33153264e-06  1.34947822e-05  1.33622009e-05  1.58745157e-06
  7.73957717e-07  7.59069658e-06  7.51189634e-06  9.17496121e-07
  3.05262992e-07  2.95184023e-06  2.92047169e-06  3.61035604e-07
 -8.88619174e-07 -8.25340262e-06 -8.15963206e-06 -1.04114955e-06
  8.10842815e-07  1.24049025e-06  6.61252655e-07  1.04835852e-06
  2.00038958e-07  3.20380437e-07  1.29840227e-07  2.70677906e-07
  2.87034539e-07  4.55471349e-07  2.14464577e-07  3.82462749e-07
  6.43171100e-07  1.00819317e-06  4.41621331e-07  8.55044038e-07
 -7.68806948e-08 -8.55646542e-08 -3.09007614e-07  6.94463737e-08
  1.04934749e-07  5.03783997e-08 -7.58187659e-08  2.59016020e-09
  9.08314446e-01]
supnorm grad right now is: 0.9083144462194178
Weights right now are: 
[-2.229989   -2.70802727 -0.7336386  -2.77123231  2.49074037  1.34420485
  0.43200727  2.52110836 -1.65965259 -1.02063246 -1.84370828 -1.99359749
 -1.41985416 -1.47664825 -1.77995216 -1.56535674  0.25132438  0.32501506
  0.19501855  0.84742896 -1.53675212 -2.06466824 -1.19283778 -0.82609543
 -1.16565982 -0.83939754 -0.91377445 -1.40166546 -0.8100216  -1.26585467
 -1.2163918  -0.64688641 -0.97822863 -2.05893758 -1.76300831 -0.91514659
 -0.45710411 -1.91305378 -1.58583551 -1.52143866  0.37652506  1.8150442
  0.26810278  0.95435008 -1.70165395 -0.79592112 -2.15262664 -1.65236026
 -1.37134903 -1.01100626 -1.51549993 -1.63454825  1.20263316  1.2372639
  0.29989753  0.64240705  0.83272999 -0.44047487 -1.30207609 -1.23584637
  0.53670732  0.88003593 -0.10414324 -0.97985729 25.02907398]
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.029771183108
gradient value of function right now is: [ 1.33166380e-03  1.33098744e-03  3.96792811e-04  1.32780920e-03
 -1.33166380e-03 -1.33098744e-03 -3.96792810e-04 -1.32780920e-03
  1.19921814e-05  1.26254336e-05  1.33303447e-05  1.32287874e-05
  1.30988523e-05  1.37901541e-05  1.45600154e-05  1.44491123e-05
  3.02886013e-04  3.18761917e-04  3.36533544e-04  3.33976395e-04
  3.27078001e-05  3.44345773e-05  3.63570807e-05  3.60801161e-05
  9.51040449e-07  9.36701943e-06  9.27643479e-06  1.15068625e-06
  5.85461524e-07  5.53399347e-06  5.47653276e-06  7.02715210e-07
  1.50667987e-07  1.43072124e-06  1.41598903e-06  1.81160364e-07
 -4.64819877e-07 -4.28705889e-06 -4.24063493e-06 -5.54770611e-07
  5.82900256e-07  8.88879379e-07  4.95823099e-07  7.50500795e-07
  1.76545170e-07  2.81070985e-07  1.30554243e-07  2.36375622e-07
  2.35226416e-07  3.70727179e-07  1.95178470e-07  3.10212766e-07
  5.50720710e-07  8.58427160e-07  4.14307690e-07  7.26156843e-07
 -5.35069490e-08 -6.38635397e-08 -2.33719337e-07  6.49821310e-08
  1.03808304e-07  2.45578721e-08 -3.81063791e-08  1.83593872e-09
  1.95537083e+00]
supnorm grad right now is: 1.955370827997679
Weights right now are: 
[-2.35387067 -2.85961173 -0.76666318 -2.950465    2.61462205  1.49578932
  0.46503184  2.70034105 -1.66973821 -1.03147989 -1.84959857 -1.99930161
 -1.44522967 -1.50343366 -1.79502506 -1.5799931   0.02658295  0.08220736
  0.06441057  0.72104415 -1.55739907 -2.08669636 -1.20498223 -0.83786914
 -1.16820986 -0.86334514 -0.93247791 -1.40641602 -0.81131607 -1.27742021
 -1.22548287 -0.64928671 -0.97885828 -2.06447159 -1.76741724 -0.9163155
 -0.45574024 -1.89948349 -1.57609462 -1.51896083  0.3726401   1.80965099
  0.26372124  0.94852085 -1.70440987 -0.79933112 -2.15553208 -1.65663469
 -1.37753746 -1.01846631 -1.52293607 -1.64380817  1.19063538  1.22031295
  0.28865955  0.62418202  0.83588263 -0.43743581 -1.29317766 -1.23791706
  0.53389782  0.87914619 -0.10010882 -0.97993322 25.12904413]
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.090189150946
gradient value of function right now is: [ 4.30763843e-04  4.30596718e-04  2.18682145e-04  4.29638026e-04
 -4.30763851e-04 -4.30596726e-04 -2.18682149e-04 -4.29638034e-04
  3.90370485e-06  4.07784843e-06  4.29113386e-06  4.25501235e-06
  4.16815861e-06  4.35400500e-06  4.58171418e-06  4.54315337e-06
  1.28464939e-04  1.34150897e-04  1.41157173e-04  1.39972088e-04
  1.04537525e-05  1.09199971e-05  1.14911319e-05  1.13944099e-05
  6.95136020e-07  6.62458963e-06  6.56624756e-06  8.86344866e-07
  4.74474805e-07  4.31519689e-06  4.27371289e-06  5.98437411e-07
  9.36022265e-08  8.58905422e-07  8.50788274e-07  1.18402928e-07
 -1.55291806e-07 -1.40261213e-06 -1.38895117e-06 -1.95461575e-07
  4.60881843e-07  7.00896134e-07  4.03166569e-07  5.92606749e-07
  1.87132077e-07  2.96940461e-07  1.49270487e-07  2.49414650e-07
  2.23957673e-07  3.51305279e-07  1.99598404e-07  2.93688908e-07
  5.59438408e-07  8.68701358e-07  4.36410657e-07  7.36025799e-07
 -3.48269801e-08 -5.86864182e-08 -1.91023665e-07  5.53331088e-08
  1.23449212e-07  1.20414502e-08 -1.81814798e-09  9.27394482e-10
 -1.47354353e+00]
supnorm grad right now is: 1.473543532869529
Weights right now are: 
[-2.46602896 -2.99686731 -0.81788148 -3.11277666  2.72678033  1.6330449
  0.51625015  2.86265271 -1.67898626 -1.04137231 -1.8549589  -2.00448838
 -1.46795099 -1.5272871  -1.8084205  -1.59299018 -0.26880699 -0.23516981
 -0.10596605  0.55630657 -1.57599022 -2.10642328 -1.21583493 -0.84838211
 -1.17354433 -0.91201871 -0.97051907 -1.41666819 -0.81428457 -1.30299744
 -1.24559813 -0.65495346 -0.97983567 -2.07285623 -1.77410294 -0.91818641
 -0.45411766 -1.8834749  -1.56459189 -1.5159188   0.36406805  1.79775589
  0.25373449  0.93569692 -1.71265023 -0.80959465 -2.16498376 -1.66921267
 -1.39344721 -1.03803278 -1.54257583 -1.66707148  1.1579254   1.17389094
  0.25663179  0.5753952   0.84080734 -0.4313372  -1.2744776  -1.24277635
  0.52472517  0.87792436 -0.09750105 -0.98008738 24.89263422]
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.143726815648
gradient value of function right now is: [ 1.42469157e-04  1.42427449e-04  1.01667166e-04  1.42134039e-04
 -1.42469169e-04 -1.42427461e-04 -1.01667175e-04 -1.42134051e-04
  1.28627543e-06  1.33014661e-06  1.39244344e-06  1.38005622e-06
  1.34609500e-06  1.39198543e-06  1.45717364e-06  1.44421247e-06
  3.91715870e-05  4.04980283e-05  4.23923796e-05  4.20160988e-05
  3.38501366e-06  3.50044236e-06  3.66437900e-06  3.63178284e-06
  3.06573729e-07  2.83744690e-06  2.81590152e-06  4.27811435e-07
  2.26805633e-07  1.99882737e-06  1.98200206e-06  3.12015586e-07
  4.34031650e-08  3.85784419e-07  3.82595739e-07  5.98984163e-08
 -2.53478727e-08 -2.21741616e-07 -2.19845258e-07 -3.47755455e-08
  2.17795048e-07  3.31324630e-07  1.89277721e-07  2.81248780e-07
  9.76284247e-08  1.54929118e-07  7.92550054e-08  1.30465310e-07
  1.10193520e-07  1.72713257e-07  1.00531540e-07  1.44718804e-07
  3.13596219e-07  4.86582718e-07  2.36446898e-07  4.14861251e-07
 -1.68852319e-08 -4.18173946e-08 -8.91018096e-08  2.03682283e-08
  6.86076506e-08  4.39609224e-09  7.84070162e-09 -1.91629402e-09
  8.21974738e-01]
supnorm grad right now is: 0.8219747381306683
Weights right now are: 
[-2.56488594 -3.1178568  -0.88895943 -3.2558745   2.82563732  1.75403439
  0.58732809  3.00575055 -1.68714633 -1.05002026 -1.85962524 -2.00900056
 -1.48755994 -1.54768316 -1.81983135 -1.60405428 -0.56077781 -0.54605814
 -0.27220911  0.39567005 -1.59210244 -2.12336198 -1.22511413 -0.85736475
 -1.1818976  -0.98574729 -1.02827239 -1.4338224  -0.81937327 -1.34524588
 -1.27888597 -0.66531628 -0.9813022  -2.08498879 -1.78380426 -0.92117063
 -0.4530098  -1.87285272 -1.55694101 -1.51370765  0.34993942  1.7781252
  0.2373102   0.91471023 -1.72708177 -0.82837826 -2.18039032 -1.69050243
 -1.41616879 -1.06825652 -1.56755299 -1.69882496  1.10359771  1.09480049
  0.20785667  0.49771346  0.84531269 -0.42271361 -1.25258283 -1.24839929
  0.5102955   0.87693548 -0.09837372 -0.9799539  25.03716   ]
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.342749398844
gradient value of function right now is: [ 5.13651645e-05  5.13540830e-05  4.28684450e-05  5.12555359e-05
 -5.13651539e-05 -5.13540724e-05 -4.28684361e-05 -5.12555253e-05
  4.61825394e-07  4.73845341e-07  4.93595374e-07  4.89078147e-07
  4.74162852e-07  4.86498866e-07  5.06775098e-07  5.02137801e-07
  1.08496588e-05  1.11303228e-05  1.15937933e-05  1.14878723e-05
  1.19542812e-06  1.22653464e-06  1.27765551e-06  1.26596360e-06
  1.04065914e-07  9.39006963e-07  9.32379433e-07  1.53461326e-07
  8.15920275e-08  7.00416777e-07  6.94890061e-07  1.18361639e-07
  1.62551683e-08  1.40663334e-07  1.39573338e-07  2.36560238e-08
 -7.07236318e-10 -5.23896087e-09 -5.18326069e-09 -9.77452368e-10
  7.79046543e-08  1.18658923e-07  6.67605210e-08  1.01006735e-07
  3.63018504e-08  5.76526551e-08  2.96056886e-08  4.86345283e-08
  3.98680747e-08  6.25079737e-08  3.66458769e-08  5.24622020e-08
  1.22894319e-07  1.90705184e-07  8.97045268e-08  1.63300275e-07
 -5.92927897e-09 -1.86632153e-08 -3.08431793e-08  5.34071807e-09
  2.64842375e-08  1.53100150e-09  4.88968257e-09 -1.21629975e-09
  4.91159454e+00]
supnorm grad right now is: 4.911594541221666
Weights right now are: 
[-2.65881302 -3.2328207  -0.97554881 -3.39186437  2.91956439  1.86899829
  0.67391747  3.14174042 -1.69485487 -1.0581127  -1.86397262 -2.01320288
 -1.50563771 -1.56631025 -1.83024097 -1.61414464 -0.79210706 -0.79009837
 -0.40218089  0.27012449 -1.60707228 -2.13895179 -1.2336134  -0.86558947
 -1.19047821 -1.0595765  -1.08663305 -1.45223817 -0.8250044  -1.39076503
 -1.31499463 -0.67735157 -0.98290554 -2.09794217 -1.79429336 -0.92451127
 -0.45265481 -1.86958194 -1.55456533 -1.51296209  0.3360718   1.75848607
  0.22251435  0.89472795 -1.73790393 -0.84399008 -2.19020793 -1.70559451
 -1.42968816 -1.08829594 -1.58061955 -1.71692127  1.06503119  1.03631642
  0.17763248  0.4452182   0.84752343 -0.41679622 -1.24133052 -1.2506756
  0.50142459  0.87641708 -0.09970929 -0.97962579 25.33195422]
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.05547046349
gradient value of function right now is: [ 1.95986673e-05  1.95956763e-05  1.75697378e-05  1.95603056e-05
 -1.95986715e-05 -1.95956805e-05 -1.75697415e-05 -1.95603097e-05
  1.76144660e-07  1.79879250e-07  1.86685440e-07  1.84948769e-07
  1.77637140e-07  1.81401994e-07  1.88265467e-07  1.86514268e-07
  3.15799767e-06  3.22460677e-06  3.34652636e-06  3.31543633e-06
  4.48773567e-07  4.58286164e-07  4.75626096e-07  4.71201792e-07
  3.46208458e-08  3.06045188e-07  3.03911750e-07  5.18627142e-08
  2.81748774e-08  2.36865772e-07  2.35014929e-07  4.14879105e-08
  5.91112761e-09  5.00770073e-08  4.96925111e-08  8.73141112e-09
  1.51166680e-09  1.30883857e-08  1.29925745e-08  2.24796713e-09
  2.68042740e-08  4.08055198e-08  2.29631761e-08  3.47690892e-08
  1.28842854e-08  2.04546925e-08  1.05999643e-08  1.72610345e-08
  1.39895573e-08  2.19137863e-08  1.29739221e-08  1.83993404e-08
  4.38739243e-08  6.80459321e-08  3.18617047e-08  5.83465905e-08
 -1.82992155e-09 -6.74508521e-09 -1.04019272e-08  1.70274521e-09
  9.62002612e-09  4.14943638e-10  2.18369082e-09 -4.73667638e-10
 -1.79469345e+00]
supnorm grad right now is: 1.7946934519618007
Weights right now are: 
[-2.75355001 -3.34877697 -1.07324321 -3.52903552  3.01430139  1.98495458
  0.77161189  3.27891159 -1.70254533 -1.06613184 -1.86828931 -2.01737511
 -1.522771   -1.58385106 -1.84028432 -1.62388295 -0.96760916 -0.97396925
 -0.50041594  0.17524276 -1.6218213  -2.15421154 -1.24192487 -0.87363121
 -1.19718985 -1.1167379  -1.13381907 -1.46538737 -0.82977861 -1.42892074
 -1.34629723 -0.68680938 -0.9841957  -2.10836427 -1.80319345 -0.92688841
 -0.45274931 -1.87048011 -1.55527705 -1.51314382  0.32782046  1.74632451
  0.2147898   0.88354304 -1.74252288 -0.85116736 -2.19409195 -1.71183277
 -1.43484791 -1.09630015 -1.58545517 -1.72372597  1.04909876  1.01169561
  0.16584748  0.42394193  0.84826514 -0.41433138 -1.23736096 -1.25136431
  0.49792704  0.87625767 -0.10041922 -0.97945622 24.83351987]
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.151922189138
gradient value of function right now is: [ 7.37653657e-06  7.37573604e-06  6.85290060e-06  7.36315084e-06
 -7.37653785e-06 -7.37573732e-06 -6.85290179e-06 -7.36315212e-06
  6.66110205e-08  6.78792381e-08  7.02951764e-08  6.96377832e-08
  6.63951700e-08  6.76588542e-08  7.00668474e-08  6.94116458e-08
  9.52418371e-07  9.70471612e-07  1.00499122e-06  9.95602933e-07
  1.67163152e-07  1.70345062e-07  1.76407773e-07  1.74758126e-07
  1.15800383e-08  1.01813402e-07  1.01110309e-07  1.74464860e-08
  9.58968401e-09  8.01674510e-08  7.95459536e-08  1.41976518e-08
  2.10326539e-09  1.77146938e-08  1.75797068e-08  3.12345598e-09
  9.22665764e-10  7.86936933e-09  7.81099478e-09  1.37496087e-09
  9.16835687e-09  1.39632972e-08  7.82291919e-09  1.19030540e-08
  4.51748701e-09  7.17365476e-09  3.71649554e-09  6.05504627e-09
  4.87850040e-09  7.64366351e-09  4.52547356e-09  6.41923646e-09
  1.53028012e-08  2.37370049e-08  1.10549794e-08  2.03655894e-08
 -5.68450676e-10 -2.34009267e-09 -3.51331335e-09  5.30177217e-10
  3.40270239e-09  1.52138212e-10  8.52874533e-10 -1.61253001e-10
 -6.89737985e-01]
supnorm grad right now is: 0.6897379847097451
Weights right now are: 
[-2.85131844 -3.46840309 -1.17952533 -3.67049512  3.11206984  2.10458071
  0.87789403  3.42037121 -1.71005765 -1.07392289 -1.87265203 -2.02159475
 -1.53683352 -1.59819819 -1.84966172 -1.63300215 -1.09942659 -1.11120647
 -0.57718305  0.10103456 -1.6363555  -2.16918715 -1.25025655 -0.8816952
 -1.20087603 -1.14856992 -1.16279427 -1.47147974 -0.83265536 -1.45233365
 -1.36730612 -0.69159215 -0.98488346 -2.11405114 -1.80851387 -0.92797464
 -0.45295612 -1.87229383 -1.55688216 -1.51348564  0.32448259  1.74128684
  0.21187966  0.8791792  -1.74419106 -0.85380249 -2.19547252 -1.7140678
 -1.43666151 -1.09913137 -1.58714533 -1.72610902  1.04341552  1.00290011
  0.16171224  0.41638513  0.84849175 -0.41345687 -1.23603109 -1.2515847
  0.49667659  0.87621096 -0.10071477 -0.97939368 24.97884187]
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.114586444708
gradient value of function right now is: [ 2.77433379e-06  2.77408326e-06  2.62283232e-06  2.76960550e-06
 -2.77433274e-06 -2.77408222e-06 -2.62283133e-06 -2.76960445e-06
  2.53420051e-08  2.58044354e-08  2.66980343e-08  2.64483063e-08
  2.52754348e-08  2.57365005e-08  2.66277114e-08  2.63786616e-08
  3.13321479e-07  3.19016363e-07  3.30057881e-07  3.26973553e-07
  6.27373767e-08  6.38819374e-08  6.60940947e-08  6.54758985e-08
  4.10196196e-09  3.59782840e-08  3.57304259e-08  6.19142168e-09
  3.41816705e-09  2.85042129e-08  2.82836639e-08  5.06947813e-09
  7.74822497e-10  6.50903004e-09  6.45951651e-09  1.15261590e-09
  4.21625986e-10  3.57822074e-09  3.55158085e-09  6.28851108e-10
  3.28539329e-09  5.00355635e-09  2.80128775e-09  4.26594764e-09
  1.64168723e-09  2.60687904e-09  1.35152144e-09  2.20052998e-09
  1.76892558e-09  2.77134627e-09  1.64202790e-09  2.32757900e-09
  5.54120493e-09  8.59474328e-09  3.99943561e-09  7.37536862e-09
 -1.93733218e-10 -8.41677620e-10 -1.25369603e-09  1.85228973e-10
  1.24153478e-09  5.45332336e-11  3.24938358e-10 -5.72903595e-11
 -1.25670069e+00]
supnorm grad right now is: 1.2567006870676662
Weights right now are: 
[-2.95166074 -3.59084039 -1.29128342 -3.81480814  3.21241214  2.22701801
  0.98965212  3.56468424 -1.71599728 -1.08003399 -1.87671928 -2.02554549
 -1.544784   -1.60629714 -1.85651332 -1.63972166 -1.18281465 -1.19714623
 -0.63527963  0.04459983 -1.64876119 -2.18191428 -1.25823844 -0.88944119
 -1.20235192 -1.16144857 -1.17534045 -1.47374246 -0.83386882 -1.46237784
 -1.37702817 -0.69343102 -0.98515902 -2.11635528 -1.81077469 -0.92838829
 -0.45308655 -1.87340591 -1.55795697 -1.51368404  0.323281    1.73945791
  0.21084764  0.87761768 -1.74478893 -0.85475207 -2.19596716 -1.71486911
 -1.43730724 -1.10014342 -1.58774726 -1.72695846  1.04139392  0.99976294
  0.16024445  0.41369506  0.84856472 -0.41315042 -1.23556636 -1.25165609
  0.49622255  0.87619354 -0.10083085 -0.9793721  24.90197245]
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.763579170121
gradient value of function right now is: [ 1.06800557e-06  1.06791329e-06  1.01727158e-06  1.06625954e-06
 -1.06800658e-06 -1.06791430e-06 -1.01727255e-06 -1.06626055e-06
  9.94952031e-09  1.01285671e-08  1.04757100e-08  1.03777087e-08
  9.99140708e-09  1.01711497e-08  1.05197384e-08  1.04213331e-08
  1.16565770e-07  1.18655707e-07  1.22720446e-07  1.21573414e-07
  2.43937180e-08  2.48326088e-08  2.56836946e-08  2.54434335e-08
  1.56999009e-09  1.38030490e-08  1.37087036e-08  2.37410607e-09
  1.31004139e-09  1.09501917e-08  1.08660762e-08  1.94643533e-09
  3.02906723e-10  2.55039482e-09  2.53113204e-09  4.51402189e-10
  1.78807383e-10  1.51976252e-09  1.50850935e-09  2.67091037e-10
  1.26643838e-09  1.93019684e-09  1.07427901e-09  1.64594987e-09
  6.37263157e-10  1.01242928e-09  5.22518026e-10  8.54780255e-10
  6.85390020e-10  1.07456558e-09  6.33875850e-10  9.02624427e-10
  2.14864388e-09  3.33396905e-09  1.54256042e-09  2.86168466e-09
 -7.32883002e-11 -3.26985257e-10 -4.81222634e-10  6.54843785e-11
  4.81553731e-10  2.50949381e-11  1.28312872e-10 -2.13475405e-11
  3.42170437e+00]
supnorm grad right now is: 3.4217043716342297
Weights right now are: 
[-3.05211006e+00 -3.71123720e+00 -1.40285922e+00 -3.95388172e+00
  3.31286146e+00  2.34741483e+00  1.10122792e+00  3.70375781e+00
 -1.71918042e+00 -1.08328404e+00 -1.87956217e+00 -2.02833527e+00
 -1.54818976e+00 -1.60976453e+00 -1.85995092e+00 -1.64311849e+00
 -1.22167074e+00 -1.23682631e+00 -6.70585576e-01  9.93230913e-03
 -1.65616720e+00 -2.18947386e+00 -1.26441070e+00 -8.95482375e-01
 -1.20290581e+00 -1.16629102e+00 -1.18013588e+00 -1.47457974e+00
 -8.34330406e-01 -1.46621132e+00 -1.38081755e+00 -6.94117078e-01
 -9.85264852e-01 -2.11724109e+00 -1.81165231e+00 -9.28545774e-01
 -4.53146145e-01 -1.87391029e+00 -1.55845557e+00 -1.51377308e+00
  3.22835479e-01  1.73878047e+00  2.10466089e-01  8.77039671e-01
 -1.74501214e+00 -8.55105954e-01 -2.19615168e+00 -1.71516800e+00
 -1.43754777e+00 -1.10051958e+00 -1.58797165e+00 -1.72727456e+00
  1.04064084e+00  9.98596356e-01  1.59699144e-01  4.12693528e-01
  8.48590649e-01 -4.13036060e-01 -1.23539684e+00 -1.25168120e+00
  4.96054161e-01  8.76186747e-01 -1.00875170e-01 -9.79364145e-01
  2.52571928e+01]
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.927374314737
gradient value of function right now is: [ 4.84738079e-07  4.84696608e-07  4.63130744e-07  4.83962380e-07
 -4.84738220e-07 -4.84696748e-07 -4.63130879e-07 -4.83962520e-07
  4.61668092e-09  4.69920488e-09  4.85955735e-09  4.81410732e-09
  4.65808638e-09  4.74132363e-09  4.90310701e-09  4.85725338e-09
  5.37014635e-08  5.46579471e-08  5.65221570e-08  5.59939821e-08
  1.12357188e-08  1.14365212e-08  1.18267654e-08  1.17161586e-08
  7.28797193e-10  6.40289806e-09  6.35911999e-09  1.10227035e-09
  6.08277869e-10  5.08083311e-09  5.04179309e-09  9.03925979e-10
  1.41805883e-10  1.19309436e-09  1.18407988e-09  2.11358487e-10
  8.58263266e-11  7.28736617e-10  7.23335242e-10  1.28209883e-10
  5.88865933e-10  8.97443005e-10  4.99694164e-10  7.65282671e-10
  2.96962455e-10  4.71771578e-10  2.43618312e-10  3.98302115e-10
  3.19307160e-10  5.00581937e-10  2.95434194e-10  4.20478666e-10
  1.00064591e-09  1.55260846e-09  7.18677065e-10  1.33266350e-09
 -3.37853362e-11 -1.52003897e-10 -2.23696413e-10  3.05659683e-11
  2.24611775e-10  1.14884267e-11  6.02327510e-11 -9.93655617e-12
  2.58828046e+00]
supnorm grad right now is: 2.5882804634058627
Weights right now are: 
[-3.14248753e+00 -3.81269576e+00 -1.49825917e+00 -4.06398418e+00
  3.40323894e+00  2.44887339e+00  1.19662786e+00  3.81386027e+00
 -1.72059755e+00 -1.08472727e+00 -1.88100800e+00 -2.02976455e+00
 -1.54963037e+00 -1.61123099e+00 -1.86145821e+00 -1.64461111e+00
 -1.23823697e+00 -1.25369697e+00 -6.87564644e-01 -6.85682707e-03
 -1.65959759e+00 -2.19296746e+00 -1.26783650e+00 -8.98864913e-01
 -1.20313184e+00 -1.16827954e+00 -1.18211010e+00 -1.47492194e+00
 -8.34518971e-01 -1.46778859e+00 -1.38238195e+00 -6.94397605e-01
 -9.85308633e-01 -2.11760999e+00 -1.81201835e+00 -9.28611086e-01
 -4.53172285e-01 -1.87413262e+00 -1.55867614e+00 -1.51381218e+00
  3.22652931e-01  1.73850188e+00  2.10311372e-01  8.76802179e-01
 -1.74510419e+00 -8.55252359e-01 -2.19622708e+00 -1.71529158e+00
 -1.43764672e+00 -1.10067490e+00 -1.58806305e+00 -1.72740500e+00
  1.04033071e+00  9.98114511e-01  1.59476649e-01  4.12280066e-01
  8.48601221e-01 -4.12989049e-01 -1.23532735e+00 -1.25169040e+00
  4.95984474e-01  8.76182809e-01 -1.00893785e-01 -9.79361139e-01
  2.51505279e+01]
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.164207680293
gradient value of function right now is: [ 2.81539821e-07  2.81515835e-07  2.69399007e-07  2.81094395e-07
 -2.81539853e-07 -2.81515867e-07 -2.69399037e-07 -2.81094427e-07
  2.72821868e-09  2.77677060e-09  2.87129802e-09  2.84445394e-09
  2.75712117e-09  2.80617160e-09  2.90169616e-09  2.87457008e-09
  3.17495202e-08  3.23125394e-08  3.34120055e-08  3.30999014e-08
  6.59628763e-09  6.71365473e-09  6.94219791e-09  6.87729771e-09
  4.32254006e-10  3.78984084e-09  3.76382989e-09  6.53422380e-10
  3.60822180e-10  3.00765540e-09  2.98446163e-09  5.35912008e-10
  8.44439307e-11  7.09005790e-10  7.03629235e-10  1.25794831e-10
  5.16148850e-11  4.37286005e-10  4.34031845e-10  7.70589879e-11
  3.49144478e-10  5.31867381e-10  2.97097429e-10  4.53517679e-10
  1.76187486e-10  2.79817374e-10  1.44905313e-10  2.36220272e-10
  1.89504084e-10  2.96954009e-10  1.75750683e-10  2.49421553e-10
  5.93365768e-10  9.20444864e-10  4.27365472e-10  7.89982534e-10
 -1.99257967e-11 -8.98462575e-11 -1.32792344e-10  1.88970123e-11
  1.33476314e-10  6.16801163e-12  3.58899556e-11 -5.99050603e-12
 -3.08161653e-01]
supnorm grad right now is: 0.3081616527055765
Weights right now are: 
[-3.21011540e+00 -3.88272293e+00 -1.56494496e+00 -4.13544379e+00
  3.47086682e+00  2.51890056e+00  1.26331367e+00  3.88531990e+00
 -1.72132563e+00 -1.08546840e+00 -1.88177104e+00 -2.03052022e+00
 -1.55036652e+00 -1.61198029e+00 -1.86223243e+00 -1.64537805e+00
 -1.24670960e+00 -1.26232091e+00 -6.96449502e-01 -1.56563513e-02
 -1.66136229e+00 -2.19476378e+00 -1.26967985e+00 -9.00690095e-01
 -1.20324702e+00 -1.16929197e+00 -1.18311557e+00 -1.47509615e+00
 -8.34615091e-01 -1.46859184e+00 -1.38317898e+00 -6.94540452e-01
 -9.85331081e-01 -2.11779895e+00 -1.81220588e+00 -9.28644545e-01
 -4.53185932e-01 -1.87424855e+00 -1.55879120e+00 -1.51383256e+00
  3.22559758e-01  1.73835999e+00  2.10232412e-01  8.76681155e-01
 -1.74515118e+00 -8.55326995e-01 -2.19626558e+00 -1.71535460e+00
 -1.43769724e+00 -1.10075408e+00 -1.58810975e+00 -1.72747151e+00
  1.04017234e+00  9.97868893e-01  1.59363054e-01  4.12069235e-01
  8.48606552e-01 -4.12965098e-01 -1.23529204e+00 -1.25169521e+00
  4.95948955e-01  8.76180952e-01 -1.00903333e-01 -9.79359608e-01
  2.49806815e+01]
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.098495855667
gradient value of function right now is: [ 1.94176834e-07  1.94160334e-07  1.85978272e-07  1.93871820e-07
 -1.94177309e-07 -1.94160809e-07 -1.85978727e-07 -1.93872294e-07
  1.90424098e-09  1.93808276e-09  2.00397249e-09  1.98523519e-09
  1.92566405e-09  1.95987551e-09  2.02650353e-09  2.00755710e-09
  2.21573973e-08  2.25497945e-08  2.33160673e-08  2.30982455e-08
  4.58197834e-09  4.66339346e-09  4.82193355e-09  4.77685025e-09
  3.01714308e-10  2.64827293e-09  2.63014773e-09  4.56337758e-10
  2.51794484e-10  2.10121259e-09  2.08505133e-09  3.74179793e-10
  5.90794148e-11  4.96590806e-10  4.92834857e-10  8.80565660e-11
  3.63385498e-11  3.08183042e-10  3.05895292e-10  5.42795550e-11
  2.43994457e-10  3.71799286e-10  2.07222053e-10  3.17045400e-10
  1.23227954e-10  1.95749488e-10  1.01183678e-10  1.65260778e-10
  1.32488805e-10  2.07673735e-10  1.22686193e-10  1.74439350e-10
  4.15038704e-10  6.43927283e-10  2.98338745e-10  5.52699283e-10
 -1.39066195e-11 -6.29414038e-11 -9.26865218e-11  1.28092880e-11
  9.32932956e-11  4.61295617e-12  2.51353012e-11 -4.13514122e-12
  1.45051854e+00]
supnorm grad right now is: 1.450518538984638
Weights right now are: 
[-3.25681973e+00 -3.92969167e+00 -1.60988197e+00 -4.18249998e+00
  3.51757114e+00  2.56586930e+00  1.30825067e+00  3.93237608e+00
 -1.72178857e+00 -1.08593958e+00 -1.88225797e+00 -2.03100258e+00
 -1.55083459e+00 -1.61245669e+00 -1.86272498e+00 -1.64586599e+00
 -1.25209710e+00 -1.26780399e+00 -7.02116308e-01 -2.12700226e-02
 -1.66247902e+00 -2.19590038e+00 -1.27085392e+00 -9.01853114e-01
 -1.20332031e+00 -1.16993568e+00 -1.18375488e+00 -1.47520707e+00
 -8.34676253e-01 -1.46910259e+00 -1.38368582e+00 -6.94631394e-01
 -9.85345412e-01 -2.11791949e+00 -1.81232551e+00 -9.28665917e-01
 -4.53194715e-01 -1.87432310e+00 -1.55886520e+00 -1.51384569e+00
  3.22500523e-01  1.73826962e+00  2.10182123e-01  8.76604107e-01
 -1.74518111e+00 -8.55374603e-01 -2.19629014e+00 -1.71539479e+00
 -1.43772942e+00 -1.10080457e+00 -1.58813952e+00 -1.72751392e+00
  1.04007156e+00  9.97712336e-01  1.59290632e-01  4.11934887e-01
  8.48609960e-01 -4.12949840e-01 -1.23526953e+00 -1.25169822e+00
  4.95926288e-01  8.76179695e-01 -1.00909432e-01 -9.79358645e-01
  2.51269770e+01]
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.168233306256
gradient value of function right now is: [ 1.48197272e-07  1.48184704e-07  1.42037233e-07  1.47965725e-07
 -1.48198750e-07 -1.48186182e-07 -1.42038650e-07 -1.47967201e-07
  1.46602373e-09  1.49202118e-09  1.54268859e-09  1.52826728e-09
  1.48313826e-09  1.50943072e-09  1.56068733e-09  1.54609894e-09
  1.70479829e-08  1.73492465e-08  1.79381321e-08  1.77705851e-08
  3.51494851e-09  3.57726892e-09  3.69874675e-09  3.66417189e-09
  2.32549169e-10  2.03892960e-09  2.02494525e-09  3.51619019e-10
  1.94087421e-10  1.61784460e-09  1.60537619e-09  2.88334820e-10
  4.56256084e-11  3.83077502e-10  3.80174236e-10  6.79828864e-11
  2.81985605e-11  2.38867061e-10  2.37089989e-10  4.21067046e-11
  1.88023468e-10  2.86432145e-10  1.59926425e-10  2.44245064e-10
  9.49868128e-11  1.50857186e-10  7.81009982e-11  1.27356125e-10
  1.02144097e-10  1.60063647e-10  9.47067463e-11  1.34445873e-10
  3.19837184e-10  4.96139335e-10  2.30256410e-10  4.25833219e-10
 -1.06874715e-11 -4.84221379e-11 -7.14698130e-11  1.01042850e-11
  7.19673232e-11  3.36111106e-12  1.94131391e-11 -3.21837164e-12
 -7.80814375e-02]
supnorm grad right now is: 0.07808143751468324
Weights right now are: 
[-3.29143635 -3.96433227 -1.64306992 -4.21710478  3.55218775  2.6005099
  1.34143862  3.96698088 -1.72213019 -1.08628728 -1.88261747 -2.03135871
 -1.55118013 -1.61280837 -1.86308862 -1.64622622 -1.25607097 -1.27184835
 -0.70629781 -0.02541242 -1.66329969 -2.19673566 -1.27171748 -0.90270858
 -1.20337431 -1.17041125 -1.18422722 -1.47528885 -0.83472128 -1.46947976
 -1.38406011 -0.69469841 -0.98535599 -2.11800871 -1.81241406 -0.9286817
 -0.45320123 -1.87437859 -1.55892028 -1.51385544  0.32245674  1.73820285
  0.21014517  0.87654717 -1.74520324 -0.85540979 -2.1963082  -1.71542449
 -1.43775319 -1.10084188 -1.58816139 -1.72754526  1.03999702  0.99759661
  0.15923735  0.41183556  0.84861247 -0.41293856 -1.23525292 -1.25170044
  0.49590954  0.87617876 -0.10091396 -0.97935794 25.02465005]
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.672971057196
gradient value of function right now is: [ 1.19169370e-07  1.19159278e-07  1.14275907e-07  1.18983931e-07
 -1.19170180e-07 -1.19160088e-07 -1.14276684e-07 -1.18984739e-07
  1.18687065e-09  1.20792392e-09  1.24891838e-09  1.23723950e-09
  1.20109952e-09  1.22239833e-09  1.26388241e-09  1.25206455e-09
  1.37898778e-08  1.40336443e-08  1.45096965e-08  1.43741272e-08
  2.83776974e-09  2.88809843e-09  2.98611274e-09  2.95819032e-09
  1.87910231e-10  1.65160711e-09  1.64034099e-09  2.84395064e-10
  1.56781626e-10  1.31008425e-09  1.30003856e-09  2.33132413e-10
  3.69144475e-11  3.10692870e-10  3.08350101e-10  5.50544763e-11
  2.29047122e-11  1.94491501e-10  1.93051866e-10  3.42334717e-11
  1.52191045e-10  2.31983782e-10  1.28958030e-10  1.97837161e-10
  7.69495421e-11  1.22259758e-10  6.30617302e-11  1.03228327e-10
  8.26895024e-11  1.29654222e-10  7.64389811e-11  1.08913997e-10
  2.59191404e-10  4.02196356e-10  1.85859375e-10  3.45256809e-10
 -8.65833745e-12 -3.93925073e-11 -5.77183393e-11  7.69552074e-12
  5.82012686e-11  3.09851173e-12  1.57211597e-11 -2.54299921e-12
  3.81968197e+00]
supnorm grad right now is: 3.819681974153927
Weights right now are: 
[-3.31838488 -3.99128132 -1.66890669 -4.24401571  3.57913627  2.62745894
  1.36727538  3.99389179 -1.72239768 -1.08655952 -1.88289895 -2.03163756
 -1.55145078 -1.61308382 -1.86337342 -1.64650837 -1.2591803  -1.27501269
 -0.70956952 -0.02865356 -1.66394023 -2.19738757 -1.27239151 -0.90337631
 -1.20341665 -1.17078354 -1.18459697 -1.47535294 -0.83475659 -1.46977506
 -1.38435314 -0.69475093 -0.9853643  -2.11807868 -1.81248351 -0.9286941
 -0.45320638 -1.8744223  -1.55896367 -1.51386313  0.32242242  1.73815053
  0.21011613  0.87650257 -1.74522058 -0.85543736 -2.1963224  -1.71544775
 -1.43777183 -1.10087112 -1.58817859 -1.72756981  1.03993862  0.99750597
  0.15919549  0.41175779  0.84861441 -0.41292976 -1.23523986 -1.25170224
  0.49589636  0.8761781  -0.10091752 -0.97935737 25.27106709]
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.946262709407
gradient value of function right now is: [ 1.00247315e-07  1.00238837e-07  9.61724940e-08  1.00091860e-07
 -1.00248744e-07 -1.00240266e-07 -9.61738643e-08 -1.00093286e-07
  1.00388671e-09  1.02166773e-09  1.05631533e-09  1.04643905e-09
  1.01618000e-09  1.03417297e-09  1.06924329e-09  1.05924694e-09
  1.16535350e-08  1.18592342e-08  1.22612281e-08  1.21466844e-08
  2.39481235e-09  2.43722217e-09  2.51987363e-09  2.49631456e-09
  1.59021484e-10  1.39652636e-09  1.38698545e-09  2.40620942e-10
  1.32684798e-10  1.10782541e-09  1.09931882e-09  1.97259845e-10
  3.12804522e-11  2.63058596e-10  2.61072202e-10  4.66421582e-11
  1.94731719e-11  1.65210497e-10  1.63985730e-10  2.90981369e-11
  1.28774026e-10  1.96257960e-10  1.09235399e-10  1.67363182e-10
  6.51207535e-11  1.03455875e-10  5.34212741e-11  8.73464757e-11
  6.99893757e-11  1.09724092e-10  6.47544823e-11  9.21681099e-11
  2.19302786e-10  3.40272160e-10  1.57444150e-10  2.92082541e-10
 -7.31132562e-12 -3.32771725e-11 -4.88684224e-11  6.62590337e-12
  4.92908467e-11  2.52871353e-12  1.33260325e-11 -2.16617535e-12
  2.49314052e+00]
supnorm grad right now is: 2.4931405184489464
Weights right now are: 
[-3.34059637 -4.01349125 -1.6902105  -4.26619317  3.60134775  2.64966885
  1.38857918  4.01606925 -1.72261944 -1.08678521 -1.8831323  -2.03186873
 -1.55167522 -1.61331225 -1.8636096  -1.64674234 -1.26175593 -1.27763384
 -0.71227955 -0.03133826 -1.66446992 -2.19792665 -1.27294888 -0.90392847
 -1.20345177 -1.17109212 -1.18490345 -1.47540603 -0.83478591 -1.47001981
 -1.38459602 -0.69479447 -0.9853712  -2.11813676 -1.81254115 -0.92870438
 -0.45321067 -1.87445871 -1.55899981 -1.51386954  0.32239396  1.73810727
  0.21009201  0.87646564 -1.74523496 -0.85546015 -2.19633418 -1.71546701
 -1.43778728 -1.10089529 -1.58819289 -1.72759013  1.03989016  0.99743094
  0.15916078  0.41169332  0.84861603 -0.4129224  -1.23522912 -1.25170383
  0.49588553  0.87617761 -0.10092045 -0.97935688 25.18769525]
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.126828846433
gradient value of function right now is: [ 8.68528304e-08  8.68454933e-08  8.33531558e-08  8.67185519e-08
 -8.68512487e-08 -8.68439116e-08 -8.33516377e-08 -8.67169725e-08
  8.73721902e-10  8.89162109e-10  9.19292739e-10  9.10700836e-10
  8.84615906e-10  9.00243572e-10  9.30748524e-10  9.22050256e-10
  1.01336787e-08  1.03121424e-08  1.06614240e-08  1.05618631e-08
  2.08029478e-09  2.11705076e-09  2.18878898e-09  2.16833304e-09
  1.38632822e-10  1.21454144e-09  1.20620194e-09  2.09599067e-10
  1.15705183e-10  9.63710709e-10  9.56275576e-10  1.71875943e-10
  2.73061123e-11  2.29080509e-10  2.27342425e-10  4.06828905e-11
  1.70475257e-11  1.44275074e-10  1.43200245e-10  2.54524145e-11
  1.12135140e-10  1.70801227e-10  9.54624592e-11  1.45643632e-10
  5.66997468e-11  9.00419909e-11  4.66603839e-11  7.60132376e-11
  6.09717737e-11  9.55315464e-11  5.65776211e-11  8.02410731e-11
  1.90854867e-10  2.96037267e-10  1.37520876e-10  2.54082242e-10
 -6.34756715e-12 -2.88543993e-11 -4.26330559e-11  6.10777921e-12
  4.29909684e-11  1.93385993e-12  1.16287538e-11 -1.92900055e-12
 -1.09375029e+00]
supnorm grad right now is: 1.093750293672788
Weights right now are: 
[-3.35948853 -4.03238184 -1.70833773 -4.28505613  3.62023992  2.66855945
  1.40670641  4.03493221 -1.72280902 -1.08697815 -1.88333178 -2.03206634
 -1.55186714 -1.61350757 -1.86381154 -1.64694239 -1.26395577 -1.27987251
 -0.71459408 -0.03363117 -1.66492178 -2.19838651 -1.27342433 -0.90439948
 -1.20348179 -1.17135585 -1.18516537 -1.47545145 -0.83481096 -1.470229
 -1.3848036  -0.6948317  -0.98537711 -2.11818646 -1.81259047 -0.92871319
 -0.45321435 -1.87448996 -1.55903083 -1.51387504  0.32236963  1.73807023
  0.21007138  0.87643405 -1.74524726 -0.85547967 -2.19634427 -1.7154835
 -1.4378005  -1.100916   -1.58820511 -1.72760753  1.03984873  0.99736672
  0.15913106  0.41163818  0.84861741 -0.4129161  -1.2352199  -1.25170517
  0.49587624  0.87617719 -0.10092296 -0.97935647 24.87624841]
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.1660034231
gradient value of function right now is: [ 7.63008022e-08  7.62943625e-08  7.32492467e-08  7.61831331e-08
 -7.62996614e-08 -7.62932217e-08 -7.32481513e-08 -7.61819940e-08
  7.70624497e-10  7.84237466e-10  8.10800477e-10  8.03222135e-10
  7.80373420e-10  7.94154146e-10  8.21051997e-10  8.13378457e-10
  8.92987038e-09  9.08707469e-09  9.39472281e-09  9.30698558e-09
  1.83179457e-09  1.86414726e-09  1.92728689e-09  1.90927387e-09
  1.22190992e-10  1.07109166e-09  1.06374688e-09  1.84786610e-10
  1.01973990e-10  8.49818263e-10  8.43269957e-10  1.51516089e-10
  2.40891720e-11  2.02204188e-10  2.00671924e-10  3.58988068e-11
  1.50773752e-11  1.27669051e-10  1.26719073e-10  2.25162460e-11
  9.88899049e-11  1.50647805e-10  8.41086012e-11  1.28461108e-10
  5.00196631e-11  7.94413176e-11  4.11306566e-11  6.70657971e-11
  5.37785008e-11  8.42728017e-11  4.98660759e-11  7.07854247e-11
  1.68377672e-10  2.61192510e-10  1.21210803e-10  2.24182632e-10
 -5.59542139e-12 -2.54763856e-11 -3.75761733e-11  5.30312101e-12
  3.79122187e-11  1.76704698e-12  1.02629661e-11 -1.69129761e-12
 -2.22403921e-01]
supnorm grad right now is: 0.22240392106233065
Weights right now are: 
[-3.37575517 -4.04864712 -1.72395113 -4.30129766  3.63650657  2.68482474
  1.42231982  4.05117375 -1.72297295 -1.08714497 -1.88350425 -2.0322372
 -1.55203313 -1.61367648 -1.86398618 -1.64711539 -1.26585633 -1.28180651
 -0.71659357 -0.03561199 -1.66531179 -2.19878342 -1.27383468 -0.90480599
 -1.20350782 -1.17158365 -1.18539161 -1.47549078 -0.83483269 -1.47040977
 -1.38498298 -0.69486396 -0.98538224 -2.11822945 -1.81263313 -0.92872083
 -0.45321756 -1.87451707 -1.55905773 -1.51387983  0.3223486   1.7380382
  0.21005344  0.87640673 -1.74525789 -0.85549656 -2.19635304 -1.71549776
 -1.43781194 -1.10093391 -1.58821575 -1.72762258  1.03981293  0.99731121
  0.15910523  0.41159053  0.8486186  -0.41291068 -1.23521189 -1.25170633
  0.49586817  0.87617685 -0.10092514 -0.9793561  24.98833291]
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.931401605469
gradient value of function right now is: [ 6.83604492e-08  6.83546846e-08  6.56447171e-08  6.82552684e-08
 -6.83607578e-08 -6.83549931e-08 -6.56450134e-08 -6.82555765e-08
  6.92815797e-10  7.05033623e-10  7.28899988e-10  7.22089021e-10
  7.01691755e-10  7.14062103e-10  7.38233150e-10  7.31335530e-10
  8.02145158e-09  8.16242545e-09  8.43860802e-09  8.35982151e-09
  1.64444303e-09  1.67343775e-09  1.73008491e-09  1.71391944e-09
  1.09970793e-10  9.62304590e-10  9.55681722e-10  1.66210509e-10
  9.17952319e-11  7.63650537e-10  7.57745852e-10  1.36313171e-10
  2.17024884e-11  1.81852314e-10  1.80469476e-10  3.23233824e-11
  1.36140538e-11  1.15073390e-10  1.14214072e-10  2.03189860e-11
  8.89245405e-11  1.35417265e-10  7.58310515e-11  1.15465407e-10
  4.49770494e-11  7.14147954e-11  3.70713752e-11  6.02842426e-11
  4.83749552e-11  7.57771764e-11  4.49550007e-11  6.36452921e-11
  1.51350970e-10  2.34733874e-10  1.09243089e-10  2.01453441e-10
 -5.01982270e-12 -2.28413211e-11 -3.38396293e-11  4.96202711e-12
  3.41375231e-11  1.43364500e-12  9.24488389e-12 -1.54616230e-12
 -2.55017338e+00]
supnorm grad right now is: 2.5501733791850687
Weights right now are: 
[-3.3901537  -4.06304443 -1.73777554 -4.31567401  3.6509051   2.69922205
  1.43614423  4.0655501  -1.7231186  -1.08729319 -1.88365749 -2.03238901
 -1.55218063 -1.61382658 -1.86414136 -1.64726912 -1.26754344 -1.28352328
 -0.71836844 -0.03737029 -1.66565777 -2.1991355  -1.27419868 -0.90516659
 -1.20353094 -1.17178599 -1.18559256 -1.47552572 -0.83485198 -1.47057034
 -1.3851423  -0.69489262 -0.9853868  -2.11826767 -1.81267106 -0.92872762
 -0.45322042 -1.87454122 -1.5590817  -1.51388409  0.32232991  1.73800973
  0.2100375   0.87638246 -1.74526735 -0.85551157 -2.19636083 -1.71551042
 -1.43782211 -1.10094984 -1.5882252  -1.72763595  1.03978112  0.99726187
  0.15908227  0.41154819  0.84861965 -0.41290589 -1.23520477 -1.25170736
  0.49586099  0.87617655 -0.10092709 -0.97935578 24.7971078 ]
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.147182684601
gradient value of function right now is: [ 6.15092455e-08  6.15040625e-08  5.90803661e-08  6.14147943e-08
 -6.15105627e-08 -6.15053796e-08 -5.90816313e-08 -6.14161094e-08
  6.25364732e-10  6.36395195e-10  6.57931630e-10  6.51782765e-10
  6.33464380e-10  6.44634103e-10  6.66448502e-10  6.60220541e-10
  7.23416904e-09  7.36133351e-09  7.61033674e-09  7.53927106e-09
  1.48238676e-09  1.50852921e-09  1.55957887e-09  1.54500410e-09
  9.91352597e-11  8.68614841e-10  8.62654237e-10  1.49909932e-10
  8.27365338e-11  6.89191514e-10  6.83877175e-10  1.22923506e-10
  1.95768771e-11  1.64254191e-10  1.63008609e-10  2.91722110e-11
  1.23065228e-11  1.04155114e-10  1.03379462e-10  1.83766598e-11
  8.02373233e-11  1.22226657e-10  6.82836872e-11  1.04223920e-10
  4.06010357e-11  6.44810952e-11  3.34044880e-11  5.44348347e-11
  4.36525382e-11  6.84015756e-11  4.04986890e-11  5.74532275e-11
  1.36648976e-10  2.11971120e-10  9.84254915e-11  1.81931835e-10
 -4.53154646e-12 -2.06624367e-11 -3.04967088e-11  4.34036725e-12
  3.07888122e-11  1.40164437e-12  8.34447839e-12 -1.37727567e-12
 -7.79161330e-01]
supnorm grad right now is: 0.7791613300078158
Weights right now are: 
[-3.40319943 -4.07608907 -1.75030436 -4.32869969  3.66395081  2.71226666
  1.44867303  4.07857575 -1.72325101 -1.08742794 -1.8837968  -2.03252702
 -1.55231474 -1.61396307 -1.86428246 -1.64740891 -1.26907584 -1.28508267
 -0.7199806  -0.03896739 -1.66597187 -2.19945515 -1.27452915 -0.90549397
 -1.20355189 -1.17197003 -1.18577534 -1.47555744 -0.83486947 -1.47071633
 -1.38528717 -0.69491862 -0.98539093 -2.11830244 -1.81270558 -0.92873379
 -0.45322301 -1.87456325 -1.55910357 -1.51388797  0.32231293  1.73798385
  0.2100231   0.87636039 -1.74527594 -0.85552523 -2.19636788 -1.71552196
 -1.43783134 -1.10096432 -1.58823375 -1.72764812  1.03975218  0.99721695
  0.1590615   0.41150963  0.84862061 -0.41290149 -1.23519833 -1.25170826
  0.49585447  0.87617622 -0.10092885 -0.97935549 24.92191597]
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.030253752368
gradient value of function right now is: [ 5.61387147e-08  5.61339876e-08  5.39341052e-08  5.60526722e-08
 -5.61382305e-08 -5.61335035e-08 -5.39336402e-08 -5.60521888e-08
  5.72360033e-10  5.82445084e-10  6.02147197e-10  5.96520475e-10
  5.79850114e-10  5.90063837e-10  6.10022889e-10  6.04323032e-10
  6.61565907e-09  6.73183079e-09  6.95944163e-09  6.89446313e-09
  1.35513031e-09  1.37900373e-09  1.42564984e-09  1.41232856e-09
  9.07658525e-11  7.94607155e-10  7.89145006e-10  1.37216491e-10
  7.57583446e-11  6.30528229e-10  6.25658456e-10  1.12525536e-10
  1.79382778e-11  1.50378494e-10  1.49236290e-10  2.67232700e-11
  1.12978524e-11  9.55350133e-11  9.48223563e-11  1.68657742e-11
  7.34415137e-11  1.11850800e-10  6.25754446e-11  9.53743824e-11
  3.71632743e-11  5.90120958e-11  3.06088020e-11  4.98165238e-11
  3.99635622e-11  6.26075963e-11  3.71124867e-11  5.25857933e-11
  1.25057079e-10  1.93964542e-10  9.01879628e-11  1.66471521e-10
 -4.14203770e-12 -1.88832579e-11 -2.79309622e-11  4.05014543e-12
  2.81977827e-11  1.22210620e-12  7.64488729e-12 -1.27041762e-12
 -1.94513346e+00]
supnorm grad right now is: 1.945133460966031
Weights right now are: 
[-3.41501441 -4.08790305 -1.76165395 -4.34049654  3.6757658   2.72408066
  1.46002264  4.09037262 -1.72337128 -1.08755033 -1.88392333 -2.03265236
 -1.55243658 -1.61408705 -1.86441064 -1.64753589 -1.27046664 -1.28649788
 -0.72144367 -0.0404168  -1.66625681 -2.1997451  -1.27482892 -0.90579094
 -1.20357098 -1.172137   -1.18594115 -1.47558628 -0.8348854  -1.47084883
 -1.38541864 -0.69494228 -0.98539471 -2.11833403 -1.81273693 -0.92873941
 -0.45322539 -1.8745833  -1.55912347 -1.51389151  0.32229749  1.73796035
  0.21000993  0.87634035 -1.74528375 -0.85553762 -2.19637432 -1.71553242
 -1.43783974 -1.10097747 -1.58824156 -1.72765916  1.0397259   0.99717621
  0.15904253  0.41147466  0.84862148 -0.41289752 -1.23519247 -1.25170908
  0.49584856  0.87617596 -0.10093045 -0.97935522 24.83585767]
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.032978501973
gradient value of function right now is: [ 5.15363678e-08  5.15320310e-08  4.95227043e-08  5.14575134e-08
 -5.15363735e-08 -5.15320367e-08 -4.95227098e-08 -5.14575191e-08
  5.26799553e-10  5.36077229e-10  5.54204754e-10  5.49026120e-10
  5.33755057e-10  5.43152189e-10  5.61518238e-10  5.56271688e-10
  6.08417938e-09  6.19096620e-09  6.40021934e-09  6.34046311e-09
  1.24590130e-09  1.26783956e-09  1.31071097e-09  1.29846390e-09
  8.35160362e-11  7.31133424e-10  7.26107953e-10  1.26260276e-10
  6.97070131e-11  5.80158644e-10  5.75678170e-10  1.03540334e-10
  1.65164105e-11  1.38457327e-10  1.37405731e-10  2.46057524e-11
  1.04207236e-11  8.81155355e-11  8.74582384e-11  1.55567106e-11
  6.75854486e-11  1.02932709e-10  5.75836332e-11  8.77701411e-11
  3.42060150e-11  5.43164470e-11  2.81726721e-11  4.58526201e-11
  3.67822576e-11  5.76240384e-11  3.41576863e-11  4.83999937e-11
  1.15101606e-10  1.78524028e-10  8.30048454e-11  1.53220005e-10
 -3.80939381e-12 -1.73793127e-11 -2.57023839e-11  3.72470784e-12
  2.59550428e-11  1.12560994e-12  7.04024411e-12 -1.16891221e-12
 -1.92583027e+00]
supnorm grad right now is: 1.9258302650306802
Weights right now are: 
[-3.42585707 -4.09874479 -1.77207171 -4.35132258  3.68660844  2.73492239
  1.47044038  4.10119865 -1.72348197 -1.08766297 -1.88403979 -2.03276773
 -1.55254872 -1.61420117 -1.86452862 -1.64765277 -1.27174547 -1.28779925
 -0.72278905 -0.04174961 -1.66651875 -2.20001167 -1.27510451 -0.90606395
 -1.20358845 -1.17229086 -1.18609397 -1.47561273 -0.83489998 -1.47097083
 -1.38553971 -0.69496395 -0.98539816 -2.11836314 -1.81276581 -0.92874456
 -0.45322756 -1.87460181 -1.55914184 -1.51389477  0.3222833   1.73793875
  0.20999795  0.87632191 -1.74529094 -0.85554903 -2.19638018 -1.71554206
 -1.43784746 -1.10098956 -1.58824867 -1.72766932  1.03970169  0.99713869
  0.15902525  0.41144242  0.84862229 -0.41289384 -1.23518713 -1.25170982
  0.49584316  0.87617567 -0.10093192 -0.97935499 24.86896039]
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.106521909485
gradient value of function right now is: [ 4.76006739e-08  4.75966707e-08  4.57494022e-08  4.75279541e-08
 -4.76006663e-08 -4.75966631e-08 -4.57493948e-08 -4.75279465e-08
  4.87725878e-10  4.96313762e-10  5.13092052e-10  5.08297360e-10
  4.94217700e-10  5.02917077e-10  5.19917935e-10  5.15059849e-10
  5.62854034e-09  5.72731192e-09  5.92084053e-09  5.86555735e-09
  1.15233876e-09  1.17262566e-09  1.21226652e-09  1.20093876e-09
  7.72724686e-11  6.76765998e-10  6.72118949e-10  1.16842726e-10
  6.44922956e-11  5.36988713e-10  5.32845551e-10  9.58119544e-11
  1.52903709e-11  1.28234173e-10  1.27261146e-10  2.27833395e-11
  9.66302468e-12  8.17423961e-11  8.11332033e-11  1.44281043e-11
  6.25571861e-11  9.52845026e-11  5.32622758e-11  8.12502607e-11
  3.16687596e-11  5.02913997e-11  2.60670643e-11  4.24557431e-11
  3.40491830e-11  5.33480317e-11  3.16019033e-11  4.48092424e-11
  1.06568017e-10  1.65298472e-10  7.67955395e-11  1.41872857e-10
 -3.52556637e-12 -1.61004939e-11 -2.37790910e-11  3.40974123e-12
  2.40227368e-11  1.07168443e-12  6.51950799e-12 -1.07645127e-12
 -1.33807070e+00]
supnorm grad right now is: 1.338070695083192
Weights right now are: 
[-3.43580963 -4.10869652 -1.78163616 -4.36125993  3.69656102  2.74487413
  1.48000484  4.111136   -1.72358382 -1.08776663 -1.88414695 -2.03287389
 -1.55265192 -1.6143062  -1.8646372  -1.64776033 -1.27292133 -1.28899581
 -0.72402607 -0.04297506 -1.66675954 -2.20025671 -1.27535784 -0.90631491
 -1.20360453 -1.17243237 -1.18623453 -1.47563709 -0.83491339 -1.47108305
 -1.38565108 -0.69498392 -0.98540134 -2.11838993 -1.81279241 -0.9287493
 -0.45322957 -1.87461887 -1.55915878 -1.51389777  0.32227025  1.73791884
  0.20998692  0.87630493 -1.74529755 -0.85555954 -2.19638559 -1.71555094
 -1.43785456 -1.1010007  -1.58825522 -1.72767869  1.03967942  0.99710411
  0.15900933  0.41141273  0.84862303 -0.41289044 -1.23518218 -1.25171048
  0.49583814  0.8761754  -0.10093328 -0.97935477 24.86158631]
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.699760237952
gradient value of function right now is: [ 4.39149911e-08  4.39112997e-08  4.22143095e-08  4.38479913e-08
 -4.39142174e-08 -4.39105261e-08 -4.22135657e-08 -4.38472188e-08
  4.50977484e-10  4.58930716e-10  4.74444866e-10  4.70009245e-10
  4.57023827e-10  4.65081089e-10  4.80802542e-10  4.76307844e-10
  5.20032778e-09  5.29172973e-09  5.47053600e-09  5.41943321e-09
  1.06452623e-09  1.08329650e-09  1.11991669e-09  1.10944697e-09
  7.12468684e-11  6.26049665e-10  6.21781288e-10  1.07860185e-10
  5.94400965e-11  4.96555116e-10  4.92749174e-10  8.84107845e-11
  1.41017437e-11  1.18653586e-10  1.17759220e-10  2.10369838e-11
  8.92590008e-12  7.57552230e-11  7.51944323e-11  1.33432330e-11
  5.77960229e-11  8.80983090e-11  4.89619794e-11  7.51328992e-11
  2.92792309e-11  4.65195518e-11  2.39943675e-11  3.92788071e-11
  3.14532562e-11  4.93172837e-11  2.90748613e-11  4.14289426e-11
  9.85810437e-11  1.52970167e-10  7.06725551e-11  1.31318254e-10
 -3.26506158e-12 -1.49702513e-11 -2.19049754e-11  2.90605614e-12
  2.21553590e-11  1.18148294e-12  6.01709764e-12 -9.63477800e-13
  3.73576727e+00]
supnorm grad right now is: 3.735767269748767
Weights right now are: 
[-3.44505331 -4.11793942 -1.79052101 -4.37048949  3.70580469  2.75411702
  1.48888969  4.12036556 -1.72367863 -1.08786311 -1.88424669 -2.0329727
 -1.552748   -1.61440397 -1.86473828 -1.64786046 -1.27401507 -1.29010877
 -0.72517664 -0.04411489 -1.66698345 -2.20048457 -1.2755934  -0.90654826
 -1.20361952 -1.17256397 -1.18636523 -1.47565978 -0.8349259  -1.47118745
 -1.38575467 -0.69500252 -0.9854043  -2.11841487 -1.81281715 -0.92875373
 -0.45323145 -1.87463478 -1.55917457 -1.51390057  0.3222581   1.73790032
  0.20997661  0.87628914 -1.74530371 -0.85556932 -2.19639064 -1.7155592
 -1.43786117 -1.10101107 -1.58826134 -1.7276874   1.0396587   0.99707196
  0.15899445  0.41138514  0.84862372 -0.4128873  -1.23517757 -1.25171112
  0.49583348  0.87617517 -0.10093455 -0.97935457 25.2306897 ]
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.167543065587
gradient value of function right now is: [ 4.12529488e-08  4.12494831e-08  3.96618753e-08  4.11901010e-08
 -4.12519162e-08 -4.12484506e-08 -3.96608824e-08 -4.11890700e-08
  4.24475488e-10  4.31947786e-10  4.46543069e-10  4.42369810e-10
  4.30207286e-10  4.37778034e-10  4.52569742e-10  4.48340501e-10
  4.89140186e-09  4.97721788e-09  5.14531951e-09  5.09727208e-09
  1.00110969e-09  1.01872980e-09  1.05315153e-09  1.04330953e-09
  6.71635081e-11  5.88775158e-10  5.84740828e-10  1.01595767e-10
  5.60487520e-11  4.67117918e-10  4.63520927e-10  8.32993045e-11
  1.33035416e-11  1.11674124e-10  1.10828420e-10  1.98302872e-11
  8.43244388e-12  7.13966354e-11  7.08655744e-11  1.25953034e-11
  5.44174403e-11  8.29038712e-11  4.62622388e-11  7.06959754e-11
  2.75601063e-11  4.37722858e-11  2.26560815e-11  3.69542378e-11
  2.96233270e-11  4.64230694e-11  2.74615591e-11  3.89940500e-11
  9.27530776e-11  1.43884445e-10  6.67375388e-11  1.23500903e-10
 -3.06596776e-12 -1.40328762e-11 -2.06653665e-11  2.89133526e-12
  2.08912224e-11  9.87090974e-13  5.67459352e-12 -9.27893368e-13
  1.79725260e-01]
supnorm grad right now is: 0.17972526010417525
Weights right now are: 
[-3.45365188 -4.12653726 -1.79878721 -4.37907495  3.71440328  2.76271489
  1.49715591  4.12895104 -1.72376701 -1.08795304 -1.88433966 -2.0330648
 -1.55283756 -1.61449511 -1.8648325  -1.6479538  -1.27503387 -1.29114545
 -0.72624833 -0.04517657 -1.66719198 -2.20069677 -1.27581277 -0.90676558
 -1.20363351 -1.17268657 -1.18648698 -1.47568092 -0.83493758 -1.47128471
 -1.38585118 -0.69501985 -0.98540708 -2.11843811 -1.81284022 -0.92875785
 -0.4532332  -1.87464963 -1.55918931 -1.51390319  0.32224676  1.7378831
  0.20996696  0.87627443 -1.74530944 -0.85557841 -2.19639536 -1.71556688
 -1.43786734 -1.10102071 -1.58826706 -1.7276955   1.03963938  0.99704206
  0.15898056  0.41135945  0.84862436 -0.41288438 -1.2351733  -1.25171175
  0.49582917  0.87617498 -0.10093572 -0.97935437 25.01671   ]
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.166283587681
gradient value of function right now is: [ 3.87203899e-08  3.87171385e-08  3.72325985e-08  3.86614748e-08
 -3.87197793e-08 -3.87165280e-08 -3.72320114e-08 -3.86608651e-08
  3.99169819e-10  4.06194265e-10  4.19915955e-10  4.15991572e-10
  4.04593409e-10  4.11710998e-10  4.25618509e-10  4.21641152e-10
  4.59662228e-09  4.67723977e-09  4.83517094e-09  4.79001993e-09
  9.40674796e-10  9.57225625e-10  9.89561194e-10  9.80313528e-10
  6.31390884e-11  5.53520344e-10  5.49728174e-10  9.55119801e-11
  5.26901643e-11  4.39144783e-10  4.35763651e-10  7.83107275e-11
  1.25126357e-11  1.05039116e-10  1.04243765e-10  1.86520515e-11
  7.94176122e-12  6.72439131e-11  6.67437945e-11  1.18627747e-11
  5.11635965e-11  7.79481366e-11  4.34919886e-11  6.64702402e-11
  2.59159994e-11  4.11615919e-11  2.13030503e-11  3.47503339e-11
  2.78550315e-11  4.36527189e-11  2.58208741e-11  3.66671451e-11
  8.72181344e-11  1.35299832e-10  6.27484392e-11  1.16133060e-10
 -2.88146590e-12 -1.31963602e-11 -1.94279458e-11  2.71379397e-12
  1.96448023e-11  9.31477675e-13  5.33801758e-12 -8.71710138e-13
  2.89182275e-01]
supnorm grad right now is: 0.2891822746310136
Weights right now are: 
[-3.46165729 -4.13454201 -1.80648439 -4.38706818  3.72240871  2.77071964
  1.5048531   4.13694428 -1.72384945 -1.08803694 -1.88442639 -2.03315072
 -1.55292112 -1.61458014 -1.8649204  -1.64804088 -1.27598361 -1.29211185
 -0.72724737 -0.04616627 -1.66738635 -2.20089456 -1.27601724 -0.90696815
 -1.20364655 -1.17280092 -1.18660055 -1.47570064 -0.83494846 -1.47137542
 -1.38594119 -0.69503602 -0.98540966 -2.11845981 -1.81286175 -0.9287617
 -0.45323484 -1.87466351 -1.55920308 -1.51390564  0.32223618  1.737867
  0.20995798  0.8762607  -1.7453148  -0.8555869  -2.19639975 -1.71557405
 -1.4378731  -1.10102971 -1.58827239 -1.72770307  1.03962135  0.99701414
  0.15896761  0.41133547  0.84862495 -0.41288166 -1.23516929 -1.25171233
  0.49582511  0.87617481 -0.10093682 -0.97935419 25.03093039]
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.165384350277
gradient value of function right now is: [ 3.65222974e-08  3.65192320e-08  3.51239287e-08  3.64667933e-08
 -3.65215288e-08 -3.65184634e-08 -3.51231895e-08 -3.64660258e-08
  3.77170384e-10  3.83804220e-10  3.96766185e-10  3.93058362e-10
  3.82325433e-10  3.89047762e-10  4.02186304e-10  3.98428132e-10
  4.34042164e-09  4.41650625e-09  4.56559493e-09  4.52296359e-09
  8.88166050e-10  9.03784836e-10  9.34307229e-10  9.25576416e-10
  5.96564515e-11  5.22831747e-10  5.19247856e-10  9.02364459e-11
  4.97852007e-11  4.14811547e-10  4.11616199e-10  7.39875047e-11
  1.18282855e-11  9.92650100e-11  9.85130059e-11  1.76305521e-11
  7.51687382e-12  6.36266412e-11  6.31531715e-11  1.12271786e-11
  4.83391100e-11  7.36402997e-11  4.11073583e-11  6.27960084e-11
  2.44871641e-11  3.88908467e-11  2.01355005e-11  3.28326771e-11
  2.63207381e-11  4.12458137e-11  2.44058731e-11  3.46449830e-11
  8.24025693e-11  1.27825726e-10  5.93090331e-11  1.09715756e-10
 -2.72056673e-12 -1.24605777e-11 -1.83597706e-11  2.58282779e-12
  1.85669257e-11  8.65801610e-13  5.04683262e-12 -8.25658775e-13
 -2.65297287e-01]
supnorm grad right now is: 0.26529728663723456
Weights right now are: 
[-3.4691741  -4.14205819 -1.81371283 -4.39457356  3.72992552  2.77823583
  1.51208154  4.14444967 -1.723927   -1.08811585 -1.88450797 -2.03323154
 -1.55299973 -1.61466013 -1.86500309 -1.6481228  -1.27687639 -1.29302029
 -0.72818647 -0.04709661 -1.66756904 -2.20108047 -1.27620943 -0.90715854
 -1.20365882 -1.17290844 -1.18670733 -1.47571918 -0.83495871 -1.47146072
 -1.38602584 -0.69505123 -0.98541209 -2.11848021 -1.812882   -0.92876532
 -0.45323638 -1.87467658 -1.55921606 -1.51390794  0.32222624  1.73785188
  0.20994952  0.8762478  -1.74531983 -0.85559489 -2.19640389 -1.7155808
 -1.43787851 -1.10103818 -1.58827741 -1.72771019  1.03960441  0.99698788
  0.15895542  0.41131292  0.84862551 -0.41287908 -1.23516553 -1.25171288
  0.49582131  0.87617464 -0.10093785 -0.97935402 24.93517973]
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.16640162996
gradient value of function right now is: [ 3.45292438e-08  3.45263469e-08  3.32115989e-08  3.44768274e-08
 -3.45303226e-08 -3.45274256e-08 -3.32126366e-08 -3.44779046e-08
  3.57190426e-10  3.63470871e-10  3.75743417e-10  3.72232089e-10
  3.62096873e-10  3.68461536e-10  3.80902107e-10  3.77342856e-10
  4.10783112e-09  4.17981640e-09  4.32088399e-09  4.28053799e-09
  8.40521293e-10  8.55297591e-10  8.84176097e-10  8.75913826e-10
  5.64795446e-11  4.94998845e-10  4.91606132e-10  8.54338800e-11
  4.71337247e-11  3.92728048e-10  3.89703151e-10  7.00493779e-11
  1.12033536e-11  9.40223865e-11  9.33101770e-11  1.66996106e-11
  7.12827991e-12  6.03379118e-11  5.98889524e-11  1.06470805e-11
  4.57702204e-11  6.97281349e-11  3.89201659e-11  5.94598271e-11
  2.31887712e-11  3.68294317e-11  1.90669327e-11  3.10922527e-11
  2.49244363e-11  3.90585643e-11  2.31099686e-11  3.28076482e-11
  7.80314749e-11  1.21046967e-10  5.61595626e-11  1.03897187e-10
 -2.57496952e-12 -1.17993151e-11 -1.73832746e-11  2.44186575e-12
  1.75830417e-11  8.22354492e-13  4.78109506e-12 -7.81209914e-13
 -2.14615779e-01]
supnorm grad right now is: 0.21461577886186028
Weights right now are: 
[-3.47627433 -4.14915782 -1.82054162 -4.401663    3.73702574  2.78533545
  1.51891032  4.1515391  -1.72400038 -1.08819052 -1.88458516 -2.03330801
 -1.55307411 -1.61473582 -1.86508134 -1.64820032 -1.27772059 -1.29387927
 -0.72907444 -0.04797629 -1.66774178 -2.20125624 -1.27639114 -0.90733855
 -1.20367044 -1.17301011 -1.1868083  -1.47573675 -0.8349684  -1.4715414
 -1.38610589 -0.69506564 -0.9854144  -2.11849952 -1.81290117 -0.92876876
 -0.45323785 -1.87468897 -1.55922835 -1.51391013  0.32221684  1.73783754
  0.20994151  0.87623559 -1.74532459 -0.85560246 -2.19640782 -1.71558718
 -1.43788363 -1.10104621 -1.58828218 -1.72771693  1.03958839  0.99696301
  0.15894385  0.41129159  0.84862604 -0.41287667 -1.23516194 -1.25171337
  0.49581768  0.87617448 -0.10093884 -0.97935386 25.01160507]
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.847075658525
gradient value of function right now is: [ 3.25841637e-08  3.25814310e-08  3.13446190e-08  3.25347488e-08
 -3.25838458e-08 -3.25811131e-08 -3.13443133e-08 -3.25344314e-08
  3.37607473e-10  3.43549205e-10  3.55148597e-10  3.51828753e-10
  3.42269348e-10  3.48291187e-10  3.60050228e-10  3.56684835e-10
  3.88008134e-09  3.94814086e-09  4.08138398e-09  4.04326300e-09
  7.93909013e-10  8.07879074e-10  8.35155350e-10  8.27348864e-10
  5.32824704e-11  4.67952409e-10  4.64759459e-10  8.06593143e-11
  4.44549076e-11  3.71179662e-10  3.68332653e-10  6.61181200e-11
  1.05716503e-11  8.89046626e-11  8.82340386e-11  1.57698442e-11
  6.73420107e-12  5.71206148e-11  5.66973949e-11  1.00660552e-11
  4.32345419e-11  6.58978042e-11  3.66478935e-11  5.61982990e-11
  2.19145553e-11  3.48171064e-11  1.79696682e-11  2.93967921e-11
  2.35419068e-11  3.69102209e-11  2.17731436e-11  3.10055349e-11
  7.37687016e-11  1.14464866e-10  5.29181637e-11  9.82600559e-11
 -2.43587101e-12 -1.11909088e-11 -1.63905416e-11  2.19263284e-12
  1.65920410e-11  8.66759581e-13  4.51382453e-12 -7.23181232e-13
  3.05196304e+00]
supnorm grad right now is: 3.0519630426448
Weights right now are: 
[-3.48299526 -4.15587818 -1.82700644 -4.40837373  3.74374668  2.79205582
  1.52537515  4.15824984 -1.72406995 -1.08826131 -1.88465834 -2.03338051
 -1.55314464 -1.61480759 -1.86515553 -1.64827381 -1.27852047 -1.29469316
 -0.7299158  -0.0488098  -1.66790544 -2.20142278 -1.27656329 -0.9075091
 -1.20368145 -1.17310649 -1.18690402 -1.47575339 -0.83497759 -1.47161788
 -1.38618178 -0.69507929 -0.98541658 -2.11851784 -1.81291934 -0.92877201
 -0.45323924 -1.87470072 -1.55924002 -1.51391221  0.32220792  1.73782397
  0.20993392  0.87622401 -1.74532911 -0.85560962 -2.19641154 -1.71559323
 -1.43788848 -1.10105381 -1.58828669 -1.72772331  1.03957319  0.99693945
  0.1589329   0.41127136  0.84862654 -0.41287437 -1.23515856 -1.25171386
  0.49581426  0.87617433 -0.10093977 -0.9793537  25.19435576]
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.166585239516
gradient value of function right now is: [ 3.11313250e-08  3.11287152e-08  2.99506798e-08  3.10841649e-08
 -3.11303576e-08 -3.11277480e-08 -2.99497490e-08 -3.10831990e-08
  3.23016028e-10  3.28692137e-10  3.39785771e-10  3.36610541e-10
  3.27499729e-10  3.33252764e-10  3.44499886e-10  3.41280864e-10
  3.71025035e-09  3.77522949e-09  3.90258962e-09  3.86615019e-09
  7.59115185e-10  7.72452268e-10  7.98522725e-10  7.91061023e-10
  5.10491508e-11  4.47397692e-10  4.44331421e-10  7.72216043e-11
  4.26020955e-11  3.54961982e-10  3.52228098e-10  6.33160500e-11
  1.01346230e-11  8.50512415e-11  8.44070151e-11  1.51069182e-11
  6.46267767e-12  5.47014960e-11  5.42944749e-11  9.65306357e-12
  4.13777032e-11  6.30350532e-11  3.51834634e-11  5.37531405e-11
  2.09677880e-11  3.33011291e-11  1.72402491e-11  2.81139925e-11
  2.25363785e-11  3.53153766e-11  2.08953464e-11  2.96639473e-11
  7.05550562e-11  1.09446664e-10  5.07758706e-11  9.39417956e-11
 -2.32611430e-12 -1.06681804e-11 -1.57127565e-11  2.20617509e-12
  1.58982804e-11  7.43868490e-13  4.32535628e-12 -7.06088718e-13
 -2.10721004e-01]
supnorm grad right now is: 0.210721004010243
Weights right now are: 
[-3.48939618 -4.16227856 -1.8331642  -4.41476495  3.75014758  2.79845619
  1.5315329   4.16464104 -1.72413631 -1.08832884 -1.88472815 -2.03344966
 -1.55321193 -1.61487605 -1.8652263  -1.64834393 -1.27928299 -1.29546903
 -0.73071785 -0.04960436 -1.66806145 -2.20158153 -1.27672741 -0.90767168
 -1.20369193 -1.17319843 -1.18699533 -1.47576926 -0.83498634 -1.47169082
 -1.38625416 -0.6950923  -0.98541866 -2.11853531 -1.81293668 -0.92877512
 -0.45324057 -1.87471196 -1.55925117 -1.51391419  0.32219942  1.73781101
  0.20992669  0.87621296 -1.74533342 -0.85561647 -2.19641508 -1.71559902
 -1.43789311 -1.10106108 -1.58829097 -1.72772941  1.0395587   0.99691695
  0.15892248  0.41125205  0.84862702 -0.41287218 -1.23515533 -1.25171431
  0.49581099  0.87617417 -0.10094066 -0.97935356 24.96657543]
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.116905668874
gradient value of function right now is: [ 2.95985255e-08  2.95960452e-08  2.84792249e-08  2.95537292e-08
 -2.95986042e-08 -2.95961238e-08 -2.84793005e-08 -2.95538078e-08
  3.07560402e-10  3.12966528e-10  3.23528216e-10  3.20504520e-10
  3.11847473e-10  3.17327186e-10  3.28035616e-10  3.24970041e-10
  3.53057004e-09  3.59242137e-09  3.71360079e-09  3.67892145e-09
  7.22353824e-10  7.35048818e-10  7.59854097e-10  7.52752812e-10
  4.85571292e-11  4.25969580e-10  4.23056416e-10  7.34796550e-11
  4.05176625e-11  3.37923638e-10  3.35326199e-10  6.02407932e-11
  9.64275256e-12  8.10016775e-11  8.03893579e-11  1.43790984e-11
  6.15552093e-12  5.21519222e-11  5.17646561e-11  9.19770446e-12
  3.93824841e-11  6.00118421e-11  3.34373800e-11  5.11761753e-11
  1.99624640e-11  3.17108320e-11  1.63927725e-11  2.67722515e-11
  2.14501216e-11  3.36225577e-11  1.98646931e-11  2.82424336e-11
  6.71812268e-11  1.04230112e-10  4.82748891e-11  8.94678629e-11
 -2.21499638e-12 -1.01724988e-11 -1.49440797e-11  2.05043237e-12
  1.51279310e-11  7.46751115e-13  4.11761236e-12 -6.65533605e-13
  1.26557450e+00]
supnorm grad right now is: 1.265574497937101
Weights right now are: 
[-3.49550763 -4.16838951 -1.83904416 -4.42086715  3.75625905  2.80456714
  1.53741287  4.17074325 -1.72419977 -1.08839342 -1.88479491 -2.03351579
 -1.55327627 -1.61494153 -1.86529399 -1.64841098 -1.28001166 -1.29621049
 -0.73148433 -0.05036367 -1.66821055 -2.20173325 -1.27688425 -0.90782705
 -1.20370193 -1.17328638 -1.18708268 -1.47578442 -0.83499468 -1.47176057
 -1.38632338 -0.69510472 -0.98542065 -2.11855203 -1.81295327 -0.92877808
 -0.45324183 -1.87472271 -1.55926185 -1.51391608  0.32219131  1.73779861
  0.20991982  0.87620239 -1.74533754 -0.85562303 -2.19641845 -1.71560455
 -1.43789754 -1.10106803 -1.58829506 -1.72773525  1.03954484  0.9968954
  0.15891255  0.41123356  0.84862748 -0.41287007 -1.23515224 -1.25171471
  0.49580786  0.87617399 -0.10094151 -0.97935343 25.06141   ]
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.796352876785
gradient value of function right now is: [ 2.81763102e-08  2.81739498e-08  2.71137093e-08  2.81337041e-08
 -2.81757192e-08 -2.81733589e-08 -2.71131408e-08 -2.81331140e-08
  2.93183955e-10  2.98339776e-10  3.08406966e-10  3.05524104e-10
  2.97288706e-10  3.02515028e-10  3.12722713e-10  3.09799744e-10
  3.36353757e-09  3.42249067e-09  3.53792797e-09  3.50488347e-09
  6.88189246e-10  7.00289444e-10  7.23919636e-10  7.17153030e-10
  4.62318564e-11  4.06063907e-10  4.03294139e-10  6.99916469e-11
  3.85721885e-11  3.22086739e-10  3.19617006e-10  5.73730998e-11
  9.18343697e-12  7.72359776e-11  7.66535410e-11  1.37000650e-11
  5.86829301e-12  4.97782129e-11  4.94094787e-11  8.77230097e-12
  3.75258325e-11  5.71973698e-11  3.18020917e-11  4.87791940e-11
  1.90271812e-11  3.02298815e-11  1.55993717e-11  2.55240860e-11
  2.04384606e-11  3.20448256e-11  1.89000127e-11  2.69188165e-11
  6.40463625e-11  9.93790933e-11  4.59330481e-11  8.53115153e-11
 -2.11222042e-12 -9.71693230e-12 -1.42231708e-11  1.89714419e-12
  1.44052262e-11  7.56435135e-13  3.92220398e-12 -6.26894089e-13
  3.27457796e+00]
supnorm grad right now is: 3.274577956348925
Weights right now are: 
[-3.50136169 -4.17424307 -1.84467711 -4.42671235  3.76211311  2.81042072
  1.54304583  4.17658846 -1.72426064 -1.08845536 -1.88485894 -2.03357922
 -1.55333798 -1.61500433 -1.86535891 -1.64847529 -1.28071019 -1.29692126
 -0.73221907 -0.05109155 -1.66835347 -2.20187868 -1.27703459 -0.90797599
 -1.20371154 -1.17337068 -1.18716641 -1.47579896 -0.83500269 -1.47182745
 -1.38638974 -0.69511663 -0.98542255 -2.11856806 -1.81296918 -0.92878093
 -0.45324305 -1.87473304 -1.5592721  -1.5139179   0.32218351  1.73778673
  0.20991321  0.87619226 -1.74534149 -0.85562931 -2.19642169 -1.71560985
 -1.43790178 -1.10107468 -1.58829898 -1.72774084  1.03953154  0.99687477
  0.158903    0.41121586  0.84862792 -0.41286806 -1.23514928 -1.25171513
  0.49580486  0.87617385 -0.10094233 -0.9793533  25.223866  ]
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.161628944365
gradient value of function right now is: [ 2.71264443e-08  2.71241726e-08  2.61061613e-08  2.70854651e-08
 -2.71266275e-08 -2.71243559e-08 -2.61063377e-08 -2.70856481e-08
  2.82608480e-10  2.87569971e-10  2.97270204e-10  2.94492437e-10
  2.86581257e-10  2.91610871e-10  3.01447028e-10  2.98630458e-10
  3.24052571e-09  3.29722686e-09  3.40839845e-09  3.37657518e-09
  6.63004075e-10  6.74641859e-10  6.97398305e-10  6.90881930e-10
  4.46348967e-11  3.91110051e-10  3.88428873e-10  6.75185320e-11
  3.72502863e-11  3.10311399e-10  3.07920808e-10  5.53616977e-11
  8.87133386e-12  7.44351526e-11  7.38711919e-11  1.32237223e-11
  5.67405037e-12  4.80157904e-11  4.76584004e-11  8.47497023e-12
  3.61832824e-11  5.51220011e-11  3.07736694e-11  4.70045595e-11
  1.83409082e-11  2.91294733e-11  1.50840093e-11  2.45916452e-11
  1.97126036e-11  3.08905267e-11  1.82813873e-11  2.59467295e-11
  6.17095627e-11  9.57266400e-11  4.44201899e-11  8.21640449e-11
 -2.03163159e-12 -9.32773711e-12 -1.37420760e-11  1.93466194e-12
  1.39106860e-11  6.45153988e-13  3.78772234e-12 -6.18241071e-13
 -4.17264411e-01]
supnorm grad right now is: 0.4172644111178535
Weights right now are: 
[-3.50687782 -4.17975875 -1.84998547 -4.43222014  3.76762925  2.81593639
  1.54835419  4.18209626 -1.72431807 -1.08851379 -1.88491934 -2.03363907
 -1.55339622 -1.61506358 -1.86542016 -1.64853598 -1.28136888 -1.29759148
 -0.73291189 -0.05177791 -1.66848823 -2.20201581 -1.27717635 -0.90811642
 -1.20372061 -1.17345016 -1.18724535 -1.47581268 -0.83501026 -1.47189051
 -1.38645232 -0.69512789 -0.98542436 -2.11858318 -1.81298419 -0.92878361
 -0.4532442  -1.87474279 -1.55928177 -1.51391963  0.32217615  1.73777552
  0.20990696  0.8761827  -1.74534522 -0.85563523 -2.19642476 -1.71561485
 -1.43790579 -1.10108097 -1.58830269 -1.72774612  1.039519    0.99685531
  0.15889397  0.41119915  0.84862833 -0.41286616 -1.23514648 -1.2517155
  0.49580204  0.8761737  -0.1009431  -0.97935318 24.96959125]
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.513988434726
gradient value of function right now is: [ 2.58311646e-08  2.58290021e-08  2.48619589e-08  2.57921706e-08
 -2.58320733e-08 -2.58299107e-08 -2.48628335e-08 -2.57930780e-08
  2.69464538e-10  2.74202769e-10  2.83452796e-10  2.80802987e-10
  2.73265103e-10  2.78068620e-10  2.87448694e-10  2.84761746e-10
  3.08800141e-09  3.14212038e-09  3.24807046e-09  3.21773080e-09
  6.31836579e-10  6.42944856e-10  6.64633752e-10  6.58420813e-10
  4.24484836e-11  3.73080409e-10  3.70539407e-10  6.42806568e-11
  3.54134509e-11  2.95902096e-10  2.93636197e-10  5.26879243e-11
  8.43732382e-12  7.10066711e-11  7.04719359e-11  1.25901446e-11
  5.40160084e-12  4.58484939e-11  4.55093251e-11  8.07664942e-12
  3.44730475e-11  5.25515217e-11  2.91850340e-11  4.48191217e-11
  1.74847407e-11  2.77816994e-11  1.43218288e-11  2.34582804e-11
  1.87777222e-11  2.94448133e-11  1.73504775e-11  2.47357878e-11
  5.88591519e-11  9.13368281e-11  4.21665490e-11  7.84122121e-11
 -1.94045493e-12 -8.94029986e-12 -1.30558676e-11  1.71087796e-12
  1.32298495e-11  7.19106024e-13  3.60424779e-12 -5.72022458e-13
  4.39068931e+00]
supnorm grad right now is: 4.390689313224206
Weights right now are: 
[-3.51215238 -4.18503286 -1.85506187 -4.43748674  3.77290382  2.82121052
  1.55343059  4.18736286 -1.72437305 -1.08856974 -1.88497718 -2.03369636
 -1.55345197 -1.61512032 -1.86547881 -1.64859408 -1.28199917 -1.2982328
 -0.73357483 -0.05243466 -1.66861719 -2.20214704 -1.27731199 -0.9082508
 -1.20372929 -1.17352624 -1.18732091 -1.47582583 -0.83501751 -1.47195088
 -1.38651222 -0.69513866 -0.98542608 -2.11859767 -1.81299857 -0.92878619
 -0.45324531 -1.87475214 -1.55929105 -1.51392128  0.32216912  1.73776479
  0.20990098  0.87617355 -1.74534879 -0.85564091 -2.19642769 -1.71561964
 -1.43790963 -1.10108699 -1.58830625 -1.72775117  1.03950699  0.99683666
  0.15888532  0.41118315  0.84862873 -0.41286435 -1.23514381 -1.25171586
  0.49579933  0.87617355 -0.10094383 -0.97935306 25.2504024 ]
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.943774748677
gradient value of function right now is: [ 2.50628430e-08  2.50607456e-08  2.41248576e-08  2.50250461e-08
 -2.50642221e-08 -2.50621246e-08 -2.41261851e-08 -2.50264232e-08
  2.61734956e-10  2.66323570e-10  2.75303063e-10  2.72731181e-10
  2.65441364e-10  2.70093453e-10  2.79199697e-10  2.76591622e-10
  2.99802414e-09  3.05040879e-09  3.15321200e-09  3.12377837e-09
  6.13402086e-10  6.24154161e-10  6.45198081e-10  6.39170903e-10
  4.13665176e-11  3.61920329e-10  3.59431301e-10  6.25424487e-11
  3.45292821e-11  2.87203452e-10  2.84984206e-10  5.12914548e-11
  8.22851860e-12  6.89362210e-11  6.84123370e-11  1.22592741e-11
  5.27226407e-12  4.45465775e-11  4.42139763e-11  7.87076075e-12
  3.35089575e-11  5.10310795e-11  2.85643679e-11  4.35132931e-11
  1.69841938e-11  2.69685914e-11  1.39968183e-11  2.27655617e-11
  1.82605556e-11  2.86055832e-11  1.69670691e-11  2.40260507e-11
  5.71276316e-11  8.86028077e-11  4.12178984e-11  7.60429414e-11
 -1.87773179e-12 -8.61516847e-12 -1.27434507e-11  1.85705200e-12
  1.28973573e-11  5.45328091e-13  3.51283493e-12 -5.81148993e-13
 -2.47315040e+00]
supnorm grad right now is: 2.4731503976762483
Weights right now are: 
[-3.51729898 -4.19017903 -1.86001557 -4.44262557  3.77805045  2.82635672
  1.55838432  4.19250173 -1.72442677 -1.0886244  -1.88503368 -2.03375234
 -1.55350645 -1.61517575 -1.86553611 -1.64865084 -1.2826146  -1.29885901
 -0.73422216 -0.05307594 -1.66874312 -2.20227517 -1.27744445 -0.90838202
 -1.20373775 -1.17360061 -1.18739476 -1.47583865 -0.83502456 -1.47200986
 -1.38657075 -0.69514917 -0.98542776 -2.11861182 -1.81301262 -0.9287887
 -0.45324638 -1.87476128 -1.55930013 -1.51392289  0.32216225  1.7377543
  0.20989516  0.87616461 -1.74535227 -0.85564646 -2.19643055 -1.71562432
 -1.43791337 -1.10109287 -1.5883097  -1.72775611  1.03949526  0.99681844
  0.15887691  0.41116752  0.84862911 -0.41286258 -1.23514119 -1.25171619
  0.49579668  0.87617341 -0.10094456 -0.97935295 24.83219379]
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.121111687547
gradient value of function right now is: [ 2.39626114e-08  2.39606066e-08  2.30678620e-08  2.39264989e-08
 -2.39625380e-08 -2.39605332e-08 -2.30677912e-08 -2.39264256e-08
  2.50538475e-10  2.54936647e-10  2.63532655e-10  2.61069824e-10
  2.54099828e-10  2.58559079e-10  2.67276891e-10  2.64779269e-10
  2.86817480e-09  2.91835819e-09  3.01671638e-09  2.98854640e-09
  5.86874284e-10  5.97175068e-10  6.17310401e-10  6.11541611e-10
  3.95081606e-11  3.46556753e-10  3.44186775e-10  5.97893236e-11
  3.29674468e-11  2.74929150e-10  2.72815976e-10  4.90177343e-11
  7.85933527e-12  6.60142887e-11  6.55152727e-11  1.17202611e-11
  5.04024628e-12  4.26971094e-11  4.23800279e-11  7.53148887e-12
  3.20546718e-11  4.88456304e-11  2.72159018e-11  4.16540020e-11
  1.62554587e-11  2.58221833e-11  1.33492755e-11  2.18006431e-11
  1.74656583e-11  2.73769831e-11  1.61754125e-11  2.29961939e-11
  5.46999028e-11  8.48655930e-11  3.93061811e-11  7.28461030e-11
 -1.79990058e-12 -8.28069512e-12 -1.21626451e-11  1.66861676e-12
  1.23206406e-11  6.06907544e-13  3.35762897e-12 -5.41745317e-13
  1.21447391e+00]
supnorm grad right now is: 1.2144739081711207
Weights right now are: 
[-3.52226571 -4.19514534 -1.86479658 -4.44758481  3.78301717  2.83132302
  1.56316533  4.19746096 -1.72447867 -1.08867721 -1.88508828 -2.03380642
 -1.55355909 -1.61522931 -1.86559148 -1.64870569 -1.2832089  -1.29946374
 -0.73484728 -0.05369522 -1.66886472 -2.20239892 -1.27757238 -0.90850875
 -1.20374591 -1.17367248 -1.18746615 -1.47585101 -0.83503137 -1.47206685
 -1.38662731 -0.6951593  -0.98542938 -2.11862551 -1.8130262  -0.92879112
 -0.45324742 -1.87477013 -1.55930891 -1.51392444  0.3221556   1.73774418
  0.20988956  0.87615598 -1.74535564 -0.8556518  -2.19643329 -1.71562884
 -1.43791699 -1.10109853 -1.58831303 -1.72776087  1.03948392  0.99680085
  0.15886882  0.41115241  0.84862949 -0.41286086 -1.23513869 -1.25171653
  0.49579414  0.87617327 -0.10094525 -0.97935284 25.07455068]
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.527513980776
gradient value of function right now is: [ 2.32714510e-08  2.32695047e-08  2.24045433e-08  2.32364116e-08
 -2.32718729e-08 -2.32699266e-08 -2.24049496e-08 -2.32368329e-08
  2.43557232e-10  2.47821819e-10  2.56173993e-10  2.53781314e-10
  2.47031814e-10  2.51355838e-10  2.59826786e-10  2.57400186e-10
  2.78700811e-09  2.83564462e-09  2.93116987e-09  2.90381461e-09
  5.70255701e-10  5.80238975e-10  5.99794050e-10  5.94192189e-10
  3.85150984e-11  3.36523396e-10  3.34202377e-10  5.82038027e-11
  3.21555662e-11  2.67091515e-10  2.65021932e-10  4.77421504e-11
  7.66734865e-12  6.41471930e-11  6.36583446e-11  1.14176918e-11
  4.92088786e-12  4.15201124e-11  4.12092229e-11  7.34260097e-12
  3.11785177e-11  4.74653118e-11  2.66324931e-11  4.04718617e-11
  1.58022833e-11  2.50853884e-11  1.30464606e-11  2.11750519e-11
  1.69946965e-11  2.66128099e-11  1.58187428e-11  2.23519172e-11
  5.31389689e-11  8.23994075e-11  3.84174710e-11  7.07156513e-11
 -1.74398753e-12 -7.99969054e-12 -1.18677156e-11  1.77952246e-12
  1.20098787e-11  4.64954165e-13  3.27200358e-12 -5.48077398e-13
 -4.10365884e+00]
supnorm grad right now is: 4.103658837956271
Weights right now are: 
[-3.52700505 -4.19988428 -1.86935914 -4.45231701  3.78775651  2.83606196
  1.56772789  4.20219316 -1.72452824 -1.08872766 -1.88514042 -2.03385808
 -1.55360936 -1.61528047 -1.86564437 -1.64875808 -1.28377631 -1.30004106
 -0.73544405 -0.05428642 -1.66898082 -2.20251705 -1.27769449 -0.90862972
 -1.20375375 -1.17374101 -1.18753421 -1.47586284 -0.8350379  -1.47212123
 -1.38668127 -0.695169   -0.98543094 -2.11863856 -1.81303916 -0.92879344
 -0.45324842 -1.87477857 -1.55931729 -1.51392593  0.32214925  1.73773452
  0.20988416  0.87614774 -1.74535886 -0.8556569  -2.19643594 -1.71563315
 -1.43792045 -1.10110394 -1.58831624 -1.72776542  1.03947309  0.99678408
  0.15886102  0.41113801  0.84862984 -0.41285923 -1.23513628 -1.25171688
  0.4957917   0.87617317 -0.10094591 -0.97935273 24.68933782]
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.160361203469
gradient value of function right now is: [ 2.23309058e-08  2.23290387e-08  2.15007619e-08  2.22973033e-08
 -2.23322663e-08 -2.23303990e-08 -2.15020718e-08 -2.22986618e-08
  2.33975530e-10  2.38078417e-10  2.46102912e-10  2.43803385e-10
  2.37322047e-10  2.41482275e-10  2.49621181e-10  2.47288967e-10
  2.67594362e-09  2.72271209e-09  2.81444114e-09  2.78816472e-09
  5.47576541e-10  5.57176992e-10  5.75956469e-10  5.70575104e-10
  3.69111215e-11  3.23414172e-10  3.21197308e-10  5.58383941e-11
  3.08047458e-11  2.56603875e-10  2.54627231e-10  4.57850600e-11
  7.34793356e-12  6.16495249e-11  6.11824759e-11  1.09535690e-11
  4.71979002e-12  3.99367242e-11  3.96394687e-11  7.04997295e-12
  2.99322625e-11  4.55999546e-11  2.54562467e-11  3.88847028e-11
  1.51788335e-11  2.41078732e-11  1.24835456e-11  2.03522091e-11
  1.63128705e-11  2.55635076e-11  1.51285343e-11  2.14721549e-11
  5.10654936e-11  7.92162961e-11  3.67570049e-11  6.79928253e-11
 -1.67811921e-12 -7.71721042e-12 -1.13670337e-11  1.59860191e-12
  1.15141873e-11  5.34343137e-13  3.13885000e-12 -5.11102940e-13
 -4.40609828e-01]
supnorm grad right now is: 0.44060982755428063
Weights right now are: 
[-3.53151328 -4.20439214 -1.8736996  -4.45681846  3.79226474  2.84056982
  1.57206835  4.2066946  -1.72457544 -1.08877569 -1.88519007 -2.03390726
 -1.55365724 -1.61532919 -1.86569472 -1.64880797 -1.28431635 -1.30059051
 -0.73601201 -0.05484908 -1.66909132 -2.20262949 -1.27781071 -0.90874486
 -1.20376121 -1.17380621 -1.18759896 -1.47587413 -0.83504414 -1.47217298
 -1.38673262 -0.69517826 -0.98543243 -2.11865099 -1.81305149 -0.92879566
 -0.45324938 -1.87478662 -1.55932528 -1.51392736  0.32214321  1.73772532
  0.20987899  0.8761399  -1.74536192 -0.85566177 -2.19643848 -1.71563725
 -1.43792374 -1.10110911 -1.58831931 -1.72776975  1.0394628   0.9967681
  0.15885356  0.41112431  0.84863018 -0.41285768 -1.23513397 -1.25171723
  0.49578937  0.87617308 -0.10094655 -0.97935262 24.9217504 ]
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.83865408058
gradient value of function right now is: [ 2.14494680e-08  2.14476750e-08  2.06537268e-08  2.14172119e-08
 -2.14498629e-08 -2.14480698e-08 -2.06541072e-08 -2.14176062e-08
  2.24974480e-10  2.28924092e-10  2.36640357e-10  2.34428532e-10
  2.28202733e-10  2.32207731e-10  2.40034373e-10  2.37791004e-10
  2.57167707e-09  2.61667587e-09  2.70483634e-09  2.67957500e-09
  5.26281880e-10  5.35519657e-10  5.53569928e-10  5.48396057e-10
  3.54211039e-11  3.11060048e-10  3.08938209e-10  5.36281016e-11
  2.95533690e-11  2.46737335e-10  2.44845287e-10  4.39607865e-11
  7.05184772e-12  5.92985005e-11  5.88513028e-11  1.05206764e-11
  4.53323782e-12  3.84449284e-11  3.81600935e-11  6.77681018e-12
  2.87630883e-11  4.38416999e-11  2.43780235e-11  3.73888479e-11
  1.45926864e-11  2.31849054e-11  1.19654483e-11  1.95755029e-11
  1.56738069e-11  2.45748293e-11  1.44957733e-11  2.06434865e-11
  4.91121187e-11  7.62072548e-11  3.52258473e-11  6.54191622e-11
 -1.61545872e-12 -7.44800848e-12 -1.09017099e-11  1.45453919e-12
  1.10512075e-11  5.78208569e-13  3.01384623e-12 -4.80506079e-13
  3.07986455e+00]
supnorm grad right now is: 3.07986454784803
Weights right now are: 
[-3.53593703 -4.20881552 -1.87795905 -4.46123555  3.79668848  2.84499319
  1.57632779  4.21111169 -1.72462181 -1.08882287 -1.88523884 -2.03395558
 -1.55370428 -1.61537705 -1.8657442  -1.64885698 -1.28484656 -1.30113
 -0.73656967 -0.05540153 -1.66919982 -2.20273989 -1.27792484 -0.90885792
 -1.20376853 -1.17387031 -1.18766261 -1.4758852  -0.83505024 -1.47222384
 -1.38678308 -0.69518733 -0.98543389 -2.11866321 -1.81306362 -0.92879783
 -0.45325031 -1.87479454 -1.55933314 -1.51392876  0.32213728  1.73771628
  0.20987395  0.87613219 -1.74536493 -0.85566655 -2.19644095 -1.71564128
 -1.43792697 -1.10111417 -1.58832231 -1.72777401  1.03945268  0.9967524
  0.15884628  0.41111084  0.84863051 -0.41285616 -1.23513172 -1.25171755
  0.49578708  0.87617297 -0.10094717 -0.97935252 25.20193445]
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.49917491452
gradient value of function right now is: [ 2.09754548e-08  2.09737019e-08  2.01989624e-08  2.09439388e-08
 -2.09737490e-08 -2.09719963e-08 -2.01973198e-08 -2.09422357e-08
  2.20178194e-10  2.24030935e-10  2.31578085e-10  2.29415203e-10
  2.23351149e-10  2.27258140e-10  2.34913710e-10  2.32719851e-10
  2.51588254e-09  2.55975962e-09  2.64595444e-09  2.62126170e-09
  5.14850554e-10  5.23858054e-10  5.41505476e-10  5.36448169e-10
  3.47965161e-11  3.04012356e-10  3.01915472e-10  5.25846623e-11
  2.90514761e-11  2.41290847e-10  2.39421074e-10  4.31335778e-11
  6.93308023e-12  5.79998326e-11  5.75578040e-11  1.03242981e-11
  4.45986824e-12  3.76267313e-11  3.73449639e-11  6.65467158e-12
  2.81734485e-11  4.28885423e-11  2.40661347e-11  3.65701969e-11
  1.42821294e-11  2.26712067e-11  1.17917797e-11  1.91375956e-11
  1.53594325e-11  2.40508573e-11  1.42971928e-11  2.02006016e-11
  4.80250753e-11  7.44667374e-11  3.47206431e-11  6.39090349e-11
 -1.57458842e-12 -7.22895987e-12 -1.07218669e-11  1.60780896e-12
  1.08537979e-11  4.19524924e-13  2.95863486e-12 -4.95300678e-13
 -4.21434369e+00]
supnorm grad right now is: 4.214343689809268
Weights right now are: 
[-3.54017862 -4.21305676 -1.88204341 -4.46547076  3.80093006  2.84923442
  1.58041214  4.21534689 -1.72466633 -1.08886816 -1.88528566 -2.03400196
 -1.55374943 -1.61542299 -1.86579169 -1.64890403 -1.28535523 -1.30164759
 -0.7371047  -0.05593157 -1.66930393 -2.20284583 -1.27803434 -0.9089664
 -1.20377552 -1.1739319  -1.18772379 -1.47589579 -0.83505607 -1.47227267
 -1.38683154 -0.69519601 -0.98543528 -2.11867495 -1.81307527 -0.9287999
 -0.45325121 -1.87480215 -1.5593407  -1.5139301   0.32213159  1.7377076
  0.20986915  0.87612479 -1.74536782 -0.85567114 -2.1964433  -1.71564516
 -1.43793007 -1.10111904 -1.58832516 -1.72777809  1.03944296  0.99673731
  0.15883935  0.41109788  0.84863083 -0.41285468 -1.23512956 -1.25171783
  0.49578489  0.87617285 -0.10094777 -0.97935243 24.64228057]
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.167835961076
gradient value of function right now is: [ 2.01621257e-08  2.01604412e-08  1.94171608e-08  2.01318483e-08
 -2.01615399e-08 -2.01598554e-08 -1.94165965e-08 -2.01312634e-08
  2.11862502e-10  2.15576152e-10  2.22839409e-10  2.20757198e-10
  2.14922243e-10  2.18688308e-10  2.26056134e-10  2.23944036e-10
  2.41961793e-09  2.46189016e-09  2.54480027e-09  2.52104064e-09
  4.95201825e-10  5.03880571e-10  5.20857181e-10  5.15990504e-10
  3.33923229e-11  2.92675285e-10  2.90670656e-10  5.05222419e-11
  2.78671999e-11  2.32207104e-10  2.30419649e-10  4.14246158e-11
  6.65279912e-12  5.58346677e-11  5.54119688e-11  9.91866594e-12
  4.28288877e-12  3.62505203e-11  3.59808828e-11  6.39819387e-12
  2.70899706e-11  4.12712301e-11  2.30258729e-11  3.51947052e-11
  1.37409619e-11  2.18240993e-11  1.12954471e-11  1.84250095e-11
  1.47658438e-11  2.31397214e-11  1.36877286e-11  1.94369998e-11
  4.62295800e-11  7.17140165e-11  3.32563660e-11  6.15561742e-11
 -1.51811101e-12 -6.98973917e-12 -1.02824924e-11  1.43328485e-12
  1.04192888e-11  4.93496734e-13  2.84188509e-12 -4.60992220e-13
  1.48457915e-01]
supnorm grad right now is: 0.14845791480784698
Weights right now are: 
[-3.54432729 -4.21720508 -1.88603861 -4.4696132   3.80507872  2.85338273
  1.58440734  4.21948932 -1.7247099  -1.0889125  -1.8853315  -2.03404737
 -1.55379363 -1.61546797 -1.86583819 -1.64895009 -1.28585299 -1.30215406
 -0.73762824 -0.05645021 -1.6694058  -2.20294949 -1.2781415  -0.90907255
 -1.20378237 -1.17399214 -1.18778362 -1.47590617 -0.83506179 -1.47232045
 -1.38687896 -0.69520452 -0.98543664 -2.11868644 -1.81308667 -0.92880194
 -0.45325209 -1.87480961 -1.5593481  -1.51393141  0.32212602  1.73769912
  0.20986444  0.87611755 -1.74537064 -0.85567563 -2.19644562 -1.71564895
 -1.43793311 -1.10112379 -1.58832796 -1.72778209  1.03943344  0.99672256
  0.15883254  0.41108521  0.84863115 -0.41285323 -1.23512744 -1.25171813
  0.49578276  0.87617275 -0.10094835 -0.97935233 25.00964924]
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16004.99112543836
gradient value of function right now is: [ 1.96112636e-08  1.96096255e-08  1.88880520e-08  1.95818339e-08
 -1.96107523e-08 -1.96091143e-08 -1.88875597e-08 -1.95813234e-08
  2.06253202e-10  2.09864286e-10  2.16933276e-10  2.14906752e-10
  2.09240567e-10  2.12902771e-10  2.20073829e-10  2.18018132e-10
  2.35455747e-09  2.39564457e-09  2.47630288e-09  2.45318849e-09
  4.81903147e-10  4.90338916e-10  5.06855029e-10  5.02120351e-10
  3.25425783e-11  2.84760903e-10  2.82803690e-10  4.92085710e-11
  2.71634796e-11  2.25971490e-10  2.24226370e-10  4.03557042e-11
  6.48622825e-12  5.43476839e-11  5.39348941e-11  9.66487562e-12
  4.17846449e-12  3.53082786e-11  3.50447727e-11  6.23864152e-12
  2.63768584e-11  4.01704891e-11  2.24754410e-11  3.42536823e-11
  1.33766759e-11  2.12404724e-11  1.10201283e-11  1.79307130e-11
  1.43799670e-11  2.25269884e-11  1.33570669e-11  1.89211235e-11
  4.49901865e-11  6.97782159e-11  3.24470589e-11  5.98888279e-11
 -1.47566653e-12 -6.78531506e-12 -1.00266457e-11  1.45259956e-12
  1.01563031e-11  4.35995649e-13  2.77015542e-12 -4.56121636e-13
 -2.22686538e+00]
supnorm grad right now is: 2.226865378522363
Weights right now are: 
[-3.54830323 -4.22118069 -1.88986777 -4.47358317  3.80905467  2.85735835
  1.5882365   4.2234593  -1.7247517  -1.08895504 -1.88537546 -2.03409092
 -1.55383603 -1.61551112 -1.86588278 -1.64899428 -1.28633025 -1.30263966
 -0.7381302  -0.05694748 -1.66950348 -2.20304888 -1.27824424 -0.90917434
 -1.20378895 -1.1740499  -1.18784099 -1.47591612 -0.83506728 -1.47236626
 -1.38692442 -0.69521268 -0.98543795 -2.11869746 -1.81309761 -0.92880389
 -0.45325293 -1.87481677 -1.5593552  -1.51393267  0.32212067  1.73769097
  0.20985991  0.87611061 -1.74537335 -0.85567993 -2.19644784 -1.71565258
 -1.43793602 -1.10112836 -1.58833065 -1.72778592  1.03942431  0.99670841
  0.15882599  0.41107307  0.84863144 -0.41285186 -1.23512541 -1.25171842
  0.4957807   0.87617265 -0.10094891 -0.97935224 24.79693606]
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.16850326888
gradient value of function right now is: [ 1.92475316e-08  1.92459241e-08  1.85383631e-08  1.92186550e-08
 -1.92489260e-08 -1.92473184e-08 -1.85397062e-08 -1.92200474e-08
  2.02534532e-10  2.06083522e-10  2.13025601e-10  2.11035135e-10
  2.05469206e-10  2.09068463e-10  2.16110817e-10  2.14091686e-10
  2.31152979e-09  2.35190074e-09  2.43109134e-09  2.40839384e-09
  4.73122692e-10  4.81411801e-10  4.97628246e-10  4.92978717e-10
  3.19138204e-11  2.79693519e-10  2.77777584e-10  4.82848501e-11
  2.66335966e-11  2.21910161e-10  2.20201796e-10  3.95905449e-11
  6.36073624e-12  5.33791505e-11  5.29749990e-11  9.48314928e-12
  4.09911508e-12  3.46918970e-11  3.44338198e-11  6.12358237e-12
  2.58916677e-11  3.94452779e-11  2.20094094e-11  3.36373872e-11
  1.31343901e-11  2.08606986e-11  1.07977904e-11  1.76114925e-11
  1.41140421e-11  2.21181428e-11  1.30844929e-11  1.85787395e-11
  4.41870812e-11  6.85454496e-11  3.17902192e-11  5.88359871e-11
 -1.45029925e-12 -6.67992943e-12 -9.82840577e-12  1.37280961e-12
  9.96066859e-12  4.69385601e-13  2.71763237e-12 -4.40964542e-13
 -1.56201849e-02]
supnorm grad right now is: 0.015620184867876936
Weights right now are: 
[-3.55216758 -4.22504472 -1.89358973 -4.47744173  3.81291902  2.86122238
  1.59195846  4.22731786 -1.72479236 -1.08899641 -1.88541823 -2.03413329
 -1.55387728 -1.61555309 -1.86592617 -1.64903726 -1.28679432 -1.30311184
 -0.73861827 -0.057431   -1.66959847 -2.20314554 -1.27834415 -0.90927331
 -1.20379535 -1.17410607 -1.18789677 -1.47592581 -0.83507262 -1.47241082
 -1.38696864 -0.69522062 -0.98543923 -2.11870818 -1.81310824 -0.9288058
 -0.45325375 -1.87482373 -1.55936212 -1.5139339   0.32211547  1.73768305
  0.2098555   0.87610385 -1.74537599 -0.85568412 -2.19645001 -1.71565612
 -1.43793885 -1.1011328  -1.58833328 -1.72778965  1.03941544  0.99669464
  0.15881962  0.41106125  0.84863174 -0.41285052 -1.23512344 -1.2517187
  0.4957787   0.87617255 -0.10094946 -0.97935216 24.78351861]
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.165902050963
gradient value of function right now is: [ 1.89662260e-08  1.89646421e-08  1.82680589e-08  1.89377802e-08
 -1.89656103e-08 -1.89640265e-08 -1.82674658e-08 -1.89371655e-08
  1.99649441e-10  2.03147317e-10  2.09990004e-10  2.08027944e-10
  2.02548570e-10  2.06096090e-10  2.13037832e-10  2.11047455e-10
  2.27812580e-09  2.31790697e-09  2.39594747e-09  2.37357856e-09
  4.66297561e-10  4.74465783e-10  4.90447122e-10  4.85864785e-10
  3.14590712e-11  2.75674762e-10  2.73785918e-10  4.75950631e-11
  2.62544419e-11  2.18724922e-10  2.17040724e-10  3.90255116e-11
  6.27094316e-12  5.26193437e-11  5.22208602e-11  9.34893821e-12
  4.04259385e-12  3.42093256e-11  3.39547813e-11  6.03891702e-12
  2.55218467e-11  3.88806511e-11  2.16986604e-11  3.31558204e-11
  1.29469577e-11  2.05626134e-11  1.06451830e-11  1.73597637e-11
  1.39129831e-11  2.18023937e-11  1.28996939e-11  1.83134904e-11
  4.35552137e-11  6.75642257e-11  3.13410684e-11  5.79934433e-11
 -1.42933718e-12 -6.58281908e-12 -9.68880310e-12  1.35711662e-12
  9.81931970e-12  4.59840281e-13  2.67925762e-12 -4.35032210e-13
 -2.30203479e-01]
supnorm grad right now is: 0.23020347900237234
Weights right now are: 
[-3.55591252 -4.22878935 -1.89719693 -4.48118105  3.81666397  2.86496701
  1.59556566  4.23105719 -1.72483179 -1.08903653 -1.88545971 -2.03417438
 -1.55391729 -1.6155938  -1.86596825 -1.64907894 -1.28724422 -1.30356959
 -0.73909143 -0.05789974 -1.66969055 -2.20323923 -1.278441   -0.90936926
 -1.20380158 -1.17416048 -1.1879508  -1.47593523 -0.83507782 -1.47245401
 -1.38701149 -0.69522835 -0.98544047 -2.11871857 -1.81311855 -0.92880765
 -0.45325455 -1.87483049 -1.55936882 -1.51393509  0.32211044  1.73767537
  0.20985119  0.8760973  -1.74537855 -0.85568819 -2.19645212 -1.71565955
 -1.4379416  -1.10113711 -1.58833584 -1.72779327  1.03940685  0.9966813
  0.1588134   0.4110498   0.84863202 -0.41284922 -1.23512153 -1.25171896
  0.49577676  0.87617246 -0.10094999 -0.97935207 24.65780481]
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.165172187488
gradient value of function right now is: [ 1.86897755e-08  1.86882149e-08  1.80023981e-08  1.86617528e-08
 -1.86885519e-08 -1.86869915e-08 -1.80012195e-08 -1.86605311e-08
  1.96819264e-10  2.00267198e-10  2.07012454e-10  2.05078229e-10
  1.99681722e-10  2.03178668e-10  2.10021721e-10  2.08059537e-10
  2.24535665e-09  2.28456157e-09  2.36147462e-09  2.33942771e-09
  4.59603798e-10  4.67653931e-10  4.83404850e-10  4.78888344e-10
  3.10106842e-11  2.71737726e-10  2.69875760e-10  4.69164100e-11
  2.58803930e-11  2.15602103e-10  2.13941861e-10  3.84692545e-11
  6.18234810e-12  5.18743413e-11  5.14814779e-11  9.21679469e-12
  3.98678568e-12  3.37359351e-11  3.34848976e-11  5.95550345e-12
  2.51583648e-11  3.83265982e-11  2.13904376e-11  3.26833978e-11
  1.27629291e-11  2.02701497e-11  1.04942589e-11  1.71128925e-11
  1.37152211e-11  2.14922683e-11  1.27168192e-11  1.80530330e-11
  4.29356945e-11  6.66027677e-11  3.08963185e-11  5.71682700e-11
 -1.40879832e-12 -6.48894870e-12 -9.55084459e-12  1.33859215e-12
  9.67985015e-12  4.52552212e-13  2.64136913e-12 -4.28968500e-13
 -2.73091507e-01]
supnorm grad right now is: 0.2730915071830876
Weights right now are: 
[-3.55956688 -4.23244339 -1.9007171  -4.48482993  3.82031832  2.86862106
  1.59908584  4.23470606 -1.72487031 -1.08907573 -1.88550022 -2.03421451
 -1.55395637 -1.61563356 -1.86600935 -1.64911966 -1.28768344 -1.30401648
 -0.73955337 -0.05835737 -1.66978047 -2.20333072 -1.27853557 -0.90946295
 -1.20380764 -1.17421367 -1.18800363 -1.4759444  -0.83508287 -1.4724962
 -1.38705336 -0.69523587 -0.98544168 -2.11872872 -1.81312863 -0.92880945
 -0.45325533 -1.8748371  -1.55937538 -1.51393626  0.32210551  1.73766787
  0.20984702  0.87609091 -1.74538104 -0.85569215 -2.19645416 -1.7156629
 -1.43794428 -1.10114132 -1.58833832 -1.7277968   1.03939844  0.99666826
  0.15880737  0.41103861  0.84863229 -0.41284795 -1.23511966 -1.25171922
  0.49577486  0.87617237 -0.10095051 -0.97935199 25.07448567]
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.168043922335
gradient value of function right now is: [ 1.84162126e-08  1.84146750e-08  1.77394876e-08  1.83886080e-08
 -1.84153094e-08 -1.84137719e-08 -1.77386176e-08 -1.83877062e-08
  1.94021237e-10  1.97420091e-10  2.04069117e-10  2.02162368e-10
  1.96846149e-10  2.00293372e-10  2.07038907e-10  2.05104565e-10
  2.21296298e-09  2.25160164e-09  2.32740123e-09  2.30567214e-09
  4.52988228e-10  4.60922339e-10  4.76445748e-10  4.71994203e-10
  3.05641987e-11  2.67853749e-10  2.66018831e-10  4.62426872e-11
  2.55074546e-11  2.12517879e-10  2.10881749e-10  3.79163540e-11
  6.09400298e-12  5.11384572e-11  5.07512528e-11  9.08543035e-12
  3.93109235e-12  3.32681094e-11  3.30206074e-11  5.87252598e-12
  2.47987170e-11  3.77789455e-11  2.10807973e-11  3.22167852e-11
  1.25809602e-11  1.99810839e-11  1.03429759e-11  1.68690927e-11
  1.35192668e-11  2.11853096e-11  1.25332752e-11  1.77954163e-11
  4.23243883e-11  6.56541522e-11  3.04506740e-11  5.63548491e-11
 -1.38861476e-12 -6.39759497e-12 -9.41281232e-12  1.31588814e-12
  9.54052448e-12  4.48857002e-13  2.60353439e-12 -4.22381917e-13
 -1.01497046e-01]
supnorm grad right now is: 0.10149704596331563
Weights right now are: 
[-3.56309937 -4.23597559 -1.90412012 -4.48835713  3.82385081  2.87215325
  1.60248885  4.23823326 -1.72490757 -1.08911364 -1.88553941 -2.03425334
 -1.55399417 -1.61567203 -1.86604912 -1.64915905 -1.28810817 -1.30444863
 -0.74000006 -0.05879989 -1.66986742 -2.20341919 -1.27862702 -0.90955354
 -1.20381351 -1.17426508 -1.18805469 -1.47595328 -0.83508777 -1.472537
 -1.38709384 -0.69524315 -0.98544285 -2.11873854 -1.81313838 -0.92881119
 -0.45325609 -1.87484349 -1.55938173 -1.51393739  0.32210075  1.73766062
  0.20984297  0.87608472 -1.74538346 -0.85569599 -2.19645615 -1.71566614
 -1.43794688 -1.10114538 -1.58834073 -1.72780022  1.03939032  0.99665566
  0.15880152  0.41102779  0.84863256 -0.41284672 -1.23511785 -1.25171948
  0.49577303  0.87617229 -0.10095101 -0.9793519  25.13098305]
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -16005.16841516864
gradient value of function right now is: [ 1.81549892e-08  1.81534736e-08  1.74884351e-08  1.81277841e-08
 -1.81538348e-08 -1.81523193e-08 -1.74873231e-08 -1.81266314e-08
  1.91345721e-10  1.94697466e-10  2.01254427e-10  1.99373977e-10
  1.94135421e-10  1.97534931e-10  2.04187194e-10  2.02279496e-10
  2.18199443e-09  2.22008971e-09  2.29482429e-09  2.27339934e-09
  4.46663153e-10  4.54485923e-10  4.69791703e-10  4.65402329e-10
  3.01392763e-11  2.64135060e-10  2.62325705e-10  4.56002798e-11
  2.51528475e-11  2.09567110e-10  2.07953761e-10  3.73896005e-11
  6.00999009e-12  5.04343249e-11  5.00524672e-11  8.96026319e-12
  3.87812509e-12  3.28203501e-11  3.25761879e-11  5.79345117e-12
  2.44548866e-11  3.72552348e-11  2.07877854e-11  3.17702737e-11
  1.24069584e-11  1.97047027e-11  1.01996562e-11  1.66358156e-11
  1.33321277e-11  2.08920709e-11  1.23595477e-11  1.75491464e-11
  4.17389907e-11  6.47459692e-11  3.00282562e-11  5.55755288e-11
 -1.36921797e-12 -6.30956118e-12 -9.28193105e-12  1.29679658e-12
  9.40835997e-12  4.43082450e-13  2.56768364e-12 -4.16506165e-13
 -4.29495869e-02]
supnorm grad right now is: 0.04294958690690564
Weights right now are: 
[-3.56654188 -4.23941782 -1.90743666 -4.49179449  3.82729332  2.87559548
  1.60580539  4.24167062 -1.72494391 -1.08915062 -1.88557763 -2.0342912
 -1.55403105 -1.61570955 -1.8660879  -1.64919747 -1.28852225 -1.30486993
 -0.74043554 -0.05923131 -1.66995219 -2.20350545 -1.27871619 -0.90964187
 -1.20381923 -1.17431523 -1.1881045  -1.47596194 -0.83509255 -1.47257679
 -1.38713332 -0.69525024 -0.98544399 -2.11874812 -1.81314789 -0.9288129
 -0.45325683 -1.87484973 -1.55938792 -1.51393849  0.32209611  1.73765354
  0.20983902  0.87607869 -1.74538582 -0.85569973 -2.19645809 -1.7156693
 -1.43794941 -1.10114935 -1.58834307 -1.72780355  1.03938239  0.99664337
  0.15879582  0.41101724  0.84863282 -0.41284552 -1.23511609 -1.25171972
  0.49577124  0.87617221 -0.1009515  -0.97935182 24.82887416]
NN weights: [-3.55765898 -4.23053566 -1.89887947 -4.48292489  3.81841042  2.86671332
  1.5972482   4.23280102 -1.72485023 -1.08905529 -1.8854791  -2.03419359
 -1.553936   -1.61561283 -1.86598793 -1.64909844 -1.2874543  -1.30378333
 -0.73931237 -0.05811862 -1.66973356 -2.203283   -1.27848624 -0.90941408
 -1.20380447 -1.17418594 -1.18797609 -1.47593961 -0.83508023 -1.4724742
 -1.38703153 -0.69523194 -0.98544105 -2.11872343 -1.81312338 -0.92880851
 -0.45325493 -1.87483366 -1.55937197 -1.51393565  0.32210808  1.73767178
  0.2098492   0.87609424 -1.74537974 -0.85569008 -2.19645309 -1.71566115
 -1.43794289 -1.10113912 -1.58833702 -1.72779496  1.03940282  0.99667506
  0.15881052  0.41104444  0.84863215 -0.41284861 -1.23512063 -1.25171909
  0.49577585  0.87617242 -0.10095024 -0.97935203]
Minimum obj value:-16005.16841516864
Optimal xi: 24.98885056401712
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1551.5577480618397
W_T_median: 1405.4383972921596
W_T_pctile_5: 624.6754498206103
W_T_CVAR_5_pct: 489.591035198856
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 10.0
F value: -16005.16841516864
-----------------------------------------------
