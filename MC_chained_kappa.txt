Starting at: 
2022-07-06 10:02:47

 Random seed:  1  



############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = basic_T30_VWD
timeseries_basket['basket_desc'] = CRSP data: T30 and VWD
timeseries_basket['basket_columns'] = 
['T30_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
5.0% of MC simulations done.
10.0% of MC simulations done.
15.0% of MC simulations done.
20.0% of MC simulations done.
25.0% of MC simulations done.
30.0% of MC simulations done.
35.0% of MC simulations done.
40.0% of MC simulations done.
45.0% of MC simulations done.
50.0% of MC simulations done.
55.00000000000001% of MC simulations done.
60.0% of MC simulations done.
65.0% of MC simulations done.
70.0% of MC simulations done.
75.0% of MC simulations done.
80.0% of MC simulations done.
85.0% of MC simulations done.
90.0% of MC simulations done.
95.0% of MC simulations done.
100.0% of MC simulations done.
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  hidden_layer    None       None         None   
4        obj.layers[4]        4  hidden_layer    None       None         None   
5        obj.layers[5]        5  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  
4    False        None  
5    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer       4  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer       4  logistic_sigmoid   
3        obj.layers[3]        3  hidden_layer       4  logistic_sigmoid   
4        obj.layers[4]        4  hidden_layer       4  logistic_sigmoid   
5        obj.layers[5]        5  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1       (2, 4)    False        None  
2       (4, 4)    False        None  
3       (4, 4)    False        None  
4       (4, 4)    False        None  
5       (4, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
{'NN': [-11.293754478458021, 2.4891212428063008, -1.6254390050354182, 5.351220289457327, 10.762283890682655, -2.623116679528016, 1.7901457270913506, -5.917276408661142, 6.506306296920331, -32.95364967529606, 9.920926480578192, -1.0854890010811051, 2.541909327991192, 44.99864604717997, 6.791188054185601, 26.809729203540105, 3.733841009004503, -4.973189028014177, 2.343176772569525, -10.564896037118288, 3.923369954786043, 27.624088863944564, 40.17399079387868, 27.567387730472138, -20.59719181451575, 24.929949938098392, 4.985137131444797, -11.836097399013916, 21.574573636074472, -7.2795550416486625, 5.117574911798893, -4.3940978421252135, 24.414937274332257, 13.950253645682341, 1.5663050274716963, 1.0617255779112285, -16.825264641191048, -2.370845036041865, 2.513698648690249, 4.220645024285879, -4.834212632584388, 38.32990699701128, 1.1769026333082042, 50.93964676384022, 20.795889242522268, -10.691408518154033, -1.4068030418328699, -3.4320933755659353, 1.754178846594497, 9.344385792811451, 3.025230154234184, 18.326746435892623, -6.684497347701, 20.889649226508457, 5.006959903386026, 44.557333767384904, 9.549607245499688, -0.576926611968687, -4.050751000669198, -1.466897615958681, -0.8010770383345439, 8.982280987042206, -3.1108763021288715, -1.494983441035384]}
[-11.29375448   2.48912124  -1.62543901   5.35122029  10.76228389
  -2.62311668   1.79014573  -5.91727641   6.5063063  -32.95364968
   9.92092648  -1.085489     2.54190933  44.99864605   6.79118805
  26.8097292    3.73384101  -4.97318903   2.34317677 -10.56489604
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92994994   4.98513713 -11.8360974   21.57457364  -7.27955504
   5.11757491  -4.39409784  24.41493727  13.95025365   1.56630503
   1.06172558 -16.82526464  -2.37084504   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690263  50.93964676  20.79588924
 -10.69140852  -1.40680304  -3.43209338   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960725  -0.57692661  -4.050751    -1.46689762
  -0.80107704   8.98228099  -3.1108763   -1.49498344]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1014077590162
gradient value of function right now is: [ 1.15956792e-09  3.53808378e-28  1.15820862e-09  2.44994140e-28
 -1.15888070e-09 -3.53790858e-28 -1.15752147e-09 -2.44999072e-28
  1.73265826e-13  1.82201675e-13  1.76271082e-13  1.81002798e-13
 -8.41970678e-28 -9.17684102e-28 -8.65736241e-28 -9.16858247e-28
  2.15806155e-12  2.38591128e-12  2.23238051e-12  2.35059410e-12
 -1.61965795e-29 -7.54579323e-28 -3.09756390e-29 -2.75824306e-27
 -9.93288381e-20 -2.60103058e-14  3.43288255e-19  7.81979000e-21
  3.28419886e-16  4.56859520e-14  3.92200020e-17  1.16573923e-17
  6.36544376e-19 -1.91954079e-14  2.72432690e-18  1.12781864e-19
  1.50083484e-17  7.71063902e-14 -7.50304861e-17 -1.75194947e-18
 -4.59433075e-16 -5.78188961e-17 -5.56393451e-15 -5.10910614e-17
  1.26521117e-12  1.41503220e-12 -1.78549645e-14  1.41526449e-12
  2.86221849e-17  2.23688255e-17 -8.84601172e-18  2.04444409e-17
  5.52970108e-18  1.12187674e-18  5.82407019e-17  1.00707211e-18
 -4.31427791e-13 -1.61645365e-11 -2.74152334e-16  7.35104533e-14
  5.09452888e-19  2.72234246e-18 -2.15375529e-18  1.71896716e-14
 -1.16746809e+00]
supnorm grad right now is: 1.1674680886076307
Weights right now are: 
[-11.29388037   2.48912124  -1.62556462   5.35122029  10.76240979
  -2.62311668   1.79027135  -5.91727641   6.50630626 -32.95364971
   9.92092644  -1.08548904   2.54190933  44.99864605   6.79118805
  26.8097292    3.73384057  -4.97318951   2.34317632 -10.56489651
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92994994   4.98513713 -11.8360974   21.57457364  -7.27955505
   5.11757491  -4.39409784  24.41493727  13.95025365   1.56630503
   1.06172558 -16.82526464  -2.37084505   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690263  50.93964676  20.79588899
 -10.6914088   -1.40680304  -3.43209366   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960733  -0.57692345  -4.050751    -1.46689763
  -0.80107704   8.98228099  -3.1108763   -1.49498344  24.90503461]
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2040.9381178546726
gradient value of function right now is: [ 1.25808632e-09  3.53507131e-28  1.25672806e-09  2.46389498e-28
 -1.25777438e-09 -3.53485067e-28 -1.25641621e-09 -2.46397124e-28
  1.73119681e-13  1.82048045e-13  1.76122414e-13  1.80850226e-13
 -8.41245066e-28 -9.16895365e-28 -8.64990300e-28 -9.16074466e-28
  2.15646862e-12  2.38451343e-12  2.23074397e-12  2.34914892e-12
 -1.62585177e-29 -7.68733643e-28 -3.18423863e-29 -2.76701369e-27
 -9.12050585e-20 -2.59918827e-14  3.37232450e-19  8.20905824e-21
  3.27070755e-16  4.56854879e-14  3.89823758e-17  1.15793858e-17
  6.85187828e-19 -1.91869232e-14  2.67689257e-18  1.14890288e-19
  1.17093879e-17  7.73195239e-14 -7.55954696e-17 -1.97090750e-18
 -4.63799552e-16 -5.92296894e-17 -5.59251116e-15 -5.23387338e-17
  1.26438905e-12  1.41405696e-12 -1.73104328e-14  1.41428371e-12
  2.84676778e-17  2.20256153e-17 -6.23275348e-18  2.01152625e-17
  5.60509044e-18  1.14084914e-18  5.88238948e-17  1.02349639e-18
 -4.31208165e-13 -1.61499836e-11 -2.35663994e-16  7.23796326e-14
  5.41981821e-19  2.67936885e-18  1.25022686e-17  1.67406528e-14
 -2.42331833e+00]
supnorm grad right now is: 2.4233183258718687
Weights right now are: 
[-11.2940925    2.48912124  -1.62577648   5.35122029  10.7626219
  -2.62311668   1.79048319  -5.91727641   6.50630623 -32.95364975
   9.92092641  -1.08548907   2.54190933  44.99864605   6.79118805
  26.8097292    3.73384014  -4.97318999   2.34317588 -10.56489698
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92994995   4.98513713 -11.8360974   21.57457364  -7.27955506
   5.11757491  -4.39409784  24.41493727  13.95025365   1.56630503
   1.06172558 -16.82526464  -2.37084507   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690264  50.93964676  20.79588874
 -10.69140908  -1.40680303  -3.43209394   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960741  -0.57692026  -4.050751    -1.46689765
  -0.80107704   8.98228099  -3.1108763   -1.49498345  24.82680533]
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.07258504669
gradient value of function right now is: [ 1.17923195e-09  3.53207639e-28  1.17787495e-09  2.44844495e-28
 -1.17776891e-09 -3.53223435e-28 -1.17641185e-09 -2.44852991e-28
  1.72987113e-13  1.81908309e-13  1.75987430e-13  1.80711495e-13
 -8.40578665e-28 -9.16167142e-28 -8.64304066e-28 -9.15350673e-28
  2.15502064e-12  2.38259543e-12  2.22923363e-12  2.34731898e-12
 -1.61804670e-29 -7.55036933e-28 -3.10491102e-29 -2.75582205e-27
 -9.65574610e-20 -2.59724447e-14  3.43191751e-19  7.99916842e-21
  3.27771938e-16  4.56211449e-14  3.90257650e-17  1.16222747e-17
  6.52972098e-19 -1.91678907e-14  2.72187364e-18  1.13834130e-19
  1.40219370e-17  7.70366925e-14 -7.54613991e-17 -1.82502903e-18
 -4.60382928e-16 -5.82268628e-17 -5.56820318e-15 -5.14539119e-17
  1.26336948e-12  1.41296682e-12 -1.77749336e-14  1.41319804e-12
  2.88014223e-17  2.23597048e-17 -6.81182578e-18  2.04320829e-17
  5.55347596e-18  1.12942150e-18  5.83959775e-17  1.01373535e-18
 -4.30805175e-13 -1.61406443e-11 -2.67759840e-16  7.32392996e-14
  5.13835876e-19  2.75595308e-18  5.73011304e-19  1.70808304e-14
 -1.45028329e+00]
supnorm grad right now is: 1.4502832858474222
Weights right now are: 
[-11.29430342   2.48912124  -1.62598715   5.35122029  10.76283284
  -2.62311668   1.79069387  -5.91727641   6.50630619 -32.95364978
   9.92092638  -1.08548911   2.54190933  44.99864605   6.79118805
  26.8097292    3.73383973  -4.97319044   2.34317545 -10.56489743
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92994995   4.98513713 -11.8360974   21.57457364  -7.27955507
   5.11757491  -4.39409784  24.41493727  13.95025366   1.56630503
   1.06172558 -16.82526464  -2.37084508   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690264  50.93964676  20.7958885
 -10.69140935  -1.40680303  -3.43209421   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.5496075   -0.57691717  -4.050751    -1.46689766
  -0.80107704   8.98228099  -3.1108763   -1.49498345  24.89060315]
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.0158473598224
gradient value of function right now is: [ 9.18252915e-10  3.52892919e-28  9.16897444e-10  2.42176852e-28
 -9.21720743e-10 -3.52888636e-28 -9.20365286e-10 -2.42175453e-28
  1.72829675e-13  1.81743025e-13  1.75827319e-13  1.80547401e-13
 -8.39799937e-28 -9.15322895e-28 -8.63503917e-28 -9.14512583e-28
  2.15330121e-12  2.37974362e-12  2.22742915e-12  2.34469193e-12
 -1.59924498e-29 -7.26182339e-28 -2.88351740e-29 -2.74167427e-27
 -1.22169414e-19 -2.59501552e-14  3.48385688e-19  6.36489479e-21
  3.31154405e-16  4.54974153e-14  4.02152085e-17  1.18502135e-17
  4.98357813e-19 -1.91376428e-14  2.76922458e-18  1.03980099e-19
  2.31462408e-17  7.62604409e-14 -7.17944801e-17 -1.14354492e-18
 -4.47918032e-16 -5.40352674e-17 -5.48347307e-15 -4.77391278e-17
  1.26204140e-12  1.41162187e-12 -1.91627613e-14  1.41186879e-12
  2.77587919e-17  2.27125734e-17 -2.18500329e-17  2.07991213e-17
  5.29820468e-18  1.05523066e-18  5.65141773e-17  9.48451282e-19
 -4.30197942e-13 -1.61349426e-11 -3.83953000e-16  7.61898830e-14
  4.46931642e-19  2.87736583e-18 -4.30485848e-17  1.82681247e-14
  1.91985701e+00]
supnorm grad right now is: 1.9198570116673366
Weights right now are: 
[-11.29452557   2.48912124  -1.62620902   5.35122029  10.76305499
  -2.62311668   1.79091575  -5.91727641   6.50630616 -32.95364982
   9.92092634  -1.08548914   2.54190933  44.99864605   6.79118805
  26.8097292    3.7338393   -4.97319091   2.34317501 -10.5648979
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92994996   4.98513713 -11.8360974   21.57457364  -7.27955508
   5.11757491  -4.39409784  24.41493727  13.95025366   1.56630503
   1.06172558 -16.82526464  -2.3708451    2.51369865   4.22064502
  -4.83421263  38.329907     1.17690264  50.93964676  20.79588824
 -10.69140964  -1.40680303  -3.43209449   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960758  -0.57691397  -4.050751    -1.46689768
  -0.80107704   8.98228099  -3.1108763   -1.49498346  25.13816578]
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.111928254017
gradient value of function right now is: [ 9.81723974e-10  3.52576605e-28  9.80369639e-10  2.42030194e-28
 -9.82165315e-10 -3.52590580e-28 -9.80810930e-10 -2.42031118e-28
  1.72683287e-13  1.81588864e-13  1.75678338e-13  1.80394252e-13
 -8.39068883e-28 -9.14525132e-28 -8.62751766e-28 -9.13718760e-28
  2.15170452e-12  2.37822236e-12  2.22578371e-12  2.34314529e-12
 -1.60239402e-29 -7.30100262e-28 -2.94422633e-29 -2.73566977e-27
 -1.16088954e-19 -2.59307855e-14  3.44587203e-19  6.67988972e-21
  3.29778148e-16  4.54860341e-14  3.99669943e-17  1.17783503e-17
  5.30126810e-19 -1.91269682e-14  2.74042789e-18  1.05668858e-19
  2.11587777e-17  7.63851701e-14 -7.21364501e-17 -1.28108538e-18
 -4.49452252e-16 -5.47518557e-17 -5.48997060e-15 -4.83749784e-17
  1.26115245e-12  1.41058580e-12 -1.87600899e-14  1.41082854e-12
  2.76964265e-17  2.24782137e-17 -1.97501719e-17  2.05741259e-17
  5.33674031e-18  1.06670032e-18  5.67810726e-17  9.58419332e-19
 -4.29935145e-13 -1.61205641e-11 -3.55913923e-16  7.53655957e-14
  4.55912888e-19  2.81825837e-18 -3.29761329e-17  1.79695428e-14
  1.06521291e+00]
supnorm grad right now is: 1.0652129148914196
Weights right now are: 
[-11.29474843   2.48912124  -1.6264316    5.35122029  10.76327784
  -2.62311668   1.79113832  -5.91727641   6.50630612 -32.95364986
   9.92092631  -1.08548918   2.54190933  44.99864605   6.79118805
  26.8097292    3.73383886  -4.97319141   2.34317455 -10.56489838
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92994996   4.98513713 -11.8360974   21.57457364  -7.27955509
   5.11757491  -4.39409784  24.41493727  13.95025366   1.56630503
   1.06172558 -16.82526464  -2.37084511   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690264  50.93964676  20.79588798
 -10.69140993  -1.40680302  -3.43209479   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960767  -0.57691058  -4.050751    -1.46689769
  -0.80107704   8.98228099  -3.1108763   -1.49498346  25.03581238]
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2040.9946719345621
gradient value of function right now is: [ 9.05257089e-10  3.52271970e-28  9.03904034e-10  2.41567067e-28
 -9.01647519e-10 -3.52266794e-28 -9.00294485e-10 -2.41557058e-28
  1.72530924e-13  1.81428919e-13  1.75523392e-13  1.80235457e-13
 -8.38315511e-28 -9.13708390e-28 -8.61977680e-28 -9.12907818e-28
  2.15004224e-12  2.37607476e-12  2.22405633e-12  2.34109005e-12
 -1.59517274e-29 -7.22301697e-28 -2.86266675e-29 -2.73607867e-27
 -1.22626634e-19 -2.59104505e-14  3.49307993e-19  6.34621123e-21
  3.30915910e-16  4.54193626e-14  4.01562561e-17  1.18427595e-17
  4.94307023e-19 -1.91070593e-14  2.77526594e-18  1.03818885e-19
  2.33659234e-17  7.60949000e-14 -7.18010344e-17 -1.12791025e-18
 -4.47056662e-16 -5.38668818e-17 -5.47568885e-15 -4.75908733e-17
  1.26008148e-12  1.40944350e-12 -1.92485198e-14  1.40969117e-12
  2.78422735e-17  2.27656120e-17 -2.15461083e-17  2.08490955e-17
  5.28762465e-18  1.05328953e-18  5.64157931e-17  9.46781794e-19
 -4.29521175e-13 -1.61110796e-11 -3.90978580e-16  7.62780525e-14
  4.46064605e-19  2.87732653e-18 -4.55214982e-17  1.83094911e-14
  2.07384671e+00]
supnorm grad right now is: 2.0738467067930664
Weights right now are: 
[-11.29496418   2.48912124  -1.62664709   5.35122029  10.76349359
  -2.62311668   1.7913538   -5.91727641   6.50630609 -32.95364989
   9.92092627  -1.08548922   2.54190933  44.99864605   6.79118805
  26.8097292    3.73383843  -4.97319188   2.3431741  -10.56489885
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92994997   4.98513713 -11.8360974   21.57457364  -7.2795551
   5.11757491  -4.39409784  24.41493727  13.95025367   1.56630503
   1.06172558 -16.82526464  -2.37084513   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690264  50.93964676  20.79588773
 -10.69141021  -1.40680302  -3.43209507   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960775  -0.57690746  -4.050751    -1.46689771
  -0.80107704   8.98228099  -3.1108763   -1.49498346  25.13714385]
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.0691314818334
gradient value of function right now is: [ 1.17891119e-09  3.51974176e-28  1.17755895e-09  2.44301043e-28
 -1.18008647e-09 -3.51990021e-28 -1.17873417e-09 -2.44306297e-28
  1.72394289e-13  1.81284974e-13  1.75384319e-13  1.80092462e-13
 -8.37632057e-28 -9.12962269e-28 -8.61274369e-28 -9.12165703e-28
  2.14855242e-12  2.37545938e-12  2.22254328e-12  2.34028616e-12
 -1.61273855e-29 -7.53683609e-28 -3.09620026e-29 -2.74944073e-27
 -9.62441807e-20 -2.58938524e-14  3.41858569e-19  7.97335550e-21
  3.26947811e-16  4.54819282e-14  3.89460291e-17  1.15933390e-17
  6.52108024e-19 -1.91094107e-14  2.71127729e-18  1.13497599e-19
  1.39122127e-17  7.68167880e-14 -7.51979723e-17 -1.82292143e-18
 -4.59417609e-16 -5.81054002e-17 -5.55535445e-15 -5.13470664e-17
  1.25952611e-12  1.40866988e-12 -1.77042673e-14  1.40890028e-12
  2.86673070e-17  2.22706140e-17 -7.06729537e-18  2.03505782e-17
  5.54076494e-18  1.12628376e-18  5.82607868e-17  1.01090519e-18
 -4.29502854e-13 -1.60919968e-11 -2.66207516e-16  7.29837063e-14
  5.14295919e-19  2.75009016e-18  8.83743700e-19  1.70131116e-14
 -1.49680024e+00]
supnorm grad right now is: 1.4968002419581734
Weights right now are: 
[-11.29517429   2.48912124  -1.62685693   5.35122029  10.76370368
  -2.62311668   1.79156363  -5.91727641   6.50630606 -32.95364993
   9.92092624  -1.08548925   2.54190933  44.99864605   6.79118805
  26.8097292    3.73383801  -4.97319234   2.34317367 -10.5648993
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92994997   4.98513713 -11.8360974   21.57457364  -7.27955511
   5.11757491  -4.39409784  24.41493727  13.95025367   1.56630503
   1.06172558 -16.82526464  -2.37084514   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690264  50.93964676  20.79588748
 -10.69141049  -1.40680302  -3.43209535   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960784  -0.57690422  -4.050751    -1.46689772
  -0.80107704   8.98228099  -3.1108763   -1.49498347  24.89199899]
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1241090891522
gradient value of function right now is: [ 9.96773815e-10  3.51674005e-28  9.95422940e-10  2.41936391e-28
 -9.97281316e-10 -3.51684857e-28 -9.95930402e-10 -2.41937105e-28
  1.72248472e-13  1.81131707e-13  1.75235990e-13  1.79940246e-13
 -8.36908533e-28 -9.12175760e-28 -8.60530595e-28 -9.11384010e-28
  2.14696004e-12  2.37305301e-12  2.22087875e-12  2.33803778e-12
 -1.59985951e-29 -7.31996373e-28 -2.95439242e-29 -2.73309986e-27
 -1.14301937e-19 -2.58734009e-14  3.43463435e-19  6.76814115e-21
  3.29046895e-16  4.53914472e-14  3.98441814e-17  1.17453749e-17
  5.39324262e-19 -1.90853347e-14  2.73079414e-18  1.06087517e-19
  2.05107438e-17  7.62703326e-14 -7.21906382e-17 -1.32323881e-18
 -4.49596630e-16 -5.49399318e-17 -5.48730467e-15 -4.85426053e-17
  1.25836384e-12  1.40745688e-12 -1.85985250e-14  1.40769780e-12
  2.76724820e-17  2.23902315e-17 -1.90026688e-17  2.04908341e-17
  5.34472631e-18  1.06946891e-18  5.68150981e-17  9.60823091e-19
 -4.29003779e-13 -1.60844483e-11 -3.46106848e-16  7.49437076e-14
  4.56784966e-19  2.81195951e-18 -2.95912925e-17  1.78294711e-14
  8.21975851e-01]
supnorm grad right now is: 0.8219758512355619
Weights right now are: 
[-11.29538648   2.48912124  -1.62706886   5.35122029  10.76391589
  -2.62311668   1.79177557  -5.91727641   6.50630602 -32.95364996
   9.9209262   -1.08548929   2.54190933  44.99864605   6.79118805
  26.8097292    3.7338376   -4.9731928    2.34317324 -10.56489976
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92994998   4.98513713 -11.8360974   21.57457364  -7.27955511
   5.11757491  -4.39409784  24.41493727  13.95025368   1.56630503
   1.06172558 -16.82526464  -2.37084516   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690264  50.93964676  20.79588723
 -10.69141077  -1.40680301  -3.43209563   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960793  -0.57690095  -4.050751    -1.46689774
  -0.80107704   8.98228099  -3.1108763   -1.49498347  25.03765617]
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2040.3417296517505
gradient value of function right now is: [ 6.87710177e-10  3.50028879e-28  6.86367608e-10  2.37764267e-28
 -6.85224491e-10 -3.50014941e-28 -6.83881974e-10 -2.37761594e-28
  1.71993562e-13  1.80813506e-13  1.74960141e-13  1.79631863e-13
 -8.33745615e-28 -9.07617547e-28 -8.56908159e-28 -9.07038668e-28
  2.13721911e-12  2.35744649e-12  2.20957264e-12  2.32340785e-12
 -1.57119789e-29 -6.88836244e-28 -2.65008415e-29 -2.70872208e-27
 -1.41790151e-19 -2.53570577e-14  3.55719168e-19  5.10487122e-21
  3.33114078e-16  4.44750195e-14  4.09025052e-17  1.19998273e-17
  3.77311589e-19 -1.87098251e-14  2.82370543e-18  9.61212823e-20
  3.01799550e-17  7.38007373e-14 -6.91993700e-17 -6.12881968e-19
 -4.37055528e-16 -5.06168631e-17 -5.40652174e-15 -4.47136684e-17
  1.23512426e-12  1.38056767e-12 -2.04822173e-14  1.38082864e-12
  2.73453651e-17  2.31760411e-17 -3.17409924e-17  2.12629084e-17
  5.08779725e-18  9.95574007e-19  5.49274589e-17  8.96065737e-19
 -4.11951018e-13 -1.56086287e-11 -4.90199821e-16  7.87127877e-14
  3.01406876e-19  3.00238488e-18 -8.22558687e-17  1.92762752e-14
  4.85941081e+00]
supnorm grad right now is: 4.859410812768908
Weights right now are: 
[-11.29560377   2.48912124  -1.62728588   5.35122029  10.76413315
  -2.62311668   1.79199257  -5.91727641   6.50630599 -32.95365
   9.92092617  -1.08548932   2.54190933  44.99864605   6.79118805
  26.8097292    3.73383717  -4.97319328   2.3431728  -10.56490022
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92994998   4.98513713 -11.8360974   21.57457364  -7.27955512
   5.11757491  -4.39409784  24.41493727  13.95025368   1.56630503
   1.06172558 -16.82526464  -2.37084517   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690264  50.93964676  20.79588698
 -10.69141105  -1.40680301  -3.43209591   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960801  -0.57689781  -4.050751    -1.46689775
  -0.80107704   8.98228099  -3.1108763   -1.49498347  25.32813524]
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.024878781748
gradient value of function right now is: [ 1.20491599e-09  3.51077958e-28  1.20356718e-09  2.44273219e-28
 -1.20860091e-09 -3.51093443e-28 -1.20725204e-09 -2.44264742e-28
  1.71963360e-13  1.80831876e-13  1.74945916e-13  1.79642488e-13
 -8.35490629e-28 -9.10633183e-28 -8.59072588e-28 -9.09850925e-28
  2.14384924e-12  2.37035995e-12  2.21768138e-12  2.33524389e-12
 -1.61077789e-29 -7.56138951e-28 -3.11234480e-29 -2.74719729e-27
 -9.34791350e-20 -2.58369370e-14  3.39998402e-19  8.10258768e-21
  3.25985807e-16  4.53882334e-14  3.87767846e-17  1.15490799e-17
  6.66429160e-19 -1.90683716e-14  2.69570685e-18  1.14078867e-19
  1.28052523e-17  7.67236953e-14 -7.53555215e-17 -1.89290496e-18
 -4.60130805e-16 -5.84680105e-17 -5.55587929e-15 -5.16677188e-17
  1.25676198e-12  1.40556684e-12 -1.75322749e-14  1.40579534e-12
  2.86413610e-17  2.21583083e-17 -5.82908615e-18  2.02426843e-17
  5.55762433e-18  1.13120236e-18  5.83629424e-17  1.01513271e-18
 -4.28579923e-13 -1.60560407e-11 -2.55515499e-16  7.25346885e-14
  5.34908684e-19  2.69842973e-18  4.94554017e-18  1.68513804e-14
 -1.85251691e+00]
supnorm grad right now is: 1.85251691419836
Weights right now are: 
[-11.29580862   2.48912124  -1.62749046   5.35122029  10.764338
  -2.62311668   1.79219715  -5.91727641   6.50630596 -32.95365003
   9.92092613  -1.08548936   2.54190933  44.99864605   6.79118805
  26.8097292    3.73383674  -4.97319375   2.34317236 -10.56490069
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92994999   4.98513713 -11.8360974   21.57457364  -7.27955513
   5.11757491  -4.39409784  24.41493727  13.95025368   1.56630503
   1.06172558 -16.82526464  -2.37084519   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690264  50.93964676  20.79588672
 -10.69141134  -1.40680301  -3.4320962    1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960809  -0.57689453  -4.050751    -1.46689776
  -0.80107704   8.98228099  -3.1108763   -1.49498348  24.82861128]
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1340690099544
gradient value of function right now is: [ 1.10683950e-09  3.50786546e-28  1.10549192e-09  2.42892423e-28
 -1.11024506e-09 -3.50796458e-28 -1.10889745e-09 -2.42889251e-28
  1.71821662e-13  1.80682927e-13  1.74801772e-13  1.79494564e-13
 -8.34787215e-28 -9.09868556e-28 -8.58349458e-28 -9.09091253e-28
  2.14230049e-12  2.36830791e-12  2.21607001e-12  2.33328627e-12
 -1.60283494e-29 -7.43916041e-28 -3.03100346e-29 -2.73743624e-27
 -1.03103965e-19 -2.58175465e-14  3.42272682e-19  7.49944424e-21
  3.27020399e-16  4.53254713e-14  3.91809911e-17  1.16261598e-17
  6.06344612e-19 -1.90491529e-14  2.71617003e-18  1.10434211e-19
  1.64158417e-17  7.64114600e-14 -7.39494801e-17 -1.62756507e-18
 -4.54657641e-16 -5.67586594e-17 -5.51669504e-15 -5.01540482e-17
  1.25573429e-12  1.40446585e-12 -1.79953019e-14  1.40469952e-12
  2.82675912e-17  2.22873144e-17 -1.13410515e-17  2.03782385e-17
  5.45409023e-18  1.10248695e-18  5.75794893e-17  9.89913112e-19
 -4.28177507e-13 -1.60468536e-11 -2.95058741e-16  7.35199149e-14
  5.00367862e-19  2.76447406e-18 -1.04477514e-17  1.72727382e-14
 -6.31357654e-01]
supnorm grad right now is: 0.6313576541519237
Weights right now are: 
[-11.29601569   2.48912124  -1.62769727   5.35122029  10.76454509
  -2.62311668   1.79240397  -5.91727641   6.50630592 -32.95365007
   9.9209261   -1.08548939   2.54190933  44.99864605   6.79118805
  26.8097292    3.73383632  -4.97319421   2.34317192 -10.56490114
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92994999   4.98513713 -11.8360974   21.57457364  -7.27955514
   5.11757491  -4.39409784  24.41493727  13.95025369   1.56630503
   1.06172558 -16.82526464  -2.3708452    2.51369865   4.22064502
  -4.83421263  38.329907     1.17690264  50.93964676  20.79588648
 -10.69141161  -1.406803    -3.43209647   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960818  -0.57689139  -4.050751    -1.46689778
  -0.80107704   8.98228099  -3.1108763   -1.49498348  24.98213169]
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.083722986134
gradient value of function right now is: [ 1.16409557e-09  3.50483781e-28  1.16274909e-09  2.43517551e-28
 -1.16111153e-09 -3.50498104e-28 -1.15976500e-09 -2.43507660e-28
  1.71677449e-13  1.80531259e-13  1.74655047e-13  1.79343941e-13
 -8.34069880e-28 -9.09087875e-28 -8.57611800e-28 -9.08314928e-28
  2.14072696e-12  2.36678119e-12  2.21444820e-12  2.33174069e-12
 -1.60556201e-29 -7.50812578e-28 -3.07653242e-29 -2.74087602e-27
 -9.70395248e-20 -2.57987542e-14  3.41034033e-19  7.89050575e-21
  3.26176823e-16  4.53112470e-14  3.88935518e-17  1.15700974e-17
  6.44032420e-19 -1.90381571e-14  2.70484532e-18  1.12820597e-19
  1.42320459e-17  7.65158970e-14 -7.48098333e-17 -1.79085296e-18
 -4.57605501e-16 -5.77708196e-17 -5.53657393e-15 -5.10495458e-17
  1.25487011e-12  1.40346948e-12 -1.76624631e-14  1.40369930e-12
  2.85167676e-17  2.22020692e-17 -7.77117445e-18  2.02893038e-17
  5.51488015e-18  1.12015267e-18  5.80240758e-17  1.00543576e-18
 -4.27920954e-13 -1.60334397e-11 -2.67340957e-16  7.27690310e-14
  5.10513600e-19  2.73769530e-18  3.51477201e-21  1.69742408e-14
 -1.37295378e+00]
supnorm grad right now is: 1.372953778339893
Weights right now are: 
[-11.29623054   2.48912124  -1.62791185   5.35122029  10.76475994
  -2.62311668   1.79261856  -5.91727641   6.50630589 -32.95365011
   9.92092606  -1.08548943   2.54190933  44.99864605   6.79118805
  26.8097292    3.7338359   -4.97319468   2.34317149 -10.5649016
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995      4.98513713 -11.8360974   21.57457364  -7.27955515
   5.11757491  -4.39409784  24.41493727  13.95025369   1.56630503
   1.06172558 -16.82526464  -2.37084522   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690265  50.93964676  20.79588623
 -10.69141189  -1.406803    -3.43209674   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960826  -0.57688828  -4.050751    -1.46689779
  -0.80107704   8.98228099  -3.1108763   -1.49498348  24.89285633]
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2040.765125749588
gradient value of function right now is: [ 8.03914152e-10  3.50172005e-28  8.02569333e-10  2.38860800e-28
 -8.02958721e-10 -3.50185414e-28 -8.01613854e-10 -2.38866593e-28
  1.71527090e-13  1.80373251e-13  1.74502081e-13  1.79187100e-13
 -8.33322815e-28 -9.08276478e-28 -8.56843689e-28 -9.07509729e-28
  2.13908119e-12  2.36355548e-12  2.21270443e-12  2.32883472e-12
 -1.57746900e-29 -7.03004659e-28 -2.74523450e-29 -2.71340832e-27
 -1.30681099e-19 -2.57757606e-14  3.52803190e-19  5.84430521e-21
  3.30968768e-16  4.51396947e-14  4.02964434e-17  1.18765694e-17
  4.43535180e-19 -1.90005479e-14  2.80081707e-18  1.00670909e-19
  2.63306056e-17  7.54078974e-14 -7.07413706e-17 -9.06517049e-19
 -4.41388751e-16 -5.23243798e-17 -5.42971681e-15 -4.62302485e-17
  1.25338889e-12  1.40202841e-12 -1.97954846e-14  1.40228159e-12
  2.77456770e-17  2.29677798e-17 -2.48944279e-17  2.10501111e-17
  5.19040721e-18  1.02787141e-18  5.56291445e-17  9.24517964e-19
 -4.27177944e-13 -1.60314957e-11 -4.37250422e-16  7.71531257e-14
  4.17837002e-19  2.98083513e-18 -6.27006870e-17  1.86894190e-14
  3.32232333e+00]
supnorm grad right now is: 3.322323327130109
Weights right now are: 
[-11.29645225   2.48912124  -1.62813328   5.35122029  10.76498163
  -2.62311668   1.79283998  -5.91727641   6.50630585 -32.95365014
   9.92092603  -1.08548947   2.54190933  44.99864605   6.79118805
  26.8097292    3.73383546  -4.97319516   2.34317104 -10.56490208
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995001   4.98513713 -11.8360974   21.57457364  -7.27955516
   5.11757491  -4.39409784  24.41493727  13.9502537    1.56630503
   1.06172558 -16.82526464  -2.37084523   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690265  50.93964676  20.79588598
 -10.69141217  -1.40680299  -3.43209702   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960835  -0.57688508  -4.050751    -1.46689781
  -0.80107704   8.98228099  -3.1108763   -1.49498349  25.25036095]
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -2040.911244909812
gradient value of function right now is: [ 8.61738484e-10  3.49868795e-28  8.60394764e-10  2.39552383e-28
 -8.63084236e-10 -3.49849823e-28 -8.61740586e-10 -2.39553181e-28
  1.71371713e-13  1.80210166e-13  1.74344105e-13  1.79025102e-13
 -8.32556806e-28 -9.07445973e-28 -8.56056920e-28 -9.06684187e-28
  2.13738510e-12  2.36191005e-12  2.21095917e-12  2.32717034e-12
 -1.58041429e-29 -7.10524627e-28 -2.79339083e-29 -2.71732653e-27
 -1.25072526e-19 -2.57565486e-14  3.51290923e-19  6.20968497e-21
  3.29866422e-16  4.51244149e-14  3.99780350e-17  1.18155359e-17
  4.78874254e-19 -1.89893745e-14  2.78860887e-18  1.02958753e-19
  2.42957164e-17  7.55159572e-14 -7.15408416e-17 -1.06013261e-18
 -4.44125771e-16 -5.32840409e-17 -5.44301767e-15 -4.70809383e-17
  1.25250296e-12  1.40100838e-12 -1.94595159e-14  1.40125776e-12
  2.79497090e-17  2.28684097e-17 -2.13264920e-17  2.09491247e-17
  5.24782514e-18  1.04487459e-18  5.60111633e-17  9.39488959e-19
 -4.26918412e-13 -1.60179552e-11 -4.10503623e-16  7.64052183e-14
  4.34112762e-19  2.87665639e-18 -5.26462875e-17  1.83959001e-14
  2.57240352e+00]
supnorm grad right now is: 2.5724035249326977
Weights right now are: 
[-11.29666766   2.48912124  -1.62834842   5.35122029  10.76519704
  -2.62311668   1.79305512  -5.91727641   6.50630582 -32.95365018
   9.92092599  -1.0854895    2.54190933  44.99864605   6.79118805
  26.8097292    3.73383503  -4.97319564   2.34317059 -10.56490255
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995001   4.98513713 -11.8360974   21.57457364  -7.27955517
   5.11757491  -4.39409784  24.41493727  13.9502537    1.56630503
   1.06172558 -16.82526464  -2.37084525   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690265  50.93964676  20.79588573
 -10.69141244  -1.40680299  -3.4320973    1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960843  -0.57688193  -4.050751    -1.46689782
  -0.80107704   8.98228099  -3.1108763   -1.49498349  25.14912313]
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1455336206825
gradient value of function right now is: [ 1.07533778e-09  3.49570746e-28  1.07399493e-09  2.41925311e-28
 -1.07360087e-09 -3.49580997e-28 -1.07225798e-09 -2.41934309e-28
  1.71237368e-13  1.80068556e-13  1.74207338e-13  1.78884432e-13
 -8.31883382e-28 -9.06710052e-28 -8.55363725e-28 -9.05952078e-28
  2.13591846e-12  2.36114649e-12  2.20946548e-12  2.32625079e-12
 -1.59553105e-29 -7.38516220e-28 -2.99706940e-29 -2.72834158e-27
 -1.05742545e-19 -2.57397585e-14  3.41655327e-19  7.29253991e-21
  3.26653907e-16  4.51777363e-14  3.92490329e-17  1.16252484e-17
  5.87411155e-19 -1.89896523e-14  2.71227390e-18  1.09025333e-19
  1.74265575e-17  7.61039854e-14 -7.32475695e-17 -1.54370045e-18
 -4.52097172e-16 -5.61190924e-17 -5.49403186e-15 -4.95863093e-17
  1.25190174e-12  1.40019744e-12 -1.80928308e-14  1.40043222e-12
  2.80168786e-17  2.22406343e-17 -1.34221305e-17  2.03401886e-17
  5.41051746e-18  1.09058315e-18  5.72215564e-17  9.79356743e-19
 -4.26860061e-13 -1.59996523e-11 -3.07563092e-16  7.36306926e-14
  4.88543689e-19  2.81841212e-18 -1.54236868e-17  1.73532772e-14
 -2.69193937e-01]
supnorm grad right now is: 0.2691939367070561
Weights right now are: 
[-11.29687984   2.48912124  -1.62856032   5.35122029  10.76540921
  -2.62311668   1.793267    -5.91727641   6.50630578 -32.95365022
   9.92092596  -1.08548954   2.54190933  44.99864605   6.79118805
  26.8097292    3.73383458  -4.97319614   2.34317012 -10.56490304
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995002   4.98513713 -11.8360974   21.57457364  -7.27955518
   5.11757491  -4.39409784  24.41493727  13.9502537    1.56630503
   1.06172558 -16.82526464  -2.37084527   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690265  50.93964676  20.79588547
 -10.69141274  -1.40680299  -3.4320976    1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960852  -0.57687856  -4.050751    -1.46689784
  -0.80107704   8.98228099  -3.1108763   -1.4949835   24.9835449 ]
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.0811533454348
gradient value of function right now is: [ 9.46584717e-10  3.49272342e-28  9.45243159e-10  2.40228487e-28
 -9.43093977e-10 -3.49265422e-28 -9.41752445e-10 -2.40216484e-28
  1.71088796e-13  1.79912508e-13  1.74056246e-13  1.78729430e-13
 -8.31148804e-28 -9.05912319e-28 -8.54608921e-28 -9.05158488e-28
  2.13429604e-12  2.35890991e-12  2.20777641e-12  2.32413003e-12
 -1.58620764e-29 -7.23233290e-28 -2.89791019e-29 -2.71627547e-27
 -1.17986849e-19 -2.57196913e-14  3.42374114e-19  6.45585844e-21
  3.27970959e-16  4.51050079e-14  3.98569312e-17  1.17251756e-17
  5.10111835e-19 -1.89685869e-14  2.72215789e-18  1.03785640e-19
  2.18337219e-17  7.57090327e-14 -7.11642463e-17 -1.20461983e-18
 -4.45396096e-16 -5.39681723e-17 -5.44709814e-15 -4.76776189e-17
  1.25081091e-12  1.39903829e-12 -1.86972844e-14  1.39928015e-12
  2.73063602e-17  2.23119855e-17 -2.16169969e-17  2.04259420e-17
  5.27527503e-18  1.05114560e-18  5.62263366e-17  9.44522661e-19
 -4.26420791e-13 -1.59908048e-11 -3.61355792e-16  7.49511836e-14
  4.47118138e-19  2.82628922e-18 -3.58862631e-17  1.79033642e-14
  1.42298212e+00]
supnorm grad right now is: 1.4229821187909615
Weights right now are: 
[-11.29709276   2.48912124  -1.62877297   5.35122029  10.7656221
  -2.62311668   1.79347963  -5.91727641   6.50630575 -32.95365025
   9.92092592  -1.08548958   2.54190933  44.99864605   6.79118805
  26.8097292    3.73383416  -4.9731966    2.34316969 -10.5649035
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995002   4.98513713 -11.8360974   21.57457364  -7.27955519
   5.11757491  -4.39409784  24.41493727  13.95025371   1.56630503
   1.06172558 -16.82526464  -2.37084528   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690265  50.93964676  20.79588524
 -10.691413    -1.40680298  -3.43209785   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.5496086   -0.57687571  -4.050751    -1.46689785
  -0.80107704   8.98228099  -3.1108763   -1.4949835   25.12530992]
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1487589421192
gradient value of function right now is: [ 1.05480059e-09  3.48964917e-28  1.05346011e-09  2.41290867e-28
 -1.05724647e-09 -3.48977676e-28 -1.05590594e-09 -2.41285990e-28
  1.70946936e-13  1.79763155e-13  1.73911860e-13  1.78581132e-13
 -8.30439751e-28 -9.05139446e-28 -8.53879333e-28 -9.04390546e-28
  2.13274557e-12  2.35756053e-12  2.20618055e-12  2.32273455e-12
 -1.59115929e-29 -7.34341394e-28 -2.97102667e-29 -2.72267318e-27
 -1.08394399e-19 -2.57009426e-14  3.40689989e-19  7.08176571e-21
  3.26623300e-16  4.51005146e-14  3.93741688e-17  1.16366060e-17
  5.70948619e-19 -1.89593431e-14  2.70677532e-18  1.07704795e-19
  1.83334063e-17  7.59341209e-14 -7.26607969e-17 -1.46976747e-18
 -4.50328727e-16 -5.56440275e-17 -5.47921960e-15 -4.91643260e-17
  1.24997973e-12  1.39806278e-12 -1.82004324e-14  1.39829883e-12
  2.77628176e-17  2.22034041e-17 -1.54968005e-17  2.03104534e-17
  5.37649877e-18  1.08091498e-18  5.69589272e-17  9.70755610e-19
 -4.26191749e-13 -1.59764060e-11 -3.18875323e-16  7.38248269e-14
  4.88043199e-19  2.81804893e-18 -1.96973129e-17  1.74453072e-14
 -2.34296735e-02]
supnorm grad right now is: 0.0234296735188515
Weights right now are: 
[-11.29731181   2.48912124  -1.62899175   5.35122029  10.76584117
  -2.62311668   1.79369842  -5.91727641   6.50630571 -32.95365029
   9.92092588  -1.08548961   2.54190933  44.99864605   6.79118805
  26.8097292    3.73383371  -4.97319709   2.34316923 -10.56490398
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995003   4.98513713 -11.8360974   21.57457364  -7.2795552
   5.11757491  -4.39409784  24.41493727  13.95025371   1.56630503
   1.06172558 -16.82526464  -2.37084529   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690265  50.93964676  20.79588498
 -10.69141328  -1.40680298  -3.43209814   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960868  -0.57687249  -4.050751    -1.46689787
  -0.80107704   8.98228099  -3.1108763   -1.4949835   25.02980886]
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2040.669103254621
gradient value of function right now is: [ 7.68154912e-10  3.48667625e-28  7.66815940e-10  2.37544391e-28
 -7.67666131e-10 -3.48671741e-28 -7.66327145e-10 -2.37555343e-28
  1.70801115e-13  1.79610005e-13  1.73763540e-13  1.78429107e-13
 -8.29717151e-28 -9.04355453e-28 -8.53136601e-28 -9.03612725e-28
  2.13114968e-12  2.35466707e-12  2.20449679e-12  2.32010054e-12
 -1.56756392e-29 -6.95277203e-28 -2.69608798e-29 -2.70121384e-27
 -1.33006192e-19 -2.56793175e-14  3.53872595e-19  5.66995233e-21
  3.30372073e-16  4.49557264e-14  4.02450674e-17  1.18652450e-17
  4.24767555e-19 -1.89266464e-14  2.80650788e-18  9.93407543e-20
  2.71799598e-17  7.50353578e-14 -7.03165133e-17 -8.33430697e-19
 -4.38730513e-16 -5.16986467e-17 -5.40543857e-15 -4.56758379e-17
  1.24864356e-12  1.39674731e-12 -1.99236753e-14  1.39700152e-12
  2.77132129e-17  2.30180224e-17 -2.57028651e-17  2.11024724e-17
  5.14829612e-18  1.01723813e-18  5.52713849e-17  9.15155427e-19
 -4.25548168e-13 -1.59731873e-11 -4.49390759e-16  7.72522518e-14
  3.37098003e-19  2.94357425e-18 -6.75649177e-17  1.87668288e-14
  3.76776090e+00]
supnorm grad right now is: 3.767760899766137
Weights right now are: 
[-11.29752398   2.48912124  -1.62920364   5.35122029  10.76605332
  -2.62311668   1.79391029  -5.91727641   6.50630567 -32.95365033
   9.92092585  -1.08548965   2.54190933  44.99864605   6.79118805
  26.8097292    3.73383327  -4.97319758   2.34316876 -10.56490447
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995003   4.98513713 -11.8360974   21.57457363  -7.2795552
   5.11757491  -4.39409784  24.41493727  13.95025371   1.56630503
   1.06172558 -16.82526464  -2.37084531   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690265  50.93964676  20.79588473
 -10.69141357  -1.40680298  -3.43209843   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960877  -0.57686924  -4.050751    -1.46689788
  -0.80107704   8.98228099  -3.1108763   -1.49498351  25.26689951]
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2040.9347728633945
gradient value of function right now is: [ 8.68784893e-10  3.48372878e-28  8.67446925e-10  2.39147096e-28
 -8.71081691e-10 -3.48385875e-28 -8.69743675e-10 -2.39139745e-28
  1.70662132e-13  1.79463776e-13  1.73622115e-13  1.78283894e-13
 -8.29024616e-28 -9.03601374e-28 -8.52424292e-28 -9.02863156e-28
  2.12963096e-12  2.35339815e-12  2.20293769e-12  2.31877159e-12
 -1.57502823e-29 -7.11437253e-28 -2.79604281e-29 -2.71119654e-27
 -1.24369615e-19 -2.56615369e-14  3.48373847e-19  6.16168193e-21
  3.28756661e-16  4.49631934e-14  3.99196723e-17  1.17759348e-17
  4.76807851e-19 -1.89195463e-14  2.76613087e-18  1.02295327e-19
  2.40477416e-17  7.52865640e-14 -7.10288330e-17 -1.05782608e-18
 -4.42730033e-16 -5.30928434e-17 -5.42682420e-15 -4.69087502e-17
  1.24787489e-12  1.39582131e-12 -1.92677600e-14  1.39606868e-12
  2.76346169e-17  2.26757512e-17 -2.22976415e-17  2.07718139e-17
  5.22756951e-18  1.03934143e-18  5.58285218e-17  9.34429440e-19
 -4.25356733e-13 -1.59583873e-11 -4.01973881e-16  7.59200551e-14
  4.37084189e-19  2.82898801e-18 -5.02157549e-17  1.82628075e-14
  2.43383092e+00]
supnorm grad right now is: 2.433830924034611
Weights right now are: 
[-11.29773455   2.48912124  -1.62941393   5.35122029  10.76626388
  -2.62311668   1.79412057  -5.91727641   6.50630564 -32.95365037
   9.92092581  -1.08548969   2.54190933  44.99864605   6.79118805
  26.8097292    3.73383282  -4.97319807   2.3431683  -10.56490495
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995004   4.98513713 -11.8360974   21.57457363  -7.27955521
   5.11757491  -4.39409784  24.41493727  13.95025372   1.56630503
   1.06172558 -16.82526464  -2.37084533   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690265  50.93964676  20.79588447
 -10.69141385  -1.40680297  -3.43209871   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960886  -0.57686594  -4.050751    -1.4668979
  -0.80107704   8.98228099  -3.1108763   -1.49498351  25.1829008 ]
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1069870412675
gradient value of function right now is: [ 1.13526075e-09  3.48070413e-28  1.13392361e-09  2.42099313e-28
 -1.13868303e-09 -3.48083921e-28 -1.13734584e-09 -2.42089893e-28
  1.70517110e-13  1.79311203e-13  1.73474575e-13  1.78132299e-13
 -8.28303845e-28 -9.02816245e-28 -8.51683175e-28 -9.02081702e-28
  2.12804840e-12  2.35269489e-12  2.20133144e-12  2.31787689e-12
 -1.59330520e-29 -7.44850580e-28 -3.03733018e-29 -2.72605608e-27
 -9.91239120e-20 -2.56445181e-14  3.39222568e-19  7.67533101e-21
  3.24928484e-16  4.50299450e-14  3.88548334e-17  1.15371722e-17
  6.25283618e-19 -1.89220523e-14  2.69087431e-18  1.11152140e-19
  1.49876304e-17  7.60101800e-14 -7.39667681e-17 -1.71523806e-18
 -4.54265023e-16 -5.70794084e-17 -5.50236170e-15 -5.04353071e-17
  1.24730520e-12  1.39502527e-12 -1.76621686e-14  1.39525502e-12
  2.81736100e-17  2.20779616e-17 -9.67084680e-18  2.01797647e-17
  5.46225467e-18  1.10654156e-18  5.75612405e-17  9.93294886e-19
 -4.25341634e-13 -1.59387650e-11 -2.75627295e-16  7.25650795e-14
  5.02459064e-19  2.69353468e-18 -3.66883352e-18  1.69641874e-14
 -1.09374591e+00]
supnorm grad right now is: 1.0937459053822374
Weights right now are: 
[-11.29795007   2.48912124  -1.62962918   5.35122029  10.7664794
  -2.62311668   1.79433582  -5.91727641   6.5063056  -32.9536504
   9.92092578  -1.08548972   2.54190933  44.99864605   6.79118805
  26.8097292    3.7338324   -4.97319854   2.34316787 -10.56490541
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995004   4.98513713 -11.8360974   21.57457363  -7.27955522
   5.11757491  -4.39409784  24.41493727  13.95025372   1.56630503
   1.06172558 -16.82526464  -2.37084534   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690265  50.93964676  20.79588422
 -10.69141413  -1.40680297  -3.43209899   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960894  -0.57686275  -4.050751    -1.46689791
  -0.80107704   8.98228099  -3.1108763   -1.49498351  24.8765265 ]
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1453957371923
gradient value of function right now is: [ 1.07015810e-09  3.47780856e-28  1.06882216e-09  2.41046844e-28
 -1.07145332e-09 -3.47776072e-28 -1.07011740e-09 -2.41048225e-28
  1.70372365e-13  1.79159177e-13  1.73327373e-13  1.77981308e-13
 -8.27588185e-28 -9.02039604e-28 -8.50947826e-28 -9.01310325e-28
  2.12646558e-12  2.35070307e-12  2.19968877e-12  2.31596124e-12
 -1.58743205e-29 -7.35706979e-28 -2.97980931e-29 -2.71844574e-27
 -1.05414430e-19 -2.56253316e-14  3.40455574e-19  7.26788091e-21
  3.25537425e-16  4.49734511e-14  3.91191713e-17  1.15855373e-17
  5.85533882e-19 -1.89042620e-14  2.70223657e-18  1.08647161e-19
  1.73379045e-17  7.57700672e-14 -7.29612501e-17 -1.53903634e-18
 -4.50604425e-16 -5.59274379e-17 -5.47563788e-15 -4.94168431e-17
  1.24630246e-12  1.39394143e-12 -1.80176378e-14  1.39417521e-12
  2.78995699e-17  2.21511869e-17 -1.34876604e-17  2.02585530e-17
  5.39224218e-18  1.08662125e-18  5.70286278e-17  9.75796354e-19
 -4.24962201e-13 -1.59290826e-11 -3.06546234e-16  7.33028354e-14
  4.86866112e-19  2.80638243e-18 -1.53816966e-17  1.72703200e-14
 -2.73089749e-01]
supnorm grad right now is: 0.2730897492095357
Weights right now are: 
[-11.29815747   2.48912124  -1.62983632   5.35122029  10.76668679
  -2.62311668   1.79454295  -5.91727641   6.50630557 -32.95365044
   9.92092574  -1.08548976   2.54190933  44.99864605   6.79118805
  26.8097292    3.73383198  -4.97319901   2.34316743 -10.56490587
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995005   4.98513713 -11.8360974   21.57457363  -7.27955523
   5.11757491  -4.39409784  24.41493727  13.95025373   1.56630503
   1.06172558 -16.82526464  -2.37084536   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690265  50.93964676  20.79588397
 -10.69141442  -1.40680297  -3.43209927   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960903  -0.57685943  -4.050751    -1.46689793
  -0.80107704   8.98228099  -3.1108763   -1.49498352  24.98513541]
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2040.9027940186072
gradient value of function right now is: [ 1.25113966e-09  3.47500707e-28  1.24980457e-09  2.43824066e-28
 -1.25008530e-09 -3.47516200e-28 -1.24875016e-09 -2.43835372e-28
  1.70243671e-13  1.79023680e-13  1.73196386e-13  1.77846773e-13
 -8.26944845e-28 -9.01338319e-28 -8.50285803e-28 -9.00613787e-28
  2.12505952e-12  2.34984473e-12  2.19825313e-12  2.31498233e-12
 -1.60010233e-29 -7.62965833e-28 -3.14286396e-29 -2.73672113e-27
 -9.02077829e-20 -2.56091385e-14  3.31080003e-19  8.06968526e-21
  3.23332220e-16  4.50107775e-14  3.86454543e-17  1.14484046e-17
  6.78240679e-19 -1.89023429e-14  2.62826322e-18  1.13213838e-19
  1.13236138e-17  7.62409957e-14 -7.43417832e-17 -1.95276070e-18
 -4.58912566e-16 -5.86121450e-17 -5.53235086e-15 -5.17883959e-17
  1.24568210e-12  1.39313712e-12 -1.69326384e-14  1.39335944e-12
  2.78194199e-17  2.16019044e-17 -7.66007660e-18  1.97273034e-17
  5.54076752e-18  1.12569218e-18  5.81756629e-17  1.00976067e-18
 -4.24866987e-13 -1.59127338e-11 -2.25026729e-16  7.10734059e-14
  5.57588499e-19  2.59678189e-18  1.50223181e-17  1.63948250e-14
 -2.61569632e+00]
supnorm grad right now is: 2.615696317696816
Weights right now are: 
[-11.298358     2.48912124  -1.63003657   5.35122029  10.76688734
  -2.62311668   1.79474323  -5.91727641   6.50630554 -32.95365047
   9.92092571  -1.08548979   2.54190933  44.99864605   6.79118805
  26.8097292    3.73383155  -4.97319948   2.34316699 -10.56490634
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995005   4.98513713 -11.8360974   21.57457363  -7.27955524
   5.11757491  -4.39409784  24.41493727  13.95025373   1.56630503
   1.06172558 -16.82526464  -2.37084537   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690266  50.93964676  20.79588371
 -10.6914147   -1.40680296  -3.43209956   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960912  -0.57685621  -4.050751    -1.46689794
  -0.80107704   8.98228099  -3.1108763   -1.49498352  24.79241795]
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.129133393283
gradient value of function right now is: [ 1.10454737e-09  3.47200558e-28  1.10321363e-09  2.41214100e-28
 -1.10296028e-09 -3.47214737e-28 -1.10162648e-09 -2.41225630e-28
  1.70099001e-13  1.78871576e-13  1.73049209e-13  1.77695714e-13
 -8.26226288e-28 -9.00556769e-28 -8.49547019e-28 -8.99836850e-28
  2.12347626e-12  2.34751580e-12  2.19659865e-12  2.31279830e-12
 -1.58709572e-29 -7.39149512e-28 -3.00222355e-29 -2.71830665e-27
 -1.01477702e-19 -2.55886979e-14  3.40136211e-19  7.52368908e-21
  3.24688341e-16  4.49185447e-14  3.88582914e-17  1.15386400e-17
  6.08922255e-19 -1.88787398e-14  2.69736715e-18  1.10142633e-19
  1.58589282e-17  7.57524823e-14 -7.36335999e-17 -1.64819414e-18
 -4.52249449e-16 -5.65715245e-17 -5.48424793e-15 -4.99876668e-17
  1.24454256e-12  1.39195636e-12 -1.78162093e-14  1.39218768e-12
  2.81327663e-17  2.21264642e-17 -1.05230471e-17  2.02289905e-17
  5.42957142e-18  1.09841957e-18  5.72847670e-17  9.86182313e-19
 -4.24383095e-13 -1.59053751e-11 -2.90088471e-16  7.27821081e-14
  4.95214647e-19  2.73988530e-18 -9.06608371e-18  1.70686540e-14
 -7.44164833e-01]
supnorm grad right now is: 0.7441648334473232
Weights right now are: 
[-11.298573     2.48912124  -1.63025132   5.35122029  10.76710236
  -2.62311668   1.79495798  -5.91727641   6.5063055  -32.95365051
   9.92092567  -1.08548983   2.54190933  44.99864605   6.79118805
  26.8097292    3.73383113  -4.97319995   2.34316655 -10.5649068
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995006   4.98513713 -11.8360974   21.57457363  -7.27955525
   5.11757491  -4.39409784  24.41493727  13.95025373   1.56630503
   1.06172558 -16.82526464  -2.37084539   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690266  50.93964676  20.79588347
 -10.69141498  -1.40680296  -3.43209983   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.5496092   -0.57685309  -4.050751    -1.46689796
  -0.80107704   8.98228099  -3.1108763   -1.49498352  24.92338171]
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.0202337765645
gradient value of function right now is: [ 1.19370165e-09  3.46905178e-28  1.19236894e-09  2.42326567e-28
 -1.19555909e-09 -3.46880435e-28 -1.19422646e-09 -2.42320767e-28
  1.69945373e-13  1.78710397e-13  1.72893035e-13  1.77535597e-13
 -8.25470704e-28 -8.99738151e-28 -8.48771188e-28 -8.99022682e-28
  2.12179866e-12  2.34600420e-12  2.19487627e-12  2.31124493e-12
 -1.59230273e-29 -7.50605353e-28 -3.07660101e-29 -2.72496561e-27
 -9.28807433e-20 -2.55701737e-14  3.36561389e-19  8.02070934e-21
  3.23397179e-16  4.49140508e-14  3.85092306e-17  1.14582085e-17
  6.61171417e-19 -1.88695679e-14  2.66795983e-18  1.13039570e-19
  1.26138960e-17  7.59464166e-14 -7.46021769e-17 -1.87855216e-18
 -4.56627621e-16 -5.80171860e-17 -5.51304311e-15 -5.12666890e-17
  1.24371153e-12  1.39098221e-12 -1.73279886e-14  1.39120818e-12
  2.82862610e-17  2.19158557e-17 -6.36669439e-18  2.00208023e-17
  5.51306625e-18  1.12126363e-18  5.79058634e-17  1.00615207e-18
 -4.24161011e-13 -1.58913734e-11 -2.51882225e-16  7.17251956e-14
  5.29153018e-19  2.67108076e-18  5.48556748e-18  1.66437546e-14
 -1.89116455e+00]
supnorm grad right now is: 1.8911645514500082
Weights right now are: 
[-11.29878467   2.48912124  -1.63046272   5.35122029  10.76731404
  -2.62311668   1.79516939  -5.91727641   6.50630547 -32.95365055
   9.92092564  -1.08548987   2.54190933  44.99864605   6.79118805
  26.8097292    3.7338307   -4.97320042   2.34316611 -10.56490726
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995006   4.98513713 -11.8360974   21.57457363  -7.27955526
   5.11757491  -4.39409784  24.41493727  13.95025374   1.56630503
   1.06172558 -16.82526464  -2.3708454    2.51369865   4.22064502
  -4.83421263  38.329907     1.17690266  50.93964676  20.79588322
 -10.69141525  -1.40680295  -3.43210011   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960928  -0.57685002  -4.050751    -1.46689797
  -0.80107704   8.98228099  -3.1108763   -1.49498353  24.84040804]
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.0065656414536
gradient value of function right now is: [ 1.19852212e-09  3.46600927e-28  1.19719058e-09  2.42336644e-28
 -1.19543979e-09 -3.46595576e-28 -1.19410826e-09 -2.42339548e-28
  1.69804841e-13  1.78562460e-13  1.72750007e-13  1.77388704e-13
 -8.24768772e-28 -8.98973133e-28 -8.48048956e-28 -8.98262608e-28
  2.12026127e-12  2.34432603e-12  2.19328528e-12  2.30958664e-12
 -1.59154237e-29 -7.51668964e-28 -3.08106209e-29 -2.72445962e-27
 -9.26780876e-20 -2.55509962e-14  3.35596443e-19  8.00860277e-21
  3.23187462e-16  4.48835565e-14  3.85135934e-17  1.14506720e-17
  6.61765264e-19 -1.88557285e-14  2.66081120e-18  1.12907272e-19
  1.25168118e-17  7.59087513e-14 -7.44615689e-17 -1.88084750e-18
 -4.56492223e-16 -5.80140006e-17 -5.51086241e-15 -5.12632781e-17
  1.24278234e-12  1.38993683e-12 -1.72591887e-14  1.39016212e-12
  2.81821858e-17  2.18534436e-17 -6.73359220e-18  1.99629663e-17
  5.51050074e-18  1.12030609e-18  5.78825958e-17  1.00525364e-18
 -4.23849421e-13 -1.58791151e-11 -2.48326809e-16  7.15724479e-14
  5.30709865e-19  2.66642715e-18  6.60631602e-18  1.65970408e-14
 -1.96826786e+00]
supnorm grad right now is: 1.9682678617362268
Weights right now are: 
[-11.2990034    2.48912124  -1.63068117   5.35122029  10.76753278
  -2.62311668   1.79538786  -5.91727641   6.50630543 -32.95365058
   9.9209256   -1.0854899    2.54190933  44.99864605   6.79118805
  26.8097292    3.73383025  -4.97320091   2.34316565 -10.56490775
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995007   4.98513713 -11.8360974   21.57457363  -7.27955527
   5.11757491  -4.39409784  24.41493727  13.95025374   1.56630503
   1.06172558 -16.82526464  -2.37084542   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690266  50.93964676  20.79588297
 -10.69141553  -1.40680295  -3.43210039   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960937  -0.57684685  -4.050751    -1.46689799
  -0.80107704   8.98228099  -3.1108763   -1.49498353  24.86569274]
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.0812294547718
gradient value of function right now is: [ 1.15194054e-09  3.46304346e-28  1.15061019e-09  2.41415491e-28
 -1.15327831e-09 -3.46319933e-28 -1.15194791e-09 -2.41407883e-28
  1.69668544e-13  1.78418963e-13  1.72611284e-13  1.77246213e-13
 -8.24087634e-28 -8.98230291e-28 -8.47348049e-28 -8.97523909e-28
  2.11876969e-12  2.34250493e-12  2.19173535e-12  2.30782490e-12
 -1.58652606e-29 -7.43864878e-28 -3.03479245e-29 -2.71748460e-27
 -9.59792241e-20 -2.55318530e-14  3.38576547e-19  7.85999165e-21
  3.23542867e-16  4.48336535e-14  3.85591540e-17  1.14748696e-17
  6.41630028e-19 -1.88389040e-14  2.68362078e-18  1.12111743e-19
  1.39160093e-17  7.57305964e-14 -7.42639544e-17 -1.78952250e-18
 -4.54299148e-16 -5.73845112e-17 -5.49491667e-15 -5.07082449e-17
  1.24180969e-12  1.38887669e-12 -1.75021592e-14  1.38910430e-12
  2.83095281e-17  2.20179262e-17 -7.46613470e-18  2.01208683e-17
  5.47635986e-18  1.11233132e-18  5.76030513e-17  9.98398134e-19
 -4.23488018e-13 -1.58687364e-11 -2.65478544e-16  7.20186255e-14
  5.05623761e-19  2.71028977e-18  5.23691133e-20  1.67813046e-14
 -1.39618903e+00]
supnorm grad right now is: 1.3961890289629328
Weights right now are: 
[-11.29921621   2.48912124  -1.63089371   5.35122029  10.76774561
  -2.62311668   1.79560042  -5.91727641   6.5063054  -32.95365062
   9.92092557  -1.08548994   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382982  -4.97320138   2.34316521 -10.56490821
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995007   4.98513713 -11.8360974   21.57457363  -7.27955528
   5.11757491  -4.39409784  24.41493727  13.95025374   1.56630503
   1.06172558 -16.82526464  -2.37084543   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690266  50.93964676  20.79588272
 -10.69141581  -1.40680295  -3.43210067   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960945  -0.57684376  -4.050751    -1.466898
  -0.80107704   8.98228099  -3.1108763   -1.49498353  24.85746808]
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2040.6432587776724
gradient value of function right now is: [ 7.55522312e-10  3.45999237e-28  7.54193645e-10  2.36177091e-28
 -7.52905192e-10 -3.45986445e-28 -7.51576573e-10 -2.36180678e-28
  1.69513223e-13  1.78255973e-13  1.72453349e-13  1.77084392e-13
 -8.23321183e-28 -8.97400021e-28 -8.46560720e-28 -8.96699776e-28
  2.11706646e-12  2.33908309e-12  2.18993009e-12  2.30475005e-12
 -1.55522133e-29 -6.90595552e-28 -2.66722481e-29 -2.68606489e-27
 -1.32985825e-19 -2.55086251e-14  3.52108730e-19  5.59906995e-21
  3.28738536e-16  4.46494027e-14  4.00708026e-17  1.18092305e-17
  4.19003204e-19 -1.87990418e-14  2.79166498e-18  9.85533737e-20
  2.71934541e-17  7.45193442e-14 -6.98149981e-17 -8.14340895e-19
 -4.36275296e-16 -5.13356744e-17 -5.37624855e-15 -4.53539775e-17
  1.24028709e-12  1.38741278e-12 -1.98367005e-14  1.38766581e-12
  2.75103833e-17  2.28875628e-17 -2.60822417e-17  2.09839610e-17
  5.11601802e-18  1.00981222e-18  5.49464248e-17  9.08492893e-19
 -4.22712405e-13 -1.58680280e-11 -4.50094932e-16  7.68189310e-14
  3.34497393e-19  2.92550506e-18 -6.83142592e-17  1.86652078e-14
  3.85953884e+00]
supnorm grad right now is: 3.859538842011505
Weights right now are: 
[-11.29943608   2.48912124  -1.63111331   5.35122029  10.7679655
  -2.62311668   1.79582004  -5.91727641   6.50630536 -32.95365066
   9.92092553  -1.08548998   2.54190933  44.99864605   6.79118805
  26.8097292    3.7338294   -4.97320186   2.34316476 -10.56490868
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995008   4.98513713 -11.8360974   21.57457363  -7.27955529
   5.11757491  -4.39409784  24.41493727  13.95025375   1.56630503
   1.06172558 -16.82526464  -2.37084545   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690266  50.93964676  20.79588248
 -10.69141608  -1.40680294  -3.43210094   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960953  -0.57684065  -4.050751    -1.46689802
  -0.80107704   8.98228099  -3.1108763   -1.49498354  25.24028387]
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1474113795498
gradient value of function right now is: [ 1.02342321e-09  3.45703596e-28  1.02209534e-09  2.39319706e-28
 -1.02405888e-09 -3.45705938e-28 -1.02273100e-09 -2.39315230e-28
  1.69375831e-13  1.78111289e-13  1.72313526e-13  1.76940648e-13
 -8.22635598e-28 -8.96651932e-28 -8.45855396e-28 -8.95955111e-28
  2.11556541e-12  2.33847880e-12  2.18840783e-12  2.30395215e-12
 -1.57472107e-29 -7.25748109e-28 -2.91967503e-29 -2.70218096e-27
 -1.09128249e-19 -2.54922782e-14  3.39748276e-19  6.97269268e-21
  3.24820149e-16  4.47194145e-14  3.91675594e-17  1.15770743e-17
  5.59487012e-19 -1.88024078e-14  2.69777902e-18  1.06655465e-19
  1.87138695e-17  7.52516410e-14 -7.20886022e-17 -1.42532625e-18
 -4.46873082e-16 -5.50585659e-17 -5.44116238e-15 -4.86476527e-17
  1.23974613e-12  1.38664050e-12 -1.81981019e-14  1.38687613e-12
  2.76122037e-17  2.21205350e-17 -1.58954426e-17  2.02375435e-17
  5.33066435e-18  1.07057868e-18  5.65117617e-17  9.61582089e-19
 -4.22706077e-13 -1.58483877e-11 -3.26998571e-16  7.34819782e-14
  4.54392716e-19  2.72115435e-18 -2.31785989e-17  1.73892232e-14
  2.38349397e-01]
supnorm grad right now is: 0.23834939749591905
Weights right now are: 
[-11.2996488    2.48912124  -1.63132575   5.35122029  10.76817823
  -2.62311668   1.79603249  -5.91727641   6.50630533 -32.95365069
   9.9209255   -1.08549001   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382896  -4.97320234   2.34316431 -10.56490915
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995008   4.98513713 -11.8360974   21.57457363  -7.27955529
   5.11757491  -4.39409784  24.41493727  13.95025375   1.56630503
   1.06172558 -16.82526464  -2.37084546   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690266  50.93964676  20.79588223
 -10.69141636  -1.40680294  -3.43210122   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960962  -0.57683737  -4.050751    -1.46689803
  -0.80107704   8.98228099  -3.1108763   -1.49498354  25.01828504]
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1451839104136
gradient value of function right now is: [ 1.01345805e-09  3.45419895e-28  1.01213128e-09  2.39163241e-28
 -1.01498250e-09 -3.45419561e-28 -1.01365573e-09 -2.39161054e-28
  1.69238547e-13  1.77966968e-13  1.72173869e-13  1.76797324e-13
 -8.21954143e-28 -8.95911125e-28 -8.45154800e-28 -8.95219289e-28
  2.11406290e-12  2.33680066e-12  2.18685354e-12  2.30230142e-12
 -1.57320694e-29 -7.25113799e-28 -2.91407385e-29 -2.70058234e-27
 -1.09818384e-19 -2.54740765e-14  3.39562076e-19  6.92186354e-21
  3.24810712e-16  4.46868810e-14  3.92032867e-17  1.15793878e-17
  5.54537107e-19 -1.87886762e-14  2.69663118e-18  1.06310572e-19
  1.89826487e-17  7.51835557e-14 -7.19002678e-17 -1.40368988e-18
 -4.46242019e-16 -5.48955602e-17 -5.43662224e-15 -4.85038852e-17
  1.23885371e-12  1.38564404e-12 -1.81944005e-14  1.38587966e-12
  2.75383829e-17  2.21011982e-17 -1.65395609e-17  2.02211041e-17
  5.31987557e-18  1.06761424e-18  5.64305542e-17  9.58968181e-19
 -4.22402393e-13 -1.58372269e-11 -3.28163174e-16  7.34610392e-14
  4.54103005e-19  2.71797578e-18 -2.38410921e-17  1.73945388e-14
  3.51768259e-01]
supnorm grad right now is: 0.35176825868618555
Weights right now are: 
[-11.29985308   2.48912124  -1.63152977   5.35122029  10.7683825
  -2.62311668   1.79623649  -5.91727641   6.50630529 -32.95365073
   9.92092546  -1.08549005   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382853  -4.97320281   2.34316387 -10.56490962
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995009   4.98513713 -11.8360974   21.57457363  -7.2795553
   5.11757491  -4.39409784  24.41493727  13.95025376   1.56630503
   1.06172558 -16.82526464  -2.37084548   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690266  50.93964676  20.79588197
 -10.69141665  -1.40680294  -3.43210151   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960971  -0.57683407  -4.050751    -1.46689804
  -0.80107704   8.98228099  -3.1108763   -1.49498354  25.03515733]
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1429764718491
gradient value of function right now is: [ 1.06965840e-09  3.45133268e-28  1.06833266e-09  2.39857681e-28
 -1.06663835e-09 -3.45146882e-28 -1.06531257e-09 -2.39859672e-28
  1.69104901e-13  1.77826313e-13  1.72037860e-13  1.76657658e-13
 -8.21287342e-28 -8.95184695e-28 -8.44468802e-28 -8.94497579e-28
  2.11259988e-12  2.33539883e-12  2.18534534e-12  2.30087777e-12
 -1.57593977e-29 -7.32672303e-28 -2.96050610e-29 -2.70460897e-27
 -1.04008461e-19 -2.54563197e-14  3.38744714e-19  7.29447604e-21
  3.23783788e-16  4.46750484e-14  3.88753748e-17  1.15190156e-17
  5.88518251e-19 -1.87786165e-14  2.68719761e-18  1.08466542e-19
  1.68525161e-17  7.52950588e-14 -7.27257405e-17 -1.55891211e-18
 -4.48926578e-16 -5.58146571e-17 -5.45227944e-15 -4.93176549e-17
  1.23804547e-12  1.38470900e-12 -1.78530457e-14  1.38494064e-12
  2.77966766e-17  2.20229312e-17 -1.28609009e-17  2.01398829e-17
  5.37574407e-18  1.08388372e-18  5.68223162e-17  9.73288204e-19
 -4.22163885e-13 -1.58243645e-11 -2.99969678e-16  7.26974942e-14
  4.83926401e-19  2.77842651e-18 -1.34410727e-17  1.70989685e-14
 -3.74409156e-01]
supnorm grad right now is: 0.3744091558513309
Weights right now are: 
[-11.30005935   2.48912124  -1.63173577   5.35122029  10.76858875
  -2.62311668   1.79644248  -5.91727641   6.50630526 -32.95365076
   9.92092543  -1.08549008   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382811  -4.97320328   2.34316343 -10.56491008
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995009   4.98513713 -11.8360974   21.57457363  -7.27955531
   5.11757491  -4.39409784  24.41493727  13.95025376   1.56630503
   1.06172558 -16.82526464  -2.37084549   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690266  50.93964676  20.79588172
 -10.69141693  -1.40680293  -3.43210178   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960979  -0.57683095  -4.050751    -1.46689806
  -0.80107704   8.98228099  -3.1108763   -1.49498355  24.92668093]
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1475287535789
gradient value of function right now is: [ 1.05344487e-09  3.44850095e-28  1.05212025e-09  2.39585309e-28
 -1.05089468e-09 -3.44860717e-28 -1.04957001e-09 -2.39590497e-28
  1.68967812e-13  1.77682200e-13  1.71898403e-13  1.76514535e-13
 -8.20607033e-28 -8.94445027e-28 -8.43769409e-28 -8.93762370e-28
  2.11109967e-12  2.33368771e-12  2.18379247e-12  2.29920240e-12
 -1.57393673e-29 -7.30942040e-28 -2.94841835e-29 -2.70221801e-27
 -1.06004141e-19 -2.54381088e-14  3.37889127e-19  7.13442962e-21
  3.23839564e-16  4.46384545e-14  3.89927948e-17  1.15304410e-17
  5.75299580e-19 -1.87643340e-14  2.68193516e-18  1.07423609e-19
  1.75447163e-17  7.52007171e-14 -7.22317705e-17 -1.50112995e-18
 -4.47627347e-16 -5.54359254e-17 -5.44221359e-15 -4.89795507e-17
  1.23713902e-12  1.38370211e-12 -1.79135655e-14  1.38393461e-12
  2.75682618e-17  2.19756241e-17 -1.46670338e-17  2.00995040e-17
  5.34963586e-18  1.07615357e-18  5.66281961e-17  9.66395763e-19
 -4.21848018e-13 -1.58135063e-11 -3.07698429e-16  7.28300256e-14
  4.82689912e-19  2.78696365e-18 -1.64588956e-17  1.71652461e-14
 -1.67820580e-01]
supnorm grad right now is: 0.16782057986172297
Weights right now are: 
[-11.30026403   2.48912124  -1.63194018   5.35122029  10.76879345
  -2.62311668   1.79664691  -5.91727641   6.50630523 -32.9536508
   9.92092539  -1.08549012   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382768  -4.97320375   2.34316299 -10.56491054
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.9299501    4.98513713 -11.8360974   21.57457363  -7.27955532
   5.11757491  -4.39409784  24.41493727  13.95025376   1.56630503
   1.06172558 -16.82526464  -2.37084551   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690267  50.93964676  20.79588148
 -10.6914172   -1.40680293  -3.43210205   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960987  -0.5768279   -4.050751    -1.46689807
  -0.80107704   8.98228099  -3.1108763   -1.49498355  25.01322299]
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2040.8225343794925
gradient value of function right now is: [ 8.09958033e-10  3.44560693e-28  8.08634857e-10  2.36223554e-28
 -8.07013591e-10 -3.44555193e-28 -8.05690437e-10 -2.36221565e-28
  1.68823580e-13  1.77530781e-13  1.71751721e-13  1.76364214e-13
 -8.19893866e-28 -8.93671781e-28 -8.43036576e-28 -8.92994859e-28
  2.10951898e-12  2.33093012e-12  2.18212829e-12  2.29668331e-12
 -1.55306640e-29 -6.95411014e-28 -2.70279418e-29 -2.68311245e-27
 -1.27361724e-19 -2.54171494e-14  3.49270097e-19  5.93285278e-21
  3.27201787e-16  4.45016851e-14  3.97506671e-17  1.17322059e-17
  4.52667703e-19 -1.87335765e-14  2.77031921e-18  1.00561502e-19
  2.52106015e-17  7.43930942e-14 -7.03491778e-17 -9.58780257e-19
 -4.37981037e-16 -5.21548745e-17 -5.38085447e-15 -4.60804804e-17
  1.23585057e-12  1.38242671e-12 -1.95139280e-14  1.38267615e-12
  2.75772672e-17  2.27111177e-17 -2.31571593e-17  2.08120088e-17
  5.15996105e-18  1.02374838e-18  5.52199429e-17  9.20684847e-19
 -4.21236545e-13 -1.58099212e-11 -4.28765388e-16  7.59855700e-14
  4.24100016e-19  2.88841466e-18 -6.01198577e-17  1.83579445e-14
  3.07184455e+00]
supnorm grad right now is: 3.071844549817887
Weights right now are: 
[-11.30047298   2.48912124  -1.63214887   5.35122029  10.76900241
  -2.62311668   1.79685561  -5.91727641   6.50630519 -32.95365084
   9.92092536  -1.08549015   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382726  -4.97320422   2.34316255 -10.564911
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.9299501    4.98513713 -11.8360974   21.57457363  -7.27955533
   5.11757491  -4.39409784  24.41493727  13.95025377   1.56630503
   1.06172558 -16.82526464  -2.37084552   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690267  50.93964676  20.79588124
 -10.69141747  -1.40680293  -3.43210233   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54960995  -0.57682481  -4.050751    -1.46689809
  -0.80107704   8.98228099  -3.1108763   -1.49498355  25.19693115]
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.141979281898
gradient value of function right now is: [ 1.06980029e-09  3.44273147e-28  1.06847787e-09  2.39412950e-28
 -1.06968478e-09 -3.44284282e-28 -1.06836232e-09 -2.39410691e-28
  1.68690575e-13  1.77390686e-13  1.71616352e-13  1.76225031e-13
 -8.19229603e-28 -8.92946672e-28 -8.42353105e-28 -8.92273083e-28
  2.10806430e-12  2.33038877e-12  2.18065390e-12  2.29594185e-12
 -1.57200933e-29 -7.31117693e-28 -2.95180748e-29 -2.69967181e-27
 -1.03591880e-19 -2.54012681e-14  3.38400700e-19  7.30549702e-21
  3.23209941e-16  4.45761873e-14  3.87856576e-17  1.14972998e-17
  5.89332682e-19 -1.87375129e-14  2.68391328e-18  1.08436861e-19
  1.67050382e-17  7.51364014e-14 -7.26974361e-17 -1.56564685e-18
 -4.48355850e-16 -5.57817359e-17 -5.44447464e-15 -4.92880128e-17
  1.23535125e-12  1.38169923e-12 -1.78243478e-14  1.38193043e-12
  2.77920436e-17  2.19977027e-17 -1.24866448e-17  2.01163385e-17
  5.37054639e-18  1.08332230e-18  5.67560178e-17  9.72759263e-19
 -4.21249591e-13 -1.57904333e-11 -2.99466108e-16  7.25423748e-14
  4.82977324e-19  2.77281670e-18 -1.32885067e-17  1.70561886e-14
 -4.09468359e-01]
supnorm grad right now is: 0.409468359473933
Weights right now are: 
[-11.3006806    2.48912124  -1.63235622   5.35122029  10.76921
  -2.62311668   1.79706294  -5.91727641   6.50630516 -32.95365087
   9.92092532  -1.08549019   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382684  -4.97320468   2.34316212 -10.56491146
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995011   4.98513713 -11.8360974   21.57457363  -7.27955534
   5.11757491  -4.39409784  24.41493727  13.95025377   1.56630503
   1.06172558 -16.82526464  -2.37084554   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690267  50.93964676  20.79588099
 -10.69141775  -1.40680292  -3.43210261   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961004  -0.57682156  -4.050751    -1.4668981
  -0.80107704   8.98228099  -3.1108763   -1.49498356  24.95059509]
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.0999085212939
gradient value of function right now is: [ 9.45439041e-10  3.43980756e-28  9.44117893e-10  2.37831562e-28
 -9.46005819e-10 -3.43990012e-28 -9.44684636e-10 -2.37829608e-28
  1.68549371e-13  1.77242239e-13  1.71472707e-13  1.76077605e-13
 -8.18528681e-28 -8.92184500e-28 -8.41632491e-28 -8.91515560e-28
  2.10651776e-12  2.32824908e-12  2.17904262e-12  2.29391458e-12
 -1.56328192e-29 -7.16902191e-28 -2.85873695e-29 -2.68856326e-27
 -1.15811534e-19 -2.53817285e-14  3.38386287e-19  6.46197913e-21
  3.24545800e-16  4.45061590e-14  3.94268680e-17  1.15982133e-17
  5.13004321e-19 -1.87171244e-14  2.68938683e-18  1.03210425e-19
  2.11242510e-17  7.47549158e-14 -7.05046412e-17 -1.22580530e-18
 -4.41711094e-16 -5.36401418e-17 -5.39773500e-15 -4.73886106e-17
  1.23428801e-12  1.38056637e-12 -1.84032804e-14  1.38080451e-12
  2.69975690e-17  2.20240330e-17 -2.11154318e-17  2.01607189e-17
  5.23551884e-18  1.04357165e-18  5.57618165e-17  9.37639759e-19
 -4.20819741e-13 -1.57816444e-11 -3.52673748e-16  7.38252088e-14
  4.40823452e-19  2.78857708e-18 -3.35779111e-17  1.75937547e-14
  1.23412113e+00]
supnorm grad right now is: 1.2341211302520598
Weights right now are: 
[-11.30089178   2.48912124  -1.63256716   5.35122029  10.76942119
  -2.62311668   1.79727388  -5.91727641   6.50630513 -32.9536509
   9.92092529  -1.08549022   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382645  -4.97320512   2.34316171 -10.56491189
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995011   4.98513713 -11.8360974   21.57457363  -7.27955535
   5.11757491  -4.39409784  24.41493727  13.95025377   1.56630503
   1.06172558 -16.82526464  -2.37084555   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690267  50.93964676  20.79588074
 -10.69141803  -1.40680292  -3.43210289   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961012  -0.57681841  -4.050751    -1.46689812
  -0.80107704   8.98228099  -3.1108763   -1.49498356  25.05981493]
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2040.7650087301554
gradient value of function right now is: [ 7.89105848e-10  3.43684554e-28  7.87786064e-10  2.35768637e-28
 -7.85989612e-10 -3.43662377e-28 -7.84669910e-10 -2.35769679e-28
  1.68397322e-13  1.77082733e-13  1.71318116e-13  1.75919236e-13
 -8.17779540e-28 -8.91373387e-28 -8.40863058e-28 -8.90710398e-28
  2.10485180e-12  2.32573999e-12  2.17730112e-12  2.29157457e-12
 -1.54851642e-29 -6.93839455e-28 -2.68759685e-29 -2.67825431e-27
 -1.29462814e-19 -2.53608550e-14  3.47996092e-19  5.74756262e-21
  3.26893839e-16  4.44011341e-14  3.98371896e-17  1.17317022e-17
  4.37263439e-19 -1.86912239e-14  2.76121108e-18  9.92622017e-20
  2.59403507e-17  7.42002300e-14 -6.96823041e-17 -8.93065319e-19
 -4.36074874e-16 -5.16654153e-17 -5.36369332e-15 -4.56458958e-17
  1.23309290e-12  1.37934719e-12 -1.94913157e-14  1.37959648e-12
  2.72754831e-17  2.26196253e-17 -2.52396332e-17  2.07313424e-17
  5.12503505e-18  1.01369238e-18  5.49402930e-17  9.11726455e-19
 -4.20300405e-13 -1.57753455e-11 -4.31608504e-16  7.59074123e-14
  4.11246716e-19  2.93389598e-18 -6.18345064e-17  1.83682424e-14
  3.32232797e+00]
supnorm grad right now is: 3.3223279665475736
Weights right now are: 
[-11.30110688   2.48912124  -1.63278199   5.35122029  10.76963628
  -2.62311668   1.7974887   -5.91727641   6.50630509 -32.95365094
   9.92092526  -1.08549026   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382602  -4.97320559   2.34316127 -10.56491235
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995012   4.98513713 -11.8360974   21.57457363  -7.27955536
   5.11757491  -4.39409784  24.41493727  13.95025378   1.56630503
   1.06172558 -16.82526464  -2.37084557   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690267  50.93964676  20.79588049
 -10.6914183   -1.40680292  -3.43210316   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.5496102   -0.57681532  -4.050751    -1.46689813
  -0.80107704   8.98228099  -3.1108763   -1.49498356  25.22702363]
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.139820825074
gradient value of function right now is: [ 1.07037138e-09  3.43406365e-28  1.06905230e-09  2.39002861e-28
 -1.07115839e-09 -3.43402866e-28 -1.06983933e-09 -2.39001217e-28
  1.68269373e-13  1.76947942e-13  1.71187887e-13  1.75785321e-13
 -8.17140201e-28 -8.90675311e-28 -8.40205192e-28 -8.90015464e-28
  2.10345253e-12  2.32530593e-12  2.17588500e-12  2.29093176e-12
 -1.56812035e-29 -7.29949610e-28 -2.94420333e-29 -2.69499551e-27
 -1.03219060e-19 -2.53456937e-14  3.38062628e-19  7.31518898e-21
  3.22648064e-16  4.44778242e-14  3.87001992e-17  1.14761338e-17
  5.89945510e-19 -1.86961513e-14  2.68071280e-18  1.08405532e-19
  1.65856227e-17  7.49821649e-14 -7.26537784e-17 -1.57078430e-18
 -4.47755931e-16 -5.57371762e-17 -5.43644023e-15 -4.92493664e-17
  1.23263544e-12  1.37866470e-12 -1.77746851e-14  1.37889520e-12
  2.77816556e-17  2.19690250e-17 -1.21716518e-17  2.00898933e-17
  5.36493501e-18  1.08261731e-18  5.66853842e-17  9.72127102e-19
 -4.20332718e-13 -1.57561601e-11 -2.97268732e-16  7.23511474e-14
  4.90596531e-19  2.71624199e-18 -1.26573325e-17  1.70031445e-14
 -4.52274605e-01]
supnorm grad right now is: 0.45227460539454933
Weights right now are: 
[-11.3013084    2.48912124  -1.63298325   5.35122029  10.76983778
  -2.62311668   1.79768994  -5.91727641   6.50630506 -32.95365097
   9.92092522  -1.08549029   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382561  -4.97320604   2.34316085 -10.5649128
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995012   4.98513713 -11.8360974   21.57457363  -7.27955537
   5.11757491  -4.39409784  24.41493727  13.95025378   1.56630503
   1.06172558 -16.82526464  -2.37084558   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690267  50.93964676  20.79588025
 -10.69141857  -1.40680291  -3.43210343   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961028  -0.57681225  -4.050751    -1.46689815
  -0.80107704   8.98228099  -3.1108763   -1.49498357  24.96495598]
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2040.4634645624576
gradient value of function right now is: [ 7.03369885e-10  3.41825827e-28  7.02058895e-10  2.34288705e-28
 -7.07065767e-10 -3.41831561e-28 -7.05754754e-10 -2.34288400e-28
  1.68041776e-13  1.76659492e-13  1.70940164e-13  1.75506328e-13
 -8.14156032e-28 -8.86335540e-28 -8.36774530e-28 -8.85883496e-28
  2.09415295e-12  2.31005494e-12  2.16505147e-12  2.27668028e-12
 -1.53704649e-29 -6.81501881e-28 -2.60678984e-29 -2.66708997e-27
 -1.36446918e-19 -2.48413860e-14  3.50043345e-19  5.26781472e-21
  3.27464788e-16  4.35651105e-14  4.00919699e-17  1.17816357e-17
  3.91935710e-19 -1.83266062e-14  2.77519314e-18  9.61020533e-20
  2.84007914e-17  7.23995868e-14 -6.85848256e-17 -6.98801583e-19
 -4.32071477e-16 -5.03805865e-17 -5.33452644e-15 -4.45070926e-17
  1.20989332e-12  1.35237546e-12 -1.99331941e-14  1.35262956e-12
  2.70380185e-17  2.27404662e-17 -2.92397011e-17  2.08571822e-17
  5.04509154e-18  9.90181600e-19  5.43382701e-17  8.91026259e-19
 -4.03592357e-13 -1.52924475e-11 -4.68731822e-16  7.67575291e-14
  3.31018065e-19  2.93325926e-18 -7.56989511e-17  1.87157083e-14
  4.46305394e+00]
supnorm grad right now is: 4.463053938455097
Weights right now are: 
[-11.30150827   2.48912124  -1.63318287   5.35122029  10.77003767
  -2.62311668   1.79788959  -5.91727641   6.50630503 -32.95365101
   9.92092519  -1.08549033   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382521  -4.97320649   2.34316043 -10.56491324
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995013   4.98513713 -11.8360974   21.57457363  -7.27955537
   5.11757491  -4.39409784  24.41493727  13.95025379   1.56630503
   1.06172558 -16.82526464  -2.3708456    2.51369865   4.22064502
  -4.83421263  38.329907     1.17690267  50.93964676  20.79588002
 -10.69141883  -1.40680291  -3.43210369   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961036  -0.57680926  -4.050751    -1.46689816
  -0.80107704   8.98228099  -3.1108763   -1.49498357  25.25428292]
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2040.9093664165305
gradient value of function right now is: [ 1.23028478e-09  3.42834599e-28  1.22896771e-09  2.41417395e-28
 -1.22891061e-09 -3.42850291e-28 -1.22759348e-09 -2.41430107e-28
  1.68000082e-13  1.76664624e-13  1.70913868e-13  1.75503980e-13
 -8.15799021e-28 -8.89215027e-28 -8.38825703e-28 -8.88564220e-28
  2.10050422e-12  2.32267299e-12  2.17285246e-12  2.28821782e-12
 -1.57845827e-29 -7.54687862e-28 -3.09157412e-29 -2.71020778e-27
 -8.99860716e-20 -2.53106988e-14  3.28198462e-19  7.96914738e-21
  3.20497801e-16  4.44759319e-14  3.83294354e-17  1.13502181e-17
  6.69774649e-19 -1.86793102e-14  2.60426948e-18  1.12030438e-19
  1.13323548e-17  7.53443847e-14 -7.35438366e-17 -1.92405563e-18
 -4.54688371e-16 -5.80145445e-17 -5.48200431e-15 -5.12588002e-17
  1.23107558e-12  1.37681861e-12 -1.67730088e-14  1.37703874e-12
  2.75061484e-17  2.13882494e-17 -8.07195604e-18  1.95330811e-17
  5.48703415e-18  1.11385057e-18  5.76261847e-17  9.99139188e-19
 -4.19907363e-13 -1.57286941e-11 -2.25202365e-16  7.02917004e-14
  5.27945448e-19  2.60787630e-18  1.40941403e-17  1.62079310e-14
 -2.56170719e+00]
supnorm grad right now is: 2.5617071897201082
Weights right now are: 
[-11.30172315   2.48912124  -1.63339748   5.35122029  10.77025256
  -2.62311668   1.79810419  -5.91727641   6.50630499 -32.95365105
   9.92092516  -1.08549036   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382477  -4.97320697   2.34315998 -10.56491372
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995013   4.98513713 -11.8360974   21.57457363  -7.27955538
   5.11757491  -4.39409784  24.41493727  13.95025379   1.56630503
   1.06172558 -16.82526464  -2.37084561   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690267  50.93964676  20.79587976
 -10.69141912  -1.40680291  -3.43210398   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961045  -0.57680609  -4.050751    -1.46689817
  -0.80107704   8.98228099  -3.1108763   -1.49498357  24.82634659]
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1088783006473
gradient value of function right now is: [ 9.49519749e-10  3.42527538e-28  9.48204202e-10  2.37181984e-28
 -9.45921104e-10 -3.42527885e-28 -9.44605558e-10 -2.37176689e-28
  1.67847856e-13  1.76504700e-13  1.70759046e-13  1.75345140e-13
 -8.15045818e-28 -8.88396958e-28 -8.38051661e-28 -8.87751074e-28
  2.09883394e-12  2.31977721e-12  2.17109580e-12  2.28556419e-12
 -1.55710243e-29 -7.15260815e-28 -2.84936628e-29 -2.68094321e-27
 -1.14980432e-19 -2.52886663e-14  3.37147167e-19  6.47543773e-21
  3.23573364e-16  4.43414797e-14  3.93007236e-17  1.15612739e-17
  5.15376763e-19 -1.86479430e-14  2.67934153e-18  1.03129031e-19
  2.08281543e-17  7.44972155e-14 -7.03579717e-17 -1.23855745e-18
 -4.40833114e-16 -5.35982995e-17 -5.38523713e-15 -4.73513481e-17
  1.22973878e-12  1.37548084e-12 -1.83163818e-14  1.37571790e-12
  2.69152518e-17  2.19404528e-17 -2.08514297e-17  2.00829586e-17
  5.22699474e-18  1.04218306e-18  5.56544465e-17  9.36339496e-19
 -4.19280300e-13 -1.57240750e-11 -3.49676316e-16  7.35027553e-14
  4.44761548e-19  2.75140024e-18 -3.26372626e-17  1.75012060e-14
  1.12016494e+00]
supnorm grad right now is: 1.1201649405719412
Weights right now are: 
[-11.30194573   2.48912124  -1.63361978   5.35122029  10.77047516
  -2.62311668   1.79832652  -5.91727641   6.50630496 -32.95365108
   9.92092512  -1.0854904    2.54190933  44.99864605   6.79118805
  26.8097292    3.73382432  -4.97320746   2.34315952 -10.5649142
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995014   4.98513713 -11.8360974   21.57457363  -7.27955539
   5.11757491  -4.39409784  24.41493727  13.95025379   1.56630503
   1.06172558 -16.82526464  -2.37084563   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690268  50.93964676  20.7958795
 -10.69141941  -1.4068029   -3.43210427   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961054  -0.57680275  -4.050751    -1.46689819
  -0.80107704   8.98228099  -3.1108763   -1.49498358  25.06826803]
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2040.5169231351092
gradient value of function right now is: [ 1.34394606e-09  3.42237646e-28  1.34263112e-09  2.43722530e-28
 -1.34679034e-09 -3.42219271e-28 -1.34547546e-09 -2.43720400e-28
  1.67702791e-13  1.76352350e-13  1.70611525e-13  1.75193815e-13
 -8.14329179e-28 -8.87619178e-28 -8.37315341e-28 -8.86977791e-28
  2.09725004e-12  2.31963589e-12  2.16950715e-12  2.28511518e-12
 -1.58851207e-29 -7.78326920e-28 -3.23034481e-29 -2.72461631e-27
 -8.01802873e-20 -2.52735451e-14  3.21475081e-19  8.44151633e-21
  3.18911188e-16  4.44693461e-14  3.80480801e-17  1.12558567e-17
  7.25514282e-19 -1.86603096e-14  2.54933715e-18  1.14399504e-19
  7.41379625e-18  7.56355955e-14 -7.40897506e-17 -2.17912472e-18
 -4.59519471e-16 -5.95781846e-17 -5.51573154e-15 -5.26417660e-17
  1.22950151e-12  1.37501824e-12 -1.59973707e-14  1.37523024e-12
  2.73213921e-17  2.09611192e-17 -5.28390832e-18  1.91256149e-17
  5.57123267e-18  1.13441714e-18  5.82904970e-17  1.01695514e-18
 -4.19453077e-13 -1.57034655e-11 -1.66876362e-16  6.87863890e-14
  5.78128338e-19  2.39838460e-18  3.50954369e-17  1.56336398e-14
 -4.07317316e+00]
supnorm grad right now is: 4.073173158750881
Weights right now are: 
[-11.30215685   2.48912124  -1.63383062   5.35122029  10.77068627
  -2.62311668   1.79853735  -5.91727641   6.50630492 -32.95365112
   9.92092508  -1.08549044   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382388  -4.97320795   2.34315906 -10.56491468
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995014   4.98513713 -11.8360974   21.57457363  -7.2795554
   5.11757491  -4.39409784  24.41493727  13.9502538    1.56630503
   1.06172558 -16.82526464  -2.37084564   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690268  50.93964676  20.79587926
 -10.69141968  -1.4068029   -3.43210454   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961061  -0.57679977  -4.050751    -1.4668982
  -0.80107704   8.98228099  -3.1108763   -1.49498358  24.6924289 ]
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1434207945103
gradient value of function right now is: [ 1.05821017e-09  3.41970408e-28  1.05689665e-09  2.38280194e-28
 -1.05533761e-09 -3.41958346e-28 -1.05402413e-09 -2.38280691e-28
  1.67576163e-13  1.76219143e-13  1.70482679e-13  1.75061537e-13
 -8.13698903e-28 -8.86933063e-28 -8.36667115e-28 -8.86296073e-28
  2.09585818e-12  2.31689043e-12  2.16802984e-12  2.28264410e-12
 -1.56150317e-29 -7.27551762e-28 -2.92825278e-29 -2.68700709e-27
 -1.03965379e-19 -2.52536779e-14  3.36248352e-19  7.20570716e-21
  3.21854500e-16  4.43128195e-14  3.86861637e-17  1.14530460e-17
  5.81937707e-19 -1.86272935e-14  2.66718494e-18  1.07516314e-19
  1.68587516e-17  7.46939954e-14 -7.20976419e-17 -1.53667884e-18
 -4.46033371e-16 -5.53982050e-17 -5.41798534e-15 -4.89477141e-17
  1.22812798e-12  1.37362894e-12 -1.77306468e-14  1.37385902e-12
  2.75172285e-17  2.18492505e-17 -1.34781602e-17  1.99816684e-17
  5.33743400e-18  1.07506176e-18  5.64406292e-17  9.65349575e-19
 -4.18802463e-13 -1.56994620e-11 -2.99604956e-16  7.21508315e-14
  4.79809869e-19  2.75649334e-18 -1.39083176e-17  1.69672357e-14
 -3.51025871e-01]
supnorm grad right now is: 0.3510258705452554
Weights right now are: 
[-11.30235109   2.48912124  -1.63402461   5.35122029  10.77088054
  -2.62311668   1.79873136  -5.91727641   6.50630489 -32.95365115
   9.92092505  -1.08549047   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382348  -4.9732084    2.34315864 -10.56491512
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995015   4.98513713 -11.8360974   21.57457363  -7.27955541
   5.11757491  -4.39409784  24.41493727  13.9502538    1.56630503
   1.06172558 -16.82526464  -2.37084566   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690268  50.93964676  20.79587902
 -10.69141995  -1.40680289  -3.43210481   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.5496107   -0.57679666  -4.050751    -1.46689822
  -0.80107704   8.98228099  -3.1108763   -1.49498358  24.92856416]
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2040.815575786
gradient value of function right now is: [ 8.01324546e-10  3.41676446e-28  8.00012500e-10  2.34859948e-28
 -8.00909223e-10 -3.41676030e-28 -7.99597179e-10 -2.34861740e-28
  1.67438110e-13  1.76073956e-13  1.70342194e-13  1.74917437e-13
 -8.13010850e-28 -8.86184590e-28 -8.35959315e-28 -8.85553158e-28
  2.09434157e-12  2.31415692e-12  2.16642852e-12  2.28015738e-12
 -1.54020591e-29 -6.91502954e-28 -2.67720308e-29 -2.66758488e-27
 -1.27185156e-19 -2.52325711e-14  3.46871551e-19  5.85856773e-21
  3.25409701e-16  4.41736027e-14  3.95742900e-17  1.16703347e-17
  4.47394790e-19 -1.85959947e-14  2.75101348e-18  9.97269976e-20
  2.51601068e-17  7.38550642e-14 -6.97735714e-17 -9.42178662e-19
 -4.35407796e-16 -5.17946529e-17 -5.34987799e-15 -4.57609828e-17
  1.22682305e-12  1.37233616e-12 -1.93752519e-14  1.37258389e-12
  2.73120593e-17  2.25407771e-17 -2.37076194e-17  2.06561132e-17
  5.12622195e-18  1.01597791e-18  5.48798959e-17  9.13686282e-19
 -4.18173043e-13 -1.56958045e-11 -4.26729686e-16  7.54345983e-14
  4.09557434e-19  2.92091958e-18 -5.99991319e-17  1.82203515e-14
  3.09575139e+00]
supnorm grad right now is: 3.095751394180639
Weights right now are: 
[-11.30256526   2.48912124  -1.63423851   5.35122029  10.77109471
  -2.62311668   1.79894526  -5.91727641   6.50630486 -32.95365119
   9.92092501  -1.08549051   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382304  -4.97320887   2.34315819 -10.56491559
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995015   4.98513713 -11.8360974   21.57457363  -7.27955542
   5.11757491  -4.39409784  24.41493727  13.9502538    1.56630503
   1.06172558 -16.82526464  -2.37084567   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690268  50.93964676  20.79587877
 -10.69142022  -1.40680289  -3.43210508   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961078  -0.57679349  -4.050751    -1.46689823
  -0.80107704   8.98228099  -3.1108763   -1.49498359  25.20371393]
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2040.4678457272653
gradient value of function right now is: [ 1.35429294e-09  3.41402556e-28  1.35298119e-09  2.43635590e-28
 -1.35385196e-09 -3.41390168e-28 -1.35254026e-09 -2.43638292e-28
  1.67302971e-13  1.75931888e-13  1.70204745e-13  1.74776264e-13
 -8.12341951e-28 -8.85457211e-28 -8.35271893e-28 -8.84829311e-28
  2.09286679e-12  2.31485046e-12  2.16497457e-12  2.28038835e-12
 -1.58615461e-29 -7.80071299e-28 -3.23884073e-29 -2.72229895e-27
 -7.92387224e-20 -2.52202391e-14  3.19728969e-19  8.45809116e-21
  3.18301788e-16  4.43814934e-14  3.80002981e-17  1.12314113e-17
  7.29360340e-19 -1.86215436e-14  2.53584208e-18  1.14354423e-19
  7.02328041e-18  7.55242891e-14 -7.39296302e-17 -2.19858041e-18
 -4.59310442e-16 -5.96424301e-17 -5.51112166e-15 -5.26965969e-17
  1.22691127e-12  1.37211174e-12 -1.58485192e-14  1.37232216e-12
  2.71736233e-17  2.08436272e-17 -5.43797988e-18  1.90161353e-17
  5.56990471e-18  1.13415915e-18  5.82658673e-17  1.01662062e-18
 -4.18585541e-13 -1.56698671e-11 -1.58574080e-16  6.84250432e-14
  6.13784438e-19  2.27902575e-18  3.78694159e-17  1.55193977e-14
 -4.24471878e+00]
supnorm grad right now is: 4.244718779199299
Weights right now are: 
[-11.30276454   2.48912124  -1.63443753   5.35122029  10.771294
  -2.62311668   1.79914429  -5.91727641   6.50630482 -32.95365123
   9.92092498  -1.08549054   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382263  -4.97320933   2.34315777 -10.56491604
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995016   4.98513713 -11.8360974   21.57457363  -7.27955543
   5.11757491  -4.39409784  24.41493727  13.95025381   1.56630503
   1.06172558 -16.82526464  -2.37084569   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690268  50.93964676  20.79587853
 -10.6914205   -1.40680289  -3.43210536   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961087  -0.57679036  -4.050751    -1.46689824
  -0.80107704   8.98228099  -3.1108763   -1.49498359  24.63971658]
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1485110654648
gradient value of function right now is: [ 1.02182030e-09  3.41105896e-28  1.02051015e-09  2.37444490e-28
 -1.01843289e-09 -3.41117771e-28 -1.01712270e-09 -2.37440917e-28
  1.67167598e-13  1.75789325e-13  1.70066949e-13  1.74634711e-13
 -8.11664741e-28 -8.84718340e-28 -8.34574901e-28 -8.84095051e-28
  2.09137765e-12  2.31180505e-12  2.16338936e-12  2.27765956e-12
 -1.55547246e-29 -7.22231509e-28 -2.89352092e-29 -2.67954716e-27
 -1.07931589e-19 -2.51982183e-14  3.35113019e-19  6.90472693e-21
  3.21907139e-16  4.42037873e-14  3.88835664e-17  1.14727921e-17
  5.57144924e-19 -1.85841593e-14  2.66114681e-18  1.05642820e-19
  1.82842701e-17  7.44369645e-14 -7.12184329e-17 -1.42378417e-18
 -4.43327577e-16 -5.46615313e-17 -5.39636905e-15 -4.82928018e-17
  1.22538238e-12  1.37057500e-12 -1.78686468e-14  1.37080677e-12
  2.71472455e-17  2.17931035e-17 -1.66372865e-17  1.99363053e-17
  5.28652832e-18  1.06071048e-18  5.60501714e-17  9.52594449e-19
 -4.17845326e-13 -1.56660026e-11 -3.15386572e-16  7.23981562e-14
  4.78998579e-19  2.76356262e-18 -2.00323496e-17  1.70942890e-14
  8.59372277e-02]
supnorm grad right now is: 0.08593722772109744
Weights right now are: 
[-11.30298095   2.48912124  -1.63465367   5.35122029  10.77151042
  -2.62311668   1.79936045  -5.91727641   6.50630479 -32.95365126
   9.92092495  -1.08549058   2.54190933  44.99864605   6.79118805
  26.8097292    3.7338222   -4.97320981   2.34315732 -10.56491651
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995016   4.98513713 -11.8360974   21.57457363  -7.27955544
   5.11757491  -4.39409784  24.41493727  13.95025381   1.56630503
   1.06172558 -16.82526464  -2.3708457    2.51369865   4.22064502
  -4.83421263  38.329907     1.17690268  50.93964676  20.79587828
 -10.69142078  -1.40680288  -3.43210564   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961095  -0.57678723  -4.050751    -1.46689826
  -0.80107704   8.98228099  -3.1108763   -1.49498359  25.00612603]
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2040.9814629474529
gradient value of function right now is: [ 1.19322763e-09  3.40821930e-28  1.19191838e-09  2.39825295e-28
 -1.19048720e-09 -3.40834642e-28 -1.18917790e-09 -2.39825310e-28
  1.67031222e-13  1.75645930e-13  1.69928207e-13  1.74492303e-13
 -8.10987527e-28 -8.83981739e-28 -8.33878603e-28 -8.83362722e-28
  2.08988470e-12  2.31080178e-12  2.16186410e-12  2.27654735e-12
 -1.56680937e-29 -7.45814354e-28 -3.04168859e-29 -2.69486036e-27
 -9.13054655e-20 -2.51814621e-14  3.29708957e-19  7.90425537e-21
  3.19472811e-16  4.42314890e-14  3.81517046e-17  1.13189658e-17
  6.57816241e-19 -1.85810043e-14  2.61435864e-18  1.11498633e-19
  1.20164701e-17  7.48676200e-14 -7.33332975e-17 -1.87403033e-18
 -4.51952180e-16 -5.74813857e-17 -5.45302231e-15 -5.07899772e-17
  1.22472090e-12  1.36974010e-12 -1.69017186e-14  1.36996118e-12
  2.75954875e-17  2.14525182e-17 -7.72683842e-18  1.95949528e-17
  5.45265668e-18  1.10697747e-18  5.72790288e-17  9.93161023e-19
 -4.17729585e-13 -1.56501729e-11 -2.38468540e-16  7.03172179e-14
  5.23790840e-19  2.61210884e-18  9.01665472e-18  1.62607762e-14
 -2.16131302e+00]
supnorm grad right now is: 2.161313015785899
Weights right now are: 
[-11.30318821   2.48912124  -1.63486065   5.35122029  10.77171768
  -2.62311668   1.79956743  -5.91727641   6.50630475 -32.9536513
   9.92092491  -1.08549061   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382176  -4.97321029   2.34315687 -10.56491699
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995017   4.98513713 -11.8360974   21.57457363  -7.27955545
   5.11757491  -4.39409784  24.41493727  13.95025382   1.56630503
   1.06172558 -16.82526464  -2.37084572   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690268  50.93964676  20.79587803
 -10.69142106  -1.40680288  -3.43210592   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961103  -0.5767841   -4.050751    -1.46689827
  -0.80107704   8.98228099  -3.1108763   -1.4949836   24.80244465]
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1487736664756
gradient value of function right now is: [ 1.02896356e-09  3.40679061e-28  1.02765506e-09  2.37276792e-28
 -1.03179680e-09 -3.40694187e-28 -1.03048824e-09 -2.37274561e-28
  1.66963236e-13  1.75574420e-13  1.69859032e-13  1.74421291e-13
 -8.10649289e-28 -8.83613624e-28 -8.33530748e-28 -8.82997071e-28
  2.08913709e-12  2.30935314e-12  2.16107189e-12  2.27524045e-12
 -1.55381936e-29 -7.21980062e-28 -2.89268046e-29 -2.67745545e-27
 -1.07370247e-19 -2.51709474e-14  3.34606496e-19  6.92366986e-21
  3.21512290e-16  4.41557836e-14  3.88203143e-17  1.14572345e-17
  5.59578791e-19 -1.85639918e-14  2.65690137e-18  1.05688903e-19
  1.80439206e-17  7.43779598e-14 -7.12185599e-17 -1.43711137e-18
 -4.43261072e-16 -5.47085750e-17 -5.39323244e-15 -4.83346221e-17
  1.22405268e-12  1.36908836e-12 -1.78375167e-14  1.36931975e-12
  2.71333575e-17  2.17632043e-17 -1.63224929e-17  1.99085336e-17
  5.28724412e-18  1.06112734e-18  5.60384987e-17  9.52940523e-19
 -4.17395434e-13 -1.56490953e-11 -3.13868981e-16  7.22901436e-14
  4.78117731e-19  2.76110915e-18 -1.94432703e-17  1.70587091e-14
 -1.17152399e-02]
supnorm grad right now is: 0.011715239900522815
Weights right now are: 
[-11.30339658   2.48912124  -1.63506875   5.35122029  10.77192604
  -2.62311668   1.79977552  -5.91727641   6.50630472 -32.95365134
   9.92092487  -1.08549065   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382133  -4.97321077   2.34315642 -10.56491746
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995017   4.98513713 -11.8360974   21.57457363  -7.27955546
   5.11757491  -4.39409784  24.41493727  13.95025382   1.56630503
   1.06172558 -16.82526464  -2.37084573   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690268  50.93964676  20.79587778
 -10.69142133  -1.40680288  -3.43210619   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961111  -0.57678098  -4.050751    -1.46689829
  -0.80107704   8.98228099  -3.1108763   -1.4949836   24.78017813]
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1463010196887
gradient value of function right now is: [ 1.04420731e-09  3.40537286e-28  1.04289933e-09  2.37556258e-28
 -1.04332043e-09 -3.40551906e-28 -1.04201240e-09 -2.37559867e-28
  1.66894989e-13  1.75502655e-13  1.69789600e-13  1.74350020e-13
 -8.10310164e-28 -8.83244632e-28 -8.33182038e-28 -8.82630159e-28
  2.08838820e-12  2.30859974e-12  2.16029984e-12  2.27448205e-12
 -1.55478426e-29 -7.25144386e-28 -2.91122553e-29 -2.67903827e-27
 -1.05118890e-19 -2.51620153e-14  3.34658420e-19  7.07567184e-21
  3.21097614e-16  4.41481852e-14  3.86816122e-17  1.14328611e-17
  5.71520499e-19 -1.85586591e-14  2.65540312e-18  1.06518783e-19
  1.72799596e-17  7.44047944e-14 -7.15220154e-17 -1.49295773e-18
 -4.44093464e-16 -5.49991522e-17 -5.39784101e-15 -4.85931733e-17
  1.22363892e-12  1.36861212e-12 -1.76945020e-14  1.36884185e-12
  2.72526083e-17  2.17392964e-17 -1.48817300e-17  1.98829450e-17
  5.30643973e-18  1.06680378e-18  5.61679694e-17  9.57966173e-19
 -4.17270406e-13 -1.56428042e-11 -3.02695444e-16  7.19694336e-14
  4.77770457e-19  2.75656873e-18 -1.55093776e-17  1.69417872e-14
 -2.22405126e-01]
supnorm grad right now is: 0.22240512624771835
Weights right now are: 
[-11.30360376   2.48912124  -1.63527568   5.35122029  10.77213322
  -2.62311668   1.79998245  -5.91727641   6.50630469 -32.95365137
   9.92092484  -1.08549068   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382093  -4.97321121   2.34315601 -10.56491789
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995018   4.98513713 -11.8360974   21.57457363  -7.27955546
   5.11757491  -4.39409784  24.41493727  13.95025382   1.56630503
   1.06172558 -16.82526464  -2.37084575   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690268  50.93964676  20.79587755
 -10.6914216   -1.40680287  -3.43210646   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961119  -0.57677801  -4.050751    -1.4668983
  -0.80107704   8.98228099  -3.1108763   -1.4949836   24.66092374]
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1458270358298
gradient value of function right now is: [ 1.04674273e-09  3.40393634e-28  1.04543530e-09  2.37433947e-28
 -1.04395972e-09 -3.40404780e-28 -1.04265226e-09 -2.37430780e-28
  1.66824851e-13  1.75428940e-13  1.69718256e-13  1.74276806e-13
 -8.09962612e-28 -8.82866891e-28 -8.32824800e-28 -8.82254507e-28
  2.08761952e-12  2.30774914e-12  2.15950500e-12  2.27364448e-12
 -1.55403162e-29 -7.24426868e-28 -2.90878234e-29 -2.67780743e-27
 -1.04520369e-19 -2.51527950e-14  3.35118651e-19  7.12605885e-21
  3.20951850e-16  4.41305447e-14  3.86167219e-17  1.14247288e-17
  5.75210964e-19 -1.85517136e-14  2.65829387e-18  1.06837838e-19
  1.70815712e-17  7.43768301e-14 -7.16930288e-17 -1.50967109e-18
 -4.44270402e-16 -5.50892032e-17 -5.39845480e-15 -4.86732962e-17
  1.22318609e-12  1.36810812e-12 -1.77025669e-14  1.36833782e-12
  2.73498450e-17  2.17665953e-17 -1.41376317e-17  1.99072081e-17
  5.31228197e-18  1.06896758e-18  5.62030857e-17  9.59899215e-19
 -4.17116559e-13 -1.56372211e-11 -3.02624818e-16  7.19489156e-14
  4.77347004e-19  2.75494592e-18 -1.53218226e-17  1.69304181e-14
 -2.61402833e-01]
supnorm grad right now is: 0.2614028326286131
Weights right now are: 
[-11.30382071   2.48912124  -1.63549236   5.35122029  10.77235015
  -2.62311668   1.80019912  -5.91727641   6.50630465 -32.95365141
   9.92092481  -1.08549072   2.54190933  44.99864605   6.79118805
  26.8097292    3.7338205   -4.97321169   2.34315556 -10.56491836
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995018   4.98513713 -11.8360974   21.57457363  -7.27955547
   5.11757491  -4.39409784  24.41493727  13.95025383   1.56630503
   1.06172558 -16.82526464  -2.37084576   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690269  50.93964676  20.79587729
 -10.69142188  -1.40680287  -3.43210674   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961128  -0.57677478  -4.050751    -1.46689832
  -0.80107704   8.98228099  -3.1108763   -1.49498361  25.07375105]
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1483918987576
gradient value of function right now is: [ 1.03351057e-09  3.40249294e-28  1.03220372e-09  2.37107081e-28
 -1.03054450e-09 -3.40237329e-28 -1.02923769e-09 -2.37098952e-28
  1.66748459e-13  1.75348833e-13  1.69640611e-13  1.74197226e-13
 -8.09587915e-28 -8.82461384e-28 -8.32440177e-28 -8.81851466e-28
  2.08678270e-12  2.30677107e-12  2.15863945e-12  2.27269227e-12
 -1.55215029e-29 -7.21720249e-28 -2.89238133e-29 -2.67528393e-27
 -1.06206927e-19 -2.51431927e-14  3.34889173e-19  7.00251702e-21
  3.21103360e-16  4.41072013e-14  3.87047541e-17  1.14376566e-17
  5.65714911e-19 -1.85435555e-14  2.65762486e-18  1.06110308e-19
  1.76515204e-17  7.43061711e-14 -7.13997894e-17 -1.46558022e-18
 -4.43439650e-16 -5.48330209e-17 -5.39222099e-15 -4.84466396e-17
  1.22269819e-12  1.36757458e-12 -1.77980746e-14  1.36780538e-12
  2.72362094e-17  2.17695860e-17 -1.53014469e-17  1.99130101e-17
  5.29479863e-18  1.06379724e-18  5.60733132e-17  9.55325697e-19
 -4.16942862e-13 -1.56319884e-11 -3.10872395e-16  7.21428273e-14
  4.78126099e-19  2.75644478e-18 -1.83131403e-17  1.70067136e-14
 -8.58850139e-02]
supnorm grad right now is: 0.0858850138816108
Weights right now are: 
[-11.30402661   2.48912124  -1.635698     5.35122029  10.77255608
  -2.62311668   1.80040478  -5.91727641   6.50630462 -32.95365144
   9.92092477  -1.08549075   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382008  -4.97321215   2.34315513 -10.56491882
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995019   4.98513713 -11.8360974   21.57457363  -7.27955548
   5.11757491  -4.39409784  24.41493727  13.95025383   1.56630503
   1.06172558 -16.82526464  -2.37084578   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690269  50.93964676  20.79587705
 -10.69142215  -1.40680287  -3.43210701   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961136  -0.57677166  -4.050751    -1.46689833
  -0.80107704   8.98228099  -3.1108763   -1.49498361  25.12866426]
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2041.1486927591795
gradient value of function right now is: [ 1.02880539e-09  3.40106681e-28  1.02749909e-09  2.37027309e-28
 -1.03003159e-09 -3.40099691e-28 -1.02872531e-09 -2.37028466e-28
  1.66681336e-13  1.75278211e-13  1.69572308e-13  1.74127098e-13
 -8.09253566e-28 -8.82097348e-28 -8.32096266e-28 -8.81489825e-28
  2.08604620e-12  2.30594829e-12  2.15787689e-12  2.27188370e-12
 -1.55139158e-29 -7.21420596e-28 -2.88916921e-29 -2.67452079e-27
 -1.07060373e-19 -2.51340778e-14  3.34300193e-19  6.93251436e-21
  3.21109245e-16  4.40907218e-14  3.87604942e-17  1.14420536e-17
  5.60269997e-19 -1.85366169e-14  2.65408645e-18  1.05672842e-19
  1.79449248e-17  7.42738889e-14 -7.11715708e-17 -1.44137530e-18
 -4.42882579e-16 -5.46802128e-17 -5.38767356e-15 -4.83101126e-17
  1.22225008e-12  1.36707394e-12 -1.78002156e-14  1.36730485e-12
  2.71147676e-17  2.17361341e-17 -1.61428146e-17  1.98832374e-17
  5.28370151e-18  1.06058596e-18  5.59902785e-17  9.52448708e-19
 -4.16788476e-13 -1.56263665e-11 -3.12488697e-16  7.21555639e-14
  4.77186456e-19  2.75701974e-18 -1.90510533e-17  1.70209793e-14
 -3.12364382e-02]
supnorm grad right now is: 0.031236438213904236
Weights right now are: 
[-11.3042319    2.48912124  -1.63590303   5.35122029  10.77276136
  -2.62311668   1.8006098   -5.91727641   6.50630458 -32.95365148
   9.92092474  -1.08549079   2.54190933  44.99864605   6.79118805
  26.8097292    3.73381966  -4.97321261   2.34315469 -10.56491927
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995019   4.98513713 -11.8360974   21.57457363  -7.27955549
   5.11757491  -4.39409784  24.41493727  13.95025383   1.56630503
   1.06172558 -16.82526464  -2.37084579   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690269  50.93964676  20.79587681
 -10.69142242  -1.40680286  -3.43210728   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961144  -0.57676864  -4.050751    -1.46689835
  -0.80107704   8.98228099  -3.1108763   -1.49498361  24.82719963]
NN weights: [-11.30371241   2.48912124  -1.6353842    5.35122029  10.77224187
  -2.62311668   1.80009097  -5.91727641   6.50630467 -32.95365139
   9.92092482  -1.0854907    2.54190933  44.99864605   6.79118805
  26.8097292    3.73382071  -4.97321146   2.34315578 -10.56491814
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995018   4.98513713 -11.8360974   21.57457363  -7.27955547
   5.11757491  -4.39409784  24.41493727  13.95025383   1.56630503
   1.06172558 -16.82526464  -2.37084575   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690269  50.93964676  20.79587742
 -10.69142174  -1.40680287  -3.4321066    1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961124  -0.57677633  -4.050751    -1.46689831
  -0.80107704   8.98228099  -3.1108763   -1.4949836 ]
Minimum obj value:-2041.1486927591795
Optimal xi: 24.989150571128985
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1551.5577480584816
W_T_median: 1405.4383972902779
W_T_pctile_5: 624.6754498255045
W_T_CVAR_5_pct: 489.59103520157026
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
F value: -2041.1486927591795
-----------------------------------------------
{'NN': [-11.303712412691997, 2.489121242806325, -1.635384195705844, 5.351220289457238, 10.772241872281649, -2.623116679527984, 1.8000909651404386, -5.917276408661255, 6.506304668247923, -32.953651387614386, 9.920924823774342, -1.085490702183808, 2.541909327991153, 44.998646047179435, 6.791188054185753, 26.809729203540698, 3.733820707149756, -4.9732114554931846, 2.343155776471638, -10.564918135257964, 3.9233699547861485, 27.62408886394426, 40.17399079387827, 27.567387730472475, -20.597191814515426, 24.929950181169986, 4.985137131444473, -11.836097399014143, 21.574573632981238, -7.27955546838667, 5.1175749114257165, -4.394097842235373, 24.41493727433286, 13.950253825000427, 1.566305027446105, 1.0617255779103805, -16.825264641362565, -2.3708457541445416, 2.513698649376145, 4.220645024299798, -4.834212628317178, 38.32990699753771, 1.1769026852301798, 50.93964676430521, 20.795877416108265, -10.691421740936468, -1.4068028717735235, -3.4321066005560414, 1.754178846332769, 9.344385792601864, 3.0252301543878697, 18.32674643570084, -6.684497347751864, 20.889649226508162, 5.006959902846662, 44.557333767384975, 9.549611237160743, -0.5767763328574514, -4.050750997786881, -1.4668983089972747, -0.8010770383388176, 8.982280987015942, -3.1108763019825245, -1.4949836043679252]}
[-11.30371241   2.48912124  -1.6353842    5.35122029  10.77224187
  -2.62311668   1.80009097  -5.91727641   6.50630467 -32.95365139
   9.92092482  -1.0854907    2.54190933  44.99864605   6.79118805
  26.8097292    3.73382071  -4.97321146   2.34315578 -10.56491814
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995018   4.98513713 -11.8360974   21.57457363  -7.27955547
   5.11757491  -4.39409784  24.41493727  13.95025383   1.56630503
   1.06172558 -16.82526464  -2.37084575   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690269  50.93964676  20.79587742
 -10.69142174  -1.40680287  -3.4321066    1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961124  -0.57677633  -4.050751    -1.46689831
  -0.80107704   8.98228099  -3.1108763   -1.4949836 ]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.622176351433
gradient value of function right now is: [ 3.46884671e-09  5.08705410e-28  3.46688500e-09  2.66478247e-28
 -3.46422993e-09 -5.08714003e-28 -3.46226819e-09 -2.66472927e-28
  2.49834293e-13  2.62718984e-13  2.54178061e-13  2.60915694e-13
 -1.21353896e-27 -1.32092363e-27 -1.24784077e-27 -1.31520306e-27
  3.12973072e-12  3.46771470e-12  3.23783853e-12  3.41509753e-12
 -2.40204770e-29 -9.43275291e-28 -5.68077057e-29 -2.90239190e-27
 -1.36169037e-19 -3.77771610e-14  3.26097936e-19  4.95087193e-21
  3.32245605e-16  6.66843211e-14  4.19756922e-17  1.19607186e-17
  4.11495257e-19 -2.79529225e-14  2.61303686e-18  9.45962927e-20
  2.73640470e-17  1.17396608e-13 -6.46460072e-17 -7.20590149e-19
 -4.40623919e-16 -5.14841461e-17 -5.43482563e-15 -4.54890162e-17
  1.83894912e-12  2.05598739e-12 -1.72416349e-14  2.05624089e-12
  2.37759413e-17  2.09360868e-17 -4.82534141e-17  1.92184706e-17
  5.10496218e-18  9.82835443e-19  5.52090251e-17  8.84072533e-19
 -6.26940829e-13 -2.34283926e-11  1.05612178e-16  9.54965372e-14
  1.44015285e-18  2.59859934e-18  1.02730400e-16  2.43033679e-14
  2.92532718e+00]
supnorm grad right now is: 2.925327178133115
Weights right now are: 
[-11.30439262   2.48912124  -1.636064     5.35122029  10.77292209
  -2.62311668   1.80077078  -5.91727641   6.50630462 -32.95365144
   9.92092477  -1.08549076   2.54190933  44.99864605   6.79118805
  26.8097292    3.73382006  -4.97321217   2.34315511 -10.56491884
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995019   4.98513713 -11.8360974   21.57457363  -7.27955548
   5.11757491  -4.39409784  24.41493727  13.95025383   1.56630503
   1.06172558 -16.82526464  -2.37084578   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690269  50.93964676  20.79587705
 -10.69142215  -1.40680287  -3.43210701   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961137  -0.5767716   -4.050751    -1.46689833
  -0.80107704   8.98228099  -3.1108763   -1.49498361  25.20316268]
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.8443062935144
gradient value of function right now is: [ 3.78641889e-09  5.07218126e-28  3.78446249e-09  2.70164109e-28
 -3.78472602e-09 -5.07241737e-28 -3.78276953e-09 -2.70173637e-28
  2.49124241e-13  2.61972120e-13  2.53455616e-13  2.60173970e-13
 -1.21000742e-27 -1.31707959e-27 -1.24420902e-27 -1.31138068e-27
  3.12194937e-12  3.46042252e-12  3.22982682e-12  3.40765816e-12
 -2.42054831e-29 -9.86856700e-28 -5.97758802e-29 -2.92021517e-27
 -1.05500943e-19 -3.76847842e-14  3.12358732e-19  6.72225959e-21
  3.26932040e-16  6.66457965e-14  4.06960166e-17  1.16506005e-17
  5.89437328e-19 -2.79034732e-14  2.50305632e-18  1.04787386e-19
  1.63853795e-17  1.18094644e-13 -6.76927106e-17 -1.50478801e-18
 -4.53779042e-16 -5.61677341e-17 -5.51448441e-15 -4.96340049e-17
  1.83476616e-12  2.05110367e-12 -1.51189453e-14  2.05133405e-12
  2.41070364e-17  2.00381603e-17 -3.42720146e-17  1.83417564e-17
  5.37486626e-18  1.05953700e-18  5.71653385e-17  9.51145619e-19
 -6.25752233e-13 -2.33592258e-11  2.66289718e-16  9.10152460e-14
  1.51343639e-18  2.42799897e-18  1.61227896e-16  2.26124792e-14
 -1.51608964e+00]
supnorm grad right now is: 1.5160896352670559
Weights right now are: 
[-11.30512466   2.48912124  -1.63679567   5.35122029  10.77365418
  -2.62311668   1.80150249  -5.91727641   6.50630457 -32.95365149
   9.92092472  -1.08549081   2.54190933  44.99864605   6.79118805
  26.8097292    3.73381947  -4.97321283   2.34315449 -10.56491949
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.9299502    4.98513713 -11.8360974   21.57457363  -7.27955549
   5.11757491  -4.39409784  24.41493727  13.95025384   1.56630503
   1.06172558 -16.82526464  -2.3708458    2.51369865   4.22064502
  -4.83421263  38.329907     1.17690269  50.93964676  20.79587668
 -10.69142256  -1.40680286  -3.43210742   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961149  -0.57676701  -4.050751    -1.46689835
  -0.80107704   8.98228099  -3.1108763   -1.49498361  24.89886954]
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.802504616405
gradient value of function right now is: [ 3.80216849e-09  5.05739373e-28  3.80021777e-09  2.69845176e-28
 -3.79736001e-09 -5.05741382e-28 -3.79540929e-09 -2.69838965e-28
  2.48407274e-13  2.61218376e-13  2.52726241e-13  2.59425442e-13
 -1.20644845e-27 -1.31320950e-27 -1.24054999e-27 -1.30753287e-27
  3.11408541e-12  3.45180079e-12  3.22169569e-12  3.39915014e-12
 -2.41552707e-29 -9.87825208e-28 -5.98363388e-29 -2.91512343e-27
 -1.02663898e-19 -3.75897431e-14  3.10438092e-19  6.84981741e-21
  3.25719892e-16  6.64819841e-14  4.04872018e-17  1.15973978e-17
  6.03178937e-19 -2.78337282e-14  2.48708438e-18  1.05319593e-19
  1.52971178e-17  1.17865626e-13 -6.78295732e-17 -1.57264594e-18
 -4.54092540e-16 -5.64786040e-17 -5.51031737e-15 -4.99091756e-17
  1.83013036e-12  2.04591253e-12 -1.49553581e-14  2.04614102e-12
  2.40882078e-17  1.99255253e-17 -3.28891288e-17  1.82335612e-17
  5.38738200e-18  1.06377292e-18  5.72195484e-17  9.54770334e-19
 -6.24197274e-13 -2.32997638e-11  2.75116651e-16  9.05157373e-14
  1.53153728e-18  2.37806170e-18  1.64636049e-16  2.24402890e-14
 -1.85635032e+00]
supnorm grad right now is: 1.8563503173641345
Weights right now are: 
[-11.30585456   2.48912124  -1.63752518   5.35122029  10.77438407
  -2.62311668   1.802232    -5.91727641   6.50630452 -32.95365154
   9.92092467  -1.08549086   2.54190933  44.99864605   6.79118805
  26.8097292    3.73381884  -4.97321352   2.34315385 -10.56492017
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.9299502    4.98513713 -11.8360974   21.57457363  -7.27955551
   5.11757491  -4.39409784  24.41493727  13.95025384   1.56630503
   1.06172558 -16.82526464  -2.37084582   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690269  50.93964676  20.79587633
 -10.69142295  -1.40680286  -3.43210781   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.5496116   -0.57676263  -4.050751    -1.46689837
  -0.80107704   8.98228099  -3.1108763   -1.49498362  24.90020011]
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.9027432072135
gradient value of function right now is: [ 3.58920473e-09  5.04270636e-28  3.58725992e-09  2.66514260e-28
 -3.59479343e-09 -5.04262823e-28 -3.59284865e-09 -2.66504667e-28
  2.47698601e-13  2.60473239e-13  2.52005266e-13  2.58685479e-13
 -1.20292848e-27 -1.30938089e-27 -1.23693074e-27 -1.30372654e-27
  3.10630629e-12  3.44245424e-12  3.21362767e-12  3.39008436e-12
 -2.39499077e-29 -9.60077091e-28 -5.80339326e-29 -2.89011435e-27
 -1.22506923e-19 -3.74937015e-14  3.12957843e-19  5.53595068e-21
  3.27526520e-16  6.62466253e-14  4.13631549e-17  1.17459307e-17
  4.78291691e-19 -2.77516229e-14  2.51386770e-18  9.72478105e-20
  2.26770232e-17  1.17025196e-13 -6.45641902e-17 -1.01768276e-18
 -4.42342457e-16 -5.28751366e-17 -5.42475353e-15 -4.67180910e-17
  1.82524510e-12  2.04055641e-12 -1.59633029e-14  2.04079637e-12
  2.30989606e-17  2.00958888e-17 -4.53616494e-17  1.84247248e-17
  5.16331387e-18  1.00053246e-18  5.55253889e-17  8.99081862e-19
 -6.22420707e-13 -2.32462994e-11  1.84872031e-16  9.25767501e-14
  1.45227635e-18  2.47809466e-18  1.30145578e-16  2.33220241e-14
  8.29813828e-01]
supnorm grad right now is: 0.8298138277100412
Weights right now are: 
[-11.30658112   2.48912124  -1.63825136   5.35122029  10.77511058
  -2.62311668   1.80295813  -5.91727641   6.50630447 -32.9536516
   9.92092462  -1.08549091   2.54190933  44.99864605   6.79118805
  26.8097292    3.73381824  -4.97321418   2.34315323 -10.56492082
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995021   4.98513713 -11.8360974   21.57457363  -7.27955552
   5.11757491  -4.39409784  24.41493727  13.95025385   1.56630503
   1.06172558 -16.82526464  -2.37084585   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690269  50.93964676  20.79587598
 -10.69142335  -1.40680286  -3.43210821   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961172  -0.57675817  -4.050751    -1.46689839
  -0.80107704   8.98228099  -3.1108763   -1.49498362  25.05035069]
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.855282835697
gradient value of function right now is: [ 3.74696748e-09  5.02767404e-28  3.74502828e-09  2.67917711e-28
 -3.74660673e-09 -5.02790739e-28 -3.74466745e-09 -2.67926491e-28
  2.46985402e-13  2.59722990e-13  2.51279568e-13  2.57940488e-13
 -1.19937869e-27 -1.30551663e-27 -1.23327979e-27 -1.29988512e-27
  3.09847701e-12  3.43437754e-12  3.20554222e-12  3.38201572e-12
 -2.39879770e-29 -9.77700708e-28 -5.91735276e-29 -2.89657001e-27
 -1.05052915e-19 -3.73985525e-14  3.10815097e-19  6.67362775e-21
  3.24585233e-16  6.61274929e-14  4.03835761e-17  1.15673653e-17
  5.83575625e-19 -2.76888907e-14  2.48993579e-18  1.04055295e-19
  1.63818328e-17  1.17186326e-13 -6.72625777e-17 -1.48793783e-18
 -4.50269490e-16 -5.57104495e-17 -5.47305460e-15 -4.92291640e-17
  1.82074301e-12  2.03544306e-12 -1.50523573e-14  2.03567213e-12
  2.39962401e-17  1.99322440e-17 -3.37905814e-17  1.82452894e-17
  5.33370051e-18  1.05173700e-18  5.67312515e-17  9.44164243e-19
 -6.20988916e-13 -2.31824022e-11  2.61349925e-16  9.04036060e-14
  1.50073940e-18  2.40797769e-18  1.59047900e-16  2.24678442e-14
 -1.43099518e+00]
supnorm grad right now is: 1.4309951786334787
Weights right now are: 
[-11.30732699   2.48912124  -1.63899681   5.35122029  10.77585645
  -2.62311668   1.80370358  -5.91727641   6.50630442 -32.95365165
   9.92092457  -1.08549097   2.54190933  44.99864605   6.79118805
  26.8097292    3.73381756  -4.97321494   2.34315252 -10.56492157
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995022   4.98513713 -11.8360974   21.57457363  -7.27955553
   5.11757491  -4.39409784  24.41493727  13.95025385   1.56630503
   1.06172558 -16.82526464  -2.37084587   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690269  50.93964676  20.79587561
 -10.69142376  -1.40680286  -3.43210862   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961184  -0.57675362  -4.050751    -1.4668984
  -0.80107704   8.98228099  -3.1108763   -1.49498363  24.84035804]
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.3303866970955
gradient value of function right now is: [ 3.32425697e-09  5.00021644e-28  3.32233037e-09  2.62024450e-28
 -3.31949281e-09 -5.00022409e-28 -3.31756621e-09 -2.62021206e-28
  2.46174032e-13  2.58822307e-13  2.50438460e-13  2.57053277e-13
 -1.19355126e-27 -1.29811458e-27 -1.22693580e-27 -1.29270284e-27
  3.08291931e-12  3.41210536e-12  3.18825756e-12  3.36086932e-12
 -2.35848532e-29 -9.21346114e-28 -5.52420060e-29 -2.85847274e-27
 -1.43564446e-19 -3.68271215e-14  3.23501360e-19  4.27524096e-21
  3.29349813e-16  6.50779926e-14  4.19146155e-17  1.18939524e-17
  3.50880577e-19 -2.72660631e-14  2.59187692e-18  8.95737329e-20
  3.01792504e-17  1.14267729e-13 -6.23536225e-17 -4.67211462e-19
 -4.30750648e-16 -4.93204131e-17 -5.33874372e-15 -4.35736381e-17
  1.79462886e-12  2.00543566e-12 -1.74064720e-14  2.00569032e-12
  2.30160070e-17  2.07494350e-17 -5.36437850e-17  1.90633518e-17
  4.94772826e-18  9.42137068e-19  5.38483872e-17  8.47968780e-19
 -6.03278430e-13 -2.26822548e-11  7.09612701e-17  9.51311329e-14
  1.33269044e-18  2.61536810e-18  8.82866982e-17  2.43536463e-14
  4.20272746e+00]
supnorm grad right now is: 4.202727464190779
Weights right now are: 
[-11.30806325   2.48912124  -1.63973271   5.35122029  10.7765927
  -2.62311668   1.80443946  -5.91727641   6.50630437 -32.9536517
   9.92092452  -1.08549101   2.54190933  44.99864605   6.79118805
  26.8097292    3.73381698  -4.97321558   2.34315193 -10.56492219
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995023   4.98513713 -11.8360974   21.57457363  -7.27955555
   5.11757491  -4.39409784  24.41493727  13.95025386   1.56630503
   1.06172558 -16.82526464  -2.37084589   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690269  50.93964676  20.79587527
 -10.69142414  -1.40680285  -3.432109     1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961196  -0.57674928  -4.050751    -1.46689842
  -0.80107704   8.98228099  -3.1108763   -1.49498363  25.28662998]
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.5320310463553
gradient value of function right now is: [ 3.86386714e-09  4.99850633e-28  3.86193902e-09  2.69140999e-28
 -3.86739658e-09 -4.99816170e-28 -3.86546859e-09 -2.69128830e-28
  2.45566136e-13  2.58231083e-13  2.49835795e-13  2.56458888e-13
 -1.19233881e-27 -1.29786298e-27 -1.22604245e-27 -1.29227580e-27
  3.08288937e-12  3.41770042e-12  3.18943801e-12  3.36547875e-12
 -2.39654371e-29 -9.97215304e-28 -6.02127156e-29 -2.89945643e-27
 -9.47695053e-20 -3.72115520e-14  2.98630302e-19  7.02342727e-21
  3.21559927e-16  6.58494231e-14  4.00889906e-17  1.14256492e-17
  6.38080478e-19 -2.75575123e-14  2.39596747e-18  1.05370068e-19
  1.21225636e-17  1.17037233e-13 -6.70517281e-17 -1.74109241e-18
 -4.53760290e-16 -5.71666275e-17 -5.48698001e-15 -5.05116108e-17
  1.81172532e-12  2.02527472e-12 -1.40448834e-14  2.02549334e-12
  2.32575129e-17  1.92013693e-17 -3.27951812e-17  1.75574240e-17
  5.39499551e-18  1.06569795e-18  5.71804079e-17  9.55815699e-19
 -6.18040399e-13 -2.30615736e-11  3.24995823e-16  8.81605191e-14
  1.54721487e-18  2.23402768e-18  1.81776398e-16  2.16747421e-14
 -3.31030753e+00]
supnorm grad right now is: 3.3103075259968135
Weights right now are: 
[-11.30878213   2.48912124  -1.6404512    5.35122029  10.77731158
  -2.62311668   1.80515796  -5.91727641   6.50630432 -32.95365175
   9.92092447  -1.08549106   2.54190933  44.99864605   6.79118805
  26.8097292    3.73381637  -4.97321626   2.34315129 -10.56492286
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995023   4.98513713 -11.8360974   21.57457363  -7.27955556
   5.11757491  -4.39409784  24.41493727  13.95025386   1.56630503
   1.06172558 -16.82526464  -2.37084592   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690269  50.93964676  20.79587489
 -10.69142456  -1.40680285  -3.43210942   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961209  -0.57674438  -4.050751    -1.46689844
  -0.80107704   8.98228099  -3.1108763   -1.49498364  24.77391193]
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.7821739481456
gradient value of function right now is: [ 3.75713254e-09  4.98409272e-28  3.75521012e-09  2.66602206e-28
 -3.75930146e-09 -4.98433573e-28 -3.75737896e-09 -2.66594486e-28
  2.44890770e-13  2.57520358e-13  2.49148501e-13  2.55753186e-13
 -1.18897212e-27 -1.29419563e-27 -1.22257900e-27 -1.28863081e-27
  3.07546148e-12  3.40902890e-12  3.18173601e-12  3.35702344e-12
 -2.38156598e-29 -9.76969935e-28 -5.90634938e-29 -2.87942253e-27
 -1.01170062e-19 -3.71183503e-14  3.05590260e-19  6.76316240e-21
  3.21702712e-16  6.56380591e-14  4.00246266e-17  1.14538108e-17
  5.98242350e-19 -2.74816718e-14  2.44877818e-18  1.04013255e-19
  1.49288435e-17  1.16436186e-13 -6.68940869e-17 -1.56250964e-18
 -4.48844834e-16 -5.58660678e-17 -5.44520103e-15 -4.93666444e-17
  1.80705762e-12  2.02012776e-12 -1.46974576e-14  2.02035275e-12
  2.36790156e-17  1.96127146e-17 -3.29070748e-17  1.79463723e-17
  5.32450030e-18  1.05090841e-18  5.65503546e-17  9.43153497e-19
 -6.16370713e-13 -2.30075620e-11  2.75732189e-16  8.92454810e-14
  1.51380662e-18  2.34568275e-18  1.64029954e-16  2.21091002e-14
 -1.99142850e+00]
supnorm grad right now is: 1.9914285027421283
Weights right now are: 
[-11.30950353   2.48912124  -1.64117221   5.35122029  10.77803302
  -2.62311668   1.80587901  -5.91727641   6.50630427 -32.9536518
   9.92092442  -1.08549112   2.54190933  44.99864605   6.79118805
  26.8097292    3.73381575  -4.97321695   2.34315065 -10.56492355
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995024   4.98513713 -11.8360974   21.57457363  -7.27955557
   5.11757491  -4.39409784  24.41493727  13.95025387   1.56630503
   1.06172558 -16.82526464  -2.37084594   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690269  50.93964676  20.79587452
 -10.69142498  -1.40680285  -3.43210984   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961223  -0.57673938  -4.050751    -1.46689846
  -0.80107704   8.98228099  -3.1108763   -1.49498364  24.91241812]
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.2610821120275
gradient value of function right now is: [ 3.28163716e-09  4.95702279e-28  3.27972724e-09  2.59892081e-28
 -3.27895266e-09 -4.95694780e-28 -3.27704278e-09 -2.59907841e-28
  2.44090536e-13  2.56631882e-13  2.48318884e-13  2.54878034e-13
 -1.18321836e-27 -1.28688397e-27 -1.21631406e-27 -1.28153752e-27
  3.06006424e-12  3.38676014e-12  3.16462075e-12  3.33591447e-12
 -2.33748257e-29 -9.12891251e-28 -5.46705832e-29 -2.83600533e-27
 -1.43794310e-19 -3.65515347e-14  3.21263013e-19  4.15728648e-21
  3.27144054e-16  6.45775162e-14  4.16827351e-17  1.18193118e-17
  3.40537568e-19 -2.70592300e-14  2.57354510e-18  8.83564040e-20
  3.03591536e-17  1.13381986e-13 -6.16487931e-17 -4.30512955e-19
 -4.27055788e-16 -4.87502586e-17 -5.29679594e-15 -4.30685125e-17
  1.78110467e-12  1.99033987e-12 -1.73536744e-14  1.99059358e-12
  2.27601819e-17  2.05939497e-17 -5.42409282e-17  1.89230559e-17
  4.89869184e-18  9.30995395e-19  5.33679136e-17  8.38007547e-19
 -5.98747357e-13 -2.25132811e-11  6.30386337e-17  9.45793403e-14
  1.32001298e-18  2.61680489e-18  8.49674980e-17  2.42332345e-14
  4.40302206e+00]
supnorm grad right now is: 4.403022063946423
Weights right now are: 
[-11.31023137   2.48912124  -1.64189967   5.35122029  10.77876081
  -2.62311668   1.80660641  -5.91727641   6.50630422 -32.95365186
   9.92092437  -1.08549117   2.54190933  44.99864605   6.79118805
  26.8097292    3.73381513  -4.97321763   2.34315001 -10.56492421
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995025   4.98513713 -11.8360974   21.57457363  -7.27955559
   5.11757491  -4.39409784  24.41493727  13.95025387   1.56630503
   1.06172558 -16.82526464  -2.37084596   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690269  50.93964676  20.79587417
 -10.69142537  -1.40680284  -3.43211023   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961235  -0.576735    -4.050751    -1.46689848
  -0.80107704   8.98228099  -3.1108763   -1.49498365  25.27720877]
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.927641150096
gradient value of function right now is: [ 3.58620663e-09  4.95552292e-28  3.58429543e-09  2.63113550e-28
 -3.58798219e-09 -4.95561516e-28 -3.58607096e-09 -2.63101004e-28
  2.43512883e-13  2.56071554e-13  2.47746688e-13  2.54314442e-13
 -1.18213034e-27 -1.28675362e-27 -1.21554418e-27 -1.28123192e-27
  3.06030373e-12  3.39167199e-12  3.16603906e-12  3.34003757e-12
 -2.35714570e-29 -9.51570680e-28 -5.74232864e-29 -2.85060152e-27
 -1.15854499e-19 -3.69328390e-14  3.07697624e-19  5.78461301e-21
  3.22125150e-16  6.52547717e-14  4.05149028e-17  1.15314363e-17
  5.03349947e-19 -2.73347817e-14  2.46988331e-18  9.79127278e-20
  2.04739998e-17  1.15445980e-13 -6.44532546e-17 -1.14455997e-18
 -4.38704315e-16 -5.30234038e-17 -5.36454157e-15 -4.68490670e-17
  1.79783741e-12  2.00991070e-12 -1.54474674e-14  2.01014379e-12
  2.30054825e-17  1.97510462e-17 -4.14190565e-17  1.80993902e-17
  5.14369240e-18  1.00218823e-18  5.51252230e-17  9.00241911e-19
 -6.13151826e-13 -2.28976287e-11  2.06716780e-16  9.05771030e-14
  1.46249968e-18  2.46440850e-18  1.37569722e-16  2.27202308e-14
  2.34325093e-02]
supnorm grad right now is: 0.02343250927755516
Weights right now are: 
[-11.31094029   2.48912124  -1.64260821   5.35122029  10.77946974
  -2.62311668   1.80731497  -5.91727641   6.50630417 -32.95365191
   9.92092432  -1.08549122   2.54190933  44.99864605   6.79118805
  26.8097292    3.73381452  -4.9732183    2.34314938 -10.56492488
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995026   4.98513713 -11.8360974   21.57457363  -7.2795556
   5.11757491  -4.39409784  24.41493727  13.95025388   1.56630503
   1.06172558 -16.82526464  -2.37084599   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690269  50.93964676  20.79587382
 -10.69142577  -1.40680284  -3.43211063   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961247  -0.57673045  -4.050751    -1.46689849
  -0.80107704   8.98228099  -3.1108763   -1.49498365  25.0184581 ]
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.8144357865667
gradient value of function right now is: [ 3.71025627e-09  4.94139280e-28  3.70835037e-09  2.64331571e-28
 -3.70998208e-09 -4.94122904e-28 -3.70807624e-09 -2.64323304e-28
  2.42825718e-13  2.55349213e-13  2.47047656e-13  2.53597097e-13
 -1.17872272e-27 -1.28304894e-27 -1.21204095e-27 -1.27754902e-27
  3.05274323e-12  3.38379847e-12  3.15823439e-12  3.33218721e-12
 -2.36009766e-29 -9.67062855e-28 -5.84128294e-29 -2.85599027e-27
 -1.01585270e-19 -3.68425634e-14  3.04405551e-19  6.66798014e-21
  3.19531648e-16  6.51361467e-14  3.97530745e-17  1.13805084e-17
  5.87813131e-19 -2.72745178e-14  2.43882764e-18  1.03049292e-19
  1.52561024e-17  1.15535594e-13 -6.63729225e-17 -1.52340459e-18
 -4.45035442e-16 -5.52848041e-17 -5.40169934e-15 -4.88531821e-17
  1.79353878e-12  2.00503681e-12 -1.46767190e-14  2.00526100e-12
  2.35513906e-17  1.95277479e-17 -3.28568575e-17  1.78705377e-17
  5.27681852e-18  1.04116675e-18  5.60656822e-17  9.34494914e-19
 -6.11781365e-13 -2.28377079e-11  2.67482361e-16  8.87487767e-14
  1.48117216e-18  2.38295476e-18  1.60564902e-16  2.20079687e-14
 -1.79468558e+00]
supnorm grad right now is: 1.7946855778419153
Weights right now are: 
[-11.31165377   2.48912124  -1.6433213    5.35122029  10.78018322
  -2.62311668   1.80802806  -5.91727641   6.50630412 -32.95365196
   9.92092427  -1.08549127   2.54190933  44.99864605   6.79118805
  26.8097292    3.7338139   -4.97321899   2.34314874 -10.56492556
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995026   4.98513713 -11.8360974   21.57457363  -7.27955561
   5.11757491  -4.39409784  24.41493727  13.95025389   1.56630503
   1.06172558 -16.82526464  -2.37084601   2.51369865   4.22064502
  -4.83421263  38.329907     1.1769027   50.93964676  20.79587345
 -10.69142617  -1.40680284  -3.43211103   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.5496126   -0.5767258   -4.050751    -1.46689851
  -0.80107704   8.98228099  -3.1108763   -1.49498366  24.85999757]
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2815.5401921084476
gradient value of function right now is: [ 3.11225875e-09  4.91475454e-28  3.11036545e-09  2.55997624e-28
 -3.11189352e-09 -4.91443277e-28 -3.11000033e-09 -2.55991829e-28
  2.42047769e-13  2.54484430e-13  2.46240798e-13  2.52745430e-13
 -1.17309216e-27 -1.27586994e-27 -1.20590353e-27 -1.27058790e-27
  3.03758956e-12  3.36107394e-12  3.14129881e-12  3.31076165e-12
 -2.27217417e-29 -8.88197076e-28 -5.21636735e-29 -2.80215606e-27
 -1.56618895e-19 -3.62662951e-14  3.17945332e-19  3.08100818e-21
  3.26313030e-16  6.39878058e-14  4.21588405e-17  1.18518226e-17
  2.51997197e-19 -2.68284861e-14  2.54943811e-18  8.09925188e-20
  3.47413281e-17  1.12041931e-13 -5.84294791e-17 -4.83077777e-20
 -4.17392623e-16 -4.60849789e-17 -5.21252984e-15 -4.06979337e-17
  1.76665785e-12  1.97409318e-12 -1.78280366e-14  1.97435184e-12
  2.15387610e-17  2.04074786e-17 -6.45921160e-17  1.87774251e-17
  4.71228007e-18  8.76103168e-19  5.18961068e-17  7.89221290e-19
 -5.93947442e-13 -2.23353970e-11  1.86751738e-19  9.48157805e-14
  1.21769818e-18  2.91738455e-18  6.09569545e-17  2.44865153e-14
  6.41285113e+00]
supnorm grad right now is: 6.412851132941855
Weights right now are: 
[-11.31237058   2.48912124  -1.64403772   5.35122029  10.78090003
  -2.62311668   1.80874449  -5.91727641   6.50630407 -32.95365201
   9.92092422  -1.08549132   2.54190933  44.99864605   6.79118805
  26.8097292    3.73381329  -4.97321967   2.3431481  -10.56492623
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995027   4.98513713 -11.8360974   21.57457363  -7.27955563
   5.11757491  -4.39409784  24.41493727  13.95025389   1.56630503
   1.06172558 -16.82526464  -2.37084603   2.51369865   4.22064502
  -4.83421263  38.329907     1.1769027   50.93964676  20.79587309
 -10.69142657  -1.40680283  -3.43211143   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961272  -0.57672123  -4.050751    -1.46689853
  -0.80107704   8.98228099  -3.1108763   -1.49498366  25.46184695]
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.892849421729
gradient value of function right now is: [ 3.48540285e-09  4.91315891e-28  3.48350810e-09  2.60180035e-28
 -3.49086624e-09 -4.91315943e-28 -3.48897149e-09 -2.60182407e-28
  2.41473025e-13  2.53926620e-13  2.45671380e-13  2.52184427e-13
 -1.17200128e-27 -1.27573547e-27 -1.20512903e-27 -1.27027880e-27
  3.03784215e-12  3.36652041e-12  3.14279486e-12  3.31531773e-12
 -2.33248079e-29 -9.35248151e-28 -5.63706139e-29 -2.82288531e-27
 -1.21164969e-19 -3.66588791e-14  3.06975825e-19  5.35552683e-21
  3.20575995e-16  6.47387400e-14  4.04961538e-17  1.15013907e-17
  4.61716210e-19 -2.71263336e-14  2.46496807e-18  9.48404890e-20
  2.25885250e-17  1.14402814e-13 -6.30638866e-17 -9.67094234e-19
 -4.32286367e-16 -5.15542489e-17 -5.30341166e-15 -4.55502310e-17
  1.78435622e-12  1.99488555e-12 -1.57119474e-14  1.99512124e-12
  2.25989409e-17  1.97009444e-17 -4.47815562e-17  1.80644980e-17
  5.04193802e-18  9.76072938e-19  5.42452841e-17  8.77162813e-19
 -6.08536562e-13 -2.27302654e-11  1.73053582e-16  9.07044431e-14
  1.41852793e-18  2.42313970e-18  1.24603001e-16  2.28696637e-14
  1.00639056e+00]
supnorm grad right now is: 1.0063905592329272
Weights right now are: 
[-11.31308575   2.48912124  -1.64475251   5.35122029  10.78161526
  -2.62311668   1.80945933  -5.91727641   6.50630403 -32.95365206
   9.92092417  -1.08549137   2.54190933  44.99864605   6.79118805
  26.8097292    3.73381267  -4.97322035   2.34314747 -10.5649269
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995028   4.98513713 -11.8360974   21.57457363  -7.27955564
   5.11757491  -4.39409784  24.41493727  13.9502539    1.56630503
   1.06172558 -16.82526464  -2.37084605   2.51369865   4.22064502
  -4.83421263  38.329907     1.1769027   50.93964676  20.79587275
 -10.69142696  -1.40680283  -3.43211182   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961284  -0.57671681  -4.050751    -1.46689855
  -0.80107704   8.98228099  -3.1108763   -1.49498367  25.06123992]
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.7901614667117
gradient value of function right now is: [ 3.68933815e-09  4.89941602e-28  3.68744846e-09  2.62454791e-28
 -3.69448358e-09 -4.89918855e-28 -3.69259396e-09 -2.62452875e-28
  2.40805240e-13  2.53224621e-13  2.44992052e-13  2.51487284e-13
 -1.16868988e-27 -1.27213515e-27 -1.20172467e-27 -1.26669945e-27
  3.03048618e-12  3.35916527e-12  3.13520947e-12  3.30792368e-12
 -2.34074779e-29 -9.60688114e-28 -5.79814012e-29 -2.83539126e-27
 -9.99732051e-20 -3.65715890e-14  3.02007691e-19  6.67556590e-21
  3.17135073e-16  6.46505936e-14  3.94301537e-17  1.12914013e-17
  5.89337395e-19 -2.70722281e-14  2.41886060e-18  1.02626917e-19
  1.47780942e-17  1.14721551e-13 -6.60070767e-17 -1.53807598e-18
 -4.42411532e-16 -5.50506682e-17 -5.36692683e-15 -4.86465831e-17
  1.78027657e-12  1.99021745e-12 -1.45321254e-14  1.99043959e-12
  2.34069154e-17  1.93690319e-17 -3.21590518e-17  1.77240119e-17
  5.24888741e-18  1.03618439e-18  5.57389148e-17  9.29969018e-19
 -6.07286965e-13 -2.26698892e-11  2.68460411e-16  8.80082031e-14
  1.49217590e-18  2.31187283e-18  1.60609306e-16  2.18065516e-14
 -1.93739925e+00]
supnorm grad right now is: 1.937399247478609
Weights right now are: 
[-11.31378616   2.48912124  -1.64545254   5.35122029  10.78231566
  -2.62311668   1.81015935  -5.91727641   6.50630398 -32.95365211
   9.92092412  -1.08549142   2.54190933  44.99864605   6.79118805
  26.8097292    3.73381207  -4.97322102   2.34314684 -10.56492756
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995028   4.98513713 -11.8360974   21.57457363  -7.27955565
   5.11757491  -4.39409784  24.41493727  13.9502539    1.56630503
   1.06172558 -16.82526464  -2.37084608   2.51369865   4.22064502
  -4.83421263  38.329907     1.1769027   50.93964676  20.79587239
 -10.69142736  -1.40680283  -3.43211222   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961296  -0.57671223  -4.050751    -1.46689857
  -0.80107704   8.98228099  -3.1108763   -1.49498367  24.87348119]
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.749923668279
gradient value of function right now is: [ 3.70013775e-09  4.88550970e-28  3.69825339e-09  2.62301316e-28
 -3.69616789e-09 -4.88528862e-28 -3.69428362e-09 -2.62299020e-28
  2.40136736e-13  2.52521638e-13  2.44311917e-13  2.50789188e-13
 -1.16537013e-27 -1.26852338e-27 -1.19831103e-27 -1.26310833e-27
  3.02311612e-12  3.35109088e-12  3.12758766e-12  3.29995295e-12
 -2.33621857e-29 -9.63051488e-28 -5.80627881e-29 -2.83181639e-27
 -9.87953572e-20 -3.64819694e-14  2.98774951e-19  6.66494960e-21
  3.16192876e-16  6.45011287e-14  3.93825849e-17  1.12558154e-17
  5.93924985e-19 -2.70067636e-14  2.39510926e-18  1.02365847e-19
  1.42755154e-17  1.14511365e-13 -6.56723036e-17 -1.55947528e-18
 -4.41992808e-16 -5.51132704e-17 -5.35830858e-15 -4.87001521e-17
  1.77591625e-12  1.98532683e-12 -1.43151750e-14  1.98554671e-12
  2.31216033e-17  1.91776818e-17 -3.27414156e-17  1.75459020e-17
  5.24354466e-18  1.03458261e-18  5.56737363e-17  9.28376380e-19
 -6.05826367e-13 -2.26133826e-11  2.79315455e-16  8.74656799e-14
  1.48857559e-18  2.28746959e-18  1.64217090e-16  2.16365146e-14
 -2.22685155e+00]
supnorm grad right now is: 2.226851554267798
Weights right now are: 
[-11.31449651   2.48912124  -1.64616252   5.35122029  10.78302607
  -2.62311668   1.8108694   -5.91727641   6.50630393 -32.95365216
   9.92092407  -1.08549147   2.54190933  44.99864605   6.79118805
  26.8097292    3.73381147  -4.97322168   2.34314622 -10.56492821
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995029   4.98513713 -11.8360974   21.57457363  -7.27955566
   5.11757491  -4.39409784  24.41493727  13.95025391   1.56630503
   1.06172558 -16.82526464  -2.3708461    2.51369865   4.22064502
  -4.83421263  38.329907     1.1769027   50.93964676  20.79587204
 -10.69142775  -1.40680282  -3.43211261   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961308  -0.57670779  -4.050751    -1.46689858
  -0.80107704   8.98228099  -3.1108763   -1.49498368  24.86195652]
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.927323291247
gradient value of function right now is: [ 3.53365034e-09  4.87143148e-28  3.53177162e-09  2.59233747e-28
 -3.53463456e-09 -4.87146095e-28 -3.53275583e-09 -2.59237176e-28
  2.39467146e-13  2.51817283e-13  2.43630597e-13  2.50089773e-13
 -1.16204017e-27 -1.26489871e-27 -1.19488622e-27 -1.25950564e-27
  3.01572673e-12  3.34229156e-12  3.11992255e-12  3.29140470e-12
 -2.31765804e-29 -9.37689759e-28 -5.64787232e-29 -2.80856181e-27
 -1.13421961e-19 -3.63898168e-14  3.03744416e-19  5.77128072e-21
  3.17399502e-16  6.42795140e-14  3.98681137e-17  1.13585634e-17
  5.02093319e-19 -2.69290020e-14  2.43655940e-18  9.69661441e-20
  1.98452612e-17  1.13781728e-13 -6.37161827e-17 -1.15399449e-18
 -4.32995401e-16 -5.24238949e-17 -5.29107159e-15 -4.63205464e-17
  1.77125684e-12  1.98021277e-12 -1.51958798e-14  1.98044208e-12
  2.27627156e-17  1.94791591e-17 -4.00377211e-17  1.78490897e-17
  5.08123947e-18  9.90913972e-19  5.44110320e-17  8.90094910e-19
 -6.04136037e-13 -2.25614997e-11  2.06240112e-16  8.91645351e-14
  1.44138897e-18  2.42671501e-18  1.36634048e-16  2.23486229e-14
 -8.19834111e-02]
supnorm grad right now is: 0.08198341110473809
Weights right now are: 
[-11.31521741   2.48912124  -1.64688301   5.35122029  10.78374697
  -2.62311668   1.81158988  -5.91727641   6.50630388 -32.95365222
   9.92092402  -1.08549153   2.54190933  44.99864605   6.79118805
  26.8097292    3.73381081  -4.97322241   2.34314554 -10.56492892
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.9299503    4.98513713 -11.8360974   21.57457363  -7.27955568
   5.11757491  -4.39409784  24.41493727  13.95025391   1.56630503
   1.06172558 -16.82526464  -2.37084612   2.51369865   4.22064502
  -4.83421263  38.329907     1.1769027   50.93964676  20.79587167
 -10.69142817  -1.40680282  -3.43211303   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961321  -0.57670305  -4.050751    -1.4668986
  -0.80107704   8.98228099  -3.1108763   -1.49498368  24.98308558]
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.9269604394294
gradient value of function right now is: [ 3.52620808e-09  4.85787216e-28  3.52433460e-09  2.58653213e-28
 -3.52296371e-09 -4.85811659e-28 -3.52109013e-09 -2.58655510e-28
  2.38821393e-13  2.51138045e-13  2.42973549e-13  2.49415282e-13
 -1.15882997e-27 -1.26140450e-27 -1.19158469e-27 -1.25603173e-27
  3.00860100e-12  3.33439485e-12  3.11254903e-12  3.28362743e-12
 -2.31151002e-29 -9.35872319e-28 -5.63499555e-29 -2.80207844e-27
 -1.12989836e-19 -3.63023723e-14  3.02872705e-19  5.76852199e-21
  3.16671970e-16  6.41235526e-14  3.97803110e-17  1.13315743e-17
  5.02285623e-19 -2.68637945e-14  2.42962807e-18  9.68141624e-20
  1.97263814e-17  1.13514950e-13 -6.35752607e-17 -1.15658087e-18
 -4.32108485e-16 -5.23411526e-17 -5.28011613e-15 -4.62463891e-17
  1.76697944e-12  1.97543073e-12 -1.51416966e-14  1.97565933e-12
  2.26991351e-17  1.94213842e-17 -3.99498527e-17  1.77956202e-17
  5.07150710e-18  9.89142700e-19  5.43042224e-17  8.88477577e-19
 -6.02683477e-13 -2.25071769e-11  2.06860849e-16  8.89132610e-14
  1.43725150e-18  2.42026616e-18  1.36738462e-16  2.22804479e-14
 -1.17102290e-01]
supnorm grad right now is: 0.1171022895022451
Weights right now are: 
[-11.31591392   2.48912124  -1.64757916   5.35122029  10.78444342
  -2.62311668   1.81228596  -5.91727641   6.50630383 -32.95365227
   9.92092397  -1.08549158   2.54190933  44.99864605   6.79118805
  26.8097292    3.73381022  -4.97322307   2.34314493 -10.56492957
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995031   4.98513713 -11.8360974   21.57457363  -7.27955569
   5.11757491  -4.39409784  24.41493727  13.95025392   1.56630503
   1.06172558 -16.82526464  -2.37084615   2.51369865   4.22064502
  -4.83421263  38.329907     1.1769027   50.93964676  20.79587131
 -10.69142856  -1.40680282  -3.43211343   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961333  -0.5766985   -4.050751    -1.46689862
  -0.80107704   8.98228099  -3.1108763   -1.49498368  24.93001883]
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.796633314859
gradient value of function right now is: [ 3.37434134e-09  4.84418482e-28  3.37247332e-09  2.56954982e-28
 -3.37805444e-09 -4.84397306e-28 -3.37618648e-09 -2.56944196e-28
  2.38149145e-13  2.50431622e-13  2.42289745e-13  2.48713791e-13
 -1.15550150e-27 -1.25778849e-27 -1.18816337e-27 -1.25243765e-27
  3.00118018e-12  3.32565634e-12  3.10486033e-12  3.27512059e-12
 -2.29550186e-29 -9.19336360e-28 -5.49605116e-29 -2.79207309e-27
 -1.25869272e-19 -3.62122245e-14  3.06811471e-19  4.96861781e-21
  3.17930645e-16  6.39141790e-14  4.02450873e-17  1.14292933e-17
  4.24201715e-19 -2.67887739e-14  2.46159987e-18  9.19575158e-20
  2.42746452e-17  1.12846135e-13 -6.18980689e-17 -8.17768842e-19
 -4.25377847e-16 -5.01956906e-17 -5.23060687e-15 -4.43464778e-17
  1.76243631e-12  1.97043688e-12 -1.58876577e-14  1.97067347e-12
  2.23441547e-17  1.96704757e-17 -4.64450466e-17  1.80463658e-17
  4.94038762e-18  9.52247321e-19  5.33006844e-17  8.56071415e-19
 -6.01062246e-13 -2.24561770e-11  1.44663604e-16  9.03205394e-14
  1.39292947e-18  2.43434820e-18  1.13560188e-16  2.28591684e-14
  1.90800282e+00]
supnorm grad right now is: 1.9080028245418943
Weights right now are: 
[-11.31661932   2.48912124  -1.64828416   5.35122029  10.78514887
  -2.62311668   1.81299102  -5.91727641   6.50630378 -32.95365232
   9.92092392  -1.08549163   2.54190933  44.99864605   6.79118805
  26.8097292    3.73380958  -4.97322378   2.34314426 -10.56493027
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995031   4.98513713 -11.8360974   21.57457363  -7.2795557
   5.11757491  -4.39409784  24.41493727  13.95025392   1.56630503
   1.06172558 -16.82526464  -2.37084617   2.51369865   4.22064502
  -4.83421263  38.329907     1.1769027   50.93964676  20.79587095
 -10.69142897  -1.40680282  -3.43211383   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961346  -0.57669388  -4.050751    -1.46689864
  -0.80107704   8.98228099  -3.1108763   -1.49498369  25.10843339]
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.245000192749
gradient value of function right now is: [ 3.18646759e-09  4.81820340e-28  3.18461126e-09  2.53312774e-28
 -3.18149354e-09 -4.81801304e-28 -3.17963728e-09 -2.53315184e-28
  2.37396240e-13  2.49593926e-13  2.41508637e-13  2.47888825e-13
 -1.15003106e-27 -1.25081103e-27 -1.18219874e-27 -1.24567095e-27
  2.98643630e-12  3.30523837e-12  3.08847616e-12  3.25562367e-12
 -2.27163254e-29 -8.88400042e-28 -5.30365618e-29 -2.76503342e-27
 -1.40815090e-19 -3.56637090e-14  3.14424689e-19  4.05692065e-21
  3.19464023e-16  6.29761623e-14  4.06839082e-17  1.15425136e-17
  3.31908623e-19 -2.63945445e-14  2.51716843e-18  8.63164365e-20
  2.97007327e-17  1.10628955e-13 -6.02301641e-17 -4.17079756e-19
 -4.16999674e-16 -4.75788946e-17 -5.17156865e-15 -4.20338484e-17
  1.73758155e-12  1.94174366e-12 -1.70047342e-14  1.94199195e-12
  2.22704566e-17  2.01391450e-17 -5.28011083e-17  1.85055834e-17
  4.78325346e-18  9.08927208e-19  5.21038908e-17  8.18164633e-19
 -5.84182153e-13 -2.19679043e-11  5.61908588e-17  9.23931977e-14
  1.28923181e-18  2.55798452e-18  8.12603296e-17  2.36769477e-14
  4.45899570e+00]
supnorm grad right now is: 4.458995699610767
Weights right now are: 
[-11.31733113   2.48912124  -1.64899558   5.35122029  10.7858606
  -2.62311668   1.81370236  -5.91727641   6.50630373 -32.95365237
   9.92092387  -1.08549168   2.54190933  44.99864605   6.79118805
  26.8097292    3.73380896  -4.97322446   2.34314363 -10.56493095
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995032   4.98513713 -11.8360974   21.57457363  -7.27955572
   5.11757491  -4.39409784  24.41493727  13.95025393   1.56630503
   1.06172558 -16.82526464  -2.37084619   2.51369865   4.22064502
  -4.83421263  38.329907     1.1769027   50.93964676  20.79587059
 -10.69142937  -1.40680281  -3.43211423   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961358  -0.57668927  -4.050751    -1.46689865
  -0.80107704   8.98228099  -3.1108763   -1.49498369  25.30224675]
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.8541858632084
gradient value of function right now is: [ 3.38471475e-09  4.81701647e-28  3.38285717e-09  2.55461265e-28
 -3.38368360e-09 -4.81689789e-28 -3.38182606e-09 -2.55472346e-28
  2.36845071e-13  2.49060161e-13  2.40962961e-13  2.47351820e-13
 -1.14902615e-27 -1.25074243e-27 -1.18150469e-27 -1.24543252e-27
  2.98677579e-12  3.30983613e-12  3.08996234e-12  3.25951081e-12
 -2.28541777e-29 -9.16659224e-28 -5.50616669e-29 -2.77301234e-27
 -1.22636350e-19 -3.60368014e-14  3.02088654e-19  5.03479960e-21
  3.15655231e-16  6.36154828e-14  4.00011951e-17  1.13395899e-17
  4.32711399e-19 -2.66601553e-14  2.42552463e-18  9.17886996e-20
  2.33277330e-17  1.12406715e-13 -6.14309267e-17 -8.61827984e-19
 -4.23522895e-16 -5.01433738e-17 -5.20462241e-15 -4.42977297e-17
  1.75388824e-12  1.96085597e-12 -1.55435021e-14  1.96108887e-12
  2.19925298e-17  1.93705256e-17 -4.63768658e-17  1.77664760e-17
  4.92332961e-18  9.49239209e-19  5.30952380e-17  8.53160648e-19
 -5.98187651e-13 -2.23459495e-11  1.60619406e-16  8.93993347e-14
  1.38819818e-18  2.41079363e-18  1.18635678e-16  2.25880195e-14
  1.49378143e+00]
supnorm grad right now is: 1.4937814340833764
Weights right now are: 
[-11.31802485   2.48912124  -1.64968894   5.35122029  10.78655423
  -2.62311668   1.81439562  -5.91727641   6.50630368 -32.95365242
   9.92092382  -1.08549173   2.54190933  44.99864605   6.79118805
  26.8097292    3.73380837  -4.97322512   2.34314302 -10.56493159
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995033   4.98513713 -11.8360974   21.57457363  -7.27955573
   5.11757491  -4.39409784  24.41493727  13.95025393   1.56630503
   1.06172558 -16.82526464  -2.37084622   2.51369865   4.22064502
  -4.83421263  38.329907     1.1769027   50.93964676  20.79587024
 -10.69142977  -1.40680281  -3.43211463   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961371  -0.57668466  -4.050751    -1.46689867
  -0.80107704   8.98228099  -3.1108763   -1.4949837   25.12909509]
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.5369097968646
gradient value of function right now is: [ 3.71090966e-09  4.80337103e-28  3.70905694e-09  2.59796967e-28
 -3.71306083e-09 -4.80328060e-28 -3.71120814e-09 -2.59792560e-28
  2.36189435e-13  2.48370693e-13  2.40295912e-13  2.46667147e-13
 -1.14577133e-27 -1.24720131e-27 -1.17815779e-27 -1.24191173e-27
  2.97953290e-12  3.30309763e-12  3.08250581e-12  3.25263085e-12
 -2.30313968e-29 -9.61648543e-28 -5.77987658e-29 -2.79952994e-27
 -9.23275117e-20 -3.59511553e-14  2.89097502e-19  6.76964717e-21
  3.11057600e-16  6.35786797e-14  3.87908540e-17  1.10546057e-17
  6.15504367e-19 -2.66143348e-14  2.31832317e-18  1.01830939e-19
  1.18418161e-17  1.13100239e-13 -6.47704287e-17 -1.67456986e-18
 -4.38814654e-16 -5.52395957e-17 -5.30593147e-15 -4.88084521e-17
  1.75001021e-12  1.95633218e-12 -1.35911577e-14  1.95654362e-12
  2.24593936e-17  1.85656562e-17 -3.20104694e-17  1.69767742e-17
  5.21549559e-18  1.02960235e-18  5.52812773e-17  9.23449455e-19
 -5.97086439e-13 -2.22819538e-11  3.11952002e-16  8.51928282e-14
  1.49509614e-18  2.15869729e-18  1.74986130e-16  2.09437069e-14
 -3.27956648e+00]
supnorm grad right now is: 3.279566476263548
Weights right now are: 
[-11.31873371   2.48912124  -1.6503974    5.35122029  10.78726319
  -2.62311668   1.8151042   -5.91727641   6.50630363 -32.95365248
   9.92092377  -1.08549178   2.54190933  44.99864605   6.79118805
  26.8097292    3.73380774  -4.97322581   2.34314237 -10.56493228
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995034   4.98513713 -11.8360974   21.57457363  -7.27955574
   5.11757491  -4.39409784  24.41493727  13.95025394   1.56630503
   1.06172558 -16.82526464  -2.37084624   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690271  50.93964676  20.79586988
 -10.69143016  -1.40680281  -3.43211502   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961383  -0.57668013  -4.050751    -1.46689869
  -0.80107704   8.98228099  -3.1108763   -1.4949837   24.74541475]
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.775093520492
gradient value of function right now is: [ 3.61300158e-09  4.79004266e-28  3.61115413e-09  2.57460039e-28
 -3.61465215e-09 -4.78962510e-28 -3.61280485e-09 -2.57455788e-28
  2.35538363e-13  2.47686354e-13  2.39633617e-13  2.45987527e-13
 -1.14254639e-27 -1.24369566e-27 -1.17484258e-27 -1.23842622e-27
  2.97233463e-12  3.29473989e-12  3.07505058e-12  3.24447439e-12
 -2.28947666e-29 -9.43096119e-28 -5.67570443e-29 -2.78098707e-27
 -9.79979457e-20 -3.58636513e-14  2.95695896e-19  6.55429895e-21
  3.11153261e-16  6.33810173e-14  3.87096501e-17  1.10779779e-17
  5.80359994e-19 -2.65433169e-14  2.36833647e-18  1.00750792e-19
  1.43606999e-17  1.12541422e-13 -6.46933050e-17 -1.51685220e-18
 -4.34369207e-16 -5.40808028e-17 -5.26789353e-15 -4.77890428e-17
  1.74562592e-12  1.95150102e-12 -1.41919660e-14  1.95171831e-12
  2.28836635e-17  1.89516009e-17 -3.18369908e-17  1.73412387e-17
  5.15338180e-18  1.01704007e-18  5.47183668e-17  9.12743382e-19
 -5.95536109e-13 -2.22316641e-11  2.66631948e-16  8.61831063e-14
  1.46330034e-18  2.26662183e-18  1.58720416e-16  2.13389133e-14
 -2.01831529e+00]
supnorm grad right now is: 2.0183152931866974
Weights right now are: 
[-11.31942825   2.48912124  -1.65109156   5.35122029  10.78795775
  -2.62311668   1.81579837  -5.91727641   6.50630358 -32.95365253
   9.92092372  -1.08549184   2.54190933  44.99864605   6.79118805
  26.8097292    3.73380711  -4.9732265    2.34314172 -10.56493296
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995034   4.98513713 -11.8360974   21.57457363  -7.27955575
   5.11757491  -4.39409784  24.41493727  13.95025394   1.56630503
   1.06172558 -16.82526464  -2.37084626   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690271  50.93964676  20.79586953
 -10.69143056  -1.4068028   -3.43211542   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961394  -0.57667584  -4.050751    -1.46689871
  -0.80107704   8.98228099  -3.1108763   -1.49498371  24.83620401]
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.4834443337645
gradient value of function right now is: [ 3.20939036e-09  4.77662984e-28  3.20754858e-09  2.51687085e-28
 -3.20621254e-09 -4.77650471e-28 -3.20437081e-09 -2.51694443e-28
  2.34901586e-13  2.47016544e-13  2.38985671e-13  2.45322485e-13
 -1.13938012e-27 -1.24024955e-27 -1.17158599e-27 -1.23500142e-27
  2.96528517e-12  3.28537152e-12  3.06770875e-12  3.23554603e-12
 -2.25320274e-29 -8.87754230e-28 -5.29600061e-29 -2.74382247e-27
 -1.34817819e-19 -3.57734840e-14  3.09946731e-19  4.33449854e-21
  3.15867175e-16  6.30767732e-14  4.01075404e-17  1.13929102e-17
  3.58344244e-19 -2.64533055e-14  2.48215329e-18  8.75220174e-20
  2.77406666e-17  1.11119947e-13 -6.03240432e-17 -5.42194698e-19
 -4.15459159e-16 -4.79421831e-17 -5.13913507e-15 -4.23575091e-17
  1.74083420e-12  1.94638316e-12 -1.64963275e-14  1.94662521e-12
  2.21896344e-17  1.98441094e-17 -4.96968542e-17  1.82252497e-17
  4.78786228e-18  9.15103686e-19  5.19793530e-17  8.23431411e-19
 -5.93642947e-13 -2.21892466e-11  8.36759939e-17  9.08013251e-14
  1.31881663e-18  2.47656720e-18  9.06659577e-17  2.31787521e-14
  3.61646618e+00]
supnorm grad right now is: 3.616466184373565
Weights right now are: 
[-11.32012913   2.48912124  -1.65179206   5.35122029  10.78865859
  -2.62311668   1.81649883  -5.91727641   6.50630353 -32.95365258
   9.92092367  -1.08549189   2.54190933  44.99864605   6.79118805
  26.8097292    3.73380651  -4.97322717   2.3431411  -10.56493361
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995035   4.98513713 -11.8360974   21.57457363  -7.27955577
   5.11757491  -4.39409784  24.41493727  13.95025395   1.56630503
   1.06172558 -16.82526464  -2.37084628   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690271  50.93964676  20.79586917
 -10.69143095  -1.4068028   -3.43211581   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961407  -0.57667125  -4.050751    -1.46689872
  -0.80107704   8.98228099  -3.1108763   -1.49498371  25.21860319]
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.751162484081
gradient value of function right now is: [ 3.60702356e-09  4.76324580e-28  3.60518644e-09  2.56481047e-28
 -3.60759680e-09 -4.76316203e-28 -3.60575971e-09 -2.56469808e-28
  2.34258868e-13  2.46340593e-13  2.38331767e-13  2.44651164e-13
 -1.13619024e-27 -1.23677811e-27 -1.16830588e-27 -1.23154902e-27
  2.95817871e-12  3.27910257e-12  3.06040428e-12  3.22906463e-12
 -2.27790359e-29 -9.41107844e-28 -5.65765227e-29 -2.76932069e-27
 -9.69484379e-20 -3.56903491e-14  2.92780849e-19  6.52679631e-21
  3.09560383e-16  6.30762634e-14  3.85516086e-17  1.10200664e-17
  5.81576555e-19 -2.64146340e-14  2.34610062e-18  1.00267329e-19
  1.39890306e-17  1.12048975e-13 -6.42856413e-17 -1.52598111e-18
 -4.32755746e-16 -5.39517220e-17 -5.24568869e-15 -4.76739570e-17
  1.73716081e-12  1.94203097e-12 -1.40155458e-14  1.94224617e-12
  2.26377743e-17  1.87756548e-17 -3.20561324e-17  1.71782841e-17
  5.13394355e-18  1.01281268e-18  5.45041187e-17  9.08847671e-19
 -5.92669044e-13 -2.21236565e-11  2.72370497e-16  8.55690437e-14
  1.45728030e-18  2.23920416e-18  1.60420686e-16  2.11640805e-14
 -2.21914222e+00]
supnorm grad right now is: 2.219142222711323
Weights right now are: 
[-11.32083037   2.48912124  -1.65249293   5.35122029  10.78935981
  -2.62311668   1.81719968  -5.91727641   6.50630349 -32.95365263
   9.92092362  -1.08549193   2.54190933  44.99864605   6.79118805
  26.8097292    3.73380591  -4.97322783   2.34314048 -10.56493427
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995036   4.98513713 -11.8360974   21.57457363  -7.27955578
   5.11757491  -4.39409784  24.41493727  13.95025396   1.56630503
   1.06172558 -16.82526464  -2.37084631   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690271  50.93964676  20.79586882
 -10.69143135  -1.4068028   -3.43211621   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961419  -0.57666673  -4.050751    -1.46689874
  -0.80107704   8.98228099  -3.1108763   -1.49498372  24.83379347]
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.896790204932
gradient value of function right now is: [ 3.37606881e-09  4.74993386e-28  3.37423710e-09  2.52602590e-28
 -3.37461245e-09 -4.75017712e-28 -3.37278066e-09 -2.52600240e-28
  2.33628237e-13  2.45677121e-13  2.37690057e-13  2.43992350e-13
 -1.13305370e-27 -1.23336281e-27 -1.16507970e-27 -1.22815379e-27
  2.95119237e-12  3.27049869e-12  3.05314889e-12  3.22075482e-12
 -2.25561244e-29 -9.07856217e-28 -5.45083250e-29 -2.74074922e-27
 -1.17061807e-19 -3.56023946e-14  2.99220345e-19  5.28979567e-21
  3.11523900e-16  6.28412985e-14  3.92864258e-17  1.11721026e-17
  4.55597705e-19 -2.63368232e-14  2.40034232e-18  9.27803391e-20
  2.15993286e-17  1.11143994e-13 -6.15556526e-17 -9.69734991e-19
 -4.20840992e-16 -5.02989755e-17 -5.15924768e-15 -4.44425418e-17
  1.73264659e-12  1.93711039e-12 -1.52408512e-14  1.93733896e-12
  2.20901843e-17  1.91742099e-17 -4.25179443e-17  1.75798628e-17
  4.91434576e-18  9.52733126e-19  5.28202071e-17  8.56158813e-19
 -5.90988528e-13 -2.20762336e-11  1.70330798e-16  8.80020278e-14
  1.37847865e-18  2.35397441e-18  1.22084193e-16  2.21671604e-14
  9.08343937e-01]
supnorm grad right now is: 0.9083439370063539
Weights right now are: 
[-11.32152982   2.48912124  -1.65319202   5.35122029  10.79005925
  -2.62311668   1.81789877  -5.91727641   6.50630344 -32.95365268
   9.92092358  -1.08549198   2.54190933  44.99864605   6.79118805
  26.8097292    3.73380534  -4.97322847   2.34313988 -10.56493489
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995036   4.98513713 -11.8360974   21.57457363  -7.27955579
   5.11757491  -4.39409784  24.41493727  13.95025396   1.56630503
   1.06172558 -16.82526464  -2.37084633   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690271  50.93964676  20.79586847
 -10.69143174  -1.40680279  -3.4321166    1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961431  -0.57666226  -4.050751    -1.46689876
  -0.80107704   8.98228099  -3.1108763   -1.49498372  25.08483295]
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.48572535031
gradient value of function right now is: [ 3.67379194e-09  4.73681728e-28  3.67196489e-09  2.57376452e-28
 -3.67666487e-09 -4.73707223e-28 -3.67483772e-09 -2.57376790e-28
  2.32997152e-13  2.45013509e-13  2.37047998e-13  2.43333359e-13
 -1.12992250e-27 -1.22995669e-27 -1.16186000e-27 -1.22476794e-27
  2.94420769e-12  3.26403995e-12  3.04596153e-12  3.21414759e-12
 -2.27583308e-29 -9.56711339e-28 -5.74310463e-29 -2.77072466e-27
 -9.03120444e-20 -3.55201851e-14  2.85024139e-19  6.74656256e-21
  3.07308469e-16  6.28191366e-14  3.83161549e-17  1.09175705e-17
  6.15057128e-19 -2.62943975e-14  2.28566079e-18  1.00992325e-19
  1.12947649e-17  1.11833085e-13 -6.40768674e-17 -1.68322105e-18
 -4.34346052e-16 -5.47855465e-17 -5.24841136e-15 -4.84077312e-17
  1.72903927e-12  1.93292549e-12 -1.33638216e-14  1.93313385e-12
  2.21680050e-17  1.82992991e-17 -3.13703811e-17  1.67311126e-17
  5.16559188e-18  1.02021987e-18  5.47200225e-17  9.14960403e-19
 -5.89960262e-13 -2.20164651e-11  3.14180195e-16  8.41227513e-14
  1.47793244e-18  2.13408131e-18  1.75130132e-16  2.06627725e-14
 -3.46694794e+00]
supnorm grad right now is: 3.4669479402483616
Weights right now are: 
[-11.32222075   2.48912124  -1.65388256   5.35122029  10.79075021
  -2.62311668   1.81858934  -5.91727641   6.50630339 -32.95365273
   9.92092353  -1.08549203   2.54190933  44.99864605   6.79118805
  26.8097292    3.73380472  -4.97322916   2.34313924 -10.56493557
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995037   4.98513713 -11.8360974   21.57457363  -7.27955581
   5.11757491  -4.39409784  24.41493727  13.95025397   1.56630503
   1.06172558 -16.82526464  -2.37084635   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690271  50.93964676  20.79586812
 -10.69143213  -1.40680279  -3.43211699   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961444  -0.57665768  -4.050751    -1.46689878
  -0.80107704   8.98228099  -3.1108763   -1.49498372  24.73530151]
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.9187638753656
gradient value of function right now is: [ 3.45186578e-09  4.72370910e-28  3.45004409e-09  2.52580918e-28
 -3.44718304e-09 -4.72395564e-28 -3.44536126e-09 -2.52582332e-28
  2.32366096e-13  2.44349913e-13  2.36405964e-13  2.42674378e-13
 -1.12679129e-27 -1.22655016e-27 -1.15864026e-27 -1.22138124e-27
  2.93721267e-12  3.25535653e-12  3.03869566e-12  3.20577632e-12
 -2.24910996e-29 -9.14909863e-28 -5.49229748e-29 -2.73556581e-27
 -1.07450400e-19 -3.54327748e-14  2.97332948e-19  5.88000002e-21
  3.08919377e-16  6.25670663e-14  3.86258166e-17  1.10400781e-17
  5.09536882e-19 -2.62148333e-14  2.38103483e-18  9.61043843e-20
  1.81797584e-17  1.10882561e-13 -6.28110300e-17 -1.21682363e-18
 -4.23660514e-16 -5.16373629e-17 -5.16765396e-15 -4.56283346e-17
  1.72443749e-12  1.92789238e-12 -1.46649599e-14  1.92811388e-12
  2.25071593e-17  1.90255744e-17 -3.61185063e-17  1.74283113e-17
  4.98891922e-18  9.77042524e-19  5.32872072e-17  8.77521368e-19
 -5.88260643e-13 -2.19685189e-11  2.14091969e-16  8.64737132e-14
  1.41327092e-18  2.30547941e-18  1.38162313e-16  2.16151721e-14
 -4.52277371e-01]
supnorm grad right now is: 0.4522773714075068
Weights right now are: 
[-11.32291337   2.48912124  -1.65457481   5.35122029  10.79144277
  -2.62311668   1.81928153  -5.91727641   6.50630334 -32.95365278
   9.92092348  -1.08549208   2.54190933  44.99864605   6.79118805
  26.8097292    3.73380412  -4.97322983   2.34313862 -10.56493623
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995038   4.98513713 -11.8360974   21.57457363  -7.27955582
   5.11757491  -4.39409784  24.41493727  13.95025397   1.56630503
   1.06172558 -16.82526464  -2.37084637   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690271  50.93964676  20.79586776
 -10.69143253  -1.40680279  -3.43211739   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961456  -0.57665318  -4.050751    -1.46689879
  -0.80107704   8.98228099  -3.1108763   -1.49498373  24.96562275]
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.159613215788
gradient value of function right now is: [ 3.72933098e-09  4.71073575e-28  3.72751389e-09  2.57753138e-28
 -3.72820791e-09 -4.71098287e-28 -3.72639073e-09 -2.57754301e-28
  2.31741671e-13  2.43693292e-13  2.35770678e-13  2.42022324e-13
 -1.12369329e-27 -1.22317992e-27 -1.15545467e-27 -1.21803057e-27
  2.93029561e-12  3.24898168e-12  3.03157926e-12  3.19924632e-12
 -2.27023379e-29 -9.67494043e-28 -5.79411236e-29 -2.76856504e-27
 -8.41608944e-20 -3.53514754e-14  2.78538244e-19  6.97665406e-21
  3.05139996e-16  6.25560830e-14  3.80570487e-17  1.08190488e-17
  6.45917902e-19 -2.61737163e-14  2.23446812e-18  1.01837859e-19
  8.86604565e-18  1.11574790e-13 -6.39888055e-17 -1.82870166e-18
 -4.35613017e-16 -5.55311101e-17 -5.24847049e-15 -4.90671503e-17
  1.72091664e-12  1.92380617e-12 -1.27362549e-14  1.92400781e-12
  2.18051720e-17  1.78866071e-17 -3.02743479e-17  1.63427991e-17
  5.19475292e-18  1.02787552e-18  5.49038341e-17  9.21409381e-19
 -5.87261405e-13 -2.19095294e-11  3.53809303e-16  8.26757502e-14
  1.52700659e-18  1.89082773e-18  1.88954082e-16  2.01686712e-14
 -4.50324298e+00]
supnorm grad right now is: 4.503242982648012
Weights right now are: 
[-11.32360064   2.48912124  -1.65526173   5.35122029  10.79213009
  -2.62311668   1.81996848  -5.91727641   6.5063033  -32.95365283
   9.92092343  -1.08549213   2.54190933  44.99864605   6.79118805
  26.8097292    3.73380354  -4.97323047   2.34313802 -10.56493686
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995039   4.98513713 -11.8360974   21.57457363  -7.27955583
   5.11757491  -4.39409784  24.41493727  13.95025398   1.56630503
   1.06172558 -16.82526464  -2.3708464    2.51369865   4.22064502
  -4.83421263  38.329907     1.17690271  50.93964676  20.79586742
 -10.69143292  -1.40680279  -3.43211778   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961468  -0.57664871  -4.050751    -1.46689881
  -0.80107704   8.98228099  -3.1108763   -1.49498373  24.65167997]
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.7709571441833
gradient value of function right now is: [ 3.54639198e-09  4.69793862e-28  3.54458011e-09  2.53123372e-28
 -3.54576276e-09 -4.69798283e-28 -3.54395087e-09 -2.53122520e-28
  2.31119605e-13  2.43039339e-13  2.35137854e-13  2.41372894e-13
 -1.12061122e-27 -1.21982865e-27 -1.15228604e-27 -1.21469859e-27
  2.92339682e-12  3.24048507e-12  3.02441723e-12  3.19104834e-12
 -2.24581590e-29 -9.27100190e-28 -5.56688530e-29 -2.73425442e-27
 -9.64311818e-20 -3.52660838e-14  2.90797199e-19  6.45215334e-21
  3.06121516e-16  6.23069217e-14  3.80879429e-17  1.08986457e-17
  5.72151683e-19 -2.60966012e-14  2.32867768e-18  9.91731063e-20
  1.40583520e-17  1.10686826e-13 -6.36345851e-17 -1.49650754e-18
 -4.27517047e-16 -5.32424448e-17 -5.18381582e-15 -4.70479037e-17
  1.71637798e-12  1.91882183e-12 -1.39482885e-14  1.91903544e-12
  2.24904859e-17  1.86297038e-17 -3.13750017e-17  1.70462900e-17
  5.07218762e-18  1.00089604e-18  5.38502714e-17  8.98232560e-19
 -5.85600362e-13 -2.18616453e-11  2.62352045e-16  8.47185364e-14
  1.43874053e-18  2.22892973e-18  1.56240785e-16  2.09691611e-14
 -2.06084197e+00]
supnorm grad right now is: 2.060841971225908
Weights right now are: 
[-11.32428051   2.48912124  -1.65594123   5.35122029  10.79280999
  -2.62311668   1.82064802  -5.91727641   6.50630325 -32.95365288
   9.92092338  -1.08549218   2.54190933  44.99864605   6.79118805
  26.8097292    3.73380294  -4.97323112   2.34313741 -10.56493751
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995039   4.98513713 -11.8360974   21.57457363  -7.27955584
   5.11757491  -4.39409784  24.41493727  13.95025398   1.56630503
   1.06172558 -16.82526464  -2.37084642   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690271  50.93964676  20.79586708
 -10.69143329  -1.40680278  -3.43211815   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961479  -0.57664441  -4.050751    -1.46689883
  -0.80107704   8.98228099  -3.1108763   -1.49498374  24.82321469]
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.927411826907
gradient value of function right now is: [ 3.38653540e-09  4.68518706e-28  3.38472863e-09  2.50355334e-28
 -3.38262106e-09 -4.68540530e-28 -3.38081422e-09 -2.50352528e-28
  2.30511124e-13  2.42399310e-13  2.34518731e-13  2.40737337e-13
 -1.11758885e-27 -1.21653885e-27 -1.14917769e-27 -1.21142786e-27
  2.91664409e-12  3.23242546e-12  3.01741255e-12  3.18322077e-12
 -2.22873591e-29 -9.04255212e-28 -5.42008277e-29 -2.71330550e-27
 -1.11435662e-19 -3.51820172e-14  2.94214393e-19  5.49993163e-21
  3.07434490e-16  6.21059138e-14  3.86712242e-17  1.10076769e-17
  4.78657635e-19 -2.60255362e-14  2.35989705e-18  9.34285217e-20
  1.96985468e-17  1.10003670e-13 -6.14379509e-17 -1.08151439e-18
 -4.18509914e-16 -5.05372905e-17 -5.11739306e-15 -4.46515656e-17
  1.71212677e-12  1.91415432e-12 -1.47454380e-14  1.91437665e-12
  2.19325452e-17  1.88478493e-17 -3.97454596e-17  1.72721404e-17
  4.90572136e-18  9.55372724e-19  5.25746466e-17  8.58199571e-19
 -5.84059127e-13 -2.18143005e-11  1.94232376e-16  8.63001939e-14
  1.39313360e-18  2.34670817e-18  1.30239541e-16  2.16444048e-14
  7.42175700e-02]
supnorm grad right now is: 0.07421757001704354
Weights right now are: 
[-11.32495997   2.48912124  -1.65662033   5.35122029  10.79348949
  -2.62311668   1.82132715  -5.91727641   6.50630321 -32.95365292
   9.92092334  -1.08549223   2.54190933  44.99864605   6.79118805
  26.8097292    3.73380236  -4.97323177   2.3431368  -10.56493815
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.9299504    4.98513713 -11.8360974   21.57457363  -7.27955586
   5.11757491  -4.39409784  24.41493727  13.95025399   1.56630503
   1.06172558 -16.82526464  -2.37084644   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690271  50.93964676  20.79586673
 -10.69143368  -1.40680278  -3.43211854   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961491  -0.57664008  -4.050751    -1.46689884
  -0.80107704   8.98228099  -3.1108763   -1.49498374  25.00648737]
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.5378644393036
gradient value of function right now is: [ 3.15644509e-09  4.67240709e-28  3.15464354e-09  2.47108881e-28
 -3.15205579e-09 -4.67232633e-28 -3.15025428e-09 -2.47107351e-28
  2.29886685e-13  2.41743031e-13  2.33883525e-13  2.40085653e-13
 -1.11449750e-27 -1.21317938e-27 -1.14599980e-27 -1.20808859e-27
  2.90971478e-12  3.22385768e-12  3.01021984e-12  3.17495503e-12
 -2.20569132e-29 -8.72852925e-28 -5.19472354e-29 -2.69296983e-27
 -1.31162356e-19 -3.50966107e-14  3.03549599e-19  4.34338198e-21
  3.09881018e-16  6.18674013e-14  3.93250301e-17  1.11720364e-17
  3.61395978e-19 -2.59485427e-14  2.43146315e-18  8.65636113e-20
  2.67363990e-17  1.09078310e-13 -5.93628296e-17 -5.70722668e-19
 -4.08658177e-16 -4.73276613e-17 -5.04948546e-15 -4.18148362e-17
  1.70772046e-12  1.90937424e-12 -1.61003905e-14  1.90961088e-12
  2.17726890e-17  1.94257602e-17 -4.81912713e-17  1.78376072e-17
  4.71586848e-18  9.02750582e-19  5.11362800e-17  8.12197610e-19
 -5.82416854e-13 -2.17696709e-11  8.76379688e-17  8.88939088e-14
  1.29552433e-18  2.42739899e-18  9.12161772e-17  2.26613382e-14
  3.35803734e+00]
supnorm grad right now is: 3.3580373398984924
Weights right now are: 
[-11.32564254   2.48912124  -1.65730254   5.35122029  10.79417203
  -2.62311668   1.82200933  -5.91727641   6.50630316 -32.95365297
   9.92092329  -1.08549228   2.54190933  44.99864605   6.79118805
  26.8097292    3.73380178  -4.97323241   2.3431362  -10.56493877
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995041   4.98513713 -11.8360974   21.57457363  -7.27955587
   5.11757491  -4.39409784  24.41493727  13.95025399   1.56630503
   1.06172558 -16.82526464  -2.37084646   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690272  50.93964676  20.7958664
 -10.69143405  -1.40680278  -3.43211891   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961502  -0.57663587  -4.050751    -1.46689886
  -0.80107704   8.98228099  -3.1108763   -1.49498375  25.21189855]
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.9107398860624
gradient value of function right now is: [ 3.32670622e-09  4.65982283e-28  3.32490929e-09  2.48711275e-28
 -3.32721315e-09 -4.65974357e-28 -3.32541625e-09 -2.48711341e-28
  2.29281015e-13  2.41106065e-13  2.33267322e-13  2.39453052e-13
 -1.11149336e-27 -1.20991028e-27 -1.14291072e-27 -1.20483777e-27
  2.90299329e-12  3.21717383e-12  3.00328916e-12  3.16822367e-12
 -2.21432626e-29 -8.95597234e-28 -5.36196302e-29 -2.69744116e-27
 -1.14354252e-19 -3.50164432e-14  2.93660031e-19  5.25307879e-21
  3.06452205e-16  6.17980695e-14  3.86508684e-17  1.09866812e-17
  4.54545833e-19 -2.59000473e-14  2.35540147e-18  9.15916920e-20
  2.08578204e-17  1.09391970e-13 -6.05920732e-17 -9.80701227e-19
 -4.14753876e-16 -4.96738228e-17 -5.08140914e-15 -4.38898024e-17
  1.70399262e-12  1.90508520e-12 -1.48499583e-14  1.90530847e-12
  2.16796975e-17  1.88013795e-17 -4.17473031e-17  1.72360665e-17
  4.84605280e-18  9.39814241e-19  5.20576448e-17  8.44465401e-19
 -5.81283208e-13 -2.17130188e-11  1.78189439e-16  8.62760709e-14
  1.35807592e-18  2.28931659e-18  1.23736803e-16  2.17015997e-14
  6.76871474e-01]
supnorm grad right now is: 0.6768714744477302
Weights right now are: 
[-11.32631652   2.48912124  -1.65797615   5.35122029  10.79484598
  -2.62311668   1.82268293  -5.91727641   6.50630311 -32.95365302
   9.92092324  -1.08549232   2.54190933  44.99864605   6.79118805
  26.8097292    3.7338012   -4.97323306   2.3431356  -10.56493941
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995041   4.98513713 -11.8360974   21.57457363  -7.27955588
   5.11757491  -4.39409784  24.41493727  13.950254     1.56630503
   1.06172558 -16.82526464  -2.37084648   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690272  50.93964676  20.79586605
 -10.69143444  -1.40680277  -3.4321193    1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961514  -0.57663137  -4.050751    -1.46689888
  -0.80107704   8.98228099  -3.1108763   -1.49498375  25.02817508]
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.1341108138117
gradient value of function right now is: [ 3.03892244e-09  4.63575549e-28  3.03713654e-09  2.44471092e-28
 -3.03629446e-09 -4.63587166e-28 -3.03450852e-09 -2.44479907e-28
  2.28605272e-13  2.40351225e-13  2.32565259e-13  2.38710178e-13
 -1.10647384e-27 -1.20346191e-27 -1.13742230e-27 -1.19859116e-27
  2.88927641e-12  3.19758859e-12  2.98798928e-12  3.14961256e-12
 -2.18371836e-29 -8.54656052e-28 -5.07342251e-29 -2.67056944e-27
 -1.40030405e-19 -3.44910290e-14  3.04272341e-19  3.67509294e-21
  3.09724203e-16  6.08595042e-14  3.95870904e-17  1.12059306e-17
  3.02168388e-19 -2.55160782e-14  2.43623958e-18  8.21023263e-20
  2.98581633e-17  1.06944956e-13 -5.76714171e-17 -3.15584145e-19
 -4.02333110e-16 -4.55644376e-17 -4.99716738e-15 -4.02494712e-17
  1.68009137e-12  1.87755055e-12 -1.65964433e-14  1.87779244e-12
  2.12978983e-17  1.94848336e-17 -5.36990731e-17  1.79103924e-17
  4.59776822e-18  8.69311868e-19  5.02072273e-17  7.82620998e-19
 -5.64918482e-13 -2.12473763e-11  4.10643076e-17  8.96690735e-14
  1.21391325e-18  2.48323346e-18  7.36994891e-17  2.30213892e-14
  4.82722210e+00]
supnorm grad right now is: 4.827222098201343
Weights right now are: 
[-11.32697983   2.48912124  -1.65863913   5.35122029  10.79550928
  -2.62311668   1.82334589  -5.91727641   6.50630307 -32.95365307
   9.9209232   -1.08549237   2.54190933  44.99864605   6.79118805
  26.8097292    3.73380066  -4.97323366   2.34313504 -10.56494
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995042   4.98513713 -11.8360974   21.57457363  -7.27955589
   5.11757491  -4.39409784  24.41493727  13.950254     1.56630503
   1.06172558 -16.82526464  -2.37084651   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690272  50.93964676  20.79586573
 -10.69143481  -1.40680277  -3.43211967   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961525  -0.57662715  -4.050751    -1.4668989
  -0.80107704   8.98228099  -3.1108763   -1.49498376  25.35331255]
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.926000620718
gradient value of function right now is: [ 3.36975133e-09  4.63531505e-28  3.36796379e-09  2.48339587e-28
 -3.37144344e-09 -4.63531356e-28 -3.36965591e-09 -2.48340711e-28
  2.28103320e-13  2.39867579e-13  2.32069118e-13  2.38223187e-13
 -1.10565095e-27 -1.20355372e-27 -1.13690296e-27 -1.19851806e-27
  2.88991325e-12  3.20289524e-12  2.98976265e-12  3.15412186e-12
 -2.20668224e-29 -8.98839036e-28 -5.38101427e-29 -2.69018598e-27
 -1.08180671e-19 -3.48573814e-14  2.91559703e-19  5.60150489e-21
  3.04312112e-16  6.15306035e-14  3.81932884e-17  1.08865071e-17
  4.87106409e-19 -2.57842158e-14  2.33633980e-18  9.34205208e-20
  1.86797050e-17  1.09063874e-13 -6.11879245e-17 -1.13132287e-18
 -4.15850482e-16 -5.04146529e-17 -5.07821756e-15 -4.45449009e-17
  1.69626154e-12  1.89641804e-12 -1.44797546e-14  1.89663683e-12
  2.18449515e-17  1.86502959e-17 -3.79922922e-17  1.70880371e-17
  4.88350647e-18  9.52683331e-19  5.22563780e-17  8.55705874e-19
 -5.78694826e-13 -2.16128596e-11  2.03032241e-16  8.52196406e-14
  1.38150330e-18  2.32344496e-18  1.32885412e-16  2.13338558e-14
 -1.91220119e-01]
supnorm grad right now is: 0.19122011916777476
Weights right now are: 
[-11.32763424   2.48912124  -1.6592932    5.35122029  10.79616369
  -2.62311668   1.82399997  -5.91727641   6.50630303 -32.95365311
   9.92092316  -1.08549241   2.54190933  44.99864605   6.79118805
  26.8097292    3.73380012  -4.97323426   2.34313448 -10.56494059
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995043   4.98513713 -11.8360974   21.57457363  -7.2795559
   5.11757491  -4.39409784  24.41493727  13.95025401   1.56630503
   1.06172558 -16.82526464  -2.37084653   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690272  50.93964676  20.7958654
 -10.69143517  -1.40680277  -3.43212003   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961537  -0.57662295  -4.050751    -1.46689892
  -0.80107704   8.98228099  -3.1108763   -1.49498376  24.9457315 ]
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.755668699923
gradient value of function right now is: [ 3.49823583e-09  4.62295980e-28  3.49645290e-09  2.49754787e-28
 -3.49759987e-09 -4.62318154e-28 -3.49581686e-09 -2.49754032e-28
  2.27515129e-13  2.39248854e-13  2.31470627e-13  2.37608787e-13
 -1.10272955e-27 -1.20037349e-27 -1.13389836e-27 -1.19535603e-27
  2.88337678e-12  3.19616262e-12  2.98301342e-12  3.14739412e-12
 -2.21088149e-29 -9.15708151e-28 -5.48585511e-29 -2.69728597e-27
 -9.50144694e-20 -3.47781837e-14  2.85962754e-19  6.35775939e-21
  3.01936362e-16  6.14350009e-14  3.75996210e-17  1.07498793e-17
  5.66251323e-19 -2.57324908e-14  2.29043510e-18  9.78004577e-20
  1.37378347e-17  1.09202100e-13 -6.26740592e-17 -1.48222636e-18
 -4.22000055e-16 -5.25817407e-17 -5.11501891e-15 -4.64632281e-17
  1.69251237e-12  1.89215027e-12 -1.36766895e-14  1.89236014e-12
  2.20778957e-17  1.83180952e-17 -3.13287659e-17  1.67598903e-17
  5.00577163e-18  9.87302951e-19  5.31410022e-17  8.85968270e-19
 -5.77503749e-13 -2.15592436e-11  2.63863750e-16  8.33986693e-14
  1.42017733e-18  2.18218128e-18  1.55839006e-16  2.06266641e-14
 -2.18440567e+00]
supnorm grad right now is: 2.1844056723150627
Weights right now are: 
[-11.32830125   2.48912124  -1.65995986   5.35122029  10.79683072
  -2.62311668   1.82466664  -5.91727641   6.50630298 -32.95365316
   9.92092311  -1.08549246   2.54190933  44.99864605   6.79118805
  26.8097292    3.73379954  -4.9732349    2.34313388 -10.56494122
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995043   4.98513713 -11.8360974   21.57457363  -7.27955592
   5.11757491  -4.39409784  24.41493727  13.95025401   1.56630503
   1.06172558 -16.82526464  -2.37084655   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690272  50.93964676  20.79586506
 -10.69143555  -1.40680276  -3.43212041   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961548  -0.5766187   -4.050751    -1.46689893
  -0.80107704   8.98228099  -3.1108763   -1.49498376  24.79139713]
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.8852276850703
gradient value of function right now is: [ 3.26050182e-09  4.61057014e-28  3.25872396e-09  2.45927858e-28
 -3.26035813e-09 -4.61045834e-28 -3.25858030e-09 -2.45931447e-28
  2.26908513e-13  2.38611266e-13  2.30853561e-13  2.36975597e-13
 -1.09972785e-27 -1.19711073e-27 -1.13081268e-27 -1.19211231e-27
  2.87663106e-12  3.18782936e-12  2.97601245e-12  3.13935083e-12
 -2.18879711e-29 -8.82674489e-28 -5.27735218e-29 -2.66928460e-27
 -1.16660614e-19 -3.46952119e-14  2.91472794e-19  4.99064548e-21
  3.04075676e-16  6.12088954e-14  3.84558449e-17  1.09157027e-17
  4.30490755e-19 -2.56581941e-14  2.33867392e-18  8.94707094e-20
  2.18283323e-17  1.08301421e-13 -5.95435518e-17 -8.83986047e-19
 -4.09396376e-16 -4.86828780e-17 -5.02400851e-15 -4.30110918e-17
  1.68823646e-12  1.88750008e-12 -1.49140607e-14  1.88772365e-12
  2.13189455e-17  1.86647644e-17 -4.33836309e-17  1.71160240e-17
  4.76885912e-18  9.21509719e-19  5.13398563e-17  8.28159878e-19
 -5.75913965e-13 -2.15153384e-11  1.59272435e-16  8.59060895e-14
  1.33730408e-18  2.32031713e-18  1.16319849e-16  2.16676293e-14
  1.15943449e+00]
supnorm grad right now is: 1.1594344885631993
Weights right now are: 
[-11.32897248   2.48912124  -1.66063073   5.35122029  10.79750196
  -2.62311668   1.82533751  -5.91727641   6.50630294 -32.95365321
   9.92092306  -1.08549251   2.54190933  44.99864605   6.79118805
  26.8097292    3.73379895  -4.97323555   2.34313328 -10.56494187
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995044   4.98513713 -11.8360974   21.57457363  -7.27955593
   5.11757491  -4.39409784  24.41493727  13.95025402   1.56630503
   1.06172558 -16.82526464  -2.37084657   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690272  50.93964676  20.79586472
 -10.69143593  -1.40680276  -3.4321208    1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961559  -0.57661438  -4.050751    -1.46689895
  -0.80107704   8.98228099  -3.1108763   -1.49498377  25.12798696]
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.8393721179636
gradient value of function right now is: [ 3.43666357e-09  4.59812809e-28  3.43489029e-09  2.47789983e-28
 -3.44142088e-09 -4.59797907e-28 -3.43964766e-09 -2.47785159e-28
  2.26308143e-13  2.37979978e-13  2.30242761e-13  2.36348694e-13
 -1.09675179e-27 -1.19387354e-27 -1.12775261e-27 -1.18889391e-27
  2.86995600e-12  3.18112520e-12  2.96912672e-12  3.13261786e-12
 -2.19542774e-29 -9.03848785e-28 -5.41310957e-29 -2.67921303e-27
 -9.77763429e-20 -3.46154841e-14  2.87856938e-19  6.19073833e-21
  3.01090890e-16  6.11251945e-14  3.74882774e-17  1.07306154e-17
  5.44818355e-19 -2.56082369e-14  2.30375453e-18  9.66237437e-20
  1.49728430e-17  1.08571432e-13 -6.22522025e-17 -1.39070443e-18
 -4.18319637e-16 -5.17791540e-17 -5.08042542e-15 -4.57549799e-17
  1.68450513e-12  1.88323614e-12 -1.38908774e-14  1.88344780e-12
  2.21285785e-17  1.84120490e-17 -3.18380595e-17  1.68531914e-17
  4.95396851e-18  9.75701287e-19  5.26710374e-17  8.75847393e-19
 -5.74762886e-13 -2.14604691e-11  2.43280272e-16  8.35436370e-14
  1.39248950e-18  2.23135582e-18  1.47996368e-16  2.07351387e-14
 -1.57031026e+00]
supnorm grad right now is: 1.5703102620069658
Weights right now are: 
[-11.32964751   2.48912124  -1.6613054    5.35122029  10.79817698
  -2.62311668   1.82601218  -5.91727641   6.50630289 -32.95365325
   9.92092302  -1.08549256   2.54190933  44.99864605   6.79118805
  26.8097292    3.73379837  -4.97323619   2.34313268 -10.5649425
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995045   4.98513713 -11.8360974   21.57457363  -7.27955594
   5.11757491  -4.39409784  24.41493727  13.95025402   1.56630503
   1.06172558 -16.82526464  -2.37084659   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690272  50.93964676  20.79586437
 -10.69143632  -1.40680276  -3.43212119   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961571  -0.57660992  -4.050751    -1.46689897
  -0.80107704   8.98228099  -3.1108763   -1.49498377  24.87242851]
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.8814538915963
gradient value of function right now is: [ 3.24039894e-09  4.58585029e-28  3.23863063e-09  2.44771538e-28
 -3.23636626e-09 -4.58545237e-28 -3.23459809e-09 -2.44771006e-28
  2.25709382e-13  2.37350578e-13  2.29633663e-13  2.35723637e-13
 -1.09378798e-27 -1.19065122e-27 -1.12470571e-27 -1.18568984e-27
  2.86329369e-12  3.17304999e-12  2.96221688e-12  3.12479578e-12
 -2.17708379e-29 -8.78488468e-28 -5.24829051e-29 -2.65675213e-27
 -1.16426124e-19 -3.45337364e-14  2.90064848e-19  4.94843430e-21
  3.02731743e-16  6.09190852e-14  3.83005458e-17  1.08686580e-17
  4.27069432e-19 -2.55375318e-14  2.32740668e-18  8.89489026e-20
  2.18011368e-17  1.07795717e-13 -5.92175751e-17 -8.73777183e-19
 -4.07457019e-16 -4.84290779e-17 -5.00071003e-15 -4.27860243e-17
  1.68033288e-12  1.87867131e-12 -1.48452790e-14  1.87889387e-12
  2.11907274e-17  1.85725499e-17 -4.34205731e-17  1.70317722e-17
  4.74489804e-18  9.16541170e-19  5.10915549e-17  8.23694524e-19
 -5.73236114e-13 -2.14155710e-11  1.58171229e-16  8.55103869e-14
  1.33055207e-18  2.30853525e-18  1.15629728e-16  2.15694493e-14
  1.20267335e+00]
supnorm grad right now is: 1.2026733507587268
Weights right now are: 
[-11.33031602   2.48912124  -1.66197355   5.35122029  10.7988455
  -2.62311668   1.82668034  -5.91727641   6.50630285 -32.9536533
   9.92092297  -1.0854926    2.54190933  44.99864605   6.79118805
  26.8097292    3.7337978   -4.97323683   2.34313208 -10.56494313
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995046   4.98513713 -11.8360974   21.57457363  -7.27955595
   5.11757491  -4.39409784  24.41493727  13.95025403   1.56630503
   1.06172558 -16.82526464  -2.37084661   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690272  50.93964676  20.79586404
 -10.6914367   -1.40680276  -3.43212156   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961582  -0.57660576  -4.050751    -1.46689898
  -0.80107704   8.98228099  -3.1108763   -1.49498378  25.10657861]
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.8794692642527
gradient value of function right now is: [ 3.23030355e-09  4.57374922e-28  3.22853991e-09  2.44165779e-28
 -3.22559746e-09 -4.57372137e-28 -3.22383383e-09 -2.44165011e-28
  2.25137700e-13  2.36749074e-13  2.29051922e-13  2.35126361e-13
 -1.09094608e-27 -1.18755623e-27 -1.12178243e-27 -1.18261297e-27
  2.85692815e-12  3.16597698e-12  2.95562770e-12  3.11783323e-12
 -2.17121662e-29 -8.76058371e-28 -5.23252773e-29 -2.65034327e-27
 -1.16152755e-19 -3.44550258e-14  2.89750016e-19  4.94848670e-21
  3.02064751e-16  6.07769979e-14  3.82007985e-17  1.08443167e-17
  4.26431552e-19 -2.54786331e-14  2.32449856e-18  8.88352499e-20
  2.17532391e-17  1.07546349e-13 -5.91403241e-17 -8.73338275e-19
 -4.06571158e-16 -4.83277680e-17 -4.98969292e-15 -4.26968737e-17
  1.67648207e-12  1.87436803e-12 -1.48238546e-14  1.87459017e-12
  2.11825195e-17  1.85489152e-17 -4.31182894e-17  1.70101390e-17
  4.73545159e-18  9.14984773e-19  5.09835664e-17  8.22307025e-19
 -5.71920770e-13 -2.13667109e-11  1.57244274e-16  8.53313776e-14
  1.32921852e-18  2.30413737e-18  1.15196745e-16  2.15248338e-14
  1.23017595e+00]
supnorm grad right now is: 1.2301759514508852
Weights right now are: 
[-11.33097621   2.48912124  -1.66263339   5.35122029  10.79950571
  -2.62311668   1.82734021  -5.91727641   6.5063028  -32.95365335
   9.92092293  -1.08549265   2.54190933  44.99864605   6.79118805
  26.8097292    3.73379723  -4.97323746   2.3431315  -10.56494374
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995046   4.98513713 -11.8360974   21.57457363  -7.27955597
   5.11757491  -4.39409784  24.41493727  13.95025403   1.56630503
   1.06172558 -16.82526464  -2.37084664   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690272  50.93964676  20.7958637
 -10.69143707  -1.40680275  -3.43212193   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961593  -0.57660149  -4.050751    -1.466899
  -0.80107704   8.98228099  -3.1108763   -1.49498378  25.05088263]
80.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.899572167368
gradient value of function right now is: [ 3.24353974e-09  4.56163093e-28  3.24178076e-09  2.43837100e-28
 -3.23941078e-09 -4.56184218e-28 -3.23765173e-09 -2.43837550e-28
  2.24561228e-13  2.36142653e-13  2.28465347e-13  2.34524183e-13
 -1.08808327e-27 -1.18443962e-27 -1.11883804e-27 -1.17951428e-27
  2.85050833e-12  3.15892740e-12  2.94898598e-12  3.11087888e-12
 -2.16684276e-29 -8.76291801e-28 -5.23466604e-29 -2.64576109e-27
 -1.13614367e-19 -3.43762764e-14  2.89112210e-19  5.09794111e-21
  3.01154945e-16  6.06412059e-14  3.79987792e-17  1.08014904e-17
  4.40227381e-19 -2.54208096e-14  2.31843634e-18  8.96583494e-20
  2.08943098e-17  1.07355803e-13 -5.94113924e-17 -9.34777964e-19
 -4.06894639e-16 -4.86197435e-17 -4.98730655e-15 -4.29581280e-17
  1.67264515e-12  1.87007021e-12 -1.46955343e-14  1.87029073e-12
  2.12843387e-17  1.85015646e-17 -4.14411592e-17  1.69630069e-17
  4.75032007e-18  9.20419889e-19  5.10560941e-17  8.27101413e-19
 -5.70626353e-13 -2.13173005e-11  1.65105911e-16  8.49124482e-14
  1.33109386e-18  2.27203766e-18  1.18165905e-16  2.13799801e-14
  8.88658402e-01]
supnorm grad right now is: 0.8886584019035496
Weights right now are: 
[-11.33163913   2.48912124  -1.66329598   5.35122029  10.80016862
  -2.62311668   1.82800277  -5.91727641   6.50630276 -32.95365339
   9.92092288  -1.0854927    2.54190933  44.99864605   6.79118805
  26.8097292    3.73379668  -4.97323807   2.34313093 -10.56494434
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995047   4.98513713 -11.8360974   21.57457363  -7.27955598
   5.11757491  -4.39409784  24.41493727  13.95025404   1.56630503
   1.06172558 -16.82526464  -2.37084666   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690272  50.93964676  20.79586338
 -10.69143743  -1.40680275  -3.4321223    1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961605  -0.57659733  -4.050751    -1.46689902
  -0.80107704   8.98228099  -3.1108763   -1.49498378  25.02773523]
82.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.892246975861
gradient value of function right now is: [ 3.22625586e-09  4.54962433e-28  3.22450153e-09  2.43056930e-28
 -3.22480061e-09 -4.54977119e-28 -3.22304622e-09 -2.43062881e-28
  2.23980912e-13  2.35532484e-13  2.27874960e-13  2.33918247e-13
 -1.08520789e-27 -1.18131204e-27 -1.11588156e-27 -1.17640462e-27
  2.84404515e-12  3.15172618e-12  2.94229859e-12  3.10379541e-12
 -2.16023803e-29 -8.72203548e-28 -5.20936809e-29 -2.63819954e-27
 -1.14292619e-19 -3.42976885e-14  2.88745567e-19  5.02803663e-21
  3.00584001e-16  6.04951671e-14  3.79486208e-17  1.07849875e-17
  4.33637270e-19 -2.53613568e-14  2.31568022e-18  8.91101565e-20
  2.11892431e-17  1.07084429e-13 -5.91479429e-17 -9.07150144e-19
 -4.05499247e-16 -4.83484559e-17 -4.97253438e-15 -4.27177979e-17
  1.66878697e-12  1.86576804e-12 -1.47388410e-14  1.86598888e-12
  2.12113291e-17  1.84818897e-17 -4.18473293e-17  1.69468478e-17
  4.73001020e-18  9.15512501e-19  5.08675168e-17  8.22743710e-19
 -5.69309293e-13 -2.12691404e-11  1.58951663e-16  8.48758728e-14
  1.32687144e-18  2.26789159e-18  1.15823944e-16  2.13901769e-14
  1.02991041e+00]
supnorm grad right now is: 1.029910407814694
Weights right now are: 
[-11.33229795   2.48912124  -1.66395447   5.35122029  10.80082746
  -2.62311668   1.82866129  -5.91727641   6.50630272 -32.95365344
   9.92092284  -1.08549274   2.54190933  44.99864605   6.79118805
  26.8097292    3.73379616  -4.97323865   2.34313039 -10.56494492
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995048   4.98513713 -11.8360974   21.57457363  -7.27955599
   5.11757491  -4.39409784  24.41493727  13.95025404   1.56630503
   1.06172558 -16.82526464  -2.37084668   2.51369865   4.22064502
  -4.83421263  38.329907     1.17690273  50.93964676  20.79586305
 -10.6914378   -1.40680275  -3.43212266   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961615  -0.57659319  -4.050751    -1.46689903
  -0.80107704   8.98228099  -3.1108763   -1.49498379  25.05707883]
84.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.8514139936615
gradient value of function right now is: [ 3.38316227e-09  4.53752427e-28  3.38141242e-09  2.44663889e-28
 -3.38073422e-09 -4.53712707e-28 -3.37898452e-09 -2.44678033e-28
  2.23381671e-13  2.34902846e-13  2.27265463e-13  2.33292924e-13
 -1.08224817e-27 -1.17809690e-27 -1.11283965e-27 -1.17320794e-27
  2.83737155e-12  3.14496593e-12  2.93541685e-12  3.09701787e-12
 -2.16561937e-29 -8.90931413e-28 -5.32893280e-29 -2.64657442e-27
 -9.71227634e-20 -3.42193651e-14  2.86193364e-19  6.13956697e-21
  2.97854664e-16  6.04085167e-14  3.70353382e-17  1.06154787e-17
  5.37561388e-19 -2.53115109e-14  2.28902906e-18  9.57572724e-20
  1.49933699e-17  1.07304957e-13 -6.17275734e-17 -1.36900308e-18
 -4.13518137e-16 -5.11607407e-17 -5.02298428e-15 -4.52088017e-17
  1.66510890e-12  1.86157633e-12 -1.38101301e-14  1.86178623e-12
  2.20287784e-17  1.82895756e-17 -3.08972863e-17  1.67416993e-17
  4.89902073e-18  9.65687168e-19  5.20774060e-17  8.66908989e-19
 -5.68173510e-13 -2.12159762e-11  2.36139383e-16  8.27114345e-14
  1.37404327e-18  2.20406874e-18  1.44878919e-16  2.05389091e-14
 -1.45028147e+00]
supnorm grad right now is: 1.4502814741463599
Weights right now are: 
[-11.33296382   2.48912124  -1.66461999   5.35122029  10.80149334
  -2.62311668   1.82932682  -5.91727641   6.50630268 -32.95365348
   9.9209228   -1.08549278   2.54190933  44.99864605   6.79118805
  26.8097292    3.7337956   -4.97323927   2.3431298  -10.56494553
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995048   4.98513713 -11.8360974   21.57457363  -7.279556
   5.11757491  -4.39409784  24.41493727  13.95025405   1.56630503
   1.06172558 -16.82526464  -2.3708467    2.51369865   4.22064502
  -4.83421262  38.329907     1.17690273  50.93964676  20.79586271
 -10.69143818  -1.40680274  -3.43212304   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961627  -0.57658885  -4.050751    -1.46689905
  -0.80107704   8.98228099  -3.1108763   -1.49498379  24.92935707]
86.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.844823983108
gradient value of function right now is: [ 3.17585343e-09  4.52558263e-28  3.17410842e-09  2.41542394e-28
 -3.17100591e-09 -4.52539157e-28 -3.16926098e-09 -2.41540738e-28
  2.22812638e-13  2.34304278e-13  2.26686467e-13  2.32698539e-13
 -1.07942338e-27 -1.17502181e-27 -1.10993442e-27 -1.17015051e-27
  2.83102515e-12  3.13719060e-12  2.92882851e-12  3.08950028e-12
 -2.14675991e-29 -8.64434516e-28 -5.15685077e-29 -2.62337248e-27
 -1.16857288e-19 -3.41399604e-14  2.88255377e-19  4.80497008e-21
  2.99566429e-16  6.02027680e-14  3.79042072e-17  1.07613734e-17
  4.11857561e-19 -2.52420161e-14  2.31112691e-18  8.74093233e-20
  2.21442585e-17  1.06509482e-13 -5.84373193e-17 -8.21084930e-19
 -4.02101015e-16 -4.76022019e-17 -4.93927242e-15 -4.20546295e-17
  1.66104280e-12  1.85713296e-12 -1.48266335e-14  1.85735454e-12
  2.09944465e-17  1.84421456e-17 -4.33949243e-17  1.69154687e-17
  4.67640058e-18  9.01947184e-19  5.04013140e-17  8.10700930e-19
 -5.66669209e-13 -2.11726182e-11  1.45280961e-16  8.48219318e-14
  1.31531430e-18  2.29400427e-18  1.10363099e-16  2.14301748e-14
  1.56081537e+00]
supnorm grad right now is: 1.560815366950761
Weights right now are: 
[-11.33362212   2.48912124  -1.66527794   5.35122029  10.80215164
  -2.62311668   1.82998478  -5.91727641   6.50630263 -32.95365353
   9.92092275  -1.08549283   2.54190933  44.99864605   6.79118805
  26.8097292    3.73379504  -4.9732399    2.34312922 -10.56494614
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995049   4.98513713 -11.8360974   21.57457363  -7.27955601
   5.11757491  -4.39409784  24.41493727  13.95025405   1.56630503
   1.06172558 -16.82526464  -2.37084672   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690273  50.93964676  20.79586239
 -10.69143854  -1.40680274  -3.43212341   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961638  -0.57658475  -4.050751    -1.46689907
  -0.80107704   8.98228099  -3.1108763   -1.4949838   25.08832047]
88.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.795480904319
gradient value of function right now is: [ 3.39688164e-09  4.51372971e-28  3.39514094e-09  2.44032622e-28
 -3.39304905e-09 -4.51395918e-28 -3.39130826e-09 -2.44040820e-28
  2.22254192e-13  2.33716656e-13  2.26118180e-13  2.32115052e-13
 -1.07664724e-27 -1.17199802e-27 -1.10707865e-27 -1.16714430e-27
  2.82479772e-12  3.13112355e-12  2.92240620e-12  3.08336798e-12
 -2.15680712e-29 -8.91465764e-28 -5.32990877e-29 -2.63771811e-27
 -9.41726830e-20 -3.40645704e-14  2.82924328e-19  6.23192408e-21
  2.96106762e-16  6.01405203e-14  3.67981625e-17  1.05443143e-17
  5.50170229e-19 -2.51975653e-14  2.26308706e-18  9.59565024e-20
  1.38594692e-17  1.06906586e-13 -6.16352714e-17 -1.43254740e-18
 -4.13154675e-16 -5.13866702e-17 -5.01011155e-15 -4.54079277e-17
  1.65756185e-12  1.85312231e-12 -1.35814992e-14  1.85332962e-12
  2.18691284e-17  1.80973482e-17 -2.99380558e-17  1.65603821e-17
  4.90163007e-18  9.67390759e-19  5.20349693e-17  8.68222876e-19
 -5.65618501e-13 -2.11188034e-11  2.46560856e-16  8.19986559e-14
  1.38774889e-18  2.15514507e-18  1.48690433e-16  2.03084091e-14
 -1.90266801e+00]
supnorm grad right now is: 1.9026680054792886
Weights right now are: 
[-11.3342775    2.48912124  -1.66593297   5.35122029  10.80280701
  -2.62311668   1.83063979  -5.91727641   6.50630258 -32.95365358
   9.9209227   -1.08549288   2.54190933  44.99864605   6.79118805
  26.8097292    3.73379446  -4.97324054   2.34312862 -10.56494678
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.9299505    4.98513713 -11.8360974   21.57457363  -7.27955603
   5.11757491  -4.39409784  24.41493727  13.95025406   1.56630503
   1.06172558 -16.82526464  -2.37084674   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690273  50.93964676  20.79586205
 -10.69143892  -1.40680274  -3.43212378   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961649  -0.57658053  -4.050751    -1.46689908
  -0.80107704   8.98228099  -3.1108763   -1.4949838   24.84426472]
90.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.7873075619455
gradient value of function right now is: [ 3.13160479e-09  4.50168207e-28  3.12986906e-09  2.40728511e-28
 -3.13403702e-09 -4.50190867e-28 -3.13230120e-09 -2.40717790e-28
  2.21673491e-13  2.33106074e-13  2.25527375e-13  2.31508783e-13
 -1.07376901e-27 -1.16886775e-27 -1.10411907e-27 -1.16403312e-27
  2.81831619e-12  3.12296881e-12  2.91567210e-12  3.07552332e-12
 -2.13315868e-29 -8.59406122e-28 -5.09293156e-29 -2.61706890e-27
 -1.19503205e-19 -3.39835999e-14  2.89494241e-19  4.64792985e-21
  2.99062393e-16  5.99088837e-14  3.78496501e-17  1.07536647e-17
  3.96731929e-19 -2.51230358e-14  2.32010584e-18  8.64771118e-20
  2.30054781e-17  1.05935931e-13 -5.81778410e-17 -7.57336298e-19
 -3.99966302e-16 -4.71421492e-17 -4.91746954e-15 -4.16481953e-17
  1.65336207e-12  1.84857384e-12 -1.49857656e-14  1.84879663e-12
  2.10197918e-17  1.85179764e-17 -4.37795239e-17  1.69897985e-17
  4.64390375e-18  8.94621611e-19  5.00986923e-17  8.04287630e-19
 -5.64030759e-13 -2.10769709e-11  1.29677954e-16  8.48529984e-14
  1.30690021e-18  2.29804498e-18  1.04719359e-16  2.14752791e-14
  1.97512046e+00]
supnorm grad right now is: 1.9751204590562945
Weights right now are: 
[-11.33494597   2.48912124  -1.66660109   5.35122029  10.8034755
  -2.62311668   1.83130792  -5.91727641   6.50630254 -32.95365362
   9.92092266  -1.08549292   2.54190933  44.99864605   6.79118805
  26.8097292    3.73379389  -4.97324117   2.34312804 -10.5649474
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.9299505    4.98513713 -11.8360974   21.57457363  -7.27955604
   5.11757491  -4.39409784  24.41493727  13.95025406   1.56630503
   1.06172558 -16.82526464  -2.37084676   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690273  50.93964676  20.79586171
 -10.6914393   -1.40680274  -3.43212416   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961661  -0.57657605  -4.050751    -1.4668991
  -0.80107704   8.98228099  -3.1108763   -1.4949838   25.18275259]
92.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.9206411133136
gradient value of function right now is: [ 3.28319153e-09  4.49579994e-28  3.28145788e-09  2.41735530e-28
 -3.28788855e-09 -4.49554789e-28 -3.28615499e-09 -2.41732009e-28
  2.21375871e-13  2.32793482e-13  2.25224726e-13  2.31198243e-13
 -1.07230355e-27 -1.16727661e-27 -1.10261347e-27 -1.16245009e-27
  2.81499878e-12  3.11990707e-12  2.91226178e-12  3.07239126e-12
 -2.14081657e-29 -8.74529683e-28 -5.22034147e-29 -2.61883162e-27
 -1.03639744e-19 -3.39459389e-14  2.85612956e-19  5.63091484e-21
  2.96394580e-16  5.98943186e-14  3.70619618e-17  1.05937805e-17
  4.88381853e-19 -2.51034844e-14  2.28561120e-18  9.22120283e-20
  1.74967083e-17  1.06265678e-13 -6.01972777e-17 -1.16272844e-18
 -4.06493966e-16 -4.95157033e-17 -4.95726245e-15 -4.37530423e-17
  1.65166587e-12  1.84659400e-12 -1.40846542e-14  1.84680658e-12
  2.15703172e-17  1.82458536e-17 -3.48367440e-17  1.67145952e-17
  4.78579508e-18  9.36709306e-19  5.11122493e-17  8.41298087e-19
 -5.63563013e-13 -2.10489023e-11  2.01971101e-16  8.28791974e-14
  1.34682539e-18  2.25435477e-18  1.31444693e-16  2.07134197e-14
 -4.17259230e-01]
supnorm grad right now is: 0.4172592298552595
Weights right now are: 
[-11.33560263   2.48912124  -1.66725739   5.35122029  10.80413217
  -2.62311668   1.83196423  -5.91727641   6.50630249 -32.95365367
   9.92092261  -1.08549297   2.54190933  44.99864605   6.79118805
  26.8097292    3.73379331  -4.97324182   2.34312743 -10.56494803
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995051   4.98513713 -11.8360974   21.57457363  -7.27955605
   5.11757491  -4.39409784  24.41493727  13.95025407   1.56630503
   1.06172558 -16.82526464  -2.37084679   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690273  50.93964676  20.79586138
 -10.69143967  -1.40680273  -3.43212453   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961673  -0.57657178  -4.050751    -1.46689912
  -0.80107704   8.98228099  -3.1108763   -1.49498381  25.15992086]
94.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.925920710907
gradient value of function right now is: [ 3.26476551e-09  4.48986083e-28  3.26303416e-09  2.41459253e-28
 -3.26529652e-09 -4.48947382e-28 -3.26356530e-09 -2.41451794e-28
  2.21085528e-13  2.32488316e-13  2.24929383e-13  2.30895177e-13
 -1.07086789e-27 -1.16571601e-27 -1.10113765e-27 -1.16089828e-27
  2.81176012e-12  3.11628675e-12  2.90891197e-12  3.06883114e-12
 -2.13775551e-29 -8.73505468e-28 -5.20984908e-29 -2.61590930e-27
 -1.05573567e-19 -3.39069536e-14  2.84064595e-19  5.45635228e-21
  2.96283029e-16  5.98237600e-14  3.71834632e-17  1.05995871e-17
  4.74750400e-19 -2.50740681e-14  2.27535387e-18  9.10301323e-20
  1.81799281e-17  1.06116409e-13 -5.95625892e-17 -1.10222480e-18
 -4.04967348e-16 -4.90947376e-17 -4.94434711e-15 -4.33785316e-17
  1.64975050e-12  1.84445532e-12 -1.40883551e-14  1.84466817e-12
  2.12585430e-17  1.81512127e-17 -3.70097452e-17  1.66305850e-17
  4.75585390e-18  9.27684666e-19  5.08820943e-17  8.33244876e-19
 -5.62911723e-13 -2.10249341e-11  1.96980099e-16  8.28807868e-14
  1.34401615e-18  2.26044091e-18  1.29187612e-16  2.07425134e-14
 -2.02924167e-01]
supnorm grad right now is: 0.20292416744834232
Weights right now are: 
[-11.33626073   2.48912124  -1.66791513   5.35122029  10.80479026
  -2.62311668   1.83262197  -5.91727641   6.50630245 -32.95365372
   9.92092256  -1.08549302   2.54190933  44.99864605   6.79118805
  26.8097292    3.73379272  -4.97324246   2.34312683 -10.56494867
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995052   4.98513713 -11.8360974   21.57457363  -7.27955606
   5.11757491  -4.39409784  24.41493727  13.95025407   1.56630503
   1.06172558 -16.82526464  -2.37084681   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690273  50.93964676  20.79586105
 -10.69144004  -1.40680273  -3.4321249    1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961684  -0.57656764  -4.050751    -1.46689913
  -0.80107704   8.98228099  -3.1108763   -1.49498381  25.32349822]
96.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.9266771224907
gradient value of function right now is: [ 3.25745466e-09  4.48398467e-28  3.25572559e-09  2.41072101e-28
 -3.26015253e-09 -4.48396603e-28 -3.25842346e-09 -2.41063609e-28
  2.20813452e-13  2.32201869e-13  2.24652459e-13  2.30610766e-13
 -1.06951223e-27 -1.16423799e-27 -1.09974265e-27 -1.15942896e-27
  2.80872082e-12  3.11288901e-12  2.90576361e-12  3.06549075e-12
 -2.13455263e-29 -8.71434343e-28 -5.19788060e-29 -2.61213978e-27
 -1.05607441e-19 -3.38687599e-14  2.84201446e-19  5.44985134e-21
  2.95985535e-16  5.97525039e-14  3.71323739e-17  1.05892401e-17
  4.73426254e-19 -2.50451922e-14  2.27597786e-18  9.09383934e-20
  1.82234187e-17  1.05984873e-13 -5.95341908e-17 -1.09768936e-18
 -4.04447269e-16 -4.90163540e-17 -4.93841013e-15 -4.33095108e-17
  1.64787725e-12  1.84236452e-12 -1.41130645e-14  1.84257752e-12
  2.12788808e-17  1.81582285e-17 -3.68284696e-17  1.66377057e-17
  4.74983587e-18  9.26592827e-19  5.08178599e-17  8.32292006e-19
 -5.62264969e-13 -2.10013165e-11  1.94133480e-16  8.28584715e-14
  1.34283653e-18  2.25896935e-18  1.28176475e-16  2.07428418e-14
 -1.56122823e-01]
supnorm grad right now is: 0.15612282260332344
Weights right now are: 
[-11.33690597   2.48912124  -1.66856002   5.35122029  10.80543548
  -2.62311668   1.83326684  -5.91727641   6.5063024  -32.95365377
   9.92092252  -1.08549307   2.54190933  44.99864605   6.79118805
  26.8097292    3.73379215  -4.97324309   2.34312624 -10.56494929
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995052   4.98513713 -11.8360974   21.57457363  -7.27955607
   5.11757491  -4.39409784  24.41493727  13.95025408   1.56630503
   1.06172558 -16.82526464  -2.37084683   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690273  50.93964676  20.79586072
 -10.69144041  -1.40680273  -3.43212527   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961696  -0.57656337  -4.050751    -1.46689915
  -0.80107704   8.98228099  -3.1108763   -1.49498382  24.85309034]
98.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.926524414952
gradient value of function right now is: [ 3.25382743e-09  4.47817809e-28  3.25210060e-09  2.40792953e-28
 -3.25267442e-09 -4.47803130e-28 -3.25094765e-09 -2.40797469e-28
  2.20529817e-13  2.31903723e-13  2.24363928e-13  2.30314679e-13
 -1.06810911e-27 -1.16271261e-27 -1.09830021e-27 -1.15791238e-27
  2.80555416e-12  3.10938011e-12  2.90248866e-12  3.06203504e-12
 -2.13180672e-29 -8.70426856e-28 -5.19108171e-29 -2.60920619e-27
 -1.05460493e-19 -3.38306000e-14  2.83930260e-19  5.44779731e-21
  2.95660272e-16  5.96839140e-14  3.70879795e-17  1.05774063e-17
  4.73244896e-19 -2.50166867e-14  2.27374166e-18  9.08696043e-20
  1.81839853e-17  1.05864120e-13 -5.94921751e-17 -1.09821861e-18
 -4.04042585e-16 -4.89759742e-17 -4.93329897e-15 -4.32734794e-17
  1.64600929e-12  1.84027830e-12 -1.40984333e-14  1.84049108e-12
  2.12637497e-17  1.81417079e-17 -3.67123105e-17  1.66222622e-17
  4.74542262e-18  9.25867843e-19  5.07684770e-17  8.31629556e-19
 -5.61633186e-13 -2.09777850e-11  1.93876658e-16  8.27646132e-14
  1.34065931e-18  2.25635152e-18  1.28056519e-16  2.07183074e-14
 -1.63922589e-01]
supnorm grad right now is: 0.16392258928059011
Weights right now are: 
[-11.33753725   2.48912124  -1.66919099   5.35122029  10.80606679
  -2.62311668   1.83389784  -5.91727641   6.50630236 -32.95365381
   9.92092248  -1.08549311   2.54190933  44.99864605   6.79118805
  26.8097292    3.73379165  -4.97324365   2.34312572 -10.56494984
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995053   4.98513713 -11.8360974   21.57457363  -7.27955609
   5.11757491  -4.39409784  24.41493727  13.95025408   1.56630503
   1.06172558 -16.82526464  -2.37084685   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690273  50.93964676  20.7958604
 -10.69144077  -1.40680273  -3.43212563   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961706  -0.57655935  -4.050751    -1.46689917
  -0.80107704   8.98228099  -3.1108763   -1.49498382  25.18026511]
100.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -2816.927217422776
gradient value of function right now is: [ 3.24537251e-09  4.47243348e-28  3.24364791e-09  2.40402933e-28
 -3.24674196e-09 -4.47260223e-28 -3.24501729e-09 -2.40399628e-28
  2.20262477e-13  2.31622309e-13  2.24091841e-13  2.30035258e-13
 -1.06677810e-27 -1.16126183e-27 -1.09693071e-27 -1.15647004e-27
  2.80256704e-12  3.10603673e-12  2.89939445e-12  3.05874942e-12
 -2.12852829e-29 -8.68253708e-28 -5.17738050e-29 -2.60545715e-27
 -1.06187123e-19 -3.37931786e-14  2.83447277e-19  5.38392815e-21
  2.95465468e-16  5.96137958e-14  3.70988306e-17  1.05741589e-17
  4.69033732e-19 -2.49882568e-14  2.27086667e-18  9.04838285e-20
  1.84341797e-17  1.05731756e-13 -5.93042241e-17 -1.07760263e-18
 -4.03350943e-16 -4.88342807e-17 -4.92606358e-15 -4.31488985e-17
  1.64417180e-12  1.83822830e-12 -1.41311192e-14  1.83844140e-12
  2.11836090e-17  1.81235339e-17 -3.72317393e-17  1.66068571e-17
  4.73393856e-18  9.22908955e-19  5.06672412e-17  8.29001073e-19
 -5.60997982e-13 -2.09547075e-11  1.89654041e-16  8.27760663e-14
  1.33958891e-18  2.25447315e-18  1.26528714e-16  2.07322225e-14
 -9.75945669e-02]
supnorm grad right now is: 0.0975945669221905
Weights right now are: 
[-11.33818061   2.48912124  -1.66983401   5.35122029  10.80671014
  -2.62311668   1.83454086  -5.91727641   6.50630232 -32.95365386
   9.92092244  -1.08549315   2.54190933  44.99864605   6.79118805
  26.8097292    3.73379111  -4.97324425   2.34312516 -10.56495044
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995054   4.98513713 -11.8360974   21.57457363  -7.2795561
   5.11757491  -4.39409784  24.41493727  13.95025409   1.56630503
   1.06172558 -16.82526464  -2.37084687   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690273  50.93964676  20.79586008
 -10.69144112  -1.40680272  -3.43212598   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961717  -0.57655531  -4.050751    -1.46689918
  -0.80107704   8.98228099  -3.1108763   -1.49498383  25.21401495]
NN weights: [-11.33657789   2.48912124  -1.66823212   5.35122029  10.80510741
  -2.62311668   1.83293896  -5.91727641   6.50630243 -32.95365374
   9.92092254  -1.08549304   2.54190933  44.99864605   6.79118805
  26.8097292    3.73379246  -4.97324276   2.34312655 -10.56494896
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995052   4.98513713 -11.8360974   21.57457363  -7.27955607
   5.11757491  -4.39409784  24.41493727  13.95025408   1.56630503
   1.06172558 -16.82526464  -2.37084682   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690273  50.93964676  20.79586089
 -10.69144022  -1.40680273  -3.43212508   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.5496169   -0.57656554  -4.050751    -1.46689914
  -0.80107704   8.98228099  -3.1108763   -1.49498381]
Minimum obj value:-2816.927217422776
Optimal xi: 24.984209132078078
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:223: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_ALL[key]["summary_df"], ignore_index=True)
/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py:235: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  res_ALL_dataframe = res_ALL_dataframe.append(res_BEST_temp["summary_df"], ignore_index=True)
-----------------------------------------------
Selected results: NN strategy on TRAINING dataset
W_T_mean: 1551.557748059139
W_T_median: 1405.4383972906453
W_T_pctile_5: 624.6754498245464
W_T_CVAR_5_pct: 489.5910352010398
-----------------------------------------------
-----------------------------------------------
Just FINISHED: 
Asset basket ID: basic_T30_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
F value: -2816.927217422776
-----------------------------------------------
{'NN': [-11.336577886976281, 2.489121242806333, -1.6682321238805853, 5.351220289457187, 10.805107414121062, -2.6231166795279734, 1.8329389608754318, -5.917276408661313, 6.506302426872658, -32.953653744035215, 9.920922543605426, -1.0854930424447387, 2.5419093279911222, 44.9986460471793, 6.791188054185814, 26.80972920354094, 3.7337924561237568, -4.973242755914027, 2.3431265528965475, -10.564948960397176, 3.923369954786178, 27.624088863944074, 40.17399079387814, 27.56738773047254, -20.597191814515224, 24.92995052069737, 4.9851371314444215, -11.836097399014259, 21.574573630142734, -7.279556068156868, 5.117574911073241, -4.394097842337032, 24.414937274333106, 13.950254076351648, 1.5663050274234604, 1.0617255779099948, -16.825264641557947, -2.3708468169240793, 2.5136986499632776, 4.220645024309757, -4.834212624453252, 38.32990699800745, 1.176902732321293, 50.939646764720386, 20.79586088510195, -10.691440221809344, -1.4068027291724825, -3.432125083564439, 1.7541788461164038, 9.344385792420413, 3.0252301546835176, 18.326746435534453, -6.6844973477974685, 20.889649226508162, 5.006959902361257, 44.55733376738498, 9.549616895768397, -0.5765655390381249, -4.0507509996381525, -1.4668991404813712, -0.8010770383521503, 8.982280986993603, -3.110876303204417, -1.494983814749038]}
[-11.33657789   2.48912124  -1.66823212   5.35122029  10.80510741
  -2.62311668   1.83293896  -5.91727641   6.50630243 -32.95365374
   9.92092254  -1.08549304   2.54190933  44.99864605   6.79118805
  26.8097292    3.73379246  -4.97324276   2.34312655 -10.56494896
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995052   4.98513713 -11.8360974   21.57457363  -7.27955607
   5.11757491  -4.39409784  24.41493727  13.95025408   1.56630503
   1.06172558 -16.82526464  -2.37084682   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690273  50.93964676  20.79586089
 -10.69144022  -1.40680273  -3.43212508   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.5496169   -0.57656554  -4.050751    -1.46689914
  -0.80107704   8.98228099  -3.1108763   -1.49498381]
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
W_T_mean: 1261.0933778858873
W_T_median: 1222.5216402372002
W_T_pctile_5: 834.4055321177691
W_T_CVAR_5_pct: 748.3017793876664
-----------------------------------------------
Running Adam.
2.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.217925921613
gradient value of function right now is: [ 1.00950458e-08  8.87176795e-28  1.00916104e-08  3.24001988e-28
 -1.00926988e-08 -8.87219572e-28 -1.00892632e-08 -3.24006500e-28
  4.37918925e-13  4.60501706e-13  4.45550892e-13  4.57207555e-13
 -2.12167898e-27 -2.30629091e-27 -2.18173173e-27 -2.28816558e-27
  5.58084253e-12  6.20079393e-12  5.77430278e-12  6.10366724e-12
 -4.38992039e-29 -1.52165635e-27 -1.29795316e-28 -3.23583239e-27
 -1.43310241e-19 -6.74149517e-14  2.30213940e-19  2.15323496e-21
  3.17988089e-16  1.19788543e-13  4.45389426e-17  1.15530594e-17
  3.00046767e-19 -5.00309104e-14  1.92841921e-18  7.23313902e-20
  2.90100263e-17  2.22107800e-13 -4.39612122e-17 -1.51901740e-19
 -4.14089129e-16 -4.62617200e-17 -5.15335681e-15 -4.08792826e-17
  3.28358605e-12  3.66939591e-12 -9.11506985e-15  3.66963266e-12
  1.11510948e-17  1.40338130e-17 -1.11971173e-16  1.29650318e-17
  4.58337768e-18  8.02980408e-19  5.10239820e-17  7.22319020e-19
 -1.12039087e-12 -4.16867336e-11  1.55363371e-15  1.37950262e-13
  4.01400825e-18  1.62482206e-18  5.39847923e-16  3.75734738e-14
  1.20659685e+00]
supnorm grad right now is: 1.2065968461535925
Weights right now are: 
[-11.33857366   2.48912124  -1.67022723   5.35122029  10.80710319
  -2.62311668   1.83493407  -5.91727641   6.50630234 -32.95365383
   9.92092246  -1.08549313   2.54190933  44.99864605   6.79118805
  26.8097292    3.73379136  -4.97324397   2.34312542 -10.56495016
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995053   4.98513713 -11.8360974   21.57457363  -7.27955609
   5.11757491  -4.39409784  24.41493727  13.95025409   1.56630503
   1.06172558 -16.82526464  -2.37084686   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690273  50.93964676  20.79586023
 -10.69144096  -1.40680273  -3.43212582   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961711  -0.57655736  -4.050751    -1.46689917
  -0.80107704   8.98228099  -3.1108763   -1.49498382  25.06346409]
4.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.013120429769
gradient value of function right now is: [ 9.92087868e-09  8.79873452e-28  9.91747174e-09  3.20635765e-28
 -9.91301968e-09 -8.79876588e-28 -9.90961273e-09 -3.20635761e-28
  4.34382043e-13  4.56782844e-13  4.41952470e-13  4.53515439e-13
 -2.10415527e-27 -2.28724987e-27 -2.16371287e-27 -2.26928523e-27
  5.54126608e-12  6.15641608e-12  5.73334402e-12  6.06006486e-12
 -4.34591581e-29 -1.49761233e-27 -1.27701345e-28 -3.20894062e-27
 -1.51150911e-19 -6.69323852e-14  2.34968110e-19  1.70087540e-21
  3.17294081e-16  1.18874333e-13  4.44719394e-17  1.15590633e-17
  2.52457496e-19 -4.96628117e-14  1.96306469e-18  6.95967326e-20
  3.18146891e-17  2.20231432e-13 -4.32623043e-17  5.11966191e-20
 -4.08133902e-16 -4.48149142e-17 -5.09778086e-15 -3.96019122e-17
  3.25984481e-12  3.64295522e-12 -9.72704092e-15  3.64319720e-12
  1.12874861e-17  1.43287652e-17 -1.13407282e-16  1.32494711e-17
  4.48951519e-18  7.80874339e-19  5.01933319e-17  7.03018860e-19
 -1.11226390e-12 -4.13927615e-11  1.49442167e-15  1.38268591e-13
  3.97278553e-18  1.65311898e-18  5.18932410e-16  3.77808525e-14
  2.65948766e+00]
supnorm grad right now is: 2.6594876613635923
Weights right now are: 
[-11.3406423    2.48912124  -1.67229519   5.35122029  10.80917185
  -2.62311668   1.83700205  -5.91727641   6.50630226 -32.95365392
   9.92092237  -1.08549322   2.54190933  44.99864605   6.79118805
  26.8097292    3.73379027  -4.97324519   2.34312429 -10.56495135
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995055   4.98513713 -11.8360974   21.57457363  -7.27955612
   5.11757491  -4.39409784  24.41493727  13.9502541    1.56630503
   1.06172558 -16.82526464  -2.37084691   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690273  50.93964676  20.79585956
 -10.6914417   -1.40680273  -3.43212656   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961734  -0.57654885  -4.050751    -1.4668992
  -0.80107704   8.98228099  -3.1108763   -1.49498383  25.21708823]
6.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.242198041738
gradient value of function right now is: [ 9.95831098e-09  8.72875051e-28  9.95493099e-09  3.19433811e-28
 -9.96058587e-09 -8.72912328e-28 -9.95720575e-09 -3.19427195e-28
  4.31014118e-13  4.53240838e-13  4.38525722e-13  4.49998873e-13
 -2.08745772e-27 -2.26909892e-27 -2.14654163e-27 -2.25128647e-27
  5.50353540e-12  6.11498942e-12  5.69431723e-12  6.01919064e-12
 -4.32095678e-29 -1.50133394e-27 -1.27904944e-28 -3.18938732e-27
 -1.38923421e-19 -6.64722953e-14  2.26749275e-19  2.29035873e-21
  3.13202090e-16  1.18091976e-13  4.37890887e-17  1.13686392e-17
  3.11774059e-19 -4.93256620e-14  1.89872815e-18  7.24023874e-20
  2.77383885e-17  2.19098936e-13 -4.37389920e-17 -2.17347173e-19
 -4.09512166e-16 -4.60368036e-17 -5.08914494e-15 -4.06832876e-17
  3.23743606e-12  3.61784277e-12 -8.86087888e-15  3.61807477e-12
  1.11236175e-17  1.38117002e-17 -1.08799185e-16  1.27558321e-17
  4.54534876e-18  7.99560370e-19  5.04958987e-17  7.19123093e-19
 -1.10473823e-12 -4.11035806e-11  1.54216916e-15  1.35730837e-13
  3.96563773e-18  1.57692305e-18  5.36209162e-16  3.69312363e-14
  7.78819420e-01]
supnorm grad right now is: 0.7788194195205532
Weights right now are: 
[-11.34263974   2.48912124  -1.67429194   5.35122029  10.8111693
  -2.62311668   1.83899881  -5.91727641   6.50630217 -32.95365402
   9.92092228  -1.08549331   2.54190933  44.99864605   6.79118805
  26.8097292    3.73378915  -4.97324643   2.34312313 -10.56495258
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995056   4.98513713 -11.8360974   21.57457363  -7.27955614
   5.11757491  -4.39409784  24.41493727  13.95025411   1.56630503
   1.06172558 -16.82526464  -2.37084695   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690274  50.93964676  20.79585892
 -10.69144242  -1.40680272  -3.43212729   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961755  -0.57654069  -4.050751    -1.46689923
  -0.80107704   8.98228099  -3.1108763   -1.49498384  25.04096432]
8.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5143.910417836239
gradient value of function right now is: [ 1.01296789e-08  8.65956586e-28  1.01263255e-08  3.20883023e-28
 -1.01232318e-08 -8.65945917e-28 -1.01198783e-08 -3.20899336e-28
  4.27659637e-13  4.49713761e-13  4.35112898e-13  4.46497088e-13
 -2.07084529e-27 -2.25104778e-27 -2.12946004e-27 -2.23338635e-27
  5.46591611e-12  6.07419227e-12  5.65542559e-12  5.97884152e-12
 -4.30518880e-29 -1.52652177e-27 -1.29095598e-28 -3.19172669e-27
 -1.15690727e-19 -6.60169113e-14  2.14902100e-19  3.53716801e-21
  3.07984543e-16  1.17361041e-13  4.26769770e-17  1.10937499e-17
  4.45071094e-19 -4.89985090e-14  1.80157492e-18  7.92047982e-20
  1.89860918e-17  2.18347750e-13 -4.58400131e-17 -8.18334814e-19
 -4.18755302e-16 -4.95817529e-17 -5.13616583e-15 -4.38180686e-17
  3.21538096e-12  3.59306483e-12 -7.29349147e-15  3.59327888e-12
  1.12873597e-17  1.30838940e-17 -9.80097129e-17  1.20491584e-17
  4.73596291e-18  8.53708557e-19  5.18324927e-17  7.66279364e-19
 -1.09742658e-12 -4.08138634e-11  1.65007008e-15  1.31696442e-13
  4.03087396e-18  1.36647691e-18  5.76459783e-16  3.54537388e-14
 -3.12632834e+00]
supnorm grad right now is: 3.12632834233574
Weights right now are: 
[-11.34463076   2.48912124  -1.67628229   5.35122029  10.81316034
  -2.62311668   1.84098918  -5.91727641   6.50630208 -32.95365411
   9.92092219  -1.0854934    2.54190933  44.99864605   6.79118805
  26.8097292    3.73378804  -4.97324766   2.34312199 -10.56495379
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995057   4.98513713 -11.8360974   21.57457363  -7.27955616
   5.11757491  -4.39409784  24.41493727  13.95025412   1.56630503
   1.06172558 -16.82526464  -2.37084699   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690274  50.93964676  20.79585828
 -10.69144313  -1.40680272  -3.43212799   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961776  -0.5765328   -4.050751    -1.46689925
  -0.80107704   8.98228099  -3.1108763   -1.49498384  24.7343678 ]
10.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.174205954667
gradient value of function right now is: [ 9.75159340e-09  8.59178070e-28  9.74826655e-09  3.14897654e-28
 -9.75379665e-09 -8.59225229e-28 -9.75046963e-09 -3.14883637e-28
  4.24403628e-13  4.46289339e-13  4.31799937e-13  4.43097416e-13
 -2.05470310e-27 -2.23349979e-27 -2.11285917e-27 -2.21598692e-27
  5.42934923e-12  6.03236234e-12  5.61755075e-12  5.93789557e-12
 -4.25029386e-29 -1.47675549e-27 -1.25453275e-28 -3.14749305e-27
 -1.42310925e-19 -6.55666382e-14  2.25405117e-19  1.95881246e-21
  3.09803007e-16  1.16438721e-13  4.34192654e-17  1.12654543e-17
  2.80211621e-19 -4.86442063e-14  1.88724109e-18  6.97800889e-20
  2.90352968e-17  2.16000593e-13 -4.26095960e-17 -9.37658395e-20
 -4.02298724e-16 -4.47609410e-17 -5.00926772e-15 -3.95529393e-17
  3.19304056e-12  3.56828963e-12 -9.01817968e-15  3.56852149e-12
  1.08542763e-17  1.37319029e-17 -1.09778211e-16  1.26893555e-17
  4.44498480e-18  7.77100291e-19  4.95243292e-17  6.99175356e-19
 -1.08963140e-12 -4.05455478e-11  1.49951316e-15  1.34448461e-13
  3.90383753e-18  1.59009197e-18  5.21012201e-16  3.66435146e-14
  1.62385547e+00]
supnorm grad right now is: 1.6238554656706745
Weights right now are: 
[-11.34659791   2.48912124  -1.67824875   5.35122029  10.81512749
  -2.62311668   1.84295564  -5.91727641   6.50630199 -32.9536542
   9.9209221   -1.08549349   2.54190933  44.99864605   6.79118805
  26.8097292    3.73378693  -4.9732489    2.34312084 -10.564955
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995059   4.98513713 -11.8360974   21.57457363  -7.27955619
   5.11757491  -4.39409784  24.41493727  13.95025413   1.56630503
   1.06172558 -16.82526464  -2.37084704   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690274  50.93964676  20.79585763
 -10.69144386  -1.40680272  -3.43212872   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961798  -0.57652468  -4.050751    -1.46689928
  -0.80107704   8.98228099  -3.1108763   -1.49498385  25.14875665]
12.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.129276913717
gradient value of function right now is: [ 9.89739709e-09  8.52466215e-28  9.89409599e-09  3.14612599e-28
 -9.89769824e-09 -8.52495289e-28 -9.89439703e-09 -3.14604041e-28
  4.21156839e-13  4.42875159e-13  4.28496585e-13  4.39707697e-13
 -2.03862405e-27 -2.21602504e-27 -2.09632523e-27 -2.19865818e-27
  5.39285226e-12  5.99263481e-12  5.57981274e-12  5.89863447e-12
 -4.23149529e-29 -1.48870479e-27 -1.26278209e-28 -3.13434094e-27
 -1.18654014e-19 -6.51237334e-14  2.19261097e-19  3.37257956e-21
  3.04434264e-16  1.15707985e-13  4.20308499e-17  1.09790558e-17
  4.13255555e-19 -4.83239071e-14  1.83098435e-18  7.77314096e-20
  2.07213708e-17  2.15165876e-13 -4.55188324e-17 -6.91691132e-19
 -4.10310802e-16 -4.80992176e-17 -5.04605844e-15 -4.25120004e-17
  3.17154956e-12  3.54417378e-12 -7.78580078e-15  3.54439073e-12
  1.17018151e-17  1.33366676e-17 -9.56372592e-17  1.22884312e-17
  4.63484456e-18  8.35644508e-19  5.07986009e-17  7.50548335e-19
 -1.08246930e-12 -4.02652892e-11  1.58740949e-15  1.30997487e-13
  3.95234164e-18  1.45078457e-18  5.54531596e-16  3.53706307e-14
 -1.92585179e+00]
supnorm grad right now is: 1.9258517872110439
Weights right now are: 
[-11.34855963   2.48912124  -1.6802098    5.35122029  10.81708922
  -2.62311668   1.8449167   -5.91727641   6.50630191 -32.95365429
   9.92092202  -1.08549358   2.54190933  44.99864605   6.79118805
  26.8097292    3.73378583  -4.97325012   2.34311969 -10.56495621
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.9299506    4.98513713 -11.8360974   21.57457363  -7.27955621
   5.11757491  -4.39409784  24.41493727  13.95025414   1.56630503
   1.06172558 -16.82526464  -2.37084708   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690274  50.93964676  20.79585699
 -10.69144458  -1.40680272  -3.43212944   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961821  -0.57651632  -4.050751    -1.46689931
  -0.80107704   8.98228099  -3.1108763   -1.49498386  24.86504324]
14.000000000000002% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.251386514321
gradient value of function right now is: [ 9.73557511e-09  8.45835532e-28  9.73229981e-09  3.11047358e-28
 -9.73490028e-09 -8.45830068e-28 -9.73162500e-09 -3.11040296e-28
  4.17943541e-13  4.39496413e-13  4.25227359e-13  4.36353198e-13
 -2.02271713e-27 -2.19873971e-27 -2.07996880e-27 -2.18151779e-27
  5.35669009e-12  5.95213101e-12  5.54238945e-12  5.85882683e-12
 -4.19210473e-29 -1.46560115e-27 -1.24527539e-28 -3.10312386e-27
 -1.26166304e-19 -6.46831090e-14  2.21261857e-19  2.86647307e-21
  3.03376759e-16  1.14882134e-13  4.20403663e-17  1.09737090e-17
  3.59770653e-19 -4.79889726e-14  1.84692260e-18  7.43783340e-20
  2.37064928e-17  2.13480779e-13 -4.42484772e-17 -4.63952323e-19
 -4.03016540e-16 -4.63386022e-17 -4.97993505e-15 -4.09554711e-17
  3.14989330e-12  3.52003951e-12 -8.21672189e-15  3.52026020e-12
  1.14849967e-17  1.34501321e-17 -9.88582280e-17  1.24070550e-17
  4.51977485e-18  8.07301473e-19  4.98144214e-17  7.25637608e-19
 -1.07506432e-12 -3.99960754e-11  1.53869205e-15  1.31121829e-13
  3.89566215e-18  1.50012612e-18  5.36320589e-16  3.55422321e-14
 -5.92455643e-01]
supnorm grad right now is: 0.5924556434352494
Weights right now are: 
[-11.35051297   2.48912124  -1.68216246   5.35122029  10.81904264
  -2.62311668   1.84686943  -5.91727641   6.50630182 -32.95365438
   9.92092193  -1.08549368   2.54190933  44.99864605   6.79118805
  26.8097292    3.73378471  -4.97325136   2.34311854 -10.56495743
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995061   4.98513713 -11.8360974   21.57457363  -7.27955623
   5.11757491  -4.39409784  24.41493727  13.95025415   1.56630503
   1.06172558 -16.82526464  -2.37084712   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690274  50.93964676  20.79585634
 -10.6914453   -1.40680272  -3.43213017   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961842  -0.5765082   -4.050751    -1.46689933
  -0.80107704   8.98228099  -3.1108763   -1.49498387  24.91311227]
16.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.238475843586
gradient value of function right now is: [ 9.67551103e-09  8.39311784e-28  9.67226099e-09  3.09035804e-28
 -9.67535183e-09 -8.39268766e-28 -9.67210194e-09 -3.09024650e-28
  4.14780290e-13  4.36170370e-13  4.22009078e-13  4.33051010e-13
 -2.00706231e-27 -2.18172855e-27 -2.06387170e-27 -2.16464896e-27
  5.32105585e-12  5.91260897e-12  5.50552418e-12  5.81991050e-12
 -4.16095108e-29 -1.45707533e-27 -1.23702997e-28 -3.08241701e-27
 -1.23803232e-19 -6.42498708e-14  2.19473829e-19  2.94596887e-21
  3.01133767e-16  1.14105661e-13  4.16861774e-17  1.08860785e-17
  3.67019778e-19 -4.76653089e-14  1.83148684e-18  7.44806891e-20
  2.29717935e-17  2.12111826e-13 -4.41864789e-17 -5.03603868e-19
 -4.01108928e-16 -4.62972495e-17 -4.95202759e-15 -4.09190955e-17
  3.12869595e-12  3.49635811e-12 -8.05965007e-15  3.49657614e-12
  1.14719020e-17  1.33378110e-17 -9.72519909e-17  1.23006667e-17
  4.50549471e-18  8.06571923e-19  4.95995787e-17  7.24883654e-19
 -1.06788705e-12 -3.97281163e-11  1.53696999e-15  1.30024253e-13
  3.87314563e-18  1.45683398e-18  5.35889707e-16  3.52185806e-14
 -8.41307139e-01]
supnorm grad right now is: 0.8413071387064325
Weights right now are: 
[-11.35245028   2.48912124  -1.68409912   5.35122029  10.82097996
  -2.62311668   1.84880611  -5.91727641   6.50630174 -32.95365447
   9.92092184  -1.08549376   2.54190933  44.99864605   6.79118805
  26.8097292    3.73378366  -4.97325253   2.34311745 -10.56495858
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995063   4.98513713 -11.8360974   21.57457363  -7.27955626
   5.11757491  -4.39409784  24.41493727  13.95025416   1.56630503
   1.06172558 -16.82526464  -2.37084717   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690274  50.93964676  20.79585571
 -10.691446    -1.40680272  -3.43213087   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961864  -0.57650022  -4.050751    -1.46689936
  -0.80107704   8.98228099  -3.1108763   -1.49498387  24.92501688]
18.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.109023563191
gradient value of function right now is: [ 9.42578618e-09  8.32924918e-28  9.42256111e-09  3.05075387e-28
 -9.42597306e-09 -8.32973840e-28 -9.42274781e-09 -3.05065201e-28
  4.11721377e-13  4.32952912e-13  4.18896551e-13  4.29856853e-13
 -1.99190102e-27 -2.16524381e-27 -2.04827858e-27 -2.14830410e-27
  5.28655068e-12  5.87353781e-12  5.46979308e-12  5.78159050e-12
 -4.11697960e-29 -1.42616359e-27 -1.21198628e-28 -3.05237286e-27
 -1.40600532e-19 -6.38248147e-14  2.22998551e-19  1.84209337e-21
  3.01728618e-16  1.13274371e-13  4.22007070e-17  1.09774108e-17
  2.61171628e-19 -4.73367822e-14  1.86194068e-18  6.76188409e-20
  2.90361141e-17  2.10201755e-13 -4.16532756e-17 -4.34887012e-20
 -3.90363347e-16 -4.32187430e-17 -4.86667327e-15 -3.81920320e-17
  3.10771200e-12  3.47302105e-12 -9.05771155e-15  3.47324935e-12
  1.08616586e-17  1.35812411e-17 -1.06120709e-16  1.25530440e-17
  4.30964029e-18  7.53206466e-19  4.80562094e-17  6.77901377e-19
 -1.06062138e-12 -3.94708630e-11  1.44165850e-15  1.31367026e-13
  3.79898821e-18  1.56339966e-18  5.00888101e-16  3.58480381e-14
  2.08174783e+00]
supnorm grad right now is: 2.081747831087398
Weights right now are: 
[-11.35436173   2.48912124  -1.68600993   5.35122029  10.82289143
  -2.62311668   1.85071694  -5.91727641   6.50630166 -32.95365455
   9.92092176  -1.08549385   2.54190933  44.99864605   6.79118805
  26.8097292    3.73378261  -4.97325369   2.34311637 -10.56495972
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995064   4.98513713 -11.8360974   21.57457363  -7.27955628
   5.11757491  -4.39409784  24.41493727  13.95025416   1.56630503
   1.06172558 -16.82526464  -2.37084721   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690274  50.93964676  20.7958551
 -10.69144669  -1.40680271  -3.43213155   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961884  -0.57649248  -4.050751    -1.46689939
  -0.80107704   8.98228099  -3.1108763   -1.49498388  25.17050034]
20.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.24043301451
gradient value of function right now is: [ 9.52685943e-09  8.26623202e-28  9.52365856e-09  3.04602228e-28
 -9.52402388e-09 -8.26584084e-28 -9.52082316e-09 -3.04609049e-28
  4.08649455e-13  4.29723278e-13  4.15771337e-13  4.26650258e-13
 -1.97671321e-27 -2.14874351e-27 -2.03266309e-27 -2.13194041e-27
  5.25187380e-12  5.83571194e-12  5.43394140e-12  5.74422341e-12
 -4.09800339e-29 -1.43564037e-27 -1.21813200e-28 -3.03870295e-27
 -1.22494548e-19 -6.34062424e-14  2.16699811e-19  2.89386999e-21
  2.97129420e-16  1.12579979e-13  4.11345991e-17  1.07424206e-17
  3.60976191e-19 -4.70331500e-14  1.80828663e-18  7.34549247e-20
  2.27674472e-17  2.09336803e-13 -4.35723796e-17 -4.90578777e-19
 -3.95628879e-16 -4.56434548e-17 -4.88462838e-15 -4.03415636e-17
  3.08738405e-12  3.45022108e-12 -7.97958392e-15  3.45043656e-12
  1.13150468e-17  1.31647556e-17 -9.60399921e-17  1.21414254e-17
  4.44335553e-18  7.95330132e-19  4.89180979e-17  7.14800611e-19
 -1.05384884e-12 -3.92069026e-11  1.51404450e-15  1.28359748e-13
  3.82174606e-18  1.44510502e-18  5.27922087e-16  3.47730300e-14
 -8.02433921e-01]
supnorm grad right now is: 0.8024339211188397
Weights right now are: 
[-11.35626195   2.48912124  -1.68790952   5.35122029  10.82479162
  -2.62311668   1.85261649  -5.91727641   6.50630158 -32.95365464
   9.92092168  -1.08549393   2.54190933  44.99864605   6.79118805
  26.8097292    3.73378157  -4.97325485   2.34311529 -10.56496087
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995065   4.98513713 -11.8360974   21.57457363  -7.2795563
   5.11757491  -4.39409784  24.41493727  13.95025417   1.56630503
   1.06172558 -16.82526464  -2.37084725   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690274  50.93964676  20.79585449
 -10.69144737  -1.40680271  -3.43213223   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961905  -0.57648484  -4.050751    -1.46689941
  -0.80107704   8.98228099  -3.1108763   -1.49498389  24.93718184]
22.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.061620387482
gradient value of function right now is: [ 9.55274535e-09  8.20465871e-28  9.54956822e-09  3.03983230e-28
 -9.56209403e-09 -8.20419744e-28 -9.55891706e-09 -3.03968441e-28
  4.05671063e-13  4.26591316e-13  4.12741035e-13  4.23540785e-13
 -1.96197490e-27 -2.13272560e-27 -2.01750763e-27 -2.11605577e-27
  5.21821327e-12  5.79870598e-12  5.39912516e-12  5.70772351e-12
 -4.07500554e-29 -1.43976714e-27 -1.21788869e-28 -3.02763991e-27
 -1.13035599e-19 -6.29969751e-14  2.10047742e-19  3.33569615e-21
  2.94011906e-16  1.11876628e-13  4.06067427e-17  1.05970392e-17
  4.11649289e-19 -4.67321340e-14  1.75535557e-18  7.56091708e-20
  1.92553634e-17  2.08292107e-13 -4.40425704e-17 -7.19294982e-19
 -3.97794303e-16 -4.68494965e-17 -4.88542283e-15 -4.14077219e-17
  3.06743895e-12  3.42788688e-12 -7.34427057e-15  3.42809484e-12
  1.12021232e-17  1.27746225e-17 -9.22241352e-17  1.17679517e-17
  4.49872974e-18  8.11938066e-19  4.92488379e-17  7.29100370e-19
 -1.04714045e-12 -3.89506591e-11  1.54841528e-15  1.26343318e-13
  3.82841162e-18  1.37630451e-18  5.41032439e-16  3.40767679e-14
 -2.36547807e+00]
supnorm grad right now is: 2.3654780717998274
Weights right now are: 
[-11.35813272   2.48912124  -1.6897797    5.35122029  10.82666245
  -2.62311668   1.85448673  -5.91727641   6.5063015  -32.95365472
   9.9209216   -1.08549401   2.54190933  44.99864605   6.79118805
  26.8097292    3.7337806   -4.97325593   2.34311428 -10.56496193
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995066   4.98513713 -11.8360974   21.57457363  -7.27955632
   5.11757491  -4.39409784  24.41493727  13.95025418   1.56630503
   1.06172558 -16.82526464  -2.37084729   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690274  50.93964676  20.79585392
 -10.69144801  -1.40680271  -3.43213287   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961925  -0.57647751  -4.050751    -1.46689944
  -0.80107704   8.98228099  -3.1108763   -1.49498389  24.79952173]
24.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.21249281338
gradient value of function right now is: [ 9.26253547e-09  8.14326090e-28  9.25938239e-09  2.98831985e-28
 -9.26963676e-09 -8.14329285e-28 -9.26648367e-09 -2.98817553e-28
  4.02717487e-13  4.23484936e-13  4.09735807e-13  4.20456794e-13
 -1.94735077e-27 -2.11682719e-27 -2.00246802e-27 -2.10029101e-27
  5.18478949e-12  5.76071455e-12  5.36451333e-12  5.67048718e-12
 -4.02927061e-29 -1.40073579e-27 -1.19051170e-28 -2.98636852e-27
 -1.33678935e-19 -6.25862461e-14  2.14734022e-19  1.99932988e-21
  2.94779598e-16  1.11062066e-13  4.12452698e-17  1.07109483e-17
  2.77904617e-19 -4.64127787e-14  1.79659055e-18  6.72802194e-20
  2.70683256e-17  2.06326246e-13 -4.08624786e-17 -1.35220265e-19
 -3.83778208e-16 -4.28574975e-17 -4.77468102e-15 -3.78734240e-17
  3.04713191e-12  3.40532311e-12 -8.49909339e-15  3.40554314e-12
  1.04428731e-17  1.30677328e-17 -1.03322614e-16  1.20727561e-17
  4.24965935e-18  7.45074350e-19  4.72810624e-17  6.70292014e-19
 -1.04009541e-12 -3.87034808e-11  1.43959880e-15  1.28089334e-13
  3.73082879e-18  1.50712502e-18  5.00140433e-16  3.48977505e-14
  1.26558153e+00]
supnorm grad right now is: 1.2655815335828693
Weights right now are: 
[-11.36001213   2.48912124  -1.69165847   5.35122029  10.82854185
  -2.62311668   1.85636549  -5.91727641   6.50630142 -32.95365481
   9.92092152  -1.0854941    2.54190933  44.99864605   6.79118805
  26.8097292    3.73377954  -4.9732571    2.34311319 -10.56496308
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995068   4.98513713 -11.8360974   21.57457363  -7.27955634
   5.11757491  -4.39409784  24.41493727  13.95025419   1.56630503
   1.06172558 -16.82526464  -2.37084733   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690274  50.93964676  20.7958533
 -10.6914487   -1.40680271  -3.43213356   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961945  -0.57646974  -4.050751    -1.46689946
  -0.80107704   8.98228099  -3.1108763   -1.4949839   25.08870323]
26.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.090746659491
gradient value of function right now is: [ 9.13976703e-09  8.08287446e-28  9.13663743e-09  2.96367778e-28
 -9.15020451e-09 -8.08336758e-28 -9.14707473e-09 -2.96368218e-28
  3.99811078e-13  4.20428251e-13  4.06778578e-13  4.17422202e-13
 -1.93296323e-27 -2.10118734e-27 -1.98767180e-27 -2.08478392e-27
  5.15186596e-12  5.72385496e-12  5.33043482e-12  5.63426035e-12
 -3.99428313e-29 -1.38369207e-27 -1.17483417e-28 -2.96658897e-27
 -1.37828236e-19 -6.21828540e-14  2.18398869e-19  1.77497845e-21
  2.93945236e-16  1.10305803e-13  4.10818663e-17  1.06965289e-17
  2.52194288e-19 -4.61064607e-14  1.82234167e-18  6.59077014e-20
  2.85553672e-17  2.04818272e-13 -4.06577305e-17 -2.90316172e-20
 -3.79957308e-16 -4.20346418e-17 -4.73719974e-15 -3.71468530e-17
  3.02731958e-12  3.38324147e-12 -8.90089936e-15  3.38346458e-12
  1.06792836e-17  1.32977447e-17 -1.03044794e-16  1.22917272e-17
  4.19491641e-18  7.33561606e-19  4.67702758e-17  6.60285700e-19
 -1.03330936e-12 -3.84566885e-11  1.39970108e-15  1.28106558e-13
  3.70085634e-18  1.50135990e-18  4.86349724e-16  3.49688037e-14
  2.21211376e+00]
supnorm grad right now is: 2.2121137601688825
Weights right now are: 
[-11.36187451   2.48912124  -1.6935202    5.35122029  10.83040422
  -2.62311668   1.85822722  -5.91727641   6.50630134 -32.95365489
   9.92092143  -1.08549418   2.54190933  44.99864605   6.79118805
  26.8097292    3.7337785   -4.97325826   2.34311211 -10.56496423
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995069   4.98513713 -11.8360974   21.57457363  -7.27955637
   5.11757491  -4.39409784  24.41493727  13.9502542    1.56630503
   1.06172558 -16.82526464  -2.37084737   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690275  50.93964676  20.79585269
 -10.69144938  -1.40680271  -3.43213425   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961968  -0.57646176  -4.050751    -1.46689949
  -0.80107704   8.98228099  -3.1108763   -1.49498391  25.19390534]
28.000000000000004% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.225663824867
gradient value of function right now is: [ 9.26262670e-09  8.02312703e-28  9.25952001e-09  2.96478313e-28
 -9.26866990e-09 -8.02361868e-28 -9.26556303e-09 -2.96467667e-28
  3.96921508e-13  4.17389586e-13  4.03838630e-13  4.14405312e-13
 -1.91867090e-27 -2.08565298e-27 -1.97297478e-27 -2.06937876e-27
  5.11910558e-12  5.68823157e-12  5.29656124e-12  5.59904485e-12
 -3.97898241e-29 -1.39796530e-27 -1.18381080e-28 -2.95723690e-27
 -1.18216000e-19 -6.17852908e-14  2.10247122e-19  2.88791842e-21
  2.89234332e-16  1.09661089e-13  4.00335842e-17  1.04523468e-17
  3.59969320e-19 -4.58201959e-14  1.75504577e-18  7.20165529e-20
  2.17238523e-17  2.04093293e-13 -4.25223462e-17 -5.10807339e-19
 -3.86030117e-16 -4.46779507e-17 -4.76188947e-15 -3.94873377e-17
  3.00805586e-12  3.36160643e-12 -7.64872683e-15  3.36181502e-12
  1.09939813e-17  1.27649949e-17 -9.31775787e-17  1.17704059e-17
  4.34053068e-18  7.77948549e-19  4.77383457e-17  6.99072708e-19
 -1.02690025e-12 -3.82041961e-11  1.48481867e-15  1.24816797e-13
  3.72841834e-18  1.38915147e-18  5.17756184e-16  3.37901763e-14
 -1.05884048e+00]
supnorm grad right now is: 1.0588404761880856
Weights right now are: 
[-11.36373061   2.48912124  -1.69537566   5.35122029  10.83226032
  -2.62311668   1.86008269  -5.91727641   6.50630125 -32.95365498
   9.92092135  -1.08549427   2.54190933  44.99864605   6.79118805
  26.8097292    3.73377744  -4.97325944   2.34311102 -10.56496538
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.9299507    4.98513713 -11.8360974   21.57457363  -7.27955639
   5.11757491  -4.39409784  24.41493727  13.95025421   1.56630503
   1.06172558 -16.82526464  -2.37084741   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690275  50.93964676  20.79585207
 -10.69145007  -1.40680271  -3.43213493   1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54961989  -0.57645393  -4.050751    -1.46689951
  -0.80107704   8.98228099  -3.1108763   -1.49498392  24.9232435 ]
30.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5143.942595260404
gradient value of function right now is: [ 8.95665833e-09  7.96471587e-28  8.95357459e-09  2.91234888e-28
 -8.96075802e-09 -7.96516996e-28 -8.95767412e-09 -2.91223127e-28
  3.94094851e-13  4.14417200e-13  4.00962690e-13  4.11454347e-13
 -1.90469200e-27 -2.07046114e-27 -1.95860010e-27 -2.05431503e-27
  5.08701908e-12  5.65155423e-12  5.26333118e-12  5.56314117e-12
 -3.93103288e-29 -1.35404904e-27 -1.15177437e-28 -2.91890172e-27
 -1.40224989e-19 -6.13920910e-14  2.19413002e-19  1.55038509e-21
  2.90875636e-16  1.08852379e-13  4.06408272e-17  1.05981882e-17
  2.25597436e-19 -4.55106228e-14  1.82664403e-18  6.39568137e-20
  2.97046554e-17  2.02054407e-13 -3.99782834e-17  7.36628896e-20
 -3.73158371e-16 -4.08494322e-17 -4.66482816e-15 -3.61029876e-17
  2.98854528e-12  3.33997871e-12 -9.21304578e-15  3.34020333e-12
  1.07312298e-17  1.33607285e-17 -1.02558610e-16  1.23562508e-17
  4.10644184e-18  7.15181294e-19  4.59082533e-17  6.44095080e-19
 -1.02009477e-12 -3.79705356e-11  1.35134077e-15  1.27283244e-13
  3.64541750e-18  1.55500618e-18  4.69406244e-16  3.48176068e-14
  3.05197272e+00]
supnorm grad right now is: 3.0519727229092206
Weights right now are: 
[-11.36555832   2.48912124  -1.69720276   5.35122029  10.83408806
  -2.62311668   1.86190982  -5.91727641   6.50630118 -32.95365506
   9.92092127  -1.08549435   2.54190933  44.99864605   6.79118805
  26.8097292    3.73377644  -4.97326055   2.34310998 -10.56496648
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995071   4.98513713 -11.8360974   21.57457363  -7.27955641
   5.11757491  -4.39409784  24.41493727  13.95025422   1.56630503
   1.06172558 -16.82526464  -2.37084745   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690275  50.93964676  20.79585147
 -10.69145074  -1.4068027   -3.4321356    1.75417885   9.34438579
   3.02523015  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962009  -0.57644629  -4.050751    -1.46689954
  -0.80107704   8.98228099  -3.1108763   -1.49498392  25.17880896]
32.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.26400570307
gradient value of function right now is: [ 9.05969607e-09  7.90628789e-28  9.05663474e-09  2.91442580e-28
 -9.06247592e-09 -7.90669558e-28 -9.05941444e-09 -2.91435816e-28
  3.91266537e-13  4.11443025e-13  3.98085092e-13  4.08501476e-13
 -1.89070919e-27 -2.05526397e-27 -1.94422160e-27 -2.03924424e-27
  5.05488979e-12  5.61659898e-12  5.23011072e-12  5.52858872e-12
 -3.91614470e-29 -1.36908065e-27 -1.16055735e-28 -2.91058477e-27
 -1.24265093e-19 -6.10025206e-14  2.08595784e-19  2.36187151e-21
  2.86501880e-16  1.08223600e-13  3.98873867e-17  1.03843969e-17
  3.10551085e-19 -4.52301088e-14  1.74375467e-18  6.82192130e-20
  2.41663267e-17  2.01324940e-13 -4.08426350e-17 -3.02717833e-19
 -3.77261358e-16 -4.28617953e-17 -4.67488631e-15 -3.78797652e-17
  2.96966754e-12  3.31877189e-12 -7.95994880e-15  3.31898253e-12
  1.05019117e-17  1.26818500e-17 -9.66274894e-17  1.17057289e-17
  4.20844710e-18  7.45963358e-19  4.65637871e-17  6.70722959e-19
 -1.01381229e-12 -3.77228944e-11  1.43124116e-15  1.24115206e-13
  3.66913803e-18  1.46418612e-18  4.98162125e-16  3.37171671e-14
  8.59372870e-02]
supnorm grad right now is: 0.08593728700063535
Weights right now are: 
[-11.36740037   2.48912124  -1.69904417   5.35122029  10.83593013
  -2.62311668   1.86375125  -5.91727641   6.50630109 -32.95365514
   9.92092119  -1.08549443   2.54190933  44.99864605   6.79118805
  26.8097292    3.73377538  -4.97326173   2.34310888 -10.56496764
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995073   4.98513713 -11.8360974   21.57457363  -7.27955643
   5.11757491  -4.39409784  24.41493727  13.95025423   1.56630503
   1.06172558 -16.82526464  -2.3708475    2.51369865   4.22064502
  -4.83421262  38.329907     1.17690275  50.93964676  20.79585086
 -10.69145143  -1.4068027   -3.43213629   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.5496203   -0.57643856  -4.050751    -1.46689956
  -0.80107704   8.98228099  -3.1108763   -1.49498393  24.99963448]
34.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5143.619316868938
gradient value of function right now is: [ 8.75291378e-09  7.84012267e-28  8.74987955e-09  2.86421893e-28
 -8.76023674e-09 -7.83935740e-28 -8.75720278e-09 -2.86419573e-28
  3.88433362e-13  4.08427700e-13  3.95190679e-13  4.05513278e-13
 -1.87532707e-27 -2.03772165e-27 -1.92813168e-27 -2.02197910e-27
  5.01732458e-12  5.57112583e-12  5.19029582e-12  5.48440084e-12
 -3.86728146e-29 -1.32714354e-27 -1.12909564e-28 -2.87400559e-27
 -1.45648935e-19 -6.02391282e-14  2.18375767e-19  1.05146504e-21
  2.88053180e-16  1.06855516e-13  4.04418581e-17  1.05245988e-17
  1.78498069e-19 -4.46668272e-14  1.81633433e-18  6.01366347e-20
  3.18063341e-17  1.98188640e-13 -3.83874192e-17  2.64185878e-19
 -3.64879038e-16 -3.91266267e-17 -4.58152628e-15 -3.45763406e-17
  2.93388279e-12  3.27808664e-12 -9.43960476e-15  3.27831225e-12
  1.03047097e-17  1.32861487e-17 -1.05529309e-16  1.22999967e-17
  3.98031133e-18  6.84035628e-19  4.47832362e-17  6.16474855e-19
 -9.94546203e-13 -3.71300642e-11  1.30576037e-15  1.26414886e-13
  3.52872585e-18  1.57436457e-18  4.52417373e-16  3.46848929e-14
  4.36264843e+00]
supnorm grad right now is: 4.362648431372889
Weights right now are: 
[-11.36918924   2.48912124  -1.70083244   5.35122029  10.83771906
  -2.62311668   1.86553957  -5.91727641   6.50630102 -32.95365523
   9.92092111  -1.08549451   2.54190933  44.99864605   6.79118805
  26.8097292    3.73377438  -4.97326284   2.34310785 -10.56496873
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995074   4.98513713 -11.8360974   21.57457363  -7.27955645
   5.11757491  -4.39409784  24.41493727  13.95025424   1.56630503
   1.06172558 -16.82526464  -2.37084754   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690275  50.93964676  20.79585028
 -10.69145207  -1.4068027   -3.43213694   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962049  -0.5764314   -4.050751    -1.46689959
  -0.80107704   8.98228099  -3.1108763   -1.49498394  25.28865058]
36.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.261274066033
gradient value of function right now is: [ 8.91706317e-09  7.79397122e-28  8.91404539e-09  2.87266945e-28
 -8.92528043e-09 -7.79437182e-28 -8.92226249e-09 -2.87270184e-28
  3.85831304e-13  4.05727420e-13  3.92555084e-13  4.02826939e-13
 -1.86384028e-27 -2.02606116e-27 -1.91659179e-27 -2.01028510e-27
  4.99304202e-12  5.54781438e-12  5.16611548e-12  5.46089462e-12
 -3.85931948e-29 -1.34782231e-27 -1.14255461e-28 -2.87006394e-27
 -1.23745830e-19 -6.02487704e-14  2.07038827e-19  2.28743942e-21
  2.83061339e-16  1.06857189e-13  3.94003392e-17  1.02628061e-17
  3.00933796e-19 -4.46649136e-14  1.72956500e-18  6.71335336e-20
  2.42558044e-17  1.98815576e-13 -4.02959679e-17 -2.73050912e-19
 -3.71986108e-16 -4.21484404e-17 -4.61268659e-15 -3.72516167e-17
  2.93276044e-12  3.27756173e-12 -7.96598360e-15  3.27777083e-12
  1.04287015e-17  1.25849914e-17 -9.55715086e-17  1.16181930e-17
  4.14670841e-18  7.34341449e-19  4.59083949e-17  6.60386850e-19
 -1.00126243e-12 -3.72577646e-11  1.40582702e-15  1.22775863e-13
  3.60208086e-18  1.38777156e-18  4.89234613e-16  3.33736401e-14
  3.20477351e-01]
supnorm grad right now is: 0.32047735106881403
Weights right now are: 
[-11.37098049   2.48912124  -1.70262308   5.35122029  10.83951029
  -2.62311668   1.8673302   -5.91727641   6.50630094 -32.95365531
   9.92092103  -1.0854946    2.54190933  44.99864605   6.79118805
  26.8097292    3.73377338  -4.97326395   2.34310681 -10.56496982
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995075   4.98513713 -11.8360974   21.57457363  -7.27955648
   5.11757491  -4.39409784  24.41493727  13.95025425   1.56630503
   1.06172558 -16.82526464  -2.37084757   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690275  50.93964676  20.79584969
 -10.69145273  -1.4068027   -3.43213759   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962068  -0.57642415  -4.05075101  -1.46689961
  -0.80107704   8.98228099  -3.11087631  -1.49498394  24.99483193]
38.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.060159915455
gradient value of function right now is: [ 9.00977550e-09  7.73833837e-28  9.00677908e-09  2.87689012e-28
 -9.01096833e-09 -7.73835941e-28 -9.00797190e-09 -2.87680114e-28
  3.83126878e-13  4.02883873e-13  3.89803631e-13  4.00003772e-13
 -1.85048276e-27 -2.01154686e-27 -1.90285712e-27 -1.99589169e-27
  4.96222789e-12  5.51419837e-12  5.13425419e-12  5.42768649e-12
 -3.84346905e-29 -1.36091092e-27 -1.14829035e-28 -2.86665395e-27
 -1.07686737e-19 -5.98759147e-14  2.00125122e-19  3.17808159e-21
  2.79186259e-16  1.06240514e-13  3.85333952e-17  1.00628381e-17
  3.91867563e-19 -4.43947661e-14  1.67125060e-18  7.19893005e-20
  1.83225520e-17  1.98049092e-13 -4.19043591e-17 -6.84218537e-19
 -3.77812709e-16 -4.45056481e-17 -4.63860507e-15 -3.93369389e-17
  2.91466252e-12  3.25725771e-12 -6.99438518e-15  3.25745542e-12
  1.06944771e-17  1.21589732e-17 -8.72460159e-17  1.12004946e-17
  4.27431049e-18  7.71887369e-19  4.67711199e-17  6.93149147e-19
 -9.95222644e-13 -3.70222625e-11  1.47040797e-15  1.20073084e-13
  3.63883143e-18  1.30709759e-18  5.13789376e-16  3.23884139e-14
 -2.36929705e+00]
supnorm grad right now is: 2.36929704963218
Weights right now are: 
[-11.37277298   2.48912124  -1.70441496   5.35122029  10.84130307
  -2.62311668   1.86912237  -5.91727641   6.50630086 -32.95365539
   9.92092095  -1.08549468   2.54190933  44.99864605   6.79118805
  26.8097292    3.73377236  -4.97326508   2.34310576 -10.56497093
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995076   4.98513713 -11.8360974   21.57457363  -7.2795565
   5.11757491  -4.39409784  24.41493727  13.95025426   1.56630503
   1.06172558 -16.82526464  -2.37084762   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690275  50.93964676  20.7958491
 -10.69145339  -1.4068027   -3.43213826   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962087  -0.5764167   -4.05075101  -1.46689964
  -0.80107704   8.98228099  -3.11087631  -1.49498395  24.83834726]
40.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.058367574791
gradient value of function right now is: [ 8.67942119e-09  7.68443884e-28  8.67644599e-09  2.82441926e-28
 -8.68751826e-09 -7.68441187e-28 -8.68454307e-09 -2.82438157e-28
  3.80516039e-13  4.00138457e-13  3.87147280e-13  3.97278132e-13
 -1.83758228e-27 -1.99752684e-27 -1.88959140e-27 -1.98198934e-27
  4.93244211e-12  5.48002420e-12  5.10340319e-12  5.39425467e-12
 -3.79662648e-29 -1.31654389e-27 -1.11566247e-28 -2.82872119e-27
 -1.33021824e-19 -5.95107801e-14  2.10091146e-19  1.65789067e-21
  2.81145206e-16  1.05482013e-13  3.92725421e-17  1.02345343e-17
  2.36362166e-19 -4.41056894e-14  1.75130832e-18  6.29074294e-20
  2.76860573e-17  1.96060081e-13 -3.88559243e-17 -4.40076444e-21
 -3.62901444e-16 -4.00439732e-17 -4.52509665e-15 -3.53907207e-17
  2.89652871e-12  3.23717055e-12 -8.58802100e-15  3.23738470e-12
  1.02728028e-17  1.27727263e-17 -9.85433978e-17  1.18077097e-17
  4.00458310e-18  6.99771605e-19  4.46515408e-17  6.29980694e-19
 -9.88878366e-13 -3.68061532e-11  1.33430040e-15  1.22712566e-13
  3.54270313e-18  1.43832874e-18  4.63436239e-16  3.35146253e-14
  2.38239807e+00]
supnorm grad right now is: 2.382398070557354
Weights right now are: 
[-11.37452179   2.48912124  -1.7061632    5.35122029  10.84305203
  -2.62311668   1.87087074  -5.91727641   6.50630079 -32.95365547
   9.92092088  -1.08549475   2.54190933  44.99864605   6.79118805
  26.8097292    3.73377141  -4.97326614   2.34310477 -10.56497198
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995077   4.98513713 -11.8360974   21.57457363  -7.27955652
   5.11757491  -4.39409784  24.41493727  13.95025426   1.56630503
   1.06172558 -16.82526464  -2.37084765   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690275  50.93964676  20.79584853
 -10.69145403  -1.4068027   -3.43213889   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962106  -0.57640959  -4.05075101  -1.46689966
  -0.80107704   8.98228099  -3.11087631  -1.49498396  25.12542259]
42.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.132141581815
gradient value of function right now is: [ 8.64444726e-09  7.63102175e-28  8.64149272e-09  2.81169094e-28
 -8.64240722e-09 -7.63122813e-28 -8.63945260e-09 -2.81158679e-28
  3.77936418e-13  3.97425600e-13  3.84522622e-13  3.94584790e-13
 -1.82483384e-27 -1.98366945e-27 -1.87648143e-27 -1.96824769e-27
  4.90298872e-12  5.44740610e-12  5.07293024e-12  5.36212487e-12
 -3.77290958e-29 -1.31316249e-27 -1.11096259e-28 -2.81426566e-27
 -1.30191570e-19 -5.91516795e-14  2.06417214e-19  1.73216185e-21
  2.79105467e-16  1.04848050e-13  3.90189587e-17  1.01535230e-17
  2.46127529e-19 -4.38389189e-14  1.72298667e-18  6.29333681e-20
  2.67218588e-17  1.94967865e-13 -3.86157644e-17 -5.47163509e-20
 -3.61652222e-16 -4.01152681e-17 -4.50423468e-15 -3.54503829e-17
  2.87898863e-12  3.21755586e-12 -8.32793859e-15  3.21776668e-12
  1.00564176e-17  1.25510296e-17 -9.78683573e-17  1.15995266e-17
  3.99601326e-18  6.99183113e-19  4.45104056e-17  6.29235515e-19
 -9.82937270e-13 -3.65828721e-11  1.34023785e-15  1.21577027e-13
  3.52299937e-18  1.44422087e-18  4.65624790e-16  3.31684077e-14
  1.91197311e+00]
supnorm grad right now is: 1.9119731050116089
Weights right now are: 
[-11.37626668   2.48912124  -1.70790749   5.35122029  10.84479753
  -2.62311668   1.87261565  -5.91727641   6.50630071 -32.95365555
   9.9209208   -1.08549483   2.54190933  44.99864605   6.79118805
  26.8097292    3.73377042  -4.97326723   2.34310376 -10.56497305
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995079   4.98513713 -11.8360974   21.57457363  -7.27955654
   5.11757491  -4.39409784  24.41493727  13.95025427   1.56630503
   1.06172558 -16.82526464  -2.37084769   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690275  50.93964676  20.79584797
 -10.69145466  -1.40680269  -3.43213952   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962125  -0.57640243  -4.05075101  -1.46689968
  -0.80107704   8.98228099  -3.11087631  -1.49498396  25.14834467]
44.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.174901345957
gradient value of function right now is: [ 8.60026913e-09  7.57759471e-28  8.59733526e-09  2.79774113e-28
 -8.60961054e-09 -7.57796844e-28 -8.60667654e-09 -2.79779510e-28
  3.75354245e-13  3.94710052e-13  3.81895365e-13  3.91888783e-13
 -1.81207461e-27 -1.96980055e-27 -1.86336038e-27 -1.95449501e-27
  4.87347208e-12  5.41470070e-12  5.04239108e-12  5.32991474e-12
 -3.74868422e-29 -1.30866340e-27 -1.10560364e-28 -2.79912049e-27
 -1.28248839e-19 -5.87918161e-14  2.03215552e-19  1.76834384e-21
  2.77236566e-16  1.04209827e-13  3.88000445e-17  1.00818236e-17
  2.52289262e-19 -4.35711345e-14  1.69893885e-18  6.28396357e-20
  2.61101971e-17  1.93861332e-13 -3.82952552e-17 -8.40217274e-20
 -3.60103984e-16 -4.00795956e-17 -4.48101000e-15 -3.54189668e-17
  2.86140319e-12  3.19789570e-12 -8.11793320e-15  3.19810375e-12
  9.83940568e-18  1.23514707e-17 -9.75680141e-17  1.14133172e-17
  3.98208229e-18  6.97151877e-19  4.43212502e-17  6.27291596e-19
 -9.76974682e-13 -3.63594365e-11  1.34192081e-15  1.20542263e-13
  3.50536077e-18  1.42358331e-18  4.66239556e-16  3.28606012e-14
  1.61990541e+00]
supnorm grad right now is: 1.6199054107939943
Weights right now are: 
[-11.37802424   2.48912124  -1.70966442   5.35122029  10.84655477
  -2.62311668   1.87437227  -5.91727641   6.50630063 -32.95365563
   9.92092072  -1.08549492   2.54190933  44.99864605   6.79118805
  26.8097292    3.73376938  -4.97326839   2.34310268 -10.56497419
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.9299508    4.98513713 -11.8360974   21.57457363  -7.27955656
   5.11757491  -4.39409784  24.41493727  13.95025428   1.56630503
   1.06172558 -16.82526464  -2.37084773   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690275  50.93964676  20.79584738
 -10.69145531  -1.40680269  -3.43214017   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962147  -0.57639482  -4.05075101  -1.46689971
  -0.80107704   8.98228099  -3.11087631  -1.49498397  25.05607835]
46.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.252040333316
gradient value of function right now is: [ 8.66006894e-09  7.52524014e-28  8.65715523e-09  2.78594599e-28
 -8.66077039e-09 -7.52574038e-28 -8.65785649e-09 -2.78581545e-28
  3.72822423e-13  3.92047468e-13  3.79319362e-13  3.89245285e-13
 -1.79956764e-27 -1.95620527e-27 -1.85049889e-27 -1.94101246e-27
  4.84450674e-12  5.38294328e-12  5.01243191e-12  5.29857321e-12
 -3.72959470e-29 -1.30934275e-27 -1.10698128e-28 -2.78183896e-27
 -1.14658026e-19 -5.84394683e-14  2.01083225e-19  2.60046428e-21
  2.73588077e-16  1.03610232e-13  3.78580734e-17  9.89702239e-18
  3.25432690e-19 -4.33135225e-14  1.67607337e-18  6.74484330e-20
  2.15456037e-17  1.93031334e-13 -4.00637122e-17 -4.15836544e-19
 -3.63443453e-16 -4.17898476e-17 -4.48861390e-15 -3.69374879e-17
  2.84427071e-12  3.17870051e-12 -7.46040715e-15  3.17890009e-12
  1.04852929e-17  1.21962959e-17 -8.84961482e-17  1.12501549e-17
  4.07898635e-18  7.29510587e-19  4.49158300e-17  6.55768208e-19
 -9.71222454e-13 -3.61382456e-11  1.38721124e-15  1.18465046e-13
  3.52304022e-18  1.35333856e-18  4.83486757e-16  3.21204842e-14
 -5.69095076e-01]
supnorm grad right now is: 0.569095076456688
Weights right now are: 
[-11.37975911   2.48912124  -1.7113987    5.35122029  10.84828965
  -2.62311668   1.87610654  -5.91727641   6.50630055 -32.95365572
   9.92092064  -1.085495     2.54190933  44.99864605   6.79118805
  26.8097292    3.73376837  -4.9732695    2.34310164 -10.56497529
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995081   4.98513713 -11.8360974   21.57457363  -7.27955658
   5.11757491  -4.39409784  24.41493727  13.95025429   1.56630503
   1.06172558 -16.82526464  -2.37084777   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690276  50.93964676  20.79584681
 -10.69145595  -1.40680269  -3.43214081   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962166  -0.57638762  -4.05075101  -1.46689973
  -0.80107704   8.98228099  -3.11087631  -1.49498397  24.97418557]
48.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.107533066629
gradient value of function right now is: [ 8.45674541e-09  7.47393834e-28  8.45385175e-09  2.75483243e-28
 -8.45219952e-09 -7.47434183e-28 -8.44930571e-09 -2.75486188e-28
  3.70334066e-13  3.89430917e-13  3.76787651e-13  3.86647577e-13
 -1.78728189e-27 -1.94285429e-27 -1.83786558e-27 -1.92777359e-27
  4.81601155e-12  5.35070139e-12  4.98293258e-12  5.26694704e-12
 -3.69423588e-29 -1.28490951e-27 -1.08674531e-28 -2.75859922e-27
 -1.28518890e-19 -5.80916163e-14  2.04003058e-19  1.68929707e-21
  2.74158441e-16  1.02931443e-13  3.82928059e-17  9.97478445e-18
  2.38669669e-19 -4.30449599e-14  1.70106954e-18  6.17962046e-20
  2.64866012e-17  1.91461054e-13 -3.80119122e-17 -4.03121000e-20
 -3.54784654e-16 -3.92948988e-17 -4.42062843e-15 -3.47267463e-17
  2.82710575e-12  3.15961485e-12 -8.27614112e-15  3.15982281e-12
  9.98990632e-18  1.24010379e-17 -9.57683372e-17  1.14616584e-17
  3.91992970e-18  6.86058365e-19  4.36702111e-17  6.17505311e-19
 -9.65291823e-13 -3.59282401e-11  1.30957421e-15  1.19561404e-13
  3.45970961e-18  1.42082838e-18  4.55011735e-16  3.26324201e-14
  2.09751850e+00]
supnorm grad right now is: 2.09751850193722
Weights right now are: 
[-11.38147094   2.48912124  -1.71310994   5.35122029  10.8500015
  -2.62311668   1.87781781  -5.91727641   6.50630048 -32.95365579
   9.92092056  -1.08549508   2.54190933  44.99864605   6.79118805
  26.8097292    3.7337674   -4.97327058   2.34310064 -10.56497635
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995082   4.98513713 -11.8360974   21.57457363  -7.2795566
   5.11757491  -4.39409784  24.41493727  13.9502543    1.56630503
   1.06172558 -16.82526464  -2.37084781   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690276  50.93964676  20.79584625
 -10.69145658  -1.40680269  -3.43214144   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962185  -0.57638057  -4.05075101  -1.46689975
  -0.80107704   8.98228099  -3.11087631  -1.49498398  25.15676089]
50.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.1462187739335
gradient value of function right now is: [ 8.61131666e-09  7.42300083e-28  8.60844247e-09  2.76058276e-28
 -8.61552462e-09 -7.42343892e-28 -8.61265028e-09 -2.76050270e-28
  3.67867531e-13  3.86837048e-13  3.74278095e-13  3.84072285e-13
 -1.77510259e-27 -1.92961561e-27 -1.82534127e-27 -1.91464379e-27
  4.78774137e-12  5.32014094e-12  4.95370615e-12  5.23670386e-12
 -3.68428650e-29 -1.30204827e-27 -1.09813338e-28 -2.75340591e-27
 -1.06451584e-19 -5.77490056e-14  1.95937195e-19  2.97607440e-21
  2.69434433e-16  1.02390122e-13  3.71468673e-17  9.71957309e-18
  3.64200947e-19 -4.28008469e-14  1.63356433e-18  6.90660171e-20
  1.87009384e-17  1.90973147e-13 -4.03997359e-17 -5.96847191e-19
 -3.62814731e-16 -4.24855044e-17 -4.46056165e-15 -3.75534004e-17
  2.81055141e-12  3.14099864e-12 -6.96953511e-15  3.14119150e-12
  1.04904210e-17  1.18862202e-17 -8.41161221e-17  1.09525862e-17
  4.10006554e-18  7.39875331e-19  4.49056619e-17  6.64628357e-19
 -9.59806918e-13 -3.57092493e-11  1.40245023e-15  1.16212848e-13
  3.49291848e-18  1.32624289e-18  4.89795760e-16  3.13964420e-14
 -1.81002880e+00]
supnorm grad right now is: 1.8100288028104217
Weights right now are: 
[-11.383182     2.48912124  -1.71482042   5.35122029  10.85171261
  -2.62311668   1.87952835  -5.91727641   6.5063004  -32.95365587
   9.92092048  -1.08549515   2.54190933  44.99864605   6.79118805
  26.8097292    3.73376645  -4.97327163   2.34309965 -10.56497738
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995083   4.98513713 -11.8360974   21.57457363  -7.27955662
   5.11757491  -4.39409784  24.41493727  13.95025431   1.56630503
   1.06172558 -16.82526464  -2.37084785   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690276  50.93964676  20.7958457
 -10.69145719  -1.40680269  -3.43214206   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962202  -0.57637383  -4.05075101  -1.46689978
  -0.80107704   8.98228099  -3.11087631  -1.49498399  24.85552585]
52.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.262803770941
gradient value of function right now is: [ 8.46305053e-09  7.37256242e-28  8.46019600e-09  2.73129786e-28
 -8.45766322e-09 -7.37282598e-28 -8.45480858e-09 -2.73132036e-28
  3.65418004e-13  3.84261356e-13  3.71785920e-13  3.81515074e-13
 -1.76301425e-27 -1.91647889e-27 -1.81291115e-27 -1.90161648e-27
  4.75963860e-12  5.28857276e-12  4.92462046e-12  5.20569397e-12
 -3.65305042e-29 -1.28247852e-27 -1.08327757e-28 -2.72829843e-27
 -1.15274428e-19 -5.74066920e-14  1.96958886e-19  2.36341529e-21
  2.69002909e-16  1.01744655e-13  3.73461538e-17  9.74232136e-18
  3.03836089e-19 -4.25399820e-14  1.64342043e-18  6.50834682e-20
  2.20640154e-17  1.89594203e-13 -3.87896933e-17 -3.36819009e-19
 -3.55607448e-16 -4.05988392e-17 -4.39935863e-15 -3.58821827e-17
  2.79372257e-12  3.12225041e-12 -7.40467024e-15  3.12244751e-12
  1.00584647e-17  1.19443088e-17 -8.91152599e-17  1.10218831e-17
  3.97746773e-18  7.07630023e-19  4.39075872e-17  6.36201973e-19
 -9.54034407e-13 -3.55005939e-11  1.35412248e-15  1.16564478e-13
  3.45330038e-18  1.37381791e-18  4.71486229e-16  3.16421844e-14
 -1.79519742e-01]
supnorm grad right now is: 0.17951974236354817
Weights right now are: 
[-11.38488747   2.48912124  -1.71652529   5.35122029  10.85341805
  -2.62311668   1.88123319  -5.91727641   6.50630033 -32.95365595
   9.92092041  -1.08549524   2.54190933  44.99864605   6.79118805
  26.8097292    3.73376545  -4.97327274   2.34309862 -10.56497848
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995084   4.98513713 -11.8360974   21.57457363  -7.27955664
   5.11757491  -4.39409784  24.41493727  13.95025432   1.56630503
   1.06172558 -16.82526464  -2.37084788   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690276  50.93964676  20.79584513
 -10.69145783  -1.40680269  -3.43214269   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962222  -0.5763666   -4.05075101  -1.4668998
  -0.80107704   8.98228099  -3.11087631  -1.49498399  25.00194945]
54.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5143.28826778659
gradient value of function right now is: [ 8.67576940e-09  7.32350662e-28  8.67293350e-09  2.76346692e-28
 -8.68041957e-09 -7.32316395e-28 -8.67758379e-09 -2.76348125e-28
  3.63022261e-13  3.81742593e-13  3.69348592e-13  3.79014332e-13
 -1.75120106e-27 -1.90364474e-27 -1.80076506e-27 -1.88888895e-27
  4.73213290e-12  5.25922841e-12  4.89620229e-12  5.17657459e-12
 -3.65230648e-29 -1.32108694e-27 -1.10365956e-28 -2.74485076e-27
 -9.03317418e-20 -5.70763340e-14  1.81436424e-19  3.61010312e-21
  2.64206823e-16  1.01269248e-13  3.64149815e-17  9.47717270e-18
  4.45474048e-19 -4.23107397e-14  1.51716717e-18  7.15110234e-20
  1.23193248e-17  1.89395917e-13 -4.06339887e-17 -9.84392161e-19
 -3.66903635e-16 -4.45311791e-17 -4.46901262e-15 -3.93588208e-17
  2.77796407e-12  3.10451173e-12 -5.63690922e-15  3.10468956e-12
  9.87780581e-18  1.10120756e-17 -7.98348042e-17  1.01259264e-17
  4.18703907e-18  7.62731958e-19  4.54828812e-17  6.84015743e-19
 -9.48871675e-13 -3.52887445e-11  1.47899780e-15  1.12503981e-13
  3.54180800e-18  9.61326277e-19  5.17747187e-16  3.01199584e-14
 -5.04904244e+00]
supnorm grad right now is: 5.049042439013326
Weights right now are: 
[-11.38655816   2.48912124  -1.71819542   5.35122029  10.85508868
  -2.62311668   1.88290325  -5.91727641   6.50630025 -32.95365603
   9.92092033  -1.08549531   2.54190933  44.99864605   6.79118805
  26.8097292    3.73376449  -4.9732738    2.34309762 -10.56497953
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995086   4.98513713 -11.8360974   21.57457363  -7.27955666
   5.11757491  -4.39409784  24.41493727  13.95025432   1.56630503
   1.06172558 -16.82526464  -2.37084792   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690276  50.93964676  20.79584458
 -10.69145844  -1.40680269  -3.4321433    1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.5496224   -0.57635974  -4.05075101  -1.46689982
  -0.80107704   8.98228099  -3.11087631  -1.494984    24.62458969]
56.00000000000001% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.256867290162
gradient value of function right now is: [ 8.31395543e-09  7.27451745e-28  8.31113893e-09  2.69156045e-28
 -8.32302212e-09 -7.27482266e-28 -8.32020550e-09 -2.69157710e-28
  3.60667087e-13  3.79265347e-13  3.66952156e-13  3.76554964e-13
 -1.73956555e-27 -1.89099317e-27 -1.78879836e-27 -1.87634330e-27
  4.70505360e-12  5.22777865e-12  4.86813714e-12  5.14588127e-12
 -3.60181141e-29 -1.26075787e-27 -1.06555028e-28 -2.69069407e-27
 -1.17681085e-19 -5.67416584e-14  1.95467389e-19  2.10294571e-21
  2.66386153e-16  1.00532793e-13  3.70847186e-17  9.66211859e-18
  2.78696582e-19 -4.20403105e-14  1.63193703e-18  6.29898276e-20
  2.31655736e-17  1.87315362e-13 -3.78266639e-17 -2.34358006e-19
 -3.49597409e-16 -3.95190694e-17 -4.33569884e-15 -3.49288784e-17
  2.76114859e-12  3.08588891e-12 -7.54201966e-15  3.08608623e-12
  9.80827786e-18  1.18655691e-17 -9.02114149e-17  1.09551145e-17
  3.89446739e-18  6.88940039e-19  4.31254183e-17  6.19627004e-19
 -9.42937149e-13 -3.50909280e-11  1.31999532e-15  1.15681650e-13
  3.39428861e-18  1.30557667e-18  4.59186073e-16  3.14605770e-14
  4.84837269e-01]
supnorm grad right now is: 0.48483726892015805
Weights right now are: 
[-11.38823702   2.48912124  -1.71987371   5.35122029  10.85676753
  -2.62311668   1.88458153  -5.91727641   6.50630018 -32.95365611
   9.92092026  -1.08549539   2.54190933  44.99864605   6.79118805
  26.8097292    3.73376354  -4.97327486   2.34309665 -10.56498056
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995087   4.98513713 -11.8360974   21.57457363  -7.27955668
   5.11757491  -4.39409784  24.41493727  13.95025433   1.56630503
   1.06172558 -16.82526464  -2.37084796   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690276  50.93964676  20.79584401
 -10.69145907  -1.40680268  -3.43214394   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.5496226   -0.57635248  -4.05075101  -1.46689984
  -0.80107704   8.98228099  -3.11087631  -1.49498401  25.00483714]
57.99999999999999% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.18488223446
gradient value of function right now is: [ 8.36535228e-09  7.22625528e-28  8.36255436e-09  2.68848624e-28
 -8.36442609e-09 -7.22597649e-28 -8.36162827e-09 -2.68842353e-28
  3.58309454e-13  3.76786681e-13  3.64553609e-13  3.74094029e-13
 -1.72794472e-27 -1.87836832e-27 -1.77685016e-27 -1.86382341e-27
  4.67793565e-12  5.19806059e-12  4.84009589e-12  5.11655049e-12
 -3.58509908e-29 -1.26576431e-27 -1.06703014e-28 -2.68309326e-27
 -1.05485152e-19 -5.64143267e-14  1.93041182e-19  2.85179348e-21
  2.63344716e-16  9.99772397e-14  3.62898779e-17  9.50456142e-18
  3.48089321e-19 -4.18011582e-14  1.60776013e-18  6.72334514e-20
  1.88660792e-17  1.86538967e-13 -3.94258242e-17 -5.45793266e-19
 -3.53554006e-16 -4.12484689e-17 -4.35047374e-15 -3.64612015e-17
  2.74523238e-12  3.06805979e-12 -6.92523400e-15  3.06824929e-12
  1.03359637e-17  1.16976299e-17 -8.22888509e-17  1.07814020e-17
  3.99225379e-18  7.19816975e-19  4.37526583e-17  6.46749873e-19
 -9.37605475e-13 -3.48858174e-11  1.36204805e-15  1.13738738e-13
  3.41116693e-18  1.28855751e-18  4.75482096e-16  3.07552370e-14
 -1.49292267e+00]
supnorm grad right now is: 1.4929226688443833
Weights right now are: 
[-11.38990227   2.48912124  -1.72153837   5.35122029  10.85843281
  -2.62311668   1.88624622  -5.91727641   6.5063001  -32.95365619
   9.92092018  -1.08549547   2.54190933  44.99864605   6.79118805
  26.8097292    3.73376256  -4.97327594   2.34309563 -10.56498163
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995088   4.98513713 -11.8360974   21.57457363  -7.2795567
   5.11757491  -4.39409784  24.41493727  13.95025434   1.56630503
   1.06172558 -16.82526464  -2.370848     2.51369865   4.22064502
  -4.83421262  38.329907     1.17690276  50.93964676  20.79584345
 -10.6914597   -1.40680268  -3.43214456   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962279  -0.57634536  -4.05075101  -1.46689987
  -0.80107704   8.98228099  -3.11087631  -1.49498401  24.86586982]
60.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.187801334279
gradient value of function right now is: [ 8.30822998e-09  7.17883506e-28  8.30545043e-09  2.67162049e-28
 -8.31704065e-09 -7.17928070e-28 -8.31426093e-09 -2.67161325e-28
  3.56032374e-13  3.74391436e-13  3.62236588e-13  3.71716082e-13
 -1.71669510e-27 -1.86613482e-27 -1.76527987e-27 -1.85169199e-27
  4.65170564e-12  5.16889011e-12  4.81294852e-12  5.08784167e-12
 -3.56146243e-29 -1.25756925e-27 -1.05984587e-28 -2.66649567e-27
 -1.05069390e-19 -5.60925426e-14  1.92154660e-19  2.83110203e-21
  2.61866638e-16  9.93971861e-14  3.60841302e-17  9.45149355e-18
  3.45237559e-19 -4.15603880e-14  1.60014524e-18  6.68393009e-20
  1.88257067e-17  1.85485311e-13 -3.91999075e-17 -5.38811598e-19
 -3.51420474e-16 -4.09855634e-17 -4.32501191e-15 -3.62285684e-17
  2.72949483e-12  3.05047848e-12 -6.89415462e-15  3.05066697e-12
  1.02894916e-17  1.16397967e-17 -8.18293615e-17  1.07283981e-17
  3.96802546e-18  7.15469819e-19  4.34917861e-17  6.42856173e-19
 -9.32241737e-13 -3.46866167e-11  1.35375823e-15  1.13102543e-13
  3.39266954e-18  1.27943508e-18  4.72556722e-16  3.05859867e-14
 -1.45416018e+00]
supnorm grad right now is: 1.4541601831744544
Weights right now are: 
[-11.39154965   2.48912124  -1.7231852    5.35122029  10.86008021
  -2.62311668   1.88789307  -5.91727641   6.50630003 -32.95365626
   9.92092011  -1.08549554   2.54190933  44.99864605   6.79118805
  26.8097292    3.73376164  -4.97327697   2.34309468 -10.56498264
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995089   4.98513713 -11.8360974   21.57457363  -7.27955672
   5.11757491  -4.39409784  24.41493727  13.95025435   1.56630503
   1.06172558 -16.82526464  -2.37084803   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690276  50.93964676  20.7958429
 -10.69146031  -1.40680268  -3.43214517   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962297  -0.57633857  -4.05075101  -1.46689989
  -0.80107704   8.98228099  -3.11087631  -1.49498402  24.90896863]
62.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.256510441277
gradient value of function right now is: [ 8.15011899e-09  7.13184652e-28  8.14735777e-09  2.64181573e-28
 -8.15553639e-09 -7.13207231e-28 -8.15277508e-09 -2.64180099e-28
  3.53746699e-13  3.71988116e-13  3.59911145e-13  3.69329997e-13
 -1.70542591e-27 -1.85388911e-27 -1.75369231e-27 -1.83954828e-27
  4.62536412e-12  5.13922488e-12  4.78568422e-12  5.05871651e-12
 -3.53112134e-29 -1.23686632e-27 -1.04443292e-28 -2.64139021e-27
 -1.15956710e-19 -5.57719207e-14  1.92322640e-19  2.05729107e-21
  2.61788527e-16  9.87857680e-14  3.64421408e-17  9.49621608e-18
  2.72936709e-19 -4.13149758e-14  1.60536534e-18  6.18863513e-20
  2.28424140e-17  1.84134783e-13 -3.71725306e-17 -2.25762990e-19
 -3.43459417e-16 -3.88098500e-17 -4.25969197e-15 -3.43018461e-17
  2.71371030e-12  3.03290383e-12 -7.42176723e-15  3.03309785e-12
  9.64662404e-18  1.16701153e-17 -8.86471633e-17  1.07747333e-17
  3.82577468e-18  6.76778793e-19  4.23656233e-17  6.08697646e-19
 -9.26812273e-13 -3.44918317e-11  1.29668379e-15  1.13712010e-13
  3.33608026e-18  1.28303593e-18  4.51052575e-16  3.09279943e-14
  5.00491645e-01]
supnorm grad right now is: 0.5004916452672377
Weights right now are: 
[-11.39319256   2.48912124  -1.72482753   5.35122029  10.86172313
  -2.62311668   1.88953541  -5.91727641   6.50629996 -32.95365634
   9.92092003  -1.08549562   2.54190933  44.99864605   6.79118805
  26.8097292    3.73376067  -4.97327805   2.34309367 -10.5649837
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.9299509    4.98513713 -11.8360974   21.57457363  -7.27955674
   5.11757491  -4.39409784  24.41493727  13.95025436   1.56630503
   1.06172558 -16.82526464  -2.37084807   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690276  50.93964676  20.79584235
 -10.69146093  -1.40680268  -3.43214579   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962317  -0.57633145  -4.05075101  -1.46689991
  -0.80107704   8.98228099  -3.11087631  -1.49498402  25.07965329]
64.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.175354056866
gradient value of function right now is: [ 8.04173819e-09  7.08550004e-28  8.03899499e-09  2.62641744e-28
 -8.03679245e-09 -7.08466897e-28 -8.03404956e-09 -2.62644642e-28
  3.51466691e-13  3.69591557e-13  3.57591719e-13  3.66950569e-13
 -1.69420212e-27 -1.84170010e-27 -1.74215361e-27 -1.82746054e-27
  4.59906930e-12  5.10981983e-12  4.75848134e-12  5.02980844e-12
 -3.50524487e-29 -1.22672819e-27 -1.03332047e-28 -2.62905490e-27
 -1.21288136e-19 -5.54548219e-14  1.92338744e-19  1.67751039e-21
  2.61257048e-16  9.81973530e-14  3.65310038e-17  9.50088998e-18
  2.38688118e-19 -4.10749957e-14  1.60661135e-18  5.94397751e-20
  2.46524318e-17  1.82932742e-13 -3.61981047e-17 -8.05949218e-20
 -3.39424344e-16 -3.77912818e-17 -4.22214630e-15 -3.33975535e-17
  2.69814381e-12  3.01555158e-12 -7.67806009e-15  3.01574794e-12
  9.34730684e-18  1.16774961e-17 -9.15151759e-17  1.07899559e-17
  3.75534654e-18  6.58090451e-19  4.17732724e-17  5.92157229e-19
 -9.21503391e-13 -3.42981404e-11  1.26411870e-15  1.13699654e-13
  3.30543388e-18  1.34141753e-18  4.39248931e-16  3.09979357e-14
  1.61596510e+00]
supnorm grad right now is: 1.6159651047377719
Weights right now are: 
[-11.39482376   2.48912124  -1.72645818   5.35122029  10.86335434
  -2.62311668   1.89116608  -5.91727641   6.50629989 -32.95365641
   9.92091996  -1.08549569   2.54190933  44.99864605   6.79118805
  26.8097292    3.73375975  -4.97327906   2.34309272 -10.56498471
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995091   4.98513713 -11.8360974   21.57457363  -7.27955676
   5.11757491  -4.39409784  24.41493727  13.95025437   1.56630503
   1.06172558 -16.82526464  -2.37084811   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690276  50.93964676  20.79584182
 -10.69146152  -1.40680268  -3.43214638   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962335  -0.5763247   -4.05075101  -1.46689994
  -0.80107704   8.98228099  -3.11087631  -1.49498403  25.08571933]
66.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.235902210333
gradient value of function right now is: [ 8.11687827e-09  7.03981003e-28  8.11415263e-09  2.61943985e-28
 -8.11353403e-09 -7.04026854e-28 -8.11080822e-09 -2.61947773e-28
  3.49289563e-13  3.67300852e-13  3.55376220e-13  3.64676431e-13
 -1.68343969e-27 -1.82999094e-27 -1.73108286e-27 -1.81584892e-27
  4.57392080e-12  5.08232577e-12  4.73246218e-12  5.00265796e-12
 -3.49047829e-29 -1.23079739e-27 -1.03681335e-28 -2.61604779e-27
 -1.06952745e-19 -5.51455897e-14  1.89366538e-19  2.54348369e-21
  2.57753558e-16  9.76820077e-14  3.56251175e-17  9.31842303e-18
  3.17039082e-19 -4.08506042e-14  1.57752916e-18  6.42486051e-20
  1.97595700e-17  1.82284940e-13 -3.79943369e-17 -4.34228012e-19
 -3.43562895e-16 -3.96842689e-17 -4.23751925e-15 -3.50769685e-17
  2.68314716e-12  2.99872569e-12 -6.93794345e-15  2.99891285e-12
  9.93793332e-18  1.14734336e-17 -8.25476155e-17  1.05805631e-17
  3.86300253e-18  6.92595753e-19  4.24700346e-17  6.22484228e-19
 -9.16465419e-13 -3.41024277e-11  1.31661602e-15  1.11542620e-13
  3.32811281e-18  1.24720072e-18  4.59067171e-16  3.02184078e-14
 -8.72361265e-01]
supnorm grad right now is: 0.8723612648168972
Weights right now are: 
[-11.39644241   2.48912124  -1.72807629   5.35122029  10.86497302
  -2.62311668   1.89278421  -5.91727641   6.50629982 -32.95365649
   9.92091989  -1.08549577   2.54190933  44.99864605   6.79118805
  26.8097292    3.73375883  -4.97328008   2.34309178 -10.56498571
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995092   4.98513713 -11.8360974   21.57457363  -7.27955678
   5.11757491  -4.39409784  24.41493727  13.95025437   1.56630503
   1.06172558 -16.82526464  -2.37084814   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690276  50.93964676  20.79584129
 -10.69146212  -1.40680268  -3.43214698   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962353  -0.57631797  -4.05075101  -1.46689996
  -0.80107704   8.98228099  -3.11087631  -1.49498404  24.93342365]
68.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.225294749879
gradient value of function right now is: [ 8.07564215e-09  6.99521683e-28  8.07293377e-09  2.60564886e-28
 -8.07409783e-09 -6.99568508e-28 -8.07138927e-09 -2.60562063e-28
  3.47126285e-13  3.65026015e-13  3.53175234e-13  3.62417923e-13
 -1.67277392e-27 -1.81839855e-27 -1.72011512e-27 -1.80435227e-27
  4.54892420e-12  5.05459762e-12  4.70659997e-12  4.97535548e-12
 -3.46931521e-29 -1.22506235e-27 -1.03129332e-28 -2.60174706e-27
 -1.05483806e-19 -5.48415222e-14  1.87788744e-19  2.58351353e-21
  2.56196205e-16  9.71393548e-14  3.54004093e-17  9.25861479e-18
  3.21015407e-19 -4.06239339e-14  1.56490723e-18  6.42349614e-20
  1.93326844e-17  1.81324132e-13 -3.78571929e-17 -4.55248325e-19
 -3.42094087e-16 -3.96150242e-17 -4.21683529e-15 -3.50148442e-17
  2.66828738e-12  2.98212011e-12 -6.82233073e-15  2.98230541e-12
  9.87452487e-18  1.13734459e-17 -8.17702864e-17  1.04866445e-17
  3.85022595e-18  6.91159651e-19  4.22978949e-17  6.21120771e-19
 -9.11423734e-13 -3.39141106e-11  1.31514805e-15  1.10772201e-13
  3.31147531e-18  1.23063791e-18  4.58620143e-16  2.99939592e-14
 -1.06271843e+00]
supnorm grad right now is: 1.0627184251096287
Weights right now are: 
[-11.39803272   2.48912124  -1.7296661    5.35122029  10.86656336
  -2.62311668   1.89437405  -5.91727641   6.50629975 -32.95365655
   9.92091983  -1.08549583   2.54190933  44.99864605   6.79118805
  26.8097292    3.73375801  -4.973281     2.34309092 -10.56498661
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995093   4.98513713 -11.8360974   21.57457363  -7.2795568
   5.11757491  -4.39409784  24.41493727  13.95025438   1.56630503
   1.06172558 -16.82526464  -2.37084818   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690277  50.93964676  20.79584079
 -10.69146267  -1.40680268  -3.43214753   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962368  -0.57631184  -4.05075101  -1.46689998
  -0.80107704   8.98228099  -3.11087631  -1.49498404  24.93548364]
70.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.263129548475
gradient value of function right now is: [ 7.97725404e-09  6.95017096e-28  7.97456317e-09  2.58332516e-28
 -7.96923361e-09 -6.94982244e-28 -7.96654287e-09 -2.58339419e-28
  3.44915792e-13  3.62702260e-13  3.50926454e-13  3.60110759e-13
 -1.66189281e-27 -1.80657929e-27 -1.70892815e-27 -1.79263083e-27
  4.52336118e-12  5.02602238e-12  4.68015250e-12  4.94726147e-12
 -3.44366524e-29 -1.21123919e-27 -1.02064856e-28 -2.58180789e-27
 -1.09851090e-19 -5.45326903e-14  1.87755865e-19  2.25042544e-21
  2.55347631e-16  9.65661128e-14  3.54217045e-17  9.24817106e-18
  2.88718189e-19 -4.03903321e-14  1.56534188e-18  6.19481477e-20
  2.10165470e-17  1.80167878e-13 -3.69041809e-17 -3.18623586e-19
 -3.37558101e-16 -3.85377677e-17 -4.17501276e-15 -3.40614404e-17
  2.65312967e-12  2.96522197e-12 -7.06790482e-15  2.96540948e-12
  9.61570315e-18  1.13742160e-17 -8.42477769e-17  1.04956748e-17
  3.77695866e-18  6.72396405e-19  4.16757692e-17  6.04548340e-19
 -9.06250391e-13 -3.37253489e-11  1.28372004e-15  1.10758242e-13
  3.28145779e-18  1.30305199e-18  4.47020012e-16  3.00705905e-14
 -1.63921910e-01]
supnorm grad right now is: 0.1639219104008237
Weights right now are: 
[-11.39964874   2.48912124  -1.73128159   5.35122029  10.86817938
  -2.62311668   1.89598954  -5.91727641   6.50629969 -32.95365662
   9.92091976  -1.0854959    2.54190933  44.99864605   6.79118805
  26.8097292    3.73375712  -4.973282     2.34309    -10.56498759
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995094   4.98513713 -11.8360974   21.57457363  -7.27955682
   5.11757491  -4.39409784  24.41493727  13.95025439   1.56630503
   1.06172558 -16.82526464  -2.37084822   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690277  50.93964676  20.79584024
 -10.69146328  -1.40680267  -3.43214815   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962388  -0.57630475  -4.05075101  -1.4669
  -0.80107704   8.98228099  -3.11087631  -1.49498405  25.01745061]
72.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.260307277496
gradient value of function right now is: [ 7.89780004e-09  6.90556250e-28  7.89512649e-09  2.56339171e-28
 -7.89163687e-09 -6.90489180e-28 -7.88896357e-09 -2.56358907e-28
  3.42741135e-13  3.60415733e-13  3.48713981e-13  3.57840621e-13
 -1.65118000e-27 -1.79493881e-27 -1.69791288e-27 -1.78108735e-27
  4.49818612e-12  4.99794125e-12  4.65410371e-12  4.91964190e-12
 -3.41941854e-29 -1.19962540e-27 -1.01147482e-28 -2.56353558e-27
 -1.12161615e-19 -5.42271358e-14  1.87350619e-19  2.05719886e-21
  2.54371663e-16  9.60056131e-14  3.53653060e-17  9.22397702e-18
  2.70591857e-19 -4.01601790e-14  1.56293150e-18  6.05912675e-20
  2.19577613e-17  1.79080265e-13 -3.63307002e-17 -2.41137915e-19
 -3.34267447e-16 -3.78687593e-17 -4.14289922e-15 -3.34707561e-17
  2.63815373e-12  2.94851290e-12 -7.20275712e-15  2.94870131e-12
  9.46813777e-18  1.13641247e-17 -8.53934892e-17  1.04907563e-17
  3.72821907e-18  6.60894827e-19  4.12432544e-17  5.94375803e-19
 -9.01144357e-13 -3.35377177e-11  1.26232491e-15  1.10501535e-13
  3.24239066e-18  1.24550365e-18  4.39284524e-16  3.00445059e-14
  3.63508943e-01]
supnorm grad right now is: 0.3635089426335931
Weights right now are: 
[-11.40125942   2.48912124  -1.73289171   5.35122029  10.86979003
  -2.62311668   1.89759963  -5.91727641   6.50629961 -32.9536567
   9.92091968  -1.08549598   2.54190933  44.99864605   6.79118805
  26.8097292    3.73375615  -4.97328306   2.343089   -10.56498864
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995096   4.98513713 -11.8360974   21.57457363  -7.27955684
   5.11757491  -4.39409784  24.41493727  13.9502544    1.56630503
   1.06172558 -16.82526464  -2.37084825   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690277  50.93964676  20.79583971
 -10.69146388  -1.40680267  -3.43214874   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962406  -0.57629795  -4.05075101  -1.46690003
  -0.80107704   8.98228099  -3.11087631  -1.49498405  25.0216025 ]
74.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.051837580582
gradient value of function right now is: [ 7.74795971e-09  6.86175656e-28  7.74530326e-09  2.53865366e-28
 -7.74520085e-09 -6.86225327e-28 -7.74254422e-09 -2.53860688e-28
  3.40649892e-13  3.58215629e-13  3.46585925e-13  3.55656504e-13
 -1.64085122e-27 -1.78370366e-27 -1.68728854e-27 -1.76994639e-27
  4.47394666e-12  4.97054793e-12  4.62899979e-12  4.89276520e-12
 -3.39002162e-29 -1.18028562e-27 -9.95237511e-29 -2.54480441e-27
 -1.21451141e-19 -5.39276118e-14  1.91316210e-19  1.48492436e-21
  2.54399425e-16  9.54254512e-14  3.55036538e-17  9.26299145e-18
  2.12697320e-19 -3.99299178e-14  1.59269448e-18  5.70534996e-20
  2.52516486e-17  1.77788033e-13 -3.52392744e-17  4.79014959e-21
 -3.28251182e-16 -3.61937001e-17 -4.09168194e-15 -3.19889080e-17
  2.62340591e-12  2.93210155e-12 -7.83292240e-15  2.93229602e-12
  9.37511034e-18  1.16109581e-17 -8.88175430e-17  1.07335921e-17
  3.62290952e-18  6.33402620e-19  4.03756893e-17  5.70270449e-19
 -8.96034344e-13 -3.33558183e-11  1.20468326e-15  1.11235695e-13
  3.21095880e-18  1.30315627e-18  4.18425361e-16  3.03903076e-14
  2.42590490e+00]
supnorm grad right now is: 2.4259049009073665
Weights right now are: 
[-11.40285148   2.48912124  -1.7344832    5.35122029  10.87138211
  -2.62311668   1.89919113  -5.91727641   6.50629954 -32.95365678
   9.92091961  -1.08549606   2.54190933  44.99864605   6.79118805
  26.8097292    3.7337552   -4.97328412   2.34308802 -10.56498968
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995097   4.98513713 -11.8360974   21.57457363  -7.27955686
   5.11757491  -4.39409784  24.41493727  13.95025441   1.56630503
   1.06172558 -16.82526464  -2.37084829   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690277  50.93964677  20.79583918
 -10.69146447  -1.40680267  -3.43214934   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962424  -0.57629131  -4.05075101  -1.46690005
  -0.80107704   8.98228099  -3.11087631  -1.49498406  25.1521034 ]
76.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.176454249214
gradient value of function right now is: [ 7.89726197e-09  6.81912220e-28  7.89462179e-09  2.54694479e-28
 -7.89083962e-09 -6.81889535e-28 -7.88819953e-09 -2.54705635e-28
  3.38558489e-13  3.56016970e-13  3.44458283e-13  3.53473495e-13
 -1.63055984e-27 -1.77252373e-27 -1.67670773e-27 -1.75885817e-27
  4.44970029e-12  4.94444153e-12  4.60394348e-12  4.86690839e-12
 -3.38368829e-29 -1.19827919e-27 -1.00716088e-28 -2.54268074e-27
 -1.00472953e-19 -5.36367113e-14  1.83258914e-19  2.70420661e-21
  2.50169071e-16  9.49773176e-14  3.44814904e-17  9.02942651e-18
  3.31733328e-19 -3.97243925e-14  1.52604070e-18  6.39232061e-20
  1.78828101e-17  1.77435784e-13 -3.74344298e-17 -5.21156888e-19
 -3.36039405e-16 -3.92226626e-17 -4.13354236e-15 -3.46702108e-17
  2.60937975e-12  2.91631461e-12 -6.55596568e-15  2.91649450e-12
  9.78755721e-18  1.10948460e-17 -7.82557199e-17  1.02254315e-17
  3.79459975e-18  6.84117315e-19  4.15764946e-17  6.14651438e-19
 -8.91418042e-13 -3.31693973e-11  1.29604994e-15  1.08060798e-13
  3.24356973e-18  1.22303789e-18  4.52435784e-16  2.92170396e-14
 -1.55866799e+00]
supnorm grad right now is: 1.558667992380807
Weights right now are: 
[-11.40441094   2.48912124  -1.73604214   5.35122029  10.87294156
  -2.62311668   1.90075008  -5.91727641   6.50629947 -32.95365685
   9.92091954  -1.08549612   2.54190933  44.99864605   6.79118805
  26.8097292    3.73375434  -4.97328507   2.34308712 -10.56499062
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995098   4.98513713 -11.8360974   21.57457363  -7.27955688
   5.11757491  -4.39409784  24.41493727  13.95025441   1.56630503
   1.06172558 -16.82526464  -2.37084832   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690277  50.93964677  20.79583866
 -10.69146505  -1.40680267  -3.43214991   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962442  -0.57628475  -4.05075101  -1.46690007
  -0.80107704   8.98228099  -3.11087631  -1.49498407  24.89247532]
78.0% of gradient descent iterations done. Method = Adam
objective value function right now is: -5144.245732232421
gradient value of function right now is: [ 7.80648847e-09  6.77735631e-28  7.80386452e-09  2.52586453e-28
 -7.81421461e-09 -6.77735269e-28 -7.81159067e-09 -2.52596830e-28
  3.36537644e-13  3.53891662e-13  3.42402129e-13  3.51363478e-13
 -1.62059816e-27 -1.76169443e-27 -1.66646330e-27 -1.74811858e-27
  4.42624168e-12  4.91820188e-12  4.57966373e-12  4.84111423e-12
 -3.35972407e-29 -1.18518152e-27 -9.97153172e-29 -2.52380386e-27
 -1.04270478e-19 -5.33501589e-14  1.84072263e-19  2.43554662e-21
  2.49310390e-16  9.44454671e-14  3.44409884e-17  9.01556622e-18
  3.03216307e-19 -3.95077075e-14  1.53227358e-18  6.20726453e-20
  1.93747712e-17  1.76375001e-13 -3.67608112e-17 -4.04228936e-19
 -3.31902846e-16 -3.82755778e-17 -4.09463482e-15 -3.38327559e-17
  2.59532506e-12  2.90064064e-12 -6.78615302e-15  2.90082244e-12
  9.66887703e-18  1.11438397e-17 -7.97372818e-17  1.02775831e-17
  3.73092759e-18  6.68846765e-19  4.10226901e-17  6.01203189e-19
 -8.86604375e-13 -3.29936957e-11  1.26801501e-15  1.08031565e-13
  3.22042621e-18  1.23281866e-18  4.42116816e-16  2.92810170e-14
 -7.20831719e-01]
supnorm grad right now is: 0.7208317185546537
Weights right now are: 
[-11.4059479    2.48912124  -1.73757861   5.35122029  10.87447848
  -2.62311668   1.9022865   -5.91727641   6.50629941 -32.95365692
   9.92091947  -1.08549619   2.54190933  44.99864605   6.79118805
  26.8097292    3.7337535   -4.97328601   2.34308626 -10.56499154
   3.92336995  27.62408886  40.17399079  27.56738773 -20.59719181
  24.92995099   4.98513713 -11.8360974   21.57457363  -7.2795569
   5.11757491  -4.39409784  24.41493727  13.95025442   1.56630503
   1.06172558 -16.82526464  -2.37084836   2.51369865   4.22064502
  -4.83421262  38.329907     1.17690277  50.93964677  20.79583817
 -10.6914656   -1.40680267  -3.43215046   1.75417885   9.34438579
   3.02523016  18.32674644  -6.68449735  20.88964923   5.0069599
  44.55733377   9.54962458  -0.57627868  -4.05075101  -1.46690009
  -0.80107704   8.98228099  -3.11087631  -1.49498407  24.90195393]
Traceback (most recent call last):
  File "Forsyth_replication.py", line 720, in <module>
    fun_RUN__wrapper.RUN__wrapper_ONE_stage_optimization(
  File "/home/ma3chen/Documents/marc_branch2/researchcode/fun_RUN__wrapper.py", line 112, in RUN__wrapper_ONE_stage_optimization
    RUN__wrapper_training_testing_NN(
  File "/home/ma3chen/Documents/marc_branch2/researchcode/fun_RUN__wrapper.py", line 185, in RUN__wrapper_training_testing_NN
    params_TRAIN, res_BEST, _, res_ALL_dataframe = fun_train_NN.train_NN( theta0 = theta0,
  File "/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN.py", line 195, in train_NN
    res_Adam = run_Gradient_Descent(method="Adam",
  File "/home/ma3chen/Documents/marc_branch2/researchcode/fun_train_NN_SGD_algorithms.py", line 264, in run_Gradient_Descent
    params_it = copy.deepcopy(params)  # Create a copy of input data for this iteration
  File "/usr/lib/python3.8/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.8/copy.py", line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.8/copy.py", line 153, in deepcopy
    y = copier(memo)
KeyboardInterrupt
