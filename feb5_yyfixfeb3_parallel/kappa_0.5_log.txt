Starting at: 
05-02-23_19:30

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 50000
batchsize: 1000
remove neg:  False


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 5000, 'itbound_SGD_algorithms': 50000, 'nit_IterateAveragingStart': 45000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1565.7395794834006
Current xi:  [77.595955]
objective value function right now is: -1565.7395794834006
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1585.8554744775215
Current xi:  [49.644814]
objective value function right now is: -1585.8554744775215
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1594.6402432071757
Current xi:  [23.316677]
objective value function right now is: -1594.6402432071757
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1598.205351394064
Current xi:  [1.8774179]
objective value function right now is: -1598.205351394064
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1601.3086107411234
Current xi:  [0.03891733]
objective value function right now is: -1601.3086107411234
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.36414334]
objective value function right now is: -1575.7566185802361
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01877763]
objective value function right now is: -1600.8219891520803
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.7781194]
objective value function right now is: -1574.9882422171534
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.05260184]
objective value function right now is: -1600.444919309749
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.13513502]
objective value function right now is: -1599.6429568957801
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.8752394]
objective value function right now is: -1456.7115894272085
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01619739]
objective value function right now is: -1600.1042488094713
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.061464]
objective value function right now is: -1598.0999604509636
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.09983799]
objective value function right now is: -1589.1018377256137
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.8167616834144
Current xi:  [-0.08580017]
objective value function right now is: -1602.8167616834144
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.07792617]
objective value function right now is: -1601.839658298267
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-3.6644475]
objective value function right now is: -1589.8660784398412
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.791525]
objective value function right now is: -1561.076483208699
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.11742676]
objective value function right now is: -1601.229590403123
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01373707]
objective value function right now is: -1600.4172682629976
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06239729]
objective value function right now is: -1601.1634941043746
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00234224]
objective value function right now is: -1594.2802811098375
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.02953]
objective value function right now is: -1599.073322951523
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.6590086]
objective value function right now is: -1584.6190859597302
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.08533176]
objective value function right now is: -1589.7370192304609
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-17.8533]
objective value function right now is: -1273.9694172261215
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-29.49094]
objective value function right now is: -1595.1041933970096
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-32.66457]
objective value function right now is: -1599.818785866199
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-35.02802]
objective value function right now is: -1602.3394429660884
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1604.2186451760983
Current xi:  [-35.000423]
objective value function right now is: -1604.2186451760983
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-72.26818]
objective value function right now is: -1597.2630328769467
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-72.09401]
objective value function right now is: -1597.8840960984799
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-72.78626]
objective value function right now is: -1591.734381576753
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.316536]
objective value function right now is: -1584.2403417436742
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-72.9545]
objective value function right now is: -1597.5732879959182
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.323814]
objective value function right now is: -1579.178878510369
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-75.03268]
objective value function right now is: -1581.4954626235892
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.74243]
objective value function right now is: -1582.792535815191
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.98734]
objective value function right now is: -1581.3295766710025
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.808075]
objective value function right now is: -1562.9736843813137
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.47501]
objective value function right now is: -1599.467072526012
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.34241]
objective value function right now is: -1594.2251392967382
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.375435]
objective value function right now is: -1601.944211911439
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.2779]
objective value function right now is: -1603.1450402449886
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.04094]
objective value function right now is: -1603.853722652301
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.243004]
objective value function right now is: -1580.7622802755006
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.14969]
objective value function right now is: -1601.9330912967496
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.15958]
objective value function right now is: -1601.0269851066648
new min fval from sgd:  -1604.2206517940465
new min fval from sgd:  -1604.2227898490357
new min fval from sgd:  -1604.2255957292332
new min fval from sgd:  -1604.2326803663136
new min fval from sgd:  -1604.2386073121638
new min fval from sgd:  -1604.2448155397221
new min fval from sgd:  -1604.2517794697612
new min fval from sgd:  -1604.2605231683222
new min fval from sgd:  -1604.2685949739753
new min fval from sgd:  -1604.2752146026571
new min fval from sgd:  -1604.2793567056312
new min fval from sgd:  -1604.2825077566235
new min fval from sgd:  -1604.283032182556
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.06659]
objective value function right now is: -1604.2423506214016
new min fval from sgd:  -1604.2836056230594
new min fval from sgd:  -1604.2851777589913
new min fval from sgd:  -1604.2937132425102
new min fval from sgd:  -1604.3001224611696
new min fval from sgd:  -1604.303364684075
new min fval from sgd:  -1604.3057314847158
new min fval from sgd:  -1604.3100821894059
new min fval from sgd:  -1604.3182360271549
new min fval from sgd:  -1604.32509457816
new min fval from sgd:  -1604.3325140535262
new min fval from sgd:  -1604.3376995794424
new min fval from sgd:  -1604.340163383708
new min fval from sgd:  -1604.3441396050794
new min fval from sgd:  -1604.347412201797
new min fval from sgd:  -1604.350795268082
new min fval from sgd:  -1604.3538472234318
new min fval from sgd:  -1604.3571455295926
new min fval from sgd:  -1604.361412891515
new min fval from sgd:  -1604.3648010384363
new min fval from sgd:  -1604.3670035828063
new min fval from sgd:  -1604.3675087243091
new min fval from sgd:  -1604.3687813811896
new min fval from sgd:  -1604.3705839024647
new min fval from sgd:  -1604.3717261445165
new min fval from sgd:  -1604.3735615578332
new min fval from sgd:  -1604.3744593488473
new min fval from sgd:  -1604.3769526689175
new min fval from sgd:  -1604.378481879578
new min fval from sgd:  -1604.378975656824
new min fval from sgd:  -1604.3810007754957
new min fval from sgd:  -1604.3860439889315
new min fval from sgd:  -1604.3904496941373
new min fval from sgd:  -1604.3920872248104
new min fval from sgd:  -1604.3947701717761
new min fval from sgd:  -1604.3992857433138
new min fval from sgd:  -1604.3995288508427
new min fval from sgd:  -1604.4021966130474
new min fval from sgd:  -1604.4034301822642
new min fval from sgd:  -1604.4050110805717
new min fval from sgd:  -1604.4062204056136
new min fval from sgd:  -1604.4072898034194
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-74.0843]
objective value function right now is: -1604.3604910227773
min fval:  -1604.4072898034194
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[  7.8849,  -7.3618],
        [ -8.1563, -11.2738],
        [  2.3680,   2.4937],
        [  1.5292,  -9.2643],
        [  6.1743,  -5.8152],
        [ -9.0515,   0.4336],
        [  2.2597,   2.7552],
        [ -0.8176,   1.0962],
        [ -9.6973, -10.3752],
        [  6.0457,  -5.7788]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-6.9868,  5.2710,  1.3052, -9.4099, -7.4982,  8.2594,  1.2351, -1.5397,
        -7.8499, -7.1985], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-8.7545e-01, -1.8790e-01, -5.6487e-01, -3.4600e-01, -6.9739e-01,
          3.7465e-01, -5.4221e-01,  3.2139e-02,  5.8872e-01, -7.1994e-01],
        [ 9.7073e+00, -6.3499e+00, -3.0498e+00,  9.4550e+00,  7.8458e+00,
         -1.0273e+01, -2.8660e+00, -1.0801e-01,  1.0528e+01,  8.3029e+00],
        [ 6.4384e+00, -5.7997e+00, -3.0168e+00,  2.3237e+00,  4.9488e+00,
         -7.7928e+00, -2.8646e+00, -6.5874e-02,  9.9182e-01,  5.4759e+00],
        [-8.7545e-01, -1.8790e-01, -5.6487e-01, -3.4600e-01, -6.9739e-01,
          3.7464e-01, -5.4221e-01,  3.2140e-02,  5.8871e-01, -7.1994e-01],
        [ 1.7482e-01,  8.9318e-01, -7.8168e-02, -5.0231e-01, -4.5604e-02,
         -3.1013e+00,  2.7896e-01, -5.2571e-02, -4.7040e-02,  9.2206e-02],
        [ 1.0847e+00,  1.0729e+00,  7.2101e-01,  3.1241e-01,  5.4199e-01,
          1.2665e+00,  6.0627e-01,  2.8676e-02,  4.8044e-01,  6.0145e-01],
        [-8.7545e-01, -1.8790e-01, -5.6487e-01, -3.4600e-01, -6.9739e-01,
          3.7464e-01, -5.4221e-01,  3.2140e-02,  5.8871e-01, -7.1994e-01],
        [ 7.6949e+00, -7.9966e+00, -2.9331e+00,  8.9151e+00,  6.8448e+00,
         -1.0457e+01, -2.7122e+00, -1.1690e-02,  1.3133e+01,  7.2548e+00],
        [-8.4314e-01, -2.7583e-01, -5.2796e-01, -3.3885e-01, -7.0396e-01,
          4.1136e-01, -5.1515e-01,  3.2070e-02,  6.3446e-01, -7.1984e-01],
        [-8.7545e-01, -1.8790e-01, -5.6487e-01, -3.4600e-01, -6.9739e-01,
          3.7464e-01, -5.4221e-01,  3.2140e-02,  5.8871e-01, -7.1994e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.4293, -2.5267, -3.6488, -0.4293, -2.9430,  1.6035, -0.4293, -2.9746,
        -0.4639, -0.4293], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.8082, -10.1832,  -3.2149,   0.8082,  -0.8132,   5.2596,   0.8082,
         -11.4912,   0.8258,   0.8082]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  3.0783,   0.8964],
        [-14.6525,  -2.0758],
        [  1.6301,   8.0028],
        [ 14.9016,   8.0964],
        [ -5.6490,   5.4903],
        [ -2.5963,   0.6866],
        [-18.2423,  -4.8271],
        [-11.6306,  -0.7732],
        [ -4.9192,   2.4612],
        [-16.2756,  -5.9141]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ 2.3492,  6.9439,  8.3469,  4.3620,  7.0181,  9.9416,  0.8214, 10.2048,
        14.7768, -3.7854], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -1.0533,   5.8213,  -0.3231,  -1.0407,  -2.6497,   1.4215,   8.1503,
          -3.7469, -11.1546,   4.7282],
        [ -9.3408,   6.3723, -11.7171,   0.2503, -11.8282,   3.6019,   0.8748,
           0.0333,   3.7791,   2.3541],
        [ -1.1234,   3.5229,  -5.1065,  -5.4431,  -2.1068,  -1.2106,   5.3574,
           0.4172,  -1.3392,   2.7667],
        [  4.6982,   6.8568, -18.8271,   0.2364,  -4.3267,   5.0095,   8.4197,
         -15.9242,  -9.5957,  19.7228],
        [  0.3008,   3.6102,   1.3937,   0.3944,  -1.0210,   0.5375,  -0.3229,
           5.3824,   3.8972,  -0.4799],
        [ -1.6909,   0.7458,  -0.9299,  -0.7475,   6.5016,  -1.5767,  -5.0757,
           2.9758,  -1.6176,  -1.9790],
        [ -1.1482,   2.9799,  -4.7475,  -3.0386, -12.2173,  -0.7308,   8.7847,
           1.1321,  -1.1045,  -7.1571],
        [  0.3341,  -1.2411,   2.1250,  -2.5667,   0.8983,   1.6474,   2.0891,
           7.2109,   1.3848,   1.2461],
        [ -0.6656,  -0.3226,  -0.4467,   0.9112,  -5.4214,  -0.4457,  -2.9364,
          -0.2594,  -0.4798,  -0.9727],
        [  1.4423,  -5.5206,  -9.0089,  -6.9763,  -0.0386,   5.0253,   4.9775,
           4.8801,   1.1648,  -2.1715]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -2.7967,  -2.0878,  -1.4241,   2.9126,   0.4561,  -1.8935,  -0.1975,
         -1.5031,  -0.9869, -10.0555], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -4.5713,  -0.7200,  -1.8536,  12.1246,   1.4627,   0.8526,  -4.9382,
          -1.3506,   1.0987,   1.7623],
        [  4.5671,   0.7187,   1.8564, -12.1349,  -1.4576,  -0.8538,   4.9385,
           1.4071,  -1.0990,  -1.8815]], device='cuda:0'))])
xi:  [-74.10404]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 375.55232223989316
W_T_median: 145.04371748614602
W_T_pctile_5: -74.24949319073278
W_T_CVAR_5_pct: -162.8470330917183
Average q (qsum/M+1):  54.381646925403224
Optimal xi:  [-74.10404]
Observed VAR:  145.04371748614602
Expected(across Rb) median(across samples) p_equity:  0.3870553091168404
obj fun:  tensor(-1604.4073, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:158: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
